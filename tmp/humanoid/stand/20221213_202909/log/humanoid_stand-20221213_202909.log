[32m[20221213 20:29:09 @logger.py:105][0m Log file set to ./tmp/humanoid/stand/20221213_202909/log/humanoid_stand-20221213_202909.log
[32m[20221213 20:29:09 @agent_ppo2.py:121][0m #------------------------ Iteration 0 --------------------------#
[32m[20221213 20:29:10 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:29:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:10 @agent_ppo2.py:185][0m |           0.0032 |           0.1201 |           0.0000 |
[32m[20221213 20:29:10 @agent_ppo2.py:185][0m |          -0.0082 |           0.0996 |           0.0000 |
[32m[20221213 20:29:10 @agent_ppo2.py:185][0m |          -0.0472 |           0.0956 |           0.0000 |
[32m[20221213 20:29:10 @agent_ppo2.py:185][0m |          -0.0151 |           0.0737 |           0.0000 |
[32m[20221213 20:29:10 @agent_ppo2.py:185][0m |          -0.0160 |           0.0663 |           0.0000 |
[32m[20221213 20:29:10 @agent_ppo2.py:185][0m |          -0.0162 |           0.0606 |           0.0000 |
[32m[20221213 20:29:10 @agent_ppo2.py:185][0m |          -0.0238 |           0.0567 |           0.0000 |
[32m[20221213 20:29:10 @agent_ppo2.py:185][0m |          -0.0162 |           0.0531 |           0.0000 |
[32m[20221213 20:29:10 @agent_ppo2.py:185][0m |          -0.0168 |           0.0506 |           0.0000 |
[32m[20221213 20:29:10 @agent_ppo2.py:185][0m |          -0.0182 |           0.0488 |           0.0000 |
[32m[20221213 20:29:10 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:29:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.07
[32m[20221213 20:29:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.28
[32m[20221213 20:29:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.08
[32m[20221213 20:29:11 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 3.08
[32m[20221213 20:29:11 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 3.08
[32m[20221213 20:29:11 @agent_ppo2.py:143][0m Total time:       0.03 min
[32m[20221213 20:29:11 @agent_ppo2.py:145][0m 4096 total steps have happened
[32m[20221213 20:29:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1 --------------------------#
[32m[20221213 20:29:11 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:29:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:11 @agent_ppo2.py:185][0m |           0.0088 |           0.0504 |           0.0000 |
[32m[20221213 20:29:11 @agent_ppo2.py:185][0m |          -0.0074 |           0.0438 |           0.0000 |
[32m[20221213 20:29:12 @agent_ppo2.py:185][0m |          -0.0130 |           0.0387 |           0.0000 |
[32m[20221213 20:29:12 @agent_ppo2.py:185][0m |          -0.0191 |           0.0326 |           0.0000 |
[32m[20221213 20:29:12 @agent_ppo2.py:185][0m |          -0.0564 |           0.0326 |           0.0000 |
[32m[20221213 20:29:12 @agent_ppo2.py:185][0m |          -0.0089 |           0.0291 |           0.0000 |
[32m[20221213 20:29:12 @agent_ppo2.py:185][0m |          -0.0127 |           0.0264 |           0.0000 |
[32m[20221213 20:29:12 @agent_ppo2.py:185][0m |          -0.0194 |           0.0245 |           0.0000 |
[32m[20221213 20:29:12 @agent_ppo2.py:185][0m |          -0.0214 |           0.0238 |           0.0000 |
[32m[20221213 20:29:12 @agent_ppo2.py:185][0m |          -0.0199 |           0.0217 |           0.0000 |
[32m[20221213 20:29:12 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:29:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.13
[32m[20221213 20:29:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.25
[32m[20221213 20:29:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.36
[32m[20221213 20:29:12 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 7.36
[32m[20221213 20:29:12 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 7.36
[32m[20221213 20:29:12 @agent_ppo2.py:143][0m Total time:       0.05 min
[32m[20221213 20:29:12 @agent_ppo2.py:145][0m 8192 total steps have happened
[32m[20221213 20:29:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2 --------------------------#
[32m[20221213 20:29:13 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:29:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:13 @agent_ppo2.py:185][0m |           0.0107 |           0.1946 |           0.0000 |
[32m[20221213 20:29:13 @agent_ppo2.py:185][0m |          -0.0056 |           0.1715 |           0.0000 |
[32m[20221213 20:29:13 @agent_ppo2.py:185][0m |          -0.0140 |           0.1563 |           0.0000 |
[32m[20221213 20:29:13 @agent_ppo2.py:185][0m |          -0.0158 |           0.1543 |           0.0000 |
[32m[20221213 20:29:13 @agent_ppo2.py:185][0m |          -0.0234 |           0.1472 |           0.0000 |
[32m[20221213 20:29:13 @agent_ppo2.py:185][0m |          -0.0229 |           0.1388 |           0.0000 |
[32m[20221213 20:29:13 @agent_ppo2.py:185][0m |          -0.0209 |           0.1457 |           0.0000 |
[32m[20221213 20:29:14 @agent_ppo2.py:185][0m |          -0.0238 |           0.1452 |           0.0000 |
[32m[20221213 20:29:14 @agent_ppo2.py:185][0m |          -0.0249 |           0.1304 |           0.0000 |
[32m[20221213 20:29:14 @agent_ppo2.py:185][0m |          -0.0234 |           0.1281 |           0.0000 |
[32m[20221213 20:29:14 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:29:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.70
[32m[20221213 20:29:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.40
[32m[20221213 20:29:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.48
[32m[20221213 20:29:14 @agent_ppo2.py:143][0m Total time:       0.08 min
[32m[20221213 20:29:14 @agent_ppo2.py:145][0m 12288 total steps have happened
[32m[20221213 20:29:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3 --------------------------#
[32m[20221213 20:29:14 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:29:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:15 @agent_ppo2.py:185][0m |           0.0062 |           0.0410 |           0.0000 |
[32m[20221213 20:29:15 @agent_ppo2.py:185][0m |          -0.0241 |           0.0208 |           0.0000 |
[32m[20221213 20:29:15 @agent_ppo2.py:185][0m |           0.0033 |           0.0217 |           0.0000 |
[32m[20221213 20:29:15 @agent_ppo2.py:185][0m |          -0.0375 |           0.0203 |           0.0000 |
[32m[20221213 20:29:15 @agent_ppo2.py:185][0m |          -0.0356 |           0.0195 |           0.0000 |
[32m[20221213 20:29:15 @agent_ppo2.py:185][0m |          -0.0478 |           0.0195 |           0.0000 |
[32m[20221213 20:29:15 @agent_ppo2.py:185][0m |          -0.0428 |           0.0192 |           0.0000 |
[32m[20221213 20:29:15 @agent_ppo2.py:185][0m |          -0.0543 |           0.0189 |           0.0000 |
[32m[20221213 20:29:15 @agent_ppo2.py:185][0m |          -0.0575 |           0.0205 |           0.0000 |
[32m[20221213 20:29:15 @agent_ppo2.py:185][0m |          -0.0498 |           0.0193 |           0.0000 |
[32m[20221213 20:29:15 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:29:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.88
[32m[20221213 20:29:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.06
[32m[20221213 20:29:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.66
[32m[20221213 20:29:16 @agent_ppo2.py:143][0m Total time:       0.11 min
[32m[20221213 20:29:16 @agent_ppo2.py:145][0m 16384 total steps have happened
[32m[20221213 20:29:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4 --------------------------#
[32m[20221213 20:29:16 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:29:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:16 @agent_ppo2.py:185][0m |          -0.0045 |           0.0142 |           0.0000 |
[32m[20221213 20:29:16 @agent_ppo2.py:185][0m |          -0.0310 |           0.0117 |           0.0000 |
[32m[20221213 20:29:16 @agent_ppo2.py:185][0m |          -0.0262 |           0.0116 |           0.0000 |
[32m[20221213 20:29:16 @agent_ppo2.py:185][0m |          -0.0369 |           0.0114 |           0.0000 |
[32m[20221213 20:29:17 @agent_ppo2.py:185][0m |          -0.0458 |           0.0114 |           0.0000 |
[32m[20221213 20:29:17 @agent_ppo2.py:185][0m |          -0.0460 |           0.0112 |           0.0000 |
[32m[20221213 20:29:17 @agent_ppo2.py:185][0m |          -0.0536 |           0.0112 |           0.0000 |
[32m[20221213 20:29:17 @agent_ppo2.py:185][0m |          -0.0548 |           0.0110 |           0.0000 |
[32m[20221213 20:29:17 @agent_ppo2.py:185][0m |          -0.0718 |           0.0109 |           0.0000 |
[32m[20221213 20:29:17 @agent_ppo2.py:185][0m |          -0.0550 |           0.0108 |           0.0000 |
[32m[20221213 20:29:17 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:29:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.23
[32m[20221213 20:29:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.85
[32m[20221213 20:29:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.98
[32m[20221213 20:29:17 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 8.98
[32m[20221213 20:29:17 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 8.98
[32m[20221213 20:29:17 @agent_ppo2.py:143][0m Total time:       0.13 min
[32m[20221213 20:29:17 @agent_ppo2.py:145][0m 20480 total steps have happened
[32m[20221213 20:29:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5 --------------------------#
[32m[20221213 20:29:18 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:29:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:18 @agent_ppo2.py:185][0m |          -0.0065 |           0.0085 |           0.0000 |
[32m[20221213 20:29:18 @agent_ppo2.py:185][0m |          -0.0253 |           0.0061 |           0.0000 |
[32m[20221213 20:29:18 @agent_ppo2.py:185][0m |          -0.0405 |           0.0060 |           0.0000 |
[32m[20221213 20:29:18 @agent_ppo2.py:185][0m |          -0.0182 |           0.0060 |           0.0000 |
[32m[20221213 20:29:18 @agent_ppo2.py:185][0m |          -0.0239 |           0.0058 |           0.0000 |
[32m[20221213 20:29:18 @agent_ppo2.py:185][0m |          -0.0428 |           0.0058 |           0.0000 |
[32m[20221213 20:29:18 @agent_ppo2.py:185][0m |          -0.0569 |           0.0057 |           0.0000 |
[32m[20221213 20:29:18 @agent_ppo2.py:185][0m |          -0.0604 |           0.0057 |           0.0000 |
[32m[20221213 20:29:18 @agent_ppo2.py:185][0m |          -0.0603 |           0.0057 |           0.0000 |
[32m[20221213 20:29:19 @agent_ppo2.py:185][0m |          -0.0732 |           0.0057 |           0.0000 |
[32m[20221213 20:29:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:29:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.99
[32m[20221213 20:29:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.49
[32m[20221213 20:29:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.37
[32m[20221213 20:29:19 @agent_ppo2.py:143][0m Total time:       0.16 min
[32m[20221213 20:29:19 @agent_ppo2.py:145][0m 24576 total steps have happened
[32m[20221213 20:29:19 @agent_ppo2.py:121][0m #------------------------ Iteration 6 --------------------------#
[32m[20221213 20:29:19 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:29:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:19 @agent_ppo2.py:185][0m |           0.0019 |           0.0045 |           0.0000 |
[32m[20221213 20:29:20 @agent_ppo2.py:185][0m |          -0.0345 |           0.0037 |           0.0000 |
[32m[20221213 20:29:20 @agent_ppo2.py:185][0m |          -0.0453 |           0.0036 |           0.0000 |
[32m[20221213 20:29:20 @agent_ppo2.py:185][0m |          -0.0442 |           0.0035 |           0.0000 |
[32m[20221213 20:29:20 @agent_ppo2.py:185][0m |          -0.0333 |           0.0034 |           0.0000 |
[32m[20221213 20:29:20 @agent_ppo2.py:185][0m |          -0.0544 |           0.0034 |           0.0000 |
[32m[20221213 20:29:20 @agent_ppo2.py:185][0m |          -0.0522 |           0.0034 |           0.0000 |
[32m[20221213 20:29:20 @agent_ppo2.py:185][0m |          -0.0583 |           0.0033 |           0.0000 |
[32m[20221213 20:29:20 @agent_ppo2.py:185][0m |          -0.0573 |           0.0033 |           0.0000 |
[32m[20221213 20:29:20 @agent_ppo2.py:185][0m |          -0.0589 |           0.0032 |           0.0000 |
[32m[20221213 20:29:20 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:29:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.45
[32m[20221213 20:29:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.49
[32m[20221213 20:29:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.54
[32m[20221213 20:29:21 @agent_ppo2.py:143][0m Total time:       0.19 min
[32m[20221213 20:29:21 @agent_ppo2.py:145][0m 28672 total steps have happened
[32m[20221213 20:29:21 @agent_ppo2.py:121][0m #------------------------ Iteration 7 --------------------------#
[32m[20221213 20:29:21 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:29:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:21 @agent_ppo2.py:185][0m |          -0.0396 |           0.0657 |           0.0000 |
[32m[20221213 20:29:21 @agent_ppo2.py:185][0m |          -0.0097 |           0.0337 |           0.0000 |
[32m[20221213 20:29:21 @agent_ppo2.py:185][0m |          -0.0138 |           0.0278 |           0.0000 |
[32m[20221213 20:29:21 @agent_ppo2.py:185][0m |          -0.0200 |           0.0281 |           0.0000 |
[32m[20221213 20:29:21 @agent_ppo2.py:185][0m |          -0.0565 |           0.0311 |           0.0000 |
[32m[20221213 20:29:21 @agent_ppo2.py:185][0m |          -0.0209 |           0.0285 |           0.0000 |
[32m[20221213 20:29:22 @agent_ppo2.py:185][0m |          -0.0212 |           0.0258 |           0.0000 |
[32m[20221213 20:29:22 @agent_ppo2.py:185][0m |          -0.0220 |           0.0253 |           0.0000 |
[32m[20221213 20:29:22 @agent_ppo2.py:185][0m |          -0.0172 |           0.0264 |           0.0000 |
[32m[20221213 20:29:22 @agent_ppo2.py:185][0m |          -0.0193 |           0.0256 |           0.0000 |
[32m[20221213 20:29:22 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:29:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.17
[32m[20221213 20:29:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.74
[32m[20221213 20:29:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.66
[32m[20221213 20:29:22 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 11.66
[32m[20221213 20:29:22 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 11.66
[32m[20221213 20:29:22 @agent_ppo2.py:143][0m Total time:       0.22 min
[32m[20221213 20:29:22 @agent_ppo2.py:145][0m 32768 total steps have happened
[32m[20221213 20:29:22 @agent_ppo2.py:121][0m #------------------------ Iteration 8 --------------------------#
[32m[20221213 20:29:23 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:29:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:23 @agent_ppo2.py:185][0m |          -0.0036 |           0.0133 |           0.0000 |
[32m[20221213 20:29:23 @agent_ppo2.py:185][0m |          -0.0210 |           0.0116 |           0.0000 |
[32m[20221213 20:29:23 @agent_ppo2.py:185][0m |          -0.0458 |           0.0113 |           0.0000 |
[32m[20221213 20:29:23 @agent_ppo2.py:185][0m |          -0.0474 |           0.0111 |           0.0000 |
[32m[20221213 20:29:23 @agent_ppo2.py:185][0m |          -0.0489 |           0.0110 |           0.0000 |
[32m[20221213 20:29:23 @agent_ppo2.py:185][0m |          -0.0604 |           0.0110 |           0.0000 |
[32m[20221213 20:29:23 @agent_ppo2.py:185][0m |          -0.0510 |           0.0110 |           0.0000 |
[32m[20221213 20:29:23 @agent_ppo2.py:185][0m |          -0.0564 |           0.0109 |           0.0000 |
[32m[20221213 20:29:23 @agent_ppo2.py:185][0m |          -0.0570 |           0.0109 |           0.0000 |
[32m[20221213 20:29:23 @agent_ppo2.py:185][0m |          -0.0594 |           0.0107 |           0.0000 |
[32m[20221213 20:29:23 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:29:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.29
[32m[20221213 20:29:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.62
[32m[20221213 20:29:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.97
[32m[20221213 20:29:24 @agent_ppo2.py:143][0m Total time:       0.24 min
[32m[20221213 20:29:24 @agent_ppo2.py:145][0m 36864 total steps have happened
[32m[20221213 20:29:24 @agent_ppo2.py:121][0m #------------------------ Iteration 9 --------------------------#
[32m[20221213 20:29:24 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:29:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:24 @agent_ppo2.py:185][0m |           0.0104 |           0.0769 |           0.0000 |
[32m[20221213 20:29:24 @agent_ppo2.py:185][0m |          -0.0385 |           0.0610 |           0.0000 |
[32m[20221213 20:29:25 @agent_ppo2.py:185][0m |           0.0121 |           0.0571 |           0.0000 |
[32m[20221213 20:29:25 @agent_ppo2.py:185][0m |          -0.0073 |           0.0538 |           0.0000 |
[32m[20221213 20:29:25 @agent_ppo2.py:185][0m |          -0.0110 |           0.0548 |           0.0000 |
[32m[20221213 20:29:25 @agent_ppo2.py:185][0m |          -0.0356 |           0.0554 |           0.0000 |
[32m[20221213 20:29:25 @agent_ppo2.py:185][0m |          -0.0210 |           0.0512 |           0.0000 |
[32m[20221213 20:29:25 @agent_ppo2.py:185][0m |          -0.0198 |           0.0495 |           0.0000 |
[32m[20221213 20:29:25 @agent_ppo2.py:185][0m |          -0.0226 |           0.0515 |           0.0000 |
[32m[20221213 20:29:25 @agent_ppo2.py:185][0m |          -0.0219 |           0.0512 |           0.0000 |
[32m[20221213 20:29:25 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:29:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.04
[32m[20221213 20:29:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.08
[32m[20221213 20:29:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.69
[32m[20221213 20:29:25 @agent_ppo2.py:143][0m Total time:       0.27 min
[32m[20221213 20:29:25 @agent_ppo2.py:145][0m 40960 total steps have happened
[32m[20221213 20:29:25 @agent_ppo2.py:121][0m #------------------------ Iteration 10 --------------------------#
[32m[20221213 20:29:26 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:29:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:26 @agent_ppo2.py:185][0m |           0.0110 |           0.0230 |           0.0000 |
[32m[20221213 20:29:26 @agent_ppo2.py:185][0m |          -0.0188 |           0.0138 |           0.0000 |
[32m[20221213 20:29:26 @agent_ppo2.py:185][0m |          -0.0139 |           0.0139 |           0.0000 |
[32m[20221213 20:29:26 @agent_ppo2.py:185][0m |          -0.0420 |           0.0137 |           0.0000 |
[32m[20221213 20:29:26 @agent_ppo2.py:185][0m |          -0.0474 |           0.0134 |           0.0000 |
[32m[20221213 20:29:26 @agent_ppo2.py:185][0m |          -0.0547 |           0.0133 |           0.0000 |
[32m[20221213 20:29:26 @agent_ppo2.py:185][0m |          -0.0422 |           0.0135 |           0.0000 |
[32m[20221213 20:29:27 @agent_ppo2.py:185][0m |          -0.0652 |           0.0130 |           0.0000 |
[32m[20221213 20:29:27 @agent_ppo2.py:185][0m |          -0.0531 |           0.0131 |           0.0000 |
[32m[20221213 20:29:27 @agent_ppo2.py:185][0m |          -0.0511 |           0.0129 |           0.0000 |
[32m[20221213 20:29:27 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:29:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.91
[32m[20221213 20:29:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.68
[32m[20221213 20:29:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.11
[32m[20221213 20:29:27 @agent_ppo2.py:143][0m Total time:       0.30 min
[32m[20221213 20:29:27 @agent_ppo2.py:145][0m 45056 total steps have happened
[32m[20221213 20:29:27 @agent_ppo2.py:121][0m #------------------------ Iteration 11 --------------------------#
[32m[20221213 20:29:28 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:29:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:28 @agent_ppo2.py:185][0m |           0.0039 |           0.0613 |           0.0000 |
[32m[20221213 20:29:28 @agent_ppo2.py:185][0m |          -0.0116 |           0.0531 |           0.0000 |
[32m[20221213 20:29:28 @agent_ppo2.py:185][0m |          -0.0189 |           0.0515 |           0.0000 |
[32m[20221213 20:29:28 @agent_ppo2.py:185][0m |          -0.0210 |           0.0504 |           0.0000 |
[32m[20221213 20:29:28 @agent_ppo2.py:185][0m |          -0.0290 |           0.0508 |           0.0000 |
[32m[20221213 20:29:28 @agent_ppo2.py:185][0m |          -0.0274 |           0.0508 |           0.0000 |
[32m[20221213 20:29:28 @agent_ppo2.py:185][0m |          -0.0244 |           0.0485 |           0.0000 |
[32m[20221213 20:29:29 @agent_ppo2.py:185][0m |          -0.0256 |           0.0485 |           0.0000 |
[32m[20221213 20:29:29 @agent_ppo2.py:185][0m |          -0.0259 |           0.0482 |           0.0000 |
[32m[20221213 20:29:29 @agent_ppo2.py:185][0m |          -0.0309 |           0.0459 |           0.0000 |
[32m[20221213 20:29:29 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:29:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.63
[32m[20221213 20:29:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.90
[32m[20221213 20:29:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.17
[32m[20221213 20:29:29 @agent_ppo2.py:143][0m Total time:       0.33 min
[32m[20221213 20:29:29 @agent_ppo2.py:145][0m 49152 total steps have happened
[32m[20221213 20:29:29 @agent_ppo2.py:121][0m #------------------------ Iteration 12 --------------------------#
[32m[20221213 20:29:29 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:29:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:30 @agent_ppo2.py:185][0m |           0.0028 |           0.0900 |           0.0000 |
[32m[20221213 20:29:30 @agent_ppo2.py:185][0m |          -0.0145 |           0.0826 |           0.0000 |
[32m[20221213 20:29:30 @agent_ppo2.py:185][0m |          -0.0168 |           0.0765 |           0.0000 |
[32m[20221213 20:29:30 @agent_ppo2.py:185][0m |          -0.0253 |           0.0753 |           0.0000 |
[32m[20221213 20:29:30 @agent_ppo2.py:185][0m |          -0.0629 |           0.0787 |           0.0000 |
[32m[20221213 20:29:30 @agent_ppo2.py:185][0m |          -0.0580 |           0.0773 |           0.0000 |
[32m[20221213 20:29:30 @agent_ppo2.py:185][0m |          -0.0339 |           0.0772 |           0.0000 |
[32m[20221213 20:29:30 @agent_ppo2.py:185][0m |          -0.0347 |           0.0687 |           0.0000 |
[32m[20221213 20:29:30 @agent_ppo2.py:185][0m |          -0.0379 |           0.0690 |           0.0000 |
[32m[20221213 20:29:30 @agent_ppo2.py:185][0m |          -0.0464 |           0.0680 |           0.0000 |
[32m[20221213 20:29:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:29:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.45
[32m[20221213 20:29:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.61
[32m[20221213 20:29:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.70
[32m[20221213 20:29:31 @agent_ppo2.py:143][0m Total time:       0.36 min
[32m[20221213 20:29:31 @agent_ppo2.py:145][0m 53248 total steps have happened
[32m[20221213 20:29:31 @agent_ppo2.py:121][0m #------------------------ Iteration 13 --------------------------#
[32m[20221213 20:29:31 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:29:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:31 @agent_ppo2.py:185][0m |           0.0154 |           0.0841 |           0.0000 |
[32m[20221213 20:29:31 @agent_ppo2.py:185][0m |          -0.0002 |           0.0811 |           0.0000 |
[32m[20221213 20:29:31 @agent_ppo2.py:185][0m |          -0.0063 |           0.0802 |           0.0000 |
[32m[20221213 20:29:31 @agent_ppo2.py:185][0m |          -0.0238 |           0.0779 |           0.0000 |
[32m[20221213 20:29:32 @agent_ppo2.py:185][0m |          -0.0394 |           0.0754 |           0.0000 |
[32m[20221213 20:29:32 @agent_ppo2.py:185][0m |          -0.0369 |           0.0751 |           0.0000 |
[32m[20221213 20:29:32 @agent_ppo2.py:185][0m |          -0.0502 |           0.0739 |           0.0000 |
[32m[20221213 20:29:32 @agent_ppo2.py:185][0m |          -0.0512 |           0.0715 |           0.0000 |
[32m[20221213 20:29:32 @agent_ppo2.py:185][0m |          -0.0781 |           0.0739 |           0.0000 |
[32m[20221213 20:29:32 @agent_ppo2.py:185][0m |          -0.0914 |           0.0774 |           0.0000 |
[32m[20221213 20:29:32 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:29:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.60
[32m[20221213 20:29:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.79
[32m[20221213 20:29:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.59
[32m[20221213 20:29:32 @agent_ppo2.py:143][0m Total time:       0.39 min
[32m[20221213 20:29:32 @agent_ppo2.py:145][0m 57344 total steps have happened
[32m[20221213 20:29:32 @agent_ppo2.py:121][0m #------------------------ Iteration 14 --------------------------#
[32m[20221213 20:29:33 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:29:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:33 @agent_ppo2.py:185][0m |           0.0156 |           0.0606 |           0.0000 |
[32m[20221213 20:29:33 @agent_ppo2.py:185][0m |          -0.0206 |           0.0182 |           0.0000 |
[32m[20221213 20:29:33 @agent_ppo2.py:185][0m |          -0.0341 |           0.0178 |           0.0000 |
[32m[20221213 20:29:33 @agent_ppo2.py:185][0m |          -0.0436 |           0.0173 |           0.0000 |
[32m[20221213 20:29:33 @agent_ppo2.py:185][0m |          -0.0374 |           0.0168 |           0.0000 |
[32m[20221213 20:29:33 @agent_ppo2.py:185][0m |          -0.0501 |           0.0168 |           0.0000 |
[32m[20221213 20:29:33 @agent_ppo2.py:185][0m |          -0.0544 |           0.0168 |           0.0000 |
[32m[20221213 20:29:33 @agent_ppo2.py:185][0m |          -0.0561 |           0.0167 |           0.0000 |
[32m[20221213 20:29:33 @agent_ppo2.py:185][0m |          -0.0545 |           0.0165 |           0.0000 |
[32m[20221213 20:29:34 @agent_ppo2.py:185][0m |          -0.0571 |           0.0164 |           0.0000 |
[32m[20221213 20:29:34 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:29:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.68
[32m[20221213 20:29:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.20
[32m[20221213 20:29:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.71
[32m[20221213 20:29:34 @agent_ppo2.py:143][0m Total time:       0.41 min
[32m[20221213 20:29:34 @agent_ppo2.py:145][0m 61440 total steps have happened
[32m[20221213 20:29:34 @agent_ppo2.py:121][0m #------------------------ Iteration 15 --------------------------#
[32m[20221213 20:29:34 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:29:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:34 @agent_ppo2.py:185][0m |           0.0117 |           0.1541 |           0.0000 |
[32m[20221213 20:29:35 @agent_ppo2.py:185][0m |          -0.0035 |           0.1376 |           0.0000 |
[32m[20221213 20:29:35 @agent_ppo2.py:185][0m |          -0.0176 |           0.1368 |           0.0000 |
[32m[20221213 20:29:35 @agent_ppo2.py:185][0m |          -0.0248 |           0.1296 |           0.0000 |
[32m[20221213 20:29:35 @agent_ppo2.py:185][0m |          -0.0238 |           0.1283 |           0.0000 |
[32m[20221213 20:29:35 @agent_ppo2.py:185][0m |          -0.0267 |           0.1290 |           0.0000 |
[32m[20221213 20:29:35 @agent_ppo2.py:185][0m |          -0.0300 |           0.1256 |           0.0000 |
[32m[20221213 20:29:35 @agent_ppo2.py:185][0m |          -0.0346 |           0.1305 |           0.0000 |
[32m[20221213 20:29:35 @agent_ppo2.py:185][0m |          -0.0285 |           0.1283 |           0.0000 |
[32m[20221213 20:29:35 @agent_ppo2.py:185][0m |          -0.0306 |           0.1238 |           0.0000 |
[32m[20221213 20:29:35 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:29:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.25
[32m[20221213 20:29:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.42
[32m[20221213 20:29:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.53
[32m[20221213 20:29:35 @agent_ppo2.py:143][0m Total time:       0.44 min
[32m[20221213 20:29:35 @agent_ppo2.py:145][0m 65536 total steps have happened
[32m[20221213 20:29:35 @agent_ppo2.py:121][0m #------------------------ Iteration 16 --------------------------#
[32m[20221213 20:29:36 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:29:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:36 @agent_ppo2.py:185][0m |           0.0025 |           0.0294 |           0.0000 |
[32m[20221213 20:29:36 @agent_ppo2.py:185][0m |          -0.0133 |           0.0201 |           0.0000 |
[32m[20221213 20:29:36 @agent_ppo2.py:185][0m |          -0.0365 |           0.0195 |           0.0000 |
[32m[20221213 20:29:36 @agent_ppo2.py:185][0m |          -0.0307 |           0.0190 |           0.0000 |
[32m[20221213 20:29:36 @agent_ppo2.py:185][0m |          -0.0534 |           0.0189 |           0.0000 |
[32m[20221213 20:29:36 @agent_ppo2.py:185][0m |          -0.0556 |           0.0189 |           0.0000 |
[32m[20221213 20:29:37 @agent_ppo2.py:185][0m |          -0.0621 |           0.0187 |           0.0000 |
[32m[20221213 20:29:37 @agent_ppo2.py:185][0m |          -0.0646 |           0.0186 |           0.0000 |
[32m[20221213 20:29:37 @agent_ppo2.py:185][0m |          -0.0718 |           0.0187 |           0.0000 |
[32m[20221213 20:29:37 @agent_ppo2.py:185][0m |          -0.0811 |           0.0187 |           0.0000 |
[32m[20221213 20:29:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:29:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.11
[32m[20221213 20:29:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.48
[32m[20221213 20:29:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.76
[32m[20221213 20:29:37 @agent_ppo2.py:143][0m Total time:       0.47 min
[32m[20221213 20:29:37 @agent_ppo2.py:145][0m 69632 total steps have happened
[32m[20221213 20:29:37 @agent_ppo2.py:121][0m #------------------------ Iteration 17 --------------------------#
[32m[20221213 20:29:38 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:29:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:38 @agent_ppo2.py:185][0m |           0.0136 |           0.1284 |           0.0000 |
[32m[20221213 20:29:38 @agent_ppo2.py:185][0m |          -0.0073 |           0.1048 |           0.0000 |
[32m[20221213 20:29:38 @agent_ppo2.py:185][0m |          -0.0158 |           0.1042 |           0.0000 |
[32m[20221213 20:29:38 @agent_ppo2.py:185][0m |          -0.0239 |           0.1006 |           0.0000 |
[32m[20221213 20:29:38 @agent_ppo2.py:185][0m |          -0.0218 |           0.0989 |           0.0000 |
[32m[20221213 20:29:38 @agent_ppo2.py:185][0m |          -0.0268 |           0.0965 |           0.0000 |
[32m[20221213 20:29:38 @agent_ppo2.py:185][0m |          -0.0228 |           0.1014 |           0.0000 |
[32m[20221213 20:29:38 @agent_ppo2.py:185][0m |          -0.0238 |           0.0977 |           0.0000 |
[32m[20221213 20:29:38 @agent_ppo2.py:185][0m |          -0.0293 |           0.0941 |           0.0000 |
[32m[20221213 20:29:38 @agent_ppo2.py:185][0m |          -0.0327 |           0.0931 |           0.0000 |
[32m[20221213 20:29:38 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:29:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.95
[32m[20221213 20:29:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.68
[32m[20221213 20:29:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.13
[32m[20221213 20:29:39 @agent_ppo2.py:143][0m Total time:       0.49 min
[32m[20221213 20:29:39 @agent_ppo2.py:145][0m 73728 total steps have happened
[32m[20221213 20:29:39 @agent_ppo2.py:121][0m #------------------------ Iteration 18 --------------------------#
[32m[20221213 20:29:39 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:29:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:39 @agent_ppo2.py:185][0m |           0.0093 |           0.0640 |           0.0000 |
[32m[20221213 20:29:39 @agent_ppo2.py:185][0m |          -0.0305 |           0.0577 |           0.0000 |
[32m[20221213 20:29:39 @agent_ppo2.py:185][0m |          -0.0330 |           0.0573 |           0.0000 |
[32m[20221213 20:29:40 @agent_ppo2.py:185][0m |          -0.0266 |           0.0558 |           0.0000 |
[32m[20221213 20:29:40 @agent_ppo2.py:185][0m |          -0.0456 |           0.0539 |           0.0000 |
[32m[20221213 20:29:40 @agent_ppo2.py:185][0m |          -0.0507 |           0.0532 |           0.0000 |
[32m[20221213 20:29:40 @agent_ppo2.py:185][0m |          -0.0310 |           0.0532 |           0.0000 |
[32m[20221213 20:29:40 @agent_ppo2.py:185][0m |          -0.0547 |           0.0533 |           0.0000 |
[32m[20221213 20:29:40 @agent_ppo2.py:185][0m |          -0.0242 |           0.0518 |           0.0000 |
[32m[20221213 20:29:40 @agent_ppo2.py:185][0m |          -0.0259 |           0.0526 |           0.0000 |
[32m[20221213 20:29:40 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:29:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.86
[32m[20221213 20:29:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.11
[32m[20221213 20:29:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.69
[32m[20221213 20:29:41 @agent_ppo2.py:143][0m Total time:       0.52 min
[32m[20221213 20:29:41 @agent_ppo2.py:145][0m 77824 total steps have happened
[32m[20221213 20:29:41 @agent_ppo2.py:121][0m #------------------------ Iteration 19 --------------------------#
[32m[20221213 20:29:41 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:29:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:41 @agent_ppo2.py:185][0m |           0.0264 |           0.0426 |           0.0000 |
[32m[20221213 20:29:41 @agent_ppo2.py:185][0m |           0.0104 |           0.0266 |           0.0000 |
[32m[20221213 20:29:41 @agent_ppo2.py:185][0m |           0.0263 |           0.0263 |           0.0000 |
[32m[20221213 20:29:41 @agent_ppo2.py:185][0m |          -0.0148 |           0.0258 |           0.0000 |
[32m[20221213 20:29:41 @agent_ppo2.py:185][0m |          -0.0367 |           0.0255 |           0.0000 |
[32m[20221213 20:29:41 @agent_ppo2.py:185][0m |          -0.0398 |           0.0253 |           0.0000 |
[32m[20221213 20:29:42 @agent_ppo2.py:185][0m |          -0.0463 |           0.0248 |           0.0000 |
[32m[20221213 20:29:42 @agent_ppo2.py:185][0m |          -0.0549 |           0.0247 |           0.0000 |
[32m[20221213 20:29:42 @agent_ppo2.py:185][0m |          -0.0654 |           0.0273 |           0.0000 |
[32m[20221213 20:29:42 @agent_ppo2.py:185][0m |          -0.0484 |           0.0290 |           0.0000 |
[32m[20221213 20:29:42 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:29:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.63
[32m[20221213 20:29:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.65
[32m[20221213 20:29:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.50
[32m[20221213 20:29:42 @agent_ppo2.py:143][0m Total time:       0.55 min
[32m[20221213 20:29:42 @agent_ppo2.py:145][0m 81920 total steps have happened
[32m[20221213 20:29:42 @agent_ppo2.py:121][0m #------------------------ Iteration 20 --------------------------#
[32m[20221213 20:29:43 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:29:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:43 @agent_ppo2.py:185][0m |           0.0037 |           0.0620 |           0.0000 |
[32m[20221213 20:29:43 @agent_ppo2.py:185][0m |          -0.0248 |           0.0530 |           0.0000 |
[32m[20221213 20:29:43 @agent_ppo2.py:185][0m |          -0.0338 |           0.0541 |           0.0000 |
[32m[20221213 20:29:43 @agent_ppo2.py:185][0m |          -0.0145 |           0.0533 |           0.0000 |
[32m[20221213 20:29:43 @agent_ppo2.py:185][0m |          -0.0302 |           0.0507 |           0.0000 |
[32m[20221213 20:29:43 @agent_ppo2.py:185][0m |          -0.0346 |           0.0503 |           0.0000 |
[32m[20221213 20:29:43 @agent_ppo2.py:185][0m |          -0.0423 |           0.0492 |           0.0000 |
[32m[20221213 20:29:43 @agent_ppo2.py:185][0m |          -0.0386 |           0.0489 |           0.0000 |
[32m[20221213 20:29:43 @agent_ppo2.py:185][0m |          -0.0425 |           0.0494 |           0.0000 |
[32m[20221213 20:29:44 @agent_ppo2.py:185][0m |          -0.0402 |           0.0498 |           0.0000 |
[32m[20221213 20:29:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:29:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.80
[32m[20221213 20:29:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.79
[32m[20221213 20:29:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.86
[32m[20221213 20:29:44 @agent_ppo2.py:143][0m Total time:       0.58 min
[32m[20221213 20:29:44 @agent_ppo2.py:145][0m 86016 total steps have happened
[32m[20221213 20:29:44 @agent_ppo2.py:121][0m #------------------------ Iteration 21 --------------------------#
[32m[20221213 20:29:44 @agent_ppo2.py:127][0m Sampling time: 0.53 s by 5 slaves
[32m[20221213 20:29:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:45 @agent_ppo2.py:185][0m |           0.0151 |           0.0308 |           0.0000 |
[32m[20221213 20:29:45 @agent_ppo2.py:185][0m |          -0.0211 |           0.0135 |           0.0000 |
[32m[20221213 20:29:45 @agent_ppo2.py:185][0m |          -0.0433 |           0.0129 |           0.0000 |
[32m[20221213 20:29:45 @agent_ppo2.py:185][0m |          -0.0292 |           0.0128 |           0.0000 |
[32m[20221213 20:29:45 @agent_ppo2.py:185][0m |          -0.0484 |           0.0130 |           0.0000 |
[32m[20221213 20:29:45 @agent_ppo2.py:185][0m |          -0.0475 |           0.0127 |           0.0000 |
[32m[20221213 20:29:45 @agent_ppo2.py:185][0m |          -0.0613 |           0.0126 |           0.0000 |
[32m[20221213 20:29:45 @agent_ppo2.py:185][0m |          -0.0441 |           0.0126 |           0.0000 |
[32m[20221213 20:29:45 @agent_ppo2.py:185][0m |          -0.0767 |           0.0126 |           0.0000 |
[32m[20221213 20:29:46 @agent_ppo2.py:185][0m |          -0.0577 |           0.0124 |           0.0000 |
[32m[20221213 20:29:46 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 20:29:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.47
[32m[20221213 20:29:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.74
[32m[20221213 20:29:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.12
[32m[20221213 20:29:46 @agent_ppo2.py:143][0m Total time:       0.61 min
[32m[20221213 20:29:46 @agent_ppo2.py:145][0m 90112 total steps have happened
[32m[20221213 20:29:46 @agent_ppo2.py:121][0m #------------------------ Iteration 22 --------------------------#
[32m[20221213 20:29:46 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:29:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:46 @agent_ppo2.py:185][0m |           0.0111 |           0.0311 |           0.0000 |
[32m[20221213 20:29:47 @agent_ppo2.py:185][0m |          -0.0065 |           0.0250 |           0.0000 |
[32m[20221213 20:29:47 @agent_ppo2.py:185][0m |          -0.0175 |           0.0239 |           0.0000 |
[32m[20221213 20:29:47 @agent_ppo2.py:185][0m |          -0.0566 |           0.0255 |           0.0000 |
[32m[20221213 20:29:47 @agent_ppo2.py:185][0m |           0.0262 |           0.0250 |           0.0000 |
[32m[20221213 20:29:47 @agent_ppo2.py:185][0m |           0.0113 |           0.0235 |           0.0000 |
[32m[20221213 20:29:47 @agent_ppo2.py:185][0m |          -0.0033 |           0.0228 |           0.0000 |
[32m[20221213 20:29:47 @agent_ppo2.py:185][0m |          -0.0087 |           0.0227 |           0.0000 |
[32m[20221213 20:29:47 @agent_ppo2.py:185][0m |          -0.0209 |           0.0231 |           0.0000 |
[32m[20221213 20:29:47 @agent_ppo2.py:185][0m |          -0.0233 |           0.0224 |           0.0000 |
[32m[20221213 20:29:47 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:29:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.98
[32m[20221213 20:29:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.54
[32m[20221213 20:29:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.26
[32m[20221213 20:29:48 @agent_ppo2.py:143][0m Total time:       0.64 min
[32m[20221213 20:29:48 @agent_ppo2.py:145][0m 94208 total steps have happened
[32m[20221213 20:29:48 @agent_ppo2.py:121][0m #------------------------ Iteration 23 --------------------------#
[32m[20221213 20:29:48 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:29:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:48 @agent_ppo2.py:185][0m |           0.0111 |           0.0686 |           0.0000 |
[32m[20221213 20:29:48 @agent_ppo2.py:185][0m |          -0.0091 |           0.0598 |           0.0000 |
[32m[20221213 20:29:48 @agent_ppo2.py:185][0m |          -0.0204 |           0.0572 |           0.0000 |
[32m[20221213 20:29:48 @agent_ppo2.py:185][0m |          -0.0201 |           0.0549 |           0.0000 |
[32m[20221213 20:29:48 @agent_ppo2.py:185][0m |          -0.0236 |           0.0548 |           0.0000 |
[32m[20221213 20:29:49 @agent_ppo2.py:185][0m |          -0.0312 |           0.0545 |           0.0000 |
[32m[20221213 20:29:49 @agent_ppo2.py:185][0m |          -0.0318 |           0.0535 |           0.0000 |
[32m[20221213 20:29:49 @agent_ppo2.py:185][0m |          -0.0296 |           0.0549 |           0.0000 |
[32m[20221213 20:29:49 @agent_ppo2.py:185][0m |          -0.0576 |           0.0532 |           0.0000 |
[32m[20221213 20:29:49 @agent_ppo2.py:185][0m |          -0.0325 |           0.0522 |           0.0000 |
[32m[20221213 20:29:49 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:29:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.38
[32m[20221213 20:29:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.01
[32m[20221213 20:29:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.22
[32m[20221213 20:29:49 @agent_ppo2.py:143][0m Total time:       0.67 min
[32m[20221213 20:29:49 @agent_ppo2.py:145][0m 98304 total steps have happened
[32m[20221213 20:29:49 @agent_ppo2.py:121][0m #------------------------ Iteration 24 --------------------------#
[32m[20221213 20:29:50 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:29:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:50 @agent_ppo2.py:185][0m |           0.0137 |           0.0354 |           0.0000 |
[32m[20221213 20:29:50 @agent_ppo2.py:185][0m |          -0.0236 |           0.0302 |           0.0000 |
[32m[20221213 20:29:50 @agent_ppo2.py:185][0m |          -0.0380 |           0.0293 |           0.0000 |
[32m[20221213 20:29:50 @agent_ppo2.py:185][0m |          -0.0445 |           0.0285 |           0.0000 |
[32m[20221213 20:29:50 @agent_ppo2.py:185][0m |          -0.0489 |           0.0283 |           0.0000 |
[32m[20221213 20:29:50 @agent_ppo2.py:185][0m |          -0.0576 |           0.0277 |           0.0000 |
[32m[20221213 20:29:50 @agent_ppo2.py:185][0m |          -0.0565 |           0.0273 |           0.0000 |
[32m[20221213 20:29:50 @agent_ppo2.py:185][0m |          -0.0639 |           0.0270 |           0.0000 |
[32m[20221213 20:29:50 @agent_ppo2.py:185][0m |          -0.0676 |           0.0263 |           0.0000 |
[32m[20221213 20:29:51 @agent_ppo2.py:185][0m |          -0.0845 |           0.0263 |           0.0000 |
[32m[20221213 20:29:51 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:29:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.23
[32m[20221213 20:29:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.72
[32m[20221213 20:29:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.96
[32m[20221213 20:29:51 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 11.96
[32m[20221213 20:29:51 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 11.96
[32m[20221213 20:29:51 @agent_ppo2.py:143][0m Total time:       0.70 min
[32m[20221213 20:29:51 @agent_ppo2.py:145][0m 102400 total steps have happened
[32m[20221213 20:29:51 @agent_ppo2.py:121][0m #------------------------ Iteration 25 --------------------------#
[32m[20221213 20:29:51 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:29:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:51 @agent_ppo2.py:185][0m |           0.0275 |           0.0262 |           0.0000 |
[32m[20221213 20:29:52 @agent_ppo2.py:185][0m |          -0.0368 |           0.0247 |           0.0000 |
[32m[20221213 20:29:52 @agent_ppo2.py:185][0m |          -0.0415 |           0.0246 |           0.0000 |
[32m[20221213 20:29:52 @agent_ppo2.py:185][0m |          -0.0555 |           0.0238 |           0.0000 |
[32m[20221213 20:29:52 @agent_ppo2.py:185][0m |          -0.0600 |           0.0234 |           0.0000 |
[32m[20221213 20:29:52 @agent_ppo2.py:185][0m |          -0.0495 |           0.0237 |           0.0000 |
[32m[20221213 20:29:52 @agent_ppo2.py:185][0m |          -0.0613 |           0.0235 |           0.0000 |
[32m[20221213 20:29:52 @agent_ppo2.py:185][0m |          -0.0668 |           0.0235 |           0.0000 |
[32m[20221213 20:29:52 @agent_ppo2.py:185][0m |          -0.0643 |           0.0238 |           0.0000 |
[32m[20221213 20:29:52 @agent_ppo2.py:185][0m |          -0.0816 |           0.0230 |           0.0000 |
[32m[20221213 20:29:52 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:29:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.36
[32m[20221213 20:29:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.14
[32m[20221213 20:29:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.49
[32m[20221213 20:29:52 @agent_ppo2.py:143][0m Total time:       0.72 min
[32m[20221213 20:29:52 @agent_ppo2.py:145][0m 106496 total steps have happened
[32m[20221213 20:29:53 @agent_ppo2.py:121][0m #------------------------ Iteration 26 --------------------------#
[32m[20221213 20:29:53 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:29:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:53 @agent_ppo2.py:185][0m |           0.0175 |           0.0366 |           0.0000 |
[32m[20221213 20:29:53 @agent_ppo2.py:185][0m |          -0.0175 |           0.0332 |           0.0000 |
[32m[20221213 20:29:53 @agent_ppo2.py:185][0m |          -0.0253 |           0.0331 |           0.0000 |
[32m[20221213 20:29:53 @agent_ppo2.py:185][0m |          -0.0275 |           0.0329 |           0.0000 |
[32m[20221213 20:29:53 @agent_ppo2.py:185][0m |          -0.0360 |           0.0329 |           0.0000 |
[32m[20221213 20:29:53 @agent_ppo2.py:185][0m |          -0.0500 |           0.0323 |           0.0000 |
[32m[20221213 20:29:54 @agent_ppo2.py:185][0m |          -0.0381 |           0.0322 |           0.0000 |
[32m[20221213 20:29:54 @agent_ppo2.py:185][0m |          -0.0646 |           0.0315 |           0.0000 |
[32m[20221213 20:29:54 @agent_ppo2.py:185][0m |          -0.0509 |           0.0316 |           0.0000 |
[32m[20221213 20:29:54 @agent_ppo2.py:185][0m |          -0.0517 |           0.0319 |           0.0000 |
[32m[20221213 20:29:54 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:29:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.58
[32m[20221213 20:29:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.26
[32m[20221213 20:29:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.84
[32m[20221213 20:29:54 @agent_ppo2.py:143][0m Total time:       0.75 min
[32m[20221213 20:29:54 @agent_ppo2.py:145][0m 110592 total steps have happened
[32m[20221213 20:29:54 @agent_ppo2.py:121][0m #------------------------ Iteration 27 --------------------------#
[32m[20221213 20:29:55 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:29:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:55 @agent_ppo2.py:185][0m |           0.0039 |           0.0229 |           0.0000 |
[32m[20221213 20:29:55 @agent_ppo2.py:185][0m |          -0.0198 |           0.0094 |           0.0000 |
[32m[20221213 20:29:55 @agent_ppo2.py:185][0m |          -0.0390 |           0.0090 |           0.0000 |
[32m[20221213 20:29:55 @agent_ppo2.py:185][0m |          -0.0474 |           0.0089 |           0.0000 |
[32m[20221213 20:29:55 @agent_ppo2.py:185][0m |          -0.0410 |           0.0088 |           0.0000 |
[32m[20221213 20:29:55 @agent_ppo2.py:185][0m |          -0.0502 |           0.0088 |           0.0000 |
[32m[20221213 20:29:55 @agent_ppo2.py:185][0m |          -0.0400 |           0.0088 |           0.0000 |
[32m[20221213 20:29:55 @agent_ppo2.py:185][0m |          -0.0142 |           0.0089 |           0.0000 |
[32m[20221213 20:29:55 @agent_ppo2.py:185][0m |          -0.0272 |           0.0087 |           0.0000 |
[32m[20221213 20:29:55 @agent_ppo2.py:185][0m |          -0.0493 |           0.0085 |           0.0000 |
[32m[20221213 20:29:55 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:29:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.86
[32m[20221213 20:29:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.47
[32m[20221213 20:29:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.70
[32m[20221213 20:29:56 @agent_ppo2.py:143][0m Total time:       0.78 min
[32m[20221213 20:29:56 @agent_ppo2.py:145][0m 114688 total steps have happened
[32m[20221213 20:29:56 @agent_ppo2.py:121][0m #------------------------ Iteration 28 --------------------------#
[32m[20221213 20:29:56 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:29:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:56 @agent_ppo2.py:185][0m |           0.0085 |           0.0218 |           0.0000 |
[32m[20221213 20:29:56 @agent_ppo2.py:185][0m |          -0.0051 |           0.0191 |           0.0000 |
[32m[20221213 20:29:56 @agent_ppo2.py:185][0m |          -0.0195 |           0.0187 |           0.0000 |
[32m[20221213 20:29:57 @agent_ppo2.py:185][0m |          -0.0185 |           0.0187 |           0.0000 |
[32m[20221213 20:29:57 @agent_ppo2.py:185][0m |          -0.0265 |           0.0182 |           0.0000 |
[32m[20221213 20:29:57 @agent_ppo2.py:185][0m |          -0.0281 |           0.0180 |           0.0000 |
[32m[20221213 20:29:57 @agent_ppo2.py:185][0m |          -0.0737 |           0.0215 |           0.0000 |
[32m[20221213 20:29:57 @agent_ppo2.py:185][0m |          -0.0277 |           0.0193 |           0.0000 |
[32m[20221213 20:29:57 @agent_ppo2.py:185][0m |          -0.0393 |           0.0182 |           0.0000 |
[32m[20221213 20:29:57 @agent_ppo2.py:185][0m |          -0.0317 |           0.0178 |           0.0000 |
[32m[20221213 20:29:57 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:29:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.58
[32m[20221213 20:29:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.04
[32m[20221213 20:29:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.02
[32m[20221213 20:29:57 @agent_ppo2.py:143][0m Total time:       0.80 min
[32m[20221213 20:29:57 @agent_ppo2.py:145][0m 118784 total steps have happened
[32m[20221213 20:29:57 @agent_ppo2.py:121][0m #------------------------ Iteration 29 --------------------------#
[32m[20221213 20:29:58 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:29:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:58 @agent_ppo2.py:185][0m |           0.0154 |           0.1287 |           0.0000 |
[32m[20221213 20:29:58 @agent_ppo2.py:185][0m |          -0.0019 |           0.1096 |           0.0000 |
[32m[20221213 20:29:58 @agent_ppo2.py:185][0m |          -0.0123 |           0.1082 |           0.0000 |
[32m[20221213 20:29:58 @agent_ppo2.py:185][0m |          -0.0166 |           0.1105 |           0.0000 |
[32m[20221213 20:29:58 @agent_ppo2.py:185][0m |          -0.0161 |           0.1044 |           0.0000 |
[32m[20221213 20:29:58 @agent_ppo2.py:185][0m |          -0.0194 |           0.0983 |           0.0000 |
[32m[20221213 20:29:58 @agent_ppo2.py:185][0m |          -0.0253 |           0.0977 |           0.0000 |
[32m[20221213 20:29:58 @agent_ppo2.py:185][0m |          -0.0255 |           0.0992 |           0.0000 |
[32m[20221213 20:29:59 @agent_ppo2.py:185][0m |          -0.0267 |           0.0941 |           0.0000 |
[32m[20221213 20:29:59 @agent_ppo2.py:185][0m |          -0.0288 |           0.0939 |           0.0000 |
[32m[20221213 20:29:59 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:29:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.84
[32m[20221213 20:29:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.03
[32m[20221213 20:29:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.00
[32m[20221213 20:29:59 @agent_ppo2.py:143][0m Total time:       0.83 min
[32m[20221213 20:29:59 @agent_ppo2.py:145][0m 122880 total steps have happened
[32m[20221213 20:29:59 @agent_ppo2.py:121][0m #------------------------ Iteration 30 --------------------------#
[32m[20221213 20:29:59 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:29:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:00 @agent_ppo2.py:185][0m |           0.0213 |           0.0418 |           0.0000 |
[32m[20221213 20:30:00 @agent_ppo2.py:185][0m |          -0.0113 |           0.0347 |           0.0000 |
[32m[20221213 20:30:00 @agent_ppo2.py:185][0m |          -0.0537 |           0.0336 |           0.0000 |
[32m[20221213 20:30:00 @agent_ppo2.py:185][0m |          -0.0472 |           0.0335 |           0.0000 |
[32m[20221213 20:30:00 @agent_ppo2.py:185][0m |          -0.0627 |           0.0330 |           0.0000 |
[32m[20221213 20:30:00 @agent_ppo2.py:185][0m |          -0.0826 |           0.0329 |           0.0000 |
[32m[20221213 20:30:00 @agent_ppo2.py:185][0m |          -0.0839 |           0.0327 |           0.0000 |
[32m[20221213 20:30:00 @agent_ppo2.py:185][0m |          -0.0828 |           0.0325 |           0.0000 |
[32m[20221213 20:30:00 @agent_ppo2.py:185][0m |          -0.0879 |           0.0321 |           0.0000 |
[32m[20221213 20:30:00 @agent_ppo2.py:185][0m |          -0.0890 |           0.0322 |           0.0000 |
[32m[20221213 20:30:00 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:30:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.43
[32m[20221213 20:30:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.37
[32m[20221213 20:30:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.63
[32m[20221213 20:30:01 @agent_ppo2.py:143][0m Total time:       0.86 min
[32m[20221213 20:30:01 @agent_ppo2.py:145][0m 126976 total steps have happened
[32m[20221213 20:30:01 @agent_ppo2.py:121][0m #------------------------ Iteration 31 --------------------------#
[32m[20221213 20:30:01 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:30:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:01 @agent_ppo2.py:185][0m |           0.0200 |           0.1310 |           0.0000 |
[32m[20221213 20:30:01 @agent_ppo2.py:185][0m |           0.0030 |           0.1045 |           0.0000 |
[32m[20221213 20:30:01 @agent_ppo2.py:185][0m |          -0.0114 |           0.0999 |           0.0000 |
[32m[20221213 20:30:01 @agent_ppo2.py:185][0m |          -0.0199 |           0.0947 |           0.0000 |
[32m[20221213 20:30:01 @agent_ppo2.py:185][0m |          -0.0241 |           0.0965 |           0.0000 |
[32m[20221213 20:30:02 @agent_ppo2.py:185][0m |          -0.0271 |           0.0920 |           0.0000 |
[32m[20221213 20:30:02 @agent_ppo2.py:185][0m |          -0.0234 |           0.0893 |           0.0000 |
[32m[20221213 20:30:02 @agent_ppo2.py:185][0m |          -0.0263 |           0.0872 |           0.0000 |
[32m[20221213 20:30:02 @agent_ppo2.py:185][0m |          -0.0357 |           0.0868 |           0.0000 |
[32m[20221213 20:30:02 @agent_ppo2.py:185][0m |          -0.0340 |           0.0853 |           0.0000 |
[32m[20221213 20:30:02 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:30:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.62
[32m[20221213 20:30:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.23
[32m[20221213 20:30:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 13.66
[32m[20221213 20:30:02 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 13.66
[32m[20221213 20:30:02 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 13.66
[32m[20221213 20:30:02 @agent_ppo2.py:143][0m Total time:       0.88 min
[32m[20221213 20:30:02 @agent_ppo2.py:145][0m 131072 total steps have happened
[32m[20221213 20:30:02 @agent_ppo2.py:121][0m #------------------------ Iteration 32 --------------------------#
[32m[20221213 20:30:03 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:03 @agent_ppo2.py:185][0m |           0.0228 |           0.0543 |           0.0000 |
[32m[20221213 20:30:03 @agent_ppo2.py:185][0m |          -0.0248 |           0.0423 |           0.0000 |
[32m[20221213 20:30:03 @agent_ppo2.py:185][0m |          -0.0492 |           0.0419 |           0.0000 |
[32m[20221213 20:30:03 @agent_ppo2.py:185][0m |          -0.0588 |           0.0415 |           0.0000 |
[32m[20221213 20:30:03 @agent_ppo2.py:185][0m |          -0.0529 |           0.0397 |           0.0000 |
[32m[20221213 20:30:03 @agent_ppo2.py:185][0m |          -0.0714 |           0.0389 |           0.0000 |
[32m[20221213 20:30:03 @agent_ppo2.py:185][0m |          -0.0748 |           0.0386 |           0.0000 |
[32m[20221213 20:30:03 @agent_ppo2.py:185][0m |          -0.0721 |           0.0378 |           0.0000 |
[32m[20221213 20:30:03 @agent_ppo2.py:185][0m |          -0.0765 |           0.0377 |           0.0000 |
[32m[20221213 20:30:04 @agent_ppo2.py:185][0m |          -0.0955 |           0.0380 |           0.0000 |
[32m[20221213 20:30:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:30:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.88
[32m[20221213 20:30:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.92
[32m[20221213 20:30:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.34
[32m[20221213 20:30:04 @agent_ppo2.py:143][0m Total time:       0.91 min
[32m[20221213 20:30:04 @agent_ppo2.py:145][0m 135168 total steps have happened
[32m[20221213 20:30:04 @agent_ppo2.py:121][0m #------------------------ Iteration 33 --------------------------#
[32m[20221213 20:30:04 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:04 @agent_ppo2.py:185][0m |           0.0051 |           0.0327 |           0.0000 |
[32m[20221213 20:30:05 @agent_ppo2.py:185][0m |          -0.0329 |           0.0278 |           0.0000 |
[32m[20221213 20:30:05 @agent_ppo2.py:185][0m |          -0.0360 |           0.0262 |           0.0000 |
[32m[20221213 20:30:05 @agent_ppo2.py:185][0m |          -0.0594 |           0.0259 |           0.0000 |
[32m[20221213 20:30:05 @agent_ppo2.py:185][0m |          -0.0824 |           0.0257 |           0.0000 |
[32m[20221213 20:30:05 @agent_ppo2.py:185][0m |          -0.0760 |           0.0259 |           0.0000 |
[32m[20221213 20:30:05 @agent_ppo2.py:185][0m |          -0.0797 |           0.0252 |           0.0000 |
[32m[20221213 20:30:05 @agent_ppo2.py:185][0m |          -0.0827 |           0.0254 |           0.0000 |
[32m[20221213 20:30:05 @agent_ppo2.py:185][0m |          -0.0989 |           0.0347 |           0.0000 |
[32m[20221213 20:30:05 @agent_ppo2.py:185][0m |          -0.0772 |           0.0300 |           0.0000 |
[32m[20221213 20:30:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:30:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.87
[32m[20221213 20:30:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.17
[32m[20221213 20:30:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.53
[32m[20221213 20:30:06 @agent_ppo2.py:143][0m Total time:       0.94 min
[32m[20221213 20:30:06 @agent_ppo2.py:145][0m 139264 total steps have happened
[32m[20221213 20:30:06 @agent_ppo2.py:121][0m #------------------------ Iteration 34 --------------------------#
[32m[20221213 20:30:06 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:06 @agent_ppo2.py:185][0m |           0.0156 |           0.0885 |           0.0000 |
[32m[20221213 20:30:06 @agent_ppo2.py:185][0m |          -0.0157 |           0.0701 |           0.0000 |
[32m[20221213 20:30:06 @agent_ppo2.py:185][0m |          -0.0175 |           0.0674 |           0.0000 |
[32m[20221213 20:30:06 @agent_ppo2.py:185][0m |          -0.0168 |           0.0665 |           0.0000 |
[32m[20221213 20:30:07 @agent_ppo2.py:185][0m |          -0.0312 |           0.0680 |           0.0000 |
[32m[20221213 20:30:07 @agent_ppo2.py:185][0m |          -0.0361 |           0.0652 |           0.0000 |
[32m[20221213 20:30:07 @agent_ppo2.py:185][0m |          -0.0346 |           0.0630 |           0.0000 |
[32m[20221213 20:30:07 @agent_ppo2.py:185][0m |          -0.0345 |           0.0611 |           0.0000 |
[32m[20221213 20:30:07 @agent_ppo2.py:185][0m |          -0.0342 |           0.0601 |           0.0000 |
[32m[20221213 20:30:07 @agent_ppo2.py:185][0m |          -0.0398 |           0.0589 |           0.0000 |
[32m[20221213 20:30:07 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:30:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.21
[32m[20221213 20:30:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.81
[32m[20221213 20:30:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.53
[32m[20221213 20:30:07 @agent_ppo2.py:143][0m Total time:       0.97 min
[32m[20221213 20:30:07 @agent_ppo2.py:145][0m 143360 total steps have happened
[32m[20221213 20:30:07 @agent_ppo2.py:121][0m #------------------------ Iteration 35 --------------------------#
[32m[20221213 20:30:08 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:08 @agent_ppo2.py:185][0m |           0.0169 |           0.0398 |           0.0000 |
[32m[20221213 20:30:08 @agent_ppo2.py:185][0m |          -0.0229 |           0.0371 |           0.0000 |
[32m[20221213 20:30:08 @agent_ppo2.py:185][0m |          -0.0355 |           0.0307 |           0.0000 |
[32m[20221213 20:30:08 @agent_ppo2.py:185][0m |          -0.0499 |           0.0291 |           0.0000 |
[32m[20221213 20:30:08 @agent_ppo2.py:185][0m |          -0.0682 |           0.0289 |           0.0000 |
[32m[20221213 20:30:08 @agent_ppo2.py:185][0m |          -0.0667 |           0.0283 |           0.0000 |
[32m[20221213 20:30:08 @agent_ppo2.py:185][0m |          -0.0793 |           0.0279 |           0.0000 |
[32m[20221213 20:30:08 @agent_ppo2.py:185][0m |          -0.0693 |           0.0278 |           0.0000 |
[32m[20221213 20:30:09 @agent_ppo2.py:185][0m |          -0.0784 |           0.0278 |           0.0000 |
[32m[20221213 20:30:09 @agent_ppo2.py:185][0m |          -0.0725 |           0.0275 |           0.0000 |
[32m[20221213 20:30:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:30:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.82
[32m[20221213 20:30:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.41
[32m[20221213 20:30:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.79
[32m[20221213 20:30:09 @agent_ppo2.py:143][0m Total time:       1.00 min
[32m[20221213 20:30:09 @agent_ppo2.py:145][0m 147456 total steps have happened
[32m[20221213 20:30:09 @agent_ppo2.py:121][0m #------------------------ Iteration 36 --------------------------#
[32m[20221213 20:30:09 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:10 @agent_ppo2.py:185][0m |           0.0185 |           0.0609 |           0.0000 |
[32m[20221213 20:30:10 @agent_ppo2.py:185][0m |          -0.0227 |           0.0571 |           0.0000 |
[32m[20221213 20:30:10 @agent_ppo2.py:185][0m |          -0.0285 |           0.0547 |           0.0000 |
[32m[20221213 20:30:10 @agent_ppo2.py:185][0m |          -0.0360 |           0.0543 |           0.0000 |
[32m[20221213 20:30:10 @agent_ppo2.py:185][0m |          -0.0401 |           0.0545 |           0.0000 |
[32m[20221213 20:30:10 @agent_ppo2.py:185][0m |          -0.0380 |           0.0534 |           0.0000 |
[32m[20221213 20:30:10 @agent_ppo2.py:185][0m |          -0.0441 |           0.0517 |           0.0000 |
[32m[20221213 20:30:10 @agent_ppo2.py:185][0m |          -0.0539 |           0.0505 |           0.0000 |
[32m[20221213 20:30:10 @agent_ppo2.py:185][0m |          -0.0498 |           0.0496 |           0.0000 |
[32m[20221213 20:30:10 @agent_ppo2.py:185][0m |          -0.0526 |           0.0493 |           0.0000 |
[32m[20221213 20:30:10 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:30:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.67
[32m[20221213 20:30:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.69
[32m[20221213 20:30:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.88
[32m[20221213 20:30:11 @agent_ppo2.py:143][0m Total time:       1.02 min
[32m[20221213 20:30:11 @agent_ppo2.py:145][0m 151552 total steps have happened
[32m[20221213 20:30:11 @agent_ppo2.py:121][0m #------------------------ Iteration 37 --------------------------#
[32m[20221213 20:30:11 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:11 @agent_ppo2.py:185][0m |           0.0099 |           0.0250 |           0.0000 |
[32m[20221213 20:30:11 @agent_ppo2.py:185][0m |          -0.0265 |           0.0142 |           0.0000 |
[32m[20221213 20:30:11 @agent_ppo2.py:185][0m |          -0.0586 |           0.0140 |           0.0000 |
[32m[20221213 20:30:11 @agent_ppo2.py:185][0m |          -0.0414 |           0.0134 |           0.0000 |
[32m[20221213 20:30:12 @agent_ppo2.py:185][0m |          -0.0598 |           0.0132 |           0.0000 |
[32m[20221213 20:30:12 @agent_ppo2.py:185][0m |          -0.0628 |           0.0131 |           0.0000 |
[32m[20221213 20:30:12 @agent_ppo2.py:185][0m |          -0.0686 |           0.0131 |           0.0000 |
[32m[20221213 20:30:12 @agent_ppo2.py:185][0m |          -0.0735 |           0.0130 |           0.0000 |
[32m[20221213 20:30:12 @agent_ppo2.py:185][0m |          -0.0779 |           0.0129 |           0.0000 |
[32m[20221213 20:30:12 @agent_ppo2.py:185][0m |          -0.0807 |           0.0132 |           0.0000 |
[32m[20221213 20:30:12 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:30:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.95
[32m[20221213 20:30:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.66
[32m[20221213 20:30:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.81
[32m[20221213 20:30:12 @agent_ppo2.py:143][0m Total time:       1.05 min
[32m[20221213 20:30:12 @agent_ppo2.py:145][0m 155648 total steps have happened
[32m[20221213 20:30:12 @agent_ppo2.py:121][0m #------------------------ Iteration 38 --------------------------#
[32m[20221213 20:30:13 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:30:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:13 @agent_ppo2.py:185][0m |           0.0180 |           0.0797 |           0.0000 |
[32m[20221213 20:30:13 @agent_ppo2.py:185][0m |          -0.0421 |           0.0899 |           0.0000 |
[32m[20221213 20:30:13 @agent_ppo2.py:185][0m |           0.0164 |           0.0661 |           0.0000 |
[32m[20221213 20:30:13 @agent_ppo2.py:185][0m |          -0.0016 |           0.0578 |           0.0000 |
[32m[20221213 20:30:13 @agent_ppo2.py:185][0m |          -0.0071 |           0.0566 |           0.0000 |
[32m[20221213 20:30:13 @agent_ppo2.py:185][0m |          -0.0138 |           0.0554 |           0.0000 |
[32m[20221213 20:30:13 @agent_ppo2.py:185][0m |          -0.0136 |           0.0552 |           0.0000 |
[32m[20221213 20:30:13 @agent_ppo2.py:185][0m |          -0.0196 |           0.0538 |           0.0000 |
[32m[20221213 20:30:13 @agent_ppo2.py:185][0m |          -0.0213 |           0.0544 |           0.0000 |
[32m[20221213 20:30:13 @agent_ppo2.py:185][0m |          -0.0241 |           0.0537 |           0.0000 |
[32m[20221213 20:30:13 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:30:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.31
[32m[20221213 20:30:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.02
[32m[20221213 20:30:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.01
[32m[20221213 20:30:14 @agent_ppo2.py:143][0m Total time:       1.08 min
[32m[20221213 20:30:14 @agent_ppo2.py:145][0m 159744 total steps have happened
[32m[20221213 20:30:14 @agent_ppo2.py:121][0m #------------------------ Iteration 39 --------------------------#
[32m[20221213 20:30:14 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:30:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:14 @agent_ppo2.py:185][0m |           0.0188 |           0.0723 |           0.0000 |
[32m[20221213 20:30:14 @agent_ppo2.py:185][0m |          -0.0066 |           0.0671 |           0.0000 |
[32m[20221213 20:30:15 @agent_ppo2.py:185][0m |          -0.0296 |           0.0627 |           0.0000 |
[32m[20221213 20:30:15 @agent_ppo2.py:185][0m |          -0.0382 |           0.0603 |           0.0000 |
[32m[20221213 20:30:15 @agent_ppo2.py:185][0m |          -0.0467 |           0.0584 |           0.0000 |
[32m[20221213 20:30:15 @agent_ppo2.py:185][0m |          -0.0488 |           0.0583 |           0.0000 |
[32m[20221213 20:30:15 @agent_ppo2.py:185][0m |          -0.0647 |           0.0577 |           0.0000 |
[32m[20221213 20:30:15 @agent_ppo2.py:185][0m |          -0.0501 |           0.0560 |           0.0000 |
[32m[20221213 20:30:15 @agent_ppo2.py:185][0m |          -0.0632 |           0.0542 |           0.0000 |
[32m[20221213 20:30:15 @agent_ppo2.py:185][0m |          -0.0664 |           0.0543 |           0.0000 |
[32m[20221213 20:30:15 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:30:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.50
[32m[20221213 20:30:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.89
[32m[20221213 20:30:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.58
[32m[20221213 20:30:15 @agent_ppo2.py:143][0m Total time:       1.10 min
[32m[20221213 20:30:15 @agent_ppo2.py:145][0m 163840 total steps have happened
[32m[20221213 20:30:15 @agent_ppo2.py:121][0m #------------------------ Iteration 40 --------------------------#
[32m[20221213 20:30:16 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:30:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:16 @agent_ppo2.py:185][0m |           0.0046 |           0.0360 |           0.0000 |
[32m[20221213 20:30:16 @agent_ppo2.py:185][0m |          -0.0097 |           0.0187 |           0.0000 |
[32m[20221213 20:30:16 @agent_ppo2.py:185][0m |          -0.0449 |           0.0182 |           0.0000 |
[32m[20221213 20:30:16 @agent_ppo2.py:185][0m |          -0.0438 |           0.0176 |           0.0000 |
[32m[20221213 20:30:16 @agent_ppo2.py:185][0m |          -0.0650 |           0.0171 |           0.0000 |
[32m[20221213 20:30:16 @agent_ppo2.py:185][0m |          -0.0543 |           0.0172 |           0.0000 |
[32m[20221213 20:30:16 @agent_ppo2.py:185][0m |          -0.0663 |           0.0172 |           0.0000 |
[32m[20221213 20:30:17 @agent_ppo2.py:185][0m |          -0.0813 |           0.0167 |           0.0000 |
[32m[20221213 20:30:17 @agent_ppo2.py:185][0m |          -0.0868 |           0.0164 |           0.0000 |
[32m[20221213 20:30:17 @agent_ppo2.py:185][0m |          -0.0896 |           0.0163 |           0.0000 |
[32m[20221213 20:30:17 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:30:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.29
[32m[20221213 20:30:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.66
[32m[20221213 20:30:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.24
[32m[20221213 20:30:17 @agent_ppo2.py:143][0m Total time:       1.13 min
[32m[20221213 20:30:17 @agent_ppo2.py:145][0m 167936 total steps have happened
[32m[20221213 20:30:17 @agent_ppo2.py:121][0m #------------------------ Iteration 41 --------------------------#
[32m[20221213 20:30:17 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:18 @agent_ppo2.py:185][0m |           0.0148 |           0.0563 |           0.0000 |
[32m[20221213 20:30:18 @agent_ppo2.py:185][0m |          -0.0090 |           0.0427 |           0.0000 |
[32m[20221213 20:30:18 @agent_ppo2.py:185][0m |          -0.0142 |           0.0396 |           0.0000 |
[32m[20221213 20:30:18 @agent_ppo2.py:185][0m |          -0.0220 |           0.0386 |           0.0000 |
[32m[20221213 20:30:18 @agent_ppo2.py:185][0m |          -0.0444 |           0.0384 |           0.0000 |
[32m[20221213 20:30:18 @agent_ppo2.py:185][0m |          -0.0296 |           0.0381 |           0.0000 |
[32m[20221213 20:30:18 @agent_ppo2.py:185][0m |          -0.0358 |           0.0383 |           0.0000 |
[32m[20221213 20:30:18 @agent_ppo2.py:185][0m |          -0.0335 |           0.0370 |           0.0000 |
[32m[20221213 20:30:18 @agent_ppo2.py:185][0m |          -0.0313 |           0.0354 |           0.0000 |
[32m[20221213 20:30:19 @agent_ppo2.py:185][0m |          -0.0369 |           0.0359 |           0.0000 |
[32m[20221213 20:30:19 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 20:30:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.54
[32m[20221213 20:30:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.53
[32m[20221213 20:30:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.47
[32m[20221213 20:30:19 @agent_ppo2.py:143][0m Total time:       1.16 min
[32m[20221213 20:30:19 @agent_ppo2.py:145][0m 172032 total steps have happened
[32m[20221213 20:30:19 @agent_ppo2.py:121][0m #------------------------ Iteration 42 --------------------------#
[32m[20221213 20:30:19 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:19 @agent_ppo2.py:185][0m |           0.0198 |           0.0268 |           0.0000 |
[32m[20221213 20:30:20 @agent_ppo2.py:185][0m |          -0.0141 |           0.0123 |           0.0000 |
[32m[20221213 20:30:20 @agent_ppo2.py:185][0m |          -0.0378 |           0.0121 |           0.0000 |
[32m[20221213 20:30:20 @agent_ppo2.py:185][0m |          -0.0390 |           0.0120 |           0.0000 |
[32m[20221213 20:30:20 @agent_ppo2.py:185][0m |          -0.0536 |           0.0117 |           0.0000 |
[32m[20221213 20:30:20 @agent_ppo2.py:185][0m |          -0.0547 |           0.0117 |           0.0000 |
[32m[20221213 20:30:20 @agent_ppo2.py:185][0m |          -0.0529 |           0.0116 |           0.0000 |
[32m[20221213 20:30:20 @agent_ppo2.py:185][0m |          -0.0656 |           0.0115 |           0.0000 |
[32m[20221213 20:30:20 @agent_ppo2.py:185][0m |          -0.0573 |           0.0114 |           0.0000 |
[32m[20221213 20:30:20 @agent_ppo2.py:185][0m |          -0.0685 |           0.0114 |           0.0000 |
[32m[20221213 20:30:20 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:30:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.91
[32m[20221213 20:30:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.51
[32m[20221213 20:30:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.22
[32m[20221213 20:30:21 @agent_ppo2.py:143][0m Total time:       1.19 min
[32m[20221213 20:30:21 @agent_ppo2.py:145][0m 176128 total steps have happened
[32m[20221213 20:30:21 @agent_ppo2.py:121][0m #------------------------ Iteration 43 --------------------------#
[32m[20221213 20:30:21 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:21 @agent_ppo2.py:185][0m |           0.0149 |           0.1170 |           0.0000 |
[32m[20221213 20:30:21 @agent_ppo2.py:185][0m |          -0.0050 |           0.0771 |           0.0000 |
[32m[20221213 20:30:21 @agent_ppo2.py:185][0m |          -0.0187 |           0.0714 |           0.0000 |
[32m[20221213 20:30:22 @agent_ppo2.py:185][0m |          -0.0216 |           0.0700 |           0.0000 |
[32m[20221213 20:30:22 @agent_ppo2.py:185][0m |          -0.0237 |           0.0678 |           0.0000 |
[32m[20221213 20:30:22 @agent_ppo2.py:185][0m |          -0.0245 |           0.0650 |           0.0000 |
[32m[20221213 20:30:22 @agent_ppo2.py:185][0m |          -0.0269 |           0.0622 |           0.0000 |
[32m[20221213 20:30:22 @agent_ppo2.py:185][0m |          -0.0274 |           0.0625 |           0.0000 |
[32m[20221213 20:30:22 @agent_ppo2.py:185][0m |          -0.0294 |           0.0602 |           0.0000 |
[32m[20221213 20:30:22 @agent_ppo2.py:185][0m |          -0.0285 |           0.0629 |           0.0000 |
[32m[20221213 20:30:22 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:30:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.56
[32m[20221213 20:30:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.19
[32m[20221213 20:30:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.48
[32m[20221213 20:30:22 @agent_ppo2.py:143][0m Total time:       1.22 min
[32m[20221213 20:30:22 @agent_ppo2.py:145][0m 180224 total steps have happened
[32m[20221213 20:30:22 @agent_ppo2.py:121][0m #------------------------ Iteration 44 --------------------------#
[32m[20221213 20:30:23 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:30:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:23 @agent_ppo2.py:185][0m |           0.0428 |           0.0452 |           0.0000 |
[32m[20221213 20:30:23 @agent_ppo2.py:185][0m |           0.1088 |           0.0155 |           0.0000 |
[32m[20221213 20:30:23 @agent_ppo2.py:185][0m |           0.0132 |           0.0146 |           0.0000 |
[32m[20221213 20:30:23 @agent_ppo2.py:185][0m |          -0.0147 |           0.0143 |           0.0000 |
[32m[20221213 20:30:23 @agent_ppo2.py:185][0m |          -0.0243 |           0.0142 |           0.0000 |
[32m[20221213 20:30:23 @agent_ppo2.py:185][0m |          -0.0448 |           0.0139 |           0.0000 |
[32m[20221213 20:30:23 @agent_ppo2.py:185][0m |          -0.0389 |           0.0138 |           0.0000 |
[32m[20221213 20:30:24 @agent_ppo2.py:185][0m |          -0.0503 |           0.0137 |           0.0000 |
[32m[20221213 20:30:24 @agent_ppo2.py:185][0m |          -0.0543 |           0.0136 |           0.0000 |
[32m[20221213 20:30:24 @agent_ppo2.py:185][0m |          -0.0586 |           0.0135 |           0.0000 |
[32m[20221213 20:30:24 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:30:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.50
[32m[20221213 20:30:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.89
[32m[20221213 20:30:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.34
[32m[20221213 20:30:24 @agent_ppo2.py:143][0m Total time:       1.25 min
[32m[20221213 20:30:24 @agent_ppo2.py:145][0m 184320 total steps have happened
[32m[20221213 20:30:24 @agent_ppo2.py:121][0m #------------------------ Iteration 45 --------------------------#
[32m[20221213 20:30:24 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:25 @agent_ppo2.py:185][0m |           0.0235 |           0.0096 |           0.0000 |
[32m[20221213 20:30:25 @agent_ppo2.py:185][0m |          -0.0235 |           0.0077 |           0.0000 |
[32m[20221213 20:30:25 @agent_ppo2.py:185][0m |          -0.0438 |           0.0075 |           0.0000 |
[32m[20221213 20:30:25 @agent_ppo2.py:185][0m |          -0.0702 |           0.0074 |           0.0000 |
[32m[20221213 20:30:25 @agent_ppo2.py:185][0m |          -0.0567 |           0.0073 |           0.0000 |
[32m[20221213 20:30:25 @agent_ppo2.py:185][0m |          -0.0530 |           0.0073 |           0.0000 |
[32m[20221213 20:30:25 @agent_ppo2.py:185][0m |          -0.0766 |           0.0072 |           0.0000 |
[32m[20221213 20:30:25 @agent_ppo2.py:185][0m |          -0.0865 |           0.0071 |           0.0000 |
[32m[20221213 20:30:25 @agent_ppo2.py:185][0m |          -0.0758 |           0.0071 |           0.0000 |
[32m[20221213 20:30:25 @agent_ppo2.py:185][0m |          -0.0764 |           0.0070 |           0.0000 |
[32m[20221213 20:30:25 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:30:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.95
[32m[20221213 20:30:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.36
[32m[20221213 20:30:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.70
[32m[20221213 20:30:26 @agent_ppo2.py:143][0m Total time:       1.28 min
[32m[20221213 20:30:26 @agent_ppo2.py:145][0m 188416 total steps have happened
[32m[20221213 20:30:26 @agent_ppo2.py:121][0m #------------------------ Iteration 46 --------------------------#
[32m[20221213 20:30:26 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:26 @agent_ppo2.py:185][0m |           0.0205 |           0.2356 |           0.0000 |
[32m[20221213 20:30:26 @agent_ppo2.py:185][0m |          -0.0034 |           0.1344 |           0.0000 |
[32m[20221213 20:30:26 @agent_ppo2.py:185][0m |          -0.0084 |           0.1220 |           0.0000 |
[32m[20221213 20:30:26 @agent_ppo2.py:185][0m |          -0.0148 |           0.1140 |           0.0000 |
[32m[20221213 20:30:27 @agent_ppo2.py:185][0m |          -0.0130 |           0.1074 |           0.0000 |
[32m[20221213 20:30:27 @agent_ppo2.py:185][0m |          -0.0144 |           0.1045 |           0.0000 |
[32m[20221213 20:30:27 @agent_ppo2.py:185][0m |          -0.0165 |           0.1004 |           0.0000 |
[32m[20221213 20:30:27 @agent_ppo2.py:185][0m |          -0.0210 |           0.0988 |           0.0000 |
[32m[20221213 20:30:27 @agent_ppo2.py:185][0m |          -0.0547 |           0.1002 |           0.0000 |
[32m[20221213 20:30:27 @agent_ppo2.py:185][0m |          -0.0234 |           0.0935 |           0.0000 |
[32m[20221213 20:30:27 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:30:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.68
[32m[20221213 20:30:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.12
[32m[20221213 20:30:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.77
[32m[20221213 20:30:27 @agent_ppo2.py:143][0m Total time:       1.30 min
[32m[20221213 20:30:27 @agent_ppo2.py:145][0m 192512 total steps have happened
[32m[20221213 20:30:27 @agent_ppo2.py:121][0m #------------------------ Iteration 47 --------------------------#
[32m[20221213 20:30:28 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:30:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:28 @agent_ppo2.py:185][0m |           0.0181 |           0.0985 |           0.0000 |
[32m[20221213 20:30:28 @agent_ppo2.py:185][0m |          -0.0088 |           0.1002 |           0.0000 |
[32m[20221213 20:30:28 @agent_ppo2.py:185][0m |          -0.0500 |           0.0952 |           0.0000 |
[32m[20221213 20:30:28 @agent_ppo2.py:185][0m |          -0.0691 |           0.0877 |           0.0000 |
[32m[20221213 20:30:28 @agent_ppo2.py:185][0m |          -0.0592 |           0.0856 |           0.0000 |
[32m[20221213 20:30:28 @agent_ppo2.py:185][0m |          -0.0670 |           0.0812 |           0.0000 |
[32m[20221213 20:30:28 @agent_ppo2.py:185][0m |          -0.0736 |           0.0810 |           0.0000 |
[32m[20221213 20:30:28 @agent_ppo2.py:185][0m |          -0.0756 |           0.0772 |           0.0000 |
[32m[20221213 20:30:29 @agent_ppo2.py:185][0m |          -0.0855 |           0.0758 |           0.0000 |
[32m[20221213 20:30:29 @agent_ppo2.py:185][0m |          -0.0896 |           0.0737 |           0.0000 |
[32m[20221213 20:30:29 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:30:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.12
[32m[20221213 20:30:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.27
[32m[20221213 20:30:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.03
[32m[20221213 20:30:29 @agent_ppo2.py:143][0m Total time:       1.33 min
[32m[20221213 20:30:29 @agent_ppo2.py:145][0m 196608 total steps have happened
[32m[20221213 20:30:29 @agent_ppo2.py:121][0m #------------------------ Iteration 48 --------------------------#
[32m[20221213 20:30:29 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:29 @agent_ppo2.py:185][0m |           0.0165 |           0.0490 |           0.0000 |
[32m[20221213 20:30:30 @agent_ppo2.py:185][0m |          -0.0299 |           0.0338 |           0.0000 |
[32m[20221213 20:30:30 @agent_ppo2.py:185][0m |          -0.0563 |           0.0322 |           0.0000 |
[32m[20221213 20:30:30 @agent_ppo2.py:185][0m |          -0.0581 |           0.0314 |           0.0000 |
[32m[20221213 20:30:30 @agent_ppo2.py:185][0m |          -0.0626 |           0.0306 |           0.0000 |
[32m[20221213 20:30:30 @agent_ppo2.py:185][0m |          -0.0831 |           0.0305 |           0.0000 |
[32m[20221213 20:30:30 @agent_ppo2.py:185][0m |          -0.0769 |           0.0294 |           0.0000 |
[32m[20221213 20:30:30 @agent_ppo2.py:185][0m |          -0.0676 |           0.0381 |           0.0000 |
[32m[20221213 20:30:30 @agent_ppo2.py:185][0m |          -0.1002 |           0.0325 |           0.0000 |
[32m[20221213 20:30:30 @agent_ppo2.py:185][0m |          -0.0939 |           0.0291 |           0.0000 |
[32m[20221213 20:30:30 @agent_ppo2.py:130][0m Policy update time: 0.84 s
[32m[20221213 20:30:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.86
[32m[20221213 20:30:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.87
[32m[20221213 20:30:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.43
[32m[20221213 20:30:31 @agent_ppo2.py:143][0m Total time:       1.36 min
[32m[20221213 20:30:31 @agent_ppo2.py:145][0m 200704 total steps have happened
[32m[20221213 20:30:31 @agent_ppo2.py:121][0m #------------------------ Iteration 49 --------------------------#
[32m[20221213 20:30:31 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:31 @agent_ppo2.py:185][0m |           0.0212 |           0.0193 |           0.0000 |
[32m[20221213 20:30:31 @agent_ppo2.py:185][0m |          -0.0201 |           0.0168 |           0.0000 |
[32m[20221213 20:30:31 @agent_ppo2.py:185][0m |          -0.0444 |           0.0165 |           0.0000 |
[32m[20221213 20:30:31 @agent_ppo2.py:185][0m |          -0.0466 |           0.0161 |           0.0000 |
[32m[20221213 20:30:31 @agent_ppo2.py:185][0m |          -0.0667 |           0.0159 |           0.0000 |
[32m[20221213 20:30:31 @agent_ppo2.py:185][0m |          -0.0838 |           0.0157 |           0.0000 |
[32m[20221213 20:30:32 @agent_ppo2.py:185][0m |          -0.0926 |           0.0156 |           0.0000 |
[32m[20221213 20:30:32 @agent_ppo2.py:185][0m |          -0.0837 |           0.0155 |           0.0000 |
[32m[20221213 20:30:32 @agent_ppo2.py:185][0m |          -0.0878 |           0.0154 |           0.0000 |
[32m[20221213 20:30:32 @agent_ppo2.py:185][0m |          -0.0937 |           0.0153 |           0.0000 |
[32m[20221213 20:30:32 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:30:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.81
[32m[20221213 20:30:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.06
[32m[20221213 20:30:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.73
[32m[20221213 20:30:32 @agent_ppo2.py:143][0m Total time:       1.38 min
[32m[20221213 20:30:32 @agent_ppo2.py:145][0m 204800 total steps have happened
[32m[20221213 20:30:32 @agent_ppo2.py:121][0m #------------------------ Iteration 50 --------------------------#
[32m[20221213 20:30:33 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:30:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:33 @agent_ppo2.py:185][0m |           0.0233 |           0.0562 |           0.0000 |
[32m[20221213 20:30:33 @agent_ppo2.py:185][0m |          -0.0092 |           0.0432 |           0.0000 |
[32m[20221213 20:30:33 @agent_ppo2.py:185][0m |          -0.0215 |           0.0419 |           0.0000 |
[32m[20221213 20:30:33 @agent_ppo2.py:185][0m |          -0.0216 |           0.0416 |           0.0000 |
[32m[20221213 20:30:33 @agent_ppo2.py:185][0m |          -0.0331 |           0.0403 |           0.0000 |
[32m[20221213 20:30:33 @agent_ppo2.py:185][0m |          -0.0322 |           0.0403 |           0.0000 |
[32m[20221213 20:30:33 @agent_ppo2.py:185][0m |          -0.0365 |           0.0393 |           0.0000 |
[32m[20221213 20:30:33 @agent_ppo2.py:185][0m |          -0.0405 |           0.0381 |           0.0000 |
[32m[20221213 20:30:33 @agent_ppo2.py:185][0m |          -0.0373 |           0.0395 |           0.0000 |
[32m[20221213 20:30:33 @agent_ppo2.py:185][0m |          -0.0411 |           0.0386 |           0.0000 |
[32m[20221213 20:30:33 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:30:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.09
[32m[20221213 20:30:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.66
[32m[20221213 20:30:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.24
[32m[20221213 20:30:34 @agent_ppo2.py:143][0m Total time:       1.41 min
[32m[20221213 20:30:34 @agent_ppo2.py:145][0m 208896 total steps have happened
[32m[20221213 20:30:34 @agent_ppo2.py:121][0m #------------------------ Iteration 51 --------------------------#
[32m[20221213 20:30:34 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:30:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:34 @agent_ppo2.py:185][0m |           0.0277 |           0.0320 |           0.0000 |
[32m[20221213 20:30:34 @agent_ppo2.py:185][0m |          -0.0015 |           0.0116 |           0.0000 |
[32m[20221213 20:30:34 @agent_ppo2.py:185][0m |          -0.0022 |           0.0107 |           0.0000 |
[32m[20221213 20:30:35 @agent_ppo2.py:185][0m |          -0.0376 |           0.0104 |           0.0000 |
[32m[20221213 20:30:35 @agent_ppo2.py:185][0m |          -0.0495 |           0.0102 |           0.0000 |
[32m[20221213 20:30:35 @agent_ppo2.py:185][0m |          -0.0512 |           0.0101 |           0.0000 |
[32m[20221213 20:30:35 @agent_ppo2.py:185][0m |          -0.0512 |           0.0100 |           0.0000 |
[32m[20221213 20:30:35 @agent_ppo2.py:185][0m |          -0.0542 |           0.0099 |           0.0000 |
[32m[20221213 20:30:35 @agent_ppo2.py:185][0m |          -0.0388 |           0.0099 |           0.0000 |
[32m[20221213 20:30:35 @agent_ppo2.py:185][0m |          -0.0719 |           0.0099 |           0.0000 |
[32m[20221213 20:30:35 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:30:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.07
[32m[20221213 20:30:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.13
[32m[20221213 20:30:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.46
[32m[20221213 20:30:35 @agent_ppo2.py:143][0m Total time:       1.44 min
[32m[20221213 20:30:35 @agent_ppo2.py:145][0m 212992 total steps have happened
[32m[20221213 20:30:35 @agent_ppo2.py:121][0m #------------------------ Iteration 52 --------------------------#
[32m[20221213 20:30:36 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:30:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:36 @agent_ppo2.py:185][0m |           0.0223 |           0.0080 |           0.0000 |
[32m[20221213 20:30:36 @agent_ppo2.py:185][0m |          -0.0041 |           0.0078 |           0.0000 |
[32m[20221213 20:30:36 @agent_ppo2.py:185][0m |          -0.0195 |           0.0076 |           0.0000 |
[32m[20221213 20:30:36 @agent_ppo2.py:185][0m |          -0.0475 |           0.0075 |           0.0000 |
[32m[20221213 20:30:36 @agent_ppo2.py:185][0m |          -0.0345 |           0.0076 |           0.0000 |
[32m[20221213 20:30:36 @agent_ppo2.py:185][0m |          -0.0551 |           0.0074 |           0.0000 |
[32m[20221213 20:30:36 @agent_ppo2.py:185][0m |          -0.0561 |           0.0073 |           0.0000 |
[32m[20221213 20:30:37 @agent_ppo2.py:185][0m |          -0.0724 |           0.0073 |           0.0000 |
[32m[20221213 20:30:37 @agent_ppo2.py:185][0m |          -0.0651 |           0.0072 |           0.0000 |
[32m[20221213 20:30:37 @agent_ppo2.py:185][0m |          -0.1090 |           0.0075 |           0.0000 |
[32m[20221213 20:30:37 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:30:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.45
[32m[20221213 20:30:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.66
[32m[20221213 20:30:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.86
[32m[20221213 20:30:37 @agent_ppo2.py:143][0m Total time:       1.46 min
[32m[20221213 20:30:37 @agent_ppo2.py:145][0m 217088 total steps have happened
[32m[20221213 20:30:37 @agent_ppo2.py:121][0m #------------------------ Iteration 53 --------------------------#
[32m[20221213 20:30:37 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:38 @agent_ppo2.py:185][0m |           0.0117 |           0.1017 |           0.0000 |
[32m[20221213 20:30:38 @agent_ppo2.py:185][0m |          -0.0018 |           0.0614 |           0.0000 |
[32m[20221213 20:30:38 @agent_ppo2.py:185][0m |          -0.0148 |           0.0552 |           0.0000 |
[32m[20221213 20:30:38 @agent_ppo2.py:185][0m |          -0.0170 |           0.0525 |           0.0000 |
[32m[20221213 20:30:38 @agent_ppo2.py:185][0m |          -0.0197 |           0.0511 |           0.0000 |
[32m[20221213 20:30:38 @agent_ppo2.py:185][0m |          -0.0215 |           0.0519 |           0.0000 |
[32m[20221213 20:30:38 @agent_ppo2.py:185][0m |          -0.0219 |           0.0492 |           0.0000 |
[32m[20221213 20:30:38 @agent_ppo2.py:185][0m |          -0.0192 |           0.0483 |           0.0000 |
[32m[20221213 20:30:38 @agent_ppo2.py:185][0m |          -0.0193 |           0.0461 |           0.0000 |
[32m[20221213 20:30:38 @agent_ppo2.py:185][0m |          -0.0242 |           0.0462 |           0.0000 |
[32m[20221213 20:30:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:30:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.09
[32m[20221213 20:30:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.42
[32m[20221213 20:30:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.13
[32m[20221213 20:30:39 @agent_ppo2.py:143][0m Total time:       1.49 min
[32m[20221213 20:30:39 @agent_ppo2.py:145][0m 221184 total steps have happened
[32m[20221213 20:30:39 @agent_ppo2.py:121][0m #------------------------ Iteration 54 --------------------------#
[32m[20221213 20:30:39 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:39 @agent_ppo2.py:185][0m |           0.0270 |           0.0284 |           0.0000 |
[32m[20221213 20:30:39 @agent_ppo2.py:185][0m |          -0.0032 |           0.0147 |           0.0000 |
[32m[20221213 20:30:39 @agent_ppo2.py:185][0m |          -0.0322 |           0.0142 |           0.0000 |
[32m[20221213 20:30:40 @agent_ppo2.py:185][0m |          -0.0371 |           0.0140 |           0.0000 |
[32m[20221213 20:30:40 @agent_ppo2.py:185][0m |          -0.0555 |           0.0141 |           0.0000 |
[32m[20221213 20:30:40 @agent_ppo2.py:185][0m |          -0.0486 |           0.0138 |           0.0000 |
[32m[20221213 20:30:40 @agent_ppo2.py:185][0m |          -0.0582 |           0.0140 |           0.0000 |
[32m[20221213 20:30:40 @agent_ppo2.py:185][0m |          -0.0853 |           0.0138 |           0.0000 |
[32m[20221213 20:30:40 @agent_ppo2.py:185][0m |          -0.0764 |           0.0136 |           0.0000 |
[32m[20221213 20:30:40 @agent_ppo2.py:185][0m |          -0.0747 |           0.0138 |           0.0000 |
[32m[20221213 20:30:40 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:30:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.95
[32m[20221213 20:30:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.39
[32m[20221213 20:30:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.84
[32m[20221213 20:30:40 @agent_ppo2.py:143][0m Total time:       1.52 min
[32m[20221213 20:30:40 @agent_ppo2.py:145][0m 225280 total steps have happened
[32m[20221213 20:30:40 @agent_ppo2.py:121][0m #------------------------ Iteration 55 --------------------------#
[32m[20221213 20:30:41 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:41 @agent_ppo2.py:185][0m |           0.0080 |           0.0110 |           0.0000 |
[32m[20221213 20:30:41 @agent_ppo2.py:185][0m |          -0.0215 |           0.0092 |           0.0000 |
[32m[20221213 20:30:41 @agent_ppo2.py:185][0m |          -0.0564 |           0.0089 |           0.0000 |
[32m[20221213 20:30:41 @agent_ppo2.py:185][0m |          -0.0637 |           0.0088 |           0.0000 |
[32m[20221213 20:30:41 @agent_ppo2.py:185][0m |          -0.0681 |           0.0087 |           0.0000 |
[32m[20221213 20:30:41 @agent_ppo2.py:185][0m |          -0.0750 |           0.0087 |           0.0000 |
[32m[20221213 20:30:41 @agent_ppo2.py:185][0m |          -0.0790 |           0.0086 |           0.0000 |
[32m[20221213 20:30:42 @agent_ppo2.py:185][0m |          -0.0803 |           0.0086 |           0.0000 |
[32m[20221213 20:30:42 @agent_ppo2.py:185][0m |          -0.1041 |           0.0085 |           0.0000 |
[32m[20221213 20:30:42 @agent_ppo2.py:185][0m |          -0.0773 |           0.0085 |           0.0000 |
[32m[20221213 20:30:42 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:30:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.42
[32m[20221213 20:30:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.24
[32m[20221213 20:30:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.48
[32m[20221213 20:30:42 @agent_ppo2.py:143][0m Total time:       1.55 min
[32m[20221213 20:30:42 @agent_ppo2.py:145][0m 229376 total steps have happened
[32m[20221213 20:30:42 @agent_ppo2.py:121][0m #------------------------ Iteration 56 --------------------------#
[32m[20221213 20:30:42 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:43 @agent_ppo2.py:185][0m |           0.0177 |           0.0070 |           0.0000 |
[32m[20221213 20:30:43 @agent_ppo2.py:185][0m |          -0.0285 |           0.0068 |           0.0000 |
[32m[20221213 20:30:43 @agent_ppo2.py:185][0m |          -0.0419 |           0.0066 |           0.0000 |
[32m[20221213 20:30:43 @agent_ppo2.py:185][0m |          -0.0613 |           0.0066 |           0.0000 |
[32m[20221213 20:30:43 @agent_ppo2.py:185][0m |          -0.0775 |           0.0065 |           0.0000 |
[32m[20221213 20:30:43 @agent_ppo2.py:185][0m |          -0.0842 |           0.0065 |           0.0000 |
[32m[20221213 20:30:43 @agent_ppo2.py:185][0m |          -0.0716 |           0.0064 |           0.0000 |
[32m[20221213 20:30:43 @agent_ppo2.py:185][0m |          -0.0552 |           0.0064 |           0.0000 |
[32m[20221213 20:30:43 @agent_ppo2.py:185][0m |          -0.0697 |           0.0063 |           0.0000 |
[32m[20221213 20:30:43 @agent_ppo2.py:185][0m |          -0.0750 |           0.0063 |           0.0000 |
[32m[20221213 20:30:43 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:30:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.26
[32m[20221213 20:30:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.69
[32m[20221213 20:30:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.49
[32m[20221213 20:30:44 @agent_ppo2.py:143][0m Total time:       1.58 min
[32m[20221213 20:30:44 @agent_ppo2.py:145][0m 233472 total steps have happened
[32m[20221213 20:30:44 @agent_ppo2.py:121][0m #------------------------ Iteration 57 --------------------------#
[32m[20221213 20:30:44 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:44 @agent_ppo2.py:185][0m |           0.0227 |           0.0373 |           0.0000 |
[32m[20221213 20:30:44 @agent_ppo2.py:185][0m |          -0.0013 |           0.0247 |           0.0000 |
[32m[20221213 20:30:45 @agent_ppo2.py:185][0m |          -0.0071 |           0.0223 |           0.0000 |
[32m[20221213 20:30:45 @agent_ppo2.py:185][0m |          -0.0240 |           0.0223 |           0.0000 |
[32m[20221213 20:30:45 @agent_ppo2.py:185][0m |          -0.0198 |           0.0211 |           0.0000 |
[32m[20221213 20:30:45 @agent_ppo2.py:185][0m |          -0.0180 |           0.0211 |           0.0000 |
[32m[20221213 20:30:45 @agent_ppo2.py:185][0m |          -0.0228 |           0.0212 |           0.0000 |
[32m[20221213 20:30:45 @agent_ppo2.py:185][0m |          -0.0242 |           0.0207 |           0.0000 |
[32m[20221213 20:30:45 @agent_ppo2.py:185][0m |          -0.0262 |           0.0202 |           0.0000 |
[32m[20221213 20:30:45 @agent_ppo2.py:185][0m |          -0.0276 |           0.0200 |           0.0000 |
[32m[20221213 20:30:45 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:30:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.45
[32m[20221213 20:30:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.22
[32m[20221213 20:30:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.61
[32m[20221213 20:30:45 @agent_ppo2.py:143][0m Total time:       1.61 min
[32m[20221213 20:30:45 @agent_ppo2.py:145][0m 237568 total steps have happened
[32m[20221213 20:30:45 @agent_ppo2.py:121][0m #------------------------ Iteration 58 --------------------------#
[32m[20221213 20:30:46 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:30:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:46 @agent_ppo2.py:185][0m |           0.0434 |           0.0146 |           0.0000 |
[32m[20221213 20:30:46 @agent_ppo2.py:185][0m |           0.0054 |           0.0073 |           0.0000 |
[32m[20221213 20:30:46 @agent_ppo2.py:185][0m |           0.0035 |           0.0072 |           0.0000 |
[32m[20221213 20:30:46 @agent_ppo2.py:185][0m |          -0.0383 |           0.0071 |           0.0000 |
[32m[20221213 20:30:46 @agent_ppo2.py:185][0m |          -0.0420 |           0.0069 |           0.0000 |
[32m[20221213 20:30:46 @agent_ppo2.py:185][0m |          -0.0614 |           0.0069 |           0.0000 |
[32m[20221213 20:30:47 @agent_ppo2.py:185][0m |          -0.0647 |           0.0068 |           0.0000 |
[32m[20221213 20:30:47 @agent_ppo2.py:185][0m |          -0.0658 |           0.0067 |           0.0000 |
[32m[20221213 20:30:47 @agent_ppo2.py:185][0m |          -0.0643 |           0.0067 |           0.0000 |
[32m[20221213 20:30:47 @agent_ppo2.py:185][0m |          -0.0445 |           0.0066 |           0.0000 |
[32m[20221213 20:30:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:30:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.74
[32m[20221213 20:30:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.83
[32m[20221213 20:30:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.50
[32m[20221213 20:30:47 @agent_ppo2.py:143][0m Total time:       1.63 min
[32m[20221213 20:30:47 @agent_ppo2.py:145][0m 241664 total steps have happened
[32m[20221213 20:30:47 @agent_ppo2.py:121][0m #------------------------ Iteration 59 --------------------------#
[32m[20221213 20:30:48 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:48 @agent_ppo2.py:185][0m |           0.0158 |           0.0272 |           0.0000 |
[32m[20221213 20:30:48 @agent_ppo2.py:185][0m |          -0.0011 |           0.0187 |           0.0000 |
[32m[20221213 20:30:48 @agent_ppo2.py:185][0m |          -0.0176 |           0.0177 |           0.0000 |
[32m[20221213 20:30:48 @agent_ppo2.py:185][0m |          -0.0188 |           0.0168 |           0.0000 |
[32m[20221213 20:30:48 @agent_ppo2.py:185][0m |          -0.0233 |           0.0166 |           0.0000 |
[32m[20221213 20:30:48 @agent_ppo2.py:185][0m |          -0.0262 |           0.0160 |           0.0000 |
[32m[20221213 20:30:48 @agent_ppo2.py:185][0m |          -0.0275 |           0.0156 |           0.0000 |
[32m[20221213 20:30:48 @agent_ppo2.py:185][0m |          -0.0288 |           0.0158 |           0.0000 |
[32m[20221213 20:30:48 @agent_ppo2.py:185][0m |          -0.0292 |           0.0152 |           0.0000 |
[32m[20221213 20:30:48 @agent_ppo2.py:185][0m |          -0.0313 |           0.0149 |           0.0000 |
[32m[20221213 20:30:48 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:30:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.07
[32m[20221213 20:30:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.65
[32m[20221213 20:30:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.50
[32m[20221213 20:30:49 @agent_ppo2.py:143][0m Total time:       1.66 min
[32m[20221213 20:30:49 @agent_ppo2.py:145][0m 245760 total steps have happened
[32m[20221213 20:30:49 @agent_ppo2.py:121][0m #------------------------ Iteration 60 --------------------------#
[32m[20221213 20:30:49 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:30:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:49 @agent_ppo2.py:185][0m |           0.0208 |           0.0434 |           0.0000 |
[32m[20221213 20:30:49 @agent_ppo2.py:185][0m |          -0.0019 |           0.0370 |           0.0000 |
[32m[20221213 20:30:50 @agent_ppo2.py:185][0m |          -0.0221 |           0.0367 |           0.0000 |
[32m[20221213 20:30:50 @agent_ppo2.py:185][0m |          -0.0269 |           0.0361 |           0.0000 |
[32m[20221213 20:30:50 @agent_ppo2.py:185][0m |          -0.0306 |           0.0352 |           0.0000 |
[32m[20221213 20:30:50 @agent_ppo2.py:185][0m |          -0.0323 |           0.0326 |           0.0000 |
[32m[20221213 20:30:50 @agent_ppo2.py:185][0m |          -0.0412 |           0.0310 |           0.0000 |
[32m[20221213 20:30:50 @agent_ppo2.py:185][0m |          -0.0408 |           0.0307 |           0.0000 |
[32m[20221213 20:30:50 @agent_ppo2.py:185][0m |          -0.0435 |           0.0287 |           0.0000 |
[32m[20221213 20:30:50 @agent_ppo2.py:185][0m |          -0.0491 |           0.0273 |           0.0000 |
[32m[20221213 20:30:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:30:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.86
[32m[20221213 20:30:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.34
[32m[20221213 20:30:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.59
[32m[20221213 20:30:51 @agent_ppo2.py:143][0m Total time:       1.69 min
[32m[20221213 20:30:51 @agent_ppo2.py:145][0m 249856 total steps have happened
[32m[20221213 20:30:51 @agent_ppo2.py:121][0m #------------------------ Iteration 61 --------------------------#
[32m[20221213 20:30:51 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:30:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:51 @agent_ppo2.py:185][0m |           0.0266 |           0.0777 |           0.0000 |
[32m[20221213 20:30:51 @agent_ppo2.py:185][0m |          -0.0051 |           0.0599 |           0.0000 |
[32m[20221213 20:30:51 @agent_ppo2.py:185][0m |          -0.0254 |           0.0522 |           0.0000 |
[32m[20221213 20:30:51 @agent_ppo2.py:185][0m |          -0.0180 |           0.0475 |           0.0000 |
[32m[20221213 20:30:51 @agent_ppo2.py:185][0m |          -0.0414 |           0.0424 |           0.0000 |
[32m[20221213 20:30:52 @agent_ppo2.py:185][0m |          -0.0382 |           0.0389 |           0.0000 |
[32m[20221213 20:30:52 @agent_ppo2.py:185][0m |          -0.0481 |           0.0368 |           0.0000 |
[32m[20221213 20:30:52 @agent_ppo2.py:185][0m |          -0.0544 |           0.0341 |           0.0000 |
[32m[20221213 20:30:52 @agent_ppo2.py:185][0m |          -0.0548 |           0.0333 |           0.0000 |
[32m[20221213 20:30:52 @agent_ppo2.py:185][0m |          -0.0593 |           0.0303 |           0.0000 |
[32m[20221213 20:30:52 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:30:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.63
[32m[20221213 20:30:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.10
[32m[20221213 20:30:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.39
[32m[20221213 20:30:52 @agent_ppo2.py:143][0m Total time:       1.72 min
[32m[20221213 20:30:52 @agent_ppo2.py:145][0m 253952 total steps have happened
[32m[20221213 20:30:52 @agent_ppo2.py:121][0m #------------------------ Iteration 62 --------------------------#
[32m[20221213 20:30:53 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:30:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:53 @agent_ppo2.py:185][0m |           0.0223 |           0.0352 |           0.0000 |
[32m[20221213 20:30:53 @agent_ppo2.py:185][0m |          -0.0079 |           0.0295 |           0.0000 |
[32m[20221213 20:30:53 @agent_ppo2.py:185][0m |          -0.0447 |           0.0281 |           0.0000 |
[32m[20221213 20:30:53 @agent_ppo2.py:185][0m |          -0.0695 |           0.0258 |           0.0000 |
[32m[20221213 20:30:53 @agent_ppo2.py:185][0m |          -0.0581 |           0.0245 |           0.0000 |
[32m[20221213 20:30:53 @agent_ppo2.py:185][0m |          -0.0682 |           0.0237 |           0.0000 |
[32m[20221213 20:30:53 @agent_ppo2.py:185][0m |          -0.0663 |           0.0226 |           0.0000 |
[32m[20221213 20:30:53 @agent_ppo2.py:185][0m |          -0.0743 |           0.0221 |           0.0000 |
[32m[20221213 20:30:53 @agent_ppo2.py:185][0m |          -0.0857 |           0.0215 |           0.0000 |
[32m[20221213 20:30:54 @agent_ppo2.py:185][0m |          -0.0830 |           0.0215 |           0.0000 |
[32m[20221213 20:30:54 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:30:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.74
[32m[20221213 20:30:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.38
[32m[20221213 20:30:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.71
[32m[20221213 20:30:54 @agent_ppo2.py:143][0m Total time:       1.75 min
[32m[20221213 20:30:54 @agent_ppo2.py:145][0m 258048 total steps have happened
[32m[20221213 20:30:54 @agent_ppo2.py:121][0m #------------------------ Iteration 63 --------------------------#
[32m[20221213 20:30:54 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:54 @agent_ppo2.py:185][0m |           0.0325 |           0.0160 |           0.0000 |
[32m[20221213 20:30:55 @agent_ppo2.py:185][0m |          -0.0268 |           0.0138 |           0.0000 |
[32m[20221213 20:30:55 @agent_ppo2.py:185][0m |          -0.0739 |           0.0134 |           0.0000 |
[32m[20221213 20:30:55 @agent_ppo2.py:185][0m |          -0.0542 |           0.0130 |           0.0000 |
[32m[20221213 20:30:55 @agent_ppo2.py:185][0m |          -0.0725 |           0.0126 |           0.0000 |
[32m[20221213 20:30:55 @agent_ppo2.py:185][0m |          -0.0912 |           0.0124 |           0.0000 |
[32m[20221213 20:30:55 @agent_ppo2.py:185][0m |          -0.0909 |           0.0126 |           0.0000 |
[32m[20221213 20:30:55 @agent_ppo2.py:185][0m |          -0.0848 |           0.0122 |           0.0000 |
[32m[20221213 20:30:55 @agent_ppo2.py:185][0m |          -0.1009 |           0.0118 |           0.0000 |
[32m[20221213 20:30:55 @agent_ppo2.py:185][0m |          -0.1022 |           0.0116 |           0.0000 |
[32m[20221213 20:30:55 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 20:30:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.17
[32m[20221213 20:30:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.48
[32m[20221213 20:30:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.04
[32m[20221213 20:30:56 @agent_ppo2.py:143][0m Total time:       1.77 min
[32m[20221213 20:30:56 @agent_ppo2.py:145][0m 262144 total steps have happened
[32m[20221213 20:30:56 @agent_ppo2.py:121][0m #------------------------ Iteration 64 --------------------------#
[32m[20221213 20:30:56 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:30:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:56 @agent_ppo2.py:185][0m |           0.0208 |           0.0286 |           0.0000 |
[32m[20221213 20:30:56 @agent_ppo2.py:185][0m |           0.0039 |           0.0248 |           0.0000 |
[32m[20221213 20:30:56 @agent_ppo2.py:185][0m |          -0.0105 |           0.0218 |           0.0000 |
[32m[20221213 20:30:56 @agent_ppo2.py:185][0m |          -0.0198 |           0.0203 |           0.0000 |
[32m[20221213 20:30:56 @agent_ppo2.py:185][0m |          -0.0320 |           0.0194 |           0.0000 |
[32m[20221213 20:30:56 @agent_ppo2.py:185][0m |          -0.0325 |           0.0190 |           0.0000 |
[32m[20221213 20:30:57 @agent_ppo2.py:185][0m |          -0.0389 |           0.0182 |           0.0000 |
[32m[20221213 20:30:57 @agent_ppo2.py:185][0m |          -0.0247 |           0.0185 |           0.0000 |
[32m[20221213 20:30:57 @agent_ppo2.py:185][0m |          -0.0350 |           0.0176 |           0.0000 |
[32m[20221213 20:30:57 @agent_ppo2.py:185][0m |          -0.0458 |           0.0172 |           0.0000 |
[32m[20221213 20:30:57 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:30:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.00
[32m[20221213 20:30:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.70
[32m[20221213 20:30:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.32
[32m[20221213 20:30:57 @agent_ppo2.py:143][0m Total time:       1.80 min
[32m[20221213 20:30:57 @agent_ppo2.py:145][0m 266240 total steps have happened
[32m[20221213 20:30:57 @agent_ppo2.py:121][0m #------------------------ Iteration 65 --------------------------#
[32m[20221213 20:30:57 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:58 @agent_ppo2.py:185][0m |           0.0258 |           0.0145 |           0.0000 |
[32m[20221213 20:30:58 @agent_ppo2.py:185][0m |          -0.0095 |           0.0109 |           0.0000 |
[32m[20221213 20:30:58 @agent_ppo2.py:185][0m |          -0.0377 |           0.0101 |           0.0000 |
[32m[20221213 20:30:58 @agent_ppo2.py:185][0m |          -0.0572 |           0.0097 |           0.0000 |
[32m[20221213 20:30:58 @agent_ppo2.py:185][0m |          -0.0675 |           0.0095 |           0.0000 |
[32m[20221213 20:30:58 @agent_ppo2.py:185][0m |          -0.0685 |           0.0093 |           0.0000 |
[32m[20221213 20:30:58 @agent_ppo2.py:185][0m |          -0.0793 |           0.0091 |           0.0000 |
[32m[20221213 20:30:58 @agent_ppo2.py:185][0m |          -0.0796 |           0.0089 |           0.0000 |
[32m[20221213 20:30:58 @agent_ppo2.py:185][0m |          -0.0921 |           0.0088 |           0.0000 |
[32m[20221213 20:30:58 @agent_ppo2.py:185][0m |          -0.0775 |           0.0089 |           0.0000 |
[32m[20221213 20:30:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:30:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.37
[32m[20221213 20:30:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.90
[32m[20221213 20:30:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.37
[32m[20221213 20:30:59 @agent_ppo2.py:143][0m Total time:       1.83 min
[32m[20221213 20:30:59 @agent_ppo2.py:145][0m 270336 total steps have happened
[32m[20221213 20:30:59 @agent_ppo2.py:121][0m #------------------------ Iteration 66 --------------------------#
[32m[20221213 20:30:59 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:30:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:30:59 @agent_ppo2.py:185][0m |           0.0280 |           0.0092 |           0.0000 |
[32m[20221213 20:30:59 @agent_ppo2.py:185][0m |           0.0054 |           0.0070 |           0.0000 |
[32m[20221213 20:30:59 @agent_ppo2.py:185][0m |          -0.0294 |           0.0067 |           0.0000 |
[32m[20221213 20:31:00 @agent_ppo2.py:185][0m |          -0.0368 |           0.0066 |           0.0000 |
[32m[20221213 20:31:00 @agent_ppo2.py:185][0m |          -0.0495 |           0.0065 |           0.0000 |
[32m[20221213 20:31:00 @agent_ppo2.py:185][0m |          -0.0459 |           0.0064 |           0.0000 |
[32m[20221213 20:31:00 @agent_ppo2.py:185][0m |          -0.0547 |           0.0064 |           0.0000 |
[32m[20221213 20:31:00 @agent_ppo2.py:185][0m |          -0.0363 |           0.0064 |           0.0000 |
[32m[20221213 20:31:00 @agent_ppo2.py:185][0m |          -0.0612 |           0.0063 |           0.0000 |
[32m[20221213 20:31:00 @agent_ppo2.py:185][0m |          -0.0639 |           0.0062 |           0.0000 |
[32m[20221213 20:31:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:31:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.94
[32m[20221213 20:31:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.24
[32m[20221213 20:31:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.89
[32m[20221213 20:31:00 @agent_ppo2.py:143][0m Total time:       1.86 min
[32m[20221213 20:31:00 @agent_ppo2.py:145][0m 274432 total steps have happened
[32m[20221213 20:31:00 @agent_ppo2.py:121][0m #------------------------ Iteration 67 --------------------------#
[32m[20221213 20:31:01 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:01 @agent_ppo2.py:185][0m |          -0.0226 |           0.0920 |           0.0000 |
[32m[20221213 20:31:01 @agent_ppo2.py:185][0m |           0.0463 |           0.0510 |           0.0000 |
[32m[20221213 20:31:01 @agent_ppo2.py:185][0m |           0.0231 |           0.0324 |           0.0000 |
[32m[20221213 20:31:01 @agent_ppo2.py:185][0m |           0.0099 |           0.0273 |           0.0000 |
[32m[20221213 20:31:01 @agent_ppo2.py:185][0m |          -0.0013 |           0.0249 |           0.0000 |
[32m[20221213 20:31:01 @agent_ppo2.py:185][0m |          -0.0085 |           0.0241 |           0.0000 |
[32m[20221213 20:31:02 @agent_ppo2.py:185][0m |          -0.0114 |           0.0227 |           0.0000 |
[32m[20221213 20:31:02 @agent_ppo2.py:185][0m |          -0.0166 |           0.0214 |           0.0000 |
[32m[20221213 20:31:02 @agent_ppo2.py:185][0m |          -0.0182 |           0.0204 |           0.0000 |
[32m[20221213 20:31:02 @agent_ppo2.py:185][0m |          -0.0210 |           0.0203 |           0.0000 |
[32m[20221213 20:31:02 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:31:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.45
[32m[20221213 20:31:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.70
[32m[20221213 20:31:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.40
[32m[20221213 20:31:02 @agent_ppo2.py:143][0m Total time:       1.88 min
[32m[20221213 20:31:02 @agent_ppo2.py:145][0m 278528 total steps have happened
[32m[20221213 20:31:02 @agent_ppo2.py:121][0m #------------------------ Iteration 68 --------------------------#
[32m[20221213 20:31:02 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:03 @agent_ppo2.py:185][0m |           0.0412 |           0.0196 |           0.0000 |
[32m[20221213 20:31:03 @agent_ppo2.py:185][0m |           0.0134 |           0.0156 |           0.0000 |
[32m[20221213 20:31:03 @agent_ppo2.py:185][0m |          -0.0206 |           0.0143 |           0.0000 |
[32m[20221213 20:31:03 @agent_ppo2.py:185][0m |          -0.0367 |           0.0141 |           0.0000 |
[32m[20221213 20:31:03 @agent_ppo2.py:185][0m |          -0.0525 |           0.0138 |           0.0000 |
[32m[20221213 20:31:03 @agent_ppo2.py:185][0m |          -0.0626 |           0.0133 |           0.0000 |
[32m[20221213 20:31:03 @agent_ppo2.py:185][0m |          -0.0712 |           0.0131 |           0.0000 |
[32m[20221213 20:31:03 @agent_ppo2.py:185][0m |          -0.0781 |           0.0128 |           0.0000 |
[32m[20221213 20:31:03 @agent_ppo2.py:185][0m |          -0.0794 |           0.0128 |           0.0000 |
[32m[20221213 20:31:03 @agent_ppo2.py:185][0m |          -0.0888 |           0.0126 |           0.0000 |
[32m[20221213 20:31:03 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:31:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.99
[32m[20221213 20:31:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.19
[32m[20221213 20:31:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.37
[32m[20221213 20:31:04 @agent_ppo2.py:143][0m Total time:       1.91 min
[32m[20221213 20:31:04 @agent_ppo2.py:145][0m 282624 total steps have happened
[32m[20221213 20:31:04 @agent_ppo2.py:121][0m #------------------------ Iteration 69 --------------------------#
[32m[20221213 20:31:04 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:31:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:04 @agent_ppo2.py:185][0m |           0.0235 |           0.0633 |           0.0000 |
[32m[20221213 20:31:04 @agent_ppo2.py:185][0m |          -0.0048 |           0.0399 |           0.0000 |
[32m[20221213 20:31:04 @agent_ppo2.py:185][0m |          -0.0212 |           0.0323 |           0.0000 |
[32m[20221213 20:31:04 @agent_ppo2.py:185][0m |          -0.0241 |           0.0283 |           0.0000 |
[32m[20221213 20:31:05 @agent_ppo2.py:185][0m |          -0.0252 |           0.0258 |           0.0000 |
[32m[20221213 20:31:05 @agent_ppo2.py:185][0m |          -0.0235 |           0.0242 |           0.0000 |
[32m[20221213 20:31:05 @agent_ppo2.py:185][0m |          -0.0253 |           0.0240 |           0.0000 |
[32m[20221213 20:31:05 @agent_ppo2.py:185][0m |          -0.0290 |           0.0234 |           0.0000 |
[32m[20221213 20:31:05 @agent_ppo2.py:185][0m |          -0.0310 |           0.0224 |           0.0000 |
[32m[20221213 20:31:05 @agent_ppo2.py:185][0m |          -0.0300 |           0.0223 |           0.0000 |
[32m[20221213 20:31:05 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:31:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.61
[32m[20221213 20:31:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.54
[32m[20221213 20:31:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.05
[32m[20221213 20:31:05 @agent_ppo2.py:143][0m Total time:       1.94 min
[32m[20221213 20:31:05 @agent_ppo2.py:145][0m 286720 total steps have happened
[32m[20221213 20:31:05 @agent_ppo2.py:121][0m #------------------------ Iteration 70 --------------------------#
[32m[20221213 20:31:06 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:31:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:06 @agent_ppo2.py:185][0m |           0.0112 |           0.0250 |           0.0000 |
[32m[20221213 20:31:06 @agent_ppo2.py:185][0m |          -0.0028 |           0.0221 |           0.0000 |
[32m[20221213 20:31:06 @agent_ppo2.py:185][0m |           0.0078 |           0.0197 |           0.0000 |
[32m[20221213 20:31:06 @agent_ppo2.py:185][0m |          -0.0373 |           0.0192 |           0.0000 |
[32m[20221213 20:31:06 @agent_ppo2.py:185][0m |          -0.0506 |           0.0173 |           0.0000 |
[32m[20221213 20:31:06 @agent_ppo2.py:185][0m |          -0.0551 |           0.0168 |           0.0000 |
[32m[20221213 20:31:06 @agent_ppo2.py:185][0m |          -0.0583 |           0.0168 |           0.0000 |
[32m[20221213 20:31:07 @agent_ppo2.py:185][0m |          -0.0562 |           0.0161 |           0.0000 |
[32m[20221213 20:31:07 @agent_ppo2.py:185][0m |          -0.0919 |           0.0164 |           0.0000 |
[32m[20221213 20:31:07 @agent_ppo2.py:185][0m |          -0.0879 |           0.0169 |           0.0000 |
[32m[20221213 20:31:07 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:31:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.46
[32m[20221213 20:31:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.57
[32m[20221213 20:31:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.88
[32m[20221213 20:31:07 @agent_ppo2.py:143][0m Total time:       1.97 min
[32m[20221213 20:31:07 @agent_ppo2.py:145][0m 290816 total steps have happened
[32m[20221213 20:31:07 @agent_ppo2.py:121][0m #------------------------ Iteration 71 --------------------------#
[32m[20221213 20:31:07 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:31:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:08 @agent_ppo2.py:185][0m |           0.0312 |           0.0299 |           0.0000 |
[32m[20221213 20:31:08 @agent_ppo2.py:185][0m |          -0.0117 |           0.0253 |           0.0000 |
[32m[20221213 20:31:08 @agent_ppo2.py:185][0m |          -0.0240 |           0.0240 |           0.0000 |
[32m[20221213 20:31:08 @agent_ppo2.py:185][0m |          -0.0353 |           0.0224 |           0.0000 |
[32m[20221213 20:31:08 @agent_ppo2.py:185][0m |          -0.0447 |           0.0219 |           0.0000 |
[32m[20221213 20:31:08 @agent_ppo2.py:185][0m |          -0.0478 |           0.0211 |           0.0000 |
[32m[20221213 20:31:08 @agent_ppo2.py:185][0m |          -0.0531 |           0.0205 |           0.0000 |
[32m[20221213 20:31:08 @agent_ppo2.py:185][0m |          -0.0497 |           0.0200 |           0.0000 |
[32m[20221213 20:31:08 @agent_ppo2.py:185][0m |          -0.0544 |           0.0192 |           0.0000 |
[32m[20221213 20:31:08 @agent_ppo2.py:185][0m |          -0.0461 |           0.0196 |           0.0000 |
[32m[20221213 20:31:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:31:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.35
[32m[20221213 20:31:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.32
[32m[20221213 20:31:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.66
[32m[20221213 20:31:09 @agent_ppo2.py:143][0m Total time:       1.99 min
[32m[20221213 20:31:09 @agent_ppo2.py:145][0m 294912 total steps have happened
[32m[20221213 20:31:09 @agent_ppo2.py:121][0m #------------------------ Iteration 72 --------------------------#
[32m[20221213 20:31:09 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:31:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:09 @agent_ppo2.py:185][0m |           0.0080 |           0.0721 |           0.0000 |
[32m[20221213 20:31:09 @agent_ppo2.py:185][0m |          -0.0074 |           0.0492 |           0.0000 |
[32m[20221213 20:31:10 @agent_ppo2.py:185][0m |          -0.0668 |           0.0507 |           0.0000 |
[32m[20221213 20:31:10 @agent_ppo2.py:185][0m |          -0.0270 |           0.0535 |           0.0000 |
[32m[20221213 20:31:10 @agent_ppo2.py:185][0m |          -0.0321 |           0.0378 |           0.0000 |
[32m[20221213 20:31:10 @agent_ppo2.py:185][0m |          -0.0259 |           0.0381 |           0.0000 |
[32m[20221213 20:31:10 @agent_ppo2.py:185][0m |          -0.0354 |           0.0415 |           0.0000 |
[32m[20221213 20:31:10 @agent_ppo2.py:185][0m |          -0.0335 |           0.0322 |           0.0000 |
[32m[20221213 20:31:10 @agent_ppo2.py:185][0m |          -0.0820 |           0.0354 |           0.0000 |
[32m[20221213 20:31:10 @agent_ppo2.py:185][0m |          -0.0395 |           0.0342 |           0.0000 |
[32m[20221213 20:31:10 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:31:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.36
[32m[20221213 20:31:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.95
[32m[20221213 20:31:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.08
[32m[20221213 20:31:11 @agent_ppo2.py:143][0m Total time:       2.02 min
[32m[20221213 20:31:11 @agent_ppo2.py:145][0m 299008 total steps have happened
[32m[20221213 20:31:11 @agent_ppo2.py:121][0m #------------------------ Iteration 73 --------------------------#
[32m[20221213 20:31:11 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:11 @agent_ppo2.py:185][0m |           0.0308 |           0.0274 |           0.0000 |
[32m[20221213 20:31:11 @agent_ppo2.py:185][0m |          -0.0031 |           0.0176 |           0.0000 |
[32m[20221213 20:31:11 @agent_ppo2.py:185][0m |          -0.0359 |           0.0161 |           0.0000 |
[32m[20221213 20:31:11 @agent_ppo2.py:185][0m |          -0.0483 |           0.0155 |           0.0000 |
[32m[20221213 20:31:11 @agent_ppo2.py:185][0m |          -0.0763 |           0.0151 |           0.0000 |
[32m[20221213 20:31:12 @agent_ppo2.py:185][0m |          -0.0671 |           0.0148 |           0.0000 |
[32m[20221213 20:31:12 @agent_ppo2.py:185][0m |          -0.0723 |           0.0145 |           0.0000 |
[32m[20221213 20:31:12 @agent_ppo2.py:185][0m |          -0.0638 |           0.0144 |           0.0000 |
[32m[20221213 20:31:12 @agent_ppo2.py:185][0m |          -0.0967 |           0.0140 |           0.0000 |
[32m[20221213 20:31:12 @agent_ppo2.py:185][0m |          -0.0856 |           0.0138 |           0.0000 |
[32m[20221213 20:31:12 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:31:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.34
[32m[20221213 20:31:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.72
[32m[20221213 20:31:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.93
[32m[20221213 20:31:12 @agent_ppo2.py:143][0m Total time:       2.05 min
[32m[20221213 20:31:12 @agent_ppo2.py:145][0m 303104 total steps have happened
[32m[20221213 20:31:12 @agent_ppo2.py:121][0m #------------------------ Iteration 74 --------------------------#
[32m[20221213 20:31:13 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:13 @agent_ppo2.py:185][0m |           0.0256 |           0.0200 |           0.0000 |
[32m[20221213 20:31:13 @agent_ppo2.py:185][0m |          -0.0073 |           0.0182 |           0.0000 |
[32m[20221213 20:31:13 @agent_ppo2.py:185][0m |          -0.0185 |           0.0176 |           0.0000 |
[32m[20221213 20:31:13 @agent_ppo2.py:185][0m |          -0.0303 |           0.0170 |           0.0000 |
[32m[20221213 20:31:13 @agent_ppo2.py:185][0m |          -0.0232 |           0.0170 |           0.0000 |
[32m[20221213 20:31:13 @agent_ppo2.py:185][0m |          -0.0430 |           0.0167 |           0.0000 |
[32m[20221213 20:31:13 @agent_ppo2.py:185][0m |          -0.0519 |           0.0161 |           0.0000 |
[32m[20221213 20:31:13 @agent_ppo2.py:185][0m |          -0.0555 |           0.0159 |           0.0000 |
[32m[20221213 20:31:14 @agent_ppo2.py:185][0m |          -0.0510 |           0.0158 |           0.0000 |
[32m[20221213 20:31:14 @agent_ppo2.py:185][0m |          -0.0518 |           0.0153 |           0.0000 |
[32m[20221213 20:31:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:31:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.25
[32m[20221213 20:31:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.48
[32m[20221213 20:31:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.49
[32m[20221213 20:31:14 @agent_ppo2.py:143][0m Total time:       2.08 min
[32m[20221213 20:31:14 @agent_ppo2.py:145][0m 307200 total steps have happened
[32m[20221213 20:31:14 @agent_ppo2.py:121][0m #------------------------ Iteration 75 --------------------------#
[32m[20221213 20:31:14 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:15 @agent_ppo2.py:185][0m |           0.0202 |           0.0721 |           0.0000 |
[32m[20221213 20:31:15 @agent_ppo2.py:185][0m |          -0.0000 |           0.0422 |           0.0000 |
[32m[20221213 20:31:15 @agent_ppo2.py:185][0m |          -0.0699 |           0.0501 |           0.0000 |
[32m[20221213 20:31:15 @agent_ppo2.py:185][0m |          -0.0170 |           0.0526 |           0.0000 |
[32m[20221213 20:31:15 @agent_ppo2.py:185][0m |          -0.0206 |           0.0266 |           0.0000 |
[32m[20221213 20:31:15 @agent_ppo2.py:185][0m |          -0.0229 |           0.0252 |           0.0000 |
[32m[20221213 20:31:15 @agent_ppo2.py:185][0m |          -0.0273 |           0.0255 |           0.0000 |
[32m[20221213 20:31:15 @agent_ppo2.py:185][0m |          -0.0282 |           0.0228 |           0.0000 |
[32m[20221213 20:31:15 @agent_ppo2.py:185][0m |          -0.0293 |           0.0224 |           0.0000 |
[32m[20221213 20:31:15 @agent_ppo2.py:185][0m |          -0.0283 |           0.0226 |           0.0000 |
[32m[20221213 20:31:15 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:31:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.40
[32m[20221213 20:31:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.80
[32m[20221213 20:31:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.92
[32m[20221213 20:31:16 @agent_ppo2.py:143][0m Total time:       2.11 min
[32m[20221213 20:31:16 @agent_ppo2.py:145][0m 311296 total steps have happened
[32m[20221213 20:31:16 @agent_ppo2.py:121][0m #------------------------ Iteration 76 --------------------------#
[32m[20221213 20:31:16 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:16 @agent_ppo2.py:185][0m |           0.0265 |           0.0443 |           0.0000 |
[32m[20221213 20:31:16 @agent_ppo2.py:185][0m |           0.0066 |           0.0129 |           0.0000 |
[32m[20221213 20:31:16 @agent_ppo2.py:185][0m |          -0.0067 |           0.0110 |           0.0000 |
[32m[20221213 20:31:16 @agent_ppo2.py:185][0m |          -0.0278 |           0.0106 |           0.0000 |
[32m[20221213 20:31:16 @agent_ppo2.py:185][0m |          -0.0312 |           0.0103 |           0.0000 |
[32m[20221213 20:31:17 @agent_ppo2.py:185][0m |          -0.0360 |           0.0101 |           0.0000 |
[32m[20221213 20:31:17 @agent_ppo2.py:185][0m |          -0.0368 |           0.0100 |           0.0000 |
[32m[20221213 20:31:17 @agent_ppo2.py:185][0m |          -0.0405 |           0.0099 |           0.0000 |
[32m[20221213 20:31:17 @agent_ppo2.py:185][0m |          -0.0469 |           0.0098 |           0.0000 |
[32m[20221213 20:31:17 @agent_ppo2.py:185][0m |          -0.0500 |           0.0097 |           0.0000 |
[32m[20221213 20:31:17 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:31:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.79
[32m[20221213 20:31:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.08
[32m[20221213 20:31:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.95
[32m[20221213 20:31:17 @agent_ppo2.py:143][0m Total time:       2.14 min
[32m[20221213 20:31:17 @agent_ppo2.py:145][0m 315392 total steps have happened
[32m[20221213 20:31:17 @agent_ppo2.py:121][0m #------------------------ Iteration 77 --------------------------#
[32m[20221213 20:31:18 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:31:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:18 @agent_ppo2.py:185][0m |           0.0147 |           0.0154 |           0.0000 |
[32m[20221213 20:31:18 @agent_ppo2.py:185][0m |           0.0121 |           0.0125 |           0.0000 |
[32m[20221213 20:31:18 @agent_ppo2.py:185][0m |          -0.0160 |           0.0118 |           0.0000 |
[32m[20221213 20:31:18 @agent_ppo2.py:185][0m |          -0.0445 |           0.0123 |           0.0000 |
[32m[20221213 20:31:18 @agent_ppo2.py:185][0m |          -0.0283 |           0.0116 |           0.0000 |
[32m[20221213 20:31:18 @agent_ppo2.py:185][0m |          -0.0359 |           0.0110 |           0.0000 |
[32m[20221213 20:31:18 @agent_ppo2.py:185][0m |          -0.0392 |           0.0108 |           0.0000 |
[32m[20221213 20:31:18 @agent_ppo2.py:185][0m |          -0.0401 |           0.0107 |           0.0000 |
[32m[20221213 20:31:18 @agent_ppo2.py:185][0m |          -0.0385 |           0.0107 |           0.0000 |
[32m[20221213 20:31:19 @agent_ppo2.py:185][0m |          -0.0435 |           0.0105 |           0.0000 |
[32m[20221213 20:31:19 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:31:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.43
[32m[20221213 20:31:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.05
[32m[20221213 20:31:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.46
[32m[20221213 20:31:19 @agent_ppo2.py:143][0m Total time:       2.16 min
[32m[20221213 20:31:19 @agent_ppo2.py:145][0m 319488 total steps have happened
[32m[20221213 20:31:19 @agent_ppo2.py:121][0m #------------------------ Iteration 78 --------------------------#
[32m[20221213 20:31:19 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:19 @agent_ppo2.py:185][0m |           0.0326 |           0.0269 |           0.0000 |
[32m[20221213 20:31:20 @agent_ppo2.py:185][0m |           0.0078 |           0.0205 |           0.0000 |
[32m[20221213 20:31:20 @agent_ppo2.py:185][0m |          -0.0197 |           0.0198 |           0.0000 |
[32m[20221213 20:31:20 @agent_ppo2.py:185][0m |          -0.0136 |           0.0186 |           0.0000 |
[32m[20221213 20:31:20 @agent_ppo2.py:185][0m |          -0.0172 |           0.0175 |           0.0000 |
[32m[20221213 20:31:20 @agent_ppo2.py:185][0m |          -0.0240 |           0.0173 |           0.0000 |
[32m[20221213 20:31:20 @agent_ppo2.py:185][0m |          -0.0180 |           0.0167 |           0.0000 |
[32m[20221213 20:31:20 @agent_ppo2.py:185][0m |          -0.0282 |           0.0160 |           0.0000 |
[32m[20221213 20:31:20 @agent_ppo2.py:185][0m |          -0.0340 |           0.0163 |           0.0000 |
[32m[20221213 20:31:20 @agent_ppo2.py:185][0m |          -0.0284 |           0.0162 |           0.0000 |
[32m[20221213 20:31:20 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:31:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.27
[32m[20221213 20:31:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.21
[32m[20221213 20:31:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.34
[32m[20221213 20:31:21 @agent_ppo2.py:143][0m Total time:       2.19 min
[32m[20221213 20:31:21 @agent_ppo2.py:145][0m 323584 total steps have happened
[32m[20221213 20:31:21 @agent_ppo2.py:121][0m #------------------------ Iteration 79 --------------------------#
[32m[20221213 20:31:21 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:21 @agent_ppo2.py:185][0m |           0.0308 |           0.0154 |           0.0000 |
[32m[20221213 20:31:21 @agent_ppo2.py:185][0m |          -0.0021 |           0.0143 |           0.0000 |
[32m[20221213 20:31:21 @agent_ppo2.py:185][0m |          -0.0307 |           0.0137 |           0.0000 |
[32m[20221213 20:31:21 @agent_ppo2.py:185][0m |          -0.0790 |           0.0139 |           0.0000 |
[32m[20221213 20:31:21 @agent_ppo2.py:185][0m |           0.0108 |           0.0133 |           0.0000 |
[32m[20221213 20:31:22 @agent_ppo2.py:185][0m |          -0.0260 |           0.0126 |           0.0000 |
[32m[20221213 20:31:22 @agent_ppo2.py:185][0m |          -0.0470 |           0.0124 |           0.0000 |
[32m[20221213 20:31:22 @agent_ppo2.py:185][0m |          -0.0610 |           0.0121 |           0.0000 |
[32m[20221213 20:31:22 @agent_ppo2.py:185][0m |          -0.0673 |           0.0121 |           0.0000 |
[32m[20221213 20:31:22 @agent_ppo2.py:185][0m |          -0.0840 |           0.0119 |           0.0000 |
[32m[20221213 20:31:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:31:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.03
[32m[20221213 20:31:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.99
[32m[20221213 20:31:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.45
[32m[20221213 20:31:22 @agent_ppo2.py:143][0m Total time:       2.22 min
[32m[20221213 20:31:22 @agent_ppo2.py:145][0m 327680 total steps have happened
[32m[20221213 20:31:22 @agent_ppo2.py:121][0m #------------------------ Iteration 80 --------------------------#
[32m[20221213 20:31:23 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:31:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:23 @agent_ppo2.py:185][0m |           0.0192 |           0.0444 |           0.0000 |
[32m[20221213 20:31:23 @agent_ppo2.py:185][0m |          -0.0048 |           0.0321 |           0.0000 |
[32m[20221213 20:31:23 @agent_ppo2.py:185][0m |          -0.0114 |           0.0281 |           0.0000 |
[32m[20221213 20:31:23 @agent_ppo2.py:185][0m |          -0.0194 |           0.0263 |           0.0000 |
[32m[20221213 20:31:23 @agent_ppo2.py:185][0m |          -0.0261 |           0.0257 |           0.0000 |
[32m[20221213 20:31:23 @agent_ppo2.py:185][0m |          -0.0249 |           0.0243 |           0.0000 |
[32m[20221213 20:31:23 @agent_ppo2.py:185][0m |          -0.0295 |           0.0239 |           0.0000 |
[32m[20221213 20:31:23 @agent_ppo2.py:185][0m |          -0.0278 |           0.0233 |           0.0000 |
[32m[20221213 20:31:24 @agent_ppo2.py:185][0m |          -0.0248 |           0.0227 |           0.0000 |
[32m[20221213 20:31:24 @agent_ppo2.py:185][0m |          -0.0316 |           0.0220 |           0.0000 |
[32m[20221213 20:31:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:31:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.77
[32m[20221213 20:31:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.53
[32m[20221213 20:31:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.31
[32m[20221213 20:31:24 @agent_ppo2.py:143][0m Total time:       2.25 min
[32m[20221213 20:31:24 @agent_ppo2.py:145][0m 331776 total steps have happened
[32m[20221213 20:31:24 @agent_ppo2.py:121][0m #------------------------ Iteration 81 --------------------------#
[32m[20221213 20:31:24 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:31:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:25 @agent_ppo2.py:185][0m |           0.0109 |           0.0341 |           0.0000 |
[32m[20221213 20:31:25 @agent_ppo2.py:185][0m |           0.0114 |           0.0099 |           0.0000 |
[32m[20221213 20:31:25 @agent_ppo2.py:185][0m |           0.0003 |           0.0090 |           0.0000 |
[32m[20221213 20:31:25 @agent_ppo2.py:185][0m |          -0.0122 |           0.0087 |           0.0000 |
[32m[20221213 20:31:25 @agent_ppo2.py:185][0m |          -0.0355 |           0.0086 |           0.0000 |
[32m[20221213 20:31:25 @agent_ppo2.py:185][0m |          -0.0281 |           0.0084 |           0.0000 |
[32m[20221213 20:31:25 @agent_ppo2.py:185][0m |          -0.0311 |           0.0083 |           0.0000 |
[32m[20221213 20:31:25 @agent_ppo2.py:185][0m |          -0.0359 |           0.0082 |           0.0000 |
[32m[20221213 20:31:25 @agent_ppo2.py:185][0m |          -0.0181 |           0.0082 |           0.0000 |
[32m[20221213 20:31:25 @agent_ppo2.py:185][0m |          -0.0391 |           0.0081 |           0.0000 |
[32m[20221213 20:31:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:31:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.65
[32m[20221213 20:31:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.97
[32m[20221213 20:31:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.37
[32m[20221213 20:31:26 @agent_ppo2.py:143][0m Total time:       2.28 min
[32m[20221213 20:31:26 @agent_ppo2.py:145][0m 335872 total steps have happened
[32m[20221213 20:31:26 @agent_ppo2.py:121][0m #------------------------ Iteration 82 --------------------------#
[32m[20221213 20:31:26 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:26 @agent_ppo2.py:185][0m |           0.0217 |           0.1217 |           0.0000 |
[32m[20221213 20:31:26 @agent_ppo2.py:185][0m |          -0.0010 |           0.0540 |           0.0000 |
[32m[20221213 20:31:26 @agent_ppo2.py:185][0m |          -0.0563 |           0.0353 |           0.0000 |
[32m[20221213 20:31:26 @agent_ppo2.py:185][0m |          -0.0178 |           0.0366 |           0.0000 |
[32m[20221213 20:31:27 @agent_ppo2.py:185][0m |          -0.0199 |           0.0275 |           0.0000 |
[32m[20221213 20:31:27 @agent_ppo2.py:185][0m |          -0.0164 |           0.0259 |           0.0000 |
[32m[20221213 20:31:27 @agent_ppo2.py:185][0m |          -0.0172 |           0.0264 |           0.0000 |
[32m[20221213 20:31:27 @agent_ppo2.py:185][0m |          -0.0228 |           0.0243 |           0.0000 |
[32m[20221213 20:31:27 @agent_ppo2.py:185][0m |          -0.0241 |           0.0237 |           0.0000 |
[32m[20221213 20:31:27 @agent_ppo2.py:185][0m |          -0.0250 |           0.0225 |           0.0000 |
[32m[20221213 20:31:27 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:31:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.15
[32m[20221213 20:31:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.70
[32m[20221213 20:31:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.51
[32m[20221213 20:31:27 @agent_ppo2.py:143][0m Total time:       2.30 min
[32m[20221213 20:31:27 @agent_ppo2.py:145][0m 339968 total steps have happened
[32m[20221213 20:31:27 @agent_ppo2.py:121][0m #------------------------ Iteration 83 --------------------------#
[32m[20221213 20:31:28 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:31:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:28 @agent_ppo2.py:185][0m |           0.0312 |           0.0421 |           0.0000 |
[32m[20221213 20:31:28 @agent_ppo2.py:185][0m |          -0.0028 |           0.0370 |           0.0000 |
[32m[20221213 20:31:28 @agent_ppo2.py:185][0m |          -0.0254 |           0.0333 |           0.0000 |
[32m[20221213 20:31:28 @agent_ppo2.py:185][0m |          -0.0577 |           0.0314 |           0.0000 |
[32m[20221213 20:31:28 @agent_ppo2.py:185][0m |          -0.0452 |           0.0312 |           0.0000 |
[32m[20221213 20:31:28 @agent_ppo2.py:185][0m |          -0.0451 |           0.0271 |           0.0000 |
[32m[20221213 20:31:28 @agent_ppo2.py:185][0m |          -0.0528 |           0.0257 |           0.0000 |
[32m[20221213 20:31:28 @agent_ppo2.py:185][0m |          -0.0874 |           0.0257 |           0.0000 |
[32m[20221213 20:31:29 @agent_ppo2.py:185][0m |          -0.0578 |           0.0253 |           0.0000 |
[32m[20221213 20:31:29 @agent_ppo2.py:185][0m |          -0.0940 |           0.0255 |           0.0000 |
[32m[20221213 20:31:29 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:31:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.04
[32m[20221213 20:31:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.88
[32m[20221213 20:31:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.38
[32m[20221213 20:31:29 @agent_ppo2.py:143][0m Total time:       2.33 min
[32m[20221213 20:31:29 @agent_ppo2.py:145][0m 344064 total steps have happened
[32m[20221213 20:31:29 @agent_ppo2.py:121][0m #------------------------ Iteration 84 --------------------------#
[32m[20221213 20:31:29 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 20:31:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:30 @agent_ppo2.py:185][0m |           0.0189 |           0.0371 |           0.0000 |
[32m[20221213 20:31:30 @agent_ppo2.py:185][0m |           0.0045 |           0.0144 |           0.0000 |
[32m[20221213 20:31:30 @agent_ppo2.py:185][0m |          -0.0261 |           0.0136 |           0.0000 |
[32m[20221213 20:31:30 @agent_ppo2.py:185][0m |          -0.0363 |           0.0133 |           0.0000 |
[32m[20221213 20:31:30 @agent_ppo2.py:185][0m |          -0.0336 |           0.0130 |           0.0000 |
[32m[20221213 20:31:30 @agent_ppo2.py:185][0m |          -0.0445 |           0.0128 |           0.0000 |
[32m[20221213 20:31:30 @agent_ppo2.py:185][0m |          -0.0503 |           0.0126 |           0.0000 |
[32m[20221213 20:31:30 @agent_ppo2.py:185][0m |          -0.0534 |           0.0125 |           0.0000 |
[32m[20221213 20:31:30 @agent_ppo2.py:185][0m |          -0.0477 |           0.0123 |           0.0000 |
[32m[20221213 20:31:30 @agent_ppo2.py:185][0m |          -0.0520 |           0.0121 |           0.0000 |
[32m[20221213 20:31:30 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:31:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.32
[32m[20221213 20:31:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.66
[32m[20221213 20:31:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.90
[32m[20221213 20:31:31 @agent_ppo2.py:143][0m Total time:       2.36 min
[32m[20221213 20:31:31 @agent_ppo2.py:145][0m 348160 total steps have happened
[32m[20221213 20:31:31 @agent_ppo2.py:121][0m #------------------------ Iteration 85 --------------------------#
[32m[20221213 20:31:31 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:31 @agent_ppo2.py:185][0m |           0.0258 |           0.0121 |           0.0000 |
[32m[20221213 20:31:31 @agent_ppo2.py:185][0m |          -0.0086 |           0.0112 |           0.0000 |
[32m[20221213 20:31:31 @agent_ppo2.py:185][0m |          -0.0282 |           0.0111 |           0.0000 |
[32m[20221213 20:31:31 @agent_ppo2.py:185][0m |          -0.0504 |           0.0109 |           0.0000 |
[32m[20221213 20:31:31 @agent_ppo2.py:185][0m |          -0.0630 |           0.0108 |           0.0000 |
[32m[20221213 20:31:32 @agent_ppo2.py:185][0m |          -0.0743 |           0.0107 |           0.0000 |
[32m[20221213 20:31:32 @agent_ppo2.py:185][0m |          -0.0713 |           0.0106 |           0.0000 |
[32m[20221213 20:31:32 @agent_ppo2.py:185][0m |          -0.0812 |           0.0105 |           0.0000 |
[32m[20221213 20:31:32 @agent_ppo2.py:185][0m |          -0.0642 |           0.0105 |           0.0000 |
[32m[20221213 20:31:32 @agent_ppo2.py:185][0m |          -0.0651 |           0.0104 |           0.0000 |
[32m[20221213 20:31:32 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:31:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.52
[32m[20221213 20:31:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.26
[32m[20221213 20:31:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.83
[32m[20221213 20:31:32 @agent_ppo2.py:143][0m Total time:       2.38 min
[32m[20221213 20:31:32 @agent_ppo2.py:145][0m 352256 total steps have happened
[32m[20221213 20:31:32 @agent_ppo2.py:121][0m #------------------------ Iteration 86 --------------------------#
[32m[20221213 20:31:33 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:33 @agent_ppo2.py:185][0m |           0.0305 |           0.0141 |           0.0000 |
[32m[20221213 20:31:33 @agent_ppo2.py:185][0m |          -0.0007 |           0.0134 |           0.0000 |
[32m[20221213 20:31:33 @agent_ppo2.py:185][0m |          -0.0329 |           0.0130 |           0.0000 |
[32m[20221213 20:31:33 @agent_ppo2.py:185][0m |          -0.0437 |           0.0127 |           0.0000 |
[32m[20221213 20:31:33 @agent_ppo2.py:185][0m |          -0.0423 |           0.0126 |           0.0000 |
[32m[20221213 20:31:33 @agent_ppo2.py:185][0m |          -0.0777 |           0.0136 |           0.0000 |
[32m[20221213 20:31:33 @agent_ppo2.py:185][0m |          -0.0462 |           0.0127 |           0.0000 |
[32m[20221213 20:31:33 @agent_ppo2.py:185][0m |          -0.0547 |           0.0120 |           0.0000 |
[32m[20221213 20:31:33 @agent_ppo2.py:185][0m |          -0.0643 |           0.0120 |           0.0000 |
[32m[20221213 20:31:33 @agent_ppo2.py:185][0m |          -0.0537 |           0.0118 |           0.0000 |
[32m[20221213 20:31:33 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:31:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.21
[32m[20221213 20:31:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.44
[32m[20221213 20:31:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.70
[32m[20221213 20:31:34 @agent_ppo2.py:143][0m Total time:       2.41 min
[32m[20221213 20:31:34 @agent_ppo2.py:145][0m 356352 total steps have happened
[32m[20221213 20:31:34 @agent_ppo2.py:121][0m #------------------------ Iteration 87 --------------------------#
[32m[20221213 20:31:34 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:31:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:34 @agent_ppo2.py:185][0m |           0.0123 |           0.0138 |           0.0000 |
[32m[20221213 20:31:35 @agent_ppo2.py:185][0m |          -0.0138 |           0.0118 |           0.0000 |
[32m[20221213 20:31:35 @agent_ppo2.py:185][0m |          -0.0168 |           0.0115 |           0.0000 |
[32m[20221213 20:31:35 @agent_ppo2.py:185][0m |          -0.0212 |           0.0115 |           0.0000 |
[32m[20221213 20:31:35 @agent_ppo2.py:185][0m |          -0.0279 |           0.0112 |           0.0000 |
[32m[20221213 20:31:35 @agent_ppo2.py:185][0m |          -0.0331 |           0.0110 |           0.0000 |
[32m[20221213 20:31:35 @agent_ppo2.py:185][0m |          -0.0365 |           0.0109 |           0.0000 |
[32m[20221213 20:31:35 @agent_ppo2.py:185][0m |          -0.0374 |           0.0107 |           0.0000 |
[32m[20221213 20:31:35 @agent_ppo2.py:185][0m |          -0.0416 |           0.0106 |           0.0000 |
[32m[20221213 20:31:35 @agent_ppo2.py:185][0m |          -0.0445 |           0.0106 |           0.0000 |
[32m[20221213 20:31:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:31:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.47
[32m[20221213 20:31:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.87
[32m[20221213 20:31:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.64
[32m[20221213 20:31:36 @agent_ppo2.py:143][0m Total time:       2.44 min
[32m[20221213 20:31:36 @agent_ppo2.py:145][0m 360448 total steps have happened
[32m[20221213 20:31:36 @agent_ppo2.py:121][0m #------------------------ Iteration 88 --------------------------#
[32m[20221213 20:31:36 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:31:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:36 @agent_ppo2.py:185][0m |           0.0313 |           0.0112 |           0.0000 |
[32m[20221213 20:31:36 @agent_ppo2.py:185][0m |          -0.0200 |           0.0106 |           0.0000 |
[32m[20221213 20:31:36 @agent_ppo2.py:185][0m |          -0.0403 |           0.0104 |           0.0000 |
[32m[20221213 20:31:36 @agent_ppo2.py:185][0m |          -0.0405 |           0.0104 |           0.0000 |
[32m[20221213 20:31:36 @agent_ppo2.py:185][0m |          -0.0516 |           0.0102 |           0.0000 |
[32m[20221213 20:31:37 @agent_ppo2.py:185][0m |          -0.0479 |           0.0099 |           0.0000 |
[32m[20221213 20:31:37 @agent_ppo2.py:185][0m |          -0.0473 |           0.0099 |           0.0000 |
[32m[20221213 20:31:37 @agent_ppo2.py:185][0m |          -0.0532 |           0.0096 |           0.0000 |
[32m[20221213 20:31:37 @agent_ppo2.py:185][0m |          -0.0691 |           0.0096 |           0.0000 |
[32m[20221213 20:31:37 @agent_ppo2.py:185][0m |          -0.0756 |           0.0095 |           0.0000 |
[32m[20221213 20:31:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:31:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.85
[32m[20221213 20:31:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.27
[32m[20221213 20:31:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.81
[32m[20221213 20:31:37 @agent_ppo2.py:143][0m Total time:       2.47 min
[32m[20221213 20:31:37 @agent_ppo2.py:145][0m 364544 total steps have happened
[32m[20221213 20:31:37 @agent_ppo2.py:121][0m #------------------------ Iteration 89 --------------------------#
[32m[20221213 20:31:38 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:38 @agent_ppo2.py:185][0m |           0.0391 |           0.0119 |           0.0000 |
[32m[20221213 20:31:38 @agent_ppo2.py:185][0m |          -0.0065 |           0.0115 |           0.0000 |
[32m[20221213 20:31:38 @agent_ppo2.py:185][0m |          -0.0484 |           0.0116 |           0.0000 |
[32m[20221213 20:31:38 @agent_ppo2.py:185][0m |           0.0028 |           0.0115 |           0.0000 |
[32m[20221213 20:31:38 @agent_ppo2.py:185][0m |          -0.0236 |           0.0108 |           0.0000 |
[32m[20221213 20:31:38 @agent_ppo2.py:185][0m |          -0.0316 |           0.0106 |           0.0000 |
[32m[20221213 20:31:38 @agent_ppo2.py:185][0m |          -0.0333 |           0.0105 |           0.0000 |
[32m[20221213 20:31:38 @agent_ppo2.py:185][0m |          -0.0478 |           0.0106 |           0.0000 |
[32m[20221213 20:31:38 @agent_ppo2.py:185][0m |          -0.0577 |           0.0102 |           0.0000 |
[32m[20221213 20:31:39 @agent_ppo2.py:185][0m |          -0.0659 |           0.0102 |           0.0000 |
[32m[20221213 20:31:39 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:31:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.98
[32m[20221213 20:31:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.94
[32m[20221213 20:31:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.60
[32m[20221213 20:31:39 @agent_ppo2.py:143][0m Total time:       2.50 min
[32m[20221213 20:31:39 @agent_ppo2.py:145][0m 368640 total steps have happened
[32m[20221213 20:31:39 @agent_ppo2.py:121][0m #------------------------ Iteration 90 --------------------------#
[32m[20221213 20:31:39 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:31:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:39 @agent_ppo2.py:185][0m |           0.0287 |           0.0093 |           0.0000 |
[32m[20221213 20:31:40 @agent_ppo2.py:185][0m |           0.0056 |           0.0087 |           0.0000 |
[32m[20221213 20:31:40 @agent_ppo2.py:185][0m |          -0.0180 |           0.0085 |           0.0000 |
[32m[20221213 20:31:40 @agent_ppo2.py:185][0m |          -0.0217 |           0.0084 |           0.0000 |
[32m[20221213 20:31:40 @agent_ppo2.py:185][0m |          -0.0515 |           0.0084 |           0.0000 |
[32m[20221213 20:31:40 @agent_ppo2.py:185][0m |          -0.0512 |           0.0082 |           0.0000 |
[32m[20221213 20:31:40 @agent_ppo2.py:185][0m |          -0.0612 |           0.0081 |           0.0000 |
[32m[20221213 20:31:40 @agent_ppo2.py:185][0m |          -0.0617 |           0.0081 |           0.0000 |
[32m[20221213 20:31:40 @agent_ppo2.py:185][0m |          -0.0642 |           0.0080 |           0.0000 |
[32m[20221213 20:31:40 @agent_ppo2.py:185][0m |          -0.0612 |           0.0079 |           0.0000 |
[32m[20221213 20:31:40 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:31:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.54
[32m[20221213 20:31:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.97
[32m[20221213 20:31:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.67
[32m[20221213 20:31:41 @agent_ppo2.py:143][0m Total time:       2.52 min
[32m[20221213 20:31:41 @agent_ppo2.py:145][0m 372736 total steps have happened
[32m[20221213 20:31:41 @agent_ppo2.py:121][0m #------------------------ Iteration 91 --------------------------#
[32m[20221213 20:31:41 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:41 @agent_ppo2.py:185][0m |           0.0239 |           0.0163 |           0.0000 |
[32m[20221213 20:31:41 @agent_ppo2.py:185][0m |           0.0068 |           0.0134 |           0.0000 |
[32m[20221213 20:31:41 @agent_ppo2.py:185][0m |          -0.0062 |           0.0124 |           0.0000 |
[32m[20221213 20:31:41 @agent_ppo2.py:185][0m |          -0.0119 |           0.0120 |           0.0000 |
[32m[20221213 20:31:42 @agent_ppo2.py:185][0m |          -0.0173 |           0.0118 |           0.0000 |
[32m[20221213 20:31:42 @agent_ppo2.py:185][0m |          -0.0253 |           0.0120 |           0.0000 |
[32m[20221213 20:31:42 @agent_ppo2.py:185][0m |          -0.0187 |           0.0119 |           0.0000 |
[32m[20221213 20:31:42 @agent_ppo2.py:185][0m |          -0.0269 |           0.0117 |           0.0000 |
[32m[20221213 20:31:42 @agent_ppo2.py:185][0m |          -0.0286 |           0.0111 |           0.0000 |
[32m[20221213 20:31:42 @agent_ppo2.py:185][0m |          -0.0310 |           0.0108 |           0.0000 |
[32m[20221213 20:31:42 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 20:31:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.03
[32m[20221213 20:31:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.91
[32m[20221213 20:31:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.56
[32m[20221213 20:31:43 @agent_ppo2.py:143][0m Total time:       2.56 min
[32m[20221213 20:31:43 @agent_ppo2.py:145][0m 376832 total steps have happened
[32m[20221213 20:31:43 @agent_ppo2.py:121][0m #------------------------ Iteration 92 --------------------------#
[32m[20221213 20:31:43 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:43 @agent_ppo2.py:185][0m |           0.0308 |           0.0142 |           0.0000 |
[32m[20221213 20:31:43 @agent_ppo2.py:185][0m |          -0.0032 |           0.0130 |           0.0000 |
[32m[20221213 20:31:43 @agent_ppo2.py:185][0m |          -0.0945 |           0.0193 |           0.0000 |
[32m[20221213 20:31:43 @agent_ppo2.py:185][0m |          -0.0349 |           0.0184 |           0.0000 |
[32m[20221213 20:31:43 @agent_ppo2.py:185][0m |          -0.0363 |           0.0121 |           0.0000 |
[32m[20221213 20:31:44 @agent_ppo2.py:185][0m |          -0.0485 |           0.0116 |           0.0000 |
[32m[20221213 20:31:44 @agent_ppo2.py:185][0m |          -0.0509 |           0.0116 |           0.0000 |
[32m[20221213 20:31:44 @agent_ppo2.py:185][0m |          -0.0561 |           0.0114 |           0.0000 |
[32m[20221213 20:31:44 @agent_ppo2.py:185][0m |          -0.0538 |           0.0113 |           0.0000 |
[32m[20221213 20:31:44 @agent_ppo2.py:185][0m |          -0.0613 |           0.0112 |           0.0000 |
[32m[20221213 20:31:44 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 20:31:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.70
[32m[20221213 20:31:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.36
[32m[20221213 20:31:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.16
[32m[20221213 20:31:44 @agent_ppo2.py:143][0m Total time:       2.59 min
[32m[20221213 20:31:44 @agent_ppo2.py:145][0m 380928 total steps have happened
[32m[20221213 20:31:44 @agent_ppo2.py:121][0m #------------------------ Iteration 93 --------------------------#
[32m[20221213 20:31:45 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:45 @agent_ppo2.py:185][0m |           0.0217 |           0.0145 |           0.0000 |
[32m[20221213 20:31:45 @agent_ppo2.py:185][0m |          -0.0094 |           0.0075 |           0.0000 |
[32m[20221213 20:31:45 @agent_ppo2.py:185][0m |          -0.0089 |           0.0068 |           0.0000 |
[32m[20221213 20:31:45 @agent_ppo2.py:185][0m |          -0.0194 |           0.0065 |           0.0000 |
[32m[20221213 20:31:45 @agent_ppo2.py:185][0m |          -0.0360 |           0.0064 |           0.0000 |
[32m[20221213 20:31:45 @agent_ppo2.py:185][0m |          -0.0332 |           0.0062 |           0.0000 |
[32m[20221213 20:31:45 @agent_ppo2.py:185][0m |          -0.0399 |           0.0062 |           0.0000 |
[32m[20221213 20:31:46 @agent_ppo2.py:185][0m |          -0.0489 |           0.0061 |           0.0000 |
[32m[20221213 20:31:46 @agent_ppo2.py:185][0m |          -0.0377 |           0.0060 |           0.0000 |
[32m[20221213 20:31:46 @agent_ppo2.py:185][0m |          -0.0391 |           0.0059 |           0.0000 |
[32m[20221213 20:31:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:31:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.41
[32m[20221213 20:31:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.54
[32m[20221213 20:31:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.40
[32m[20221213 20:31:46 @agent_ppo2.py:143][0m Total time:       2.62 min
[32m[20221213 20:31:46 @agent_ppo2.py:145][0m 385024 total steps have happened
[32m[20221213 20:31:46 @agent_ppo2.py:121][0m #------------------------ Iteration 94 --------------------------#
[32m[20221213 20:31:47 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:31:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:47 @agent_ppo2.py:185][0m |           0.0213 |           0.0689 |           0.0000 |
[32m[20221213 20:31:47 @agent_ppo2.py:185][0m |           0.0094 |           0.0349 |           0.0000 |
[32m[20221213 20:31:47 @agent_ppo2.py:185][0m |          -0.0097 |           0.0261 |           0.0000 |
[32m[20221213 20:31:47 @agent_ppo2.py:185][0m |          -0.0488 |           0.0220 |           0.0000 |
[32m[20221213 20:31:47 @agent_ppo2.py:185][0m |          -0.0147 |           0.0199 |           0.0000 |
[32m[20221213 20:31:47 @agent_ppo2.py:185][0m |          -0.0197 |           0.0198 |           0.0000 |
[32m[20221213 20:31:47 @agent_ppo2.py:185][0m |          -0.0220 |           0.0195 |           0.0000 |
[32m[20221213 20:31:47 @agent_ppo2.py:185][0m |          -0.0229 |           0.0178 |           0.0000 |
[32m[20221213 20:31:47 @agent_ppo2.py:185][0m |          -0.0246 |           0.0170 |           0.0000 |
[32m[20221213 20:31:48 @agent_ppo2.py:185][0m |          -0.0244 |           0.0172 |           0.0000 |
[32m[20221213 20:31:48 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:31:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.59
[32m[20221213 20:31:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.86
[32m[20221213 20:31:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.79
[32m[20221213 20:31:48 @agent_ppo2.py:143][0m Total time:       2.65 min
[32m[20221213 20:31:48 @agent_ppo2.py:145][0m 389120 total steps have happened
[32m[20221213 20:31:48 @agent_ppo2.py:121][0m #------------------------ Iteration 95 --------------------------#
[32m[20221213 20:31:48 @agent_ppo2.py:127][0m Sampling time: 0.42 s by 5 slaves
[32m[20221213 20:31:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:49 @agent_ppo2.py:185][0m |           0.0357 |           0.0189 |           0.0000 |
[32m[20221213 20:31:49 @agent_ppo2.py:185][0m |           0.0055 |           0.0104 |           0.0000 |
[32m[20221213 20:31:49 @agent_ppo2.py:185][0m |          -0.0117 |           0.0098 |           0.0000 |
[32m[20221213 20:31:49 @agent_ppo2.py:185][0m |          -0.0317 |           0.0095 |           0.0000 |
[32m[20221213 20:31:49 @agent_ppo2.py:185][0m |          -0.0364 |           0.0094 |           0.0000 |
[32m[20221213 20:31:49 @agent_ppo2.py:185][0m |          -0.0474 |           0.0091 |           0.0000 |
[32m[20221213 20:31:49 @agent_ppo2.py:185][0m |          -0.0499 |           0.0091 |           0.0000 |
[32m[20221213 20:31:49 @agent_ppo2.py:185][0m |          -0.0528 |           0.0090 |           0.0000 |
[32m[20221213 20:31:49 @agent_ppo2.py:185][0m |          -0.0474 |           0.0088 |           0.0000 |
[32m[20221213 20:31:49 @agent_ppo2.py:185][0m |          -0.0587 |           0.0087 |           0.0000 |
[32m[20221213 20:31:49 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:31:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.00
[32m[20221213 20:31:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.61
[32m[20221213 20:31:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.91
[32m[20221213 20:31:50 @agent_ppo2.py:143][0m Total time:       2.68 min
[32m[20221213 20:31:50 @agent_ppo2.py:145][0m 393216 total steps have happened
[32m[20221213 20:31:50 @agent_ppo2.py:121][0m #------------------------ Iteration 96 --------------------------#
[32m[20221213 20:31:50 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:31:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:50 @agent_ppo2.py:185][0m |           0.0326 |           0.1192 |           0.0000 |
[32m[20221213 20:31:50 @agent_ppo2.py:185][0m |           0.0043 |           0.0616 |           0.0000 |
[32m[20221213 20:31:50 @agent_ppo2.py:185][0m |          -0.0068 |           0.0431 |           0.0000 |
[32m[20221213 20:31:51 @agent_ppo2.py:185][0m |          -0.0099 |           0.0364 |           0.0000 |
[32m[20221213 20:31:51 @agent_ppo2.py:185][0m |          -0.0149 |           0.0338 |           0.0000 |
[32m[20221213 20:31:51 @agent_ppo2.py:185][0m |          -0.0171 |           0.0305 |           0.0000 |
[32m[20221213 20:31:51 @agent_ppo2.py:185][0m |          -0.0225 |           0.0294 |           0.0000 |
[32m[20221213 20:31:51 @agent_ppo2.py:185][0m |          -0.0240 |           0.0267 |           0.0000 |
[32m[20221213 20:31:51 @agent_ppo2.py:185][0m |          -0.0268 |           0.0257 |           0.0000 |
[32m[20221213 20:31:51 @agent_ppo2.py:185][0m |          -0.0285 |           0.0252 |           0.0000 |
[32m[20221213 20:31:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:31:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.13
[32m[20221213 20:31:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.92
[32m[20221213 20:31:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.93
[32m[20221213 20:31:51 @agent_ppo2.py:143][0m Total time:       2.70 min
[32m[20221213 20:31:51 @agent_ppo2.py:145][0m 397312 total steps have happened
[32m[20221213 20:31:51 @agent_ppo2.py:121][0m #------------------------ Iteration 97 --------------------------#
[32m[20221213 20:31:52 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:52 @agent_ppo2.py:185][0m |           0.0403 |           0.0421 |           0.0000 |
[32m[20221213 20:31:52 @agent_ppo2.py:185][0m |          -0.0113 |           0.0106 |           0.0000 |
[32m[20221213 20:31:52 @agent_ppo2.py:185][0m |          -0.0229 |           0.0095 |           0.0000 |
[32m[20221213 20:31:52 @agent_ppo2.py:185][0m |          -0.0257 |           0.0093 |           0.0000 |
[32m[20221213 20:31:52 @agent_ppo2.py:185][0m |          -0.0333 |           0.0091 |           0.0000 |
[32m[20221213 20:31:52 @agent_ppo2.py:185][0m |          -0.0382 |           0.0090 |           0.0000 |
[32m[20221213 20:31:53 @agent_ppo2.py:185][0m |          -0.0417 |           0.0088 |           0.0000 |
[32m[20221213 20:31:53 @agent_ppo2.py:185][0m |          -0.0444 |           0.0088 |           0.0000 |
[32m[20221213 20:31:53 @agent_ppo2.py:185][0m |          -0.0435 |           0.0087 |           0.0000 |
[32m[20221213 20:31:53 @agent_ppo2.py:185][0m |          -0.0467 |           0.0086 |           0.0000 |
[32m[20221213 20:31:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:31:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.49
[32m[20221213 20:31:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.65
[32m[20221213 20:31:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.02
[32m[20221213 20:31:53 @agent_ppo2.py:143][0m Total time:       2.73 min
[32m[20221213 20:31:53 @agent_ppo2.py:145][0m 401408 total steps have happened
[32m[20221213 20:31:53 @agent_ppo2.py:121][0m #------------------------ Iteration 98 --------------------------#
[32m[20221213 20:31:54 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:31:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:54 @agent_ppo2.py:185][0m |           0.0232 |           0.0107 |           0.0000 |
[32m[20221213 20:31:54 @agent_ppo2.py:185][0m |           0.0017 |           0.0094 |           0.0000 |
[32m[20221213 20:31:54 @agent_ppo2.py:185][0m |          -0.0165 |           0.0091 |           0.0000 |
[32m[20221213 20:31:54 @agent_ppo2.py:185][0m |          -0.0260 |           0.0090 |           0.0000 |
[32m[20221213 20:31:54 @agent_ppo2.py:185][0m |          -0.0255 |           0.0089 |           0.0000 |
[32m[20221213 20:31:54 @agent_ppo2.py:185][0m |          -0.0267 |           0.0088 |           0.0000 |
[32m[20221213 20:31:54 @agent_ppo2.py:185][0m |          -0.0316 |           0.0087 |           0.0000 |
[32m[20221213 20:31:54 @agent_ppo2.py:185][0m |          -0.0275 |           0.0087 |           0.0000 |
[32m[20221213 20:31:54 @agent_ppo2.py:185][0m |          -0.0426 |           0.0086 |           0.0000 |
[32m[20221213 20:31:55 @agent_ppo2.py:185][0m |          -0.0431 |           0.0084 |           0.0000 |
[32m[20221213 20:31:55 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:31:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.92
[32m[20221213 20:31:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.30
[32m[20221213 20:31:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.20
[32m[20221213 20:31:55 @agent_ppo2.py:143][0m Total time:       2.76 min
[32m[20221213 20:31:55 @agent_ppo2.py:145][0m 405504 total steps have happened
[32m[20221213 20:31:55 @agent_ppo2.py:121][0m #------------------------ Iteration 99 --------------------------#
[32m[20221213 20:31:55 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:31:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:56 @agent_ppo2.py:185][0m |           0.0319 |           0.0092 |           0.0000 |
[32m[20221213 20:31:56 @agent_ppo2.py:185][0m |           0.0221 |           0.0090 |           0.0000 |
[32m[20221213 20:31:56 @agent_ppo2.py:185][0m |           0.0042 |           0.0090 |           0.0000 |
[32m[20221213 20:31:56 @agent_ppo2.py:185][0m |          -0.0275 |           0.0088 |           0.0000 |
[32m[20221213 20:31:56 @agent_ppo2.py:185][0m |          -0.0316 |           0.0087 |           0.0000 |
[32m[20221213 20:31:56 @agent_ppo2.py:185][0m |          -0.0521 |           0.0087 |           0.0000 |
[32m[20221213 20:31:56 @agent_ppo2.py:185][0m |          -0.0820 |           0.0089 |           0.0000 |
[32m[20221213 20:31:56 @agent_ppo2.py:185][0m |          -0.0610 |           0.0088 |           0.0000 |
[32m[20221213 20:31:56 @agent_ppo2.py:185][0m |          -0.0625 |           0.0085 |           0.0000 |
[32m[20221213 20:31:56 @agent_ppo2.py:185][0m |          -0.0564 |           0.0084 |           0.0000 |
[32m[20221213 20:31:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:31:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.49
[32m[20221213 20:31:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.20
[32m[20221213 20:31:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.68
[32m[20221213 20:31:57 @agent_ppo2.py:143][0m Total time:       2.79 min
[32m[20221213 20:31:57 @agent_ppo2.py:145][0m 409600 total steps have happened
[32m[20221213 20:31:57 @agent_ppo2.py:121][0m #------------------------ Iteration 100 --------------------------#
[32m[20221213 20:31:57 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:31:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:57 @agent_ppo2.py:185][0m |           0.0200 |           0.0105 |           0.0000 |
[32m[20221213 20:31:57 @agent_ppo2.py:185][0m |           0.0069 |           0.0101 |           0.0000 |
[32m[20221213 20:31:57 @agent_ppo2.py:185][0m |          -0.0573 |           0.0105 |           0.0000 |
[32m[20221213 20:31:57 @agent_ppo2.py:185][0m |          -0.0250 |           0.0101 |           0.0000 |
[32m[20221213 20:31:58 @agent_ppo2.py:185][0m |          -0.0279 |           0.0096 |           0.0000 |
[32m[20221213 20:31:58 @agent_ppo2.py:185][0m |          -0.0482 |           0.0095 |           0.0000 |
[32m[20221213 20:31:58 @agent_ppo2.py:185][0m |          -0.0441 |           0.0093 |           0.0000 |
[32m[20221213 20:31:58 @agent_ppo2.py:185][0m |          -0.0455 |           0.0093 |           0.0000 |
[32m[20221213 20:31:58 @agent_ppo2.py:185][0m |          -0.0396 |           0.0093 |           0.0000 |
[32m[20221213 20:31:58 @agent_ppo2.py:185][0m |          -0.0476 |           0.0091 |           0.0000 |
[32m[20221213 20:31:58 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:31:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.19
[32m[20221213 20:31:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.60
[32m[20221213 20:31:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.94
[32m[20221213 20:31:58 @agent_ppo2.py:143][0m Total time:       2.82 min
[32m[20221213 20:31:58 @agent_ppo2.py:145][0m 413696 total steps have happened
[32m[20221213 20:31:58 @agent_ppo2.py:121][0m #------------------------ Iteration 101 --------------------------#
[32m[20221213 20:31:59 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:31:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:31:59 @agent_ppo2.py:185][0m |           0.0171 |           0.0381 |           0.0000 |
[32m[20221213 20:31:59 @agent_ppo2.py:185][0m |          -0.0009 |           0.0234 |           0.0000 |
[32m[20221213 20:31:59 @agent_ppo2.py:185][0m |          -0.0110 |           0.0216 |           0.0000 |
[32m[20221213 20:31:59 @agent_ppo2.py:185][0m |          -0.0183 |           0.0191 |           0.0000 |
[32m[20221213 20:31:59 @agent_ppo2.py:185][0m |          -0.0200 |           0.0186 |           0.0000 |
[32m[20221213 20:31:59 @agent_ppo2.py:185][0m |          -0.0247 |           0.0193 |           0.0000 |
[32m[20221213 20:31:59 @agent_ppo2.py:185][0m |          -0.0222 |           0.0176 |           0.0000 |
[32m[20221213 20:31:59 @agent_ppo2.py:185][0m |          -0.0235 |           0.0169 |           0.0000 |
[32m[20221213 20:32:00 @agent_ppo2.py:185][0m |          -0.0254 |           0.0167 |           0.0000 |
[32m[20221213 20:32:00 @agent_ppo2.py:185][0m |          -0.0250 |           0.0165 |           0.0000 |
[32m[20221213 20:32:00 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:32:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.81
[32m[20221213 20:32:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.56
[32m[20221213 20:32:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.16
[32m[20221213 20:32:00 @agent_ppo2.py:143][0m Total time:       2.85 min
[32m[20221213 20:32:00 @agent_ppo2.py:145][0m 417792 total steps have happened
[32m[20221213 20:32:00 @agent_ppo2.py:121][0m #------------------------ Iteration 102 --------------------------#
[32m[20221213 20:32:00 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:01 @agent_ppo2.py:185][0m |           0.0370 |           0.0172 |           0.0000 |
[32m[20221213 20:32:01 @agent_ppo2.py:185][0m |           0.0261 |           0.0103 |           0.0000 |
[32m[20221213 20:32:01 @agent_ppo2.py:185][0m |           0.0106 |           0.0099 |           0.0000 |
[32m[20221213 20:32:01 @agent_ppo2.py:185][0m |          -0.0159 |           0.0097 |           0.0000 |
[32m[20221213 20:32:01 @agent_ppo2.py:185][0m |          -0.0314 |           0.0096 |           0.0000 |
[32m[20221213 20:32:01 @agent_ppo2.py:185][0m |          -0.0338 |           0.0095 |           0.0000 |
[32m[20221213 20:32:01 @agent_ppo2.py:185][0m |          -0.0414 |           0.0094 |           0.0000 |
[32m[20221213 20:32:01 @agent_ppo2.py:185][0m |          -0.0440 |           0.0093 |           0.0000 |
[32m[20221213 20:32:01 @agent_ppo2.py:185][0m |          -0.0597 |           0.0092 |           0.0000 |
[32m[20221213 20:32:01 @agent_ppo2.py:185][0m |          -0.0525 |           0.0091 |           0.0000 |
[32m[20221213 20:32:01 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:32:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.58
[32m[20221213 20:32:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.28
[32m[20221213 20:32:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.03
[32m[20221213 20:32:02 @agent_ppo2.py:143][0m Total time:       2.88 min
[32m[20221213 20:32:02 @agent_ppo2.py:145][0m 421888 total steps have happened
[32m[20221213 20:32:02 @agent_ppo2.py:121][0m #------------------------ Iteration 103 --------------------------#
[32m[20221213 20:32:02 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:02 @agent_ppo2.py:185][0m |           0.0329 |           0.0136 |           0.0000 |
[32m[20221213 20:32:02 @agent_ppo2.py:185][0m |          -0.0018 |           0.0115 |           0.0000 |
[32m[20221213 20:32:02 @agent_ppo2.py:185][0m |          -0.0215 |           0.0107 |           0.0000 |
[32m[20221213 20:32:03 @agent_ppo2.py:185][0m |          -0.0297 |           0.0104 |           0.0000 |
[32m[20221213 20:32:03 @agent_ppo2.py:185][0m |          -0.0333 |           0.0101 |           0.0000 |
[32m[20221213 20:32:03 @agent_ppo2.py:185][0m |          -0.0369 |           0.0098 |           0.0000 |
[32m[20221213 20:32:03 @agent_ppo2.py:185][0m |          -0.0413 |           0.0097 |           0.0000 |
[32m[20221213 20:32:03 @agent_ppo2.py:185][0m |          -0.0471 |           0.0096 |           0.0000 |
[32m[20221213 20:32:03 @agent_ppo2.py:185][0m |          -0.0473 |           0.0094 |           0.0000 |
[32m[20221213 20:32:03 @agent_ppo2.py:185][0m |          -0.0455 |           0.0094 |           0.0000 |
[32m[20221213 20:32:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:32:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.06
[32m[20221213 20:32:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.24
[32m[20221213 20:32:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.15
[32m[20221213 20:32:03 @agent_ppo2.py:143][0m Total time:       2.90 min
[32m[20221213 20:32:03 @agent_ppo2.py:145][0m 425984 total steps have happened
[32m[20221213 20:32:03 @agent_ppo2.py:121][0m #------------------------ Iteration 104 --------------------------#
[32m[20221213 20:32:04 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:04 @agent_ppo2.py:185][0m |           0.0167 |           0.0120 |           0.0000 |
[32m[20221213 20:32:04 @agent_ppo2.py:185][0m |           0.0016 |           0.0069 |           0.0000 |
[32m[20221213 20:32:04 @agent_ppo2.py:185][0m |          -0.0113 |           0.0063 |           0.0000 |
[32m[20221213 20:32:04 @agent_ppo2.py:185][0m |          -0.0190 |           0.0061 |           0.0000 |
[32m[20221213 20:32:04 @agent_ppo2.py:185][0m |          -0.0255 |           0.0060 |           0.0000 |
[32m[20221213 20:32:04 @agent_ppo2.py:185][0m |          -0.0258 |           0.0059 |           0.0000 |
[32m[20221213 20:32:04 @agent_ppo2.py:185][0m |          -0.0343 |           0.0058 |           0.0000 |
[32m[20221213 20:32:05 @agent_ppo2.py:185][0m |          -0.0373 |           0.0057 |           0.0000 |
[32m[20221213 20:32:05 @agent_ppo2.py:185][0m |          -0.0412 |           0.0056 |           0.0000 |
[32m[20221213 20:32:05 @agent_ppo2.py:185][0m |          -0.0403 |           0.0055 |           0.0000 |
[32m[20221213 20:32:05 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 20:32:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.31
[32m[20221213 20:32:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.44
[32m[20221213 20:32:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.76
[32m[20221213 20:32:05 @agent_ppo2.py:143][0m Total time:       2.93 min
[32m[20221213 20:32:05 @agent_ppo2.py:145][0m 430080 total steps have happened
[32m[20221213 20:32:05 @agent_ppo2.py:121][0m #------------------------ Iteration 105 --------------------------#
[32m[20221213 20:32:05 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:32:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:06 @agent_ppo2.py:185][0m |           0.0185 |           0.0127 |           0.0000 |
[32m[20221213 20:32:06 @agent_ppo2.py:185][0m |           0.0108 |           0.0086 |           0.0000 |
[32m[20221213 20:32:06 @agent_ppo2.py:185][0m |           0.0013 |           0.0078 |           0.0000 |
[32m[20221213 20:32:06 @agent_ppo2.py:185][0m |          -0.0072 |           0.0078 |           0.0000 |
[32m[20221213 20:32:06 @agent_ppo2.py:185][0m |          -0.0083 |           0.0077 |           0.0000 |
[32m[20221213 20:32:06 @agent_ppo2.py:185][0m |          -0.0146 |           0.0076 |           0.0000 |
[32m[20221213 20:32:06 @agent_ppo2.py:185][0m |          -0.0190 |           0.0075 |           0.0000 |
[32m[20221213 20:32:06 @agent_ppo2.py:185][0m |          -0.0186 |           0.0074 |           0.0000 |
[32m[20221213 20:32:06 @agent_ppo2.py:185][0m |          -0.0234 |           0.0074 |           0.0000 |
[32m[20221213 20:32:06 @agent_ppo2.py:185][0m |          -0.0244 |           0.0074 |           0.0000 |
[32m[20221213 20:32:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:32:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.05
[32m[20221213 20:32:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.50
[32m[20221213 20:32:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.75
[32m[20221213 20:32:07 @agent_ppo2.py:143][0m Total time:       2.96 min
[32m[20221213 20:32:07 @agent_ppo2.py:145][0m 434176 total steps have happened
[32m[20221213 20:32:07 @agent_ppo2.py:121][0m #------------------------ Iteration 106 --------------------------#
[32m[20221213 20:32:07 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:07 @agent_ppo2.py:185][0m |           0.0248 |           0.0517 |           0.0000 |
[32m[20221213 20:32:07 @agent_ppo2.py:185][0m |          -0.0004 |           0.0326 |           0.0000 |
[32m[20221213 20:32:07 @agent_ppo2.py:185][0m |          -0.0039 |           0.0242 |           0.0000 |
[32m[20221213 20:32:08 @agent_ppo2.py:185][0m |          -0.0513 |           0.0217 |           0.0000 |
[32m[20221213 20:32:08 @agent_ppo2.py:185][0m |          -0.0158 |           0.0215 |           0.0000 |
[32m[20221213 20:32:08 @agent_ppo2.py:185][0m |          -0.0206 |           0.0188 |           0.0000 |
[32m[20221213 20:32:08 @agent_ppo2.py:185][0m |          -0.0226 |           0.0184 |           0.0000 |
[32m[20221213 20:32:08 @agent_ppo2.py:185][0m |          -0.0246 |           0.0182 |           0.0000 |
[32m[20221213 20:32:08 @agent_ppo2.py:185][0m |          -0.0350 |           0.0171 |           0.0000 |
[32m[20221213 20:32:08 @agent_ppo2.py:185][0m |          -0.0272 |           0.0165 |           0.0000 |
[32m[20221213 20:32:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:32:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.60
[32m[20221213 20:32:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.22
[32m[20221213 20:32:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.65
[32m[20221213 20:32:08 @agent_ppo2.py:143][0m Total time:       2.99 min
[32m[20221213 20:32:08 @agent_ppo2.py:145][0m 438272 total steps have happened
[32m[20221213 20:32:08 @agent_ppo2.py:121][0m #------------------------ Iteration 107 --------------------------#
[32m[20221213 20:32:09 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:09 @agent_ppo2.py:185][0m |           0.0284 |           0.0218 |           0.0000 |
[32m[20221213 20:32:09 @agent_ppo2.py:185][0m |          -0.0062 |           0.0190 |           0.0000 |
[32m[20221213 20:32:09 @agent_ppo2.py:185][0m |          -0.0373 |           0.0175 |           0.0000 |
[32m[20221213 20:32:09 @agent_ppo2.py:185][0m |          -0.0542 |           0.0163 |           0.0000 |
[32m[20221213 20:32:09 @agent_ppo2.py:185][0m |          -0.0611 |           0.0156 |           0.0000 |
[32m[20221213 20:32:09 @agent_ppo2.py:185][0m |          -0.0664 |           0.0155 |           0.0000 |
[32m[20221213 20:32:09 @agent_ppo2.py:185][0m |          -0.0701 |           0.0147 |           0.0000 |
[32m[20221213 20:32:10 @agent_ppo2.py:185][0m |          -0.0745 |           0.0144 |           0.0000 |
[32m[20221213 20:32:10 @agent_ppo2.py:185][0m |          -0.0811 |           0.0142 |           0.0000 |
[32m[20221213 20:32:10 @agent_ppo2.py:185][0m |          -0.0674 |           0.0141 |           0.0000 |
[32m[20221213 20:32:10 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:32:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.43
[32m[20221213 20:32:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.34
[32m[20221213 20:32:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.19
[32m[20221213 20:32:10 @agent_ppo2.py:143][0m Total time:       3.02 min
[32m[20221213 20:32:10 @agent_ppo2.py:145][0m 442368 total steps have happened
[32m[20221213 20:32:10 @agent_ppo2.py:121][0m #------------------------ Iteration 108 --------------------------#
[32m[20221213 20:32:10 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:11 @agent_ppo2.py:185][0m |           0.0206 |           0.0949 |           0.0000 |
[32m[20221213 20:32:11 @agent_ppo2.py:185][0m |           0.0017 |           0.0614 |           0.0000 |
[32m[20221213 20:32:11 @agent_ppo2.py:185][0m |          -0.0080 |           0.0462 |           0.0000 |
[32m[20221213 20:32:11 @agent_ppo2.py:185][0m |          -0.0172 |           0.0410 |           0.0000 |
[32m[20221213 20:32:11 @agent_ppo2.py:185][0m |          -0.0251 |           0.0370 |           0.0000 |
[32m[20221213 20:32:11 @agent_ppo2.py:185][0m |          -0.0277 |           0.0362 |           0.0000 |
[32m[20221213 20:32:11 @agent_ppo2.py:185][0m |          -0.0267 |           0.0359 |           0.0000 |
[32m[20221213 20:32:11 @agent_ppo2.py:185][0m |          -0.0292 |           0.0368 |           0.0000 |
[32m[20221213 20:32:11 @agent_ppo2.py:185][0m |          -0.0298 |           0.0313 |           0.0000 |
[32m[20221213 20:32:11 @agent_ppo2.py:185][0m |          -0.0361 |           0.0308 |           0.0000 |
[32m[20221213 20:32:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:32:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.32
[32m[20221213 20:32:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.99
[32m[20221213 20:32:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.14
[32m[20221213 20:32:12 @agent_ppo2.py:143][0m Total time:       3.04 min
[32m[20221213 20:32:12 @agent_ppo2.py:145][0m 446464 total steps have happened
[32m[20221213 20:32:12 @agent_ppo2.py:121][0m #------------------------ Iteration 109 --------------------------#
[32m[20221213 20:32:12 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:12 @agent_ppo2.py:185][0m |           0.0302 |           0.0366 |           0.0000 |
[32m[20221213 20:32:12 @agent_ppo2.py:185][0m |          -0.0156 |           0.0331 |           0.0000 |
[32m[20221213 20:32:12 @agent_ppo2.py:185][0m |          -0.0323 |           0.0297 |           0.0000 |
[32m[20221213 20:32:13 @agent_ppo2.py:185][0m |          -0.0584 |           0.0284 |           0.0000 |
[32m[20221213 20:32:13 @agent_ppo2.py:185][0m |          -0.0536 |           0.0281 |           0.0000 |
[32m[20221213 20:32:13 @agent_ppo2.py:185][0m |          -0.0567 |           0.0265 |           0.0000 |
[32m[20221213 20:32:13 @agent_ppo2.py:185][0m |          -0.0627 |           0.0263 |           0.0000 |
[32m[20221213 20:32:13 @agent_ppo2.py:185][0m |          -0.0810 |           0.0250 |           0.0000 |
[32m[20221213 20:32:13 @agent_ppo2.py:185][0m |          -0.0732 |           0.0252 |           0.0000 |
[32m[20221213 20:32:13 @agent_ppo2.py:185][0m |          -0.0827 |           0.0242 |           0.0000 |
[32m[20221213 20:32:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:32:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.62
[32m[20221213 20:32:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.50
[32m[20221213 20:32:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.77
[32m[20221213 20:32:13 @agent_ppo2.py:143][0m Total time:       3.07 min
[32m[20221213 20:32:13 @agent_ppo2.py:145][0m 450560 total steps have happened
[32m[20221213 20:32:13 @agent_ppo2.py:121][0m #------------------------ Iteration 110 --------------------------#
[32m[20221213 20:32:14 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:32:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:14 @agent_ppo2.py:185][0m |           0.0612 |           0.0617 |           0.0000 |
[32m[20221213 20:32:14 @agent_ppo2.py:185][0m |           0.0122 |           0.0123 |           0.0000 |
[32m[20221213 20:32:14 @agent_ppo2.py:185][0m |          -0.0031 |           0.0115 |           0.0000 |
[32m[20221213 20:32:14 @agent_ppo2.py:185][0m |          -0.0124 |           0.0111 |           0.0000 |
[32m[20221213 20:32:14 @agent_ppo2.py:185][0m |          -0.0162 |           0.0110 |           0.0000 |
[32m[20221213 20:32:14 @agent_ppo2.py:185][0m |          -0.0235 |           0.0109 |           0.0000 |
[32m[20221213 20:32:15 @agent_ppo2.py:185][0m |           0.0126 |           0.0110 |           0.0000 |
[32m[20221213 20:32:15 @agent_ppo2.py:185][0m |          -0.0314 |           0.0107 |           0.0000 |
[32m[20221213 20:32:15 @agent_ppo2.py:185][0m |          -0.0315 |           0.0108 |           0.0000 |
[32m[20221213 20:32:15 @agent_ppo2.py:185][0m |          -0.0345 |           0.0104 |           0.0000 |
[32m[20221213 20:32:15 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:32:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.05
[32m[20221213 20:32:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.07
[32m[20221213 20:32:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.17
[32m[20221213 20:32:15 @agent_ppo2.py:143][0m Total time:       3.10 min
[32m[20221213 20:32:15 @agent_ppo2.py:145][0m 454656 total steps have happened
[32m[20221213 20:32:15 @agent_ppo2.py:121][0m #------------------------ Iteration 111 --------------------------#
[32m[20221213 20:32:15 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:16 @agent_ppo2.py:185][0m |           0.0212 |           0.0256 |           0.0000 |
[32m[20221213 20:32:16 @agent_ppo2.py:185][0m |          -0.0068 |           0.0146 |           0.0000 |
[32m[20221213 20:32:16 @agent_ppo2.py:185][0m |          -0.0191 |           0.0140 |           0.0000 |
[32m[20221213 20:32:16 @agent_ppo2.py:185][0m |          -0.0170 |           0.0137 |           0.0000 |
[32m[20221213 20:32:16 @agent_ppo2.py:185][0m |          -0.0197 |           0.0132 |           0.0000 |
[32m[20221213 20:32:16 @agent_ppo2.py:185][0m |          -0.0193 |           0.0131 |           0.0000 |
[32m[20221213 20:32:16 @agent_ppo2.py:185][0m |          -0.0209 |           0.0129 |           0.0000 |
[32m[20221213 20:32:16 @agent_ppo2.py:185][0m |          -0.0282 |           0.0131 |           0.0000 |
[32m[20221213 20:32:16 @agent_ppo2.py:185][0m |          -0.0280 |           0.0128 |           0.0000 |
[32m[20221213 20:32:16 @agent_ppo2.py:185][0m |          -0.0300 |           0.0125 |           0.0000 |
[32m[20221213 20:32:16 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:32:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.75
[32m[20221213 20:32:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.01
[32m[20221213 20:32:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.65
[32m[20221213 20:32:17 @agent_ppo2.py:143][0m Total time:       3.13 min
[32m[20221213 20:32:17 @agent_ppo2.py:145][0m 458752 total steps have happened
[32m[20221213 20:32:17 @agent_ppo2.py:121][0m #------------------------ Iteration 112 --------------------------#
[32m[20221213 20:32:17 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:17 @agent_ppo2.py:185][0m |           0.0331 |           0.0131 |           0.0000 |
[32m[20221213 20:32:17 @agent_ppo2.py:185][0m |           0.0017 |           0.0126 |           0.0000 |
[32m[20221213 20:32:17 @agent_ppo2.py:185][0m |          -0.0298 |           0.0125 |           0.0000 |
[32m[20221213 20:32:18 @agent_ppo2.py:185][0m |          -0.0181 |           0.0131 |           0.0000 |
[32m[20221213 20:32:18 @agent_ppo2.py:185][0m |          -0.0343 |           0.0125 |           0.0000 |
[32m[20221213 20:32:18 @agent_ppo2.py:185][0m |          -0.0493 |           0.0120 |           0.0000 |
[32m[20221213 20:32:18 @agent_ppo2.py:185][0m |          -0.0525 |           0.0118 |           0.0000 |
[32m[20221213 20:32:18 @agent_ppo2.py:185][0m |          -0.0627 |           0.0117 |           0.0000 |
[32m[20221213 20:32:18 @agent_ppo2.py:185][0m |          -0.0629 |           0.0115 |           0.0000 |
[32m[20221213 20:32:18 @agent_ppo2.py:185][0m |          -0.0720 |           0.0115 |           0.0000 |
[32m[20221213 20:32:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:32:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.08
[32m[20221213 20:32:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.65
[32m[20221213 20:32:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.51
[32m[20221213 20:32:18 @agent_ppo2.py:143][0m Total time:       3.15 min
[32m[20221213 20:32:18 @agent_ppo2.py:145][0m 462848 total steps have happened
[32m[20221213 20:32:18 @agent_ppo2.py:121][0m #------------------------ Iteration 113 --------------------------#
[32m[20221213 20:32:19 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:19 @agent_ppo2.py:185][0m |           0.0203 |           0.1067 |           0.0000 |
[32m[20221213 20:32:19 @agent_ppo2.py:185][0m |          -0.0070 |           0.0615 |           0.0000 |
[32m[20221213 20:32:19 @agent_ppo2.py:185][0m |          -0.0174 |           0.0449 |           0.0000 |
[32m[20221213 20:32:19 @agent_ppo2.py:185][0m |          -0.0212 |           0.0371 |           0.0000 |
[32m[20221213 20:32:19 @agent_ppo2.py:185][0m |          -0.0195 |           0.0376 |           0.0000 |
[32m[20221213 20:32:19 @agent_ppo2.py:185][0m |          -0.0220 |           0.0369 |           0.0000 |
[32m[20221213 20:32:19 @agent_ppo2.py:185][0m |          -0.0259 |           0.0306 |           0.0000 |
[32m[20221213 20:32:19 @agent_ppo2.py:185][0m |          -0.0261 |           0.0297 |           0.0000 |
[32m[20221213 20:32:20 @agent_ppo2.py:185][0m |          -0.0282 |           0.0299 |           0.0000 |
[32m[20221213 20:32:20 @agent_ppo2.py:185][0m |          -0.0311 |           0.0288 |           0.0000 |
[32m[20221213 20:32:20 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:32:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.39
[32m[20221213 20:32:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.52
[32m[20221213 20:32:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.51
[32m[20221213 20:32:20 @agent_ppo2.py:143][0m Total time:       3.18 min
[32m[20221213 20:32:20 @agent_ppo2.py:145][0m 466944 total steps have happened
[32m[20221213 20:32:20 @agent_ppo2.py:121][0m #------------------------ Iteration 114 --------------------------#
[32m[20221213 20:32:20 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:21 @agent_ppo2.py:185][0m |           0.0415 |           0.0634 |           0.0000 |
[32m[20221213 20:32:21 @agent_ppo2.py:185][0m |          -0.0110 |           0.0611 |           0.0000 |
[32m[20221213 20:32:21 @agent_ppo2.py:185][0m |          -0.0639 |           0.0414 |           0.0000 |
[32m[20221213 20:32:21 @agent_ppo2.py:185][0m |           0.0199 |           0.0384 |           0.0000 |
[32m[20221213 20:32:21 @agent_ppo2.py:185][0m |          -0.0055 |           0.0378 |           0.0000 |
[32m[20221213 20:32:21 @agent_ppo2.py:185][0m |          -0.0618 |           0.0415 |           0.0000 |
[32m[20221213 20:32:21 @agent_ppo2.py:185][0m |          -0.0401 |           0.0405 |           0.0000 |
[32m[20221213 20:32:21 @agent_ppo2.py:185][0m |          -0.0885 |           0.0320 |           0.0000 |
[32m[20221213 20:32:21 @agent_ppo2.py:185][0m |          -0.0533 |           0.0309 |           0.0000 |
[32m[20221213 20:32:21 @agent_ppo2.py:185][0m |          -0.0566 |           0.0284 |           0.0000 |
[32m[20221213 20:32:21 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:32:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.11
[32m[20221213 20:32:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.88
[32m[20221213 20:32:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.92
[32m[20221213 20:32:22 @agent_ppo2.py:143][0m Total time:       3.21 min
[32m[20221213 20:32:22 @agent_ppo2.py:145][0m 471040 total steps have happened
[32m[20221213 20:32:22 @agent_ppo2.py:121][0m #------------------------ Iteration 115 --------------------------#
[32m[20221213 20:32:22 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:22 @agent_ppo2.py:185][0m |           0.0148 |           0.0958 |           0.0000 |
[32m[20221213 20:32:22 @agent_ppo2.py:185][0m |           0.0012 |           0.0209 |           0.0000 |
[32m[20221213 20:32:22 @agent_ppo2.py:185][0m |          -0.0099 |           0.0177 |           0.0000 |
[32m[20221213 20:32:22 @agent_ppo2.py:185][0m |          -0.0197 |           0.0173 |           0.0000 |
[32m[20221213 20:32:23 @agent_ppo2.py:185][0m |          -0.0285 |           0.0172 |           0.0000 |
[32m[20221213 20:32:23 @agent_ppo2.py:185][0m |          -0.0314 |           0.0168 |           0.0000 |
[32m[20221213 20:32:23 @agent_ppo2.py:185][0m |          -0.0352 |           0.0167 |           0.0000 |
[32m[20221213 20:32:23 @agent_ppo2.py:185][0m |          -0.0364 |           0.0165 |           0.0000 |
[32m[20221213 20:32:23 @agent_ppo2.py:185][0m |          -0.0367 |           0.0165 |           0.0000 |
[32m[20221213 20:32:23 @agent_ppo2.py:185][0m |          -0.0380 |           0.0163 |           0.0000 |
[32m[20221213 20:32:23 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:32:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.03
[32m[20221213 20:32:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.06
[32m[20221213 20:32:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.91
[32m[20221213 20:32:23 @agent_ppo2.py:143][0m Total time:       3.24 min
[32m[20221213 20:32:23 @agent_ppo2.py:145][0m 475136 total steps have happened
[32m[20221213 20:32:23 @agent_ppo2.py:121][0m #------------------------ Iteration 116 --------------------------#
[32m[20221213 20:32:24 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:24 @agent_ppo2.py:185][0m |           0.0266 |           0.0150 |           0.0000 |
[32m[20221213 20:32:24 @agent_ppo2.py:185][0m |          -0.0102 |           0.0120 |           0.0000 |
[32m[20221213 20:32:24 @agent_ppo2.py:185][0m |          -0.0673 |           0.0114 |           0.0000 |
[32m[20221213 20:32:24 @agent_ppo2.py:185][0m |          -0.0293 |           0.0109 |           0.0000 |
[32m[20221213 20:32:24 @agent_ppo2.py:185][0m |          -0.0509 |           0.0106 |           0.0000 |
[32m[20221213 20:32:24 @agent_ppo2.py:185][0m |           0.0043 |           0.0105 |           0.0000 |
[32m[20221213 20:32:24 @agent_ppo2.py:185][0m |          -0.0103 |           0.0104 |           0.0000 |
[32m[20221213 20:32:24 @agent_ppo2.py:185][0m |          -0.0323 |           0.0103 |           0.0000 |
[32m[20221213 20:32:25 @agent_ppo2.py:185][0m |          -0.0316 |           0.0102 |           0.0000 |
[32m[20221213 20:32:25 @agent_ppo2.py:185][0m |          -0.0342 |           0.0102 |           0.0000 |
[32m[20221213 20:32:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:32:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.97
[32m[20221213 20:32:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.53
[32m[20221213 20:32:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.59
[32m[20221213 20:32:25 @agent_ppo2.py:143][0m Total time:       3.26 min
[32m[20221213 20:32:25 @agent_ppo2.py:145][0m 479232 total steps have happened
[32m[20221213 20:32:25 @agent_ppo2.py:121][0m #------------------------ Iteration 117 --------------------------#
[32m[20221213 20:32:25 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:26 @agent_ppo2.py:185][0m |           0.0206 |           0.0134 |           0.0000 |
[32m[20221213 20:32:26 @agent_ppo2.py:185][0m |          -0.0143 |           0.0129 |           0.0000 |
[32m[20221213 20:32:26 @agent_ppo2.py:185][0m |          -0.0299 |           0.0121 |           0.0000 |
[32m[20221213 20:32:26 @agent_ppo2.py:185][0m |          -0.0284 |           0.0121 |           0.0000 |
[32m[20221213 20:32:26 @agent_ppo2.py:185][0m |          -0.0394 |           0.0118 |           0.0000 |
[32m[20221213 20:32:26 @agent_ppo2.py:185][0m |          -0.0474 |           0.0116 |           0.0000 |
[32m[20221213 20:32:26 @agent_ppo2.py:185][0m |          -0.0398 |           0.0116 |           0.0000 |
[32m[20221213 20:32:26 @agent_ppo2.py:185][0m |          -0.0466 |           0.0115 |           0.0000 |
[32m[20221213 20:32:26 @agent_ppo2.py:185][0m |          -0.0506 |           0.0113 |           0.0000 |
[32m[20221213 20:32:26 @agent_ppo2.py:185][0m |          -0.0558 |           0.0111 |           0.0000 |
[32m[20221213 20:32:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:32:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.68
[32m[20221213 20:32:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.73
[32m[20221213 20:32:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.99
[32m[20221213 20:32:27 @agent_ppo2.py:143][0m Total time:       3.29 min
[32m[20221213 20:32:27 @agent_ppo2.py:145][0m 483328 total steps have happened
[32m[20221213 20:32:27 @agent_ppo2.py:121][0m #------------------------ Iteration 118 --------------------------#
[32m[20221213 20:32:27 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:27 @agent_ppo2.py:185][0m |           0.0192 |           0.0352 |           0.0000 |
[32m[20221213 20:32:27 @agent_ppo2.py:185][0m |          -0.0005 |           0.0233 |           0.0000 |
[32m[20221213 20:32:27 @agent_ppo2.py:185][0m |          -0.0104 |           0.0203 |           0.0000 |
[32m[20221213 20:32:27 @agent_ppo2.py:185][0m |          -0.0208 |           0.0192 |           0.0000 |
[32m[20221213 20:32:28 @agent_ppo2.py:185][0m |          -0.0177 |           0.0192 |           0.0000 |
[32m[20221213 20:32:28 @agent_ppo2.py:185][0m |          -0.0272 |           0.0201 |           0.0000 |
[32m[20221213 20:32:28 @agent_ppo2.py:185][0m |          -0.0276 |           0.0169 |           0.0000 |
[32m[20221213 20:32:28 @agent_ppo2.py:185][0m |          -0.0318 |           0.0168 |           0.0000 |
[32m[20221213 20:32:28 @agent_ppo2.py:185][0m |          -0.0331 |           0.0163 |           0.0000 |
[32m[20221213 20:32:28 @agent_ppo2.py:185][0m |          -0.0310 |           0.0161 |           0.0000 |
[32m[20221213 20:32:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:32:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.25
[32m[20221213 20:32:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.84
[32m[20221213 20:32:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.81
[32m[20221213 20:32:28 @agent_ppo2.py:143][0m Total time:       3.32 min
[32m[20221213 20:32:28 @agent_ppo2.py:145][0m 487424 total steps have happened
[32m[20221213 20:32:28 @agent_ppo2.py:121][0m #------------------------ Iteration 119 --------------------------#
[32m[20221213 20:32:29 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:32:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:29 @agent_ppo2.py:185][0m |           0.0417 |           0.0251 |           0.0000 |
[32m[20221213 20:32:29 @agent_ppo2.py:185][0m |          -0.0002 |           0.0223 |           0.0000 |
[32m[20221213 20:32:29 @agent_ppo2.py:185][0m |          -0.0276 |           0.0213 |           0.0000 |
[32m[20221213 20:32:29 @agent_ppo2.py:185][0m |          -0.0513 |           0.0204 |           0.0000 |
[32m[20221213 20:32:29 @agent_ppo2.py:185][0m |          -0.0526 |           0.0199 |           0.0000 |
[32m[20221213 20:32:29 @agent_ppo2.py:185][0m |          -0.0630 |           0.0195 |           0.0000 |
[32m[20221213 20:32:29 @agent_ppo2.py:185][0m |          -0.0599 |           0.0192 |           0.0000 |
[32m[20221213 20:32:30 @agent_ppo2.py:185][0m |          -0.0586 |           0.0184 |           0.0000 |
[32m[20221213 20:32:30 @agent_ppo2.py:185][0m |          -0.0803 |           0.0182 |           0.0000 |
[32m[20221213 20:32:30 @agent_ppo2.py:185][0m |          -0.0643 |           0.0176 |           0.0000 |
[32m[20221213 20:32:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:32:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.29
[32m[20221213 20:32:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.87
[32m[20221213 20:32:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.13
[32m[20221213 20:32:30 @agent_ppo2.py:143][0m Total time:       3.35 min
[32m[20221213 20:32:30 @agent_ppo2.py:145][0m 491520 total steps have happened
[32m[20221213 20:32:30 @agent_ppo2.py:121][0m #------------------------ Iteration 120 --------------------------#
[32m[20221213 20:32:30 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:32:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:31 @agent_ppo2.py:185][0m |           0.0255 |           0.0830 |           0.0000 |
[32m[20221213 20:32:31 @agent_ppo2.py:185][0m |          -0.0049 |           0.0529 |           0.0000 |
[32m[20221213 20:32:31 @agent_ppo2.py:185][0m |          -0.0200 |           0.0476 |           0.0000 |
[32m[20221213 20:32:31 @agent_ppo2.py:185][0m |          -0.0197 |           0.0339 |           0.0000 |
[32m[20221213 20:32:31 @agent_ppo2.py:185][0m |          -0.0221 |           0.0318 |           0.0000 |
[32m[20221213 20:32:31 @agent_ppo2.py:185][0m |          -0.0253 |           0.0320 |           0.0000 |
[32m[20221213 20:32:31 @agent_ppo2.py:185][0m |          -0.0324 |           0.0289 |           0.0000 |
[32m[20221213 20:32:31 @agent_ppo2.py:185][0m |          -0.0324 |           0.0293 |           0.0000 |
[32m[20221213 20:32:31 @agent_ppo2.py:185][0m |          -0.0381 |           0.0279 |           0.0000 |
[32m[20221213 20:32:31 @agent_ppo2.py:185][0m |          -0.0376 |           0.0271 |           0.0000 |
[32m[20221213 20:32:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:32:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.52
[32m[20221213 20:32:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.99
[32m[20221213 20:32:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.01
[32m[20221213 20:32:32 @agent_ppo2.py:143][0m Total time:       3.38 min
[32m[20221213 20:32:32 @agent_ppo2.py:145][0m 495616 total steps have happened
[32m[20221213 20:32:32 @agent_ppo2.py:121][0m #------------------------ Iteration 121 --------------------------#
[32m[20221213 20:32:32 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:32 @agent_ppo2.py:185][0m |           0.0157 |           0.0450 |           0.0000 |
[32m[20221213 20:32:32 @agent_ppo2.py:185][0m |           0.0238 |           0.0134 |           0.0000 |
[32m[20221213 20:32:32 @agent_ppo2.py:185][0m |          -0.0129 |           0.0124 |           0.0000 |
[32m[20221213 20:32:33 @agent_ppo2.py:185][0m |          -0.0235 |           0.0115 |           0.0000 |
[32m[20221213 20:32:33 @agent_ppo2.py:185][0m |          -0.0308 |           0.0114 |           0.0000 |
[32m[20221213 20:32:33 @agent_ppo2.py:185][0m |          -0.0331 |           0.0112 |           0.0000 |
[32m[20221213 20:32:33 @agent_ppo2.py:185][0m |          -0.0365 |           0.0111 |           0.0000 |
[32m[20221213 20:32:33 @agent_ppo2.py:185][0m |          -0.0381 |           0.0110 |           0.0000 |
[32m[20221213 20:32:33 @agent_ppo2.py:185][0m |          -0.0483 |           0.0110 |           0.0000 |
[32m[20221213 20:32:33 @agent_ppo2.py:185][0m |          -0.0456 |           0.0109 |           0.0000 |
[32m[20221213 20:32:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:32:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.46
[32m[20221213 20:32:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.12
[32m[20221213 20:32:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.47
[32m[20221213 20:32:33 @agent_ppo2.py:143][0m Total time:       3.40 min
[32m[20221213 20:32:33 @agent_ppo2.py:145][0m 499712 total steps have happened
[32m[20221213 20:32:33 @agent_ppo2.py:121][0m #------------------------ Iteration 122 --------------------------#
[32m[20221213 20:32:34 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:34 @agent_ppo2.py:185][0m |           0.0201 |           0.0613 |           0.0000 |
[32m[20221213 20:32:34 @agent_ppo2.py:185][0m |           0.0011 |           0.0292 |           0.0000 |
[32m[20221213 20:32:34 @agent_ppo2.py:185][0m |          -0.0032 |           0.0254 |           0.0000 |
[32m[20221213 20:32:34 @agent_ppo2.py:185][0m |          -0.0161 |           0.0234 |           0.0000 |
[32m[20221213 20:32:34 @agent_ppo2.py:185][0m |          -0.0171 |           0.0223 |           0.0000 |
[32m[20221213 20:32:34 @agent_ppo2.py:185][0m |          -0.0219 |           0.0218 |           0.0000 |
[32m[20221213 20:32:34 @agent_ppo2.py:185][0m |          -0.0247 |           0.0201 |           0.0000 |
[32m[20221213 20:32:35 @agent_ppo2.py:185][0m |          -0.0490 |           0.0198 |           0.0000 |
[32m[20221213 20:32:35 @agent_ppo2.py:185][0m |          -0.0286 |           0.0198 |           0.0000 |
[32m[20221213 20:32:35 @agent_ppo2.py:185][0m |          -0.0297 |           0.0193 |           0.0000 |
[32m[20221213 20:32:35 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:32:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.93
[32m[20221213 20:32:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.03
[32m[20221213 20:32:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.21
[32m[20221213 20:32:35 @agent_ppo2.py:143][0m Total time:       3.43 min
[32m[20221213 20:32:35 @agent_ppo2.py:145][0m 503808 total steps have happened
[32m[20221213 20:32:35 @agent_ppo2.py:121][0m #------------------------ Iteration 123 --------------------------#
[32m[20221213 20:32:35 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:36 @agent_ppo2.py:185][0m |           0.0086 |           0.0262 |           0.0000 |
[32m[20221213 20:32:36 @agent_ppo2.py:185][0m |          -0.0130 |           0.0243 |           0.0000 |
[32m[20221213 20:32:36 @agent_ppo2.py:185][0m |          -0.0245 |           0.0206 |           0.0000 |
[32m[20221213 20:32:36 @agent_ppo2.py:185][0m |          -0.0390 |           0.0201 |           0.0000 |
[32m[20221213 20:32:36 @agent_ppo2.py:185][0m |          -0.0578 |           0.0195 |           0.0000 |
[32m[20221213 20:32:36 @agent_ppo2.py:185][0m |          -0.0594 |           0.0192 |           0.0000 |
[32m[20221213 20:32:36 @agent_ppo2.py:185][0m |          -0.0603 |           0.0184 |           0.0000 |
[32m[20221213 20:32:36 @agent_ppo2.py:185][0m |          -0.0680 |           0.0183 |           0.0000 |
[32m[20221213 20:32:36 @agent_ppo2.py:185][0m |          -0.0642 |           0.0181 |           0.0000 |
[32m[20221213 20:32:36 @agent_ppo2.py:185][0m |          -0.0636 |           0.0176 |           0.0000 |
[32m[20221213 20:32:36 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:32:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.41
[32m[20221213 20:32:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.19
[32m[20221213 20:32:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.15
[32m[20221213 20:32:37 @agent_ppo2.py:143][0m Total time:       3.46 min
[32m[20221213 20:32:37 @agent_ppo2.py:145][0m 507904 total steps have happened
[32m[20221213 20:32:37 @agent_ppo2.py:121][0m #------------------------ Iteration 124 --------------------------#
[32m[20221213 20:32:37 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:37 @agent_ppo2.py:185][0m |           0.0424 |           0.0208 |           0.0000 |
[32m[20221213 20:32:37 @agent_ppo2.py:185][0m |           0.0048 |           0.0177 |           0.0000 |
[32m[20221213 20:32:37 @agent_ppo2.py:185][0m |          -0.0243 |           0.0171 |           0.0000 |
[32m[20221213 20:32:38 @agent_ppo2.py:185][0m |          -0.0325 |           0.0166 |           0.0000 |
[32m[20221213 20:32:38 @agent_ppo2.py:185][0m |          -0.0452 |           0.0162 |           0.0000 |
[32m[20221213 20:32:38 @agent_ppo2.py:185][0m |          -0.0279 |           0.0166 |           0.0000 |
[32m[20221213 20:32:38 @agent_ppo2.py:185][0m |          -0.0522 |           0.0163 |           0.0000 |
[32m[20221213 20:32:38 @agent_ppo2.py:185][0m |          -0.0557 |           0.0157 |           0.0000 |
[32m[20221213 20:32:38 @agent_ppo2.py:185][0m |          -0.0613 |           0.0155 |           0.0000 |
[32m[20221213 20:32:38 @agent_ppo2.py:185][0m |          -0.0653 |           0.0153 |           0.0000 |
[32m[20221213 20:32:38 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:32:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.84
[32m[20221213 20:32:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.53
[32m[20221213 20:32:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.06
[32m[20221213 20:32:38 @agent_ppo2.py:143][0m Total time:       3.49 min
[32m[20221213 20:32:38 @agent_ppo2.py:145][0m 512000 total steps have happened
[32m[20221213 20:32:38 @agent_ppo2.py:121][0m #------------------------ Iteration 125 --------------------------#
[32m[20221213 20:32:39 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:39 @agent_ppo2.py:185][0m |           0.0259 |           0.0162 |           0.0000 |
[32m[20221213 20:32:39 @agent_ppo2.py:185][0m |          -0.0271 |           0.0177 |           0.0000 |
[32m[20221213 20:32:39 @agent_ppo2.py:185][0m |          -0.0406 |           0.0197 |           0.0000 |
[32m[20221213 20:32:39 @agent_ppo2.py:185][0m |          -0.0502 |           0.0154 |           0.0000 |
[32m[20221213 20:32:39 @agent_ppo2.py:185][0m |          -0.0556 |           0.0153 |           0.0000 |
[32m[20221213 20:32:39 @agent_ppo2.py:185][0m |          -0.0674 |           0.0148 |           0.0000 |
[32m[20221213 20:32:40 @agent_ppo2.py:185][0m |          -0.0793 |           0.0149 |           0.0000 |
[32m[20221213 20:32:40 @agent_ppo2.py:185][0m |          -0.0848 |           0.0145 |           0.0000 |
[32m[20221213 20:32:40 @agent_ppo2.py:185][0m |          -0.0782 |           0.0143 |           0.0000 |
[32m[20221213 20:32:40 @agent_ppo2.py:185][0m |          -0.0814 |           0.0143 |           0.0000 |
[32m[20221213 20:32:40 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:32:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.36
[32m[20221213 20:32:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.20
[32m[20221213 20:32:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.40
[32m[20221213 20:32:40 @agent_ppo2.py:143][0m Total time:       3.52 min
[32m[20221213 20:32:40 @agent_ppo2.py:145][0m 516096 total steps have happened
[32m[20221213 20:32:40 @agent_ppo2.py:121][0m #------------------------ Iteration 126 --------------------------#
[32m[20221213 20:32:40 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:41 @agent_ppo2.py:185][0m |           0.0235 |           0.0153 |           0.0000 |
[32m[20221213 20:32:41 @agent_ppo2.py:185][0m |           0.0160 |           0.0129 |           0.0000 |
[32m[20221213 20:32:41 @agent_ppo2.py:185][0m |          -0.0072 |           0.0125 |           0.0000 |
[32m[20221213 20:32:41 @agent_ppo2.py:185][0m |          -0.0374 |           0.0121 |           0.0000 |
[32m[20221213 20:32:41 @agent_ppo2.py:185][0m |          -0.0378 |           0.0120 |           0.0000 |
[32m[20221213 20:32:41 @agent_ppo2.py:185][0m |          -0.0533 |           0.0116 |           0.0000 |
[32m[20221213 20:32:41 @agent_ppo2.py:185][0m |          -0.0626 |           0.0114 |           0.0000 |
[32m[20221213 20:32:41 @agent_ppo2.py:185][0m |          -0.0639 |           0.0113 |           0.0000 |
[32m[20221213 20:32:41 @agent_ppo2.py:185][0m |          -0.0654 |           0.0111 |           0.0000 |
[32m[20221213 20:32:41 @agent_ppo2.py:185][0m |          -0.0684 |           0.0110 |           0.0000 |
[32m[20221213 20:32:41 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:32:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.66
[32m[20221213 20:32:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.17
[32m[20221213 20:32:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.29
[32m[20221213 20:32:42 @agent_ppo2.py:143][0m Total time:       3.54 min
[32m[20221213 20:32:42 @agent_ppo2.py:145][0m 520192 total steps have happened
[32m[20221213 20:32:42 @agent_ppo2.py:121][0m #------------------------ Iteration 127 --------------------------#
[32m[20221213 20:32:42 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:32:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:42 @agent_ppo2.py:185][0m |           0.0256 |           0.0133 |           0.0000 |
[32m[20221213 20:32:42 @agent_ppo2.py:185][0m |          -0.0102 |           0.0130 |           0.0000 |
[32m[20221213 20:32:43 @agent_ppo2.py:185][0m |          -0.0313 |           0.0128 |           0.0000 |
[32m[20221213 20:32:43 @agent_ppo2.py:185][0m |          -0.0414 |           0.0126 |           0.0000 |
[32m[20221213 20:32:43 @agent_ppo2.py:185][0m |          -0.0537 |           0.0123 |           0.0000 |
[32m[20221213 20:32:43 @agent_ppo2.py:185][0m |          -0.0534 |           0.0121 |           0.0000 |
[32m[20221213 20:32:43 @agent_ppo2.py:185][0m |          -0.0556 |           0.0119 |           0.0000 |
[32m[20221213 20:32:43 @agent_ppo2.py:185][0m |          -0.0545 |           0.0119 |           0.0000 |
[32m[20221213 20:32:43 @agent_ppo2.py:185][0m |          -0.0717 |           0.0118 |           0.0000 |
[32m[20221213 20:32:43 @agent_ppo2.py:185][0m |          -0.0575 |           0.0117 |           0.0000 |
[32m[20221213 20:32:43 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 20:32:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.01
[32m[20221213 20:32:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.03
[32m[20221213 20:32:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.55
[32m[20221213 20:32:44 @agent_ppo2.py:143][0m Total time:       3.58 min
[32m[20221213 20:32:44 @agent_ppo2.py:145][0m 524288 total steps have happened
[32m[20221213 20:32:44 @agent_ppo2.py:121][0m #------------------------ Iteration 128 --------------------------#
[32m[20221213 20:32:44 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:32:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:44 @agent_ppo2.py:185][0m |           0.0266 |           0.0720 |           0.0000 |
[32m[20221213 20:32:44 @agent_ppo2.py:185][0m |          -0.0000 |           0.0302 |           0.0000 |
[32m[20221213 20:32:44 @agent_ppo2.py:185][0m |          -0.0145 |           0.0279 |           0.0000 |
[32m[20221213 20:32:45 @agent_ppo2.py:185][0m |          -0.0207 |           0.0255 |           0.0000 |
[32m[20221213 20:32:45 @agent_ppo2.py:185][0m |          -0.0223 |           0.0242 |           0.0000 |
[32m[20221213 20:32:45 @agent_ppo2.py:185][0m |          -0.0240 |           0.0240 |           0.0000 |
[32m[20221213 20:32:45 @agent_ppo2.py:185][0m |          -0.0187 |           0.0227 |           0.0000 |
[32m[20221213 20:32:45 @agent_ppo2.py:185][0m |          -0.0222 |           0.0225 |           0.0000 |
[32m[20221213 20:32:45 @agent_ppo2.py:185][0m |          -0.0252 |           0.0219 |           0.0000 |
[32m[20221213 20:32:45 @agent_ppo2.py:185][0m |          -0.0254 |           0.0216 |           0.0000 |
[32m[20221213 20:32:45 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:32:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.18
[32m[20221213 20:32:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.40
[32m[20221213 20:32:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.44
[32m[20221213 20:32:46 @agent_ppo2.py:143][0m Total time:       3.61 min
[32m[20221213 20:32:46 @agent_ppo2.py:145][0m 528384 total steps have happened
[32m[20221213 20:32:46 @agent_ppo2.py:121][0m #------------------------ Iteration 129 --------------------------#
[32m[20221213 20:32:46 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:32:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:46 @agent_ppo2.py:185][0m |           0.0072 |           0.0370 |           0.0000 |
[32m[20221213 20:32:46 @agent_ppo2.py:185][0m |           0.0334 |           0.0135 |           0.0000 |
[32m[20221213 20:32:46 @agent_ppo2.py:185][0m |          -0.0081 |           0.0126 |           0.0000 |
[32m[20221213 20:32:46 @agent_ppo2.py:185][0m |          -0.0136 |           0.0122 |           0.0000 |
[32m[20221213 20:32:47 @agent_ppo2.py:185][0m |          -0.0274 |           0.0121 |           0.0000 |
[32m[20221213 20:32:47 @agent_ppo2.py:185][0m |          -0.0285 |           0.0119 |           0.0000 |
[32m[20221213 20:32:47 @agent_ppo2.py:185][0m |          -0.0360 |           0.0117 |           0.0000 |
[32m[20221213 20:32:47 @agent_ppo2.py:185][0m |          -0.0364 |           0.0116 |           0.0000 |
[32m[20221213 20:32:47 @agent_ppo2.py:185][0m |          -0.0409 |           0.0115 |           0.0000 |
[32m[20221213 20:32:47 @agent_ppo2.py:185][0m |          -0.0444 |           0.0113 |           0.0000 |
[32m[20221213 20:32:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:32:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.21
[32m[20221213 20:32:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.91
[32m[20221213 20:32:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.94
[32m[20221213 20:32:47 @agent_ppo2.py:143][0m Total time:       3.64 min
[32m[20221213 20:32:47 @agent_ppo2.py:145][0m 532480 total steps have happened
[32m[20221213 20:32:47 @agent_ppo2.py:121][0m #------------------------ Iteration 130 --------------------------#
[32m[20221213 20:32:48 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:32:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:48 @agent_ppo2.py:185][0m |           0.0172 |           0.0135 |           0.0000 |
[32m[20221213 20:32:48 @agent_ppo2.py:185][0m |          -0.0149 |           0.0126 |           0.0000 |
[32m[20221213 20:32:48 @agent_ppo2.py:185][0m |          -0.0323 |           0.0124 |           0.0000 |
[32m[20221213 20:32:48 @agent_ppo2.py:185][0m |          -0.0405 |           0.0123 |           0.0000 |
[32m[20221213 20:32:48 @agent_ppo2.py:185][0m |          -0.0400 |           0.0121 |           0.0000 |
[32m[20221213 20:32:48 @agent_ppo2.py:185][0m |          -0.0458 |           0.0121 |           0.0000 |
[32m[20221213 20:32:48 @agent_ppo2.py:185][0m |          -0.0461 |           0.0120 |           0.0000 |
[32m[20221213 20:32:49 @agent_ppo2.py:185][0m |          -0.0498 |           0.0118 |           0.0000 |
[32m[20221213 20:32:49 @agent_ppo2.py:185][0m |          -0.0443 |           0.0119 |           0.0000 |
[32m[20221213 20:32:49 @agent_ppo2.py:185][0m |          -0.0535 |           0.0118 |           0.0000 |
[32m[20221213 20:32:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:32:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.23
[32m[20221213 20:32:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.52
[32m[20221213 20:32:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.38
[32m[20221213 20:32:49 @agent_ppo2.py:143][0m Total time:       3.67 min
[32m[20221213 20:32:49 @agent_ppo2.py:145][0m 536576 total steps have happened
[32m[20221213 20:32:49 @agent_ppo2.py:121][0m #------------------------ Iteration 131 --------------------------#
[32m[20221213 20:32:49 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:50 @agent_ppo2.py:185][0m |           0.0292 |           0.0551 |           0.0000 |
[32m[20221213 20:32:50 @agent_ppo2.py:185][0m |          -0.0065 |           0.0232 |           0.0000 |
[32m[20221213 20:32:50 @agent_ppo2.py:185][0m |          -0.0132 |           0.0213 |           0.0000 |
[32m[20221213 20:32:50 @agent_ppo2.py:185][0m |          -0.0196 |           0.0193 |           0.0000 |
[32m[20221213 20:32:50 @agent_ppo2.py:185][0m |          -0.0191 |           0.0188 |           0.0000 |
[32m[20221213 20:32:50 @agent_ppo2.py:185][0m |          -0.0208 |           0.0190 |           0.0000 |
[32m[20221213 20:32:50 @agent_ppo2.py:185][0m |          -0.0253 |           0.0187 |           0.0000 |
[32m[20221213 20:32:50 @agent_ppo2.py:185][0m |          -0.0270 |           0.0174 |           0.0000 |
[32m[20221213 20:32:50 @agent_ppo2.py:185][0m |          -0.0300 |           0.0172 |           0.0000 |
[32m[20221213 20:32:50 @agent_ppo2.py:185][0m |          -0.0298 |           0.0170 |           0.0000 |
[32m[20221213 20:32:50 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:32:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.15
[32m[20221213 20:32:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.36
[32m[20221213 20:32:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.73
[32m[20221213 20:32:51 @agent_ppo2.py:143][0m Total time:       3.69 min
[32m[20221213 20:32:51 @agent_ppo2.py:145][0m 540672 total steps have happened
[32m[20221213 20:32:51 @agent_ppo2.py:121][0m #------------------------ Iteration 132 --------------------------#
[32m[20221213 20:32:51 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:51 @agent_ppo2.py:185][0m |           0.1024 |           0.0506 |           0.0000 |
[32m[20221213 20:32:51 @agent_ppo2.py:185][0m |           0.0362 |           0.0464 |           0.0000 |
[32m[20221213 20:32:52 @agent_ppo2.py:185][0m |           0.0178 |           0.0328 |           0.0000 |
[32m[20221213 20:32:52 @agent_ppo2.py:185][0m |           0.0031 |           0.0296 |           0.0000 |
[32m[20221213 20:32:52 @agent_ppo2.py:185][0m |          -0.0089 |           0.0332 |           0.0000 |
[32m[20221213 20:32:52 @agent_ppo2.py:185][0m |          -0.0237 |           0.0320 |           0.0000 |
[32m[20221213 20:32:52 @agent_ppo2.py:185][0m |          -0.0278 |           0.0260 |           0.0000 |
[32m[20221213 20:32:52 @agent_ppo2.py:185][0m |          -0.0352 |           0.0253 |           0.0000 |
[32m[20221213 20:32:52 @agent_ppo2.py:185][0m |          -0.0380 |           0.0248 |           0.0000 |
[32m[20221213 20:32:52 @agent_ppo2.py:185][0m |          -0.0366 |           0.0247 |           0.0000 |
[32m[20221213 20:32:52 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:32:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.01
[32m[20221213 20:32:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.33
[32m[20221213 20:32:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.60
[32m[20221213 20:32:53 @agent_ppo2.py:143][0m Total time:       3.72 min
[32m[20221213 20:32:53 @agent_ppo2.py:145][0m 544768 total steps have happened
[32m[20221213 20:32:53 @agent_ppo2.py:121][0m #------------------------ Iteration 133 --------------------------#
[32m[20221213 20:32:53 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:32:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:53 @agent_ppo2.py:185][0m |           0.0115 |           0.0425 |           0.0000 |
[32m[20221213 20:32:53 @agent_ppo2.py:185][0m |          -0.0022 |           0.0136 |           0.0000 |
[32m[20221213 20:32:53 @agent_ppo2.py:185][0m |          -0.0242 |           0.0119 |           0.0000 |
[32m[20221213 20:32:53 @agent_ppo2.py:185][0m |          -0.0370 |           0.0115 |           0.0000 |
[32m[20221213 20:32:53 @agent_ppo2.py:185][0m |          -0.0327 |           0.0111 |           0.0000 |
[32m[20221213 20:32:54 @agent_ppo2.py:185][0m |          -0.0091 |           0.0110 |           0.0000 |
[32m[20221213 20:32:54 @agent_ppo2.py:185][0m |          -0.0432 |           0.0108 |           0.0000 |
[32m[20221213 20:32:54 @agent_ppo2.py:185][0m |          -0.0455 |           0.0107 |           0.0000 |
[32m[20221213 20:32:54 @agent_ppo2.py:185][0m |          -0.0414 |           0.0106 |           0.0000 |
[32m[20221213 20:32:54 @agent_ppo2.py:185][0m |          -0.0610 |           0.0105 |           0.0000 |
[32m[20221213 20:32:54 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:32:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.15
[32m[20221213 20:32:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.23
[32m[20221213 20:32:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.10
[32m[20221213 20:32:54 @agent_ppo2.py:143][0m Total time:       3.75 min
[32m[20221213 20:32:54 @agent_ppo2.py:145][0m 548864 total steps have happened
[32m[20221213 20:32:54 @agent_ppo2.py:121][0m #------------------------ Iteration 134 --------------------------#
[32m[20221213 20:32:55 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:55 @agent_ppo2.py:185][0m |           0.0260 |           0.1438 |           0.0000 |
[32m[20221213 20:32:55 @agent_ppo2.py:185][0m |           0.0028 |           0.0855 |           0.0000 |
[32m[20221213 20:32:55 @agent_ppo2.py:185][0m |          -0.0076 |           0.0579 |           0.0000 |
[32m[20221213 20:32:55 @agent_ppo2.py:185][0m |          -0.0158 |           0.0468 |           0.0000 |
[32m[20221213 20:32:55 @agent_ppo2.py:185][0m |          -0.0221 |           0.0423 |           0.0000 |
[32m[20221213 20:32:55 @agent_ppo2.py:185][0m |          -0.0237 |           0.0353 |           0.0000 |
[32m[20221213 20:32:55 @agent_ppo2.py:185][0m |          -0.0259 |           0.0315 |           0.0000 |
[32m[20221213 20:32:55 @agent_ppo2.py:185][0m |          -0.0269 |           0.0298 |           0.0000 |
[32m[20221213 20:32:56 @agent_ppo2.py:185][0m |          -0.0272 |           0.0294 |           0.0000 |
[32m[20221213 20:32:56 @agent_ppo2.py:185][0m |          -0.0284 |           0.0288 |           0.0000 |
[32m[20221213 20:32:56 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:32:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.05
[32m[20221213 20:32:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.52
[32m[20221213 20:32:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.45
[32m[20221213 20:32:56 @agent_ppo2.py:143][0m Total time:       3.78 min
[32m[20221213 20:32:56 @agent_ppo2.py:145][0m 552960 total steps have happened
[32m[20221213 20:32:56 @agent_ppo2.py:121][0m #------------------------ Iteration 135 --------------------------#
[32m[20221213 20:32:56 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:32:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:57 @agent_ppo2.py:185][0m |           0.0100 |           0.0291 |           0.0000 |
[32m[20221213 20:32:57 @agent_ppo2.py:185][0m |          -0.0030 |           0.0138 |           0.0000 |
[32m[20221213 20:32:57 @agent_ppo2.py:185][0m |          -0.0148 |           0.0126 |           0.0000 |
[32m[20221213 20:32:57 @agent_ppo2.py:185][0m |          -0.0329 |           0.0122 |           0.0000 |
[32m[20221213 20:32:57 @agent_ppo2.py:185][0m |          -0.0408 |           0.0119 |           0.0000 |
[32m[20221213 20:32:57 @agent_ppo2.py:185][0m |          -0.0492 |           0.0116 |           0.0000 |
[32m[20221213 20:32:57 @agent_ppo2.py:185][0m |          -0.0503 |           0.0115 |           0.0000 |
[32m[20221213 20:32:57 @agent_ppo2.py:185][0m |          -0.0570 |           0.0114 |           0.0000 |
[32m[20221213 20:32:57 @agent_ppo2.py:185][0m |          -0.0651 |           0.0112 |           0.0000 |
[32m[20221213 20:32:57 @agent_ppo2.py:185][0m |          -0.0549 |           0.0111 |           0.0000 |
[32m[20221213 20:32:57 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:32:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.00
[32m[20221213 20:32:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.68
[32m[20221213 20:32:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.23
[32m[20221213 20:32:58 @agent_ppo2.py:143][0m Total time:       3.81 min
[32m[20221213 20:32:58 @agent_ppo2.py:145][0m 557056 total steps have happened
[32m[20221213 20:32:58 @agent_ppo2.py:121][0m #------------------------ Iteration 136 --------------------------#
[32m[20221213 20:32:58 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:32:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:32:58 @agent_ppo2.py:185][0m |           0.0416 |           0.0113 |           0.0000 |
[32m[20221213 20:32:58 @agent_ppo2.py:185][0m |           0.0011 |           0.0093 |           0.0000 |
[32m[20221213 20:32:58 @agent_ppo2.py:185][0m |          -0.0173 |           0.0087 |           0.0000 |
[32m[20221213 20:32:59 @agent_ppo2.py:185][0m |          -0.0343 |           0.0084 |           0.0000 |
[32m[20221213 20:32:59 @agent_ppo2.py:185][0m |          -0.0289 |           0.0082 |           0.0000 |
[32m[20221213 20:32:59 @agent_ppo2.py:185][0m |          -0.0452 |           0.0081 |           0.0000 |
[32m[20221213 20:32:59 @agent_ppo2.py:185][0m |          -0.0377 |           0.0080 |           0.0000 |
[32m[20221213 20:32:59 @agent_ppo2.py:185][0m |          -0.0504 |           0.0079 |           0.0000 |
[32m[20221213 20:32:59 @agent_ppo2.py:185][0m |          -0.0648 |           0.0078 |           0.0000 |
[32m[20221213 20:32:59 @agent_ppo2.py:185][0m |          -0.0485 |           0.0078 |           0.0000 |
[32m[20221213 20:32:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:32:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.99
[32m[20221213 20:32:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.58
[32m[20221213 20:32:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 15.11
[32m[20221213 20:32:59 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 15.11
[32m[20221213 20:32:59 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 15.11
[32m[20221213 20:32:59 @agent_ppo2.py:143][0m Total time:       3.84 min
[32m[20221213 20:32:59 @agent_ppo2.py:145][0m 561152 total steps have happened
[32m[20221213 20:33:00 @agent_ppo2.py:121][0m #------------------------ Iteration 137 --------------------------#
[32m[20221213 20:33:00 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:00 @agent_ppo2.py:185][0m |           0.0121 |           0.0622 |           0.0000 |
[32m[20221213 20:33:00 @agent_ppo2.py:185][0m |           0.0090 |           0.0281 |           0.0000 |
[32m[20221213 20:33:00 @agent_ppo2.py:185][0m |          -0.0061 |           0.0183 |           0.0000 |
[32m[20221213 20:33:00 @agent_ppo2.py:185][0m |          -0.0097 |           0.0162 |           0.0000 |
[32m[20221213 20:33:00 @agent_ppo2.py:185][0m |          -0.0118 |           0.0156 |           0.0000 |
[32m[20221213 20:33:01 @agent_ppo2.py:185][0m |          -0.0177 |           0.0152 |           0.0000 |
[32m[20221213 20:33:01 @agent_ppo2.py:185][0m |          -0.0223 |           0.0147 |           0.0000 |
[32m[20221213 20:33:01 @agent_ppo2.py:185][0m |          -0.0259 |           0.0144 |           0.0000 |
[32m[20221213 20:33:01 @agent_ppo2.py:185][0m |          -0.0264 |           0.0141 |           0.0000 |
[32m[20221213 20:33:01 @agent_ppo2.py:185][0m |          -0.0282 |           0.0139 |           0.0000 |
[32m[20221213 20:33:01 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:33:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.58
[32m[20221213 20:33:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.63
[32m[20221213 20:33:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.86
[32m[20221213 20:33:01 @agent_ppo2.py:143][0m Total time:       3.87 min
[32m[20221213 20:33:01 @agent_ppo2.py:145][0m 565248 total steps have happened
[32m[20221213 20:33:01 @agent_ppo2.py:121][0m #------------------------ Iteration 138 --------------------------#
[32m[20221213 20:33:02 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:02 @agent_ppo2.py:185][0m |           0.0144 |           0.0149 |           0.0000 |
[32m[20221213 20:33:02 @agent_ppo2.py:185][0m |          -0.0033 |           0.0098 |           0.0000 |
[32m[20221213 20:33:02 @agent_ppo2.py:185][0m |          -0.0279 |           0.0092 |           0.0000 |
[32m[20221213 20:33:02 @agent_ppo2.py:185][0m |          -0.0371 |           0.0090 |           0.0000 |
[32m[20221213 20:33:02 @agent_ppo2.py:185][0m |          -0.0432 |           0.0088 |           0.0000 |
[32m[20221213 20:33:02 @agent_ppo2.py:185][0m |          -0.0466 |           0.0087 |           0.0000 |
[32m[20221213 20:33:02 @agent_ppo2.py:185][0m |          -0.0528 |           0.0086 |           0.0000 |
[32m[20221213 20:33:02 @agent_ppo2.py:185][0m |          -0.0524 |           0.0085 |           0.0000 |
[32m[20221213 20:33:02 @agent_ppo2.py:185][0m |          -0.0573 |           0.0085 |           0.0000 |
[32m[20221213 20:33:03 @agent_ppo2.py:185][0m |          -0.0666 |           0.0084 |           0.0000 |
[32m[20221213 20:33:03 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:33:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.66
[32m[20221213 20:33:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.14
[32m[20221213 20:33:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.48
[32m[20221213 20:33:03 @agent_ppo2.py:143][0m Total time:       3.90 min
[32m[20221213 20:33:03 @agent_ppo2.py:145][0m 569344 total steps have happened
[32m[20221213 20:33:03 @agent_ppo2.py:121][0m #------------------------ Iteration 139 --------------------------#
[32m[20221213 20:33:03 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:03 @agent_ppo2.py:185][0m |           0.0221 |           0.0131 |           0.0000 |
[32m[20221213 20:33:04 @agent_ppo2.py:185][0m |          -0.0004 |           0.0108 |           0.0000 |
[32m[20221213 20:33:04 @agent_ppo2.py:185][0m |          -0.0145 |           0.0102 |           0.0000 |
[32m[20221213 20:33:04 @agent_ppo2.py:185][0m |          -0.0191 |           0.0100 |           0.0000 |
[32m[20221213 20:33:04 @agent_ppo2.py:185][0m |          -0.0207 |           0.0098 |           0.0000 |
[32m[20221213 20:33:04 @agent_ppo2.py:185][0m |          -0.0266 |           0.0096 |           0.0000 |
[32m[20221213 20:33:04 @agent_ppo2.py:185][0m |          -0.0282 |           0.0095 |           0.0000 |
[32m[20221213 20:33:04 @agent_ppo2.py:185][0m |          -0.0323 |           0.0094 |           0.0000 |
[32m[20221213 20:33:04 @agent_ppo2.py:185][0m |          -0.0304 |           0.0094 |           0.0000 |
[32m[20221213 20:33:04 @agent_ppo2.py:185][0m |          -0.0288 |           0.0093 |           0.0000 |
[32m[20221213 20:33:04 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 20:33:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.58
[32m[20221213 20:33:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.98
[32m[20221213 20:33:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.77
[32m[20221213 20:33:05 @agent_ppo2.py:143][0m Total time:       3.93 min
[32m[20221213 20:33:05 @agent_ppo2.py:145][0m 573440 total steps have happened
[32m[20221213 20:33:05 @agent_ppo2.py:121][0m #------------------------ Iteration 140 --------------------------#
[32m[20221213 20:33:05 @agent_ppo2.py:127][0m Sampling time: 0.43 s by 5 slaves
[32m[20221213 20:33:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:05 @agent_ppo2.py:185][0m |           0.0178 |           0.0145 |           0.0000 |
[32m[20221213 20:33:06 @agent_ppo2.py:185][0m |          -0.0015 |           0.0128 |           0.0000 |
[32m[20221213 20:33:06 @agent_ppo2.py:185][0m |          -0.0228 |           0.0127 |           0.0000 |
[32m[20221213 20:33:06 @agent_ppo2.py:185][0m |          -0.0302 |           0.0122 |           0.0000 |
[32m[20221213 20:33:06 @agent_ppo2.py:185][0m |          -0.0342 |           0.0120 |           0.0000 |
[32m[20221213 20:33:06 @agent_ppo2.py:185][0m |          -0.0370 |           0.0117 |           0.0000 |
[32m[20221213 20:33:06 @agent_ppo2.py:185][0m |          -0.0413 |           0.0127 |           0.0000 |
[32m[20221213 20:33:06 @agent_ppo2.py:185][0m |          -0.0352 |           0.0137 |           0.0000 |
[32m[20221213 20:33:06 @agent_ppo2.py:185][0m |          -0.0981 |           0.0129 |           0.0000 |
[32m[20221213 20:33:06 @agent_ppo2.py:185][0m |          -0.0487 |           0.0130 |           0.0000 |
[32m[20221213 20:33:06 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:33:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.67
[32m[20221213 20:33:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.96
[32m[20221213 20:33:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.22
[32m[20221213 20:33:07 @agent_ppo2.py:143][0m Total time:       3.96 min
[32m[20221213 20:33:07 @agent_ppo2.py:145][0m 577536 total steps have happened
[32m[20221213 20:33:07 @agent_ppo2.py:121][0m #------------------------ Iteration 141 --------------------------#
[32m[20221213 20:33:07 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:33:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:07 @agent_ppo2.py:185][0m |           0.0946 |           0.0088 |           0.0000 |
[32m[20221213 20:33:07 @agent_ppo2.py:185][0m |           0.0620 |           0.0079 |           0.0000 |
[32m[20221213 20:33:07 @agent_ppo2.py:185][0m |           0.0255 |           0.0078 |           0.0000 |
[32m[20221213 20:33:08 @agent_ppo2.py:185][0m |          -0.0158 |           0.0077 |           0.0000 |
[32m[20221213 20:33:08 @agent_ppo2.py:185][0m |          -0.0265 |           0.0077 |           0.0000 |
[32m[20221213 20:33:08 @agent_ppo2.py:185][0m |          -0.0526 |           0.0076 |           0.0000 |
[32m[20221213 20:33:08 @agent_ppo2.py:185][0m |          -0.0455 |           0.0075 |           0.0000 |
[32m[20221213 20:33:08 @agent_ppo2.py:185][0m |          -0.0592 |           0.0075 |           0.0000 |
[32m[20221213 20:33:08 @agent_ppo2.py:185][0m |          -0.0682 |           0.0074 |           0.0000 |
[32m[20221213 20:33:08 @agent_ppo2.py:185][0m |          -0.0629 |           0.0074 |           0.0000 |
[32m[20221213 20:33:08 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 20:33:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.28
[32m[20221213 20:33:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.66
[32m[20221213 20:33:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.11
[32m[20221213 20:33:08 @agent_ppo2.py:143][0m Total time:       3.99 min
[32m[20221213 20:33:08 @agent_ppo2.py:145][0m 581632 total steps have happened
[32m[20221213 20:33:08 @agent_ppo2.py:121][0m #------------------------ Iteration 142 --------------------------#
[32m[20221213 20:33:09 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:33:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:09 @agent_ppo2.py:185][0m |           0.0235 |           0.0097 |           0.0000 |
[32m[20221213 20:33:09 @agent_ppo2.py:185][0m |           0.0271 |           0.0068 |           0.0000 |
[32m[20221213 20:33:09 @agent_ppo2.py:185][0m |          -0.0085 |           0.0062 |           0.0000 |
[32m[20221213 20:33:09 @agent_ppo2.py:185][0m |          -0.0153 |           0.0060 |           0.0000 |
[32m[20221213 20:33:09 @agent_ppo2.py:185][0m |          -0.0215 |           0.0058 |           0.0000 |
[32m[20221213 20:33:10 @agent_ppo2.py:185][0m |          -0.0222 |           0.0057 |           0.0000 |
[32m[20221213 20:33:10 @agent_ppo2.py:185][0m |          -0.0268 |           0.0056 |           0.0000 |
[32m[20221213 20:33:10 @agent_ppo2.py:185][0m |          -0.0280 |           0.0055 |           0.0000 |
[32m[20221213 20:33:10 @agent_ppo2.py:185][0m |          -0.0331 |           0.0054 |           0.0000 |
[32m[20221213 20:33:10 @agent_ppo2.py:185][0m |          -0.0384 |           0.0054 |           0.0000 |
[32m[20221213 20:33:10 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:33:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.58
[32m[20221213 20:33:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.69
[32m[20221213 20:33:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.57
[32m[20221213 20:33:10 @agent_ppo2.py:143][0m Total time:       4.02 min
[32m[20221213 20:33:10 @agent_ppo2.py:145][0m 585728 total steps have happened
[32m[20221213 20:33:10 @agent_ppo2.py:121][0m #------------------------ Iteration 143 --------------------------#
[32m[20221213 20:33:11 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:33:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:11 @agent_ppo2.py:185][0m |           0.0188 |           0.0758 |           0.0000 |
[32m[20221213 20:33:11 @agent_ppo2.py:185][0m |          -0.0040 |           0.0378 |           0.0000 |
[32m[20221213 20:33:11 @agent_ppo2.py:185][0m |          -0.0113 |           0.0242 |           0.0000 |
[32m[20221213 20:33:11 @agent_ppo2.py:185][0m |          -0.0118 |           0.0212 |           0.0000 |
[32m[20221213 20:33:11 @agent_ppo2.py:185][0m |          -0.0183 |           0.0198 |           0.0000 |
[32m[20221213 20:33:11 @agent_ppo2.py:185][0m |          -0.0150 |           0.0191 |           0.0000 |
[32m[20221213 20:33:11 @agent_ppo2.py:185][0m |          -0.0149 |           0.0179 |           0.0000 |
[32m[20221213 20:33:11 @agent_ppo2.py:185][0m |          -0.0208 |           0.0173 |           0.0000 |
[32m[20221213 20:33:12 @agent_ppo2.py:185][0m |          -0.0218 |           0.0168 |           0.0000 |
[32m[20221213 20:33:12 @agent_ppo2.py:185][0m |          -0.0231 |           0.0168 |           0.0000 |
[32m[20221213 20:33:12 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:33:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.66
[32m[20221213 20:33:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.62
[32m[20221213 20:33:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.38
[32m[20221213 20:33:12 @agent_ppo2.py:143][0m Total time:       4.05 min
[32m[20221213 20:33:12 @agent_ppo2.py:145][0m 589824 total steps have happened
[32m[20221213 20:33:12 @agent_ppo2.py:121][0m #------------------------ Iteration 144 --------------------------#
[32m[20221213 20:33:12 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:13 @agent_ppo2.py:185][0m |           0.0124 |           0.0680 |           0.0000 |
[32m[20221213 20:33:13 @agent_ppo2.py:185][0m |           0.0267 |           0.0492 |           0.0000 |
[32m[20221213 20:33:13 @agent_ppo2.py:185][0m |           0.0043 |           0.0326 |           0.0000 |
[32m[20221213 20:33:13 @agent_ppo2.py:185][0m |          -0.0139 |           0.0296 |           0.0000 |
[32m[20221213 20:33:13 @agent_ppo2.py:185][0m |          -0.0246 |           0.0273 |           0.0000 |
[32m[20221213 20:33:13 @agent_ppo2.py:185][0m |          -0.0212 |           0.0257 |           0.0000 |
[32m[20221213 20:33:13 @agent_ppo2.py:185][0m |          -0.0269 |           0.0252 |           0.0000 |
[32m[20221213 20:33:13 @agent_ppo2.py:185][0m |          -0.0300 |           0.0253 |           0.0000 |
[32m[20221213 20:33:13 @agent_ppo2.py:185][0m |          -0.0324 |           0.0260 |           0.0000 |
[32m[20221213 20:33:13 @agent_ppo2.py:185][0m |          -0.0369 |           0.0239 |           0.0000 |
[32m[20221213 20:33:13 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:33:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.72
[32m[20221213 20:33:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.20
[32m[20221213 20:33:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.04
[32m[20221213 20:33:14 @agent_ppo2.py:143][0m Total time:       4.08 min
[32m[20221213 20:33:14 @agent_ppo2.py:145][0m 593920 total steps have happened
[32m[20221213 20:33:14 @agent_ppo2.py:121][0m #------------------------ Iteration 145 --------------------------#
[32m[20221213 20:33:14 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:14 @agent_ppo2.py:185][0m |           0.0466 |           0.0637 |           0.0000 |
[32m[20221213 20:33:14 @agent_ppo2.py:185][0m |          -0.0020 |           0.0483 |           0.0000 |
[32m[20221213 20:33:14 @agent_ppo2.py:185][0m |          -0.0192 |           0.0355 |           0.0000 |
[32m[20221213 20:33:15 @agent_ppo2.py:185][0m |          -0.0186 |           0.0330 |           0.0000 |
[32m[20221213 20:33:15 @agent_ppo2.py:185][0m |          -0.0363 |           0.0329 |           0.0000 |
[32m[20221213 20:33:15 @agent_ppo2.py:185][0m |          -0.0415 |           0.0293 |           0.0000 |
[32m[20221213 20:33:15 @agent_ppo2.py:185][0m |          -0.0371 |           0.0289 |           0.0000 |
[32m[20221213 20:33:15 @agent_ppo2.py:185][0m |          -0.0456 |           0.0288 |           0.0000 |
[32m[20221213 20:33:15 @agent_ppo2.py:185][0m |          -0.0533 |           0.0271 |           0.0000 |
[32m[20221213 20:33:15 @agent_ppo2.py:185][0m |          -0.0537 |           0.0265 |           0.0000 |
[32m[20221213 20:33:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:33:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.92
[32m[20221213 20:33:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.53
[32m[20221213 20:33:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.60
[32m[20221213 20:33:15 @agent_ppo2.py:143][0m Total time:       4.10 min
[32m[20221213 20:33:15 @agent_ppo2.py:145][0m 598016 total steps have happened
[32m[20221213 20:33:15 @agent_ppo2.py:121][0m #------------------------ Iteration 146 --------------------------#
[32m[20221213 20:33:16 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:16 @agent_ppo2.py:185][0m |           0.0614 |           0.0342 |           0.0000 |
[32m[20221213 20:33:16 @agent_ppo2.py:185][0m |           0.0483 |           0.0199 |           0.0000 |
[32m[20221213 20:33:16 @agent_ppo2.py:185][0m |          -0.0021 |           0.0188 |           0.0000 |
[32m[20221213 20:33:16 @agent_ppo2.py:185][0m |          -0.0167 |           0.0181 |           0.0000 |
[32m[20221213 20:33:16 @agent_ppo2.py:185][0m |          -0.0284 |           0.0178 |           0.0000 |
[32m[20221213 20:33:16 @agent_ppo2.py:185][0m |          -0.0409 |           0.0173 |           0.0000 |
[32m[20221213 20:33:17 @agent_ppo2.py:185][0m |          -0.0243 |           0.0190 |           0.0000 |
[32m[20221213 20:33:17 @agent_ppo2.py:185][0m |          -0.0387 |           0.0195 |           0.0000 |
[32m[20221213 20:33:17 @agent_ppo2.py:185][0m |          -0.0557 |           0.0168 |           0.0000 |
[32m[20221213 20:33:17 @agent_ppo2.py:185][0m |          -0.0533 |           0.0162 |           0.0000 |
[32m[20221213 20:33:17 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:33:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.65
[32m[20221213 20:33:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.99
[32m[20221213 20:33:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.24
[32m[20221213 20:33:17 @agent_ppo2.py:143][0m Total time:       4.13 min
[32m[20221213 20:33:17 @agent_ppo2.py:145][0m 602112 total steps have happened
[32m[20221213 20:33:17 @agent_ppo2.py:121][0m #------------------------ Iteration 147 --------------------------#
[32m[20221213 20:33:18 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:18 @agent_ppo2.py:185][0m |           0.0201 |           0.0352 |           0.0000 |
[32m[20221213 20:33:18 @agent_ppo2.py:185][0m |          -0.0151 |           0.0234 |           0.0000 |
[32m[20221213 20:33:18 @agent_ppo2.py:185][0m |          -0.0152 |           0.0225 |           0.0000 |
[32m[20221213 20:33:18 @agent_ppo2.py:185][0m |          -0.0250 |           0.0218 |           0.0000 |
[32m[20221213 20:33:18 @agent_ppo2.py:185][0m |          -0.0290 |           0.0203 |           0.0000 |
[32m[20221213 20:33:18 @agent_ppo2.py:185][0m |          -0.0356 |           0.0200 |           0.0000 |
[32m[20221213 20:33:18 @agent_ppo2.py:185][0m |          -0.0349 |           0.0194 |           0.0000 |
[32m[20221213 20:33:18 @agent_ppo2.py:185][0m |          -0.0361 |           0.0189 |           0.0000 |
[32m[20221213 20:33:18 @agent_ppo2.py:185][0m |          -0.0509 |           0.0187 |           0.0000 |
[32m[20221213 20:33:19 @agent_ppo2.py:185][0m |          -0.0504 |           0.0188 |           0.0000 |
[32m[20221213 20:33:19 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:33:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.61
[32m[20221213 20:33:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.26
[32m[20221213 20:33:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.67
[32m[20221213 20:33:19 @agent_ppo2.py:143][0m Total time:       4.16 min
[32m[20221213 20:33:19 @agent_ppo2.py:145][0m 606208 total steps have happened
[32m[20221213 20:33:19 @agent_ppo2.py:121][0m #------------------------ Iteration 148 --------------------------#
[32m[20221213 20:33:19 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:33:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:20 @agent_ppo2.py:185][0m |           0.0320 |           0.0245 |           0.0000 |
[32m[20221213 20:33:20 @agent_ppo2.py:185][0m |          -0.0083 |           0.0218 |           0.0000 |
[32m[20221213 20:33:20 @agent_ppo2.py:185][0m |          -0.0427 |           0.0204 |           0.0000 |
[32m[20221213 20:33:20 @agent_ppo2.py:185][0m |          -0.0546 |           0.0197 |           0.0000 |
[32m[20221213 20:33:20 @agent_ppo2.py:185][0m |          -0.0582 |           0.0190 |           0.0000 |
[32m[20221213 20:33:20 @agent_ppo2.py:185][0m |          -0.0641 |           0.0187 |           0.0000 |
[32m[20221213 20:33:20 @agent_ppo2.py:185][0m |          -0.0857 |           0.0181 |           0.0000 |
[32m[20221213 20:33:20 @agent_ppo2.py:185][0m |          -0.0899 |           0.0179 |           0.0000 |
[32m[20221213 20:33:20 @agent_ppo2.py:185][0m |          -0.0814 |           0.0175 |           0.0000 |
[32m[20221213 20:33:20 @agent_ppo2.py:185][0m |          -0.0892 |           0.0173 |           0.0000 |
[32m[20221213 20:33:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:33:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.01
[32m[20221213 20:33:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.25
[32m[20221213 20:33:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.93
[32m[20221213 20:33:21 @agent_ppo2.py:143][0m Total time:       4.19 min
[32m[20221213 20:33:21 @agent_ppo2.py:145][0m 610304 total steps have happened
[32m[20221213 20:33:21 @agent_ppo2.py:121][0m #------------------------ Iteration 149 --------------------------#
[32m[20221213 20:33:21 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:33:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:21 @agent_ppo2.py:185][0m |           0.0220 |           0.0244 |           0.0000 |
[32m[20221213 20:33:21 @agent_ppo2.py:185][0m |           0.0145 |           0.0110 |           0.0000 |
[32m[20221213 20:33:21 @agent_ppo2.py:185][0m |          -0.0134 |           0.0104 |           0.0000 |
[32m[20221213 20:33:22 @agent_ppo2.py:185][0m |          -0.0239 |           0.0102 |           0.0000 |
[32m[20221213 20:33:22 @agent_ppo2.py:185][0m |          -0.0249 |           0.0101 |           0.0000 |
[32m[20221213 20:33:22 @agent_ppo2.py:185][0m |          -0.0312 |           0.0099 |           0.0000 |
[32m[20221213 20:33:22 @agent_ppo2.py:185][0m |          -0.0047 |           0.0098 |           0.0000 |
[32m[20221213 20:33:22 @agent_ppo2.py:185][0m |          -0.0443 |           0.0097 |           0.0000 |
[32m[20221213 20:33:22 @agent_ppo2.py:185][0m |          -0.0442 |           0.0096 |           0.0000 |
[32m[20221213 20:33:22 @agent_ppo2.py:185][0m |          -0.0447 |           0.0096 |           0.0000 |
[32m[20221213 20:33:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:33:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.11
[32m[20221213 20:33:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.45
[32m[20221213 20:33:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.28
[32m[20221213 20:33:22 @agent_ppo2.py:143][0m Total time:       4.22 min
[32m[20221213 20:33:22 @agent_ppo2.py:145][0m 614400 total steps have happened
[32m[20221213 20:33:22 @agent_ppo2.py:121][0m #------------------------ Iteration 150 --------------------------#
[32m[20221213 20:33:23 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:33:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:23 @agent_ppo2.py:185][0m |           0.0215 |           0.0132 |           0.0000 |
[32m[20221213 20:33:23 @agent_ppo2.py:185][0m |          -0.0052 |           0.0121 |           0.0000 |
[32m[20221213 20:33:23 @agent_ppo2.py:185][0m |          -0.0219 |           0.0120 |           0.0000 |
[32m[20221213 20:33:23 @agent_ppo2.py:185][0m |          -0.0226 |           0.0119 |           0.0000 |
[32m[20221213 20:33:23 @agent_ppo2.py:185][0m |          -0.0333 |           0.0115 |           0.0000 |
[32m[20221213 20:33:23 @agent_ppo2.py:185][0m |          -0.0359 |           0.0114 |           0.0000 |
[32m[20221213 20:33:23 @agent_ppo2.py:185][0m |          -0.0398 |           0.0113 |           0.0000 |
[32m[20221213 20:33:24 @agent_ppo2.py:185][0m |          -0.0385 |           0.0111 |           0.0000 |
[32m[20221213 20:33:24 @agent_ppo2.py:185][0m |          -0.0462 |           0.0110 |           0.0000 |
[32m[20221213 20:33:24 @agent_ppo2.py:185][0m |          -0.0394 |           0.0110 |           0.0000 |
[32m[20221213 20:33:24 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:33:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.38
[32m[20221213 20:33:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.79
[32m[20221213 20:33:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.59
[32m[20221213 20:33:24 @agent_ppo2.py:143][0m Total time:       4.25 min
[32m[20221213 20:33:24 @agent_ppo2.py:145][0m 618496 total steps have happened
[32m[20221213 20:33:24 @agent_ppo2.py:121][0m #------------------------ Iteration 151 --------------------------#
[32m[20221213 20:33:24 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:25 @agent_ppo2.py:185][0m |           0.0253 |           0.0614 |           0.0000 |
[32m[20221213 20:33:25 @agent_ppo2.py:185][0m |           0.0048 |           0.0284 |           0.0000 |
[32m[20221213 20:33:25 @agent_ppo2.py:185][0m |          -0.0106 |           0.0244 |           0.0000 |
[32m[20221213 20:33:25 @agent_ppo2.py:185][0m |          -0.0137 |           0.0233 |           0.0000 |
[32m[20221213 20:33:25 @agent_ppo2.py:185][0m |          -0.0200 |           0.0220 |           0.0000 |
[32m[20221213 20:33:25 @agent_ppo2.py:185][0m |          -0.0235 |           0.0219 |           0.0000 |
[32m[20221213 20:33:25 @agent_ppo2.py:185][0m |          -0.0242 |           0.0213 |           0.0000 |
[32m[20221213 20:33:25 @agent_ppo2.py:185][0m |          -0.0265 |           0.0210 |           0.0000 |
[32m[20221213 20:33:25 @agent_ppo2.py:185][0m |          -0.0270 |           0.0204 |           0.0000 |
[32m[20221213 20:33:25 @agent_ppo2.py:185][0m |          -0.1265 |           0.0284 |           0.0000 |
[32m[20221213 20:33:25 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:33:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.56
[32m[20221213 20:33:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.86
[32m[20221213 20:33:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.79
[32m[20221213 20:33:26 @agent_ppo2.py:143][0m Total time:       4.28 min
[32m[20221213 20:33:26 @agent_ppo2.py:145][0m 622592 total steps have happened
[32m[20221213 20:33:26 @agent_ppo2.py:121][0m #------------------------ Iteration 152 --------------------------#
[32m[20221213 20:33:26 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:33:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:26 @agent_ppo2.py:185][0m |           0.0192 |           0.0564 |           0.0000 |
[32m[20221213 20:33:27 @agent_ppo2.py:185][0m |          -0.0050 |           0.0255 |           0.0000 |
[32m[20221213 20:33:27 @agent_ppo2.py:185][0m |          -0.0277 |           0.0235 |           0.0000 |
[32m[20221213 20:33:27 @agent_ppo2.py:185][0m |          -0.0187 |           0.0233 |           0.0000 |
[32m[20221213 20:33:27 @agent_ppo2.py:185][0m |          -0.0277 |           0.0237 |           0.0000 |
[32m[20221213 20:33:27 @agent_ppo2.py:185][0m |          -0.0587 |           0.0227 |           0.0000 |
[32m[20221213 20:33:27 @agent_ppo2.py:185][0m |          -0.0544 |           0.0215 |           0.0000 |
[32m[20221213 20:33:27 @agent_ppo2.py:185][0m |          -0.0390 |           0.0222 |           0.0000 |
[32m[20221213 20:33:27 @agent_ppo2.py:185][0m |          -0.0718 |           0.0214 |           0.0000 |
[32m[20221213 20:33:27 @agent_ppo2.py:185][0m |          -0.0306 |           0.0237 |           0.0000 |
[32m[20221213 20:33:27 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:33:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.62
[32m[20221213 20:33:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.62
[32m[20221213 20:33:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.27
[32m[20221213 20:33:28 @agent_ppo2.py:143][0m Total time:       4.31 min
[32m[20221213 20:33:28 @agent_ppo2.py:145][0m 626688 total steps have happened
[32m[20221213 20:33:28 @agent_ppo2.py:121][0m #------------------------ Iteration 153 --------------------------#
[32m[20221213 20:33:28 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:33:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:28 @agent_ppo2.py:185][0m |           0.0478 |           0.0192 |           0.0000 |
[32m[20221213 20:33:28 @agent_ppo2.py:185][0m |           0.0500 |           0.0169 |           0.0000 |
[32m[20221213 20:33:29 @agent_ppo2.py:185][0m |           0.0255 |           0.0160 |           0.0000 |
[32m[20221213 20:33:29 @agent_ppo2.py:185][0m |          -0.0047 |           0.0152 |           0.0000 |
[32m[20221213 20:33:29 @agent_ppo2.py:185][0m |          -0.0183 |           0.0154 |           0.0000 |
[32m[20221213 20:33:29 @agent_ppo2.py:185][0m |          -0.0457 |           0.0150 |           0.0000 |
[32m[20221213 20:33:29 @agent_ppo2.py:185][0m |          -0.0441 |           0.0145 |           0.0000 |
[32m[20221213 20:33:29 @agent_ppo2.py:185][0m |          -0.0492 |           0.0143 |           0.0000 |
[32m[20221213 20:33:29 @agent_ppo2.py:185][0m |          -0.0557 |           0.0143 |           0.0000 |
[32m[20221213 20:33:29 @agent_ppo2.py:185][0m |          -0.0692 |           0.0139 |           0.0000 |
[32m[20221213 20:33:29 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:33:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.84
[32m[20221213 20:33:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.71
[32m[20221213 20:33:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.49
[32m[20221213 20:33:30 @agent_ppo2.py:143][0m Total time:       4.34 min
[32m[20221213 20:33:30 @agent_ppo2.py:145][0m 630784 total steps have happened
[32m[20221213 20:33:30 @agent_ppo2.py:121][0m #------------------------ Iteration 154 --------------------------#
[32m[20221213 20:33:30 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:33:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:30 @agent_ppo2.py:185][0m |           0.0239 |           0.0150 |           0.0000 |
[32m[20221213 20:33:30 @agent_ppo2.py:185][0m |          -0.0262 |           0.0144 |           0.0000 |
[32m[20221213 20:33:30 @agent_ppo2.py:185][0m |          -0.0431 |           0.0140 |           0.0000 |
[32m[20221213 20:33:30 @agent_ppo2.py:185][0m |          -0.0483 |           0.0138 |           0.0000 |
[32m[20221213 20:33:31 @agent_ppo2.py:185][0m |          -0.0596 |           0.0137 |           0.0000 |
[32m[20221213 20:33:31 @agent_ppo2.py:185][0m |          -0.0616 |           0.0134 |           0.0000 |
[32m[20221213 20:33:31 @agent_ppo2.py:185][0m |          -0.0651 |           0.0132 |           0.0000 |
[32m[20221213 20:33:31 @agent_ppo2.py:185][0m |          -0.0634 |           0.0135 |           0.0000 |
[32m[20221213 20:33:31 @agent_ppo2.py:185][0m |          -0.0774 |           0.0135 |           0.0000 |
[32m[20221213 20:33:31 @agent_ppo2.py:185][0m |          -0.0839 |           0.0128 |           0.0000 |
[32m[20221213 20:33:31 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:33:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.60
[32m[20221213 20:33:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.78
[32m[20221213 20:33:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.96
[32m[20221213 20:33:31 @agent_ppo2.py:143][0m Total time:       4.37 min
[32m[20221213 20:33:31 @agent_ppo2.py:145][0m 634880 total steps have happened
[32m[20221213 20:33:31 @agent_ppo2.py:121][0m #------------------------ Iteration 155 --------------------------#
[32m[20221213 20:33:32 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:33:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:32 @agent_ppo2.py:185][0m |           0.0305 |           0.0131 |           0.0000 |
[32m[20221213 20:33:32 @agent_ppo2.py:185][0m |          -0.0173 |           0.0112 |           0.0000 |
[32m[20221213 20:33:32 @agent_ppo2.py:185][0m |          -0.0329 |           0.0110 |           0.0000 |
[32m[20221213 20:33:32 @agent_ppo2.py:185][0m |          -0.0487 |           0.0108 |           0.0000 |
[32m[20221213 20:33:32 @agent_ppo2.py:185][0m |          -0.0628 |           0.0107 |           0.0000 |
[32m[20221213 20:33:32 @agent_ppo2.py:185][0m |          -0.0457 |           0.0108 |           0.0000 |
[32m[20221213 20:33:32 @agent_ppo2.py:185][0m |          -0.0665 |           0.0106 |           0.0000 |
[32m[20221213 20:33:33 @agent_ppo2.py:185][0m |          -0.0675 |           0.0103 |           0.0000 |
[32m[20221213 20:33:33 @agent_ppo2.py:185][0m |          -0.0646 |           0.0103 |           0.0000 |
[32m[20221213 20:33:33 @agent_ppo2.py:185][0m |          -0.0733 |           0.0102 |           0.0000 |
[32m[20221213 20:33:33 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:33:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.22
[32m[20221213 20:33:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.67
[32m[20221213 20:33:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.56
[32m[20221213 20:33:33 @agent_ppo2.py:143][0m Total time:       4.40 min
[32m[20221213 20:33:33 @agent_ppo2.py:145][0m 638976 total steps have happened
[32m[20221213 20:33:33 @agent_ppo2.py:121][0m #------------------------ Iteration 156 --------------------------#
[32m[20221213 20:33:33 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:34 @agent_ppo2.py:185][0m |           0.0208 |           0.0105 |           0.0000 |
[32m[20221213 20:33:34 @agent_ppo2.py:185][0m |          -0.0087 |           0.0090 |           0.0000 |
[32m[20221213 20:33:34 @agent_ppo2.py:185][0m |          -0.0015 |           0.0091 |           0.0000 |
[32m[20221213 20:33:34 @agent_ppo2.py:185][0m |          -0.0426 |           0.0089 |           0.0000 |
[32m[20221213 20:33:34 @agent_ppo2.py:185][0m |          -0.0372 |           0.0087 |           0.0000 |
[32m[20221213 20:33:34 @agent_ppo2.py:185][0m |          -0.0511 |           0.0086 |           0.0000 |
[32m[20221213 20:33:34 @agent_ppo2.py:185][0m |          -0.0513 |           0.0085 |           0.0000 |
[32m[20221213 20:33:34 @agent_ppo2.py:185][0m |          -0.0606 |           0.0085 |           0.0000 |
[32m[20221213 20:33:34 @agent_ppo2.py:185][0m |          -0.0517 |           0.0084 |           0.0000 |
[32m[20221213 20:33:34 @agent_ppo2.py:185][0m |          -0.0601 |           0.0084 |           0.0000 |
[32m[20221213 20:33:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:33:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.16
[32m[20221213 20:33:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.88
[32m[20221213 20:33:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.82
[32m[20221213 20:33:35 @agent_ppo2.py:143][0m Total time:       4.43 min
[32m[20221213 20:33:35 @agent_ppo2.py:145][0m 643072 total steps have happened
[32m[20221213 20:33:35 @agent_ppo2.py:121][0m #------------------------ Iteration 157 --------------------------#
[32m[20221213 20:33:35 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:35 @agent_ppo2.py:185][0m |           0.0251 |           0.0471 |           0.0000 |
[32m[20221213 20:33:35 @agent_ppo2.py:185][0m |           0.0041 |           0.0219 |           0.0000 |
[32m[20221213 20:33:36 @agent_ppo2.py:185][0m |          -0.0107 |           0.0184 |           0.0000 |
[32m[20221213 20:33:36 @agent_ppo2.py:185][0m |          -0.0135 |           0.0183 |           0.0000 |
[32m[20221213 20:33:36 @agent_ppo2.py:185][0m |          -0.0196 |           0.0174 |           0.0000 |
[32m[20221213 20:33:36 @agent_ppo2.py:185][0m |          -0.0237 |           0.0169 |           0.0000 |
[32m[20221213 20:33:36 @agent_ppo2.py:185][0m |          -0.0259 |           0.0168 |           0.0000 |
[32m[20221213 20:33:36 @agent_ppo2.py:185][0m |          -0.0267 |           0.0163 |           0.0000 |
[32m[20221213 20:33:36 @agent_ppo2.py:185][0m |          -0.0283 |           0.0168 |           0.0000 |
[32m[20221213 20:33:36 @agent_ppo2.py:185][0m |          -0.0289 |           0.0158 |           0.0000 |
[32m[20221213 20:33:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:33:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.60
[32m[20221213 20:33:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.35
[32m[20221213 20:33:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.10
[32m[20221213 20:33:37 @agent_ppo2.py:143][0m Total time:       4.46 min
[32m[20221213 20:33:37 @agent_ppo2.py:145][0m 647168 total steps have happened
[32m[20221213 20:33:37 @agent_ppo2.py:121][0m #------------------------ Iteration 158 --------------------------#
[32m[20221213 20:33:37 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:37 @agent_ppo2.py:185][0m |           0.0268 |           0.0264 |           0.0000 |
[32m[20221213 20:33:37 @agent_ppo2.py:185][0m |          -0.0128 |           0.0229 |           0.0000 |
[32m[20221213 20:33:37 @agent_ppo2.py:185][0m |          -0.0217 |           0.0214 |           0.0000 |
[32m[20221213 20:33:37 @agent_ppo2.py:185][0m |          -0.0307 |           0.0202 |           0.0000 |
[32m[20221213 20:33:37 @agent_ppo2.py:185][0m |          -0.0427 |           0.0183 |           0.0000 |
[32m[20221213 20:33:38 @agent_ppo2.py:185][0m |          -0.0483 |           0.0181 |           0.0000 |
[32m[20221213 20:33:38 @agent_ppo2.py:185][0m |          -0.0519 |           0.0187 |           0.0000 |
[32m[20221213 20:33:38 @agent_ppo2.py:185][0m |          -0.0576 |           0.0174 |           0.0000 |
[32m[20221213 20:33:38 @agent_ppo2.py:185][0m |          -0.0515 |           0.0174 |           0.0000 |
[32m[20221213 20:33:38 @agent_ppo2.py:185][0m |          -0.0520 |           0.0169 |           0.0000 |
[32m[20221213 20:33:38 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:33:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.75
[32m[20221213 20:33:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.59
[32m[20221213 20:33:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.10
[32m[20221213 20:33:38 @agent_ppo2.py:143][0m Total time:       4.49 min
[32m[20221213 20:33:38 @agent_ppo2.py:145][0m 651264 total steps have happened
[32m[20221213 20:33:38 @agent_ppo2.py:121][0m #------------------------ Iteration 159 --------------------------#
[32m[20221213 20:33:39 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:33:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:39 @agent_ppo2.py:185][0m |          -0.0017 |           0.1001 |           0.0000 |
[32m[20221213 20:33:39 @agent_ppo2.py:185][0m |           0.0312 |           0.0557 |           0.0000 |
[32m[20221213 20:33:39 @agent_ppo2.py:185][0m |           0.0122 |           0.0387 |           0.0000 |
[32m[20221213 20:33:39 @agent_ppo2.py:185][0m |          -0.0014 |           0.0354 |           0.0000 |
[32m[20221213 20:33:39 @agent_ppo2.py:185][0m |          -0.0046 |           0.0348 |           0.0000 |
[32m[20221213 20:33:39 @agent_ppo2.py:185][0m |          -0.0150 |           0.0322 |           0.0000 |
[32m[20221213 20:33:39 @agent_ppo2.py:185][0m |          -0.0165 |           0.0301 |           0.0000 |
[32m[20221213 20:33:40 @agent_ppo2.py:185][0m |          -0.0235 |           0.0293 |           0.0000 |
[32m[20221213 20:33:40 @agent_ppo2.py:185][0m |          -0.0133 |           0.0284 |           0.0000 |
[32m[20221213 20:33:40 @agent_ppo2.py:185][0m |          -0.0278 |           0.0296 |           0.0000 |
[32m[20221213 20:33:40 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:33:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.37
[32m[20221213 20:33:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.91
[32m[20221213 20:33:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.66
[32m[20221213 20:33:40 @agent_ppo2.py:143][0m Total time:       4.52 min
[32m[20221213 20:33:40 @agent_ppo2.py:145][0m 655360 total steps have happened
[32m[20221213 20:33:40 @agent_ppo2.py:121][0m #------------------------ Iteration 160 --------------------------#
[32m[20221213 20:33:41 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:33:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:41 @agent_ppo2.py:185][0m |           0.0262 |           0.0484 |           0.0000 |
[32m[20221213 20:33:41 @agent_ppo2.py:185][0m |          -0.0060 |           0.0193 |           0.0000 |
[32m[20221213 20:33:41 @agent_ppo2.py:185][0m |          -0.0233 |           0.0186 |           0.0000 |
[32m[20221213 20:33:41 @agent_ppo2.py:185][0m |          -0.0267 |           0.0185 |           0.0000 |
[32m[20221213 20:33:41 @agent_ppo2.py:185][0m |          -0.0328 |           0.0182 |           0.0000 |
[32m[20221213 20:33:41 @agent_ppo2.py:185][0m |          -0.0354 |           0.0179 |           0.0000 |
[32m[20221213 20:33:41 @agent_ppo2.py:185][0m |          -0.0527 |           0.0177 |           0.0000 |
[32m[20221213 20:33:41 @agent_ppo2.py:185][0m |          -0.0508 |           0.0172 |           0.0000 |
[32m[20221213 20:33:41 @agent_ppo2.py:185][0m |          -0.0536 |           0.0169 |           0.0000 |
[32m[20221213 20:33:41 @agent_ppo2.py:185][0m |          -0.0534 |           0.0166 |           0.0000 |
[32m[20221213 20:33:41 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:33:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.60
[32m[20221213 20:33:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.32
[32m[20221213 20:33:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.80
[32m[20221213 20:33:42 @agent_ppo2.py:143][0m Total time:       4.54 min
[32m[20221213 20:33:42 @agent_ppo2.py:145][0m 659456 total steps have happened
[32m[20221213 20:33:42 @agent_ppo2.py:121][0m #------------------------ Iteration 161 --------------------------#
[32m[20221213 20:33:42 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:42 @agent_ppo2.py:185][0m |           0.0246 |           0.0447 |           0.0000 |
[32m[20221213 20:33:42 @agent_ppo2.py:185][0m |          -0.0040 |           0.0366 |           0.0000 |
[32m[20221213 20:33:43 @agent_ppo2.py:185][0m |          -0.0215 |           0.0297 |           0.0000 |
[32m[20221213 20:33:43 @agent_ppo2.py:185][0m |          -0.0285 |           0.0264 |           0.0000 |
[32m[20221213 20:33:43 @agent_ppo2.py:185][0m |          -0.0315 |           0.0248 |           0.0000 |
[32m[20221213 20:33:43 @agent_ppo2.py:185][0m |          -0.0318 |           0.0238 |           0.0000 |
[32m[20221213 20:33:43 @agent_ppo2.py:185][0m |          -0.0377 |           0.0229 |           0.0000 |
[32m[20221213 20:33:43 @agent_ppo2.py:185][0m |          -0.0358 |           0.0222 |           0.0000 |
[32m[20221213 20:33:43 @agent_ppo2.py:185][0m |          -0.0356 |           0.0214 |           0.0000 |
[32m[20221213 20:33:43 @agent_ppo2.py:185][0m |          -0.0566 |           0.0211 |           0.0000 |
[32m[20221213 20:33:43 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:33:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.54
[32m[20221213 20:33:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.30
[32m[20221213 20:33:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.43
[32m[20221213 20:33:43 @agent_ppo2.py:143][0m Total time:       4.57 min
[32m[20221213 20:33:43 @agent_ppo2.py:145][0m 663552 total steps have happened
[32m[20221213 20:33:43 @agent_ppo2.py:121][0m #------------------------ Iteration 162 --------------------------#
[32m[20221213 20:33:44 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:44 @agent_ppo2.py:185][0m |           0.0220 |           0.0217 |           0.0000 |
[32m[20221213 20:33:44 @agent_ppo2.py:185][0m |          -0.0070 |           0.0168 |           0.0000 |
[32m[20221213 20:33:44 @agent_ppo2.py:185][0m |          -0.0244 |           0.0163 |           0.0000 |
[32m[20221213 20:33:44 @agent_ppo2.py:185][0m |          -0.0304 |           0.0163 |           0.0000 |
[32m[20221213 20:33:44 @agent_ppo2.py:185][0m |          -0.0464 |           0.0159 |           0.0000 |
[32m[20221213 20:33:44 @agent_ppo2.py:185][0m |          -0.0509 |           0.0154 |           0.0000 |
[32m[20221213 20:33:44 @agent_ppo2.py:185][0m |          -0.0586 |           0.0153 |           0.0000 |
[32m[20221213 20:33:45 @agent_ppo2.py:185][0m |          -0.0513 |           0.0150 |           0.0000 |
[32m[20221213 20:33:45 @agent_ppo2.py:185][0m |          -0.0690 |           0.0150 |           0.0000 |
[32m[20221213 20:33:45 @agent_ppo2.py:185][0m |          -0.0671 |           0.0149 |           0.0000 |
[32m[20221213 20:33:45 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:33:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.29
[32m[20221213 20:33:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.68
[32m[20221213 20:33:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 12.07
[32m[20221213 20:33:45 @agent_ppo2.py:143][0m Total time:       4.60 min
[32m[20221213 20:33:45 @agent_ppo2.py:145][0m 667648 total steps have happened
[32m[20221213 20:33:45 @agent_ppo2.py:121][0m #------------------------ Iteration 163 --------------------------#
[32m[20221213 20:33:45 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:46 @agent_ppo2.py:185][0m |           0.0262 |           0.0143 |           0.0000 |
[32m[20221213 20:33:46 @agent_ppo2.py:185][0m |          -0.0208 |           0.0135 |           0.0000 |
[32m[20221213 20:33:46 @agent_ppo2.py:185][0m |          -0.0211 |           0.0134 |           0.0000 |
[32m[20221213 20:33:46 @agent_ppo2.py:185][0m |          -0.0447 |           0.0133 |           0.0000 |
[32m[20221213 20:33:46 @agent_ppo2.py:185][0m |          -0.0535 |           0.0130 |           0.0000 |
[32m[20221213 20:33:46 @agent_ppo2.py:185][0m |          -0.0615 |           0.0129 |           0.0000 |
[32m[20221213 20:33:46 @agent_ppo2.py:185][0m |          -0.0728 |           0.0128 |           0.0000 |
[32m[20221213 20:33:46 @agent_ppo2.py:185][0m |          -0.0712 |           0.0127 |           0.0000 |
[32m[20221213 20:33:46 @agent_ppo2.py:185][0m |          -0.0659 |           0.0126 |           0.0000 |
[32m[20221213 20:33:46 @agent_ppo2.py:185][0m |          -0.0720 |           0.0126 |           0.0000 |
[32m[20221213 20:33:46 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:33:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.36
[32m[20221213 20:33:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.89
[32m[20221213 20:33:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.75
[32m[20221213 20:33:47 @agent_ppo2.py:143][0m Total time:       4.63 min
[32m[20221213 20:33:47 @agent_ppo2.py:145][0m 671744 total steps have happened
[32m[20221213 20:33:47 @agent_ppo2.py:121][0m #------------------------ Iteration 164 --------------------------#
[32m[20221213 20:33:47 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:47 @agent_ppo2.py:185][0m |           0.0261 |           0.0166 |           0.0000 |
[32m[20221213 20:33:47 @agent_ppo2.py:185][0m |           0.0138 |           0.0154 |           0.0000 |
[32m[20221213 20:33:48 @agent_ppo2.py:185][0m |          -0.0315 |           0.0151 |           0.0000 |
[32m[20221213 20:33:48 @agent_ppo2.py:185][0m |          -0.0295 |           0.0145 |           0.0000 |
[32m[20221213 20:33:48 @agent_ppo2.py:185][0m |          -0.0391 |           0.0138 |           0.0000 |
[32m[20221213 20:33:48 @agent_ppo2.py:185][0m |          -0.0250 |           0.0145 |           0.0000 |
[32m[20221213 20:33:48 @agent_ppo2.py:185][0m |          -0.0498 |           0.0143 |           0.0000 |
[32m[20221213 20:33:48 @agent_ppo2.py:185][0m |          -0.0551 |           0.0132 |           0.0000 |
[32m[20221213 20:33:48 @agent_ppo2.py:185][0m |          -0.0528 |           0.0130 |           0.0000 |
[32m[20221213 20:33:48 @agent_ppo2.py:185][0m |          -0.0628 |           0.0128 |           0.0000 |
[32m[20221213 20:33:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:33:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.99
[32m[20221213 20:33:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.70
[32m[20221213 20:33:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.37
[32m[20221213 20:33:48 @agent_ppo2.py:143][0m Total time:       4.66 min
[32m[20221213 20:33:48 @agent_ppo2.py:145][0m 675840 total steps have happened
[32m[20221213 20:33:48 @agent_ppo2.py:121][0m #------------------------ Iteration 165 --------------------------#
[32m[20221213 20:33:49 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:49 @agent_ppo2.py:185][0m |           0.0128 |           0.0193 |           0.0000 |
[32m[20221213 20:33:49 @agent_ppo2.py:185][0m |           0.0459 |           0.0092 |           0.0000 |
[32m[20221213 20:33:49 @agent_ppo2.py:185][0m |           0.0012 |           0.0080 |           0.0000 |
[32m[20221213 20:33:49 @agent_ppo2.py:185][0m |          -0.0069 |           0.0076 |           0.0000 |
[32m[20221213 20:33:49 @agent_ppo2.py:185][0m |          -0.0177 |           0.0073 |           0.0000 |
[32m[20221213 20:33:50 @agent_ppo2.py:185][0m |          -0.0242 |           0.0072 |           0.0000 |
[32m[20221213 20:33:50 @agent_ppo2.py:185][0m |          -0.0256 |           0.0070 |           0.0000 |
[32m[20221213 20:33:50 @agent_ppo2.py:185][0m |          -0.0254 |           0.0070 |           0.0000 |
[32m[20221213 20:33:50 @agent_ppo2.py:185][0m |          -0.0416 |           0.0069 |           0.0000 |
[32m[20221213 20:33:50 @agent_ppo2.py:185][0m |          -0.0305 |           0.0068 |           0.0000 |
[32m[20221213 20:33:50 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:33:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.03
[32m[20221213 20:33:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.04
[32m[20221213 20:33:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.61
[32m[20221213 20:33:50 @agent_ppo2.py:143][0m Total time:       4.69 min
[32m[20221213 20:33:50 @agent_ppo2.py:145][0m 679936 total steps have happened
[32m[20221213 20:33:50 @agent_ppo2.py:121][0m #------------------------ Iteration 166 --------------------------#
[32m[20221213 20:33:51 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:51 @agent_ppo2.py:185][0m |           0.0225 |           0.0512 |           0.0000 |
[32m[20221213 20:33:51 @agent_ppo2.py:185][0m |          -0.0038 |           0.0236 |           0.0000 |
[32m[20221213 20:33:51 @agent_ppo2.py:185][0m |          -0.0140 |           0.0179 |           0.0000 |
[32m[20221213 20:33:51 @agent_ppo2.py:185][0m |          -0.0172 |           0.0167 |           0.0000 |
[32m[20221213 20:33:51 @agent_ppo2.py:185][0m |          -0.0158 |           0.0163 |           0.0000 |
[32m[20221213 20:33:51 @agent_ppo2.py:185][0m |          -0.0668 |           0.0184 |           0.0000 |
[32m[20221213 20:33:51 @agent_ppo2.py:185][0m |          -0.0223 |           0.0186 |           0.0000 |
[32m[20221213 20:33:51 @agent_ppo2.py:185][0m |          -0.0213 |           0.0156 |           0.0000 |
[32m[20221213 20:33:52 @agent_ppo2.py:185][0m |          -0.0238 |           0.0151 |           0.0000 |
[32m[20221213 20:33:52 @agent_ppo2.py:185][0m |          -0.0260 |           0.0147 |           0.0000 |
[32m[20221213 20:33:52 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:33:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.40
[32m[20221213 20:33:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.57
[32m[20221213 20:33:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.11
[32m[20221213 20:33:52 @agent_ppo2.py:143][0m Total time:       4.72 min
[32m[20221213 20:33:52 @agent_ppo2.py:145][0m 684032 total steps have happened
[32m[20221213 20:33:52 @agent_ppo2.py:121][0m #------------------------ Iteration 167 --------------------------#
[32m[20221213 20:33:52 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:53 @agent_ppo2.py:185][0m |           0.0166 |           0.0334 |           0.0000 |
[32m[20221213 20:33:53 @agent_ppo2.py:185][0m |          -0.0025 |           0.0243 |           0.0000 |
[32m[20221213 20:33:53 @agent_ppo2.py:185][0m |          -0.0268 |           0.0210 |           0.0000 |
[32m[20221213 20:33:53 @agent_ppo2.py:185][0m |          -0.0232 |           0.0197 |           0.0000 |
[32m[20221213 20:33:53 @agent_ppo2.py:185][0m |          -0.0327 |           0.0194 |           0.0000 |
[32m[20221213 20:33:53 @agent_ppo2.py:185][0m |          -0.0364 |           0.0185 |           0.0000 |
[32m[20221213 20:33:53 @agent_ppo2.py:185][0m |          -0.0396 |           0.0184 |           0.0000 |
[32m[20221213 20:33:53 @agent_ppo2.py:185][0m |          -0.0420 |           0.0178 |           0.0000 |
[32m[20221213 20:33:53 @agent_ppo2.py:185][0m |          -0.0416 |           0.0180 |           0.0000 |
[32m[20221213 20:33:53 @agent_ppo2.py:185][0m |          -0.0452 |           0.0176 |           0.0000 |
[32m[20221213 20:33:53 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:33:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.51
[32m[20221213 20:33:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.39
[32m[20221213 20:33:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.94
[32m[20221213 20:33:54 @agent_ppo2.py:143][0m Total time:       4.74 min
[32m[20221213 20:33:54 @agent_ppo2.py:145][0m 688128 total steps have happened
[32m[20221213 20:33:54 @agent_ppo2.py:121][0m #------------------------ Iteration 168 --------------------------#
[32m[20221213 20:33:54 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:33:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:54 @agent_ppo2.py:185][0m |           0.0278 |           0.0197 |           0.0000 |
[32m[20221213 20:33:54 @agent_ppo2.py:185][0m |          -0.0180 |           0.0107 |           0.0000 |
[32m[20221213 20:33:54 @agent_ppo2.py:185][0m |          -0.0240 |           0.0102 |           0.0000 |
[32m[20221213 20:33:55 @agent_ppo2.py:185][0m |          -0.0292 |           0.0100 |           0.0000 |
[32m[20221213 20:33:55 @agent_ppo2.py:185][0m |          -0.0229 |           0.0101 |           0.0000 |
[32m[20221213 20:33:55 @agent_ppo2.py:185][0m |          -0.0427 |           0.0098 |           0.0000 |
[32m[20221213 20:33:55 @agent_ppo2.py:185][0m |          -0.0448 |           0.0097 |           0.0000 |
[32m[20221213 20:33:55 @agent_ppo2.py:185][0m |          -0.0229 |           0.0097 |           0.0000 |
[32m[20221213 20:33:55 @agent_ppo2.py:185][0m |          -0.0529 |           0.0095 |           0.0000 |
[32m[20221213 20:33:55 @agent_ppo2.py:185][0m |          -0.0556 |           0.0094 |           0.0000 |
[32m[20221213 20:33:55 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:33:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.38
[32m[20221213 20:33:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.68
[32m[20221213 20:33:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.83
[32m[20221213 20:33:55 @agent_ppo2.py:143][0m Total time:       4.77 min
[32m[20221213 20:33:55 @agent_ppo2.py:145][0m 692224 total steps have happened
[32m[20221213 20:33:55 @agent_ppo2.py:121][0m #------------------------ Iteration 169 --------------------------#
[32m[20221213 20:33:56 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:33:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:56 @agent_ppo2.py:185][0m |           0.0185 |           0.0130 |           0.0000 |
[32m[20221213 20:33:56 @agent_ppo2.py:185][0m |          -0.0013 |           0.0125 |           0.0000 |
[32m[20221213 20:33:56 @agent_ppo2.py:185][0m |          -0.0242 |           0.0122 |           0.0000 |
[32m[20221213 20:33:56 @agent_ppo2.py:185][0m |          -0.0335 |           0.0121 |           0.0000 |
[32m[20221213 20:33:56 @agent_ppo2.py:185][0m |          -0.0343 |           0.0120 |           0.0000 |
[32m[20221213 20:33:56 @agent_ppo2.py:185][0m |          -0.0418 |           0.0118 |           0.0000 |
[32m[20221213 20:33:56 @agent_ppo2.py:185][0m |          -0.0411 |           0.0117 |           0.0000 |
[32m[20221213 20:33:57 @agent_ppo2.py:185][0m |          -0.0497 |           0.0117 |           0.0000 |
[32m[20221213 20:33:57 @agent_ppo2.py:185][0m |          -0.0413 |           0.0118 |           0.0000 |
[32m[20221213 20:33:57 @agent_ppo2.py:185][0m |          -0.0480 |           0.0115 |           0.0000 |
[32m[20221213 20:33:57 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:33:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.66
[32m[20221213 20:33:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.27
[32m[20221213 20:33:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.52
[32m[20221213 20:33:57 @agent_ppo2.py:143][0m Total time:       4.80 min
[32m[20221213 20:33:57 @agent_ppo2.py:145][0m 696320 total steps have happened
[32m[20221213 20:33:57 @agent_ppo2.py:121][0m #------------------------ Iteration 170 --------------------------#
[32m[20221213 20:33:57 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:33:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:58 @agent_ppo2.py:185][0m |           0.0370 |           0.0110 |           0.0000 |
[32m[20221213 20:33:58 @agent_ppo2.py:185][0m |           0.0121 |           0.0095 |           0.0000 |
[32m[20221213 20:33:58 @agent_ppo2.py:185][0m |          -0.0095 |           0.0109 |           0.0000 |
[32m[20221213 20:33:58 @agent_ppo2.py:185][0m |          -0.0153 |           0.0100 |           0.0000 |
[32m[20221213 20:33:58 @agent_ppo2.py:185][0m |          -0.0254 |           0.0089 |           0.0000 |
[32m[20221213 20:33:58 @agent_ppo2.py:185][0m |          -0.0386 |           0.0089 |           0.0000 |
[32m[20221213 20:33:58 @agent_ppo2.py:185][0m |          -0.0520 |           0.0088 |           0.0000 |
[32m[20221213 20:33:58 @agent_ppo2.py:185][0m |          -0.0446 |           0.0087 |           0.0000 |
[32m[20221213 20:33:58 @agent_ppo2.py:185][0m |          -0.0457 |           0.0087 |           0.0000 |
[32m[20221213 20:33:58 @agent_ppo2.py:185][0m |          -0.0500 |           0.0086 |           0.0000 |
[32m[20221213 20:33:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:33:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.94
[32m[20221213 20:33:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.57
[32m[20221213 20:33:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.72
[32m[20221213 20:33:59 @agent_ppo2.py:143][0m Total time:       4.83 min
[32m[20221213 20:33:59 @agent_ppo2.py:145][0m 700416 total steps have happened
[32m[20221213 20:33:59 @agent_ppo2.py:121][0m #------------------------ Iteration 171 --------------------------#
[32m[20221213 20:33:59 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:33:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:33:59 @agent_ppo2.py:185][0m |           0.0155 |           0.0086 |           0.0000 |
[32m[20221213 20:33:59 @agent_ppo2.py:185][0m |           0.0025 |           0.0067 |           0.0000 |
[32m[20221213 20:33:59 @agent_ppo2.py:185][0m |          -0.0184 |           0.0063 |           0.0000 |
[32m[20221213 20:34:00 @agent_ppo2.py:185][0m |          -0.0350 |           0.0063 |           0.0000 |
[32m[20221213 20:34:00 @agent_ppo2.py:185][0m |          -0.0281 |           0.0061 |           0.0000 |
[32m[20221213 20:34:00 @agent_ppo2.py:185][0m |          -0.0353 |           0.0060 |           0.0000 |
[32m[20221213 20:34:00 @agent_ppo2.py:185][0m |          -0.0383 |           0.0060 |           0.0000 |
[32m[20221213 20:34:00 @agent_ppo2.py:185][0m |          -0.0346 |           0.0059 |           0.0000 |
[32m[20221213 20:34:00 @agent_ppo2.py:185][0m |          -0.0440 |           0.0058 |           0.0000 |
[32m[20221213 20:34:00 @agent_ppo2.py:185][0m |          -0.0448 |           0.0058 |           0.0000 |
[32m[20221213 20:34:00 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:34:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.78
[32m[20221213 20:34:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.17
[32m[20221213 20:34:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.78
[32m[20221213 20:34:00 @agent_ppo2.py:143][0m Total time:       4.85 min
[32m[20221213 20:34:00 @agent_ppo2.py:145][0m 704512 total steps have happened
[32m[20221213 20:34:00 @agent_ppo2.py:121][0m #------------------------ Iteration 172 --------------------------#
[32m[20221213 20:34:01 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:34:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:01 @agent_ppo2.py:185][0m |           0.0188 |           0.0138 |           0.0000 |
[32m[20221213 20:34:01 @agent_ppo2.py:185][0m |           0.0021 |           0.0094 |           0.0000 |
[32m[20221213 20:34:01 @agent_ppo2.py:185][0m |          -0.0095 |           0.0087 |           0.0000 |
[32m[20221213 20:34:01 @agent_ppo2.py:185][0m |          -0.0129 |           0.0086 |           0.0000 |
[32m[20221213 20:34:01 @agent_ppo2.py:185][0m |          -0.0200 |           0.0084 |           0.0000 |
[32m[20221213 20:34:01 @agent_ppo2.py:185][0m |          -0.0221 |           0.0083 |           0.0000 |
[32m[20221213 20:34:02 @agent_ppo2.py:185][0m |          -0.0235 |           0.0082 |           0.0000 |
[32m[20221213 20:34:02 @agent_ppo2.py:185][0m |          -0.0240 |           0.0081 |           0.0000 |
[32m[20221213 20:34:02 @agent_ppo2.py:185][0m |          -0.0605 |           0.0081 |           0.0000 |
[32m[20221213 20:34:02 @agent_ppo2.py:185][0m |          -0.0257 |           0.0081 |           0.0000 |
[32m[20221213 20:34:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:34:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.02
[32m[20221213 20:34:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.66
[32m[20221213 20:34:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.59
[32m[20221213 20:34:02 @agent_ppo2.py:143][0m Total time:       4.88 min
[32m[20221213 20:34:02 @agent_ppo2.py:145][0m 708608 total steps have happened
[32m[20221213 20:34:02 @agent_ppo2.py:121][0m #------------------------ Iteration 173 --------------------------#
[32m[20221213 20:34:03 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:34:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:03 @agent_ppo2.py:185][0m |           0.0226 |           0.0202 |           0.0000 |
[32m[20221213 20:34:03 @agent_ppo2.py:185][0m |           0.0102 |           0.0142 |           0.0000 |
[32m[20221213 20:34:03 @agent_ppo2.py:185][0m |          -0.0110 |           0.0128 |           0.0000 |
[32m[20221213 20:34:03 @agent_ppo2.py:185][0m |          -0.0178 |           0.0120 |           0.0000 |
[32m[20221213 20:34:03 @agent_ppo2.py:185][0m |          -0.0205 |           0.0117 |           0.0000 |
[32m[20221213 20:34:03 @agent_ppo2.py:185][0m |          -0.0215 |           0.0117 |           0.0000 |
[32m[20221213 20:34:03 @agent_ppo2.py:185][0m |          -0.0248 |           0.0113 |           0.0000 |
[32m[20221213 20:34:03 @agent_ppo2.py:185][0m |          -0.0261 |           0.0113 |           0.0000 |
[32m[20221213 20:34:03 @agent_ppo2.py:185][0m |          -0.0287 |           0.0113 |           0.0000 |
[32m[20221213 20:34:03 @agent_ppo2.py:185][0m |          -0.0305 |           0.0108 |           0.0000 |
[32m[20221213 20:34:03 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:34:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.46
[32m[20221213 20:34:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.30
[32m[20221213 20:34:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.52
[32m[20221213 20:34:04 @agent_ppo2.py:143][0m Total time:       4.91 min
[32m[20221213 20:34:04 @agent_ppo2.py:145][0m 712704 total steps have happened
[32m[20221213 20:34:04 @agent_ppo2.py:121][0m #------------------------ Iteration 174 --------------------------#
[32m[20221213 20:34:04 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:34:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:04 @agent_ppo2.py:185][0m |           0.0520 |           0.0122 |           0.0000 |
[32m[20221213 20:34:05 @agent_ppo2.py:185][0m |           0.0141 |           0.0070 |           0.0000 |
[32m[20221213 20:34:05 @agent_ppo2.py:185][0m |          -0.0064 |           0.0067 |           0.0000 |
[32m[20221213 20:34:05 @agent_ppo2.py:185][0m |          -0.0118 |           0.0066 |           0.0000 |
[32m[20221213 20:34:05 @agent_ppo2.py:185][0m |          -0.0244 |           0.0065 |           0.0000 |
[32m[20221213 20:34:05 @agent_ppo2.py:185][0m |          -0.0260 |           0.0064 |           0.0000 |
[32m[20221213 20:34:05 @agent_ppo2.py:185][0m |          -0.0342 |           0.0064 |           0.0000 |
[32m[20221213 20:34:05 @agent_ppo2.py:185][0m |          -0.0342 |           0.0063 |           0.0000 |
[32m[20221213 20:34:05 @agent_ppo2.py:185][0m |          -0.0400 |           0.0062 |           0.0000 |
[32m[20221213 20:34:05 @agent_ppo2.py:185][0m |          -0.0335 |           0.0062 |           0.0000 |
[32m[20221213 20:34:05 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:34:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.15
[32m[20221213 20:34:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.44
[32m[20221213 20:34:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.82
[32m[20221213 20:34:06 @agent_ppo2.py:143][0m Total time:       4.94 min
[32m[20221213 20:34:06 @agent_ppo2.py:145][0m 716800 total steps have happened
[32m[20221213 20:34:06 @agent_ppo2.py:121][0m #------------------------ Iteration 175 --------------------------#
[32m[20221213 20:34:06 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:06 @agent_ppo2.py:185][0m |           0.0236 |           0.0098 |           0.0000 |
[32m[20221213 20:34:06 @agent_ppo2.py:185][0m |          -0.0073 |           0.0085 |           0.0000 |
[32m[20221213 20:34:06 @agent_ppo2.py:185][0m |          -0.0153 |           0.0082 |           0.0000 |
[32m[20221213 20:34:06 @agent_ppo2.py:185][0m |          -0.0214 |           0.0081 |           0.0000 |
[32m[20221213 20:34:07 @agent_ppo2.py:185][0m |          -0.0229 |           0.0080 |           0.0000 |
[32m[20221213 20:34:07 @agent_ppo2.py:185][0m |          -0.0260 |           0.0079 |           0.0000 |
[32m[20221213 20:34:07 @agent_ppo2.py:185][0m |          -0.0291 |           0.0079 |           0.0000 |
[32m[20221213 20:34:07 @agent_ppo2.py:185][0m |          -0.0319 |           0.0079 |           0.0000 |
[32m[20221213 20:34:07 @agent_ppo2.py:185][0m |          -0.0356 |           0.0078 |           0.0000 |
[32m[20221213 20:34:07 @agent_ppo2.py:185][0m |          -0.0298 |           0.0078 |           0.0000 |
[32m[20221213 20:34:07 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:34:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.58
[32m[20221213 20:34:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.37
[32m[20221213 20:34:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.16
[32m[20221213 20:34:07 @agent_ppo2.py:143][0m Total time:       4.97 min
[32m[20221213 20:34:07 @agent_ppo2.py:145][0m 720896 total steps have happened
[32m[20221213 20:34:07 @agent_ppo2.py:121][0m #------------------------ Iteration 176 --------------------------#
[32m[20221213 20:34:08 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:08 @agent_ppo2.py:185][0m |           0.0268 |           0.0154 |           0.0000 |
[32m[20221213 20:34:08 @agent_ppo2.py:185][0m |          -0.0542 |           0.0166 |           0.0000 |
[32m[20221213 20:34:08 @agent_ppo2.py:185][0m |          -0.0066 |           0.0157 |           0.0000 |
[32m[20221213 20:34:08 @agent_ppo2.py:185][0m |          -0.0142 |           0.0122 |           0.0000 |
[32m[20221213 20:34:08 @agent_ppo2.py:185][0m |          -0.0214 |           0.0121 |           0.0000 |
[32m[20221213 20:34:08 @agent_ppo2.py:185][0m |          -0.0231 |           0.0117 |           0.0000 |
[32m[20221213 20:34:09 @agent_ppo2.py:185][0m |          -0.0266 |           0.0116 |           0.0000 |
[32m[20221213 20:34:09 @agent_ppo2.py:185][0m |          -0.0267 |           0.0111 |           0.0000 |
[32m[20221213 20:34:09 @agent_ppo2.py:185][0m |          -0.0281 |           0.0111 |           0.0000 |
[32m[20221213 20:34:09 @agent_ppo2.py:185][0m |          -0.0421 |           0.0110 |           0.0000 |
[32m[20221213 20:34:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:34:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.05
[32m[20221213 20:34:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.28
[32m[20221213 20:34:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.56
[32m[20221213 20:34:09 @agent_ppo2.py:143][0m Total time:       5.00 min
[32m[20221213 20:34:09 @agent_ppo2.py:145][0m 724992 total steps have happened
[32m[20221213 20:34:09 @agent_ppo2.py:121][0m #------------------------ Iteration 177 --------------------------#
[32m[20221213 20:34:10 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:10 @agent_ppo2.py:185][0m |           0.0256 |           0.0115 |           0.0000 |
[32m[20221213 20:34:10 @agent_ppo2.py:185][0m |          -0.0046 |           0.0063 |           0.0000 |
[32m[20221213 20:34:10 @agent_ppo2.py:185][0m |           0.0049 |           0.0061 |           0.0000 |
[32m[20221213 20:34:10 @agent_ppo2.py:185][0m |          -0.0239 |           0.0059 |           0.0000 |
[32m[20221213 20:34:10 @agent_ppo2.py:185][0m |          -0.0140 |           0.0060 |           0.0000 |
[32m[20221213 20:34:10 @agent_ppo2.py:185][0m |          -0.0348 |           0.0058 |           0.0000 |
[32m[20221213 20:34:10 @agent_ppo2.py:185][0m |          -0.0376 |           0.0057 |           0.0000 |
[32m[20221213 20:34:10 @agent_ppo2.py:185][0m |          -0.0399 |           0.0057 |           0.0000 |
[32m[20221213 20:34:10 @agent_ppo2.py:185][0m |          -0.0401 |           0.0056 |           0.0000 |
[32m[20221213 20:34:11 @agent_ppo2.py:185][0m |          -0.0444 |           0.0056 |           0.0000 |
[32m[20221213 20:34:11 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:34:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.01
[32m[20221213 20:34:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.30
[32m[20221213 20:34:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.64
[32m[20221213 20:34:11 @agent_ppo2.py:143][0m Total time:       5.03 min
[32m[20221213 20:34:11 @agent_ppo2.py:145][0m 729088 total steps have happened
[32m[20221213 20:34:11 @agent_ppo2.py:121][0m #------------------------ Iteration 178 --------------------------#
[32m[20221213 20:34:11 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:34:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:11 @agent_ppo2.py:185][0m |           0.0209 |           0.1369 |           0.0000 |
[32m[20221213 20:34:12 @agent_ppo2.py:185][0m |           0.0035 |           0.0619 |           0.0000 |
[32m[20221213 20:34:12 @agent_ppo2.py:185][0m |          -0.0074 |           0.0386 |           0.0000 |
[32m[20221213 20:34:12 @agent_ppo2.py:185][0m |          -0.0167 |           0.0280 |           0.0000 |
[32m[20221213 20:34:12 @agent_ppo2.py:185][0m |          -0.0195 |           0.0258 |           0.0000 |
[32m[20221213 20:34:12 @agent_ppo2.py:185][0m |          -0.0231 |           0.0244 |           0.0000 |
[32m[20221213 20:34:12 @agent_ppo2.py:185][0m |          -0.0252 |           0.0228 |           0.0000 |
[32m[20221213 20:34:12 @agent_ppo2.py:185][0m |          -0.0269 |           0.0241 |           0.0000 |
[32m[20221213 20:34:12 @agent_ppo2.py:185][0m |          -0.0280 |           0.0214 |           0.0000 |
[32m[20221213 20:34:12 @agent_ppo2.py:185][0m |          -0.0287 |           0.0207 |           0.0000 |
[32m[20221213 20:34:12 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:34:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.48
[32m[20221213 20:34:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.53
[32m[20221213 20:34:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.08
[32m[20221213 20:34:13 @agent_ppo2.py:143][0m Total time:       5.06 min
[32m[20221213 20:34:13 @agent_ppo2.py:145][0m 733184 total steps have happened
[32m[20221213 20:34:13 @agent_ppo2.py:121][0m #------------------------ Iteration 179 --------------------------#
[32m[20221213 20:34:13 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:34:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:13 @agent_ppo2.py:185][0m |           0.0292 |           0.0462 |           0.0000 |
[32m[20221213 20:34:13 @agent_ppo2.py:185][0m |          -0.0054 |           0.0364 |           0.0000 |
[32m[20221213 20:34:13 @agent_ppo2.py:185][0m |          -0.0253 |           0.0330 |           0.0000 |
[32m[20221213 20:34:13 @agent_ppo2.py:185][0m |          -0.0322 |           0.0320 |           0.0000 |
[32m[20221213 20:34:14 @agent_ppo2.py:185][0m |          -0.0283 |           0.0287 |           0.0000 |
[32m[20221213 20:34:14 @agent_ppo2.py:185][0m |          -0.0373 |           0.0280 |           0.0000 |
[32m[20221213 20:34:14 @agent_ppo2.py:185][0m |          -0.0478 |           0.0275 |           0.0000 |
[32m[20221213 20:34:14 @agent_ppo2.py:185][0m |          -0.0468 |           0.0263 |           0.0000 |
[32m[20221213 20:34:14 @agent_ppo2.py:185][0m |          -0.0414 |           0.0258 |           0.0000 |
[32m[20221213 20:34:14 @agent_ppo2.py:185][0m |          -0.0467 |           0.0258 |           0.0000 |
[32m[20221213 20:34:14 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:34:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.99
[32m[20221213 20:34:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.95
[32m[20221213 20:34:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.06
[32m[20221213 20:34:14 @agent_ppo2.py:143][0m Total time:       5.09 min
[32m[20221213 20:34:14 @agent_ppo2.py:145][0m 737280 total steps have happened
[32m[20221213 20:34:14 @agent_ppo2.py:121][0m #------------------------ Iteration 180 --------------------------#
[32m[20221213 20:34:15 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:34:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:15 @agent_ppo2.py:185][0m |           0.0793 |           0.0348 |           0.0000 |
[32m[20221213 20:34:15 @agent_ppo2.py:185][0m |           0.0740 |           0.0144 |           0.0000 |
[32m[20221213 20:34:15 @agent_ppo2.py:185][0m |           0.0297 |           0.0135 |           0.0000 |
[32m[20221213 20:34:15 @agent_ppo2.py:185][0m |           0.0247 |           0.0134 |           0.0000 |
[32m[20221213 20:34:15 @agent_ppo2.py:185][0m |           0.0031 |           0.0128 |           0.0000 |
[32m[20221213 20:34:15 @agent_ppo2.py:185][0m |          -0.0065 |           0.0126 |           0.0000 |
[32m[20221213 20:34:16 @agent_ppo2.py:185][0m |          -0.0173 |           0.0125 |           0.0000 |
[32m[20221213 20:34:16 @agent_ppo2.py:185][0m |          -0.0214 |           0.0123 |           0.0000 |
[32m[20221213 20:34:16 @agent_ppo2.py:185][0m |          -0.0313 |           0.0123 |           0.0000 |
[32m[20221213 20:34:16 @agent_ppo2.py:185][0m |          -0.0322 |           0.0120 |           0.0000 |
[32m[20221213 20:34:16 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:34:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.13
[32m[20221213 20:34:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.48
[32m[20221213 20:34:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.68
[32m[20221213 20:34:16 @agent_ppo2.py:143][0m Total time:       5.12 min
[32m[20221213 20:34:16 @agent_ppo2.py:145][0m 741376 total steps have happened
[32m[20221213 20:34:16 @agent_ppo2.py:121][0m #------------------------ Iteration 181 --------------------------#
[32m[20221213 20:34:17 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:17 @agent_ppo2.py:185][0m |           0.0456 |           0.0201 |           0.0000 |
[32m[20221213 20:34:17 @agent_ppo2.py:185][0m |           0.0214 |           0.0092 |           0.0000 |
[32m[20221213 20:34:17 @agent_ppo2.py:185][0m |           0.0004 |           0.0083 |           0.0000 |
[32m[20221213 20:34:17 @agent_ppo2.py:185][0m |          -0.0101 |           0.0079 |           0.0000 |
[32m[20221213 20:34:17 @agent_ppo2.py:185][0m |          -0.0154 |           0.0077 |           0.0000 |
[32m[20221213 20:34:17 @agent_ppo2.py:185][0m |          -0.0248 |           0.0076 |           0.0000 |
[32m[20221213 20:34:17 @agent_ppo2.py:185][0m |          -0.0262 |           0.0075 |           0.0000 |
[32m[20221213 20:34:17 @agent_ppo2.py:185][0m |          -0.0309 |           0.0074 |           0.0000 |
[32m[20221213 20:34:18 @agent_ppo2.py:185][0m |          -0.0307 |           0.0073 |           0.0000 |
[32m[20221213 20:34:18 @agent_ppo2.py:185][0m |          -0.0354 |           0.0072 |           0.0000 |
[32m[20221213 20:34:18 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:34:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.21
[32m[20221213 20:34:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.38
[32m[20221213 20:34:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.65
[32m[20221213 20:34:18 @agent_ppo2.py:143][0m Total time:       5.15 min
[32m[20221213 20:34:18 @agent_ppo2.py:145][0m 745472 total steps have happened
[32m[20221213 20:34:18 @agent_ppo2.py:121][0m #------------------------ Iteration 182 --------------------------#
[32m[20221213 20:34:18 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:34:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:19 @agent_ppo2.py:185][0m |           0.0174 |           0.0123 |           0.0000 |
[32m[20221213 20:34:19 @agent_ppo2.py:185][0m |          -0.0078 |           0.0090 |           0.0000 |
[32m[20221213 20:34:19 @agent_ppo2.py:185][0m |          -0.0164 |           0.0082 |           0.0000 |
[32m[20221213 20:34:19 @agent_ppo2.py:185][0m |          -0.0236 |           0.0080 |           0.0000 |
[32m[20221213 20:34:19 @agent_ppo2.py:185][0m |          -0.0261 |           0.0079 |           0.0000 |
[32m[20221213 20:34:19 @agent_ppo2.py:185][0m |          -0.0285 |           0.0078 |           0.0000 |
[32m[20221213 20:34:19 @agent_ppo2.py:185][0m |          -0.0297 |           0.0077 |           0.0000 |
[32m[20221213 20:34:19 @agent_ppo2.py:185][0m |          -0.0302 |           0.0077 |           0.0000 |
[32m[20221213 20:34:19 @agent_ppo2.py:185][0m |          -0.0285 |           0.0076 |           0.0000 |
[32m[20221213 20:34:19 @agent_ppo2.py:185][0m |          -0.0302 |           0.0076 |           0.0000 |
[32m[20221213 20:34:19 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:34:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.50
[32m[20221213 20:34:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.89
[32m[20221213 20:34:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.43
[32m[20221213 20:34:20 @agent_ppo2.py:143][0m Total time:       5.18 min
[32m[20221213 20:34:20 @agent_ppo2.py:145][0m 749568 total steps have happened
[32m[20221213 20:34:20 @agent_ppo2.py:121][0m #------------------------ Iteration 183 --------------------------#
[32m[20221213 20:34:20 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:20 @agent_ppo2.py:185][0m |           0.0200 |           0.0126 |           0.0000 |
[32m[20221213 20:34:21 @agent_ppo2.py:185][0m |          -0.0065 |           0.0101 |           0.0000 |
[32m[20221213 20:34:21 @agent_ppo2.py:185][0m |          -0.0176 |           0.0098 |           0.0000 |
[32m[20221213 20:34:21 @agent_ppo2.py:185][0m |          -0.0214 |           0.0097 |           0.0000 |
[32m[20221213 20:34:21 @agent_ppo2.py:185][0m |          -0.0241 |           0.0096 |           0.0000 |
[32m[20221213 20:34:21 @agent_ppo2.py:185][0m |          -0.0269 |           0.0095 |           0.0000 |
[32m[20221213 20:34:21 @agent_ppo2.py:185][0m |          -0.0307 |           0.0095 |           0.0000 |
[32m[20221213 20:34:21 @agent_ppo2.py:185][0m |          -0.0326 |           0.0094 |           0.0000 |
[32m[20221213 20:34:21 @agent_ppo2.py:185][0m |          -0.0297 |           0.0094 |           0.0000 |
[32m[20221213 20:34:21 @agent_ppo2.py:185][0m |          -0.0308 |           0.0094 |           0.0000 |
[32m[20221213 20:34:21 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:34:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.71
[32m[20221213 20:34:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.29
[32m[20221213 20:34:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.35
[32m[20221213 20:34:22 @agent_ppo2.py:143][0m Total time:       5.21 min
[32m[20221213 20:34:22 @agent_ppo2.py:145][0m 753664 total steps have happened
[32m[20221213 20:34:22 @agent_ppo2.py:121][0m #------------------------ Iteration 184 --------------------------#
[32m[20221213 20:34:22 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:22 @agent_ppo2.py:185][0m |           0.0178 |           0.1059 |           0.0000 |
[32m[20221213 20:34:22 @agent_ppo2.py:185][0m |          -0.0128 |           0.0493 |           0.0000 |
[32m[20221213 20:34:22 @agent_ppo2.py:185][0m |          -0.0150 |           0.0386 |           0.0000 |
[32m[20221213 20:34:22 @agent_ppo2.py:185][0m |          -0.0219 |           0.0322 |           0.0000 |
[32m[20221213 20:34:23 @agent_ppo2.py:185][0m |          -0.0219 |           0.0321 |           0.0000 |
[32m[20221213 20:34:23 @agent_ppo2.py:185][0m |          -0.0229 |           0.0324 |           0.0000 |
[32m[20221213 20:34:23 @agent_ppo2.py:185][0m |          -0.0240 |           0.0279 |           0.0000 |
[32m[20221213 20:34:23 @agent_ppo2.py:185][0m |          -0.0281 |           0.0271 |           0.0000 |
[32m[20221213 20:34:23 @agent_ppo2.py:185][0m |          -0.0293 |           0.0256 |           0.0000 |
[32m[20221213 20:34:23 @agent_ppo2.py:185][0m |          -0.0304 |           0.0253 |           0.0000 |
[32m[20221213 20:34:23 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:34:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.99
[32m[20221213 20:34:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.58
[32m[20221213 20:34:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.46
[32m[20221213 20:34:23 @agent_ppo2.py:143][0m Total time:       5.24 min
[32m[20221213 20:34:23 @agent_ppo2.py:145][0m 757760 total steps have happened
[32m[20221213 20:34:23 @agent_ppo2.py:121][0m #------------------------ Iteration 185 --------------------------#
[32m[20221213 20:34:24 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:24 @agent_ppo2.py:185][0m |           0.0305 |           0.0367 |           0.0000 |
[32m[20221213 20:34:24 @agent_ppo2.py:185][0m |          -0.0023 |           0.0298 |           0.0000 |
[32m[20221213 20:34:24 @agent_ppo2.py:185][0m |          -0.0419 |           0.0276 |           0.0000 |
[32m[20221213 20:34:24 @agent_ppo2.py:185][0m |          -0.0438 |           0.0264 |           0.0000 |
[32m[20221213 20:34:24 @agent_ppo2.py:185][0m |          -0.0562 |           0.0247 |           0.0000 |
[32m[20221213 20:34:24 @agent_ppo2.py:185][0m |          -0.0595 |           0.0250 |           0.0000 |
[32m[20221213 20:34:24 @agent_ppo2.py:185][0m |          -0.0537 |           0.0241 |           0.0000 |
[32m[20221213 20:34:25 @agent_ppo2.py:185][0m |          -0.0684 |           0.0236 |           0.0000 |
[32m[20221213 20:34:25 @agent_ppo2.py:185][0m |          -0.0623 |           0.0225 |           0.0000 |
[32m[20221213 20:34:25 @agent_ppo2.py:185][0m |          -0.0707 |           0.0221 |           0.0000 |
[32m[20221213 20:34:25 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:34:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.64
[32m[20221213 20:34:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.51
[32m[20221213 20:34:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.20
[32m[20221213 20:34:25 @agent_ppo2.py:143][0m Total time:       5.27 min
[32m[20221213 20:34:25 @agent_ppo2.py:145][0m 761856 total steps have happened
[32m[20221213 20:34:25 @agent_ppo2.py:121][0m #------------------------ Iteration 186 --------------------------#
[32m[20221213 20:34:25 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:26 @agent_ppo2.py:185][0m |           0.0202 |           0.0280 |           0.0000 |
[32m[20221213 20:34:26 @agent_ppo2.py:185][0m |           0.0047 |           0.0145 |           0.0000 |
[32m[20221213 20:34:26 @agent_ppo2.py:185][0m |          -0.0108 |           0.0142 |           0.0000 |
[32m[20221213 20:34:26 @agent_ppo2.py:185][0m |          -0.0278 |           0.0138 |           0.0000 |
[32m[20221213 20:34:26 @agent_ppo2.py:185][0m |          -0.0316 |           0.0137 |           0.0000 |
[32m[20221213 20:34:26 @agent_ppo2.py:185][0m |          -0.0285 |           0.0136 |           0.0000 |
[32m[20221213 20:34:26 @agent_ppo2.py:185][0m |          -0.0307 |           0.0134 |           0.0000 |
[32m[20221213 20:34:26 @agent_ppo2.py:185][0m |          -0.0423 |           0.0131 |           0.0000 |
[32m[20221213 20:34:26 @agent_ppo2.py:185][0m |          -0.0466 |           0.0130 |           0.0000 |
[32m[20221213 20:34:26 @agent_ppo2.py:185][0m |          -0.0485 |           0.0129 |           0.0000 |
[32m[20221213 20:34:26 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:34:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.06
[32m[20221213 20:34:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.74
[32m[20221213 20:34:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.84
[32m[20221213 20:34:27 @agent_ppo2.py:143][0m Total time:       5.30 min
[32m[20221213 20:34:27 @agent_ppo2.py:145][0m 765952 total steps have happened
[32m[20221213 20:34:27 @agent_ppo2.py:121][0m #------------------------ Iteration 187 --------------------------#
[32m[20221213 20:34:27 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:34:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:27 @agent_ppo2.py:185][0m |           0.0251 |           0.0164 |           0.0000 |
[32m[20221213 20:34:28 @agent_ppo2.py:185][0m |          -0.0023 |           0.0133 |           0.0000 |
[32m[20221213 20:34:28 @agent_ppo2.py:185][0m |          -0.0224 |           0.0128 |           0.0000 |
[32m[20221213 20:34:28 @agent_ppo2.py:185][0m |          -0.0266 |           0.0126 |           0.0000 |
[32m[20221213 20:34:28 @agent_ppo2.py:185][0m |          -0.0303 |           0.0124 |           0.0000 |
[32m[20221213 20:34:28 @agent_ppo2.py:185][0m |          -0.0340 |           0.0124 |           0.0000 |
[32m[20221213 20:34:28 @agent_ppo2.py:185][0m |          -0.0429 |           0.0124 |           0.0000 |
[32m[20221213 20:34:28 @agent_ppo2.py:185][0m |          -0.0415 |           0.0122 |           0.0000 |
[32m[20221213 20:34:28 @agent_ppo2.py:185][0m |          -0.0418 |           0.0120 |           0.0000 |
[32m[20221213 20:34:28 @agent_ppo2.py:185][0m |          -0.0497 |           0.0120 |           0.0000 |
[32m[20221213 20:34:28 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:34:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.43
[32m[20221213 20:34:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.47
[32m[20221213 20:34:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.70
[32m[20221213 20:34:29 @agent_ppo2.py:143][0m Total time:       5.33 min
[32m[20221213 20:34:29 @agent_ppo2.py:145][0m 770048 total steps have happened
[32m[20221213 20:34:29 @agent_ppo2.py:121][0m #------------------------ Iteration 188 --------------------------#
[32m[20221213 20:34:29 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:29 @agent_ppo2.py:185][0m |           0.0258 |           0.0169 |           0.0000 |
[32m[20221213 20:34:29 @agent_ppo2.py:185][0m |          -0.0061 |           0.0097 |           0.0000 |
[32m[20221213 20:34:29 @agent_ppo2.py:185][0m |          -0.0177 |           0.0093 |           0.0000 |
[32m[20221213 20:34:30 @agent_ppo2.py:185][0m |          -0.0289 |           0.0091 |           0.0000 |
[32m[20221213 20:34:30 @agent_ppo2.py:185][0m |          -0.0313 |           0.0090 |           0.0000 |
[32m[20221213 20:34:30 @agent_ppo2.py:185][0m |          -0.0391 |           0.0089 |           0.0000 |
[32m[20221213 20:34:30 @agent_ppo2.py:185][0m |          -0.0423 |           0.0087 |           0.0000 |
[32m[20221213 20:34:30 @agent_ppo2.py:185][0m |          -0.0479 |           0.0087 |           0.0000 |
[32m[20221213 20:34:30 @agent_ppo2.py:185][0m |          -0.0495 |           0.0086 |           0.0000 |
[32m[20221213 20:34:30 @agent_ppo2.py:185][0m |          -0.0585 |           0.0085 |           0.0000 |
[32m[20221213 20:34:30 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:34:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.51
[32m[20221213 20:34:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.83
[32m[20221213 20:34:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.46
[32m[20221213 20:34:30 @agent_ppo2.py:143][0m Total time:       5.35 min
[32m[20221213 20:34:30 @agent_ppo2.py:145][0m 774144 total steps have happened
[32m[20221213 20:34:30 @agent_ppo2.py:121][0m #------------------------ Iteration 189 --------------------------#
[32m[20221213 20:34:31 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:34:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:31 @agent_ppo2.py:185][0m |           0.0285 |           0.0408 |           0.0000 |
[32m[20221213 20:34:31 @agent_ppo2.py:185][0m |           0.0016 |           0.0207 |           0.0000 |
[32m[20221213 20:34:31 @agent_ppo2.py:185][0m |          -0.0152 |           0.0158 |           0.0000 |
[32m[20221213 20:34:31 @agent_ppo2.py:185][0m |          -0.0174 |           0.0152 |           0.0000 |
[32m[20221213 20:34:31 @agent_ppo2.py:185][0m |          -0.0242 |           0.0150 |           0.0000 |
[32m[20221213 20:34:31 @agent_ppo2.py:185][0m |          -0.0259 |           0.0146 |           0.0000 |
[32m[20221213 20:34:32 @agent_ppo2.py:185][0m |          -0.0243 |           0.0142 |           0.0000 |
[32m[20221213 20:34:32 @agent_ppo2.py:185][0m |          -0.0277 |           0.0140 |           0.0000 |
[32m[20221213 20:34:32 @agent_ppo2.py:185][0m |          -0.0293 |           0.0138 |           0.0000 |
[32m[20221213 20:34:32 @agent_ppo2.py:185][0m |          -0.0281 |           0.0138 |           0.0000 |
[32m[20221213 20:34:32 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:34:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.75
[32m[20221213 20:34:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.77
[32m[20221213 20:34:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.17
[32m[20221213 20:34:32 @agent_ppo2.py:143][0m Total time:       5.38 min
[32m[20221213 20:34:32 @agent_ppo2.py:145][0m 778240 total steps have happened
[32m[20221213 20:34:32 @agent_ppo2.py:121][0m #------------------------ Iteration 190 --------------------------#
[32m[20221213 20:34:33 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:34:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:33 @agent_ppo2.py:185][0m |           0.0161 |           0.0228 |           0.0000 |
[32m[20221213 20:34:33 @agent_ppo2.py:185][0m |           0.0090 |           0.0214 |           0.0000 |
[32m[20221213 20:34:33 @agent_ppo2.py:185][0m |          -0.0220 |           0.0195 |           0.0000 |
[32m[20221213 20:34:33 @agent_ppo2.py:185][0m |          -0.0407 |           0.0189 |           0.0000 |
[32m[20221213 20:34:33 @agent_ppo2.py:185][0m |          -0.0462 |           0.0182 |           0.0000 |
[32m[20221213 20:34:33 @agent_ppo2.py:185][0m |          -0.0561 |           0.0178 |           0.0000 |
[32m[20221213 20:34:33 @agent_ppo2.py:185][0m |          -0.0573 |           0.0174 |           0.0000 |
[32m[20221213 20:34:33 @agent_ppo2.py:185][0m |          -0.0616 |           0.0171 |           0.0000 |
[32m[20221213 20:34:33 @agent_ppo2.py:185][0m |          -0.0540 |           0.0169 |           0.0000 |
[32m[20221213 20:34:34 @agent_ppo2.py:185][0m |          -0.0585 |           0.0167 |           0.0000 |
[32m[20221213 20:34:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:34:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.72
[32m[20221213 20:34:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.27
[32m[20221213 20:34:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.04
[32m[20221213 20:34:34 @agent_ppo2.py:143][0m Total time:       5.41 min
[32m[20221213 20:34:34 @agent_ppo2.py:145][0m 782336 total steps have happened
[32m[20221213 20:34:34 @agent_ppo2.py:121][0m #------------------------ Iteration 191 --------------------------#
[32m[20221213 20:34:34 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:34:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:34 @agent_ppo2.py:185][0m |           0.0223 |           0.0240 |           0.0000 |
[32m[20221213 20:34:35 @agent_ppo2.py:185][0m |          -0.0047 |           0.0202 |           0.0000 |
[32m[20221213 20:34:35 @agent_ppo2.py:185][0m |          -0.0179 |           0.0185 |           0.0000 |
[32m[20221213 20:34:35 @agent_ppo2.py:185][0m |          -0.0320 |           0.0178 |           0.0000 |
[32m[20221213 20:34:35 @agent_ppo2.py:185][0m |          -0.0172 |           0.0175 |           0.0000 |
[32m[20221213 20:34:35 @agent_ppo2.py:185][0m |          -0.0306 |           0.0170 |           0.0000 |
[32m[20221213 20:34:35 @agent_ppo2.py:185][0m |          -0.0376 |           0.0167 |           0.0000 |
[32m[20221213 20:34:35 @agent_ppo2.py:185][0m |          -0.0383 |           0.0164 |           0.0000 |
[32m[20221213 20:34:35 @agent_ppo2.py:185][0m |          -0.0496 |           0.0161 |           0.0000 |
[32m[20221213 20:34:35 @agent_ppo2.py:185][0m |          -0.0430 |           0.0158 |           0.0000 |
[32m[20221213 20:34:35 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:34:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.48
[32m[20221213 20:34:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.74
[32m[20221213 20:34:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.96
[32m[20221213 20:34:36 @agent_ppo2.py:143][0m Total time:       5.44 min
[32m[20221213 20:34:36 @agent_ppo2.py:145][0m 786432 total steps have happened
[32m[20221213 20:34:36 @agent_ppo2.py:121][0m #------------------------ Iteration 192 --------------------------#
[32m[20221213 20:34:36 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:36 @agent_ppo2.py:185][0m |           0.0268 |           0.0200 |           0.0000 |
[32m[20221213 20:34:36 @agent_ppo2.py:185][0m |           0.0044 |           0.0122 |           0.0000 |
[32m[20221213 20:34:36 @agent_ppo2.py:185][0m |          -0.0113 |           0.0117 |           0.0000 |
[32m[20221213 20:34:36 @agent_ppo2.py:185][0m |          -0.0328 |           0.0116 |           0.0000 |
[32m[20221213 20:34:37 @agent_ppo2.py:185][0m |          -0.0304 |           0.0114 |           0.0000 |
[32m[20221213 20:34:37 @agent_ppo2.py:185][0m |          -0.0428 |           0.0113 |           0.0000 |
[32m[20221213 20:34:37 @agent_ppo2.py:185][0m |          -0.0486 |           0.0111 |           0.0000 |
[32m[20221213 20:34:37 @agent_ppo2.py:185][0m |          -0.0547 |           0.0110 |           0.0000 |
[32m[20221213 20:34:37 @agent_ppo2.py:185][0m |          -0.0516 |           0.0109 |           0.0000 |
[32m[20221213 20:34:37 @agent_ppo2.py:185][0m |          -0.0404 |           0.0109 |           0.0000 |
[32m[20221213 20:34:37 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:34:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.93
[32m[20221213 20:34:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.66
[32m[20221213 20:34:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.44
[32m[20221213 20:34:37 @agent_ppo2.py:143][0m Total time:       5.47 min
[32m[20221213 20:34:37 @agent_ppo2.py:145][0m 790528 total steps have happened
[32m[20221213 20:34:37 @agent_ppo2.py:121][0m #------------------------ Iteration 193 --------------------------#
[32m[20221213 20:34:38 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:34:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:38 @agent_ppo2.py:185][0m |           0.0405 |           0.0130 |           0.0000 |
[32m[20221213 20:34:38 @agent_ppo2.py:185][0m |           0.0266 |           0.0125 |           0.0000 |
[32m[20221213 20:34:38 @agent_ppo2.py:185][0m |          -0.0098 |           0.0118 |           0.0000 |
[32m[20221213 20:34:38 @agent_ppo2.py:185][0m |          -0.0285 |           0.0118 |           0.0000 |
[32m[20221213 20:34:38 @agent_ppo2.py:185][0m |          -0.0248 |           0.0115 |           0.0000 |
[32m[20221213 20:34:38 @agent_ppo2.py:185][0m |          -0.0392 |           0.0114 |           0.0000 |
[32m[20221213 20:34:38 @agent_ppo2.py:185][0m |          -0.0455 |           0.0114 |           0.0000 |
[32m[20221213 20:34:38 @agent_ppo2.py:185][0m |          -0.0360 |           0.0119 |           0.0000 |
[32m[20221213 20:34:39 @agent_ppo2.py:185][0m |          -0.0566 |           0.0116 |           0.0000 |
[32m[20221213 20:34:39 @agent_ppo2.py:185][0m |          -0.0655 |           0.0111 |           0.0000 |
[32m[20221213 20:34:39 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:34:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.88
[32m[20221213 20:34:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.96
[32m[20221213 20:34:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.01
[32m[20221213 20:34:39 @agent_ppo2.py:143][0m Total time:       5.50 min
[32m[20221213 20:34:39 @agent_ppo2.py:145][0m 794624 total steps have happened
[32m[20221213 20:34:39 @agent_ppo2.py:121][0m #------------------------ Iteration 194 --------------------------#
[32m[20221213 20:34:39 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:40 @agent_ppo2.py:185][0m |           0.0148 |           0.0156 |           0.0000 |
[32m[20221213 20:34:40 @agent_ppo2.py:185][0m |           0.0026 |           0.0129 |           0.0000 |
[32m[20221213 20:34:40 @agent_ppo2.py:185][0m |          -0.0115 |           0.0127 |           0.0000 |
[32m[20221213 20:34:40 @agent_ppo2.py:185][0m |          -0.0256 |           0.0125 |           0.0000 |
[32m[20221213 20:34:40 @agent_ppo2.py:185][0m |          -0.0300 |           0.0123 |           0.0000 |
[32m[20221213 20:34:40 @agent_ppo2.py:185][0m |          -0.0365 |           0.0125 |           0.0000 |
[32m[20221213 20:34:40 @agent_ppo2.py:185][0m |          -0.0335 |           0.0122 |           0.0000 |
[32m[20221213 20:34:40 @agent_ppo2.py:185][0m |          -0.0356 |           0.0119 |           0.0000 |
[32m[20221213 20:34:40 @agent_ppo2.py:185][0m |          -0.0321 |           0.0118 |           0.0000 |
[32m[20221213 20:34:40 @agent_ppo2.py:185][0m |          -0.0354 |           0.0117 |           0.0000 |
[32m[20221213 20:34:40 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:34:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.20
[32m[20221213 20:34:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.18
[32m[20221213 20:34:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.53
[32m[20221213 20:34:41 @agent_ppo2.py:143][0m Total time:       5.53 min
[32m[20221213 20:34:41 @agent_ppo2.py:145][0m 798720 total steps have happened
[32m[20221213 20:34:41 @agent_ppo2.py:121][0m #------------------------ Iteration 195 --------------------------#
[32m[20221213 20:34:41 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:41 @agent_ppo2.py:185][0m |           0.0159 |           0.0197 |           0.0000 |
[32m[20221213 20:34:41 @agent_ppo2.py:185][0m |           0.0046 |           0.0166 |           0.0000 |
[32m[20221213 20:34:42 @agent_ppo2.py:185][0m |          -0.0144 |           0.0156 |           0.0000 |
[32m[20221213 20:34:42 @agent_ppo2.py:185][0m |          -0.0240 |           0.0151 |           0.0000 |
[32m[20221213 20:34:42 @agent_ppo2.py:185][0m |          -0.0317 |           0.0147 |           0.0000 |
[32m[20221213 20:34:42 @agent_ppo2.py:185][0m |          -0.0528 |           0.0144 |           0.0000 |
[32m[20221213 20:34:42 @agent_ppo2.py:185][0m |          -0.0328 |           0.0151 |           0.0000 |
[32m[20221213 20:34:42 @agent_ppo2.py:185][0m |          -0.0392 |           0.0140 |           0.0000 |
[32m[20221213 20:34:42 @agent_ppo2.py:185][0m |          -0.0593 |           0.0146 |           0.0000 |
[32m[20221213 20:34:42 @agent_ppo2.py:185][0m |          -0.0382 |           0.0161 |           0.0000 |
[32m[20221213 20:34:42 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 20:34:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.64
[32m[20221213 20:34:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.26
[32m[20221213 20:34:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.41
[32m[20221213 20:34:43 @agent_ppo2.py:143][0m Total time:       5.56 min
[32m[20221213 20:34:43 @agent_ppo2.py:145][0m 802816 total steps have happened
[32m[20221213 20:34:43 @agent_ppo2.py:121][0m #------------------------ Iteration 196 --------------------------#
[32m[20221213 20:34:43 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:43 @agent_ppo2.py:185][0m |           0.0266 |           0.0243 |           0.0000 |
[32m[20221213 20:34:43 @agent_ppo2.py:185][0m |          -0.0083 |           0.0204 |           0.0000 |
[32m[20221213 20:34:43 @agent_ppo2.py:185][0m |          -0.0202 |           0.0192 |           0.0000 |
[32m[20221213 20:34:43 @agent_ppo2.py:185][0m |          -0.0304 |           0.0180 |           0.0000 |
[32m[20221213 20:34:44 @agent_ppo2.py:185][0m |          -0.0377 |           0.0175 |           0.0000 |
[32m[20221213 20:34:44 @agent_ppo2.py:185][0m |          -0.0356 |           0.0173 |           0.0000 |
[32m[20221213 20:34:44 @agent_ppo2.py:185][0m |          -0.0653 |           0.0170 |           0.0000 |
[32m[20221213 20:34:44 @agent_ppo2.py:185][0m |          -0.0444 |           0.0170 |           0.0000 |
[32m[20221213 20:34:44 @agent_ppo2.py:185][0m |          -0.0404 |           0.0164 |           0.0000 |
[32m[20221213 20:34:44 @agent_ppo2.py:185][0m |          -0.0540 |           0.0175 |           0.0000 |
[32m[20221213 20:34:44 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:34:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.97
[32m[20221213 20:34:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.59
[32m[20221213 20:34:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.72
[32m[20221213 20:34:44 @agent_ppo2.py:143][0m Total time:       5.59 min
[32m[20221213 20:34:44 @agent_ppo2.py:145][0m 806912 total steps have happened
[32m[20221213 20:34:44 @agent_ppo2.py:121][0m #------------------------ Iteration 197 --------------------------#
[32m[20221213 20:34:45 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:45 @agent_ppo2.py:185][0m |           0.0297 |           0.0409 |           0.0000 |
[32m[20221213 20:34:45 @agent_ppo2.py:185][0m |           0.0060 |           0.0319 |           0.0000 |
[32m[20221213 20:34:45 @agent_ppo2.py:185][0m |          -0.0213 |           0.0283 |           0.0000 |
[32m[20221213 20:34:45 @agent_ppo2.py:185][0m |          -0.0356 |           0.0263 |           0.0000 |
[32m[20221213 20:34:45 @agent_ppo2.py:185][0m |          -0.0448 |           0.0252 |           0.0000 |
[32m[20221213 20:34:46 @agent_ppo2.py:185][0m |          -0.0498 |           0.0249 |           0.0000 |
[32m[20221213 20:34:46 @agent_ppo2.py:185][0m |          -0.0555 |           0.0237 |           0.0000 |
[32m[20221213 20:34:46 @agent_ppo2.py:185][0m |          -0.0541 |           0.0232 |           0.0000 |
[32m[20221213 20:34:46 @agent_ppo2.py:185][0m |          -0.0569 |           0.0225 |           0.0000 |
[32m[20221213 20:34:46 @agent_ppo2.py:185][0m |          -0.0676 |           0.0227 |           0.0000 |
[32m[20221213 20:34:46 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:34:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.67
[32m[20221213 20:34:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.36
[32m[20221213 20:34:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.93
[32m[20221213 20:34:46 @agent_ppo2.py:143][0m Total time:       5.62 min
[32m[20221213 20:34:46 @agent_ppo2.py:145][0m 811008 total steps have happened
[32m[20221213 20:34:46 @agent_ppo2.py:121][0m #------------------------ Iteration 198 --------------------------#
[32m[20221213 20:34:47 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:47 @agent_ppo2.py:185][0m |           0.0447 |           0.0207 |           0.0000 |
[32m[20221213 20:34:47 @agent_ppo2.py:185][0m |           0.0345 |           0.0134 |           0.0000 |
[32m[20221213 20:34:47 @agent_ppo2.py:185][0m |           0.0185 |           0.0129 |           0.0000 |
[32m[20221213 20:34:47 @agent_ppo2.py:185][0m |           0.0122 |           0.0146 |           0.0000 |
[32m[20221213 20:34:47 @agent_ppo2.py:185][0m |          -0.0201 |           0.0151 |           0.0000 |
[32m[20221213 20:34:47 @agent_ppo2.py:185][0m |          -0.0264 |           0.0124 |           0.0000 |
[32m[20221213 20:34:47 @agent_ppo2.py:185][0m |          -0.0363 |           0.0122 |           0.0000 |
[32m[20221213 20:34:47 @agent_ppo2.py:185][0m |          -0.0369 |           0.0121 |           0.0000 |
[32m[20221213 20:34:48 @agent_ppo2.py:185][0m |          -0.0448 |           0.0119 |           0.0000 |
[32m[20221213 20:34:48 @agent_ppo2.py:185][0m |          -0.0456 |           0.0118 |           0.0000 |
[32m[20221213 20:34:48 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:34:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.07
[32m[20221213 20:34:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.65
[32m[20221213 20:34:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.60
[32m[20221213 20:34:48 @agent_ppo2.py:143][0m Total time:       5.65 min
[32m[20221213 20:34:48 @agent_ppo2.py:145][0m 815104 total steps have happened
[32m[20221213 20:34:48 @agent_ppo2.py:121][0m #------------------------ Iteration 199 --------------------------#
[32m[20221213 20:34:48 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:49 @agent_ppo2.py:185][0m |           0.0332 |           0.0177 |           0.0000 |
[32m[20221213 20:34:49 @agent_ppo2.py:185][0m |          -0.0028 |           0.0147 |           0.0000 |
[32m[20221213 20:34:49 @agent_ppo2.py:185][0m |          -0.0219 |           0.0143 |           0.0000 |
[32m[20221213 20:34:49 @agent_ppo2.py:185][0m |          -0.0264 |           0.0137 |           0.0000 |
[32m[20221213 20:34:49 @agent_ppo2.py:185][0m |          -0.0344 |           0.0134 |           0.0000 |
[32m[20221213 20:34:49 @agent_ppo2.py:185][0m |          -0.0418 |           0.0133 |           0.0000 |
[32m[20221213 20:34:49 @agent_ppo2.py:185][0m |          -0.0413 |           0.0131 |           0.0000 |
[32m[20221213 20:34:49 @agent_ppo2.py:185][0m |          -0.0441 |           0.0129 |           0.0000 |
[32m[20221213 20:34:49 @agent_ppo2.py:185][0m |          -0.0450 |           0.0129 |           0.0000 |
[32m[20221213 20:34:49 @agent_ppo2.py:185][0m |          -0.0468 |           0.0129 |           0.0000 |
[32m[20221213 20:34:49 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:34:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.99
[32m[20221213 20:34:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.66
[32m[20221213 20:34:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.26
[32m[20221213 20:34:50 @agent_ppo2.py:143][0m Total time:       5.68 min
[32m[20221213 20:34:50 @agent_ppo2.py:145][0m 819200 total steps have happened
[32m[20221213 20:34:50 @agent_ppo2.py:121][0m #------------------------ Iteration 200 --------------------------#
[32m[20221213 20:34:50 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:34:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:50 @agent_ppo2.py:185][0m |           0.0192 |           0.0177 |           0.0000 |
[32m[20221213 20:34:50 @agent_ppo2.py:185][0m |          -0.0119 |           0.0160 |           0.0000 |
[32m[20221213 20:34:51 @agent_ppo2.py:185][0m |          -0.0268 |           0.0157 |           0.0000 |
[32m[20221213 20:34:51 @agent_ppo2.py:185][0m |          -0.0712 |           0.0151 |           0.0000 |
[32m[20221213 20:34:51 @agent_ppo2.py:185][0m |          -0.0392 |           0.0152 |           0.0000 |
[32m[20221213 20:34:51 @agent_ppo2.py:185][0m |          -0.0299 |           0.0150 |           0.0000 |
[32m[20221213 20:34:51 @agent_ppo2.py:185][0m |          -0.0393 |           0.0149 |           0.0000 |
[32m[20221213 20:34:51 @agent_ppo2.py:185][0m |          -0.0484 |           0.0143 |           0.0000 |
[32m[20221213 20:34:51 @agent_ppo2.py:185][0m |          -0.0508 |           0.0144 |           0.0000 |
[32m[20221213 20:34:51 @agent_ppo2.py:185][0m |          -0.0479 |           0.0143 |           0.0000 |
[32m[20221213 20:34:51 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:34:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.33
[32m[20221213 20:34:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.25
[32m[20221213 20:34:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.15
[32m[20221213 20:34:52 @agent_ppo2.py:143][0m Total time:       5.71 min
[32m[20221213 20:34:52 @agent_ppo2.py:145][0m 823296 total steps have happened
[32m[20221213 20:34:52 @agent_ppo2.py:121][0m #------------------------ Iteration 201 --------------------------#
[32m[20221213 20:34:52 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:52 @agent_ppo2.py:185][0m |           0.0214 |           0.0320 |           0.0000 |
[32m[20221213 20:34:52 @agent_ppo2.py:185][0m |          -0.0013 |           0.0099 |           0.0000 |
[32m[20221213 20:34:52 @agent_ppo2.py:185][0m |          -0.0063 |           0.0088 |           0.0000 |
[32m[20221213 20:34:52 @agent_ppo2.py:185][0m |          -0.0187 |           0.0085 |           0.0000 |
[32m[20221213 20:34:53 @agent_ppo2.py:185][0m |          -0.0214 |           0.0082 |           0.0000 |
[32m[20221213 20:34:53 @agent_ppo2.py:185][0m |          -0.0283 |           0.0081 |           0.0000 |
[32m[20221213 20:34:53 @agent_ppo2.py:185][0m |          -0.0301 |           0.0080 |           0.0000 |
[32m[20221213 20:34:53 @agent_ppo2.py:185][0m |          -0.0336 |           0.0079 |           0.0000 |
[32m[20221213 20:34:53 @agent_ppo2.py:185][0m |          -0.0353 |           0.0078 |           0.0000 |
[32m[20221213 20:34:53 @agent_ppo2.py:185][0m |          -0.0412 |           0.0077 |           0.0000 |
[32m[20221213 20:34:53 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:34:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.11
[32m[20221213 20:34:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.25
[32m[20221213 20:34:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.45
[32m[20221213 20:34:53 @agent_ppo2.py:143][0m Total time:       5.74 min
[32m[20221213 20:34:53 @agent_ppo2.py:145][0m 827392 total steps have happened
[32m[20221213 20:34:53 @agent_ppo2.py:121][0m #------------------------ Iteration 202 --------------------------#
[32m[20221213 20:34:54 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:54 @agent_ppo2.py:185][0m |           0.0172 |           0.0080 |           0.0000 |
[32m[20221213 20:34:54 @agent_ppo2.py:185][0m |          -0.0038 |           0.0077 |           0.0000 |
[32m[20221213 20:34:54 @agent_ppo2.py:185][0m |          -0.0029 |           0.0075 |           0.0000 |
[32m[20221213 20:34:54 @agent_ppo2.py:185][0m |          -0.0184 |           0.0074 |           0.0000 |
[32m[20221213 20:34:54 @agent_ppo2.py:185][0m |          -0.0347 |           0.0074 |           0.0000 |
[32m[20221213 20:34:54 @agent_ppo2.py:185][0m |          -0.0437 |           0.0073 |           0.0000 |
[32m[20221213 20:34:55 @agent_ppo2.py:185][0m |          -0.0474 |           0.0072 |           0.0000 |
[32m[20221213 20:34:55 @agent_ppo2.py:185][0m |          -0.0480 |           0.0071 |           0.0000 |
[32m[20221213 20:34:55 @agent_ppo2.py:185][0m |          -0.0497 |           0.0071 |           0.0000 |
[32m[20221213 20:34:55 @agent_ppo2.py:185][0m |          -0.0533 |           0.0070 |           0.0000 |
[32m[20221213 20:34:55 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:34:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.55
[32m[20221213 20:34:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.95
[32m[20221213 20:34:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.06
[32m[20221213 20:34:55 @agent_ppo2.py:143][0m Total time:       5.77 min
[32m[20221213 20:34:55 @agent_ppo2.py:145][0m 831488 total steps have happened
[32m[20221213 20:34:55 @agent_ppo2.py:121][0m #------------------------ Iteration 203 --------------------------#
[32m[20221213 20:34:56 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:34:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:56 @agent_ppo2.py:185][0m |           0.0140 |           0.1228 |           0.0000 |
[32m[20221213 20:34:56 @agent_ppo2.py:185][0m |          -0.0370 |           0.0498 |           0.0000 |
[32m[20221213 20:34:56 @agent_ppo2.py:185][0m |           0.0291 |           0.0326 |           0.0000 |
[32m[20221213 20:34:56 @agent_ppo2.py:185][0m |           0.0117 |           0.0253 |           0.0000 |
[32m[20221213 20:34:56 @agent_ppo2.py:185][0m |          -0.0042 |           0.0243 |           0.0000 |
[32m[20221213 20:34:56 @agent_ppo2.py:185][0m |          -0.0110 |           0.0225 |           0.0000 |
[32m[20221213 20:34:56 @agent_ppo2.py:185][0m |          -0.0153 |           0.0220 |           0.0000 |
[32m[20221213 20:34:56 @agent_ppo2.py:185][0m |          -0.0194 |           0.0216 |           0.0000 |
[32m[20221213 20:34:56 @agent_ppo2.py:185][0m |          -0.0225 |           0.0214 |           0.0000 |
[32m[20221213 20:34:57 @agent_ppo2.py:185][0m |          -0.0263 |           0.0211 |           0.0000 |
[32m[20221213 20:34:57 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:34:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.27
[32m[20221213 20:34:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.05
[32m[20221213 20:34:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.05
[32m[20221213 20:34:57 @agent_ppo2.py:143][0m Total time:       5.80 min
[32m[20221213 20:34:57 @agent_ppo2.py:145][0m 835584 total steps have happened
[32m[20221213 20:34:57 @agent_ppo2.py:121][0m #------------------------ Iteration 204 --------------------------#
[32m[20221213 20:34:57 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:58 @agent_ppo2.py:185][0m |           0.0220 |           0.0190 |           0.0000 |
[32m[20221213 20:34:58 @agent_ppo2.py:185][0m |          -0.0311 |           0.0160 |           0.0000 |
[32m[20221213 20:34:58 @agent_ppo2.py:185][0m |          -0.0366 |           0.0159 |           0.0000 |
[32m[20221213 20:34:58 @agent_ppo2.py:185][0m |          -0.0419 |           0.0152 |           0.0000 |
[32m[20221213 20:34:58 @agent_ppo2.py:185][0m |          -0.0667 |           0.0149 |           0.0000 |
[32m[20221213 20:34:58 @agent_ppo2.py:185][0m |          -0.0666 |           0.0148 |           0.0000 |
[32m[20221213 20:34:58 @agent_ppo2.py:185][0m |          -0.0712 |           0.0149 |           0.0000 |
[32m[20221213 20:34:58 @agent_ppo2.py:185][0m |          -0.0783 |           0.0145 |           0.0000 |
[32m[20221213 20:34:58 @agent_ppo2.py:185][0m |          -0.0778 |           0.0144 |           0.0000 |
[32m[20221213 20:34:58 @agent_ppo2.py:185][0m |          -0.0815 |           0.0143 |           0.0000 |
[32m[20221213 20:34:58 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:34:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.91
[32m[20221213 20:34:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.91
[32m[20221213 20:34:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.02
[32m[20221213 20:34:59 @agent_ppo2.py:143][0m Total time:       5.83 min
[32m[20221213 20:34:59 @agent_ppo2.py:145][0m 839680 total steps have happened
[32m[20221213 20:34:59 @agent_ppo2.py:121][0m #------------------------ Iteration 205 --------------------------#
[32m[20221213 20:34:59 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:34:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:34:59 @agent_ppo2.py:185][0m |           0.0205 |           0.0537 |           0.0000 |
[32m[20221213 20:34:59 @agent_ppo2.py:185][0m |          -0.0071 |           0.0311 |           0.0000 |
[32m[20221213 20:34:59 @agent_ppo2.py:185][0m |          -0.0186 |           0.0274 |           0.0000 |
[32m[20221213 20:35:00 @agent_ppo2.py:185][0m |          -0.0214 |           0.0252 |           0.0000 |
[32m[20221213 20:35:00 @agent_ppo2.py:185][0m |          -0.0269 |           0.0233 |           0.0000 |
[32m[20221213 20:35:00 @agent_ppo2.py:185][0m |          -0.0328 |           0.0227 |           0.0000 |
[32m[20221213 20:35:00 @agent_ppo2.py:185][0m |          -0.0305 |           0.0221 |           0.0000 |
[32m[20221213 20:35:00 @agent_ppo2.py:185][0m |          -0.0338 |           0.0217 |           0.0000 |
[32m[20221213 20:35:00 @agent_ppo2.py:185][0m |          -0.0318 |           0.0210 |           0.0000 |
[32m[20221213 20:35:00 @agent_ppo2.py:185][0m |          -0.0583 |           0.0208 |           0.0000 |
[32m[20221213 20:35:00 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:35:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.24
[32m[20221213 20:35:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.40
[32m[20221213 20:35:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.05
[32m[20221213 20:35:00 @agent_ppo2.py:143][0m Total time:       5.85 min
[32m[20221213 20:35:00 @agent_ppo2.py:145][0m 843776 total steps have happened
[32m[20221213 20:35:00 @agent_ppo2.py:121][0m #------------------------ Iteration 206 --------------------------#
[32m[20221213 20:35:01 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:35:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:01 @agent_ppo2.py:185][0m |           0.0182 |           0.0326 |           0.0000 |
[32m[20221213 20:35:01 @agent_ppo2.py:185][0m |           0.1707 |           0.0130 |           0.0000 |
[32m[20221213 20:35:01 @agent_ppo2.py:185][0m |           0.0010 |           0.0123 |           0.0000 |
[32m[20221213 20:35:01 @agent_ppo2.py:185][0m |          -0.0110 |           0.0119 |           0.0000 |
[32m[20221213 20:35:01 @agent_ppo2.py:185][0m |          -0.0175 |           0.0119 |           0.0000 |
[32m[20221213 20:35:01 @agent_ppo2.py:185][0m |          -0.0239 |           0.0118 |           0.0000 |
[32m[20221213 20:35:02 @agent_ppo2.py:185][0m |          -0.0255 |           0.0117 |           0.0000 |
[32m[20221213 20:35:02 @agent_ppo2.py:185][0m |          -0.0307 |           0.0116 |           0.0000 |
[32m[20221213 20:35:02 @agent_ppo2.py:185][0m |          -0.0368 |           0.0114 |           0.0000 |
[32m[20221213 20:35:02 @agent_ppo2.py:185][0m |          -0.0429 |           0.0115 |           0.0000 |
[32m[20221213 20:35:02 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:35:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.84
[32m[20221213 20:35:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.44
[32m[20221213 20:35:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.32
[32m[20221213 20:35:02 @agent_ppo2.py:143][0m Total time:       5.88 min
[32m[20221213 20:35:02 @agent_ppo2.py:145][0m 847872 total steps have happened
[32m[20221213 20:35:02 @agent_ppo2.py:121][0m #------------------------ Iteration 207 --------------------------#
[32m[20221213 20:35:03 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:03 @agent_ppo2.py:185][0m |           0.0296 |           0.0542 |           0.0000 |
[32m[20221213 20:35:03 @agent_ppo2.py:185][0m |          -0.0013 |           0.0278 |           0.0000 |
[32m[20221213 20:35:03 @agent_ppo2.py:185][0m |          -0.0532 |           0.0249 |           0.0000 |
[32m[20221213 20:35:03 @agent_ppo2.py:185][0m |          -0.0088 |           0.0245 |           0.0000 |
[32m[20221213 20:35:03 @agent_ppo2.py:185][0m |          -0.0198 |           0.0227 |           0.0000 |
[32m[20221213 20:35:03 @agent_ppo2.py:185][0m |          -0.0187 |           0.0202 |           0.0000 |
[32m[20221213 20:35:03 @agent_ppo2.py:185][0m |          -0.0253 |           0.0194 |           0.0000 |
[32m[20221213 20:35:03 @agent_ppo2.py:185][0m |          -0.0235 |           0.0189 |           0.0000 |
[32m[20221213 20:35:03 @agent_ppo2.py:185][0m |          -0.0852 |           0.0243 |           0.0000 |
[32m[20221213 20:35:04 @agent_ppo2.py:185][0m |          -0.0298 |           0.0267 |           0.0000 |
[32m[20221213 20:35:04 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:35:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.17
[32m[20221213 20:35:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.99
[32m[20221213 20:35:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.70
[32m[20221213 20:35:04 @agent_ppo2.py:143][0m Total time:       5.91 min
[32m[20221213 20:35:04 @agent_ppo2.py:145][0m 851968 total steps have happened
[32m[20221213 20:35:04 @agent_ppo2.py:121][0m #------------------------ Iteration 208 --------------------------#
[32m[20221213 20:35:04 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:04 @agent_ppo2.py:185][0m |           0.0196 |           0.0302 |           0.0000 |
[32m[20221213 20:35:05 @agent_ppo2.py:185][0m |          -0.0027 |           0.0160 |           0.0000 |
[32m[20221213 20:35:05 @agent_ppo2.py:185][0m |          -0.0217 |           0.0153 |           0.0000 |
[32m[20221213 20:35:05 @agent_ppo2.py:185][0m |          -0.0457 |           0.0152 |           0.0000 |
[32m[20221213 20:35:05 @agent_ppo2.py:185][0m |          -0.0389 |           0.0150 |           0.0000 |
[32m[20221213 20:35:05 @agent_ppo2.py:185][0m |          -0.0512 |           0.0145 |           0.0000 |
[32m[20221213 20:35:05 @agent_ppo2.py:185][0m |          -0.0427 |           0.0143 |           0.0000 |
[32m[20221213 20:35:05 @agent_ppo2.py:185][0m |          -0.0532 |           0.0140 |           0.0000 |
[32m[20221213 20:35:05 @agent_ppo2.py:185][0m |          -0.0491 |           0.0139 |           0.0000 |
[32m[20221213 20:35:05 @agent_ppo2.py:185][0m |          -0.0531 |           0.0137 |           0.0000 |
[32m[20221213 20:35:05 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:35:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.84
[32m[20221213 20:35:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.71
[32m[20221213 20:35:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.02
[32m[20221213 20:35:06 @agent_ppo2.py:143][0m Total time:       5.94 min
[32m[20221213 20:35:06 @agent_ppo2.py:145][0m 856064 total steps have happened
[32m[20221213 20:35:06 @agent_ppo2.py:121][0m #------------------------ Iteration 209 --------------------------#
[32m[20221213 20:35:06 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:06 @agent_ppo2.py:185][0m |           0.0528 |           0.0131 |           0.0000 |
[32m[20221213 20:35:06 @agent_ppo2.py:185][0m |           0.0357 |           0.0128 |           0.0000 |
[32m[20221213 20:35:06 @agent_ppo2.py:185][0m |          -0.0100 |           0.0125 |           0.0000 |
[32m[20221213 20:35:07 @agent_ppo2.py:185][0m |          -0.0373 |           0.0125 |           0.0000 |
[32m[20221213 20:35:07 @agent_ppo2.py:185][0m |          -0.0475 |           0.0121 |           0.0000 |
[32m[20221213 20:35:07 @agent_ppo2.py:185][0m |          -0.0581 |           0.0121 |           0.0000 |
[32m[20221213 20:35:07 @agent_ppo2.py:185][0m |          -0.0642 |           0.0119 |           0.0000 |
[32m[20221213 20:35:07 @agent_ppo2.py:185][0m |          -0.0791 |           0.0118 |           0.0000 |
[32m[20221213 20:35:07 @agent_ppo2.py:185][0m |          -0.0708 |           0.0118 |           0.0000 |
[32m[20221213 20:35:07 @agent_ppo2.py:185][0m |          -0.0767 |           0.0116 |           0.0000 |
[32m[20221213 20:35:07 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:35:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.46
[32m[20221213 20:35:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.61
[32m[20221213 20:35:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.23
[32m[20221213 20:35:07 @agent_ppo2.py:143][0m Total time:       5.97 min
[32m[20221213 20:35:07 @agent_ppo2.py:145][0m 860160 total steps have happened
[32m[20221213 20:35:07 @agent_ppo2.py:121][0m #------------------------ Iteration 210 --------------------------#
[32m[20221213 20:35:08 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:35:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:08 @agent_ppo2.py:185][0m |           0.0251 |           0.0721 |           0.0000 |
[32m[20221213 20:35:08 @agent_ppo2.py:185][0m |          -0.0046 |           0.0420 |           0.0000 |
[32m[20221213 20:35:08 @agent_ppo2.py:185][0m |          -0.0162 |           0.0366 |           0.0000 |
[32m[20221213 20:35:08 @agent_ppo2.py:185][0m |          -0.0239 |           0.0337 |           0.0000 |
[32m[20221213 20:35:08 @agent_ppo2.py:185][0m |          -0.0226 |           0.0323 |           0.0000 |
[32m[20221213 20:35:08 @agent_ppo2.py:185][0m |          -0.0295 |           0.0321 |           0.0000 |
[32m[20221213 20:35:09 @agent_ppo2.py:185][0m |          -0.0326 |           0.0295 |           0.0000 |
[32m[20221213 20:35:09 @agent_ppo2.py:185][0m |          -0.0292 |           0.0295 |           0.0000 |
[32m[20221213 20:35:09 @agent_ppo2.py:185][0m |          -0.0330 |           0.0299 |           0.0000 |
[32m[20221213 20:35:09 @agent_ppo2.py:185][0m |          -0.0353 |           0.0283 |           0.0000 |
[32m[20221213 20:35:09 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:35:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.63
[32m[20221213 20:35:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.33
[32m[20221213 20:35:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.41
[32m[20221213 20:35:09 @agent_ppo2.py:143][0m Total time:       6.00 min
[32m[20221213 20:35:09 @agent_ppo2.py:145][0m 864256 total steps have happened
[32m[20221213 20:35:09 @agent_ppo2.py:121][0m #------------------------ Iteration 211 --------------------------#
[32m[20221213 20:35:10 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:10 @agent_ppo2.py:185][0m |           0.0165 |           0.0478 |           0.0000 |
[32m[20221213 20:35:10 @agent_ppo2.py:185][0m |          -0.0072 |           0.0138 |           0.0000 |
[32m[20221213 20:35:10 @agent_ppo2.py:185][0m |          -0.0227 |           0.0117 |           0.0000 |
[32m[20221213 20:35:10 @agent_ppo2.py:185][0m |          -0.0342 |           0.0111 |           0.0000 |
[32m[20221213 20:35:10 @agent_ppo2.py:185][0m |          -0.0339 |           0.0109 |           0.0000 |
[32m[20221213 20:35:10 @agent_ppo2.py:185][0m |          -0.0143 |           0.0107 |           0.0000 |
[32m[20221213 20:35:10 @agent_ppo2.py:185][0m |          -0.0463 |           0.0105 |           0.0000 |
[32m[20221213 20:35:10 @agent_ppo2.py:185][0m |          -0.0456 |           0.0103 |           0.0000 |
[32m[20221213 20:35:11 @agent_ppo2.py:185][0m |          -0.0507 |           0.0107 |           0.0000 |
[32m[20221213 20:35:11 @agent_ppo2.py:185][0m |          -0.0485 |           0.0104 |           0.0000 |
[32m[20221213 20:35:11 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:35:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.14
[32m[20221213 20:35:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.21
[32m[20221213 20:35:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.26
[32m[20221213 20:35:11 @agent_ppo2.py:143][0m Total time:       6.03 min
[32m[20221213 20:35:11 @agent_ppo2.py:145][0m 868352 total steps have happened
[32m[20221213 20:35:11 @agent_ppo2.py:121][0m #------------------------ Iteration 212 --------------------------#
[32m[20221213 20:35:11 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:12 @agent_ppo2.py:185][0m |           0.0253 |           0.0863 |           0.0000 |
[32m[20221213 20:35:12 @agent_ppo2.py:185][0m |          -0.0039 |           0.0292 |           0.0000 |
[32m[20221213 20:35:12 @agent_ppo2.py:185][0m |          -0.0051 |           0.0230 |           0.0000 |
[32m[20221213 20:35:12 @agent_ppo2.py:185][0m |          -0.0203 |           0.0223 |           0.0000 |
[32m[20221213 20:35:12 @agent_ppo2.py:185][0m |          -0.0211 |           0.0221 |           0.0000 |
[32m[20221213 20:35:12 @agent_ppo2.py:185][0m |          -0.0239 |           0.0200 |           0.0000 |
[32m[20221213 20:35:12 @agent_ppo2.py:185][0m |          -0.0264 |           0.0195 |           0.0000 |
[32m[20221213 20:35:12 @agent_ppo2.py:185][0m |          -0.0291 |           0.0189 |           0.0000 |
[32m[20221213 20:35:12 @agent_ppo2.py:185][0m |          -0.0289 |           0.0182 |           0.0000 |
[32m[20221213 20:35:12 @agent_ppo2.py:185][0m |          -0.0223 |           0.0183 |           0.0000 |
[32m[20221213 20:35:12 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:35:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.03
[32m[20221213 20:35:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.15
[32m[20221213 20:35:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.83
[32m[20221213 20:35:13 @agent_ppo2.py:143][0m Total time:       6.06 min
[32m[20221213 20:35:13 @agent_ppo2.py:145][0m 872448 total steps have happened
[32m[20221213 20:35:13 @agent_ppo2.py:121][0m #------------------------ Iteration 213 --------------------------#
[32m[20221213 20:35:13 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:13 @agent_ppo2.py:185][0m |           0.0357 |           0.0235 |           0.0000 |
[32m[20221213 20:35:13 @agent_ppo2.py:185][0m |          -0.0055 |           0.0210 |           0.0000 |
[32m[20221213 20:35:14 @agent_ppo2.py:185][0m |          -0.0380 |           0.0200 |           0.0000 |
[32m[20221213 20:35:14 @agent_ppo2.py:185][0m |          -0.0512 |           0.0194 |           0.0000 |
[32m[20221213 20:35:14 @agent_ppo2.py:185][0m |          -0.0537 |           0.0182 |           0.0000 |
[32m[20221213 20:35:14 @agent_ppo2.py:185][0m |          -0.0608 |           0.0178 |           0.0000 |
[32m[20221213 20:35:14 @agent_ppo2.py:185][0m |          -0.0645 |           0.0177 |           0.0000 |
[32m[20221213 20:35:14 @agent_ppo2.py:185][0m |          -0.0634 |           0.0172 |           0.0000 |
[32m[20221213 20:35:14 @agent_ppo2.py:185][0m |          -0.0707 |           0.0173 |           0.0000 |
[32m[20221213 20:35:14 @agent_ppo2.py:185][0m |          -0.0742 |           0.0172 |           0.0000 |
[32m[20221213 20:35:14 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:35:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.94
[32m[20221213 20:35:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.70
[32m[20221213 20:35:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.74
[32m[20221213 20:35:15 @agent_ppo2.py:143][0m Total time:       6.09 min
[32m[20221213 20:35:15 @agent_ppo2.py:145][0m 876544 total steps have happened
[32m[20221213 20:35:15 @agent_ppo2.py:121][0m #------------------------ Iteration 214 --------------------------#
[32m[20221213 20:35:15 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:15 @agent_ppo2.py:185][0m |           0.0148 |           0.0592 |           0.0000 |
[32m[20221213 20:35:15 @agent_ppo2.py:185][0m |          -0.0070 |           0.0365 |           0.0000 |
[32m[20221213 20:35:15 @agent_ppo2.py:185][0m |          -0.0215 |           0.0318 |           0.0000 |
[32m[20221213 20:35:15 @agent_ppo2.py:185][0m |          -0.0250 |           0.0298 |           0.0000 |
[32m[20221213 20:35:15 @agent_ppo2.py:185][0m |          -0.0322 |           0.0270 |           0.0000 |
[32m[20221213 20:35:16 @agent_ppo2.py:185][0m |          -0.0343 |           0.0250 |           0.0000 |
[32m[20221213 20:35:16 @agent_ppo2.py:185][0m |          -0.0379 |           0.0242 |           0.0000 |
[32m[20221213 20:35:16 @agent_ppo2.py:185][0m |          -0.0365 |           0.0234 |           0.0000 |
[32m[20221213 20:35:16 @agent_ppo2.py:185][0m |          -0.0376 |           0.0234 |           0.0000 |
[32m[20221213 20:35:16 @agent_ppo2.py:185][0m |          -0.0386 |           0.0228 |           0.0000 |
[32m[20221213 20:35:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:35:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.45
[32m[20221213 20:35:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.38
[32m[20221213 20:35:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.33
[32m[20221213 20:35:16 @agent_ppo2.py:143][0m Total time:       6.12 min
[32m[20221213 20:35:16 @agent_ppo2.py:145][0m 880640 total steps have happened
[32m[20221213 20:35:16 @agent_ppo2.py:121][0m #------------------------ Iteration 215 --------------------------#
[32m[20221213 20:35:17 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:35:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:17 @agent_ppo2.py:185][0m |           0.0347 |           0.0282 |           0.0000 |
[32m[20221213 20:35:17 @agent_ppo2.py:185][0m |           0.0168 |           0.0149 |           0.0000 |
[32m[20221213 20:35:17 @agent_ppo2.py:185][0m |          -0.0096 |           0.0144 |           0.0000 |
[32m[20221213 20:35:17 @agent_ppo2.py:185][0m |          -0.0167 |           0.0139 |           0.0000 |
[32m[20221213 20:35:17 @agent_ppo2.py:185][0m |          -0.0260 |           0.0136 |           0.0000 |
[32m[20221213 20:35:17 @agent_ppo2.py:185][0m |          -0.0254 |           0.0133 |           0.0000 |
[32m[20221213 20:35:17 @agent_ppo2.py:185][0m |          -0.0473 |           0.0134 |           0.0000 |
[32m[20221213 20:35:17 @agent_ppo2.py:185][0m |          -0.0463 |           0.0132 |           0.0000 |
[32m[20221213 20:35:18 @agent_ppo2.py:185][0m |          -0.0472 |           0.0129 |           0.0000 |
[32m[20221213 20:35:18 @agent_ppo2.py:185][0m |          -0.0527 |           0.0126 |           0.0000 |
[32m[20221213 20:35:18 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:35:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.07
[32m[20221213 20:35:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.44
[32m[20221213 20:35:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.32
[32m[20221213 20:35:18 @agent_ppo2.py:143][0m Total time:       6.15 min
[32m[20221213 20:35:18 @agent_ppo2.py:145][0m 884736 total steps have happened
[32m[20221213 20:35:18 @agent_ppo2.py:121][0m #------------------------ Iteration 216 --------------------------#
[32m[20221213 20:35:18 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:35:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:19 @agent_ppo2.py:185][0m |           0.0307 |           0.0140 |           0.0000 |
[32m[20221213 20:35:19 @agent_ppo2.py:185][0m |          -0.0193 |           0.0134 |           0.0000 |
[32m[20221213 20:35:19 @agent_ppo2.py:185][0m |          -0.0318 |           0.0133 |           0.0000 |
[32m[20221213 20:35:19 @agent_ppo2.py:185][0m |          -0.0531 |           0.0130 |           0.0000 |
[32m[20221213 20:35:19 @agent_ppo2.py:185][0m |          -0.0638 |           0.0127 |           0.0000 |
[32m[20221213 20:35:19 @agent_ppo2.py:185][0m |          -0.0703 |           0.0125 |           0.0000 |
[32m[20221213 20:35:19 @agent_ppo2.py:185][0m |          -0.0691 |           0.0123 |           0.0000 |
[32m[20221213 20:35:19 @agent_ppo2.py:185][0m |          -0.0690 |           0.0124 |           0.0000 |
[32m[20221213 20:35:19 @agent_ppo2.py:185][0m |          -0.0755 |           0.0123 |           0.0000 |
[32m[20221213 20:35:19 @agent_ppo2.py:185][0m |          -0.0812 |           0.0121 |           0.0000 |
[32m[20221213 20:35:19 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:35:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.31
[32m[20221213 20:35:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.08
[32m[20221213 20:35:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.56
[32m[20221213 20:35:20 @agent_ppo2.py:143][0m Total time:       6.18 min
[32m[20221213 20:35:20 @agent_ppo2.py:145][0m 888832 total steps have happened
[32m[20221213 20:35:20 @agent_ppo2.py:121][0m #------------------------ Iteration 217 --------------------------#
[32m[20221213 20:35:20 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:35:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:20 @agent_ppo2.py:185][0m |           0.0332 |           0.0151 |           0.0000 |
[32m[20221213 20:35:20 @agent_ppo2.py:185][0m |          -0.0078 |           0.0142 |           0.0000 |
[32m[20221213 20:35:20 @agent_ppo2.py:185][0m |          -0.0307 |           0.0142 |           0.0000 |
[32m[20221213 20:35:21 @agent_ppo2.py:185][0m |          -0.0450 |           0.0138 |           0.0000 |
[32m[20221213 20:35:21 @agent_ppo2.py:185][0m |          -0.0512 |           0.0136 |           0.0000 |
[32m[20221213 20:35:21 @agent_ppo2.py:185][0m |          -0.0550 |           0.0135 |           0.0000 |
[32m[20221213 20:35:21 @agent_ppo2.py:185][0m |          -0.0513 |           0.0135 |           0.0000 |
[32m[20221213 20:35:21 @agent_ppo2.py:185][0m |          -0.0645 |           0.0133 |           0.0000 |
[32m[20221213 20:35:21 @agent_ppo2.py:185][0m |          -0.0672 |           0.0130 |           0.0000 |
[32m[20221213 20:35:21 @agent_ppo2.py:185][0m |          -0.0644 |           0.0131 |           0.0000 |
[32m[20221213 20:35:21 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:35:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.99
[32m[20221213 20:35:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.43
[32m[20221213 20:35:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.56
[32m[20221213 20:35:22 @agent_ppo2.py:143][0m Total time:       6.21 min
[32m[20221213 20:35:22 @agent_ppo2.py:145][0m 892928 total steps have happened
[32m[20221213 20:35:22 @agent_ppo2.py:121][0m #------------------------ Iteration 218 --------------------------#
[32m[20221213 20:35:22 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:22 @agent_ppo2.py:185][0m |           0.0166 |           0.0264 |           0.0000 |
[32m[20221213 20:35:22 @agent_ppo2.py:185][0m |          -0.0005 |           0.0095 |           0.0000 |
[32m[20221213 20:35:22 @agent_ppo2.py:185][0m |          -0.0117 |           0.0085 |           0.0000 |
[32m[20221213 20:35:22 @agent_ppo2.py:185][0m |          -0.0175 |           0.0082 |           0.0000 |
[32m[20221213 20:35:22 @agent_ppo2.py:185][0m |          -0.0243 |           0.0080 |           0.0000 |
[32m[20221213 20:35:23 @agent_ppo2.py:185][0m |          -0.0243 |           0.0078 |           0.0000 |
[32m[20221213 20:35:23 @agent_ppo2.py:185][0m |          -0.0315 |           0.0077 |           0.0000 |
[32m[20221213 20:35:23 @agent_ppo2.py:185][0m |          -0.0320 |           0.0076 |           0.0000 |
[32m[20221213 20:35:23 @agent_ppo2.py:185][0m |          -0.0337 |           0.0075 |           0.0000 |
[32m[20221213 20:35:23 @agent_ppo2.py:185][0m |          -0.0395 |           0.0074 |           0.0000 |
[32m[20221213 20:35:23 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:35:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.27
[32m[20221213 20:35:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.40
[32m[20221213 20:35:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.11
[32m[20221213 20:35:23 @agent_ppo2.py:143][0m Total time:       6.24 min
[32m[20221213 20:35:23 @agent_ppo2.py:145][0m 897024 total steps have happened
[32m[20221213 20:35:23 @agent_ppo2.py:121][0m #------------------------ Iteration 219 --------------------------#
[32m[20221213 20:35:24 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:35:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:24 @agent_ppo2.py:185][0m |           0.0273 |           0.0804 |           0.0000 |
[32m[20221213 20:35:24 @agent_ppo2.py:185][0m |          -0.0016 |           0.0221 |           0.0000 |
[32m[20221213 20:35:24 @agent_ppo2.py:185][0m |          -0.0100 |           0.0185 |           0.0000 |
[32m[20221213 20:35:24 @agent_ppo2.py:185][0m |          -0.0176 |           0.0169 |           0.0000 |
[32m[20221213 20:35:24 @agent_ppo2.py:185][0m |          -0.0176 |           0.0169 |           0.0000 |
[32m[20221213 20:35:24 @agent_ppo2.py:185][0m |          -0.0203 |           0.0162 |           0.0000 |
[32m[20221213 20:35:24 @agent_ppo2.py:185][0m |          -0.0252 |           0.0157 |           0.0000 |
[32m[20221213 20:35:24 @agent_ppo2.py:185][0m |          -0.0264 |           0.0155 |           0.0000 |
[32m[20221213 20:35:25 @agent_ppo2.py:185][0m |          -0.0277 |           0.0152 |           0.0000 |
[32m[20221213 20:35:25 @agent_ppo2.py:185][0m |          -0.0285 |           0.0149 |           0.0000 |
[32m[20221213 20:35:25 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:35:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.72
[32m[20221213 20:35:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.54
[32m[20221213 20:35:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.69
[32m[20221213 20:35:25 @agent_ppo2.py:143][0m Total time:       6.26 min
[32m[20221213 20:35:25 @agent_ppo2.py:145][0m 901120 total steps have happened
[32m[20221213 20:35:25 @agent_ppo2.py:121][0m #------------------------ Iteration 220 --------------------------#
[32m[20221213 20:35:25 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:35:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:26 @agent_ppo2.py:185][0m |           0.0533 |           0.0356 |           0.0000 |
[32m[20221213 20:35:26 @agent_ppo2.py:185][0m |           0.0158 |           0.0271 |           0.0000 |
[32m[20221213 20:35:26 @agent_ppo2.py:185][0m |          -0.0075 |           0.0240 |           0.0000 |
[32m[20221213 20:35:26 @agent_ppo2.py:185][0m |          -0.0210 |           0.0222 |           0.0000 |
[32m[20221213 20:35:26 @agent_ppo2.py:185][0m |          -0.0361 |           0.0219 |           0.0000 |
[32m[20221213 20:35:26 @agent_ppo2.py:185][0m |          -0.0442 |           0.0214 |           0.0000 |
[32m[20221213 20:35:26 @agent_ppo2.py:185][0m |          -0.0435 |           0.0203 |           0.0000 |
[32m[20221213 20:35:26 @agent_ppo2.py:185][0m |          -0.0393 |           0.0199 |           0.0000 |
[32m[20221213 20:35:26 @agent_ppo2.py:185][0m |          -0.0503 |           0.0203 |           0.0000 |
[32m[20221213 20:35:26 @agent_ppo2.py:185][0m |          -0.0542 |           0.0201 |           0.0000 |
[32m[20221213 20:35:26 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:35:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.46
[32m[20221213 20:35:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.64
[32m[20221213 20:35:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.28
[32m[20221213 20:35:27 @agent_ppo2.py:143][0m Total time:       6.29 min
[32m[20221213 20:35:27 @agent_ppo2.py:145][0m 905216 total steps have happened
[32m[20221213 20:35:27 @agent_ppo2.py:121][0m #------------------------ Iteration 221 --------------------------#
[32m[20221213 20:35:27 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:27 @agent_ppo2.py:185][0m |           0.0332 |           0.0682 |           0.0000 |
[32m[20221213 20:35:27 @agent_ppo2.py:185][0m |          -0.0110 |           0.0394 |           0.0000 |
[32m[20221213 20:35:28 @agent_ppo2.py:185][0m |          -0.0276 |           0.0313 |           0.0000 |
[32m[20221213 20:35:28 @agent_ppo2.py:185][0m |          -0.0336 |           0.0299 |           0.0000 |
[32m[20221213 20:35:28 @agent_ppo2.py:185][0m |          -0.0323 |           0.0285 |           0.0000 |
[32m[20221213 20:35:28 @agent_ppo2.py:185][0m |          -0.0365 |           0.0265 |           0.0000 |
[32m[20221213 20:35:28 @agent_ppo2.py:185][0m |          -0.0415 |           0.0260 |           0.0000 |
[32m[20221213 20:35:28 @agent_ppo2.py:185][0m |          -0.0409 |           0.0244 |           0.0000 |
[32m[20221213 20:35:28 @agent_ppo2.py:185][0m |          -0.0451 |           0.0236 |           0.0000 |
[32m[20221213 20:35:28 @agent_ppo2.py:185][0m |          -0.0762 |           0.0235 |           0.0000 |
[32m[20221213 20:35:28 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:35:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.91
[32m[20221213 20:35:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.27
[32m[20221213 20:35:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.27
[32m[20221213 20:35:29 @agent_ppo2.py:143][0m Total time:       6.32 min
[32m[20221213 20:35:29 @agent_ppo2.py:145][0m 909312 total steps have happened
[32m[20221213 20:35:29 @agent_ppo2.py:121][0m #------------------------ Iteration 222 --------------------------#
[32m[20221213 20:35:29 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:29 @agent_ppo2.py:185][0m |           0.0603 |           0.0396 |           0.0000 |
[32m[20221213 20:35:29 @agent_ppo2.py:185][0m |           0.0479 |           0.0171 |           0.0000 |
[32m[20221213 20:35:29 @agent_ppo2.py:185][0m |           0.0238 |           0.0156 |           0.0000 |
[32m[20221213 20:35:29 @agent_ppo2.py:185][0m |           0.0102 |           0.0152 |           0.0000 |
[32m[20221213 20:35:30 @agent_ppo2.py:185][0m |          -0.0005 |           0.0149 |           0.0000 |
[32m[20221213 20:35:30 @agent_ppo2.py:185][0m |          -0.0096 |           0.0147 |           0.0000 |
[32m[20221213 20:35:30 @agent_ppo2.py:185][0m |          -0.0140 |           0.0145 |           0.0000 |
[32m[20221213 20:35:30 @agent_ppo2.py:185][0m |          -0.0245 |           0.0143 |           0.0000 |
[32m[20221213 20:35:30 @agent_ppo2.py:185][0m |          -0.0287 |           0.0141 |           0.0000 |
[32m[20221213 20:35:30 @agent_ppo2.py:185][0m |          -0.0350 |           0.0140 |           0.0000 |
[32m[20221213 20:35:30 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:35:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.28
[32m[20221213 20:35:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.72
[32m[20221213 20:35:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.70
[32m[20221213 20:35:30 @agent_ppo2.py:143][0m Total time:       6.35 min
[32m[20221213 20:35:30 @agent_ppo2.py:145][0m 913408 total steps have happened
[32m[20221213 20:35:30 @agent_ppo2.py:121][0m #------------------------ Iteration 223 --------------------------#
[32m[20221213 20:35:31 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:35:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:31 @agent_ppo2.py:185][0m |           0.0350 |           0.0414 |           0.0000 |
[32m[20221213 20:35:31 @agent_ppo2.py:185][0m |           0.0419 |           0.0391 |           0.0000 |
[32m[20221213 20:35:31 @agent_ppo2.py:185][0m |          -0.0023 |           0.0333 |           0.0000 |
[32m[20221213 20:35:31 @agent_ppo2.py:185][0m |          -0.0169 |           0.0244 |           0.0000 |
[32m[20221213 20:35:31 @agent_ppo2.py:185][0m |          -0.0234 |           0.0229 |           0.0000 |
[32m[20221213 20:35:31 @agent_ppo2.py:185][0m |          -0.0297 |           0.0222 |           0.0000 |
[32m[20221213 20:35:31 @agent_ppo2.py:185][0m |          -0.0310 |           0.0217 |           0.0000 |
[32m[20221213 20:35:31 @agent_ppo2.py:185][0m |          -0.0575 |           0.0216 |           0.0000 |
[32m[20221213 20:35:32 @agent_ppo2.py:185][0m |          -0.0351 |           0.0210 |           0.0000 |
[32m[20221213 20:35:32 @agent_ppo2.py:185][0m |          -0.0408 |           0.0208 |           0.0000 |
[32m[20221213 20:35:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:35:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.43
[32m[20221213 20:35:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.16
[32m[20221213 20:35:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.41
[32m[20221213 20:35:32 @agent_ppo2.py:143][0m Total time:       6.38 min
[32m[20221213 20:35:32 @agent_ppo2.py:145][0m 917504 total steps have happened
[32m[20221213 20:35:32 @agent_ppo2.py:121][0m #------------------------ Iteration 224 --------------------------#
[32m[20221213 20:35:32 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:33 @agent_ppo2.py:185][0m |           0.0238 |           0.0185 |           0.0000 |
[32m[20221213 20:35:33 @agent_ppo2.py:185][0m |           0.0008 |           0.0169 |           0.0000 |
[32m[20221213 20:35:33 @agent_ppo2.py:185][0m |          -0.0268 |           0.0165 |           0.0000 |
[32m[20221213 20:35:33 @agent_ppo2.py:185][0m |          -0.0460 |           0.0166 |           0.0000 |
[32m[20221213 20:35:33 @agent_ppo2.py:185][0m |          -0.0673 |           0.0164 |           0.0000 |
[32m[20221213 20:35:33 @agent_ppo2.py:185][0m |          -0.0682 |           0.0158 |           0.0000 |
[32m[20221213 20:35:33 @agent_ppo2.py:185][0m |          -0.0666 |           0.0158 |           0.0000 |
[32m[20221213 20:35:33 @agent_ppo2.py:185][0m |          -0.0892 |           0.0153 |           0.0000 |
[32m[20221213 20:35:33 @agent_ppo2.py:185][0m |          -0.0508 |           0.0164 |           0.0000 |
[32m[20221213 20:35:33 @agent_ppo2.py:185][0m |          -0.0910 |           0.0159 |           0.0000 |
[32m[20221213 20:35:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:35:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.53
[32m[20221213 20:35:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.20
[32m[20221213 20:35:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.07
[32m[20221213 20:35:34 @agent_ppo2.py:143][0m Total time:       6.41 min
[32m[20221213 20:35:34 @agent_ppo2.py:145][0m 921600 total steps have happened
[32m[20221213 20:35:34 @agent_ppo2.py:121][0m #------------------------ Iteration 225 --------------------------#
[32m[20221213 20:35:34 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:34 @agent_ppo2.py:185][0m |           0.0179 |           0.1536 |           0.0000 |
[32m[20221213 20:35:34 @agent_ppo2.py:185][0m |          -0.0079 |           0.0798 |           0.0000 |
[32m[20221213 20:35:35 @agent_ppo2.py:185][0m |          -0.0607 |           0.0666 |           0.0000 |
[32m[20221213 20:35:35 @agent_ppo2.py:185][0m |          -0.0203 |           0.0594 |           0.0000 |
[32m[20221213 20:35:35 @agent_ppo2.py:185][0m |          -0.0251 |           0.0447 |           0.0000 |
[32m[20221213 20:35:35 @agent_ppo2.py:185][0m |          -0.0295 |           0.0424 |           0.0000 |
[32m[20221213 20:35:35 @agent_ppo2.py:185][0m |          -0.0303 |           0.0415 |           0.0000 |
[32m[20221213 20:35:35 @agent_ppo2.py:185][0m |          -0.0315 |           0.0390 |           0.0000 |
[32m[20221213 20:35:35 @agent_ppo2.py:185][0m |          -0.0318 |           0.0378 |           0.0000 |
[32m[20221213 20:35:35 @agent_ppo2.py:185][0m |          -0.0298 |           0.0362 |           0.0000 |
[32m[20221213 20:35:35 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:35:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.46
[32m[20221213 20:35:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 12.64
[32m[20221213 20:35:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.87
[32m[20221213 20:35:36 @agent_ppo2.py:143][0m Total time:       6.44 min
[32m[20221213 20:35:36 @agent_ppo2.py:145][0m 925696 total steps have happened
[32m[20221213 20:35:36 @agent_ppo2.py:121][0m #------------------------ Iteration 226 --------------------------#
[32m[20221213 20:35:36 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:36 @agent_ppo2.py:185][0m |           0.0630 |           0.0395 |           0.0000 |
[32m[20221213 20:35:36 @agent_ppo2.py:185][0m |           0.0168 |           0.0223 |           0.0000 |
[32m[20221213 20:35:36 @agent_ppo2.py:185][0m |          -0.0037 |           0.0212 |           0.0000 |
[32m[20221213 20:35:36 @agent_ppo2.py:185][0m |          -0.0041 |           0.0225 |           0.0000 |
[32m[20221213 20:35:36 @agent_ppo2.py:185][0m |          -0.0327 |           0.0204 |           0.0000 |
[32m[20221213 20:35:37 @agent_ppo2.py:185][0m |          -0.0393 |           0.0193 |           0.0000 |
[32m[20221213 20:35:37 @agent_ppo2.py:185][0m |          -0.0444 |           0.0189 |           0.0000 |
[32m[20221213 20:35:37 @agent_ppo2.py:185][0m |          -0.0451 |           0.0191 |           0.0000 |
[32m[20221213 20:35:37 @agent_ppo2.py:185][0m |          -0.0499 |           0.0185 |           0.0000 |
[32m[20221213 20:35:37 @agent_ppo2.py:185][0m |          -0.0544 |           0.0181 |           0.0000 |
[32m[20221213 20:35:37 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:35:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.69
[32m[20221213 20:35:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.08
[32m[20221213 20:35:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.97
[32m[20221213 20:35:37 @agent_ppo2.py:143][0m Total time:       6.47 min
[32m[20221213 20:35:37 @agent_ppo2.py:145][0m 929792 total steps have happened
[32m[20221213 20:35:37 @agent_ppo2.py:121][0m #------------------------ Iteration 227 --------------------------#
[32m[20221213 20:35:38 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:38 @agent_ppo2.py:185][0m |           0.0217 |           0.0177 |           0.0000 |
[32m[20221213 20:35:38 @agent_ppo2.py:185][0m |          -0.0089 |           0.0165 |           0.0000 |
[32m[20221213 20:35:38 @agent_ppo2.py:185][0m |          -0.0490 |           0.0163 |           0.0000 |
[32m[20221213 20:35:38 @agent_ppo2.py:185][0m |          -0.0605 |           0.0157 |           0.0000 |
[32m[20221213 20:35:38 @agent_ppo2.py:185][0m |          -0.0721 |           0.0153 |           0.0000 |
[32m[20221213 20:35:38 @agent_ppo2.py:185][0m |          -0.0671 |           0.0152 |           0.0000 |
[32m[20221213 20:35:38 @agent_ppo2.py:185][0m |          -0.0787 |           0.0150 |           0.0000 |
[32m[20221213 20:35:38 @agent_ppo2.py:185][0m |          -0.0837 |           0.0150 |           0.0000 |
[32m[20221213 20:35:39 @agent_ppo2.py:185][0m |          -0.0778 |           0.0148 |           0.0000 |
[32m[20221213 20:35:39 @agent_ppo2.py:185][0m |          -0.0962 |           0.0146 |           0.0000 |
[32m[20221213 20:35:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:35:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.65
[32m[20221213 20:35:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.49
[32m[20221213 20:35:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.37
[32m[20221213 20:35:39 @agent_ppo2.py:143][0m Total time:       6.50 min
[32m[20221213 20:35:39 @agent_ppo2.py:145][0m 933888 total steps have happened
[32m[20221213 20:35:39 @agent_ppo2.py:121][0m #------------------------ Iteration 228 --------------------------#
[32m[20221213 20:35:39 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:35:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:40 @agent_ppo2.py:185][0m |           0.0050 |           0.0190 |           0.0000 |
[32m[20221213 20:35:40 @agent_ppo2.py:185][0m |           0.0086 |           0.0113 |           0.0000 |
[32m[20221213 20:35:40 @agent_ppo2.py:185][0m |          -0.0118 |           0.0105 |           0.0000 |
[32m[20221213 20:35:40 @agent_ppo2.py:185][0m |          -0.0264 |           0.0103 |           0.0000 |
[32m[20221213 20:35:40 @agent_ppo2.py:185][0m |          -0.0349 |           0.0102 |           0.0000 |
[32m[20221213 20:35:40 @agent_ppo2.py:185][0m |          -0.0257 |           0.0102 |           0.0000 |
[32m[20221213 20:35:40 @agent_ppo2.py:185][0m |          -0.0422 |           0.0101 |           0.0000 |
[32m[20221213 20:35:40 @agent_ppo2.py:185][0m |          -0.0432 |           0.0100 |           0.0000 |
[32m[20221213 20:35:40 @agent_ppo2.py:185][0m |          -0.0479 |           0.0100 |           0.0000 |
[32m[20221213 20:35:40 @agent_ppo2.py:185][0m |          -0.0478 |           0.0099 |           0.0000 |
[32m[20221213 20:35:40 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:35:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.94
[32m[20221213 20:35:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.50
[32m[20221213 20:35:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.46
[32m[20221213 20:35:41 @agent_ppo2.py:143][0m Total time:       6.53 min
[32m[20221213 20:35:41 @agent_ppo2.py:145][0m 937984 total steps have happened
[32m[20221213 20:35:41 @agent_ppo2.py:121][0m #------------------------ Iteration 229 --------------------------#
[32m[20221213 20:35:41 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:35:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:41 @agent_ppo2.py:185][0m |           0.1309 |           0.0117 |           0.0000 |
[32m[20221213 20:35:41 @agent_ppo2.py:185][0m |           0.0125 |           0.0092 |           0.0000 |
[32m[20221213 20:35:41 @agent_ppo2.py:185][0m |           0.0028 |           0.0088 |           0.0000 |
[32m[20221213 20:35:42 @agent_ppo2.py:185][0m |          -0.0157 |           0.0086 |           0.0000 |
[32m[20221213 20:35:42 @agent_ppo2.py:185][0m |          -0.0251 |           0.0085 |           0.0000 |
[32m[20221213 20:35:42 @agent_ppo2.py:185][0m |          -0.0299 |           0.0085 |           0.0000 |
[32m[20221213 20:35:42 @agent_ppo2.py:185][0m |          -0.0354 |           0.0084 |           0.0000 |
[32m[20221213 20:35:42 @agent_ppo2.py:185][0m |          -0.0424 |           0.0083 |           0.0000 |
[32m[20221213 20:35:42 @agent_ppo2.py:185][0m |          -0.0462 |           0.0082 |           0.0000 |
[32m[20221213 20:35:42 @agent_ppo2.py:185][0m |          -0.0479 |           0.0082 |           0.0000 |
[32m[20221213 20:35:42 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:35:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.01
[32m[20221213 20:35:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.27
[32m[20221213 20:35:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.88
[32m[20221213 20:35:42 @agent_ppo2.py:143][0m Total time:       6.55 min
[32m[20221213 20:35:42 @agent_ppo2.py:145][0m 942080 total steps have happened
[32m[20221213 20:35:42 @agent_ppo2.py:121][0m #------------------------ Iteration 230 --------------------------#
[32m[20221213 20:35:43 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:35:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:43 @agent_ppo2.py:185][0m |           0.0286 |           0.0756 |           0.0000 |
[32m[20221213 20:35:43 @agent_ppo2.py:185][0m |          -0.0058 |           0.0214 |           0.0000 |
[32m[20221213 20:35:43 @agent_ppo2.py:185][0m |          -0.0100 |           0.0167 |           0.0000 |
[32m[20221213 20:35:43 @agent_ppo2.py:185][0m |          -0.0171 |           0.0156 |           0.0000 |
[32m[20221213 20:35:43 @agent_ppo2.py:185][0m |          -0.0204 |           0.0154 |           0.0000 |
[32m[20221213 20:35:43 @agent_ppo2.py:185][0m |          -0.0229 |           0.0148 |           0.0000 |
[32m[20221213 20:35:44 @agent_ppo2.py:185][0m |          -0.0245 |           0.0146 |           0.0000 |
[32m[20221213 20:35:44 @agent_ppo2.py:185][0m |          -0.0274 |           0.0143 |           0.0000 |
[32m[20221213 20:35:44 @agent_ppo2.py:185][0m |          -0.0285 |           0.0143 |           0.0000 |
[32m[20221213 20:35:44 @agent_ppo2.py:185][0m |          -0.0260 |           0.0144 |           0.0000 |
[32m[20221213 20:35:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:35:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.59
[32m[20221213 20:35:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.93
[32m[20221213 20:35:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.56
[32m[20221213 20:35:44 @agent_ppo2.py:143][0m Total time:       6.58 min
[32m[20221213 20:35:44 @agent_ppo2.py:145][0m 946176 total steps have happened
[32m[20221213 20:35:44 @agent_ppo2.py:121][0m #------------------------ Iteration 231 --------------------------#
[32m[20221213 20:35:45 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:45 @agent_ppo2.py:185][0m |           0.0155 |           0.0222 |           0.0000 |
[32m[20221213 20:35:45 @agent_ppo2.py:185][0m |          -0.0088 |           0.0109 |           0.0000 |
[32m[20221213 20:35:45 @agent_ppo2.py:185][0m |          -0.0247 |           0.0103 |           0.0000 |
[32m[20221213 20:35:45 @agent_ppo2.py:185][0m |          -0.0320 |           0.0100 |           0.0000 |
[32m[20221213 20:35:45 @agent_ppo2.py:185][0m |          -0.0370 |           0.0099 |           0.0000 |
[32m[20221213 20:35:45 @agent_ppo2.py:185][0m |          -0.0439 |           0.0097 |           0.0000 |
[32m[20221213 20:35:45 @agent_ppo2.py:185][0m |          -0.0448 |           0.0096 |           0.0000 |
[32m[20221213 20:35:45 @agent_ppo2.py:185][0m |          -0.0496 |           0.0095 |           0.0000 |
[32m[20221213 20:35:45 @agent_ppo2.py:185][0m |          -0.0257 |           0.0095 |           0.0000 |
[32m[20221213 20:35:46 @agent_ppo2.py:185][0m |          -0.0433 |           0.0094 |           0.0000 |
[32m[20221213 20:35:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:35:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.68
[32m[20221213 20:35:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.87
[32m[20221213 20:35:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.41
[32m[20221213 20:35:46 @agent_ppo2.py:143][0m Total time:       6.61 min
[32m[20221213 20:35:46 @agent_ppo2.py:145][0m 950272 total steps have happened
[32m[20221213 20:35:46 @agent_ppo2.py:121][0m #------------------------ Iteration 232 --------------------------#
[32m[20221213 20:35:46 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:35:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:46 @agent_ppo2.py:185][0m |           0.0122 |           0.0092 |           0.0000 |
[32m[20221213 20:35:47 @agent_ppo2.py:185][0m |          -0.0125 |           0.0082 |           0.0000 |
[32m[20221213 20:35:47 @agent_ppo2.py:185][0m |           0.0140 |           0.0083 |           0.0000 |
[32m[20221213 20:35:47 @agent_ppo2.py:185][0m |          -0.0404 |           0.0080 |           0.0000 |
[32m[20221213 20:35:47 @agent_ppo2.py:185][0m |          -0.0424 |           0.0079 |           0.0000 |
[32m[20221213 20:35:47 @agent_ppo2.py:185][0m |          -0.0496 |           0.0078 |           0.0000 |
[32m[20221213 20:35:47 @agent_ppo2.py:185][0m |          -0.0482 |           0.0078 |           0.0000 |
[32m[20221213 20:35:47 @agent_ppo2.py:185][0m |          -0.0514 |           0.0077 |           0.0000 |
[32m[20221213 20:35:47 @agent_ppo2.py:185][0m |          -0.0527 |           0.0076 |           0.0000 |
[32m[20221213 20:35:47 @agent_ppo2.py:185][0m |          -0.0609 |           0.0076 |           0.0000 |
[32m[20221213 20:35:47 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:35:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.51
[32m[20221213 20:35:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.00
[32m[20221213 20:35:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.73
[32m[20221213 20:35:48 @agent_ppo2.py:143][0m Total time:       6.64 min
[32m[20221213 20:35:48 @agent_ppo2.py:145][0m 954368 total steps have happened
[32m[20221213 20:35:48 @agent_ppo2.py:121][0m #------------------------ Iteration 233 --------------------------#
[32m[20221213 20:35:48 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:48 @agent_ppo2.py:185][0m |           0.0210 |           0.0816 |           0.0000 |
[32m[20221213 20:35:48 @agent_ppo2.py:185][0m |          -0.0098 |           0.0314 |           0.0000 |
[32m[20221213 20:35:48 @agent_ppo2.py:185][0m |          -0.0096 |           0.0235 |           0.0000 |
[32m[20221213 20:35:48 @agent_ppo2.py:185][0m |          -0.0136 |           0.0218 |           0.0000 |
[32m[20221213 20:35:49 @agent_ppo2.py:185][0m |          -0.0202 |           0.0208 |           0.0000 |
[32m[20221213 20:35:49 @agent_ppo2.py:185][0m |          -0.0234 |           0.0196 |           0.0000 |
[32m[20221213 20:35:49 @agent_ppo2.py:185][0m |          -0.0262 |           0.0197 |           0.0000 |
[32m[20221213 20:35:49 @agent_ppo2.py:185][0m |          -0.0274 |           0.0191 |           0.0000 |
[32m[20221213 20:35:49 @agent_ppo2.py:185][0m |          -0.0289 |           0.0183 |           0.0000 |
[32m[20221213 20:35:49 @agent_ppo2.py:185][0m |          -0.0414 |           0.0187 |           0.0000 |
[32m[20221213 20:35:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:35:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.80
[32m[20221213 20:35:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.25
[32m[20221213 20:35:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.99
[32m[20221213 20:35:49 @agent_ppo2.py:143][0m Total time:       6.67 min
[32m[20221213 20:35:49 @agent_ppo2.py:145][0m 958464 total steps have happened
[32m[20221213 20:35:49 @agent_ppo2.py:121][0m #------------------------ Iteration 234 --------------------------#
[32m[20221213 20:35:50 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:35:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:50 @agent_ppo2.py:185][0m |           0.0333 |           0.0457 |           0.0000 |
[32m[20221213 20:35:50 @agent_ppo2.py:185][0m |          -0.0122 |           0.0286 |           0.0000 |
[32m[20221213 20:35:50 @agent_ppo2.py:185][0m |          -0.0286 |           0.0247 |           0.0000 |
[32m[20221213 20:35:50 @agent_ppo2.py:185][0m |          -0.0437 |           0.0229 |           0.0000 |
[32m[20221213 20:35:50 @agent_ppo2.py:185][0m |          -0.0453 |           0.0223 |           0.0000 |
[32m[20221213 20:35:50 @agent_ppo2.py:185][0m |          -0.0466 |           0.0215 |           0.0000 |
[32m[20221213 20:35:50 @agent_ppo2.py:185][0m |          -0.0587 |           0.0206 |           0.0000 |
[32m[20221213 20:35:50 @agent_ppo2.py:185][0m |          -0.0522 |           0.0207 |           0.0000 |
[32m[20221213 20:35:51 @agent_ppo2.py:185][0m |          -0.0531 |           0.0200 |           0.0000 |
[32m[20221213 20:35:51 @agent_ppo2.py:185][0m |          -0.0607 |           0.0197 |           0.0000 |
[32m[20221213 20:35:51 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:35:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.73
[32m[20221213 20:35:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.45
[32m[20221213 20:35:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.88
[32m[20221213 20:35:51 @agent_ppo2.py:143][0m Total time:       6.70 min
[32m[20221213 20:35:51 @agent_ppo2.py:145][0m 962560 total steps have happened
[32m[20221213 20:35:51 @agent_ppo2.py:121][0m #------------------------ Iteration 235 --------------------------#
[32m[20221213 20:35:51 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:35:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:52 @agent_ppo2.py:185][0m |           0.0273 |           0.0208 |           0.0000 |
[32m[20221213 20:35:52 @agent_ppo2.py:185][0m |           0.0094 |           0.0202 |           0.0000 |
[32m[20221213 20:35:52 @agent_ppo2.py:185][0m |          -0.0253 |           0.0170 |           0.0000 |
[32m[20221213 20:35:52 @agent_ppo2.py:185][0m |          -0.0439 |           0.0162 |           0.0000 |
[32m[20221213 20:35:52 @agent_ppo2.py:185][0m |          -0.0373 |           0.0166 |           0.0000 |
[32m[20221213 20:35:52 @agent_ppo2.py:185][0m |          -0.0732 |           0.0166 |           0.0000 |
[32m[20221213 20:35:52 @agent_ppo2.py:185][0m |          -0.0715 |           0.0153 |           0.0000 |
[32m[20221213 20:35:52 @agent_ppo2.py:185][0m |          -0.0790 |           0.0153 |           0.0000 |
[32m[20221213 20:35:52 @agent_ppo2.py:185][0m |          -0.0782 |           0.0147 |           0.0000 |
[32m[20221213 20:35:52 @agent_ppo2.py:185][0m |          -0.0791 |           0.0147 |           0.0000 |
[32m[20221213 20:35:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:35:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.48
[32m[20221213 20:35:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.18
[32m[20221213 20:35:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.86
[32m[20221213 20:35:53 @agent_ppo2.py:143][0m Total time:       6.73 min
[32m[20221213 20:35:53 @agent_ppo2.py:145][0m 966656 total steps have happened
[32m[20221213 20:35:53 @agent_ppo2.py:121][0m #------------------------ Iteration 236 --------------------------#
[32m[20221213 20:35:53 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:53 @agent_ppo2.py:185][0m |           0.0205 |           0.0306 |           0.0000 |
[32m[20221213 20:35:53 @agent_ppo2.py:185][0m |          -0.0025 |           0.0225 |           0.0000 |
[32m[20221213 20:35:53 @agent_ppo2.py:185][0m |          -0.0199 |           0.0204 |           0.0000 |
[32m[20221213 20:35:54 @agent_ppo2.py:185][0m |          -0.0253 |           0.0194 |           0.0000 |
[32m[20221213 20:35:54 @agent_ppo2.py:185][0m |          -0.0331 |           0.0187 |           0.0000 |
[32m[20221213 20:35:54 @agent_ppo2.py:185][0m |          -0.0330 |           0.0185 |           0.0000 |
[32m[20221213 20:35:54 @agent_ppo2.py:185][0m |          -0.0315 |           0.0182 |           0.0000 |
[32m[20221213 20:35:54 @agent_ppo2.py:185][0m |          -0.0365 |           0.0181 |           0.0000 |
[32m[20221213 20:35:54 @agent_ppo2.py:185][0m |          -0.0400 |           0.0174 |           0.0000 |
[32m[20221213 20:35:54 @agent_ppo2.py:185][0m |          -0.0392 |           0.0171 |           0.0000 |
[32m[20221213 20:35:54 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:35:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.94
[32m[20221213 20:35:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.97
[32m[20221213 20:35:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.00
[32m[20221213 20:35:54 @agent_ppo2.py:143][0m Total time:       6.75 min
[32m[20221213 20:35:54 @agent_ppo2.py:145][0m 970752 total steps have happened
[32m[20221213 20:35:54 @agent_ppo2.py:121][0m #------------------------ Iteration 237 --------------------------#
[32m[20221213 20:35:55 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:35:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:55 @agent_ppo2.py:185][0m |           0.0691 |           0.0180 |           0.0000 |
[32m[20221213 20:35:55 @agent_ppo2.py:185][0m |           0.0484 |           0.0164 |           0.0000 |
[32m[20221213 20:35:55 @agent_ppo2.py:185][0m |           0.0107 |           0.0156 |           0.0000 |
[32m[20221213 20:35:55 @agent_ppo2.py:185][0m |          -0.0254 |           0.0172 |           0.0000 |
[32m[20221213 20:35:55 @agent_ppo2.py:185][0m |          -0.0255 |           0.0182 |           0.0000 |
[32m[20221213 20:35:55 @agent_ppo2.py:185][0m |          -0.0314 |           0.0149 |           0.0000 |
[32m[20221213 20:35:56 @agent_ppo2.py:185][0m |          -0.0484 |           0.0147 |           0.0000 |
[32m[20221213 20:35:56 @agent_ppo2.py:185][0m |          -0.0563 |           0.0142 |           0.0000 |
[32m[20221213 20:35:56 @agent_ppo2.py:185][0m |          -0.0691 |           0.0140 |           0.0000 |
[32m[20221213 20:35:56 @agent_ppo2.py:185][0m |          -0.0711 |           0.0139 |           0.0000 |
[32m[20221213 20:35:56 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:35:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.14
[32m[20221213 20:35:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.68
[32m[20221213 20:35:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.90
[32m[20221213 20:35:56 @agent_ppo2.py:143][0m Total time:       6.78 min
[32m[20221213 20:35:56 @agent_ppo2.py:145][0m 974848 total steps have happened
[32m[20221213 20:35:56 @agent_ppo2.py:121][0m #------------------------ Iteration 238 --------------------------#
[32m[20221213 20:35:57 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:57 @agent_ppo2.py:185][0m |           0.0182 |           0.0334 |           0.0000 |
[32m[20221213 20:35:57 @agent_ppo2.py:185][0m |          -0.0076 |           0.0246 |           0.0000 |
[32m[20221213 20:35:57 @agent_ppo2.py:185][0m |          -0.0156 |           0.0229 |           0.0000 |
[32m[20221213 20:35:57 @agent_ppo2.py:185][0m |          -0.0244 |           0.0217 |           0.0000 |
[32m[20221213 20:35:57 @agent_ppo2.py:185][0m |          -0.0302 |           0.0212 |           0.0000 |
[32m[20221213 20:35:57 @agent_ppo2.py:185][0m |          -0.0231 |           0.0206 |           0.0000 |
[32m[20221213 20:35:57 @agent_ppo2.py:185][0m |          -0.0355 |           0.0209 |           0.0000 |
[32m[20221213 20:35:57 @agent_ppo2.py:185][0m |          -0.0370 |           0.0196 |           0.0000 |
[32m[20221213 20:35:57 @agent_ppo2.py:185][0m |          -0.0360 |           0.0192 |           0.0000 |
[32m[20221213 20:35:58 @agent_ppo2.py:185][0m |          -0.0358 |           0.0189 |           0.0000 |
[32m[20221213 20:35:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:35:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.60
[32m[20221213 20:35:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.79
[32m[20221213 20:35:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.45
[32m[20221213 20:35:58 @agent_ppo2.py:143][0m Total time:       6.81 min
[32m[20221213 20:35:58 @agent_ppo2.py:145][0m 978944 total steps have happened
[32m[20221213 20:35:58 @agent_ppo2.py:121][0m #------------------------ Iteration 239 --------------------------#
[32m[20221213 20:35:58 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:35:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:35:58 @agent_ppo2.py:185][0m |           0.0299 |           0.0195 |           0.0000 |
[32m[20221213 20:35:59 @agent_ppo2.py:185][0m |           0.0023 |           0.0162 |           0.0000 |
[32m[20221213 20:35:59 @agent_ppo2.py:185][0m |          -0.0291 |           0.0156 |           0.0000 |
[32m[20221213 20:35:59 @agent_ppo2.py:185][0m |          -0.0461 |           0.0155 |           0.0000 |
[32m[20221213 20:35:59 @agent_ppo2.py:185][0m |          -0.0585 |           0.0151 |           0.0000 |
[32m[20221213 20:35:59 @agent_ppo2.py:185][0m |          -0.0685 |           0.0150 |           0.0000 |
[32m[20221213 20:35:59 @agent_ppo2.py:185][0m |          -0.0680 |           0.0148 |           0.0000 |
[32m[20221213 20:35:59 @agent_ppo2.py:185][0m |          -0.0718 |           0.0147 |           0.0000 |
[32m[20221213 20:35:59 @agent_ppo2.py:185][0m |          -0.0769 |           0.0146 |           0.0000 |
[32m[20221213 20:35:59 @agent_ppo2.py:185][0m |          -0.0767 |           0.0144 |           0.0000 |
[32m[20221213 20:35:59 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:36:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.93
[32m[20221213 20:36:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.72
[32m[20221213 20:36:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.75
[32m[20221213 20:36:00 @agent_ppo2.py:143][0m Total time:       6.84 min
[32m[20221213 20:36:00 @agent_ppo2.py:145][0m 983040 total steps have happened
[32m[20221213 20:36:00 @agent_ppo2.py:121][0m #------------------------ Iteration 240 --------------------------#
[32m[20221213 20:36:00 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:36:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:00 @agent_ppo2.py:185][0m |           0.0918 |           0.0340 |           0.0000 |
[32m[20221213 20:36:00 @agent_ppo2.py:185][0m |           0.0702 |           0.0243 |           0.0000 |
[32m[20221213 20:36:00 @agent_ppo2.py:185][0m |           0.0353 |           0.0209 |           0.0000 |
[32m[20221213 20:36:00 @agent_ppo2.py:185][0m |           0.0138 |           0.0193 |           0.0000 |
[32m[20221213 20:36:01 @agent_ppo2.py:185][0m |           0.0013 |           0.0193 |           0.0000 |
[32m[20221213 20:36:01 @agent_ppo2.py:185][0m |          -0.0115 |           0.0190 |           0.0000 |
[32m[20221213 20:36:01 @agent_ppo2.py:185][0m |          -0.0178 |           0.0178 |           0.0000 |
[32m[20221213 20:36:01 @agent_ppo2.py:185][0m |          -0.0217 |           0.0173 |           0.0000 |
[32m[20221213 20:36:01 @agent_ppo2.py:185][0m |          -0.0279 |           0.0171 |           0.0000 |
[32m[20221213 20:36:01 @agent_ppo2.py:185][0m |          -0.0283 |           0.0171 |           0.0000 |
[32m[20221213 20:36:01 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:36:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.21
[32m[20221213 20:36:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.47
[32m[20221213 20:36:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.77
[32m[20221213 20:36:01 @agent_ppo2.py:143][0m Total time:       6.87 min
[32m[20221213 20:36:01 @agent_ppo2.py:145][0m 987136 total steps have happened
[32m[20221213 20:36:01 @agent_ppo2.py:121][0m #------------------------ Iteration 241 --------------------------#
[32m[20221213 20:36:02 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:36:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:02 @agent_ppo2.py:185][0m |           0.0142 |           0.0245 |           0.0000 |
[32m[20221213 20:36:02 @agent_ppo2.py:185][0m |          -0.0028 |           0.0113 |           0.0000 |
[32m[20221213 20:36:02 @agent_ppo2.py:185][0m |          -0.0158 |           0.0109 |           0.0000 |
[32m[20221213 20:36:02 @agent_ppo2.py:185][0m |          -0.0289 |           0.0108 |           0.0000 |
[32m[20221213 20:36:02 @agent_ppo2.py:185][0m |          -0.0331 |           0.0107 |           0.0000 |
[32m[20221213 20:36:02 @agent_ppo2.py:185][0m |          -0.0381 |           0.0105 |           0.0000 |
[32m[20221213 20:36:03 @agent_ppo2.py:185][0m |          -0.0419 |           0.0103 |           0.0000 |
[32m[20221213 20:36:03 @agent_ppo2.py:185][0m |          -0.0468 |           0.0103 |           0.0000 |
[32m[20221213 20:36:03 @agent_ppo2.py:185][0m |          -0.0419 |           0.0102 |           0.0000 |
[32m[20221213 20:36:03 @agent_ppo2.py:185][0m |          -0.0520 |           0.0102 |           0.0000 |
[32m[20221213 20:36:03 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:36:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.12
[32m[20221213 20:36:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.03
[32m[20221213 20:36:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.71
[32m[20221213 20:36:03 @agent_ppo2.py:143][0m Total time:       6.90 min
[32m[20221213 20:36:03 @agent_ppo2.py:145][0m 991232 total steps have happened
[32m[20221213 20:36:03 @agent_ppo2.py:121][0m #------------------------ Iteration 242 --------------------------#
[32m[20221213 20:36:04 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:04 @agent_ppo2.py:185][0m |           0.0169 |           0.0134 |           0.0000 |
[32m[20221213 20:36:04 @agent_ppo2.py:185][0m |          -0.0074 |           0.0125 |           0.0000 |
[32m[20221213 20:36:04 @agent_ppo2.py:185][0m |          -0.0241 |           0.0122 |           0.0000 |
[32m[20221213 20:36:04 @agent_ppo2.py:185][0m |          -0.0160 |           0.0124 |           0.0000 |
[32m[20221213 20:36:04 @agent_ppo2.py:185][0m |          -0.0283 |           0.0127 |           0.0000 |
[32m[20221213 20:36:04 @agent_ppo2.py:185][0m |          -0.0435 |           0.0116 |           0.0000 |
[32m[20221213 20:36:04 @agent_ppo2.py:185][0m |          -0.0359 |           0.0114 |           0.0000 |
[32m[20221213 20:36:04 @agent_ppo2.py:185][0m |          -0.0460 |           0.0113 |           0.0000 |
[32m[20221213 20:36:04 @agent_ppo2.py:185][0m |          -0.0439 |           0.0117 |           0.0000 |
[32m[20221213 20:36:04 @agent_ppo2.py:185][0m |          -0.0523 |           0.0121 |           0.0000 |
[32m[20221213 20:36:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:36:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.79
[32m[20221213 20:36:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.79
[32m[20221213 20:36:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.09
[32m[20221213 20:36:05 @agent_ppo2.py:143][0m Total time:       6.93 min
[32m[20221213 20:36:05 @agent_ppo2.py:145][0m 995328 total steps have happened
[32m[20221213 20:36:05 @agent_ppo2.py:121][0m #------------------------ Iteration 243 --------------------------#
[32m[20221213 20:36:05 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:36:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:05 @agent_ppo2.py:185][0m |           0.0391 |           0.0122 |           0.0000 |
[32m[20221213 20:36:06 @agent_ppo2.py:185][0m |           0.0222 |           0.0111 |           0.0000 |
[32m[20221213 20:36:06 @agent_ppo2.py:185][0m |           0.0085 |           0.0108 |           0.0000 |
[32m[20221213 20:36:06 @agent_ppo2.py:185][0m |          -0.0091 |           0.0105 |           0.0000 |
[32m[20221213 20:36:06 @agent_ppo2.py:185][0m |          -0.0356 |           0.0104 |           0.0000 |
[32m[20221213 20:36:06 @agent_ppo2.py:185][0m |          -0.0117 |           0.0104 |           0.0000 |
[32m[20221213 20:36:06 @agent_ppo2.py:185][0m |          -0.0424 |           0.0102 |           0.0000 |
[32m[20221213 20:36:06 @agent_ppo2.py:185][0m |          -0.0448 |           0.0100 |           0.0000 |
[32m[20221213 20:36:06 @agent_ppo2.py:185][0m |          -0.0280 |           0.0104 |           0.0000 |
[32m[20221213 20:36:06 @agent_ppo2.py:185][0m |          -0.0558 |           0.0111 |           0.0000 |
[32m[20221213 20:36:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:36:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.99
[32m[20221213 20:36:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.99
[32m[20221213 20:36:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.07
[32m[20221213 20:36:07 @agent_ppo2.py:143][0m Total time:       6.96 min
[32m[20221213 20:36:07 @agent_ppo2.py:145][0m 999424 total steps have happened
[32m[20221213 20:36:07 @agent_ppo2.py:121][0m #------------------------ Iteration 244 --------------------------#
[32m[20221213 20:36:07 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:36:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:07 @agent_ppo2.py:185][0m |           0.0155 |           0.0317 |           0.0000 |
[32m[20221213 20:36:07 @agent_ppo2.py:185][0m |          -0.0017 |           0.0177 |           0.0000 |
[32m[20221213 20:36:07 @agent_ppo2.py:185][0m |          -0.0093 |           0.0153 |           0.0000 |
[32m[20221213 20:36:07 @agent_ppo2.py:185][0m |          -0.0185 |           0.0145 |           0.0000 |
[32m[20221213 20:36:08 @agent_ppo2.py:185][0m |          -0.0224 |           0.0143 |           0.0000 |
[32m[20221213 20:36:08 @agent_ppo2.py:185][0m |          -0.0249 |           0.0143 |           0.0000 |
[32m[20221213 20:36:08 @agent_ppo2.py:185][0m |          -0.0261 |           0.0139 |           0.0000 |
[32m[20221213 20:36:08 @agent_ppo2.py:185][0m |          -0.0282 |           0.0137 |           0.0000 |
[32m[20221213 20:36:08 @agent_ppo2.py:185][0m |          -0.0296 |           0.0134 |           0.0000 |
[32m[20221213 20:36:08 @agent_ppo2.py:185][0m |          -0.0312 |           0.0136 |           0.0000 |
[32m[20221213 20:36:08 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:36:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.26
[32m[20221213 20:36:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.74
[32m[20221213 20:36:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.12
[32m[20221213 20:36:08 @agent_ppo2.py:143][0m Total time:       6.99 min
[32m[20221213 20:36:08 @agent_ppo2.py:145][0m 1003520 total steps have happened
[32m[20221213 20:36:08 @agent_ppo2.py:121][0m #------------------------ Iteration 245 --------------------------#
[32m[20221213 20:36:09 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:36:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:09 @agent_ppo2.py:185][0m |           0.0491 |           0.0127 |           0.0000 |
[32m[20221213 20:36:09 @agent_ppo2.py:185][0m |           0.0036 |           0.0115 |           0.0000 |
[32m[20221213 20:36:09 @agent_ppo2.py:185][0m |          -0.0316 |           0.0111 |           0.0000 |
[32m[20221213 20:36:09 @agent_ppo2.py:185][0m |          -0.0554 |           0.0107 |           0.0000 |
[32m[20221213 20:36:09 @agent_ppo2.py:185][0m |          -0.0627 |           0.0107 |           0.0000 |
[32m[20221213 20:36:09 @agent_ppo2.py:185][0m |          -0.0597 |           0.0106 |           0.0000 |
[32m[20221213 20:36:09 @agent_ppo2.py:185][0m |          -0.0680 |           0.0106 |           0.0000 |
[32m[20221213 20:36:10 @agent_ppo2.py:185][0m |          -0.0732 |           0.0105 |           0.0000 |
[32m[20221213 20:36:10 @agent_ppo2.py:185][0m |          -0.0776 |           0.0103 |           0.0000 |
[32m[20221213 20:36:10 @agent_ppo2.py:185][0m |          -0.0656 |           0.0104 |           0.0000 |
[32m[20221213 20:36:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:36:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.76
[32m[20221213 20:36:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.17
[32m[20221213 20:36:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.31
[32m[20221213 20:36:10 @agent_ppo2.py:143][0m Total time:       7.01 min
[32m[20221213 20:36:10 @agent_ppo2.py:145][0m 1007616 total steps have happened
[32m[20221213 20:36:10 @agent_ppo2.py:121][0m #------------------------ Iteration 246 --------------------------#
[32m[20221213 20:36:10 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:11 @agent_ppo2.py:185][0m |           0.0267 |           0.0301 |           0.0000 |
[32m[20221213 20:36:11 @agent_ppo2.py:185][0m |           0.0038 |           0.0238 |           0.0000 |
[32m[20221213 20:36:11 @agent_ppo2.py:185][0m |          -0.0114 |           0.0207 |           0.0000 |
[32m[20221213 20:36:11 @agent_ppo2.py:185][0m |          -0.0228 |           0.0212 |           0.0000 |
[32m[20221213 20:36:11 @agent_ppo2.py:185][0m |          -0.0262 |           0.0176 |           0.0000 |
[32m[20221213 20:36:11 @agent_ppo2.py:185][0m |          -0.0273 |           0.0170 |           0.0000 |
[32m[20221213 20:36:11 @agent_ppo2.py:185][0m |          -0.0308 |           0.0163 |           0.0000 |
[32m[20221213 20:36:11 @agent_ppo2.py:185][0m |          -0.0319 |           0.0159 |           0.0000 |
[32m[20221213 20:36:11 @agent_ppo2.py:185][0m |          -0.0321 |           0.0160 |           0.0000 |
[32m[20221213 20:36:11 @agent_ppo2.py:185][0m |          -0.0318 |           0.0152 |           0.0000 |
[32m[20221213 20:36:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:36:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.25
[32m[20221213 20:36:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.04
[32m[20221213 20:36:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.06
[32m[20221213 20:36:12 @agent_ppo2.py:143][0m Total time:       7.04 min
[32m[20221213 20:36:12 @agent_ppo2.py:145][0m 1011712 total steps have happened
[32m[20221213 20:36:12 @agent_ppo2.py:121][0m #------------------------ Iteration 247 --------------------------#
[32m[20221213 20:36:12 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:12 @agent_ppo2.py:185][0m |           0.0317 |           0.0217 |           0.0000 |
[32m[20221213 20:36:12 @agent_ppo2.py:185][0m |           0.0008 |           0.0180 |           0.0000 |
[32m[20221213 20:36:12 @agent_ppo2.py:185][0m |          -0.0220 |           0.0174 |           0.0000 |
[32m[20221213 20:36:13 @agent_ppo2.py:185][0m |          -0.0094 |           0.0162 |           0.0000 |
[32m[20221213 20:36:13 @agent_ppo2.py:185][0m |          -0.0364 |           0.0161 |           0.0000 |
[32m[20221213 20:36:13 @agent_ppo2.py:185][0m |          -0.0440 |           0.0154 |           0.0000 |
[32m[20221213 20:36:13 @agent_ppo2.py:185][0m |          -0.0447 |           0.0150 |           0.0000 |
[32m[20221213 20:36:13 @agent_ppo2.py:185][0m |          -0.0379 |           0.0148 |           0.0000 |
[32m[20221213 20:36:13 @agent_ppo2.py:185][0m |          -0.0503 |           0.0148 |           0.0000 |
[32m[20221213 20:36:13 @agent_ppo2.py:185][0m |          -0.0515 |           0.0145 |           0.0000 |
[32m[20221213 20:36:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:36:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.72
[32m[20221213 20:36:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.79
[32m[20221213 20:36:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.39
[32m[20221213 20:36:13 @agent_ppo2.py:143][0m Total time:       7.07 min
[32m[20221213 20:36:13 @agent_ppo2.py:145][0m 1015808 total steps have happened
[32m[20221213 20:36:13 @agent_ppo2.py:121][0m #------------------------ Iteration 248 --------------------------#
[32m[20221213 20:36:14 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:14 @agent_ppo2.py:185][0m |           0.0589 |           0.0121 |           0.0000 |
[32m[20221213 20:36:14 @agent_ppo2.py:185][0m |           0.0303 |           0.0104 |           0.0000 |
[32m[20221213 20:36:14 @agent_ppo2.py:185][0m |           0.0008 |           0.0102 |           0.0000 |
[32m[20221213 20:36:14 @agent_ppo2.py:185][0m |          -0.0277 |           0.0102 |           0.0000 |
[32m[20221213 20:36:14 @agent_ppo2.py:185][0m |          -0.0330 |           0.0101 |           0.0000 |
[32m[20221213 20:36:14 @agent_ppo2.py:185][0m |          -0.0439 |           0.0100 |           0.0000 |
[32m[20221213 20:36:15 @agent_ppo2.py:185][0m |          -0.0564 |           0.0099 |           0.0000 |
[32m[20221213 20:36:15 @agent_ppo2.py:185][0m |          -0.0397 |           0.0100 |           0.0000 |
[32m[20221213 20:36:15 @agent_ppo2.py:185][0m |          -0.0556 |           0.0099 |           0.0000 |
[32m[20221213 20:36:15 @agent_ppo2.py:185][0m |          -0.0601 |           0.0098 |           0.0000 |
[32m[20221213 20:36:15 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:36:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.57
[32m[20221213 20:36:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.93
[32m[20221213 20:36:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.68
[32m[20221213 20:36:15 @agent_ppo2.py:143][0m Total time:       7.10 min
[32m[20221213 20:36:15 @agent_ppo2.py:145][0m 1019904 total steps have happened
[32m[20221213 20:36:15 @agent_ppo2.py:121][0m #------------------------ Iteration 249 --------------------------#
[32m[20221213 20:36:16 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:36:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:16 @agent_ppo2.py:185][0m |           0.0199 |           0.0182 |           0.0000 |
[32m[20221213 20:36:16 @agent_ppo2.py:185][0m |           0.0001 |           0.0137 |           0.0000 |
[32m[20221213 20:36:16 @agent_ppo2.py:185][0m |          -0.0330 |           0.0128 |           0.0000 |
[32m[20221213 20:36:16 @agent_ppo2.py:185][0m |           0.0218 |           0.0127 |           0.0000 |
[32m[20221213 20:36:16 @agent_ppo2.py:185][0m |           0.0097 |           0.0123 |           0.0000 |
[32m[20221213 20:36:16 @agent_ppo2.py:185][0m |          -0.0043 |           0.0121 |           0.0000 |
[32m[20221213 20:36:16 @agent_ppo2.py:185][0m |          -0.0105 |           0.0121 |           0.0000 |
[32m[20221213 20:36:16 @agent_ppo2.py:185][0m |          -0.0198 |           0.0119 |           0.0000 |
[32m[20221213 20:36:16 @agent_ppo2.py:185][0m |          -0.0232 |           0.0118 |           0.0000 |
[32m[20221213 20:36:17 @agent_ppo2.py:185][0m |          -0.0485 |           0.0120 |           0.0000 |
[32m[20221213 20:36:17 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:36:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.82
[32m[20221213 20:36:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.40
[32m[20221213 20:36:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.05
[32m[20221213 20:36:17 @agent_ppo2.py:143][0m Total time:       7.13 min
[32m[20221213 20:36:17 @agent_ppo2.py:145][0m 1024000 total steps have happened
[32m[20221213 20:36:17 @agent_ppo2.py:121][0m #------------------------ Iteration 250 --------------------------#
[32m[20221213 20:36:17 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:36:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:18 @agent_ppo2.py:185][0m |           0.0076 |           0.0126 |           0.0000 |
[32m[20221213 20:36:18 @agent_ppo2.py:185][0m |          -0.0074 |           0.0087 |           0.0000 |
[32m[20221213 20:36:18 @agent_ppo2.py:185][0m |          -0.0231 |           0.0085 |           0.0000 |
[32m[20221213 20:36:18 @agent_ppo2.py:185][0m |          -0.0295 |           0.0084 |           0.0000 |
[32m[20221213 20:36:18 @agent_ppo2.py:185][0m |          -0.0327 |           0.0083 |           0.0000 |
[32m[20221213 20:36:18 @agent_ppo2.py:185][0m |          -0.0452 |           0.0083 |           0.0000 |
[32m[20221213 20:36:18 @agent_ppo2.py:185][0m |          -0.0401 |           0.0082 |           0.0000 |
[32m[20221213 20:36:18 @agent_ppo2.py:185][0m |          -0.0441 |           0.0082 |           0.0000 |
[32m[20221213 20:36:18 @agent_ppo2.py:185][0m |          -0.0286 |           0.0081 |           0.0000 |
[32m[20221213 20:36:18 @agent_ppo2.py:185][0m |          -0.0252 |           0.0081 |           0.0000 |
[32m[20221213 20:36:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:36:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.16
[32m[20221213 20:36:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.45
[32m[20221213 20:36:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.61
[32m[20221213 20:36:19 @agent_ppo2.py:143][0m Total time:       7.16 min
[32m[20221213 20:36:19 @agent_ppo2.py:145][0m 1028096 total steps have happened
[32m[20221213 20:36:19 @agent_ppo2.py:121][0m #------------------------ Iteration 251 --------------------------#
[32m[20221213 20:36:19 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:19 @agent_ppo2.py:185][0m |           0.0270 |           0.0927 |           0.0000 |
[32m[20221213 20:36:19 @agent_ppo2.py:185][0m |          -0.0053 |           0.0332 |           0.0000 |
[32m[20221213 20:36:19 @agent_ppo2.py:185][0m |          -0.0105 |           0.0239 |           0.0000 |
[32m[20221213 20:36:19 @agent_ppo2.py:185][0m |          -0.0153 |           0.0226 |           0.0000 |
[32m[20221213 20:36:20 @agent_ppo2.py:185][0m |          -0.0188 |           0.0204 |           0.0000 |
[32m[20221213 20:36:20 @agent_ppo2.py:185][0m |          -0.0209 |           0.0198 |           0.0000 |
[32m[20221213 20:36:20 @agent_ppo2.py:185][0m |          -0.0218 |           0.0199 |           0.0000 |
[32m[20221213 20:36:20 @agent_ppo2.py:185][0m |          -0.0245 |           0.0185 |           0.0000 |
[32m[20221213 20:36:20 @agent_ppo2.py:185][0m |          -0.0267 |           0.0182 |           0.0000 |
[32m[20221213 20:36:20 @agent_ppo2.py:185][0m |          -0.0261 |           0.0185 |           0.0000 |
[32m[20221213 20:36:20 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:36:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.12
[32m[20221213 20:36:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.62
[32m[20221213 20:36:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.76
[32m[20221213 20:36:20 @agent_ppo2.py:143][0m Total time:       7.19 min
[32m[20221213 20:36:20 @agent_ppo2.py:145][0m 1032192 total steps have happened
[32m[20221213 20:36:20 @agent_ppo2.py:121][0m #------------------------ Iteration 252 --------------------------#
[32m[20221213 20:36:21 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:36:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:21 @agent_ppo2.py:185][0m |           0.0189 |           0.0246 |           0.0000 |
[32m[20221213 20:36:21 @agent_ppo2.py:185][0m |           0.0044 |           0.0112 |           0.0000 |
[32m[20221213 20:36:21 @agent_ppo2.py:185][0m |          -0.0116 |           0.0107 |           0.0000 |
[32m[20221213 20:36:21 @agent_ppo2.py:185][0m |          -0.0234 |           0.0104 |           0.0000 |
[32m[20221213 20:36:21 @agent_ppo2.py:185][0m |          -0.0380 |           0.0102 |           0.0000 |
[32m[20221213 20:36:21 @agent_ppo2.py:185][0m |          -0.0404 |           0.0101 |           0.0000 |
[32m[20221213 20:36:21 @agent_ppo2.py:185][0m |          -0.0462 |           0.0100 |           0.0000 |
[32m[20221213 20:36:21 @agent_ppo2.py:185][0m |          -0.0465 |           0.0099 |           0.0000 |
[32m[20221213 20:36:22 @agent_ppo2.py:185][0m |          -0.0506 |           0.0099 |           0.0000 |
[32m[20221213 20:36:22 @agent_ppo2.py:185][0m |          -0.0541 |           0.0098 |           0.0000 |
[32m[20221213 20:36:22 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:36:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.32
[32m[20221213 20:36:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.81
[32m[20221213 20:36:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.72
[32m[20221213 20:36:22 @agent_ppo2.py:143][0m Total time:       7.21 min
[32m[20221213 20:36:22 @agent_ppo2.py:145][0m 1036288 total steps have happened
[32m[20221213 20:36:22 @agent_ppo2.py:121][0m #------------------------ Iteration 253 --------------------------#
[32m[20221213 20:36:22 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:23 @agent_ppo2.py:185][0m |           0.0225 |           0.0346 |           0.0000 |
[32m[20221213 20:36:23 @agent_ppo2.py:185][0m |           0.0074 |           0.0222 |           0.0000 |
[32m[20221213 20:36:23 @agent_ppo2.py:185][0m |          -0.0120 |           0.0198 |           0.0000 |
[32m[20221213 20:36:23 @agent_ppo2.py:185][0m |          -0.0173 |           0.0186 |           0.0000 |
[32m[20221213 20:36:23 @agent_ppo2.py:185][0m |          -0.0251 |           0.0178 |           0.0000 |
[32m[20221213 20:36:23 @agent_ppo2.py:185][0m |          -0.0298 |           0.0172 |           0.0000 |
[32m[20221213 20:36:23 @agent_ppo2.py:185][0m |          -0.0294 |           0.0170 |           0.0000 |
[32m[20221213 20:36:23 @agent_ppo2.py:185][0m |          -0.0307 |           0.0165 |           0.0000 |
[32m[20221213 20:36:23 @agent_ppo2.py:185][0m |          -0.0332 |           0.0166 |           0.0000 |
[32m[20221213 20:36:23 @agent_ppo2.py:185][0m |          -0.0345 |           0.0163 |           0.0000 |
[32m[20221213 20:36:23 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:36:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.61
[32m[20221213 20:36:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.87
[32m[20221213 20:36:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.40
[32m[20221213 20:36:24 @agent_ppo2.py:143][0m Total time:       7.24 min
[32m[20221213 20:36:24 @agent_ppo2.py:145][0m 1040384 total steps have happened
[32m[20221213 20:36:24 @agent_ppo2.py:121][0m #------------------------ Iteration 254 --------------------------#
[32m[20221213 20:36:24 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:36:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:24 @agent_ppo2.py:185][0m |           0.0297 |           0.0320 |           0.0000 |
[32m[20221213 20:36:25 @agent_ppo2.py:185][0m |          -0.0039 |           0.0230 |           0.0000 |
[32m[20221213 20:36:25 @agent_ppo2.py:185][0m |          -0.0221 |           0.0218 |           0.0000 |
[32m[20221213 20:36:25 @agent_ppo2.py:185][0m |          -0.0284 |           0.0208 |           0.0000 |
[32m[20221213 20:36:25 @agent_ppo2.py:185][0m |          -0.0282 |           0.0209 |           0.0000 |
[32m[20221213 20:36:25 @agent_ppo2.py:185][0m |          -0.0351 |           0.0201 |           0.0000 |
[32m[20221213 20:36:25 @agent_ppo2.py:185][0m |          -0.0393 |           0.0194 |           0.0000 |
[32m[20221213 20:36:25 @agent_ppo2.py:185][0m |          -0.0396 |           0.0201 |           0.0000 |
[32m[20221213 20:36:25 @agent_ppo2.py:185][0m |          -0.0472 |           0.0192 |           0.0000 |
[32m[20221213 20:36:25 @agent_ppo2.py:185][0m |          -0.0468 |           0.0184 |           0.0000 |
[32m[20221213 20:36:25 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:36:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.20
[32m[20221213 20:36:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.25
[32m[20221213 20:36:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.72
[32m[20221213 20:36:26 @agent_ppo2.py:143][0m Total time:       7.27 min
[32m[20221213 20:36:26 @agent_ppo2.py:145][0m 1044480 total steps have happened
[32m[20221213 20:36:26 @agent_ppo2.py:121][0m #------------------------ Iteration 255 --------------------------#
[32m[20221213 20:36:26 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:36:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:26 @agent_ppo2.py:185][0m |           0.0279 |           0.0242 |           0.0000 |
[32m[20221213 20:36:26 @agent_ppo2.py:185][0m |           0.0196 |           0.0141 |           0.0000 |
[32m[20221213 20:36:26 @agent_ppo2.py:185][0m |          -0.0059 |           0.0135 |           0.0000 |
[32m[20221213 20:36:26 @agent_ppo2.py:185][0m |          -0.0143 |           0.0133 |           0.0000 |
[32m[20221213 20:36:26 @agent_ppo2.py:185][0m |          -0.0236 |           0.0132 |           0.0000 |
[32m[20221213 20:36:27 @agent_ppo2.py:185][0m |          -0.0315 |           0.0130 |           0.0000 |
[32m[20221213 20:36:27 @agent_ppo2.py:185][0m |          -0.0374 |           0.0127 |           0.0000 |
[32m[20221213 20:36:27 @agent_ppo2.py:185][0m |          -0.0519 |           0.0124 |           0.0000 |
[32m[20221213 20:36:27 @agent_ppo2.py:185][0m |          -0.0457 |           0.0122 |           0.0000 |
[32m[20221213 20:36:27 @agent_ppo2.py:185][0m |          -0.0496 |           0.0122 |           0.0000 |
[32m[20221213 20:36:27 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:36:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.22
[32m[20221213 20:36:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.82
[32m[20221213 20:36:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.28
[32m[20221213 20:36:27 @agent_ppo2.py:143][0m Total time:       7.30 min
[32m[20221213 20:36:27 @agent_ppo2.py:145][0m 1048576 total steps have happened
[32m[20221213 20:36:27 @agent_ppo2.py:121][0m #------------------------ Iteration 256 --------------------------#
[32m[20221213 20:36:28 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:28 @agent_ppo2.py:185][0m |           0.0072 |           0.0175 |           0.0000 |
[32m[20221213 20:36:28 @agent_ppo2.py:185][0m |          -0.0127 |           0.0096 |           0.0000 |
[32m[20221213 20:36:28 @agent_ppo2.py:185][0m |           0.0114 |           0.0086 |           0.0000 |
[32m[20221213 20:36:28 @agent_ppo2.py:185][0m |          -0.0314 |           0.0079 |           0.0000 |
[32m[20221213 20:36:28 @agent_ppo2.py:185][0m |          -0.0349 |           0.0076 |           0.0000 |
[32m[20221213 20:36:28 @agent_ppo2.py:185][0m |          -0.0391 |           0.0075 |           0.0000 |
[32m[20221213 20:36:28 @agent_ppo2.py:185][0m |          -0.0392 |           0.0073 |           0.0000 |
[32m[20221213 20:36:28 @agent_ppo2.py:185][0m |          -0.0387 |           0.0073 |           0.0000 |
[32m[20221213 20:36:29 @agent_ppo2.py:185][0m |          -0.0371 |           0.0072 |           0.0000 |
[32m[20221213 20:36:29 @agent_ppo2.py:185][0m |          -0.0410 |           0.0071 |           0.0000 |
[32m[20221213 20:36:29 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:36:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.07
[32m[20221213 20:36:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.20
[32m[20221213 20:36:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.30
[32m[20221213 20:36:29 @agent_ppo2.py:143][0m Total time:       7.33 min
[32m[20221213 20:36:29 @agent_ppo2.py:145][0m 1052672 total steps have happened
[32m[20221213 20:36:29 @agent_ppo2.py:121][0m #------------------------ Iteration 257 --------------------------#
[32m[20221213 20:36:29 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:36:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:30 @agent_ppo2.py:185][0m |           0.0305 |           0.0231 |           0.0000 |
[32m[20221213 20:36:30 @agent_ppo2.py:185][0m |           0.0077 |           0.0121 |           0.0000 |
[32m[20221213 20:36:30 @agent_ppo2.py:185][0m |          -0.0139 |           0.0108 |           0.0000 |
[32m[20221213 20:36:30 @agent_ppo2.py:185][0m |          -0.0185 |           0.0104 |           0.0000 |
[32m[20221213 20:36:30 @agent_ppo2.py:185][0m |          -0.0222 |           0.0103 |           0.0000 |
[32m[20221213 20:36:30 @agent_ppo2.py:185][0m |          -0.0240 |           0.0101 |           0.0000 |
[32m[20221213 20:36:30 @agent_ppo2.py:185][0m |          -0.0253 |           0.0099 |           0.0000 |
[32m[20221213 20:36:30 @agent_ppo2.py:185][0m |          -0.0256 |           0.0098 |           0.0000 |
[32m[20221213 20:36:30 @agent_ppo2.py:185][0m |          -0.0276 |           0.0097 |           0.0000 |
[32m[20221213 20:36:30 @agent_ppo2.py:185][0m |          -0.0280 |           0.0096 |           0.0000 |
[32m[20221213 20:36:30 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:36:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.86
[32m[20221213 20:36:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.13
[32m[20221213 20:36:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.62
[32m[20221213 20:36:31 @agent_ppo2.py:143][0m Total time:       7.36 min
[32m[20221213 20:36:31 @agent_ppo2.py:145][0m 1056768 total steps have happened
[32m[20221213 20:36:31 @agent_ppo2.py:121][0m #------------------------ Iteration 258 --------------------------#
[32m[20221213 20:36:31 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:31 @agent_ppo2.py:185][0m |           0.0179 |           0.0438 |           0.0000 |
[32m[20221213 20:36:31 @agent_ppo2.py:185][0m |           0.0033 |           0.0246 |           0.0000 |
[32m[20221213 20:36:31 @agent_ppo2.py:185][0m |          -0.0073 |           0.0207 |           0.0000 |
[32m[20221213 20:36:32 @agent_ppo2.py:185][0m |          -0.0087 |           0.0190 |           0.0000 |
[32m[20221213 20:36:32 @agent_ppo2.py:185][0m |          -0.0117 |           0.0186 |           0.0000 |
[32m[20221213 20:36:32 @agent_ppo2.py:185][0m |          -0.0625 |           0.0190 |           0.0000 |
[32m[20221213 20:36:32 @agent_ppo2.py:185][0m |          -0.0244 |           0.0202 |           0.0000 |
[32m[20221213 20:36:32 @agent_ppo2.py:185][0m |          -0.0266 |           0.0178 |           0.0000 |
[32m[20221213 20:36:32 @agent_ppo2.py:185][0m |          -0.0287 |           0.0172 |           0.0000 |
[32m[20221213 20:36:32 @agent_ppo2.py:185][0m |          -0.0289 |           0.0168 |           0.0000 |
[32m[20221213 20:36:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:36:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.30
[32m[20221213 20:36:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.80
[32m[20221213 20:36:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.74
[32m[20221213 20:36:32 @agent_ppo2.py:143][0m Total time:       7.39 min
[32m[20221213 20:36:32 @agent_ppo2.py:145][0m 1060864 total steps have happened
[32m[20221213 20:36:32 @agent_ppo2.py:121][0m #------------------------ Iteration 259 --------------------------#
[32m[20221213 20:36:33 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:33 @agent_ppo2.py:185][0m |           0.0388 |           0.0168 |           0.0000 |
[32m[20221213 20:36:33 @agent_ppo2.py:185][0m |           0.0238 |           0.0134 |           0.0000 |
[32m[20221213 20:36:33 @agent_ppo2.py:185][0m |          -0.0177 |           0.0131 |           0.0000 |
[32m[20221213 20:36:33 @agent_ppo2.py:185][0m |          -0.0211 |           0.0130 |           0.0000 |
[32m[20221213 20:36:33 @agent_ppo2.py:185][0m |          -0.0595 |           0.0126 |           0.0000 |
[32m[20221213 20:36:33 @agent_ppo2.py:185][0m |          -0.0526 |           0.0123 |           0.0000 |
[32m[20221213 20:36:33 @agent_ppo2.py:185][0m |          -0.0529 |           0.0122 |           0.0000 |
[32m[20221213 20:36:34 @agent_ppo2.py:185][0m |          -0.0681 |           0.0121 |           0.0000 |
[32m[20221213 20:36:34 @agent_ppo2.py:185][0m |          -0.0789 |           0.0120 |           0.0000 |
[32m[20221213 20:36:34 @agent_ppo2.py:185][0m |          -0.0745 |           0.0119 |           0.0000 |
[32m[20221213 20:36:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:36:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.16
[32m[20221213 20:36:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.94
[32m[20221213 20:36:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.36
[32m[20221213 20:36:34 @agent_ppo2.py:143][0m Total time:       7.42 min
[32m[20221213 20:36:34 @agent_ppo2.py:145][0m 1064960 total steps have happened
[32m[20221213 20:36:34 @agent_ppo2.py:121][0m #------------------------ Iteration 260 --------------------------#
[32m[20221213 20:36:34 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:36:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:35 @agent_ppo2.py:185][0m |           0.0379 |           0.0225 |           0.0000 |
[32m[20221213 20:36:35 @agent_ppo2.py:185][0m |           0.0036 |           0.0176 |           0.0000 |
[32m[20221213 20:36:35 @agent_ppo2.py:185][0m |          -0.0220 |           0.0165 |           0.0000 |
[32m[20221213 20:36:35 @agent_ppo2.py:185][0m |          -0.0288 |           0.0156 |           0.0000 |
[32m[20221213 20:36:35 @agent_ppo2.py:185][0m |          -0.0336 |           0.0153 |           0.0000 |
[32m[20221213 20:36:35 @agent_ppo2.py:185][0m |          -0.0364 |           0.0151 |           0.0000 |
[32m[20221213 20:36:35 @agent_ppo2.py:185][0m |          -0.0458 |           0.0149 |           0.0000 |
[32m[20221213 20:36:35 @agent_ppo2.py:185][0m |          -0.0411 |           0.0147 |           0.0000 |
[32m[20221213 20:36:35 @agent_ppo2.py:185][0m |          -0.0433 |           0.0144 |           0.0000 |
[32m[20221213 20:36:35 @agent_ppo2.py:185][0m |          -0.0447 |           0.0143 |           0.0000 |
[32m[20221213 20:36:35 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:36:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.97
[32m[20221213 20:36:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.54
[32m[20221213 20:36:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.71
[32m[20221213 20:36:36 @agent_ppo2.py:143][0m Total time:       7.44 min
[32m[20221213 20:36:36 @agent_ppo2.py:145][0m 1069056 total steps have happened
[32m[20221213 20:36:36 @agent_ppo2.py:121][0m #------------------------ Iteration 261 --------------------------#
[32m[20221213 20:36:36 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:36 @agent_ppo2.py:185][0m |           0.0347 |           0.0169 |           0.0000 |
[32m[20221213 20:36:36 @agent_ppo2.py:185][0m |           0.0624 |           0.0113 |           0.0000 |
[32m[20221213 20:36:37 @agent_ppo2.py:185][0m |           0.0236 |           0.0110 |           0.0000 |
[32m[20221213 20:36:37 @agent_ppo2.py:185][0m |           0.0344 |           0.0112 |           0.0000 |
[32m[20221213 20:36:37 @agent_ppo2.py:185][0m |           0.0181 |           0.0111 |           0.0000 |
[32m[20221213 20:36:37 @agent_ppo2.py:185][0m |          -0.0125 |           0.0106 |           0.0000 |
[32m[20221213 20:36:37 @agent_ppo2.py:185][0m |          -0.0169 |           0.0105 |           0.0000 |
[32m[20221213 20:36:37 @agent_ppo2.py:185][0m |          -0.0104 |           0.0105 |           0.0000 |
[32m[20221213 20:36:37 @agent_ppo2.py:185][0m |          -0.0318 |           0.0103 |           0.0000 |
[32m[20221213 20:36:37 @agent_ppo2.py:185][0m |          -0.0345 |           0.0101 |           0.0000 |
[32m[20221213 20:36:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:36:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.46
[32m[20221213 20:36:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.90
[32m[20221213 20:36:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.81
[32m[20221213 20:36:37 @agent_ppo2.py:143][0m Total time:       7.47 min
[32m[20221213 20:36:37 @agent_ppo2.py:145][0m 1073152 total steps have happened
[32m[20221213 20:36:37 @agent_ppo2.py:121][0m #------------------------ Iteration 262 --------------------------#
[32m[20221213 20:36:38 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:38 @agent_ppo2.py:185][0m |           0.0236 |           0.0154 |           0.0000 |
[32m[20221213 20:36:38 @agent_ppo2.py:185][0m |          -0.0019 |           0.0141 |           0.0000 |
[32m[20221213 20:36:38 @agent_ppo2.py:185][0m |          -0.0175 |           0.0136 |           0.0000 |
[32m[20221213 20:36:38 @agent_ppo2.py:185][0m |          -0.0240 |           0.0133 |           0.0000 |
[32m[20221213 20:36:38 @agent_ppo2.py:185][0m |          -0.0309 |           0.0130 |           0.0000 |
[32m[20221213 20:36:38 @agent_ppo2.py:185][0m |          -0.0331 |           0.0128 |           0.0000 |
[32m[20221213 20:36:39 @agent_ppo2.py:185][0m |          -0.0337 |           0.0127 |           0.0000 |
[32m[20221213 20:36:39 @agent_ppo2.py:185][0m |          -0.0387 |           0.0125 |           0.0000 |
[32m[20221213 20:36:39 @agent_ppo2.py:185][0m |          -0.0393 |           0.0122 |           0.0000 |
[32m[20221213 20:36:39 @agent_ppo2.py:185][0m |          -0.0595 |           0.0122 |           0.0000 |
[32m[20221213 20:36:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:36:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.84
[32m[20221213 20:36:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.37
[32m[20221213 20:36:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.07
[32m[20221213 20:36:39 @agent_ppo2.py:143][0m Total time:       7.50 min
[32m[20221213 20:36:39 @agent_ppo2.py:145][0m 1077248 total steps have happened
[32m[20221213 20:36:39 @agent_ppo2.py:121][0m #------------------------ Iteration 263 --------------------------#
[32m[20221213 20:36:40 @agent_ppo2.py:127][0m Sampling time: 0.43 s by 5 slaves
[32m[20221213 20:36:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:40 @agent_ppo2.py:185][0m |           0.0314 |           0.0130 |           0.0000 |
[32m[20221213 20:36:40 @agent_ppo2.py:185][0m |          -0.0067 |           0.0126 |           0.0000 |
[32m[20221213 20:36:40 @agent_ppo2.py:185][0m |          -0.0466 |           0.0125 |           0.0000 |
[32m[20221213 20:36:40 @agent_ppo2.py:185][0m |          -0.0425 |           0.0122 |           0.0000 |
[32m[20221213 20:36:40 @agent_ppo2.py:185][0m |          -0.0740 |           0.0122 |           0.0000 |
[32m[20221213 20:36:40 @agent_ppo2.py:185][0m |          -0.0640 |           0.0120 |           0.0000 |
[32m[20221213 20:36:40 @agent_ppo2.py:185][0m |          -0.0694 |           0.0117 |           0.0000 |
[32m[20221213 20:36:40 @agent_ppo2.py:185][0m |          -0.0674 |           0.0117 |           0.0000 |
[32m[20221213 20:36:41 @agent_ppo2.py:185][0m |          -0.0727 |           0.0114 |           0.0000 |
[32m[20221213 20:36:41 @agent_ppo2.py:185][0m |          -0.0802 |           0.0114 |           0.0000 |
[32m[20221213 20:36:41 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:36:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.74
[32m[20221213 20:36:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.18
[32m[20221213 20:36:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.26
[32m[20221213 20:36:41 @agent_ppo2.py:143][0m Total time:       7.53 min
[32m[20221213 20:36:41 @agent_ppo2.py:145][0m 1081344 total steps have happened
[32m[20221213 20:36:41 @agent_ppo2.py:121][0m #------------------------ Iteration 264 --------------------------#
[32m[20221213 20:36:41 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:42 @agent_ppo2.py:185][0m |           0.0194 |           0.0356 |           0.0000 |
[32m[20221213 20:36:42 @agent_ppo2.py:185][0m |          -0.0102 |           0.0239 |           0.0000 |
[32m[20221213 20:36:42 @agent_ppo2.py:185][0m |          -0.0152 |           0.0234 |           0.0000 |
[32m[20221213 20:36:42 @agent_ppo2.py:185][0m |          -0.0229 |           0.0249 |           0.0000 |
[32m[20221213 20:36:42 @agent_ppo2.py:185][0m |          -0.0273 |           0.0214 |           0.0000 |
[32m[20221213 20:36:42 @agent_ppo2.py:185][0m |          -0.0306 |           0.0209 |           0.0000 |
[32m[20221213 20:36:42 @agent_ppo2.py:185][0m |          -0.0334 |           0.0210 |           0.0000 |
[32m[20221213 20:36:42 @agent_ppo2.py:185][0m |          -0.0338 |           0.0204 |           0.0000 |
[32m[20221213 20:36:42 @agent_ppo2.py:185][0m |          -0.0360 |           0.0196 |           0.0000 |
[32m[20221213 20:36:42 @agent_ppo2.py:185][0m |          -0.0361 |           0.0197 |           0.0000 |
[32m[20221213 20:36:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:36:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.90
[32m[20221213 20:36:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.29
[32m[20221213 20:36:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.44
[32m[20221213 20:36:43 @agent_ppo2.py:143][0m Total time:       7.56 min
[32m[20221213 20:36:43 @agent_ppo2.py:145][0m 1085440 total steps have happened
[32m[20221213 20:36:43 @agent_ppo2.py:121][0m #------------------------ Iteration 265 --------------------------#
[32m[20221213 20:36:43 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:36:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:43 @agent_ppo2.py:185][0m |           0.0392 |           0.0213 |           0.0000 |
[32m[20221213 20:36:43 @agent_ppo2.py:185][0m |           0.0122 |           0.0096 |           0.0000 |
[32m[20221213 20:36:43 @agent_ppo2.py:185][0m |          -0.0032 |           0.0089 |           0.0000 |
[32m[20221213 20:36:44 @agent_ppo2.py:185][0m |          -0.0165 |           0.0086 |           0.0000 |
[32m[20221213 20:36:44 @agent_ppo2.py:185][0m |          -0.0268 |           0.0085 |           0.0000 |
[32m[20221213 20:36:44 @agent_ppo2.py:185][0m |          -0.0296 |           0.0084 |           0.0000 |
[32m[20221213 20:36:44 @agent_ppo2.py:185][0m |          -0.0154 |           0.0083 |           0.0000 |
[32m[20221213 20:36:44 @agent_ppo2.py:185][0m |          -0.0421 |           0.0082 |           0.0000 |
[32m[20221213 20:36:44 @agent_ppo2.py:185][0m |          -0.0396 |           0.0081 |           0.0000 |
[32m[20221213 20:36:44 @agent_ppo2.py:185][0m |          -0.0049 |           0.0082 |           0.0000 |
[32m[20221213 20:36:44 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:36:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.13
[32m[20221213 20:36:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.28
[32m[20221213 20:36:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.01
[32m[20221213 20:36:44 @agent_ppo2.py:143][0m Total time:       7.59 min
[32m[20221213 20:36:44 @agent_ppo2.py:145][0m 1089536 total steps have happened
[32m[20221213 20:36:44 @agent_ppo2.py:121][0m #------------------------ Iteration 266 --------------------------#
[32m[20221213 20:36:45 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:45 @agent_ppo2.py:185][0m |           0.0391 |           0.0095 |           0.0000 |
[32m[20221213 20:36:45 @agent_ppo2.py:185][0m |           0.0118 |           0.0093 |           0.0000 |
[32m[20221213 20:36:45 @agent_ppo2.py:185][0m |          -0.0147 |           0.0091 |           0.0000 |
[32m[20221213 20:36:45 @agent_ppo2.py:185][0m |          -0.0150 |           0.0090 |           0.0000 |
[32m[20221213 20:36:45 @agent_ppo2.py:185][0m |          -0.0255 |           0.0088 |           0.0000 |
[32m[20221213 20:36:45 @agent_ppo2.py:185][0m |          -0.0382 |           0.0087 |           0.0000 |
[32m[20221213 20:36:46 @agent_ppo2.py:185][0m |          -0.0366 |           0.0086 |           0.0000 |
[32m[20221213 20:36:46 @agent_ppo2.py:185][0m |          -0.0400 |           0.0085 |           0.0000 |
[32m[20221213 20:36:46 @agent_ppo2.py:185][0m |          -0.0485 |           0.0084 |           0.0000 |
[32m[20221213 20:36:46 @agent_ppo2.py:185][0m |          -0.0481 |           0.0084 |           0.0000 |
[32m[20221213 20:36:46 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:36:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.79
[32m[20221213 20:36:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.37
[32m[20221213 20:36:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.07
[32m[20221213 20:36:46 @agent_ppo2.py:143][0m Total time:       7.62 min
[32m[20221213 20:36:46 @agent_ppo2.py:145][0m 1093632 total steps have happened
[32m[20221213 20:36:46 @agent_ppo2.py:121][0m #------------------------ Iteration 267 --------------------------#
[32m[20221213 20:36:47 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:47 @agent_ppo2.py:185][0m |           0.0538 |           0.0087 |           0.0000 |
[32m[20221213 20:36:47 @agent_ppo2.py:185][0m |           0.0407 |           0.0078 |           0.0000 |
[32m[20221213 20:36:47 @agent_ppo2.py:185][0m |           0.0030 |           0.0076 |           0.0000 |
[32m[20221213 20:36:47 @agent_ppo2.py:185][0m |          -0.0153 |           0.0075 |           0.0000 |
[32m[20221213 20:36:47 @agent_ppo2.py:185][0m |          -0.0277 |           0.0074 |           0.0000 |
[32m[20221213 20:36:47 @agent_ppo2.py:185][0m |          -0.0298 |           0.0074 |           0.0000 |
[32m[20221213 20:36:47 @agent_ppo2.py:185][0m |          -0.0408 |           0.0073 |           0.0000 |
[32m[20221213 20:36:47 @agent_ppo2.py:185][0m |          -0.0383 |           0.0073 |           0.0000 |
[32m[20221213 20:36:47 @agent_ppo2.py:185][0m |          -0.0460 |           0.0072 |           0.0000 |
[32m[20221213 20:36:47 @agent_ppo2.py:185][0m |          -0.0514 |           0.0071 |           0.0000 |
[32m[20221213 20:36:47 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:36:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.81
[32m[20221213 20:36:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.29
[32m[20221213 20:36:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.55
[32m[20221213 20:36:48 @agent_ppo2.py:143][0m Total time:       7.64 min
[32m[20221213 20:36:48 @agent_ppo2.py:145][0m 1097728 total steps have happened
[32m[20221213 20:36:48 @agent_ppo2.py:121][0m #------------------------ Iteration 268 --------------------------#
[32m[20221213 20:36:48 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:36:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:48 @agent_ppo2.py:185][0m |           0.0212 |           0.0340 |           0.0000 |
[32m[20221213 20:36:48 @agent_ppo2.py:185][0m |           0.0100 |           0.0151 |           0.0000 |
[32m[20221213 20:36:49 @agent_ppo2.py:185][0m |          -0.0067 |           0.0130 |           0.0000 |
[32m[20221213 20:36:49 @agent_ppo2.py:185][0m |          -0.0172 |           0.0127 |           0.0000 |
[32m[20221213 20:36:49 @agent_ppo2.py:185][0m |          -0.0634 |           0.0123 |           0.0000 |
[32m[20221213 20:36:49 @agent_ppo2.py:185][0m |          -0.0235 |           0.0122 |           0.0000 |
[32m[20221213 20:36:49 @agent_ppo2.py:185][0m |          -0.0250 |           0.0119 |           0.0000 |
[32m[20221213 20:36:49 @agent_ppo2.py:185][0m |          -0.0258 |           0.0118 |           0.0000 |
[32m[20221213 20:36:49 @agent_ppo2.py:185][0m |          -0.0255 |           0.0115 |           0.0000 |
[32m[20221213 20:36:49 @agent_ppo2.py:185][0m |          -0.0227 |           0.0115 |           0.0000 |
[32m[20221213 20:36:49 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:36:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.82
[32m[20221213 20:36:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.28
[32m[20221213 20:36:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.44
[32m[20221213 20:36:50 @agent_ppo2.py:143][0m Total time:       7.67 min
[32m[20221213 20:36:50 @agent_ppo2.py:145][0m 1101824 total steps have happened
[32m[20221213 20:36:50 @agent_ppo2.py:121][0m #------------------------ Iteration 269 --------------------------#
[32m[20221213 20:36:50 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:36:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:50 @agent_ppo2.py:185][0m |           0.0303 |           0.0157 |           0.0000 |
[32m[20221213 20:36:50 @agent_ppo2.py:185][0m |          -0.0004 |           0.0140 |           0.0000 |
[32m[20221213 20:36:50 @agent_ppo2.py:185][0m |          -0.0218 |           0.0133 |           0.0000 |
[32m[20221213 20:36:50 @agent_ppo2.py:185][0m |          -0.0547 |           0.0129 |           0.0000 |
[32m[20221213 20:36:50 @agent_ppo2.py:185][0m |          -0.0400 |           0.0126 |           0.0000 |
[32m[20221213 20:36:51 @agent_ppo2.py:185][0m |          -0.0527 |           0.0125 |           0.0000 |
[32m[20221213 20:36:51 @agent_ppo2.py:185][0m |          -0.0448 |           0.0122 |           0.0000 |
[32m[20221213 20:36:51 @agent_ppo2.py:185][0m |          -0.0508 |           0.0122 |           0.0000 |
[32m[20221213 20:36:51 @agent_ppo2.py:185][0m |          -0.0483 |           0.0120 |           0.0000 |
[32m[20221213 20:36:51 @agent_ppo2.py:185][0m |          -0.0480 |           0.0119 |           0.0000 |
[32m[20221213 20:36:51 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:36:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.58
[32m[20221213 20:36:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.03
[32m[20221213 20:36:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.71
[32m[20221213 20:36:51 @agent_ppo2.py:143][0m Total time:       7.70 min
[32m[20221213 20:36:51 @agent_ppo2.py:145][0m 1105920 total steps have happened
[32m[20221213 20:36:51 @agent_ppo2.py:121][0m #------------------------ Iteration 270 --------------------------#
[32m[20221213 20:36:52 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:36:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:52 @agent_ppo2.py:185][0m |           0.0230 |           0.0194 |           0.0000 |
[32m[20221213 20:36:52 @agent_ppo2.py:185][0m |           0.0044 |           0.0167 |           0.0000 |
[32m[20221213 20:36:52 @agent_ppo2.py:185][0m |          -0.0252 |           0.0159 |           0.0000 |
[32m[20221213 20:36:52 @agent_ppo2.py:185][0m |          -0.0589 |           0.0153 |           0.0000 |
[32m[20221213 20:36:52 @agent_ppo2.py:185][0m |          -0.0425 |           0.0151 |           0.0000 |
[32m[20221213 20:36:52 @agent_ppo2.py:185][0m |          -0.0480 |           0.0149 |           0.0000 |
[32m[20221213 20:36:52 @agent_ppo2.py:185][0m |          -0.0426 |           0.0151 |           0.0000 |
[32m[20221213 20:36:52 @agent_ppo2.py:185][0m |          -0.0464 |           0.0147 |           0.0000 |
[32m[20221213 20:36:52 @agent_ppo2.py:185][0m |          -0.0639 |           0.0151 |           0.0000 |
[32m[20221213 20:36:53 @agent_ppo2.py:185][0m |          -0.0571 |           0.0149 |           0.0000 |
[32m[20221213 20:36:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:36:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.64
[32m[20221213 20:36:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.83
[32m[20221213 20:36:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.80
[32m[20221213 20:36:53 @agent_ppo2.py:143][0m Total time:       7.73 min
[32m[20221213 20:36:53 @agent_ppo2.py:145][0m 1110016 total steps have happened
[32m[20221213 20:36:53 @agent_ppo2.py:121][0m #------------------------ Iteration 271 --------------------------#
[32m[20221213 20:36:53 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:36:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:54 @agent_ppo2.py:185][0m |           0.0257 |           0.0202 |           0.0000 |
[32m[20221213 20:36:54 @agent_ppo2.py:185][0m |          -0.0005 |           0.0156 |           0.0000 |
[32m[20221213 20:36:54 @agent_ppo2.py:185][0m |          -0.0168 |           0.0148 |           0.0000 |
[32m[20221213 20:36:54 @agent_ppo2.py:185][0m |          -0.0310 |           0.0141 |           0.0000 |
[32m[20221213 20:36:54 @agent_ppo2.py:185][0m |          -0.0396 |           0.0139 |           0.0000 |
[32m[20221213 20:36:54 @agent_ppo2.py:185][0m |          -0.0489 |           0.0133 |           0.0000 |
[32m[20221213 20:36:54 @agent_ppo2.py:185][0m |          -0.0490 |           0.0130 |           0.0000 |
[32m[20221213 20:36:54 @agent_ppo2.py:185][0m |          -0.0555 |           0.0128 |           0.0000 |
[32m[20221213 20:36:54 @agent_ppo2.py:185][0m |          -0.0544 |           0.0128 |           0.0000 |
[32m[20221213 20:36:54 @agent_ppo2.py:185][0m |          -0.0216 |           0.0126 |           0.0000 |
[32m[20221213 20:36:54 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:36:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.24
[32m[20221213 20:36:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.83
[32m[20221213 20:36:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.34
[32m[20221213 20:36:55 @agent_ppo2.py:143][0m Total time:       7.76 min
[32m[20221213 20:36:55 @agent_ppo2.py:145][0m 1114112 total steps have happened
[32m[20221213 20:36:55 @agent_ppo2.py:121][0m #------------------------ Iteration 272 --------------------------#
[32m[20221213 20:36:55 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:55 @agent_ppo2.py:185][0m |           0.0410 |           0.0145 |           0.0000 |
[32m[20221213 20:36:55 @agent_ppo2.py:185][0m |           0.0184 |           0.0137 |           0.0000 |
[32m[20221213 20:36:55 @agent_ppo2.py:185][0m |          -0.0249 |           0.0129 |           0.0000 |
[32m[20221213 20:36:55 @agent_ppo2.py:185][0m |          -0.0356 |           0.0123 |           0.0000 |
[32m[20221213 20:36:56 @agent_ppo2.py:185][0m |          -0.0488 |           0.0120 |           0.0000 |
[32m[20221213 20:36:56 @agent_ppo2.py:185][0m |          -0.0549 |           0.0117 |           0.0000 |
[32m[20221213 20:36:56 @agent_ppo2.py:185][0m |          -0.0600 |           0.0115 |           0.0000 |
[32m[20221213 20:36:56 @agent_ppo2.py:185][0m |          -0.0593 |           0.0112 |           0.0000 |
[32m[20221213 20:36:56 @agent_ppo2.py:185][0m |          -0.0644 |           0.0112 |           0.0000 |
[32m[20221213 20:36:56 @agent_ppo2.py:185][0m |          -0.0762 |           0.0130 |           0.0000 |
[32m[20221213 20:36:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:36:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.30
[32m[20221213 20:36:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.22
[32m[20221213 20:36:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.47
[32m[20221213 20:36:56 @agent_ppo2.py:143][0m Total time:       7.79 min
[32m[20221213 20:36:56 @agent_ppo2.py:145][0m 1118208 total steps have happened
[32m[20221213 20:36:56 @agent_ppo2.py:121][0m #------------------------ Iteration 273 --------------------------#
[32m[20221213 20:36:57 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:36:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:57 @agent_ppo2.py:185][0m |           0.0646 |           0.0237 |           0.0000 |
[32m[20221213 20:36:57 @agent_ppo2.py:185][0m |           0.0187 |           0.0127 |           0.0000 |
[32m[20221213 20:36:57 @agent_ppo2.py:185][0m |           0.0032 |           0.0118 |           0.0000 |
[32m[20221213 20:36:57 @agent_ppo2.py:185][0m |          -0.0088 |           0.0116 |           0.0000 |
[32m[20221213 20:36:57 @agent_ppo2.py:185][0m |          -0.0183 |           0.0113 |           0.0000 |
[32m[20221213 20:36:57 @agent_ppo2.py:185][0m |          -0.0263 |           0.0111 |           0.0000 |
[32m[20221213 20:36:57 @agent_ppo2.py:185][0m |          -0.0312 |           0.0109 |           0.0000 |
[32m[20221213 20:36:58 @agent_ppo2.py:185][0m |          -0.0369 |           0.0108 |           0.0000 |
[32m[20221213 20:36:58 @agent_ppo2.py:185][0m |          -0.0126 |           0.0108 |           0.0000 |
[32m[20221213 20:36:58 @agent_ppo2.py:185][0m |          -0.0431 |           0.0106 |           0.0000 |
[32m[20221213 20:36:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:36:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.02
[32m[20221213 20:36:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.82
[32m[20221213 20:36:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.48
[32m[20221213 20:36:58 @agent_ppo2.py:143][0m Total time:       7.82 min
[32m[20221213 20:36:58 @agent_ppo2.py:145][0m 1122304 total steps have happened
[32m[20221213 20:36:58 @agent_ppo2.py:121][0m #------------------------ Iteration 274 --------------------------#
[32m[20221213 20:36:58 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:36:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:36:59 @agent_ppo2.py:185][0m |           0.0378 |           0.0090 |           0.0000 |
[32m[20221213 20:36:59 @agent_ppo2.py:185][0m |           0.0175 |           0.0068 |           0.0000 |
[32m[20221213 20:36:59 @agent_ppo2.py:185][0m |           0.0022 |           0.0066 |           0.0000 |
[32m[20221213 20:36:59 @agent_ppo2.py:185][0m |          -0.0097 |           0.0064 |           0.0000 |
[32m[20221213 20:36:59 @agent_ppo2.py:185][0m |          -0.0215 |           0.0063 |           0.0000 |
[32m[20221213 20:36:59 @agent_ppo2.py:185][0m |          -0.0269 |           0.0063 |           0.0000 |
[32m[20221213 20:36:59 @agent_ppo2.py:185][0m |          -0.0301 |           0.0062 |           0.0000 |
[32m[20221213 20:36:59 @agent_ppo2.py:185][0m |          -0.0347 |           0.0061 |           0.0000 |
[32m[20221213 20:36:59 @agent_ppo2.py:185][0m |          -0.0393 |           0.0060 |           0.0000 |
[32m[20221213 20:36:59 @agent_ppo2.py:185][0m |          -0.0418 |           0.0060 |           0.0000 |
[32m[20221213 20:36:59 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:37:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.92
[32m[20221213 20:37:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.06
[32m[20221213 20:37:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.63
[32m[20221213 20:37:00 @agent_ppo2.py:143][0m Total time:       7.84 min
[32m[20221213 20:37:00 @agent_ppo2.py:145][0m 1126400 total steps have happened
[32m[20221213 20:37:00 @agent_ppo2.py:121][0m #------------------------ Iteration 275 --------------------------#
[32m[20221213 20:37:00 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:00 @agent_ppo2.py:185][0m |           0.0253 |           0.0617 |           0.0000 |
[32m[20221213 20:37:00 @agent_ppo2.py:185][0m |           0.0009 |           0.0237 |           0.0000 |
[32m[20221213 20:37:00 @agent_ppo2.py:185][0m |          -0.0120 |           0.0155 |           0.0000 |
[32m[20221213 20:37:01 @agent_ppo2.py:185][0m |          -0.0184 |           0.0143 |           0.0000 |
[32m[20221213 20:37:01 @agent_ppo2.py:185][0m |          -0.0169 |           0.0137 |           0.0000 |
[32m[20221213 20:37:01 @agent_ppo2.py:185][0m |          -0.0204 |           0.0134 |           0.0000 |
[32m[20221213 20:37:01 @agent_ppo2.py:185][0m |          -0.0222 |           0.0130 |           0.0000 |
[32m[20221213 20:37:01 @agent_ppo2.py:185][0m |          -0.0238 |           0.0130 |           0.0000 |
[32m[20221213 20:37:01 @agent_ppo2.py:185][0m |          -0.0243 |           0.0128 |           0.0000 |
[32m[20221213 20:37:01 @agent_ppo2.py:185][0m |          -0.0528 |           0.0128 |           0.0000 |
[32m[20221213 20:37:01 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:37:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.40
[32m[20221213 20:37:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.17
[32m[20221213 20:37:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.26
[32m[20221213 20:37:01 @agent_ppo2.py:143][0m Total time:       7.87 min
[32m[20221213 20:37:01 @agent_ppo2.py:145][0m 1130496 total steps have happened
[32m[20221213 20:37:01 @agent_ppo2.py:121][0m #------------------------ Iteration 276 --------------------------#
[32m[20221213 20:37:02 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:02 @agent_ppo2.py:185][0m |           0.0251 |           0.1041 |           0.0000 |
[32m[20221213 20:37:02 @agent_ppo2.py:185][0m |           0.0086 |           0.0648 |           0.0000 |
[32m[20221213 20:37:02 @agent_ppo2.py:185][0m |          -0.0051 |           0.0484 |           0.0000 |
[32m[20221213 20:37:02 @agent_ppo2.py:185][0m |          -0.0153 |           0.0417 |           0.0000 |
[32m[20221213 20:37:02 @agent_ppo2.py:185][0m |          -0.0219 |           0.0406 |           0.0000 |
[32m[20221213 20:37:02 @agent_ppo2.py:185][0m |          -0.0215 |           0.0371 |           0.0000 |
[32m[20221213 20:37:02 @agent_ppo2.py:185][0m |          -0.0245 |           0.0358 |           0.0000 |
[32m[20221213 20:37:03 @agent_ppo2.py:185][0m |          -0.0262 |           0.0351 |           0.0000 |
[32m[20221213 20:37:03 @agent_ppo2.py:185][0m |          -0.0276 |           0.0331 |           0.0000 |
[32m[20221213 20:37:03 @agent_ppo2.py:185][0m |          -0.0290 |           0.0326 |           0.0000 |
[32m[20221213 20:37:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:37:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.76
[32m[20221213 20:37:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 13.51
[32m[20221213 20:37:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.65
[32m[20221213 20:37:03 @agent_ppo2.py:143][0m Total time:       7.90 min
[32m[20221213 20:37:03 @agent_ppo2.py:145][0m 1134592 total steps have happened
[32m[20221213 20:37:03 @agent_ppo2.py:121][0m #------------------------ Iteration 277 --------------------------#
[32m[20221213 20:37:04 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:04 @agent_ppo2.py:185][0m |           0.0037 |           0.0597 |           0.0000 |
[32m[20221213 20:37:04 @agent_ppo2.py:185][0m |           0.0053 |           0.0137 |           0.0000 |
[32m[20221213 20:37:04 @agent_ppo2.py:185][0m |          -0.0115 |           0.0120 |           0.0000 |
[32m[20221213 20:37:04 @agent_ppo2.py:185][0m |          -0.0257 |           0.0116 |           0.0000 |
[32m[20221213 20:37:04 @agent_ppo2.py:185][0m |          -0.0323 |           0.0115 |           0.0000 |
[32m[20221213 20:37:04 @agent_ppo2.py:185][0m |          -0.0401 |           0.0113 |           0.0000 |
[32m[20221213 20:37:04 @agent_ppo2.py:185][0m |          -0.0415 |           0.0113 |           0.0000 |
[32m[20221213 20:37:04 @agent_ppo2.py:185][0m |          -0.0468 |           0.0111 |           0.0000 |
[32m[20221213 20:37:04 @agent_ppo2.py:185][0m |          -0.0230 |           0.0114 |           0.0000 |
[32m[20221213 20:37:04 @agent_ppo2.py:185][0m |          -0.0519 |           0.0111 |           0.0000 |
[32m[20221213 20:37:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:37:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.44
[32m[20221213 20:37:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.55
[32m[20221213 20:37:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.23
[32m[20221213 20:37:05 @agent_ppo2.py:143][0m Total time:       7.93 min
[32m[20221213 20:37:05 @agent_ppo2.py:145][0m 1138688 total steps have happened
[32m[20221213 20:37:05 @agent_ppo2.py:121][0m #------------------------ Iteration 278 --------------------------#
[32m[20221213 20:37:05 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:05 @agent_ppo2.py:185][0m |           0.0467 |           0.0127 |           0.0000 |
[32m[20221213 20:37:05 @agent_ppo2.py:185][0m |           0.0408 |           0.0086 |           0.0000 |
[32m[20221213 20:37:05 @agent_ppo2.py:185][0m |          -0.0030 |           0.0080 |           0.0000 |
[32m[20221213 20:37:06 @agent_ppo2.py:185][0m |          -0.0118 |           0.0079 |           0.0000 |
[32m[20221213 20:37:06 @agent_ppo2.py:185][0m |          -0.0310 |           0.0078 |           0.0000 |
[32m[20221213 20:37:06 @agent_ppo2.py:185][0m |          -0.0394 |           0.0077 |           0.0000 |
[32m[20221213 20:37:06 @agent_ppo2.py:185][0m |          -0.0483 |           0.0076 |           0.0000 |
[32m[20221213 20:37:06 @agent_ppo2.py:185][0m |          -0.0513 |           0.0076 |           0.0000 |
[32m[20221213 20:37:06 @agent_ppo2.py:185][0m |          -0.0597 |           0.0075 |           0.0000 |
[32m[20221213 20:37:06 @agent_ppo2.py:185][0m |          -0.0591 |           0.0074 |           0.0000 |
[32m[20221213 20:37:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:37:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.15
[32m[20221213 20:37:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.20
[32m[20221213 20:37:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.86
[32m[20221213 20:37:06 @agent_ppo2.py:143][0m Total time:       7.95 min
[32m[20221213 20:37:06 @agent_ppo2.py:145][0m 1142784 total steps have happened
[32m[20221213 20:37:06 @agent_ppo2.py:121][0m #------------------------ Iteration 279 --------------------------#
[32m[20221213 20:37:07 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:07 @agent_ppo2.py:185][0m |           0.0331 |           0.1039 |           0.0000 |
[32m[20221213 20:37:07 @agent_ppo2.py:185][0m |           0.0051 |           0.0362 |           0.0000 |
[32m[20221213 20:37:07 @agent_ppo2.py:185][0m |          -0.0085 |           0.0291 |           0.0000 |
[32m[20221213 20:37:07 @agent_ppo2.py:185][0m |          -0.0162 |           0.0257 |           0.0000 |
[32m[20221213 20:37:07 @agent_ppo2.py:185][0m |          -0.0200 |           0.0229 |           0.0000 |
[32m[20221213 20:37:07 @agent_ppo2.py:185][0m |          -0.0587 |           0.0237 |           0.0000 |
[32m[20221213 20:37:07 @agent_ppo2.py:185][0m |          -0.0258 |           0.0267 |           0.0000 |
[32m[20221213 20:37:08 @agent_ppo2.py:185][0m |          -0.0274 |           0.0204 |           0.0000 |
[32m[20221213 20:37:08 @agent_ppo2.py:185][0m |          -0.0282 |           0.0195 |           0.0000 |
[32m[20221213 20:37:08 @agent_ppo2.py:185][0m |          -0.0289 |           0.0192 |           0.0000 |
[32m[20221213 20:37:08 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:37:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.23
[32m[20221213 20:37:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.51
[32m[20221213 20:37:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.25
[32m[20221213 20:37:08 @agent_ppo2.py:143][0m Total time:       7.98 min
[32m[20221213 20:37:08 @agent_ppo2.py:145][0m 1146880 total steps have happened
[32m[20221213 20:37:08 @agent_ppo2.py:121][0m #------------------------ Iteration 280 --------------------------#
[32m[20221213 20:37:08 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:37:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:09 @agent_ppo2.py:185][0m |           0.0455 |           0.0221 |           0.0000 |
[32m[20221213 20:37:09 @agent_ppo2.py:185][0m |           0.0110 |           0.0137 |           0.0000 |
[32m[20221213 20:37:09 @agent_ppo2.py:185][0m |          -0.0090 |           0.0131 |           0.0000 |
[32m[20221213 20:37:09 @agent_ppo2.py:185][0m |          -0.0212 |           0.0130 |           0.0000 |
[32m[20221213 20:37:09 @agent_ppo2.py:185][0m |          -0.0390 |           0.0130 |           0.0000 |
[32m[20221213 20:37:09 @agent_ppo2.py:185][0m |          -0.0390 |           0.0129 |           0.0000 |
[32m[20221213 20:37:09 @agent_ppo2.py:185][0m |          -0.0508 |           0.0128 |           0.0000 |
[32m[20221213 20:37:09 @agent_ppo2.py:185][0m |          -0.0465 |           0.0127 |           0.0000 |
[32m[20221213 20:37:09 @agent_ppo2.py:185][0m |          -0.0469 |           0.0124 |           0.0000 |
[32m[20221213 20:37:09 @agent_ppo2.py:185][0m |          -0.0567 |           0.0125 |           0.0000 |
[32m[20221213 20:37:09 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:37:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.81
[32m[20221213 20:37:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.83
[32m[20221213 20:37:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.61
[32m[20221213 20:37:10 @agent_ppo2.py:143][0m Total time:       8.01 min
[32m[20221213 20:37:10 @agent_ppo2.py:145][0m 1150976 total steps have happened
[32m[20221213 20:37:10 @agent_ppo2.py:121][0m #------------------------ Iteration 281 --------------------------#
[32m[20221213 20:37:10 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:10 @agent_ppo2.py:185][0m |           0.0050 |           0.0259 |           0.0000 |
[32m[20221213 20:37:10 @agent_ppo2.py:185][0m |           0.0106 |           0.0237 |           0.0000 |
[32m[20221213 20:37:10 @agent_ppo2.py:185][0m |          -0.0016 |           0.0226 |           0.0000 |
[32m[20221213 20:37:11 @agent_ppo2.py:185][0m |          -0.0477 |           0.0222 |           0.0000 |
[32m[20221213 20:37:11 @agent_ppo2.py:185][0m |          -0.0159 |           0.0219 |           0.0000 |
[32m[20221213 20:37:11 @agent_ppo2.py:185][0m |          -0.0182 |           0.0212 |           0.0000 |
[32m[20221213 20:37:11 @agent_ppo2.py:185][0m |          -0.0244 |           0.0209 |           0.0000 |
[32m[20221213 20:37:11 @agent_ppo2.py:185][0m |          -0.0246 |           0.0204 |           0.0000 |
[32m[20221213 20:37:11 @agent_ppo2.py:185][0m |          -0.0290 |           0.0204 |           0.0000 |
[32m[20221213 20:37:11 @agent_ppo2.py:185][0m |          -0.0323 |           0.0199 |           0.0000 |
[32m[20221213 20:37:11 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:37:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.06
[32m[20221213 20:37:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.22
[32m[20221213 20:37:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.95
[32m[20221213 20:37:11 @agent_ppo2.py:143][0m Total time:       8.04 min
[32m[20221213 20:37:11 @agent_ppo2.py:145][0m 1155072 total steps have happened
[32m[20221213 20:37:11 @agent_ppo2.py:121][0m #------------------------ Iteration 282 --------------------------#
[32m[20221213 20:37:12 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:12 @agent_ppo2.py:185][0m |           0.0205 |           0.0212 |           0.0000 |
[32m[20221213 20:37:12 @agent_ppo2.py:185][0m |          -0.0040 |           0.0182 |           0.0000 |
[32m[20221213 20:37:12 @agent_ppo2.py:185][0m |          -0.0141 |           0.0169 |           0.0000 |
[32m[20221213 20:37:12 @agent_ppo2.py:185][0m |          -0.0301 |           0.0167 |           0.0000 |
[32m[20221213 20:37:12 @agent_ppo2.py:185][0m |          -0.0406 |           0.0163 |           0.0000 |
[32m[20221213 20:37:12 @agent_ppo2.py:185][0m |          -0.0431 |           0.0162 |           0.0000 |
[32m[20221213 20:37:13 @agent_ppo2.py:185][0m |          -0.0497 |           0.0158 |           0.0000 |
[32m[20221213 20:37:13 @agent_ppo2.py:185][0m |          -0.0476 |           0.0157 |           0.0000 |
[32m[20221213 20:37:13 @agent_ppo2.py:185][0m |          -0.0565 |           0.0153 |           0.0000 |
[32m[20221213 20:37:13 @agent_ppo2.py:185][0m |          -0.0426 |           0.0153 |           0.0000 |
[32m[20221213 20:37:13 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:37:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.72
[32m[20221213 20:37:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.51
[32m[20221213 20:37:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.14
[32m[20221213 20:37:13 @agent_ppo2.py:143][0m Total time:       8.07 min
[32m[20221213 20:37:13 @agent_ppo2.py:145][0m 1159168 total steps have happened
[32m[20221213 20:37:13 @agent_ppo2.py:121][0m #------------------------ Iteration 283 --------------------------#
[32m[20221213 20:37:14 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:14 @agent_ppo2.py:185][0m |           0.0318 |           0.0245 |           0.0000 |
[32m[20221213 20:37:14 @agent_ppo2.py:185][0m |           0.0102 |           0.0105 |           0.0000 |
[32m[20221213 20:37:14 @agent_ppo2.py:185][0m |          -0.0005 |           0.0099 |           0.0000 |
[32m[20221213 20:37:14 @agent_ppo2.py:185][0m |          -0.0159 |           0.0097 |           0.0000 |
[32m[20221213 20:37:14 @agent_ppo2.py:185][0m |          -0.0202 |           0.0096 |           0.0000 |
[32m[20221213 20:37:14 @agent_ppo2.py:185][0m |          -0.0259 |           0.0095 |           0.0000 |
[32m[20221213 20:37:14 @agent_ppo2.py:185][0m |          -0.0354 |           0.0095 |           0.0000 |
[32m[20221213 20:37:14 @agent_ppo2.py:185][0m |          -0.0338 |           0.0094 |           0.0000 |
[32m[20221213 20:37:14 @agent_ppo2.py:185][0m |          -0.0383 |           0.0093 |           0.0000 |
[32m[20221213 20:37:15 @agent_ppo2.py:185][0m |          -0.0369 |           0.0091 |           0.0000 |
[32m[20221213 20:37:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:37:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.36
[32m[20221213 20:37:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.47
[32m[20221213 20:37:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.56
[32m[20221213 20:37:15 @agent_ppo2.py:143][0m Total time:       8.10 min
[32m[20221213 20:37:15 @agent_ppo2.py:145][0m 1163264 total steps have happened
[32m[20221213 20:37:15 @agent_ppo2.py:121][0m #------------------------ Iteration 284 --------------------------#
[32m[20221213 20:37:15 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:15 @agent_ppo2.py:185][0m |           0.0281 |           0.0097 |           0.0000 |
[32m[20221213 20:37:16 @agent_ppo2.py:185][0m |           0.0016 |           0.0079 |           0.0000 |
[32m[20221213 20:37:16 @agent_ppo2.py:185][0m |          -0.0076 |           0.0075 |           0.0000 |
[32m[20221213 20:37:16 @agent_ppo2.py:185][0m |          -0.0278 |           0.0073 |           0.0000 |
[32m[20221213 20:37:16 @agent_ppo2.py:185][0m |          -0.0447 |           0.0073 |           0.0000 |
[32m[20221213 20:37:16 @agent_ppo2.py:185][0m |          -0.0418 |           0.0071 |           0.0000 |
[32m[20221213 20:37:16 @agent_ppo2.py:185][0m |          -0.0450 |           0.0071 |           0.0000 |
[32m[20221213 20:37:16 @agent_ppo2.py:185][0m |          -0.0353 |           0.0070 |           0.0000 |
[32m[20221213 20:37:16 @agent_ppo2.py:185][0m |          -0.0483 |           0.0069 |           0.0000 |
[32m[20221213 20:37:16 @agent_ppo2.py:185][0m |          -0.0548 |           0.0068 |           0.0000 |
[32m[20221213 20:37:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:37:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.50
[32m[20221213 20:37:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.01
[32m[20221213 20:37:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.69
[32m[20221213 20:37:17 @agent_ppo2.py:143][0m Total time:       8.12 min
[32m[20221213 20:37:17 @agent_ppo2.py:145][0m 1167360 total steps have happened
[32m[20221213 20:37:17 @agent_ppo2.py:121][0m #------------------------ Iteration 285 --------------------------#
[32m[20221213 20:37:17 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:17 @agent_ppo2.py:185][0m |           0.0334 |           0.0976 |           0.0000 |
[32m[20221213 20:37:17 @agent_ppo2.py:185][0m |          -0.0016 |           0.0229 |           0.0000 |
[32m[20221213 20:37:17 @agent_ppo2.py:185][0m |          -0.0148 |           0.0180 |           0.0000 |
[32m[20221213 20:37:17 @agent_ppo2.py:185][0m |          -0.0145 |           0.0165 |           0.0000 |
[32m[20221213 20:37:17 @agent_ppo2.py:185][0m |          -0.0244 |           0.0160 |           0.0000 |
[32m[20221213 20:37:18 @agent_ppo2.py:185][0m |          -0.0235 |           0.0154 |           0.0000 |
[32m[20221213 20:37:18 @agent_ppo2.py:185][0m |          -0.0233 |           0.0153 |           0.0000 |
[32m[20221213 20:37:18 @agent_ppo2.py:185][0m |          -0.0290 |           0.0147 |           0.0000 |
[32m[20221213 20:37:18 @agent_ppo2.py:185][0m |          -0.0268 |           0.0145 |           0.0000 |
[32m[20221213 20:37:18 @agent_ppo2.py:185][0m |          -0.0285 |           0.0145 |           0.0000 |
[32m[20221213 20:37:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:37:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.05
[32m[20221213 20:37:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.83
[32m[20221213 20:37:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.20
[32m[20221213 20:37:18 @agent_ppo2.py:143][0m Total time:       8.15 min
[32m[20221213 20:37:18 @agent_ppo2.py:145][0m 1171456 total steps have happened
[32m[20221213 20:37:18 @agent_ppo2.py:121][0m #------------------------ Iteration 286 --------------------------#
[32m[20221213 20:37:19 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:19 @agent_ppo2.py:185][0m |           0.0284 |           0.0275 |           0.0000 |
[32m[20221213 20:37:19 @agent_ppo2.py:185][0m |           0.0040 |           0.0216 |           0.0000 |
[32m[20221213 20:37:19 @agent_ppo2.py:185][0m |          -0.0138 |           0.0199 |           0.0000 |
[32m[20221213 20:37:19 @agent_ppo2.py:185][0m |          -0.0126 |           0.0181 |           0.0000 |
[32m[20221213 20:37:19 @agent_ppo2.py:185][0m |          -0.0256 |           0.0175 |           0.0000 |
[32m[20221213 20:37:19 @agent_ppo2.py:185][0m |          -0.0360 |           0.0169 |           0.0000 |
[32m[20221213 20:37:19 @agent_ppo2.py:185][0m |          -0.0344 |           0.0165 |           0.0000 |
[32m[20221213 20:37:19 @agent_ppo2.py:185][0m |          -0.0432 |           0.0163 |           0.0000 |
[32m[20221213 20:37:19 @agent_ppo2.py:185][0m |          -0.0469 |           0.0159 |           0.0000 |
[32m[20221213 20:37:19 @agent_ppo2.py:185][0m |          -0.0528 |           0.0158 |           0.0000 |
[32m[20221213 20:37:19 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:37:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.83
[32m[20221213 20:37:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.12
[32m[20221213 20:37:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.96
[32m[20221213 20:37:20 @agent_ppo2.py:143][0m Total time:       8.18 min
[32m[20221213 20:37:20 @agent_ppo2.py:145][0m 1175552 total steps have happened
[32m[20221213 20:37:20 @agent_ppo2.py:121][0m #------------------------ Iteration 287 --------------------------#
[32m[20221213 20:37:20 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:20 @agent_ppo2.py:185][0m |           0.0834 |           0.0243 |           0.0000 |
[32m[20221213 20:37:21 @agent_ppo2.py:185][0m |           0.1659 |           0.0154 |           0.0000 |
[32m[20221213 20:37:21 @agent_ppo2.py:185][0m |           0.0603 |           0.0146 |           0.0000 |
[32m[20221213 20:37:21 @agent_ppo2.py:185][0m |           0.0231 |           0.0141 |           0.0000 |
[32m[20221213 20:37:21 @agent_ppo2.py:185][0m |           0.0047 |           0.0137 |           0.0000 |
[32m[20221213 20:37:21 @agent_ppo2.py:185][0m |           0.0124 |           0.0139 |           0.0000 |
[32m[20221213 20:37:21 @agent_ppo2.py:185][0m |          -0.0115 |           0.0137 |           0.0000 |
[32m[20221213 20:37:21 @agent_ppo2.py:185][0m |          -0.0161 |           0.0132 |           0.0000 |
[32m[20221213 20:37:21 @agent_ppo2.py:185][0m |          -0.0312 |           0.0129 |           0.0000 |
[32m[20221213 20:37:21 @agent_ppo2.py:185][0m |          -0.0283 |           0.0129 |           0.0000 |
[32m[20221213 20:37:21 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:37:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.43
[32m[20221213 20:37:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.09
[32m[20221213 20:37:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.35
[32m[20221213 20:37:22 @agent_ppo2.py:143][0m Total time:       8.21 min
[32m[20221213 20:37:22 @agent_ppo2.py:145][0m 1179648 total steps have happened
[32m[20221213 20:37:22 @agent_ppo2.py:121][0m #------------------------ Iteration 288 --------------------------#
[32m[20221213 20:37:22 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:37:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:22 @agent_ppo2.py:185][0m |           0.0155 |           0.0135 |           0.0000 |
[32m[20221213 20:37:22 @agent_ppo2.py:185][0m |          -0.0151 |           0.0133 |           0.0000 |
[32m[20221213 20:37:22 @agent_ppo2.py:185][0m |          -0.0372 |           0.0128 |           0.0000 |
[32m[20221213 20:37:23 @agent_ppo2.py:185][0m |          -0.0450 |           0.0127 |           0.0000 |
[32m[20221213 20:37:23 @agent_ppo2.py:185][0m |          -0.0539 |           0.0124 |           0.0000 |
[32m[20221213 20:37:23 @agent_ppo2.py:185][0m |          -0.0621 |           0.0123 |           0.0000 |
[32m[20221213 20:37:23 @agent_ppo2.py:185][0m |          -0.0630 |           0.0123 |           0.0000 |
[32m[20221213 20:37:23 @agent_ppo2.py:185][0m |          -0.0708 |           0.0121 |           0.0000 |
[32m[20221213 20:37:23 @agent_ppo2.py:185][0m |          -0.0725 |           0.0120 |           0.0000 |
[32m[20221213 20:37:23 @agent_ppo2.py:185][0m |          -0.0697 |           0.0120 |           0.0000 |
[32m[20221213 20:37:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:37:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.43
[32m[20221213 20:37:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.18
[32m[20221213 20:37:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.84
[32m[20221213 20:37:23 @agent_ppo2.py:143][0m Total time:       8.24 min
[32m[20221213 20:37:23 @agent_ppo2.py:145][0m 1183744 total steps have happened
[32m[20221213 20:37:23 @agent_ppo2.py:121][0m #------------------------ Iteration 289 --------------------------#
[32m[20221213 20:37:24 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:24 @agent_ppo2.py:185][0m |           0.0260 |           0.0157 |           0.0000 |
[32m[20221213 20:37:24 @agent_ppo2.py:185][0m |          -0.0023 |           0.0123 |           0.0000 |
[32m[20221213 20:37:24 @agent_ppo2.py:185][0m |          -0.0171 |           0.0119 |           0.0000 |
[32m[20221213 20:37:24 @agent_ppo2.py:185][0m |          -0.0261 |           0.0117 |           0.0000 |
[32m[20221213 20:37:24 @agent_ppo2.py:185][0m |          -0.0297 |           0.0115 |           0.0000 |
[32m[20221213 20:37:24 @agent_ppo2.py:185][0m |          -0.0332 |           0.0113 |           0.0000 |
[32m[20221213 20:37:24 @agent_ppo2.py:185][0m |          -0.0314 |           0.0114 |           0.0000 |
[32m[20221213 20:37:25 @agent_ppo2.py:185][0m |          -0.0340 |           0.0112 |           0.0000 |
[32m[20221213 20:37:25 @agent_ppo2.py:185][0m |          -0.0875 |           0.0118 |           0.0000 |
[32m[20221213 20:37:25 @agent_ppo2.py:185][0m |          -0.0403 |           0.0123 |           0.0000 |
[32m[20221213 20:37:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:37:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.05
[32m[20221213 20:37:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.57
[32m[20221213 20:37:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.46
[32m[20221213 20:37:25 @agent_ppo2.py:143][0m Total time:       8.27 min
[32m[20221213 20:37:25 @agent_ppo2.py:145][0m 1187840 total steps have happened
[32m[20221213 20:37:25 @agent_ppo2.py:121][0m #------------------------ Iteration 290 --------------------------#
[32m[20221213 20:37:25 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:37:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:26 @agent_ppo2.py:185][0m |           0.0266 |           0.0220 |           0.0000 |
[32m[20221213 20:37:26 @agent_ppo2.py:185][0m |           0.0084 |           0.0176 |           0.0000 |
[32m[20221213 20:37:26 @agent_ppo2.py:185][0m |          -0.0151 |           0.0175 |           0.0000 |
[32m[20221213 20:37:26 @agent_ppo2.py:185][0m |          -0.0143 |           0.0161 |           0.0000 |
[32m[20221213 20:37:26 @agent_ppo2.py:185][0m |          -0.0615 |           0.0169 |           0.0000 |
[32m[20221213 20:37:26 @agent_ppo2.py:185][0m |           0.0013 |           0.0165 |           0.0000 |
[32m[20221213 20:37:26 @agent_ppo2.py:185][0m |          -0.0046 |           0.0150 |           0.0000 |
[32m[20221213 20:37:26 @agent_ppo2.py:185][0m |          -0.0134 |           0.0154 |           0.0000 |
[32m[20221213 20:37:26 @agent_ppo2.py:185][0m |          -0.0185 |           0.0150 |           0.0000 |
[32m[20221213 20:37:26 @agent_ppo2.py:185][0m |          -0.0199 |           0.0147 |           0.0000 |
[32m[20221213 20:37:26 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:37:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.59
[32m[20221213 20:37:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.66
[32m[20221213 20:37:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.65
[32m[20221213 20:37:27 @agent_ppo2.py:143][0m Total time:       8.29 min
[32m[20221213 20:37:27 @agent_ppo2.py:145][0m 1191936 total steps have happened
[32m[20221213 20:37:27 @agent_ppo2.py:121][0m #------------------------ Iteration 291 --------------------------#
[32m[20221213 20:37:27 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:37:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:27 @agent_ppo2.py:185][0m |           0.0365 |           0.0176 |           0.0000 |
[32m[20221213 20:37:28 @agent_ppo2.py:185][0m |          -0.0001 |           0.0164 |           0.0000 |
[32m[20221213 20:37:28 @agent_ppo2.py:185][0m |          -0.0209 |           0.0159 |           0.0000 |
[32m[20221213 20:37:28 @agent_ppo2.py:185][0m |          -0.0353 |           0.0157 |           0.0000 |
[32m[20221213 20:37:28 @agent_ppo2.py:185][0m |          -0.0450 |           0.0153 |           0.0000 |
[32m[20221213 20:37:28 @agent_ppo2.py:185][0m |          -0.0446 |           0.0155 |           0.0000 |
[32m[20221213 20:37:28 @agent_ppo2.py:185][0m |          -0.0587 |           0.0148 |           0.0000 |
[32m[20221213 20:37:28 @agent_ppo2.py:185][0m |          -0.0578 |           0.0145 |           0.0000 |
[32m[20221213 20:37:28 @agent_ppo2.py:185][0m |          -0.0593 |           0.0149 |           0.0000 |
[32m[20221213 20:37:28 @agent_ppo2.py:185][0m |          -0.0667 |           0.0146 |           0.0000 |
[32m[20221213 20:37:28 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:37:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.74
[32m[20221213 20:37:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.54
[32m[20221213 20:37:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.82
[32m[20221213 20:37:29 @agent_ppo2.py:143][0m Total time:       8.32 min
[32m[20221213 20:37:29 @agent_ppo2.py:145][0m 1196032 total steps have happened
[32m[20221213 20:37:29 @agent_ppo2.py:121][0m #------------------------ Iteration 292 --------------------------#
[32m[20221213 20:37:29 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:37:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:29 @agent_ppo2.py:185][0m |           0.0178 |           0.0163 |           0.0000 |
[32m[20221213 20:37:29 @agent_ppo2.py:185][0m |          -0.0009 |           0.0116 |           0.0000 |
[32m[20221213 20:37:29 @agent_ppo2.py:185][0m |          -0.0275 |           0.0112 |           0.0000 |
[32m[20221213 20:37:29 @agent_ppo2.py:185][0m |          -0.0360 |           0.0109 |           0.0000 |
[32m[20221213 20:37:30 @agent_ppo2.py:185][0m |          -0.0414 |           0.0107 |           0.0000 |
[32m[20221213 20:37:30 @agent_ppo2.py:185][0m |          -0.0458 |           0.0105 |           0.0000 |
[32m[20221213 20:37:30 @agent_ppo2.py:185][0m |          -0.0458 |           0.0104 |           0.0000 |
[32m[20221213 20:37:30 @agent_ppo2.py:185][0m |          -0.0507 |           0.0103 |           0.0000 |
[32m[20221213 20:37:30 @agent_ppo2.py:185][0m |          -0.0611 |           0.0102 |           0.0000 |
[32m[20221213 20:37:30 @agent_ppo2.py:185][0m |          -0.0149 |           0.0106 |           0.0000 |
[32m[20221213 20:37:30 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:37:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.74
[32m[20221213 20:37:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.64
[32m[20221213 20:37:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.02
[32m[20221213 20:37:30 @agent_ppo2.py:143][0m Total time:       8.35 min
[32m[20221213 20:37:30 @agent_ppo2.py:145][0m 1200128 total steps have happened
[32m[20221213 20:37:30 @agent_ppo2.py:121][0m #------------------------ Iteration 293 --------------------------#
[32m[20221213 20:37:31 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:37:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:31 @agent_ppo2.py:185][0m |          -0.0180 |           0.0219 |           0.0000 |
[32m[20221213 20:37:31 @agent_ppo2.py:185][0m |           0.0268 |           0.0180 |           0.0000 |
[32m[20221213 20:37:31 @agent_ppo2.py:185][0m |           0.0182 |           0.0136 |           0.0000 |
[32m[20221213 20:37:31 @agent_ppo2.py:185][0m |           0.0010 |           0.0134 |           0.0000 |
[32m[20221213 20:37:31 @agent_ppo2.py:185][0m |          -0.0457 |           0.0141 |           0.0000 |
[32m[20221213 20:37:31 @agent_ppo2.py:185][0m |          -0.0137 |           0.0133 |           0.0000 |
[32m[20221213 20:37:31 @agent_ppo2.py:185][0m |          -0.0474 |           0.0136 |           0.0000 |
[32m[20221213 20:37:32 @agent_ppo2.py:185][0m |          -0.0247 |           0.0132 |           0.0000 |
[32m[20221213 20:37:32 @agent_ppo2.py:185][0m |          -0.0276 |           0.0123 |           0.0000 |
[32m[20221213 20:37:32 @agent_ppo2.py:185][0m |          -0.0338 |           0.0122 |           0.0000 |
[32m[20221213 20:37:32 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:37:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.47
[32m[20221213 20:37:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.51
[32m[20221213 20:37:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.40
[32m[20221213 20:37:32 @agent_ppo2.py:143][0m Total time:       8.38 min
[32m[20221213 20:37:32 @agent_ppo2.py:145][0m 1204224 total steps have happened
[32m[20221213 20:37:32 @agent_ppo2.py:121][0m #------------------------ Iteration 294 --------------------------#
[32m[20221213 20:37:33 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:37:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:33 @agent_ppo2.py:185][0m |           0.0271 |           0.0963 |           0.0000 |
[32m[20221213 20:37:33 @agent_ppo2.py:185][0m |           0.0034 |           0.0377 |           0.0000 |
[32m[20221213 20:37:33 @agent_ppo2.py:185][0m |          -0.0142 |           0.0265 |           0.0000 |
[32m[20221213 20:37:33 @agent_ppo2.py:185][0m |          -0.0154 |           0.0247 |           0.0000 |
[32m[20221213 20:37:33 @agent_ppo2.py:185][0m |          -0.0187 |           0.0250 |           0.0000 |
[32m[20221213 20:37:33 @agent_ppo2.py:185][0m |          -0.0252 |           0.0229 |           0.0000 |
[32m[20221213 20:37:33 @agent_ppo2.py:185][0m |          -0.0269 |           0.0225 |           0.0000 |
[32m[20221213 20:37:33 @agent_ppo2.py:185][0m |          -0.0663 |           0.0212 |           0.0000 |
[32m[20221213 20:37:34 @agent_ppo2.py:185][0m |          -0.0303 |           0.0211 |           0.0000 |
[32m[20221213 20:37:34 @agent_ppo2.py:185][0m |          -0.0316 |           0.0208 |           0.0000 |
[32m[20221213 20:37:34 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 20:37:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.43
[32m[20221213 20:37:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.03
[32m[20221213 20:37:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.77
[32m[20221213 20:37:34 @agent_ppo2.py:143][0m Total time:       8.41 min
[32m[20221213 20:37:34 @agent_ppo2.py:145][0m 1208320 total steps have happened
[32m[20221213 20:37:34 @agent_ppo2.py:121][0m #------------------------ Iteration 295 --------------------------#
[32m[20221213 20:37:34 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:37:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:35 @agent_ppo2.py:185][0m |           0.0019 |           0.0523 |           0.0000 |
[32m[20221213 20:37:35 @agent_ppo2.py:185][0m |          -0.0147 |           0.0116 |           0.0000 |
[32m[20221213 20:37:35 @agent_ppo2.py:185][0m |          -0.0281 |           0.0108 |           0.0000 |
[32m[20221213 20:37:35 @agent_ppo2.py:185][0m |          -0.0293 |           0.0106 |           0.0000 |
[32m[20221213 20:37:35 @agent_ppo2.py:185][0m |          -0.0390 |           0.0110 |           0.0000 |
[32m[20221213 20:37:35 @agent_ppo2.py:185][0m |          -0.0425 |           0.0111 |           0.0000 |
[32m[20221213 20:37:35 @agent_ppo2.py:185][0m |          -0.0464 |           0.0104 |           0.0000 |
[32m[20221213 20:37:35 @agent_ppo2.py:185][0m |          -0.0429 |           0.0100 |           0.0000 |
[32m[20221213 20:37:35 @agent_ppo2.py:185][0m |          -0.0476 |           0.0099 |           0.0000 |
[32m[20221213 20:37:35 @agent_ppo2.py:185][0m |          -0.0434 |           0.0098 |           0.0000 |
[32m[20221213 20:37:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:37:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.59
[32m[20221213 20:37:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.71
[32m[20221213 20:37:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.90
[32m[20221213 20:37:36 @agent_ppo2.py:143][0m Total time:       8.44 min
[32m[20221213 20:37:36 @agent_ppo2.py:145][0m 1212416 total steps have happened
[32m[20221213 20:37:36 @agent_ppo2.py:121][0m #------------------------ Iteration 296 --------------------------#
[32m[20221213 20:37:36 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:36 @agent_ppo2.py:185][0m |           0.0552 |           0.0095 |           0.0000 |
[32m[20221213 20:37:36 @agent_ppo2.py:185][0m |          -0.0043 |           0.0093 |           0.0000 |
[32m[20221213 20:37:36 @agent_ppo2.py:185][0m |          -0.0313 |           0.0091 |           0.0000 |
[32m[20221213 20:37:37 @agent_ppo2.py:185][0m |          -0.0583 |           0.0090 |           0.0000 |
[32m[20221213 20:37:37 @agent_ppo2.py:185][0m |          -0.0544 |           0.0089 |           0.0000 |
[32m[20221213 20:37:37 @agent_ppo2.py:185][0m |          -0.0614 |           0.0088 |           0.0000 |
[32m[20221213 20:37:37 @agent_ppo2.py:185][0m |          -0.0715 |           0.0088 |           0.0000 |
[32m[20221213 20:37:37 @agent_ppo2.py:185][0m |          -0.0860 |           0.0087 |           0.0000 |
[32m[20221213 20:37:37 @agent_ppo2.py:185][0m |          -0.0695 |           0.0087 |           0.0000 |
[32m[20221213 20:37:37 @agent_ppo2.py:185][0m |          -0.0799 |           0.0086 |           0.0000 |
[32m[20221213 20:37:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:37:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.79
[32m[20221213 20:37:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.60
[32m[20221213 20:37:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.54
[32m[20221213 20:37:37 @agent_ppo2.py:143][0m Total time:       8.47 min
[32m[20221213 20:37:37 @agent_ppo2.py:145][0m 1216512 total steps have happened
[32m[20221213 20:37:37 @agent_ppo2.py:121][0m #------------------------ Iteration 297 --------------------------#
[32m[20221213 20:37:38 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:38 @agent_ppo2.py:185][0m |           0.0239 |           0.0143 |           0.0000 |
[32m[20221213 20:37:38 @agent_ppo2.py:185][0m |          -0.0117 |           0.0103 |           0.0000 |
[32m[20221213 20:37:38 @agent_ppo2.py:185][0m |          -0.0178 |           0.0100 |           0.0000 |
[32m[20221213 20:37:38 @agent_ppo2.py:185][0m |          -0.0209 |           0.0098 |           0.0000 |
[32m[20221213 20:37:38 @agent_ppo2.py:185][0m |          -0.0243 |           0.0097 |           0.0000 |
[32m[20221213 20:37:38 @agent_ppo2.py:185][0m |          -0.0256 |           0.0096 |           0.0000 |
[32m[20221213 20:37:39 @agent_ppo2.py:185][0m |          -0.0297 |           0.0098 |           0.0000 |
[32m[20221213 20:37:39 @agent_ppo2.py:185][0m |          -0.0292 |           0.0097 |           0.0000 |
[32m[20221213 20:37:39 @agent_ppo2.py:185][0m |          -0.0240 |           0.0093 |           0.0000 |
[32m[20221213 20:37:39 @agent_ppo2.py:185][0m |          -0.0326 |           0.0093 |           0.0000 |
[32m[20221213 20:37:39 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:37:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.90
[32m[20221213 20:37:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.37
[32m[20221213 20:37:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.94
[32m[20221213 20:37:39 @agent_ppo2.py:143][0m Total time:       8.50 min
[32m[20221213 20:37:39 @agent_ppo2.py:145][0m 1220608 total steps have happened
[32m[20221213 20:37:39 @agent_ppo2.py:121][0m #------------------------ Iteration 298 --------------------------#
[32m[20221213 20:37:40 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:40 @agent_ppo2.py:185][0m |           0.0769 |           0.0120 |           0.0000 |
[32m[20221213 20:37:40 @agent_ppo2.py:185][0m |           0.0254 |           0.0076 |           0.0000 |
[32m[20221213 20:37:40 @agent_ppo2.py:185][0m |           0.0115 |           0.0072 |           0.0000 |
[32m[20221213 20:37:40 @agent_ppo2.py:185][0m |           0.0024 |           0.0070 |           0.0000 |
[32m[20221213 20:37:40 @agent_ppo2.py:185][0m |          -0.0052 |           0.0069 |           0.0000 |
[32m[20221213 20:37:40 @agent_ppo2.py:185][0m |          -0.0149 |           0.0068 |           0.0000 |
[32m[20221213 20:37:40 @agent_ppo2.py:185][0m |          -0.0197 |           0.0067 |           0.0000 |
[32m[20221213 20:37:40 @agent_ppo2.py:185][0m |          -0.0230 |           0.0067 |           0.0000 |
[32m[20221213 20:37:40 @agent_ppo2.py:185][0m |          -0.0301 |           0.0066 |           0.0000 |
[32m[20221213 20:37:41 @agent_ppo2.py:185][0m |          -0.0305 |           0.0065 |           0.0000 |
[32m[20221213 20:37:41 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:37:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.92
[32m[20221213 20:37:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.23
[32m[20221213 20:37:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.66
[32m[20221213 20:37:41 @agent_ppo2.py:143][0m Total time:       8.53 min
[32m[20221213 20:37:41 @agent_ppo2.py:145][0m 1224704 total steps have happened
[32m[20221213 20:37:41 @agent_ppo2.py:121][0m #------------------------ Iteration 299 --------------------------#
[32m[20221213 20:37:41 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:41 @agent_ppo2.py:185][0m |           0.0232 |           0.0110 |           0.0000 |
[32m[20221213 20:37:42 @agent_ppo2.py:185][0m |           0.0069 |           0.0090 |           0.0000 |
[32m[20221213 20:37:42 @agent_ppo2.py:185][0m |           0.0043 |           0.0085 |           0.0000 |
[32m[20221213 20:37:42 @agent_ppo2.py:185][0m |          -0.0065 |           0.0083 |           0.0000 |
[32m[20221213 20:37:42 @agent_ppo2.py:185][0m |          -0.0136 |           0.0083 |           0.0000 |
[32m[20221213 20:37:42 @agent_ppo2.py:185][0m |          -0.0172 |           0.0082 |           0.0000 |
[32m[20221213 20:37:42 @agent_ppo2.py:185][0m |          -0.0183 |           0.0081 |           0.0000 |
[32m[20221213 20:37:42 @agent_ppo2.py:185][0m |          -0.0129 |           0.0081 |           0.0000 |
[32m[20221213 20:37:42 @agent_ppo2.py:185][0m |          -0.0154 |           0.0081 |           0.0000 |
[32m[20221213 20:37:42 @agent_ppo2.py:185][0m |          -0.0165 |           0.0080 |           0.0000 |
[32m[20221213 20:37:42 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:37:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.90
[32m[20221213 20:37:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.75
[32m[20221213 20:37:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.59
[32m[20221213 20:37:43 @agent_ppo2.py:143][0m Total time:       8.56 min
[32m[20221213 20:37:43 @agent_ppo2.py:145][0m 1228800 total steps have happened
[32m[20221213 20:37:43 @agent_ppo2.py:121][0m #------------------------ Iteration 300 --------------------------#
[32m[20221213 20:37:43 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:37:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:43 @agent_ppo2.py:185][0m |           0.0256 |           0.0173 |           0.0000 |
[32m[20221213 20:37:43 @agent_ppo2.py:185][0m |          -0.0039 |           0.0119 |           0.0000 |
[32m[20221213 20:37:43 @agent_ppo2.py:185][0m |          -0.0076 |           0.0113 |           0.0000 |
[32m[20221213 20:37:43 @agent_ppo2.py:185][0m |          -0.0146 |           0.0110 |           0.0000 |
[32m[20221213 20:37:44 @agent_ppo2.py:185][0m |          -0.0523 |           0.0108 |           0.0000 |
[32m[20221213 20:37:44 @agent_ppo2.py:185][0m |          -0.0246 |           0.0106 |           0.0000 |
[32m[20221213 20:37:44 @agent_ppo2.py:185][0m |          -0.0289 |           0.0104 |           0.0000 |
[32m[20221213 20:37:44 @agent_ppo2.py:185][0m |          -0.0309 |           0.0104 |           0.0000 |
[32m[20221213 20:37:44 @agent_ppo2.py:185][0m |          -0.0333 |           0.0103 |           0.0000 |
[32m[20221213 20:37:44 @agent_ppo2.py:185][0m |          -0.0348 |           0.0104 |           0.0000 |
[32m[20221213 20:37:44 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:37:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.07
[32m[20221213 20:37:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.40
[32m[20221213 20:37:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.67
[32m[20221213 20:37:44 @agent_ppo2.py:143][0m Total time:       8.59 min
[32m[20221213 20:37:44 @agent_ppo2.py:145][0m 1232896 total steps have happened
[32m[20221213 20:37:44 @agent_ppo2.py:121][0m #------------------------ Iteration 301 --------------------------#
[32m[20221213 20:37:45 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:37:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:45 @agent_ppo2.py:185][0m |           0.0750 |           0.0112 |           0.0000 |
[32m[20221213 20:37:45 @agent_ppo2.py:185][0m |           0.0510 |           0.0079 |           0.0000 |
[32m[20221213 20:37:45 @agent_ppo2.py:185][0m |           0.0095 |           0.0077 |           0.0000 |
[32m[20221213 20:37:45 @agent_ppo2.py:185][0m |          -0.0051 |           0.0076 |           0.0000 |
[32m[20221213 20:37:45 @agent_ppo2.py:185][0m |          -0.0162 |           0.0075 |           0.0000 |
[32m[20221213 20:37:45 @agent_ppo2.py:185][0m |          -0.0209 |           0.0075 |           0.0000 |
[32m[20221213 20:37:46 @agent_ppo2.py:185][0m |          -0.0266 |           0.0074 |           0.0000 |
[32m[20221213 20:37:46 @agent_ppo2.py:185][0m |          -0.0309 |           0.0073 |           0.0000 |
[32m[20221213 20:37:46 @agent_ppo2.py:185][0m |          -0.0262 |           0.0073 |           0.0000 |
[32m[20221213 20:37:46 @agent_ppo2.py:185][0m |          -0.0387 |           0.0072 |           0.0000 |
[32m[20221213 20:37:46 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:37:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.87
[32m[20221213 20:37:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.32
[32m[20221213 20:37:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.91
[32m[20221213 20:37:46 @agent_ppo2.py:143][0m Total time:       8.62 min
[32m[20221213 20:37:46 @agent_ppo2.py:145][0m 1236992 total steps have happened
[32m[20221213 20:37:46 @agent_ppo2.py:121][0m #------------------------ Iteration 302 --------------------------#
[32m[20221213 20:37:47 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:37:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:47 @agent_ppo2.py:185][0m |           0.0083 |           0.0389 |           0.0000 |
[32m[20221213 20:37:47 @agent_ppo2.py:185][0m |           0.0098 |           0.0180 |           0.0000 |
[32m[20221213 20:37:47 @agent_ppo2.py:185][0m |          -0.0039 |           0.0158 |           0.0000 |
[32m[20221213 20:37:47 @agent_ppo2.py:185][0m |          -0.0125 |           0.0152 |           0.0000 |
[32m[20221213 20:37:47 @agent_ppo2.py:185][0m |          -0.0161 |           0.0151 |           0.0000 |
[32m[20221213 20:37:47 @agent_ppo2.py:185][0m |          -0.0198 |           0.0143 |           0.0000 |
[32m[20221213 20:37:47 @agent_ppo2.py:185][0m |          -0.0187 |           0.0144 |           0.0000 |
[32m[20221213 20:37:47 @agent_ppo2.py:185][0m |          -0.0201 |           0.0137 |           0.0000 |
[32m[20221213 20:37:47 @agent_ppo2.py:185][0m |          -0.0205 |           0.0138 |           0.0000 |
[32m[20221213 20:37:48 @agent_ppo2.py:185][0m |          -0.0216 |           0.0136 |           0.0000 |
[32m[20221213 20:37:48 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:37:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.69
[32m[20221213 20:37:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.62
[32m[20221213 20:37:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.27
[32m[20221213 20:37:48 @agent_ppo2.py:143][0m Total time:       8.65 min
[32m[20221213 20:37:48 @agent_ppo2.py:145][0m 1241088 total steps have happened
[32m[20221213 20:37:48 @agent_ppo2.py:121][0m #------------------------ Iteration 303 --------------------------#
[32m[20221213 20:37:48 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:48 @agent_ppo2.py:185][0m |           0.0297 |           0.0182 |           0.0000 |
[32m[20221213 20:37:49 @agent_ppo2.py:185][0m |          -0.0107 |           0.0140 |           0.0000 |
[32m[20221213 20:37:49 @agent_ppo2.py:185][0m |          -0.0322 |           0.0131 |           0.0000 |
[32m[20221213 20:37:49 @agent_ppo2.py:185][0m |          -0.0463 |           0.0128 |           0.0000 |
[32m[20221213 20:37:49 @agent_ppo2.py:185][0m |          -0.0564 |           0.0127 |           0.0000 |
[32m[20221213 20:37:49 @agent_ppo2.py:185][0m |          -0.0706 |           0.0124 |           0.0000 |
[32m[20221213 20:37:49 @agent_ppo2.py:185][0m |          -0.0543 |           0.0122 |           0.0000 |
[32m[20221213 20:37:49 @agent_ppo2.py:185][0m |          -0.0779 |           0.0124 |           0.0000 |
[32m[20221213 20:37:49 @agent_ppo2.py:185][0m |          -0.0797 |           0.0120 |           0.0000 |
[32m[20221213 20:37:49 @agent_ppo2.py:185][0m |          -0.0599 |           0.0116 |           0.0000 |
[32m[20221213 20:37:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:37:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.12
[32m[20221213 20:37:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.93
[32m[20221213 20:37:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.87
[32m[20221213 20:37:50 @agent_ppo2.py:143][0m Total time:       8.67 min
[32m[20221213 20:37:50 @agent_ppo2.py:145][0m 1245184 total steps have happened
[32m[20221213 20:37:50 @agent_ppo2.py:121][0m #------------------------ Iteration 304 --------------------------#
[32m[20221213 20:37:50 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:37:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:50 @agent_ppo2.py:185][0m |           0.0319 |           0.0338 |           0.0000 |
[32m[20221213 20:37:50 @agent_ppo2.py:185][0m |           0.0029 |           0.0259 |           0.0000 |
[32m[20221213 20:37:50 @agent_ppo2.py:185][0m |          -0.0092 |           0.0247 |           0.0000 |
[32m[20221213 20:37:50 @agent_ppo2.py:185][0m |          -0.0170 |           0.0234 |           0.0000 |
[32m[20221213 20:37:51 @agent_ppo2.py:185][0m |          -0.0209 |           0.0224 |           0.0000 |
[32m[20221213 20:37:51 @agent_ppo2.py:185][0m |          -0.0256 |           0.0216 |           0.0000 |
[32m[20221213 20:37:51 @agent_ppo2.py:185][0m |          -0.0290 |           0.0219 |           0.0000 |
[32m[20221213 20:37:51 @agent_ppo2.py:185][0m |          -0.0307 |           0.0207 |           0.0000 |
[32m[20221213 20:37:51 @agent_ppo2.py:185][0m |          -0.0314 |           0.0206 |           0.0000 |
[32m[20221213 20:37:51 @agent_ppo2.py:185][0m |          -0.0319 |           0.0206 |           0.0000 |
[32m[20221213 20:37:51 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:37:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.95
[32m[20221213 20:37:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.05
[32m[20221213 20:37:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.08
[32m[20221213 20:37:51 @agent_ppo2.py:143][0m Total time:       8.70 min
[32m[20221213 20:37:51 @agent_ppo2.py:145][0m 1249280 total steps have happened
[32m[20221213 20:37:51 @agent_ppo2.py:121][0m #------------------------ Iteration 305 --------------------------#
[32m[20221213 20:37:52 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:37:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:52 @agent_ppo2.py:185][0m |           0.0314 |           0.0167 |           0.0000 |
[32m[20221213 20:37:52 @agent_ppo2.py:185][0m |           0.0248 |           0.0090 |           0.0000 |
[32m[20221213 20:37:52 @agent_ppo2.py:185][0m |          -0.0038 |           0.0087 |           0.0000 |
[32m[20221213 20:37:52 @agent_ppo2.py:185][0m |          -0.0104 |           0.0086 |           0.0000 |
[32m[20221213 20:37:52 @agent_ppo2.py:185][0m |          -0.0300 |           0.0085 |           0.0000 |
[32m[20221213 20:37:52 @agent_ppo2.py:185][0m |          -0.0278 |           0.0084 |           0.0000 |
[32m[20221213 20:37:53 @agent_ppo2.py:185][0m |          -0.0082 |           0.0086 |           0.0000 |
[32m[20221213 20:37:53 @agent_ppo2.py:185][0m |          -0.0157 |           0.0084 |           0.0000 |
[32m[20221213 20:37:53 @agent_ppo2.py:185][0m |          -0.0200 |           0.0082 |           0.0000 |
[32m[20221213 20:37:53 @agent_ppo2.py:185][0m |          -0.0310 |           0.0082 |           0.0000 |
[32m[20221213 20:37:53 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:37:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.04
[32m[20221213 20:37:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.33
[32m[20221213 20:37:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.15
[32m[20221213 20:37:53 @agent_ppo2.py:143][0m Total time:       8.73 min
[32m[20221213 20:37:53 @agent_ppo2.py:145][0m 1253376 total steps have happened
[32m[20221213 20:37:53 @agent_ppo2.py:121][0m #------------------------ Iteration 306 --------------------------#
[32m[20221213 20:37:54 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:37:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:54 @agent_ppo2.py:185][0m |           0.0233 |           0.0212 |           0.0000 |
[32m[20221213 20:37:54 @agent_ppo2.py:185][0m |           0.0029 |           0.0152 |           0.0000 |
[32m[20221213 20:37:54 @agent_ppo2.py:185][0m |          -0.0038 |           0.0144 |           0.0000 |
[32m[20221213 20:37:54 @agent_ppo2.py:185][0m |          -0.0141 |           0.0142 |           0.0000 |
[32m[20221213 20:37:54 @agent_ppo2.py:185][0m |          -0.0209 |           0.0140 |           0.0000 |
[32m[20221213 20:37:54 @agent_ppo2.py:185][0m |          -0.0196 |           0.0139 |           0.0000 |
[32m[20221213 20:37:54 @agent_ppo2.py:185][0m |          -0.0247 |           0.0138 |           0.0000 |
[32m[20221213 20:37:54 @agent_ppo2.py:185][0m |          -0.0815 |           0.0152 |           0.0000 |
[32m[20221213 20:37:54 @agent_ppo2.py:185][0m |          -0.0291 |           0.0150 |           0.0000 |
[32m[20221213 20:37:55 @agent_ppo2.py:185][0m |          -0.0315 |           0.0133 |           0.0000 |
[32m[20221213 20:37:55 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:37:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.09
[32m[20221213 20:37:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.60
[32m[20221213 20:37:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.62
[32m[20221213 20:37:55 @agent_ppo2.py:143][0m Total time:       8.76 min
[32m[20221213 20:37:55 @agent_ppo2.py:145][0m 1257472 total steps have happened
[32m[20221213 20:37:55 @agent_ppo2.py:121][0m #------------------------ Iteration 307 --------------------------#
[32m[20221213 20:37:55 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:56 @agent_ppo2.py:185][0m |           0.0306 |           0.0215 |           0.0000 |
[32m[20221213 20:37:56 @agent_ppo2.py:185][0m |          -0.0111 |           0.0163 |           0.0000 |
[32m[20221213 20:37:56 @agent_ppo2.py:185][0m |          -0.0135 |           0.0151 |           0.0000 |
[32m[20221213 20:37:56 @agent_ppo2.py:185][0m |          -0.0287 |           0.0147 |           0.0000 |
[32m[20221213 20:37:56 @agent_ppo2.py:185][0m |          -0.0356 |           0.0141 |           0.0000 |
[32m[20221213 20:37:56 @agent_ppo2.py:185][0m |          -0.0340 |           0.0140 |           0.0000 |
[32m[20221213 20:37:56 @agent_ppo2.py:185][0m |          -0.0362 |           0.0136 |           0.0000 |
[32m[20221213 20:37:56 @agent_ppo2.py:185][0m |          -0.0414 |           0.0135 |           0.0000 |
[32m[20221213 20:37:56 @agent_ppo2.py:185][0m |          -0.0430 |           0.0133 |           0.0000 |
[32m[20221213 20:37:56 @agent_ppo2.py:185][0m |          -0.0460 |           0.0131 |           0.0000 |
[32m[20221213 20:37:56 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:37:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.07
[32m[20221213 20:37:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.77
[32m[20221213 20:37:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.67
[32m[20221213 20:37:57 @agent_ppo2.py:143][0m Total time:       8.79 min
[32m[20221213 20:37:57 @agent_ppo2.py:145][0m 1261568 total steps have happened
[32m[20221213 20:37:57 @agent_ppo2.py:121][0m #------------------------ Iteration 308 --------------------------#
[32m[20221213 20:37:57 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:37:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:57 @agent_ppo2.py:185][0m |           0.0220 |           0.0190 |           0.0000 |
[32m[20221213 20:37:57 @agent_ppo2.py:185][0m |           0.0071 |           0.0102 |           0.0000 |
[32m[20221213 20:37:57 @agent_ppo2.py:185][0m |          -0.0126 |           0.0095 |           0.0000 |
[32m[20221213 20:37:58 @agent_ppo2.py:185][0m |          -0.0260 |           0.0092 |           0.0000 |
[32m[20221213 20:37:58 @agent_ppo2.py:185][0m |          -0.0287 |           0.0091 |           0.0000 |
[32m[20221213 20:37:58 @agent_ppo2.py:185][0m |          -0.0414 |           0.0090 |           0.0000 |
[32m[20221213 20:37:58 @agent_ppo2.py:185][0m |          -0.0408 |           0.0089 |           0.0000 |
[32m[20221213 20:37:58 @agent_ppo2.py:185][0m |          -0.0476 |           0.0088 |           0.0000 |
[32m[20221213 20:37:58 @agent_ppo2.py:185][0m |          -0.0448 |           0.0088 |           0.0000 |
[32m[20221213 20:37:58 @agent_ppo2.py:185][0m |          -0.0480 |           0.0086 |           0.0000 |
[32m[20221213 20:37:58 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:37:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.95
[32m[20221213 20:37:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.68
[32m[20221213 20:37:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.10
[32m[20221213 20:37:58 @agent_ppo2.py:143][0m Total time:       8.82 min
[32m[20221213 20:37:58 @agent_ppo2.py:145][0m 1265664 total steps have happened
[32m[20221213 20:37:58 @agent_ppo2.py:121][0m #------------------------ Iteration 309 --------------------------#
[32m[20221213 20:37:59 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:37:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:37:59 @agent_ppo2.py:185][0m |           0.0185 |           0.0120 |           0.0000 |
[32m[20221213 20:37:59 @agent_ppo2.py:185][0m |          -0.0038 |           0.0101 |           0.0000 |
[32m[20221213 20:37:59 @agent_ppo2.py:185][0m |          -0.0179 |           0.0098 |           0.0000 |
[32m[20221213 20:37:59 @agent_ppo2.py:185][0m |          -0.0285 |           0.0097 |           0.0000 |
[32m[20221213 20:37:59 @agent_ppo2.py:185][0m |          -0.0329 |           0.0096 |           0.0000 |
[32m[20221213 20:38:00 @agent_ppo2.py:185][0m |          -0.0394 |           0.0096 |           0.0000 |
[32m[20221213 20:38:00 @agent_ppo2.py:185][0m |          -0.0438 |           0.0095 |           0.0000 |
[32m[20221213 20:38:00 @agent_ppo2.py:185][0m |          -0.0433 |           0.0094 |           0.0000 |
[32m[20221213 20:38:00 @agent_ppo2.py:185][0m |          -0.0422 |           0.0094 |           0.0000 |
[32m[20221213 20:38:00 @agent_ppo2.py:185][0m |          -0.0442 |           0.0093 |           0.0000 |
[32m[20221213 20:38:00 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:38:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.69
[32m[20221213 20:38:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.29
[32m[20221213 20:38:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.85
[32m[20221213 20:38:00 @agent_ppo2.py:143][0m Total time:       8.85 min
[32m[20221213 20:38:00 @agent_ppo2.py:145][0m 1269760 total steps have happened
[32m[20221213 20:38:00 @agent_ppo2.py:121][0m #------------------------ Iteration 310 --------------------------#
[32m[20221213 20:38:01 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:38:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:01 @agent_ppo2.py:185][0m |           0.0352 |           0.0096 |           0.0000 |
[32m[20221213 20:38:01 @agent_ppo2.py:185][0m |           0.0222 |           0.0092 |           0.0000 |
[32m[20221213 20:38:01 @agent_ppo2.py:185][0m |          -0.0050 |           0.0091 |           0.0000 |
[32m[20221213 20:38:01 @agent_ppo2.py:185][0m |          -0.0297 |           0.0090 |           0.0000 |
[32m[20221213 20:38:01 @agent_ppo2.py:185][0m |          -0.0424 |           0.0090 |           0.0000 |
[32m[20221213 20:38:01 @agent_ppo2.py:185][0m |          -0.0535 |           0.0090 |           0.0000 |
[32m[20221213 20:38:01 @agent_ppo2.py:185][0m |          -0.0536 |           0.0089 |           0.0000 |
[32m[20221213 20:38:01 @agent_ppo2.py:185][0m |          -0.0563 |           0.0088 |           0.0000 |
[32m[20221213 20:38:02 @agent_ppo2.py:185][0m |          -0.0620 |           0.0088 |           0.0000 |
[32m[20221213 20:38:02 @agent_ppo2.py:185][0m |          -0.0601 |           0.0087 |           0.0000 |
[32m[20221213 20:38:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:38:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.65
[32m[20221213 20:38:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.53
[32m[20221213 20:38:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.63
[32m[20221213 20:38:02 @agent_ppo2.py:143][0m Total time:       8.88 min
[32m[20221213 20:38:02 @agent_ppo2.py:145][0m 1273856 total steps have happened
[32m[20221213 20:38:02 @agent_ppo2.py:121][0m #------------------------ Iteration 311 --------------------------#
[32m[20221213 20:38:02 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:03 @agent_ppo2.py:185][0m |           0.0250 |           0.0104 |           0.0000 |
[32m[20221213 20:38:03 @agent_ppo2.py:185][0m |           0.0036 |           0.0080 |           0.0000 |
[32m[20221213 20:38:03 @agent_ppo2.py:185][0m |          -0.0132 |           0.0078 |           0.0000 |
[32m[20221213 20:38:03 @agent_ppo2.py:185][0m |          -0.0164 |           0.0077 |           0.0000 |
[32m[20221213 20:38:03 @agent_ppo2.py:185][0m |          -0.0287 |           0.0076 |           0.0000 |
[32m[20221213 20:38:03 @agent_ppo2.py:185][0m |          -0.0302 |           0.0074 |           0.0000 |
[32m[20221213 20:38:03 @agent_ppo2.py:185][0m |          -0.0352 |           0.0074 |           0.0000 |
[32m[20221213 20:38:03 @agent_ppo2.py:185][0m |          -0.0384 |           0.0073 |           0.0000 |
[32m[20221213 20:38:03 @agent_ppo2.py:185][0m |          -0.0436 |           0.0072 |           0.0000 |
[32m[20221213 20:38:03 @agent_ppo2.py:185][0m |          -0.0463 |           0.0071 |           0.0000 |
[32m[20221213 20:38:03 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:38:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.16
[32m[20221213 20:38:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.03
[32m[20221213 20:38:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.29
[32m[20221213 20:38:04 @agent_ppo2.py:143][0m Total time:       8.91 min
[32m[20221213 20:38:04 @agent_ppo2.py:145][0m 1277952 total steps have happened
[32m[20221213 20:38:04 @agent_ppo2.py:121][0m #------------------------ Iteration 312 --------------------------#
[32m[20221213 20:38:04 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:38:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:04 @agent_ppo2.py:185][0m |           0.0209 |           0.0143 |           0.0000 |
[32m[20221213 20:38:04 @agent_ppo2.py:185][0m |          -0.0032 |           0.0119 |           0.0000 |
[32m[20221213 20:38:04 @agent_ppo2.py:185][0m |          -0.0114 |           0.0114 |           0.0000 |
[32m[20221213 20:38:05 @agent_ppo2.py:185][0m |          -0.0180 |           0.0113 |           0.0000 |
[32m[20221213 20:38:05 @agent_ppo2.py:185][0m |          -0.0179 |           0.0111 |           0.0000 |
[32m[20221213 20:38:05 @agent_ppo2.py:185][0m |          -0.0200 |           0.0109 |           0.0000 |
[32m[20221213 20:38:05 @agent_ppo2.py:185][0m |          -0.0216 |           0.0108 |           0.0000 |
[32m[20221213 20:38:05 @agent_ppo2.py:185][0m |          -0.0243 |           0.0105 |           0.0000 |
[32m[20221213 20:38:05 @agent_ppo2.py:185][0m |          -0.0240 |           0.0105 |           0.0000 |
[32m[20221213 20:38:05 @agent_ppo2.py:185][0m |          -0.0247 |           0.0103 |           0.0000 |
[32m[20221213 20:38:05 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 20:38:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.04
[32m[20221213 20:38:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.83
[32m[20221213 20:38:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.49
[32m[20221213 20:38:06 @agent_ppo2.py:143][0m Total time:       8.94 min
[32m[20221213 20:38:06 @agent_ppo2.py:145][0m 1282048 total steps have happened
[32m[20221213 20:38:06 @agent_ppo2.py:121][0m #------------------------ Iteration 313 --------------------------#
[32m[20221213 20:38:06 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:38:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:06 @agent_ppo2.py:185][0m |           0.0168 |           0.0093 |           0.0000 |
[32m[20221213 20:38:06 @agent_ppo2.py:185][0m |          -0.0040 |           0.0091 |           0.0000 |
[32m[20221213 20:38:06 @agent_ppo2.py:185][0m |          -0.0250 |           0.0089 |           0.0000 |
[32m[20221213 20:38:06 @agent_ppo2.py:185][0m |          -0.0323 |           0.0089 |           0.0000 |
[32m[20221213 20:38:06 @agent_ppo2.py:185][0m |          -0.1027 |           0.0105 |           0.0000 |
[32m[20221213 20:38:07 @agent_ppo2.py:185][0m |          -0.0406 |           0.0112 |           0.0000 |
[32m[20221213 20:38:07 @agent_ppo2.py:185][0m |          -0.0799 |           0.0093 |           0.0000 |
[32m[20221213 20:38:07 @agent_ppo2.py:185][0m |          -0.0206 |           0.0094 |           0.0000 |
[32m[20221213 20:38:07 @agent_ppo2.py:185][0m |          -0.0252 |           0.0085 |           0.0000 |
[32m[20221213 20:38:07 @agent_ppo2.py:185][0m |          -0.0369 |           0.0084 |           0.0000 |
[32m[20221213 20:38:07 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:38:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.88
[32m[20221213 20:38:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.16
[32m[20221213 20:38:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.65
[32m[20221213 20:38:07 @agent_ppo2.py:143][0m Total time:       8.97 min
[32m[20221213 20:38:07 @agent_ppo2.py:145][0m 1286144 total steps have happened
[32m[20221213 20:38:07 @agent_ppo2.py:121][0m #------------------------ Iteration 314 --------------------------#
[32m[20221213 20:38:08 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:38:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:08 @agent_ppo2.py:185][0m |           0.0342 |           0.0074 |           0.0000 |
[32m[20221213 20:38:08 @agent_ppo2.py:185][0m |           0.0047 |           0.0065 |           0.0000 |
[32m[20221213 20:38:08 @agent_ppo2.py:185][0m |           0.0001 |           0.0063 |           0.0000 |
[32m[20221213 20:38:08 @agent_ppo2.py:185][0m |          -0.0174 |           0.0062 |           0.0000 |
[32m[20221213 20:38:08 @agent_ppo2.py:185][0m |          -0.0318 |           0.0062 |           0.0000 |
[32m[20221213 20:38:08 @agent_ppo2.py:185][0m |          -0.0394 |           0.0061 |           0.0000 |
[32m[20221213 20:38:08 @agent_ppo2.py:185][0m |          -0.0480 |           0.0061 |           0.0000 |
[32m[20221213 20:38:09 @agent_ppo2.py:185][0m |          -0.0580 |           0.0060 |           0.0000 |
[32m[20221213 20:38:09 @agent_ppo2.py:185][0m |          -0.0516 |           0.0059 |           0.0000 |
[32m[20221213 20:38:09 @agent_ppo2.py:185][0m |          -0.0565 |           0.0059 |           0.0000 |
[32m[20221213 20:38:09 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:38:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.71
[32m[20221213 20:38:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.05
[32m[20221213 20:38:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.19
[32m[20221213 20:38:09 @agent_ppo2.py:143][0m Total time:       9.00 min
[32m[20221213 20:38:09 @agent_ppo2.py:145][0m 1290240 total steps have happened
[32m[20221213 20:38:09 @agent_ppo2.py:121][0m #------------------------ Iteration 315 --------------------------#
[32m[20221213 20:38:09 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:38:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:10 @agent_ppo2.py:185][0m |           0.0190 |           0.0069 |           0.0000 |
[32m[20221213 20:38:10 @agent_ppo2.py:185][0m |          -0.0098 |           0.0067 |           0.0000 |
[32m[20221213 20:38:10 @agent_ppo2.py:185][0m |          -0.0254 |           0.0066 |           0.0000 |
[32m[20221213 20:38:10 @agent_ppo2.py:185][0m |          -0.0292 |           0.0065 |           0.0000 |
[32m[20221213 20:38:10 @agent_ppo2.py:185][0m |          -0.0360 |           0.0065 |           0.0000 |
[32m[20221213 20:38:10 @agent_ppo2.py:185][0m |          -0.0430 |           0.0064 |           0.0000 |
[32m[20221213 20:38:10 @agent_ppo2.py:185][0m |          -0.0416 |           0.0063 |           0.0000 |
[32m[20221213 20:38:10 @agent_ppo2.py:185][0m |          -0.0449 |           0.0063 |           0.0000 |
[32m[20221213 20:38:10 @agent_ppo2.py:185][0m |          -0.0473 |           0.0063 |           0.0000 |
[32m[20221213 20:38:11 @agent_ppo2.py:185][0m |          -0.0532 |           0.0062 |           0.0000 |
[32m[20221213 20:38:11 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 20:38:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.99
[32m[20221213 20:38:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.59
[32m[20221213 20:38:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.71
[32m[20221213 20:38:11 @agent_ppo2.py:143][0m Total time:       9.03 min
[32m[20221213 20:38:11 @agent_ppo2.py:145][0m 1294336 total steps have happened
[32m[20221213 20:38:11 @agent_ppo2.py:121][0m #------------------------ Iteration 316 --------------------------#
[32m[20221213 20:38:11 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:38:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:11 @agent_ppo2.py:185][0m |           0.0266 |           0.0322 |           0.0000 |
[32m[20221213 20:38:12 @agent_ppo2.py:185][0m |          -0.0053 |           0.0192 |           0.0000 |
[32m[20221213 20:38:12 @agent_ppo2.py:185][0m |          -0.0016 |           0.0157 |           0.0000 |
[32m[20221213 20:38:12 @agent_ppo2.py:185][0m |          -0.0133 |           0.0147 |           0.0000 |
[32m[20221213 20:38:12 @agent_ppo2.py:185][0m |          -0.0173 |           0.0146 |           0.0000 |
[32m[20221213 20:38:12 @agent_ppo2.py:185][0m |          -0.0123 |           0.0148 |           0.0000 |
[32m[20221213 20:38:12 @agent_ppo2.py:185][0m |          -0.0195 |           0.0149 |           0.0000 |
[32m[20221213 20:38:12 @agent_ppo2.py:185][0m |          -0.0230 |           0.0129 |           0.0000 |
[32m[20221213 20:38:12 @agent_ppo2.py:185][0m |          -0.0220 |           0.0132 |           0.0000 |
[32m[20221213 20:38:12 @agent_ppo2.py:185][0m |          -0.0248 |           0.0133 |           0.0000 |
[32m[20221213 20:38:12 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 20:38:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.46
[32m[20221213 20:38:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.81
[32m[20221213 20:38:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.47
[32m[20221213 20:38:13 @agent_ppo2.py:143][0m Total time:       9.06 min
[32m[20221213 20:38:13 @agent_ppo2.py:145][0m 1298432 total steps have happened
[32m[20221213 20:38:13 @agent_ppo2.py:121][0m #------------------------ Iteration 317 --------------------------#
[32m[20221213 20:38:13 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:38:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:13 @agent_ppo2.py:185][0m |           0.0359 |           0.0796 |           0.0000 |
[32m[20221213 20:38:13 @agent_ppo2.py:185][0m |          -0.0040 |           0.0413 |           0.0000 |
[32m[20221213 20:38:14 @agent_ppo2.py:185][0m |          -0.0131 |           0.0324 |           0.0000 |
[32m[20221213 20:38:14 @agent_ppo2.py:185][0m |          -0.0221 |           0.0310 |           0.0000 |
[32m[20221213 20:38:14 @agent_ppo2.py:185][0m |          -0.0288 |           0.0274 |           0.0000 |
[32m[20221213 20:38:14 @agent_ppo2.py:185][0m |          -0.0237 |           0.0249 |           0.0000 |
[32m[20221213 20:38:14 @agent_ppo2.py:185][0m |          -0.0300 |           0.0238 |           0.0000 |
[32m[20221213 20:38:14 @agent_ppo2.py:185][0m |          -0.0346 |           0.0227 |           0.0000 |
[32m[20221213 20:38:14 @agent_ppo2.py:185][0m |          -0.0345 |           0.0230 |           0.0000 |
[32m[20221213 20:38:14 @agent_ppo2.py:185][0m |          -0.0364 |           0.0220 |           0.0000 |
[32m[20221213 20:38:14 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:38:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.10
[32m[20221213 20:38:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.10
[32m[20221213 20:38:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.23
[32m[20221213 20:38:14 @agent_ppo2.py:143][0m Total time:       9.09 min
[32m[20221213 20:38:14 @agent_ppo2.py:145][0m 1302528 total steps have happened
[32m[20221213 20:38:14 @agent_ppo2.py:121][0m #------------------------ Iteration 318 --------------------------#
[32m[20221213 20:38:15 @agent_ppo2.py:127][0m Sampling time: 0.43 s by 5 slaves
[32m[20221213 20:38:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:15 @agent_ppo2.py:185][0m |           0.0319 |           0.0348 |           0.0000 |
[32m[20221213 20:38:15 @agent_ppo2.py:185][0m |          -0.0085 |           0.0278 |           0.0000 |
[32m[20221213 20:38:15 @agent_ppo2.py:185][0m |          -0.0012 |           0.0275 |           0.0000 |
[32m[20221213 20:38:15 @agent_ppo2.py:185][0m |          -0.0349 |           0.0244 |           0.0000 |
[32m[20221213 20:38:15 @agent_ppo2.py:185][0m |          -0.0397 |           0.0239 |           0.0000 |
[32m[20221213 20:38:16 @agent_ppo2.py:185][0m |          -0.0490 |           0.0235 |           0.0000 |
[32m[20221213 20:38:16 @agent_ppo2.py:185][0m |          -0.0576 |           0.0228 |           0.0000 |
[32m[20221213 20:38:16 @agent_ppo2.py:185][0m |          -0.0544 |           0.0216 |           0.0000 |
[32m[20221213 20:38:16 @agent_ppo2.py:185][0m |          -0.0609 |           0.0209 |           0.0000 |
[32m[20221213 20:38:16 @agent_ppo2.py:185][0m |          -0.0642 |           0.0209 |           0.0000 |
[32m[20221213 20:38:16 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:38:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.96
[32m[20221213 20:38:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.13
[32m[20221213 20:38:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.50
[32m[20221213 20:38:16 @agent_ppo2.py:143][0m Total time:       9.12 min
[32m[20221213 20:38:16 @agent_ppo2.py:145][0m 1306624 total steps have happened
[32m[20221213 20:38:16 @agent_ppo2.py:121][0m #------------------------ Iteration 319 --------------------------#
[32m[20221213 20:38:17 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:38:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:17 @agent_ppo2.py:185][0m |           0.0259 |           0.0155 |           0.0000 |
[32m[20221213 20:38:17 @agent_ppo2.py:185][0m |          -0.0137 |           0.0122 |           0.0000 |
[32m[20221213 20:38:17 @agent_ppo2.py:185][0m |          -0.0313 |           0.0118 |           0.0000 |
[32m[20221213 20:38:17 @agent_ppo2.py:185][0m |          -0.0455 |           0.0116 |           0.0000 |
[32m[20221213 20:38:17 @agent_ppo2.py:185][0m |          -0.0399 |           0.0114 |           0.0000 |
[32m[20221213 20:38:17 @agent_ppo2.py:185][0m |          -0.0395 |           0.0117 |           0.0000 |
[32m[20221213 20:38:17 @agent_ppo2.py:185][0m |          -0.0696 |           0.0115 |           0.0000 |
[32m[20221213 20:38:17 @agent_ppo2.py:185][0m |          -0.0591 |           0.0113 |           0.0000 |
[32m[20221213 20:38:17 @agent_ppo2.py:185][0m |          -0.0525 |           0.0138 |           0.0000 |
[32m[20221213 20:38:18 @agent_ppo2.py:185][0m |          -0.0753 |           0.0133 |           0.0000 |
[32m[20221213 20:38:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:38:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.58
[32m[20221213 20:38:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.07
[32m[20221213 20:38:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.95
[32m[20221213 20:38:18 @agent_ppo2.py:143][0m Total time:       9.15 min
[32m[20221213 20:38:18 @agent_ppo2.py:145][0m 1310720 total steps have happened
[32m[20221213 20:38:18 @agent_ppo2.py:121][0m #------------------------ Iteration 320 --------------------------#
[32m[20221213 20:38:18 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:38:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:19 @agent_ppo2.py:185][0m |           0.0198 |           0.0166 |           0.0000 |
[32m[20221213 20:38:19 @agent_ppo2.py:185][0m |          -0.0021 |           0.0145 |           0.0000 |
[32m[20221213 20:38:19 @agent_ppo2.py:185][0m |          -0.0336 |           0.0138 |           0.0000 |
[32m[20221213 20:38:19 @agent_ppo2.py:185][0m |          -0.0390 |           0.0133 |           0.0000 |
[32m[20221213 20:38:19 @agent_ppo2.py:185][0m |          -0.0393 |           0.0130 |           0.0000 |
[32m[20221213 20:38:19 @agent_ppo2.py:185][0m |          -0.0426 |           0.0126 |           0.0000 |
[32m[20221213 20:38:19 @agent_ppo2.py:185][0m |          -0.0439 |           0.0124 |           0.0000 |
[32m[20221213 20:38:19 @agent_ppo2.py:185][0m |          -0.0478 |           0.0125 |           0.0000 |
[32m[20221213 20:38:19 @agent_ppo2.py:185][0m |          -0.0496 |           0.0125 |           0.0000 |
[32m[20221213 20:38:19 @agent_ppo2.py:185][0m |          -0.0452 |           0.0121 |           0.0000 |
[32m[20221213 20:38:19 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:38:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.10
[32m[20221213 20:38:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.72
[32m[20221213 20:38:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.15
[32m[20221213 20:38:20 @agent_ppo2.py:143][0m Total time:       9.18 min
[32m[20221213 20:38:20 @agent_ppo2.py:145][0m 1314816 total steps have happened
[32m[20221213 20:38:20 @agent_ppo2.py:121][0m #------------------------ Iteration 321 --------------------------#
[32m[20221213 20:38:20 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:38:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:20 @agent_ppo2.py:185][0m |           0.0417 |           0.0128 |           0.0000 |
[32m[20221213 20:38:20 @agent_ppo2.py:185][0m |           0.0269 |           0.0130 |           0.0000 |
[32m[20221213 20:38:21 @agent_ppo2.py:185][0m |          -0.0254 |           0.0129 |           0.0000 |
[32m[20221213 20:38:21 @agent_ppo2.py:185][0m |          -0.0334 |           0.0121 |           0.0000 |
[32m[20221213 20:38:21 @agent_ppo2.py:185][0m |          -0.0413 |           0.0120 |           0.0000 |
[32m[20221213 20:38:21 @agent_ppo2.py:185][0m |          -0.0553 |           0.0119 |           0.0000 |
[32m[20221213 20:38:21 @agent_ppo2.py:185][0m |          -0.0665 |           0.0117 |           0.0000 |
[32m[20221213 20:38:21 @agent_ppo2.py:185][0m |          -0.0733 |           0.0117 |           0.0000 |
[32m[20221213 20:38:21 @agent_ppo2.py:185][0m |          -0.0739 |           0.0116 |           0.0000 |
[32m[20221213 20:38:21 @agent_ppo2.py:185][0m |          -0.0729 |           0.0117 |           0.0000 |
[32m[20221213 20:38:21 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:38:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.14
[32m[20221213 20:38:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.93
[32m[20221213 20:38:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.82
[32m[20221213 20:38:21 @agent_ppo2.py:143][0m Total time:       9.21 min
[32m[20221213 20:38:21 @agent_ppo2.py:145][0m 1318912 total steps have happened
[32m[20221213 20:38:21 @agent_ppo2.py:121][0m #------------------------ Iteration 322 --------------------------#
[32m[20221213 20:38:22 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:22 @agent_ppo2.py:185][0m |           0.0308 |           0.0150 |           0.0000 |
[32m[20221213 20:38:22 @agent_ppo2.py:185][0m |          -0.0053 |           0.0140 |           0.0000 |
[32m[20221213 20:38:22 @agent_ppo2.py:185][0m |          -0.0234 |           0.0134 |           0.0000 |
[32m[20221213 20:38:22 @agent_ppo2.py:185][0m |          -0.0327 |           0.0133 |           0.0000 |
[32m[20221213 20:38:22 @agent_ppo2.py:185][0m |          -0.0386 |           0.0133 |           0.0000 |
[32m[20221213 20:38:22 @agent_ppo2.py:185][0m |          -0.0434 |           0.0130 |           0.0000 |
[32m[20221213 20:38:23 @agent_ppo2.py:185][0m |          -0.0421 |           0.0126 |           0.0000 |
[32m[20221213 20:38:23 @agent_ppo2.py:185][0m |          -0.0494 |           0.0126 |           0.0000 |
[32m[20221213 20:38:23 @agent_ppo2.py:185][0m |          -0.0467 |           0.0123 |           0.0000 |
[32m[20221213 20:38:23 @agent_ppo2.py:185][0m |          -0.0479 |           0.0123 |           0.0000 |
[32m[20221213 20:38:23 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:38:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.13
[32m[20221213 20:38:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.08
[32m[20221213 20:38:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.77
[32m[20221213 20:38:23 @agent_ppo2.py:143][0m Total time:       9.23 min
[32m[20221213 20:38:23 @agent_ppo2.py:145][0m 1323008 total steps have happened
[32m[20221213 20:38:23 @agent_ppo2.py:121][0m #------------------------ Iteration 323 --------------------------#
[32m[20221213 20:38:24 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:38:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:24 @agent_ppo2.py:185][0m |           0.0288 |           0.0405 |           0.0000 |
[32m[20221213 20:38:24 @agent_ppo2.py:185][0m |           0.0012 |           0.0225 |           0.0000 |
[32m[20221213 20:38:24 @agent_ppo2.py:185][0m |          -0.0057 |           0.0208 |           0.0000 |
[32m[20221213 20:38:24 @agent_ppo2.py:185][0m |          -0.0214 |           0.0202 |           0.0000 |
[32m[20221213 20:38:24 @agent_ppo2.py:185][0m |          -0.0245 |           0.0186 |           0.0000 |
[32m[20221213 20:38:24 @agent_ppo2.py:185][0m |          -0.0243 |           0.0181 |           0.0000 |
[32m[20221213 20:38:24 @agent_ppo2.py:185][0m |          -0.0231 |           0.0178 |           0.0000 |
[32m[20221213 20:38:24 @agent_ppo2.py:185][0m |          -0.0252 |           0.0175 |           0.0000 |
[32m[20221213 20:38:24 @agent_ppo2.py:185][0m |          -0.0279 |           0.0175 |           0.0000 |
[32m[20221213 20:38:25 @agent_ppo2.py:185][0m |          -0.0272 |           0.0166 |           0.0000 |
[32m[20221213 20:38:25 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:38:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.00
[32m[20221213 20:38:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.55
[32m[20221213 20:38:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.50
[32m[20221213 20:38:25 @agent_ppo2.py:143][0m Total time:       9.26 min
[32m[20221213 20:38:25 @agent_ppo2.py:145][0m 1327104 total steps have happened
[32m[20221213 20:38:25 @agent_ppo2.py:121][0m #------------------------ Iteration 324 --------------------------#
[32m[20221213 20:38:25 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:38:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:26 @agent_ppo2.py:185][0m |           0.0346 |           0.0258 |           0.0000 |
[32m[20221213 20:38:26 @agent_ppo2.py:185][0m |           0.0076 |           0.0129 |           0.0000 |
[32m[20221213 20:38:26 @agent_ppo2.py:185][0m |          -0.0067 |           0.0122 |           0.0000 |
[32m[20221213 20:38:26 @agent_ppo2.py:185][0m |          -0.0177 |           0.0120 |           0.0000 |
[32m[20221213 20:38:26 @agent_ppo2.py:185][0m |          -0.0197 |           0.0118 |           0.0000 |
[32m[20221213 20:38:26 @agent_ppo2.py:185][0m |          -0.0249 |           0.0115 |           0.0000 |
[32m[20221213 20:38:26 @agent_ppo2.py:185][0m |          -0.0314 |           0.0114 |           0.0000 |
[32m[20221213 20:38:26 @agent_ppo2.py:185][0m |          -0.0375 |           0.0113 |           0.0000 |
[32m[20221213 20:38:26 @agent_ppo2.py:185][0m |          -0.0389 |           0.0112 |           0.0000 |
[32m[20221213 20:38:26 @agent_ppo2.py:185][0m |          -0.0427 |           0.0111 |           0.0000 |
[32m[20221213 20:38:26 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:38:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.77
[32m[20221213 20:38:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.28
[32m[20221213 20:38:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.07
[32m[20221213 20:38:27 @agent_ppo2.py:143][0m Total time:       9.29 min
[32m[20221213 20:38:27 @agent_ppo2.py:145][0m 1331200 total steps have happened
[32m[20221213 20:38:27 @agent_ppo2.py:121][0m #------------------------ Iteration 325 --------------------------#
[32m[20221213 20:38:27 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:27 @agent_ppo2.py:185][0m |           0.0251 |           0.0156 |           0.0000 |
[32m[20221213 20:38:27 @agent_ppo2.py:185][0m |          -0.0045 |           0.0144 |           0.0000 |
[32m[20221213 20:38:28 @agent_ppo2.py:185][0m |          -0.0230 |           0.0139 |           0.0000 |
[32m[20221213 20:38:28 @agent_ppo2.py:185][0m |          -0.0308 |           0.0136 |           0.0000 |
[32m[20221213 20:38:28 @agent_ppo2.py:185][0m |          -0.0357 |           0.0132 |           0.0000 |
[32m[20221213 20:38:28 @agent_ppo2.py:185][0m |          -0.0426 |           0.0132 |           0.0000 |
[32m[20221213 20:38:28 @agent_ppo2.py:185][0m |          -0.0462 |           0.0131 |           0.0000 |
[32m[20221213 20:38:28 @agent_ppo2.py:185][0m |          -0.0492 |           0.0130 |           0.0000 |
[32m[20221213 20:38:28 @agent_ppo2.py:185][0m |          -0.0518 |           0.0128 |           0.0000 |
[32m[20221213 20:38:28 @agent_ppo2.py:185][0m |          -0.0561 |           0.0128 |           0.0000 |
[32m[20221213 20:38:28 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:38:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.45
[32m[20221213 20:38:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.02
[32m[20221213 20:38:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.86
[32m[20221213 20:38:29 @agent_ppo2.py:143][0m Total time:       9.32 min
[32m[20221213 20:38:29 @agent_ppo2.py:145][0m 1335296 total steps have happened
[32m[20221213 20:38:29 @agent_ppo2.py:121][0m #------------------------ Iteration 326 --------------------------#
[32m[20221213 20:38:29 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:38:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:29 @agent_ppo2.py:185][0m |           0.0510 |           0.0126 |           0.0000 |
[32m[20221213 20:38:29 @agent_ppo2.py:185][0m |           0.0057 |           0.0120 |           0.0000 |
[32m[20221213 20:38:29 @agent_ppo2.py:185][0m |          -0.0322 |           0.0117 |           0.0000 |
[32m[20221213 20:38:29 @agent_ppo2.py:185][0m |          -0.0551 |           0.0115 |           0.0000 |
[32m[20221213 20:38:29 @agent_ppo2.py:185][0m |          -0.0582 |           0.0115 |           0.0000 |
[32m[20221213 20:38:30 @agent_ppo2.py:185][0m |          -0.0666 |           0.0114 |           0.0000 |
[32m[20221213 20:38:30 @agent_ppo2.py:185][0m |          -0.0790 |           0.0114 |           0.0000 |
[32m[20221213 20:38:30 @agent_ppo2.py:185][0m |          -0.0941 |           0.0117 |           0.0000 |
[32m[20221213 20:38:30 @agent_ppo2.py:185][0m |          -0.0773 |           0.0117 |           0.0000 |
[32m[20221213 20:38:30 @agent_ppo2.py:185][0m |          -0.0801 |           0.0112 |           0.0000 |
[32m[20221213 20:38:30 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:38:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.85
[32m[20221213 20:38:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.53
[32m[20221213 20:38:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.34
[32m[20221213 20:38:30 @agent_ppo2.py:143][0m Total time:       9.35 min
[32m[20221213 20:38:30 @agent_ppo2.py:145][0m 1339392 total steps have happened
[32m[20221213 20:38:30 @agent_ppo2.py:121][0m #------------------------ Iteration 327 --------------------------#
[32m[20221213 20:38:31 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:31 @agent_ppo2.py:185][0m |           0.0900 |           0.0123 |           0.0000 |
[32m[20221213 20:38:31 @agent_ppo2.py:185][0m |           0.0097 |           0.0118 |           0.0000 |
[32m[20221213 20:38:31 @agent_ppo2.py:185][0m |          -0.0167 |           0.0114 |           0.0000 |
[32m[20221213 20:38:31 @agent_ppo2.py:185][0m |          -0.0311 |           0.0113 |           0.0000 |
[32m[20221213 20:38:31 @agent_ppo2.py:185][0m |          -0.0585 |           0.0113 |           0.0000 |
[32m[20221213 20:38:31 @agent_ppo2.py:185][0m |          -0.0591 |           0.0112 |           0.0000 |
[32m[20221213 20:38:31 @agent_ppo2.py:185][0m |          -0.0610 |           0.0109 |           0.0000 |
[32m[20221213 20:38:32 @agent_ppo2.py:185][0m |          -0.0598 |           0.0110 |           0.0000 |
[32m[20221213 20:38:32 @agent_ppo2.py:185][0m |          -0.0742 |           0.0111 |           0.0000 |
[32m[20221213 20:38:32 @agent_ppo2.py:185][0m |          -0.0862 |           0.0108 |           0.0000 |
[32m[20221213 20:38:32 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:38:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.51
[32m[20221213 20:38:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.21
[32m[20221213 20:38:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.88
[32m[20221213 20:38:32 @agent_ppo2.py:143][0m Total time:       9.38 min
[32m[20221213 20:38:32 @agent_ppo2.py:145][0m 1343488 total steps have happened
[32m[20221213 20:38:32 @agent_ppo2.py:121][0m #------------------------ Iteration 328 --------------------------#
[32m[20221213 20:38:32 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:33 @agent_ppo2.py:185][0m |           0.0204 |           0.1459 |           0.0000 |
[32m[20221213 20:38:33 @agent_ppo2.py:185][0m |          -0.0038 |           0.0659 |           0.0000 |
[32m[20221213 20:38:33 @agent_ppo2.py:185][0m |          -0.0227 |           0.0488 |           0.0000 |
[32m[20221213 20:38:33 @agent_ppo2.py:185][0m |          -0.0108 |           0.0448 |           0.0000 |
[32m[20221213 20:38:33 @agent_ppo2.py:185][0m |          -0.0130 |           0.0384 |           0.0000 |
[32m[20221213 20:38:33 @agent_ppo2.py:185][0m |          -0.0189 |           0.0353 |           0.0000 |
[32m[20221213 20:38:33 @agent_ppo2.py:185][0m |          -0.0202 |           0.0328 |           0.0000 |
[32m[20221213 20:38:33 @agent_ppo2.py:185][0m |          -0.0255 |           0.0318 |           0.0000 |
[32m[20221213 20:38:33 @agent_ppo2.py:185][0m |          -0.0274 |           0.0308 |           0.0000 |
[32m[20221213 20:38:33 @agent_ppo2.py:185][0m |          -0.0282 |           0.0293 |           0.0000 |
[32m[20221213 20:38:33 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:38:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.53
[32m[20221213 20:38:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.96
[32m[20221213 20:38:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.26
[32m[20221213 20:38:34 @agent_ppo2.py:143][0m Total time:       9.41 min
[32m[20221213 20:38:34 @agent_ppo2.py:145][0m 1347584 total steps have happened
[32m[20221213 20:38:34 @agent_ppo2.py:121][0m #------------------------ Iteration 329 --------------------------#
[32m[20221213 20:38:34 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:38:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:34 @agent_ppo2.py:185][0m |           0.0190 |           0.0517 |           0.0000 |
[32m[20221213 20:38:34 @agent_ppo2.py:185][0m |          -0.0000 |           0.0161 |           0.0000 |
[32m[20221213 20:38:35 @agent_ppo2.py:185][0m |          -0.0187 |           0.0147 |           0.0000 |
[32m[20221213 20:38:35 @agent_ppo2.py:185][0m |          -0.0288 |           0.0141 |           0.0000 |
[32m[20221213 20:38:35 @agent_ppo2.py:185][0m |          -0.0355 |           0.0138 |           0.0000 |
[32m[20221213 20:38:35 @agent_ppo2.py:185][0m |          -0.0420 |           0.0136 |           0.0000 |
[32m[20221213 20:38:35 @agent_ppo2.py:185][0m |          -0.0447 |           0.0135 |           0.0000 |
[32m[20221213 20:38:35 @agent_ppo2.py:185][0m |          -0.0528 |           0.0134 |           0.0000 |
[32m[20221213 20:38:35 @agent_ppo2.py:185][0m |          -0.0544 |           0.0132 |           0.0000 |
[32m[20221213 20:38:35 @agent_ppo2.py:185][0m |          -0.0561 |           0.0132 |           0.0000 |
[32m[20221213 20:38:35 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:38:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.88
[32m[20221213 20:38:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.16
[32m[20221213 20:38:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.18
[32m[20221213 20:38:36 @agent_ppo2.py:143][0m Total time:       9.44 min
[32m[20221213 20:38:36 @agent_ppo2.py:145][0m 1351680 total steps have happened
[32m[20221213 20:38:36 @agent_ppo2.py:121][0m #------------------------ Iteration 330 --------------------------#
[32m[20221213 20:38:36 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:38:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:36 @agent_ppo2.py:185][0m |           0.0373 |           0.0123 |           0.0000 |
[32m[20221213 20:38:36 @agent_ppo2.py:185][0m |           0.0035 |           0.0114 |           0.0000 |
[32m[20221213 20:38:36 @agent_ppo2.py:185][0m |          -0.0308 |           0.0113 |           0.0000 |
[32m[20221213 20:38:36 @agent_ppo2.py:185][0m |          -0.0664 |           0.0114 |           0.0000 |
[32m[20221213 20:38:36 @agent_ppo2.py:185][0m |          -0.0671 |           0.0116 |           0.0000 |
[32m[20221213 20:38:37 @agent_ppo2.py:185][0m |          -0.0769 |           0.0112 |           0.0000 |
[32m[20221213 20:38:37 @agent_ppo2.py:185][0m |          -0.0761 |           0.0108 |           0.0000 |
[32m[20221213 20:38:37 @agent_ppo2.py:185][0m |          -0.0860 |           0.0107 |           0.0000 |
[32m[20221213 20:38:37 @agent_ppo2.py:185][0m |          -0.0987 |           0.0107 |           0.0000 |
[32m[20221213 20:38:37 @agent_ppo2.py:185][0m |          -0.0949 |           0.0106 |           0.0000 |
[32m[20221213 20:38:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:38:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.79
[32m[20221213 20:38:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.01
[32m[20221213 20:38:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.74
[32m[20221213 20:38:37 @agent_ppo2.py:143][0m Total time:       9.47 min
[32m[20221213 20:38:37 @agent_ppo2.py:145][0m 1355776 total steps have happened
[32m[20221213 20:38:37 @agent_ppo2.py:121][0m #------------------------ Iteration 331 --------------------------#
[32m[20221213 20:38:38 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:38 @agent_ppo2.py:185][0m |           0.1055 |           0.0104 |           0.0000 |
[32m[20221213 20:38:38 @agent_ppo2.py:185][0m |           0.0566 |           0.0098 |           0.0000 |
[32m[20221213 20:38:38 @agent_ppo2.py:185][0m |           0.0246 |           0.0097 |           0.0000 |
[32m[20221213 20:38:38 @agent_ppo2.py:185][0m |           0.0036 |           0.0096 |           0.0000 |
[32m[20221213 20:38:38 @agent_ppo2.py:185][0m |          -0.0110 |           0.0095 |           0.0000 |
[32m[20221213 20:38:38 @agent_ppo2.py:185][0m |          -0.0376 |           0.0094 |           0.0000 |
[32m[20221213 20:38:38 @agent_ppo2.py:185][0m |          -0.0320 |           0.0094 |           0.0000 |
[32m[20221213 20:38:38 @agent_ppo2.py:185][0m |          -0.0430 |           0.0093 |           0.0000 |
[32m[20221213 20:38:38 @agent_ppo2.py:185][0m |          -0.0508 |           0.0092 |           0.0000 |
[32m[20221213 20:38:39 @agent_ppo2.py:185][0m |          -0.0566 |           0.0092 |           0.0000 |
[32m[20221213 20:38:39 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:38:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.67
[32m[20221213 20:38:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.19
[32m[20221213 20:38:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.51
[32m[20221213 20:38:39 @agent_ppo2.py:143][0m Total time:       9.50 min
[32m[20221213 20:38:39 @agent_ppo2.py:145][0m 1359872 total steps have happened
[32m[20221213 20:38:39 @agent_ppo2.py:121][0m #------------------------ Iteration 332 --------------------------#
[32m[20221213 20:38:39 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:39 @agent_ppo2.py:185][0m |           0.0132 |           0.0144 |           0.0000 |
[32m[20221213 20:38:40 @agent_ppo2.py:185][0m |          -0.0060 |           0.0079 |           0.0000 |
[32m[20221213 20:38:40 @agent_ppo2.py:185][0m |          -0.0186 |           0.0073 |           0.0000 |
[32m[20221213 20:38:40 @agent_ppo2.py:185][0m |          -0.0247 |           0.0071 |           0.0000 |
[32m[20221213 20:38:40 @agent_ppo2.py:185][0m |          -0.0264 |           0.0070 |           0.0000 |
[32m[20221213 20:38:40 @agent_ppo2.py:185][0m |          -0.0345 |           0.0069 |           0.0000 |
[32m[20221213 20:38:40 @agent_ppo2.py:185][0m |          -0.0365 |           0.0068 |           0.0000 |
[32m[20221213 20:38:40 @agent_ppo2.py:185][0m |          -0.0381 |           0.0067 |           0.0000 |
[32m[20221213 20:38:40 @agent_ppo2.py:185][0m |          -0.0376 |           0.0067 |           0.0000 |
[32m[20221213 20:38:40 @agent_ppo2.py:185][0m |          -0.0433 |           0.0066 |           0.0000 |
[32m[20221213 20:38:40 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:38:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.38
[32m[20221213 20:38:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.43
[32m[20221213 20:38:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.82
[32m[20221213 20:38:41 @agent_ppo2.py:143][0m Total time:       9.52 min
[32m[20221213 20:38:41 @agent_ppo2.py:145][0m 1363968 total steps have happened
[32m[20221213 20:38:41 @agent_ppo2.py:121][0m #------------------------ Iteration 333 --------------------------#
[32m[20221213 20:38:41 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:41 @agent_ppo2.py:185][0m |           0.0244 |           0.0184 |           0.0000 |
[32m[20221213 20:38:41 @agent_ppo2.py:185][0m |           0.0100 |           0.0100 |           0.0000 |
[32m[20221213 20:38:41 @agent_ppo2.py:185][0m |          -0.0057 |           0.0088 |           0.0000 |
[32m[20221213 20:38:41 @agent_ppo2.py:185][0m |          -0.0118 |           0.0085 |           0.0000 |
[32m[20221213 20:38:41 @agent_ppo2.py:185][0m |          -0.0175 |           0.0085 |           0.0000 |
[32m[20221213 20:38:42 @agent_ppo2.py:185][0m |          -0.0226 |           0.0084 |           0.0000 |
[32m[20221213 20:38:42 @agent_ppo2.py:185][0m |          -0.0231 |           0.0083 |           0.0000 |
[32m[20221213 20:38:42 @agent_ppo2.py:185][0m |          -0.0243 |           0.0083 |           0.0000 |
[32m[20221213 20:38:42 @agent_ppo2.py:185][0m |          -0.0253 |           0.0083 |           0.0000 |
[32m[20221213 20:38:42 @agent_ppo2.py:185][0m |          -0.0247 |           0.0082 |           0.0000 |
[32m[20221213 20:38:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:38:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.32
[32m[20221213 20:38:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.96
[32m[20221213 20:38:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.72
[32m[20221213 20:38:42 @agent_ppo2.py:143][0m Total time:       9.55 min
[32m[20221213 20:38:42 @agent_ppo2.py:145][0m 1368064 total steps have happened
[32m[20221213 20:38:42 @agent_ppo2.py:121][0m #------------------------ Iteration 334 --------------------------#
[32m[20221213 20:38:43 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:43 @agent_ppo2.py:185][0m |           0.0315 |           0.0094 |           0.0000 |
[32m[20221213 20:38:43 @agent_ppo2.py:185][0m |          -0.0120 |           0.0091 |           0.0000 |
[32m[20221213 20:38:43 @agent_ppo2.py:185][0m |          -0.0307 |           0.0089 |           0.0000 |
[32m[20221213 20:38:43 @agent_ppo2.py:185][0m |          -0.0422 |           0.0089 |           0.0000 |
[32m[20221213 20:38:43 @agent_ppo2.py:185][0m |          -0.0447 |           0.0088 |           0.0000 |
[32m[20221213 20:38:43 @agent_ppo2.py:185][0m |          -0.0525 |           0.0087 |           0.0000 |
[32m[20221213 20:38:43 @agent_ppo2.py:185][0m |          -0.0486 |           0.0087 |           0.0000 |
[32m[20221213 20:38:43 @agent_ppo2.py:185][0m |          -0.0559 |           0.0087 |           0.0000 |
[32m[20221213 20:38:44 @agent_ppo2.py:185][0m |          -0.0920 |           0.0086 |           0.0000 |
[32m[20221213 20:38:44 @agent_ppo2.py:185][0m |          -0.0594 |           0.0088 |           0.0000 |
[32m[20221213 20:38:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:38:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.97
[32m[20221213 20:38:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.96
[32m[20221213 20:38:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.85
[32m[20221213 20:38:44 @agent_ppo2.py:143][0m Total time:       9.58 min
[32m[20221213 20:38:44 @agent_ppo2.py:145][0m 1372160 total steps have happened
[32m[20221213 20:38:44 @agent_ppo2.py:121][0m #------------------------ Iteration 335 --------------------------#
[32m[20221213 20:38:44 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:45 @agent_ppo2.py:185][0m |           0.0032 |           0.0353 |           0.0000 |
[32m[20221213 20:38:45 @agent_ppo2.py:185][0m |          -0.0014 |           0.0187 |           0.0000 |
[32m[20221213 20:38:45 @agent_ppo2.py:185][0m |          -0.0170 |           0.0161 |           0.0000 |
[32m[20221213 20:38:45 @agent_ppo2.py:185][0m |          -0.0247 |           0.0154 |           0.0000 |
[32m[20221213 20:38:45 @agent_ppo2.py:185][0m |          -0.0271 |           0.0149 |           0.0000 |
[32m[20221213 20:38:45 @agent_ppo2.py:185][0m |          -0.0288 |           0.0145 |           0.0000 |
[32m[20221213 20:38:45 @agent_ppo2.py:185][0m |          -0.0214 |           0.0145 |           0.0000 |
[32m[20221213 20:38:45 @agent_ppo2.py:185][0m |          -0.0257 |           0.0144 |           0.0000 |
[32m[20221213 20:38:45 @agent_ppo2.py:185][0m |          -0.0280 |           0.0147 |           0.0000 |
[32m[20221213 20:38:45 @agent_ppo2.py:185][0m |          -0.0321 |           0.0137 |           0.0000 |
[32m[20221213 20:38:45 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:38:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.00
[32m[20221213 20:38:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.79
[32m[20221213 20:38:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.87
[32m[20221213 20:38:46 @agent_ppo2.py:143][0m Total time:       9.61 min
[32m[20221213 20:38:46 @agent_ppo2.py:145][0m 1376256 total steps have happened
[32m[20221213 20:38:46 @agent_ppo2.py:121][0m #------------------------ Iteration 336 --------------------------#
[32m[20221213 20:38:46 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:46 @agent_ppo2.py:185][0m |           0.0256 |           0.0179 |           0.0000 |
[32m[20221213 20:38:46 @agent_ppo2.py:185][0m |          -0.0068 |           0.0161 |           0.0000 |
[32m[20221213 20:38:46 @agent_ppo2.py:185][0m |          -0.0300 |           0.0161 |           0.0000 |
[32m[20221213 20:38:47 @agent_ppo2.py:185][0m |          -0.0458 |           0.0155 |           0.0000 |
[32m[20221213 20:38:47 @agent_ppo2.py:185][0m |          -0.0523 |           0.0146 |           0.0000 |
[32m[20221213 20:38:47 @agent_ppo2.py:185][0m |          -0.0582 |           0.0142 |           0.0000 |
[32m[20221213 20:38:47 @agent_ppo2.py:185][0m |          -0.0617 |           0.0141 |           0.0000 |
[32m[20221213 20:38:47 @agent_ppo2.py:185][0m |          -0.0619 |           0.0139 |           0.0000 |
[32m[20221213 20:38:47 @agent_ppo2.py:185][0m |          -0.0594 |           0.0136 |           0.0000 |
[32m[20221213 20:38:47 @agent_ppo2.py:185][0m |          -0.0695 |           0.0136 |           0.0000 |
[32m[20221213 20:38:47 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:38:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.27
[32m[20221213 20:38:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.78
[32m[20221213 20:38:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.76
[32m[20221213 20:38:47 @agent_ppo2.py:143][0m Total time:       9.64 min
[32m[20221213 20:38:47 @agent_ppo2.py:145][0m 1380352 total steps have happened
[32m[20221213 20:38:47 @agent_ppo2.py:121][0m #------------------------ Iteration 337 --------------------------#
[32m[20221213 20:38:48 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:48 @agent_ppo2.py:185][0m |           0.0317 |           0.0200 |           0.0000 |
[32m[20221213 20:38:48 @agent_ppo2.py:185][0m |          -0.0008 |           0.0131 |           0.0000 |
[32m[20221213 20:38:48 @agent_ppo2.py:185][0m |          -0.0082 |           0.0129 |           0.0000 |
[32m[20221213 20:38:48 @agent_ppo2.py:185][0m |          -0.0108 |           0.0183 |           0.0000 |
[32m[20221213 20:38:48 @agent_ppo2.py:185][0m |          -0.0278 |           0.0173 |           0.0000 |
[32m[20221213 20:38:48 @agent_ppo2.py:185][0m |          -0.0342 |           0.0128 |           0.0000 |
[32m[20221213 20:38:48 @agent_ppo2.py:185][0m |          -0.0058 |           0.0132 |           0.0000 |
[32m[20221213 20:38:49 @agent_ppo2.py:185][0m |          -0.0404 |           0.0127 |           0.0000 |
[32m[20221213 20:38:49 @agent_ppo2.py:185][0m |          -0.0477 |           0.0123 |           0.0000 |
[32m[20221213 20:38:49 @agent_ppo2.py:185][0m |          -0.0379 |           0.0121 |           0.0000 |
[32m[20221213 20:38:49 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:38:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.59
[32m[20221213 20:38:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.36
[32m[20221213 20:38:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.23
[32m[20221213 20:38:49 @agent_ppo2.py:143][0m Total time:       9.67 min
[32m[20221213 20:38:49 @agent_ppo2.py:145][0m 1384448 total steps have happened
[32m[20221213 20:38:49 @agent_ppo2.py:121][0m #------------------------ Iteration 338 --------------------------#
[32m[20221213 20:38:49 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:50 @agent_ppo2.py:185][0m |           0.0196 |           0.0373 |           0.0000 |
[32m[20221213 20:38:50 @agent_ppo2.py:185][0m |           0.0042 |           0.0247 |           0.0000 |
[32m[20221213 20:38:50 @agent_ppo2.py:185][0m |          -0.0054 |           0.0216 |           0.0000 |
[32m[20221213 20:38:50 @agent_ppo2.py:185][0m |          -0.0087 |           0.0208 |           0.0000 |
[32m[20221213 20:38:50 @agent_ppo2.py:185][0m |          -0.0167 |           0.0192 |           0.0000 |
[32m[20221213 20:38:50 @agent_ppo2.py:185][0m |          -0.0191 |           0.0191 |           0.0000 |
[32m[20221213 20:38:50 @agent_ppo2.py:185][0m |          -0.0203 |           0.0193 |           0.0000 |
[32m[20221213 20:38:50 @agent_ppo2.py:185][0m |          -0.0226 |           0.0178 |           0.0000 |
[32m[20221213 20:38:50 @agent_ppo2.py:185][0m |          -0.0220 |           0.0178 |           0.0000 |
[32m[20221213 20:38:50 @agent_ppo2.py:185][0m |          -0.0279 |           0.0182 |           0.0000 |
[32m[20221213 20:38:50 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:38:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.14
[32m[20221213 20:38:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.93
[32m[20221213 20:38:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.41
[32m[20221213 20:38:51 @agent_ppo2.py:143][0m Total time:       9.69 min
[32m[20221213 20:38:51 @agent_ppo2.py:145][0m 1388544 total steps have happened
[32m[20221213 20:38:51 @agent_ppo2.py:121][0m #------------------------ Iteration 339 --------------------------#
[32m[20221213 20:38:51 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:51 @agent_ppo2.py:185][0m |           0.0448 |           0.0249 |           0.0000 |
[32m[20221213 20:38:51 @agent_ppo2.py:185][0m |          -0.0210 |           0.0232 |           0.0000 |
[32m[20221213 20:38:51 @agent_ppo2.py:185][0m |          -0.0208 |           0.0194 |           0.0000 |
[32m[20221213 20:38:52 @agent_ppo2.py:185][0m |          -0.0450 |           0.0181 |           0.0000 |
[32m[20221213 20:38:52 @agent_ppo2.py:185][0m |          -0.0608 |           0.0178 |           0.0000 |
[32m[20221213 20:38:52 @agent_ppo2.py:185][0m |          -0.0715 |           0.0170 |           0.0000 |
[32m[20221213 20:38:52 @agent_ppo2.py:185][0m |          -0.0728 |           0.0167 |           0.0000 |
[32m[20221213 20:38:52 @agent_ppo2.py:185][0m |          -0.0862 |           0.0164 |           0.0000 |
[32m[20221213 20:38:52 @agent_ppo2.py:185][0m |          -0.0819 |           0.0163 |           0.0000 |
[32m[20221213 20:38:52 @agent_ppo2.py:185][0m |          -0.1231 |           0.0166 |           0.0000 |
[32m[20221213 20:38:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:38:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.86
[32m[20221213 20:38:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.44
[32m[20221213 20:38:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.33
[32m[20221213 20:38:52 @agent_ppo2.py:143][0m Total time:       9.72 min
[32m[20221213 20:38:52 @agent_ppo2.py:145][0m 1392640 total steps have happened
[32m[20221213 20:38:52 @agent_ppo2.py:121][0m #------------------------ Iteration 340 --------------------------#
[32m[20221213 20:38:53 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:38:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:53 @agent_ppo2.py:185][0m |           0.0293 |           0.0398 |           0.0000 |
[32m[20221213 20:38:53 @agent_ppo2.py:185][0m |           0.0133 |           0.0093 |           0.0000 |
[32m[20221213 20:38:53 @agent_ppo2.py:185][0m |          -0.0018 |           0.0085 |           0.0000 |
[32m[20221213 20:38:53 @agent_ppo2.py:185][0m |          -0.0089 |           0.0082 |           0.0000 |
[32m[20221213 20:38:53 @agent_ppo2.py:185][0m |          -0.0156 |           0.0081 |           0.0000 |
[32m[20221213 20:38:53 @agent_ppo2.py:185][0m |          -0.0207 |           0.0080 |           0.0000 |
[32m[20221213 20:38:54 @agent_ppo2.py:185][0m |          -0.0268 |           0.0079 |           0.0000 |
[32m[20221213 20:38:54 @agent_ppo2.py:185][0m |          -0.0273 |           0.0078 |           0.0000 |
[32m[20221213 20:38:54 @agent_ppo2.py:185][0m |          -0.0262 |           0.0077 |           0.0000 |
[32m[20221213 20:38:54 @agent_ppo2.py:185][0m |          -0.0360 |           0.0076 |           0.0000 |
[32m[20221213 20:38:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:38:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.06
[32m[20221213 20:38:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.13
[32m[20221213 20:38:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.97
[32m[20221213 20:38:54 @agent_ppo2.py:143][0m Total time:       9.75 min
[32m[20221213 20:38:54 @agent_ppo2.py:145][0m 1396736 total steps have happened
[32m[20221213 20:38:54 @agent_ppo2.py:121][0m #------------------------ Iteration 341 --------------------------#
[32m[20221213 20:38:55 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:38:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:55 @agent_ppo2.py:185][0m |           0.0219 |           0.0077 |           0.0000 |
[32m[20221213 20:38:55 @agent_ppo2.py:185][0m |          -0.0135 |           0.0074 |           0.0000 |
[32m[20221213 20:38:55 @agent_ppo2.py:185][0m |          -0.0863 |           0.0074 |           0.0000 |
[32m[20221213 20:38:55 @agent_ppo2.py:185][0m |          -0.0389 |           0.0073 |           0.0000 |
[32m[20221213 20:38:55 @agent_ppo2.py:185][0m |          -0.0410 |           0.0072 |           0.0000 |
[32m[20221213 20:38:55 @agent_ppo2.py:185][0m |          -0.0423 |           0.0071 |           0.0000 |
[32m[20221213 20:38:55 @agent_ppo2.py:185][0m |          -0.0534 |           0.0070 |           0.0000 |
[32m[20221213 20:38:55 @agent_ppo2.py:185][0m |          -0.0516 |           0.0069 |           0.0000 |
[32m[20221213 20:38:55 @agent_ppo2.py:185][0m |          -0.0534 |           0.0069 |           0.0000 |
[32m[20221213 20:38:56 @agent_ppo2.py:185][0m |          -0.0643 |           0.0068 |           0.0000 |
[32m[20221213 20:38:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:38:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.50
[32m[20221213 20:38:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.79
[32m[20221213 20:38:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.13
[32m[20221213 20:38:56 @agent_ppo2.py:143][0m Total time:       9.78 min
[32m[20221213 20:38:56 @agent_ppo2.py:145][0m 1400832 total steps have happened
[32m[20221213 20:38:56 @agent_ppo2.py:121][0m #------------------------ Iteration 342 --------------------------#
[32m[20221213 20:38:56 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:38:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:56 @agent_ppo2.py:185][0m |           0.0276 |           0.0843 |           0.0000 |
[32m[20221213 20:38:57 @agent_ppo2.py:185][0m |           0.0131 |           0.0282 |           0.0000 |
[32m[20221213 20:38:57 @agent_ppo2.py:185][0m |          -0.0030 |           0.0211 |           0.0000 |
[32m[20221213 20:38:57 @agent_ppo2.py:185][0m |          -0.0125 |           0.0194 |           0.0000 |
[32m[20221213 20:38:57 @agent_ppo2.py:185][0m |          -0.0195 |           0.0194 |           0.0000 |
[32m[20221213 20:38:57 @agent_ppo2.py:185][0m |          -0.0225 |           0.0180 |           0.0000 |
[32m[20221213 20:38:57 @agent_ppo2.py:185][0m |          -0.0229 |           0.0177 |           0.0000 |
[32m[20221213 20:38:57 @agent_ppo2.py:185][0m |          -0.0205 |           0.0172 |           0.0000 |
[32m[20221213 20:38:57 @agent_ppo2.py:185][0m |          -0.0240 |           0.0172 |           0.0000 |
[32m[20221213 20:38:57 @agent_ppo2.py:185][0m |          -0.0275 |           0.0171 |           0.0000 |
[32m[20221213 20:38:57 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:38:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.16
[32m[20221213 20:38:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.31
[32m[20221213 20:38:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.90
[32m[20221213 20:38:58 @agent_ppo2.py:143][0m Total time:       9.81 min
[32m[20221213 20:38:58 @agent_ppo2.py:145][0m 1404928 total steps have happened
[32m[20221213 20:38:58 @agent_ppo2.py:121][0m #------------------------ Iteration 343 --------------------------#
[32m[20221213 20:38:58 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:38:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:38:58 @agent_ppo2.py:185][0m |           0.0652 |           0.0187 |           0.0000 |
[32m[20221213 20:38:58 @agent_ppo2.py:185][0m |           0.0298 |           0.0122 |           0.0000 |
[32m[20221213 20:38:58 @agent_ppo2.py:185][0m |           0.0027 |           0.0118 |           0.0000 |
[32m[20221213 20:38:58 @agent_ppo2.py:185][0m |          -0.0065 |           0.0116 |           0.0000 |
[32m[20221213 20:38:58 @agent_ppo2.py:185][0m |          -0.0273 |           0.0113 |           0.0000 |
[32m[20221213 20:38:59 @agent_ppo2.py:185][0m |          -0.0370 |           0.0112 |           0.0000 |
[32m[20221213 20:38:59 @agent_ppo2.py:185][0m |          -0.0208 |           0.0115 |           0.0000 |
[32m[20221213 20:38:59 @agent_ppo2.py:185][0m |          -0.0422 |           0.0112 |           0.0000 |
[32m[20221213 20:38:59 @agent_ppo2.py:185][0m |          -0.0408 |           0.0112 |           0.0000 |
[32m[20221213 20:38:59 @agent_ppo2.py:185][0m |          -0.0517 |           0.0112 |           0.0000 |
[32m[20221213 20:38:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:38:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.56
[32m[20221213 20:38:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.04
[32m[20221213 20:38:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.00
[32m[20221213 20:38:59 @agent_ppo2.py:143][0m Total time:       9.84 min
[32m[20221213 20:38:59 @agent_ppo2.py:145][0m 1409024 total steps have happened
[32m[20221213 20:38:59 @agent_ppo2.py:121][0m #------------------------ Iteration 344 --------------------------#
[32m[20221213 20:39:00 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:00 @agent_ppo2.py:185][0m |           0.0344 |           0.0138 |           0.0000 |
[32m[20221213 20:39:00 @agent_ppo2.py:185][0m |          -0.0024 |           0.0127 |           0.0000 |
[32m[20221213 20:39:00 @agent_ppo2.py:185][0m |          -0.0276 |           0.0124 |           0.0000 |
[32m[20221213 20:39:00 @agent_ppo2.py:185][0m |          -0.0318 |           0.0124 |           0.0000 |
[32m[20221213 20:39:00 @agent_ppo2.py:185][0m |          -0.0448 |           0.0131 |           0.0000 |
[32m[20221213 20:39:00 @agent_ppo2.py:185][0m |          -0.0573 |           0.0120 |           0.0000 |
[32m[20221213 20:39:00 @agent_ppo2.py:185][0m |          -0.0625 |           0.0117 |           0.0000 |
[32m[20221213 20:39:00 @agent_ppo2.py:185][0m |          -0.0616 |           0.0115 |           0.0000 |
[32m[20221213 20:39:00 @agent_ppo2.py:185][0m |          -0.0629 |           0.0114 |           0.0000 |
[32m[20221213 20:39:01 @agent_ppo2.py:185][0m |          -0.0657 |           0.0114 |           0.0000 |
[32m[20221213 20:39:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:39:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.72
[32m[20221213 20:39:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.53
[32m[20221213 20:39:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.66
[32m[20221213 20:39:01 @agent_ppo2.py:143][0m Total time:       9.86 min
[32m[20221213 20:39:01 @agent_ppo2.py:145][0m 1413120 total steps have happened
[32m[20221213 20:39:01 @agent_ppo2.py:121][0m #------------------------ Iteration 345 --------------------------#
[32m[20221213 20:39:01 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:39:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:01 @agent_ppo2.py:185][0m |           0.0325 |           0.0315 |           0.0000 |
[32m[20221213 20:39:02 @agent_ppo2.py:185][0m |           0.0031 |           0.0192 |           0.0000 |
[32m[20221213 20:39:02 @agent_ppo2.py:185][0m |          -0.0104 |           0.0173 |           0.0000 |
[32m[20221213 20:39:02 @agent_ppo2.py:185][0m |          -0.0118 |           0.0169 |           0.0000 |
[32m[20221213 20:39:02 @agent_ppo2.py:185][0m |          -0.0207 |           0.0164 |           0.0000 |
[32m[20221213 20:39:02 @agent_ppo2.py:185][0m |          -0.0246 |           0.0160 |           0.0000 |
[32m[20221213 20:39:02 @agent_ppo2.py:185][0m |          -0.0320 |           0.0158 |           0.0000 |
[32m[20221213 20:39:02 @agent_ppo2.py:185][0m |          -0.0313 |           0.0155 |           0.0000 |
[32m[20221213 20:39:02 @agent_ppo2.py:185][0m |          -0.0343 |           0.0152 |           0.0000 |
[32m[20221213 20:39:02 @agent_ppo2.py:185][0m |          -0.0352 |           0.0150 |           0.0000 |
[32m[20221213 20:39:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:39:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.76
[32m[20221213 20:39:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.24
[32m[20221213 20:39:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.99
[32m[20221213 20:39:03 @agent_ppo2.py:143][0m Total time:       9.89 min
[32m[20221213 20:39:03 @agent_ppo2.py:145][0m 1417216 total steps have happened
[32m[20221213 20:39:03 @agent_ppo2.py:121][0m #------------------------ Iteration 346 --------------------------#
[32m[20221213 20:39:03 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:03 @agent_ppo2.py:185][0m |           0.0256 |           0.0246 |           0.0000 |
[32m[20221213 20:39:03 @agent_ppo2.py:185][0m |          -0.0036 |           0.0211 |           0.0000 |
[32m[20221213 20:39:03 @agent_ppo2.py:185][0m |          -0.0306 |           0.0204 |           0.0000 |
[32m[20221213 20:39:03 @agent_ppo2.py:185][0m |          -0.0471 |           0.0194 |           0.0000 |
[32m[20221213 20:39:03 @agent_ppo2.py:185][0m |          -0.0536 |           0.0191 |           0.0000 |
[32m[20221213 20:39:04 @agent_ppo2.py:185][0m |          -0.0440 |           0.0180 |           0.0000 |
[32m[20221213 20:39:04 @agent_ppo2.py:185][0m |          -0.0494 |           0.0175 |           0.0000 |
[32m[20221213 20:39:04 @agent_ppo2.py:185][0m |          -0.0586 |           0.0174 |           0.0000 |
[32m[20221213 20:39:04 @agent_ppo2.py:185][0m |          -0.0621 |           0.0171 |           0.0000 |
[32m[20221213 20:39:04 @agent_ppo2.py:185][0m |          -0.0933 |           0.0170 |           0.0000 |
[32m[20221213 20:39:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:39:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.72
[32m[20221213 20:39:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.10
[32m[20221213 20:39:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.11
[32m[20221213 20:39:04 @agent_ppo2.py:143][0m Total time:       9.92 min
[32m[20221213 20:39:04 @agent_ppo2.py:145][0m 1421312 total steps have happened
[32m[20221213 20:39:04 @agent_ppo2.py:121][0m #------------------------ Iteration 347 --------------------------#
[32m[20221213 20:39:05 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:05 @agent_ppo2.py:185][0m |           0.0698 |           0.0193 |           0.0000 |
[32m[20221213 20:39:05 @agent_ppo2.py:185][0m |           0.0095 |           0.0120 |           0.0000 |
[32m[20221213 20:39:05 @agent_ppo2.py:185][0m |          -0.0059 |           0.0118 |           0.0000 |
[32m[20221213 20:39:05 @agent_ppo2.py:185][0m |          -0.0213 |           0.0115 |           0.0000 |
[32m[20221213 20:39:05 @agent_ppo2.py:185][0m |          -0.0331 |           0.0114 |           0.0000 |
[32m[20221213 20:39:05 @agent_ppo2.py:185][0m |          -0.0403 |           0.0113 |           0.0000 |
[32m[20221213 20:39:05 @agent_ppo2.py:185][0m |          -0.0435 |           0.0112 |           0.0000 |
[32m[20221213 20:39:05 @agent_ppo2.py:185][0m |          -0.0448 |           0.0111 |           0.0000 |
[32m[20221213 20:39:06 @agent_ppo2.py:185][0m |          -0.0472 |           0.0110 |           0.0000 |
[32m[20221213 20:39:06 @agent_ppo2.py:185][0m |          -0.0534 |           0.0110 |           0.0000 |
[32m[20221213 20:39:06 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:39:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.29
[32m[20221213 20:39:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.39
[32m[20221213 20:39:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.32
[32m[20221213 20:39:06 @agent_ppo2.py:143][0m Total time:       9.95 min
[32m[20221213 20:39:06 @agent_ppo2.py:145][0m 1425408 total steps have happened
[32m[20221213 20:39:06 @agent_ppo2.py:121][0m #------------------------ Iteration 348 --------------------------#
[32m[20221213 20:39:06 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:07 @agent_ppo2.py:185][0m |           0.0391 |           0.0274 |           0.0000 |
[32m[20221213 20:39:07 @agent_ppo2.py:185][0m |           0.0040 |           0.0185 |           0.0000 |
[32m[20221213 20:39:07 @agent_ppo2.py:185][0m |          -0.0102 |           0.0174 |           0.0000 |
[32m[20221213 20:39:07 @agent_ppo2.py:185][0m |          -0.0190 |           0.0174 |           0.0000 |
[32m[20221213 20:39:07 @agent_ppo2.py:185][0m |          -0.0292 |           0.0161 |           0.0000 |
[32m[20221213 20:39:07 @agent_ppo2.py:185][0m |          -0.0326 |           0.0158 |           0.0000 |
[32m[20221213 20:39:07 @agent_ppo2.py:185][0m |          -0.0360 |           0.0156 |           0.0000 |
[32m[20221213 20:39:07 @agent_ppo2.py:185][0m |          -0.0380 |           0.0155 |           0.0000 |
[32m[20221213 20:39:07 @agent_ppo2.py:185][0m |          -0.0357 |           0.0154 |           0.0000 |
[32m[20221213 20:39:07 @agent_ppo2.py:185][0m |          -0.0383 |           0.0154 |           0.0000 |
[32m[20221213 20:39:07 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:39:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.04
[32m[20221213 20:39:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.93
[32m[20221213 20:39:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.77
[32m[20221213 20:39:08 @agent_ppo2.py:143][0m Total time:       9.98 min
[32m[20221213 20:39:08 @agent_ppo2.py:145][0m 1429504 total steps have happened
[32m[20221213 20:39:08 @agent_ppo2.py:121][0m #------------------------ Iteration 349 --------------------------#
[32m[20221213 20:39:08 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:39:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:08 @agent_ppo2.py:185][0m |           0.0399 |           0.0182 |           0.0000 |
[32m[20221213 20:39:08 @agent_ppo2.py:185][0m |           0.0522 |           0.0124 |           0.0000 |
[32m[20221213 20:39:09 @agent_ppo2.py:185][0m |           0.0292 |           0.0125 |           0.0000 |
[32m[20221213 20:39:09 @agent_ppo2.py:185][0m |           0.0008 |           0.0120 |           0.0000 |
[32m[20221213 20:39:09 @agent_ppo2.py:185][0m |          -0.0080 |           0.0117 |           0.0000 |
[32m[20221213 20:39:09 @agent_ppo2.py:185][0m |          -0.0204 |           0.0116 |           0.0000 |
[32m[20221213 20:39:09 @agent_ppo2.py:185][0m |          -0.0264 |           0.0115 |           0.0000 |
[32m[20221213 20:39:09 @agent_ppo2.py:185][0m |          -0.0310 |           0.0114 |           0.0000 |
[32m[20221213 20:39:09 @agent_ppo2.py:185][0m |          -0.0393 |           0.0114 |           0.0000 |
[32m[20221213 20:39:09 @agent_ppo2.py:185][0m |          -0.0412 |           0.0113 |           0.0000 |
[32m[20221213 20:39:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:39:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.69
[32m[20221213 20:39:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.41
[32m[20221213 20:39:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.67
[32m[20221213 20:39:10 @agent_ppo2.py:143][0m Total time:      10.01 min
[32m[20221213 20:39:10 @agent_ppo2.py:145][0m 1433600 total steps have happened
[32m[20221213 20:39:10 @agent_ppo2.py:121][0m #------------------------ Iteration 350 --------------------------#
[32m[20221213 20:39:10 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:39:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:10 @agent_ppo2.py:185][0m |           0.0215 |           0.0176 |           0.0000 |
[32m[20221213 20:39:10 @agent_ppo2.py:185][0m |          -0.0055 |           0.0137 |           0.0000 |
[32m[20221213 20:39:10 @agent_ppo2.py:185][0m |          -0.0178 |           0.0132 |           0.0000 |
[32m[20221213 20:39:10 @agent_ppo2.py:185][0m |          -0.0228 |           0.0131 |           0.0000 |
[32m[20221213 20:39:10 @agent_ppo2.py:185][0m |          -0.0226 |           0.0130 |           0.0000 |
[32m[20221213 20:39:11 @agent_ppo2.py:185][0m |          -0.0335 |           0.0133 |           0.0000 |
[32m[20221213 20:39:11 @agent_ppo2.py:185][0m |          -0.0348 |           0.0128 |           0.0000 |
[32m[20221213 20:39:11 @agent_ppo2.py:185][0m |          -0.0364 |           0.0126 |           0.0000 |
[32m[20221213 20:39:11 @agent_ppo2.py:185][0m |          -0.0368 |           0.0127 |           0.0000 |
[32m[20221213 20:39:11 @agent_ppo2.py:185][0m |          -0.0384 |           0.0126 |           0.0000 |
[32m[20221213 20:39:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:39:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.70
[32m[20221213 20:39:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.34
[32m[20221213 20:39:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.94
[32m[20221213 20:39:11 @agent_ppo2.py:143][0m Total time:      10.03 min
[32m[20221213 20:39:11 @agent_ppo2.py:145][0m 1437696 total steps have happened
[32m[20221213 20:39:11 @agent_ppo2.py:121][0m #------------------------ Iteration 351 --------------------------#
[32m[20221213 20:39:12 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:39:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:12 @agent_ppo2.py:185][0m |           0.0434 |           0.0154 |           0.0000 |
[32m[20221213 20:39:12 @agent_ppo2.py:185][0m |           0.0133 |           0.0135 |           0.0000 |
[32m[20221213 20:39:12 @agent_ppo2.py:185][0m |          -0.0034 |           0.0133 |           0.0000 |
[32m[20221213 20:39:12 @agent_ppo2.py:185][0m |          -0.0196 |           0.0128 |           0.0000 |
[32m[20221213 20:39:12 @agent_ppo2.py:185][0m |          -0.0281 |           0.0127 |           0.0000 |
[32m[20221213 20:39:12 @agent_ppo2.py:185][0m |          -0.0335 |           0.0124 |           0.0000 |
[32m[20221213 20:39:12 @agent_ppo2.py:185][0m |          -0.0368 |           0.0124 |           0.0000 |
[32m[20221213 20:39:12 @agent_ppo2.py:185][0m |          -0.0365 |           0.0121 |           0.0000 |
[32m[20221213 20:39:12 @agent_ppo2.py:185][0m |          -0.0457 |           0.0121 |           0.0000 |
[32m[20221213 20:39:13 @agent_ppo2.py:185][0m |          -0.0467 |           0.0120 |           0.0000 |
[32m[20221213 20:39:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:39:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.50
[32m[20221213 20:39:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.32
[32m[20221213 20:39:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.50
[32m[20221213 20:39:13 @agent_ppo2.py:143][0m Total time:      10.06 min
[32m[20221213 20:39:13 @agent_ppo2.py:145][0m 1441792 total steps have happened
[32m[20221213 20:39:13 @agent_ppo2.py:121][0m #------------------------ Iteration 352 --------------------------#
[32m[20221213 20:39:13 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:14 @agent_ppo2.py:185][0m |           0.0231 |           0.0134 |           0.0000 |
[32m[20221213 20:39:14 @agent_ppo2.py:185][0m |          -0.0223 |           0.0125 |           0.0000 |
[32m[20221213 20:39:14 @agent_ppo2.py:185][0m |          -0.0251 |           0.0123 |           0.0000 |
[32m[20221213 20:39:14 @agent_ppo2.py:185][0m |          -0.0329 |           0.0122 |           0.0000 |
[32m[20221213 20:39:14 @agent_ppo2.py:185][0m |          -0.0455 |           0.0121 |           0.0000 |
[32m[20221213 20:39:14 @agent_ppo2.py:185][0m |          -0.0440 |           0.0120 |           0.0000 |
[32m[20221213 20:39:14 @agent_ppo2.py:185][0m |          -0.0501 |           0.0121 |           0.0000 |
[32m[20221213 20:39:14 @agent_ppo2.py:185][0m |          -0.0484 |           0.0119 |           0.0000 |
[32m[20221213 20:39:14 @agent_ppo2.py:185][0m |          -0.0535 |           0.0119 |           0.0000 |
[32m[20221213 20:39:14 @agent_ppo2.py:185][0m |          -0.0561 |           0.0118 |           0.0000 |
[32m[20221213 20:39:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:39:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.86
[32m[20221213 20:39:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.64
[32m[20221213 20:39:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.66
[32m[20221213 20:39:15 @agent_ppo2.py:143][0m Total time:      10.09 min
[32m[20221213 20:39:15 @agent_ppo2.py:145][0m 1445888 total steps have happened
[32m[20221213 20:39:15 @agent_ppo2.py:121][0m #------------------------ Iteration 353 --------------------------#
[32m[20221213 20:39:15 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:39:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:15 @agent_ppo2.py:185][0m |           0.0155 |           0.0326 |           0.0000 |
[32m[20221213 20:39:15 @agent_ppo2.py:185][0m |           0.0010 |           0.0194 |           0.0000 |
[32m[20221213 20:39:15 @agent_ppo2.py:185][0m |          -0.0094 |           0.0176 |           0.0000 |
[32m[20221213 20:39:16 @agent_ppo2.py:185][0m |          -0.0160 |           0.0172 |           0.0000 |
[32m[20221213 20:39:16 @agent_ppo2.py:185][0m |          -0.0389 |           0.0214 |           0.0000 |
[32m[20221213 20:39:16 @agent_ppo2.py:185][0m |          -0.0043 |           0.0230 |           0.0000 |
[32m[20221213 20:39:16 @agent_ppo2.py:185][0m |          -0.0097 |           0.0169 |           0.0000 |
[32m[20221213 20:39:16 @agent_ppo2.py:185][0m |          -0.0123 |           0.0161 |           0.0000 |
[32m[20221213 20:39:16 @agent_ppo2.py:185][0m |          -0.0145 |           0.0160 |           0.0000 |
[32m[20221213 20:39:16 @agent_ppo2.py:185][0m |          -0.0150 |           0.0158 |           0.0000 |
[32m[20221213 20:39:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:39:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.35
[32m[20221213 20:39:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.03
[32m[20221213 20:39:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.07
[32m[20221213 20:39:16 @agent_ppo2.py:143][0m Total time:      10.12 min
[32m[20221213 20:39:16 @agent_ppo2.py:145][0m 1449984 total steps have happened
[32m[20221213 20:39:16 @agent_ppo2.py:121][0m #------------------------ Iteration 354 --------------------------#
[32m[20221213 20:39:17 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:39:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:17 @agent_ppo2.py:185][0m |           0.0786 |           0.0348 |           0.0000 |
[32m[20221213 20:39:17 @agent_ppo2.py:185][0m |           0.0588 |           0.0105 |           0.0000 |
[32m[20221213 20:39:17 @agent_ppo2.py:185][0m |           0.0209 |           0.0098 |           0.0000 |
[32m[20221213 20:39:17 @agent_ppo2.py:185][0m |           0.0102 |           0.0091 |           0.0000 |
[32m[20221213 20:39:17 @agent_ppo2.py:185][0m |           0.0038 |           0.0090 |           0.0000 |
[32m[20221213 20:39:17 @agent_ppo2.py:185][0m |          -0.0019 |           0.0089 |           0.0000 |
[32m[20221213 20:39:18 @agent_ppo2.py:185][0m |          -0.0084 |           0.0088 |           0.0000 |
[32m[20221213 20:39:18 @agent_ppo2.py:185][0m |          -0.0154 |           0.0088 |           0.0000 |
[32m[20221213 20:39:18 @agent_ppo2.py:185][0m |          -0.0178 |           0.0087 |           0.0000 |
[32m[20221213 20:39:18 @agent_ppo2.py:185][0m |           0.0065 |           0.0089 |           0.0000 |
[32m[20221213 20:39:18 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:39:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.53
[32m[20221213 20:39:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.77
[32m[20221213 20:39:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.76
[32m[20221213 20:39:18 @agent_ppo2.py:143][0m Total time:      10.15 min
[32m[20221213 20:39:18 @agent_ppo2.py:145][0m 1454080 total steps have happened
[32m[20221213 20:39:18 @agent_ppo2.py:121][0m #------------------------ Iteration 355 --------------------------#
[32m[20221213 20:39:19 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:19 @agent_ppo2.py:185][0m |           0.0198 |           0.0140 |           0.0000 |
[32m[20221213 20:39:19 @agent_ppo2.py:185][0m |          -0.0004 |           0.0106 |           0.0000 |
[32m[20221213 20:39:19 @agent_ppo2.py:185][0m |          -0.0194 |           0.0103 |           0.0000 |
[32m[20221213 20:39:19 @agent_ppo2.py:185][0m |          -0.0191 |           0.0102 |           0.0000 |
[32m[20221213 20:39:19 @agent_ppo2.py:185][0m |          -0.0253 |           0.0102 |           0.0000 |
[32m[20221213 20:39:19 @agent_ppo2.py:185][0m |          -0.0281 |           0.0102 |           0.0000 |
[32m[20221213 20:39:19 @agent_ppo2.py:185][0m |          -0.0296 |           0.0100 |           0.0000 |
[32m[20221213 20:39:19 @agent_ppo2.py:185][0m |          -0.0252 |           0.0100 |           0.0000 |
[32m[20221213 20:39:19 @agent_ppo2.py:185][0m |          -0.0364 |           0.0100 |           0.0000 |
[32m[20221213 20:39:20 @agent_ppo2.py:185][0m |          -0.0378 |           0.0099 |           0.0000 |
[32m[20221213 20:39:20 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:39:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.63
[32m[20221213 20:39:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.10
[32m[20221213 20:39:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.68
[32m[20221213 20:39:20 @agent_ppo2.py:143][0m Total time:      10.18 min
[32m[20221213 20:39:20 @agent_ppo2.py:145][0m 1458176 total steps have happened
[32m[20221213 20:39:20 @agent_ppo2.py:121][0m #------------------------ Iteration 356 --------------------------#
[32m[20221213 20:39:20 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:20 @agent_ppo2.py:185][0m |           0.3788 |           0.0108 |           0.0000 |
[32m[20221213 20:39:21 @agent_ppo2.py:185][0m |           0.0273 |           0.0100 |           0.0000 |
[32m[20221213 20:39:21 @agent_ppo2.py:185][0m |           0.0065 |           0.0095 |           0.0000 |
[32m[20221213 20:39:21 @agent_ppo2.py:185][0m |          -0.0263 |           0.0095 |           0.0000 |
[32m[20221213 20:39:21 @agent_ppo2.py:185][0m |          -0.0301 |           0.0094 |           0.0000 |
[32m[20221213 20:39:21 @agent_ppo2.py:185][0m |          -0.0335 |           0.0093 |           0.0000 |
[32m[20221213 20:39:21 @agent_ppo2.py:185][0m |          -0.0349 |           0.0092 |           0.0000 |
[32m[20221213 20:39:21 @agent_ppo2.py:185][0m |          -0.0510 |           0.0091 |           0.0000 |
[32m[20221213 20:39:21 @agent_ppo2.py:185][0m |          -0.0478 |           0.0091 |           0.0000 |
[32m[20221213 20:39:21 @agent_ppo2.py:185][0m |          -0.0500 |           0.0090 |           0.0000 |
[32m[20221213 20:39:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:39:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.43
[32m[20221213 20:39:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.86
[32m[20221213 20:39:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.85
[32m[20221213 20:39:22 @agent_ppo2.py:143][0m Total time:      10.21 min
[32m[20221213 20:39:22 @agent_ppo2.py:145][0m 1462272 total steps have happened
[32m[20221213 20:39:22 @agent_ppo2.py:121][0m #------------------------ Iteration 357 --------------------------#
[32m[20221213 20:39:22 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:39:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:22 @agent_ppo2.py:185][0m |           0.0263 |           0.0147 |           0.0000 |
[32m[20221213 20:39:22 @agent_ppo2.py:185][0m |          -0.0109 |           0.0121 |           0.0000 |
[32m[20221213 20:39:22 @agent_ppo2.py:185][0m |          -0.0233 |           0.0118 |           0.0000 |
[32m[20221213 20:39:22 @agent_ppo2.py:185][0m |          -0.0243 |           0.0116 |           0.0000 |
[32m[20221213 20:39:23 @agent_ppo2.py:185][0m |          -0.0287 |           0.0112 |           0.0000 |
[32m[20221213 20:39:23 @agent_ppo2.py:185][0m |          -0.0331 |           0.0113 |           0.0000 |
[32m[20221213 20:39:23 @agent_ppo2.py:185][0m |          -0.0355 |           0.0111 |           0.0000 |
[32m[20221213 20:39:23 @agent_ppo2.py:185][0m |          -0.0388 |           0.0109 |           0.0000 |
[32m[20221213 20:39:23 @agent_ppo2.py:185][0m |          -0.0385 |           0.0110 |           0.0000 |
[32m[20221213 20:39:23 @agent_ppo2.py:185][0m |          -0.0840 |           0.0112 |           0.0000 |
[32m[20221213 20:39:23 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:39:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.11
[32m[20221213 20:39:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.55
[32m[20221213 20:39:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.75
[32m[20221213 20:39:23 @agent_ppo2.py:143][0m Total time:      10.24 min
[32m[20221213 20:39:23 @agent_ppo2.py:145][0m 1466368 total steps have happened
[32m[20221213 20:39:23 @agent_ppo2.py:121][0m #------------------------ Iteration 358 --------------------------#
[32m[20221213 20:39:24 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:24 @agent_ppo2.py:185][0m |           0.0219 |           0.0195 |           0.0000 |
[32m[20221213 20:39:24 @agent_ppo2.py:185][0m |          -0.0052 |           0.0171 |           0.0000 |
[32m[20221213 20:39:24 @agent_ppo2.py:185][0m |          -0.0207 |           0.0167 |           0.0000 |
[32m[20221213 20:39:24 @agent_ppo2.py:185][0m |          -0.0311 |           0.0165 |           0.0000 |
[32m[20221213 20:39:24 @agent_ppo2.py:185][0m |          -0.0343 |           0.0158 |           0.0000 |
[32m[20221213 20:39:24 @agent_ppo2.py:185][0m |          -0.0379 |           0.0155 |           0.0000 |
[32m[20221213 20:39:24 @agent_ppo2.py:185][0m |          -0.0392 |           0.0148 |           0.0000 |
[32m[20221213 20:39:24 @agent_ppo2.py:185][0m |          -0.0698 |           0.0150 |           0.0000 |
[32m[20221213 20:39:25 @agent_ppo2.py:185][0m |          -0.0435 |           0.0155 |           0.0000 |
[32m[20221213 20:39:25 @agent_ppo2.py:185][0m |          -0.0423 |           0.0145 |           0.0000 |
[32m[20221213 20:39:25 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:39:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.42
[32m[20221213 20:39:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.86
[32m[20221213 20:39:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.69
[32m[20221213 20:39:25 @agent_ppo2.py:143][0m Total time:      10.26 min
[32m[20221213 20:39:25 @agent_ppo2.py:145][0m 1470464 total steps have happened
[32m[20221213 20:39:25 @agent_ppo2.py:121][0m #------------------------ Iteration 359 --------------------------#
[32m[20221213 20:39:25 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:39:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:26 @agent_ppo2.py:185][0m |           0.0545 |           0.0179 |           0.0000 |
[32m[20221213 20:39:26 @agent_ppo2.py:185][0m |           0.0032 |           0.0119 |           0.0000 |
[32m[20221213 20:39:26 @agent_ppo2.py:185][0m |          -0.0101 |           0.0116 |           0.0000 |
[32m[20221213 20:39:26 @agent_ppo2.py:185][0m |           0.0026 |           0.0132 |           0.0000 |
[32m[20221213 20:39:26 @agent_ppo2.py:185][0m |          -0.0202 |           0.0145 |           0.0000 |
[32m[20221213 20:39:26 @agent_ppo2.py:185][0m |          -0.0214 |           0.0113 |           0.0000 |
[32m[20221213 20:39:26 @agent_ppo2.py:185][0m |          -0.0316 |           0.0112 |           0.0000 |
[32m[20221213 20:39:26 @agent_ppo2.py:185][0m |          -0.0389 |           0.0111 |           0.0000 |
[32m[20221213 20:39:26 @agent_ppo2.py:185][0m |          -0.0415 |           0.0109 |           0.0000 |
[32m[20221213 20:39:26 @agent_ppo2.py:185][0m |          -0.0406 |           0.0108 |           0.0000 |
[32m[20221213 20:39:26 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:39:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.75
[32m[20221213 20:39:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.84
[32m[20221213 20:39:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.58
[32m[20221213 20:39:27 @agent_ppo2.py:143][0m Total time:      10.29 min
[32m[20221213 20:39:27 @agent_ppo2.py:145][0m 1474560 total steps have happened
[32m[20221213 20:39:27 @agent_ppo2.py:121][0m #------------------------ Iteration 360 --------------------------#
[32m[20221213 20:39:27 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:39:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:27 @agent_ppo2.py:185][0m |           0.0284 |           0.0160 |           0.0000 |
[32m[20221213 20:39:27 @agent_ppo2.py:185][0m |           0.0093 |           0.0125 |           0.0000 |
[32m[20221213 20:39:28 @agent_ppo2.py:185][0m |          -0.0106 |           0.0121 |           0.0000 |
[32m[20221213 20:39:28 @agent_ppo2.py:185][0m |          -0.0232 |           0.0121 |           0.0000 |
[32m[20221213 20:39:28 @agent_ppo2.py:185][0m |          -0.0249 |           0.0120 |           0.0000 |
[32m[20221213 20:39:28 @agent_ppo2.py:185][0m |          -0.0238 |           0.0119 |           0.0000 |
[32m[20221213 20:39:28 @agent_ppo2.py:185][0m |          -0.0265 |           0.0119 |           0.0000 |
[32m[20221213 20:39:28 @agent_ppo2.py:185][0m |          -0.0319 |           0.0119 |           0.0000 |
[32m[20221213 20:39:28 @agent_ppo2.py:185][0m |          -0.0352 |           0.0117 |           0.0000 |
[32m[20221213 20:39:28 @agent_ppo2.py:185][0m |          -0.0344 |           0.0117 |           0.0000 |
[32m[20221213 20:39:28 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:39:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.90
[32m[20221213 20:39:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.49
[32m[20221213 20:39:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.63
[32m[20221213 20:39:29 @agent_ppo2.py:143][0m Total time:      10.32 min
[32m[20221213 20:39:29 @agent_ppo2.py:145][0m 1478656 total steps have happened
[32m[20221213 20:39:29 @agent_ppo2.py:121][0m #------------------------ Iteration 361 --------------------------#
[32m[20221213 20:39:29 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:39:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:29 @agent_ppo2.py:185][0m |           0.0300 |           0.0569 |           0.0000 |
[32m[20221213 20:39:29 @agent_ppo2.py:185][0m |           0.0105 |           0.0321 |           0.0000 |
[32m[20221213 20:39:29 @agent_ppo2.py:185][0m |          -0.0090 |           0.0264 |           0.0000 |
[32m[20221213 20:39:29 @agent_ppo2.py:185][0m |          -0.0156 |           0.0263 |           0.0000 |
[32m[20221213 20:39:29 @agent_ppo2.py:185][0m |          -0.0150 |           0.0250 |           0.0000 |
[32m[20221213 20:39:30 @agent_ppo2.py:185][0m |          -0.0284 |           0.0251 |           0.0000 |
[32m[20221213 20:39:30 @agent_ppo2.py:185][0m |          -0.0284 |           0.0216 |           0.0000 |
[32m[20221213 20:39:30 @agent_ppo2.py:185][0m |          -0.0747 |           0.0215 |           0.0000 |
[32m[20221213 20:39:30 @agent_ppo2.py:185][0m |          -0.0309 |           0.0224 |           0.0000 |
[32m[20221213 20:39:30 @agent_ppo2.py:185][0m |          -0.0304 |           0.0208 |           0.0000 |
[32m[20221213 20:39:30 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:39:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.38
[32m[20221213 20:39:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.86
[32m[20221213 20:39:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.79
[32m[20221213 20:39:30 @agent_ppo2.py:143][0m Total time:      10.35 min
[32m[20221213 20:39:30 @agent_ppo2.py:145][0m 1482752 total steps have happened
[32m[20221213 20:39:30 @agent_ppo2.py:121][0m #------------------------ Iteration 362 --------------------------#
[32m[20221213 20:39:31 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:31 @agent_ppo2.py:185][0m |           0.0398 |           0.0297 |           0.0000 |
[32m[20221213 20:39:31 @agent_ppo2.py:185][0m |           0.0333 |           0.0156 |           0.0000 |
[32m[20221213 20:39:31 @agent_ppo2.py:185][0m |           0.0085 |           0.0149 |           0.0000 |
[32m[20221213 20:39:31 @agent_ppo2.py:185][0m |          -0.0061 |           0.0146 |           0.0000 |
[32m[20221213 20:39:31 @agent_ppo2.py:185][0m |          -0.0222 |           0.0145 |           0.0000 |
[32m[20221213 20:39:31 @agent_ppo2.py:185][0m |          -0.0316 |           0.0141 |           0.0000 |
[32m[20221213 20:39:31 @agent_ppo2.py:185][0m |          -0.0335 |           0.0139 |           0.0000 |
[32m[20221213 20:39:32 @agent_ppo2.py:185][0m |          -0.0470 |           0.0142 |           0.0000 |
[32m[20221213 20:39:32 @agent_ppo2.py:185][0m |          -0.0339 |           0.0142 |           0.0000 |
[32m[20221213 20:39:32 @agent_ppo2.py:185][0m |          -0.0396 |           0.0138 |           0.0000 |
[32m[20221213 20:39:32 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:39:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.99
[32m[20221213 20:39:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.04
[32m[20221213 20:39:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.57
[32m[20221213 20:39:32 @agent_ppo2.py:143][0m Total time:      10.38 min
[32m[20221213 20:39:32 @agent_ppo2.py:145][0m 1486848 total steps have happened
[32m[20221213 20:39:32 @agent_ppo2.py:121][0m #------------------------ Iteration 363 --------------------------#
[32m[20221213 20:39:32 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:33 @agent_ppo2.py:185][0m |           0.0999 |           0.0156 |           0.0000 |
[32m[20221213 20:39:33 @agent_ppo2.py:185][0m |           0.0050 |           0.0117 |           0.0000 |
[32m[20221213 20:39:33 @agent_ppo2.py:185][0m |          -0.0211 |           0.0114 |           0.0000 |
[32m[20221213 20:39:33 @agent_ppo2.py:185][0m |          -0.0292 |           0.0113 |           0.0000 |
[32m[20221213 20:39:33 @agent_ppo2.py:185][0m |          -0.0018 |           0.0128 |           0.0000 |
[32m[20221213 20:39:33 @agent_ppo2.py:185][0m |          -0.0442 |           0.0124 |           0.0000 |
[32m[20221213 20:39:33 @agent_ppo2.py:185][0m |          -0.0493 |           0.0109 |           0.0000 |
[32m[20221213 20:39:33 @agent_ppo2.py:185][0m |          -0.0546 |           0.0107 |           0.0000 |
[32m[20221213 20:39:33 @agent_ppo2.py:185][0m |          -0.0413 |           0.0108 |           0.0000 |
[32m[20221213 20:39:33 @agent_ppo2.py:185][0m |          -0.0602 |           0.0108 |           0.0000 |
[32m[20221213 20:39:33 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:39:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.98
[32m[20221213 20:39:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.62
[32m[20221213 20:39:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.56
[32m[20221213 20:39:34 @agent_ppo2.py:143][0m Total time:      10.41 min
[32m[20221213 20:39:34 @agent_ppo2.py:145][0m 1490944 total steps have happened
[32m[20221213 20:39:34 @agent_ppo2.py:121][0m #------------------------ Iteration 364 --------------------------#
[32m[20221213 20:39:34 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:34 @agent_ppo2.py:185][0m |           0.0198 |           0.0380 |           0.0000 |
[32m[20221213 20:39:34 @agent_ppo2.py:185][0m |           0.0060 |           0.0198 |           0.0000 |
[32m[20221213 20:39:34 @agent_ppo2.py:185][0m |          -0.0049 |           0.0181 |           0.0000 |
[32m[20221213 20:39:35 @agent_ppo2.py:185][0m |          -0.0096 |           0.0172 |           0.0000 |
[32m[20221213 20:39:35 @agent_ppo2.py:185][0m |          -0.0157 |           0.0167 |           0.0000 |
[32m[20221213 20:39:35 @agent_ppo2.py:185][0m |          -0.0188 |           0.0163 |           0.0000 |
[32m[20221213 20:39:35 @agent_ppo2.py:185][0m |          -0.0208 |           0.0161 |           0.0000 |
[32m[20221213 20:39:35 @agent_ppo2.py:185][0m |          -0.0208 |           0.0161 |           0.0000 |
[32m[20221213 20:39:35 @agent_ppo2.py:185][0m |          -0.0286 |           0.0154 |           0.0000 |
[32m[20221213 20:39:35 @agent_ppo2.py:185][0m |          -0.0235 |           0.0154 |           0.0000 |
[32m[20221213 20:39:35 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:39:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.95
[32m[20221213 20:39:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.61
[32m[20221213 20:39:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.91
[32m[20221213 20:39:35 @agent_ppo2.py:143][0m Total time:      10.44 min
[32m[20221213 20:39:35 @agent_ppo2.py:145][0m 1495040 total steps have happened
[32m[20221213 20:39:35 @agent_ppo2.py:121][0m #------------------------ Iteration 365 --------------------------#
[32m[20221213 20:39:36 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:36 @agent_ppo2.py:185][0m |           0.0201 |           0.0275 |           0.0000 |
[32m[20221213 20:39:36 @agent_ppo2.py:185][0m |          -0.0047 |           0.0238 |           0.0000 |
[32m[20221213 20:39:36 @agent_ppo2.py:185][0m |          -0.0193 |           0.0223 |           0.0000 |
[32m[20221213 20:39:36 @agent_ppo2.py:185][0m |          -0.0282 |           0.0209 |           0.0000 |
[32m[20221213 20:39:36 @agent_ppo2.py:185][0m |          -0.0338 |           0.0202 |           0.0000 |
[32m[20221213 20:39:36 @agent_ppo2.py:185][0m |          -0.0417 |           0.0202 |           0.0000 |
[32m[20221213 20:39:37 @agent_ppo2.py:185][0m |          -0.0493 |           0.0209 |           0.0000 |
[32m[20221213 20:39:37 @agent_ppo2.py:185][0m |          -0.0480 |           0.0210 |           0.0000 |
[32m[20221213 20:39:37 @agent_ppo2.py:185][0m |          -0.0473 |           0.0195 |           0.0000 |
[32m[20221213 20:39:37 @agent_ppo2.py:185][0m |          -0.0469 |           0.0189 |           0.0000 |
[32m[20221213 20:39:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:39:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.69
[32m[20221213 20:39:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.88
[32m[20221213 20:39:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.53
[32m[20221213 20:39:37 @agent_ppo2.py:143][0m Total time:      10.47 min
[32m[20221213 20:39:37 @agent_ppo2.py:145][0m 1499136 total steps have happened
[32m[20221213 20:39:37 @agent_ppo2.py:121][0m #------------------------ Iteration 366 --------------------------#
[32m[20221213 20:39:38 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:38 @agent_ppo2.py:185][0m |           0.0197 |           0.0417 |           0.0000 |
[32m[20221213 20:39:38 @agent_ppo2.py:185][0m |          -0.0068 |           0.0139 |           0.0000 |
[32m[20221213 20:39:38 @agent_ppo2.py:185][0m |          -0.0173 |           0.0133 |           0.0000 |
[32m[20221213 20:39:38 @agent_ppo2.py:185][0m |          -0.0263 |           0.0130 |           0.0000 |
[32m[20221213 20:39:38 @agent_ppo2.py:185][0m |          -0.0305 |           0.0129 |           0.0000 |
[32m[20221213 20:39:38 @agent_ppo2.py:185][0m |          -0.0367 |           0.0127 |           0.0000 |
[32m[20221213 20:39:38 @agent_ppo2.py:185][0m |          -0.0374 |           0.0125 |           0.0000 |
[32m[20221213 20:39:38 @agent_ppo2.py:185][0m |          -0.0411 |           0.0124 |           0.0000 |
[32m[20221213 20:39:38 @agent_ppo2.py:185][0m |          -0.0287 |           0.0127 |           0.0000 |
[32m[20221213 20:39:38 @agent_ppo2.py:185][0m |          -0.0444 |           0.0124 |           0.0000 |
[32m[20221213 20:39:38 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:39:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.66
[32m[20221213 20:39:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.30
[32m[20221213 20:39:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.35
[32m[20221213 20:39:39 @agent_ppo2.py:143][0m Total time:      10.50 min
[32m[20221213 20:39:39 @agent_ppo2.py:145][0m 1503232 total steps have happened
[32m[20221213 20:39:39 @agent_ppo2.py:121][0m #------------------------ Iteration 367 --------------------------#
[32m[20221213 20:39:39 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:39 @agent_ppo2.py:185][0m |           0.0287 |           0.0351 |           0.0000 |
[32m[20221213 20:39:40 @agent_ppo2.py:185][0m |          -0.0006 |           0.0183 |           0.0000 |
[32m[20221213 20:39:40 @agent_ppo2.py:185][0m |          -0.0149 |           0.0170 |           0.0000 |
[32m[20221213 20:39:40 @agent_ppo2.py:185][0m |          -0.0201 |           0.0161 |           0.0000 |
[32m[20221213 20:39:40 @agent_ppo2.py:185][0m |          -0.0196 |           0.0164 |           0.0000 |
[32m[20221213 20:39:40 @agent_ppo2.py:185][0m |          -0.0266 |           0.0160 |           0.0000 |
[32m[20221213 20:39:40 @agent_ppo2.py:185][0m |          -0.0284 |           0.0157 |           0.0000 |
[32m[20221213 20:39:40 @agent_ppo2.py:185][0m |          -0.0315 |           0.0157 |           0.0000 |
[32m[20221213 20:39:40 @agent_ppo2.py:185][0m |          -0.0327 |           0.0149 |           0.0000 |
[32m[20221213 20:39:40 @agent_ppo2.py:185][0m |          -0.0449 |           0.0149 |           0.0000 |
[32m[20221213 20:39:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:39:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.04
[32m[20221213 20:39:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.03
[32m[20221213 20:39:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.08
[32m[20221213 20:39:41 @agent_ppo2.py:143][0m Total time:      10.52 min
[32m[20221213 20:39:41 @agent_ppo2.py:145][0m 1507328 total steps have happened
[32m[20221213 20:39:41 @agent_ppo2.py:121][0m #------------------------ Iteration 368 --------------------------#
[32m[20221213 20:39:41 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:39:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:41 @agent_ppo2.py:185][0m |           0.0475 |           0.0177 |           0.0000 |
[32m[20221213 20:39:41 @agent_ppo2.py:185][0m |          -0.0040 |           0.0159 |           0.0000 |
[32m[20221213 20:39:41 @agent_ppo2.py:185][0m |          -0.0401 |           0.0153 |           0.0000 |
[32m[20221213 20:39:41 @agent_ppo2.py:185][0m |          -0.0520 |           0.0151 |           0.0000 |
[32m[20221213 20:39:41 @agent_ppo2.py:185][0m |          -0.0563 |           0.0145 |           0.0000 |
[32m[20221213 20:39:42 @agent_ppo2.py:185][0m |          -0.0573 |           0.0149 |           0.0000 |
[32m[20221213 20:39:42 @agent_ppo2.py:185][0m |          -0.0815 |           0.0144 |           0.0000 |
[32m[20221213 20:39:42 @agent_ppo2.py:185][0m |          -0.0691 |           0.0142 |           0.0000 |
[32m[20221213 20:39:42 @agent_ppo2.py:185][0m |          -0.0523 |           0.0142 |           0.0000 |
[32m[20221213 20:39:42 @agent_ppo2.py:185][0m |          -0.0616 |           0.0151 |           0.0000 |
[32m[20221213 20:39:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:39:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.00
[32m[20221213 20:39:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.19
[32m[20221213 20:39:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.16
[32m[20221213 20:39:42 @agent_ppo2.py:143][0m Total time:      10.55 min
[32m[20221213 20:39:42 @agent_ppo2.py:145][0m 1511424 total steps have happened
[32m[20221213 20:39:42 @agent_ppo2.py:121][0m #------------------------ Iteration 369 --------------------------#
[32m[20221213 20:39:43 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:43 @agent_ppo2.py:185][0m |           0.0377 |           0.0159 |           0.0000 |
[32m[20221213 20:39:43 @agent_ppo2.py:185][0m |           0.0111 |           0.0115 |           0.0000 |
[32m[20221213 20:39:43 @agent_ppo2.py:185][0m |           0.0014 |           0.0111 |           0.0000 |
[32m[20221213 20:39:43 @agent_ppo2.py:185][0m |          -0.0310 |           0.0110 |           0.0000 |
[32m[20221213 20:39:43 @agent_ppo2.py:185][0m |          -0.0400 |           0.0109 |           0.0000 |
[32m[20221213 20:39:43 @agent_ppo2.py:185][0m |          -0.0225 |           0.0109 |           0.0000 |
[32m[20221213 20:39:43 @agent_ppo2.py:185][0m |          -0.0470 |           0.0110 |           0.0000 |
[32m[20221213 20:39:43 @agent_ppo2.py:185][0m |          -0.0524 |           0.0107 |           0.0000 |
[32m[20221213 20:39:43 @agent_ppo2.py:185][0m |          -0.0564 |           0.0107 |           0.0000 |
[32m[20221213 20:39:44 @agent_ppo2.py:185][0m |          -0.0579 |           0.0106 |           0.0000 |
[32m[20221213 20:39:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:39:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.07
[32m[20221213 20:39:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.46
[32m[20221213 20:39:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.66
[32m[20221213 20:39:44 @agent_ppo2.py:143][0m Total time:      10.58 min
[32m[20221213 20:39:44 @agent_ppo2.py:145][0m 1515520 total steps have happened
[32m[20221213 20:39:44 @agent_ppo2.py:121][0m #------------------------ Iteration 370 --------------------------#
[32m[20221213 20:39:44 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:39:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:45 @agent_ppo2.py:185][0m |           0.0296 |           0.0110 |           0.0000 |
[32m[20221213 20:39:45 @agent_ppo2.py:185][0m |           0.0042 |           0.0105 |           0.0000 |
[32m[20221213 20:39:45 @agent_ppo2.py:185][0m |          -0.0225 |           0.0104 |           0.0000 |
[32m[20221213 20:39:45 @agent_ppo2.py:185][0m |          -0.0440 |           0.0100 |           0.0000 |
[32m[20221213 20:39:45 @agent_ppo2.py:185][0m |          -0.0535 |           0.0098 |           0.0000 |
[32m[20221213 20:39:45 @agent_ppo2.py:185][0m |          -0.0613 |           0.0098 |           0.0000 |
[32m[20221213 20:39:45 @agent_ppo2.py:185][0m |          -0.0686 |           0.0097 |           0.0000 |
[32m[20221213 20:39:45 @agent_ppo2.py:185][0m |          -0.0630 |           0.0096 |           0.0000 |
[32m[20221213 20:39:45 @agent_ppo2.py:185][0m |          -0.0662 |           0.0096 |           0.0000 |
[32m[20221213 20:39:45 @agent_ppo2.py:185][0m |          -0.0701 |           0.0095 |           0.0000 |
[32m[20221213 20:39:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:39:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.75
[32m[20221213 20:39:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.95
[32m[20221213 20:39:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.44
[32m[20221213 20:39:46 @agent_ppo2.py:143][0m Total time:      10.61 min
[32m[20221213 20:39:46 @agent_ppo2.py:145][0m 1519616 total steps have happened
[32m[20221213 20:39:46 @agent_ppo2.py:121][0m #------------------------ Iteration 371 --------------------------#
[32m[20221213 20:39:46 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:39:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:46 @agent_ppo2.py:185][0m |           0.0347 |           0.0109 |           0.0000 |
[32m[20221213 20:39:46 @agent_ppo2.py:185][0m |          -0.0032 |           0.0105 |           0.0000 |
[32m[20221213 20:39:47 @agent_ppo2.py:185][0m |          -0.0240 |           0.0104 |           0.0000 |
[32m[20221213 20:39:47 @agent_ppo2.py:185][0m |          -0.0316 |           0.0103 |           0.0000 |
[32m[20221213 20:39:47 @agent_ppo2.py:185][0m |          -0.0398 |           0.0101 |           0.0000 |
[32m[20221213 20:39:47 @agent_ppo2.py:185][0m |          -0.0496 |           0.0100 |           0.0000 |
[32m[20221213 20:39:47 @agent_ppo2.py:185][0m |          -0.0512 |           0.0100 |           0.0000 |
[32m[20221213 20:39:47 @agent_ppo2.py:185][0m |          -0.0462 |           0.0103 |           0.0000 |
[32m[20221213 20:39:47 @agent_ppo2.py:185][0m |          -0.0481 |           0.0107 |           0.0000 |
[32m[20221213 20:39:47 @agent_ppo2.py:185][0m |          -0.0486 |           0.0098 |           0.0000 |
[32m[20221213 20:39:47 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:39:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.06
[32m[20221213 20:39:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.63
[32m[20221213 20:39:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.10
[32m[20221213 20:39:48 @agent_ppo2.py:143][0m Total time:      10.64 min
[32m[20221213 20:39:48 @agent_ppo2.py:145][0m 1523712 total steps have happened
[32m[20221213 20:39:48 @agent_ppo2.py:121][0m #------------------------ Iteration 372 --------------------------#
[32m[20221213 20:39:48 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:39:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:48 @agent_ppo2.py:185][0m |           0.0372 |           0.0126 |           0.0000 |
[32m[20221213 20:39:48 @agent_ppo2.py:185][0m |           0.0073 |           0.0113 |           0.0000 |
[32m[20221213 20:39:48 @agent_ppo2.py:185][0m |          -0.0246 |           0.0111 |           0.0000 |
[32m[20221213 20:39:49 @agent_ppo2.py:185][0m |          -0.0251 |           0.0110 |           0.0000 |
[32m[20221213 20:39:49 @agent_ppo2.py:185][0m |          -0.0785 |           0.0108 |           0.0000 |
[32m[20221213 20:39:49 @agent_ppo2.py:185][0m |          -0.0431 |           0.0111 |           0.0000 |
[32m[20221213 20:39:49 @agent_ppo2.py:185][0m |          -0.0487 |           0.0107 |           0.0000 |
[32m[20221213 20:39:49 @agent_ppo2.py:185][0m |          -0.0554 |           0.0106 |           0.0000 |
[32m[20221213 20:39:49 @agent_ppo2.py:185][0m |          -0.0551 |           0.0106 |           0.0000 |
[32m[20221213 20:39:49 @agent_ppo2.py:185][0m |          -0.0557 |           0.0106 |           0.0000 |
[32m[20221213 20:39:49 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 20:39:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.00
[32m[20221213 20:39:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.22
[32m[20221213 20:39:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.36
[32m[20221213 20:39:50 @agent_ppo2.py:143][0m Total time:      10.67 min
[32m[20221213 20:39:50 @agent_ppo2.py:145][0m 1527808 total steps have happened
[32m[20221213 20:39:50 @agent_ppo2.py:121][0m #------------------------ Iteration 373 --------------------------#
[32m[20221213 20:39:50 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:39:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:50 @agent_ppo2.py:185][0m |           0.0227 |           0.0185 |           0.0000 |
[32m[20221213 20:39:50 @agent_ppo2.py:185][0m |          -0.0115 |           0.0154 |           0.0000 |
[32m[20221213 20:39:50 @agent_ppo2.py:185][0m |          -0.0190 |           0.0150 |           0.0000 |
[32m[20221213 20:39:50 @agent_ppo2.py:185][0m |          -0.0189 |           0.0141 |           0.0000 |
[32m[20221213 20:39:50 @agent_ppo2.py:185][0m |          -0.0256 |           0.0140 |           0.0000 |
[32m[20221213 20:39:51 @agent_ppo2.py:185][0m |          -0.0357 |           0.0139 |           0.0000 |
[32m[20221213 20:39:51 @agent_ppo2.py:185][0m |          -0.0394 |           0.0139 |           0.0000 |
[32m[20221213 20:39:51 @agent_ppo2.py:185][0m |          -0.0417 |           0.0138 |           0.0000 |
[32m[20221213 20:39:51 @agent_ppo2.py:185][0m |          -0.0411 |           0.0136 |           0.0000 |
[32m[20221213 20:39:51 @agent_ppo2.py:185][0m |          -0.0395 |           0.0135 |           0.0000 |
[32m[20221213 20:39:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:39:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.48
[32m[20221213 20:39:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.38
[32m[20221213 20:39:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.19
[32m[20221213 20:39:51 @agent_ppo2.py:143][0m Total time:      10.70 min
[32m[20221213 20:39:51 @agent_ppo2.py:145][0m 1531904 total steps have happened
[32m[20221213 20:39:51 @agent_ppo2.py:121][0m #------------------------ Iteration 374 --------------------------#
[32m[20221213 20:39:52 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:39:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:52 @agent_ppo2.py:185][0m |           0.0483 |           0.0189 |           0.0000 |
[32m[20221213 20:39:52 @agent_ppo2.py:185][0m |           0.0291 |           0.0183 |           0.0000 |
[32m[20221213 20:39:52 @agent_ppo2.py:185][0m |           0.0104 |           0.0183 |           0.0000 |
[32m[20221213 20:39:52 @agent_ppo2.py:185][0m |          -0.0238 |           0.0184 |           0.0000 |
[32m[20221213 20:39:52 @agent_ppo2.py:185][0m |          -0.0405 |           0.0168 |           0.0000 |
[32m[20221213 20:39:52 @agent_ppo2.py:185][0m |          -0.0366 |           0.0164 |           0.0000 |
[32m[20221213 20:39:52 @agent_ppo2.py:185][0m |          -0.0512 |           0.0162 |           0.0000 |
[32m[20221213 20:39:52 @agent_ppo2.py:185][0m |          -0.0503 |           0.0156 |           0.0000 |
[32m[20221213 20:39:53 @agent_ppo2.py:185][0m |          -0.0496 |           0.0164 |           0.0000 |
[32m[20221213 20:39:53 @agent_ppo2.py:185][0m |          -0.0564 |           0.0160 |           0.0000 |
[32m[20221213 20:39:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:39:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.28
[32m[20221213 20:39:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.36
[32m[20221213 20:39:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.50
[32m[20221213 20:39:53 @agent_ppo2.py:143][0m Total time:      10.73 min
[32m[20221213 20:39:53 @agent_ppo2.py:145][0m 1536000 total steps have happened
[32m[20221213 20:39:53 @agent_ppo2.py:121][0m #------------------------ Iteration 375 --------------------------#
[32m[20221213 20:39:53 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:39:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:54 @agent_ppo2.py:185][0m |           0.0334 |           0.0223 |           0.0000 |
[32m[20221213 20:39:54 @agent_ppo2.py:185][0m |           0.0190 |           0.0095 |           0.0000 |
[32m[20221213 20:39:54 @agent_ppo2.py:185][0m |           0.0101 |           0.0090 |           0.0000 |
[32m[20221213 20:39:54 @agent_ppo2.py:185][0m |          -0.0001 |           0.0088 |           0.0000 |
[32m[20221213 20:39:54 @agent_ppo2.py:185][0m |           0.0126 |           0.0086 |           0.0000 |
[32m[20221213 20:39:54 @agent_ppo2.py:185][0m |          -0.0164 |           0.0085 |           0.0000 |
[32m[20221213 20:39:54 @agent_ppo2.py:185][0m |           0.0097 |           0.0084 |           0.0000 |
[32m[20221213 20:39:54 @agent_ppo2.py:185][0m |          -0.0238 |           0.0082 |           0.0000 |
[32m[20221213 20:39:54 @agent_ppo2.py:185][0m |          -0.0276 |           0.0082 |           0.0000 |
[32m[20221213 20:39:54 @agent_ppo2.py:185][0m |          -0.0265 |           0.0081 |           0.0000 |
[32m[20221213 20:39:54 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:39:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.34
[32m[20221213 20:39:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.80
[32m[20221213 20:39:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.30
[32m[20221213 20:39:55 @agent_ppo2.py:143][0m Total time:      10.76 min
[32m[20221213 20:39:55 @agent_ppo2.py:145][0m 1540096 total steps have happened
[32m[20221213 20:39:55 @agent_ppo2.py:121][0m #------------------------ Iteration 376 --------------------------#
[32m[20221213 20:39:55 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:39:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:55 @agent_ppo2.py:185][0m |           0.0338 |           0.0085 |           0.0000 |
[32m[20221213 20:39:55 @agent_ppo2.py:185][0m |           0.0056 |           0.0079 |           0.0000 |
[32m[20221213 20:39:55 @agent_ppo2.py:185][0m |          -0.0113 |           0.0078 |           0.0000 |
[32m[20221213 20:39:56 @agent_ppo2.py:185][0m |          -0.0255 |           0.0077 |           0.0000 |
[32m[20221213 20:39:56 @agent_ppo2.py:185][0m |          -0.0390 |           0.0076 |           0.0000 |
[32m[20221213 20:39:56 @agent_ppo2.py:185][0m |          -0.0227 |           0.0078 |           0.0000 |
[32m[20221213 20:39:56 @agent_ppo2.py:185][0m |          -0.0582 |           0.0076 |           0.0000 |
[32m[20221213 20:39:56 @agent_ppo2.py:185][0m |          -0.0591 |           0.0074 |           0.0000 |
[32m[20221213 20:39:56 @agent_ppo2.py:185][0m |          -0.0639 |           0.0073 |           0.0000 |
[32m[20221213 20:39:56 @agent_ppo2.py:185][0m |          -0.0634 |           0.0072 |           0.0000 |
[32m[20221213 20:39:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:39:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.46
[32m[20221213 20:39:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.08
[32m[20221213 20:39:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.12
[32m[20221213 20:39:56 @agent_ppo2.py:143][0m Total time:      10.79 min
[32m[20221213 20:39:56 @agent_ppo2.py:145][0m 1544192 total steps have happened
[32m[20221213 20:39:56 @agent_ppo2.py:121][0m #------------------------ Iteration 377 --------------------------#
[32m[20221213 20:39:57 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:39:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:57 @agent_ppo2.py:185][0m |           0.0267 |           0.0591 |           0.0000 |
[32m[20221213 20:39:57 @agent_ppo2.py:185][0m |           0.0036 |           0.0184 |           0.0000 |
[32m[20221213 20:39:57 @agent_ppo2.py:185][0m |          -0.0320 |           0.0159 |           0.0000 |
[32m[20221213 20:39:57 @agent_ppo2.py:185][0m |           0.0182 |           0.0162 |           0.0000 |
[32m[20221213 20:39:57 @agent_ppo2.py:185][0m |          -0.0031 |           0.0140 |           0.0000 |
[32m[20221213 20:39:57 @agent_ppo2.py:185][0m |          -0.0110 |           0.0137 |           0.0000 |
[32m[20221213 20:39:58 @agent_ppo2.py:185][0m |          -0.0141 |           0.0136 |           0.0000 |
[32m[20221213 20:39:58 @agent_ppo2.py:185][0m |          -0.0184 |           0.0131 |           0.0000 |
[32m[20221213 20:39:58 @agent_ppo2.py:185][0m |          -0.0360 |           0.0131 |           0.0000 |
[32m[20221213 20:39:58 @agent_ppo2.py:185][0m |          -0.0240 |           0.0133 |           0.0000 |
[32m[20221213 20:39:58 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:39:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.43
[32m[20221213 20:39:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.96
[32m[20221213 20:39:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.00
[32m[20221213 20:39:58 @agent_ppo2.py:143][0m Total time:      10.82 min
[32m[20221213 20:39:58 @agent_ppo2.py:145][0m 1548288 total steps have happened
[32m[20221213 20:39:58 @agent_ppo2.py:121][0m #------------------------ Iteration 378 --------------------------#
[32m[20221213 20:39:59 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:39:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:39:59 @agent_ppo2.py:185][0m |           0.0382 |           0.0231 |           0.0000 |
[32m[20221213 20:39:59 @agent_ppo2.py:185][0m |           0.0256 |           0.0117 |           0.0000 |
[32m[20221213 20:39:59 @agent_ppo2.py:185][0m |          -0.0022 |           0.0111 |           0.0000 |
[32m[20221213 20:39:59 @agent_ppo2.py:185][0m |          -0.0139 |           0.0108 |           0.0000 |
[32m[20221213 20:39:59 @agent_ppo2.py:185][0m |          -0.0184 |           0.0106 |           0.0000 |
[32m[20221213 20:39:59 @agent_ppo2.py:185][0m |          -0.0230 |           0.0106 |           0.0000 |
[32m[20221213 20:39:59 @agent_ppo2.py:185][0m |          -0.0016 |           0.0115 |           0.0000 |
[32m[20221213 20:39:59 @agent_ppo2.py:185][0m |          -0.0283 |           0.0108 |           0.0000 |
[32m[20221213 20:39:59 @agent_ppo2.py:185][0m |          -0.0216 |           0.0103 |           0.0000 |
[32m[20221213 20:40:00 @agent_ppo2.py:185][0m |          -0.0339 |           0.0102 |           0.0000 |
[32m[20221213 20:40:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:40:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.37
[32m[20221213 20:40:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.24
[32m[20221213 20:40:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.46
[32m[20221213 20:40:00 @agent_ppo2.py:143][0m Total time:      10.85 min
[32m[20221213 20:40:00 @agent_ppo2.py:145][0m 1552384 total steps have happened
[32m[20221213 20:40:00 @agent_ppo2.py:121][0m #------------------------ Iteration 379 --------------------------#
[32m[20221213 20:40:00 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:00 @agent_ppo2.py:185][0m |           0.0093 |           0.0100 |           0.0000 |
[32m[20221213 20:40:01 @agent_ppo2.py:185][0m |           0.0025 |           0.0079 |           0.0000 |
[32m[20221213 20:40:01 @agent_ppo2.py:185][0m |           0.0219 |           0.0076 |           0.0000 |
[32m[20221213 20:40:01 @agent_ppo2.py:185][0m |          -0.0155 |           0.0075 |           0.0000 |
[32m[20221213 20:40:01 @agent_ppo2.py:185][0m |          -0.0300 |           0.0074 |           0.0000 |
[32m[20221213 20:40:01 @agent_ppo2.py:185][0m |          -0.0362 |           0.0073 |           0.0000 |
[32m[20221213 20:40:01 @agent_ppo2.py:185][0m |          -0.0341 |           0.0073 |           0.0000 |
[32m[20221213 20:40:01 @agent_ppo2.py:185][0m |          -0.0396 |           0.0072 |           0.0000 |
[32m[20221213 20:40:01 @agent_ppo2.py:185][0m |          -0.0394 |           0.0072 |           0.0000 |
[32m[20221213 20:40:01 @agent_ppo2.py:185][0m |          -0.0216 |           0.0072 |           0.0000 |
[32m[20221213 20:40:01 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:40:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.47
[32m[20221213 20:40:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.35
[32m[20221213 20:40:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.81
[32m[20221213 20:40:02 @agent_ppo2.py:143][0m Total time:      10.87 min
[32m[20221213 20:40:02 @agent_ppo2.py:145][0m 1556480 total steps have happened
[32m[20221213 20:40:02 @agent_ppo2.py:121][0m #------------------------ Iteration 380 --------------------------#
[32m[20221213 20:40:02 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:40:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:02 @agent_ppo2.py:185][0m |           0.0268 |           0.0315 |           0.0000 |
[32m[20221213 20:40:02 @agent_ppo2.py:185][0m |           0.0138 |           0.0187 |           0.0000 |
[32m[20221213 20:40:02 @agent_ppo2.py:185][0m |           0.0078 |           0.0173 |           0.0000 |
[32m[20221213 20:40:02 @agent_ppo2.py:185][0m |          -0.0116 |           0.0157 |           0.0000 |
[32m[20221213 20:40:02 @agent_ppo2.py:185][0m |          -0.0159 |           0.0146 |           0.0000 |
[32m[20221213 20:40:03 @agent_ppo2.py:185][0m |          -0.0192 |           0.0142 |           0.0000 |
[32m[20221213 20:40:03 @agent_ppo2.py:185][0m |          -0.0222 |           0.0136 |           0.0000 |
[32m[20221213 20:40:03 @agent_ppo2.py:185][0m |          -0.0254 |           0.0135 |           0.0000 |
[32m[20221213 20:40:03 @agent_ppo2.py:185][0m |          -0.0248 |           0.0132 |           0.0000 |
[32m[20221213 20:40:03 @agent_ppo2.py:185][0m |          -0.0261 |           0.0131 |           0.0000 |
[32m[20221213 20:40:03 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:40:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.24
[32m[20221213 20:40:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.03
[32m[20221213 20:40:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.76
[32m[20221213 20:40:03 @agent_ppo2.py:143][0m Total time:      10.90 min
[32m[20221213 20:40:03 @agent_ppo2.py:145][0m 1560576 total steps have happened
[32m[20221213 20:40:03 @agent_ppo2.py:121][0m #------------------------ Iteration 381 --------------------------#
[32m[20221213 20:40:04 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:04 @agent_ppo2.py:185][0m |           0.0995 |           0.0126 |           0.0000 |
[32m[20221213 20:40:04 @agent_ppo2.py:185][0m |           0.0392 |           0.0116 |           0.0000 |
[32m[20221213 20:40:04 @agent_ppo2.py:185][0m |           0.0021 |           0.0113 |           0.0000 |
[32m[20221213 20:40:04 @agent_ppo2.py:185][0m |          -0.0175 |           0.0111 |           0.0000 |
[32m[20221213 20:40:04 @agent_ppo2.py:185][0m |          -0.0256 |           0.0109 |           0.0000 |
[32m[20221213 20:40:04 @agent_ppo2.py:185][0m |          -0.0362 |           0.0109 |           0.0000 |
[32m[20221213 20:40:04 @agent_ppo2.py:185][0m |          -0.0322 |           0.0108 |           0.0000 |
[32m[20221213 20:40:04 @agent_ppo2.py:185][0m |          -0.0548 |           0.0107 |           0.0000 |
[32m[20221213 20:40:05 @agent_ppo2.py:185][0m |          -0.0607 |           0.0105 |           0.0000 |
[32m[20221213 20:40:05 @agent_ppo2.py:185][0m |          -0.0703 |           0.0104 |           0.0000 |
[32m[20221213 20:40:05 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:40:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.11
[32m[20221213 20:40:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.98
[32m[20221213 20:40:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.72
[32m[20221213 20:40:05 @agent_ppo2.py:143][0m Total time:      10.93 min
[32m[20221213 20:40:05 @agent_ppo2.py:145][0m 1564672 total steps have happened
[32m[20221213 20:40:05 @agent_ppo2.py:121][0m #------------------------ Iteration 382 --------------------------#
[32m[20221213 20:40:05 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:06 @agent_ppo2.py:185][0m |           0.1883 |           0.0126 |           0.0000 |
[32m[20221213 20:40:06 @agent_ppo2.py:185][0m |           0.0111 |           0.0070 |           0.0000 |
[32m[20221213 20:40:06 @agent_ppo2.py:185][0m |          -0.0045 |           0.0067 |           0.0000 |
[32m[20221213 20:40:06 @agent_ppo2.py:185][0m |          -0.0044 |           0.0067 |           0.0000 |
[32m[20221213 20:40:06 @agent_ppo2.py:185][0m |          -0.0198 |           0.0065 |           0.0000 |
[32m[20221213 20:40:06 @agent_ppo2.py:185][0m |          -0.0283 |           0.0064 |           0.0000 |
[32m[20221213 20:40:06 @agent_ppo2.py:185][0m |          -0.0323 |           0.0063 |           0.0000 |
[32m[20221213 20:40:06 @agent_ppo2.py:185][0m |          -0.0376 |           0.0063 |           0.0000 |
[32m[20221213 20:40:06 @agent_ppo2.py:185][0m |          -0.0397 |           0.0062 |           0.0000 |
[32m[20221213 20:40:06 @agent_ppo2.py:185][0m |          -0.0328 |           0.0062 |           0.0000 |
[32m[20221213 20:40:06 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:40:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.08
[32m[20221213 20:40:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.29
[32m[20221213 20:40:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.38
[32m[20221213 20:40:07 @agent_ppo2.py:143][0m Total time:      10.96 min
[32m[20221213 20:40:07 @agent_ppo2.py:145][0m 1568768 total steps have happened
[32m[20221213 20:40:07 @agent_ppo2.py:121][0m #------------------------ Iteration 383 --------------------------#
[32m[20221213 20:40:07 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:07 @agent_ppo2.py:185][0m |           0.0222 |           0.0349 |           0.0000 |
[32m[20221213 20:40:07 @agent_ppo2.py:185][0m |           0.0024 |           0.0157 |           0.0000 |
[32m[20221213 20:40:07 @agent_ppo2.py:185][0m |          -0.0048 |           0.0143 |           0.0000 |
[32m[20221213 20:40:07 @agent_ppo2.py:185][0m |          -0.0112 |           0.0136 |           0.0000 |
[32m[20221213 20:40:07 @agent_ppo2.py:185][0m |          -0.0160 |           0.0135 |           0.0000 |
[32m[20221213 20:40:08 @agent_ppo2.py:185][0m |          -0.0194 |           0.0130 |           0.0000 |
[32m[20221213 20:40:08 @agent_ppo2.py:185][0m |          -0.0223 |           0.0129 |           0.0000 |
[32m[20221213 20:40:08 @agent_ppo2.py:185][0m |          -0.0251 |           0.0129 |           0.0000 |
[32m[20221213 20:40:08 @agent_ppo2.py:185][0m |          -0.0257 |           0.0125 |           0.0000 |
[32m[20221213 20:40:08 @agent_ppo2.py:185][0m |          -0.0272 |           0.0125 |           0.0000 |
[32m[20221213 20:40:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:40:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.36
[32m[20221213 20:40:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.69
[32m[20221213 20:40:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.29
[32m[20221213 20:40:08 @agent_ppo2.py:143][0m Total time:      10.99 min
[32m[20221213 20:40:08 @agent_ppo2.py:145][0m 1572864 total steps have happened
[32m[20221213 20:40:08 @agent_ppo2.py:121][0m #------------------------ Iteration 384 --------------------------#
[32m[20221213 20:40:09 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:40:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:09 @agent_ppo2.py:185][0m |           0.0274 |           0.0108 |           0.0000 |
[32m[20221213 20:40:09 @agent_ppo2.py:185][0m |           0.0029 |           0.0098 |           0.0000 |
[32m[20221213 20:40:09 @agent_ppo2.py:185][0m |          -0.0165 |           0.0095 |           0.0000 |
[32m[20221213 20:40:09 @agent_ppo2.py:185][0m |          -0.0453 |           0.0092 |           0.0000 |
[32m[20221213 20:40:09 @agent_ppo2.py:185][0m |          -0.0478 |           0.0091 |           0.0000 |
[32m[20221213 20:40:09 @agent_ppo2.py:185][0m |          -0.0516 |           0.0091 |           0.0000 |
[32m[20221213 20:40:09 @agent_ppo2.py:185][0m |          -0.0564 |           0.0089 |           0.0000 |
[32m[20221213 20:40:09 @agent_ppo2.py:185][0m |          -0.0676 |           0.0088 |           0.0000 |
[32m[20221213 20:40:10 @agent_ppo2.py:185][0m |          -0.0862 |           0.0088 |           0.0000 |
[32m[20221213 20:40:10 @agent_ppo2.py:185][0m |          -0.0662 |           0.0088 |           0.0000 |
[32m[20221213 20:40:10 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:40:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.03
[32m[20221213 20:40:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.90
[32m[20221213 20:40:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.84
[32m[20221213 20:40:10 @agent_ppo2.py:143][0m Total time:      11.01 min
[32m[20221213 20:40:10 @agent_ppo2.py:145][0m 1576960 total steps have happened
[32m[20221213 20:40:10 @agent_ppo2.py:121][0m #------------------------ Iteration 385 --------------------------#
[32m[20221213 20:40:10 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:11 @agent_ppo2.py:185][0m |           0.0288 |           0.0521 |           0.0000 |
[32m[20221213 20:40:11 @agent_ppo2.py:185][0m |           0.0031 |           0.0235 |           0.0000 |
[32m[20221213 20:40:11 @agent_ppo2.py:185][0m |          -0.0088 |           0.0184 |           0.0000 |
[32m[20221213 20:40:11 @agent_ppo2.py:185][0m |          -0.0182 |           0.0175 |           0.0000 |
[32m[20221213 20:40:11 @agent_ppo2.py:185][0m |          -0.0203 |           0.0165 |           0.0000 |
[32m[20221213 20:40:11 @agent_ppo2.py:185][0m |          -0.0256 |           0.0160 |           0.0000 |
[32m[20221213 20:40:11 @agent_ppo2.py:185][0m |          -0.0258 |           0.0160 |           0.0000 |
[32m[20221213 20:40:11 @agent_ppo2.py:185][0m |          -0.0289 |           0.0176 |           0.0000 |
[32m[20221213 20:40:11 @agent_ppo2.py:185][0m |          -0.0262 |           0.0161 |           0.0000 |
[32m[20221213 20:40:11 @agent_ppo2.py:185][0m |          -0.0304 |           0.0149 |           0.0000 |
[32m[20221213 20:40:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:40:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.45
[32m[20221213 20:40:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.88
[32m[20221213 20:40:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.62
[32m[20221213 20:40:12 @agent_ppo2.py:143][0m Total time:      11.04 min
[32m[20221213 20:40:12 @agent_ppo2.py:145][0m 1581056 total steps have happened
[32m[20221213 20:40:12 @agent_ppo2.py:121][0m #------------------------ Iteration 386 --------------------------#
[32m[20221213 20:40:12 @agent_ppo2.py:127][0m Sampling time: 0.45 s by 5 slaves
[32m[20221213 20:40:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:12 @agent_ppo2.py:185][0m |           0.0185 |           0.0399 |           0.0000 |
[32m[20221213 20:40:12 @agent_ppo2.py:185][0m |           0.0293 |           0.0340 |           0.0000 |
[32m[20221213 20:40:13 @agent_ppo2.py:185][0m |           0.0105 |           0.0275 |           0.0000 |
[32m[20221213 20:40:13 @agent_ppo2.py:185][0m |          -0.0019 |           0.0226 |           0.0000 |
[32m[20221213 20:40:13 @agent_ppo2.py:185][0m |          -0.0121 |           0.0223 |           0.0000 |
[32m[20221213 20:40:13 @agent_ppo2.py:185][0m |          -0.0231 |           0.0246 |           0.0000 |
[32m[20221213 20:40:13 @agent_ppo2.py:185][0m |          -0.0263 |           0.0205 |           0.0000 |
[32m[20221213 20:40:13 @agent_ppo2.py:185][0m |          -0.0313 |           0.0196 |           0.0000 |
[32m[20221213 20:40:13 @agent_ppo2.py:185][0m |          -0.0920 |           0.0234 |           0.0000 |
[32m[20221213 20:40:13 @agent_ppo2.py:185][0m |          -0.0348 |           0.0255 |           0.0000 |
[32m[20221213 20:40:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:40:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.70
[32m[20221213 20:40:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.40
[32m[20221213 20:40:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.43
[32m[20221213 20:40:14 @agent_ppo2.py:143][0m Total time:      11.07 min
[32m[20221213 20:40:14 @agent_ppo2.py:145][0m 1585152 total steps have happened
[32m[20221213 20:40:14 @agent_ppo2.py:121][0m #------------------------ Iteration 387 --------------------------#
[32m[20221213 20:40:14 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:40:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:14 @agent_ppo2.py:185][0m |           0.0332 |           0.0239 |           0.0000 |
[32m[20221213 20:40:14 @agent_ppo2.py:185][0m |          -0.0138 |           0.0222 |           0.0000 |
[32m[20221213 20:40:14 @agent_ppo2.py:185][0m |          -0.0443 |           0.0212 |           0.0000 |
[32m[20221213 20:40:15 @agent_ppo2.py:185][0m |          -0.0646 |           0.0205 |           0.0000 |
[32m[20221213 20:40:15 @agent_ppo2.py:185][0m |          -0.0766 |           0.0198 |           0.0000 |
[32m[20221213 20:40:15 @agent_ppo2.py:185][0m |          -0.0871 |           0.0198 |           0.0000 |
[32m[20221213 20:40:15 @agent_ppo2.py:185][0m |          -0.0988 |           0.0194 |           0.0000 |
[32m[20221213 20:40:15 @agent_ppo2.py:185][0m |          -0.0761 |           0.0193 |           0.0000 |
[32m[20221213 20:40:15 @agent_ppo2.py:185][0m |          -0.0906 |           0.0188 |           0.0000 |
[32m[20221213 20:40:15 @agent_ppo2.py:185][0m |          -0.0953 |           0.0181 |           0.0000 |
[32m[20221213 20:40:15 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 20:40:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.11
[32m[20221213 20:40:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.75
[32m[20221213 20:40:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.44
[32m[20221213 20:40:15 @agent_ppo2.py:143][0m Total time:      11.10 min
[32m[20221213 20:40:15 @agent_ppo2.py:145][0m 1589248 total steps have happened
[32m[20221213 20:40:15 @agent_ppo2.py:121][0m #------------------------ Iteration 388 --------------------------#
[32m[20221213 20:40:16 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:16 @agent_ppo2.py:185][0m |           0.0514 |           0.0550 |           0.0000 |
[32m[20221213 20:40:16 @agent_ppo2.py:185][0m |           0.0187 |           0.0100 |           0.0000 |
[32m[20221213 20:40:16 @agent_ppo2.py:185][0m |           0.0280 |           0.0096 |           0.0000 |
[32m[20221213 20:40:16 @agent_ppo2.py:185][0m |          -0.0018 |           0.0094 |           0.0000 |
[32m[20221213 20:40:16 @agent_ppo2.py:185][0m |          -0.0089 |           0.0093 |           0.0000 |
[32m[20221213 20:40:16 @agent_ppo2.py:185][0m |          -0.0159 |           0.0092 |           0.0000 |
[32m[20221213 20:40:17 @agent_ppo2.py:185][0m |          -0.0194 |           0.0091 |           0.0000 |
[32m[20221213 20:40:17 @agent_ppo2.py:185][0m |          -0.0236 |           0.0091 |           0.0000 |
[32m[20221213 20:40:17 @agent_ppo2.py:185][0m |          -0.0252 |           0.0090 |           0.0000 |
[32m[20221213 20:40:17 @agent_ppo2.py:185][0m |          -0.0294 |           0.0089 |           0.0000 |
[32m[20221213 20:40:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:40:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.53
[32m[20221213 20:40:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.77
[32m[20221213 20:40:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.56
[32m[20221213 20:40:17 @agent_ppo2.py:143][0m Total time:      11.13 min
[32m[20221213 20:40:17 @agent_ppo2.py:145][0m 1593344 total steps have happened
[32m[20221213 20:40:17 @agent_ppo2.py:121][0m #------------------------ Iteration 389 --------------------------#
[32m[20221213 20:40:18 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:40:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:18 @agent_ppo2.py:185][0m |           0.0064 |           0.0115 |           0.0000 |
[32m[20221213 20:40:18 @agent_ppo2.py:185][0m |          -0.0094 |           0.0081 |           0.0000 |
[32m[20221213 20:40:18 @agent_ppo2.py:185][0m |          -0.0256 |           0.0078 |           0.0000 |
[32m[20221213 20:40:18 @agent_ppo2.py:185][0m |          -0.0279 |           0.0076 |           0.0000 |
[32m[20221213 20:40:18 @agent_ppo2.py:185][0m |          -0.0333 |           0.0075 |           0.0000 |
[32m[20221213 20:40:18 @agent_ppo2.py:185][0m |          -0.0380 |           0.0075 |           0.0000 |
[32m[20221213 20:40:18 @agent_ppo2.py:185][0m |          -0.0069 |           0.0074 |           0.0000 |
[32m[20221213 20:40:18 @agent_ppo2.py:185][0m |          -0.0419 |           0.0073 |           0.0000 |
[32m[20221213 20:40:18 @agent_ppo2.py:185][0m |          -0.0463 |           0.0072 |           0.0000 |
[32m[20221213 20:40:18 @agent_ppo2.py:185][0m |          -0.0432 |           0.0072 |           0.0000 |
[32m[20221213 20:40:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:40:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.16
[32m[20221213 20:40:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.32
[32m[20221213 20:40:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.72
[32m[20221213 20:40:19 @agent_ppo2.py:143][0m Total time:      11.16 min
[32m[20221213 20:40:19 @agent_ppo2.py:145][0m 1597440 total steps have happened
[32m[20221213 20:40:19 @agent_ppo2.py:121][0m #------------------------ Iteration 390 --------------------------#
[32m[20221213 20:40:19 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:40:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:19 @agent_ppo2.py:185][0m |           0.0323 |           0.0102 |           0.0000 |
[32m[20221213 20:40:19 @agent_ppo2.py:185][0m |           0.0058 |           0.0079 |           0.0000 |
[32m[20221213 20:40:20 @agent_ppo2.py:185][0m |           0.0160 |           0.0076 |           0.0000 |
[32m[20221213 20:40:20 @agent_ppo2.py:185][0m |          -0.0198 |           0.0076 |           0.0000 |
[32m[20221213 20:40:20 @agent_ppo2.py:185][0m |           0.0011 |           0.0075 |           0.0000 |
[32m[20221213 20:40:20 @agent_ppo2.py:185][0m |          -0.0060 |           0.0074 |           0.0000 |
[32m[20221213 20:40:20 @agent_ppo2.py:185][0m |          -0.0070 |           0.0074 |           0.0000 |
[32m[20221213 20:40:20 @agent_ppo2.py:185][0m |          -0.0123 |           0.0073 |           0.0000 |
[32m[20221213 20:40:20 @agent_ppo2.py:185][0m |          -0.0151 |           0.0072 |           0.0000 |
[32m[20221213 20:40:20 @agent_ppo2.py:185][0m |          -0.0146 |           0.0072 |           0.0000 |
[32m[20221213 20:40:20 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:40:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.23
[32m[20221213 20:40:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.43
[32m[20221213 20:40:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.31
[32m[20221213 20:40:20 @agent_ppo2.py:143][0m Total time:      11.19 min
[32m[20221213 20:40:20 @agent_ppo2.py:145][0m 1601536 total steps have happened
[32m[20221213 20:40:20 @agent_ppo2.py:121][0m #------------------------ Iteration 391 --------------------------#
[32m[20221213 20:40:21 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:21 @agent_ppo2.py:185][0m |           0.0243 |           0.0108 |           0.0000 |
[32m[20221213 20:40:21 @agent_ppo2.py:185][0m |          -0.0083 |           0.0098 |           0.0000 |
[32m[20221213 20:40:21 @agent_ppo2.py:185][0m |          -0.0203 |           0.0097 |           0.0000 |
[32m[20221213 20:40:21 @agent_ppo2.py:185][0m |          -0.0248 |           0.0096 |           0.0000 |
[32m[20221213 20:40:21 @agent_ppo2.py:185][0m |          -0.0249 |           0.0095 |           0.0000 |
[32m[20221213 20:40:21 @agent_ppo2.py:185][0m |          -0.0284 |           0.0094 |           0.0000 |
[32m[20221213 20:40:22 @agent_ppo2.py:185][0m |          -0.0300 |           0.0093 |           0.0000 |
[32m[20221213 20:40:22 @agent_ppo2.py:185][0m |          -0.0344 |           0.0092 |           0.0000 |
[32m[20221213 20:40:22 @agent_ppo2.py:185][0m |          -0.0350 |           0.0092 |           0.0000 |
[32m[20221213 20:40:22 @agent_ppo2.py:185][0m |          -0.0357 |           0.0091 |           0.0000 |
[32m[20221213 20:40:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:40:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.71
[32m[20221213 20:40:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.72
[32m[20221213 20:40:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.62
[32m[20221213 20:40:22 @agent_ppo2.py:143][0m Total time:      11.22 min
[32m[20221213 20:40:22 @agent_ppo2.py:145][0m 1605632 total steps have happened
[32m[20221213 20:40:22 @agent_ppo2.py:121][0m #------------------------ Iteration 392 --------------------------#
[32m[20221213 20:40:22 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:23 @agent_ppo2.py:185][0m |           0.0260 |           0.0401 |           0.0000 |
[32m[20221213 20:40:23 @agent_ppo2.py:185][0m |          -0.0309 |           0.0181 |           0.0000 |
[32m[20221213 20:40:23 @agent_ppo2.py:185][0m |          -0.0177 |           0.0159 |           0.0000 |
[32m[20221213 20:40:23 @agent_ppo2.py:185][0m |           0.0288 |           0.0150 |           0.0000 |
[32m[20221213 20:40:23 @agent_ppo2.py:185][0m |           0.0091 |           0.0154 |           0.0000 |
[32m[20221213 20:40:23 @agent_ppo2.py:185][0m |          -0.0015 |           0.0143 |           0.0000 |
[32m[20221213 20:40:23 @agent_ppo2.py:185][0m |          -0.0060 |           0.0142 |           0.0000 |
[32m[20221213 20:40:23 @agent_ppo2.py:185][0m |          -0.0094 |           0.0139 |           0.0000 |
[32m[20221213 20:40:23 @agent_ppo2.py:185][0m |          -0.0141 |           0.0135 |           0.0000 |
[32m[20221213 20:40:23 @agent_ppo2.py:185][0m |          -0.0139 |           0.0137 |           0.0000 |
[32m[20221213 20:40:23 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:40:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.18
[32m[20221213 20:40:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.72
[32m[20221213 20:40:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.52
[32m[20221213 20:40:24 @agent_ppo2.py:143][0m Total time:      11.24 min
[32m[20221213 20:40:24 @agent_ppo2.py:145][0m 1609728 total steps have happened
[32m[20221213 20:40:24 @agent_ppo2.py:121][0m #------------------------ Iteration 393 --------------------------#
[32m[20221213 20:40:24 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:24 @agent_ppo2.py:185][0m |           0.0433 |           0.0169 |           0.0000 |
[32m[20221213 20:40:24 @agent_ppo2.py:185][0m |           0.0297 |           0.0106 |           0.0000 |
[32m[20221213 20:40:24 @agent_ppo2.py:185][0m |           0.0111 |           0.0103 |           0.0000 |
[32m[20221213 20:40:25 @agent_ppo2.py:185][0m |          -0.0034 |           0.0102 |           0.0000 |
[32m[20221213 20:40:25 @agent_ppo2.py:185][0m |          -0.0154 |           0.0102 |           0.0000 |
[32m[20221213 20:40:25 @agent_ppo2.py:185][0m |          -0.0210 |           0.0101 |           0.0000 |
[32m[20221213 20:40:25 @agent_ppo2.py:185][0m |          -0.0209 |           0.0100 |           0.0000 |
[32m[20221213 20:40:25 @agent_ppo2.py:185][0m |          -0.0046 |           0.0102 |           0.0000 |
[32m[20221213 20:40:25 @agent_ppo2.py:185][0m |          -0.0372 |           0.0100 |           0.0000 |
[32m[20221213 20:40:25 @agent_ppo2.py:185][0m |          -0.0388 |           0.0097 |           0.0000 |
[32m[20221213 20:40:25 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:40:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.73
[32m[20221213 20:40:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.76
[32m[20221213 20:40:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.59
[32m[20221213 20:40:25 @agent_ppo2.py:143][0m Total time:      11.27 min
[32m[20221213 20:40:25 @agent_ppo2.py:145][0m 1613824 total steps have happened
[32m[20221213 20:40:25 @agent_ppo2.py:121][0m #------------------------ Iteration 394 --------------------------#
[32m[20221213 20:40:26 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:26 @agent_ppo2.py:185][0m |           0.0387 |           0.0146 |           0.0000 |
[32m[20221213 20:40:26 @agent_ppo2.py:185][0m |           0.0224 |           0.0074 |           0.0000 |
[32m[20221213 20:40:26 @agent_ppo2.py:185][0m |           0.0041 |           0.0067 |           0.0000 |
[32m[20221213 20:40:26 @agent_ppo2.py:185][0m |           0.0034 |           0.0064 |           0.0000 |
[32m[20221213 20:40:26 @agent_ppo2.py:185][0m |          -0.0107 |           0.0063 |           0.0000 |
[32m[20221213 20:40:26 @agent_ppo2.py:185][0m |          -0.0162 |           0.0062 |           0.0000 |
[32m[20221213 20:40:26 @agent_ppo2.py:185][0m |          -0.0227 |           0.0061 |           0.0000 |
[32m[20221213 20:40:26 @agent_ppo2.py:185][0m |          -0.0216 |           0.0060 |           0.0000 |
[32m[20221213 20:40:27 @agent_ppo2.py:185][0m |          -0.0282 |           0.0059 |           0.0000 |
[32m[20221213 20:40:27 @agent_ppo2.py:185][0m |          -0.0290 |           0.0058 |           0.0000 |
[32m[20221213 20:40:27 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:40:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.12
[32m[20221213 20:40:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.16
[32m[20221213 20:40:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.55
[32m[20221213 20:40:27 @agent_ppo2.py:143][0m Total time:      11.30 min
[32m[20221213 20:40:27 @agent_ppo2.py:145][0m 1617920 total steps have happened
[32m[20221213 20:40:27 @agent_ppo2.py:121][0m #------------------------ Iteration 395 --------------------------#
[32m[20221213 20:40:27 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:28 @agent_ppo2.py:185][0m |           0.0283 |           0.0716 |           0.0000 |
[32m[20221213 20:40:28 @agent_ppo2.py:185][0m |          -0.0326 |           0.0259 |           0.0000 |
[32m[20221213 20:40:28 @agent_ppo2.py:185][0m |           0.0077 |           0.0222 |           0.0000 |
[32m[20221213 20:40:28 @agent_ppo2.py:185][0m |           0.0032 |           0.0221 |           0.0000 |
[32m[20221213 20:40:28 @agent_ppo2.py:185][0m |          -0.0107 |           0.0199 |           0.0000 |
[32m[20221213 20:40:28 @agent_ppo2.py:185][0m |          -0.0123 |           0.0185 |           0.0000 |
[32m[20221213 20:40:28 @agent_ppo2.py:185][0m |          -0.0201 |           0.0181 |           0.0000 |
[32m[20221213 20:40:28 @agent_ppo2.py:185][0m |          -0.0227 |           0.0183 |           0.0000 |
[32m[20221213 20:40:28 @agent_ppo2.py:185][0m |          -0.0246 |           0.0178 |           0.0000 |
[32m[20221213 20:40:28 @agent_ppo2.py:185][0m |          -0.0267 |           0.0175 |           0.0000 |
[32m[20221213 20:40:28 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:40:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.92
[32m[20221213 20:40:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.21
[32m[20221213 20:40:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.87
[32m[20221213 20:40:29 @agent_ppo2.py:143][0m Total time:      11.32 min
[32m[20221213 20:40:29 @agent_ppo2.py:145][0m 1622016 total steps have happened
[32m[20221213 20:40:29 @agent_ppo2.py:121][0m #------------------------ Iteration 396 --------------------------#
[32m[20221213 20:40:29 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:29 @agent_ppo2.py:185][0m |           0.0842 |           0.0241 |           0.0000 |
[32m[20221213 20:40:29 @agent_ppo2.py:185][0m |           0.0123 |           0.0087 |           0.0000 |
[32m[20221213 20:40:29 @agent_ppo2.py:185][0m |          -0.0035 |           0.0075 |           0.0000 |
[32m[20221213 20:40:29 @agent_ppo2.py:185][0m |          -0.0123 |           0.0072 |           0.0000 |
[32m[20221213 20:40:30 @agent_ppo2.py:185][0m |          -0.0157 |           0.0071 |           0.0000 |
[32m[20221213 20:40:30 @agent_ppo2.py:185][0m |          -0.0055 |           0.0071 |           0.0000 |
[32m[20221213 20:40:30 @agent_ppo2.py:185][0m |          -0.0289 |           0.0070 |           0.0000 |
[32m[20221213 20:40:30 @agent_ppo2.py:185][0m |           0.0099 |           0.0069 |           0.0000 |
[32m[20221213 20:40:30 @agent_ppo2.py:185][0m |          -0.0356 |           0.0067 |           0.0000 |
[32m[20221213 20:40:30 @agent_ppo2.py:185][0m |          -0.0381 |           0.0067 |           0.0000 |
[32m[20221213 20:40:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:40:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.41
[32m[20221213 20:40:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.48
[32m[20221213 20:40:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.65
[32m[20221213 20:40:30 @agent_ppo2.py:143][0m Total time:      11.35 min
[32m[20221213 20:40:30 @agent_ppo2.py:145][0m 1626112 total steps have happened
[32m[20221213 20:40:30 @agent_ppo2.py:121][0m #------------------------ Iteration 397 --------------------------#
[32m[20221213 20:40:31 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:40:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:31 @agent_ppo2.py:185][0m |           0.0287 |           0.0244 |           0.0000 |
[32m[20221213 20:40:31 @agent_ppo2.py:185][0m |           0.0054 |           0.0118 |           0.0000 |
[32m[20221213 20:40:31 @agent_ppo2.py:185][0m |          -0.0044 |           0.0104 |           0.0000 |
[32m[20221213 20:40:31 @agent_ppo2.py:185][0m |          -0.0107 |           0.0101 |           0.0000 |
[32m[20221213 20:40:31 @agent_ppo2.py:185][0m |          -0.0128 |           0.0099 |           0.0000 |
[32m[20221213 20:40:31 @agent_ppo2.py:185][0m |          -0.0152 |           0.0098 |           0.0000 |
[32m[20221213 20:40:31 @agent_ppo2.py:185][0m |          -0.0185 |           0.0098 |           0.0000 |
[32m[20221213 20:40:31 @agent_ppo2.py:185][0m |          -0.0200 |           0.0098 |           0.0000 |
[32m[20221213 20:40:32 @agent_ppo2.py:185][0m |          -0.0210 |           0.0096 |           0.0000 |
[32m[20221213 20:40:32 @agent_ppo2.py:185][0m |          -0.0231 |           0.0096 |           0.0000 |
[32m[20221213 20:40:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:40:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.70
[32m[20221213 20:40:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.28
[32m[20221213 20:40:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.90
[32m[20221213 20:40:32 @agent_ppo2.py:143][0m Total time:      11.38 min
[32m[20221213 20:40:32 @agent_ppo2.py:145][0m 1630208 total steps have happened
[32m[20221213 20:40:32 @agent_ppo2.py:121][0m #------------------------ Iteration 398 --------------------------#
[32m[20221213 20:40:32 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:40:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:33 @agent_ppo2.py:185][0m |           0.0426 |           0.0105 |           0.0000 |
[32m[20221213 20:40:33 @agent_ppo2.py:185][0m |           0.0130 |           0.0077 |           0.0000 |
[32m[20221213 20:40:33 @agent_ppo2.py:185][0m |          -0.0024 |           0.0075 |           0.0000 |
[32m[20221213 20:40:33 @agent_ppo2.py:185][0m |          -0.0179 |           0.0074 |           0.0000 |
[32m[20221213 20:40:33 @agent_ppo2.py:185][0m |          -0.0198 |           0.0073 |           0.0000 |
[32m[20221213 20:40:33 @agent_ppo2.py:185][0m |          -0.0389 |           0.0072 |           0.0000 |
[32m[20221213 20:40:33 @agent_ppo2.py:185][0m |          -0.0433 |           0.0071 |           0.0000 |
[32m[20221213 20:40:33 @agent_ppo2.py:185][0m |          -0.0424 |           0.0070 |           0.0000 |
[32m[20221213 20:40:33 @agent_ppo2.py:185][0m |          -0.0515 |           0.0069 |           0.0000 |
[32m[20221213 20:40:33 @agent_ppo2.py:185][0m |          -0.0519 |           0.0069 |           0.0000 |
[32m[20221213 20:40:33 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 20:40:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.74
[32m[20221213 20:40:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.90
[32m[20221213 20:40:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.15
[32m[20221213 20:40:34 @agent_ppo2.py:143][0m Total time:      11.41 min
[32m[20221213 20:40:34 @agent_ppo2.py:145][0m 1634304 total steps have happened
[32m[20221213 20:40:34 @agent_ppo2.py:121][0m #------------------------ Iteration 399 --------------------------#
[32m[20221213 20:40:34 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:40:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:34 @agent_ppo2.py:185][0m |           0.0254 |           0.0467 |           0.0000 |
[32m[20221213 20:40:35 @agent_ppo2.py:185][0m |          -0.0049 |           0.0194 |           0.0000 |
[32m[20221213 20:40:35 @agent_ppo2.py:185][0m |          -0.0168 |           0.0171 |           0.0000 |
[32m[20221213 20:40:35 @agent_ppo2.py:185][0m |          -0.0183 |           0.0167 |           0.0000 |
[32m[20221213 20:40:35 @agent_ppo2.py:185][0m |          -0.0204 |           0.0155 |           0.0000 |
[32m[20221213 20:40:35 @agent_ppo2.py:185][0m |          -0.0190 |           0.0152 |           0.0000 |
[32m[20221213 20:40:35 @agent_ppo2.py:185][0m |          -0.0214 |           0.0149 |           0.0000 |
[32m[20221213 20:40:35 @agent_ppo2.py:185][0m |          -0.0249 |           0.0148 |           0.0000 |
[32m[20221213 20:40:35 @agent_ppo2.py:185][0m |          -0.0233 |           0.0145 |           0.0000 |
[32m[20221213 20:40:35 @agent_ppo2.py:185][0m |          -0.0276 |           0.0142 |           0.0000 |
[32m[20221213 20:40:35 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:40:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.11
[32m[20221213 20:40:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.25
[32m[20221213 20:40:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.82
[32m[20221213 20:40:36 @agent_ppo2.py:143][0m Total time:      11.44 min
[32m[20221213 20:40:36 @agent_ppo2.py:145][0m 1638400 total steps have happened
[32m[20221213 20:40:36 @agent_ppo2.py:121][0m #------------------------ Iteration 400 --------------------------#
[32m[20221213 20:40:36 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:40:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:36 @agent_ppo2.py:185][0m |           0.0327 |           0.0149 |           0.0000 |
[32m[20221213 20:40:36 @agent_ppo2.py:185][0m |           0.0013 |           0.0120 |           0.0000 |
[32m[20221213 20:40:36 @agent_ppo2.py:185][0m |          -0.0237 |           0.0116 |           0.0000 |
[32m[20221213 20:40:37 @agent_ppo2.py:185][0m |          -0.0380 |           0.0114 |           0.0000 |
[32m[20221213 20:40:37 @agent_ppo2.py:185][0m |          -0.0470 |           0.0112 |           0.0000 |
[32m[20221213 20:40:37 @agent_ppo2.py:185][0m |          -0.0478 |           0.0111 |           0.0000 |
[32m[20221213 20:40:37 @agent_ppo2.py:185][0m |          -0.0589 |           0.0109 |           0.0000 |
[32m[20221213 20:40:37 @agent_ppo2.py:185][0m |          -0.0640 |           0.0115 |           0.0000 |
[32m[20221213 20:40:37 @agent_ppo2.py:185][0m |          -0.0602 |           0.0116 |           0.0000 |
[32m[20221213 20:40:37 @agent_ppo2.py:185][0m |          -0.0653 |           0.0107 |           0.0000 |
[32m[20221213 20:40:37 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:40:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.44
[32m[20221213 20:40:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.13
[32m[20221213 20:40:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.82
[32m[20221213 20:40:37 @agent_ppo2.py:143][0m Total time:      11.47 min
[32m[20221213 20:40:37 @agent_ppo2.py:145][0m 1642496 total steps have happened
[32m[20221213 20:40:37 @agent_ppo2.py:121][0m #------------------------ Iteration 401 --------------------------#
[32m[20221213 20:40:38 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:40:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:38 @agent_ppo2.py:185][0m |           0.0394 |           0.0120 |           0.0000 |
[32m[20221213 20:40:38 @agent_ppo2.py:185][0m |          -0.0120 |           0.0115 |           0.0000 |
[32m[20221213 20:40:38 @agent_ppo2.py:185][0m |          -0.0260 |           0.0112 |           0.0000 |
[32m[20221213 20:40:38 @agent_ppo2.py:185][0m |          -0.0401 |           0.0108 |           0.0000 |
[32m[20221213 20:40:38 @agent_ppo2.py:185][0m |          -0.0508 |           0.0106 |           0.0000 |
[32m[20221213 20:40:38 @agent_ppo2.py:185][0m |          -0.0567 |           0.0105 |           0.0000 |
[32m[20221213 20:40:39 @agent_ppo2.py:185][0m |          -0.0649 |           0.0104 |           0.0000 |
[32m[20221213 20:40:39 @agent_ppo2.py:185][0m |          -0.0674 |           0.0103 |           0.0000 |
[32m[20221213 20:40:39 @agent_ppo2.py:185][0m |          -0.0667 |           0.0102 |           0.0000 |
[32m[20221213 20:40:39 @agent_ppo2.py:185][0m |          -0.0696 |           0.0102 |           0.0000 |
[32m[20221213 20:40:39 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:40:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.87
[32m[20221213 20:40:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.05
[32m[20221213 20:40:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.37
[32m[20221213 20:40:39 @agent_ppo2.py:143][0m Total time:      11.50 min
[32m[20221213 20:40:39 @agent_ppo2.py:145][0m 1646592 total steps have happened
[32m[20221213 20:40:39 @agent_ppo2.py:121][0m #------------------------ Iteration 402 --------------------------#
[32m[20221213 20:40:40 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:40:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:40 @agent_ppo2.py:185][0m |           0.0243 |           0.0263 |           0.0000 |
[32m[20221213 20:40:40 @agent_ppo2.py:185][0m |          -0.0028 |           0.0189 |           0.0000 |
[32m[20221213 20:40:40 @agent_ppo2.py:185][0m |          -0.0122 |           0.0174 |           0.0000 |
[32m[20221213 20:40:40 @agent_ppo2.py:185][0m |          -0.0147 |           0.0170 |           0.0000 |
[32m[20221213 20:40:40 @agent_ppo2.py:185][0m |          -0.0235 |           0.0161 |           0.0000 |
[32m[20221213 20:40:40 @agent_ppo2.py:185][0m |          -0.0269 |           0.0158 |           0.0000 |
[32m[20221213 20:40:40 @agent_ppo2.py:185][0m |          -0.0263 |           0.0168 |           0.0000 |
[32m[20221213 20:40:40 @agent_ppo2.py:185][0m |          -0.0283 |           0.0155 |           0.0000 |
[32m[20221213 20:40:40 @agent_ppo2.py:185][0m |          -0.0320 |           0.0152 |           0.0000 |
[32m[20221213 20:40:40 @agent_ppo2.py:185][0m |          -0.0334 |           0.0151 |           0.0000 |
[32m[20221213 20:40:40 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:40:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.31
[32m[20221213 20:40:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.50
[32m[20221213 20:40:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.02
[32m[20221213 20:40:41 @agent_ppo2.py:143][0m Total time:      11.53 min
[32m[20221213 20:40:41 @agent_ppo2.py:145][0m 1650688 total steps have happened
[32m[20221213 20:40:41 @agent_ppo2.py:121][0m #------------------------ Iteration 403 --------------------------#
[32m[20221213 20:40:41 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:41 @agent_ppo2.py:185][0m |           0.0246 |           0.0377 |           0.0000 |
[32m[20221213 20:40:41 @agent_ppo2.py:185][0m |           0.0385 |           0.0278 |           0.0000 |
[32m[20221213 20:40:42 @agent_ppo2.py:185][0m |           0.0183 |           0.0255 |           0.0000 |
[32m[20221213 20:40:42 @agent_ppo2.py:185][0m |           0.0028 |           0.0224 |           0.0000 |
[32m[20221213 20:40:42 @agent_ppo2.py:185][0m |          -0.0013 |           0.0214 |           0.0000 |
[32m[20221213 20:40:42 @agent_ppo2.py:185][0m |          -0.0120 |           0.0208 |           0.0000 |
[32m[20221213 20:40:42 @agent_ppo2.py:185][0m |          -0.0151 |           0.0207 |           0.0000 |
[32m[20221213 20:40:42 @agent_ppo2.py:185][0m |          -0.0132 |           0.0199 |           0.0000 |
[32m[20221213 20:40:42 @agent_ppo2.py:185][0m |          -0.0194 |           0.0196 |           0.0000 |
[32m[20221213 20:40:42 @agent_ppo2.py:185][0m |          -0.0272 |           0.0193 |           0.0000 |
[32m[20221213 20:40:42 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:40:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.05
[32m[20221213 20:40:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.47
[32m[20221213 20:40:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.42
[32m[20221213 20:40:42 @agent_ppo2.py:143][0m Total time:      11.56 min
[32m[20221213 20:40:42 @agent_ppo2.py:145][0m 1654784 total steps have happened
[32m[20221213 20:40:43 @agent_ppo2.py:121][0m #------------------------ Iteration 404 --------------------------#
[32m[20221213 20:40:43 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:43 @agent_ppo2.py:185][0m |           0.0341 |           0.0495 |           0.0000 |
[32m[20221213 20:40:43 @agent_ppo2.py:185][0m |          -0.0058 |           0.0381 |           0.0000 |
[32m[20221213 20:40:43 @agent_ppo2.py:185][0m |          -0.0240 |           0.0335 |           0.0000 |
[32m[20221213 20:40:43 @agent_ppo2.py:185][0m |          -0.0321 |           0.0321 |           0.0000 |
[32m[20221213 20:40:43 @agent_ppo2.py:185][0m |          -0.0332 |           0.0318 |           0.0000 |
[32m[20221213 20:40:43 @agent_ppo2.py:185][0m |          -0.0482 |           0.0361 |           0.0000 |
[32m[20221213 20:40:44 @agent_ppo2.py:185][0m |          -0.0604 |           0.0302 |           0.0000 |
[32m[20221213 20:40:44 @agent_ppo2.py:185][0m |          -0.0487 |           0.0300 |           0.0000 |
[32m[20221213 20:40:44 @agent_ppo2.py:185][0m |          -0.0556 |           0.0274 |           0.0000 |
[32m[20221213 20:40:44 @agent_ppo2.py:185][0m |          -0.0562 |           0.0259 |           0.0000 |
[32m[20221213 20:40:44 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:40:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.17
[32m[20221213 20:40:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.58
[32m[20221213 20:40:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.53
[32m[20221213 20:40:44 @agent_ppo2.py:143][0m Total time:      11.58 min
[32m[20221213 20:40:44 @agent_ppo2.py:145][0m 1658880 total steps have happened
[32m[20221213 20:40:44 @agent_ppo2.py:121][0m #------------------------ Iteration 405 --------------------------#
[32m[20221213 20:40:45 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:45 @agent_ppo2.py:185][0m |           0.0120 |           0.0513 |           0.0000 |
[32m[20221213 20:40:45 @agent_ppo2.py:185][0m |          -0.0036 |           0.0141 |           0.0000 |
[32m[20221213 20:40:45 @agent_ppo2.py:185][0m |          -0.0028 |           0.0133 |           0.0000 |
[32m[20221213 20:40:45 @agent_ppo2.py:185][0m |          -0.0314 |           0.0130 |           0.0000 |
[32m[20221213 20:40:45 @agent_ppo2.py:185][0m |          -0.0361 |           0.0129 |           0.0000 |
[32m[20221213 20:40:45 @agent_ppo2.py:185][0m |          -0.0427 |           0.0127 |           0.0000 |
[32m[20221213 20:40:45 @agent_ppo2.py:185][0m |          -0.0449 |           0.0125 |           0.0000 |
[32m[20221213 20:40:45 @agent_ppo2.py:185][0m |          -0.0434 |           0.0124 |           0.0000 |
[32m[20221213 20:40:45 @agent_ppo2.py:185][0m |          -0.0400 |           0.0124 |           0.0000 |
[32m[20221213 20:40:45 @agent_ppo2.py:185][0m |          -0.0499 |           0.0123 |           0.0000 |
[32m[20221213 20:40:45 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:40:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.92
[32m[20221213 20:40:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.33
[32m[20221213 20:40:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.78
[32m[20221213 20:40:46 @agent_ppo2.py:143][0m Total time:      11.61 min
[32m[20221213 20:40:46 @agent_ppo2.py:145][0m 1662976 total steps have happened
[32m[20221213 20:40:46 @agent_ppo2.py:121][0m #------------------------ Iteration 406 --------------------------#
[32m[20221213 20:40:46 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:46 @agent_ppo2.py:185][0m |           0.0258 |           0.0201 |           0.0000 |
[32m[20221213 20:40:46 @agent_ppo2.py:185][0m |           0.0032 |           0.0168 |           0.0000 |
[32m[20221213 20:40:47 @agent_ppo2.py:185][0m |          -0.0244 |           0.0208 |           0.0000 |
[32m[20221213 20:40:47 @agent_ppo2.py:185][0m |          -0.0291 |           0.0203 |           0.0000 |
[32m[20221213 20:40:47 @agent_ppo2.py:185][0m |          -0.0365 |           0.0161 |           0.0000 |
[32m[20221213 20:40:47 @agent_ppo2.py:185][0m |          -0.0359 |           0.0154 |           0.0000 |
[32m[20221213 20:40:47 @agent_ppo2.py:185][0m |          -0.0451 |           0.0149 |           0.0000 |
[32m[20221213 20:40:47 @agent_ppo2.py:185][0m |          -0.0457 |           0.0147 |           0.0000 |
[32m[20221213 20:40:47 @agent_ppo2.py:185][0m |          -0.0436 |           0.0144 |           0.0000 |
[32m[20221213 20:40:47 @agent_ppo2.py:185][0m |          -0.0455 |           0.0142 |           0.0000 |
[32m[20221213 20:40:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:40:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.70
[32m[20221213 20:40:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.37
[32m[20221213 20:40:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.65
[32m[20221213 20:40:48 @agent_ppo2.py:143][0m Total time:      11.64 min
[32m[20221213 20:40:48 @agent_ppo2.py:145][0m 1667072 total steps have happened
[32m[20221213 20:40:48 @agent_ppo2.py:121][0m #------------------------ Iteration 407 --------------------------#
[32m[20221213 20:40:48 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:40:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:48 @agent_ppo2.py:185][0m |           0.0278 |           0.0119 |           0.0000 |
[32m[20221213 20:40:48 @agent_ppo2.py:185][0m |          -0.0101 |           0.0103 |           0.0000 |
[32m[20221213 20:40:48 @agent_ppo2.py:185][0m |          -0.0469 |           0.0101 |           0.0000 |
[32m[20221213 20:40:48 @agent_ppo2.py:185][0m |          -0.0473 |           0.0100 |           0.0000 |
[32m[20221213 20:40:49 @agent_ppo2.py:185][0m |          -0.0587 |           0.0099 |           0.0000 |
[32m[20221213 20:40:49 @agent_ppo2.py:185][0m |          -0.0555 |           0.0099 |           0.0000 |
[32m[20221213 20:40:49 @agent_ppo2.py:185][0m |          -0.0603 |           0.0098 |           0.0000 |
[32m[20221213 20:40:49 @agent_ppo2.py:185][0m |          -0.0656 |           0.0097 |           0.0000 |
[32m[20221213 20:40:49 @agent_ppo2.py:185][0m |          -0.0750 |           0.0097 |           0.0000 |
[32m[20221213 20:40:49 @agent_ppo2.py:185][0m |          -0.0724 |           0.0096 |           0.0000 |
[32m[20221213 20:40:49 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:40:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.00
[32m[20221213 20:40:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.22
[32m[20221213 20:40:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.22
[32m[20221213 20:40:49 @agent_ppo2.py:143][0m Total time:      11.67 min
[32m[20221213 20:40:49 @agent_ppo2.py:145][0m 1671168 total steps have happened
[32m[20221213 20:40:49 @agent_ppo2.py:121][0m #------------------------ Iteration 408 --------------------------#
[32m[20221213 20:40:50 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:50 @agent_ppo2.py:185][0m |           0.0220 |           0.0196 |           0.0000 |
[32m[20221213 20:40:50 @agent_ppo2.py:185][0m |           0.0069 |           0.0144 |           0.0000 |
[32m[20221213 20:40:50 @agent_ppo2.py:185][0m |          -0.0161 |           0.0141 |           0.0000 |
[32m[20221213 20:40:50 @agent_ppo2.py:185][0m |          -0.0235 |           0.0135 |           0.0000 |
[32m[20221213 20:40:50 @agent_ppo2.py:185][0m |          -0.0279 |           0.0134 |           0.0000 |
[32m[20221213 20:40:50 @agent_ppo2.py:185][0m |          -0.0333 |           0.0131 |           0.0000 |
[32m[20221213 20:40:50 @agent_ppo2.py:185][0m |          -0.0340 |           0.0129 |           0.0000 |
[32m[20221213 20:40:51 @agent_ppo2.py:185][0m |          -0.0342 |           0.0126 |           0.0000 |
[32m[20221213 20:40:51 @agent_ppo2.py:185][0m |          -0.0340 |           0.0127 |           0.0000 |
[32m[20221213 20:40:51 @agent_ppo2.py:185][0m |          -0.0367 |           0.0124 |           0.0000 |
[32m[20221213 20:40:51 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:40:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.81
[32m[20221213 20:40:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.50
[32m[20221213 20:40:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.99
[32m[20221213 20:40:51 @agent_ppo2.py:143][0m Total time:      11.70 min
[32m[20221213 20:40:51 @agent_ppo2.py:145][0m 1675264 total steps have happened
[32m[20221213 20:40:51 @agent_ppo2.py:121][0m #------------------------ Iteration 409 --------------------------#
[32m[20221213 20:40:51 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:40:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:52 @agent_ppo2.py:185][0m |           0.0968 |           0.0176 |           0.0000 |
[32m[20221213 20:40:52 @agent_ppo2.py:185][0m |           0.0266 |           0.0086 |           0.0000 |
[32m[20221213 20:40:52 @agent_ppo2.py:185][0m |           0.0456 |           0.0087 |           0.0000 |
[32m[20221213 20:40:52 @agent_ppo2.py:185][0m |           0.0073 |           0.0085 |           0.0000 |
[32m[20221213 20:40:52 @agent_ppo2.py:185][0m |           0.0014 |           0.0083 |           0.0000 |
[32m[20221213 20:40:52 @agent_ppo2.py:185][0m |          -0.0078 |           0.0082 |           0.0000 |
[32m[20221213 20:40:52 @agent_ppo2.py:185][0m |          -0.0120 |           0.0081 |           0.0000 |
[32m[20221213 20:40:52 @agent_ppo2.py:185][0m |          -0.0159 |           0.0081 |           0.0000 |
[32m[20221213 20:40:52 @agent_ppo2.py:185][0m |          -0.0217 |           0.0080 |           0.0000 |
[32m[20221213 20:40:52 @agent_ppo2.py:185][0m |          -0.0236 |           0.0079 |           0.0000 |
[32m[20221213 20:40:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:40:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.81
[32m[20221213 20:40:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.13
[32m[20221213 20:40:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.60
[32m[20221213 20:40:53 @agent_ppo2.py:143][0m Total time:      11.73 min
[32m[20221213 20:40:53 @agent_ppo2.py:145][0m 1679360 total steps have happened
[32m[20221213 20:40:53 @agent_ppo2.py:121][0m #------------------------ Iteration 410 --------------------------#
[32m[20221213 20:40:53 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:40:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:53 @agent_ppo2.py:185][0m |           0.0174 |           0.0280 |           0.0000 |
[32m[20221213 20:40:53 @agent_ppo2.py:185][0m |          -0.0479 |           0.0174 |           0.0000 |
[32m[20221213 20:40:54 @agent_ppo2.py:185][0m |          -0.0300 |           0.0171 |           0.0000 |
[32m[20221213 20:40:54 @agent_ppo2.py:185][0m |          -0.0085 |           0.0142 |           0.0000 |
[32m[20221213 20:40:54 @agent_ppo2.py:185][0m |          -0.0349 |           0.0143 |           0.0000 |
[32m[20221213 20:40:54 @agent_ppo2.py:185][0m |          -0.0223 |           0.0138 |           0.0000 |
[32m[20221213 20:40:54 @agent_ppo2.py:185][0m |          -0.0228 |           0.0136 |           0.0000 |
[32m[20221213 20:40:54 @agent_ppo2.py:185][0m |          -0.0246 |           0.0134 |           0.0000 |
[32m[20221213 20:40:54 @agent_ppo2.py:185][0m |          -0.0287 |           0.0133 |           0.0000 |
[32m[20221213 20:40:54 @agent_ppo2.py:185][0m |          -0.0282 |           0.0135 |           0.0000 |
[32m[20221213 20:40:54 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:40:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.86
[32m[20221213 20:40:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.16
[32m[20221213 20:40:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.57
[32m[20221213 20:40:54 @agent_ppo2.py:143][0m Total time:      11.75 min
[32m[20221213 20:40:54 @agent_ppo2.py:145][0m 1683456 total steps have happened
[32m[20221213 20:40:54 @agent_ppo2.py:121][0m #------------------------ Iteration 411 --------------------------#
[32m[20221213 20:40:55 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:40:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:55 @agent_ppo2.py:185][0m |           0.0385 |           0.0143 |           0.0000 |
[32m[20221213 20:40:55 @agent_ppo2.py:185][0m |           0.0102 |           0.0093 |           0.0000 |
[32m[20221213 20:40:55 @agent_ppo2.py:185][0m |          -0.0076 |           0.0090 |           0.0000 |
[32m[20221213 20:40:55 @agent_ppo2.py:185][0m |          -0.0262 |           0.0089 |           0.0000 |
[32m[20221213 20:40:55 @agent_ppo2.py:185][0m |          -0.0349 |           0.0088 |           0.0000 |
[32m[20221213 20:40:55 @agent_ppo2.py:185][0m |          -0.0440 |           0.0087 |           0.0000 |
[32m[20221213 20:40:56 @agent_ppo2.py:185][0m |          -0.0466 |           0.0087 |           0.0000 |
[32m[20221213 20:40:56 @agent_ppo2.py:185][0m |          -0.0294 |           0.0087 |           0.0000 |
[32m[20221213 20:40:56 @agent_ppo2.py:185][0m |          -0.0541 |           0.0086 |           0.0000 |
[32m[20221213 20:40:56 @agent_ppo2.py:185][0m |          -0.0621 |           0.0085 |           0.0000 |
[32m[20221213 20:40:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:40:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.64
[32m[20221213 20:40:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.90
[32m[20221213 20:40:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.78
[32m[20221213 20:40:56 @agent_ppo2.py:143][0m Total time:      11.79 min
[32m[20221213 20:40:56 @agent_ppo2.py:145][0m 1687552 total steps have happened
[32m[20221213 20:40:56 @agent_ppo2.py:121][0m #------------------------ Iteration 412 --------------------------#
[32m[20221213 20:40:57 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:40:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:57 @agent_ppo2.py:185][0m |           0.0176 |           0.0507 |           0.0000 |
[32m[20221213 20:40:57 @agent_ppo2.py:185][0m |           0.0027 |           0.0189 |           0.0000 |
[32m[20221213 20:40:57 @agent_ppo2.py:185][0m |          -0.0132 |           0.0172 |           0.0000 |
[32m[20221213 20:40:57 @agent_ppo2.py:185][0m |          -0.0201 |           0.0165 |           0.0000 |
[32m[20221213 20:40:57 @agent_ppo2.py:185][0m |          -0.0241 |           0.0161 |           0.0000 |
[32m[20221213 20:40:57 @agent_ppo2.py:185][0m |          -0.0262 |           0.0160 |           0.0000 |
[32m[20221213 20:40:57 @agent_ppo2.py:185][0m |          -0.0280 |           0.0157 |           0.0000 |
[32m[20221213 20:40:57 @agent_ppo2.py:185][0m |          -0.0302 |           0.0155 |           0.0000 |
[32m[20221213 20:40:58 @agent_ppo2.py:185][0m |          -0.0304 |           0.0153 |           0.0000 |
[32m[20221213 20:40:58 @agent_ppo2.py:185][0m |          -0.0314 |           0.0151 |           0.0000 |
[32m[20221213 20:40:58 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:40:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.07
[32m[20221213 20:40:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.56
[32m[20221213 20:40:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.25
[32m[20221213 20:40:58 @agent_ppo2.py:143][0m Total time:      11.81 min
[32m[20221213 20:40:58 @agent_ppo2.py:145][0m 1691648 total steps have happened
[32m[20221213 20:40:58 @agent_ppo2.py:121][0m #------------------------ Iteration 413 --------------------------#
[32m[20221213 20:40:58 @agent_ppo2.py:127][0m Sampling time: 0.43 s by 5 slaves
[32m[20221213 20:40:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:40:59 @agent_ppo2.py:185][0m |           0.0208 |           0.0273 |           0.0000 |
[32m[20221213 20:40:59 @agent_ppo2.py:185][0m |           0.0284 |           0.0168 |           0.0000 |
[32m[20221213 20:40:59 @agent_ppo2.py:185][0m |          -0.0031 |           0.0159 |           0.0000 |
[32m[20221213 20:40:59 @agent_ppo2.py:185][0m |          -0.0305 |           0.0131 |           0.0000 |
[32m[20221213 20:40:59 @agent_ppo2.py:185][0m |          -0.0367 |           0.0127 |           0.0000 |
[32m[20221213 20:40:59 @agent_ppo2.py:185][0m |          -0.0463 |           0.0125 |           0.0000 |
[32m[20221213 20:40:59 @agent_ppo2.py:185][0m |          -0.0449 |           0.0124 |           0.0000 |
[32m[20221213 20:40:59 @agent_ppo2.py:185][0m |          -0.0486 |           0.0122 |           0.0000 |
[32m[20221213 20:40:59 @agent_ppo2.py:185][0m |          -0.0529 |           0.0122 |           0.0000 |
[32m[20221213 20:41:00 @agent_ppo2.py:185][0m |          -0.0578 |           0.0122 |           0.0000 |
[32m[20221213 20:41:00 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 20:41:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.27
[32m[20221213 20:41:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.44
[32m[20221213 20:41:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.72
[32m[20221213 20:41:00 @agent_ppo2.py:143][0m Total time:      11.85 min
[32m[20221213 20:41:00 @agent_ppo2.py:145][0m 1695744 total steps have happened
[32m[20221213 20:41:00 @agent_ppo2.py:121][0m #------------------------ Iteration 414 --------------------------#
[32m[20221213 20:41:00 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:01 @agent_ppo2.py:185][0m |           0.0320 |           0.0526 |           0.0000 |
[32m[20221213 20:41:01 @agent_ppo2.py:185][0m |          -0.0167 |           0.0220 |           0.0000 |
[32m[20221213 20:41:01 @agent_ppo2.py:185][0m |          -0.0133 |           0.0206 |           0.0000 |
[32m[20221213 20:41:01 @agent_ppo2.py:185][0m |          -0.0185 |           0.0186 |           0.0000 |
[32m[20221213 20:41:01 @agent_ppo2.py:185][0m |          -0.0234 |           0.0185 |           0.0000 |
[32m[20221213 20:41:01 @agent_ppo2.py:185][0m |          -0.0269 |           0.0189 |           0.0000 |
[32m[20221213 20:41:01 @agent_ppo2.py:185][0m |          -0.0305 |           0.0175 |           0.0000 |
[32m[20221213 20:41:01 @agent_ppo2.py:185][0m |          -0.0315 |           0.0172 |           0.0000 |
[32m[20221213 20:41:01 @agent_ppo2.py:185][0m |          -0.0332 |           0.0171 |           0.0000 |
[32m[20221213 20:41:01 @agent_ppo2.py:185][0m |          -0.0342 |           0.0169 |           0.0000 |
[32m[20221213 20:41:01 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 20:41:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.12
[32m[20221213 20:41:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.55
[32m[20221213 20:41:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.43
[32m[20221213 20:41:02 @agent_ppo2.py:143][0m Total time:      11.88 min
[32m[20221213 20:41:02 @agent_ppo2.py:145][0m 1699840 total steps have happened
[32m[20221213 20:41:02 @agent_ppo2.py:121][0m #------------------------ Iteration 415 --------------------------#
[32m[20221213 20:41:02 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:02 @agent_ppo2.py:185][0m |           0.0292 |           0.0230 |           0.0000 |
[32m[20221213 20:41:02 @agent_ppo2.py:185][0m |          -0.0050 |           0.0123 |           0.0000 |
[32m[20221213 20:41:03 @agent_ppo2.py:185][0m |          -0.0200 |           0.0116 |           0.0000 |
[32m[20221213 20:41:03 @agent_ppo2.py:185][0m |          -0.0297 |           0.0114 |           0.0000 |
[32m[20221213 20:41:03 @agent_ppo2.py:185][0m |          -0.0364 |           0.0112 |           0.0000 |
[32m[20221213 20:41:03 @agent_ppo2.py:185][0m |          -0.0438 |           0.0111 |           0.0000 |
[32m[20221213 20:41:03 @agent_ppo2.py:185][0m |          -0.0438 |           0.0110 |           0.0000 |
[32m[20221213 20:41:03 @agent_ppo2.py:185][0m |          -0.0483 |           0.0109 |           0.0000 |
[32m[20221213 20:41:03 @agent_ppo2.py:185][0m |          -0.0464 |           0.0109 |           0.0000 |
[32m[20221213 20:41:03 @agent_ppo2.py:185][0m |          -0.0255 |           0.0109 |           0.0000 |
[32m[20221213 20:41:03 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:41:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.32
[32m[20221213 20:41:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.52
[32m[20221213 20:41:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.91
[32m[20221213 20:41:04 @agent_ppo2.py:143][0m Total time:      11.91 min
[32m[20221213 20:41:04 @agent_ppo2.py:145][0m 1703936 total steps have happened
[32m[20221213 20:41:04 @agent_ppo2.py:121][0m #------------------------ Iteration 416 --------------------------#
[32m[20221213 20:41:04 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:41:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:04 @agent_ppo2.py:185][0m |           0.0485 |           0.0157 |           0.0000 |
[32m[20221213 20:41:04 @agent_ppo2.py:185][0m |           0.0060 |           0.0148 |           0.0000 |
[32m[20221213 20:41:04 @agent_ppo2.py:185][0m |          -0.0218 |           0.0143 |           0.0000 |
[32m[20221213 20:41:04 @agent_ppo2.py:185][0m |          -0.0667 |           0.0144 |           0.0000 |
[32m[20221213 20:41:04 @agent_ppo2.py:185][0m |          -0.0451 |           0.0141 |           0.0000 |
[32m[20221213 20:41:04 @agent_ppo2.py:185][0m |          -0.0522 |           0.0143 |           0.0000 |
[32m[20221213 20:41:05 @agent_ppo2.py:185][0m |          -0.0905 |           0.0163 |           0.0000 |
[32m[20221213 20:41:05 @agent_ppo2.py:185][0m |          -0.0563 |           0.0150 |           0.0000 |
[32m[20221213 20:41:05 @agent_ppo2.py:185][0m |          -0.0626 |           0.0134 |           0.0000 |
[32m[20221213 20:41:05 @agent_ppo2.py:185][0m |          -0.0590 |           0.0130 |           0.0000 |
[32m[20221213 20:41:05 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:41:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.77
[32m[20221213 20:41:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.55
[32m[20221213 20:41:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.40
[32m[20221213 20:41:05 @agent_ppo2.py:143][0m Total time:      11.93 min
[32m[20221213 20:41:05 @agent_ppo2.py:145][0m 1708032 total steps have happened
[32m[20221213 20:41:05 @agent_ppo2.py:121][0m #------------------------ Iteration 417 --------------------------#
[32m[20221213 20:41:06 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:41:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:06 @agent_ppo2.py:185][0m |           0.0252 |           0.0117 |           0.0000 |
[32m[20221213 20:41:06 @agent_ppo2.py:185][0m |           0.0293 |           0.0091 |           0.0000 |
[32m[20221213 20:41:06 @agent_ppo2.py:185][0m |          -0.0264 |           0.0087 |           0.0000 |
[32m[20221213 20:41:06 @agent_ppo2.py:185][0m |          -0.0461 |           0.0086 |           0.0000 |
[32m[20221213 20:41:06 @agent_ppo2.py:185][0m |          -0.0509 |           0.0085 |           0.0000 |
[32m[20221213 20:41:06 @agent_ppo2.py:185][0m |          -0.0542 |           0.0084 |           0.0000 |
[32m[20221213 20:41:06 @agent_ppo2.py:185][0m |          -0.0368 |           0.0096 |           0.0000 |
[32m[20221213 20:41:06 @agent_ppo2.py:185][0m |          -0.0614 |           0.0094 |           0.0000 |
[32m[20221213 20:41:07 @agent_ppo2.py:185][0m |          -0.0640 |           0.0083 |           0.0000 |
[32m[20221213 20:41:07 @agent_ppo2.py:185][0m |          -0.0459 |           0.0089 |           0.0000 |
[32m[20221213 20:41:07 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:41:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.30
[32m[20221213 20:41:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.81
[32m[20221213 20:41:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.57
[32m[20221213 20:41:07 @agent_ppo2.py:143][0m Total time:      11.96 min
[32m[20221213 20:41:07 @agent_ppo2.py:145][0m 1712128 total steps have happened
[32m[20221213 20:41:07 @agent_ppo2.py:121][0m #------------------------ Iteration 418 --------------------------#
[32m[20221213 20:41:07 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:41:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:08 @agent_ppo2.py:185][0m |           0.0377 |           0.0226 |           0.0000 |
[32m[20221213 20:41:08 @agent_ppo2.py:185][0m |          -0.0001 |           0.0166 |           0.0000 |
[32m[20221213 20:41:08 @agent_ppo2.py:185][0m |          -0.0145 |           0.0158 |           0.0000 |
[32m[20221213 20:41:08 @agent_ppo2.py:185][0m |          -0.0199 |           0.0152 |           0.0000 |
[32m[20221213 20:41:08 @agent_ppo2.py:185][0m |          -0.0134 |           0.0147 |           0.0000 |
[32m[20221213 20:41:08 @agent_ppo2.py:185][0m |          -0.0241 |           0.0143 |           0.0000 |
[32m[20221213 20:41:08 @agent_ppo2.py:185][0m |          -0.0301 |           0.0140 |           0.0000 |
[32m[20221213 20:41:08 @agent_ppo2.py:185][0m |          -0.0330 |           0.0139 |           0.0000 |
[32m[20221213 20:41:08 @agent_ppo2.py:185][0m |          -0.0288 |           0.0137 |           0.0000 |
[32m[20221213 20:41:08 @agent_ppo2.py:185][0m |          -0.0309 |           0.0135 |           0.0000 |
[32m[20221213 20:41:08 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:41:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.12
[32m[20221213 20:41:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.06
[32m[20221213 20:41:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.51
[32m[20221213 20:41:09 @agent_ppo2.py:143][0m Total time:      11.99 min
[32m[20221213 20:41:09 @agent_ppo2.py:145][0m 1716224 total steps have happened
[32m[20221213 20:41:09 @agent_ppo2.py:121][0m #------------------------ Iteration 419 --------------------------#
[32m[20221213 20:41:09 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:41:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:09 @agent_ppo2.py:185][0m |           0.0528 |           0.0209 |           0.0000 |
[32m[20221213 20:41:09 @agent_ppo2.py:185][0m |           0.0109 |           0.0082 |           0.0000 |
[32m[20221213 20:41:09 @agent_ppo2.py:185][0m |           0.0017 |           0.0072 |           0.0000 |
[32m[20221213 20:41:09 @agent_ppo2.py:185][0m |          -0.0053 |           0.0069 |           0.0000 |
[32m[20221213 20:41:10 @agent_ppo2.py:185][0m |          -0.0130 |           0.0067 |           0.0000 |
[32m[20221213 20:41:10 @agent_ppo2.py:185][0m |          -0.0164 |           0.0065 |           0.0000 |
[32m[20221213 20:41:10 @agent_ppo2.py:185][0m |          -0.0205 |           0.0064 |           0.0000 |
[32m[20221213 20:41:10 @agent_ppo2.py:185][0m |          -0.0241 |           0.0063 |           0.0000 |
[32m[20221213 20:41:10 @agent_ppo2.py:185][0m |          -0.0261 |           0.0062 |           0.0000 |
[32m[20221213 20:41:10 @agent_ppo2.py:185][0m |          -0.0291 |           0.0061 |           0.0000 |
[32m[20221213 20:41:10 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:41:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.24
[32m[20221213 20:41:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.49
[32m[20221213 20:41:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.60
[32m[20221213 20:41:10 @agent_ppo2.py:143][0m Total time:      12.02 min
[32m[20221213 20:41:10 @agent_ppo2.py:145][0m 1720320 total steps have happened
[32m[20221213 20:41:10 @agent_ppo2.py:121][0m #------------------------ Iteration 420 --------------------------#
[32m[20221213 20:41:11 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:41:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:11 @agent_ppo2.py:185][0m |           0.0273 |           0.0238 |           0.0000 |
[32m[20221213 20:41:11 @agent_ppo2.py:185][0m |           0.0024 |           0.0116 |           0.0000 |
[32m[20221213 20:41:11 @agent_ppo2.py:185][0m |          -0.0124 |           0.0095 |           0.0000 |
[32m[20221213 20:41:11 @agent_ppo2.py:185][0m |          -0.0164 |           0.0089 |           0.0000 |
[32m[20221213 20:41:11 @agent_ppo2.py:185][0m |          -0.0153 |           0.0088 |           0.0000 |
[32m[20221213 20:41:11 @agent_ppo2.py:185][0m |          -0.0238 |           0.0087 |           0.0000 |
[32m[20221213 20:41:11 @agent_ppo2.py:185][0m |          -0.0256 |           0.0086 |           0.0000 |
[32m[20221213 20:41:12 @agent_ppo2.py:185][0m |          -0.0284 |           0.0085 |           0.0000 |
[32m[20221213 20:41:12 @agent_ppo2.py:185][0m |          -0.0317 |           0.0084 |           0.0000 |
[32m[20221213 20:41:12 @agent_ppo2.py:185][0m |          -0.0324 |           0.0083 |           0.0000 |
[32m[20221213 20:41:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:41:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.76
[32m[20221213 20:41:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.19
[32m[20221213 20:41:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.05
[32m[20221213 20:41:12 @agent_ppo2.py:143][0m Total time:      12.05 min
[32m[20221213 20:41:12 @agent_ppo2.py:145][0m 1724416 total steps have happened
[32m[20221213 20:41:12 @agent_ppo2.py:121][0m #------------------------ Iteration 421 --------------------------#
[32m[20221213 20:41:12 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:41:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:13 @agent_ppo2.py:185][0m |           0.0303 |           0.0431 |           0.0000 |
[32m[20221213 20:41:13 @agent_ppo2.py:185][0m |          -0.0017 |           0.0204 |           0.0000 |
[32m[20221213 20:41:13 @agent_ppo2.py:185][0m |          -0.0175 |           0.0165 |           0.0000 |
[32m[20221213 20:41:13 @agent_ppo2.py:185][0m |          -0.0217 |           0.0152 |           0.0000 |
[32m[20221213 20:41:13 @agent_ppo2.py:185][0m |          -0.0212 |           0.0145 |           0.0000 |
[32m[20221213 20:41:13 @agent_ppo2.py:185][0m |          -0.0274 |           0.0145 |           0.0000 |
[32m[20221213 20:41:13 @agent_ppo2.py:185][0m |          -0.0287 |           0.0145 |           0.0000 |
[32m[20221213 20:41:13 @agent_ppo2.py:185][0m |          -0.0300 |           0.0139 |           0.0000 |
[32m[20221213 20:41:13 @agent_ppo2.py:185][0m |          -0.0311 |           0.0135 |           0.0000 |
[32m[20221213 20:41:13 @agent_ppo2.py:185][0m |          -0.0316 |           0.0132 |           0.0000 |
[32m[20221213 20:41:13 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:41:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.58
[32m[20221213 20:41:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.00
[32m[20221213 20:41:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.30
[32m[20221213 20:41:14 @agent_ppo2.py:143][0m Total time:      12.08 min
[32m[20221213 20:41:14 @agent_ppo2.py:145][0m 1728512 total steps have happened
[32m[20221213 20:41:14 @agent_ppo2.py:121][0m #------------------------ Iteration 422 --------------------------#
[32m[20221213 20:41:14 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:41:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:14 @agent_ppo2.py:185][0m |           0.0421 |           0.0203 |           0.0000 |
[32m[20221213 20:41:14 @agent_ppo2.py:185][0m |           0.0235 |           0.0150 |           0.0000 |
[32m[20221213 20:41:14 @agent_ppo2.py:185][0m |           0.0087 |           0.0142 |           0.0000 |
[32m[20221213 20:41:15 @agent_ppo2.py:185][0m |          -0.0045 |           0.0137 |           0.0000 |
[32m[20221213 20:41:15 @agent_ppo2.py:185][0m |          -0.0165 |           0.0135 |           0.0000 |
[32m[20221213 20:41:15 @agent_ppo2.py:185][0m |          -0.0228 |           0.0136 |           0.0000 |
[32m[20221213 20:41:15 @agent_ppo2.py:185][0m |          -0.0292 |           0.0134 |           0.0000 |
[32m[20221213 20:41:15 @agent_ppo2.py:185][0m |          -0.0348 |           0.0133 |           0.0000 |
[32m[20221213 20:41:15 @agent_ppo2.py:185][0m |          -0.0347 |           0.0129 |           0.0000 |
[32m[20221213 20:41:15 @agent_ppo2.py:185][0m |          -0.0431 |           0.0130 |           0.0000 |
[32m[20221213 20:41:15 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:41:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.53
[32m[20221213 20:41:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.49
[32m[20221213 20:41:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.57
[32m[20221213 20:41:15 @agent_ppo2.py:143][0m Total time:      12.10 min
[32m[20221213 20:41:15 @agent_ppo2.py:145][0m 1732608 total steps have happened
[32m[20221213 20:41:15 @agent_ppo2.py:121][0m #------------------------ Iteration 423 --------------------------#
[32m[20221213 20:41:16 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:16 @agent_ppo2.py:185][0m |           0.0295 |           0.0126 |           0.0000 |
[32m[20221213 20:41:16 @agent_ppo2.py:185][0m |           0.0128 |           0.0108 |           0.0000 |
[32m[20221213 20:41:16 @agent_ppo2.py:185][0m |          -0.0127 |           0.0107 |           0.0000 |
[32m[20221213 20:41:16 @agent_ppo2.py:185][0m |          -0.0298 |           0.0106 |           0.0000 |
[32m[20221213 20:41:16 @agent_ppo2.py:185][0m |          -0.0412 |           0.0106 |           0.0000 |
[32m[20221213 20:41:16 @agent_ppo2.py:185][0m |          -0.0468 |           0.0104 |           0.0000 |
[32m[20221213 20:41:17 @agent_ppo2.py:185][0m |          -0.0506 |           0.0103 |           0.0000 |
[32m[20221213 20:41:17 @agent_ppo2.py:185][0m |          -0.0505 |           0.0103 |           0.0000 |
[32m[20221213 20:41:17 @agent_ppo2.py:185][0m |          -0.0682 |           0.0103 |           0.0000 |
[32m[20221213 20:41:17 @agent_ppo2.py:185][0m |          -0.0553 |           0.0104 |           0.0000 |
[32m[20221213 20:41:17 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:41:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.39
[32m[20221213 20:41:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.40
[32m[20221213 20:41:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.10
[32m[20221213 20:41:17 @agent_ppo2.py:143][0m Total time:      12.13 min
[32m[20221213 20:41:17 @agent_ppo2.py:145][0m 1736704 total steps have happened
[32m[20221213 20:41:17 @agent_ppo2.py:121][0m #------------------------ Iteration 424 --------------------------#
[32m[20221213 20:41:18 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:18 @agent_ppo2.py:185][0m |           0.0225 |           0.0335 |           0.0000 |
[32m[20221213 20:41:18 @agent_ppo2.py:185][0m |          -0.0131 |           0.0191 |           0.0000 |
[32m[20221213 20:41:18 @agent_ppo2.py:185][0m |          -0.0063 |           0.0175 |           0.0000 |
[32m[20221213 20:41:18 @agent_ppo2.py:185][0m |          -0.0116 |           0.0175 |           0.0000 |
[32m[20221213 20:41:18 @agent_ppo2.py:185][0m |          -0.0103 |           0.0164 |           0.0000 |
[32m[20221213 20:41:18 @agent_ppo2.py:185][0m |          -0.0207 |           0.0163 |           0.0000 |
[32m[20221213 20:41:18 @agent_ppo2.py:185][0m |          -0.0164 |           0.0156 |           0.0000 |
[32m[20221213 20:41:18 @agent_ppo2.py:185][0m |          -0.0154 |           0.0156 |           0.0000 |
[32m[20221213 20:41:18 @agent_ppo2.py:185][0m |          -0.0228 |           0.0152 |           0.0000 |
[32m[20221213 20:41:18 @agent_ppo2.py:185][0m |          -0.0254 |           0.0150 |           0.0000 |
[32m[20221213 20:41:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:41:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.95
[32m[20221213 20:41:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.84
[32m[20221213 20:41:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.25
[32m[20221213 20:41:19 @agent_ppo2.py:143][0m Total time:      12.16 min
[32m[20221213 20:41:19 @agent_ppo2.py:145][0m 1740800 total steps have happened
[32m[20221213 20:41:19 @agent_ppo2.py:121][0m #------------------------ Iteration 425 --------------------------#
[32m[20221213 20:41:19 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:41:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:19 @agent_ppo2.py:185][0m |           0.0328 |           0.0187 |           0.0000 |
[32m[20221213 20:41:19 @agent_ppo2.py:185][0m |           0.0115 |           0.0120 |           0.0000 |
[32m[20221213 20:41:20 @agent_ppo2.py:185][0m |           0.0126 |           0.0117 |           0.0000 |
[32m[20221213 20:41:20 @agent_ppo2.py:185][0m |          -0.0108 |           0.0113 |           0.0000 |
[32m[20221213 20:41:20 @agent_ppo2.py:185][0m |          -0.0172 |           0.0110 |           0.0000 |
[32m[20221213 20:41:20 @agent_ppo2.py:185][0m |          -0.0278 |           0.0109 |           0.0000 |
[32m[20221213 20:41:20 @agent_ppo2.py:185][0m |          -0.0349 |           0.0107 |           0.0000 |
[32m[20221213 20:41:20 @agent_ppo2.py:185][0m |          -0.0418 |           0.0107 |           0.0000 |
[32m[20221213 20:41:20 @agent_ppo2.py:185][0m |          -0.0399 |           0.0105 |           0.0000 |
[32m[20221213 20:41:20 @agent_ppo2.py:185][0m |          -0.0520 |           0.0104 |           0.0000 |
[32m[20221213 20:41:20 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:41:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.98
[32m[20221213 20:41:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.36
[32m[20221213 20:41:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.07
[32m[20221213 20:41:20 @agent_ppo2.py:143][0m Total time:      12.19 min
[32m[20221213 20:41:20 @agent_ppo2.py:145][0m 1744896 total steps have happened
[32m[20221213 20:41:20 @agent_ppo2.py:121][0m #------------------------ Iteration 426 --------------------------#
[32m[20221213 20:41:21 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:41:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:21 @agent_ppo2.py:185][0m |           0.0225 |           0.0443 |           0.0000 |
[32m[20221213 20:41:21 @agent_ppo2.py:185][0m |           0.0033 |           0.0168 |           0.0000 |
[32m[20221213 20:41:21 @agent_ppo2.py:185][0m |          -0.0111 |           0.0155 |           0.0000 |
[32m[20221213 20:41:21 @agent_ppo2.py:185][0m |          -0.0162 |           0.0154 |           0.0000 |
[32m[20221213 20:41:21 @agent_ppo2.py:185][0m |          -0.0198 |           0.0147 |           0.0000 |
[32m[20221213 20:41:21 @agent_ppo2.py:185][0m |          -0.0268 |           0.0154 |           0.0000 |
[32m[20221213 20:41:21 @agent_ppo2.py:185][0m |          -0.0649 |           0.0166 |           0.0000 |
[32m[20221213 20:41:22 @agent_ppo2.py:185][0m |          -0.0616 |           0.0148 |           0.0000 |
[32m[20221213 20:41:22 @agent_ppo2.py:185][0m |          -0.0321 |           0.0154 |           0.0000 |
[32m[20221213 20:41:22 @agent_ppo2.py:185][0m |          -0.0327 |           0.0139 |           0.0000 |
[32m[20221213 20:41:22 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:41:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.18
[32m[20221213 20:41:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.52
[32m[20221213 20:41:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.56
[32m[20221213 20:41:22 @agent_ppo2.py:143][0m Total time:      12.22 min
[32m[20221213 20:41:22 @agent_ppo2.py:145][0m 1748992 total steps have happened
[32m[20221213 20:41:22 @agent_ppo2.py:121][0m #------------------------ Iteration 427 --------------------------#
[32m[20221213 20:41:23 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:41:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:23 @agent_ppo2.py:185][0m |           0.0466 |           0.0243 |           0.0000 |
[32m[20221213 20:41:23 @agent_ppo2.py:185][0m |           0.0215 |           0.0134 |           0.0000 |
[32m[20221213 20:41:23 @agent_ppo2.py:185][0m |          -0.0019 |           0.0129 |           0.0000 |
[32m[20221213 20:41:23 @agent_ppo2.py:185][0m |          -0.0182 |           0.0126 |           0.0000 |
[32m[20221213 20:41:23 @agent_ppo2.py:185][0m |          -0.0264 |           0.0125 |           0.0000 |
[32m[20221213 20:41:23 @agent_ppo2.py:185][0m |          -0.0347 |           0.0123 |           0.0000 |
[32m[20221213 20:41:23 @agent_ppo2.py:185][0m |          -0.0394 |           0.0122 |           0.0000 |
[32m[20221213 20:41:23 @agent_ppo2.py:185][0m |          -0.0422 |           0.0121 |           0.0000 |
[32m[20221213 20:41:23 @agent_ppo2.py:185][0m |          -0.0498 |           0.0119 |           0.0000 |
[32m[20221213 20:41:23 @agent_ppo2.py:185][0m |          -0.0514 |           0.0118 |           0.0000 |
[32m[20221213 20:41:23 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:41:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.00
[32m[20221213 20:41:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.27
[32m[20221213 20:41:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.99
[32m[20221213 20:41:24 @agent_ppo2.py:143][0m Total time:      12.24 min
[32m[20221213 20:41:24 @agent_ppo2.py:145][0m 1753088 total steps have happened
[32m[20221213 20:41:24 @agent_ppo2.py:121][0m #------------------------ Iteration 428 --------------------------#
[32m[20221213 20:41:24 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:41:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:24 @agent_ppo2.py:185][0m |           0.0240 |           0.0370 |           0.0000 |
[32m[20221213 20:41:24 @agent_ppo2.py:185][0m |           0.0130 |           0.0185 |           0.0000 |
[32m[20221213 20:41:24 @agent_ppo2.py:185][0m |          -0.0114 |           0.0173 |           0.0000 |
[32m[20221213 20:41:25 @agent_ppo2.py:185][0m |          -0.0165 |           0.0168 |           0.0000 |
[32m[20221213 20:41:25 @agent_ppo2.py:185][0m |          -0.0164 |           0.0167 |           0.0000 |
[32m[20221213 20:41:25 @agent_ppo2.py:185][0m |          -0.0197 |           0.0161 |           0.0000 |
[32m[20221213 20:41:25 @agent_ppo2.py:185][0m |          -0.0249 |           0.0158 |           0.0000 |
[32m[20221213 20:41:25 @agent_ppo2.py:185][0m |          -0.0629 |           0.0156 |           0.0000 |
[32m[20221213 20:41:25 @agent_ppo2.py:185][0m |          -0.0082 |           0.0155 |           0.0000 |
[32m[20221213 20:41:25 @agent_ppo2.py:185][0m |          -0.0107 |           0.0152 |           0.0000 |
[32m[20221213 20:41:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:41:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.17
[32m[20221213 20:41:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.73
[32m[20221213 20:41:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.49
[32m[20221213 20:41:25 @agent_ppo2.py:143][0m Total time:      12.27 min
[32m[20221213 20:41:25 @agent_ppo2.py:145][0m 1757184 total steps have happened
[32m[20221213 20:41:25 @agent_ppo2.py:121][0m #------------------------ Iteration 429 --------------------------#
[32m[20221213 20:41:26 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:41:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:26 @agent_ppo2.py:185][0m |           0.0381 |           0.0205 |           0.0000 |
[32m[20221213 20:41:26 @agent_ppo2.py:185][0m |           0.0097 |           0.0125 |           0.0000 |
[32m[20221213 20:41:26 @agent_ppo2.py:185][0m |          -0.0070 |           0.0120 |           0.0000 |
[32m[20221213 20:41:26 @agent_ppo2.py:185][0m |          -0.0235 |           0.0118 |           0.0000 |
[32m[20221213 20:41:26 @agent_ppo2.py:185][0m |           0.0129 |           0.0128 |           0.0000 |
[32m[20221213 20:41:26 @agent_ppo2.py:185][0m |          -0.0326 |           0.0134 |           0.0000 |
[32m[20221213 20:41:26 @agent_ppo2.py:185][0m |          -0.0363 |           0.0117 |           0.0000 |
[32m[20221213 20:41:26 @agent_ppo2.py:185][0m |          -0.0403 |           0.0116 |           0.0000 |
[32m[20221213 20:41:27 @agent_ppo2.py:185][0m |          -0.0432 |           0.0113 |           0.0000 |
[32m[20221213 20:41:27 @agent_ppo2.py:185][0m |          -0.0461 |           0.0114 |           0.0000 |
[32m[20221213 20:41:27 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:41:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.26
[32m[20221213 20:41:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.13
[32m[20221213 20:41:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.60
[32m[20221213 20:41:27 @agent_ppo2.py:143][0m Total time:      12.30 min
[32m[20221213 20:41:27 @agent_ppo2.py:145][0m 1761280 total steps have happened
[32m[20221213 20:41:27 @agent_ppo2.py:121][0m #------------------------ Iteration 430 --------------------------#
[32m[20221213 20:41:27 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:41:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:28 @agent_ppo2.py:185][0m |           0.0212 |           0.0288 |           0.0000 |
[32m[20221213 20:41:28 @agent_ppo2.py:185][0m |          -0.0003 |           0.0193 |           0.0000 |
[32m[20221213 20:41:28 @agent_ppo2.py:185][0m |          -0.0146 |           0.0178 |           0.0000 |
[32m[20221213 20:41:28 @agent_ppo2.py:185][0m |          -0.0401 |           0.0176 |           0.0000 |
[32m[20221213 20:41:28 @agent_ppo2.py:185][0m |          -0.0280 |           0.0180 |           0.0000 |
[32m[20221213 20:41:28 @agent_ppo2.py:185][0m |          -0.0315 |           0.0161 |           0.0000 |
[32m[20221213 20:41:28 @agent_ppo2.py:185][0m |          -0.0273 |           0.0159 |           0.0000 |
[32m[20221213 20:41:28 @agent_ppo2.py:185][0m |          -0.0354 |           0.0156 |           0.0000 |
[32m[20221213 20:41:28 @agent_ppo2.py:185][0m |          -0.0330 |           0.0155 |           0.0000 |
[32m[20221213 20:41:28 @agent_ppo2.py:185][0m |          -0.0321 |           0.0153 |           0.0000 |
[32m[20221213 20:41:28 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:41:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.45
[32m[20221213 20:41:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.03
[32m[20221213 20:41:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.53
[32m[20221213 20:41:29 @agent_ppo2.py:143][0m Total time:      12.33 min
[32m[20221213 20:41:29 @agent_ppo2.py:145][0m 1765376 total steps have happened
[32m[20221213 20:41:29 @agent_ppo2.py:121][0m #------------------------ Iteration 431 --------------------------#
[32m[20221213 20:41:29 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:29 @agent_ppo2.py:185][0m |           0.0322 |           0.0264 |           0.0000 |
[32m[20221213 20:41:29 @agent_ppo2.py:185][0m |           0.0007 |           0.0134 |           0.0000 |
[32m[20221213 20:41:29 @agent_ppo2.py:185][0m |          -0.0161 |           0.0129 |           0.0000 |
[32m[20221213 20:41:30 @agent_ppo2.py:185][0m |          -0.0192 |           0.0125 |           0.0000 |
[32m[20221213 20:41:30 @agent_ppo2.py:185][0m |          -0.0319 |           0.0121 |           0.0000 |
[32m[20221213 20:41:30 @agent_ppo2.py:185][0m |          -0.0161 |           0.0130 |           0.0000 |
[32m[20221213 20:41:30 @agent_ppo2.py:185][0m |          -0.0426 |           0.0127 |           0.0000 |
[32m[20221213 20:41:30 @agent_ppo2.py:185][0m |          -0.0444 |           0.0115 |           0.0000 |
[32m[20221213 20:41:30 @agent_ppo2.py:185][0m |          -0.0454 |           0.0114 |           0.0000 |
[32m[20221213 20:41:30 @agent_ppo2.py:185][0m |          -0.0513 |           0.0112 |           0.0000 |
[32m[20221213 20:41:30 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:41:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.65
[32m[20221213 20:41:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.09
[32m[20221213 20:41:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.98
[32m[20221213 20:41:30 @agent_ppo2.py:143][0m Total time:      12.35 min
[32m[20221213 20:41:30 @agent_ppo2.py:145][0m 1769472 total steps have happened
[32m[20221213 20:41:30 @agent_ppo2.py:121][0m #------------------------ Iteration 432 --------------------------#
[32m[20221213 20:41:31 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:31 @agent_ppo2.py:185][0m |           0.0195 |           0.0113 |           0.0000 |
[32m[20221213 20:41:31 @agent_ppo2.py:185][0m |          -0.0148 |           0.0091 |           0.0000 |
[32m[20221213 20:41:31 @agent_ppo2.py:185][0m |          -0.0346 |           0.0089 |           0.0000 |
[32m[20221213 20:41:31 @agent_ppo2.py:185][0m |          -0.0516 |           0.0088 |           0.0000 |
[32m[20221213 20:41:31 @agent_ppo2.py:185][0m |          -0.0553 |           0.0087 |           0.0000 |
[32m[20221213 20:41:31 @agent_ppo2.py:185][0m |          -0.0629 |           0.0087 |           0.0000 |
[32m[20221213 20:41:32 @agent_ppo2.py:185][0m |          -0.0629 |           0.0086 |           0.0000 |
[32m[20221213 20:41:32 @agent_ppo2.py:185][0m |          -0.0653 |           0.0085 |           0.0000 |
[32m[20221213 20:41:32 @agent_ppo2.py:185][0m |          -0.0697 |           0.0085 |           0.0000 |
[32m[20221213 20:41:32 @agent_ppo2.py:185][0m |          -0.0710 |           0.0084 |           0.0000 |
[32m[20221213 20:41:32 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:41:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.43
[32m[20221213 20:41:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.74
[32m[20221213 20:41:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.70
[32m[20221213 20:41:32 @agent_ppo2.py:143][0m Total time:      12.38 min
[32m[20221213 20:41:32 @agent_ppo2.py:145][0m 1773568 total steps have happened
[32m[20221213 20:41:32 @agent_ppo2.py:121][0m #------------------------ Iteration 433 --------------------------#
[32m[20221213 20:41:33 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:41:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:33 @agent_ppo2.py:185][0m |           0.0231 |           0.0209 |           0.0000 |
[32m[20221213 20:41:33 @agent_ppo2.py:185][0m |          -0.0041 |           0.0120 |           0.0000 |
[32m[20221213 20:41:33 @agent_ppo2.py:185][0m |          -0.0139 |           0.0115 |           0.0000 |
[32m[20221213 20:41:33 @agent_ppo2.py:185][0m |          -0.0200 |           0.0114 |           0.0000 |
[32m[20221213 20:41:33 @agent_ppo2.py:185][0m |          -0.0258 |           0.0114 |           0.0000 |
[32m[20221213 20:41:33 @agent_ppo2.py:185][0m |          -0.0271 |           0.0113 |           0.0000 |
[32m[20221213 20:41:33 @agent_ppo2.py:185][0m |          -0.0301 |           0.0112 |           0.0000 |
[32m[20221213 20:41:33 @agent_ppo2.py:185][0m |          -0.0312 |           0.0111 |           0.0000 |
[32m[20221213 20:41:33 @agent_ppo2.py:185][0m |          -0.0314 |           0.0111 |           0.0000 |
[32m[20221213 20:41:33 @agent_ppo2.py:185][0m |          -0.0332 |           0.0110 |           0.0000 |
[32m[20221213 20:41:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:41:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.47
[32m[20221213 20:41:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.19
[32m[20221213 20:41:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.82
[32m[20221213 20:41:34 @agent_ppo2.py:143][0m Total time:      12.41 min
[32m[20221213 20:41:34 @agent_ppo2.py:145][0m 1777664 total steps have happened
[32m[20221213 20:41:34 @agent_ppo2.py:121][0m #------------------------ Iteration 434 --------------------------#
[32m[20221213 20:41:34 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:41:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:35 @agent_ppo2.py:185][0m |           0.0386 |           0.0431 |           0.0000 |
[32m[20221213 20:41:35 @agent_ppo2.py:185][0m |           0.0108 |           0.0199 |           0.0000 |
[32m[20221213 20:41:35 @agent_ppo2.py:185][0m |          -0.0014 |           0.0183 |           0.0000 |
[32m[20221213 20:41:35 @agent_ppo2.py:185][0m |          -0.0132 |           0.0173 |           0.0000 |
[32m[20221213 20:41:35 @agent_ppo2.py:185][0m |          -0.0190 |           0.0168 |           0.0000 |
[32m[20221213 20:41:35 @agent_ppo2.py:185][0m |          -0.0237 |           0.0163 |           0.0000 |
[32m[20221213 20:41:35 @agent_ppo2.py:185][0m |          -0.0284 |           0.0161 |           0.0000 |
[32m[20221213 20:41:35 @agent_ppo2.py:185][0m |          -0.0309 |           0.0159 |           0.0000 |
[32m[20221213 20:41:35 @agent_ppo2.py:185][0m |          -0.0314 |           0.0163 |           0.0000 |
[32m[20221213 20:41:35 @agent_ppo2.py:185][0m |          -0.0309 |           0.0157 |           0.0000 |
[32m[20221213 20:41:35 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:41:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.49
[32m[20221213 20:41:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.99
[32m[20221213 20:41:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.34
[32m[20221213 20:41:36 @agent_ppo2.py:143][0m Total time:      12.44 min
[32m[20221213 20:41:36 @agent_ppo2.py:145][0m 1781760 total steps have happened
[32m[20221213 20:41:36 @agent_ppo2.py:121][0m #------------------------ Iteration 435 --------------------------#
[32m[20221213 20:41:36 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:41:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:36 @agent_ppo2.py:185][0m |           0.0184 |           0.0276 |           0.0000 |
[32m[20221213 20:41:36 @agent_ppo2.py:185][0m |           0.0019 |           0.0146 |           0.0000 |
[32m[20221213 20:41:36 @agent_ppo2.py:185][0m |          -0.0116 |           0.0140 |           0.0000 |
[32m[20221213 20:41:36 @agent_ppo2.py:185][0m |          -0.0215 |           0.0133 |           0.0000 |
[32m[20221213 20:41:37 @agent_ppo2.py:185][0m |          -0.0193 |           0.0132 |           0.0000 |
[32m[20221213 20:41:37 @agent_ppo2.py:185][0m |          -0.0385 |           0.0132 |           0.0000 |
[32m[20221213 20:41:37 @agent_ppo2.py:185][0m |          -0.0425 |           0.0128 |           0.0000 |
[32m[20221213 20:41:37 @agent_ppo2.py:185][0m |          -0.0402 |           0.0125 |           0.0000 |
[32m[20221213 20:41:37 @agent_ppo2.py:185][0m |          -0.0534 |           0.0123 |           0.0000 |
[32m[20221213 20:41:37 @agent_ppo2.py:185][0m |          -0.0544 |           0.0121 |           0.0000 |
[32m[20221213 20:41:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:41:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.17
[32m[20221213 20:41:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.59
[32m[20221213 20:41:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.47
[32m[20221213 20:41:37 @agent_ppo2.py:143][0m Total time:      12.47 min
[32m[20221213 20:41:37 @agent_ppo2.py:145][0m 1785856 total steps have happened
[32m[20221213 20:41:37 @agent_ppo2.py:121][0m #------------------------ Iteration 436 --------------------------#
[32m[20221213 20:41:38 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:38 @agent_ppo2.py:185][0m |           0.0256 |           0.0130 |           0.0000 |
[32m[20221213 20:41:38 @agent_ppo2.py:185][0m |           0.0055 |           0.0093 |           0.0000 |
[32m[20221213 20:41:38 @agent_ppo2.py:185][0m |          -0.0246 |           0.0089 |           0.0000 |
[32m[20221213 20:41:38 @agent_ppo2.py:185][0m |          -0.0341 |           0.0087 |           0.0000 |
[32m[20221213 20:41:38 @agent_ppo2.py:185][0m |          -0.0487 |           0.0086 |           0.0000 |
[32m[20221213 20:41:38 @agent_ppo2.py:185][0m |          -0.0523 |           0.0085 |           0.0000 |
[32m[20221213 20:41:38 @agent_ppo2.py:185][0m |          -0.0294 |           0.0090 |           0.0000 |
[32m[20221213 20:41:38 @agent_ppo2.py:185][0m |          -0.0578 |           0.0087 |           0.0000 |
[32m[20221213 20:41:39 @agent_ppo2.py:185][0m |          -0.0591 |           0.0083 |           0.0000 |
[32m[20221213 20:41:39 @agent_ppo2.py:185][0m |          -0.0599 |           0.0082 |           0.0000 |
[32m[20221213 20:41:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:41:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.70
[32m[20221213 20:41:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.92
[32m[20221213 20:41:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.19
[32m[20221213 20:41:39 @agent_ppo2.py:143][0m Total time:      12.50 min
[32m[20221213 20:41:39 @agent_ppo2.py:145][0m 1789952 total steps have happened
[32m[20221213 20:41:39 @agent_ppo2.py:121][0m #------------------------ Iteration 437 --------------------------#
[32m[20221213 20:41:39 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:40 @agent_ppo2.py:185][0m |           0.0242 |           0.0086 |           0.0000 |
[32m[20221213 20:41:40 @agent_ppo2.py:185][0m |          -0.0065 |           0.0082 |           0.0000 |
[32m[20221213 20:41:40 @agent_ppo2.py:185][0m |          -0.0328 |           0.0081 |           0.0000 |
[32m[20221213 20:41:40 @agent_ppo2.py:185][0m |          -0.0431 |           0.0080 |           0.0000 |
[32m[20221213 20:41:40 @agent_ppo2.py:185][0m |          -0.0536 |           0.0079 |           0.0000 |
[32m[20221213 20:41:40 @agent_ppo2.py:185][0m |          -0.0620 |           0.0079 |           0.0000 |
[32m[20221213 20:41:40 @agent_ppo2.py:185][0m |          -0.0561 |           0.0078 |           0.0000 |
[32m[20221213 20:41:40 @agent_ppo2.py:185][0m |          -0.0673 |           0.0077 |           0.0000 |
[32m[20221213 20:41:40 @agent_ppo2.py:185][0m |          -0.0670 |           0.0077 |           0.0000 |
[32m[20221213 20:41:40 @agent_ppo2.py:185][0m |          -0.0693 |           0.0076 |           0.0000 |
[32m[20221213 20:41:40 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:41:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.04
[32m[20221213 20:41:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.64
[32m[20221213 20:41:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.01
[32m[20221213 20:41:41 @agent_ppo2.py:143][0m Total time:      12.53 min
[32m[20221213 20:41:41 @agent_ppo2.py:145][0m 1794048 total steps have happened
[32m[20221213 20:41:41 @agent_ppo2.py:121][0m #------------------------ Iteration 438 --------------------------#
[32m[20221213 20:41:41 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:41 @agent_ppo2.py:185][0m |           0.0311 |           0.0134 |           0.0000 |
[32m[20221213 20:41:41 @agent_ppo2.py:185][0m |          -0.0037 |           0.0105 |           0.0000 |
[32m[20221213 20:41:42 @agent_ppo2.py:185][0m |          -0.0135 |           0.0100 |           0.0000 |
[32m[20221213 20:41:42 @agent_ppo2.py:185][0m |          -0.0181 |           0.0098 |           0.0000 |
[32m[20221213 20:41:42 @agent_ppo2.py:185][0m |          -0.0278 |           0.0099 |           0.0000 |
[32m[20221213 20:41:42 @agent_ppo2.py:185][0m |          -0.0251 |           0.0099 |           0.0000 |
[32m[20221213 20:41:42 @agent_ppo2.py:185][0m |          -0.0226 |           0.0095 |           0.0000 |
[32m[20221213 20:41:42 @agent_ppo2.py:185][0m |          -0.0281 |           0.0095 |           0.0000 |
[32m[20221213 20:41:42 @agent_ppo2.py:185][0m |          -0.0245 |           0.0095 |           0.0000 |
[32m[20221213 20:41:42 @agent_ppo2.py:185][0m |          -0.0225 |           0.0094 |           0.0000 |
[32m[20221213 20:41:42 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:41:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.03
[32m[20221213 20:41:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.74
[32m[20221213 20:41:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.84
[32m[20221213 20:41:43 @agent_ppo2.py:143][0m Total time:      12.56 min
[32m[20221213 20:41:43 @agent_ppo2.py:145][0m 1798144 total steps have happened
[32m[20221213 20:41:43 @agent_ppo2.py:121][0m #------------------------ Iteration 439 --------------------------#
[32m[20221213 20:41:43 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:43 @agent_ppo2.py:185][0m |           0.0336 |           0.0449 |           0.0000 |
[32m[20221213 20:41:43 @agent_ppo2.py:185][0m |           0.0005 |           0.0295 |           0.0000 |
[32m[20221213 20:41:43 @agent_ppo2.py:185][0m |          -0.0363 |           0.0246 |           0.0000 |
[32m[20221213 20:41:43 @agent_ppo2.py:185][0m |          -0.0199 |           0.0236 |           0.0000 |
[32m[20221213 20:41:44 @agent_ppo2.py:185][0m |          -0.0236 |           0.0220 |           0.0000 |
[32m[20221213 20:41:44 @agent_ppo2.py:185][0m |          -0.0267 |           0.0204 |           0.0000 |
[32m[20221213 20:41:44 @agent_ppo2.py:185][0m |          -0.0306 |           0.0197 |           0.0000 |
[32m[20221213 20:41:44 @agent_ppo2.py:185][0m |          -0.0297 |           0.0192 |           0.0000 |
[32m[20221213 20:41:44 @agent_ppo2.py:185][0m |          -0.0529 |           0.0192 |           0.0000 |
[32m[20221213 20:41:44 @agent_ppo2.py:185][0m |           0.0160 |           0.0185 |           0.0000 |
[32m[20221213 20:41:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:41:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.95
[32m[20221213 20:41:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.66
[32m[20221213 20:41:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.30
[32m[20221213 20:41:44 @agent_ppo2.py:143][0m Total time:      12.59 min
[32m[20221213 20:41:44 @agent_ppo2.py:145][0m 1802240 total steps have happened
[32m[20221213 20:41:44 @agent_ppo2.py:121][0m #------------------------ Iteration 440 --------------------------#
[32m[20221213 20:41:45 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:41:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:45 @agent_ppo2.py:185][0m |           0.0328 |           0.0162 |           0.0000 |
[32m[20221213 20:41:45 @agent_ppo2.py:185][0m |          -0.0039 |           0.0136 |           0.0000 |
[32m[20221213 20:41:45 @agent_ppo2.py:185][0m |          -0.0341 |           0.0132 |           0.0000 |
[32m[20221213 20:41:45 @agent_ppo2.py:185][0m |          -0.0507 |           0.0129 |           0.0000 |
[32m[20221213 20:41:45 @agent_ppo2.py:185][0m |          -0.0571 |           0.0128 |           0.0000 |
[32m[20221213 20:41:45 @agent_ppo2.py:185][0m |          -0.0628 |           0.0126 |           0.0000 |
[32m[20221213 20:41:45 @agent_ppo2.py:185][0m |          -0.0772 |           0.0125 |           0.0000 |
[32m[20221213 20:41:45 @agent_ppo2.py:185][0m |          -0.0642 |           0.0123 |           0.0000 |
[32m[20221213 20:41:46 @agent_ppo2.py:185][0m |          -0.0699 |           0.0121 |           0.0000 |
[32m[20221213 20:41:46 @agent_ppo2.py:185][0m |          -0.0749 |           0.0121 |           0.0000 |
[32m[20221213 20:41:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:41:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.80
[32m[20221213 20:41:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.64
[32m[20221213 20:41:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.28
[32m[20221213 20:41:46 @agent_ppo2.py:143][0m Total time:      12.61 min
[32m[20221213 20:41:46 @agent_ppo2.py:145][0m 1806336 total steps have happened
[32m[20221213 20:41:46 @agent_ppo2.py:121][0m #------------------------ Iteration 441 --------------------------#
[32m[20221213 20:41:46 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:47 @agent_ppo2.py:185][0m |           0.0402 |           0.0165 |           0.0000 |
[32m[20221213 20:41:47 @agent_ppo2.py:185][0m |          -0.0043 |           0.0155 |           0.0000 |
[32m[20221213 20:41:47 @agent_ppo2.py:185][0m |          -0.0230 |           0.0152 |           0.0000 |
[32m[20221213 20:41:47 @agent_ppo2.py:185][0m |          -0.0422 |           0.0148 |           0.0000 |
[32m[20221213 20:41:47 @agent_ppo2.py:185][0m |          -0.0470 |           0.0144 |           0.0000 |
[32m[20221213 20:41:47 @agent_ppo2.py:185][0m |          -0.0535 |           0.0144 |           0.0000 |
[32m[20221213 20:41:47 @agent_ppo2.py:185][0m |          -0.0575 |           0.0141 |           0.0000 |
[32m[20221213 20:41:47 @agent_ppo2.py:185][0m |          -0.0619 |           0.0135 |           0.0000 |
[32m[20221213 20:41:47 @agent_ppo2.py:185][0m |          -0.0591 |           0.0134 |           0.0000 |
[32m[20221213 20:41:47 @agent_ppo2.py:185][0m |          -0.0579 |           0.0135 |           0.0000 |
[32m[20221213 20:41:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:41:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.52
[32m[20221213 20:41:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.55
[32m[20221213 20:41:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.75
[32m[20221213 20:41:48 @agent_ppo2.py:143][0m Total time:      12.64 min
[32m[20221213 20:41:48 @agent_ppo2.py:145][0m 1810432 total steps have happened
[32m[20221213 20:41:48 @agent_ppo2.py:121][0m #------------------------ Iteration 442 --------------------------#
[32m[20221213 20:41:48 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:48 @agent_ppo2.py:185][0m |           0.0451 |           0.0286 |           0.0000 |
[32m[20221213 20:41:48 @agent_ppo2.py:185][0m |           0.0037 |           0.0209 |           0.0000 |
[32m[20221213 20:41:48 @agent_ppo2.py:185][0m |          -0.0108 |           0.0196 |           0.0000 |
[32m[20221213 20:41:49 @agent_ppo2.py:185][0m |          -0.0214 |           0.0194 |           0.0000 |
[32m[20221213 20:41:49 @agent_ppo2.py:185][0m |          -0.0236 |           0.0186 |           0.0000 |
[32m[20221213 20:41:49 @agent_ppo2.py:185][0m |          -0.0268 |           0.0184 |           0.0000 |
[32m[20221213 20:41:49 @agent_ppo2.py:185][0m |          -0.0339 |           0.0184 |           0.0000 |
[32m[20221213 20:41:49 @agent_ppo2.py:185][0m |          -0.0365 |           0.0176 |           0.0000 |
[32m[20221213 20:41:49 @agent_ppo2.py:185][0m |          -0.0332 |           0.0175 |           0.0000 |
[32m[20221213 20:41:49 @agent_ppo2.py:185][0m |          -0.0344 |           0.0173 |           0.0000 |
[32m[20221213 20:41:49 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 20:41:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.99
[32m[20221213 20:41:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.46
[32m[20221213 20:41:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.07
[32m[20221213 20:41:50 @agent_ppo2.py:143][0m Total time:      12.67 min
[32m[20221213 20:41:50 @agent_ppo2.py:145][0m 1814528 total steps have happened
[32m[20221213 20:41:50 @agent_ppo2.py:121][0m #------------------------ Iteration 443 --------------------------#
[32m[20221213 20:41:50 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:41:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:50 @agent_ppo2.py:185][0m |           0.0256 |           0.0159 |           0.0000 |
[32m[20221213 20:41:50 @agent_ppo2.py:185][0m |           0.0067 |           0.0128 |           0.0000 |
[32m[20221213 20:41:50 @agent_ppo2.py:185][0m |          -0.0166 |           0.0125 |           0.0000 |
[32m[20221213 20:41:50 @agent_ppo2.py:185][0m |          -0.0381 |           0.0122 |           0.0000 |
[32m[20221213 20:41:51 @agent_ppo2.py:185][0m |          -0.0183 |           0.0121 |           0.0000 |
[32m[20221213 20:41:51 @agent_ppo2.py:185][0m |          -0.0389 |           0.0119 |           0.0000 |
[32m[20221213 20:41:51 @agent_ppo2.py:185][0m |          -0.0500 |           0.0118 |           0.0000 |
[32m[20221213 20:41:51 @agent_ppo2.py:185][0m |          -0.0531 |           0.0117 |           0.0000 |
[32m[20221213 20:41:51 @agent_ppo2.py:185][0m |          -0.0529 |           0.0115 |           0.0000 |
[32m[20221213 20:41:51 @agent_ppo2.py:185][0m |          -0.0581 |           0.0114 |           0.0000 |
[32m[20221213 20:41:51 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 20:41:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.88
[32m[20221213 20:41:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.47
[32m[20221213 20:41:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.45
[32m[20221213 20:41:52 @agent_ppo2.py:143][0m Total time:      12.71 min
[32m[20221213 20:41:52 @agent_ppo2.py:145][0m 1818624 total steps have happened
[32m[20221213 20:41:52 @agent_ppo2.py:121][0m #------------------------ Iteration 444 --------------------------#
[32m[20221213 20:41:52 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:52 @agent_ppo2.py:185][0m |           0.0389 |           0.0343 |           0.0000 |
[32m[20221213 20:41:52 @agent_ppo2.py:185][0m |           0.0024 |           0.0205 |           0.0000 |
[32m[20221213 20:41:52 @agent_ppo2.py:185][0m |          -0.0117 |           0.0180 |           0.0000 |
[32m[20221213 20:41:52 @agent_ppo2.py:185][0m |          -0.0192 |           0.0175 |           0.0000 |
[32m[20221213 20:41:52 @agent_ppo2.py:185][0m |          -0.0134 |           0.0169 |           0.0000 |
[32m[20221213 20:41:53 @agent_ppo2.py:185][0m |          -0.0761 |           0.0174 |           0.0000 |
[32m[20221213 20:41:53 @agent_ppo2.py:185][0m |          -0.0269 |           0.0183 |           0.0000 |
[32m[20221213 20:41:53 @agent_ppo2.py:185][0m |          -0.0321 |           0.0160 |           0.0000 |
[32m[20221213 20:41:53 @agent_ppo2.py:185][0m |          -0.0301 |           0.0159 |           0.0000 |
[32m[20221213 20:41:53 @agent_ppo2.py:185][0m |          -0.0360 |           0.0159 |           0.0000 |
[32m[20221213 20:41:53 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:41:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.48
[32m[20221213 20:41:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.89
[32m[20221213 20:41:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.28
[32m[20221213 20:41:53 @agent_ppo2.py:143][0m Total time:      12.74 min
[32m[20221213 20:41:53 @agent_ppo2.py:145][0m 1822720 total steps have happened
[32m[20221213 20:41:53 @agent_ppo2.py:121][0m #------------------------ Iteration 445 --------------------------#
[32m[20221213 20:41:54 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:54 @agent_ppo2.py:185][0m |           0.0638 |           0.0166 |           0.0000 |
[32m[20221213 20:41:54 @agent_ppo2.py:185][0m |           0.0599 |           0.0148 |           0.0000 |
[32m[20221213 20:41:54 @agent_ppo2.py:185][0m |           0.0373 |           0.0143 |           0.0000 |
[32m[20221213 20:41:54 @agent_ppo2.py:185][0m |          -0.0065 |           0.0140 |           0.0000 |
[32m[20221213 20:41:54 @agent_ppo2.py:185][0m |          -0.0211 |           0.0139 |           0.0000 |
[32m[20221213 20:41:54 @agent_ppo2.py:185][0m |          -0.0281 |           0.0137 |           0.0000 |
[32m[20221213 20:41:54 @agent_ppo2.py:185][0m |          -0.0356 |           0.0138 |           0.0000 |
[32m[20221213 20:41:55 @agent_ppo2.py:185][0m |          -0.0399 |           0.0136 |           0.0000 |
[32m[20221213 20:41:55 @agent_ppo2.py:185][0m |          -0.0547 |           0.0135 |           0.0000 |
[32m[20221213 20:41:55 @agent_ppo2.py:185][0m |          -0.0529 |           0.0134 |           0.0000 |
[32m[20221213 20:41:55 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:41:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.52
[32m[20221213 20:41:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.53
[32m[20221213 20:41:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.24
[32m[20221213 20:41:55 @agent_ppo2.py:143][0m Total time:      12.77 min
[32m[20221213 20:41:55 @agent_ppo2.py:145][0m 1826816 total steps have happened
[32m[20221213 20:41:55 @agent_ppo2.py:121][0m #------------------------ Iteration 446 --------------------------#
[32m[20221213 20:41:55 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:41:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:56 @agent_ppo2.py:185][0m |           0.0237 |           0.0144 |           0.0000 |
[32m[20221213 20:41:56 @agent_ppo2.py:185][0m |           0.0252 |           0.0114 |           0.0000 |
[32m[20221213 20:41:56 @agent_ppo2.py:185][0m |          -0.0058 |           0.0111 |           0.0000 |
[32m[20221213 20:41:56 @agent_ppo2.py:185][0m |           0.0055 |           0.0110 |           0.0000 |
[32m[20221213 20:41:56 @agent_ppo2.py:185][0m |          -0.0309 |           0.0110 |           0.0000 |
[32m[20221213 20:41:56 @agent_ppo2.py:185][0m |          -0.0251 |           0.0111 |           0.0000 |
[32m[20221213 20:41:56 @agent_ppo2.py:185][0m |          -0.0390 |           0.0111 |           0.0000 |
[32m[20221213 20:41:56 @agent_ppo2.py:185][0m |          -0.0474 |           0.0106 |           0.0000 |
[32m[20221213 20:41:56 @agent_ppo2.py:185][0m |          -0.0268 |           0.0111 |           0.0000 |
[32m[20221213 20:41:57 @agent_ppo2.py:185][0m |          -0.0521 |           0.0111 |           0.0000 |
[32m[20221213 20:41:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:41:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.93
[32m[20221213 20:41:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.39
[32m[20221213 20:41:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.61
[32m[20221213 20:41:57 @agent_ppo2.py:143][0m Total time:      12.80 min
[32m[20221213 20:41:57 @agent_ppo2.py:145][0m 1830912 total steps have happened
[32m[20221213 20:41:57 @agent_ppo2.py:121][0m #------------------------ Iteration 447 --------------------------#
[32m[20221213 20:41:57 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:41:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:57 @agent_ppo2.py:185][0m |           0.0363 |           0.0123 |           0.0000 |
[32m[20221213 20:41:58 @agent_ppo2.py:185][0m |           0.0092 |           0.0091 |           0.0000 |
[32m[20221213 20:41:58 @agent_ppo2.py:185][0m |           0.0199 |           0.0091 |           0.0000 |
[32m[20221213 20:41:58 @agent_ppo2.py:185][0m |          -0.0067 |           0.0088 |           0.0000 |
[32m[20221213 20:41:58 @agent_ppo2.py:185][0m |          -0.0127 |           0.0087 |           0.0000 |
[32m[20221213 20:41:58 @agent_ppo2.py:185][0m |          -0.0257 |           0.0086 |           0.0000 |
[32m[20221213 20:41:58 @agent_ppo2.py:185][0m |          -0.0320 |           0.0085 |           0.0000 |
[32m[20221213 20:41:58 @agent_ppo2.py:185][0m |          -0.0340 |           0.0084 |           0.0000 |
[32m[20221213 20:41:58 @agent_ppo2.py:185][0m |          -0.0383 |           0.0084 |           0.0000 |
[32m[20221213 20:41:58 @agent_ppo2.py:185][0m |          -0.0413 |           0.0083 |           0.0000 |
[32m[20221213 20:41:58 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:41:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.43
[32m[20221213 20:41:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.76
[32m[20221213 20:41:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.14
[32m[20221213 20:41:59 @agent_ppo2.py:143][0m Total time:      12.82 min
[32m[20221213 20:41:59 @agent_ppo2.py:145][0m 1835008 total steps have happened
[32m[20221213 20:41:59 @agent_ppo2.py:121][0m #------------------------ Iteration 448 --------------------------#
[32m[20221213 20:41:59 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:41:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:41:59 @agent_ppo2.py:185][0m |           0.0181 |           0.0413 |           0.0000 |
[32m[20221213 20:41:59 @agent_ppo2.py:185][0m |          -0.0046 |           0.0172 |           0.0000 |
[32m[20221213 20:41:59 @agent_ppo2.py:185][0m |          -0.0166 |           0.0152 |           0.0000 |
[32m[20221213 20:41:59 @agent_ppo2.py:185][0m |          -0.0221 |           0.0140 |           0.0000 |
[32m[20221213 20:42:00 @agent_ppo2.py:185][0m |          -0.0269 |           0.0141 |           0.0000 |
[32m[20221213 20:42:00 @agent_ppo2.py:185][0m |          -0.0291 |           0.0135 |           0.0000 |
[32m[20221213 20:42:00 @agent_ppo2.py:185][0m |          -0.0301 |           0.0129 |           0.0000 |
[32m[20221213 20:42:00 @agent_ppo2.py:185][0m |          -0.0326 |           0.0129 |           0.0000 |
[32m[20221213 20:42:00 @agent_ppo2.py:185][0m |          -0.0319 |           0.0130 |           0.0000 |
[32m[20221213 20:42:00 @agent_ppo2.py:185][0m |          -0.0334 |           0.0127 |           0.0000 |
[32m[20221213 20:42:00 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:42:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.12
[32m[20221213 20:42:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.87
[32m[20221213 20:42:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.82
[32m[20221213 20:42:00 @agent_ppo2.py:143][0m Total time:      12.85 min
[32m[20221213 20:42:00 @agent_ppo2.py:145][0m 1839104 total steps have happened
[32m[20221213 20:42:00 @agent_ppo2.py:121][0m #------------------------ Iteration 449 --------------------------#
[32m[20221213 20:42:01 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:42:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:01 @agent_ppo2.py:185][0m |           0.0232 |           0.0152 |           0.0000 |
[32m[20221213 20:42:01 @agent_ppo2.py:185][0m |          -0.0087 |           0.0149 |           0.0000 |
[32m[20221213 20:42:01 @agent_ppo2.py:185][0m |          -0.0342 |           0.0146 |           0.0000 |
[32m[20221213 20:42:01 @agent_ppo2.py:185][0m |          -0.0507 |           0.0144 |           0.0000 |
[32m[20221213 20:42:01 @agent_ppo2.py:185][0m |          -0.0529 |           0.0138 |           0.0000 |
[32m[20221213 20:42:01 @agent_ppo2.py:185][0m |          -0.0486 |           0.0136 |           0.0000 |
[32m[20221213 20:42:01 @agent_ppo2.py:185][0m |          -0.0716 |           0.0143 |           0.0000 |
[32m[20221213 20:42:02 @agent_ppo2.py:185][0m |          -0.0696 |           0.0138 |           0.0000 |
[32m[20221213 20:42:02 @agent_ppo2.py:185][0m |          -0.0750 |           0.0135 |           0.0000 |
[32m[20221213 20:42:02 @agent_ppo2.py:185][0m |          -0.0734 |           0.0131 |           0.0000 |
[32m[20221213 20:42:02 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:42:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.99
[32m[20221213 20:42:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.55
[32m[20221213 20:42:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.88
[32m[20221213 20:42:02 @agent_ppo2.py:143][0m Total time:      12.88 min
[32m[20221213 20:42:02 @agent_ppo2.py:145][0m 1843200 total steps have happened
[32m[20221213 20:42:02 @agent_ppo2.py:121][0m #------------------------ Iteration 450 --------------------------#
[32m[20221213 20:42:03 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:42:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:03 @agent_ppo2.py:185][0m |           0.0318 |           0.0151 |           0.0000 |
[32m[20221213 20:42:03 @agent_ppo2.py:185][0m |          -0.0001 |           0.0142 |           0.0000 |
[32m[20221213 20:42:03 @agent_ppo2.py:185][0m |          -0.0152 |           0.0135 |           0.0000 |
[32m[20221213 20:42:03 @agent_ppo2.py:185][0m |          -0.0251 |           0.0128 |           0.0000 |
[32m[20221213 20:42:03 @agent_ppo2.py:185][0m |          -0.0369 |           0.0130 |           0.0000 |
[32m[20221213 20:42:03 @agent_ppo2.py:185][0m |          -0.0484 |           0.0128 |           0.0000 |
[32m[20221213 20:42:03 @agent_ppo2.py:185][0m |          -0.0510 |           0.0126 |           0.0000 |
[32m[20221213 20:42:03 @agent_ppo2.py:185][0m |          -0.0540 |           0.0123 |           0.0000 |
[32m[20221213 20:42:03 @agent_ppo2.py:185][0m |          -0.1115 |           0.0138 |           0.0000 |
[32m[20221213 20:42:04 @agent_ppo2.py:185][0m |          -0.0501 |           0.0153 |           0.0000 |
[32m[20221213 20:42:04 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:42:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.64
[32m[20221213 20:42:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.51
[32m[20221213 20:42:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.56
[32m[20221213 20:42:04 @agent_ppo2.py:143][0m Total time:      12.91 min
[32m[20221213 20:42:04 @agent_ppo2.py:145][0m 1847296 total steps have happened
[32m[20221213 20:42:04 @agent_ppo2.py:121][0m #------------------------ Iteration 451 --------------------------#
[32m[20221213 20:42:04 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:04 @agent_ppo2.py:185][0m |           0.0487 |           0.0127 |           0.0000 |
[32m[20221213 20:42:05 @agent_ppo2.py:185][0m |           0.0362 |           0.0119 |           0.0000 |
[32m[20221213 20:42:05 @agent_ppo2.py:185][0m |          -0.0012 |           0.0116 |           0.0000 |
[32m[20221213 20:42:05 @agent_ppo2.py:185][0m |          -0.0160 |           0.0116 |           0.0000 |
[32m[20221213 20:42:05 @agent_ppo2.py:185][0m |          -0.0320 |           0.0116 |           0.0000 |
[32m[20221213 20:42:05 @agent_ppo2.py:185][0m |          -0.0432 |           0.0113 |           0.0000 |
[32m[20221213 20:42:05 @agent_ppo2.py:185][0m |          -0.0570 |           0.0112 |           0.0000 |
[32m[20221213 20:42:05 @agent_ppo2.py:185][0m |          -0.0557 |           0.0113 |           0.0000 |
[32m[20221213 20:42:05 @agent_ppo2.py:185][0m |          -0.0616 |           0.0112 |           0.0000 |
[32m[20221213 20:42:05 @agent_ppo2.py:185][0m |          -0.0483 |           0.0112 |           0.0000 |
[32m[20221213 20:42:05 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:42:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.85
[32m[20221213 20:42:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.47
[32m[20221213 20:42:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.01
[32m[20221213 20:42:06 @agent_ppo2.py:143][0m Total time:      12.94 min
[32m[20221213 20:42:06 @agent_ppo2.py:145][0m 1851392 total steps have happened
[32m[20221213 20:42:06 @agent_ppo2.py:121][0m #------------------------ Iteration 452 --------------------------#
[32m[20221213 20:42:06 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:42:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:06 @agent_ppo2.py:185][0m |           0.0184 |           0.0113 |           0.0000 |
[32m[20221213 20:42:06 @agent_ppo2.py:185][0m |           0.0049 |           0.0110 |           0.0000 |
[32m[20221213 20:42:06 @agent_ppo2.py:185][0m |          -0.0267 |           0.0108 |           0.0000 |
[32m[20221213 20:42:06 @agent_ppo2.py:185][0m |          -0.0426 |           0.0106 |           0.0000 |
[32m[20221213 20:42:06 @agent_ppo2.py:185][0m |          -0.0568 |           0.0107 |           0.0000 |
[32m[20221213 20:42:07 @agent_ppo2.py:185][0m |          -0.0584 |           0.0105 |           0.0000 |
[32m[20221213 20:42:07 @agent_ppo2.py:185][0m |          -0.0714 |           0.0104 |           0.0000 |
[32m[20221213 20:42:07 @agent_ppo2.py:185][0m |          -0.0741 |           0.0102 |           0.0000 |
[32m[20221213 20:42:07 @agent_ppo2.py:185][0m |          -0.0670 |           0.0101 |           0.0000 |
[32m[20221213 20:42:07 @agent_ppo2.py:185][0m |          -0.0734 |           0.0101 |           0.0000 |
[32m[20221213 20:42:07 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:42:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.55
[32m[20221213 20:42:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.90
[32m[20221213 20:42:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.94
[32m[20221213 20:42:07 @agent_ppo2.py:143][0m Total time:      12.97 min
[32m[20221213 20:42:07 @agent_ppo2.py:145][0m 1855488 total steps have happened
[32m[20221213 20:42:07 @agent_ppo2.py:121][0m #------------------------ Iteration 453 --------------------------#
[32m[20221213 20:42:08 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:42:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:08 @agent_ppo2.py:185][0m |           0.0113 |           0.0129 |           0.0000 |
[32m[20221213 20:42:08 @agent_ppo2.py:185][0m |          -0.0053 |           0.0093 |           0.0000 |
[32m[20221213 20:42:08 @agent_ppo2.py:185][0m |          -0.0079 |           0.0095 |           0.0000 |
[32m[20221213 20:42:08 @agent_ppo2.py:185][0m |          -0.0312 |           0.0090 |           0.0000 |
[32m[20221213 20:42:08 @agent_ppo2.py:185][0m |          -0.0356 |           0.0087 |           0.0000 |
[32m[20221213 20:42:08 @agent_ppo2.py:185][0m |          -0.0369 |           0.0087 |           0.0000 |
[32m[20221213 20:42:08 @agent_ppo2.py:185][0m |          -0.0470 |           0.0086 |           0.0000 |
[32m[20221213 20:42:08 @agent_ppo2.py:185][0m |          -0.0482 |           0.0084 |           0.0000 |
[32m[20221213 20:42:08 @agent_ppo2.py:185][0m |          -0.0538 |           0.0084 |           0.0000 |
[32m[20221213 20:42:09 @agent_ppo2.py:185][0m |          -0.0522 |           0.0083 |           0.0000 |
[32m[20221213 20:42:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:42:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.09
[32m[20221213 20:42:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.38
[32m[20221213 20:42:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.89
[32m[20221213 20:42:09 @agent_ppo2.py:143][0m Total time:      13.00 min
[32m[20221213 20:42:09 @agent_ppo2.py:145][0m 1859584 total steps have happened
[32m[20221213 20:42:09 @agent_ppo2.py:121][0m #------------------------ Iteration 454 --------------------------#
[32m[20221213 20:42:09 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:10 @agent_ppo2.py:185][0m |           0.0178 |           0.0451 |           0.0000 |
[32m[20221213 20:42:10 @agent_ppo2.py:185][0m |           0.0071 |           0.0209 |           0.0000 |
[32m[20221213 20:42:10 @agent_ppo2.py:185][0m |          -0.0084 |           0.0166 |           0.0000 |
[32m[20221213 20:42:10 @agent_ppo2.py:185][0m |          -0.0043 |           0.0155 |           0.0000 |
[32m[20221213 20:42:10 @agent_ppo2.py:185][0m |          -0.0152 |           0.0149 |           0.0000 |
[32m[20221213 20:42:10 @agent_ppo2.py:185][0m |          -0.0171 |           0.0147 |           0.0000 |
[32m[20221213 20:42:10 @agent_ppo2.py:185][0m |          -0.0190 |           0.0144 |           0.0000 |
[32m[20221213 20:42:10 @agent_ppo2.py:185][0m |          -0.0346 |           0.0140 |           0.0000 |
[32m[20221213 20:42:10 @agent_ppo2.py:185][0m |          -0.0429 |           0.0149 |           0.0000 |
[32m[20221213 20:42:10 @agent_ppo2.py:185][0m |          -0.0269 |           0.0160 |           0.0000 |
[32m[20221213 20:42:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:42:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.21
[32m[20221213 20:42:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.36
[32m[20221213 20:42:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.47
[32m[20221213 20:42:11 @agent_ppo2.py:143][0m Total time:      13.02 min
[32m[20221213 20:42:11 @agent_ppo2.py:145][0m 1863680 total steps have happened
[32m[20221213 20:42:11 @agent_ppo2.py:121][0m #------------------------ Iteration 455 --------------------------#
[32m[20221213 20:42:11 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:11 @agent_ppo2.py:185][0m |           0.0800 |           0.0237 |           0.0000 |
[32m[20221213 20:42:11 @agent_ppo2.py:185][0m |           0.0163 |           0.0139 |           0.0000 |
[32m[20221213 20:42:11 @agent_ppo2.py:185][0m |          -0.0055 |           0.0135 |           0.0000 |
[32m[20221213 20:42:11 @agent_ppo2.py:185][0m |          -0.0149 |           0.0127 |           0.0000 |
[32m[20221213 20:42:12 @agent_ppo2.py:185][0m |          -0.0026 |           0.0125 |           0.0000 |
[32m[20221213 20:42:12 @agent_ppo2.py:185][0m |          -0.0286 |           0.0123 |           0.0000 |
[32m[20221213 20:42:12 @agent_ppo2.py:185][0m |          -0.0285 |           0.0120 |           0.0000 |
[32m[20221213 20:42:12 @agent_ppo2.py:185][0m |          -0.0363 |           0.0125 |           0.0000 |
[32m[20221213 20:42:12 @agent_ppo2.py:185][0m |          -0.0134 |           0.0123 |           0.0000 |
[32m[20221213 20:42:12 @agent_ppo2.py:185][0m |          -0.0434 |           0.0116 |           0.0000 |
[32m[20221213 20:42:12 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:42:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.12
[32m[20221213 20:42:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.53
[32m[20221213 20:42:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.99
[32m[20221213 20:42:12 @agent_ppo2.py:143][0m Total time:      13.05 min
[32m[20221213 20:42:12 @agent_ppo2.py:145][0m 1867776 total steps have happened
[32m[20221213 20:42:12 @agent_ppo2.py:121][0m #------------------------ Iteration 456 --------------------------#
[32m[20221213 20:42:13 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:42:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:13 @agent_ppo2.py:185][0m |           0.0334 |           0.0132 |           0.0000 |
[32m[20221213 20:42:13 @agent_ppo2.py:185][0m |          -0.0213 |           0.0125 |           0.0000 |
[32m[20221213 20:42:13 @agent_ppo2.py:185][0m |          -0.0320 |           0.0124 |           0.0000 |
[32m[20221213 20:42:13 @agent_ppo2.py:185][0m |          -0.0502 |           0.0120 |           0.0000 |
[32m[20221213 20:42:13 @agent_ppo2.py:185][0m |          -0.0633 |           0.0117 |           0.0000 |
[32m[20221213 20:42:13 @agent_ppo2.py:185][0m |          -0.0597 |           0.0116 |           0.0000 |
[32m[20221213 20:42:13 @agent_ppo2.py:185][0m |          -0.0621 |           0.0114 |           0.0000 |
[32m[20221213 20:42:13 @agent_ppo2.py:185][0m |          -0.0719 |           0.0114 |           0.0000 |
[32m[20221213 20:42:14 @agent_ppo2.py:185][0m |          -0.0748 |           0.0112 |           0.0000 |
[32m[20221213 20:42:14 @agent_ppo2.py:185][0m |          -0.0873 |           0.0111 |           0.0000 |
[32m[20221213 20:42:14 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:42:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.49
[32m[20221213 20:42:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.92
[32m[20221213 20:42:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.29
[32m[20221213 20:42:14 @agent_ppo2.py:143][0m Total time:      13.08 min
[32m[20221213 20:42:14 @agent_ppo2.py:145][0m 1871872 total steps have happened
[32m[20221213 20:42:14 @agent_ppo2.py:121][0m #------------------------ Iteration 457 --------------------------#
[32m[20221213 20:42:14 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:15 @agent_ppo2.py:185][0m |           0.0417 |           0.0113 |           0.0000 |
[32m[20221213 20:42:15 @agent_ppo2.py:185][0m |           0.0004 |           0.0105 |           0.0000 |
[32m[20221213 20:42:15 @agent_ppo2.py:185][0m |          -0.0191 |           0.0102 |           0.0000 |
[32m[20221213 20:42:15 @agent_ppo2.py:185][0m |          -0.0381 |           0.0100 |           0.0000 |
[32m[20221213 20:42:15 @agent_ppo2.py:185][0m |          -0.0491 |           0.0099 |           0.0000 |
[32m[20221213 20:42:15 @agent_ppo2.py:185][0m |          -0.0504 |           0.0097 |           0.0000 |
[32m[20221213 20:42:15 @agent_ppo2.py:185][0m |          -0.0601 |           0.0096 |           0.0000 |
[32m[20221213 20:42:15 @agent_ppo2.py:185][0m |          -0.0586 |           0.0095 |           0.0000 |
[32m[20221213 20:42:15 @agent_ppo2.py:185][0m |          -0.0465 |           0.0094 |           0.0000 |
[32m[20221213 20:42:15 @agent_ppo2.py:185][0m |          -0.0483 |           0.0093 |           0.0000 |
[32m[20221213 20:42:15 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:42:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.74
[32m[20221213 20:42:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.26
[32m[20221213 20:42:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.15
[32m[20221213 20:42:16 @agent_ppo2.py:143][0m Total time:      13.11 min
[32m[20221213 20:42:16 @agent_ppo2.py:145][0m 1875968 total steps have happened
[32m[20221213 20:42:16 @agent_ppo2.py:121][0m #------------------------ Iteration 458 --------------------------#
[32m[20221213 20:42:16 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:16 @agent_ppo2.py:185][0m |           0.0221 |           0.0344 |           0.0000 |
[32m[20221213 20:42:16 @agent_ppo2.py:185][0m |          -0.0346 |           0.0171 |           0.0000 |
[32m[20221213 20:42:16 @agent_ppo2.py:185][0m |          -0.0121 |           0.0172 |           0.0000 |
[32m[20221213 20:42:16 @agent_ppo2.py:185][0m |          -0.0218 |           0.0143 |           0.0000 |
[32m[20221213 20:42:17 @agent_ppo2.py:185][0m |          -0.0167 |           0.0136 |           0.0000 |
[32m[20221213 20:42:17 @agent_ppo2.py:185][0m |          -0.0191 |           0.0134 |           0.0000 |
[32m[20221213 20:42:17 @agent_ppo2.py:185][0m |          -0.0257 |           0.0133 |           0.0000 |
[32m[20221213 20:42:17 @agent_ppo2.py:185][0m |          -0.0288 |           0.0131 |           0.0000 |
[32m[20221213 20:42:17 @agent_ppo2.py:185][0m |          -0.0309 |           0.0132 |           0.0000 |
[32m[20221213 20:42:17 @agent_ppo2.py:185][0m |          -0.0320 |           0.0127 |           0.0000 |
[32m[20221213 20:42:17 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:42:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.54
[32m[20221213 20:42:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.13
[32m[20221213 20:42:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.12
[32m[20221213 20:42:17 @agent_ppo2.py:143][0m Total time:      13.14 min
[32m[20221213 20:42:17 @agent_ppo2.py:145][0m 1880064 total steps have happened
[32m[20221213 20:42:17 @agent_ppo2.py:121][0m #------------------------ Iteration 459 --------------------------#
[32m[20221213 20:42:18 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:18 @agent_ppo2.py:185][0m |           0.0577 |           0.0218 |           0.0000 |
[32m[20221213 20:42:18 @agent_ppo2.py:185][0m |           0.0196 |           0.0092 |           0.0000 |
[32m[20221213 20:42:18 @agent_ppo2.py:185][0m |           0.0083 |           0.0086 |           0.0000 |
[32m[20221213 20:42:18 @agent_ppo2.py:185][0m |           0.0059 |           0.0085 |           0.0000 |
[32m[20221213 20:42:18 @agent_ppo2.py:185][0m |          -0.0054 |           0.0084 |           0.0000 |
[32m[20221213 20:42:18 @agent_ppo2.py:185][0m |          -0.0094 |           0.0083 |           0.0000 |
[32m[20221213 20:42:18 @agent_ppo2.py:185][0m |          -0.0151 |           0.0081 |           0.0000 |
[32m[20221213 20:42:18 @agent_ppo2.py:185][0m |          -0.0194 |           0.0081 |           0.0000 |
[32m[20221213 20:42:18 @agent_ppo2.py:185][0m |          -0.0173 |           0.0080 |           0.0000 |
[32m[20221213 20:42:19 @agent_ppo2.py:185][0m |          -0.0214 |           0.0080 |           0.0000 |
[32m[20221213 20:42:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:42:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.33
[32m[20221213 20:42:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.95
[32m[20221213 20:42:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.25
[32m[20221213 20:42:19 @agent_ppo2.py:143][0m Total time:      13.16 min
[32m[20221213 20:42:19 @agent_ppo2.py:145][0m 1884160 total steps have happened
[32m[20221213 20:42:19 @agent_ppo2.py:121][0m #------------------------ Iteration 460 --------------------------#
[32m[20221213 20:42:19 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:42:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:20 @agent_ppo2.py:185][0m |           0.0176 |           0.0095 |           0.0000 |
[32m[20221213 20:42:20 @agent_ppo2.py:185][0m |          -0.0109 |           0.0089 |           0.0000 |
[32m[20221213 20:42:20 @agent_ppo2.py:185][0m |          -0.0837 |           0.0097 |           0.0000 |
[32m[20221213 20:42:20 @agent_ppo2.py:185][0m |          -0.0352 |           0.0090 |           0.0000 |
[32m[20221213 20:42:20 @agent_ppo2.py:185][0m |          -0.0341 |           0.0085 |           0.0000 |
[32m[20221213 20:42:20 @agent_ppo2.py:185][0m |          -0.0391 |           0.0085 |           0.0000 |
[32m[20221213 20:42:20 @agent_ppo2.py:185][0m |          -0.0380 |           0.0084 |           0.0000 |
[32m[20221213 20:42:20 @agent_ppo2.py:185][0m |          -0.0456 |           0.0084 |           0.0000 |
[32m[20221213 20:42:20 @agent_ppo2.py:185][0m |          -0.0471 |           0.0083 |           0.0000 |
[32m[20221213 20:42:20 @agent_ppo2.py:185][0m |          -0.0465 |           0.0082 |           0.0000 |
[32m[20221213 20:42:20 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:42:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.29
[32m[20221213 20:42:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.78
[32m[20221213 20:42:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.55
[32m[20221213 20:42:21 @agent_ppo2.py:143][0m Total time:      13.19 min
[32m[20221213 20:42:21 @agent_ppo2.py:145][0m 1888256 total steps have happened
[32m[20221213 20:42:21 @agent_ppo2.py:121][0m #------------------------ Iteration 461 --------------------------#
[32m[20221213 20:42:21 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:21 @agent_ppo2.py:185][0m |           0.0170 |           0.0086 |           0.0000 |
[32m[20221213 20:42:21 @agent_ppo2.py:185][0m |          -0.0075 |           0.0082 |           0.0000 |
[32m[20221213 20:42:21 @agent_ppo2.py:185][0m |          -0.0273 |           0.0081 |           0.0000 |
[32m[20221213 20:42:22 @agent_ppo2.py:185][0m |          -0.0376 |           0.0080 |           0.0000 |
[32m[20221213 20:42:22 @agent_ppo2.py:185][0m |          -0.0242 |           0.0080 |           0.0000 |
[32m[20221213 20:42:22 @agent_ppo2.py:185][0m |          -0.0509 |           0.0086 |           0.0000 |
[32m[20221213 20:42:22 @agent_ppo2.py:185][0m |          -0.0436 |           0.0082 |           0.0000 |
[32m[20221213 20:42:22 @agent_ppo2.py:185][0m |          -0.0547 |           0.0082 |           0.0000 |
[32m[20221213 20:42:22 @agent_ppo2.py:185][0m |          -0.0526 |           0.0081 |           0.0000 |
[32m[20221213 20:42:22 @agent_ppo2.py:185][0m |          -0.0229 |           0.0080 |           0.0000 |
[32m[20221213 20:42:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:42:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.48
[32m[20221213 20:42:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.99
[32m[20221213 20:42:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.19
[32m[20221213 20:42:22 @agent_ppo2.py:143][0m Total time:      13.22 min
[32m[20221213 20:42:22 @agent_ppo2.py:145][0m 1892352 total steps have happened
[32m[20221213 20:42:23 @agent_ppo2.py:121][0m #------------------------ Iteration 462 --------------------------#
[32m[20221213 20:42:23 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:23 @agent_ppo2.py:185][0m |           0.0270 |           0.0784 |           0.0000 |
[32m[20221213 20:42:23 @agent_ppo2.py:185][0m |           0.0080 |           0.0276 |           0.0000 |
[32m[20221213 20:42:23 @agent_ppo2.py:185][0m |          -0.0133 |           0.0218 |           0.0000 |
[32m[20221213 20:42:23 @agent_ppo2.py:185][0m |          -0.0411 |           0.0197 |           0.0000 |
[32m[20221213 20:42:23 @agent_ppo2.py:185][0m |          -0.0247 |           0.0196 |           0.0000 |
[32m[20221213 20:42:23 @agent_ppo2.py:185][0m |          -0.0237 |           0.0183 |           0.0000 |
[32m[20221213 20:42:24 @agent_ppo2.py:185][0m |          -0.0247 |           0.0173 |           0.0000 |
[32m[20221213 20:42:24 @agent_ppo2.py:185][0m |          -0.0184 |           0.0170 |           0.0000 |
[32m[20221213 20:42:24 @agent_ppo2.py:185][0m |          -0.0248 |           0.0166 |           0.0000 |
[32m[20221213 20:42:24 @agent_ppo2.py:185][0m |          -0.0278 |           0.0168 |           0.0000 |
[32m[20221213 20:42:24 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:42:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.99
[32m[20221213 20:42:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.16
[32m[20221213 20:42:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.31
[32m[20221213 20:42:24 @agent_ppo2.py:143][0m Total time:      13.25 min
[32m[20221213 20:42:24 @agent_ppo2.py:145][0m 1896448 total steps have happened
[32m[20221213 20:42:24 @agent_ppo2.py:121][0m #------------------------ Iteration 463 --------------------------#
[32m[20221213 20:42:25 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:25 @agent_ppo2.py:185][0m |           0.0173 |           0.0255 |           0.0000 |
[32m[20221213 20:42:25 @agent_ppo2.py:185][0m |          -0.0052 |           0.0110 |           0.0000 |
[32m[20221213 20:42:25 @agent_ppo2.py:185][0m |          -0.0190 |           0.0106 |           0.0000 |
[32m[20221213 20:42:25 @agent_ppo2.py:185][0m |          -0.0309 |           0.0102 |           0.0000 |
[32m[20221213 20:42:25 @agent_ppo2.py:185][0m |          -0.0424 |           0.0100 |           0.0000 |
[32m[20221213 20:42:25 @agent_ppo2.py:185][0m |          -0.0412 |           0.0099 |           0.0000 |
[32m[20221213 20:42:25 @agent_ppo2.py:185][0m |          -0.0429 |           0.0098 |           0.0000 |
[32m[20221213 20:42:25 @agent_ppo2.py:185][0m |          -0.0450 |           0.0097 |           0.0000 |
[32m[20221213 20:42:25 @agent_ppo2.py:185][0m |          -0.0483 |           0.0096 |           0.0000 |
[32m[20221213 20:42:25 @agent_ppo2.py:185][0m |          -0.0520 |           0.0095 |           0.0000 |
[32m[20221213 20:42:25 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:42:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.04
[32m[20221213 20:42:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.65
[32m[20221213 20:42:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.98
[32m[20221213 20:42:26 @agent_ppo2.py:143][0m Total time:      13.28 min
[32m[20221213 20:42:26 @agent_ppo2.py:145][0m 1900544 total steps have happened
[32m[20221213 20:42:26 @agent_ppo2.py:121][0m #------------------------ Iteration 464 --------------------------#
[32m[20221213 20:42:26 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:42:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:26 @agent_ppo2.py:185][0m |           0.0423 |           0.1132 |           0.0000 |
[32m[20221213 20:42:26 @agent_ppo2.py:185][0m |           0.0256 |           0.0379 |           0.0000 |
[32m[20221213 20:42:27 @agent_ppo2.py:185][0m |           0.0109 |           0.0262 |           0.0000 |
[32m[20221213 20:42:27 @agent_ppo2.py:185][0m |          -0.0010 |           0.0239 |           0.0000 |
[32m[20221213 20:42:27 @agent_ppo2.py:185][0m |          -0.0151 |           0.0257 |           0.0000 |
[32m[20221213 20:42:27 @agent_ppo2.py:185][0m |          -0.0479 |           0.0227 |           0.0000 |
[32m[20221213 20:42:27 @agent_ppo2.py:185][0m |          -0.0243 |           0.0242 |           0.0000 |
[32m[20221213 20:42:27 @agent_ppo2.py:185][0m |          -0.0279 |           0.0201 |           0.0000 |
[32m[20221213 20:42:27 @agent_ppo2.py:185][0m |          -0.0287 |           0.0203 |           0.0000 |
[32m[20221213 20:42:27 @agent_ppo2.py:185][0m |          -0.0287 |           0.0191 |           0.0000 |
[32m[20221213 20:42:27 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:42:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.85
[32m[20221213 20:42:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.68
[32m[20221213 20:42:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.44
[32m[20221213 20:42:28 @agent_ppo2.py:143][0m Total time:      13.31 min
[32m[20221213 20:42:28 @agent_ppo2.py:145][0m 1904640 total steps have happened
[32m[20221213 20:42:28 @agent_ppo2.py:121][0m #------------------------ Iteration 465 --------------------------#
[32m[20221213 20:42:28 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:28 @agent_ppo2.py:185][0m |           0.0986 |           0.0461 |           0.0000 |
[32m[20221213 20:42:28 @agent_ppo2.py:185][0m |           0.0282 |           0.0392 |           0.0000 |
[32m[20221213 20:42:28 @agent_ppo2.py:185][0m |          -0.0020 |           0.0285 |           0.0000 |
[32m[20221213 20:42:28 @agent_ppo2.py:185][0m |          -0.0098 |           0.0282 |           0.0000 |
[32m[20221213 20:42:28 @agent_ppo2.py:185][0m |          -0.0323 |           0.0273 |           0.0000 |
[32m[20221213 20:42:28 @agent_ppo2.py:185][0m |          -0.0486 |           0.0259 |           0.0000 |
[32m[20221213 20:42:29 @agent_ppo2.py:185][0m |          -0.0501 |           0.0257 |           0.0000 |
[32m[20221213 20:42:29 @agent_ppo2.py:185][0m |          -0.0526 |           0.0251 |           0.0000 |
[32m[20221213 20:42:29 @agent_ppo2.py:185][0m |          -0.0543 |           0.0242 |           0.0000 |
[32m[20221213 20:42:29 @agent_ppo2.py:185][0m |          -0.0615 |           0.0247 |           0.0000 |
[32m[20221213 20:42:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:42:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.04
[32m[20221213 20:42:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.54
[32m[20221213 20:42:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.18
[32m[20221213 20:42:29 @agent_ppo2.py:143][0m Total time:      13.33 min
[32m[20221213 20:42:29 @agent_ppo2.py:145][0m 1908736 total steps have happened
[32m[20221213 20:42:29 @agent_ppo2.py:121][0m #------------------------ Iteration 466 --------------------------#
[32m[20221213 20:42:30 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:30 @agent_ppo2.py:185][0m |           0.0375 |           0.0251 |           0.0000 |
[32m[20221213 20:42:30 @agent_ppo2.py:185][0m |          -0.0045 |           0.0224 |           0.0000 |
[32m[20221213 20:42:30 @agent_ppo2.py:185][0m |          -0.0120 |           0.0212 |           0.0000 |
[32m[20221213 20:42:30 @agent_ppo2.py:185][0m |          -0.0483 |           0.0202 |           0.0000 |
[32m[20221213 20:42:30 @agent_ppo2.py:185][0m |          -0.0593 |           0.0197 |           0.0000 |
[32m[20221213 20:42:30 @agent_ppo2.py:185][0m |          -0.0774 |           0.0195 |           0.0000 |
[32m[20221213 20:42:30 @agent_ppo2.py:185][0m |          -0.0715 |           0.0191 |           0.0000 |
[32m[20221213 20:42:30 @agent_ppo2.py:185][0m |          -0.0762 |           0.0189 |           0.0000 |
[32m[20221213 20:42:30 @agent_ppo2.py:185][0m |          -0.0835 |           0.0185 |           0.0000 |
[32m[20221213 20:42:30 @agent_ppo2.py:185][0m |          -0.0829 |           0.0188 |           0.0000 |
[32m[20221213 20:42:30 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:42:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.42
[32m[20221213 20:42:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.50
[32m[20221213 20:42:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.64
[32m[20221213 20:42:31 @agent_ppo2.py:143][0m Total time:      13.36 min
[32m[20221213 20:42:31 @agent_ppo2.py:145][0m 1912832 total steps have happened
[32m[20221213 20:42:31 @agent_ppo2.py:121][0m #------------------------ Iteration 467 --------------------------#
[32m[20221213 20:42:31 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:42:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:31 @agent_ppo2.py:185][0m |           0.0304 |           0.0175 |           0.0000 |
[32m[20221213 20:42:32 @agent_ppo2.py:185][0m |          -0.0144 |           0.0159 |           0.0000 |
[32m[20221213 20:42:32 @agent_ppo2.py:185][0m |          -0.0405 |           0.0156 |           0.0000 |
[32m[20221213 20:42:32 @agent_ppo2.py:185][0m |          -0.0576 |           0.0154 |           0.0000 |
[32m[20221213 20:42:32 @agent_ppo2.py:185][0m |          -0.0707 |           0.0150 |           0.0000 |
[32m[20221213 20:42:32 @agent_ppo2.py:185][0m |          -0.0710 |           0.0148 |           0.0000 |
[32m[20221213 20:42:32 @agent_ppo2.py:185][0m |          -0.0776 |           0.0147 |           0.0000 |
[32m[20221213 20:42:32 @agent_ppo2.py:185][0m |          -0.0765 |           0.0147 |           0.0000 |
[32m[20221213 20:42:32 @agent_ppo2.py:185][0m |          -0.0786 |           0.0144 |           0.0000 |
[32m[20221213 20:42:32 @agent_ppo2.py:185][0m |          -0.0786 |           0.0143 |           0.0000 |
[32m[20221213 20:42:32 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:42:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.20
[32m[20221213 20:42:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.04
[32m[20221213 20:42:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.42
[32m[20221213 20:42:33 @agent_ppo2.py:143][0m Total time:      13.39 min
[32m[20221213 20:42:33 @agent_ppo2.py:145][0m 1916928 total steps have happened
[32m[20221213 20:42:33 @agent_ppo2.py:121][0m #------------------------ Iteration 468 --------------------------#
[32m[20221213 20:42:33 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:33 @agent_ppo2.py:185][0m |           0.0095 |           0.0240 |           0.0000 |
[32m[20221213 20:42:33 @agent_ppo2.py:185][0m |           0.0016 |           0.0115 |           0.0000 |
[32m[20221213 20:42:33 @agent_ppo2.py:185][0m |          -0.0147 |           0.0110 |           0.0000 |
[32m[20221213 20:42:33 @agent_ppo2.py:185][0m |          -0.0202 |           0.0110 |           0.0000 |
[32m[20221213 20:42:33 @agent_ppo2.py:185][0m |          -0.0298 |           0.0110 |           0.0000 |
[32m[20221213 20:42:34 @agent_ppo2.py:185][0m |          -0.0348 |           0.0108 |           0.0000 |
[32m[20221213 20:42:34 @agent_ppo2.py:185][0m |          -0.0367 |           0.0108 |           0.0000 |
[32m[20221213 20:42:34 @agent_ppo2.py:185][0m |          -0.0387 |           0.0107 |           0.0000 |
[32m[20221213 20:42:34 @agent_ppo2.py:185][0m |          -0.0160 |           0.0123 |           0.0000 |
[32m[20221213 20:42:34 @agent_ppo2.py:185][0m |          -0.0070 |           0.0137 |           0.0000 |
[32m[20221213 20:42:34 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:42:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.23
[32m[20221213 20:42:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.77
[32m[20221213 20:42:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.99
[32m[20221213 20:42:34 @agent_ppo2.py:143][0m Total time:      13.42 min
[32m[20221213 20:42:34 @agent_ppo2.py:145][0m 1921024 total steps have happened
[32m[20221213 20:42:34 @agent_ppo2.py:121][0m #------------------------ Iteration 469 --------------------------#
[32m[20221213 20:42:35 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:35 @agent_ppo2.py:185][0m |           0.0227 |           0.0256 |           0.0000 |
[32m[20221213 20:42:35 @agent_ppo2.py:185][0m |           0.0151 |           0.0167 |           0.0000 |
[32m[20221213 20:42:35 @agent_ppo2.py:185][0m |          -0.0155 |           0.0155 |           0.0000 |
[32m[20221213 20:42:35 @agent_ppo2.py:185][0m |          -0.0249 |           0.0148 |           0.0000 |
[32m[20221213 20:42:35 @agent_ppo2.py:185][0m |          -0.0188 |           0.0145 |           0.0000 |
[32m[20221213 20:42:35 @agent_ppo2.py:185][0m |          -0.0293 |           0.0141 |           0.0000 |
[32m[20221213 20:42:35 @agent_ppo2.py:185][0m |          -0.0320 |           0.0139 |           0.0000 |
[32m[20221213 20:42:35 @agent_ppo2.py:185][0m |          -0.0313 |           0.0137 |           0.0000 |
[32m[20221213 20:42:35 @agent_ppo2.py:185][0m |          -0.0316 |           0.0136 |           0.0000 |
[32m[20221213 20:42:36 @agent_ppo2.py:185][0m |          -0.0326 |           0.0135 |           0.0000 |
[32m[20221213 20:42:36 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:42:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.70
[32m[20221213 20:42:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.41
[32m[20221213 20:42:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.69
[32m[20221213 20:42:36 @agent_ppo2.py:143][0m Total time:      13.45 min
[32m[20221213 20:42:36 @agent_ppo2.py:145][0m 1925120 total steps have happened
[32m[20221213 20:42:36 @agent_ppo2.py:121][0m #------------------------ Iteration 470 --------------------------#
[32m[20221213 20:42:36 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:42:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:36 @agent_ppo2.py:185][0m |           0.0303 |           0.0152 |           0.0000 |
[32m[20221213 20:42:37 @agent_ppo2.py:185][0m |           0.0113 |           0.0112 |           0.0000 |
[32m[20221213 20:42:37 @agent_ppo2.py:185][0m |          -0.0056 |           0.0109 |           0.0000 |
[32m[20221213 20:42:37 @agent_ppo2.py:185][0m |          -0.0292 |           0.0108 |           0.0000 |
[32m[20221213 20:42:37 @agent_ppo2.py:185][0m |          -0.0295 |           0.0107 |           0.0000 |
[32m[20221213 20:42:37 @agent_ppo2.py:185][0m |          -0.0426 |           0.0105 |           0.0000 |
[32m[20221213 20:42:37 @agent_ppo2.py:185][0m |          -0.0441 |           0.0104 |           0.0000 |
[32m[20221213 20:42:37 @agent_ppo2.py:185][0m |          -0.0486 |           0.0103 |           0.0000 |
[32m[20221213 20:42:37 @agent_ppo2.py:185][0m |          -0.0504 |           0.0103 |           0.0000 |
[32m[20221213 20:42:37 @agent_ppo2.py:185][0m |          -0.0511 |           0.0102 |           0.0000 |
[32m[20221213 20:42:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:42:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.36
[32m[20221213 20:42:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.77
[32m[20221213 20:42:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.98
[32m[20221213 20:42:38 @agent_ppo2.py:143][0m Total time:      13.47 min
[32m[20221213 20:42:38 @agent_ppo2.py:145][0m 1929216 total steps have happened
[32m[20221213 20:42:38 @agent_ppo2.py:121][0m #------------------------ Iteration 471 --------------------------#
[32m[20221213 20:42:38 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:38 @agent_ppo2.py:185][0m |           0.0304 |           0.0339 |           0.0000 |
[32m[20221213 20:42:38 @agent_ppo2.py:185][0m |           0.0068 |           0.0162 |           0.0000 |
[32m[20221213 20:42:38 @agent_ppo2.py:185][0m |          -0.0119 |           0.0146 |           0.0000 |
[32m[20221213 20:42:38 @agent_ppo2.py:185][0m |          -0.0174 |           0.0144 |           0.0000 |
[32m[20221213 20:42:38 @agent_ppo2.py:185][0m |          -0.0259 |           0.0144 |           0.0000 |
[32m[20221213 20:42:39 @agent_ppo2.py:185][0m |          -0.0275 |           0.0149 |           0.0000 |
[32m[20221213 20:42:39 @agent_ppo2.py:185][0m |          -0.0276 |           0.0138 |           0.0000 |
[32m[20221213 20:42:39 @agent_ppo2.py:185][0m |          -0.0325 |           0.0138 |           0.0000 |
[32m[20221213 20:42:39 @agent_ppo2.py:185][0m |          -0.0345 |           0.0135 |           0.0000 |
[32m[20221213 20:42:39 @agent_ppo2.py:185][0m |          -0.0326 |           0.0137 |           0.0000 |
[32m[20221213 20:42:39 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:42:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.41
[32m[20221213 20:42:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.80
[32m[20221213 20:42:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.36
[32m[20221213 20:42:39 @agent_ppo2.py:143][0m Total time:      13.50 min
[32m[20221213 20:42:39 @agent_ppo2.py:145][0m 1933312 total steps have happened
[32m[20221213 20:42:39 @agent_ppo2.py:121][0m #------------------------ Iteration 472 --------------------------#
[32m[20221213 20:42:40 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:40 @agent_ppo2.py:185][0m |           0.0190 |           0.0282 |           0.0000 |
[32m[20221213 20:42:40 @agent_ppo2.py:185][0m |          -0.0034 |           0.0111 |           0.0000 |
[32m[20221213 20:42:40 @agent_ppo2.py:185][0m |          -0.0116 |           0.0105 |           0.0000 |
[32m[20221213 20:42:40 @agent_ppo2.py:185][0m |          -0.0251 |           0.0103 |           0.0000 |
[32m[20221213 20:42:40 @agent_ppo2.py:185][0m |          -0.0013 |           0.0102 |           0.0000 |
[32m[20221213 20:42:40 @agent_ppo2.py:185][0m |          -0.0313 |           0.0100 |           0.0000 |
[32m[20221213 20:42:40 @agent_ppo2.py:185][0m |          -0.0338 |           0.0098 |           0.0000 |
[32m[20221213 20:42:40 @agent_ppo2.py:185][0m |          -0.0081 |           0.0098 |           0.0000 |
[32m[20221213 20:42:40 @agent_ppo2.py:185][0m |          -0.0376 |           0.0097 |           0.0000 |
[32m[20221213 20:42:40 @agent_ppo2.py:185][0m |          -0.0437 |           0.0095 |           0.0000 |
[32m[20221213 20:42:40 @agent_ppo2.py:130][0m Policy update time: 0.86 s
[32m[20221213 20:42:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.89
[32m[20221213 20:42:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.06
[32m[20221213 20:42:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.04
[32m[20221213 20:42:41 @agent_ppo2.py:143][0m Total time:      13.53 min
[32m[20221213 20:42:41 @agent_ppo2.py:145][0m 1937408 total steps have happened
[32m[20221213 20:42:41 @agent_ppo2.py:121][0m #------------------------ Iteration 473 --------------------------#
[32m[20221213 20:42:41 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:41 @agent_ppo2.py:185][0m |           0.0369 |           0.0327 |           0.0000 |
[32m[20221213 20:42:41 @agent_ppo2.py:185][0m |           0.0057 |           0.0149 |           0.0000 |
[32m[20221213 20:42:42 @agent_ppo2.py:185][0m |          -0.0167 |           0.0130 |           0.0000 |
[32m[20221213 20:42:42 @agent_ppo2.py:185][0m |          -0.0613 |           0.0129 |           0.0000 |
[32m[20221213 20:42:42 @agent_ppo2.py:185][0m |          -0.0258 |           0.0135 |           0.0000 |
[32m[20221213 20:42:42 @agent_ppo2.py:185][0m |          -0.0203 |           0.0121 |           0.0000 |
[32m[20221213 20:42:42 @agent_ppo2.py:185][0m |          -0.0247 |           0.0120 |           0.0000 |
[32m[20221213 20:42:42 @agent_ppo2.py:185][0m |          -0.0250 |           0.0120 |           0.0000 |
[32m[20221213 20:42:42 @agent_ppo2.py:185][0m |          -0.0756 |           0.0129 |           0.0000 |
[32m[20221213 20:42:42 @agent_ppo2.py:185][0m |          -0.0306 |           0.0140 |           0.0000 |
[32m[20221213 20:42:42 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:42:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.45
[32m[20221213 20:42:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.98
[32m[20221213 20:42:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.60
[32m[20221213 20:42:43 @agent_ppo2.py:143][0m Total time:      13.56 min
[32m[20221213 20:42:43 @agent_ppo2.py:145][0m 1941504 total steps have happened
[32m[20221213 20:42:43 @agent_ppo2.py:121][0m #------------------------ Iteration 474 --------------------------#
[32m[20221213 20:42:43 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:42:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:43 @agent_ppo2.py:185][0m |           0.0070 |           0.0171 |           0.0000 |
[32m[20221213 20:42:43 @agent_ppo2.py:185][0m |           0.0131 |           0.0154 |           0.0000 |
[32m[20221213 20:42:43 @agent_ppo2.py:185][0m |          -0.0144 |           0.0136 |           0.0000 |
[32m[20221213 20:42:43 @agent_ppo2.py:185][0m |          -0.0262 |           0.0131 |           0.0000 |
[32m[20221213 20:42:44 @agent_ppo2.py:185][0m |          -0.0562 |           0.0132 |           0.0000 |
[32m[20221213 20:42:44 @agent_ppo2.py:185][0m |          -0.0451 |           0.0130 |           0.0000 |
[32m[20221213 20:42:44 @agent_ppo2.py:185][0m |          -0.0473 |           0.0129 |           0.0000 |
[32m[20221213 20:42:44 @agent_ppo2.py:185][0m |          -0.0544 |           0.0129 |           0.0000 |
[32m[20221213 20:42:44 @agent_ppo2.py:185][0m |          -0.0486 |           0.0123 |           0.0000 |
[32m[20221213 20:42:44 @agent_ppo2.py:185][0m |          -0.0557 |           0.0120 |           0.0000 |
[32m[20221213 20:42:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:42:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.52
[32m[20221213 20:42:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.76
[32m[20221213 20:42:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.51
[32m[20221213 20:42:44 @agent_ppo2.py:143][0m Total time:      13.59 min
[32m[20221213 20:42:44 @agent_ppo2.py:145][0m 1945600 total steps have happened
[32m[20221213 20:42:44 @agent_ppo2.py:121][0m #------------------------ Iteration 475 --------------------------#
[32m[20221213 20:42:45 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:45 @agent_ppo2.py:185][0m |           0.0299 |           0.0503 |           0.0000 |
[32m[20221213 20:42:45 @agent_ppo2.py:185][0m |           0.0064 |           0.0221 |           0.0000 |
[32m[20221213 20:42:45 @agent_ppo2.py:185][0m |          -0.0042 |           0.0202 |           0.0000 |
[32m[20221213 20:42:45 @agent_ppo2.py:185][0m |          -0.0135 |           0.0191 |           0.0000 |
[32m[20221213 20:42:45 @agent_ppo2.py:185][0m |          -0.0506 |           0.0194 |           0.0000 |
[32m[20221213 20:42:45 @agent_ppo2.py:185][0m |          -0.0277 |           0.0200 |           0.0000 |
[32m[20221213 20:42:45 @agent_ppo2.py:185][0m |          -0.0290 |           0.0178 |           0.0000 |
[32m[20221213 20:42:45 @agent_ppo2.py:185][0m |          -0.0268 |           0.0183 |           0.0000 |
[32m[20221213 20:42:46 @agent_ppo2.py:185][0m |          -0.0332 |           0.0198 |           0.0000 |
[32m[20221213 20:42:46 @agent_ppo2.py:185][0m |          -0.0324 |           0.0176 |           0.0000 |
[32m[20221213 20:42:46 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:42:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.52
[32m[20221213 20:42:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.03
[32m[20221213 20:42:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.09
[32m[20221213 20:42:46 @agent_ppo2.py:143][0m Total time:      13.61 min
[32m[20221213 20:42:46 @agent_ppo2.py:145][0m 1949696 total steps have happened
[32m[20221213 20:42:46 @agent_ppo2.py:121][0m #------------------------ Iteration 476 --------------------------#
[32m[20221213 20:42:46 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:47 @agent_ppo2.py:185][0m |           0.0336 |           0.0186 |           0.0000 |
[32m[20221213 20:42:47 @agent_ppo2.py:185][0m |           0.0075 |           0.0145 |           0.0000 |
[32m[20221213 20:42:47 @agent_ppo2.py:185][0m |          -0.0195 |           0.0140 |           0.0000 |
[32m[20221213 20:42:47 @agent_ppo2.py:185][0m |          -0.0389 |           0.0139 |           0.0000 |
[32m[20221213 20:42:47 @agent_ppo2.py:185][0m |          -0.0491 |           0.0136 |           0.0000 |
[32m[20221213 20:42:47 @agent_ppo2.py:185][0m |          -0.0512 |           0.0134 |           0.0000 |
[32m[20221213 20:42:47 @agent_ppo2.py:185][0m |          -0.0602 |           0.0131 |           0.0000 |
[32m[20221213 20:42:47 @agent_ppo2.py:185][0m |          -0.0559 |           0.0132 |           0.0000 |
[32m[20221213 20:42:47 @agent_ppo2.py:185][0m |          -0.0785 |           0.0132 |           0.0000 |
[32m[20221213 20:42:47 @agent_ppo2.py:185][0m |          -0.0561 |           0.0131 |           0.0000 |
[32m[20221213 20:42:47 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:42:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.61
[32m[20221213 20:42:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.97
[32m[20221213 20:42:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.58
[32m[20221213 20:42:48 @agent_ppo2.py:143][0m Total time:      13.64 min
[32m[20221213 20:42:48 @agent_ppo2.py:145][0m 1953792 total steps have happened
[32m[20221213 20:42:48 @agent_ppo2.py:121][0m #------------------------ Iteration 477 --------------------------#
[32m[20221213 20:42:48 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:48 @agent_ppo2.py:185][0m |           0.0311 |           0.0263 |           0.0000 |
[32m[20221213 20:42:48 @agent_ppo2.py:185][0m |           0.0094 |           0.0235 |           0.0000 |
[32m[20221213 20:42:48 @agent_ppo2.py:185][0m |          -0.0241 |           0.0229 |           0.0000 |
[32m[20221213 20:42:48 @agent_ppo2.py:185][0m |          -0.0363 |           0.0213 |           0.0000 |
[32m[20221213 20:42:48 @agent_ppo2.py:185][0m |          -0.0351 |           0.0207 |           0.0000 |
[32m[20221213 20:42:49 @agent_ppo2.py:185][0m |          -0.0465 |           0.0206 |           0.0000 |
[32m[20221213 20:42:49 @agent_ppo2.py:185][0m |          -0.0536 |           0.0199 |           0.0000 |
[32m[20221213 20:42:49 @agent_ppo2.py:185][0m |          -0.0602 |           0.0194 |           0.0000 |
[32m[20221213 20:42:49 @agent_ppo2.py:185][0m |          -0.0491 |           0.0189 |           0.0000 |
[32m[20221213 20:42:49 @agent_ppo2.py:185][0m |          -0.0616 |           0.0193 |           0.0000 |
[32m[20221213 20:42:49 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:42:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.08
[32m[20221213 20:42:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.50
[32m[20221213 20:42:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.40
[32m[20221213 20:42:49 @agent_ppo2.py:143][0m Total time:      13.67 min
[32m[20221213 20:42:49 @agent_ppo2.py:145][0m 1957888 total steps have happened
[32m[20221213 20:42:49 @agent_ppo2.py:121][0m #------------------------ Iteration 478 --------------------------#
[32m[20221213 20:42:50 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:50 @agent_ppo2.py:185][0m |           0.0455 |           0.0154 |           0.0000 |
[32m[20221213 20:42:50 @agent_ppo2.py:185][0m |           0.0096 |           0.0099 |           0.0000 |
[32m[20221213 20:42:50 @agent_ppo2.py:185][0m |          -0.0091 |           0.0096 |           0.0000 |
[32m[20221213 20:42:50 @agent_ppo2.py:185][0m |          -0.0210 |           0.0096 |           0.0000 |
[32m[20221213 20:42:50 @agent_ppo2.py:185][0m |          -0.0210 |           0.0096 |           0.0000 |
[32m[20221213 20:42:50 @agent_ppo2.py:185][0m |          -0.0157 |           0.0096 |           0.0000 |
[32m[20221213 20:42:50 @agent_ppo2.py:185][0m |          -0.0439 |           0.0094 |           0.0000 |
[32m[20221213 20:42:50 @agent_ppo2.py:185][0m |          -0.0506 |           0.0091 |           0.0000 |
[32m[20221213 20:42:50 @agent_ppo2.py:185][0m |          -0.0544 |           0.0091 |           0.0000 |
[32m[20221213 20:42:51 @agent_ppo2.py:185][0m |          -0.0547 |           0.0090 |           0.0000 |
[32m[20221213 20:42:51 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:42:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.40
[32m[20221213 20:42:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.66
[32m[20221213 20:42:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.91
[32m[20221213 20:42:51 @agent_ppo2.py:143][0m Total time:      13.70 min
[32m[20221213 20:42:51 @agent_ppo2.py:145][0m 1961984 total steps have happened
[32m[20221213 20:42:51 @agent_ppo2.py:121][0m #------------------------ Iteration 479 --------------------------#
[32m[20221213 20:42:51 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:51 @agent_ppo2.py:185][0m |           0.0243 |           0.0497 |           0.0000 |
[32m[20221213 20:42:52 @agent_ppo2.py:185][0m |           0.0062 |           0.0204 |           0.0000 |
[32m[20221213 20:42:52 @agent_ppo2.py:185][0m |          -0.0059 |           0.0179 |           0.0000 |
[32m[20221213 20:42:52 @agent_ppo2.py:185][0m |          -0.0154 |           0.0167 |           0.0000 |
[32m[20221213 20:42:52 @agent_ppo2.py:185][0m |          -0.0248 |           0.0166 |           0.0000 |
[32m[20221213 20:42:52 @agent_ppo2.py:185][0m |          -0.0262 |           0.0163 |           0.0000 |
[32m[20221213 20:42:52 @agent_ppo2.py:185][0m |          -0.0233 |           0.0157 |           0.0000 |
[32m[20221213 20:42:52 @agent_ppo2.py:185][0m |          -0.0226 |           0.0154 |           0.0000 |
[32m[20221213 20:42:52 @agent_ppo2.py:185][0m |          -0.0263 |           0.0152 |           0.0000 |
[32m[20221213 20:42:52 @agent_ppo2.py:185][0m |          -0.0212 |           0.0150 |           0.0000 |
[32m[20221213 20:42:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:42:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.27
[32m[20221213 20:42:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.92
[32m[20221213 20:42:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.27
[32m[20221213 20:42:53 @agent_ppo2.py:143][0m Total time:      13.72 min
[32m[20221213 20:42:53 @agent_ppo2.py:145][0m 1966080 total steps have happened
[32m[20221213 20:42:53 @agent_ppo2.py:121][0m #------------------------ Iteration 480 --------------------------#
[32m[20221213 20:42:53 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:42:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:53 @agent_ppo2.py:185][0m |           0.0251 |           0.0263 |           0.0000 |
[32m[20221213 20:42:53 @agent_ppo2.py:185][0m |           0.0024 |           0.0097 |           0.0000 |
[32m[20221213 20:42:53 @agent_ppo2.py:185][0m |          -0.0063 |           0.0092 |           0.0000 |
[32m[20221213 20:42:53 @agent_ppo2.py:185][0m |          -0.0163 |           0.0090 |           0.0000 |
[32m[20221213 20:42:54 @agent_ppo2.py:185][0m |          -0.0220 |           0.0090 |           0.0000 |
[32m[20221213 20:42:54 @agent_ppo2.py:185][0m |          -0.0301 |           0.0089 |           0.0000 |
[32m[20221213 20:42:54 @agent_ppo2.py:185][0m |          -0.0320 |           0.0088 |           0.0000 |
[32m[20221213 20:42:54 @agent_ppo2.py:185][0m |          -0.0332 |           0.0088 |           0.0000 |
[32m[20221213 20:42:54 @agent_ppo2.py:185][0m |          -0.0391 |           0.0087 |           0.0000 |
[32m[20221213 20:42:54 @agent_ppo2.py:185][0m |          -0.0392 |           0.0086 |           0.0000 |
[32m[20221213 20:42:54 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:42:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.84
[32m[20221213 20:42:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.01
[32m[20221213 20:42:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.99
[32m[20221213 20:42:54 @agent_ppo2.py:143][0m Total time:      13.75 min
[32m[20221213 20:42:54 @agent_ppo2.py:145][0m 1970176 total steps have happened
[32m[20221213 20:42:54 @agent_ppo2.py:121][0m #------------------------ Iteration 481 --------------------------#
[32m[20221213 20:42:55 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:55 @agent_ppo2.py:185][0m |           0.0101 |           0.0125 |           0.0000 |
[32m[20221213 20:42:55 @agent_ppo2.py:185][0m |          -0.0015 |           0.0111 |           0.0000 |
[32m[20221213 20:42:55 @agent_ppo2.py:185][0m |          -0.0208 |           0.0106 |           0.0000 |
[32m[20221213 20:42:55 @agent_ppo2.py:185][0m |          -0.0277 |           0.0107 |           0.0000 |
[32m[20221213 20:42:55 @agent_ppo2.py:185][0m |          -0.0336 |           0.0104 |           0.0000 |
[32m[20221213 20:42:55 @agent_ppo2.py:185][0m |          -0.0378 |           0.0103 |           0.0000 |
[32m[20221213 20:42:55 @agent_ppo2.py:185][0m |          -0.0409 |           0.0101 |           0.0000 |
[32m[20221213 20:42:55 @agent_ppo2.py:185][0m |          -0.0449 |           0.0100 |           0.0000 |
[32m[20221213 20:42:56 @agent_ppo2.py:185][0m |          -0.0441 |           0.0099 |           0.0000 |
[32m[20221213 20:42:56 @agent_ppo2.py:185][0m |          -0.0500 |           0.0099 |           0.0000 |
[32m[20221213 20:42:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:42:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.84
[32m[20221213 20:42:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.58
[32m[20221213 20:42:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.28
[32m[20221213 20:42:56 @agent_ppo2.py:143][0m Total time:      13.78 min
[32m[20221213 20:42:56 @agent_ppo2.py:145][0m 1974272 total steps have happened
[32m[20221213 20:42:56 @agent_ppo2.py:121][0m #------------------------ Iteration 482 --------------------------#
[32m[20221213 20:42:56 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:42:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:57 @agent_ppo2.py:185][0m |           0.0410 |           0.0136 |           0.0000 |
[32m[20221213 20:42:57 @agent_ppo2.py:185][0m |           0.0111 |           0.0074 |           0.0000 |
[32m[20221213 20:42:57 @agent_ppo2.py:185][0m |           0.0135 |           0.0072 |           0.0000 |
[32m[20221213 20:42:57 @agent_ppo2.py:185][0m |           0.0036 |           0.0071 |           0.0000 |
[32m[20221213 20:42:57 @agent_ppo2.py:185][0m |          -0.0151 |           0.0069 |           0.0000 |
[32m[20221213 20:42:57 @agent_ppo2.py:185][0m |          -0.0198 |           0.0068 |           0.0000 |
[32m[20221213 20:42:57 @agent_ppo2.py:185][0m |          -0.0268 |           0.0068 |           0.0000 |
[32m[20221213 20:42:57 @agent_ppo2.py:185][0m |          -0.0269 |           0.0067 |           0.0000 |
[32m[20221213 20:42:57 @agent_ppo2.py:185][0m |          -0.0291 |           0.0066 |           0.0000 |
[32m[20221213 20:42:57 @agent_ppo2.py:185][0m |          -0.0096 |           0.0070 |           0.0000 |
[32m[20221213 20:42:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:42:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.03
[32m[20221213 20:42:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.14
[32m[20221213 20:42:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.20
[32m[20221213 20:42:58 @agent_ppo2.py:143][0m Total time:      13.81 min
[32m[20221213 20:42:58 @agent_ppo2.py:145][0m 1978368 total steps have happened
[32m[20221213 20:42:58 @agent_ppo2.py:121][0m #------------------------ Iteration 483 --------------------------#
[32m[20221213 20:42:58 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:42:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:42:58 @agent_ppo2.py:185][0m |           0.0287 |           0.0358 |           0.0000 |
[32m[20221213 20:42:58 @agent_ppo2.py:185][0m |          -0.0060 |           0.0170 |           0.0000 |
[32m[20221213 20:42:58 @agent_ppo2.py:185][0m |          -0.0117 |           0.0145 |           0.0000 |
[32m[20221213 20:42:59 @agent_ppo2.py:185][0m |          -0.0150 |           0.0139 |           0.0000 |
[32m[20221213 20:42:59 @agent_ppo2.py:185][0m |          -0.0116 |           0.0134 |           0.0000 |
[32m[20221213 20:42:59 @agent_ppo2.py:185][0m |          -0.0240 |           0.0132 |           0.0000 |
[32m[20221213 20:42:59 @agent_ppo2.py:185][0m |          -0.0263 |           0.0131 |           0.0000 |
[32m[20221213 20:42:59 @agent_ppo2.py:185][0m |          -0.0471 |           0.0127 |           0.0000 |
[32m[20221213 20:42:59 @agent_ppo2.py:185][0m |          -0.0641 |           0.0127 |           0.0000 |
[32m[20221213 20:42:59 @agent_ppo2.py:185][0m |          -0.0281 |           0.0125 |           0.0000 |
[32m[20221213 20:42:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:42:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.59
[32m[20221213 20:42:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.98
[32m[20221213 20:42:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.37
[32m[20221213 20:42:59 @agent_ppo2.py:143][0m Total time:      13.84 min
[32m[20221213 20:42:59 @agent_ppo2.py:145][0m 1982464 total steps have happened
[32m[20221213 20:42:59 @agent_ppo2.py:121][0m #------------------------ Iteration 484 --------------------------#
[32m[20221213 20:43:00 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:00 @agent_ppo2.py:185][0m |           0.0340 |           0.0128 |           0.0000 |
[32m[20221213 20:43:00 @agent_ppo2.py:185][0m |          -0.0050 |           0.0124 |           0.0000 |
[32m[20221213 20:43:00 @agent_ppo2.py:185][0m |          -0.0238 |           0.0120 |           0.0000 |
[32m[20221213 20:43:00 @agent_ppo2.py:185][0m |          -0.0339 |           0.0119 |           0.0000 |
[32m[20221213 20:43:00 @agent_ppo2.py:185][0m |          -0.0524 |           0.0119 |           0.0000 |
[32m[20221213 20:43:00 @agent_ppo2.py:185][0m |          -0.0564 |           0.0117 |           0.0000 |
[32m[20221213 20:43:00 @agent_ppo2.py:185][0m |          -0.0600 |           0.0115 |           0.0000 |
[32m[20221213 20:43:01 @agent_ppo2.py:185][0m |          -0.0685 |           0.0114 |           0.0000 |
[32m[20221213 20:43:01 @agent_ppo2.py:185][0m |          -0.0429 |           0.0117 |           0.0000 |
[32m[20221213 20:43:01 @agent_ppo2.py:185][0m |          -0.0628 |           0.0115 |           0.0000 |
[32m[20221213 20:43:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:43:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.70
[32m[20221213 20:43:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.95
[32m[20221213 20:43:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.19
[32m[20221213 20:43:01 @agent_ppo2.py:143][0m Total time:      13.87 min
[32m[20221213 20:43:01 @agent_ppo2.py:145][0m 1986560 total steps have happened
[32m[20221213 20:43:01 @agent_ppo2.py:121][0m #------------------------ Iteration 485 --------------------------#
[32m[20221213 20:43:01 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:02 @agent_ppo2.py:185][0m |           0.0257 |           0.0134 |           0.0000 |
[32m[20221213 20:43:02 @agent_ppo2.py:185][0m |           0.0033 |           0.0082 |           0.0000 |
[32m[20221213 20:43:02 @agent_ppo2.py:185][0m |          -0.0141 |           0.0079 |           0.0000 |
[32m[20221213 20:43:02 @agent_ppo2.py:185][0m |          -0.0233 |           0.0078 |           0.0000 |
[32m[20221213 20:43:02 @agent_ppo2.py:185][0m |          -0.0326 |           0.0077 |           0.0000 |
[32m[20221213 20:43:02 @agent_ppo2.py:185][0m |          -0.0366 |           0.0076 |           0.0000 |
[32m[20221213 20:43:02 @agent_ppo2.py:185][0m |          -0.0130 |           0.0077 |           0.0000 |
[32m[20221213 20:43:02 @agent_ppo2.py:185][0m |          -0.0459 |           0.0075 |           0.0000 |
[32m[20221213 20:43:02 @agent_ppo2.py:185][0m |          -0.0458 |           0.0074 |           0.0000 |
[32m[20221213 20:43:02 @agent_ppo2.py:185][0m |          -0.0497 |           0.0073 |           0.0000 |
[32m[20221213 20:43:02 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:43:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.89
[32m[20221213 20:43:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.97
[32m[20221213 20:43:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.84
[32m[20221213 20:43:03 @agent_ppo2.py:143][0m Total time:      13.89 min
[32m[20221213 20:43:03 @agent_ppo2.py:145][0m 1990656 total steps have happened
[32m[20221213 20:43:03 @agent_ppo2.py:121][0m #------------------------ Iteration 486 --------------------------#
[32m[20221213 20:43:03 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:03 @agent_ppo2.py:185][0m |           0.0268 |           0.0737 |           0.0000 |
[32m[20221213 20:43:03 @agent_ppo2.py:185][0m |          -0.0378 |           0.0238 |           0.0000 |
[32m[20221213 20:43:03 @agent_ppo2.py:185][0m |          -0.0147 |           0.0186 |           0.0000 |
[32m[20221213 20:43:04 @agent_ppo2.py:185][0m |          -0.0230 |           0.0166 |           0.0000 |
[32m[20221213 20:43:04 @agent_ppo2.py:185][0m |          -0.0254 |           0.0164 |           0.0000 |
[32m[20221213 20:43:04 @agent_ppo2.py:185][0m |          -0.0278 |           0.0160 |           0.0000 |
[32m[20221213 20:43:04 @agent_ppo2.py:185][0m |          -0.0300 |           0.0155 |           0.0000 |
[32m[20221213 20:43:04 @agent_ppo2.py:185][0m |          -0.0290 |           0.0154 |           0.0000 |
[32m[20221213 20:43:04 @agent_ppo2.py:185][0m |          -0.0310 |           0.0148 |           0.0000 |
[32m[20221213 20:43:04 @agent_ppo2.py:185][0m |          -0.0328 |           0.0146 |           0.0000 |
[32m[20221213 20:43:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:43:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.38
[32m[20221213 20:43:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.11
[32m[20221213 20:43:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.45
[32m[20221213 20:43:04 @agent_ppo2.py:143][0m Total time:      13.92 min
[32m[20221213 20:43:04 @agent_ppo2.py:145][0m 1994752 total steps have happened
[32m[20221213 20:43:04 @agent_ppo2.py:121][0m #------------------------ Iteration 487 --------------------------#
[32m[20221213 20:43:05 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:05 @agent_ppo2.py:185][0m |           0.0496 |           0.0213 |           0.0000 |
[32m[20221213 20:43:05 @agent_ppo2.py:185][0m |           0.0472 |           0.0174 |           0.0000 |
[32m[20221213 20:43:05 @agent_ppo2.py:185][0m |           0.0059 |           0.0166 |           0.0000 |
[32m[20221213 20:43:05 @agent_ppo2.py:185][0m |           0.0006 |           0.0162 |           0.0000 |
[32m[20221213 20:43:05 @agent_ppo2.py:185][0m |          -0.0133 |           0.0154 |           0.0000 |
[32m[20221213 20:43:05 @agent_ppo2.py:185][0m |          -0.0252 |           0.0153 |           0.0000 |
[32m[20221213 20:43:05 @agent_ppo2.py:185][0m |          -0.0342 |           0.0151 |           0.0000 |
[32m[20221213 20:43:06 @agent_ppo2.py:185][0m |          -0.0423 |           0.0146 |           0.0000 |
[32m[20221213 20:43:06 @agent_ppo2.py:185][0m |          -0.0488 |           0.0145 |           0.0000 |
[32m[20221213 20:43:06 @agent_ppo2.py:185][0m |          -0.0503 |           0.0142 |           0.0000 |
[32m[20221213 20:43:06 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:43:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.06
[32m[20221213 20:43:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.50
[32m[20221213 20:43:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.26
[32m[20221213 20:43:06 @agent_ppo2.py:143][0m Total time:      13.95 min
[32m[20221213 20:43:06 @agent_ppo2.py:145][0m 1998848 total steps have happened
[32m[20221213 20:43:06 @agent_ppo2.py:121][0m #------------------------ Iteration 488 --------------------------#
[32m[20221213 20:43:06 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:07 @agent_ppo2.py:185][0m |           0.0493 |           0.0135 |           0.0000 |
[32m[20221213 20:43:07 @agent_ppo2.py:185][0m |           0.0391 |           0.0121 |           0.0000 |
[32m[20221213 20:43:07 @agent_ppo2.py:185][0m |          -0.0043 |           0.0119 |           0.0000 |
[32m[20221213 20:43:07 @agent_ppo2.py:185][0m |          -0.0316 |           0.0117 |           0.0000 |
[32m[20221213 20:43:07 @agent_ppo2.py:185][0m |          -0.0446 |           0.0117 |           0.0000 |
[32m[20221213 20:43:07 @agent_ppo2.py:185][0m |          -0.0393 |           0.0117 |           0.0000 |
[32m[20221213 20:43:07 @agent_ppo2.py:185][0m |          -0.0627 |           0.0116 |           0.0000 |
[32m[20221213 20:43:07 @agent_ppo2.py:185][0m |          -0.0632 |           0.0113 |           0.0000 |
[32m[20221213 20:43:07 @agent_ppo2.py:185][0m |          -0.0662 |           0.0113 |           0.0000 |
[32m[20221213 20:43:07 @agent_ppo2.py:185][0m |          -0.0795 |           0.0113 |           0.0000 |
[32m[20221213 20:43:07 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:43:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.25
[32m[20221213 20:43:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.87
[32m[20221213 20:43:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.63
[32m[20221213 20:43:08 @agent_ppo2.py:143][0m Total time:      13.98 min
[32m[20221213 20:43:08 @agent_ppo2.py:145][0m 2002944 total steps have happened
[32m[20221213 20:43:08 @agent_ppo2.py:121][0m #------------------------ Iteration 489 --------------------------#
[32m[20221213 20:43:08 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:08 @agent_ppo2.py:185][0m |           0.0370 |           0.0119 |           0.0000 |
[32m[20221213 20:43:08 @agent_ppo2.py:185][0m |          -0.0110 |           0.0115 |           0.0000 |
[32m[20221213 20:43:08 @agent_ppo2.py:185][0m |          -0.0323 |           0.0113 |           0.0000 |
[32m[20221213 20:43:08 @agent_ppo2.py:185][0m |          -0.0496 |           0.0111 |           0.0000 |
[32m[20221213 20:43:09 @agent_ppo2.py:185][0m |          -0.0582 |           0.0110 |           0.0000 |
[32m[20221213 20:43:09 @agent_ppo2.py:185][0m |          -0.0613 |           0.0108 |           0.0000 |
[32m[20221213 20:43:09 @agent_ppo2.py:185][0m |          -0.0641 |           0.0108 |           0.0000 |
[32m[20221213 20:43:09 @agent_ppo2.py:185][0m |          -0.0786 |           0.0108 |           0.0000 |
[32m[20221213 20:43:09 @agent_ppo2.py:185][0m |          -0.0754 |           0.0107 |           0.0000 |
[32m[20221213 20:43:09 @agent_ppo2.py:185][0m |          -0.0788 |           0.0105 |           0.0000 |
[32m[20221213 20:43:09 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:43:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.63
[32m[20221213 20:43:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.09
[32m[20221213 20:43:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.83
[32m[20221213 20:43:09 @agent_ppo2.py:143][0m Total time:      14.00 min
[32m[20221213 20:43:09 @agent_ppo2.py:145][0m 2007040 total steps have happened
[32m[20221213 20:43:09 @agent_ppo2.py:121][0m #------------------------ Iteration 490 --------------------------#
[32m[20221213 20:43:10 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:43:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:10 @agent_ppo2.py:185][0m |           0.0226 |           0.0384 |           0.0000 |
[32m[20221213 20:43:10 @agent_ppo2.py:185][0m |           0.0020 |           0.0257 |           0.0000 |
[32m[20221213 20:43:10 @agent_ppo2.py:185][0m |          -0.0134 |           0.0221 |           0.0000 |
[32m[20221213 20:43:10 @agent_ppo2.py:185][0m |          -0.0183 |           0.0212 |           0.0000 |
[32m[20221213 20:43:10 @agent_ppo2.py:185][0m |          -0.0185 |           0.0199 |           0.0000 |
[32m[20221213 20:43:10 @agent_ppo2.py:185][0m |          -0.0231 |           0.0194 |           0.0000 |
[32m[20221213 20:43:10 @agent_ppo2.py:185][0m |          -0.0170 |           0.0195 |           0.0000 |
[32m[20221213 20:43:10 @agent_ppo2.py:185][0m |          -0.0201 |           0.0197 |           0.0000 |
[32m[20221213 20:43:11 @agent_ppo2.py:185][0m |          -0.0231 |           0.0192 |           0.0000 |
[32m[20221213 20:43:11 @agent_ppo2.py:185][0m |          -0.0297 |           0.0199 |           0.0000 |
[32m[20221213 20:43:11 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:43:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.91
[32m[20221213 20:43:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.78
[32m[20221213 20:43:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.71
[32m[20221213 20:43:11 @agent_ppo2.py:143][0m Total time:      14.03 min
[32m[20221213 20:43:11 @agent_ppo2.py:145][0m 2011136 total steps have happened
[32m[20221213 20:43:11 @agent_ppo2.py:121][0m #------------------------ Iteration 491 --------------------------#
[32m[20221213 20:43:11 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:12 @agent_ppo2.py:185][0m |           0.1361 |           0.0333 |           0.0000 |
[32m[20221213 20:43:12 @agent_ppo2.py:185][0m |           0.0097 |           0.0115 |           0.0000 |
[32m[20221213 20:43:12 @agent_ppo2.py:185][0m |          -0.0024 |           0.0104 |           0.0000 |
[32m[20221213 20:43:12 @agent_ppo2.py:185][0m |          -0.0113 |           0.0102 |           0.0000 |
[32m[20221213 20:43:12 @agent_ppo2.py:185][0m |          -0.0182 |           0.0100 |           0.0000 |
[32m[20221213 20:43:12 @agent_ppo2.py:185][0m |          -0.0213 |           0.0099 |           0.0000 |
[32m[20221213 20:43:12 @agent_ppo2.py:185][0m |          -0.0244 |           0.0098 |           0.0000 |
[32m[20221213 20:43:12 @agent_ppo2.py:185][0m |          -0.0296 |           0.0097 |           0.0000 |
[32m[20221213 20:43:12 @agent_ppo2.py:185][0m |          -0.0321 |           0.0096 |           0.0000 |
[32m[20221213 20:43:12 @agent_ppo2.py:185][0m |          -0.0373 |           0.0095 |           0.0000 |
[32m[20221213 20:43:12 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:43:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.49
[32m[20221213 20:43:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.99
[32m[20221213 20:43:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.54
[32m[20221213 20:43:13 @agent_ppo2.py:143][0m Total time:      14.06 min
[32m[20221213 20:43:13 @agent_ppo2.py:145][0m 2015232 total steps have happened
[32m[20221213 20:43:13 @agent_ppo2.py:121][0m #------------------------ Iteration 492 --------------------------#
[32m[20221213 20:43:13 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:13 @agent_ppo2.py:185][0m |           0.0387 |           0.0111 |           0.0000 |
[32m[20221213 20:43:13 @agent_ppo2.py:185][0m |           0.0285 |           0.0077 |           0.0000 |
[32m[20221213 20:43:13 @agent_ppo2.py:185][0m |           0.0184 |           0.0073 |           0.0000 |
[32m[20221213 20:43:13 @agent_ppo2.py:185][0m |          -0.0060 |           0.0072 |           0.0000 |
[32m[20221213 20:43:13 @agent_ppo2.py:185][0m |          -0.0149 |           0.0071 |           0.0000 |
[32m[20221213 20:43:14 @agent_ppo2.py:185][0m |          -0.0265 |           0.0070 |           0.0000 |
[32m[20221213 20:43:14 @agent_ppo2.py:185][0m |          -0.0280 |           0.0070 |           0.0000 |
[32m[20221213 20:43:14 @agent_ppo2.py:185][0m |           0.0019 |           0.0070 |           0.0000 |
[32m[20221213 20:43:14 @agent_ppo2.py:185][0m |          -0.0333 |           0.0069 |           0.0000 |
[32m[20221213 20:43:14 @agent_ppo2.py:185][0m |          -0.0409 |           0.0068 |           0.0000 |
[32m[20221213 20:43:14 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:43:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.14
[32m[20221213 20:43:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.31
[32m[20221213 20:43:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.26
[32m[20221213 20:43:14 @agent_ppo2.py:143][0m Total time:      14.09 min
[32m[20221213 20:43:14 @agent_ppo2.py:145][0m 2019328 total steps have happened
[32m[20221213 20:43:14 @agent_ppo2.py:121][0m #------------------------ Iteration 493 --------------------------#
[32m[20221213 20:43:15 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:15 @agent_ppo2.py:185][0m |           0.0289 |           0.0178 |           0.0000 |
[32m[20221213 20:43:15 @agent_ppo2.py:185][0m |           0.0113 |           0.0107 |           0.0000 |
[32m[20221213 20:43:15 @agent_ppo2.py:185][0m |          -0.0072 |           0.0094 |           0.0000 |
[32m[20221213 20:43:15 @agent_ppo2.py:185][0m |          -0.0172 |           0.0091 |           0.0000 |
[32m[20221213 20:43:15 @agent_ppo2.py:185][0m |          -0.0149 |           0.0091 |           0.0000 |
[32m[20221213 20:43:15 @agent_ppo2.py:185][0m |          -0.0153 |           0.0091 |           0.0000 |
[32m[20221213 20:43:15 @agent_ppo2.py:185][0m |          -0.0205 |           0.0090 |           0.0000 |
[32m[20221213 20:43:15 @agent_ppo2.py:185][0m |          -0.0182 |           0.0089 |           0.0000 |
[32m[20221213 20:43:15 @agent_ppo2.py:185][0m |          -0.0185 |           0.0089 |           0.0000 |
[32m[20221213 20:43:16 @agent_ppo2.py:185][0m |          -0.0256 |           0.0088 |           0.0000 |
[32m[20221213 20:43:16 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:43:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.88
[32m[20221213 20:43:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.23
[32m[20221213 20:43:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.29
[32m[20221213 20:43:16 @agent_ppo2.py:143][0m Total time:      14.11 min
[32m[20221213 20:43:16 @agent_ppo2.py:145][0m 2023424 total steps have happened
[32m[20221213 20:43:16 @agent_ppo2.py:121][0m #------------------------ Iteration 494 --------------------------#
[32m[20221213 20:43:16 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:16 @agent_ppo2.py:185][0m |           0.0236 |           0.0328 |           0.0000 |
[32m[20221213 20:43:17 @agent_ppo2.py:185][0m |          -0.0037 |           0.0174 |           0.0000 |
[32m[20221213 20:43:17 @agent_ppo2.py:185][0m |          -0.0457 |           0.0169 |           0.0000 |
[32m[20221213 20:43:17 @agent_ppo2.py:185][0m |          -0.0176 |           0.0184 |           0.0000 |
[32m[20221213 20:43:17 @agent_ppo2.py:185][0m |          -0.0206 |           0.0154 |           0.0000 |
[32m[20221213 20:43:17 @agent_ppo2.py:185][0m |          -0.0218 |           0.0148 |           0.0000 |
[32m[20221213 20:43:17 @agent_ppo2.py:185][0m |          -0.0266 |           0.0144 |           0.0000 |
[32m[20221213 20:43:17 @agent_ppo2.py:185][0m |          -0.0298 |           0.0141 |           0.0000 |
[32m[20221213 20:43:17 @agent_ppo2.py:185][0m |          -0.0312 |           0.0142 |           0.0000 |
[32m[20221213 20:43:17 @agent_ppo2.py:185][0m |          -0.0315 |           0.0140 |           0.0000 |
[32m[20221213 20:43:17 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:43:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.97
[32m[20221213 20:43:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.93
[32m[20221213 20:43:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.43
[32m[20221213 20:43:18 @agent_ppo2.py:143][0m Total time:      14.14 min
[32m[20221213 20:43:18 @agent_ppo2.py:145][0m 2027520 total steps have happened
[32m[20221213 20:43:18 @agent_ppo2.py:121][0m #------------------------ Iteration 495 --------------------------#
[32m[20221213 20:43:18 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:18 @agent_ppo2.py:185][0m |           0.0224 |           0.0250 |           0.0000 |
[32m[20221213 20:43:18 @agent_ppo2.py:185][0m |          -0.0054 |           0.0175 |           0.0000 |
[32m[20221213 20:43:18 @agent_ppo2.py:185][0m |          -0.0270 |           0.0169 |           0.0000 |
[32m[20221213 20:43:18 @agent_ppo2.py:185][0m |          -0.0348 |           0.0157 |           0.0000 |
[32m[20221213 20:43:18 @agent_ppo2.py:185][0m |          -0.0269 |           0.0156 |           0.0000 |
[32m[20221213 20:43:18 @agent_ppo2.py:185][0m |          -0.0299 |           0.0151 |           0.0000 |
[32m[20221213 20:43:19 @agent_ppo2.py:185][0m |          -0.0374 |           0.0151 |           0.0000 |
[32m[20221213 20:43:19 @agent_ppo2.py:185][0m |          -0.0434 |           0.0147 |           0.0000 |
[32m[20221213 20:43:19 @agent_ppo2.py:185][0m |          -0.0955 |           0.0158 |           0.0000 |
[32m[20221213 20:43:19 @agent_ppo2.py:185][0m |          -0.0433 |           0.0172 |           0.0000 |
[32m[20221213 20:43:19 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:43:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.75
[32m[20221213 20:43:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.03
[32m[20221213 20:43:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.37
[32m[20221213 20:43:19 @agent_ppo2.py:143][0m Total time:      14.17 min
[32m[20221213 20:43:19 @agent_ppo2.py:145][0m 2031616 total steps have happened
[32m[20221213 20:43:19 @agent_ppo2.py:121][0m #------------------------ Iteration 496 --------------------------#
[32m[20221213 20:43:20 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:43:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:20 @agent_ppo2.py:185][0m |           0.0241 |           0.0311 |           0.0000 |
[32m[20221213 20:43:20 @agent_ppo2.py:185][0m |           0.0045 |           0.0119 |           0.0000 |
[32m[20221213 20:43:20 @agent_ppo2.py:185][0m |          -0.0124 |           0.0112 |           0.0000 |
[32m[20221213 20:43:20 @agent_ppo2.py:185][0m |          -0.0209 |           0.0109 |           0.0000 |
[32m[20221213 20:43:20 @agent_ppo2.py:185][0m |          -0.0296 |           0.0107 |           0.0000 |
[32m[20221213 20:43:20 @agent_ppo2.py:185][0m |          -0.0288 |           0.0107 |           0.0000 |
[32m[20221213 20:43:20 @agent_ppo2.py:185][0m |          -0.0369 |           0.0106 |           0.0000 |
[32m[20221213 20:43:20 @agent_ppo2.py:185][0m |          -0.0412 |           0.0104 |           0.0000 |
[32m[20221213 20:43:20 @agent_ppo2.py:185][0m |          -0.0412 |           0.0102 |           0.0000 |
[32m[20221213 20:43:20 @agent_ppo2.py:185][0m |          -0.0449 |           0.0102 |           0.0000 |
[32m[20221213 20:43:20 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:43:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.02
[32m[20221213 20:43:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.15
[32m[20221213 20:43:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.62
[32m[20221213 20:43:21 @agent_ppo2.py:143][0m Total time:      14.19 min
[32m[20221213 20:43:21 @agent_ppo2.py:145][0m 2035712 total steps have happened
[32m[20221213 20:43:21 @agent_ppo2.py:121][0m #------------------------ Iteration 497 --------------------------#
[32m[20221213 20:43:21 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:21 @agent_ppo2.py:185][0m |           0.0269 |           0.0525 |           0.0000 |
[32m[20221213 20:43:21 @agent_ppo2.py:185][0m |           0.0015 |           0.0214 |           0.0000 |
[32m[20221213 20:43:22 @agent_ppo2.py:185][0m |          -0.0119 |           0.0184 |           0.0000 |
[32m[20221213 20:43:22 @agent_ppo2.py:185][0m |          -0.0199 |           0.0174 |           0.0000 |
[32m[20221213 20:43:22 @agent_ppo2.py:185][0m |          -0.0256 |           0.0167 |           0.0000 |
[32m[20221213 20:43:22 @agent_ppo2.py:185][0m |          -0.0283 |           0.0163 |           0.0000 |
[32m[20221213 20:43:22 @agent_ppo2.py:185][0m |          -0.0296 |           0.0165 |           0.0000 |
[32m[20221213 20:43:22 @agent_ppo2.py:185][0m |          -0.0301 |           0.0171 |           0.0000 |
[32m[20221213 20:43:22 @agent_ppo2.py:185][0m |          -0.0310 |           0.0156 |           0.0000 |
[32m[20221213 20:43:22 @agent_ppo2.py:185][0m |          -0.0318 |           0.0161 |           0.0000 |
[32m[20221213 20:43:22 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:43:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.07
[32m[20221213 20:43:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.50
[32m[20221213 20:43:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.29
[32m[20221213 20:43:22 @agent_ppo2.py:143][0m Total time:      14.22 min
[32m[20221213 20:43:22 @agent_ppo2.py:145][0m 2039808 total steps have happened
[32m[20221213 20:43:22 @agent_ppo2.py:121][0m #------------------------ Iteration 498 --------------------------#
[32m[20221213 20:43:23 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:23 @agent_ppo2.py:185][0m |           0.0283 |           0.0254 |           0.0000 |
[32m[20221213 20:43:23 @agent_ppo2.py:185][0m |          -0.0016 |           0.0151 |           0.0000 |
[32m[20221213 20:43:23 @agent_ppo2.py:185][0m |          -0.0167 |           0.0140 |           0.0000 |
[32m[20221213 20:43:23 @agent_ppo2.py:185][0m |          -0.0280 |           0.0137 |           0.0000 |
[32m[20221213 20:43:23 @agent_ppo2.py:185][0m |          -0.0430 |           0.0135 |           0.0000 |
[32m[20221213 20:43:23 @agent_ppo2.py:185][0m |          -0.0439 |           0.0133 |           0.0000 |
[32m[20221213 20:43:23 @agent_ppo2.py:185][0m |          -0.0511 |           0.0131 |           0.0000 |
[32m[20221213 20:43:24 @agent_ppo2.py:185][0m |          -0.0552 |           0.0129 |           0.0000 |
[32m[20221213 20:43:24 @agent_ppo2.py:185][0m |          -0.0561 |           0.0127 |           0.0000 |
[32m[20221213 20:43:24 @agent_ppo2.py:185][0m |          -0.0487 |           0.0126 |           0.0000 |
[32m[20221213 20:43:24 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:43:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.64
[32m[20221213 20:43:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.46
[32m[20221213 20:43:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.20
[32m[20221213 20:43:24 @agent_ppo2.py:143][0m Total time:      14.25 min
[32m[20221213 20:43:24 @agent_ppo2.py:145][0m 2043904 total steps have happened
[32m[20221213 20:43:24 @agent_ppo2.py:121][0m #------------------------ Iteration 499 --------------------------#
[32m[20221213 20:43:24 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:25 @agent_ppo2.py:185][0m |           0.0741 |           0.0105 |           0.0000 |
[32m[20221213 20:43:25 @agent_ppo2.py:185][0m |           0.0233 |           0.0091 |           0.0000 |
[32m[20221213 20:43:25 @agent_ppo2.py:185][0m |           0.0043 |           0.0091 |           0.0000 |
[32m[20221213 20:43:25 @agent_ppo2.py:185][0m |          -0.0245 |           0.0090 |           0.0000 |
[32m[20221213 20:43:25 @agent_ppo2.py:185][0m |          -0.0344 |           0.0089 |           0.0000 |
[32m[20221213 20:43:25 @agent_ppo2.py:185][0m |          -0.0449 |           0.0088 |           0.0000 |
[32m[20221213 20:43:25 @agent_ppo2.py:185][0m |          -0.0521 |           0.0087 |           0.0000 |
[32m[20221213 20:43:25 @agent_ppo2.py:185][0m |          -0.0562 |           0.0087 |           0.0000 |
[32m[20221213 20:43:25 @agent_ppo2.py:185][0m |          -0.0588 |           0.0086 |           0.0000 |
[32m[20221213 20:43:25 @agent_ppo2.py:185][0m |          -0.0646 |           0.0086 |           0.0000 |
[32m[20221213 20:43:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:43:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.23
[32m[20221213 20:43:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.47
[32m[20221213 20:43:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.11
[32m[20221213 20:43:26 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 15.11
[32m[20221213 20:43:26 @agent_ppo2.py:143][0m Total time:      14.28 min
[32m[20221213 20:43:26 @agent_ppo2.py:145][0m 2048000 total steps have happened
[32m[20221213 20:43:26 @agent_ppo2.py:121][0m #------------------------ Iteration 500 --------------------------#
[32m[20221213 20:43:26 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:43:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:26 @agent_ppo2.py:185][0m |           0.0230 |           0.0124 |           0.0000 |
[32m[20221213 20:43:26 @agent_ppo2.py:185][0m |          -0.0080 |           0.0119 |           0.0000 |
[32m[20221213 20:43:27 @agent_ppo2.py:185][0m |          -0.0239 |           0.0115 |           0.0000 |
[32m[20221213 20:43:27 @agent_ppo2.py:185][0m |          -0.0388 |           0.0112 |           0.0000 |
[32m[20221213 20:43:27 @agent_ppo2.py:185][0m |          -0.0404 |           0.0110 |           0.0000 |
[32m[20221213 20:43:27 @agent_ppo2.py:185][0m |          -0.0411 |           0.0110 |           0.0000 |
[32m[20221213 20:43:27 @agent_ppo2.py:185][0m |          -0.0481 |           0.0108 |           0.0000 |
[32m[20221213 20:43:27 @agent_ppo2.py:185][0m |          -0.0515 |           0.0108 |           0.0000 |
[32m[20221213 20:43:27 @agent_ppo2.py:185][0m |          -0.0517 |           0.0106 |           0.0000 |
[32m[20221213 20:43:27 @agent_ppo2.py:185][0m |          -0.0533 |           0.0105 |           0.0000 |
[32m[20221213 20:43:27 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:43:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.44
[32m[20221213 20:43:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.76
[32m[20221213 20:43:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.50
[32m[20221213 20:43:27 @agent_ppo2.py:143][0m Total time:      14.30 min
[32m[20221213 20:43:27 @agent_ppo2.py:145][0m 2052096 total steps have happened
[32m[20221213 20:43:27 @agent_ppo2.py:121][0m #------------------------ Iteration 501 --------------------------#
[32m[20221213 20:43:28 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:28 @agent_ppo2.py:185][0m |           0.0065 |           0.0113 |           0.0000 |
[32m[20221213 20:43:28 @agent_ppo2.py:185][0m |          -0.0131 |           0.0071 |           0.0000 |
[32m[20221213 20:43:28 @agent_ppo2.py:185][0m |          -0.0231 |           0.0069 |           0.0000 |
[32m[20221213 20:43:28 @agent_ppo2.py:185][0m |          -0.0306 |           0.0067 |           0.0000 |
[32m[20221213 20:43:28 @agent_ppo2.py:185][0m |          -0.0377 |           0.0066 |           0.0000 |
[32m[20221213 20:43:28 @agent_ppo2.py:185][0m |          -0.0384 |           0.0065 |           0.0000 |
[32m[20221213 20:43:28 @agent_ppo2.py:185][0m |          -0.0411 |           0.0065 |           0.0000 |
[32m[20221213 20:43:29 @agent_ppo2.py:185][0m |          -0.0444 |           0.0064 |           0.0000 |
[32m[20221213 20:43:29 @agent_ppo2.py:185][0m |          -0.0471 |           0.0063 |           0.0000 |
[32m[20221213 20:43:29 @agent_ppo2.py:185][0m |          -0.0087 |           0.0062 |           0.0000 |
[32m[20221213 20:43:29 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:43:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.86
[32m[20221213 20:43:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.21
[32m[20221213 20:43:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.24
[32m[20221213 20:43:29 @agent_ppo2.py:143][0m Total time:      14.33 min
[32m[20221213 20:43:29 @agent_ppo2.py:145][0m 2056192 total steps have happened
[32m[20221213 20:43:29 @agent_ppo2.py:121][0m #------------------------ Iteration 502 --------------------------#
[32m[20221213 20:43:29 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:30 @agent_ppo2.py:185][0m |           0.0266 |           0.0231 |           0.0000 |
[32m[20221213 20:43:30 @agent_ppo2.py:185][0m |          -0.0487 |           0.0117 |           0.0000 |
[32m[20221213 20:43:30 @agent_ppo2.py:185][0m |          -0.0074 |           0.0104 |           0.0000 |
[32m[20221213 20:43:30 @agent_ppo2.py:185][0m |          -0.0385 |           0.0091 |           0.0000 |
[32m[20221213 20:43:30 @agent_ppo2.py:185][0m |          -0.0369 |           0.0089 |           0.0000 |
[32m[20221213 20:43:30 @agent_ppo2.py:185][0m |          -0.0172 |           0.0087 |           0.0000 |
[32m[20221213 20:43:30 @agent_ppo2.py:185][0m |          -0.0223 |           0.0087 |           0.0000 |
[32m[20221213 20:43:30 @agent_ppo2.py:185][0m |          -0.0238 |           0.0087 |           0.0000 |
[32m[20221213 20:43:30 @agent_ppo2.py:185][0m |          -0.0254 |           0.0087 |           0.0000 |
[32m[20221213 20:43:30 @agent_ppo2.py:185][0m |          -0.0273 |           0.0085 |           0.0000 |
[32m[20221213 20:43:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:43:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.28
[32m[20221213 20:43:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.71
[32m[20221213 20:43:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.05
[32m[20221213 20:43:31 @agent_ppo2.py:143][0m Total time:      14.36 min
[32m[20221213 20:43:31 @agent_ppo2.py:145][0m 2060288 total steps have happened
[32m[20221213 20:43:31 @agent_ppo2.py:121][0m #------------------------ Iteration 503 --------------------------#
[32m[20221213 20:43:31 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:31 @agent_ppo2.py:185][0m |           0.0428 |           0.0104 |           0.0000 |
[32m[20221213 20:43:31 @agent_ppo2.py:185][0m |           0.0082 |           0.0095 |           0.0000 |
[32m[20221213 20:43:31 @agent_ppo2.py:185][0m |          -0.0132 |           0.0100 |           0.0000 |
[32m[20221213 20:43:32 @agent_ppo2.py:185][0m |          -0.0148 |           0.0096 |           0.0000 |
[32m[20221213 20:43:32 @agent_ppo2.py:185][0m |          -0.0252 |           0.0089 |           0.0000 |
[32m[20221213 20:43:32 @agent_ppo2.py:185][0m |          -0.0307 |           0.0087 |           0.0000 |
[32m[20221213 20:43:32 @agent_ppo2.py:185][0m |          -0.0364 |           0.0086 |           0.0000 |
[32m[20221213 20:43:32 @agent_ppo2.py:185][0m |          -0.0407 |           0.0085 |           0.0000 |
[32m[20221213 20:43:32 @agent_ppo2.py:185][0m |          -0.0442 |           0.0085 |           0.0000 |
[32m[20221213 20:43:32 @agent_ppo2.py:185][0m |          -0.0448 |           0.0085 |           0.0000 |
[32m[20221213 20:43:32 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:43:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.90
[32m[20221213 20:43:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.77
[32m[20221213 20:43:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.58
[32m[20221213 20:43:32 @agent_ppo2.py:143][0m Total time:      14.39 min
[32m[20221213 20:43:32 @agent_ppo2.py:145][0m 2064384 total steps have happened
[32m[20221213 20:43:32 @agent_ppo2.py:121][0m #------------------------ Iteration 504 --------------------------#
[32m[20221213 20:43:33 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:33 @agent_ppo2.py:185][0m |           0.0353 |           0.0235 |           0.0000 |
[32m[20221213 20:43:33 @agent_ppo2.py:185][0m |          -0.0098 |           0.0126 |           0.0000 |
[32m[20221213 20:43:33 @agent_ppo2.py:185][0m |          -0.0172 |           0.0117 |           0.0000 |
[32m[20221213 20:43:33 @agent_ppo2.py:185][0m |          -0.0241 |           0.0114 |           0.0000 |
[32m[20221213 20:43:33 @agent_ppo2.py:185][0m |          -0.0556 |           0.0111 |           0.0000 |
[32m[20221213 20:43:33 @agent_ppo2.py:185][0m |          -0.0257 |           0.0110 |           0.0000 |
[32m[20221213 20:43:33 @agent_ppo2.py:185][0m |          -0.0288 |           0.0109 |           0.0000 |
[32m[20221213 20:43:34 @agent_ppo2.py:185][0m |          -0.0519 |           0.0107 |           0.0000 |
[32m[20221213 20:43:34 @agent_ppo2.py:185][0m |          -0.0341 |           0.0106 |           0.0000 |
[32m[20221213 20:43:34 @agent_ppo2.py:185][0m |          -0.0345 |           0.0105 |           0.0000 |
[32m[20221213 20:43:34 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:43:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.58
[32m[20221213 20:43:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.11
[32m[20221213 20:43:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.64
[32m[20221213 20:43:34 @agent_ppo2.py:143][0m Total time:      14.42 min
[32m[20221213 20:43:34 @agent_ppo2.py:145][0m 2068480 total steps have happened
[32m[20221213 20:43:34 @agent_ppo2.py:121][0m #------------------------ Iteration 505 --------------------------#
[32m[20221213 20:43:34 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:35 @agent_ppo2.py:185][0m |           0.0330 |           0.0185 |           0.0000 |
[32m[20221213 20:43:35 @agent_ppo2.py:185][0m |          -0.0043 |           0.0160 |           0.0000 |
[32m[20221213 20:43:35 @agent_ppo2.py:185][0m |          -0.0095 |           0.0153 |           0.0000 |
[32m[20221213 20:43:35 @agent_ppo2.py:185][0m |          -0.0257 |           0.0147 |           0.0000 |
[32m[20221213 20:43:35 @agent_ppo2.py:185][0m |          -0.0265 |           0.0145 |           0.0000 |
[32m[20221213 20:43:35 @agent_ppo2.py:185][0m |          -0.0660 |           0.0144 |           0.0000 |
[32m[20221213 20:43:35 @agent_ppo2.py:185][0m |          -0.0392 |           0.0146 |           0.0000 |
[32m[20221213 20:43:35 @agent_ppo2.py:185][0m |          -0.0342 |           0.0136 |           0.0000 |
[32m[20221213 20:43:35 @agent_ppo2.py:185][0m |          -0.0414 |           0.0141 |           0.0000 |
[32m[20221213 20:43:35 @agent_ppo2.py:185][0m |          -0.0414 |           0.0136 |           0.0000 |
[32m[20221213 20:43:35 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:43:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.29
[32m[20221213 20:43:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.26
[32m[20221213 20:43:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.33
[32m[20221213 20:43:36 @agent_ppo2.py:143][0m Total time:      14.44 min
[32m[20221213 20:43:36 @agent_ppo2.py:145][0m 2072576 total steps have happened
[32m[20221213 20:43:36 @agent_ppo2.py:121][0m #------------------------ Iteration 506 --------------------------#
[32m[20221213 20:43:36 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:36 @agent_ppo2.py:185][0m |           0.0361 |           0.0404 |           0.0000 |
[32m[20221213 20:43:36 @agent_ppo2.py:185][0m |          -0.0004 |           0.0254 |           0.0000 |
[32m[20221213 20:43:36 @agent_ppo2.py:185][0m |          -0.0085 |           0.0218 |           0.0000 |
[32m[20221213 20:43:36 @agent_ppo2.py:185][0m |          -0.0037 |           0.0222 |           0.0000 |
[32m[20221213 20:43:37 @agent_ppo2.py:185][0m |          -0.0121 |           0.0203 |           0.0000 |
[32m[20221213 20:43:37 @agent_ppo2.py:185][0m |          -0.0200 |           0.0189 |           0.0000 |
[32m[20221213 20:43:37 @agent_ppo2.py:185][0m |          -0.0280 |           0.0192 |           0.0000 |
[32m[20221213 20:43:37 @agent_ppo2.py:185][0m |          -0.0323 |           0.0176 |           0.0000 |
[32m[20221213 20:43:37 @agent_ppo2.py:185][0m |          -0.0634 |           0.0185 |           0.0000 |
[32m[20221213 20:43:37 @agent_ppo2.py:185][0m |          -0.0375 |           0.0198 |           0.0000 |
[32m[20221213 20:43:37 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:43:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.48
[32m[20221213 20:43:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.18
[32m[20221213 20:43:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.73
[32m[20221213 20:43:37 @agent_ppo2.py:143][0m Total time:      14.47 min
[32m[20221213 20:43:37 @agent_ppo2.py:145][0m 2076672 total steps have happened
[32m[20221213 20:43:37 @agent_ppo2.py:121][0m #------------------------ Iteration 507 --------------------------#
[32m[20221213 20:43:38 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:38 @agent_ppo2.py:185][0m |           0.1086 |           0.0253 |           0.0000 |
[32m[20221213 20:43:38 @agent_ppo2.py:185][0m |           0.0344 |           0.0155 |           0.0000 |
[32m[20221213 20:43:38 @agent_ppo2.py:185][0m |           0.0140 |           0.0150 |           0.0000 |
[32m[20221213 20:43:38 @agent_ppo2.py:185][0m |          -0.0013 |           0.0143 |           0.0000 |
[32m[20221213 20:43:38 @agent_ppo2.py:185][0m |          -0.0144 |           0.0140 |           0.0000 |
[32m[20221213 20:43:38 @agent_ppo2.py:185][0m |          -0.0296 |           0.0139 |           0.0000 |
[32m[20221213 20:43:38 @agent_ppo2.py:185][0m |          -0.0301 |           0.0137 |           0.0000 |
[32m[20221213 20:43:38 @agent_ppo2.py:185][0m |          -0.0411 |           0.0135 |           0.0000 |
[32m[20221213 20:43:39 @agent_ppo2.py:185][0m |          -0.0430 |           0.0133 |           0.0000 |
[32m[20221213 20:43:39 @agent_ppo2.py:185][0m |          -0.0540 |           0.0134 |           0.0000 |
[32m[20221213 20:43:39 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:43:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.74
[32m[20221213 20:43:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.29
[32m[20221213 20:43:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.14
[32m[20221213 20:43:39 @agent_ppo2.py:143][0m Total time:      14.50 min
[32m[20221213 20:43:39 @agent_ppo2.py:145][0m 2080768 total steps have happened
[32m[20221213 20:43:39 @agent_ppo2.py:121][0m #------------------------ Iteration 508 --------------------------#
[32m[20221213 20:43:39 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:43:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:40 @agent_ppo2.py:185][0m |           0.0615 |           0.0143 |           0.0000 |
[32m[20221213 20:43:40 @agent_ppo2.py:185][0m |           0.0045 |           0.0107 |           0.0000 |
[32m[20221213 20:43:40 @agent_ppo2.py:185][0m |          -0.0229 |           0.0104 |           0.0000 |
[32m[20221213 20:43:40 @agent_ppo2.py:185][0m |          -0.0344 |           0.0102 |           0.0000 |
[32m[20221213 20:43:40 @agent_ppo2.py:185][0m |          -0.0284 |           0.0101 |           0.0000 |
[32m[20221213 20:43:40 @agent_ppo2.py:185][0m |          -0.0581 |           0.0100 |           0.0000 |
[32m[20221213 20:43:40 @agent_ppo2.py:185][0m |          -0.0505 |           0.0099 |           0.0000 |
[32m[20221213 20:43:40 @agent_ppo2.py:185][0m |          -0.0528 |           0.0098 |           0.0000 |
[32m[20221213 20:43:40 @agent_ppo2.py:185][0m |          -0.0568 |           0.0097 |           0.0000 |
[32m[20221213 20:43:40 @agent_ppo2.py:185][0m |          -0.0596 |           0.0096 |           0.0000 |
[32m[20221213 20:43:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:43:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.52
[32m[20221213 20:43:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.98
[32m[20221213 20:43:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.95
[32m[20221213 20:43:41 @agent_ppo2.py:143][0m Total time:      14.53 min
[32m[20221213 20:43:41 @agent_ppo2.py:145][0m 2084864 total steps have happened
[32m[20221213 20:43:41 @agent_ppo2.py:121][0m #------------------------ Iteration 509 --------------------------#
[32m[20221213 20:43:41 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:43:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:41 @agent_ppo2.py:185][0m |           0.0272 |           0.0110 |           0.0000 |
[32m[20221213 20:43:41 @agent_ppo2.py:185][0m |          -0.0061 |           0.0080 |           0.0000 |
[32m[20221213 20:43:41 @agent_ppo2.py:185][0m |          -0.0181 |           0.0078 |           0.0000 |
[32m[20221213 20:43:41 @agent_ppo2.py:185][0m |          -0.0272 |           0.0078 |           0.0000 |
[32m[20221213 20:43:42 @agent_ppo2.py:185][0m |          -0.0371 |           0.0077 |           0.0000 |
[32m[20221213 20:43:42 @agent_ppo2.py:185][0m |          -0.0411 |           0.0076 |           0.0000 |
[32m[20221213 20:43:42 @agent_ppo2.py:185][0m |          -0.0449 |           0.0076 |           0.0000 |
[32m[20221213 20:43:42 @agent_ppo2.py:185][0m |          -0.0461 |           0.0075 |           0.0000 |
[32m[20221213 20:43:42 @agent_ppo2.py:185][0m |          -0.0414 |           0.0077 |           0.0000 |
[32m[20221213 20:43:42 @agent_ppo2.py:185][0m |          -0.0367 |           0.0075 |           0.0000 |
[32m[20221213 20:43:42 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:43:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.08
[32m[20221213 20:43:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.31
[32m[20221213 20:43:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.79
[32m[20221213 20:43:42 @agent_ppo2.py:143][0m Total time:      14.55 min
[32m[20221213 20:43:42 @agent_ppo2.py:145][0m 2088960 total steps have happened
[32m[20221213 20:43:42 @agent_ppo2.py:121][0m #------------------------ Iteration 510 --------------------------#
[32m[20221213 20:43:43 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:43:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:43 @agent_ppo2.py:185][0m |           0.0416 |           0.0082 |           0.0000 |
[32m[20221213 20:43:43 @agent_ppo2.py:185][0m |           0.0031 |           0.0079 |           0.0000 |
[32m[20221213 20:43:43 @agent_ppo2.py:185][0m |          -0.0147 |           0.0079 |           0.0000 |
[32m[20221213 20:43:43 @agent_ppo2.py:185][0m |           0.0121 |           0.0083 |           0.0000 |
[32m[20221213 20:43:43 @agent_ppo2.py:185][0m |          -0.0400 |           0.0079 |           0.0000 |
[32m[20221213 20:43:43 @agent_ppo2.py:185][0m |          -0.0412 |           0.0077 |           0.0000 |
[32m[20221213 20:43:43 @agent_ppo2.py:185][0m |          -0.0462 |           0.0076 |           0.0000 |
[32m[20221213 20:43:43 @agent_ppo2.py:185][0m |          -0.0521 |           0.0075 |           0.0000 |
[32m[20221213 20:43:44 @agent_ppo2.py:185][0m |          -0.0560 |           0.0075 |           0.0000 |
[32m[20221213 20:43:44 @agent_ppo2.py:185][0m |          -0.0593 |           0.0074 |           0.0000 |
[32m[20221213 20:43:44 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:43:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.02
[32m[20221213 20:43:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.61
[32m[20221213 20:43:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.16
[32m[20221213 20:43:44 @agent_ppo2.py:143][0m Total time:      14.58 min
[32m[20221213 20:43:44 @agent_ppo2.py:145][0m 2093056 total steps have happened
[32m[20221213 20:43:44 @agent_ppo2.py:121][0m #------------------------ Iteration 511 --------------------------#
[32m[20221213 20:43:44 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:45 @agent_ppo2.py:185][0m |           0.0300 |           0.0089 |           0.0000 |
[32m[20221213 20:43:45 @agent_ppo2.py:185][0m |           0.0275 |           0.0059 |           0.0000 |
[32m[20221213 20:43:45 @agent_ppo2.py:185][0m |           0.0062 |           0.0056 |           0.0000 |
[32m[20221213 20:43:45 @agent_ppo2.py:185][0m |          -0.0038 |           0.0055 |           0.0000 |
[32m[20221213 20:43:45 @agent_ppo2.py:185][0m |          -0.0092 |           0.0054 |           0.0000 |
[32m[20221213 20:43:45 @agent_ppo2.py:185][0m |          -0.0131 |           0.0053 |           0.0000 |
[32m[20221213 20:43:45 @agent_ppo2.py:185][0m |          -0.0207 |           0.0052 |           0.0000 |
[32m[20221213 20:43:45 @agent_ppo2.py:185][0m |          -0.0217 |           0.0051 |           0.0000 |
[32m[20221213 20:43:45 @agent_ppo2.py:185][0m |          -0.0257 |           0.0051 |           0.0000 |
[32m[20221213 20:43:45 @agent_ppo2.py:185][0m |          -0.0272 |           0.0050 |           0.0000 |
[32m[20221213 20:43:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:43:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.32
[32m[20221213 20:43:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.43
[32m[20221213 20:43:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.73
[32m[20221213 20:43:46 @agent_ppo2.py:143][0m Total time:      14.61 min
[32m[20221213 20:43:46 @agent_ppo2.py:145][0m 2097152 total steps have happened
[32m[20221213 20:43:46 @agent_ppo2.py:121][0m #------------------------ Iteration 512 --------------------------#
[32m[20221213 20:43:46 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:43:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:46 @agent_ppo2.py:185][0m |           0.0273 |           0.1166 |           0.0000 |
[32m[20221213 20:43:46 @agent_ppo2.py:185][0m |          -0.0083 |           0.0395 |           0.0000 |
[32m[20221213 20:43:46 @agent_ppo2.py:185][0m |          -0.0135 |           0.0212 |           0.0000 |
[32m[20221213 20:43:47 @agent_ppo2.py:185][0m |          -0.0746 |           0.0222 |           0.0000 |
[32m[20221213 20:43:47 @agent_ppo2.py:185][0m |          -0.0242 |           0.0241 |           0.0000 |
[32m[20221213 20:43:47 @agent_ppo2.py:185][0m |          -0.0260 |           0.0168 |           0.0000 |
[32m[20221213 20:43:47 @agent_ppo2.py:185][0m |          -0.0269 |           0.0162 |           0.0000 |
[32m[20221213 20:43:47 @agent_ppo2.py:185][0m |          -0.0284 |           0.0161 |           0.0000 |
[32m[20221213 20:43:47 @agent_ppo2.py:185][0m |          -0.0293 |           0.0161 |           0.0000 |
[32m[20221213 20:43:47 @agent_ppo2.py:185][0m |          -0.0303 |           0.0158 |           0.0000 |
[32m[20221213 20:43:47 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:43:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.07
[32m[20221213 20:43:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.25
[32m[20221213 20:43:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.32
[32m[20221213 20:43:47 @agent_ppo2.py:143][0m Total time:      14.64 min
[32m[20221213 20:43:47 @agent_ppo2.py:145][0m 2101248 total steps have happened
[32m[20221213 20:43:47 @agent_ppo2.py:121][0m #------------------------ Iteration 513 --------------------------#
[32m[20221213 20:43:48 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:48 @agent_ppo2.py:185][0m |           0.0432 |           0.0171 |           0.0000 |
[32m[20221213 20:43:48 @agent_ppo2.py:185][0m |           0.0068 |           0.0166 |           0.0000 |
[32m[20221213 20:43:48 @agent_ppo2.py:185][0m |          -0.0387 |           0.0158 |           0.0000 |
[32m[20221213 20:43:48 @agent_ppo2.py:185][0m |          -0.0648 |           0.0152 |           0.0000 |
[32m[20221213 20:43:48 @agent_ppo2.py:185][0m |          -0.0699 |           0.0149 |           0.0000 |
[32m[20221213 20:43:48 @agent_ppo2.py:185][0m |          -0.0666 |           0.0151 |           0.0000 |
[32m[20221213 20:43:49 @agent_ppo2.py:185][0m |          -0.0873 |           0.0153 |           0.0000 |
[32m[20221213 20:43:49 @agent_ppo2.py:185][0m |          -0.0977 |           0.0143 |           0.0000 |
[32m[20221213 20:43:49 @agent_ppo2.py:185][0m |          -0.0896 |           0.0145 |           0.0000 |
[32m[20221213 20:43:49 @agent_ppo2.py:185][0m |          -0.1040 |           0.0144 |           0.0000 |
[32m[20221213 20:43:49 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:43:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.72
[32m[20221213 20:43:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.42
[32m[20221213 20:43:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.87
[32m[20221213 20:43:49 @agent_ppo2.py:143][0m Total time:      14.67 min
[32m[20221213 20:43:49 @agent_ppo2.py:145][0m 2105344 total steps have happened
[32m[20221213 20:43:49 @agent_ppo2.py:121][0m #------------------------ Iteration 514 --------------------------#
[32m[20221213 20:43:50 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:50 @agent_ppo2.py:185][0m |           0.0304 |           0.0171 |           0.0000 |
[32m[20221213 20:43:50 @agent_ppo2.py:185][0m |          -0.0294 |           0.0165 |           0.0000 |
[32m[20221213 20:43:50 @agent_ppo2.py:185][0m |          -0.0423 |           0.0157 |           0.0000 |
[32m[20221213 20:43:50 @agent_ppo2.py:185][0m |          -0.0592 |           0.0151 |           0.0000 |
[32m[20221213 20:43:50 @agent_ppo2.py:185][0m |          -0.0677 |           0.0149 |           0.0000 |
[32m[20221213 20:43:50 @agent_ppo2.py:185][0m |          -0.1231 |           0.0158 |           0.0000 |
[32m[20221213 20:43:50 @agent_ppo2.py:185][0m |          -0.0748 |           0.0167 |           0.0000 |
[32m[20221213 20:43:50 @agent_ppo2.py:185][0m |          -0.0768 |           0.0146 |           0.0000 |
[32m[20221213 20:43:50 @agent_ppo2.py:185][0m |          -0.0819 |           0.0142 |           0.0000 |
[32m[20221213 20:43:50 @agent_ppo2.py:185][0m |          -0.0775 |           0.0142 |           0.0000 |
[32m[20221213 20:43:50 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:43:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.75
[32m[20221213 20:43:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.19
[32m[20221213 20:43:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.46
[32m[20221213 20:43:51 @agent_ppo2.py:143][0m Total time:      14.70 min
[32m[20221213 20:43:51 @agent_ppo2.py:145][0m 2109440 total steps have happened
[32m[20221213 20:43:51 @agent_ppo2.py:121][0m #------------------------ Iteration 515 --------------------------#
[32m[20221213 20:43:51 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:43:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:51 @agent_ppo2.py:185][0m |           0.0355 |           0.0214 |           0.0000 |
[32m[20221213 20:43:52 @agent_ppo2.py:185][0m |           0.0109 |           0.0201 |           0.0000 |
[32m[20221213 20:43:52 @agent_ppo2.py:185][0m |          -0.0085 |           0.0193 |           0.0000 |
[32m[20221213 20:43:52 @agent_ppo2.py:185][0m |          -0.0146 |           0.0190 |           0.0000 |
[32m[20221213 20:43:52 @agent_ppo2.py:185][0m |          -0.0407 |           0.0196 |           0.0000 |
[32m[20221213 20:43:52 @agent_ppo2.py:185][0m |          -0.0436 |           0.0177 |           0.0000 |
[32m[20221213 20:43:52 @agent_ppo2.py:185][0m |          -0.0568 |           0.0170 |           0.0000 |
[32m[20221213 20:43:52 @agent_ppo2.py:185][0m |          -0.0576 |           0.0169 |           0.0000 |
[32m[20221213 20:43:52 @agent_ppo2.py:185][0m |          -0.0690 |           0.0166 |           0.0000 |
[32m[20221213 20:43:52 @agent_ppo2.py:185][0m |          -0.0695 |           0.0164 |           0.0000 |
[32m[20221213 20:43:52 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:43:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.37
[32m[20221213 20:43:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.64
[32m[20221213 20:43:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.17
[32m[20221213 20:43:53 @agent_ppo2.py:143][0m Total time:      14.73 min
[32m[20221213 20:43:53 @agent_ppo2.py:145][0m 2113536 total steps have happened
[32m[20221213 20:43:53 @agent_ppo2.py:121][0m #------------------------ Iteration 516 --------------------------#
[32m[20221213 20:43:53 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:43:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:53 @agent_ppo2.py:185][0m |           0.0309 |           0.0164 |           0.0000 |
[32m[20221213 20:43:53 @agent_ppo2.py:185][0m |           0.0157 |           0.0134 |           0.0000 |
[32m[20221213 20:43:53 @agent_ppo2.py:185][0m |          -0.0399 |           0.0129 |           0.0000 |
[32m[20221213 20:43:54 @agent_ppo2.py:185][0m |          -0.0442 |           0.0122 |           0.0000 |
[32m[20221213 20:43:54 @agent_ppo2.py:185][0m |          -0.0559 |           0.0119 |           0.0000 |
[32m[20221213 20:43:54 @agent_ppo2.py:185][0m |          -0.0600 |           0.0118 |           0.0000 |
[32m[20221213 20:43:54 @agent_ppo2.py:185][0m |          -0.0668 |           0.0116 |           0.0000 |
[32m[20221213 20:43:54 @agent_ppo2.py:185][0m |          -0.0614 |           0.0115 |           0.0000 |
[32m[20221213 20:43:54 @agent_ppo2.py:185][0m |          -0.0639 |           0.0113 |           0.0000 |
[32m[20221213 20:43:54 @agent_ppo2.py:185][0m |          -0.0817 |           0.0112 |           0.0000 |
[32m[20221213 20:43:54 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:43:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.92
[32m[20221213 20:43:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.62
[32m[20221213 20:43:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.95
[32m[20221213 20:43:55 @agent_ppo2.py:143][0m Total time:      14.76 min
[32m[20221213 20:43:55 @agent_ppo2.py:145][0m 2117632 total steps have happened
[32m[20221213 20:43:55 @agent_ppo2.py:121][0m #------------------------ Iteration 517 --------------------------#
[32m[20221213 20:43:55 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:43:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:55 @agent_ppo2.py:185][0m |           0.0251 |           0.0714 |           0.0000 |
[32m[20221213 20:43:55 @agent_ppo2.py:185][0m |          -0.0060 |           0.0333 |           0.0000 |
[32m[20221213 20:43:55 @agent_ppo2.py:185][0m |          -0.0158 |           0.0282 |           0.0000 |
[32m[20221213 20:43:55 @agent_ppo2.py:185][0m |          -0.0200 |           0.0257 |           0.0000 |
[32m[20221213 20:43:55 @agent_ppo2.py:185][0m |          -0.0238 |           0.0241 |           0.0000 |
[32m[20221213 20:43:56 @agent_ppo2.py:185][0m |          -0.0271 |           0.0228 |           0.0000 |
[32m[20221213 20:43:56 @agent_ppo2.py:185][0m |          -0.0291 |           0.0228 |           0.0000 |
[32m[20221213 20:43:56 @agent_ppo2.py:185][0m |          -0.0296 |           0.0216 |           0.0000 |
[32m[20221213 20:43:56 @agent_ppo2.py:185][0m |          -0.0309 |           0.0216 |           0.0000 |
[32m[20221213 20:43:56 @agent_ppo2.py:185][0m |          -0.0296 |           0.0216 |           0.0000 |
[32m[20221213 20:43:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:43:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.55
[32m[20221213 20:43:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.67
[32m[20221213 20:43:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.89
[32m[20221213 20:43:56 @agent_ppo2.py:143][0m Total time:      14.78 min
[32m[20221213 20:43:56 @agent_ppo2.py:145][0m 2121728 total steps have happened
[32m[20221213 20:43:56 @agent_ppo2.py:121][0m #------------------------ Iteration 518 --------------------------#
[32m[20221213 20:43:57 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:43:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:57 @agent_ppo2.py:185][0m |           0.0210 |           0.0246 |           0.0000 |
[32m[20221213 20:43:57 @agent_ppo2.py:185][0m |           0.0024 |           0.0136 |           0.0000 |
[32m[20221213 20:43:57 @agent_ppo2.py:185][0m |           0.0110 |           0.0129 |           0.0000 |
[32m[20221213 20:43:57 @agent_ppo2.py:185][0m |          -0.0378 |           0.0127 |           0.0000 |
[32m[20221213 20:43:57 @agent_ppo2.py:185][0m |          -0.0463 |           0.0124 |           0.0000 |
[32m[20221213 20:43:57 @agent_ppo2.py:185][0m |          -0.0493 |           0.0122 |           0.0000 |
[32m[20221213 20:43:57 @agent_ppo2.py:185][0m |          -0.0449 |           0.0121 |           0.0000 |
[32m[20221213 20:43:57 @agent_ppo2.py:185][0m |          -0.0413 |           0.0121 |           0.0000 |
[32m[20221213 20:43:57 @agent_ppo2.py:185][0m |          -0.0555 |           0.0119 |           0.0000 |
[32m[20221213 20:43:58 @agent_ppo2.py:185][0m |          -0.0608 |           0.0118 |           0.0000 |
[32m[20221213 20:43:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:43:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.59
[32m[20221213 20:43:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.84
[32m[20221213 20:43:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.37
[32m[20221213 20:43:58 @agent_ppo2.py:143][0m Total time:      14.81 min
[32m[20221213 20:43:58 @agent_ppo2.py:145][0m 2125824 total steps have happened
[32m[20221213 20:43:58 @agent_ppo2.py:121][0m #------------------------ Iteration 519 --------------------------#
[32m[20221213 20:43:58 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:43:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:43:59 @agent_ppo2.py:185][0m |           0.0320 |           0.0148 |           0.0000 |
[32m[20221213 20:43:59 @agent_ppo2.py:185][0m |          -0.0075 |           0.0137 |           0.0000 |
[32m[20221213 20:43:59 @agent_ppo2.py:185][0m |          -0.0356 |           0.0133 |           0.0000 |
[32m[20221213 20:43:59 @agent_ppo2.py:185][0m |          -0.0180 |           0.0143 |           0.0000 |
[32m[20221213 20:43:59 @agent_ppo2.py:185][0m |          -0.0617 |           0.0147 |           0.0000 |
[32m[20221213 20:43:59 @agent_ppo2.py:185][0m |          -0.0659 |           0.0127 |           0.0000 |
[32m[20221213 20:43:59 @agent_ppo2.py:185][0m |          -0.0679 |           0.0126 |           0.0000 |
[32m[20221213 20:43:59 @agent_ppo2.py:185][0m |          -0.0756 |           0.0124 |           0.0000 |
[32m[20221213 20:43:59 @agent_ppo2.py:185][0m |          -0.1056 |           0.0129 |           0.0000 |
[32m[20221213 20:43:59 @agent_ppo2.py:185][0m |          -0.0754 |           0.0125 |           0.0000 |
[32m[20221213 20:43:59 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:44:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.91
[32m[20221213 20:44:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.90
[32m[20221213 20:44:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.71
[32m[20221213 20:44:00 @agent_ppo2.py:143][0m Total time:      14.84 min
[32m[20221213 20:44:00 @agent_ppo2.py:145][0m 2129920 total steps have happened
[32m[20221213 20:44:00 @agent_ppo2.py:121][0m #------------------------ Iteration 520 --------------------------#
[32m[20221213 20:44:00 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:44:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:00 @agent_ppo2.py:185][0m |           0.0250 |           0.0461 |           0.0000 |
[32m[20221213 20:44:00 @agent_ppo2.py:185][0m |           0.0026 |           0.0223 |           0.0000 |
[32m[20221213 20:44:00 @agent_ppo2.py:185][0m |          -0.0070 |           0.0196 |           0.0000 |
[32m[20221213 20:44:00 @agent_ppo2.py:185][0m |          -0.0161 |           0.0187 |           0.0000 |
[32m[20221213 20:44:01 @agent_ppo2.py:185][0m |          -0.0200 |           0.0186 |           0.0000 |
[32m[20221213 20:44:01 @agent_ppo2.py:185][0m |          -0.0240 |           0.0178 |           0.0000 |
[32m[20221213 20:44:01 @agent_ppo2.py:185][0m |          -0.0270 |           0.0177 |           0.0000 |
[32m[20221213 20:44:01 @agent_ppo2.py:185][0m |          -0.0287 |           0.0178 |           0.0000 |
[32m[20221213 20:44:01 @agent_ppo2.py:185][0m |          -0.0296 |           0.0173 |           0.0000 |
[32m[20221213 20:44:01 @agent_ppo2.py:185][0m |          -0.0310 |           0.0176 |           0.0000 |
[32m[20221213 20:44:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:44:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.62
[32m[20221213 20:44:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.14
[32m[20221213 20:44:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.92
[32m[20221213 20:44:01 @agent_ppo2.py:143][0m Total time:      14.87 min
[32m[20221213 20:44:01 @agent_ppo2.py:145][0m 2134016 total steps have happened
[32m[20221213 20:44:01 @agent_ppo2.py:121][0m #------------------------ Iteration 521 --------------------------#
[32m[20221213 20:44:02 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:44:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:02 @agent_ppo2.py:185][0m |           0.0684 |           0.0274 |           0.0000 |
[32m[20221213 20:44:02 @agent_ppo2.py:185][0m |           0.0603 |           0.0126 |           0.0000 |
[32m[20221213 20:44:02 @agent_ppo2.py:185][0m |           0.0224 |           0.0121 |           0.0000 |
[32m[20221213 20:44:02 @agent_ppo2.py:185][0m |           0.0096 |           0.0118 |           0.0000 |
[32m[20221213 20:44:02 @agent_ppo2.py:185][0m |          -0.0020 |           0.0116 |           0.0000 |
[32m[20221213 20:44:02 @agent_ppo2.py:185][0m |          -0.0053 |           0.0115 |           0.0000 |
[32m[20221213 20:44:02 @agent_ppo2.py:185][0m |          -0.0150 |           0.0115 |           0.0000 |
[32m[20221213 20:44:02 @agent_ppo2.py:185][0m |          -0.0192 |           0.0113 |           0.0000 |
[32m[20221213 20:44:03 @agent_ppo2.py:185][0m |          -0.0249 |           0.0112 |           0.0000 |
[32m[20221213 20:44:03 @agent_ppo2.py:185][0m |          -0.0258 |           0.0111 |           0.0000 |
[32m[20221213 20:44:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:44:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.55
[32m[20221213 20:44:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.86
[32m[20221213 20:44:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.82
[32m[20221213 20:44:03 @agent_ppo2.py:143][0m Total time:      14.90 min
[32m[20221213 20:44:03 @agent_ppo2.py:145][0m 2138112 total steps have happened
[32m[20221213 20:44:03 @agent_ppo2.py:121][0m #------------------------ Iteration 522 --------------------------#
[32m[20221213 20:44:03 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:04 @agent_ppo2.py:185][0m |           0.0330 |           0.0206 |           0.0000 |
[32m[20221213 20:44:04 @agent_ppo2.py:185][0m |           0.0082 |           0.0189 |           0.0000 |
[32m[20221213 20:44:04 @agent_ppo2.py:185][0m |          -0.0112 |           0.0182 |           0.0000 |
[32m[20221213 20:44:04 @agent_ppo2.py:185][0m |          -0.0261 |           0.0177 |           0.0000 |
[32m[20221213 20:44:04 @agent_ppo2.py:185][0m |          -0.0256 |           0.0178 |           0.0000 |
[32m[20221213 20:44:04 @agent_ppo2.py:185][0m |          -0.0326 |           0.0172 |           0.0000 |
[32m[20221213 20:44:04 @agent_ppo2.py:185][0m |          -0.0373 |           0.0169 |           0.0000 |
[32m[20221213 20:44:04 @agent_ppo2.py:185][0m |          -0.0421 |           0.0168 |           0.0000 |
[32m[20221213 20:44:04 @agent_ppo2.py:185][0m |          -0.0445 |           0.0166 |           0.0000 |
[32m[20221213 20:44:04 @agent_ppo2.py:185][0m |          -0.0483 |           0.0163 |           0.0000 |
[32m[20221213 20:44:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:44:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.37
[32m[20221213 20:44:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.39
[32m[20221213 20:44:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.52
[32m[20221213 20:44:05 @agent_ppo2.py:143][0m Total time:      14.93 min
[32m[20221213 20:44:05 @agent_ppo2.py:145][0m 2142208 total steps have happened
[32m[20221213 20:44:05 @agent_ppo2.py:121][0m #------------------------ Iteration 523 --------------------------#
[32m[20221213 20:44:05 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:05 @agent_ppo2.py:185][0m |           0.0600 |           0.0182 |           0.0000 |
[32m[20221213 20:44:05 @agent_ppo2.py:185][0m |           0.0149 |           0.0112 |           0.0000 |
[32m[20221213 20:44:05 @agent_ppo2.py:185][0m |          -0.0019 |           0.0101 |           0.0000 |
[32m[20221213 20:44:06 @agent_ppo2.py:185][0m |          -0.0141 |           0.0100 |           0.0000 |
[32m[20221213 20:44:06 @agent_ppo2.py:185][0m |          -0.0212 |           0.0099 |           0.0000 |
[32m[20221213 20:44:06 @agent_ppo2.py:185][0m |          -0.0296 |           0.0098 |           0.0000 |
[32m[20221213 20:44:06 @agent_ppo2.py:185][0m |          -0.0319 |           0.0097 |           0.0000 |
[32m[20221213 20:44:06 @agent_ppo2.py:185][0m |          -0.0389 |           0.0096 |           0.0000 |
[32m[20221213 20:44:06 @agent_ppo2.py:185][0m |          -0.0396 |           0.0095 |           0.0000 |
[32m[20221213 20:44:06 @agent_ppo2.py:185][0m |          -0.0199 |           0.0096 |           0.0000 |
[32m[20221213 20:44:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:44:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.69
[32m[20221213 20:44:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.07
[32m[20221213 20:44:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.52
[32m[20221213 20:44:06 @agent_ppo2.py:143][0m Total time:      14.95 min
[32m[20221213 20:44:06 @agent_ppo2.py:145][0m 2146304 total steps have happened
[32m[20221213 20:44:06 @agent_ppo2.py:121][0m #------------------------ Iteration 524 --------------------------#
[32m[20221213 20:44:07 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:07 @agent_ppo2.py:185][0m |           0.0421 |           0.0096 |           0.0000 |
[32m[20221213 20:44:07 @agent_ppo2.py:185][0m |           0.0098 |           0.0087 |           0.0000 |
[32m[20221213 20:44:07 @agent_ppo2.py:185][0m |          -0.0142 |           0.0086 |           0.0000 |
[32m[20221213 20:44:07 @agent_ppo2.py:185][0m |          -0.0309 |           0.0085 |           0.0000 |
[32m[20221213 20:44:07 @agent_ppo2.py:185][0m |          -0.0505 |           0.0085 |           0.0000 |
[32m[20221213 20:44:07 @agent_ppo2.py:185][0m |          -0.0480 |           0.0084 |           0.0000 |
[32m[20221213 20:44:07 @agent_ppo2.py:185][0m |          -0.0527 |           0.0083 |           0.0000 |
[32m[20221213 20:44:08 @agent_ppo2.py:185][0m |          -0.0566 |           0.0083 |           0.0000 |
[32m[20221213 20:44:08 @agent_ppo2.py:185][0m |          -0.0666 |           0.0083 |           0.0000 |
[32m[20221213 20:44:08 @agent_ppo2.py:185][0m |          -0.0613 |           0.0082 |           0.0000 |
[32m[20221213 20:44:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:44:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.63
[32m[20221213 20:44:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.36
[32m[20221213 20:44:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.46
[32m[20221213 20:44:08 @agent_ppo2.py:143][0m Total time:      14.98 min
[32m[20221213 20:44:08 @agent_ppo2.py:145][0m 2150400 total steps have happened
[32m[20221213 20:44:08 @agent_ppo2.py:121][0m #------------------------ Iteration 525 --------------------------#
[32m[20221213 20:44:08 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:44:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:09 @agent_ppo2.py:185][0m |           0.0389 |           0.0078 |           0.0000 |
[32m[20221213 20:44:09 @agent_ppo2.py:185][0m |          -0.0024 |           0.0075 |           0.0000 |
[32m[20221213 20:44:09 @agent_ppo2.py:185][0m |          -0.0435 |           0.0074 |           0.0000 |
[32m[20221213 20:44:09 @agent_ppo2.py:185][0m |          -0.0459 |           0.0074 |           0.0000 |
[32m[20221213 20:44:09 @agent_ppo2.py:185][0m |          -0.0593 |           0.0073 |           0.0000 |
[32m[20221213 20:44:09 @agent_ppo2.py:185][0m |          -0.0622 |           0.0072 |           0.0000 |
[32m[20221213 20:44:09 @agent_ppo2.py:185][0m |          -0.0512 |           0.0072 |           0.0000 |
[32m[20221213 20:44:09 @agent_ppo2.py:185][0m |          -0.0654 |           0.0072 |           0.0000 |
[32m[20221213 20:44:09 @agent_ppo2.py:185][0m |          -0.0773 |           0.0071 |           0.0000 |
[32m[20221213 20:44:09 @agent_ppo2.py:185][0m |          -0.0751 |           0.0071 |           0.0000 |
[32m[20221213 20:44:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:44:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.11
[32m[20221213 20:44:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.63
[32m[20221213 20:44:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.58
[32m[20221213 20:44:10 @agent_ppo2.py:143][0m Total time:      15.01 min
[32m[20221213 20:44:10 @agent_ppo2.py:145][0m 2154496 total steps have happened
[32m[20221213 20:44:10 @agent_ppo2.py:121][0m #------------------------ Iteration 526 --------------------------#
[32m[20221213 20:44:10 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:10 @agent_ppo2.py:185][0m |           0.0467 |           0.0075 |           0.0000 |
[32m[20221213 20:44:10 @agent_ppo2.py:185][0m |           0.0076 |           0.0063 |           0.0000 |
[32m[20221213 20:44:11 @agent_ppo2.py:185][0m |          -0.0019 |           0.0062 |           0.0000 |
[32m[20221213 20:44:11 @agent_ppo2.py:185][0m |          -0.0158 |           0.0061 |           0.0000 |
[32m[20221213 20:44:11 @agent_ppo2.py:185][0m |          -0.0199 |           0.0060 |           0.0000 |
[32m[20221213 20:44:11 @agent_ppo2.py:185][0m |          -0.0262 |           0.0060 |           0.0000 |
[32m[20221213 20:44:11 @agent_ppo2.py:185][0m |          -0.0295 |           0.0059 |           0.0000 |
[32m[20221213 20:44:11 @agent_ppo2.py:185][0m |          -0.0308 |           0.0059 |           0.0000 |
[32m[20221213 20:44:11 @agent_ppo2.py:185][0m |          -0.0332 |           0.0058 |           0.0000 |
[32m[20221213 20:44:11 @agent_ppo2.py:185][0m |          -0.0355 |           0.0058 |           0.0000 |
[32m[20221213 20:44:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:44:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.13
[32m[20221213 20:44:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.55
[32m[20221213 20:44:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.56
[32m[20221213 20:44:11 @agent_ppo2.py:143][0m Total time:      15.04 min
[32m[20221213 20:44:11 @agent_ppo2.py:145][0m 2158592 total steps have happened
[32m[20221213 20:44:11 @agent_ppo2.py:121][0m #------------------------ Iteration 527 --------------------------#
[32m[20221213 20:44:12 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:44:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:12 @agent_ppo2.py:185][0m |           0.0278 |           0.0066 |           0.0000 |
[32m[20221213 20:44:12 @agent_ppo2.py:185][0m |          -0.0081 |           0.0064 |           0.0000 |
[32m[20221213 20:44:12 @agent_ppo2.py:185][0m |          -0.0176 |           0.0063 |           0.0000 |
[32m[20221213 20:44:12 @agent_ppo2.py:185][0m |          -0.0174 |           0.0063 |           0.0000 |
[32m[20221213 20:44:12 @agent_ppo2.py:185][0m |          -0.0637 |           0.0066 |           0.0000 |
[32m[20221213 20:44:12 @agent_ppo2.py:185][0m |          -0.0060 |           0.0063 |           0.0000 |
[32m[20221213 20:44:13 @agent_ppo2.py:185][0m |          -0.0043 |           0.0062 |           0.0000 |
[32m[20221213 20:44:13 @agent_ppo2.py:185][0m |          -0.0145 |           0.0061 |           0.0000 |
[32m[20221213 20:44:13 @agent_ppo2.py:185][0m |          -0.0227 |           0.0061 |           0.0000 |
[32m[20221213 20:44:13 @agent_ppo2.py:185][0m |          -0.0275 |           0.0061 |           0.0000 |
[32m[20221213 20:44:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:44:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.79
[32m[20221213 20:44:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.44
[32m[20221213 20:44:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.62
[32m[20221213 20:44:13 @agent_ppo2.py:143][0m Total time:      15.07 min
[32m[20221213 20:44:13 @agent_ppo2.py:145][0m 2162688 total steps have happened
[32m[20221213 20:44:13 @agent_ppo2.py:121][0m #------------------------ Iteration 528 --------------------------#
[32m[20221213 20:44:14 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:44:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:14 @agent_ppo2.py:185][0m |           0.0195 |           0.0190 |           0.0000 |
[32m[20221213 20:44:14 @agent_ppo2.py:185][0m |           0.0065 |           0.0113 |           0.0000 |
[32m[20221213 20:44:14 @agent_ppo2.py:185][0m |          -0.0053 |           0.0102 |           0.0000 |
[32m[20221213 20:44:14 @agent_ppo2.py:185][0m |          -0.0181 |           0.0099 |           0.0000 |
[32m[20221213 20:44:14 @agent_ppo2.py:185][0m |          -0.0176 |           0.0096 |           0.0000 |
[32m[20221213 20:44:14 @agent_ppo2.py:185][0m |          -0.0186 |           0.0094 |           0.0000 |
[32m[20221213 20:44:14 @agent_ppo2.py:185][0m |          -0.0117 |           0.0093 |           0.0000 |
[32m[20221213 20:44:14 @agent_ppo2.py:185][0m |          -0.0176 |           0.0093 |           0.0000 |
[32m[20221213 20:44:14 @agent_ppo2.py:185][0m |          -0.0252 |           0.0092 |           0.0000 |
[32m[20221213 20:44:15 @agent_ppo2.py:185][0m |          -0.0252 |           0.0092 |           0.0000 |
[32m[20221213 20:44:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:44:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.36
[32m[20221213 20:44:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.70
[32m[20221213 20:44:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.19
[32m[20221213 20:44:15 @agent_ppo2.py:143][0m Total time:      15.10 min
[32m[20221213 20:44:15 @agent_ppo2.py:145][0m 2166784 total steps have happened
[32m[20221213 20:44:15 @agent_ppo2.py:121][0m #------------------------ Iteration 529 --------------------------#
[32m[20221213 20:44:15 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:15 @agent_ppo2.py:185][0m |           0.0201 |           0.0334 |           0.0000 |
[32m[20221213 20:44:16 @agent_ppo2.py:185][0m |           0.0102 |           0.0224 |           0.0000 |
[32m[20221213 20:44:16 @agent_ppo2.py:185][0m |          -0.0090 |           0.0184 |           0.0000 |
[32m[20221213 20:44:16 @agent_ppo2.py:185][0m |          -0.0552 |           0.0163 |           0.0000 |
[32m[20221213 20:44:16 @agent_ppo2.py:185][0m |          -0.0235 |           0.0163 |           0.0000 |
[32m[20221213 20:44:16 @agent_ppo2.py:185][0m |          -0.0289 |           0.0145 |           0.0000 |
[32m[20221213 20:44:16 @agent_ppo2.py:185][0m |          -0.0304 |           0.0141 |           0.0000 |
[32m[20221213 20:44:16 @agent_ppo2.py:185][0m |          -0.0335 |           0.0140 |           0.0000 |
[32m[20221213 20:44:16 @agent_ppo2.py:185][0m |          -0.0359 |           0.0137 |           0.0000 |
[32m[20221213 20:44:16 @agent_ppo2.py:185][0m |          -0.0382 |           0.0139 |           0.0000 |
[32m[20221213 20:44:16 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:44:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.69
[32m[20221213 20:44:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.18
[32m[20221213 20:44:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.32
[32m[20221213 20:44:17 @agent_ppo2.py:143][0m Total time:      15.12 min
[32m[20221213 20:44:17 @agent_ppo2.py:145][0m 2170880 total steps have happened
[32m[20221213 20:44:17 @agent_ppo2.py:121][0m #------------------------ Iteration 530 --------------------------#
[32m[20221213 20:44:17 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:44:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:17 @agent_ppo2.py:185][0m |           0.0255 |           0.0212 |           0.0000 |
[32m[20221213 20:44:17 @agent_ppo2.py:185][0m |           0.0020 |           0.0097 |           0.0000 |
[32m[20221213 20:44:17 @agent_ppo2.py:185][0m |          -0.0125 |           0.0092 |           0.0000 |
[32m[20221213 20:44:17 @agent_ppo2.py:185][0m |          -0.0169 |           0.0091 |           0.0000 |
[32m[20221213 20:44:18 @agent_ppo2.py:185][0m |          -0.0206 |           0.0088 |           0.0000 |
[32m[20221213 20:44:18 @agent_ppo2.py:185][0m |          -0.0347 |           0.0086 |           0.0000 |
[32m[20221213 20:44:18 @agent_ppo2.py:185][0m |          -0.0352 |           0.0085 |           0.0000 |
[32m[20221213 20:44:18 @agent_ppo2.py:185][0m |          -0.0363 |           0.0084 |           0.0000 |
[32m[20221213 20:44:18 @agent_ppo2.py:185][0m |          -0.0421 |           0.0083 |           0.0000 |
[32m[20221213 20:44:18 @agent_ppo2.py:185][0m |          -0.0450 |           0.0083 |           0.0000 |
[32m[20221213 20:44:18 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 20:44:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.97
[32m[20221213 20:44:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.23
[32m[20221213 20:44:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.60
[32m[20221213 20:44:18 @agent_ppo2.py:143][0m Total time:      15.15 min
[32m[20221213 20:44:18 @agent_ppo2.py:145][0m 2174976 total steps have happened
[32m[20221213 20:44:18 @agent_ppo2.py:121][0m #------------------------ Iteration 531 --------------------------#
[32m[20221213 20:44:19 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:44:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:19 @agent_ppo2.py:185][0m |           0.0231 |           0.0347 |           0.0000 |
[32m[20221213 20:44:19 @agent_ppo2.py:185][0m |          -0.0016 |           0.0160 |           0.0000 |
[32m[20221213 20:44:19 @agent_ppo2.py:185][0m |          -0.0144 |           0.0148 |           0.0000 |
[32m[20221213 20:44:19 @agent_ppo2.py:185][0m |          -0.0195 |           0.0148 |           0.0000 |
[32m[20221213 20:44:19 @agent_ppo2.py:185][0m |          -0.0247 |           0.0140 |           0.0000 |
[32m[20221213 20:44:19 @agent_ppo2.py:185][0m |          -0.0410 |           0.0141 |           0.0000 |
[32m[20221213 20:44:19 @agent_ppo2.py:185][0m |          -0.0308 |           0.0136 |           0.0000 |
[32m[20221213 20:44:20 @agent_ppo2.py:185][0m |          -0.0318 |           0.0135 |           0.0000 |
[32m[20221213 20:44:20 @agent_ppo2.py:185][0m |          -0.0330 |           0.0133 |           0.0000 |
[32m[20221213 20:44:20 @agent_ppo2.py:185][0m |          -0.0337 |           0.0134 |           0.0000 |
[32m[20221213 20:44:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:44:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.06
[32m[20221213 20:44:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.49
[32m[20221213 20:44:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.19
[32m[20221213 20:44:20 @agent_ppo2.py:143][0m Total time:      15.18 min
[32m[20221213 20:44:20 @agent_ppo2.py:145][0m 2179072 total steps have happened
[32m[20221213 20:44:20 @agent_ppo2.py:121][0m #------------------------ Iteration 532 --------------------------#
[32m[20221213 20:44:21 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:44:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:21 @agent_ppo2.py:185][0m |           0.0495 |           0.0165 |           0.0000 |
[32m[20221213 20:44:21 @agent_ppo2.py:185][0m |           0.0110 |           0.0149 |           0.0000 |
[32m[20221213 20:44:21 @agent_ppo2.py:185][0m |          -0.0151 |           0.0148 |           0.0000 |
[32m[20221213 20:44:21 @agent_ppo2.py:185][0m |          -0.0324 |           0.0149 |           0.0000 |
[32m[20221213 20:44:21 @agent_ppo2.py:185][0m |          -0.0332 |           0.0135 |           0.0000 |
[32m[20221213 20:44:21 @agent_ppo2.py:185][0m |          -0.0539 |           0.0134 |           0.0000 |
[32m[20221213 20:44:21 @agent_ppo2.py:185][0m |          -0.0530 |           0.0132 |           0.0000 |
[32m[20221213 20:44:21 @agent_ppo2.py:185][0m |          -0.0545 |           0.0131 |           0.0000 |
[32m[20221213 20:44:22 @agent_ppo2.py:185][0m |          -0.0577 |           0.0131 |           0.0000 |
[32m[20221213 20:44:22 @agent_ppo2.py:185][0m |          -0.0599 |           0.0130 |           0.0000 |
[32m[20221213 20:44:22 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 20:44:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.99
[32m[20221213 20:44:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.63
[32m[20221213 20:44:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.00
[32m[20221213 20:44:22 @agent_ppo2.py:143][0m Total time:      15.21 min
[32m[20221213 20:44:22 @agent_ppo2.py:145][0m 2183168 total steps have happened
[32m[20221213 20:44:22 @agent_ppo2.py:121][0m #------------------------ Iteration 533 --------------------------#
[32m[20221213 20:44:22 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:44:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:23 @agent_ppo2.py:185][0m |           0.0383 |           0.0164 |           0.0000 |
[32m[20221213 20:44:23 @agent_ppo2.py:185][0m |          -0.0015 |           0.0157 |           0.0000 |
[32m[20221213 20:44:23 @agent_ppo2.py:185][0m |          -0.0288 |           0.0153 |           0.0000 |
[32m[20221213 20:44:23 @agent_ppo2.py:185][0m |          -0.0401 |           0.0149 |           0.0000 |
[32m[20221213 20:44:23 @agent_ppo2.py:185][0m |          -0.0494 |           0.0149 |           0.0000 |
[32m[20221213 20:44:23 @agent_ppo2.py:185][0m |          -0.0542 |           0.0146 |           0.0000 |
[32m[20221213 20:44:23 @agent_ppo2.py:185][0m |          -0.0525 |           0.0147 |           0.0000 |
[32m[20221213 20:44:23 @agent_ppo2.py:185][0m |          -0.0645 |           0.0142 |           0.0000 |
[32m[20221213 20:44:23 @agent_ppo2.py:185][0m |          -0.0812 |           0.0143 |           0.0000 |
[32m[20221213 20:44:23 @agent_ppo2.py:185][0m |          -0.0593 |           0.0146 |           0.0000 |
[32m[20221213 20:44:23 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:44:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.58
[32m[20221213 20:44:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.66
[32m[20221213 20:44:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.76
[32m[20221213 20:44:24 @agent_ppo2.py:143][0m Total time:      15.24 min
[32m[20221213 20:44:24 @agent_ppo2.py:145][0m 2187264 total steps have happened
[32m[20221213 20:44:24 @agent_ppo2.py:121][0m #------------------------ Iteration 534 --------------------------#
[32m[20221213 20:44:24 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:44:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:24 @agent_ppo2.py:185][0m |           0.0128 |           0.0188 |           0.0000 |
[32m[20221213 20:44:24 @agent_ppo2.py:185][0m |           0.0295 |           0.0095 |           0.0000 |
[32m[20221213 20:44:25 @agent_ppo2.py:185][0m |          -0.0099 |           0.0092 |           0.0000 |
[32m[20221213 20:44:25 @agent_ppo2.py:185][0m |          -0.0206 |           0.0088 |           0.0000 |
[32m[20221213 20:44:25 @agent_ppo2.py:185][0m |          -0.0221 |           0.0086 |           0.0000 |
[32m[20221213 20:44:25 @agent_ppo2.py:185][0m |          -0.0300 |           0.0086 |           0.0000 |
[32m[20221213 20:44:25 @agent_ppo2.py:185][0m |          -0.0103 |           0.0086 |           0.0000 |
[32m[20221213 20:44:25 @agent_ppo2.py:185][0m |          -0.0286 |           0.0085 |           0.0000 |
[32m[20221213 20:44:25 @agent_ppo2.py:185][0m |          -0.0320 |           0.0084 |           0.0000 |
[32m[20221213 20:44:25 @agent_ppo2.py:185][0m |          -0.0250 |           0.0084 |           0.0000 |
[32m[20221213 20:44:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:44:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.31
[32m[20221213 20:44:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.51
[32m[20221213 20:44:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.60
[32m[20221213 20:44:26 @agent_ppo2.py:143][0m Total time:      15.27 min
[32m[20221213 20:44:26 @agent_ppo2.py:145][0m 2191360 total steps have happened
[32m[20221213 20:44:26 @agent_ppo2.py:121][0m #------------------------ Iteration 535 --------------------------#
[32m[20221213 20:44:26 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:26 @agent_ppo2.py:185][0m |           0.0308 |           0.0403 |           0.0000 |
[32m[20221213 20:44:26 @agent_ppo2.py:185][0m |          -0.0014 |           0.0181 |           0.0000 |
[32m[20221213 20:44:26 @agent_ppo2.py:185][0m |          -0.0095 |           0.0156 |           0.0000 |
[32m[20221213 20:44:26 @agent_ppo2.py:185][0m |          -0.0203 |           0.0154 |           0.0000 |
[32m[20221213 20:44:26 @agent_ppo2.py:185][0m |          -0.0249 |           0.0146 |           0.0000 |
[32m[20221213 20:44:27 @agent_ppo2.py:185][0m |          -0.0284 |           0.0148 |           0.0000 |
[32m[20221213 20:44:27 @agent_ppo2.py:185][0m |          -0.0296 |           0.0144 |           0.0000 |
[32m[20221213 20:44:27 @agent_ppo2.py:185][0m |          -0.0302 |           0.0141 |           0.0000 |
[32m[20221213 20:44:27 @agent_ppo2.py:185][0m |          -0.0352 |           0.0141 |           0.0000 |
[32m[20221213 20:44:27 @agent_ppo2.py:185][0m |          -0.0347 |           0.0145 |           0.0000 |
[32m[20221213 20:44:27 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:44:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.83
[32m[20221213 20:44:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.20
[32m[20221213 20:44:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.58
[32m[20221213 20:44:27 @agent_ppo2.py:143][0m Total time:      15.30 min
[32m[20221213 20:44:27 @agent_ppo2.py:145][0m 2195456 total steps have happened
[32m[20221213 20:44:27 @agent_ppo2.py:121][0m #------------------------ Iteration 536 --------------------------#
[32m[20221213 20:44:28 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:44:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:28 @agent_ppo2.py:185][0m |           0.0471 |           0.0200 |           0.0000 |
[32m[20221213 20:44:28 @agent_ppo2.py:185][0m |           0.0222 |           0.0177 |           0.0000 |
[32m[20221213 20:44:28 @agent_ppo2.py:185][0m |          -0.0116 |           0.0161 |           0.0000 |
[32m[20221213 20:44:28 @agent_ppo2.py:185][0m |          -0.0370 |           0.0165 |           0.0000 |
[32m[20221213 20:44:28 @agent_ppo2.py:185][0m |          -0.0460 |           0.0161 |           0.0000 |
[32m[20221213 20:44:28 @agent_ppo2.py:185][0m |          -0.0485 |           0.0154 |           0.0000 |
[32m[20221213 20:44:28 @agent_ppo2.py:185][0m |          -0.0527 |           0.0155 |           0.0000 |
[32m[20221213 20:44:29 @agent_ppo2.py:185][0m |          -0.0502 |           0.0151 |           0.0000 |
[32m[20221213 20:44:29 @agent_ppo2.py:185][0m |          -0.0520 |           0.0151 |           0.0000 |
[32m[20221213 20:44:29 @agent_ppo2.py:185][0m |          -0.0635 |           0.0153 |           0.0000 |
[32m[20221213 20:44:29 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:44:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.56
[32m[20221213 20:44:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.38
[32m[20221213 20:44:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.44
[32m[20221213 20:44:29 @agent_ppo2.py:143][0m Total time:      15.33 min
[32m[20221213 20:44:29 @agent_ppo2.py:145][0m 2199552 total steps have happened
[32m[20221213 20:44:29 @agent_ppo2.py:121][0m #------------------------ Iteration 537 --------------------------#
[32m[20221213 20:44:30 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:44:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:30 @agent_ppo2.py:185][0m |           0.0371 |           0.0417 |           0.0000 |
[32m[20221213 20:44:30 @agent_ppo2.py:185][0m |           0.0043 |           0.0298 |           0.0000 |
[32m[20221213 20:44:30 @agent_ppo2.py:185][0m |          -0.0057 |           0.0241 |           0.0000 |
[32m[20221213 20:44:30 @agent_ppo2.py:185][0m |          -0.0104 |           0.0225 |           0.0000 |
[32m[20221213 20:44:30 @agent_ppo2.py:185][0m |          -0.0214 |           0.0211 |           0.0000 |
[32m[20221213 20:44:30 @agent_ppo2.py:185][0m |          -0.0195 |           0.0201 |           0.0000 |
[32m[20221213 20:44:30 @agent_ppo2.py:185][0m |          -0.0222 |           0.0194 |           0.0000 |
[32m[20221213 20:44:30 @agent_ppo2.py:185][0m |          -0.0313 |           0.0197 |           0.0000 |
[32m[20221213 20:44:30 @agent_ppo2.py:185][0m |          -0.0364 |           0.0193 |           0.0000 |
[32m[20221213 20:44:30 @agent_ppo2.py:185][0m |          -0.0382 |           0.0190 |           0.0000 |
[32m[20221213 20:44:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:44:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.02
[32m[20221213 20:44:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.76
[32m[20221213 20:44:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.05
[32m[20221213 20:44:31 @agent_ppo2.py:143][0m Total time:      15.36 min
[32m[20221213 20:44:31 @agent_ppo2.py:145][0m 2203648 total steps have happened
[32m[20221213 20:44:31 @agent_ppo2.py:121][0m #------------------------ Iteration 538 --------------------------#
[32m[20221213 20:44:31 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:44:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:31 @agent_ppo2.py:185][0m |           0.0369 |           0.0423 |           0.0000 |
[32m[20221213 20:44:31 @agent_ppo2.py:185][0m |           0.0102 |           0.0154 |           0.0000 |
[32m[20221213 20:44:32 @agent_ppo2.py:185][0m |          -0.0072 |           0.0144 |           0.0000 |
[32m[20221213 20:44:32 @agent_ppo2.py:185][0m |          -0.0195 |           0.0138 |           0.0000 |
[32m[20221213 20:44:32 @agent_ppo2.py:185][0m |          -0.0284 |           0.0136 |           0.0000 |
[32m[20221213 20:44:32 @agent_ppo2.py:185][0m |          -0.0340 |           0.0133 |           0.0000 |
[32m[20221213 20:44:32 @agent_ppo2.py:185][0m |          -0.0375 |           0.0134 |           0.0000 |
[32m[20221213 20:44:32 @agent_ppo2.py:185][0m |          -0.0445 |           0.0133 |           0.0000 |
[32m[20221213 20:44:32 @agent_ppo2.py:185][0m |          -0.0451 |           0.0129 |           0.0000 |
[32m[20221213 20:44:32 @agent_ppo2.py:185][0m |          -0.0458 |           0.0127 |           0.0000 |
[32m[20221213 20:44:32 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:44:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.37
[32m[20221213 20:44:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.10
[32m[20221213 20:44:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.21
[32m[20221213 20:44:32 @agent_ppo2.py:143][0m Total time:      15.39 min
[32m[20221213 20:44:32 @agent_ppo2.py:145][0m 2207744 total steps have happened
[32m[20221213 20:44:32 @agent_ppo2.py:121][0m #------------------------ Iteration 539 --------------------------#
[32m[20221213 20:44:33 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:33 @agent_ppo2.py:185][0m |           0.0632 |           0.0107 |           0.0000 |
[32m[20221213 20:44:33 @agent_ppo2.py:185][0m |          -0.0101 |           0.0101 |           0.0000 |
[32m[20221213 20:44:33 @agent_ppo2.py:185][0m |          -0.0125 |           0.0098 |           0.0000 |
[32m[20221213 20:44:33 @agent_ppo2.py:185][0m |          -0.0381 |           0.0098 |           0.0000 |
[32m[20221213 20:44:33 @agent_ppo2.py:185][0m |          -0.0646 |           0.0097 |           0.0000 |
[32m[20221213 20:44:33 @agent_ppo2.py:185][0m |          -0.0727 |           0.0096 |           0.0000 |
[32m[20221213 20:44:33 @agent_ppo2.py:185][0m |          -0.0697 |           0.0095 |           0.0000 |
[32m[20221213 20:44:34 @agent_ppo2.py:185][0m |          -0.0673 |           0.0095 |           0.0000 |
[32m[20221213 20:44:34 @agent_ppo2.py:185][0m |          -0.0762 |           0.0094 |           0.0000 |
[32m[20221213 20:44:34 @agent_ppo2.py:185][0m |          -0.0776 |           0.0094 |           0.0000 |
[32m[20221213 20:44:34 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:44:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.00
[32m[20221213 20:44:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.59
[32m[20221213 20:44:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.26
[32m[20221213 20:44:34 @agent_ppo2.py:143][0m Total time:      15.42 min
[32m[20221213 20:44:34 @agent_ppo2.py:145][0m 2211840 total steps have happened
[32m[20221213 20:44:34 @agent_ppo2.py:121][0m #------------------------ Iteration 540 --------------------------#
[32m[20221213 20:44:35 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:44:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:35 @agent_ppo2.py:185][0m |           0.0361 |           0.0092 |           0.0000 |
[32m[20221213 20:44:35 @agent_ppo2.py:185][0m |           0.0004 |           0.0080 |           0.0000 |
[32m[20221213 20:44:35 @agent_ppo2.py:185][0m |          -0.0257 |           0.0079 |           0.0000 |
[32m[20221213 20:44:35 @agent_ppo2.py:185][0m |          -0.0358 |           0.0078 |           0.0000 |
[32m[20221213 20:44:35 @agent_ppo2.py:185][0m |          -0.0457 |           0.0077 |           0.0000 |
[32m[20221213 20:44:35 @agent_ppo2.py:185][0m |          -0.0210 |           0.0077 |           0.0000 |
[32m[20221213 20:44:35 @agent_ppo2.py:185][0m |          -0.0539 |           0.0076 |           0.0000 |
[32m[20221213 20:44:35 @agent_ppo2.py:185][0m |          -0.0587 |           0.0076 |           0.0000 |
[32m[20221213 20:44:35 @agent_ppo2.py:185][0m |          -0.0553 |           0.0075 |           0.0000 |
[32m[20221213 20:44:35 @agent_ppo2.py:185][0m |          -0.0588 |           0.0075 |           0.0000 |
[32m[20221213 20:44:35 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:44:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.91
[32m[20221213 20:44:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.07
[32m[20221213 20:44:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.98
[32m[20221213 20:44:36 @agent_ppo2.py:143][0m Total time:      15.44 min
[32m[20221213 20:44:36 @agent_ppo2.py:145][0m 2215936 total steps have happened
[32m[20221213 20:44:36 @agent_ppo2.py:121][0m #------------------------ Iteration 541 --------------------------#
[32m[20221213 20:44:36 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:36 @agent_ppo2.py:185][0m |           0.0234 |           0.0093 |           0.0000 |
[32m[20221213 20:44:36 @agent_ppo2.py:185][0m |           0.0025 |           0.0091 |           0.0000 |
[32m[20221213 20:44:37 @agent_ppo2.py:185][0m |          -0.0153 |           0.0089 |           0.0000 |
[32m[20221213 20:44:37 @agent_ppo2.py:185][0m |          -0.0245 |           0.0089 |           0.0000 |
[32m[20221213 20:44:37 @agent_ppo2.py:185][0m |          -0.0661 |           0.0097 |           0.0000 |
[32m[20221213 20:44:37 @agent_ppo2.py:185][0m |          -0.0054 |           0.0091 |           0.0000 |
[32m[20221213 20:44:37 @agent_ppo2.py:185][0m |          -0.0113 |           0.0087 |           0.0000 |
[32m[20221213 20:44:37 @agent_ppo2.py:185][0m |          -0.0161 |           0.0086 |           0.0000 |
[32m[20221213 20:44:37 @agent_ppo2.py:185][0m |          -0.0254 |           0.0086 |           0.0000 |
[32m[20221213 20:44:37 @agent_ppo2.py:185][0m |          -0.0288 |           0.0085 |           0.0000 |
[32m[20221213 20:44:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:44:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.67
[32m[20221213 20:44:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.76
[32m[20221213 20:44:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.00
[32m[20221213 20:44:37 @agent_ppo2.py:143][0m Total time:      15.47 min
[32m[20221213 20:44:37 @agent_ppo2.py:145][0m 2220032 total steps have happened
[32m[20221213 20:44:37 @agent_ppo2.py:121][0m #------------------------ Iteration 542 --------------------------#
[32m[20221213 20:44:38 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:38 @agent_ppo2.py:185][0m |           0.0304 |           0.0448 |           0.0000 |
[32m[20221213 20:44:38 @agent_ppo2.py:185][0m |           0.0113 |           0.0201 |           0.0000 |
[32m[20221213 20:44:38 @agent_ppo2.py:185][0m |          -0.0004 |           0.0171 |           0.0000 |
[32m[20221213 20:44:38 @agent_ppo2.py:185][0m |          -0.0130 |           0.0165 |           0.0000 |
[32m[20221213 20:44:38 @agent_ppo2.py:185][0m |          -0.0221 |           0.0162 |           0.0000 |
[32m[20221213 20:44:38 @agent_ppo2.py:185][0m |          -0.0608 |           0.0159 |           0.0000 |
[32m[20221213 20:44:38 @agent_ppo2.py:185][0m |          -0.0242 |           0.0150 |           0.0000 |
[32m[20221213 20:44:39 @agent_ppo2.py:185][0m |          -0.0260 |           0.0144 |           0.0000 |
[32m[20221213 20:44:39 @agent_ppo2.py:185][0m |          -0.0281 |           0.0143 |           0.0000 |
[32m[20221213 20:44:39 @agent_ppo2.py:185][0m |          -0.0753 |           0.0152 |           0.0000 |
[32m[20221213 20:44:39 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:44:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.52
[32m[20221213 20:44:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.43
[32m[20221213 20:44:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.68
[32m[20221213 20:44:39 @agent_ppo2.py:143][0m Total time:      15.50 min
[32m[20221213 20:44:39 @agent_ppo2.py:145][0m 2224128 total steps have happened
[32m[20221213 20:44:39 @agent_ppo2.py:121][0m #------------------------ Iteration 543 --------------------------#
[32m[20221213 20:44:40 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:44:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:40 @agent_ppo2.py:185][0m |           0.0346 |           0.0278 |           0.0000 |
[32m[20221213 20:44:40 @agent_ppo2.py:185][0m |           0.0076 |           0.0219 |           0.0000 |
[32m[20221213 20:44:40 @agent_ppo2.py:185][0m |          -0.0244 |           0.0206 |           0.0000 |
[32m[20221213 20:44:40 @agent_ppo2.py:185][0m |          -0.0371 |           0.0196 |           0.0000 |
[32m[20221213 20:44:40 @agent_ppo2.py:185][0m |          -0.0532 |           0.0190 |           0.0000 |
[32m[20221213 20:44:40 @agent_ppo2.py:185][0m |          -0.0623 |           0.0183 |           0.0000 |
[32m[20221213 20:44:40 @agent_ppo2.py:185][0m |          -0.0643 |           0.0180 |           0.0000 |
[32m[20221213 20:44:40 @agent_ppo2.py:185][0m |          -0.0705 |           0.0174 |           0.0000 |
[32m[20221213 20:44:40 @agent_ppo2.py:185][0m |          -0.0677 |           0.0173 |           0.0000 |
[32m[20221213 20:44:41 @agent_ppo2.py:185][0m |          -0.0712 |           0.0170 |           0.0000 |
[32m[20221213 20:44:41 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:44:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.21
[32m[20221213 20:44:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.04
[32m[20221213 20:44:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.70
[32m[20221213 20:44:41 @agent_ppo2.py:143][0m Total time:      15.53 min
[32m[20221213 20:44:41 @agent_ppo2.py:145][0m 2228224 total steps have happened
[32m[20221213 20:44:41 @agent_ppo2.py:121][0m #------------------------ Iteration 544 --------------------------#
[32m[20221213 20:44:41 @agent_ppo2.py:127][0m Sampling time: 0.45 s by 5 slaves
[32m[20221213 20:44:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:42 @agent_ppo2.py:185][0m |           0.0296 |           0.0251 |           0.0000 |
[32m[20221213 20:44:42 @agent_ppo2.py:185][0m |           0.0117 |           0.0101 |           0.0000 |
[32m[20221213 20:44:42 @agent_ppo2.py:185][0m |          -0.0019 |           0.0097 |           0.0000 |
[32m[20221213 20:44:42 @agent_ppo2.py:185][0m |          -0.0113 |           0.0095 |           0.0000 |
[32m[20221213 20:44:42 @agent_ppo2.py:185][0m |          -0.0160 |           0.0094 |           0.0000 |
[32m[20221213 20:44:42 @agent_ppo2.py:185][0m |          -0.0260 |           0.0092 |           0.0000 |
[32m[20221213 20:44:42 @agent_ppo2.py:185][0m |          -0.0256 |           0.0092 |           0.0000 |
[32m[20221213 20:44:42 @agent_ppo2.py:185][0m |          -0.0310 |           0.0091 |           0.0000 |
[32m[20221213 20:44:42 @agent_ppo2.py:185][0m |          -0.0096 |           0.0097 |           0.0000 |
[32m[20221213 20:44:42 @agent_ppo2.py:185][0m |          -0.0377 |           0.0094 |           0.0000 |
[32m[20221213 20:44:42 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:44:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.13
[32m[20221213 20:44:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.26
[32m[20221213 20:44:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.13
[32m[20221213 20:44:43 @agent_ppo2.py:143][0m Total time:      15.56 min
[32m[20221213 20:44:43 @agent_ppo2.py:145][0m 2232320 total steps have happened
[32m[20221213 20:44:43 @agent_ppo2.py:121][0m #------------------------ Iteration 545 --------------------------#
[32m[20221213 20:44:43 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:44:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:43 @agent_ppo2.py:185][0m |           0.0255 |           0.0242 |           0.0000 |
[32m[20221213 20:44:44 @agent_ppo2.py:185][0m |          -0.0069 |           0.0176 |           0.0000 |
[32m[20221213 20:44:44 @agent_ppo2.py:185][0m |          -0.0164 |           0.0163 |           0.0000 |
[32m[20221213 20:44:44 @agent_ppo2.py:185][0m |          -0.0216 |           0.0157 |           0.0000 |
[32m[20221213 20:44:44 @agent_ppo2.py:185][0m |          -0.0279 |           0.0150 |           0.0000 |
[32m[20221213 20:44:44 @agent_ppo2.py:185][0m |          -0.0335 |           0.0151 |           0.0000 |
[32m[20221213 20:44:44 @agent_ppo2.py:185][0m |          -0.0338 |           0.0146 |           0.0000 |
[32m[20221213 20:44:44 @agent_ppo2.py:185][0m |          -0.0352 |           0.0144 |           0.0000 |
[32m[20221213 20:44:44 @agent_ppo2.py:185][0m |          -0.0379 |           0.0141 |           0.0000 |
[32m[20221213 20:44:44 @agent_ppo2.py:185][0m |          -0.0378 |           0.0142 |           0.0000 |
[32m[20221213 20:44:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:44:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.53
[32m[20221213 20:44:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.19
[32m[20221213 20:44:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.10
[32m[20221213 20:44:45 @agent_ppo2.py:143][0m Total time:      15.59 min
[32m[20221213 20:44:45 @agent_ppo2.py:145][0m 2236416 total steps have happened
[32m[20221213 20:44:45 @agent_ppo2.py:121][0m #------------------------ Iteration 546 --------------------------#
[32m[20221213 20:44:45 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:45 @agent_ppo2.py:185][0m |           0.1873 |           0.0118 |           0.0000 |
[32m[20221213 20:44:45 @agent_ppo2.py:185][0m |           0.0101 |           0.0097 |           0.0000 |
[32m[20221213 20:44:45 @agent_ppo2.py:185][0m |          -0.0108 |           0.0095 |           0.0000 |
[32m[20221213 20:44:45 @agent_ppo2.py:185][0m |           0.0346 |           0.0110 |           0.0000 |
[32m[20221213 20:44:45 @agent_ppo2.py:185][0m |          -0.0384 |           0.0096 |           0.0000 |
[32m[20221213 20:44:46 @agent_ppo2.py:185][0m |          -0.0482 |           0.0094 |           0.0000 |
[32m[20221213 20:44:46 @agent_ppo2.py:185][0m |          -0.0490 |           0.0093 |           0.0000 |
[32m[20221213 20:44:46 @agent_ppo2.py:185][0m |          -0.0552 |           0.0093 |           0.0000 |
[32m[20221213 20:44:46 @agent_ppo2.py:185][0m |          -0.0595 |           0.0092 |           0.0000 |
[32m[20221213 20:44:46 @agent_ppo2.py:185][0m |          -0.0614 |           0.0092 |           0.0000 |
[32m[20221213 20:44:46 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:44:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.39
[32m[20221213 20:44:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.89
[32m[20221213 20:44:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.86
[32m[20221213 20:44:46 @agent_ppo2.py:143][0m Total time:      15.62 min
[32m[20221213 20:44:46 @agent_ppo2.py:145][0m 2240512 total steps have happened
[32m[20221213 20:44:46 @agent_ppo2.py:121][0m #------------------------ Iteration 547 --------------------------#
[32m[20221213 20:44:47 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:47 @agent_ppo2.py:185][0m |           0.0196 |           0.0160 |           0.0000 |
[32m[20221213 20:44:47 @agent_ppo2.py:185][0m |          -0.0056 |           0.0136 |           0.0000 |
[32m[20221213 20:44:47 @agent_ppo2.py:185][0m |          -0.0189 |           0.0127 |           0.0000 |
[32m[20221213 20:44:47 @agent_ppo2.py:185][0m |          -0.0236 |           0.0123 |           0.0000 |
[32m[20221213 20:44:47 @agent_ppo2.py:185][0m |          -0.0285 |           0.0119 |           0.0000 |
[32m[20221213 20:44:47 @agent_ppo2.py:185][0m |          -0.0222 |           0.0128 |           0.0000 |
[32m[20221213 20:44:47 @agent_ppo2.py:185][0m |          -0.0349 |           0.0129 |           0.0000 |
[32m[20221213 20:44:47 @agent_ppo2.py:185][0m |          -0.0368 |           0.0115 |           0.0000 |
[32m[20221213 20:44:47 @agent_ppo2.py:185][0m |          -0.0377 |           0.0115 |           0.0000 |
[32m[20221213 20:44:48 @agent_ppo2.py:185][0m |          -0.0387 |           0.0112 |           0.0000 |
[32m[20221213 20:44:48 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:44:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.68
[32m[20221213 20:44:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.32
[32m[20221213 20:44:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.01
[32m[20221213 20:44:48 @agent_ppo2.py:143][0m Total time:      15.65 min
[32m[20221213 20:44:48 @agent_ppo2.py:145][0m 2244608 total steps have happened
[32m[20221213 20:44:48 @agent_ppo2.py:121][0m #------------------------ Iteration 548 --------------------------#
[32m[20221213 20:44:48 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:48 @agent_ppo2.py:185][0m |           0.0273 |           0.0177 |           0.0000 |
[32m[20221213 20:44:49 @agent_ppo2.py:185][0m |          -0.0100 |           0.0137 |           0.0000 |
[32m[20221213 20:44:49 @agent_ppo2.py:185][0m |          -0.0162 |           0.0132 |           0.0000 |
[32m[20221213 20:44:49 @agent_ppo2.py:185][0m |          -0.0257 |           0.0124 |           0.0000 |
[32m[20221213 20:44:49 @agent_ppo2.py:185][0m |          -0.0325 |           0.0124 |           0.0000 |
[32m[20221213 20:44:49 @agent_ppo2.py:185][0m |          -0.0311 |           0.0124 |           0.0000 |
[32m[20221213 20:44:49 @agent_ppo2.py:185][0m |          -0.0531 |           0.0124 |           0.0000 |
[32m[20221213 20:44:49 @agent_ppo2.py:185][0m |          -0.0416 |           0.0120 |           0.0000 |
[32m[20221213 20:44:49 @agent_ppo2.py:185][0m |          -0.0409 |           0.0117 |           0.0000 |
[32m[20221213 20:44:49 @agent_ppo2.py:185][0m |          -0.0440 |           0.0118 |           0.0000 |
[32m[20221213 20:44:49 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:44:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.90
[32m[20221213 20:44:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.37
[32m[20221213 20:44:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.96
[32m[20221213 20:44:50 @agent_ppo2.py:143][0m Total time:      15.67 min
[32m[20221213 20:44:50 @agent_ppo2.py:145][0m 2248704 total steps have happened
[32m[20221213 20:44:50 @agent_ppo2.py:121][0m #------------------------ Iteration 549 --------------------------#
[32m[20221213 20:44:50 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:50 @agent_ppo2.py:185][0m |           0.0674 |           0.0212 |           0.0000 |
[32m[20221213 20:44:50 @agent_ppo2.py:185][0m |           0.0188 |           0.0078 |           0.0000 |
[32m[20221213 20:44:50 @agent_ppo2.py:185][0m |           0.0232 |           0.0074 |           0.0000 |
[32m[20221213 20:44:50 @agent_ppo2.py:185][0m |           0.0060 |           0.0072 |           0.0000 |
[32m[20221213 20:44:50 @agent_ppo2.py:185][0m |          -0.0019 |           0.0071 |           0.0000 |
[32m[20221213 20:44:50 @agent_ppo2.py:185][0m |          -0.0100 |           0.0070 |           0.0000 |
[32m[20221213 20:44:51 @agent_ppo2.py:185][0m |          -0.0136 |           0.0069 |           0.0000 |
[32m[20221213 20:44:51 @agent_ppo2.py:185][0m |           0.0143 |           0.0070 |           0.0000 |
[32m[20221213 20:44:51 @agent_ppo2.py:185][0m |          -0.0192 |           0.0068 |           0.0000 |
[32m[20221213 20:44:51 @agent_ppo2.py:185][0m |          -0.0192 |           0.0067 |           0.0000 |
[32m[20221213 20:44:51 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:44:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.54
[32m[20221213 20:44:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.57
[32m[20221213 20:44:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.70
[32m[20221213 20:44:51 @agent_ppo2.py:143][0m Total time:      15.70 min
[32m[20221213 20:44:51 @agent_ppo2.py:145][0m 2252800 total steps have happened
[32m[20221213 20:44:51 @agent_ppo2.py:121][0m #------------------------ Iteration 550 --------------------------#
[32m[20221213 20:44:52 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:44:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:52 @agent_ppo2.py:185][0m |           0.0263 |           0.1163 |           0.0000 |
[32m[20221213 20:44:52 @agent_ppo2.py:185][0m |          -0.0068 |           0.0324 |           0.0000 |
[32m[20221213 20:44:52 @agent_ppo2.py:185][0m |          -0.0245 |           0.0217 |           0.0000 |
[32m[20221213 20:44:52 @agent_ppo2.py:185][0m |          -0.0152 |           0.0203 |           0.0000 |
[32m[20221213 20:44:52 @agent_ppo2.py:185][0m |          -0.0196 |           0.0183 |           0.0000 |
[32m[20221213 20:44:52 @agent_ppo2.py:185][0m |          -0.0245 |           0.0176 |           0.0000 |
[32m[20221213 20:44:52 @agent_ppo2.py:185][0m |          -0.0270 |           0.0175 |           0.0000 |
[32m[20221213 20:44:52 @agent_ppo2.py:185][0m |          -0.0273 |           0.0175 |           0.0000 |
[32m[20221213 20:44:52 @agent_ppo2.py:185][0m |          -0.0291 |           0.0173 |           0.0000 |
[32m[20221213 20:44:52 @agent_ppo2.py:185][0m |          -0.0297 |           0.0164 |           0.0000 |
[32m[20221213 20:44:52 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:44:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.19
[32m[20221213 20:44:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.10
[32m[20221213 20:44:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.79
[32m[20221213 20:44:53 @agent_ppo2.py:143][0m Total time:      15.73 min
[32m[20221213 20:44:53 @agent_ppo2.py:145][0m 2256896 total steps have happened
[32m[20221213 20:44:53 @agent_ppo2.py:121][0m #------------------------ Iteration 551 --------------------------#
[32m[20221213 20:44:53 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:53 @agent_ppo2.py:185][0m |           0.0203 |           0.0383 |           0.0000 |
[32m[20221213 20:44:53 @agent_ppo2.py:185][0m |           0.0435 |           0.0099 |           0.0000 |
[32m[20221213 20:44:54 @agent_ppo2.py:185][0m |          -0.0000 |           0.0094 |           0.0000 |
[32m[20221213 20:44:54 @agent_ppo2.py:185][0m |          -0.0177 |           0.0092 |           0.0000 |
[32m[20221213 20:44:54 @agent_ppo2.py:185][0m |          -0.0248 |           0.0090 |           0.0000 |
[32m[20221213 20:44:54 @agent_ppo2.py:185][0m |          -0.0265 |           0.0089 |           0.0000 |
[32m[20221213 20:44:54 @agent_ppo2.py:185][0m |          -0.0204 |           0.0090 |           0.0000 |
[32m[20221213 20:44:54 @agent_ppo2.py:185][0m |          -0.0296 |           0.0089 |           0.0000 |
[32m[20221213 20:44:54 @agent_ppo2.py:185][0m |          -0.0397 |           0.0088 |           0.0000 |
[32m[20221213 20:44:54 @agent_ppo2.py:185][0m |          -0.0463 |           0.0086 |           0.0000 |
[32m[20221213 20:44:54 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:44:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.26
[32m[20221213 20:44:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.50
[32m[20221213 20:44:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.18
[32m[20221213 20:44:54 @agent_ppo2.py:143][0m Total time:      15.76 min
[32m[20221213 20:44:54 @agent_ppo2.py:145][0m 2260992 total steps have happened
[32m[20221213 20:44:54 @agent_ppo2.py:121][0m #------------------------ Iteration 552 --------------------------#
[32m[20221213 20:44:55 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:55 @agent_ppo2.py:185][0m |           0.0286 |           0.0131 |           0.0000 |
[32m[20221213 20:44:55 @agent_ppo2.py:185][0m |          -0.0100 |           0.0116 |           0.0000 |
[32m[20221213 20:44:55 @agent_ppo2.py:185][0m |          -0.0180 |           0.0118 |           0.0000 |
[32m[20221213 20:44:55 @agent_ppo2.py:185][0m |          -0.0309 |           0.0115 |           0.0000 |
[32m[20221213 20:44:55 @agent_ppo2.py:185][0m |          -0.0443 |           0.0108 |           0.0000 |
[32m[20221213 20:44:55 @agent_ppo2.py:185][0m |          -0.0413 |           0.0112 |           0.0000 |
[32m[20221213 20:44:56 @agent_ppo2.py:185][0m |          -0.0448 |           0.0108 |           0.0000 |
[32m[20221213 20:44:56 @agent_ppo2.py:185][0m |          -0.0489 |           0.0105 |           0.0000 |
[32m[20221213 20:44:56 @agent_ppo2.py:185][0m |          -0.0429 |           0.0105 |           0.0000 |
[32m[20221213 20:44:56 @agent_ppo2.py:185][0m |          -0.0511 |           0.0103 |           0.0000 |
[32m[20221213 20:44:56 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:44:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.37
[32m[20221213 20:44:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.09
[32m[20221213 20:44:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.92
[32m[20221213 20:44:56 @agent_ppo2.py:143][0m Total time:      15.78 min
[32m[20221213 20:44:56 @agent_ppo2.py:145][0m 2265088 total steps have happened
[32m[20221213 20:44:56 @agent_ppo2.py:121][0m #------------------------ Iteration 553 --------------------------#
[32m[20221213 20:44:56 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:57 @agent_ppo2.py:185][0m |           0.0209 |           0.0150 |           0.0000 |
[32m[20221213 20:44:57 @agent_ppo2.py:185][0m |           0.0127 |           0.0075 |           0.0000 |
[32m[20221213 20:44:57 @agent_ppo2.py:185][0m |           0.0033 |           0.0070 |           0.0000 |
[32m[20221213 20:44:57 @agent_ppo2.py:185][0m |          -0.0092 |           0.0068 |           0.0000 |
[32m[20221213 20:44:57 @agent_ppo2.py:185][0m |          -0.0167 |           0.0067 |           0.0000 |
[32m[20221213 20:44:57 @agent_ppo2.py:185][0m |          -0.0211 |           0.0066 |           0.0000 |
[32m[20221213 20:44:57 @agent_ppo2.py:185][0m |          -0.0245 |           0.0065 |           0.0000 |
[32m[20221213 20:44:57 @agent_ppo2.py:185][0m |          -0.0305 |           0.0064 |           0.0000 |
[32m[20221213 20:44:57 @agent_ppo2.py:185][0m |          -0.0317 |           0.0064 |           0.0000 |
[32m[20221213 20:44:57 @agent_ppo2.py:185][0m |          -0.0361 |           0.0063 |           0.0000 |
[32m[20221213 20:44:57 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:44:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.31
[32m[20221213 20:44:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.46
[32m[20221213 20:44:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.68
[32m[20221213 20:44:58 @agent_ppo2.py:143][0m Total time:      15.81 min
[32m[20221213 20:44:58 @agent_ppo2.py:145][0m 2269184 total steps have happened
[32m[20221213 20:44:58 @agent_ppo2.py:121][0m #------------------------ Iteration 554 --------------------------#
[32m[20221213 20:44:58 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:44:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:44:58 @agent_ppo2.py:185][0m |           0.0417 |           0.0662 |           0.0000 |
[32m[20221213 20:44:58 @agent_ppo2.py:185][0m |           0.0157 |           0.0213 |           0.0000 |
[32m[20221213 20:44:59 @agent_ppo2.py:185][0m |          -0.0017 |           0.0163 |           0.0000 |
[32m[20221213 20:44:59 @agent_ppo2.py:185][0m |          -0.0134 |           0.0154 |           0.0000 |
[32m[20221213 20:44:59 @agent_ppo2.py:185][0m |          -0.0136 |           0.0152 |           0.0000 |
[32m[20221213 20:44:59 @agent_ppo2.py:185][0m |          -0.0201 |           0.0147 |           0.0000 |
[32m[20221213 20:44:59 @agent_ppo2.py:185][0m |          -0.0309 |           0.0148 |           0.0000 |
[32m[20221213 20:44:59 @agent_ppo2.py:185][0m |          -0.0053 |           0.0152 |           0.0000 |
[32m[20221213 20:44:59 @agent_ppo2.py:185][0m |          -0.0150 |           0.0140 |           0.0000 |
[32m[20221213 20:44:59 @agent_ppo2.py:185][0m |          -0.0208 |           0.0136 |           0.0000 |
[32m[20221213 20:44:59 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:44:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.79
[32m[20221213 20:44:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.21
[32m[20221213 20:44:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.75
[32m[20221213 20:44:59 @agent_ppo2.py:143][0m Total time:      15.84 min
[32m[20221213 20:44:59 @agent_ppo2.py:145][0m 2273280 total steps have happened
[32m[20221213 20:44:59 @agent_ppo2.py:121][0m #------------------------ Iteration 555 --------------------------#
[32m[20221213 20:45:00 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:00 @agent_ppo2.py:185][0m |           0.0265 |           0.0166 |           0.0000 |
[32m[20221213 20:45:00 @agent_ppo2.py:185][0m |           0.0026 |           0.0081 |           0.0000 |
[32m[20221213 20:45:00 @agent_ppo2.py:185][0m |          -0.0119 |           0.0077 |           0.0000 |
[32m[20221213 20:45:00 @agent_ppo2.py:185][0m |          -0.0204 |           0.0076 |           0.0000 |
[32m[20221213 20:45:00 @agent_ppo2.py:185][0m |          -0.0326 |           0.0075 |           0.0000 |
[32m[20221213 20:45:00 @agent_ppo2.py:185][0m |          -0.0350 |           0.0075 |           0.0000 |
[32m[20221213 20:45:00 @agent_ppo2.py:185][0m |          -0.0364 |           0.0074 |           0.0000 |
[32m[20221213 20:45:01 @agent_ppo2.py:185][0m |          -0.0416 |           0.0074 |           0.0000 |
[32m[20221213 20:45:01 @agent_ppo2.py:185][0m |          -0.0429 |           0.0073 |           0.0000 |
[32m[20221213 20:45:01 @agent_ppo2.py:185][0m |          -0.0427 |           0.0073 |           0.0000 |
[32m[20221213 20:45:01 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:45:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.88
[32m[20221213 20:45:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.29
[32m[20221213 20:45:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.77
[32m[20221213 20:45:01 @agent_ppo2.py:143][0m Total time:      15.86 min
[32m[20221213 20:45:01 @agent_ppo2.py:145][0m 2277376 total steps have happened
[32m[20221213 20:45:01 @agent_ppo2.py:121][0m #------------------------ Iteration 556 --------------------------#
[32m[20221213 20:45:01 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:45:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:02 @agent_ppo2.py:185][0m |           0.0293 |           0.0083 |           0.0000 |
[32m[20221213 20:45:02 @agent_ppo2.py:185][0m |           0.0054 |           0.0080 |           0.0000 |
[32m[20221213 20:45:02 @agent_ppo2.py:185][0m |          -0.0169 |           0.0079 |           0.0000 |
[32m[20221213 20:45:02 @agent_ppo2.py:185][0m |          -0.0301 |           0.0079 |           0.0000 |
[32m[20221213 20:45:02 @agent_ppo2.py:185][0m |          -0.0402 |           0.0079 |           0.0000 |
[32m[20221213 20:45:02 @agent_ppo2.py:185][0m |          -0.0428 |           0.0078 |           0.0000 |
[32m[20221213 20:45:02 @agent_ppo2.py:185][0m |          -0.0482 |           0.0078 |           0.0000 |
[32m[20221213 20:45:02 @agent_ppo2.py:185][0m |          -0.0509 |           0.0078 |           0.0000 |
[32m[20221213 20:45:02 @agent_ppo2.py:185][0m |          -0.0511 |           0.0077 |           0.0000 |
[32m[20221213 20:45:02 @agent_ppo2.py:185][0m |          -0.0568 |           0.0077 |           0.0000 |
[32m[20221213 20:45:02 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:45:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.78
[32m[20221213 20:45:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.24
[32m[20221213 20:45:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.38
[32m[20221213 20:45:03 @agent_ppo2.py:143][0m Total time:      15.89 min
[32m[20221213 20:45:03 @agent_ppo2.py:145][0m 2281472 total steps have happened
[32m[20221213 20:45:03 @agent_ppo2.py:121][0m #------------------------ Iteration 557 --------------------------#
[32m[20221213 20:45:03 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:03 @agent_ppo2.py:185][0m |           0.0295 |           0.0118 |           0.0000 |
[32m[20221213 20:45:03 @agent_ppo2.py:185][0m |           0.0095 |           0.0099 |           0.0000 |
[32m[20221213 20:45:03 @agent_ppo2.py:185][0m |          -0.0013 |           0.0098 |           0.0000 |
[32m[20221213 20:45:03 @agent_ppo2.py:185][0m |          -0.0047 |           0.0096 |           0.0000 |
[32m[20221213 20:45:04 @agent_ppo2.py:185][0m |          -0.0193 |           0.0094 |           0.0000 |
[32m[20221213 20:45:04 @agent_ppo2.py:185][0m |          -0.0195 |           0.0094 |           0.0000 |
[32m[20221213 20:45:04 @agent_ppo2.py:185][0m |          -0.0256 |           0.0094 |           0.0000 |
[32m[20221213 20:45:04 @agent_ppo2.py:185][0m |          -0.0269 |           0.0092 |           0.0000 |
[32m[20221213 20:45:04 @agent_ppo2.py:185][0m |          -0.0180 |           0.0091 |           0.0000 |
[32m[20221213 20:45:04 @agent_ppo2.py:185][0m |          -0.0227 |           0.0090 |           0.0000 |
[32m[20221213 20:45:04 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:45:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.73
[32m[20221213 20:45:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.52
[32m[20221213 20:45:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.44
[32m[20221213 20:45:04 @agent_ppo2.py:143][0m Total time:      15.92 min
[32m[20221213 20:45:04 @agent_ppo2.py:145][0m 2285568 total steps have happened
[32m[20221213 20:45:04 @agent_ppo2.py:121][0m #------------------------ Iteration 558 --------------------------#
[32m[20221213 20:45:05 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:05 @agent_ppo2.py:185][0m |           0.0289 |           0.0169 |           0.0000 |
[32m[20221213 20:45:05 @agent_ppo2.py:185][0m |           0.0083 |           0.0132 |           0.0000 |
[32m[20221213 20:45:05 @agent_ppo2.py:185][0m |          -0.0056 |           0.0126 |           0.0000 |
[32m[20221213 20:45:05 @agent_ppo2.py:185][0m |          -0.0154 |           0.0124 |           0.0000 |
[32m[20221213 20:45:05 @agent_ppo2.py:185][0m |          -0.0154 |           0.0119 |           0.0000 |
[32m[20221213 20:45:05 @agent_ppo2.py:185][0m |          -0.0225 |           0.0118 |           0.0000 |
[32m[20221213 20:45:05 @agent_ppo2.py:185][0m |          -0.0212 |           0.0118 |           0.0000 |
[32m[20221213 20:45:05 @agent_ppo2.py:185][0m |          -0.0196 |           0.0117 |           0.0000 |
[32m[20221213 20:45:06 @agent_ppo2.py:185][0m |          -0.0290 |           0.0116 |           0.0000 |
[32m[20221213 20:45:06 @agent_ppo2.py:185][0m |          -0.0241 |           0.0115 |           0.0000 |
[32m[20221213 20:45:06 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:45:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.41
[32m[20221213 20:45:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.41
[32m[20221213 20:45:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.55
[32m[20221213 20:45:06 @agent_ppo2.py:143][0m Total time:      15.95 min
[32m[20221213 20:45:06 @agent_ppo2.py:145][0m 2289664 total steps have happened
[32m[20221213 20:45:06 @agent_ppo2.py:121][0m #------------------------ Iteration 559 --------------------------#
[32m[20221213 20:45:06 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:07 @agent_ppo2.py:185][0m |           0.0381 |           0.0619 |           0.0000 |
[32m[20221213 20:45:07 @agent_ppo2.py:185][0m |          -0.0360 |           0.0391 |           0.0000 |
[32m[20221213 20:45:07 @agent_ppo2.py:185][0m |          -0.0206 |           0.0264 |           0.0000 |
[32m[20221213 20:45:07 @agent_ppo2.py:185][0m |          -0.0377 |           0.0224 |           0.0000 |
[32m[20221213 20:45:07 @agent_ppo2.py:185][0m |          -0.0184 |           0.0231 |           0.0000 |
[32m[20221213 20:45:07 @agent_ppo2.py:185][0m |          -0.0255 |           0.0229 |           0.0000 |
[32m[20221213 20:45:07 @agent_ppo2.py:185][0m |          -0.0279 |           0.0198 |           0.0000 |
[32m[20221213 20:45:07 @agent_ppo2.py:185][0m |          -0.0287 |           0.0193 |           0.0000 |
[32m[20221213 20:45:07 @agent_ppo2.py:185][0m |          -0.0278 |           0.0190 |           0.0000 |
[32m[20221213 20:45:07 @agent_ppo2.py:185][0m |          -0.0348 |           0.0202 |           0.0000 |
[32m[20221213 20:45:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:45:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.11
[32m[20221213 20:45:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.64
[32m[20221213 20:45:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.24
[32m[20221213 20:45:08 @agent_ppo2.py:143][0m Total time:      15.97 min
[32m[20221213 20:45:08 @agent_ppo2.py:145][0m 2293760 total steps have happened
[32m[20221213 20:45:08 @agent_ppo2.py:121][0m #------------------------ Iteration 560 --------------------------#
[32m[20221213 20:45:08 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:45:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:08 @agent_ppo2.py:185][0m |           0.0194 |           0.0263 |           0.0000 |
[32m[20221213 20:45:08 @agent_ppo2.py:185][0m |          -0.0012 |           0.0106 |           0.0000 |
[32m[20221213 20:45:08 @agent_ppo2.py:185][0m |          -0.0181 |           0.0100 |           0.0000 |
[32m[20221213 20:45:08 @agent_ppo2.py:185][0m |          -0.0336 |           0.0099 |           0.0000 |
[32m[20221213 20:45:09 @agent_ppo2.py:185][0m |          -0.0264 |           0.0097 |           0.0000 |
[32m[20221213 20:45:09 @agent_ppo2.py:185][0m |          -0.0359 |           0.0096 |           0.0000 |
[32m[20221213 20:45:09 @agent_ppo2.py:185][0m |          -0.0414 |           0.0095 |           0.0000 |
[32m[20221213 20:45:09 @agent_ppo2.py:185][0m |          -0.0479 |           0.0095 |           0.0000 |
[32m[20221213 20:45:09 @agent_ppo2.py:185][0m |          -0.0465 |           0.0094 |           0.0000 |
[32m[20221213 20:45:09 @agent_ppo2.py:185][0m |          -0.0445 |           0.0094 |           0.0000 |
[32m[20221213 20:45:09 @agent_ppo2.py:130][0m Policy update time: 0.87 s
[32m[20221213 20:45:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.27
[32m[20221213 20:45:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.48
[32m[20221213 20:45:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.34
[32m[20221213 20:45:09 @agent_ppo2.py:143][0m Total time:      16.00 min
[32m[20221213 20:45:09 @agent_ppo2.py:145][0m 2297856 total steps have happened
[32m[20221213 20:45:09 @agent_ppo2.py:121][0m #------------------------ Iteration 561 --------------------------#
[32m[20221213 20:45:10 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:10 @agent_ppo2.py:185][0m |           0.0547 |           0.0092 |           0.0000 |
[32m[20221213 20:45:10 @agent_ppo2.py:185][0m |          -0.0019 |           0.0090 |           0.0000 |
[32m[20221213 20:45:10 @agent_ppo2.py:185][0m |          -0.0400 |           0.0089 |           0.0000 |
[32m[20221213 20:45:10 @agent_ppo2.py:185][0m |          -0.0631 |           0.0089 |           0.0000 |
[32m[20221213 20:45:10 @agent_ppo2.py:185][0m |          -0.0796 |           0.0089 |           0.0000 |
[32m[20221213 20:45:10 @agent_ppo2.py:185][0m |          -0.1098 |           0.0092 |           0.0000 |
[32m[20221213 20:45:10 @agent_ppo2.py:185][0m |          -0.0787 |           0.0092 |           0.0000 |
[32m[20221213 20:45:10 @agent_ppo2.py:185][0m |          -0.0897 |           0.0087 |           0.0000 |
[32m[20221213 20:45:10 @agent_ppo2.py:185][0m |          -0.0943 |           0.0087 |           0.0000 |
[32m[20221213 20:45:11 @agent_ppo2.py:185][0m |          -0.1099 |           0.0087 |           0.0000 |
[32m[20221213 20:45:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:45:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.38
[32m[20221213 20:45:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.95
[32m[20221213 20:45:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.79
[32m[20221213 20:45:11 @agent_ppo2.py:143][0m Total time:      16.03 min
[32m[20221213 20:45:11 @agent_ppo2.py:145][0m 2301952 total steps have happened
[32m[20221213 20:45:11 @agent_ppo2.py:121][0m #------------------------ Iteration 562 --------------------------#
[32m[20221213 20:45:11 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:11 @agent_ppo2.py:185][0m |           0.0156 |           0.0158 |           0.0000 |
[32m[20221213 20:45:12 @agent_ppo2.py:185][0m |          -0.0001 |           0.0075 |           0.0000 |
[32m[20221213 20:45:12 @agent_ppo2.py:185][0m |          -0.0104 |           0.0069 |           0.0000 |
[32m[20221213 20:45:12 @agent_ppo2.py:185][0m |          -0.0223 |           0.0067 |           0.0000 |
[32m[20221213 20:45:12 @agent_ppo2.py:185][0m |          -0.0256 |           0.0066 |           0.0000 |
[32m[20221213 20:45:12 @agent_ppo2.py:185][0m |          -0.0323 |           0.0065 |           0.0000 |
[32m[20221213 20:45:12 @agent_ppo2.py:185][0m |          -0.0363 |           0.0064 |           0.0000 |
[32m[20221213 20:45:12 @agent_ppo2.py:185][0m |          -0.0389 |           0.0063 |           0.0000 |
[32m[20221213 20:45:12 @agent_ppo2.py:185][0m |          -0.0386 |           0.0062 |           0.0000 |
[32m[20221213 20:45:12 @agent_ppo2.py:185][0m |          -0.0401 |           0.0062 |           0.0000 |
[32m[20221213 20:45:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:45:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.29
[32m[20221213 20:45:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.38
[32m[20221213 20:45:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.07
[32m[20221213 20:45:13 @agent_ppo2.py:143][0m Total time:      16.06 min
[32m[20221213 20:45:13 @agent_ppo2.py:145][0m 2306048 total steps have happened
[32m[20221213 20:45:13 @agent_ppo2.py:121][0m #------------------------ Iteration 563 --------------------------#
[32m[20221213 20:45:13 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:13 @agent_ppo2.py:185][0m |           0.0349 |           0.0837 |           0.0000 |
[32m[20221213 20:45:13 @agent_ppo2.py:185][0m |           0.0083 |           0.0217 |           0.0000 |
[32m[20221213 20:45:13 @agent_ppo2.py:185][0m |          -0.0091 |           0.0174 |           0.0000 |
[32m[20221213 20:45:13 @agent_ppo2.py:185][0m |          -0.0171 |           0.0166 |           0.0000 |
[32m[20221213 20:45:13 @agent_ppo2.py:185][0m |          -0.0213 |           0.0160 |           0.0000 |
[32m[20221213 20:45:14 @agent_ppo2.py:185][0m |          -0.0275 |           0.0155 |           0.0000 |
[32m[20221213 20:45:14 @agent_ppo2.py:185][0m |          -0.0226 |           0.0154 |           0.0000 |
[32m[20221213 20:45:14 @agent_ppo2.py:185][0m |          -0.0254 |           0.0155 |           0.0000 |
[32m[20221213 20:45:14 @agent_ppo2.py:185][0m |          -0.0254 |           0.0149 |           0.0000 |
[32m[20221213 20:45:14 @agent_ppo2.py:185][0m |          -0.0293 |           0.0148 |           0.0000 |
[32m[20221213 20:45:14 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:45:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.54
[32m[20221213 20:45:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.84
[32m[20221213 20:45:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.75
[32m[20221213 20:45:14 @agent_ppo2.py:143][0m Total time:      16.09 min
[32m[20221213 20:45:14 @agent_ppo2.py:145][0m 2310144 total steps have happened
[32m[20221213 20:45:14 @agent_ppo2.py:121][0m #------------------------ Iteration 564 --------------------------#
[32m[20221213 20:45:15 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:45:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:15 @agent_ppo2.py:185][0m |           0.1117 |           0.0221 |           0.0000 |
[32m[20221213 20:45:15 @agent_ppo2.py:185][0m |           0.0661 |           0.0196 |           0.0000 |
[32m[20221213 20:45:15 @agent_ppo2.py:185][0m |           0.0356 |           0.0170 |           0.0000 |
[32m[20221213 20:45:15 @agent_ppo2.py:185][0m |           0.0168 |           0.0167 |           0.0000 |
[32m[20221213 20:45:15 @agent_ppo2.py:185][0m |           0.0040 |           0.0162 |           0.0000 |
[32m[20221213 20:45:15 @agent_ppo2.py:185][0m |          -0.0030 |           0.0155 |           0.0000 |
[32m[20221213 20:45:15 @agent_ppo2.py:185][0m |          -0.0081 |           0.0156 |           0.0000 |
[32m[20221213 20:45:15 @agent_ppo2.py:185][0m |          -0.0187 |           0.0152 |           0.0000 |
[32m[20221213 20:45:16 @agent_ppo2.py:185][0m |          -0.0148 |           0.0156 |           0.0000 |
[32m[20221213 20:45:16 @agent_ppo2.py:185][0m |          -0.0218 |           0.0148 |           0.0000 |
[32m[20221213 20:45:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:45:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.56
[32m[20221213 20:45:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.30
[32m[20221213 20:45:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.89
[32m[20221213 20:45:16 @agent_ppo2.py:143][0m Total time:      16.11 min
[32m[20221213 20:45:16 @agent_ppo2.py:145][0m 2314240 total steps have happened
[32m[20221213 20:45:16 @agent_ppo2.py:121][0m #------------------------ Iteration 565 --------------------------#
[32m[20221213 20:45:16 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:45:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:17 @agent_ppo2.py:185][0m |           0.0059 |           0.0175 |           0.0000 |
[32m[20221213 20:45:17 @agent_ppo2.py:185][0m |          -0.0114 |           0.0120 |           0.0000 |
[32m[20221213 20:45:17 @agent_ppo2.py:185][0m |          -0.0218 |           0.0120 |           0.0000 |
[32m[20221213 20:45:17 @agent_ppo2.py:185][0m |          -0.0321 |           0.0117 |           0.0000 |
[32m[20221213 20:45:17 @agent_ppo2.py:185][0m |          -0.0325 |           0.0114 |           0.0000 |
[32m[20221213 20:45:17 @agent_ppo2.py:185][0m |          -0.0452 |           0.0113 |           0.0000 |
[32m[20221213 20:45:17 @agent_ppo2.py:185][0m |          -0.0428 |           0.0111 |           0.0000 |
[32m[20221213 20:45:17 @agent_ppo2.py:185][0m |          -0.0453 |           0.0112 |           0.0000 |
[32m[20221213 20:45:17 @agent_ppo2.py:185][0m |          -0.0519 |           0.0109 |           0.0000 |
[32m[20221213 20:45:17 @agent_ppo2.py:185][0m |          -0.0528 |           0.0109 |           0.0000 |
[32m[20221213 20:45:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:45:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.95
[32m[20221213 20:45:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.90
[32m[20221213 20:45:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.64
[32m[20221213 20:45:18 @agent_ppo2.py:143][0m Total time:      16.14 min
[32m[20221213 20:45:18 @agent_ppo2.py:145][0m 2318336 total steps have happened
[32m[20221213 20:45:18 @agent_ppo2.py:121][0m #------------------------ Iteration 566 --------------------------#
[32m[20221213 20:45:18 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:45:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:18 @agent_ppo2.py:185][0m |           0.0184 |           0.0133 |           0.0000 |
[32m[20221213 20:45:18 @agent_ppo2.py:185][0m |          -0.0037 |           0.0075 |           0.0000 |
[32m[20221213 20:45:18 @agent_ppo2.py:185][0m |          -0.0123 |           0.0072 |           0.0000 |
[32m[20221213 20:45:19 @agent_ppo2.py:185][0m |          -0.0203 |           0.0071 |           0.0000 |
[32m[20221213 20:45:19 @agent_ppo2.py:185][0m |          -0.0271 |           0.0069 |           0.0000 |
[32m[20221213 20:45:19 @agent_ppo2.py:185][0m |          -0.0324 |           0.0069 |           0.0000 |
[32m[20221213 20:45:19 @agent_ppo2.py:185][0m |          -0.0346 |           0.0068 |           0.0000 |
[32m[20221213 20:45:19 @agent_ppo2.py:185][0m |          -0.0362 |           0.0067 |           0.0000 |
[32m[20221213 20:45:19 @agent_ppo2.py:185][0m |          -0.0383 |           0.0066 |           0.0000 |
[32m[20221213 20:45:19 @agent_ppo2.py:185][0m |          -0.0391 |           0.0065 |           0.0000 |
[32m[20221213 20:45:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:45:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.00
[32m[20221213 20:45:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.16
[32m[20221213 20:45:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.56
[32m[20221213 20:45:19 @agent_ppo2.py:143][0m Total time:      16.17 min
[32m[20221213 20:45:19 @agent_ppo2.py:145][0m 2322432 total steps have happened
[32m[20221213 20:45:19 @agent_ppo2.py:121][0m #------------------------ Iteration 567 --------------------------#
[32m[20221213 20:45:20 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:20 @agent_ppo2.py:185][0m |           0.0214 |           0.1326 |           0.0000 |
[32m[20221213 20:45:20 @agent_ppo2.py:185][0m |           0.0180 |           0.0479 |           0.0000 |
[32m[20221213 20:45:20 @agent_ppo2.py:185][0m |          -0.0007 |           0.0328 |           0.0000 |
[32m[20221213 20:45:20 @agent_ppo2.py:185][0m |          -0.0114 |           0.0281 |           0.0000 |
[32m[20221213 20:45:20 @agent_ppo2.py:185][0m |          -0.0227 |           0.0267 |           0.0000 |
[32m[20221213 20:45:21 @agent_ppo2.py:185][0m |          -0.0276 |           0.0262 |           0.0000 |
[32m[20221213 20:45:21 @agent_ppo2.py:185][0m |          -0.0291 |           0.0239 |           0.0000 |
[32m[20221213 20:45:21 @agent_ppo2.py:185][0m |          -0.0309 |           0.0231 |           0.0000 |
[32m[20221213 20:45:21 @agent_ppo2.py:185][0m |          -0.0319 |           0.0219 |           0.0000 |
[32m[20221213 20:45:21 @agent_ppo2.py:185][0m |          -0.0329 |           0.0220 |           0.0000 |
[32m[20221213 20:45:21 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 20:45:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.45
[32m[20221213 20:45:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.88
[32m[20221213 20:45:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.24
[32m[20221213 20:45:21 @agent_ppo2.py:143][0m Total time:      16.20 min
[32m[20221213 20:45:21 @agent_ppo2.py:145][0m 2326528 total steps have happened
[32m[20221213 20:45:21 @agent_ppo2.py:121][0m #------------------------ Iteration 568 --------------------------#
[32m[20221213 20:45:22 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:45:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:22 @agent_ppo2.py:185][0m |           0.0360 |           0.0250 |           0.0000 |
[32m[20221213 20:45:22 @agent_ppo2.py:185][0m |          -0.0017 |           0.0151 |           0.0000 |
[32m[20221213 20:45:22 @agent_ppo2.py:185][0m |          -0.0136 |           0.0147 |           0.0000 |
[32m[20221213 20:45:22 @agent_ppo2.py:185][0m |          -0.0346 |           0.0140 |           0.0000 |
[32m[20221213 20:45:22 @agent_ppo2.py:185][0m |          -0.0443 |           0.0136 |           0.0000 |
[32m[20221213 20:45:22 @agent_ppo2.py:185][0m |          -0.0499 |           0.0134 |           0.0000 |
[32m[20221213 20:45:22 @agent_ppo2.py:185][0m |          -0.0513 |           0.0134 |           0.0000 |
[32m[20221213 20:45:23 @agent_ppo2.py:185][0m |          -0.0574 |           0.0132 |           0.0000 |
[32m[20221213 20:45:23 @agent_ppo2.py:185][0m |          -0.0454 |           0.0158 |           0.0000 |
[32m[20221213 20:45:23 @agent_ppo2.py:185][0m |          -0.0577 |           0.0159 |           0.0000 |
[32m[20221213 20:45:23 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:45:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.38
[32m[20221213 20:45:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.76
[32m[20221213 20:45:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.54
[32m[20221213 20:45:23 @agent_ppo2.py:143][0m Total time:      16.23 min
[32m[20221213 20:45:23 @agent_ppo2.py:145][0m 2330624 total steps have happened
[32m[20221213 20:45:23 @agent_ppo2.py:121][0m #------------------------ Iteration 569 --------------------------#
[32m[20221213 20:45:23 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:45:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:24 @agent_ppo2.py:185][0m |           0.0514 |           0.0129 |           0.0000 |
[32m[20221213 20:45:24 @agent_ppo2.py:185][0m |           0.0073 |           0.0111 |           0.0000 |
[32m[20221213 20:45:24 @agent_ppo2.py:185][0m |          -0.0106 |           0.0104 |           0.0000 |
[32m[20221213 20:45:24 @agent_ppo2.py:185][0m |          -0.0510 |           0.0102 |           0.0000 |
[32m[20221213 20:45:24 @agent_ppo2.py:185][0m |          -0.0471 |           0.0101 |           0.0000 |
[32m[20221213 20:45:24 @agent_ppo2.py:185][0m |          -0.0607 |           0.0099 |           0.0000 |
[32m[20221213 20:45:24 @agent_ppo2.py:185][0m |          -0.0695 |           0.0098 |           0.0000 |
[32m[20221213 20:45:24 @agent_ppo2.py:185][0m |          -0.0724 |           0.0097 |           0.0000 |
[32m[20221213 20:45:24 @agent_ppo2.py:185][0m |          -0.0770 |           0.0096 |           0.0000 |
[32m[20221213 20:45:25 @agent_ppo2.py:185][0m |          -0.0823 |           0.0095 |           0.0000 |
[32m[20221213 20:45:25 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:45:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.04
[32m[20221213 20:45:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.53
[32m[20221213 20:45:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.11
[32m[20221213 20:45:25 @agent_ppo2.py:143][0m Total time:      16.26 min
[32m[20221213 20:45:25 @agent_ppo2.py:145][0m 2334720 total steps have happened
[32m[20221213 20:45:25 @agent_ppo2.py:121][0m #------------------------ Iteration 570 --------------------------#
[32m[20221213 20:45:25 @agent_ppo2.py:127][0m Sampling time: 0.42 s by 5 slaves
[32m[20221213 20:45:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:26 @agent_ppo2.py:185][0m |           0.0128 |           0.0138 |           0.0000 |
[32m[20221213 20:45:26 @agent_ppo2.py:185][0m |          -0.0090 |           0.0078 |           0.0000 |
[32m[20221213 20:45:26 @agent_ppo2.py:185][0m |          -0.0152 |           0.0076 |           0.0000 |
[32m[20221213 20:45:26 @agent_ppo2.py:185][0m |          -0.0063 |           0.0078 |           0.0000 |
[32m[20221213 20:45:26 @agent_ppo2.py:185][0m |          -0.0291 |           0.0074 |           0.0000 |
[32m[20221213 20:45:26 @agent_ppo2.py:185][0m |          -0.0355 |           0.0072 |           0.0000 |
[32m[20221213 20:45:26 @agent_ppo2.py:185][0m |          -0.0360 |           0.0071 |           0.0000 |
[32m[20221213 20:45:26 @agent_ppo2.py:185][0m |          -0.0350 |           0.0070 |           0.0000 |
[32m[20221213 20:45:26 @agent_ppo2.py:185][0m |          -0.0383 |           0.0070 |           0.0000 |
[32m[20221213 20:45:26 @agent_ppo2.py:185][0m |          -0.0385 |           0.0069 |           0.0000 |
[32m[20221213 20:45:26 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 20:45:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.99
[32m[20221213 20:45:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.34
[32m[20221213 20:45:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.54
[32m[20221213 20:45:27 @agent_ppo2.py:143][0m Total time:      16.29 min
[32m[20221213 20:45:27 @agent_ppo2.py:145][0m 2338816 total steps have happened
[32m[20221213 20:45:27 @agent_ppo2.py:121][0m #------------------------ Iteration 571 --------------------------#
[32m[20221213 20:45:27 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:45:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:27 @agent_ppo2.py:185][0m |           0.0065 |           0.0077 |           0.0000 |
[32m[20221213 20:45:27 @agent_ppo2.py:185][0m |          -0.0104 |           0.0066 |           0.0000 |
[32m[20221213 20:45:28 @agent_ppo2.py:185][0m |          -0.0098 |           0.0064 |           0.0000 |
[32m[20221213 20:45:28 @agent_ppo2.py:185][0m |          -0.0279 |           0.0062 |           0.0000 |
[32m[20221213 20:45:28 @agent_ppo2.py:185][0m |          -0.0293 |           0.0063 |           0.0000 |
[32m[20221213 20:45:28 @agent_ppo2.py:185][0m |          -0.0397 |           0.0061 |           0.0000 |
[32m[20221213 20:45:28 @agent_ppo2.py:185][0m |          -0.0436 |           0.0060 |           0.0000 |
[32m[20221213 20:45:28 @agent_ppo2.py:185][0m |          -0.0457 |           0.0063 |           0.0000 |
[32m[20221213 20:45:28 @agent_ppo2.py:185][0m |          -0.0364 |           0.0059 |           0.0000 |
[32m[20221213 20:45:28 @agent_ppo2.py:185][0m |          -0.0432 |           0.0058 |           0.0000 |
[32m[20221213 20:45:28 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:45:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.44
[32m[20221213 20:45:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.70
[32m[20221213 20:45:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.37
[32m[20221213 20:45:29 @agent_ppo2.py:143][0m Total time:      16.32 min
[32m[20221213 20:45:29 @agent_ppo2.py:145][0m 2342912 total steps have happened
[32m[20221213 20:45:29 @agent_ppo2.py:121][0m #------------------------ Iteration 572 --------------------------#
[32m[20221213 20:45:29 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:29 @agent_ppo2.py:185][0m |           0.0369 |           0.0142 |           0.0000 |
[32m[20221213 20:45:29 @agent_ppo2.py:185][0m |          -0.0024 |           0.0092 |           0.0000 |
[32m[20221213 20:45:29 @agent_ppo2.py:185][0m |          -0.0505 |           0.0083 |           0.0000 |
[32m[20221213 20:45:29 @agent_ppo2.py:185][0m |          -0.0086 |           0.0079 |           0.0000 |
[32m[20221213 20:45:29 @agent_ppo2.py:185][0m |          -0.0645 |           0.0079 |           0.0000 |
[32m[20221213 20:45:29 @agent_ppo2.py:185][0m |          -0.0228 |           0.0078 |           0.0000 |
[32m[20221213 20:45:30 @agent_ppo2.py:185][0m |          -0.0308 |           0.0079 |           0.0000 |
[32m[20221213 20:45:30 @agent_ppo2.py:185][0m |          -0.0269 |           0.0077 |           0.0000 |
[32m[20221213 20:45:30 @agent_ppo2.py:185][0m |          -0.0280 |           0.0075 |           0.0000 |
[32m[20221213 20:45:30 @agent_ppo2.py:185][0m |          -0.0283 |           0.0075 |           0.0000 |
[32m[20221213 20:45:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:45:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.81
[32m[20221213 20:45:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.40
[32m[20221213 20:45:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.86
[32m[20221213 20:45:30 @agent_ppo2.py:143][0m Total time:      16.35 min
[32m[20221213 20:45:30 @agent_ppo2.py:145][0m 2347008 total steps have happened
[32m[20221213 20:45:30 @agent_ppo2.py:121][0m #------------------------ Iteration 573 --------------------------#
[32m[20221213 20:45:31 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:31 @agent_ppo2.py:185][0m |           0.0217 |           0.0143 |           0.0000 |
[32m[20221213 20:45:31 @agent_ppo2.py:185][0m |           0.0030 |           0.0118 |           0.0000 |
[32m[20221213 20:45:31 @agent_ppo2.py:185][0m |          -0.0125 |           0.0111 |           0.0000 |
[32m[20221213 20:45:31 @agent_ppo2.py:185][0m |          -0.0095 |           0.0108 |           0.0000 |
[32m[20221213 20:45:31 @agent_ppo2.py:185][0m |          -0.0158 |           0.0104 |           0.0000 |
[32m[20221213 20:45:31 @agent_ppo2.py:185][0m |          -0.0176 |           0.0103 |           0.0000 |
[32m[20221213 20:45:31 @agent_ppo2.py:185][0m |          -0.0228 |           0.0103 |           0.0000 |
[32m[20221213 20:45:31 @agent_ppo2.py:185][0m |          -0.0247 |           0.0102 |           0.0000 |
[32m[20221213 20:45:31 @agent_ppo2.py:185][0m |          -0.0282 |           0.0102 |           0.0000 |
[32m[20221213 20:45:31 @agent_ppo2.py:185][0m |          -0.0275 |           0.0100 |           0.0000 |
[32m[20221213 20:45:31 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:45:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.48
[32m[20221213 20:45:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.39
[32m[20221213 20:45:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.41
[32m[20221213 20:45:32 @agent_ppo2.py:143][0m Total time:      16.38 min
[32m[20221213 20:45:32 @agent_ppo2.py:145][0m 2351104 total steps have happened
[32m[20221213 20:45:32 @agent_ppo2.py:121][0m #------------------------ Iteration 574 --------------------------#
[32m[20221213 20:45:32 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:32 @agent_ppo2.py:185][0m |           0.0551 |           0.0208 |           0.0000 |
[32m[20221213 20:45:32 @agent_ppo2.py:185][0m |           0.0161 |           0.0196 |           0.0000 |
[32m[20221213 20:45:33 @agent_ppo2.py:185][0m |           0.0115 |           0.0172 |           0.0000 |
[32m[20221213 20:45:33 @agent_ppo2.py:185][0m |          -0.0060 |           0.0165 |           0.0000 |
[32m[20221213 20:45:33 @agent_ppo2.py:185][0m |          -0.0143 |           0.0213 |           0.0000 |
[32m[20221213 20:45:33 @agent_ppo2.py:185][0m |          -0.0821 |           0.0305 |           0.0000 |
[32m[20221213 20:45:33 @agent_ppo2.py:185][0m |          -0.0292 |           0.0223 |           0.0000 |
[32m[20221213 20:45:33 @agent_ppo2.py:185][0m |          -0.0201 |           0.0155 |           0.0000 |
[32m[20221213 20:45:33 @agent_ppo2.py:185][0m |          -0.0298 |           0.0149 |           0.0000 |
[32m[20221213 20:45:33 @agent_ppo2.py:185][0m |          -0.0328 |           0.0139 |           0.0000 |
[32m[20221213 20:45:33 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:45:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.62
[32m[20221213 20:45:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.71
[32m[20221213 20:45:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.24
[32m[20221213 20:45:34 @agent_ppo2.py:143][0m Total time:      16.41 min
[32m[20221213 20:45:34 @agent_ppo2.py:145][0m 2355200 total steps have happened
[32m[20221213 20:45:34 @agent_ppo2.py:121][0m #------------------------ Iteration 575 --------------------------#
[32m[20221213 20:45:34 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:45:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:34 @agent_ppo2.py:185][0m |           0.0343 |           0.0132 |           0.0000 |
[32m[20221213 20:45:34 @agent_ppo2.py:185][0m |          -0.0001 |           0.0131 |           0.0000 |
[32m[20221213 20:45:34 @agent_ppo2.py:185][0m |          -0.0137 |           0.0126 |           0.0000 |
[32m[20221213 20:45:34 @agent_ppo2.py:185][0m |          -0.0309 |           0.0121 |           0.0000 |
[32m[20221213 20:45:34 @agent_ppo2.py:185][0m |          -0.0466 |           0.0119 |           0.0000 |
[32m[20221213 20:45:35 @agent_ppo2.py:185][0m |          -0.0500 |           0.0117 |           0.0000 |
[32m[20221213 20:45:35 @agent_ppo2.py:185][0m |          -0.0583 |           0.0115 |           0.0000 |
[32m[20221213 20:45:35 @agent_ppo2.py:185][0m |          -0.0693 |           0.0113 |           0.0000 |
[32m[20221213 20:45:35 @agent_ppo2.py:185][0m |          -0.0647 |           0.0112 |           0.0000 |
[32m[20221213 20:45:35 @agent_ppo2.py:185][0m |          -0.0681 |           0.0111 |           0.0000 |
[32m[20221213 20:45:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:45:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.21
[32m[20221213 20:45:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.12
[32m[20221213 20:45:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.83
[32m[20221213 20:45:35 @agent_ppo2.py:143][0m Total time:      16.43 min
[32m[20221213 20:45:35 @agent_ppo2.py:145][0m 2359296 total steps have happened
[32m[20221213 20:45:35 @agent_ppo2.py:121][0m #------------------------ Iteration 576 --------------------------#
[32m[20221213 20:45:36 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:36 @agent_ppo2.py:185][0m |           0.0305 |           0.0283 |           0.0000 |
[32m[20221213 20:45:36 @agent_ppo2.py:185][0m |          -0.0056 |           0.0181 |           0.0000 |
[32m[20221213 20:45:36 @agent_ppo2.py:185][0m |          -0.0105 |           0.0171 |           0.0000 |
[32m[20221213 20:45:36 @agent_ppo2.py:185][0m |          -0.0219 |           0.0159 |           0.0000 |
[32m[20221213 20:45:36 @agent_ppo2.py:185][0m |          -0.0287 |           0.0152 |           0.0000 |
[32m[20221213 20:45:36 @agent_ppo2.py:185][0m |          -0.0287 |           0.0147 |           0.0000 |
[32m[20221213 20:45:36 @agent_ppo2.py:185][0m |          -0.0328 |           0.0179 |           0.0000 |
[32m[20221213 20:45:36 @agent_ppo2.py:185][0m |          -0.0361 |           0.0185 |           0.0000 |
[32m[20221213 20:45:36 @agent_ppo2.py:185][0m |          -0.0339 |           0.0143 |           0.0000 |
[32m[20221213 20:45:37 @agent_ppo2.py:185][0m |          -0.0324 |           0.0138 |           0.0000 |
[32m[20221213 20:45:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:45:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.85
[32m[20221213 20:45:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.78
[32m[20221213 20:45:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.24
[32m[20221213 20:45:37 @agent_ppo2.py:143][0m Total time:      16.46 min
[32m[20221213 20:45:37 @agent_ppo2.py:145][0m 2363392 total steps have happened
[32m[20221213 20:45:37 @agent_ppo2.py:121][0m #------------------------ Iteration 577 --------------------------#
[32m[20221213 20:45:37 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:37 @agent_ppo2.py:185][0m |           0.0133 |           0.0263 |           0.0000 |
[32m[20221213 20:45:38 @agent_ppo2.py:185][0m |          -0.0066 |           0.0125 |           0.0000 |
[32m[20221213 20:45:38 @agent_ppo2.py:185][0m |          -0.0196 |           0.0116 |           0.0000 |
[32m[20221213 20:45:38 @agent_ppo2.py:185][0m |           0.0159 |           0.0133 |           0.0000 |
[32m[20221213 20:45:38 @agent_ppo2.py:185][0m |          -0.0367 |           0.0120 |           0.0000 |
[32m[20221213 20:45:38 @agent_ppo2.py:185][0m |          -0.0384 |           0.0105 |           0.0000 |
[32m[20221213 20:45:38 @agent_ppo2.py:185][0m |          -0.0322 |           0.0103 |           0.0000 |
[32m[20221213 20:45:38 @agent_ppo2.py:185][0m |          -0.0444 |           0.0100 |           0.0000 |
[32m[20221213 20:45:38 @agent_ppo2.py:185][0m |          -0.0487 |           0.0101 |           0.0000 |
[32m[20221213 20:45:38 @agent_ppo2.py:185][0m |          -0.0448 |           0.0100 |           0.0000 |
[32m[20221213 20:45:38 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:45:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.17
[32m[20221213 20:45:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.63
[32m[20221213 20:45:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.22
[32m[20221213 20:45:39 @agent_ppo2.py:143][0m Total time:      16.49 min
[32m[20221213 20:45:39 @agent_ppo2.py:145][0m 2367488 total steps have happened
[32m[20221213 20:45:39 @agent_ppo2.py:121][0m #------------------------ Iteration 578 --------------------------#
[32m[20221213 20:45:39 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:39 @agent_ppo2.py:185][0m |           0.0455 |           0.0655 |           0.0000 |
[32m[20221213 20:45:39 @agent_ppo2.py:185][0m |           0.0255 |           0.0287 |           0.0000 |
[32m[20221213 20:45:39 @agent_ppo2.py:185][0m |           0.0068 |           0.0232 |           0.0000 |
[32m[20221213 20:45:40 @agent_ppo2.py:185][0m |          -0.0097 |           0.0219 |           0.0000 |
[32m[20221213 20:45:40 @agent_ppo2.py:185][0m |          -0.0204 |           0.0214 |           0.0000 |
[32m[20221213 20:45:40 @agent_ppo2.py:185][0m |          -0.0109 |           0.0198 |           0.0000 |
[32m[20221213 20:45:40 @agent_ppo2.py:185][0m |          -0.0096 |           0.0186 |           0.0000 |
[32m[20221213 20:45:40 @agent_ppo2.py:185][0m |          -0.0127 |           0.0178 |           0.0000 |
[32m[20221213 20:45:40 @agent_ppo2.py:185][0m |          -0.0153 |           0.0180 |           0.0000 |
[32m[20221213 20:45:40 @agent_ppo2.py:185][0m |          -0.0194 |           0.0181 |           0.0000 |
[32m[20221213 20:45:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:45:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.15
[32m[20221213 20:45:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.30
[32m[20221213 20:45:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.52
[32m[20221213 20:45:40 @agent_ppo2.py:143][0m Total time:      16.52 min
[32m[20221213 20:45:40 @agent_ppo2.py:145][0m 2371584 total steps have happened
[32m[20221213 20:45:40 @agent_ppo2.py:121][0m #------------------------ Iteration 579 --------------------------#
[32m[20221213 20:45:41 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:45:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:41 @agent_ppo2.py:185][0m |           0.0199 |           0.0322 |           0.0000 |
[32m[20221213 20:45:41 @agent_ppo2.py:185][0m |          -0.0108 |           0.0146 |           0.0000 |
[32m[20221213 20:45:41 @agent_ppo2.py:185][0m |          -0.0244 |           0.0138 |           0.0000 |
[32m[20221213 20:45:41 @agent_ppo2.py:185][0m |          -0.0352 |           0.0135 |           0.0000 |
[32m[20221213 20:45:41 @agent_ppo2.py:185][0m |          -0.0446 |           0.0133 |           0.0000 |
[32m[20221213 20:45:41 @agent_ppo2.py:185][0m |          -0.0478 |           0.0132 |           0.0000 |
[32m[20221213 20:45:41 @agent_ppo2.py:185][0m |          -0.0280 |           0.0133 |           0.0000 |
[32m[20221213 20:45:42 @agent_ppo2.py:185][0m |          -0.0466 |           0.0130 |           0.0000 |
[32m[20221213 20:45:42 @agent_ppo2.py:185][0m |          -0.0442 |           0.0129 |           0.0000 |
[32m[20221213 20:45:42 @agent_ppo2.py:185][0m |          -0.0490 |           0.0127 |           0.0000 |
[32m[20221213 20:45:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:45:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.37
[32m[20221213 20:45:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.75
[32m[20221213 20:45:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.04
[32m[20221213 20:45:42 @agent_ppo2.py:143][0m Total time:      16.55 min
[32m[20221213 20:45:42 @agent_ppo2.py:145][0m 2375680 total steps have happened
[32m[20221213 20:45:42 @agent_ppo2.py:121][0m #------------------------ Iteration 580 --------------------------#
[32m[20221213 20:45:42 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:45:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:43 @agent_ppo2.py:185][0m |           0.0364 |           0.0191 |           0.0000 |
[32m[20221213 20:45:43 @agent_ppo2.py:185][0m |          -0.0076 |           0.0133 |           0.0000 |
[32m[20221213 20:45:43 @agent_ppo2.py:185][0m |          -0.0128 |           0.0125 |           0.0000 |
[32m[20221213 20:45:43 @agent_ppo2.py:185][0m |          -0.0235 |           0.0122 |           0.0000 |
[32m[20221213 20:45:43 @agent_ppo2.py:185][0m |          -0.0316 |           0.0121 |           0.0000 |
[32m[20221213 20:45:43 @agent_ppo2.py:185][0m |          -0.0216 |           0.0120 |           0.0000 |
[32m[20221213 20:45:43 @agent_ppo2.py:185][0m |          -0.0249 |           0.0116 |           0.0000 |
[32m[20221213 20:45:43 @agent_ppo2.py:185][0m |          -0.0299 |           0.0114 |           0.0000 |
[32m[20221213 20:45:43 @agent_ppo2.py:185][0m |          -0.0300 |           0.0113 |           0.0000 |
[32m[20221213 20:45:43 @agent_ppo2.py:185][0m |          -0.0362 |           0.0114 |           0.0000 |
[32m[20221213 20:45:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:45:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.96
[32m[20221213 20:45:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.35
[32m[20221213 20:45:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.86
[32m[20221213 20:45:44 @agent_ppo2.py:143][0m Total time:      16.58 min
[32m[20221213 20:45:44 @agent_ppo2.py:145][0m 2379776 total steps have happened
[32m[20221213 20:45:44 @agent_ppo2.py:121][0m #------------------------ Iteration 581 --------------------------#
[32m[20221213 20:45:44 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:45:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:44 @agent_ppo2.py:185][0m |           0.0323 |           0.0194 |           0.0000 |
[32m[20221213 20:45:44 @agent_ppo2.py:185][0m |           0.0134 |           0.0089 |           0.0000 |
[32m[20221213 20:45:45 @agent_ppo2.py:185][0m |           0.0014 |           0.0086 |           0.0000 |
[32m[20221213 20:45:45 @agent_ppo2.py:185][0m |          -0.0062 |           0.0085 |           0.0000 |
[32m[20221213 20:45:45 @agent_ppo2.py:185][0m |          -0.0187 |           0.0084 |           0.0000 |
[32m[20221213 20:45:45 @agent_ppo2.py:185][0m |          -0.0214 |           0.0083 |           0.0000 |
[32m[20221213 20:45:45 @agent_ppo2.py:185][0m |          -0.0262 |           0.0083 |           0.0000 |
[32m[20221213 20:45:45 @agent_ppo2.py:185][0m |          -0.0088 |           0.0087 |           0.0000 |
[32m[20221213 20:45:45 @agent_ppo2.py:185][0m |          -0.0325 |           0.0084 |           0.0000 |
[32m[20221213 20:45:45 @agent_ppo2.py:185][0m |          -0.0348 |           0.0081 |           0.0000 |
[32m[20221213 20:45:45 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:45:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.96
[32m[20221213 20:45:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.36
[32m[20221213 20:45:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.74
[32m[20221213 20:45:46 @agent_ppo2.py:143][0m Total time:      16.61 min
[32m[20221213 20:45:46 @agent_ppo2.py:145][0m 2383872 total steps have happened
[32m[20221213 20:45:46 @agent_ppo2.py:121][0m #------------------------ Iteration 582 --------------------------#
[32m[20221213 20:45:46 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:45:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:46 @agent_ppo2.py:185][0m |           0.0331 |           0.0087 |           0.0000 |
[32m[20221213 20:45:46 @agent_ppo2.py:185][0m |           0.0309 |           0.0080 |           0.0000 |
[32m[20221213 20:45:46 @agent_ppo2.py:185][0m |           0.0039 |           0.0080 |           0.0000 |
[32m[20221213 20:45:46 @agent_ppo2.py:185][0m |          -0.0161 |           0.0079 |           0.0000 |
[32m[20221213 20:45:47 @agent_ppo2.py:185][0m |          -0.0276 |           0.0078 |           0.0000 |
[32m[20221213 20:45:47 @agent_ppo2.py:185][0m |          -0.0365 |           0.0078 |           0.0000 |
[32m[20221213 20:45:47 @agent_ppo2.py:185][0m |          -0.0383 |           0.0077 |           0.0000 |
[32m[20221213 20:45:47 @agent_ppo2.py:185][0m |          -0.0438 |           0.0077 |           0.0000 |
[32m[20221213 20:45:47 @agent_ppo2.py:185][0m |          -0.0435 |           0.0076 |           0.0000 |
[32m[20221213 20:45:47 @agent_ppo2.py:185][0m |          -0.0447 |           0.0076 |           0.0000 |
[32m[20221213 20:45:47 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:45:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.08
[32m[20221213 20:45:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.80
[32m[20221213 20:45:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.70
[32m[20221213 20:45:47 @agent_ppo2.py:143][0m Total time:      16.64 min
[32m[20221213 20:45:47 @agent_ppo2.py:145][0m 2387968 total steps have happened
[32m[20221213 20:45:47 @agent_ppo2.py:121][0m #------------------------ Iteration 583 --------------------------#
[32m[20221213 20:45:48 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:45:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:48 @agent_ppo2.py:185][0m |           0.0292 |           0.0858 |           0.0000 |
[32m[20221213 20:45:48 @agent_ppo2.py:185][0m |           0.0079 |           0.0371 |           0.0000 |
[32m[20221213 20:45:48 @agent_ppo2.py:185][0m |          -0.0009 |           0.0282 |           0.0000 |
[32m[20221213 20:45:48 @agent_ppo2.py:185][0m |          -0.0053 |           0.0277 |           0.0000 |
[32m[20221213 20:45:48 @agent_ppo2.py:185][0m |          -0.0149 |           0.0244 |           0.0000 |
[32m[20221213 20:45:48 @agent_ppo2.py:185][0m |          -0.0153 |           0.0228 |           0.0000 |
[32m[20221213 20:45:48 @agent_ppo2.py:185][0m |          -0.0185 |           0.0222 |           0.0000 |
[32m[20221213 20:45:49 @agent_ppo2.py:185][0m |          -0.0278 |           0.0216 |           0.0000 |
[32m[20221213 20:45:49 @agent_ppo2.py:185][0m |          -0.0251 |           0.0213 |           0.0000 |
[32m[20221213 20:45:49 @agent_ppo2.py:185][0m |          -0.0256 |           0.0213 |           0.0000 |
[32m[20221213 20:45:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:45:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.51
[32m[20221213 20:45:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.41
[32m[20221213 20:45:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.66
[32m[20221213 20:45:49 @agent_ppo2.py:143][0m Total time:      16.67 min
[32m[20221213 20:45:49 @agent_ppo2.py:145][0m 2392064 total steps have happened
[32m[20221213 20:45:49 @agent_ppo2.py:121][0m #------------------------ Iteration 584 --------------------------#
[32m[20221213 20:45:50 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:45:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:50 @agent_ppo2.py:185][0m |           0.0330 |           0.0371 |           0.0000 |
[32m[20221213 20:45:50 @agent_ppo2.py:185][0m |           0.0168 |           0.0087 |           0.0000 |
[32m[20221213 20:45:50 @agent_ppo2.py:185][0m |          -0.0028 |           0.0079 |           0.0000 |
[32m[20221213 20:45:50 @agent_ppo2.py:185][0m |          -0.0105 |           0.0077 |           0.0000 |
[32m[20221213 20:45:50 @agent_ppo2.py:185][0m |          -0.0200 |           0.0076 |           0.0000 |
[32m[20221213 20:45:50 @agent_ppo2.py:185][0m |          -0.0273 |           0.0074 |           0.0000 |
[32m[20221213 20:45:50 @agent_ppo2.py:185][0m |          -0.0316 |           0.0073 |           0.0000 |
[32m[20221213 20:45:50 @agent_ppo2.py:185][0m |          -0.0338 |           0.0073 |           0.0000 |
[32m[20221213 20:45:50 @agent_ppo2.py:185][0m |          -0.0338 |           0.0072 |           0.0000 |
[32m[20221213 20:45:50 @agent_ppo2.py:185][0m |          -0.0345 |           0.0071 |           0.0000 |
[32m[20221213 20:45:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:45:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.33
[32m[20221213 20:45:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.45
[32m[20221213 20:45:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.45
[32m[20221213 20:45:51 @agent_ppo2.py:143][0m Total time:      16.69 min
[32m[20221213 20:45:51 @agent_ppo2.py:145][0m 2396160 total steps have happened
[32m[20221213 20:45:51 @agent_ppo2.py:121][0m #------------------------ Iteration 585 --------------------------#
[32m[20221213 20:45:51 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:45:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:51 @agent_ppo2.py:185][0m |           0.0319 |           0.0317 |           0.0000 |
[32m[20221213 20:45:51 @agent_ppo2.py:185][0m |           0.0039 |           0.0129 |           0.0000 |
[32m[20221213 20:45:52 @agent_ppo2.py:185][0m |          -0.0045 |           0.0115 |           0.0000 |
[32m[20221213 20:45:52 @agent_ppo2.py:185][0m |          -0.0178 |           0.0112 |           0.0000 |
[32m[20221213 20:45:52 @agent_ppo2.py:185][0m |          -0.0213 |           0.0111 |           0.0000 |
[32m[20221213 20:45:52 @agent_ppo2.py:185][0m |          -0.0259 |           0.0109 |           0.0000 |
[32m[20221213 20:45:52 @agent_ppo2.py:185][0m |          -0.0721 |           0.0115 |           0.0000 |
[32m[20221213 20:45:52 @agent_ppo2.py:185][0m |          -0.0291 |           0.0114 |           0.0000 |
[32m[20221213 20:45:52 @agent_ppo2.py:185][0m |          -0.0299 |           0.0105 |           0.0000 |
[32m[20221213 20:45:52 @agent_ppo2.py:185][0m |          -0.0305 |           0.0104 |           0.0000 |
[32m[20221213 20:45:52 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:45:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.21
[32m[20221213 20:45:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.67
[32m[20221213 20:45:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.60
[32m[20221213 20:45:53 @agent_ppo2.py:143][0m Total time:      16.72 min
[32m[20221213 20:45:53 @agent_ppo2.py:145][0m 2400256 total steps have happened
[32m[20221213 20:45:53 @agent_ppo2.py:121][0m #------------------------ Iteration 586 --------------------------#
[32m[20221213 20:45:53 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:45:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:53 @agent_ppo2.py:185][0m |           0.0479 |           0.0162 |           0.0000 |
[32m[20221213 20:45:53 @agent_ppo2.py:185][0m |           0.0060 |           0.0148 |           0.0000 |
[32m[20221213 20:45:53 @agent_ppo2.py:185][0m |          -0.0087 |           0.0144 |           0.0000 |
[32m[20221213 20:45:54 @agent_ppo2.py:185][0m |          -0.0262 |           0.0139 |           0.0000 |
[32m[20221213 20:45:54 @agent_ppo2.py:185][0m |          -0.0327 |           0.0134 |           0.0000 |
[32m[20221213 20:45:54 @agent_ppo2.py:185][0m |          -0.0381 |           0.0131 |           0.0000 |
[32m[20221213 20:45:54 @agent_ppo2.py:185][0m |          -0.0424 |           0.0130 |           0.0000 |
[32m[20221213 20:45:54 @agent_ppo2.py:185][0m |          -0.0469 |           0.0130 |           0.0000 |
[32m[20221213 20:45:54 @agent_ppo2.py:185][0m |          -0.0455 |           0.0127 |           0.0000 |
[32m[20221213 20:45:54 @agent_ppo2.py:185][0m |          -0.0423 |           0.0126 |           0.0000 |
[32m[20221213 20:45:54 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:45:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.17
[32m[20221213 20:45:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.95
[32m[20221213 20:45:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.43
[32m[20221213 20:45:54 @agent_ppo2.py:143][0m Total time:      16.75 min
[32m[20221213 20:45:54 @agent_ppo2.py:145][0m 2404352 total steps have happened
[32m[20221213 20:45:54 @agent_ppo2.py:121][0m #------------------------ Iteration 587 --------------------------#
[32m[20221213 20:45:55 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:45:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:55 @agent_ppo2.py:185][0m |          -0.0027 |           0.1065 |           0.0000 |
[32m[20221213 20:45:55 @agent_ppo2.py:185][0m |           0.0504 |           0.0628 |           0.0000 |
[32m[20221213 20:45:55 @agent_ppo2.py:185][0m |           0.0378 |           0.0398 |           0.0000 |
[32m[20221213 20:45:55 @agent_ppo2.py:185][0m |           0.0133 |           0.0356 |           0.0000 |
[32m[20221213 20:45:55 @agent_ppo2.py:185][0m |          -0.0029 |           0.0316 |           0.0000 |
[32m[20221213 20:45:55 @agent_ppo2.py:185][0m |          -0.0019 |           0.0298 |           0.0000 |
[32m[20221213 20:45:56 @agent_ppo2.py:185][0m |          -0.0125 |           0.0282 |           0.0000 |
[32m[20221213 20:45:56 @agent_ppo2.py:185][0m |          -0.0152 |           0.0278 |           0.0000 |
[32m[20221213 20:45:56 @agent_ppo2.py:185][0m |          -0.0202 |           0.0271 |           0.0000 |
[32m[20221213 20:45:56 @agent_ppo2.py:185][0m |          -0.0225 |           0.0266 |           0.0000 |
[32m[20221213 20:45:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:45:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.94
[32m[20221213 20:45:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.92
[32m[20221213 20:45:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.71
[32m[20221213 20:45:56 @agent_ppo2.py:143][0m Total time:      16.78 min
[32m[20221213 20:45:56 @agent_ppo2.py:145][0m 2408448 total steps have happened
[32m[20221213 20:45:56 @agent_ppo2.py:121][0m #------------------------ Iteration 588 --------------------------#
[32m[20221213 20:45:57 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:45:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:57 @agent_ppo2.py:185][0m |           0.0873 |           0.0275 |           0.0000 |
[32m[20221213 20:45:57 @agent_ppo2.py:185][0m |           0.0181 |           0.0156 |           0.0000 |
[32m[20221213 20:45:57 @agent_ppo2.py:185][0m |          -0.0052 |           0.0146 |           0.0000 |
[32m[20221213 20:45:57 @agent_ppo2.py:185][0m |          -0.0172 |           0.0141 |           0.0000 |
[32m[20221213 20:45:57 @agent_ppo2.py:185][0m |          -0.0336 |           0.0136 |           0.0000 |
[32m[20221213 20:45:57 @agent_ppo2.py:185][0m |          -0.0414 |           0.0134 |           0.0000 |
[32m[20221213 20:45:57 @agent_ppo2.py:185][0m |          -0.0454 |           0.0132 |           0.0000 |
[32m[20221213 20:45:57 @agent_ppo2.py:185][0m |          -0.0271 |           0.0130 |           0.0000 |
[32m[20221213 20:45:58 @agent_ppo2.py:185][0m |          -0.0562 |           0.0128 |           0.0000 |
[32m[20221213 20:45:58 @agent_ppo2.py:185][0m |          -0.0513 |           0.0127 |           0.0000 |
[32m[20221213 20:45:58 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:45:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.18
[32m[20221213 20:45:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.42
[32m[20221213 20:45:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.56
[32m[20221213 20:45:58 @agent_ppo2.py:143][0m Total time:      16.81 min
[32m[20221213 20:45:58 @agent_ppo2.py:145][0m 2412544 total steps have happened
[32m[20221213 20:45:58 @agent_ppo2.py:121][0m #------------------------ Iteration 589 --------------------------#
[32m[20221213 20:45:58 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:45:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:45:59 @agent_ppo2.py:185][0m |           0.0247 |           0.0292 |           0.0000 |
[32m[20221213 20:45:59 @agent_ppo2.py:185][0m |          -0.0047 |           0.0222 |           0.0000 |
[32m[20221213 20:45:59 @agent_ppo2.py:185][0m |          -0.0243 |           0.0210 |           0.0000 |
[32m[20221213 20:45:59 @agent_ppo2.py:185][0m |          -0.0270 |           0.0200 |           0.0000 |
[32m[20221213 20:45:59 @agent_ppo2.py:185][0m |          -0.0347 |           0.0196 |           0.0000 |
[32m[20221213 20:45:59 @agent_ppo2.py:185][0m |          -0.0352 |           0.0191 |           0.0000 |
[32m[20221213 20:45:59 @agent_ppo2.py:185][0m |          -0.0380 |           0.0186 |           0.0000 |
[32m[20221213 20:45:59 @agent_ppo2.py:185][0m |          -0.0376 |           0.0177 |           0.0000 |
[32m[20221213 20:45:59 @agent_ppo2.py:185][0m |          -0.0389 |           0.0175 |           0.0000 |
[32m[20221213 20:45:59 @agent_ppo2.py:185][0m |          -0.0473 |           0.0174 |           0.0000 |
[32m[20221213 20:45:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:46:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.68
[32m[20221213 20:46:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.32
[32m[20221213 20:46:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.65
[32m[20221213 20:46:00 @agent_ppo2.py:143][0m Total time:      16.84 min
[32m[20221213 20:46:00 @agent_ppo2.py:145][0m 2416640 total steps have happened
[32m[20221213 20:46:00 @agent_ppo2.py:121][0m #------------------------ Iteration 590 --------------------------#
[32m[20221213 20:46:00 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:46:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:00 @agent_ppo2.py:185][0m |           0.0525 |           0.0193 |           0.0000 |
[32m[20221213 20:46:00 @agent_ppo2.py:185][0m |           0.0576 |           0.0163 |           0.0000 |
[32m[20221213 20:46:00 @agent_ppo2.py:185][0m |           0.0065 |           0.0158 |           0.0000 |
[32m[20221213 20:46:01 @agent_ppo2.py:185][0m |          -0.0069 |           0.0156 |           0.0000 |
[32m[20221213 20:46:01 @agent_ppo2.py:185][0m |          -0.0468 |           0.0151 |           0.0000 |
[32m[20221213 20:46:01 @agent_ppo2.py:185][0m |          -0.0663 |           0.0146 |           0.0000 |
[32m[20221213 20:46:01 @agent_ppo2.py:185][0m |          -0.0684 |           0.0144 |           0.0000 |
[32m[20221213 20:46:01 @agent_ppo2.py:185][0m |          -0.0703 |           0.0143 |           0.0000 |
[32m[20221213 20:46:01 @agent_ppo2.py:185][0m |          -0.0733 |           0.0146 |           0.0000 |
[32m[20221213 20:46:01 @agent_ppo2.py:185][0m |          -0.0726 |           0.0145 |           0.0000 |
[32m[20221213 20:46:01 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:46:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.81
[32m[20221213 20:46:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.42
[32m[20221213 20:46:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.63
[32m[20221213 20:46:01 @agent_ppo2.py:143][0m Total time:      16.87 min
[32m[20221213 20:46:01 @agent_ppo2.py:145][0m 2420736 total steps have happened
[32m[20221213 20:46:01 @agent_ppo2.py:121][0m #------------------------ Iteration 591 --------------------------#
[32m[20221213 20:46:02 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:02 @agent_ppo2.py:185][0m |           0.0150 |           0.0815 |           0.0000 |
[32m[20221213 20:46:02 @agent_ppo2.py:185][0m |           0.0059 |           0.0358 |           0.0000 |
[32m[20221213 20:46:02 @agent_ppo2.py:185][0m |          -0.0107 |           0.0289 |           0.0000 |
[32m[20221213 20:46:02 @agent_ppo2.py:185][0m |          -0.0198 |           0.0252 |           0.0000 |
[32m[20221213 20:46:02 @agent_ppo2.py:185][0m |          -0.0235 |           0.0240 |           0.0000 |
[32m[20221213 20:46:02 @agent_ppo2.py:185][0m |          -0.0215 |           0.0235 |           0.0000 |
[32m[20221213 20:46:02 @agent_ppo2.py:185][0m |          -0.0251 |           0.0234 |           0.0000 |
[32m[20221213 20:46:03 @agent_ppo2.py:185][0m |          -0.0284 |           0.0238 |           0.0000 |
[32m[20221213 20:46:03 @agent_ppo2.py:185][0m |          -0.0296 |           0.0224 |           0.0000 |
[32m[20221213 20:46:03 @agent_ppo2.py:185][0m |          -0.0282 |           0.0209 |           0.0000 |
[32m[20221213 20:46:03 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:46:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.65
[32m[20221213 20:46:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.26
[32m[20221213 20:46:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.13
[32m[20221213 20:46:03 @agent_ppo2.py:143][0m Total time:      16.90 min
[32m[20221213 20:46:03 @agent_ppo2.py:145][0m 2424832 total steps have happened
[32m[20221213 20:46:03 @agent_ppo2.py:121][0m #------------------------ Iteration 592 --------------------------#
[32m[20221213 20:46:03 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:04 @agent_ppo2.py:185][0m |           0.0283 |           0.0263 |           0.0000 |
[32m[20221213 20:46:04 @agent_ppo2.py:185][0m |           0.0151 |           0.0208 |           0.0000 |
[32m[20221213 20:46:04 @agent_ppo2.py:185][0m |          -0.0265 |           0.0195 |           0.0000 |
[32m[20221213 20:46:04 @agent_ppo2.py:185][0m |          -0.0388 |           0.0183 |           0.0000 |
[32m[20221213 20:46:04 @agent_ppo2.py:185][0m |          -0.0502 |           0.0180 |           0.0000 |
[32m[20221213 20:46:04 @agent_ppo2.py:185][0m |          -0.0523 |           0.0173 |           0.0000 |
[32m[20221213 20:46:04 @agent_ppo2.py:185][0m |          -0.0621 |           0.0170 |           0.0000 |
[32m[20221213 20:46:04 @agent_ppo2.py:185][0m |          -0.0402 |           0.0168 |           0.0000 |
[32m[20221213 20:46:04 @agent_ppo2.py:185][0m |          -0.0481 |           0.0173 |           0.0000 |
[32m[20221213 20:46:04 @agent_ppo2.py:185][0m |          -0.0726 |           0.0171 |           0.0000 |
[32m[20221213 20:46:04 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:46:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.15
[32m[20221213 20:46:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.61
[32m[20221213 20:46:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.31
[32m[20221213 20:46:05 @agent_ppo2.py:143][0m Total time:      16.93 min
[32m[20221213 20:46:05 @agent_ppo2.py:145][0m 2428928 total steps have happened
[32m[20221213 20:46:05 @agent_ppo2.py:121][0m #------------------------ Iteration 593 --------------------------#
[32m[20221213 20:46:05 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:46:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:05 @agent_ppo2.py:185][0m |           0.0461 |           0.0241 |           0.0000 |
[32m[20221213 20:46:05 @agent_ppo2.py:185][0m |           0.0379 |           0.0113 |           0.0000 |
[32m[20221213 20:46:06 @agent_ppo2.py:185][0m |           0.0110 |           0.0109 |           0.0000 |
[32m[20221213 20:46:06 @agent_ppo2.py:185][0m |           0.0015 |           0.0107 |           0.0000 |
[32m[20221213 20:46:06 @agent_ppo2.py:185][0m |          -0.0088 |           0.0105 |           0.0000 |
[32m[20221213 20:46:06 @agent_ppo2.py:185][0m |          -0.0138 |           0.0104 |           0.0000 |
[32m[20221213 20:46:06 @agent_ppo2.py:185][0m |          -0.0234 |           0.0103 |           0.0000 |
[32m[20221213 20:46:06 @agent_ppo2.py:185][0m |          -0.0271 |           0.0102 |           0.0000 |
[32m[20221213 20:46:06 @agent_ppo2.py:185][0m |          -0.0323 |           0.0102 |           0.0000 |
[32m[20221213 20:46:06 @agent_ppo2.py:185][0m |          -0.0356 |           0.0101 |           0.0000 |
[32m[20221213 20:46:06 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 20:46:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.25
[32m[20221213 20:46:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.53
[32m[20221213 20:46:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.65
[32m[20221213 20:46:07 @agent_ppo2.py:143][0m Total time:      16.96 min
[32m[20221213 20:46:07 @agent_ppo2.py:145][0m 2433024 total steps have happened
[32m[20221213 20:46:07 @agent_ppo2.py:121][0m #------------------------ Iteration 594 --------------------------#
[32m[20221213 20:46:07 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:46:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:07 @agent_ppo2.py:185][0m |           0.0179 |           0.0144 |           0.0000 |
[32m[20221213 20:46:07 @agent_ppo2.py:185][0m |          -0.0095 |           0.0116 |           0.0000 |
[32m[20221213 20:46:07 @agent_ppo2.py:185][0m |          -0.0188 |           0.0113 |           0.0000 |
[32m[20221213 20:46:07 @agent_ppo2.py:185][0m |          -0.0261 |           0.0113 |           0.0000 |
[32m[20221213 20:46:08 @agent_ppo2.py:185][0m |          -0.0313 |           0.0112 |           0.0000 |
[32m[20221213 20:46:08 @agent_ppo2.py:185][0m |          -0.0358 |           0.0111 |           0.0000 |
[32m[20221213 20:46:08 @agent_ppo2.py:185][0m |          -0.0369 |           0.0111 |           0.0000 |
[32m[20221213 20:46:08 @agent_ppo2.py:185][0m |          -0.0359 |           0.0109 |           0.0000 |
[32m[20221213 20:46:08 @agent_ppo2.py:185][0m |          -0.0725 |           0.0113 |           0.0000 |
[32m[20221213 20:46:08 @agent_ppo2.py:185][0m |          -0.0390 |           0.0111 |           0.0000 |
[32m[20221213 20:46:08 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:46:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.66
[32m[20221213 20:46:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.25
[32m[20221213 20:46:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.31
[32m[20221213 20:46:08 @agent_ppo2.py:143][0m Total time:      16.99 min
[32m[20221213 20:46:08 @agent_ppo2.py:145][0m 2437120 total steps have happened
[32m[20221213 20:46:08 @agent_ppo2.py:121][0m #------------------------ Iteration 595 --------------------------#
[32m[20221213 20:46:09 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:09 @agent_ppo2.py:185][0m |           0.0317 |           0.0330 |           0.0000 |
[32m[20221213 20:46:09 @agent_ppo2.py:185][0m |          -0.0024 |           0.0221 |           0.0000 |
[32m[20221213 20:46:09 @agent_ppo2.py:185][0m |          -0.0114 |           0.0213 |           0.0000 |
[32m[20221213 20:46:09 @agent_ppo2.py:185][0m |          -0.0195 |           0.0201 |           0.0000 |
[32m[20221213 20:46:09 @agent_ppo2.py:185][0m |          -0.0235 |           0.0190 |           0.0000 |
[32m[20221213 20:46:09 @agent_ppo2.py:185][0m |          -0.0221 |           0.0187 |           0.0000 |
[32m[20221213 20:46:09 @agent_ppo2.py:185][0m |          -0.0283 |           0.0183 |           0.0000 |
[32m[20221213 20:46:09 @agent_ppo2.py:185][0m |          -0.0332 |           0.0187 |           0.0000 |
[32m[20221213 20:46:10 @agent_ppo2.py:185][0m |          -0.0306 |           0.0179 |           0.0000 |
[32m[20221213 20:46:10 @agent_ppo2.py:185][0m |          -0.0331 |           0.0173 |           0.0000 |
[32m[20221213 20:46:10 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:46:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.18
[32m[20221213 20:46:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.44
[32m[20221213 20:46:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.51
[32m[20221213 20:46:10 @agent_ppo2.py:143][0m Total time:      17.01 min
[32m[20221213 20:46:10 @agent_ppo2.py:145][0m 2441216 total steps have happened
[32m[20221213 20:46:10 @agent_ppo2.py:121][0m #------------------------ Iteration 596 --------------------------#
[32m[20221213 20:46:10 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:11 @agent_ppo2.py:185][0m |           0.0241 |           0.0175 |           0.0000 |
[32m[20221213 20:46:11 @agent_ppo2.py:185][0m |          -0.0046 |           0.0152 |           0.0000 |
[32m[20221213 20:46:11 @agent_ppo2.py:185][0m |          -0.0239 |           0.0148 |           0.0000 |
[32m[20221213 20:46:11 @agent_ppo2.py:185][0m |          -0.0437 |           0.0143 |           0.0000 |
[32m[20221213 20:46:11 @agent_ppo2.py:185][0m |          -0.0151 |           0.0147 |           0.0000 |
[32m[20221213 20:46:11 @agent_ppo2.py:185][0m |          -0.0605 |           0.0151 |           0.0000 |
[32m[20221213 20:46:11 @agent_ppo2.py:185][0m |          -0.0642 |           0.0137 |           0.0000 |
[32m[20221213 20:46:11 @agent_ppo2.py:185][0m |          -0.0604 |           0.0136 |           0.0000 |
[32m[20221213 20:46:11 @agent_ppo2.py:185][0m |          -0.0683 |           0.0135 |           0.0000 |
[32m[20221213 20:46:11 @agent_ppo2.py:185][0m |          -0.0694 |           0.0135 |           0.0000 |
[32m[20221213 20:46:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:46:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.55
[32m[20221213 20:46:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.92
[32m[20221213 20:46:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.68
[32m[20221213 20:46:12 @agent_ppo2.py:143][0m Total time:      17.04 min
[32m[20221213 20:46:12 @agent_ppo2.py:145][0m 2445312 total steps have happened
[32m[20221213 20:46:12 @agent_ppo2.py:121][0m #------------------------ Iteration 597 --------------------------#
[32m[20221213 20:46:12 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:12 @agent_ppo2.py:185][0m |           0.0271 |           0.0254 |           0.0000 |
[32m[20221213 20:46:12 @agent_ppo2.py:185][0m |           0.0101 |           0.0201 |           0.0000 |
[32m[20221213 20:46:12 @agent_ppo2.py:185][0m |          -0.0105 |           0.0185 |           0.0000 |
[32m[20221213 20:46:12 @agent_ppo2.py:185][0m |          -0.0166 |           0.0183 |           0.0000 |
[32m[20221213 20:46:13 @agent_ppo2.py:185][0m |          -0.0284 |           0.0175 |           0.0000 |
[32m[20221213 20:46:13 @agent_ppo2.py:185][0m |          -0.0348 |           0.0168 |           0.0000 |
[32m[20221213 20:46:13 @agent_ppo2.py:185][0m |          -0.0334 |           0.0163 |           0.0000 |
[32m[20221213 20:46:13 @agent_ppo2.py:185][0m |          -0.0679 |           0.0165 |           0.0000 |
[32m[20221213 20:46:13 @agent_ppo2.py:185][0m |          -0.0331 |           0.0164 |           0.0000 |
[32m[20221213 20:46:13 @agent_ppo2.py:185][0m |          -0.0466 |           0.0158 |           0.0000 |
[32m[20221213 20:46:13 @agent_ppo2.py:130][0m Policy update time: 0.88 s
[32m[20221213 20:46:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.04
[32m[20221213 20:46:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.77
[32m[20221213 20:46:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.76
[32m[20221213 20:46:13 @agent_ppo2.py:143][0m Total time:      17.07 min
[32m[20221213 20:46:13 @agent_ppo2.py:145][0m 2449408 total steps have happened
[32m[20221213 20:46:13 @agent_ppo2.py:121][0m #------------------------ Iteration 598 --------------------------#
[32m[20221213 20:46:14 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:14 @agent_ppo2.py:185][0m |           0.0352 |           0.0226 |           0.0000 |
[32m[20221213 20:46:14 @agent_ppo2.py:185][0m |          -0.0069 |           0.0218 |           0.0000 |
[32m[20221213 20:46:14 @agent_ppo2.py:185][0m |          -0.0193 |           0.0190 |           0.0000 |
[32m[20221213 20:46:14 @agent_ppo2.py:185][0m |          -0.0426 |           0.0185 |           0.0000 |
[32m[20221213 20:46:14 @agent_ppo2.py:185][0m |          -0.0312 |           0.0175 |           0.0000 |
[32m[20221213 20:46:14 @agent_ppo2.py:185][0m |          -0.0460 |           0.0170 |           0.0000 |
[32m[20221213 20:46:14 @agent_ppo2.py:185][0m |          -0.0503 |           0.0168 |           0.0000 |
[32m[20221213 20:46:14 @agent_ppo2.py:185][0m |          -0.0897 |           0.0173 |           0.0000 |
[32m[20221213 20:46:15 @agent_ppo2.py:185][0m |          -0.0594 |           0.0169 |           0.0000 |
[32m[20221213 20:46:15 @agent_ppo2.py:185][0m |          -0.0622 |           0.0159 |           0.0000 |
[32m[20221213 20:46:15 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:46:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.18
[32m[20221213 20:46:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.36
[32m[20221213 20:46:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.37
[32m[20221213 20:46:15 @agent_ppo2.py:143][0m Total time:      17.10 min
[32m[20221213 20:46:15 @agent_ppo2.py:145][0m 2453504 total steps have happened
[32m[20221213 20:46:15 @agent_ppo2.py:121][0m #------------------------ Iteration 599 --------------------------#
[32m[20221213 20:46:15 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:46:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:16 @agent_ppo2.py:185][0m |           0.0326 |           0.0168 |           0.0000 |
[32m[20221213 20:46:16 @agent_ppo2.py:185][0m |          -0.0158 |           0.0158 |           0.0000 |
[32m[20221213 20:46:16 @agent_ppo2.py:185][0m |          -0.0386 |           0.0156 |           0.0000 |
[32m[20221213 20:46:16 @agent_ppo2.py:185][0m |          -0.0593 |           0.0151 |           0.0000 |
[32m[20221213 20:46:16 @agent_ppo2.py:185][0m |          -0.0707 |           0.0149 |           0.0000 |
[32m[20221213 20:46:16 @agent_ppo2.py:185][0m |          -0.0788 |           0.0152 |           0.0000 |
[32m[20221213 20:46:16 @agent_ppo2.py:185][0m |          -0.0946 |           0.0181 |           0.0000 |
[32m[20221213 20:46:16 @agent_ppo2.py:185][0m |          -0.0856 |           0.0182 |           0.0000 |
[32m[20221213 20:46:16 @agent_ppo2.py:185][0m |          -0.0868 |           0.0151 |           0.0000 |
[32m[20221213 20:46:16 @agent_ppo2.py:185][0m |          -0.0731 |           0.0153 |           0.0000 |
[32m[20221213 20:46:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:46:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.07
[32m[20221213 20:46:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.00
[32m[20221213 20:46:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.09
[32m[20221213 20:46:17 @agent_ppo2.py:143][0m Total time:      17.13 min
[32m[20221213 20:46:17 @agent_ppo2.py:145][0m 2457600 total steps have happened
[32m[20221213 20:46:17 @agent_ppo2.py:121][0m #------------------------ Iteration 600 --------------------------#
[32m[20221213 20:46:17 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:46:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:17 @agent_ppo2.py:185][0m |           0.0065 |           0.0167 |           0.0000 |
[32m[20221213 20:46:17 @agent_ppo2.py:185][0m |          -0.0128 |           0.0104 |           0.0000 |
[32m[20221213 20:46:17 @agent_ppo2.py:185][0m |          -0.0264 |           0.0102 |           0.0000 |
[32m[20221213 20:46:18 @agent_ppo2.py:185][0m |          -0.0199 |           0.0104 |           0.0000 |
[32m[20221213 20:46:18 @agent_ppo2.py:185][0m |          -0.0407 |           0.0101 |           0.0000 |
[32m[20221213 20:46:18 @agent_ppo2.py:185][0m |          -0.0430 |           0.0099 |           0.0000 |
[32m[20221213 20:46:18 @agent_ppo2.py:185][0m |          -0.0465 |           0.0099 |           0.0000 |
[32m[20221213 20:46:18 @agent_ppo2.py:185][0m |          -0.0452 |           0.0098 |           0.0000 |
[32m[20221213 20:46:18 @agent_ppo2.py:185][0m |          -0.0488 |           0.0097 |           0.0000 |
[32m[20221213 20:46:18 @agent_ppo2.py:185][0m |          -0.0595 |           0.0097 |           0.0000 |
[32m[20221213 20:46:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:46:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.42
[32m[20221213 20:46:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.71
[32m[20221213 20:46:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.57
[32m[20221213 20:46:18 @agent_ppo2.py:143][0m Total time:      17.15 min
[32m[20221213 20:46:18 @agent_ppo2.py:145][0m 2461696 total steps have happened
[32m[20221213 20:46:18 @agent_ppo2.py:121][0m #------------------------ Iteration 601 --------------------------#
[32m[20221213 20:46:19 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:19 @agent_ppo2.py:185][0m |           0.0362 |           0.0352 |           0.0000 |
[32m[20221213 20:46:19 @agent_ppo2.py:185][0m |           0.0045 |           0.0192 |           0.0000 |
[32m[20221213 20:46:19 @agent_ppo2.py:185][0m |          -0.0093 |           0.0171 |           0.0000 |
[32m[20221213 20:46:19 @agent_ppo2.py:185][0m |          -0.0119 |           0.0164 |           0.0000 |
[32m[20221213 20:46:19 @agent_ppo2.py:185][0m |          -0.0215 |           0.0158 |           0.0000 |
[32m[20221213 20:46:19 @agent_ppo2.py:185][0m |          -0.0271 |           0.0155 |           0.0000 |
[32m[20221213 20:46:19 @agent_ppo2.py:185][0m |          -0.0305 |           0.0153 |           0.0000 |
[32m[20221213 20:46:20 @agent_ppo2.py:185][0m |          -0.0252 |           0.0150 |           0.0000 |
[32m[20221213 20:46:20 @agent_ppo2.py:185][0m |          -0.0292 |           0.0150 |           0.0000 |
[32m[20221213 20:46:20 @agent_ppo2.py:185][0m |          -0.0308 |           0.0148 |           0.0000 |
[32m[20221213 20:46:20 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:46:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.73
[32m[20221213 20:46:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.36
[32m[20221213 20:46:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.62
[32m[20221213 20:46:20 @agent_ppo2.py:143][0m Total time:      17.18 min
[32m[20221213 20:46:20 @agent_ppo2.py:145][0m 2465792 total steps have happened
[32m[20221213 20:46:20 @agent_ppo2.py:121][0m #------------------------ Iteration 602 --------------------------#
[32m[20221213 20:46:20 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:21 @agent_ppo2.py:185][0m |           0.0728 |           0.0244 |           0.0000 |
[32m[20221213 20:46:21 @agent_ppo2.py:185][0m |           0.0176 |           0.0106 |           0.0000 |
[32m[20221213 20:46:21 @agent_ppo2.py:185][0m |           0.0013 |           0.0100 |           0.0000 |
[32m[20221213 20:46:21 @agent_ppo2.py:185][0m |          -0.0084 |           0.0098 |           0.0000 |
[32m[20221213 20:46:21 @agent_ppo2.py:185][0m |          -0.0153 |           0.0097 |           0.0000 |
[32m[20221213 20:46:21 @agent_ppo2.py:185][0m |          -0.0211 |           0.0096 |           0.0000 |
[32m[20221213 20:46:21 @agent_ppo2.py:185][0m |          -0.0255 |           0.0096 |           0.0000 |
[32m[20221213 20:46:21 @agent_ppo2.py:185][0m |          -0.0313 |           0.0096 |           0.0000 |
[32m[20221213 20:46:21 @agent_ppo2.py:185][0m |           0.0002 |           0.0095 |           0.0000 |
[32m[20221213 20:46:21 @agent_ppo2.py:185][0m |          -0.0429 |           0.0094 |           0.0000 |
[32m[20221213 20:46:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:46:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.12
[32m[20221213 20:46:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.44
[32m[20221213 20:46:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.33
[32m[20221213 20:46:22 @agent_ppo2.py:143][0m Total time:      17.21 min
[32m[20221213 20:46:22 @agent_ppo2.py:145][0m 2469888 total steps have happened
[32m[20221213 20:46:22 @agent_ppo2.py:121][0m #------------------------ Iteration 603 --------------------------#
[32m[20221213 20:46:22 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:22 @agent_ppo2.py:185][0m |           0.0280 |           0.0264 |           0.0000 |
[32m[20221213 20:46:22 @agent_ppo2.py:185][0m |          -0.0375 |           0.0204 |           0.0000 |
[32m[20221213 20:46:22 @agent_ppo2.py:185][0m |           0.0068 |           0.0201 |           0.0000 |
[32m[20221213 20:46:23 @agent_ppo2.py:185][0m |           0.0002 |           0.0197 |           0.0000 |
[32m[20221213 20:46:23 @agent_ppo2.py:185][0m |          -0.0172 |           0.0170 |           0.0000 |
[32m[20221213 20:46:23 @agent_ppo2.py:185][0m |          -0.0232 |           0.0162 |           0.0000 |
[32m[20221213 20:46:23 @agent_ppo2.py:185][0m |          -0.0209 |           0.0160 |           0.0000 |
[32m[20221213 20:46:23 @agent_ppo2.py:185][0m |          -0.0242 |           0.0159 |           0.0000 |
[32m[20221213 20:46:23 @agent_ppo2.py:185][0m |          -0.0266 |           0.0158 |           0.0000 |
[32m[20221213 20:46:23 @agent_ppo2.py:185][0m |          -0.0266 |           0.0157 |           0.0000 |
[32m[20221213 20:46:23 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:46:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.50
[32m[20221213 20:46:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.67
[32m[20221213 20:46:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.17
[32m[20221213 20:46:23 @agent_ppo2.py:143][0m Total time:      17.24 min
[32m[20221213 20:46:23 @agent_ppo2.py:145][0m 2473984 total steps have happened
[32m[20221213 20:46:23 @agent_ppo2.py:121][0m #------------------------ Iteration 604 --------------------------#
[32m[20221213 20:46:24 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:24 @agent_ppo2.py:185][0m |           0.0428 |           0.0238 |           0.0000 |
[32m[20221213 20:46:24 @agent_ppo2.py:185][0m |           0.0121 |           0.0091 |           0.0000 |
[32m[20221213 20:46:24 @agent_ppo2.py:185][0m |          -0.0055 |           0.0082 |           0.0000 |
[32m[20221213 20:46:24 @agent_ppo2.py:185][0m |          -0.0156 |           0.0079 |           0.0000 |
[32m[20221213 20:46:24 @agent_ppo2.py:185][0m |          -0.0222 |           0.0078 |           0.0000 |
[32m[20221213 20:46:24 @agent_ppo2.py:185][0m |          -0.0276 |           0.0076 |           0.0000 |
[32m[20221213 20:46:24 @agent_ppo2.py:185][0m |          -0.0316 |           0.0075 |           0.0000 |
[32m[20221213 20:46:25 @agent_ppo2.py:185][0m |          -0.0369 |           0.0074 |           0.0000 |
[32m[20221213 20:46:25 @agent_ppo2.py:185][0m |          -0.0367 |           0.0073 |           0.0000 |
[32m[20221213 20:46:25 @agent_ppo2.py:185][0m |          -0.0429 |           0.0073 |           0.0000 |
[32m[20221213 20:46:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:46:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.07
[32m[20221213 20:46:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.08
[32m[20221213 20:46:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.57
[32m[20221213 20:46:25 @agent_ppo2.py:143][0m Total time:      17.26 min
[32m[20221213 20:46:25 @agent_ppo2.py:145][0m 2478080 total steps have happened
[32m[20221213 20:46:25 @agent_ppo2.py:121][0m #------------------------ Iteration 605 --------------------------#
[32m[20221213 20:46:25 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:26 @agent_ppo2.py:185][0m |           0.0211 |           0.1360 |           0.0000 |
[32m[20221213 20:46:26 @agent_ppo2.py:185][0m |           0.0066 |           0.0528 |           0.0000 |
[32m[20221213 20:46:26 @agent_ppo2.py:185][0m |           0.0002 |           0.0326 |           0.0000 |
[32m[20221213 20:46:26 @agent_ppo2.py:185][0m |          -0.0157 |           0.0265 |           0.0000 |
[32m[20221213 20:46:26 @agent_ppo2.py:185][0m |          -0.0326 |           0.0256 |           0.0000 |
[32m[20221213 20:46:26 @agent_ppo2.py:185][0m |          -0.0221 |           0.0255 |           0.0000 |
[32m[20221213 20:46:26 @agent_ppo2.py:185][0m |          -0.0244 |           0.0227 |           0.0000 |
[32m[20221213 20:46:26 @agent_ppo2.py:185][0m |          -0.0279 |           0.0209 |           0.0000 |
[32m[20221213 20:46:26 @agent_ppo2.py:185][0m |          -0.0285 |           0.0210 |           0.0000 |
[32m[20221213 20:46:26 @agent_ppo2.py:185][0m |          -0.0287 |           0.0209 |           0.0000 |
[32m[20221213 20:46:26 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:46:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.34
[32m[20221213 20:46:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.17
[32m[20221213 20:46:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.86
[32m[20221213 20:46:27 @agent_ppo2.py:143][0m Total time:      17.29 min
[32m[20221213 20:46:27 @agent_ppo2.py:145][0m 2482176 total steps have happened
[32m[20221213 20:46:27 @agent_ppo2.py:121][0m #------------------------ Iteration 606 --------------------------#
[32m[20221213 20:46:27 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:27 @agent_ppo2.py:185][0m |           0.0649 |           0.0215 |           0.0000 |
[32m[20221213 20:46:27 @agent_ppo2.py:185][0m |           0.0057 |           0.0157 |           0.0000 |
[32m[20221213 20:46:27 @agent_ppo2.py:185][0m |          -0.0160 |           0.0148 |           0.0000 |
[32m[20221213 20:46:27 @agent_ppo2.py:185][0m |          -0.0001 |           0.0158 |           0.0000 |
[32m[20221213 20:46:28 @agent_ppo2.py:185][0m |          -0.0169 |           0.0156 |           0.0000 |
[32m[20221213 20:46:28 @agent_ppo2.py:185][0m |          -0.0426 |           0.0144 |           0.0000 |
[32m[20221213 20:46:28 @agent_ppo2.py:185][0m |          -0.0478 |           0.0138 |           0.0000 |
[32m[20221213 20:46:28 @agent_ppo2.py:185][0m |          -0.0491 |           0.0137 |           0.0000 |
[32m[20221213 20:46:28 @agent_ppo2.py:185][0m |          -0.0529 |           0.0140 |           0.0000 |
[32m[20221213 20:46:28 @agent_ppo2.py:185][0m |          -0.0599 |           0.0134 |           0.0000 |
[32m[20221213 20:46:28 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:46:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.04
[32m[20221213 20:46:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.44
[32m[20221213 20:46:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.73
[32m[20221213 20:46:28 @agent_ppo2.py:143][0m Total time:      17.32 min
[32m[20221213 20:46:28 @agent_ppo2.py:145][0m 2486272 total steps have happened
[32m[20221213 20:46:28 @agent_ppo2.py:121][0m #------------------------ Iteration 607 --------------------------#
[32m[20221213 20:46:29 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:29 @agent_ppo2.py:185][0m |           0.0236 |           0.0289 |           0.0000 |
[32m[20221213 20:46:29 @agent_ppo2.py:185][0m |          -0.0049 |           0.0192 |           0.0000 |
[32m[20221213 20:46:29 @agent_ppo2.py:185][0m |          -0.0205 |           0.0178 |           0.0000 |
[32m[20221213 20:46:29 @agent_ppo2.py:185][0m |          -0.0220 |           0.0177 |           0.0000 |
[32m[20221213 20:46:29 @agent_ppo2.py:185][0m |          -0.0301 |           0.0167 |           0.0000 |
[32m[20221213 20:46:29 @agent_ppo2.py:185][0m |          -0.0310 |           0.0165 |           0.0000 |
[32m[20221213 20:46:29 @agent_ppo2.py:185][0m |          -0.0347 |           0.0161 |           0.0000 |
[32m[20221213 20:46:29 @agent_ppo2.py:185][0m |          -0.0345 |           0.0158 |           0.0000 |
[32m[20221213 20:46:30 @agent_ppo2.py:185][0m |          -0.0374 |           0.0156 |           0.0000 |
[32m[20221213 20:46:30 @agent_ppo2.py:185][0m |          -0.0415 |           0.0154 |           0.0000 |
[32m[20221213 20:46:30 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:46:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.29
[32m[20221213 20:46:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.23
[32m[20221213 20:46:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.65
[32m[20221213 20:46:30 @agent_ppo2.py:143][0m Total time:      17.35 min
[32m[20221213 20:46:30 @agent_ppo2.py:145][0m 2490368 total steps have happened
[32m[20221213 20:46:30 @agent_ppo2.py:121][0m #------------------------ Iteration 608 --------------------------#
[32m[20221213 20:46:30 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:46:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:31 @agent_ppo2.py:185][0m |           0.0427 |           0.0281 |           0.0000 |
[32m[20221213 20:46:31 @agent_ppo2.py:185][0m |           0.0105 |           0.0213 |           0.0000 |
[32m[20221213 20:46:31 @agent_ppo2.py:185][0m |          -0.0150 |           0.0205 |           0.0000 |
[32m[20221213 20:46:31 @agent_ppo2.py:185][0m |          -0.0207 |           0.0194 |           0.0000 |
[32m[20221213 20:46:31 @agent_ppo2.py:185][0m |          -0.0288 |           0.0186 |           0.0000 |
[32m[20221213 20:46:31 @agent_ppo2.py:185][0m |          -0.0311 |           0.0182 |           0.0000 |
[32m[20221213 20:46:31 @agent_ppo2.py:185][0m |          -0.0368 |           0.0180 |           0.0000 |
[32m[20221213 20:46:31 @agent_ppo2.py:185][0m |          -0.0381 |           0.0180 |           0.0000 |
[32m[20221213 20:46:31 @agent_ppo2.py:185][0m |          -0.0438 |           0.0182 |           0.0000 |
[32m[20221213 20:46:32 @agent_ppo2.py:185][0m |          -0.0360 |           0.0173 |           0.0000 |
[32m[20221213 20:46:32 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 20:46:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.16
[32m[20221213 20:46:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.35
[32m[20221213 20:46:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.60
[32m[20221213 20:46:32 @agent_ppo2.py:143][0m Total time:      17.39 min
[32m[20221213 20:46:32 @agent_ppo2.py:145][0m 2494464 total steps have happened
[32m[20221213 20:46:32 @agent_ppo2.py:121][0m #------------------------ Iteration 609 --------------------------#
[32m[20221213 20:46:33 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:46:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:33 @agent_ppo2.py:185][0m |           0.0383 |           0.0555 |           0.0000 |
[32m[20221213 20:46:33 @agent_ppo2.py:185][0m |          -0.0339 |           0.0453 |           0.0000 |
[32m[20221213 20:46:33 @agent_ppo2.py:185][0m |          -0.0262 |           0.0445 |           0.0000 |
[32m[20221213 20:46:33 @agent_ppo2.py:185][0m |          -0.0296 |           0.0326 |           0.0000 |
[32m[20221213 20:46:33 @agent_ppo2.py:185][0m |          -0.0314 |           0.0300 |           0.0000 |
[32m[20221213 20:46:33 @agent_ppo2.py:185][0m |          -0.0402 |           0.0300 |           0.0000 |
[32m[20221213 20:46:33 @agent_ppo2.py:185][0m |          -0.0454 |           0.0272 |           0.0000 |
[32m[20221213 20:46:34 @agent_ppo2.py:185][0m |          -0.0447 |           0.0270 |           0.0000 |
[32m[20221213 20:46:34 @agent_ppo2.py:185][0m |          -0.0454 |           0.0258 |           0.0000 |
[32m[20221213 20:46:34 @agent_ppo2.py:185][0m |          -0.0500 |           0.0259 |           0.0000 |
[32m[20221213 20:46:34 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:46:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.05
[32m[20221213 20:46:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.29
[32m[20221213 20:46:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.11
[32m[20221213 20:46:34 @agent_ppo2.py:143][0m Total time:      17.42 min
[32m[20221213 20:46:34 @agent_ppo2.py:145][0m 2498560 total steps have happened
[32m[20221213 20:46:34 @agent_ppo2.py:121][0m #------------------------ Iteration 610 --------------------------#
[32m[20221213 20:46:35 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:46:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:35 @agent_ppo2.py:185][0m |           0.0448 |           0.0563 |           0.0000 |
[32m[20221213 20:46:35 @agent_ppo2.py:185][0m |           0.0475 |           0.0219 |           0.0000 |
[32m[20221213 20:46:35 @agent_ppo2.py:185][0m |           0.0114 |           0.0239 |           0.0000 |
[32m[20221213 20:46:35 @agent_ppo2.py:185][0m |          -0.0124 |           0.0235 |           0.0000 |
[32m[20221213 20:46:35 @agent_ppo2.py:185][0m |          -0.0086 |           0.0198 |           0.0000 |
[32m[20221213 20:46:35 @agent_ppo2.py:185][0m |          -0.0197 |           0.0185 |           0.0000 |
[32m[20221213 20:46:35 @agent_ppo2.py:185][0m |          -0.0273 |           0.0183 |           0.0000 |
[32m[20221213 20:46:35 @agent_ppo2.py:185][0m |          -0.0317 |           0.0180 |           0.0000 |
[32m[20221213 20:46:35 @agent_ppo2.py:185][0m |          -0.0494 |           0.0199 |           0.0000 |
[32m[20221213 20:46:35 @agent_ppo2.py:185][0m |          -0.0415 |           0.0191 |           0.0000 |
[32m[20221213 20:46:35 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:46:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.24
[32m[20221213 20:46:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.86
[32m[20221213 20:46:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.36
[32m[20221213 20:46:36 @agent_ppo2.py:143][0m Total time:      17.45 min
[32m[20221213 20:46:36 @agent_ppo2.py:145][0m 2502656 total steps have happened
[32m[20221213 20:46:36 @agent_ppo2.py:121][0m #------------------------ Iteration 611 --------------------------#
[32m[20221213 20:46:36 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:46:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:36 @agent_ppo2.py:185][0m |           0.0328 |           0.0183 |           0.0000 |
[32m[20221213 20:46:37 @agent_ppo2.py:185][0m |          -0.0148 |           0.0137 |           0.0000 |
[32m[20221213 20:46:37 @agent_ppo2.py:185][0m |          -0.0382 |           0.0133 |           0.0000 |
[32m[20221213 20:46:37 @agent_ppo2.py:185][0m |          -0.0477 |           0.0131 |           0.0000 |
[32m[20221213 20:46:37 @agent_ppo2.py:185][0m |          -0.0518 |           0.0131 |           0.0000 |
[32m[20221213 20:46:37 @agent_ppo2.py:185][0m |          -0.0595 |           0.0130 |           0.0000 |
[32m[20221213 20:46:37 @agent_ppo2.py:185][0m |          -0.0565 |           0.0128 |           0.0000 |
[32m[20221213 20:46:37 @agent_ppo2.py:185][0m |          -0.0656 |           0.0127 |           0.0000 |
[32m[20221213 20:46:37 @agent_ppo2.py:185][0m |          -0.0635 |           0.0126 |           0.0000 |
[32m[20221213 20:46:37 @agent_ppo2.py:185][0m |          -0.0728 |           0.0126 |           0.0000 |
[32m[20221213 20:46:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:46:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.28
[32m[20221213 20:46:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.63
[32m[20221213 20:46:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.08
[32m[20221213 20:46:38 @agent_ppo2.py:143][0m Total time:      17.47 min
[32m[20221213 20:46:38 @agent_ppo2.py:145][0m 2506752 total steps have happened
[32m[20221213 20:46:38 @agent_ppo2.py:121][0m #------------------------ Iteration 612 --------------------------#
[32m[20221213 20:46:38 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:46:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:38 @agent_ppo2.py:185][0m |           0.0324 |           0.0441 |           0.0000 |
[32m[20221213 20:46:38 @agent_ppo2.py:185][0m |          -0.0078 |           0.0197 |           0.0000 |
[32m[20221213 20:46:38 @agent_ppo2.py:185][0m |          -0.0121 |           0.0173 |           0.0000 |
[32m[20221213 20:46:38 @agent_ppo2.py:185][0m |          -0.0213 |           0.0168 |           0.0000 |
[32m[20221213 20:46:38 @agent_ppo2.py:185][0m |          -0.0200 |           0.0154 |           0.0000 |
[32m[20221213 20:46:39 @agent_ppo2.py:185][0m |          -0.0244 |           0.0148 |           0.0000 |
[32m[20221213 20:46:39 @agent_ppo2.py:185][0m |          -0.0273 |           0.0148 |           0.0000 |
[32m[20221213 20:46:39 @agent_ppo2.py:185][0m |          -0.0294 |           0.0146 |           0.0000 |
[32m[20221213 20:46:39 @agent_ppo2.py:185][0m |          -0.0300 |           0.0145 |           0.0000 |
[32m[20221213 20:46:39 @agent_ppo2.py:185][0m |          -0.0322 |           0.0143 |           0.0000 |
[32m[20221213 20:46:39 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:46:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.91
[32m[20221213 20:46:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.61
[32m[20221213 20:46:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.72
[32m[20221213 20:46:39 @agent_ppo2.py:143][0m Total time:      17.50 min
[32m[20221213 20:46:39 @agent_ppo2.py:145][0m 2510848 total steps have happened
[32m[20221213 20:46:39 @agent_ppo2.py:121][0m #------------------------ Iteration 613 --------------------------#
[32m[20221213 20:46:40 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:40 @agent_ppo2.py:185][0m |           0.0296 |           0.0156 |           0.0000 |
[32m[20221213 20:46:40 @agent_ppo2.py:185][0m |           0.0074 |           0.0130 |           0.0000 |
[32m[20221213 20:46:40 @agent_ppo2.py:185][0m |          -0.0283 |           0.0125 |           0.0000 |
[32m[20221213 20:46:40 @agent_ppo2.py:185][0m |          -0.0442 |           0.0123 |           0.0000 |
[32m[20221213 20:46:40 @agent_ppo2.py:185][0m |          -0.0505 |           0.0122 |           0.0000 |
[32m[20221213 20:46:40 @agent_ppo2.py:185][0m |          -0.0497 |           0.0121 |           0.0000 |
[32m[20221213 20:46:40 @agent_ppo2.py:185][0m |          -0.0554 |           0.0119 |           0.0000 |
[32m[20221213 20:46:40 @agent_ppo2.py:185][0m |          -0.0568 |           0.0120 |           0.0000 |
[32m[20221213 20:46:40 @agent_ppo2.py:185][0m |          -0.0604 |           0.0118 |           0.0000 |
[32m[20221213 20:46:41 @agent_ppo2.py:185][0m |          -0.0604 |           0.0118 |           0.0000 |
[32m[20221213 20:46:41 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:46:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.61
[32m[20221213 20:46:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.80
[32m[20221213 20:46:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.75
[32m[20221213 20:46:41 @agent_ppo2.py:143][0m Total time:      17.53 min
[32m[20221213 20:46:41 @agent_ppo2.py:145][0m 2514944 total steps have happened
[32m[20221213 20:46:41 @agent_ppo2.py:121][0m #------------------------ Iteration 614 --------------------------#
[32m[20221213 20:46:41 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:46:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:41 @agent_ppo2.py:185][0m |           0.0385 |           0.0348 |           0.0000 |
[32m[20221213 20:46:42 @agent_ppo2.py:185][0m |          -0.0022 |           0.0200 |           0.0000 |
[32m[20221213 20:46:42 @agent_ppo2.py:185][0m |          -0.0174 |           0.0187 |           0.0000 |
[32m[20221213 20:46:42 @agent_ppo2.py:185][0m |          -0.0203 |           0.0180 |           0.0000 |
[32m[20221213 20:46:42 @agent_ppo2.py:185][0m |          -0.0215 |           0.0175 |           0.0000 |
[32m[20221213 20:46:42 @agent_ppo2.py:185][0m |          -0.0222 |           0.0170 |           0.0000 |
[32m[20221213 20:46:42 @agent_ppo2.py:185][0m |          -0.0295 |           0.0169 |           0.0000 |
[32m[20221213 20:46:42 @agent_ppo2.py:185][0m |          -0.0303 |           0.0164 |           0.0000 |
[32m[20221213 20:46:42 @agent_ppo2.py:185][0m |          -0.0341 |           0.0161 |           0.0000 |
[32m[20221213 20:46:42 @agent_ppo2.py:185][0m |          -0.0913 |           0.0188 |           0.0000 |
[32m[20221213 20:46:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:46:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.30
[32m[20221213 20:46:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.63
[32m[20221213 20:46:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.70
[32m[20221213 20:46:43 @agent_ppo2.py:143][0m Total time:      17.56 min
[32m[20221213 20:46:43 @agent_ppo2.py:145][0m 2519040 total steps have happened
[32m[20221213 20:46:43 @agent_ppo2.py:121][0m #------------------------ Iteration 615 --------------------------#
[32m[20221213 20:46:43 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:46:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:43 @agent_ppo2.py:185][0m |           0.0523 |           0.0314 |           0.0000 |
[32m[20221213 20:46:43 @agent_ppo2.py:185][0m |           0.0108 |           0.0144 |           0.0000 |
[32m[20221213 20:46:43 @agent_ppo2.py:185][0m |          -0.0023 |           0.0132 |           0.0000 |
[32m[20221213 20:46:43 @agent_ppo2.py:185][0m |          -0.0153 |           0.0129 |           0.0000 |
[32m[20221213 20:46:44 @agent_ppo2.py:185][0m |          -0.0310 |           0.0126 |           0.0000 |
[32m[20221213 20:46:44 @agent_ppo2.py:185][0m |          -0.0391 |           0.0125 |           0.0000 |
[32m[20221213 20:46:44 @agent_ppo2.py:185][0m |          -0.0467 |           0.0123 |           0.0000 |
[32m[20221213 20:46:44 @agent_ppo2.py:185][0m |          -0.0441 |           0.0122 |           0.0000 |
[32m[20221213 20:46:44 @agent_ppo2.py:185][0m |          -0.0464 |           0.0122 |           0.0000 |
[32m[20221213 20:46:44 @agent_ppo2.py:185][0m |          -0.0516 |           0.0121 |           0.0000 |
[32m[20221213 20:46:44 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:46:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.66
[32m[20221213 20:46:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.53
[32m[20221213 20:46:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.20
[32m[20221213 20:46:44 @agent_ppo2.py:143][0m Total time:      17.59 min
[32m[20221213 20:46:44 @agent_ppo2.py:145][0m 2523136 total steps have happened
[32m[20221213 20:46:44 @agent_ppo2.py:121][0m #------------------------ Iteration 616 --------------------------#
[32m[20221213 20:46:45 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:46:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:45 @agent_ppo2.py:185][0m |           0.0336 |           0.0333 |           0.0000 |
[32m[20221213 20:46:45 @agent_ppo2.py:185][0m |           0.0095 |           0.0227 |           0.0000 |
[32m[20221213 20:46:45 @agent_ppo2.py:185][0m |          -0.0109 |           0.0186 |           0.0000 |
[32m[20221213 20:46:45 @agent_ppo2.py:185][0m |          -0.0205 |           0.0184 |           0.0000 |
[32m[20221213 20:46:45 @agent_ppo2.py:185][0m |          -0.0232 |           0.0180 |           0.0000 |
[32m[20221213 20:46:45 @agent_ppo2.py:185][0m |          -0.0304 |           0.0172 |           0.0000 |
[32m[20221213 20:46:45 @agent_ppo2.py:185][0m |          -0.0364 |           0.0169 |           0.0000 |
[32m[20221213 20:46:46 @agent_ppo2.py:185][0m |          -0.0391 |           0.0169 |           0.0000 |
[32m[20221213 20:46:46 @agent_ppo2.py:185][0m |          -0.0431 |           0.0165 |           0.0000 |
[32m[20221213 20:46:46 @agent_ppo2.py:185][0m |          -0.0386 |           0.0166 |           0.0000 |
[32m[20221213 20:46:46 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:46:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.15
[32m[20221213 20:46:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.90
[32m[20221213 20:46:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.14
[32m[20221213 20:46:46 @agent_ppo2.py:143][0m Total time:      17.62 min
[32m[20221213 20:46:46 @agent_ppo2.py:145][0m 2527232 total steps have happened
[32m[20221213 20:46:46 @agent_ppo2.py:121][0m #------------------------ Iteration 617 --------------------------#
[32m[20221213 20:46:47 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:46:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:47 @agent_ppo2.py:185][0m |           0.0273 |           0.0192 |           0.0000 |
[32m[20221213 20:46:47 @agent_ppo2.py:185][0m |           0.0071 |           0.0126 |           0.0000 |
[32m[20221213 20:46:47 @agent_ppo2.py:185][0m |          -0.0109 |           0.0122 |           0.0000 |
[32m[20221213 20:46:47 @agent_ppo2.py:185][0m |          -0.0274 |           0.0119 |           0.0000 |
[32m[20221213 20:46:47 @agent_ppo2.py:185][0m |          -0.0152 |           0.0118 |           0.0000 |
[32m[20221213 20:46:47 @agent_ppo2.py:185][0m |          -0.0447 |           0.0117 |           0.0000 |
[32m[20221213 20:46:47 @agent_ppo2.py:185][0m |          -0.0400 |           0.0120 |           0.0000 |
[32m[20221213 20:46:48 @agent_ppo2.py:185][0m |          -0.0481 |           0.0117 |           0.0000 |
[32m[20221213 20:46:48 @agent_ppo2.py:185][0m |          -0.0522 |           0.0114 |           0.0000 |
[32m[20221213 20:46:48 @agent_ppo2.py:185][0m |          -0.0531 |           0.0114 |           0.0000 |
[32m[20221213 20:46:48 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:46:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.62
[32m[20221213 20:46:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.30
[32m[20221213 20:46:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.22
[32m[20221213 20:46:48 @agent_ppo2.py:143][0m Total time:      17.65 min
[32m[20221213 20:46:48 @agent_ppo2.py:145][0m 2531328 total steps have happened
[32m[20221213 20:46:48 @agent_ppo2.py:121][0m #------------------------ Iteration 618 --------------------------#
[32m[20221213 20:46:48 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:46:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:49 @agent_ppo2.py:185][0m |           0.0382 |           0.0110 |           0.0000 |
[32m[20221213 20:46:49 @agent_ppo2.py:185][0m |           0.0198 |           0.0104 |           0.0000 |
[32m[20221213 20:46:49 @agent_ppo2.py:185][0m |           0.0077 |           0.0106 |           0.0000 |
[32m[20221213 20:46:49 @agent_ppo2.py:185][0m |          -0.0362 |           0.0105 |           0.0000 |
[32m[20221213 20:46:49 @agent_ppo2.py:185][0m |          -0.0538 |           0.0101 |           0.0000 |
[32m[20221213 20:46:49 @agent_ppo2.py:185][0m |          -0.0648 |           0.0101 |           0.0000 |
[32m[20221213 20:46:49 @agent_ppo2.py:185][0m |          -0.0629 |           0.0100 |           0.0000 |
[32m[20221213 20:46:49 @agent_ppo2.py:185][0m |          -0.0705 |           0.0098 |           0.0000 |
[32m[20221213 20:46:49 @agent_ppo2.py:185][0m |          -0.0764 |           0.0098 |           0.0000 |
[32m[20221213 20:46:50 @agent_ppo2.py:185][0m |          -0.0497 |           0.0098 |           0.0000 |
[32m[20221213 20:46:50 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:46:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.50
[32m[20221213 20:46:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.01
[32m[20221213 20:46:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.90
[32m[20221213 20:46:50 @agent_ppo2.py:143][0m Total time:      17.68 min
[32m[20221213 20:46:50 @agent_ppo2.py:145][0m 2535424 total steps have happened
[32m[20221213 20:46:50 @agent_ppo2.py:121][0m #------------------------ Iteration 619 --------------------------#
[32m[20221213 20:46:50 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:46:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:50 @agent_ppo2.py:185][0m |           0.0216 |           0.0223 |           0.0000 |
[32m[20221213 20:46:51 @agent_ppo2.py:185][0m |          -0.0029 |           0.0150 |           0.0000 |
[32m[20221213 20:46:51 @agent_ppo2.py:185][0m |          -0.0230 |           0.0141 |           0.0000 |
[32m[20221213 20:46:51 @agent_ppo2.py:185][0m |          -0.0271 |           0.0137 |           0.0000 |
[32m[20221213 20:46:51 @agent_ppo2.py:185][0m |          -0.0306 |           0.0133 |           0.0000 |
[32m[20221213 20:46:51 @agent_ppo2.py:185][0m |          -0.0303 |           0.0132 |           0.0000 |
[32m[20221213 20:46:51 @agent_ppo2.py:185][0m |          -0.0330 |           0.0129 |           0.0000 |
[32m[20221213 20:46:51 @agent_ppo2.py:185][0m |          -0.0346 |           0.0127 |           0.0000 |
[32m[20221213 20:46:51 @agent_ppo2.py:185][0m |          -0.0320 |           0.0129 |           0.0000 |
[32m[20221213 20:46:51 @agent_ppo2.py:185][0m |          -0.0358 |           0.0128 |           0.0000 |
[32m[20221213 20:46:51 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:46:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.62
[32m[20221213 20:46:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.86
[32m[20221213 20:46:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.15
[32m[20221213 20:46:52 @agent_ppo2.py:143][0m Total time:      17.71 min
[32m[20221213 20:46:52 @agent_ppo2.py:145][0m 2539520 total steps have happened
[32m[20221213 20:46:52 @agent_ppo2.py:121][0m #------------------------ Iteration 620 --------------------------#
[32m[20221213 20:46:52 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:46:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:52 @agent_ppo2.py:185][0m |           0.0485 |           0.0339 |           0.0000 |
[32m[20221213 20:46:52 @agent_ppo2.py:185][0m |           0.0087 |           0.0217 |           0.0000 |
[32m[20221213 20:46:52 @agent_ppo2.py:185][0m |          -0.0042 |           0.0200 |           0.0000 |
[32m[20221213 20:46:53 @agent_ppo2.py:185][0m |          -0.0196 |           0.0194 |           0.0000 |
[32m[20221213 20:46:53 @agent_ppo2.py:185][0m |          -0.0172 |           0.0183 |           0.0000 |
[32m[20221213 20:46:53 @agent_ppo2.py:185][0m |          -0.0129 |           0.0175 |           0.0000 |
[32m[20221213 20:46:53 @agent_ppo2.py:185][0m |          -0.0178 |           0.0174 |           0.0000 |
[32m[20221213 20:46:53 @agent_ppo2.py:185][0m |          -0.0178 |           0.0169 |           0.0000 |
[32m[20221213 20:46:53 @agent_ppo2.py:185][0m |          -0.0208 |           0.0169 |           0.0000 |
[32m[20221213 20:46:53 @agent_ppo2.py:185][0m |          -0.0217 |           0.0166 |           0.0000 |
[32m[20221213 20:46:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:46:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.28
[32m[20221213 20:46:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.82
[32m[20221213 20:46:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.43
[32m[20221213 20:46:53 @agent_ppo2.py:143][0m Total time:      17.74 min
[32m[20221213 20:46:53 @agent_ppo2.py:145][0m 2543616 total steps have happened
[32m[20221213 20:46:53 @agent_ppo2.py:121][0m #------------------------ Iteration 621 --------------------------#
[32m[20221213 20:46:54 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:46:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:54 @agent_ppo2.py:185][0m |           0.0596 |           0.0204 |           0.0000 |
[32m[20221213 20:46:54 @agent_ppo2.py:185][0m |           0.0294 |           0.0185 |           0.0000 |
[32m[20221213 20:46:54 @agent_ppo2.py:185][0m |          -0.0122 |           0.0180 |           0.0000 |
[32m[20221213 20:46:54 @agent_ppo2.py:185][0m |          -0.0420 |           0.0180 |           0.0000 |
[32m[20221213 20:46:54 @agent_ppo2.py:185][0m |          -0.0536 |           0.0170 |           0.0000 |
[32m[20221213 20:46:54 @agent_ppo2.py:185][0m |          -0.0663 |           0.0167 |           0.0000 |
[32m[20221213 20:46:55 @agent_ppo2.py:185][0m |          -0.0696 |           0.0164 |           0.0000 |
[32m[20221213 20:46:55 @agent_ppo2.py:185][0m |          -0.0747 |           0.0164 |           0.0000 |
[32m[20221213 20:46:55 @agent_ppo2.py:185][0m |          -0.0767 |           0.0162 |           0.0000 |
[32m[20221213 20:46:55 @agent_ppo2.py:185][0m |          -0.0850 |           0.0161 |           0.0000 |
[32m[20221213 20:46:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:46:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.96
[32m[20221213 20:46:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.80
[32m[20221213 20:46:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.14
[32m[20221213 20:46:55 @agent_ppo2.py:143][0m Total time:      17.77 min
[32m[20221213 20:46:55 @agent_ppo2.py:145][0m 2547712 total steps have happened
[32m[20221213 20:46:55 @agent_ppo2.py:121][0m #------------------------ Iteration 622 --------------------------#
[32m[20221213 20:46:56 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:46:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:56 @agent_ppo2.py:185][0m |           0.0650 |           0.0236 |           0.0000 |
[32m[20221213 20:46:56 @agent_ppo2.py:185][0m |           0.0283 |           0.0179 |           0.0000 |
[32m[20221213 20:46:56 @agent_ppo2.py:185][0m |          -0.0038 |           0.0169 |           0.0000 |
[32m[20221213 20:46:56 @agent_ppo2.py:185][0m |          -0.0165 |           0.0163 |           0.0000 |
[32m[20221213 20:46:56 @agent_ppo2.py:185][0m |          -0.0250 |           0.0160 |           0.0000 |
[32m[20221213 20:46:56 @agent_ppo2.py:185][0m |          -0.0294 |           0.0156 |           0.0000 |
[32m[20221213 20:46:56 @agent_ppo2.py:185][0m |          -0.0351 |           0.0157 |           0.0000 |
[32m[20221213 20:46:56 @agent_ppo2.py:185][0m |          -0.0378 |           0.0157 |           0.0000 |
[32m[20221213 20:46:56 @agent_ppo2.py:185][0m |          -0.0452 |           0.0152 |           0.0000 |
[32m[20221213 20:46:57 @agent_ppo2.py:185][0m |          -0.0395 |           0.0149 |           0.0000 |
[32m[20221213 20:46:57 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:46:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.63
[32m[20221213 20:46:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.01
[32m[20221213 20:46:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.38
[32m[20221213 20:46:57 @agent_ppo2.py:143][0m Total time:      17.80 min
[32m[20221213 20:46:57 @agent_ppo2.py:145][0m 2551808 total steps have happened
[32m[20221213 20:46:57 @agent_ppo2.py:121][0m #------------------------ Iteration 623 --------------------------#
[32m[20221213 20:46:57 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:46:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:58 @agent_ppo2.py:185][0m |           0.0299 |           0.0154 |           0.0000 |
[32m[20221213 20:46:58 @agent_ppo2.py:185][0m |          -0.0165 |           0.0144 |           0.0000 |
[32m[20221213 20:46:58 @agent_ppo2.py:185][0m |          -0.0358 |           0.0138 |           0.0000 |
[32m[20221213 20:46:58 @agent_ppo2.py:185][0m |          -0.0431 |           0.0135 |           0.0000 |
[32m[20221213 20:46:58 @agent_ppo2.py:185][0m |          -0.0524 |           0.0130 |           0.0000 |
[32m[20221213 20:46:58 @agent_ppo2.py:185][0m |          -0.0588 |           0.0129 |           0.0000 |
[32m[20221213 20:46:58 @agent_ppo2.py:185][0m |          -0.1014 |           0.0133 |           0.0000 |
[32m[20221213 20:46:58 @agent_ppo2.py:185][0m |          -0.0626 |           0.0134 |           0.0000 |
[32m[20221213 20:46:58 @agent_ppo2.py:185][0m |          -0.0648 |           0.0124 |           0.0000 |
[32m[20221213 20:46:58 @agent_ppo2.py:185][0m |          -0.1088 |           0.0138 |           0.0000 |
[32m[20221213 20:46:58 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:46:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.79
[32m[20221213 20:46:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.62
[32m[20221213 20:46:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.95
[32m[20221213 20:46:59 @agent_ppo2.py:143][0m Total time:      17.83 min
[32m[20221213 20:46:59 @agent_ppo2.py:145][0m 2555904 total steps have happened
[32m[20221213 20:46:59 @agent_ppo2.py:121][0m #------------------------ Iteration 624 --------------------------#
[32m[20221213 20:46:59 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:46:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:46:59 @agent_ppo2.py:185][0m |           0.0205 |           0.0224 |           0.0000 |
[32m[20221213 20:46:59 @agent_ppo2.py:185][0m |          -0.0020 |           0.0184 |           0.0000 |
[32m[20221213 20:46:59 @agent_ppo2.py:185][0m |          -0.0192 |           0.0173 |           0.0000 |
[32m[20221213 20:47:00 @agent_ppo2.py:185][0m |          -0.0226 |           0.0172 |           0.0000 |
[32m[20221213 20:47:00 @agent_ppo2.py:185][0m |          -0.0346 |           0.0173 |           0.0000 |
[32m[20221213 20:47:00 @agent_ppo2.py:185][0m |          -0.0316 |           0.0166 |           0.0000 |
[32m[20221213 20:47:00 @agent_ppo2.py:185][0m |          -0.0385 |           0.0161 |           0.0000 |
[32m[20221213 20:47:00 @agent_ppo2.py:185][0m |          -0.0363 |           0.0157 |           0.0000 |
[32m[20221213 20:47:00 @agent_ppo2.py:185][0m |          -0.0422 |           0.0157 |           0.0000 |
[32m[20221213 20:47:00 @agent_ppo2.py:185][0m |          -0.0454 |           0.0156 |           0.0000 |
[32m[20221213 20:47:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:47:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.53
[32m[20221213 20:47:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.82
[32m[20221213 20:47:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.79
[32m[20221213 20:47:00 @agent_ppo2.py:143][0m Total time:      17.85 min
[32m[20221213 20:47:00 @agent_ppo2.py:145][0m 2560000 total steps have happened
[32m[20221213 20:47:00 @agent_ppo2.py:121][0m #------------------------ Iteration 625 --------------------------#
[32m[20221213 20:47:01 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:47:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:01 @agent_ppo2.py:185][0m |           0.0483 |           0.0156 |           0.0000 |
[32m[20221213 20:47:01 @agent_ppo2.py:185][0m |           0.0072 |           0.0142 |           0.0000 |
[32m[20221213 20:47:01 @agent_ppo2.py:185][0m |          -0.0212 |           0.0138 |           0.0000 |
[32m[20221213 20:47:01 @agent_ppo2.py:185][0m |          -0.0505 |           0.0136 |           0.0000 |
[32m[20221213 20:47:01 @agent_ppo2.py:185][0m |          -0.0636 |           0.0132 |           0.0000 |
[32m[20221213 20:47:01 @agent_ppo2.py:185][0m |          -0.0637 |           0.0131 |           0.0000 |
[32m[20221213 20:47:01 @agent_ppo2.py:185][0m |          -0.0718 |           0.0128 |           0.0000 |
[32m[20221213 20:47:02 @agent_ppo2.py:185][0m |          -0.0597 |           0.0129 |           0.0000 |
[32m[20221213 20:47:02 @agent_ppo2.py:185][0m |          -0.0756 |           0.0132 |           0.0000 |
[32m[20221213 20:47:02 @agent_ppo2.py:185][0m |          -0.0754 |           0.0126 |           0.0000 |
[32m[20221213 20:47:02 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:47:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.50
[32m[20221213 20:47:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.09
[32m[20221213 20:47:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.29
[32m[20221213 20:47:02 @agent_ppo2.py:143][0m Total time:      17.88 min
[32m[20221213 20:47:02 @agent_ppo2.py:145][0m 2564096 total steps have happened
[32m[20221213 20:47:02 @agent_ppo2.py:121][0m #------------------------ Iteration 626 --------------------------#
[32m[20221213 20:47:03 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:47:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:03 @agent_ppo2.py:185][0m |           0.0319 |           0.0150 |           0.0000 |
[32m[20221213 20:47:03 @agent_ppo2.py:185][0m |          -0.0023 |           0.0133 |           0.0000 |
[32m[20221213 20:47:03 @agent_ppo2.py:185][0m |          -0.0192 |           0.0128 |           0.0000 |
[32m[20221213 20:47:03 @agent_ppo2.py:185][0m |          -0.0341 |           0.0125 |           0.0000 |
[32m[20221213 20:47:03 @agent_ppo2.py:185][0m |          -0.0364 |           0.0124 |           0.0000 |
[32m[20221213 20:47:03 @agent_ppo2.py:185][0m |          -0.0533 |           0.0122 |           0.0000 |
[32m[20221213 20:47:03 @agent_ppo2.py:185][0m |          -0.0511 |           0.0122 |           0.0000 |
[32m[20221213 20:47:03 @agent_ppo2.py:185][0m |          -0.0567 |           0.0119 |           0.0000 |
[32m[20221213 20:47:03 @agent_ppo2.py:185][0m |          -0.0546 |           0.0119 |           0.0000 |
[32m[20221213 20:47:03 @agent_ppo2.py:185][0m |          -0.0572 |           0.0118 |           0.0000 |
[32m[20221213 20:47:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:47:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.96
[32m[20221213 20:47:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.35
[32m[20221213 20:47:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.32
[32m[20221213 20:47:04 @agent_ppo2.py:143][0m Total time:      17.91 min
[32m[20221213 20:47:04 @agent_ppo2.py:145][0m 2568192 total steps have happened
[32m[20221213 20:47:04 @agent_ppo2.py:121][0m #------------------------ Iteration 627 --------------------------#
[32m[20221213 20:47:04 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:47:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:04 @agent_ppo2.py:185][0m |           0.0422 |           0.0183 |           0.0000 |
[32m[20221213 20:47:04 @agent_ppo2.py:185][0m |           0.0002 |           0.0161 |           0.0000 |
[32m[20221213 20:47:05 @agent_ppo2.py:185][0m |          -0.0372 |           0.0154 |           0.0000 |
[32m[20221213 20:47:05 @agent_ppo2.py:185][0m |          -0.0297 |           0.0154 |           0.0000 |
[32m[20221213 20:47:05 @agent_ppo2.py:185][0m |          -0.0327 |           0.0147 |           0.0000 |
[32m[20221213 20:47:05 @agent_ppo2.py:185][0m |          -0.0349 |           0.0145 |           0.0000 |
[32m[20221213 20:47:05 @agent_ppo2.py:185][0m |          -0.0348 |           0.0142 |           0.0000 |
[32m[20221213 20:47:05 @agent_ppo2.py:185][0m |          -0.0363 |           0.0140 |           0.0000 |
[32m[20221213 20:47:05 @agent_ppo2.py:185][0m |          -0.0413 |           0.0139 |           0.0000 |
[32m[20221213 20:47:05 @agent_ppo2.py:185][0m |          -0.0462 |           0.0141 |           0.0000 |
[32m[20221213 20:47:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:47:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.87
[32m[20221213 20:47:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.42
[32m[20221213 20:47:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.16
[32m[20221213 20:47:06 @agent_ppo2.py:143][0m Total time:      17.94 min
[32m[20221213 20:47:06 @agent_ppo2.py:145][0m 2572288 total steps have happened
[32m[20221213 20:47:06 @agent_ppo2.py:121][0m #------------------------ Iteration 628 --------------------------#
[32m[20221213 20:47:06 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:47:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:06 @agent_ppo2.py:185][0m |           0.0637 |           0.0282 |           0.0000 |
[32m[20221213 20:47:06 @agent_ppo2.py:185][0m |           0.0466 |           0.0109 |           0.0000 |
[32m[20221213 20:47:06 @agent_ppo2.py:185][0m |           0.0206 |           0.0104 |           0.0000 |
[32m[20221213 20:47:06 @agent_ppo2.py:185][0m |           0.0091 |           0.0102 |           0.0000 |
[32m[20221213 20:47:06 @agent_ppo2.py:185][0m |           0.0259 |           0.0102 |           0.0000 |
[32m[20221213 20:47:06 @agent_ppo2.py:185][0m |           0.0036 |           0.0100 |           0.0000 |
[32m[20221213 20:47:07 @agent_ppo2.py:185][0m |          -0.0138 |           0.0099 |           0.0000 |
[32m[20221213 20:47:07 @agent_ppo2.py:185][0m |          -0.0166 |           0.0098 |           0.0000 |
[32m[20221213 20:47:07 @agent_ppo2.py:185][0m |          -0.0164 |           0.0097 |           0.0000 |
[32m[20221213 20:47:07 @agent_ppo2.py:185][0m |          -0.0229 |           0.0096 |           0.0000 |
[32m[20221213 20:47:07 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:47:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.59
[32m[20221213 20:47:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.95
[32m[20221213 20:47:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.45
[32m[20221213 20:47:07 @agent_ppo2.py:143][0m Total time:      17.97 min
[32m[20221213 20:47:07 @agent_ppo2.py:145][0m 2576384 total steps have happened
[32m[20221213 20:47:07 @agent_ppo2.py:121][0m #------------------------ Iteration 629 --------------------------#
[32m[20221213 20:47:08 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:47:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:08 @agent_ppo2.py:185][0m |           0.0380 |           0.0092 |           0.0000 |
[32m[20221213 20:47:08 @agent_ppo2.py:185][0m |           0.0021 |           0.0075 |           0.0000 |
[32m[20221213 20:47:08 @agent_ppo2.py:185][0m |          -0.0229 |           0.0073 |           0.0000 |
[32m[20221213 20:47:08 @agent_ppo2.py:185][0m |          -0.0239 |           0.0071 |           0.0000 |
[32m[20221213 20:47:08 @agent_ppo2.py:185][0m |          -0.0244 |           0.0070 |           0.0000 |
[32m[20221213 20:47:08 @agent_ppo2.py:185][0m |          -0.0375 |           0.0070 |           0.0000 |
[32m[20221213 20:47:08 @agent_ppo2.py:185][0m |          -0.0415 |           0.0069 |           0.0000 |
[32m[20221213 20:47:08 @agent_ppo2.py:185][0m |          -0.0229 |           0.0068 |           0.0000 |
[32m[20221213 20:47:08 @agent_ppo2.py:185][0m |          -0.0507 |           0.0067 |           0.0000 |
[32m[20221213 20:47:08 @agent_ppo2.py:185][0m |          -0.0529 |           0.0066 |           0.0000 |
[32m[20221213 20:47:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:47:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.19
[32m[20221213 20:47:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.36
[32m[20221213 20:47:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.49
[32m[20221213 20:47:09 @agent_ppo2.py:143][0m Total time:      17.99 min
[32m[20221213 20:47:09 @agent_ppo2.py:145][0m 2580480 total steps have happened
[32m[20221213 20:47:09 @agent_ppo2.py:121][0m #------------------------ Iteration 630 --------------------------#
[32m[20221213 20:47:09 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:47:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:09 @agent_ppo2.py:185][0m |           0.0337 |           0.0090 |           0.0000 |
[32m[20221213 20:47:10 @agent_ppo2.py:185][0m |           0.0010 |           0.0078 |           0.0000 |
[32m[20221213 20:47:10 @agent_ppo2.py:185][0m |          -0.0165 |           0.0074 |           0.0000 |
[32m[20221213 20:47:10 @agent_ppo2.py:185][0m |          -0.0543 |           0.0072 |           0.0000 |
[32m[20221213 20:47:10 @agent_ppo2.py:185][0m |          -0.0248 |           0.0071 |           0.0000 |
[32m[20221213 20:47:10 @agent_ppo2.py:185][0m |          -0.0299 |           0.0069 |           0.0000 |
[32m[20221213 20:47:10 @agent_ppo2.py:185][0m |          -0.0304 |           0.0069 |           0.0000 |
[32m[20221213 20:47:10 @agent_ppo2.py:185][0m |          -0.0352 |           0.0068 |           0.0000 |
[32m[20221213 20:47:10 @agent_ppo2.py:185][0m |          -0.0365 |           0.0067 |           0.0000 |
[32m[20221213 20:47:10 @agent_ppo2.py:185][0m |          -0.0374 |           0.0066 |           0.0000 |
[32m[20221213 20:47:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:47:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.09
[32m[20221213 20:47:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.37
[32m[20221213 20:47:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.25
[32m[20221213 20:47:11 @agent_ppo2.py:143][0m Total time:      18.02 min
[32m[20221213 20:47:11 @agent_ppo2.py:145][0m 2584576 total steps have happened
[32m[20221213 20:47:11 @agent_ppo2.py:121][0m #------------------------ Iteration 631 --------------------------#
[32m[20221213 20:47:11 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:47:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:11 @agent_ppo2.py:185][0m |           0.0326 |           0.1060 |           0.0000 |
[32m[20221213 20:47:11 @agent_ppo2.py:185][0m |           0.0038 |           0.0339 |           0.0000 |
[32m[20221213 20:47:11 @agent_ppo2.py:185][0m |          -0.0121 |           0.0261 |           0.0000 |
[32m[20221213 20:47:11 @agent_ppo2.py:185][0m |          -0.0226 |           0.0213 |           0.0000 |
[32m[20221213 20:47:12 @agent_ppo2.py:185][0m |          -0.0264 |           0.0205 |           0.0000 |
[32m[20221213 20:47:12 @agent_ppo2.py:185][0m |          -0.0288 |           0.0197 |           0.0000 |
[32m[20221213 20:47:12 @agent_ppo2.py:185][0m |          -0.0260 |           0.0193 |           0.0000 |
[32m[20221213 20:47:12 @agent_ppo2.py:185][0m |          -0.0293 |           0.0181 |           0.0000 |
[32m[20221213 20:47:12 @agent_ppo2.py:185][0m |          -0.0346 |           0.0184 |           0.0000 |
[32m[20221213 20:47:12 @agent_ppo2.py:185][0m |          -0.0316 |           0.0170 |           0.0000 |
[32m[20221213 20:47:12 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:47:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.69
[32m[20221213 20:47:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.90
[32m[20221213 20:47:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.62
[32m[20221213 20:47:12 @agent_ppo2.py:143][0m Total time:      18.05 min
[32m[20221213 20:47:12 @agent_ppo2.py:145][0m 2588672 total steps have happened
[32m[20221213 20:47:12 @agent_ppo2.py:121][0m #------------------------ Iteration 632 --------------------------#
[32m[20221213 20:47:13 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:47:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:13 @agent_ppo2.py:185][0m |           0.0243 |           0.0322 |           0.0000 |
[32m[20221213 20:47:13 @agent_ppo2.py:185][0m |          -0.0124 |           0.0257 |           0.0000 |
[32m[20221213 20:47:13 @agent_ppo2.py:185][0m |          -0.0319 |           0.0248 |           0.0000 |
[32m[20221213 20:47:13 @agent_ppo2.py:185][0m |          -0.0479 |           0.0227 |           0.0000 |
[32m[20221213 20:47:13 @agent_ppo2.py:185][0m |          -0.0538 |           0.0228 |           0.0000 |
[32m[20221213 20:47:13 @agent_ppo2.py:185][0m |          -0.0619 |           0.0226 |           0.0000 |
[32m[20221213 20:47:13 @agent_ppo2.py:185][0m |          -0.0667 |           0.0218 |           0.0000 |
[32m[20221213 20:47:14 @agent_ppo2.py:185][0m |          -0.0613 |           0.0209 |           0.0000 |
[32m[20221213 20:47:14 @agent_ppo2.py:185][0m |          -0.0661 |           0.0231 |           0.0000 |
[32m[20221213 20:47:14 @agent_ppo2.py:185][0m |          -0.0628 |           0.0208 |           0.0000 |
[32m[20221213 20:47:14 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:47:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.62
[32m[20221213 20:47:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.55
[32m[20221213 20:47:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.50
[32m[20221213 20:47:14 @agent_ppo2.py:143][0m Total time:      18.08 min
[32m[20221213 20:47:14 @agent_ppo2.py:145][0m 2592768 total steps have happened
[32m[20221213 20:47:14 @agent_ppo2.py:121][0m #------------------------ Iteration 633 --------------------------#
[32m[20221213 20:47:14 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:47:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:15 @agent_ppo2.py:185][0m |           0.0308 |           0.0291 |           0.0000 |
[32m[20221213 20:47:15 @agent_ppo2.py:185][0m |           0.0034 |           0.0187 |           0.0000 |
[32m[20221213 20:47:15 @agent_ppo2.py:185][0m |          -0.0162 |           0.0161 |           0.0000 |
[32m[20221213 20:47:15 @agent_ppo2.py:185][0m |          -0.0289 |           0.0148 |           0.0000 |
[32m[20221213 20:47:15 @agent_ppo2.py:185][0m |          -0.0358 |           0.0143 |           0.0000 |
[32m[20221213 20:47:15 @agent_ppo2.py:185][0m |          -0.0451 |           0.0140 |           0.0000 |
[32m[20221213 20:47:15 @agent_ppo2.py:185][0m |          -0.0384 |           0.0136 |           0.0000 |
[32m[20221213 20:47:15 @agent_ppo2.py:185][0m |          -0.0497 |           0.0134 |           0.0000 |
[32m[20221213 20:47:15 @agent_ppo2.py:185][0m |          -0.0536 |           0.0132 |           0.0000 |
[32m[20221213 20:47:15 @agent_ppo2.py:185][0m |          -0.0535 |           0.0130 |           0.0000 |
[32m[20221213 20:47:15 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:47:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.42
[32m[20221213 20:47:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.92
[32m[20221213 20:47:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.42
[32m[20221213 20:47:16 @agent_ppo2.py:143][0m Total time:      18.11 min
[32m[20221213 20:47:16 @agent_ppo2.py:145][0m 2596864 total steps have happened
[32m[20221213 20:47:16 @agent_ppo2.py:121][0m #------------------------ Iteration 634 --------------------------#
[32m[20221213 20:47:16 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:47:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:16 @agent_ppo2.py:185][0m |           0.0435 |           0.0196 |           0.0000 |
[32m[20221213 20:47:17 @agent_ppo2.py:185][0m |          -0.0071 |           0.0091 |           0.0000 |
[32m[20221213 20:47:17 @agent_ppo2.py:185][0m |          -0.0199 |           0.0086 |           0.0000 |
[32m[20221213 20:47:17 @agent_ppo2.py:185][0m |          -0.0272 |           0.0084 |           0.0000 |
[32m[20221213 20:47:17 @agent_ppo2.py:185][0m |          -0.0340 |           0.0083 |           0.0000 |
[32m[20221213 20:47:17 @agent_ppo2.py:185][0m |          -0.0339 |           0.0082 |           0.0000 |
[32m[20221213 20:47:17 @agent_ppo2.py:185][0m |          -0.0116 |           0.0083 |           0.0000 |
[32m[20221213 20:47:17 @agent_ppo2.py:185][0m |          -0.0382 |           0.0081 |           0.0000 |
[32m[20221213 20:47:17 @agent_ppo2.py:185][0m |          -0.0377 |           0.0080 |           0.0000 |
[32m[20221213 20:47:17 @agent_ppo2.py:185][0m |          -0.0432 |           0.0079 |           0.0000 |
[32m[20221213 20:47:17 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:47:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.64
[32m[20221213 20:47:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.76
[32m[20221213 20:47:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.87
[32m[20221213 20:47:18 @agent_ppo2.py:143][0m Total time:      18.14 min
[32m[20221213 20:47:18 @agent_ppo2.py:145][0m 2600960 total steps have happened
[32m[20221213 20:47:18 @agent_ppo2.py:121][0m #------------------------ Iteration 635 --------------------------#
[32m[20221213 20:47:18 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:47:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:18 @agent_ppo2.py:185][0m |           0.0349 |           0.0845 |           0.0000 |
[32m[20221213 20:47:18 @agent_ppo2.py:185][0m |           0.0065 |           0.0276 |           0.0000 |
[32m[20221213 20:47:18 @agent_ppo2.py:185][0m |          -0.0433 |           0.0215 |           0.0000 |
[32m[20221213 20:47:18 @agent_ppo2.py:185][0m |           0.0243 |           0.0194 |           0.0000 |
[32m[20221213 20:47:19 @agent_ppo2.py:185][0m |           0.0017 |           0.0177 |           0.0000 |
[32m[20221213 20:47:19 @agent_ppo2.py:185][0m |          -0.0151 |           0.0178 |           0.0000 |
[32m[20221213 20:47:19 @agent_ppo2.py:185][0m |          -0.0182 |           0.0167 |           0.0000 |
[32m[20221213 20:47:19 @agent_ppo2.py:185][0m |          -0.0227 |           0.0167 |           0.0000 |
[32m[20221213 20:47:19 @agent_ppo2.py:185][0m |          -0.0237 |           0.0177 |           0.0000 |
[32m[20221213 20:47:19 @agent_ppo2.py:185][0m |          -0.0278 |           0.0158 |           0.0000 |
[32m[20221213 20:47:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:47:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.18
[32m[20221213 20:47:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.42
[32m[20221213 20:47:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.53
[32m[20221213 20:47:19 @agent_ppo2.py:143][0m Total time:      18.17 min
[32m[20221213 20:47:19 @agent_ppo2.py:145][0m 2605056 total steps have happened
[32m[20221213 20:47:19 @agent_ppo2.py:121][0m #------------------------ Iteration 636 --------------------------#
[32m[20221213 20:47:20 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:47:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:20 @agent_ppo2.py:185][0m |           0.0655 |           0.0190 |           0.0000 |
[32m[20221213 20:47:20 @agent_ppo2.py:185][0m |           0.0338 |           0.0166 |           0.0000 |
[32m[20221213 20:47:20 @agent_ppo2.py:185][0m |          -0.0088 |           0.0155 |           0.0000 |
[32m[20221213 20:47:20 @agent_ppo2.py:185][0m |          -0.0146 |           0.0155 |           0.0000 |
[32m[20221213 20:47:20 @agent_ppo2.py:185][0m |          -0.0550 |           0.0157 |           0.0000 |
[32m[20221213 20:47:20 @agent_ppo2.py:185][0m |          -0.0434 |           0.0143 |           0.0000 |
[32m[20221213 20:47:20 @agent_ppo2.py:185][0m |          -0.0704 |           0.0142 |           0.0000 |
[32m[20221213 20:47:21 @agent_ppo2.py:185][0m |          -0.0835 |           0.0139 |           0.0000 |
[32m[20221213 20:47:21 @agent_ppo2.py:185][0m |          -0.0825 |           0.0136 |           0.0000 |
[32m[20221213 20:47:21 @agent_ppo2.py:185][0m |          -0.0882 |           0.0134 |           0.0000 |
[32m[20221213 20:47:21 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:47:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.47
[32m[20221213 20:47:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.71
[32m[20221213 20:47:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.99
[32m[20221213 20:47:21 @agent_ppo2.py:143][0m Total time:      18.20 min
[32m[20221213 20:47:21 @agent_ppo2.py:145][0m 2609152 total steps have happened
[32m[20221213 20:47:21 @agent_ppo2.py:121][0m #------------------------ Iteration 637 --------------------------#
[32m[20221213 20:47:21 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:47:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:22 @agent_ppo2.py:185][0m |           0.0270 |           0.0141 |           0.0000 |
[32m[20221213 20:47:22 @agent_ppo2.py:185][0m |          -0.0110 |           0.0108 |           0.0000 |
[32m[20221213 20:47:22 @agent_ppo2.py:185][0m |          -0.0331 |           0.0104 |           0.0000 |
[32m[20221213 20:47:22 @agent_ppo2.py:185][0m |          -0.0466 |           0.0103 |           0.0000 |
[32m[20221213 20:47:22 @agent_ppo2.py:185][0m |          -0.0425 |           0.0102 |           0.0000 |
[32m[20221213 20:47:22 @agent_ppo2.py:185][0m |          -0.0557 |           0.0101 |           0.0000 |
[32m[20221213 20:47:22 @agent_ppo2.py:185][0m |          -0.0264 |           0.0102 |           0.0000 |
[32m[20221213 20:47:22 @agent_ppo2.py:185][0m |          -0.0651 |           0.0101 |           0.0000 |
[32m[20221213 20:47:22 @agent_ppo2.py:185][0m |          -0.0645 |           0.0100 |           0.0000 |
[32m[20221213 20:47:22 @agent_ppo2.py:185][0m |          -0.0616 |           0.0099 |           0.0000 |
[32m[20221213 20:47:22 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:47:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.41
[32m[20221213 20:47:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.79
[32m[20221213 20:47:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.05
[32m[20221213 20:47:23 @agent_ppo2.py:143][0m Total time:      18.23 min
[32m[20221213 20:47:23 @agent_ppo2.py:145][0m 2613248 total steps have happened
[32m[20221213 20:47:23 @agent_ppo2.py:121][0m #------------------------ Iteration 638 --------------------------#
[32m[20221213 20:47:23 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:47:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:23 @agent_ppo2.py:185][0m |           0.0377 |           0.0199 |           0.0000 |
[32m[20221213 20:47:23 @agent_ppo2.py:185][0m |           0.0043 |           0.0149 |           0.0000 |
[32m[20221213 20:47:23 @agent_ppo2.py:185][0m |          -0.0027 |           0.0146 |           0.0000 |
[32m[20221213 20:47:24 @agent_ppo2.py:185][0m |          -0.0132 |           0.0143 |           0.0000 |
[32m[20221213 20:47:24 @agent_ppo2.py:185][0m |          -0.0166 |           0.0143 |           0.0000 |
[32m[20221213 20:47:24 @agent_ppo2.py:185][0m |          -0.0256 |           0.0134 |           0.0000 |
[32m[20221213 20:47:24 @agent_ppo2.py:185][0m |          -0.0814 |           0.0157 |           0.0000 |
[32m[20221213 20:47:24 @agent_ppo2.py:185][0m |          -0.0323 |           0.0177 |           0.0000 |
[32m[20221213 20:47:24 @agent_ppo2.py:185][0m |          -0.0305 |           0.0130 |           0.0000 |
[32m[20221213 20:47:24 @agent_ppo2.py:185][0m |          -0.0361 |           0.0125 |           0.0000 |
[32m[20221213 20:47:24 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:47:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.28
[32m[20221213 20:47:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.30
[32m[20221213 20:47:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.39
[32m[20221213 20:47:24 @agent_ppo2.py:143][0m Total time:      18.25 min
[32m[20221213 20:47:24 @agent_ppo2.py:145][0m 2617344 total steps have happened
[32m[20221213 20:47:24 @agent_ppo2.py:121][0m #------------------------ Iteration 639 --------------------------#
[32m[20221213 20:47:25 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:47:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:25 @agent_ppo2.py:185][0m |           0.0440 |           0.0195 |           0.0000 |
[32m[20221213 20:47:25 @agent_ppo2.py:185][0m |          -0.0032 |           0.0186 |           0.0000 |
[32m[20221213 20:47:25 @agent_ppo2.py:185][0m |          -0.0240 |           0.0167 |           0.0000 |
[32m[20221213 20:47:25 @agent_ppo2.py:185][0m |          -0.0344 |           0.0160 |           0.0000 |
[32m[20221213 20:47:25 @agent_ppo2.py:185][0m |          -0.0730 |           0.0160 |           0.0000 |
[32m[20221213 20:47:25 @agent_ppo2.py:185][0m |          -0.0448 |           0.0156 |           0.0000 |
[32m[20221213 20:47:25 @agent_ppo2.py:185][0m |          -0.0433 |           0.0153 |           0.0000 |
[32m[20221213 20:47:26 @agent_ppo2.py:185][0m |          -0.0409 |           0.0153 |           0.0000 |
[32m[20221213 20:47:26 @agent_ppo2.py:185][0m |          -0.1077 |           0.0166 |           0.0000 |
[32m[20221213 20:47:26 @agent_ppo2.py:185][0m |          -0.0402 |           0.0177 |           0.0000 |
[32m[20221213 20:47:26 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:47:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.35
[32m[20221213 20:47:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.18
[32m[20221213 20:47:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.61
[32m[20221213 20:47:26 @agent_ppo2.py:143][0m Total time:      18.28 min
[32m[20221213 20:47:26 @agent_ppo2.py:145][0m 2621440 total steps have happened
[32m[20221213 20:47:26 @agent_ppo2.py:121][0m #------------------------ Iteration 640 --------------------------#
[32m[20221213 20:47:26 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:47:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:27 @agent_ppo2.py:185][0m |           0.0242 |           0.0264 |           0.0000 |
[32m[20221213 20:47:27 @agent_ppo2.py:185][0m |           0.0062 |           0.0186 |           0.0000 |
[32m[20221213 20:47:27 @agent_ppo2.py:185][0m |          -0.0088 |           0.0176 |           0.0000 |
[32m[20221213 20:47:27 @agent_ppo2.py:185][0m |          -0.0348 |           0.0177 |           0.0000 |
[32m[20221213 20:47:27 @agent_ppo2.py:185][0m |          -0.0245 |           0.0169 |           0.0000 |
[32m[20221213 20:47:27 @agent_ppo2.py:185][0m |          -0.0334 |           0.0160 |           0.0000 |
[32m[20221213 20:47:27 @agent_ppo2.py:185][0m |          -0.0373 |           0.0158 |           0.0000 |
[32m[20221213 20:47:27 @agent_ppo2.py:185][0m |          -0.0440 |           0.0154 |           0.0000 |
[32m[20221213 20:47:27 @agent_ppo2.py:185][0m |          -0.0470 |           0.0153 |           0.0000 |
[32m[20221213 20:47:27 @agent_ppo2.py:185][0m |          -0.0536 |           0.0151 |           0.0000 |
[32m[20221213 20:47:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:47:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.97
[32m[20221213 20:47:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.83
[32m[20221213 20:47:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.51
[32m[20221213 20:47:28 @agent_ppo2.py:143][0m Total time:      18.31 min
[32m[20221213 20:47:28 @agent_ppo2.py:145][0m 2625536 total steps have happened
[32m[20221213 20:47:28 @agent_ppo2.py:121][0m #------------------------ Iteration 641 --------------------------#
[32m[20221213 20:47:28 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:47:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:28 @agent_ppo2.py:185][0m |           0.0326 |           0.0726 |           0.0000 |
[32m[20221213 20:47:28 @agent_ppo2.py:185][0m |           0.0051 |           0.0282 |           0.0000 |
[32m[20221213 20:47:29 @agent_ppo2.py:185][0m |          -0.0140 |           0.0214 |           0.0000 |
[32m[20221213 20:47:29 @agent_ppo2.py:185][0m |          -0.0496 |           0.0203 |           0.0000 |
[32m[20221213 20:47:29 @agent_ppo2.py:185][0m |          -0.0255 |           0.0203 |           0.0000 |
[32m[20221213 20:47:29 @agent_ppo2.py:185][0m |          -0.0281 |           0.0191 |           0.0000 |
[32m[20221213 20:47:29 @agent_ppo2.py:185][0m |          -0.0307 |           0.0191 |           0.0000 |
[32m[20221213 20:47:29 @agent_ppo2.py:185][0m |          -0.0291 |           0.0185 |           0.0000 |
[32m[20221213 20:47:29 @agent_ppo2.py:185][0m |          -0.0296 |           0.0180 |           0.0000 |
[32m[20221213 20:47:29 @agent_ppo2.py:185][0m |          -0.0322 |           0.0180 |           0.0000 |
[32m[20221213 20:47:29 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:47:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.86
[32m[20221213 20:47:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.10
[32m[20221213 20:47:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.39
[32m[20221213 20:47:30 @agent_ppo2.py:143][0m Total time:      18.34 min
[32m[20221213 20:47:30 @agent_ppo2.py:145][0m 2629632 total steps have happened
[32m[20221213 20:47:30 @agent_ppo2.py:121][0m #------------------------ Iteration 642 --------------------------#
[32m[20221213 20:47:30 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:47:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:30 @agent_ppo2.py:185][0m |           0.0139 |           0.0386 |           0.0000 |
[32m[20221213 20:47:30 @agent_ppo2.py:185][0m |           0.0249 |           0.0114 |           0.0000 |
[32m[20221213 20:47:30 @agent_ppo2.py:185][0m |          -0.0071 |           0.0107 |           0.0000 |
[32m[20221213 20:47:30 @agent_ppo2.py:185][0m |          -0.0124 |           0.0105 |           0.0000 |
[32m[20221213 20:47:31 @agent_ppo2.py:185][0m |          -0.0188 |           0.0102 |           0.0000 |
[32m[20221213 20:47:31 @agent_ppo2.py:185][0m |           0.0010 |           0.0103 |           0.0000 |
[32m[20221213 20:47:31 @agent_ppo2.py:185][0m |          -0.0342 |           0.0100 |           0.0000 |
[32m[20221213 20:47:31 @agent_ppo2.py:185][0m |          -0.0340 |           0.0098 |           0.0000 |
[32m[20221213 20:47:31 @agent_ppo2.py:185][0m |          -0.0371 |           0.0098 |           0.0000 |
[32m[20221213 20:47:31 @agent_ppo2.py:185][0m |          -0.0425 |           0.0097 |           0.0000 |
[32m[20221213 20:47:31 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 20:47:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.47
[32m[20221213 20:47:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.67
[32m[20221213 20:47:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.27
[32m[20221213 20:47:32 @agent_ppo2.py:143][0m Total time:      18.37 min
[32m[20221213 20:47:32 @agent_ppo2.py:145][0m 2633728 total steps have happened
[32m[20221213 20:47:32 @agent_ppo2.py:121][0m #------------------------ Iteration 643 --------------------------#
[32m[20221213 20:47:32 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:47:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:32 @agent_ppo2.py:185][0m |           0.0380 |           0.0444 |           0.0000 |
[32m[20221213 20:47:32 @agent_ppo2.py:185][0m |           0.0072 |           0.0251 |           0.0000 |
[32m[20221213 20:47:32 @agent_ppo2.py:185][0m |          -0.0076 |           0.0222 |           0.0000 |
[32m[20221213 20:47:32 @agent_ppo2.py:185][0m |          -0.0191 |           0.0207 |           0.0000 |
[32m[20221213 20:47:32 @agent_ppo2.py:185][0m |          -0.0186 |           0.0193 |           0.0000 |
[32m[20221213 20:47:32 @agent_ppo2.py:185][0m |          -0.0216 |           0.0190 |           0.0000 |
[32m[20221213 20:47:33 @agent_ppo2.py:185][0m |          -0.0252 |           0.0185 |           0.0000 |
[32m[20221213 20:47:33 @agent_ppo2.py:185][0m |          -0.0250 |           0.0181 |           0.0000 |
[32m[20221213 20:47:33 @agent_ppo2.py:185][0m |          -0.0267 |           0.0177 |           0.0000 |
[32m[20221213 20:47:33 @agent_ppo2.py:185][0m |          -0.0321 |           0.0172 |           0.0000 |
[32m[20221213 20:47:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:47:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.32
[32m[20221213 20:47:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.97
[32m[20221213 20:47:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.23
[32m[20221213 20:47:33 @agent_ppo2.py:143][0m Total time:      18.40 min
[32m[20221213 20:47:33 @agent_ppo2.py:145][0m 2637824 total steps have happened
[32m[20221213 20:47:33 @agent_ppo2.py:121][0m #------------------------ Iteration 644 --------------------------#
[32m[20221213 20:47:34 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:47:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:34 @agent_ppo2.py:185][0m |           0.0331 |           0.0173 |           0.0000 |
[32m[20221213 20:47:34 @agent_ppo2.py:185][0m |           0.0101 |           0.0110 |           0.0000 |
[32m[20221213 20:47:34 @agent_ppo2.py:185][0m |          -0.0025 |           0.0105 |           0.0000 |
[32m[20221213 20:47:34 @agent_ppo2.py:185][0m |          -0.0214 |           0.0103 |           0.0000 |
[32m[20221213 20:47:34 @agent_ppo2.py:185][0m |          -0.0371 |           0.0101 |           0.0000 |
[32m[20221213 20:47:34 @agent_ppo2.py:185][0m |          -0.0457 |           0.0100 |           0.0000 |
[32m[20221213 20:47:34 @agent_ppo2.py:185][0m |          -0.0457 |           0.0099 |           0.0000 |
[32m[20221213 20:47:34 @agent_ppo2.py:185][0m |          -0.0531 |           0.0098 |           0.0000 |
[32m[20221213 20:47:35 @agent_ppo2.py:185][0m |          -0.0607 |           0.0097 |           0.0000 |
[32m[20221213 20:47:35 @agent_ppo2.py:185][0m |          -0.0636 |           0.0096 |           0.0000 |
[32m[20221213 20:47:35 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:47:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.90
[32m[20221213 20:47:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.02
[32m[20221213 20:47:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.91
[32m[20221213 20:47:35 @agent_ppo2.py:143][0m Total time:      18.43 min
[32m[20221213 20:47:35 @agent_ppo2.py:145][0m 2641920 total steps have happened
[32m[20221213 20:47:35 @agent_ppo2.py:121][0m #------------------------ Iteration 645 --------------------------#
[32m[20221213 20:47:35 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:47:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:36 @agent_ppo2.py:185][0m |           0.0507 |           0.0099 |           0.0000 |
[32m[20221213 20:47:36 @agent_ppo2.py:185][0m |          -0.0095 |           0.0097 |           0.0000 |
[32m[20221213 20:47:36 @agent_ppo2.py:185][0m |          -0.0317 |           0.0096 |           0.0000 |
[32m[20221213 20:47:36 @agent_ppo2.py:185][0m |          -0.0498 |           0.0095 |           0.0000 |
[32m[20221213 20:47:36 @agent_ppo2.py:185][0m |          -0.0607 |           0.0094 |           0.0000 |
[32m[20221213 20:47:36 @agent_ppo2.py:185][0m |          -0.0732 |           0.0093 |           0.0000 |
[32m[20221213 20:47:36 @agent_ppo2.py:185][0m |          -0.0567 |           0.0094 |           0.0000 |
[32m[20221213 20:47:36 @agent_ppo2.py:185][0m |          -0.0879 |           0.0093 |           0.0000 |
[32m[20221213 20:47:36 @agent_ppo2.py:185][0m |          -0.0884 |           0.0091 |           0.0000 |
[32m[20221213 20:47:36 @agent_ppo2.py:185][0m |          -0.0888 |           0.0091 |           0.0000 |
[32m[20221213 20:47:36 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:47:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.24
[32m[20221213 20:47:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.45
[32m[20221213 20:47:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.11
[32m[20221213 20:47:37 @agent_ppo2.py:143][0m Total time:      18.46 min
[32m[20221213 20:47:37 @agent_ppo2.py:145][0m 2646016 total steps have happened
[32m[20221213 20:47:37 @agent_ppo2.py:121][0m #------------------------ Iteration 646 --------------------------#
[32m[20221213 20:47:37 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:47:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:37 @agent_ppo2.py:185][0m |           0.0389 |           0.0313 |           0.0000 |
[32m[20221213 20:47:37 @agent_ppo2.py:185][0m |           0.0040 |           0.0163 |           0.0000 |
[32m[20221213 20:47:38 @agent_ppo2.py:185][0m |          -0.0038 |           0.0143 |           0.0000 |
[32m[20221213 20:47:38 @agent_ppo2.py:185][0m |          -0.0109 |           0.0139 |           0.0000 |
[32m[20221213 20:47:38 @agent_ppo2.py:185][0m |          -0.0179 |           0.0135 |           0.0000 |
[32m[20221213 20:47:38 @agent_ppo2.py:185][0m |          -0.0453 |           0.0144 |           0.0000 |
[32m[20221213 20:47:38 @agent_ppo2.py:185][0m |          -0.0207 |           0.0156 |           0.0000 |
[32m[20221213 20:47:38 @agent_ppo2.py:185][0m |          -0.0260 |           0.0141 |           0.0000 |
[32m[20221213 20:47:38 @agent_ppo2.py:185][0m |          -0.0276 |           0.0134 |           0.0000 |
[32m[20221213 20:47:38 @agent_ppo2.py:185][0m |          -0.0317 |           0.0142 |           0.0000 |
[32m[20221213 20:47:38 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:47:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.01
[32m[20221213 20:47:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.46
[32m[20221213 20:47:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.89
[32m[20221213 20:47:39 @agent_ppo2.py:143][0m Total time:      18.49 min
[32m[20221213 20:47:39 @agent_ppo2.py:145][0m 2650112 total steps have happened
[32m[20221213 20:47:39 @agent_ppo2.py:121][0m #------------------------ Iteration 647 --------------------------#
[32m[20221213 20:47:39 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:47:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:39 @agent_ppo2.py:185][0m |           0.0495 |           0.0163 |           0.0000 |
[32m[20221213 20:47:39 @agent_ppo2.py:185][0m |           0.0521 |           0.0147 |           0.0000 |
[32m[20221213 20:47:39 @agent_ppo2.py:185][0m |           0.0006 |           0.0140 |           0.0000 |
[32m[20221213 20:47:39 @agent_ppo2.py:185][0m |          -0.0077 |           0.0137 |           0.0000 |
[32m[20221213 20:47:39 @agent_ppo2.py:185][0m |          -0.0289 |           0.0131 |           0.0000 |
[32m[20221213 20:47:40 @agent_ppo2.py:185][0m |          -0.0356 |           0.0130 |           0.0000 |
[32m[20221213 20:47:40 @agent_ppo2.py:185][0m |          -0.0484 |           0.0127 |           0.0000 |
[32m[20221213 20:47:40 @agent_ppo2.py:185][0m |          -0.0526 |           0.0126 |           0.0000 |
[32m[20221213 20:47:40 @agent_ppo2.py:185][0m |          -0.0573 |           0.0123 |           0.0000 |
[32m[20221213 20:47:40 @agent_ppo2.py:185][0m |          -0.0586 |           0.0122 |           0.0000 |
[32m[20221213 20:47:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:47:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.47
[32m[20221213 20:47:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.77
[32m[20221213 20:47:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.98
[32m[20221213 20:47:40 @agent_ppo2.py:143][0m Total time:      18.52 min
[32m[20221213 20:47:40 @agent_ppo2.py:145][0m 2654208 total steps have happened
[32m[20221213 20:47:40 @agent_ppo2.py:121][0m #------------------------ Iteration 648 --------------------------#
[32m[20221213 20:47:41 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:47:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:41 @agent_ppo2.py:185][0m |          -0.0028 |           0.0460 |           0.0000 |
[32m[20221213 20:47:41 @agent_ppo2.py:185][0m |           0.0018 |           0.0315 |           0.0000 |
[32m[20221213 20:47:41 @agent_ppo2.py:185][0m |          -0.0125 |           0.0260 |           0.0000 |
[32m[20221213 20:47:41 @agent_ppo2.py:185][0m |          -0.0244 |           0.0250 |           0.0000 |
[32m[20221213 20:47:41 @agent_ppo2.py:185][0m |          -0.0284 |           0.0232 |           0.0000 |
[32m[20221213 20:47:41 @agent_ppo2.py:185][0m |          -0.0312 |           0.0226 |           0.0000 |
[32m[20221213 20:47:41 @agent_ppo2.py:185][0m |          -0.0287 |           0.0223 |           0.0000 |
[32m[20221213 20:47:41 @agent_ppo2.py:185][0m |          -0.0305 |           0.0212 |           0.0000 |
[32m[20221213 20:47:42 @agent_ppo2.py:185][0m |          -0.0297 |           0.0211 |           0.0000 |
[32m[20221213 20:47:42 @agent_ppo2.py:185][0m |          -0.0349 |           0.0215 |           0.0000 |
[32m[20221213 20:47:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:47:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.37
[32m[20221213 20:47:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.11
[32m[20221213 20:47:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.99
[32m[20221213 20:47:42 @agent_ppo2.py:143][0m Total time:      18.55 min
[32m[20221213 20:47:42 @agent_ppo2.py:145][0m 2658304 total steps have happened
[32m[20221213 20:47:42 @agent_ppo2.py:121][0m #------------------------ Iteration 649 --------------------------#
[32m[20221213 20:47:42 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:47:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:43 @agent_ppo2.py:185][0m |           0.0412 |           0.0229 |           0.0000 |
[32m[20221213 20:47:43 @agent_ppo2.py:185][0m |          -0.0105 |           0.0156 |           0.0000 |
[32m[20221213 20:47:43 @agent_ppo2.py:185][0m |          -0.0392 |           0.0151 |           0.0000 |
[32m[20221213 20:47:43 @agent_ppo2.py:185][0m |          -0.0514 |           0.0147 |           0.0000 |
[32m[20221213 20:47:43 @agent_ppo2.py:185][0m |          -0.0619 |           0.0141 |           0.0000 |
[32m[20221213 20:47:43 @agent_ppo2.py:185][0m |          -0.0627 |           0.0139 |           0.0000 |
[32m[20221213 20:47:43 @agent_ppo2.py:185][0m |          -0.0725 |           0.0137 |           0.0000 |
[32m[20221213 20:47:43 @agent_ppo2.py:185][0m |          -0.0745 |           0.0134 |           0.0000 |
[32m[20221213 20:47:43 @agent_ppo2.py:185][0m |          -0.0821 |           0.0135 |           0.0000 |
[32m[20221213 20:47:43 @agent_ppo2.py:185][0m |          -0.0727 |           0.0132 |           0.0000 |
[32m[20221213 20:47:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:47:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.07
[32m[20221213 20:47:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.36
[32m[20221213 20:47:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.52
[32m[20221213 20:47:44 @agent_ppo2.py:143][0m Total time:      18.58 min
[32m[20221213 20:47:44 @agent_ppo2.py:145][0m 2662400 total steps have happened
[32m[20221213 20:47:44 @agent_ppo2.py:121][0m #------------------------ Iteration 650 --------------------------#
[32m[20221213 20:47:44 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:47:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:44 @agent_ppo2.py:185][0m |           0.0301 |           0.0384 |           0.0000 |
[32m[20221213 20:47:44 @agent_ppo2.py:185][0m |           0.0008 |           0.0255 |           0.0000 |
[32m[20221213 20:47:44 @agent_ppo2.py:185][0m |          -0.0130 |           0.0227 |           0.0000 |
[32m[20221213 20:47:45 @agent_ppo2.py:185][0m |          -0.0224 |           0.0215 |           0.0000 |
[32m[20221213 20:47:45 @agent_ppo2.py:185][0m |          -0.0269 |           0.0206 |           0.0000 |
[32m[20221213 20:47:45 @agent_ppo2.py:185][0m |          -0.0326 |           0.0201 |           0.0000 |
[32m[20221213 20:47:45 @agent_ppo2.py:185][0m |          -0.0400 |           0.0196 |           0.0000 |
[32m[20221213 20:47:45 @agent_ppo2.py:185][0m |          -0.0400 |           0.0199 |           0.0000 |
[32m[20221213 20:47:45 @agent_ppo2.py:185][0m |          -0.0297 |           0.0191 |           0.0000 |
[32m[20221213 20:47:45 @agent_ppo2.py:185][0m |          -0.0352 |           0.0191 |           0.0000 |
[32m[20221213 20:47:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:47:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.08
[32m[20221213 20:47:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.68
[32m[20221213 20:47:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.47
[32m[20221213 20:47:45 @agent_ppo2.py:143][0m Total time:      18.61 min
[32m[20221213 20:47:45 @agent_ppo2.py:145][0m 2666496 total steps have happened
[32m[20221213 20:47:45 @agent_ppo2.py:121][0m #------------------------ Iteration 651 --------------------------#
[32m[20221213 20:47:46 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:47:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:46 @agent_ppo2.py:185][0m |           0.0056 |           0.0442 |           0.0000 |
[32m[20221213 20:47:46 @agent_ppo2.py:185][0m |          -0.0060 |           0.0104 |           0.0000 |
[32m[20221213 20:47:46 @agent_ppo2.py:185][0m |          -0.0139 |           0.0095 |           0.0000 |
[32m[20221213 20:47:46 @agent_ppo2.py:185][0m |          -0.0197 |           0.0093 |           0.0000 |
[32m[20221213 20:47:46 @agent_ppo2.py:185][0m |          -0.0255 |           0.0092 |           0.0000 |
[32m[20221213 20:47:47 @agent_ppo2.py:185][0m |          -0.0312 |           0.0090 |           0.0000 |
[32m[20221213 20:47:47 @agent_ppo2.py:185][0m |          -0.0332 |           0.0089 |           0.0000 |
[32m[20221213 20:47:47 @agent_ppo2.py:185][0m |          -0.0186 |           0.0088 |           0.0000 |
[32m[20221213 20:47:47 @agent_ppo2.py:185][0m |          -0.0249 |           0.0088 |           0.0000 |
[32m[20221213 20:47:47 @agent_ppo2.py:185][0m |          -0.0340 |           0.0087 |           0.0000 |
[32m[20221213 20:47:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:47:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.18
[32m[20221213 20:47:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.33
[32m[20221213 20:47:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.71
[32m[20221213 20:47:47 @agent_ppo2.py:143][0m Total time:      18.64 min
[32m[20221213 20:47:47 @agent_ppo2.py:145][0m 2670592 total steps have happened
[32m[20221213 20:47:47 @agent_ppo2.py:121][0m #------------------------ Iteration 652 --------------------------#
[32m[20221213 20:47:48 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:47:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:48 @agent_ppo2.py:185][0m |           0.0392 |           0.0098 |           0.0000 |
[32m[20221213 20:47:48 @agent_ppo2.py:185][0m |          -0.0022 |           0.0087 |           0.0000 |
[32m[20221213 20:47:48 @agent_ppo2.py:185][0m |          -0.0257 |           0.0086 |           0.0000 |
[32m[20221213 20:47:48 @agent_ppo2.py:185][0m |          -0.0307 |           0.0085 |           0.0000 |
[32m[20221213 20:47:48 @agent_ppo2.py:185][0m |          -0.0355 |           0.0084 |           0.0000 |
[32m[20221213 20:47:48 @agent_ppo2.py:185][0m |          -0.0368 |           0.0084 |           0.0000 |
[32m[20221213 20:47:48 @agent_ppo2.py:185][0m |          -0.0410 |           0.0083 |           0.0000 |
[32m[20221213 20:47:49 @agent_ppo2.py:185][0m |          -0.0398 |           0.0082 |           0.0000 |
[32m[20221213 20:47:49 @agent_ppo2.py:185][0m |          -0.0716 |           0.0082 |           0.0000 |
[32m[20221213 20:47:49 @agent_ppo2.py:185][0m |          -0.0441 |           0.0081 |           0.0000 |
[32m[20221213 20:47:49 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:47:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.57
[32m[20221213 20:47:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.88
[32m[20221213 20:47:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.22
[32m[20221213 20:47:49 @agent_ppo2.py:143][0m Total time:      18.67 min
[32m[20221213 20:47:49 @agent_ppo2.py:145][0m 2674688 total steps have happened
[32m[20221213 20:47:49 @agent_ppo2.py:121][0m #------------------------ Iteration 653 --------------------------#
[32m[20221213 20:47:49 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:47:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:50 @agent_ppo2.py:185][0m |           0.0263 |           0.0312 |           0.0000 |
[32m[20221213 20:47:50 @agent_ppo2.py:185][0m |          -0.0331 |           0.0215 |           0.0000 |
[32m[20221213 20:47:50 @agent_ppo2.py:185][0m |          -0.0095 |           0.0188 |           0.0000 |
[32m[20221213 20:47:50 @agent_ppo2.py:185][0m |          -0.0170 |           0.0167 |           0.0000 |
[32m[20221213 20:47:50 @agent_ppo2.py:185][0m |          -0.0220 |           0.0161 |           0.0000 |
[32m[20221213 20:47:50 @agent_ppo2.py:185][0m |          -0.0252 |           0.0158 |           0.0000 |
[32m[20221213 20:47:50 @agent_ppo2.py:185][0m |          -0.0523 |           0.0155 |           0.0000 |
[32m[20221213 20:47:50 @agent_ppo2.py:185][0m |          -0.0275 |           0.0155 |           0.0000 |
[32m[20221213 20:47:50 @agent_ppo2.py:185][0m |          -0.0289 |           0.0150 |           0.0000 |
[32m[20221213 20:47:50 @agent_ppo2.py:185][0m |          -0.0279 |           0.0148 |           0.0000 |
[32m[20221213 20:47:50 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:47:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.34
[32m[20221213 20:47:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.22
[32m[20221213 20:47:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.45
[32m[20221213 20:47:51 @agent_ppo2.py:143][0m Total time:      18.70 min
[32m[20221213 20:47:51 @agent_ppo2.py:145][0m 2678784 total steps have happened
[32m[20221213 20:47:51 @agent_ppo2.py:121][0m #------------------------ Iteration 654 --------------------------#
[32m[20221213 20:47:51 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:47:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:51 @agent_ppo2.py:185][0m |           0.0386 |           0.0135 |           0.0000 |
[32m[20221213 20:47:52 @agent_ppo2.py:185][0m |           0.0179 |           0.0093 |           0.0000 |
[32m[20221213 20:47:52 @agent_ppo2.py:185][0m |           0.0054 |           0.0091 |           0.0000 |
[32m[20221213 20:47:52 @agent_ppo2.py:185][0m |          -0.0194 |           0.0090 |           0.0000 |
[32m[20221213 20:47:52 @agent_ppo2.py:185][0m |          -0.0169 |           0.0089 |           0.0000 |
[32m[20221213 20:47:52 @agent_ppo2.py:185][0m |          -0.0082 |           0.0094 |           0.0000 |
[32m[20221213 20:47:52 @agent_ppo2.py:185][0m |           0.0256 |           0.0095 |           0.0000 |
[32m[20221213 20:47:52 @agent_ppo2.py:185][0m |          -0.0487 |           0.0090 |           0.0000 |
[32m[20221213 20:47:52 @agent_ppo2.py:185][0m |          -0.0481 |           0.0088 |           0.0000 |
[32m[20221213 20:47:52 @agent_ppo2.py:185][0m |          -0.0511 |           0.0086 |           0.0000 |
[32m[20221213 20:47:52 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:47:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.47
[32m[20221213 20:47:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.80
[32m[20221213 20:47:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.45
[32m[20221213 20:47:53 @agent_ppo2.py:143][0m Total time:      18.72 min
[32m[20221213 20:47:53 @agent_ppo2.py:145][0m 2682880 total steps have happened
[32m[20221213 20:47:53 @agent_ppo2.py:121][0m #------------------------ Iteration 655 --------------------------#
[32m[20221213 20:47:53 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:47:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:53 @agent_ppo2.py:185][0m |           0.0359 |           0.0186 |           0.0000 |
[32m[20221213 20:47:53 @agent_ppo2.py:185][0m |           0.0153 |           0.0125 |           0.0000 |
[32m[20221213 20:47:53 @agent_ppo2.py:185][0m |          -0.0010 |           0.0110 |           0.0000 |
[32m[20221213 20:47:53 @agent_ppo2.py:185][0m |           0.0061 |           0.0107 |           0.0000 |
[32m[20221213 20:47:54 @agent_ppo2.py:185][0m |          -0.0073 |           0.0106 |           0.0000 |
[32m[20221213 20:47:54 @agent_ppo2.py:185][0m |          -0.0146 |           0.0104 |           0.0000 |
[32m[20221213 20:47:54 @agent_ppo2.py:185][0m |          -0.0164 |           0.0103 |           0.0000 |
[32m[20221213 20:47:54 @agent_ppo2.py:185][0m |          -0.0197 |           0.0102 |           0.0000 |
[32m[20221213 20:47:54 @agent_ppo2.py:185][0m |          -0.0210 |           0.0101 |           0.0000 |
[32m[20221213 20:47:54 @agent_ppo2.py:185][0m |          -0.0218 |           0.0102 |           0.0000 |
[32m[20221213 20:47:54 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:47:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.98
[32m[20221213 20:47:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.09
[32m[20221213 20:47:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.40
[32m[20221213 20:47:54 @agent_ppo2.py:143][0m Total time:      18.75 min
[32m[20221213 20:47:54 @agent_ppo2.py:145][0m 2686976 total steps have happened
[32m[20221213 20:47:54 @agent_ppo2.py:121][0m #------------------------ Iteration 656 --------------------------#
[32m[20221213 20:47:55 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:47:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:55 @agent_ppo2.py:185][0m |           0.0282 |           0.0309 |           0.0000 |
[32m[20221213 20:47:55 @agent_ppo2.py:185][0m |          -0.0015 |           0.0200 |           0.0000 |
[32m[20221213 20:47:55 @agent_ppo2.py:185][0m |          -0.0162 |           0.0187 |           0.0000 |
[32m[20221213 20:47:55 @agent_ppo2.py:185][0m |          -0.0160 |           0.0177 |           0.0000 |
[32m[20221213 20:47:55 @agent_ppo2.py:185][0m |          -0.0236 |           0.0176 |           0.0000 |
[32m[20221213 20:47:55 @agent_ppo2.py:185][0m |          -0.0253 |           0.0178 |           0.0000 |
[32m[20221213 20:47:56 @agent_ppo2.py:185][0m |          -0.0294 |           0.0173 |           0.0000 |
[32m[20221213 20:47:56 @agent_ppo2.py:185][0m |          -0.0306 |           0.0163 |           0.0000 |
[32m[20221213 20:47:56 @agent_ppo2.py:185][0m |          -0.0319 |           0.0166 |           0.0000 |
[32m[20221213 20:47:56 @agent_ppo2.py:185][0m |          -0.0329 |           0.0162 |           0.0000 |
[32m[20221213 20:47:56 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:47:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.60
[32m[20221213 20:47:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.18
[32m[20221213 20:47:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.04
[32m[20221213 20:47:56 @agent_ppo2.py:143][0m Total time:      18.78 min
[32m[20221213 20:47:56 @agent_ppo2.py:145][0m 2691072 total steps have happened
[32m[20221213 20:47:56 @agent_ppo2.py:121][0m #------------------------ Iteration 657 --------------------------#
[32m[20221213 20:47:57 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:47:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:57 @agent_ppo2.py:185][0m |           0.0382 |           0.0317 |           0.0000 |
[32m[20221213 20:47:57 @agent_ppo2.py:185][0m |          -0.0030 |           0.0244 |           0.0000 |
[32m[20221213 20:47:57 @agent_ppo2.py:185][0m |          -0.0242 |           0.0235 |           0.0000 |
[32m[20221213 20:47:57 @agent_ppo2.py:185][0m |          -0.0173 |           0.0219 |           0.0000 |
[32m[20221213 20:47:57 @agent_ppo2.py:185][0m |          -0.0430 |           0.0209 |           0.0000 |
[32m[20221213 20:47:57 @agent_ppo2.py:185][0m |          -0.0482 |           0.0202 |           0.0000 |
[32m[20221213 20:47:57 @agent_ppo2.py:185][0m |          -0.0439 |           0.0201 |           0.0000 |
[32m[20221213 20:47:57 @agent_ppo2.py:185][0m |          -0.0526 |           0.0200 |           0.0000 |
[32m[20221213 20:47:58 @agent_ppo2.py:185][0m |          -0.0445 |           0.0197 |           0.0000 |
[32m[20221213 20:47:58 @agent_ppo2.py:185][0m |          -0.0471 |           0.0194 |           0.0000 |
[32m[20221213 20:47:58 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 20:47:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.78
[32m[20221213 20:47:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.64
[32m[20221213 20:47:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.05
[32m[20221213 20:47:58 @agent_ppo2.py:143][0m Total time:      18.82 min
[32m[20221213 20:47:58 @agent_ppo2.py:145][0m 2695168 total steps have happened
[32m[20221213 20:47:58 @agent_ppo2.py:121][0m #------------------------ Iteration 658 --------------------------#
[32m[20221213 20:47:58 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:47:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:47:59 @agent_ppo2.py:185][0m |           0.0337 |           0.0529 |           0.0000 |
[32m[20221213 20:47:59 @agent_ppo2.py:185][0m |           0.0012 |           0.0366 |           0.0000 |
[32m[20221213 20:47:59 @agent_ppo2.py:185][0m |          -0.0709 |           0.0376 |           0.0000 |
[32m[20221213 20:47:59 @agent_ppo2.py:185][0m |          -0.0303 |           0.0345 |           0.0000 |
[32m[20221213 20:47:59 @agent_ppo2.py:185][0m |          -0.0351 |           0.0266 |           0.0000 |
[32m[20221213 20:47:59 @agent_ppo2.py:185][0m |          -0.0365 |           0.0252 |           0.0000 |
[32m[20221213 20:47:59 @agent_ppo2.py:185][0m |          -0.0450 |           0.0248 |           0.0000 |
[32m[20221213 20:47:59 @agent_ppo2.py:185][0m |          -0.0453 |           0.0236 |           0.0000 |
[32m[20221213 20:47:59 @agent_ppo2.py:185][0m |          -0.0589 |           0.0226 |           0.0000 |
[32m[20221213 20:47:59 @agent_ppo2.py:185][0m |          -0.0432 |           0.0226 |           0.0000 |
[32m[20221213 20:47:59 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:48:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.61
[32m[20221213 20:48:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.52
[32m[20221213 20:48:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.52
[32m[20221213 20:48:00 @agent_ppo2.py:143][0m Total time:      18.85 min
[32m[20221213 20:48:00 @agent_ppo2.py:145][0m 2699264 total steps have happened
[32m[20221213 20:48:00 @agent_ppo2.py:121][0m #------------------------ Iteration 659 --------------------------#
[32m[20221213 20:48:00 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:48:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:00 @agent_ppo2.py:185][0m |           0.0410 |           0.0290 |           0.0000 |
[32m[20221213 20:48:01 @agent_ppo2.py:185][0m |           0.0041 |           0.0254 |           0.0000 |
[32m[20221213 20:48:01 @agent_ppo2.py:185][0m |          -0.0150 |           0.0235 |           0.0000 |
[32m[20221213 20:48:01 @agent_ppo2.py:185][0m |          -0.0412 |           0.0233 |           0.0000 |
[32m[20221213 20:48:01 @agent_ppo2.py:185][0m |          -0.0528 |           0.0216 |           0.0000 |
[32m[20221213 20:48:01 @agent_ppo2.py:185][0m |          -0.0577 |           0.0212 |           0.0000 |
[32m[20221213 20:48:01 @agent_ppo2.py:185][0m |          -0.0626 |           0.0210 |           0.0000 |
[32m[20221213 20:48:01 @agent_ppo2.py:185][0m |          -0.0709 |           0.0203 |           0.0000 |
[32m[20221213 20:48:01 @agent_ppo2.py:185][0m |          -0.0758 |           0.0203 |           0.0000 |
[32m[20221213 20:48:01 @agent_ppo2.py:185][0m |          -0.0713 |           0.0201 |           0.0000 |
[32m[20221213 20:48:01 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:48:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.23
[32m[20221213 20:48:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.36
[32m[20221213 20:48:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.05
[32m[20221213 20:48:02 @agent_ppo2.py:143][0m Total time:      18.88 min
[32m[20221213 20:48:02 @agent_ppo2.py:145][0m 2703360 total steps have happened
[32m[20221213 20:48:02 @agent_ppo2.py:121][0m #------------------------ Iteration 660 --------------------------#
[32m[20221213 20:48:02 @agent_ppo2.py:127][0m Sampling time: 0.42 s by 5 slaves
[32m[20221213 20:48:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:02 @agent_ppo2.py:185][0m |           0.0699 |           0.0475 |           0.0000 |
[32m[20221213 20:48:02 @agent_ppo2.py:185][0m |           0.0569 |           0.0389 |           0.0000 |
[32m[20221213 20:48:02 @agent_ppo2.py:185][0m |           0.0266 |           0.0338 |           0.0000 |
[32m[20221213 20:48:03 @agent_ppo2.py:185][0m |          -0.0061 |           0.0351 |           0.0000 |
[32m[20221213 20:48:03 @agent_ppo2.py:185][0m |          -0.0502 |           0.0477 |           0.0000 |
[32m[20221213 20:48:03 @agent_ppo2.py:185][0m |           0.0074 |           0.0408 |           0.0000 |
[32m[20221213 20:48:03 @agent_ppo2.py:185][0m |          -0.0060 |           0.0287 |           0.0000 |
[32m[20221213 20:48:03 @agent_ppo2.py:185][0m |          -0.0188 |           0.0276 |           0.0000 |
[32m[20221213 20:48:03 @agent_ppo2.py:185][0m |          -0.0247 |           0.0270 |           0.0000 |
[32m[20221213 20:48:03 @agent_ppo2.py:185][0m |          -0.0286 |           0.0266 |           0.0000 |
[32m[20221213 20:48:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:48:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.01
[32m[20221213 20:48:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.38
[32m[20221213 20:48:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.57
[32m[20221213 20:48:03 @agent_ppo2.py:143][0m Total time:      18.91 min
[32m[20221213 20:48:03 @agent_ppo2.py:145][0m 2707456 total steps have happened
[32m[20221213 20:48:04 @agent_ppo2.py:121][0m #------------------------ Iteration 661 --------------------------#
[32m[20221213 20:48:04 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:48:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:04 @agent_ppo2.py:185][0m |           0.0346 |           0.0326 |           0.0000 |
[32m[20221213 20:48:04 @agent_ppo2.py:185][0m |           0.0083 |           0.0148 |           0.0000 |
[32m[20221213 20:48:04 @agent_ppo2.py:185][0m |          -0.0074 |           0.0140 |           0.0000 |
[32m[20221213 20:48:04 @agent_ppo2.py:185][0m |          -0.0312 |           0.0138 |           0.0000 |
[32m[20221213 20:48:04 @agent_ppo2.py:185][0m |          -0.0330 |           0.0137 |           0.0000 |
[32m[20221213 20:48:05 @agent_ppo2.py:185][0m |          -0.0414 |           0.0135 |           0.0000 |
[32m[20221213 20:48:05 @agent_ppo2.py:185][0m |          -0.0402 |           0.0133 |           0.0000 |
[32m[20221213 20:48:05 @agent_ppo2.py:185][0m |          -0.0494 |           0.0132 |           0.0000 |
[32m[20221213 20:48:05 @agent_ppo2.py:185][0m |          -0.0509 |           0.0131 |           0.0000 |
[32m[20221213 20:48:05 @agent_ppo2.py:185][0m |          -0.0297 |           0.0130 |           0.0000 |
[32m[20221213 20:48:05 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:48:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.38
[32m[20221213 20:48:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.08
[32m[20221213 20:48:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.44
[32m[20221213 20:48:05 @agent_ppo2.py:143][0m Total time:      18.94 min
[32m[20221213 20:48:05 @agent_ppo2.py:145][0m 2711552 total steps have happened
[32m[20221213 20:48:05 @agent_ppo2.py:121][0m #------------------------ Iteration 662 --------------------------#
[32m[20221213 20:48:06 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:48:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:06 @agent_ppo2.py:185][0m |           0.0431 |           0.0324 |           0.0000 |
[32m[20221213 20:48:06 @agent_ppo2.py:185][0m |           0.0057 |           0.0255 |           0.0000 |
[32m[20221213 20:48:06 @agent_ppo2.py:185][0m |          -0.0246 |           0.0236 |           0.0000 |
[32m[20221213 20:48:06 @agent_ppo2.py:185][0m |          -0.0314 |           0.0208 |           0.0000 |
[32m[20221213 20:48:06 @agent_ppo2.py:185][0m |          -0.0771 |           0.0220 |           0.0000 |
[32m[20221213 20:48:06 @agent_ppo2.py:185][0m |          -0.0378 |           0.0219 |           0.0000 |
[32m[20221213 20:48:06 @agent_ppo2.py:185][0m |          -0.0399 |           0.0190 |           0.0000 |
[32m[20221213 20:48:06 @agent_ppo2.py:185][0m |          -0.0288 |           0.0200 |           0.0000 |
[32m[20221213 20:48:07 @agent_ppo2.py:185][0m |          -0.0396 |           0.0206 |           0.0000 |
[32m[20221213 20:48:07 @agent_ppo2.py:185][0m |          -0.0354 |           0.0178 |           0.0000 |
[32m[20221213 20:48:07 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:48:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.24
[32m[20221213 20:48:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.05
[32m[20221213 20:48:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.86
[32m[20221213 20:48:07 @agent_ppo2.py:143][0m Total time:      18.96 min
[32m[20221213 20:48:07 @agent_ppo2.py:145][0m 2715648 total steps have happened
[32m[20221213 20:48:07 @agent_ppo2.py:121][0m #------------------------ Iteration 663 --------------------------#
[32m[20221213 20:48:07 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:08 @agent_ppo2.py:185][0m |           0.0331 |           0.0174 |           0.0000 |
[32m[20221213 20:48:08 @agent_ppo2.py:185][0m |          -0.0052 |           0.0154 |           0.0000 |
[32m[20221213 20:48:08 @agent_ppo2.py:185][0m |          -0.0321 |           0.0151 |           0.0000 |
[32m[20221213 20:48:08 @agent_ppo2.py:185][0m |          -0.0472 |           0.0149 |           0.0000 |
[32m[20221213 20:48:08 @agent_ppo2.py:185][0m |          -0.0605 |           0.0147 |           0.0000 |
[32m[20221213 20:48:08 @agent_ppo2.py:185][0m |          -0.0786 |           0.0146 |           0.0000 |
[32m[20221213 20:48:08 @agent_ppo2.py:185][0m |          -0.0726 |           0.0145 |           0.0000 |
[32m[20221213 20:48:08 @agent_ppo2.py:185][0m |          -0.0694 |           0.0142 |           0.0000 |
[32m[20221213 20:48:08 @agent_ppo2.py:185][0m |          -0.0745 |           0.0140 |           0.0000 |
[32m[20221213 20:48:08 @agent_ppo2.py:185][0m |          -0.0646 |           0.0139 |           0.0000 |
[32m[20221213 20:48:08 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:48:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.54
[32m[20221213 20:48:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.56
[32m[20221213 20:48:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.55
[32m[20221213 20:48:09 @agent_ppo2.py:143][0m Total time:      19.00 min
[32m[20221213 20:48:09 @agent_ppo2.py:145][0m 2719744 total steps have happened
[32m[20221213 20:48:09 @agent_ppo2.py:121][0m #------------------------ Iteration 664 --------------------------#
[32m[20221213 20:48:09 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:48:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:10 @agent_ppo2.py:185][0m |           0.0496 |           0.0176 |           0.0000 |
[32m[20221213 20:48:10 @agent_ppo2.py:185][0m |          -0.0028 |           0.0148 |           0.0000 |
[32m[20221213 20:48:10 @agent_ppo2.py:185][0m |          -0.0287 |           0.0143 |           0.0000 |
[32m[20221213 20:48:10 @agent_ppo2.py:185][0m |          -0.0333 |           0.0140 |           0.0000 |
[32m[20221213 20:48:10 @agent_ppo2.py:185][0m |          -0.0453 |           0.0139 |           0.0000 |
[32m[20221213 20:48:10 @agent_ppo2.py:185][0m |          -0.0480 |           0.0137 |           0.0000 |
[32m[20221213 20:48:10 @agent_ppo2.py:185][0m |          -0.0495 |           0.0136 |           0.0000 |
[32m[20221213 20:48:10 @agent_ppo2.py:185][0m |          -0.0548 |           0.0135 |           0.0000 |
[32m[20221213 20:48:10 @agent_ppo2.py:185][0m |          -0.0570 |           0.0133 |           0.0000 |
[32m[20221213 20:48:10 @agent_ppo2.py:185][0m |          -0.0603 |           0.0132 |           0.0000 |
[32m[20221213 20:48:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:48:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.70
[32m[20221213 20:48:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.20
[32m[20221213 20:48:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.60
[32m[20221213 20:48:11 @agent_ppo2.py:143][0m Total time:      19.03 min
[32m[20221213 20:48:11 @agent_ppo2.py:145][0m 2723840 total steps have happened
[32m[20221213 20:48:11 @agent_ppo2.py:121][0m #------------------------ Iteration 665 --------------------------#
[32m[20221213 20:48:11 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:11 @agent_ppo2.py:185][0m |           0.0367 |           0.0150 |           0.0000 |
[32m[20221213 20:48:11 @agent_ppo2.py:185][0m |           0.0107 |           0.0142 |           0.0000 |
[32m[20221213 20:48:11 @agent_ppo2.py:185][0m |          -0.0093 |           0.0139 |           0.0000 |
[32m[20221213 20:48:11 @agent_ppo2.py:185][0m |          -0.0278 |           0.0138 |           0.0000 |
[32m[20221213 20:48:12 @agent_ppo2.py:185][0m |          -0.0461 |           0.0137 |           0.0000 |
[32m[20221213 20:48:12 @agent_ppo2.py:185][0m |          -0.0457 |           0.0135 |           0.0000 |
[32m[20221213 20:48:12 @agent_ppo2.py:185][0m |          -0.0562 |           0.0133 |           0.0000 |
[32m[20221213 20:48:12 @agent_ppo2.py:185][0m |          -0.0569 |           0.0134 |           0.0000 |
[32m[20221213 20:48:12 @agent_ppo2.py:185][0m |          -0.0592 |           0.0132 |           0.0000 |
[32m[20221213 20:48:12 @agent_ppo2.py:185][0m |          -0.0567 |           0.0131 |           0.0000 |
[32m[20221213 20:48:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:48:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.14
[32m[20221213 20:48:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.70
[32m[20221213 20:48:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.14
[32m[20221213 20:48:12 @agent_ppo2.py:143][0m Total time:      19.05 min
[32m[20221213 20:48:12 @agent_ppo2.py:145][0m 2727936 total steps have happened
[32m[20221213 20:48:12 @agent_ppo2.py:121][0m #------------------------ Iteration 666 --------------------------#
[32m[20221213 20:48:13 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:13 @agent_ppo2.py:185][0m |           0.0262 |           0.0211 |           0.0000 |
[32m[20221213 20:48:13 @agent_ppo2.py:185][0m |           0.0334 |           0.0106 |           0.0000 |
[32m[20221213 20:48:13 @agent_ppo2.py:185][0m |           0.0063 |           0.0102 |           0.0000 |
[32m[20221213 20:48:13 @agent_ppo2.py:185][0m |          -0.0027 |           0.0101 |           0.0000 |
[32m[20221213 20:48:13 @agent_ppo2.py:185][0m |          -0.0068 |           0.0100 |           0.0000 |
[32m[20221213 20:48:13 @agent_ppo2.py:185][0m |          -0.0107 |           0.0100 |           0.0000 |
[32m[20221213 20:48:13 @agent_ppo2.py:185][0m |          -0.0153 |           0.0099 |           0.0000 |
[32m[20221213 20:48:14 @agent_ppo2.py:185][0m |           0.0153 |           0.0105 |           0.0000 |
[32m[20221213 20:48:14 @agent_ppo2.py:185][0m |          -0.0244 |           0.0100 |           0.0000 |
[32m[20221213 20:48:14 @agent_ppo2.py:185][0m |          -0.0267 |           0.0098 |           0.0000 |
[32m[20221213 20:48:14 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:48:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.36
[32m[20221213 20:48:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.79
[32m[20221213 20:48:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.81
[32m[20221213 20:48:14 @agent_ppo2.py:143][0m Total time:      19.08 min
[32m[20221213 20:48:14 @agent_ppo2.py:145][0m 2732032 total steps have happened
[32m[20221213 20:48:14 @agent_ppo2.py:121][0m #------------------------ Iteration 667 --------------------------#
[32m[20221213 20:48:15 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:48:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:15 @agent_ppo2.py:185][0m |           0.0210 |           0.0099 |           0.0000 |
[32m[20221213 20:48:15 @agent_ppo2.py:185][0m |          -0.0052 |           0.0084 |           0.0000 |
[32m[20221213 20:48:15 @agent_ppo2.py:185][0m |          -0.0128 |           0.0082 |           0.0000 |
[32m[20221213 20:48:15 @agent_ppo2.py:185][0m |          -0.0174 |           0.0082 |           0.0000 |
[32m[20221213 20:48:15 @agent_ppo2.py:185][0m |          -0.0345 |           0.0081 |           0.0000 |
[32m[20221213 20:48:15 @agent_ppo2.py:185][0m |          -0.0407 |           0.0080 |           0.0000 |
[32m[20221213 20:48:15 @agent_ppo2.py:185][0m |          -0.0410 |           0.0079 |           0.0000 |
[32m[20221213 20:48:15 @agent_ppo2.py:185][0m |          -0.0423 |           0.0079 |           0.0000 |
[32m[20221213 20:48:15 @agent_ppo2.py:185][0m |          -0.0445 |           0.0078 |           0.0000 |
[32m[20221213 20:48:16 @agent_ppo2.py:185][0m |          -0.0503 |           0.0077 |           0.0000 |
[32m[20221213 20:48:16 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:48:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.72
[32m[20221213 20:48:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.02
[32m[20221213 20:48:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.56
[32m[20221213 20:48:16 @agent_ppo2.py:143][0m Total time:      19.11 min
[32m[20221213 20:48:16 @agent_ppo2.py:145][0m 2736128 total steps have happened
[32m[20221213 20:48:16 @agent_ppo2.py:121][0m #------------------------ Iteration 668 --------------------------#
[32m[20221213 20:48:16 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:48:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:17 @agent_ppo2.py:185][0m |           0.0366 |           0.0130 |           0.0000 |
[32m[20221213 20:48:17 @agent_ppo2.py:185][0m |           0.0084 |           0.0106 |           0.0000 |
[32m[20221213 20:48:17 @agent_ppo2.py:185][0m |          -0.0090 |           0.0103 |           0.0000 |
[32m[20221213 20:48:17 @agent_ppo2.py:185][0m |          -0.0160 |           0.0102 |           0.0000 |
[32m[20221213 20:48:17 @agent_ppo2.py:185][0m |          -0.0251 |           0.0100 |           0.0000 |
[32m[20221213 20:48:17 @agent_ppo2.py:185][0m |          -0.0688 |           0.0102 |           0.0000 |
[32m[20221213 20:48:17 @agent_ppo2.py:185][0m |          -0.0297 |           0.0100 |           0.0000 |
[32m[20221213 20:48:17 @agent_ppo2.py:185][0m |          -0.0266 |           0.0097 |           0.0000 |
[32m[20221213 20:48:17 @agent_ppo2.py:185][0m |          -0.0280 |           0.0096 |           0.0000 |
[32m[20221213 20:48:17 @agent_ppo2.py:185][0m |          -0.0745 |           0.0107 |           0.0000 |
[32m[20221213 20:48:17 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:48:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.37
[32m[20221213 20:48:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.05
[32m[20221213 20:48:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.97
[32m[20221213 20:48:18 @agent_ppo2.py:143][0m Total time:      19.14 min
[32m[20221213 20:48:18 @agent_ppo2.py:145][0m 2740224 total steps have happened
[32m[20221213 20:48:18 @agent_ppo2.py:121][0m #------------------------ Iteration 669 --------------------------#
[32m[20221213 20:48:18 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:48:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:18 @agent_ppo2.py:185][0m |           0.0206 |           0.0113 |           0.0000 |
[32m[20221213 20:48:18 @agent_ppo2.py:185][0m |           0.0093 |           0.0101 |           0.0000 |
[32m[20221213 20:48:18 @agent_ppo2.py:185][0m |          -0.0190 |           0.0099 |           0.0000 |
[32m[20221213 20:48:19 @agent_ppo2.py:185][0m |          -0.0229 |           0.0098 |           0.0000 |
[32m[20221213 20:48:19 @agent_ppo2.py:185][0m |          -0.0269 |           0.0097 |           0.0000 |
[32m[20221213 20:48:19 @agent_ppo2.py:185][0m |          -0.0280 |           0.0098 |           0.0000 |
[32m[20221213 20:48:19 @agent_ppo2.py:185][0m |          -0.0390 |           0.0098 |           0.0000 |
[32m[20221213 20:48:19 @agent_ppo2.py:185][0m |          -0.0397 |           0.0095 |           0.0000 |
[32m[20221213 20:48:19 @agent_ppo2.py:185][0m |          -0.0456 |           0.0094 |           0.0000 |
[32m[20221213 20:48:19 @agent_ppo2.py:185][0m |          -0.0452 |           0.0094 |           0.0000 |
[32m[20221213 20:48:19 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:48:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.18
[32m[20221213 20:48:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.42
[32m[20221213 20:48:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.28
[32m[20221213 20:48:20 @agent_ppo2.py:143][0m Total time:      19.17 min
[32m[20221213 20:48:20 @agent_ppo2.py:145][0m 2744320 total steps have happened
[32m[20221213 20:48:20 @agent_ppo2.py:121][0m #------------------------ Iteration 670 --------------------------#
[32m[20221213 20:48:20 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 5 slaves
[32m[20221213 20:48:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:20 @agent_ppo2.py:185][0m |           0.0117 |           0.0104 |           0.0000 |
[32m[20221213 20:48:20 @agent_ppo2.py:185][0m |           0.0164 |           0.0081 |           0.0000 |
[32m[20221213 20:48:20 @agent_ppo2.py:185][0m |          -0.0085 |           0.0079 |           0.0000 |
[32m[20221213 20:48:20 @agent_ppo2.py:185][0m |          -0.0182 |           0.0078 |           0.0000 |
[32m[20221213 20:48:21 @agent_ppo2.py:185][0m |          -0.0254 |           0.0078 |           0.0000 |
[32m[20221213 20:48:21 @agent_ppo2.py:185][0m |          -0.0348 |           0.0077 |           0.0000 |
[32m[20221213 20:48:21 @agent_ppo2.py:185][0m |          -0.0113 |           0.0078 |           0.0000 |
[32m[20221213 20:48:21 @agent_ppo2.py:185][0m |          -0.0374 |           0.0077 |           0.0000 |
[32m[20221213 20:48:21 @agent_ppo2.py:185][0m |          -0.0391 |           0.0075 |           0.0000 |
[32m[20221213 20:48:21 @agent_ppo2.py:185][0m |          -0.0428 |           0.0075 |           0.0000 |
[32m[20221213 20:48:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:48:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.22
[32m[20221213 20:48:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.70
[32m[20221213 20:48:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.51
[32m[20221213 20:48:21 @agent_ppo2.py:143][0m Total time:      19.20 min
[32m[20221213 20:48:21 @agent_ppo2.py:145][0m 2748416 total steps have happened
[32m[20221213 20:48:21 @agent_ppo2.py:121][0m #------------------------ Iteration 671 --------------------------#
[32m[20221213 20:48:22 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:22 @agent_ppo2.py:185][0m |           0.0383 |           0.0718 |           0.0000 |
[32m[20221213 20:48:22 @agent_ppo2.py:185][0m |           0.0137 |           0.0234 |           0.0000 |
[32m[20221213 20:48:22 @agent_ppo2.py:185][0m |          -0.0056 |           0.0172 |           0.0000 |
[32m[20221213 20:48:22 @agent_ppo2.py:185][0m |          -0.0176 |           0.0161 |           0.0000 |
[32m[20221213 20:48:22 @agent_ppo2.py:185][0m |          -0.0222 |           0.0157 |           0.0000 |
[32m[20221213 20:48:22 @agent_ppo2.py:185][0m |          -0.0267 |           0.0155 |           0.0000 |
[32m[20221213 20:48:22 @agent_ppo2.py:185][0m |          -0.0256 |           0.0150 |           0.0000 |
[32m[20221213 20:48:23 @agent_ppo2.py:185][0m |          -0.0291 |           0.0148 |           0.0000 |
[32m[20221213 20:48:23 @agent_ppo2.py:185][0m |          -0.0312 |           0.0147 |           0.0000 |
[32m[20221213 20:48:23 @agent_ppo2.py:185][0m |          -0.0327 |           0.0144 |           0.0000 |
[32m[20221213 20:48:23 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 20:48:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.79
[32m[20221213 20:48:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.12
[32m[20221213 20:48:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.34
[32m[20221213 20:48:23 @agent_ppo2.py:143][0m Total time:      19.23 min
[32m[20221213 20:48:23 @agent_ppo2.py:145][0m 2752512 total steps have happened
[32m[20221213 20:48:23 @agent_ppo2.py:121][0m #------------------------ Iteration 672 --------------------------#
[32m[20221213 20:48:24 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:48:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:24 @agent_ppo2.py:185][0m |           0.0169 |           0.0259 |           0.0000 |
[32m[20221213 20:48:24 @agent_ppo2.py:185][0m |           0.0078 |           0.0135 |           0.0000 |
[32m[20221213 20:48:24 @agent_ppo2.py:185][0m |          -0.0021 |           0.0129 |           0.0000 |
[32m[20221213 20:48:24 @agent_ppo2.py:185][0m |          -0.0154 |           0.0123 |           0.0000 |
[32m[20221213 20:48:24 @agent_ppo2.py:185][0m |          -0.0263 |           0.0121 |           0.0000 |
[32m[20221213 20:48:24 @agent_ppo2.py:185][0m |          -0.0354 |           0.0123 |           0.0000 |
[32m[20221213 20:48:24 @agent_ppo2.py:185][0m |          -0.0386 |           0.0118 |           0.0000 |
[32m[20221213 20:48:24 @agent_ppo2.py:185][0m |          -0.0364 |           0.0117 |           0.0000 |
[32m[20221213 20:48:25 @agent_ppo2.py:185][0m |          -0.0389 |           0.0116 |           0.0000 |
[32m[20221213 20:48:25 @agent_ppo2.py:185][0m |          -0.0069 |           0.0117 |           0.0000 |
[32m[20221213 20:48:25 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 20:48:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.07
[32m[20221213 20:48:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.01
[32m[20221213 20:48:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.66
[32m[20221213 20:48:25 @agent_ppo2.py:143][0m Total time:      19.27 min
[32m[20221213 20:48:25 @agent_ppo2.py:145][0m 2756608 total steps have happened
[32m[20221213 20:48:25 @agent_ppo2.py:121][0m #------------------------ Iteration 673 --------------------------#
[32m[20221213 20:48:25 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:48:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:26 @agent_ppo2.py:185][0m |           0.0412 |           0.0117 |           0.0000 |
[32m[20221213 20:48:26 @agent_ppo2.py:185][0m |           0.0355 |           0.0102 |           0.0000 |
[32m[20221213 20:48:26 @agent_ppo2.py:185][0m |          -0.0168 |           0.0099 |           0.0000 |
[32m[20221213 20:48:26 @agent_ppo2.py:185][0m |          -0.0389 |           0.0097 |           0.0000 |
[32m[20221213 20:48:26 @agent_ppo2.py:185][0m |          -0.0482 |           0.0096 |           0.0000 |
[32m[20221213 20:48:26 @agent_ppo2.py:185][0m |          -0.0585 |           0.0095 |           0.0000 |
[32m[20221213 20:48:26 @agent_ppo2.py:185][0m |          -0.0657 |           0.0094 |           0.0000 |
[32m[20221213 20:48:26 @agent_ppo2.py:185][0m |          -0.0671 |           0.0093 |           0.0000 |
[32m[20221213 20:48:26 @agent_ppo2.py:185][0m |          -0.0716 |           0.0092 |           0.0000 |
[32m[20221213 20:48:27 @agent_ppo2.py:185][0m |          -0.0821 |           0.0092 |           0.0000 |
[32m[20221213 20:48:27 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:48:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.39
[32m[20221213 20:48:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.72
[32m[20221213 20:48:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.68
[32m[20221213 20:48:27 @agent_ppo2.py:143][0m Total time:      19.30 min
[32m[20221213 20:48:27 @agent_ppo2.py:145][0m 2760704 total steps have happened
[32m[20221213 20:48:27 @agent_ppo2.py:121][0m #------------------------ Iteration 674 --------------------------#
[32m[20221213 20:48:27 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:48:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:27 @agent_ppo2.py:185][0m |           0.0374 |           0.0491 |           0.0000 |
[32m[20221213 20:48:28 @agent_ppo2.py:185][0m |           0.0041 |           0.0232 |           0.0000 |
[32m[20221213 20:48:28 @agent_ppo2.py:185][0m |          -0.0074 |           0.0199 |           0.0000 |
[32m[20221213 20:48:28 @agent_ppo2.py:185][0m |          -0.0112 |           0.0181 |           0.0000 |
[32m[20221213 20:48:28 @agent_ppo2.py:185][0m |          -0.0196 |           0.0180 |           0.0000 |
[32m[20221213 20:48:28 @agent_ppo2.py:185][0m |          -0.0740 |           0.0178 |           0.0000 |
[32m[20221213 20:48:28 @agent_ppo2.py:185][0m |          -0.0259 |           0.0174 |           0.0000 |
[32m[20221213 20:48:28 @agent_ppo2.py:185][0m |          -0.0285 |           0.0169 |           0.0000 |
[32m[20221213 20:48:28 @agent_ppo2.py:185][0m |          -0.0271 |           0.0163 |           0.0000 |
[32m[20221213 20:48:28 @agent_ppo2.py:185][0m |          -0.0294 |           0.0167 |           0.0000 |
[32m[20221213 20:48:28 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:48:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.84
[32m[20221213 20:48:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.20
[32m[20221213 20:48:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.25
[32m[20221213 20:48:29 @agent_ppo2.py:143][0m Total time:      19.32 min
[32m[20221213 20:48:29 @agent_ppo2.py:145][0m 2764800 total steps have happened
[32m[20221213 20:48:29 @agent_ppo2.py:121][0m #------------------------ Iteration 675 --------------------------#
[32m[20221213 20:48:29 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:29 @agent_ppo2.py:185][0m |           0.0434 |           0.0175 |           0.0000 |
[32m[20221213 20:48:29 @agent_ppo2.py:185][0m |           0.0237 |           0.0175 |           0.0000 |
[32m[20221213 20:48:29 @agent_ppo2.py:185][0m |          -0.0361 |           0.0175 |           0.0000 |
[32m[20221213 20:48:29 @agent_ppo2.py:185][0m |          -0.0599 |           0.0148 |           0.0000 |
[32m[20221213 20:48:30 @agent_ppo2.py:185][0m |          -0.0711 |           0.0147 |           0.0000 |
[32m[20221213 20:48:30 @agent_ppo2.py:185][0m |          -0.0515 |           0.0147 |           0.0000 |
[32m[20221213 20:48:30 @agent_ppo2.py:185][0m |          -0.0781 |           0.0145 |           0.0000 |
[32m[20221213 20:48:30 @agent_ppo2.py:185][0m |          -0.0817 |           0.0140 |           0.0000 |
[32m[20221213 20:48:30 @agent_ppo2.py:185][0m |          -0.0808 |           0.0138 |           0.0000 |
[32m[20221213 20:48:30 @agent_ppo2.py:185][0m |          -0.0978 |           0.0139 |           0.0000 |
[32m[20221213 20:48:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:48:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.96
[32m[20221213 20:48:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.32
[32m[20221213 20:48:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.46
[32m[20221213 20:48:30 @agent_ppo2.py:143][0m Total time:      19.35 min
[32m[20221213 20:48:30 @agent_ppo2.py:145][0m 2768896 total steps have happened
[32m[20221213 20:48:30 @agent_ppo2.py:121][0m #------------------------ Iteration 676 --------------------------#
[32m[20221213 20:48:31 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:31 @agent_ppo2.py:185][0m |           0.0634 |           0.0181 |           0.0000 |
[32m[20221213 20:48:31 @agent_ppo2.py:185][0m |           0.0323 |           0.0169 |           0.0000 |
[32m[20221213 20:48:31 @agent_ppo2.py:185][0m |          -0.0099 |           0.0167 |           0.0000 |
[32m[20221213 20:48:31 @agent_ppo2.py:185][0m |          -0.0250 |           0.0159 |           0.0000 |
[32m[20221213 20:48:31 @agent_ppo2.py:185][0m |          -0.0306 |           0.0160 |           0.0000 |
[32m[20221213 20:48:31 @agent_ppo2.py:185][0m |          -0.0379 |           0.0157 |           0.0000 |
[32m[20221213 20:48:31 @agent_ppo2.py:185][0m |          -0.0470 |           0.0154 |           0.0000 |
[32m[20221213 20:48:31 @agent_ppo2.py:185][0m |          -0.0325 |           0.0157 |           0.0000 |
[32m[20221213 20:48:32 @agent_ppo2.py:185][0m |          -0.0632 |           0.0159 |           0.0000 |
[32m[20221213 20:48:32 @agent_ppo2.py:185][0m |          -0.0598 |           0.0152 |           0.0000 |
[32m[20221213 20:48:32 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:48:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.31
[32m[20221213 20:48:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.35
[32m[20221213 20:48:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.03
[32m[20221213 20:48:32 @agent_ppo2.py:143][0m Total time:      19.38 min
[32m[20221213 20:48:32 @agent_ppo2.py:145][0m 2772992 total steps have happened
[32m[20221213 20:48:32 @agent_ppo2.py:121][0m #------------------------ Iteration 677 --------------------------#
[32m[20221213 20:48:32 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:48:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:33 @agent_ppo2.py:185][0m |           0.0292 |           0.0283 |           0.0000 |
[32m[20221213 20:48:33 @agent_ppo2.py:185][0m |          -0.0486 |           0.0255 |           0.0000 |
[32m[20221213 20:48:33 @agent_ppo2.py:185][0m |          -0.0256 |           0.0236 |           0.0000 |
[32m[20221213 20:48:33 @agent_ppo2.py:185][0m |          -0.0343 |           0.0207 |           0.0000 |
[32m[20221213 20:48:33 @agent_ppo2.py:185][0m |          -0.0387 |           0.0201 |           0.0000 |
[32m[20221213 20:48:33 @agent_ppo2.py:185][0m |          -0.0405 |           0.0197 |           0.0000 |
[32m[20221213 20:48:33 @agent_ppo2.py:185][0m |          -0.0435 |           0.0194 |           0.0000 |
[32m[20221213 20:48:33 @agent_ppo2.py:185][0m |          -0.0508 |           0.0193 |           0.0000 |
[32m[20221213 20:48:33 @agent_ppo2.py:185][0m |          -0.0470 |           0.0190 |           0.0000 |
[32m[20221213 20:48:33 @agent_ppo2.py:185][0m |          -0.0539 |           0.0186 |           0.0000 |
[32m[20221213 20:48:33 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:48:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.42
[32m[20221213 20:48:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.77
[32m[20221213 20:48:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.85
[32m[20221213 20:48:34 @agent_ppo2.py:143][0m Total time:      19.41 min
[32m[20221213 20:48:34 @agent_ppo2.py:145][0m 2777088 total steps have happened
[32m[20221213 20:48:34 @agent_ppo2.py:121][0m #------------------------ Iteration 678 --------------------------#
[32m[20221213 20:48:34 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:34 @agent_ppo2.py:185][0m |           0.0309 |           0.0182 |           0.0000 |
[32m[20221213 20:48:34 @agent_ppo2.py:185][0m |          -0.0004 |           0.0138 |           0.0000 |
[32m[20221213 20:48:35 @agent_ppo2.py:185][0m |          -0.0219 |           0.0135 |           0.0000 |
[32m[20221213 20:48:35 @agent_ppo2.py:185][0m |          -0.0247 |           0.0132 |           0.0000 |
[32m[20221213 20:48:35 @agent_ppo2.py:185][0m |          -0.0232 |           0.0132 |           0.0000 |
[32m[20221213 20:48:35 @agent_ppo2.py:185][0m |          -0.0470 |           0.0130 |           0.0000 |
[32m[20221213 20:48:35 @agent_ppo2.py:185][0m |          -0.0343 |           0.0128 |           0.0000 |
[32m[20221213 20:48:35 @agent_ppo2.py:185][0m |          -0.0557 |           0.0127 |           0.0000 |
[32m[20221213 20:48:35 @agent_ppo2.py:185][0m |          -0.0649 |           0.0126 |           0.0000 |
[32m[20221213 20:48:35 @agent_ppo2.py:185][0m |          -0.0578 |           0.0127 |           0.0000 |
[32m[20221213 20:48:35 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:48:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.05
[32m[20221213 20:48:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.73
[32m[20221213 20:48:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.13
[32m[20221213 20:48:35 @agent_ppo2.py:143][0m Total time:      19.44 min
[32m[20221213 20:48:35 @agent_ppo2.py:145][0m 2781184 total steps have happened
[32m[20221213 20:48:35 @agent_ppo2.py:121][0m #------------------------ Iteration 679 --------------------------#
[32m[20221213 20:48:36 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:36 @agent_ppo2.py:185][0m |           0.0712 |           0.0148 |           0.0000 |
[32m[20221213 20:48:36 @agent_ppo2.py:185][0m |           0.0302 |           0.0111 |           0.0000 |
[32m[20221213 20:48:36 @agent_ppo2.py:185][0m |           0.0121 |           0.0106 |           0.0000 |
[32m[20221213 20:48:36 @agent_ppo2.py:185][0m |          -0.0037 |           0.0104 |           0.0000 |
[32m[20221213 20:48:36 @agent_ppo2.py:185][0m |          -0.0048 |           0.0112 |           0.0000 |
[32m[20221213 20:48:36 @agent_ppo2.py:185][0m |          -0.0050 |           0.0119 |           0.0000 |
[32m[20221213 20:48:36 @agent_ppo2.py:185][0m |          -0.0239 |           0.0112 |           0.0000 |
[32m[20221213 20:48:37 @agent_ppo2.py:185][0m |          -0.0300 |           0.0101 |           0.0000 |
[32m[20221213 20:48:37 @agent_ppo2.py:185][0m |          -0.0350 |           0.0100 |           0.0000 |
[32m[20221213 20:48:37 @agent_ppo2.py:185][0m |          -0.0409 |           0.0099 |           0.0000 |
[32m[20221213 20:48:37 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:48:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.97
[32m[20221213 20:48:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.61
[32m[20221213 20:48:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.25
[32m[20221213 20:48:37 @agent_ppo2.py:143][0m Total time:      19.47 min
[32m[20221213 20:48:37 @agent_ppo2.py:145][0m 2785280 total steps have happened
[32m[20221213 20:48:37 @agent_ppo2.py:121][0m #------------------------ Iteration 680 --------------------------#
[32m[20221213 20:48:37 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:48:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:38 @agent_ppo2.py:185][0m |           0.0756 |           0.0099 |           0.0000 |
[32m[20221213 20:48:38 @agent_ppo2.py:185][0m |           0.0461 |           0.0095 |           0.0000 |
[32m[20221213 20:48:38 @agent_ppo2.py:185][0m |           0.0205 |           0.0094 |           0.0000 |
[32m[20221213 20:48:38 @agent_ppo2.py:185][0m |          -0.0078 |           0.0093 |           0.0000 |
[32m[20221213 20:48:38 @agent_ppo2.py:185][0m |          -0.0248 |           0.0092 |           0.0000 |
[32m[20221213 20:48:38 @agent_ppo2.py:185][0m |          -0.0258 |           0.0091 |           0.0000 |
[32m[20221213 20:48:38 @agent_ppo2.py:185][0m |          -0.0341 |           0.0090 |           0.0000 |
[32m[20221213 20:48:38 @agent_ppo2.py:185][0m |          -0.0433 |           0.0089 |           0.0000 |
[32m[20221213 20:48:38 @agent_ppo2.py:185][0m |          -0.0502 |           0.0089 |           0.0000 |
[32m[20221213 20:48:38 @agent_ppo2.py:185][0m |          -0.0470 |           0.0089 |           0.0000 |
[32m[20221213 20:48:38 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:48:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.05
[32m[20221213 20:48:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.31
[32m[20221213 20:48:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.31
[32m[20221213 20:48:39 @agent_ppo2.py:143][0m Total time:      19.49 min
[32m[20221213 20:48:39 @agent_ppo2.py:145][0m 2789376 total steps have happened
[32m[20221213 20:48:39 @agent_ppo2.py:121][0m #------------------------ Iteration 681 --------------------------#
[32m[20221213 20:48:39 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:39 @agent_ppo2.py:185][0m |           0.0288 |           0.0226 |           0.0000 |
[32m[20221213 20:48:39 @agent_ppo2.py:185][0m |           0.0079 |           0.0144 |           0.0000 |
[32m[20221213 20:48:40 @agent_ppo2.py:185][0m |          -0.0077 |           0.0144 |           0.0000 |
[32m[20221213 20:48:40 @agent_ppo2.py:185][0m |          -0.0128 |           0.0133 |           0.0000 |
[32m[20221213 20:48:40 @agent_ppo2.py:185][0m |          -0.0151 |           0.0132 |           0.0000 |
[32m[20221213 20:48:40 @agent_ppo2.py:185][0m |          -0.0154 |           0.0130 |           0.0000 |
[32m[20221213 20:48:40 @agent_ppo2.py:185][0m |          -0.0195 |           0.0130 |           0.0000 |
[32m[20221213 20:48:40 @agent_ppo2.py:185][0m |          -0.0208 |           0.0125 |           0.0000 |
[32m[20221213 20:48:40 @agent_ppo2.py:185][0m |          -0.0216 |           0.0124 |           0.0000 |
[32m[20221213 20:48:40 @agent_ppo2.py:185][0m |          -0.0227 |           0.0124 |           0.0000 |
[32m[20221213 20:48:40 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:48:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.33
[32m[20221213 20:48:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.05
[32m[20221213 20:48:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.08
[32m[20221213 20:48:40 @agent_ppo2.py:143][0m Total time:      19.52 min
[32m[20221213 20:48:40 @agent_ppo2.py:145][0m 2793472 total steps have happened
[32m[20221213 20:48:40 @agent_ppo2.py:121][0m #------------------------ Iteration 682 --------------------------#
[32m[20221213 20:48:41 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:41 @agent_ppo2.py:185][0m |           0.0426 |           0.0134 |           0.0000 |
[32m[20221213 20:48:41 @agent_ppo2.py:185][0m |           0.0199 |           0.0104 |           0.0000 |
[32m[20221213 20:48:41 @agent_ppo2.py:185][0m |           0.0191 |           0.0105 |           0.0000 |
[32m[20221213 20:48:41 @agent_ppo2.py:185][0m |          -0.0152 |           0.0102 |           0.0000 |
[32m[20221213 20:48:41 @agent_ppo2.py:185][0m |          -0.0232 |           0.0097 |           0.0000 |
[32m[20221213 20:48:41 @agent_ppo2.py:185][0m |          -0.0285 |           0.0096 |           0.0000 |
[32m[20221213 20:48:41 @agent_ppo2.py:185][0m |          -0.0360 |           0.0095 |           0.0000 |
[32m[20221213 20:48:42 @agent_ppo2.py:185][0m |          -0.0371 |           0.0094 |           0.0000 |
[32m[20221213 20:48:42 @agent_ppo2.py:185][0m |          -0.0416 |           0.0093 |           0.0000 |
[32m[20221213 20:48:42 @agent_ppo2.py:185][0m |          -0.0431 |           0.0093 |           0.0000 |
[32m[20221213 20:48:42 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:48:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.35
[32m[20221213 20:48:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.67
[32m[20221213 20:48:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.33
[32m[20221213 20:48:42 @agent_ppo2.py:143][0m Total time:      19.55 min
[32m[20221213 20:48:42 @agent_ppo2.py:145][0m 2797568 total steps have happened
[32m[20221213 20:48:42 @agent_ppo2.py:121][0m #------------------------ Iteration 683 --------------------------#
[32m[20221213 20:48:42 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:43 @agent_ppo2.py:185][0m |           0.0346 |           0.0118 |           0.0000 |
[32m[20221213 20:48:43 @agent_ppo2.py:185][0m |          -0.0049 |           0.0099 |           0.0000 |
[32m[20221213 20:48:43 @agent_ppo2.py:185][0m |          -0.0205 |           0.0097 |           0.0000 |
[32m[20221213 20:48:43 @agent_ppo2.py:185][0m |          -0.0274 |           0.0095 |           0.0000 |
[32m[20221213 20:48:43 @agent_ppo2.py:185][0m |          -0.0354 |           0.0094 |           0.0000 |
[32m[20221213 20:48:43 @agent_ppo2.py:185][0m |          -0.0360 |           0.0093 |           0.0000 |
[32m[20221213 20:48:43 @agent_ppo2.py:185][0m |          -0.0361 |           0.0092 |           0.0000 |
[32m[20221213 20:48:43 @agent_ppo2.py:185][0m |          -0.0450 |           0.0092 |           0.0000 |
[32m[20221213 20:48:43 @agent_ppo2.py:185][0m |          -0.0465 |           0.0092 |           0.0000 |
[32m[20221213 20:48:43 @agent_ppo2.py:185][0m |          -0.0437 |           0.0091 |           0.0000 |
[32m[20221213 20:48:43 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:48:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.31
[32m[20221213 20:48:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.73
[32m[20221213 20:48:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.36
[32m[20221213 20:48:44 @agent_ppo2.py:143][0m Total time:      19.58 min
[32m[20221213 20:48:44 @agent_ppo2.py:145][0m 2801664 total steps have happened
[32m[20221213 20:48:44 @agent_ppo2.py:121][0m #------------------------ Iteration 684 --------------------------#
[32m[20221213 20:48:44 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:44 @agent_ppo2.py:185][0m |           0.0246 |           0.0098 |           0.0000 |
[32m[20221213 20:48:44 @agent_ppo2.py:185][0m |           0.0163 |           0.0074 |           0.0000 |
[32m[20221213 20:48:45 @agent_ppo2.py:185][0m |          -0.0021 |           0.0073 |           0.0000 |
[32m[20221213 20:48:45 @agent_ppo2.py:185][0m |          -0.0105 |           0.0073 |           0.0000 |
[32m[20221213 20:48:45 @agent_ppo2.py:185][0m |          -0.0184 |           0.0072 |           0.0000 |
[32m[20221213 20:48:45 @agent_ppo2.py:185][0m |          -0.0080 |           0.0072 |           0.0000 |
[32m[20221213 20:48:45 @agent_ppo2.py:185][0m |          -0.0241 |           0.0071 |           0.0000 |
[32m[20221213 20:48:45 @agent_ppo2.py:185][0m |          -0.0360 |           0.0070 |           0.0000 |
[32m[20221213 20:48:45 @agent_ppo2.py:185][0m |          -0.0391 |           0.0070 |           0.0000 |
[32m[20221213 20:48:45 @agent_ppo2.py:185][0m |          -0.0425 |           0.0069 |           0.0000 |
[32m[20221213 20:48:45 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:48:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.00
[32m[20221213 20:48:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.09
[32m[20221213 20:48:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.90
[32m[20221213 20:48:45 @agent_ppo2.py:143][0m Total time:      19.60 min
[32m[20221213 20:48:45 @agent_ppo2.py:145][0m 2805760 total steps have happened
[32m[20221213 20:48:45 @agent_ppo2.py:121][0m #------------------------ Iteration 685 --------------------------#
[32m[20221213 20:48:46 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:48:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:46 @agent_ppo2.py:185][0m |           0.0348 |           0.1350 |           0.0000 |
[32m[20221213 20:48:46 @agent_ppo2.py:185][0m |           0.0094 |           0.0404 |           0.0000 |
[32m[20221213 20:48:46 @agent_ppo2.py:185][0m |          -0.0058 |           0.0284 |           0.0000 |
[32m[20221213 20:48:46 @agent_ppo2.py:185][0m |          -0.0158 |           0.0250 |           0.0000 |
[32m[20221213 20:48:46 @agent_ppo2.py:185][0m |          -0.0199 |           0.0233 |           0.0000 |
[32m[20221213 20:48:46 @agent_ppo2.py:185][0m |          -0.0264 |           0.0225 |           0.0000 |
[32m[20221213 20:48:47 @agent_ppo2.py:185][0m |          -0.0253 |           0.0252 |           0.0000 |
[32m[20221213 20:48:47 @agent_ppo2.py:185][0m |          -0.0175 |           0.0211 |           0.0000 |
[32m[20221213 20:48:47 @agent_ppo2.py:185][0m |          -0.0532 |           0.0201 |           0.0000 |
[32m[20221213 20:48:47 @agent_ppo2.py:185][0m |          -0.0217 |           0.0194 |           0.0000 |
[32m[20221213 20:48:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:48:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.42
[32m[20221213 20:48:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.68
[32m[20221213 20:48:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.93
[32m[20221213 20:48:47 @agent_ppo2.py:143][0m Total time:      19.63 min
[32m[20221213 20:48:47 @agent_ppo2.py:145][0m 2809856 total steps have happened
[32m[20221213 20:48:47 @agent_ppo2.py:121][0m #------------------------ Iteration 686 --------------------------#
[32m[20221213 20:48:48 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:48 @agent_ppo2.py:185][0m |           0.0400 |           0.0291 |           0.0000 |
[32m[20221213 20:48:48 @agent_ppo2.py:185][0m |           0.0031 |           0.0135 |           0.0000 |
[32m[20221213 20:48:48 @agent_ppo2.py:185][0m |          -0.0186 |           0.0127 |           0.0000 |
[32m[20221213 20:48:48 @agent_ppo2.py:185][0m |          -0.0209 |           0.0123 |           0.0000 |
[32m[20221213 20:48:48 @agent_ppo2.py:185][0m |          -0.0306 |           0.0121 |           0.0000 |
[32m[20221213 20:48:48 @agent_ppo2.py:185][0m |          -0.0372 |           0.0121 |           0.0000 |
[32m[20221213 20:48:48 @agent_ppo2.py:185][0m |          -0.0405 |           0.0119 |           0.0000 |
[32m[20221213 20:48:48 @agent_ppo2.py:185][0m |          -0.0455 |           0.0118 |           0.0000 |
[32m[20221213 20:48:48 @agent_ppo2.py:185][0m |          -0.0516 |           0.0118 |           0.0000 |
[32m[20221213 20:48:48 @agent_ppo2.py:185][0m |          -0.0524 |           0.0118 |           0.0000 |
[32m[20221213 20:48:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:48:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.88
[32m[20221213 20:48:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.31
[32m[20221213 20:48:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.51
[32m[20221213 20:48:49 @agent_ppo2.py:143][0m Total time:      19.66 min
[32m[20221213 20:48:49 @agent_ppo2.py:145][0m 2813952 total steps have happened
[32m[20221213 20:48:49 @agent_ppo2.py:121][0m #------------------------ Iteration 687 --------------------------#
[32m[20221213 20:48:49 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:49 @agent_ppo2.py:185][0m |           0.0344 |           0.0207 |           0.0000 |
[32m[20221213 20:48:49 @agent_ppo2.py:185][0m |           0.0005 |           0.0179 |           0.0000 |
[32m[20221213 20:48:50 @agent_ppo2.py:185][0m |          -0.0197 |           0.0173 |           0.0000 |
[32m[20221213 20:48:50 @agent_ppo2.py:185][0m |          -0.0861 |           0.0185 |           0.0000 |
[32m[20221213 20:48:50 @agent_ppo2.py:185][0m |          -0.0388 |           0.0199 |           0.0000 |
[32m[20221213 20:48:50 @agent_ppo2.py:185][0m |          -0.0426 |           0.0170 |           0.0000 |
[32m[20221213 20:48:50 @agent_ppo2.py:185][0m |          -0.0387 |           0.0148 |           0.0000 |
[32m[20221213 20:48:50 @agent_ppo2.py:185][0m |          -0.0482 |           0.0144 |           0.0000 |
[32m[20221213 20:48:50 @agent_ppo2.py:185][0m |          -0.0804 |           0.0141 |           0.0000 |
[32m[20221213 20:48:50 @agent_ppo2.py:185][0m |          -0.0460 |           0.0140 |           0.0000 |
[32m[20221213 20:48:50 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:48:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.95
[32m[20221213 20:48:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.84
[32m[20221213 20:48:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.89
[32m[20221213 20:48:50 @agent_ppo2.py:143][0m Total time:      19.69 min
[32m[20221213 20:48:50 @agent_ppo2.py:145][0m 2818048 total steps have happened
[32m[20221213 20:48:50 @agent_ppo2.py:121][0m #------------------------ Iteration 688 --------------------------#
[32m[20221213 20:48:51 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:51 @agent_ppo2.py:185][0m |           0.0301 |           0.0190 |           0.0000 |
[32m[20221213 20:48:51 @agent_ppo2.py:185][0m |          -0.0024 |           0.0178 |           0.0000 |
[32m[20221213 20:48:51 @agent_ppo2.py:185][0m |          -0.0348 |           0.0168 |           0.0000 |
[32m[20221213 20:48:51 @agent_ppo2.py:185][0m |          -0.0331 |           0.0163 |           0.0000 |
[32m[20221213 20:48:51 @agent_ppo2.py:185][0m |          -0.0402 |           0.0159 |           0.0000 |
[32m[20221213 20:48:51 @agent_ppo2.py:185][0m |          -0.0538 |           0.0157 |           0.0000 |
[32m[20221213 20:48:52 @agent_ppo2.py:185][0m |          -0.0476 |           0.0154 |           0.0000 |
[32m[20221213 20:48:52 @agent_ppo2.py:185][0m |          -0.0585 |           0.0152 |           0.0000 |
[32m[20221213 20:48:52 @agent_ppo2.py:185][0m |          -0.1198 |           0.0183 |           0.0000 |
[32m[20221213 20:48:52 @agent_ppo2.py:185][0m |          -0.0591 |           0.0189 |           0.0000 |
[32m[20221213 20:48:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:48:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.79
[32m[20221213 20:48:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.78
[32m[20221213 20:48:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.44
[32m[20221213 20:48:52 @agent_ppo2.py:143][0m Total time:      19.72 min
[32m[20221213 20:48:52 @agent_ppo2.py:145][0m 2822144 total steps have happened
[32m[20221213 20:48:52 @agent_ppo2.py:121][0m #------------------------ Iteration 689 --------------------------#
[32m[20221213 20:48:53 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:48:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:53 @agent_ppo2.py:185][0m |           0.0378 |           0.0342 |           0.0000 |
[32m[20221213 20:48:53 @agent_ppo2.py:185][0m |           0.0012 |           0.0209 |           0.0000 |
[32m[20221213 20:48:53 @agent_ppo2.py:185][0m |          -0.0121 |           0.0203 |           0.0000 |
[32m[20221213 20:48:53 @agent_ppo2.py:185][0m |          -0.0213 |           0.0223 |           0.0000 |
[32m[20221213 20:48:53 @agent_ppo2.py:185][0m |          -0.0304 |           0.0182 |           0.0000 |
[32m[20221213 20:48:53 @agent_ppo2.py:185][0m |          -0.0335 |           0.0175 |           0.0000 |
[32m[20221213 20:48:53 @agent_ppo2.py:185][0m |          -0.0346 |           0.0171 |           0.0000 |
[32m[20221213 20:48:53 @agent_ppo2.py:185][0m |          -0.0379 |           0.0167 |           0.0000 |
[32m[20221213 20:48:53 @agent_ppo2.py:185][0m |          -0.0322 |           0.0168 |           0.0000 |
[32m[20221213 20:48:54 @agent_ppo2.py:185][0m |          -0.0408 |           0.0172 |           0.0000 |
[32m[20221213 20:48:54 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:48:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.85
[32m[20221213 20:48:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.40
[32m[20221213 20:48:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.08
[32m[20221213 20:48:54 @agent_ppo2.py:143][0m Total time:      19.75 min
[32m[20221213 20:48:54 @agent_ppo2.py:145][0m 2826240 total steps have happened
[32m[20221213 20:48:54 @agent_ppo2.py:121][0m #------------------------ Iteration 690 --------------------------#
[32m[20221213 20:48:54 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:48:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:54 @agent_ppo2.py:185][0m |           0.0482 |           0.0224 |           0.0000 |
[32m[20221213 20:48:55 @agent_ppo2.py:185][0m |          -0.0016 |           0.0147 |           0.0000 |
[32m[20221213 20:48:55 @agent_ppo2.py:185][0m |          -0.0059 |           0.0163 |           0.0000 |
[32m[20221213 20:48:55 @agent_ppo2.py:185][0m |          -0.0389 |           0.0154 |           0.0000 |
[32m[20221213 20:48:55 @agent_ppo2.py:185][0m |          -0.0395 |           0.0142 |           0.0000 |
[32m[20221213 20:48:55 @agent_ppo2.py:185][0m |          -0.0461 |           0.0140 |           0.0000 |
[32m[20221213 20:48:55 @agent_ppo2.py:185][0m |          -0.0549 |           0.0137 |           0.0000 |
[32m[20221213 20:48:55 @agent_ppo2.py:185][0m |          -0.0538 |           0.0137 |           0.0000 |
[32m[20221213 20:48:55 @agent_ppo2.py:185][0m |          -0.0619 |           0.0140 |           0.0000 |
[32m[20221213 20:48:55 @agent_ppo2.py:185][0m |          -0.0234 |           0.0148 |           0.0000 |
[32m[20221213 20:48:55 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:48:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.78
[32m[20221213 20:48:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.52
[32m[20221213 20:48:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.97
[32m[20221213 20:48:56 @agent_ppo2.py:143][0m Total time:      19.77 min
[32m[20221213 20:48:56 @agent_ppo2.py:145][0m 2830336 total steps have happened
[32m[20221213 20:48:56 @agent_ppo2.py:121][0m #------------------------ Iteration 691 --------------------------#
[32m[20221213 20:48:56 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:48:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:56 @agent_ppo2.py:185][0m |          -0.0358 |           0.0209 |           0.0000 |
[32m[20221213 20:48:56 @agent_ppo2.py:185][0m |          -0.0033 |           0.0187 |           0.0000 |
[32m[20221213 20:48:56 @agent_ppo2.py:185][0m |          -0.0288 |           0.0139 |           0.0000 |
[32m[20221213 20:48:56 @agent_ppo2.py:185][0m |          -0.0435 |           0.0135 |           0.0000 |
[32m[20221213 20:48:57 @agent_ppo2.py:185][0m |          -0.0445 |           0.0133 |           0.0000 |
[32m[20221213 20:48:57 @agent_ppo2.py:185][0m |          -0.0510 |           0.0132 |           0.0000 |
[32m[20221213 20:48:57 @agent_ppo2.py:185][0m |          -0.0537 |           0.0130 |           0.0000 |
[32m[20221213 20:48:57 @agent_ppo2.py:185][0m |          -0.0584 |           0.0131 |           0.0000 |
[32m[20221213 20:48:57 @agent_ppo2.py:185][0m |          -0.0590 |           0.0129 |           0.0000 |
[32m[20221213 20:48:57 @agent_ppo2.py:185][0m |          -0.0626 |           0.0129 |           0.0000 |
[32m[20221213 20:48:57 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 20:48:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.20
[32m[20221213 20:48:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.29
[32m[20221213 20:48:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.79
[32m[20221213 20:48:57 @agent_ppo2.py:143][0m Total time:      19.80 min
[32m[20221213 20:48:57 @agent_ppo2.py:145][0m 2834432 total steps have happened
[32m[20221213 20:48:57 @agent_ppo2.py:121][0m #------------------------ Iteration 692 --------------------------#
[32m[20221213 20:48:58 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:48:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:48:58 @agent_ppo2.py:185][0m |           0.0262 |           0.0202 |           0.0000 |
[32m[20221213 20:48:58 @agent_ppo2.py:185][0m |          -0.0068 |           0.0102 |           0.0000 |
[32m[20221213 20:48:58 @agent_ppo2.py:185][0m |          -0.0109 |           0.0099 |           0.0000 |
[32m[20221213 20:48:58 @agent_ppo2.py:185][0m |          -0.0279 |           0.0098 |           0.0000 |
[32m[20221213 20:48:58 @agent_ppo2.py:185][0m |          -0.0344 |           0.0097 |           0.0000 |
[32m[20221213 20:48:58 @agent_ppo2.py:185][0m |          -0.0357 |           0.0096 |           0.0000 |
[32m[20221213 20:48:58 @agent_ppo2.py:185][0m |          -0.0424 |           0.0095 |           0.0000 |
[32m[20221213 20:48:59 @agent_ppo2.py:185][0m |          -0.0438 |           0.0095 |           0.0000 |
[32m[20221213 20:48:59 @agent_ppo2.py:185][0m |          -0.0431 |           0.0094 |           0.0000 |
[32m[20221213 20:48:59 @agent_ppo2.py:185][0m |          -0.0450 |           0.0093 |           0.0000 |
[32m[20221213 20:48:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:48:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.99
[32m[20221213 20:48:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.51
[32m[20221213 20:48:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.98
[32m[20221213 20:48:59 @agent_ppo2.py:143][0m Total time:      19.83 min
[32m[20221213 20:48:59 @agent_ppo2.py:145][0m 2838528 total steps have happened
[32m[20221213 20:48:59 @agent_ppo2.py:121][0m #------------------------ Iteration 693 --------------------------#
[32m[20221213 20:49:00 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:49:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:00 @agent_ppo2.py:185][0m |           0.0772 |           0.0106 |           0.0000 |
[32m[20221213 20:49:00 @agent_ppo2.py:185][0m |           0.0228 |           0.0075 |           0.0000 |
[32m[20221213 20:49:00 @agent_ppo2.py:185][0m |           0.0107 |           0.0071 |           0.0000 |
[32m[20221213 20:49:00 @agent_ppo2.py:185][0m |           0.0048 |           0.0069 |           0.0000 |
[32m[20221213 20:49:00 @agent_ppo2.py:185][0m |           0.0011 |           0.0068 |           0.0000 |
[32m[20221213 20:49:00 @agent_ppo2.py:185][0m |          -0.0079 |           0.0067 |           0.0000 |
[32m[20221213 20:49:00 @agent_ppo2.py:185][0m |          -0.0149 |           0.0066 |           0.0000 |
[32m[20221213 20:49:00 @agent_ppo2.py:185][0m |          -0.0089 |           0.0066 |           0.0000 |
[32m[20221213 20:49:00 @agent_ppo2.py:185][0m |          -0.0125 |           0.0065 |           0.0000 |
[32m[20221213 20:49:00 @agent_ppo2.py:185][0m |          -0.0217 |           0.0064 |           0.0000 |
[32m[20221213 20:49:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:49:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.70
[32m[20221213 20:49:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.88
[32m[20221213 20:49:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 12.88
[32m[20221213 20:49:01 @agent_ppo2.py:143][0m Total time:      19.86 min
[32m[20221213 20:49:01 @agent_ppo2.py:145][0m 2842624 total steps have happened
[32m[20221213 20:49:01 @agent_ppo2.py:121][0m #------------------------ Iteration 694 --------------------------#
[32m[20221213 20:49:01 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:49:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:01 @agent_ppo2.py:185][0m |           0.0287 |           0.0729 |           0.0000 |
[32m[20221213 20:49:02 @agent_ppo2.py:185][0m |           0.0023 |           0.0319 |           0.0000 |
[32m[20221213 20:49:02 @agent_ppo2.py:185][0m |          -0.0144 |           0.0211 |           0.0000 |
[32m[20221213 20:49:02 @agent_ppo2.py:185][0m |          -0.0197 |           0.0176 |           0.0000 |
[32m[20221213 20:49:02 @agent_ppo2.py:185][0m |          -0.0212 |           0.0170 |           0.0000 |
[32m[20221213 20:49:02 @agent_ppo2.py:185][0m |          -0.0177 |           0.0163 |           0.0000 |
[32m[20221213 20:49:02 @agent_ppo2.py:185][0m |          -0.0230 |           0.0159 |           0.0000 |
[32m[20221213 20:49:02 @agent_ppo2.py:185][0m |          -0.0239 |           0.0156 |           0.0000 |
[32m[20221213 20:49:02 @agent_ppo2.py:185][0m |          -0.0281 |           0.0154 |           0.0000 |
[32m[20221213 20:49:02 @agent_ppo2.py:185][0m |          -0.0289 |           0.0151 |           0.0000 |
[32m[20221213 20:49:02 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:49:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.91
[32m[20221213 20:49:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.25
[32m[20221213 20:49:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.84
[32m[20221213 20:49:03 @agent_ppo2.py:143][0m Total time:      19.89 min
[32m[20221213 20:49:03 @agent_ppo2.py:145][0m 2846720 total steps have happened
[32m[20221213 20:49:03 @agent_ppo2.py:121][0m #------------------------ Iteration 695 --------------------------#
[32m[20221213 20:49:03 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:49:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:03 @agent_ppo2.py:185][0m |           0.0306 |           0.0188 |           0.0000 |
[32m[20221213 20:49:03 @agent_ppo2.py:185][0m |          -0.0102 |           0.0151 |           0.0000 |
[32m[20221213 20:49:03 @agent_ppo2.py:185][0m |          -0.0565 |           0.0143 |           0.0000 |
[32m[20221213 20:49:04 @agent_ppo2.py:185][0m |          -0.0359 |           0.0139 |           0.0000 |
[32m[20221213 20:49:04 @agent_ppo2.py:185][0m |          -0.0434 |           0.0136 |           0.0000 |
[32m[20221213 20:49:04 @agent_ppo2.py:185][0m |          -0.0492 |           0.0131 |           0.0000 |
[32m[20221213 20:49:04 @agent_ppo2.py:185][0m |          -0.0484 |           0.0129 |           0.0000 |
[32m[20221213 20:49:04 @agent_ppo2.py:185][0m |          -0.0478 |           0.0129 |           0.0000 |
[32m[20221213 20:49:04 @agent_ppo2.py:185][0m |          -0.0564 |           0.0127 |           0.0000 |
[32m[20221213 20:49:04 @agent_ppo2.py:185][0m |          -0.0604 |           0.0125 |           0.0000 |
[32m[20221213 20:49:04 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 20:49:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.32
[32m[20221213 20:49:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.55
[32m[20221213 20:49:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.38
[32m[20221213 20:49:04 @agent_ppo2.py:143][0m Total time:      19.92 min
[32m[20221213 20:49:04 @agent_ppo2.py:145][0m 2850816 total steps have happened
[32m[20221213 20:49:04 @agent_ppo2.py:121][0m #------------------------ Iteration 696 --------------------------#
[32m[20221213 20:49:05 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:49:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:05 @agent_ppo2.py:185][0m |           0.0657 |           0.0149 |           0.0000 |
[32m[20221213 20:49:05 @agent_ppo2.py:185][0m |           0.0121 |           0.0116 |           0.0000 |
[32m[20221213 20:49:05 @agent_ppo2.py:185][0m |          -0.0282 |           0.0117 |           0.0000 |
[32m[20221213 20:49:05 @agent_ppo2.py:185][0m |          -0.0253 |           0.0114 |           0.0000 |
[32m[20221213 20:49:05 @agent_ppo2.py:185][0m |          -0.0322 |           0.0109 |           0.0000 |
[32m[20221213 20:49:06 @agent_ppo2.py:185][0m |          -0.0369 |           0.0108 |           0.0000 |
[32m[20221213 20:49:06 @agent_ppo2.py:185][0m |          -0.0217 |           0.0108 |           0.0000 |
[32m[20221213 20:49:06 @agent_ppo2.py:185][0m |          -0.0351 |           0.0109 |           0.0000 |
[32m[20221213 20:49:06 @agent_ppo2.py:185][0m |          -0.0519 |           0.0105 |           0.0000 |
[32m[20221213 20:49:06 @agent_ppo2.py:185][0m |          -0.0591 |           0.0103 |           0.0000 |
[32m[20221213 20:49:06 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:49:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.57
[32m[20221213 20:49:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.08
[32m[20221213 20:49:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.98
[32m[20221213 20:49:06 @agent_ppo2.py:143][0m Total time:      19.95 min
[32m[20221213 20:49:06 @agent_ppo2.py:145][0m 2854912 total steps have happened
[32m[20221213 20:49:06 @agent_ppo2.py:121][0m #------------------------ Iteration 697 --------------------------#
[32m[20221213 20:49:07 @agent_ppo2.py:127][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 20:49:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:07 @agent_ppo2.py:185][0m |           0.0337 |           0.0176 |           0.0000 |
[32m[20221213 20:49:07 @agent_ppo2.py:185][0m |           0.0193 |           0.0166 |           0.0000 |
[32m[20221213 20:49:07 @agent_ppo2.py:185][0m |          -0.0104 |           0.0170 |           0.0000 |
[32m[20221213 20:49:07 @agent_ppo2.py:185][0m |          -0.0145 |           0.0148 |           0.0000 |
[32m[20221213 20:49:07 @agent_ppo2.py:185][0m |          -0.0211 |           0.0150 |           0.0000 |
[32m[20221213 20:49:07 @agent_ppo2.py:185][0m |          -0.0208 |           0.0145 |           0.0000 |
[32m[20221213 20:49:07 @agent_ppo2.py:185][0m |          -0.0317 |           0.0144 |           0.0000 |
[32m[20221213 20:49:07 @agent_ppo2.py:185][0m |          -0.0374 |           0.0147 |           0.0000 |
[32m[20221213 20:49:08 @agent_ppo2.py:185][0m |          -0.0371 |           0.0138 |           0.0000 |
[32m[20221213 20:49:08 @agent_ppo2.py:185][0m |          -0.0398 |           0.0138 |           0.0000 |
[32m[20221213 20:49:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:49:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.40
[32m[20221213 20:49:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.50
[32m[20221213 20:49:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.42
[32m[20221213 20:49:08 @agent_ppo2.py:143][0m Total time:      19.98 min
[32m[20221213 20:49:08 @agent_ppo2.py:145][0m 2859008 total steps have happened
[32m[20221213 20:49:08 @agent_ppo2.py:121][0m #------------------------ Iteration 698 --------------------------#
[32m[20221213 20:49:08 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:49:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:09 @agent_ppo2.py:185][0m |           0.0394 |           0.0368 |           0.0000 |
[32m[20221213 20:49:09 @agent_ppo2.py:185][0m |           0.0030 |           0.0225 |           0.0000 |
[32m[20221213 20:49:09 @agent_ppo2.py:185][0m |          -0.0167 |           0.0201 |           0.0000 |
[32m[20221213 20:49:09 @agent_ppo2.py:185][0m |          -0.0224 |           0.0187 |           0.0000 |
[32m[20221213 20:49:09 @agent_ppo2.py:185][0m |          -0.0299 |           0.0181 |           0.0000 |
[32m[20221213 20:49:09 @agent_ppo2.py:185][0m |          -0.0324 |           0.0180 |           0.0000 |
[32m[20221213 20:49:09 @agent_ppo2.py:185][0m |          -0.0349 |           0.0174 |           0.0000 |
[32m[20221213 20:49:09 @agent_ppo2.py:185][0m |          -0.0347 |           0.0176 |           0.0000 |
[32m[20221213 20:49:09 @agent_ppo2.py:185][0m |          -0.0389 |           0.0173 |           0.0000 |
[32m[20221213 20:49:09 @agent_ppo2.py:185][0m |          -0.0375 |           0.0171 |           0.0000 |
[32m[20221213 20:49:09 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 20:49:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.44
[32m[20221213 20:49:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.20
[32m[20221213 20:49:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.20
[32m[20221213 20:49:10 @agent_ppo2.py:143][0m Total time:      20.01 min
[32m[20221213 20:49:10 @agent_ppo2.py:145][0m 2863104 total steps have happened
[32m[20221213 20:49:10 @agent_ppo2.py:121][0m #------------------------ Iteration 699 --------------------------#
[32m[20221213 20:49:10 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:49:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:10 @agent_ppo2.py:185][0m |           0.0475 |           0.0272 |           0.0000 |
[32m[20221213 20:49:10 @agent_ppo2.py:185][0m |           0.0156 |           0.0249 |           0.0000 |
[32m[20221213 20:49:10 @agent_ppo2.py:185][0m |          -0.0173 |           0.0229 |           0.0000 |
[32m[20221213 20:49:10 @agent_ppo2.py:185][0m |          -0.0328 |           0.0211 |           0.0000 |
[32m[20221213 20:49:11 @agent_ppo2.py:185][0m |          -0.0393 |           0.0202 |           0.0000 |
[32m[20221213 20:49:11 @agent_ppo2.py:185][0m |          -0.0197 |           0.0199 |           0.0000 |
[32m[20221213 20:49:11 @agent_ppo2.py:185][0m |          -0.0486 |           0.0201 |           0.0000 |
[32m[20221213 20:49:11 @agent_ppo2.py:185][0m |          -0.0542 |           0.0187 |           0.0000 |
[32m[20221213 20:49:11 @agent_ppo2.py:185][0m |          -0.0626 |           0.0183 |           0.0000 |
[32m[20221213 20:49:11 @agent_ppo2.py:185][0m |          -0.0717 |           0.0182 |           0.0000 |
[32m[20221213 20:49:11 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 20:49:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.49
[32m[20221213 20:49:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.60
[32m[20221213 20:49:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.60
[32m[20221213 20:49:11 @agent_ppo2.py:143][0m Total time:      20.04 min
[32m[20221213 20:49:11 @agent_ppo2.py:145][0m 2867200 total steps have happened
[32m[20221213 20:49:11 @agent_ppo2.py:121][0m #------------------------ Iteration 700 --------------------------#
[32m[20221213 20:49:12 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:49:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:12 @agent_ppo2.py:185][0m |           0.0217 |           0.0197 |           0.0000 |
[32m[20221213 20:49:12 @agent_ppo2.py:185][0m |           0.0222 |           0.0127 |           0.0000 |
[32m[20221213 20:49:12 @agent_ppo2.py:185][0m |           0.0144 |           0.0119 |           0.0000 |
[32m[20221213 20:49:12 @agent_ppo2.py:185][0m |          -0.0120 |           0.0115 |           0.0000 |
[32m[20221213 20:49:12 @agent_ppo2.py:185][0m |          -0.0258 |           0.0113 |           0.0000 |
[32m[20221213 20:49:12 @agent_ppo2.py:185][0m |          -0.0376 |           0.0112 |           0.0000 |
[32m[20221213 20:49:12 @agent_ppo2.py:185][0m |          -0.0379 |           0.0110 |           0.0000 |
[32m[20221213 20:49:12 @agent_ppo2.py:185][0m |          -0.0449 |           0.0109 |           0.0000 |
[32m[20221213 20:49:12 @agent_ppo2.py:185][0m |          -0.0500 |           0.0116 |           0.0000 |
[32m[20221213 20:49:13 @agent_ppo2.py:185][0m |          -0.0494 |           0.0114 |           0.0000 |
[32m[20221213 20:49:13 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 20:49:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.89
[32m[20221213 20:49:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.31
[32m[20221213 20:49:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.56
[32m[20221213 20:49:13 @agent_ppo2.py:143][0m Total time:      20.06 min
[32m[20221213 20:49:13 @agent_ppo2.py:145][0m 2871296 total steps have happened
[32m[20221213 20:49:13 @agent_ppo2.py:121][0m #------------------------ Iteration 701 --------------------------#
[32m[20221213 20:49:13 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:49:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:14 @agent_ppo2.py:185][0m |           0.0399 |           0.0146 |           0.0000 |
[32m[20221213 20:49:14 @agent_ppo2.py:185][0m |          -0.0093 |           0.0127 |           0.0000 |
[32m[20221213 20:49:14 @agent_ppo2.py:185][0m |          -0.0715 |           0.0131 |           0.0000 |
[32m[20221213 20:49:14 @agent_ppo2.py:185][0m |          -0.0296 |           0.0131 |           0.0000 |
[32m[20221213 20:49:14 @agent_ppo2.py:185][0m |          -0.0310 |           0.0119 |           0.0000 |
[32m[20221213 20:49:14 @agent_ppo2.py:185][0m |          -0.0414 |           0.0117 |           0.0000 |
[32m[20221213 20:49:14 @agent_ppo2.py:185][0m |          -0.0488 |           0.0116 |           0.0000 |
[32m[20221213 20:49:14 @agent_ppo2.py:185][0m |          -0.0423 |           0.0115 |           0.0000 |
[32m[20221213 20:49:14 @agent_ppo2.py:185][0m |          -0.0467 |           0.0113 |           0.0000 |
[32m[20221213 20:49:14 @agent_ppo2.py:185][0m |          -0.0507 |           0.0113 |           0.0000 |
[32m[20221213 20:49:14 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:49:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.30
[32m[20221213 20:49:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.91
[32m[20221213 20:49:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.26
[32m[20221213 20:49:15 @agent_ppo2.py:143][0m Total time:      20.09 min
[32m[20221213 20:49:15 @agent_ppo2.py:145][0m 2875392 total steps have happened
[32m[20221213 20:49:15 @agent_ppo2.py:121][0m #------------------------ Iteration 702 --------------------------#
[32m[20221213 20:49:15 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:49:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:15 @agent_ppo2.py:185][0m |           0.0243 |           0.0143 |           0.0000 |
[32m[20221213 20:49:15 @agent_ppo2.py:185][0m |           0.0010 |           0.0115 |           0.0000 |
[32m[20221213 20:49:15 @agent_ppo2.py:185][0m |          -0.0180 |           0.0108 |           0.0000 |
[32m[20221213 20:49:15 @agent_ppo2.py:185][0m |          -0.0318 |           0.0103 |           0.0000 |
[32m[20221213 20:49:16 @agent_ppo2.py:185][0m |          -0.0314 |           0.0101 |           0.0000 |
[32m[20221213 20:49:16 @agent_ppo2.py:185][0m |          -0.0360 |           0.0098 |           0.0000 |
[32m[20221213 20:49:16 @agent_ppo2.py:185][0m |          -0.0393 |           0.0096 |           0.0000 |
[32m[20221213 20:49:16 @agent_ppo2.py:185][0m |          -0.0229 |           0.0096 |           0.0000 |
[32m[20221213 20:49:16 @agent_ppo2.py:185][0m |          -0.0486 |           0.0094 |           0.0000 |
[32m[20221213 20:49:16 @agent_ppo2.py:185][0m |          -0.0485 |           0.0092 |           0.0000 |
[32m[20221213 20:49:16 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:49:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.71
[32m[20221213 20:49:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.00
[32m[20221213 20:49:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.88
[32m[20221213 20:49:16 @agent_ppo2.py:143][0m Total time:      20.12 min
[32m[20221213 20:49:16 @agent_ppo2.py:145][0m 2879488 total steps have happened
[32m[20221213 20:49:16 @agent_ppo2.py:121][0m #------------------------ Iteration 703 --------------------------#
[32m[20221213 20:49:17 @agent_ppo2.py:127][0m Sampling time: 0.37 s by 5 slaves
[32m[20221213 20:49:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:17 @agent_ppo2.py:185][0m |           0.0467 |           0.0160 |           0.0000 |
[32m[20221213 20:49:17 @agent_ppo2.py:185][0m |           0.0059 |           0.0131 |           0.0000 |
[32m[20221213 20:49:17 @agent_ppo2.py:185][0m |          -0.0136 |           0.0128 |           0.0000 |
[32m[20221213 20:49:17 @agent_ppo2.py:185][0m |          -0.0267 |           0.0126 |           0.0000 |
[32m[20221213 20:49:17 @agent_ppo2.py:185][0m |          -0.0299 |           0.0124 |           0.0000 |
[32m[20221213 20:49:17 @agent_ppo2.py:185][0m |          -0.0327 |           0.0124 |           0.0000 |
[32m[20221213 20:49:17 @agent_ppo2.py:185][0m |          -0.0330 |           0.0120 |           0.0000 |
[32m[20221213 20:49:17 @agent_ppo2.py:185][0m |          -0.0635 |           0.0120 |           0.0000 |
[32m[20221213 20:49:18 @agent_ppo2.py:185][0m |          -0.0380 |           0.0118 |           0.0000 |
[32m[20221213 20:49:18 @agent_ppo2.py:185][0m |          -0.0369 |           0.0118 |           0.0000 |
[32m[20221213 20:49:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:49:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.87
[32m[20221213 20:49:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.95
[32m[20221213 20:49:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.46
[32m[20221213 20:49:18 @agent_ppo2.py:143][0m Total time:      20.15 min
[32m[20221213 20:49:18 @agent_ppo2.py:145][0m 2883584 total steps have happened
[32m[20221213 20:49:18 @agent_ppo2.py:121][0m #------------------------ Iteration 704 --------------------------#
[32m[20221213 20:49:18 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:49:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:19 @agent_ppo2.py:185][0m |           0.0201 |           0.0153 |           0.0000 |
[32m[20221213 20:49:19 @agent_ppo2.py:185][0m |           0.0350 |           0.0079 |           0.0000 |
[32m[20221213 20:49:19 @agent_ppo2.py:185][0m |          -0.0061 |           0.0074 |           0.0000 |
[32m[20221213 20:49:19 @agent_ppo2.py:185][0m |          -0.0156 |           0.0073 |           0.0000 |
[32m[20221213 20:49:19 @agent_ppo2.py:185][0m |          -0.0217 |           0.0072 |           0.0000 |
[32m[20221213 20:49:19 @agent_ppo2.py:185][0m |          -0.0273 |           0.0071 |           0.0000 |
[32m[20221213 20:49:19 @agent_ppo2.py:185][0m |          -0.0288 |           0.0070 |           0.0000 |
[32m[20221213 20:49:20 @agent_ppo2.py:185][0m |          -0.0348 |           0.0069 |           0.0000 |
[32m[20221213 20:49:20 @agent_ppo2.py:185][0m |          -0.0346 |           0.0069 |           0.0000 |
[32m[20221213 20:49:20 @agent_ppo2.py:185][0m |          -0.0342 |           0.0068 |           0.0000 |
[32m[20221213 20:49:20 @agent_ppo2.py:130][0m Policy update time: 1.34 s
[32m[20221213 20:49:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.15
[32m[20221213 20:49:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.50
[32m[20221213 20:49:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.41
[32m[20221213 20:49:20 @agent_ppo2.py:143][0m Total time:      20.19 min
[32m[20221213 20:49:20 @agent_ppo2.py:145][0m 2887680 total steps have happened
[32m[20221213 20:49:20 @agent_ppo2.py:121][0m #------------------------ Iteration 705 --------------------------#
[32m[20221213 20:49:21 @agent_ppo2.py:127][0m Sampling time: 0.42 s by 5 slaves
[32m[20221213 20:49:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:21 @agent_ppo2.py:185][0m |          -0.0101 |           0.0450 |           0.0000 |
[32m[20221213 20:49:21 @agent_ppo2.py:185][0m |           0.0029 |           0.0215 |           0.0000 |
[32m[20221213 20:49:21 @agent_ppo2.py:185][0m |          -0.0065 |           0.0166 |           0.0000 |
[32m[20221213 20:49:21 @agent_ppo2.py:185][0m |          -0.0167 |           0.0151 |           0.0000 |
[32m[20221213 20:49:22 @agent_ppo2.py:185][0m |          -0.0230 |           0.0143 |           0.0000 |
[32m[20221213 20:49:22 @agent_ppo2.py:185][0m |          -0.0264 |           0.0140 |           0.0000 |
[32m[20221213 20:49:22 @agent_ppo2.py:185][0m |          -0.0273 |           0.0138 |           0.0000 |
[32m[20221213 20:49:22 @agent_ppo2.py:185][0m |          -0.0298 |           0.0135 |           0.0000 |
[32m[20221213 20:49:22 @agent_ppo2.py:185][0m |          -0.0301 |           0.0134 |           0.0000 |
[32m[20221213 20:49:22 @agent_ppo2.py:185][0m |          -0.0323 |           0.0134 |           0.0000 |
[32m[20221213 20:49:22 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 20:49:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.08
[32m[20221213 20:49:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.89
[32m[20221213 20:49:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.58
[32m[20221213 20:49:22 @agent_ppo2.py:143][0m Total time:      20.22 min
[32m[20221213 20:49:22 @agent_ppo2.py:145][0m 2891776 total steps have happened
[32m[20221213 20:49:22 @agent_ppo2.py:121][0m #------------------------ Iteration 706 --------------------------#
[32m[20221213 20:49:23 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:49:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:23 @agent_ppo2.py:185][0m |           0.0481 |           0.0203 |           0.0000 |
[32m[20221213 20:49:23 @agent_ppo2.py:185][0m |           0.0128 |           0.0179 |           0.0000 |
[32m[20221213 20:49:23 @agent_ppo2.py:185][0m |          -0.0143 |           0.0165 |           0.0000 |
[32m[20221213 20:49:23 @agent_ppo2.py:185][0m |          -0.0265 |           0.0163 |           0.0000 |
[32m[20221213 20:49:24 @agent_ppo2.py:185][0m |          -0.0252 |           0.0156 |           0.0000 |
[32m[20221213 20:49:24 @agent_ppo2.py:185][0m |          -0.0299 |           0.0151 |           0.0000 |
[32m[20221213 20:49:24 @agent_ppo2.py:185][0m |          -0.0467 |           0.0148 |           0.0000 |
[32m[20221213 20:49:24 @agent_ppo2.py:185][0m |          -0.0454 |           0.0146 |           0.0000 |
[32m[20221213 20:49:24 @agent_ppo2.py:185][0m |          -0.0507 |           0.0147 |           0.0000 |
[32m[20221213 20:49:24 @agent_ppo2.py:185][0m |          -0.0368 |           0.0141 |           0.0000 |
[32m[20221213 20:49:24 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221213 20:49:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.30
[32m[20221213 20:49:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.89
[32m[20221213 20:49:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.68
[32m[20221213 20:49:25 @agent_ppo2.py:143][0m Total time:      20.26 min
[32m[20221213 20:49:25 @agent_ppo2.py:145][0m 2895872 total steps have happened
[32m[20221213 20:49:25 @agent_ppo2.py:121][0m #------------------------ Iteration 707 --------------------------#
[32m[20221213 20:49:25 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:49:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:25 @agent_ppo2.py:185][0m |           0.0270 |           0.0156 |           0.0000 |
[32m[20221213 20:49:25 @agent_ppo2.py:185][0m |           0.0099 |           0.0117 |           0.0000 |
[32m[20221213 20:49:25 @agent_ppo2.py:185][0m |          -0.0083 |           0.0113 |           0.0000 |
[32m[20221213 20:49:26 @agent_ppo2.py:185][0m |          -0.0225 |           0.0112 |           0.0000 |
[32m[20221213 20:49:26 @agent_ppo2.py:185][0m |          -0.0323 |           0.0112 |           0.0000 |
[32m[20221213 20:49:26 @agent_ppo2.py:185][0m |          -0.0388 |           0.0110 |           0.0000 |
[32m[20221213 20:49:26 @agent_ppo2.py:185][0m |          -0.0408 |           0.0109 |           0.0000 |
[32m[20221213 20:49:26 @agent_ppo2.py:185][0m |          -0.0494 |           0.0108 |           0.0000 |
[32m[20221213 20:49:26 @agent_ppo2.py:185][0m |          -0.0514 |           0.0108 |           0.0000 |
[32m[20221213 20:49:26 @agent_ppo2.py:185][0m |          -0.0562 |           0.0107 |           0.0000 |
[32m[20221213 20:49:26 @agent_ppo2.py:130][0m Policy update time: 1.24 s
[32m[20221213 20:49:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.75
[32m[20221213 20:49:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.52
[32m[20221213 20:49:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.40
[32m[20221213 20:49:27 @agent_ppo2.py:143][0m Total time:      20.29 min
[32m[20221213 20:49:27 @agent_ppo2.py:145][0m 2899968 total steps have happened
[32m[20221213 20:49:27 @agent_ppo2.py:121][0m #------------------------ Iteration 708 --------------------------#
[32m[20221213 20:49:27 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:49:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:27 @agent_ppo2.py:185][0m |           0.0472 |           0.0136 |           0.0000 |
[32m[20221213 20:49:27 @agent_ppo2.py:185][0m |           0.0250 |           0.0125 |           0.0000 |
[32m[20221213 20:49:27 @agent_ppo2.py:185][0m |          -0.0017 |           0.0119 |           0.0000 |
[32m[20221213 20:49:28 @agent_ppo2.py:185][0m |          -0.0057 |           0.0119 |           0.0000 |
[32m[20221213 20:49:28 @agent_ppo2.py:185][0m |          -0.0173 |           0.0118 |           0.0000 |
[32m[20221213 20:49:28 @agent_ppo2.py:185][0m |          -0.0363 |           0.0117 |           0.0000 |
[32m[20221213 20:49:28 @agent_ppo2.py:185][0m |          -0.0320 |           0.0117 |           0.0000 |
[32m[20221213 20:49:28 @agent_ppo2.py:185][0m |          -0.0335 |           0.0115 |           0.0000 |
[32m[20221213 20:49:28 @agent_ppo2.py:185][0m |          -0.0430 |           0.0114 |           0.0000 |
[32m[20221213 20:49:28 @agent_ppo2.py:185][0m |          -0.0479 |           0.0113 |           0.0000 |
[32m[20221213 20:49:28 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221213 20:49:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.01
[32m[20221213 20:49:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.62
[32m[20221213 20:49:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.40
[32m[20221213 20:49:29 @agent_ppo2.py:143][0m Total time:      20.33 min
[32m[20221213 20:49:29 @agent_ppo2.py:145][0m 2904064 total steps have happened
[32m[20221213 20:49:29 @agent_ppo2.py:121][0m #------------------------ Iteration 709 --------------------------#
[32m[20221213 20:49:29 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:49:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:29 @agent_ppo2.py:185][0m |           0.0450 |           0.0155 |           0.0000 |
[32m[20221213 20:49:29 @agent_ppo2.py:185][0m |           0.0044 |           0.0081 |           0.0000 |
[32m[20221213 20:49:30 @agent_ppo2.py:185][0m |          -0.0040 |           0.0079 |           0.0000 |
[32m[20221213 20:49:30 @agent_ppo2.py:185][0m |          -0.0150 |           0.0078 |           0.0000 |
[32m[20221213 20:49:30 @agent_ppo2.py:185][0m |          -0.0201 |           0.0077 |           0.0000 |
[32m[20221213 20:49:30 @agent_ppo2.py:185][0m |          -0.0261 |           0.0076 |           0.0000 |
[32m[20221213 20:49:30 @agent_ppo2.py:185][0m |          -0.0165 |           0.0076 |           0.0000 |
[32m[20221213 20:49:30 @agent_ppo2.py:185][0m |          -0.0276 |           0.0075 |           0.0000 |
[32m[20221213 20:49:30 @agent_ppo2.py:185][0m |          -0.0319 |           0.0074 |           0.0000 |
[32m[20221213 20:49:30 @agent_ppo2.py:185][0m |          -0.0325 |           0.0073 |           0.0000 |
[32m[20221213 20:49:30 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221213 20:49:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.63
[32m[20221213 20:49:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.01
[32m[20221213 20:49:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.13
[32m[20221213 20:49:31 @agent_ppo2.py:143][0m Total time:      20.36 min
[32m[20221213 20:49:31 @agent_ppo2.py:145][0m 2908160 total steps have happened
[32m[20221213 20:49:31 @agent_ppo2.py:121][0m #------------------------ Iteration 710 --------------------------#
[32m[20221213 20:49:31 @agent_ppo2.py:127][0m Sampling time: 0.44 s by 5 slaves
[32m[20221213 20:49:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:31 @agent_ppo2.py:185][0m |           0.0252 |           0.1271 |           0.0000 |
[32m[20221213 20:49:32 @agent_ppo2.py:185][0m |           0.0036 |           0.0436 |           0.0000 |
[32m[20221213 20:49:32 @agent_ppo2.py:185][0m |          -0.0030 |           0.0263 |           0.0000 |
[32m[20221213 20:49:32 @agent_ppo2.py:185][0m |          -0.0160 |           0.0229 |           0.0000 |
[32m[20221213 20:49:32 @agent_ppo2.py:185][0m |          -0.0192 |           0.0215 |           0.0000 |
[32m[20221213 20:49:32 @agent_ppo2.py:185][0m |          -0.0213 |           0.0197 |           0.0000 |
[32m[20221213 20:49:32 @agent_ppo2.py:185][0m |          -0.0263 |           0.0191 |           0.0000 |
[32m[20221213 20:49:32 @agent_ppo2.py:185][0m |          -0.0244 |           0.0189 |           0.0000 |
[32m[20221213 20:49:32 @agent_ppo2.py:185][0m |          -0.0532 |           0.0192 |           0.0000 |
[32m[20221213 20:49:32 @agent_ppo2.py:185][0m |          -0.0302 |           0.0180 |           0.0000 |
[32m[20221213 20:49:32 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221213 20:49:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.95
[32m[20221213 20:49:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.08
[32m[20221213 20:49:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.70
[32m[20221213 20:49:33 @agent_ppo2.py:143][0m Total time:      20.39 min
[32m[20221213 20:49:33 @agent_ppo2.py:145][0m 2912256 total steps have happened
[32m[20221213 20:49:33 @agent_ppo2.py:121][0m #------------------------ Iteration 711 --------------------------#
[32m[20221213 20:49:33 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:49:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:33 @agent_ppo2.py:185][0m |           0.0463 |           0.0215 |           0.0000 |
[32m[20221213 20:49:34 @agent_ppo2.py:185][0m |           0.0190 |           0.0184 |           0.0000 |
[32m[20221213 20:49:34 @agent_ppo2.py:185][0m |          -0.0072 |           0.0178 |           0.0000 |
[32m[20221213 20:49:34 @agent_ppo2.py:185][0m |          -0.0294 |           0.0173 |           0.0000 |
[32m[20221213 20:49:34 @agent_ppo2.py:185][0m |          -0.0523 |           0.0169 |           0.0000 |
[32m[20221213 20:49:34 @agent_ppo2.py:185][0m |          -0.0638 |           0.0164 |           0.0000 |
[32m[20221213 20:49:34 @agent_ppo2.py:185][0m |          -0.0689 |           0.0159 |           0.0000 |
[32m[20221213 20:49:34 @agent_ppo2.py:185][0m |          -0.0588 |           0.0157 |           0.0000 |
[32m[20221213 20:49:34 @agent_ppo2.py:185][0m |          -0.0711 |           0.0155 |           0.0000 |
[32m[20221213 20:49:34 @agent_ppo2.py:185][0m |          -0.0812 |           0.0155 |           0.0000 |
[32m[20221213 20:49:34 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221213 20:49:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.77
[32m[20221213 20:49:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.08
[32m[20221213 20:49:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.57
[32m[20221213 20:49:35 @agent_ppo2.py:143][0m Total time:      20.43 min
[32m[20221213 20:49:35 @agent_ppo2.py:145][0m 2916352 total steps have happened
[32m[20221213 20:49:35 @agent_ppo2.py:121][0m #------------------------ Iteration 712 --------------------------#
[32m[20221213 20:49:35 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:49:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:36 @agent_ppo2.py:185][0m |           0.0274 |           0.0488 |           0.0000 |
[32m[20221213 20:49:36 @agent_ppo2.py:185][0m |          -0.0124 |           0.0358 |           0.0000 |
[32m[20221213 20:49:36 @agent_ppo2.py:185][0m |          -0.0277 |           0.0319 |           0.0000 |
[32m[20221213 20:49:36 @agent_ppo2.py:185][0m |          -0.0604 |           0.0299 |           0.0000 |
[32m[20221213 20:49:36 @agent_ppo2.py:185][0m |          -0.0243 |           0.0311 |           0.0000 |
[32m[20221213 20:49:36 @agent_ppo2.py:185][0m |          -0.0264 |           0.0291 |           0.0000 |
[32m[20221213 20:49:36 @agent_ppo2.py:185][0m |          -0.0324 |           0.0264 |           0.0000 |
[32m[20221213 20:49:36 @agent_ppo2.py:185][0m |          -0.0405 |           0.0249 |           0.0000 |
[32m[20221213 20:49:36 @agent_ppo2.py:185][0m |          -0.0380 |           0.0260 |           0.0000 |
[32m[20221213 20:49:37 @agent_ppo2.py:185][0m |          -0.0435 |           0.0239 |           0.0000 |
[32m[20221213 20:49:37 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221213 20:49:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.14
[32m[20221213 20:49:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.97
[32m[20221213 20:49:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.14
[32m[20221213 20:49:37 @agent_ppo2.py:143][0m Total time:      20.46 min
[32m[20221213 20:49:37 @agent_ppo2.py:145][0m 2920448 total steps have happened
[32m[20221213 20:49:37 @agent_ppo2.py:121][0m #------------------------ Iteration 713 --------------------------#
[32m[20221213 20:49:37 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:49:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:38 @agent_ppo2.py:185][0m |           0.0097 |           0.0548 |           0.0000 |
[32m[20221213 20:49:38 @agent_ppo2.py:185][0m |          -0.0104 |           0.0113 |           0.0000 |
[32m[20221213 20:49:38 @agent_ppo2.py:185][0m |          -0.0194 |           0.0104 |           0.0000 |
[32m[20221213 20:49:38 @agent_ppo2.py:185][0m |          -0.0312 |           0.0102 |           0.0000 |
[32m[20221213 20:49:38 @agent_ppo2.py:185][0m |           0.0012 |           0.0101 |           0.0000 |
[32m[20221213 20:49:38 @agent_ppo2.py:185][0m |          -0.0342 |           0.0099 |           0.0000 |
[32m[20221213 20:49:38 @agent_ppo2.py:185][0m |          -0.0379 |           0.0098 |           0.0000 |
[32m[20221213 20:49:38 @agent_ppo2.py:185][0m |          -0.0420 |           0.0098 |           0.0000 |
[32m[20221213 20:49:38 @agent_ppo2.py:185][0m |          -0.0448 |           0.0097 |           0.0000 |
[32m[20221213 20:49:39 @agent_ppo2.py:185][0m |          -0.0449 |           0.0096 |           0.0000 |
[32m[20221213 20:49:39 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 20:49:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.96
[32m[20221213 20:49:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.47
[32m[20221213 20:49:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.77
[32m[20221213 20:49:39 @agent_ppo2.py:143][0m Total time:      20.50 min
[32m[20221213 20:49:39 @agent_ppo2.py:145][0m 2924544 total steps have happened
[32m[20221213 20:49:39 @agent_ppo2.py:121][0m #------------------------ Iteration 714 --------------------------#
[32m[20221213 20:49:39 @agent_ppo2.py:127][0m Sampling time: 0.42 s by 5 slaves
[32m[20221213 20:49:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:40 @agent_ppo2.py:185][0m |           0.0318 |           0.0266 |           0.0000 |
[32m[20221213 20:49:40 @agent_ppo2.py:185][0m |           0.0031 |           0.0136 |           0.0000 |
[32m[20221213 20:49:40 @agent_ppo2.py:185][0m |          -0.0166 |           0.0132 |           0.0000 |
[32m[20221213 20:49:40 @agent_ppo2.py:185][0m |          -0.0171 |           0.0122 |           0.0000 |
[32m[20221213 20:49:40 @agent_ppo2.py:185][0m |          -0.0204 |           0.0120 |           0.0000 |
[32m[20221213 20:49:40 @agent_ppo2.py:185][0m |          -0.0242 |           0.0119 |           0.0000 |
[32m[20221213 20:49:40 @agent_ppo2.py:185][0m |          -0.0279 |           0.0119 |           0.0000 |
[32m[20221213 20:49:40 @agent_ppo2.py:185][0m |          -0.0503 |           0.0118 |           0.0000 |
[32m[20221213 20:49:40 @agent_ppo2.py:185][0m |          -0.0227 |           0.0116 |           0.0000 |
[32m[20221213 20:49:41 @agent_ppo2.py:185][0m |          -0.0290 |           0.0115 |           0.0000 |
[32m[20221213 20:49:41 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 20:49:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.98
[32m[20221213 20:49:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.29
[32m[20221213 20:49:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.35
[32m[20221213 20:49:41 @agent_ppo2.py:143][0m Total time:      20.53 min
[32m[20221213 20:49:41 @agent_ppo2.py:145][0m 2928640 total steps have happened
[32m[20221213 20:49:41 @agent_ppo2.py:121][0m #------------------------ Iteration 715 --------------------------#
[32m[20221213 20:49:41 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:49:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:42 @agent_ppo2.py:185][0m |           0.0493 |           0.0198 |           0.0000 |
[32m[20221213 20:49:42 @agent_ppo2.py:185][0m |          -0.0405 |           0.0168 |           0.0000 |
[32m[20221213 20:49:42 @agent_ppo2.py:185][0m |          -0.0195 |           0.0174 |           0.0000 |
[32m[20221213 20:49:42 @agent_ppo2.py:185][0m |          -0.0274 |           0.0148 |           0.0000 |
[32m[20221213 20:49:42 @agent_ppo2.py:185][0m |          -0.0704 |           0.0144 |           0.0000 |
[32m[20221213 20:49:42 @agent_ppo2.py:185][0m |          -0.0573 |           0.0146 |           0.0000 |
[32m[20221213 20:49:42 @agent_ppo2.py:185][0m |          -0.0405 |           0.0143 |           0.0000 |
[32m[20221213 20:49:42 @agent_ppo2.py:185][0m |          -0.0378 |           0.0137 |           0.0000 |
[32m[20221213 20:49:42 @agent_ppo2.py:185][0m |          -0.0403 |           0.0136 |           0.0000 |
[32m[20221213 20:49:43 @agent_ppo2.py:185][0m |          -0.0386 |           0.0135 |           0.0000 |
[32m[20221213 20:49:43 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 20:49:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.21
[32m[20221213 20:49:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.65
[32m[20221213 20:49:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.08
[32m[20221213 20:49:43 @agent_ppo2.py:143][0m Total time:      20.56 min
[32m[20221213 20:49:43 @agent_ppo2.py:145][0m 2932736 total steps have happened
[32m[20221213 20:49:43 @agent_ppo2.py:121][0m #------------------------ Iteration 716 --------------------------#
[32m[20221213 20:49:43 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:49:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:44 @agent_ppo2.py:185][0m |          -0.0148 |           0.0391 |           0.0000 |
[32m[20221213 20:49:44 @agent_ppo2.py:185][0m |          -0.0110 |           0.0264 |           0.0000 |
[32m[20221213 20:49:44 @agent_ppo2.py:185][0m |          -0.0289 |           0.0238 |           0.0000 |
[32m[20221213 20:49:44 @agent_ppo2.py:185][0m |          -0.0334 |           0.0229 |           0.0000 |
[32m[20221213 20:49:44 @agent_ppo2.py:185][0m |          -0.0344 |           0.0219 |           0.0000 |
[32m[20221213 20:49:44 @agent_ppo2.py:185][0m |          -0.0581 |           0.0213 |           0.0000 |
[32m[20221213 20:49:44 @agent_ppo2.py:185][0m |          -0.0323 |           0.0209 |           0.0000 |
[32m[20221213 20:49:44 @agent_ppo2.py:185][0m |          -0.0524 |           0.0203 |           0.0000 |
[32m[20221213 20:49:44 @agent_ppo2.py:185][0m |          -0.0369 |           0.0200 |           0.0000 |
[32m[20221213 20:49:45 @agent_ppo2.py:185][0m |          -0.0386 |           0.0197 |           0.0000 |
[32m[20221213 20:49:45 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 20:49:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.30
[32m[20221213 20:49:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.78
[32m[20221213 20:49:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.88
[32m[20221213 20:49:45 @agent_ppo2.py:143][0m Total time:      20.60 min
[32m[20221213 20:49:45 @agent_ppo2.py:145][0m 2936832 total steps have happened
[32m[20221213 20:49:45 @agent_ppo2.py:121][0m #------------------------ Iteration 717 --------------------------#
[32m[20221213 20:49:45 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:49:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:46 @agent_ppo2.py:185][0m |           0.0974 |           0.0536 |           0.0000 |
[32m[20221213 20:49:46 @agent_ppo2.py:185][0m |           0.0338 |           0.0115 |           0.0000 |
[32m[20221213 20:49:46 @agent_ppo2.py:185][0m |           0.0515 |           0.0106 |           0.0000 |
[32m[20221213 20:49:46 @agent_ppo2.py:185][0m |           0.0133 |           0.0103 |           0.0000 |
[32m[20221213 20:49:46 @agent_ppo2.py:185][0m |           0.0010 |           0.0101 |           0.0000 |
[32m[20221213 20:49:46 @agent_ppo2.py:185][0m |          -0.0069 |           0.0100 |           0.0000 |
[32m[20221213 20:49:46 @agent_ppo2.py:185][0m |          -0.0117 |           0.0099 |           0.0000 |
[32m[20221213 20:49:46 @agent_ppo2.py:185][0m |          -0.0168 |           0.0098 |           0.0000 |
[32m[20221213 20:49:47 @agent_ppo2.py:185][0m |          -0.0224 |           0.0097 |           0.0000 |
[32m[20221213 20:49:47 @agent_ppo2.py:185][0m |          -0.0273 |           0.0096 |           0.0000 |
[32m[20221213 20:49:47 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 20:49:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.66
[32m[20221213 20:49:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.90
[32m[20221213 20:49:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.85
[32m[20221213 20:49:47 @agent_ppo2.py:143][0m Total time:      20.63 min
[32m[20221213 20:49:47 @agent_ppo2.py:145][0m 2940928 total steps have happened
[32m[20221213 20:49:47 @agent_ppo2.py:121][0m #------------------------ Iteration 718 --------------------------#
[32m[20221213 20:49:47 @agent_ppo2.py:127][0m Sampling time: 0.45 s by 5 slaves
[32m[20221213 20:49:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:48 @agent_ppo2.py:185][0m |           0.0133 |           0.0103 |           0.0000 |
[32m[20221213 20:49:48 @agent_ppo2.py:185][0m |          -0.0161 |           0.0084 |           0.0000 |
[32m[20221213 20:49:48 @agent_ppo2.py:185][0m |          -0.0274 |           0.0082 |           0.0000 |
[32m[20221213 20:49:48 @agent_ppo2.py:185][0m |          -0.0244 |           0.0081 |           0.0000 |
[32m[20221213 20:49:48 @agent_ppo2.py:185][0m |          -0.0370 |           0.0080 |           0.0000 |
[32m[20221213 20:49:48 @agent_ppo2.py:185][0m |          -0.0461 |           0.0079 |           0.0000 |
[32m[20221213 20:49:48 @agent_ppo2.py:185][0m |          -0.0478 |           0.0078 |           0.0000 |
[32m[20221213 20:49:49 @agent_ppo2.py:185][0m |          -0.0466 |           0.0078 |           0.0000 |
[32m[20221213 20:49:49 @agent_ppo2.py:185][0m |          -0.0514 |           0.0077 |           0.0000 |
[32m[20221213 20:49:49 @agent_ppo2.py:185][0m |          -0.0557 |           0.0076 |           0.0000 |
[32m[20221213 20:49:49 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221213 20:49:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.86
[32m[20221213 20:49:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.17
[32m[20221213 20:49:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.17
[32m[20221213 20:49:49 @agent_ppo2.py:143][0m Total time:      20.67 min
[32m[20221213 20:49:49 @agent_ppo2.py:145][0m 2945024 total steps have happened
[32m[20221213 20:49:49 @agent_ppo2.py:121][0m #------------------------ Iteration 719 --------------------------#
[32m[20221213 20:49:50 @agent_ppo2.py:127][0m Sampling time: 0.43 s by 5 slaves
[32m[20221213 20:49:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:50 @agent_ppo2.py:185][0m |           0.0360 |           0.0207 |           0.0000 |
[32m[20221213 20:49:50 @agent_ppo2.py:185][0m |           0.0185 |           0.0115 |           0.0000 |
[32m[20221213 20:49:50 @agent_ppo2.py:185][0m |          -0.0027 |           0.0104 |           0.0000 |
[32m[20221213 20:49:50 @agent_ppo2.py:185][0m |          -0.0083 |           0.0100 |           0.0000 |
[32m[20221213 20:49:50 @agent_ppo2.py:185][0m |          -0.0204 |           0.0099 |           0.0000 |
[32m[20221213 20:49:50 @agent_ppo2.py:185][0m |          -0.0245 |           0.0097 |           0.0000 |
[32m[20221213 20:49:50 @agent_ppo2.py:185][0m |          -0.0202 |           0.0096 |           0.0000 |
[32m[20221213 20:49:51 @agent_ppo2.py:185][0m |          -0.0258 |           0.0095 |           0.0000 |
[32m[20221213 20:49:51 @agent_ppo2.py:185][0m |          -0.0238 |           0.0094 |           0.0000 |
[32m[20221213 20:49:51 @agent_ppo2.py:185][0m |          -0.0258 |           0.0095 |           0.0000 |
[32m[20221213 20:49:51 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 20:49:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.20
[32m[20221213 20:49:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.66
[32m[20221213 20:49:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.35
[32m[20221213 20:49:51 @agent_ppo2.py:143][0m Total time:      20.70 min
[32m[20221213 20:49:51 @agent_ppo2.py:145][0m 2949120 total steps have happened
[32m[20221213 20:49:51 @agent_ppo2.py:121][0m #------------------------ Iteration 720 --------------------------#
[32m[20221213 20:49:52 @agent_ppo2.py:127][0m Sampling time: 0.44 s by 5 slaves
[32m[20221213 20:49:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:52 @agent_ppo2.py:185][0m |           0.0529 |           0.0111 |           0.0000 |
[32m[20221213 20:49:52 @agent_ppo2.py:185][0m |           0.0098 |           0.0078 |           0.0000 |
[32m[20221213 20:49:52 @agent_ppo2.py:185][0m |          -0.0061 |           0.0076 |           0.0000 |
[32m[20221213 20:49:52 @agent_ppo2.py:185][0m |          -0.0178 |           0.0075 |           0.0000 |
[32m[20221213 20:49:52 @agent_ppo2.py:185][0m |          -0.0249 |           0.0075 |           0.0000 |
[32m[20221213 20:49:52 @agent_ppo2.py:185][0m |          -0.0291 |           0.0074 |           0.0000 |
[32m[20221213 20:49:53 @agent_ppo2.py:185][0m |          -0.0343 |           0.0073 |           0.0000 |
[32m[20221213 20:49:53 @agent_ppo2.py:185][0m |          -0.0001 |           0.0076 |           0.0000 |
[32m[20221213 20:49:53 @agent_ppo2.py:185][0m |          -0.0395 |           0.0074 |           0.0000 |
[32m[20221213 20:49:53 @agent_ppo2.py:185][0m |          -0.0420 |           0.0072 |           0.0000 |
[32m[20221213 20:49:53 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221213 20:49:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.65
[32m[20221213 20:49:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.96
[32m[20221213 20:49:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.45
[32m[20221213 20:49:53 @agent_ppo2.py:143][0m Total time:      20.74 min
[32m[20221213 20:49:53 @agent_ppo2.py:145][0m 2953216 total steps have happened
[32m[20221213 20:49:53 @agent_ppo2.py:121][0m #------------------------ Iteration 721 --------------------------#
[32m[20221213 20:49:54 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:49:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:54 @agent_ppo2.py:185][0m |           0.0385 |           0.0509 |           0.0000 |
[32m[20221213 20:49:54 @agent_ppo2.py:185][0m |           0.0028 |           0.0246 |           0.0000 |
[32m[20221213 20:49:54 @agent_ppo2.py:185][0m |          -0.0101 |           0.0188 |           0.0000 |
[32m[20221213 20:49:54 @agent_ppo2.py:185][0m |          -0.0137 |           0.0176 |           0.0000 |
[32m[20221213 20:49:54 @agent_ppo2.py:185][0m |          -0.0100 |           0.0166 |           0.0000 |
[32m[20221213 20:49:54 @agent_ppo2.py:185][0m |          -0.0187 |           0.0156 |           0.0000 |
[32m[20221213 20:49:55 @agent_ppo2.py:185][0m |          -0.0229 |           0.0151 |           0.0000 |
[32m[20221213 20:49:55 @agent_ppo2.py:185][0m |          -0.0263 |           0.0149 |           0.0000 |
[32m[20221213 20:49:55 @agent_ppo2.py:185][0m |          -0.0281 |           0.0150 |           0.0000 |
[32m[20221213 20:49:55 @agent_ppo2.py:185][0m |          -0.0302 |           0.0148 |           0.0000 |
[32m[20221213 20:49:55 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 20:49:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.31
[32m[20221213 20:49:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.81
[32m[20221213 20:49:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.29
[32m[20221213 20:49:55 @agent_ppo2.py:143][0m Total time:      20.77 min
[32m[20221213 20:49:55 @agent_ppo2.py:145][0m 2957312 total steps have happened
[32m[20221213 20:49:55 @agent_ppo2.py:121][0m #------------------------ Iteration 722 --------------------------#
[32m[20221213 20:49:56 @agent_ppo2.py:127][0m Sampling time: 0.41 s by 5 slaves
[32m[20221213 20:49:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:56 @agent_ppo2.py:185][0m |           0.0448 |           0.0147 |           0.0000 |
[32m[20221213 20:49:56 @agent_ppo2.py:185][0m |           0.0209 |           0.0117 |           0.0000 |
[32m[20221213 20:49:56 @agent_ppo2.py:185][0m |          -0.0154 |           0.0114 |           0.0000 |
[32m[20221213 20:49:56 @agent_ppo2.py:185][0m |          -0.0071 |           0.0112 |           0.0000 |
[32m[20221213 20:49:56 @agent_ppo2.py:185][0m |          -0.0367 |           0.0111 |           0.0000 |
[32m[20221213 20:49:57 @agent_ppo2.py:185][0m |          -0.0455 |           0.0111 |           0.0000 |
[32m[20221213 20:49:57 @agent_ppo2.py:185][0m |          -0.0504 |           0.0109 |           0.0000 |
[32m[20221213 20:49:57 @agent_ppo2.py:185][0m |          -0.0417 |           0.0116 |           0.0000 |
[32m[20221213 20:49:57 @agent_ppo2.py:185][0m |          -0.0578 |           0.0121 |           0.0000 |
[32m[20221213 20:49:57 @agent_ppo2.py:185][0m |          -0.0607 |           0.0108 |           0.0000 |
[32m[20221213 20:49:57 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 20:49:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.14
[32m[20221213 20:49:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.56
[32m[20221213 20:49:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.15
[32m[20221213 20:49:57 @agent_ppo2.py:143][0m Total time:      20.80 min
[32m[20221213 20:49:57 @agent_ppo2.py:145][0m 2961408 total steps have happened
[32m[20221213 20:49:57 @agent_ppo2.py:121][0m #------------------------ Iteration 723 --------------------------#
[32m[20221213 20:49:58 @agent_ppo2.py:127][0m Sampling time: 0.43 s by 5 slaves
[32m[20221213 20:49:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:49:58 @agent_ppo2.py:185][0m |           0.0326 |           0.0106 |           0.0000 |
[32m[20221213 20:49:58 @agent_ppo2.py:185][0m |           0.0015 |           0.0091 |           0.0000 |
[32m[20221213 20:49:58 @agent_ppo2.py:185][0m |          -0.0167 |           0.0090 |           0.0000 |
[32m[20221213 20:49:58 @agent_ppo2.py:185][0m |          -0.0329 |           0.0088 |           0.0000 |
[32m[20221213 20:49:59 @agent_ppo2.py:185][0m |          -0.0458 |           0.0087 |           0.0000 |
[32m[20221213 20:49:59 @agent_ppo2.py:185][0m |          -0.0518 |           0.0087 |           0.0000 |
[32m[20221213 20:49:59 @agent_ppo2.py:185][0m |          -0.0453 |           0.0086 |           0.0000 |
[32m[20221213 20:49:59 @agent_ppo2.py:185][0m |          -0.0644 |           0.0085 |           0.0000 |
[32m[20221213 20:49:59 @agent_ppo2.py:185][0m |          -0.0554 |           0.0085 |           0.0000 |
[32m[20221213 20:49:59 @agent_ppo2.py:185][0m |          -0.0612 |           0.0084 |           0.0000 |
[32m[20221213 20:49:59 @agent_ppo2.py:130][0m Policy update time: 1.27 s
[32m[20221213 20:50:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.83
[32m[20221213 20:50:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.27
[32m[20221213 20:50:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.27
[32m[20221213 20:50:00 @agent_ppo2.py:143][0m Total time:      20.84 min
[32m[20221213 20:50:00 @agent_ppo2.py:145][0m 2965504 total steps have happened
[32m[20221213 20:50:00 @agent_ppo2.py:121][0m #------------------------ Iteration 724 --------------------------#
[32m[20221213 20:50:00 @agent_ppo2.py:127][0m Sampling time: 0.44 s by 5 slaves
[32m[20221213 20:50:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:50:00 @agent_ppo2.py:185][0m |           0.0290 |           0.0348 |           0.0000 |
[32m[20221213 20:50:00 @agent_ppo2.py:185][0m |           0.0030 |           0.0159 |           0.0000 |
[32m[20221213 20:50:00 @agent_ppo2.py:185][0m |          -0.0104 |           0.0145 |           0.0000 |
[32m[20221213 20:50:01 @agent_ppo2.py:185][0m |          -0.0179 |           0.0140 |           0.0000 |
[32m[20221213 20:50:01 @agent_ppo2.py:185][0m |          -0.0145 |           0.0134 |           0.0000 |
[32m[20221213 20:50:01 @agent_ppo2.py:185][0m |          -0.0198 |           0.0133 |           0.0000 |
[32m[20221213 20:50:01 @agent_ppo2.py:185][0m |          -0.0525 |           0.0133 |           0.0000 |
[32m[20221213 20:50:01 @agent_ppo2.py:185][0m |          -0.0267 |           0.0138 |           0.0000 |
[32m[20221213 20:50:01 @agent_ppo2.py:185][0m |          -0.0285 |           0.0130 |           0.0000 |
[32m[20221213 20:50:01 @agent_ppo2.py:185][0m |          -0.0316 |           0.0128 |           0.0000 |
[32m[20221213 20:50:01 @agent_ppo2.py:130][0m Policy update time: 1.30 s
[32m[20221213 20:50:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.46
[32m[20221213 20:50:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.02
[32m[20221213 20:50:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.19
[32m[20221213 20:50:02 @agent_ppo2.py:143][0m Total time:      20.88 min
[32m[20221213 20:50:02 @agent_ppo2.py:145][0m 2969600 total steps have happened
[32m[20221213 20:50:02 @agent_ppo2.py:121][0m #------------------------ Iteration 725 --------------------------#
[32m[20221213 20:50:02 @agent_ppo2.py:127][0m Sampling time: 0.44 s by 5 slaves
[32m[20221213 20:50:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:50:02 @agent_ppo2.py:185][0m |           0.0311 |           0.0148 |           0.0000 |
[32m[20221213 20:50:03 @agent_ppo2.py:185][0m |           0.0142 |           0.0141 |           0.0000 |
[32m[20221213 20:50:03 @agent_ppo2.py:185][0m |          -0.0097 |           0.0137 |           0.0000 |
[32m[20221213 20:50:03 @agent_ppo2.py:185][0m |          -0.0311 |           0.0122 |           0.0000 |
[32m[20221213 20:50:03 @agent_ppo2.py:185][0m |          -0.0398 |           0.0121 |           0.0000 |
[32m[20221213 20:50:03 @agent_ppo2.py:185][0m |          -0.0566 |           0.0120 |           0.0000 |
[32m[20221213 20:50:03 @agent_ppo2.py:185][0m |          -0.0617 |           0.0117 |           0.0000 |
[32m[20221213 20:50:03 @agent_ppo2.py:185][0m |          -0.0622 |           0.0118 |           0.0000 |
[32m[20221213 20:50:03 @agent_ppo2.py:185][0m |          -0.0767 |           0.0119 |           0.0000 |
[32m[20221213 20:50:03 @agent_ppo2.py:185][0m |          -0.0675 |           0.0114 |           0.0000 |
[32m[20221213 20:50:03 @agent_ppo2.py:130][0m Policy update time: 1.28 s
[32m[20221213 20:50:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.88
[32m[20221213 20:50:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.25
[32m[20221213 20:50:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.12
[32m[20221213 20:50:04 @agent_ppo2.py:143][0m Total time:      20.91 min
[32m[20221213 20:50:04 @agent_ppo2.py:145][0m 2973696 total steps have happened
[32m[20221213 20:50:04 @agent_ppo2.py:121][0m #------------------------ Iteration 726 --------------------------#
[32m[20221213 20:50:04 @agent_ppo2.py:127][0m Sampling time: 0.44 s by 5 slaves
[32m[20221213 20:50:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:50:05 @agent_ppo2.py:185][0m |           0.0388 |           0.0137 |           0.0000 |
[32m[20221213 20:50:05 @agent_ppo2.py:185][0m |           0.0016 |           0.0133 |           0.0000 |
[32m[20221213 20:50:05 @agent_ppo2.py:185][0m |          -0.0443 |           0.0147 |           0.0000 |
[32m[20221213 20:50:05 @agent_ppo2.py:185][0m |          -0.0444 |           0.0153 |           0.0000 |
[32m[20221213 20:50:05 @agent_ppo2.py:185][0m |          -0.0723 |           0.0143 |           0.0000 |
[32m[20221213 20:50:05 @agent_ppo2.py:185][0m |          -0.0568 |           0.0133 |           0.0000 |
[32m[20221213 20:50:05 @agent_ppo2.py:185][0m |          -0.0618 |           0.0129 |           0.0000 |
[32m[20221213 20:50:05 @agent_ppo2.py:185][0m |          -0.0614 |           0.0126 |           0.0000 |
[32m[20221213 20:50:05 @agent_ppo2.py:185][0m |          -0.0607 |           0.0126 |           0.0000 |
[32m[20221213 20:50:06 @agent_ppo2.py:185][0m |          -0.0706 |           0.0124 |           0.0000 |
[32m[20221213 20:50:06 @agent_ppo2.py:130][0m Policy update time: 1.28 s
[32m[20221213 20:50:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.62
[32m[20221213 20:50:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.37
[32m[20221213 20:50:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.76
[32m[20221213 20:50:06 @agent_ppo2.py:143][0m Total time:      20.95 min
[32m[20221213 20:50:06 @agent_ppo2.py:145][0m 2977792 total steps have happened
[32m[20221213 20:50:06 @agent_ppo2.py:121][0m #------------------------ Iteration 727 --------------------------#
[32m[20221213 20:50:07 @agent_ppo2.py:127][0m Sampling time: 0.44 s by 5 slaves
[32m[20221213 20:50:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:50:07 @agent_ppo2.py:185][0m |           0.0582 |           0.0119 |           0.0000 |
[32m[20221213 20:50:07 @agent_ppo2.py:185][0m |           0.0279 |           0.0113 |           0.0000 |
[32m[20221213 20:50:07 @agent_ppo2.py:185][0m |          -0.0084 |           0.0110 |           0.0000 |
[32m[20221213 20:50:07 @agent_ppo2.py:185][0m |          -0.0184 |           0.0108 |           0.0000 |
[32m[20221213 20:50:07 @agent_ppo2.py:185][0m |          -0.0341 |           0.0133 |           0.0000 |
[32m[20221213 20:50:07 @agent_ppo2.py:185][0m |          -0.0389 |           0.0130 |           0.0000 |
[32m[20221213 20:50:07 @agent_ppo2.py:185][0m |          -0.0457 |           0.0107 |           0.0000 |
[32m[20221213 20:50:08 @agent_ppo2.py:185][0m |          -0.0505 |           0.0104 |           0.0000 |
[32m[20221213 20:50:08 @agent_ppo2.py:185][0m |          -0.0534 |           0.0104 |           0.0000 |
[32m[20221213 20:50:08 @agent_ppo2.py:185][0m |          -0.0545 |           0.0103 |           0.0000 |
[32m[20221213 20:50:08 @agent_ppo2.py:130][0m Policy update time: 1.24 s
[32m[20221213 20:50:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.87
[32m[20221213 20:50:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.41
[32m[20221213 20:50:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.10
[32m[20221213 20:50:08 @agent_ppo2.py:143][0m Total time:      20.98 min
[32m[20221213 20:50:08 @agent_ppo2.py:145][0m 2981888 total steps have happened
[32m[20221213 20:50:08 @agent_ppo2.py:121][0m #------------------------ Iteration 728 --------------------------#
[32m[20221213 20:50:09 @agent_ppo2.py:127][0m Sampling time: 0.44 s by 5 slaves
[32m[20221213 20:50:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:50:09 @agent_ppo2.py:185][0m |           0.0324 |           0.0226 |           0.0000 |
[32m[20221213 20:50:09 @agent_ppo2.py:185][0m |          -0.0054 |           0.0160 |           0.0000 |
[32m[20221213 20:50:09 @agent_ppo2.py:185][0m |          -0.0120 |           0.0153 |           0.0000 |
[32m[20221213 20:50:09 @agent_ppo2.py:185][0m |          -0.0217 |           0.0144 |           0.0000 |
[32m[20221213 20:50:09 @agent_ppo2.py:185][0m |          -0.0232 |           0.0144 |           0.0000 |
[32m[20221213 20:50:09 @agent_ppo2.py:185][0m |          -0.0672 |           0.0150 |           0.0000 |
[32m[20221213 20:50:10 @agent_ppo2.py:185][0m |          -0.0262 |           0.0147 |           0.0000 |
[32m[20221213 20:50:10 @agent_ppo2.py:185][0m |          -0.0266 |           0.0135 |           0.0000 |
[32m[20221213 20:50:10 @agent_ppo2.py:185][0m |          -0.0286 |           0.0134 |           0.0000 |
[32m[20221213 20:50:10 @agent_ppo2.py:185][0m |          -0.0400 |           0.0133 |           0.0000 |
[32m[20221213 20:50:10 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221213 20:50:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.93
[32m[20221213 20:50:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.80
[32m[20221213 20:50:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.22
[32m[20221213 20:50:10 @agent_ppo2.py:143][0m Total time:      21.02 min
[32m[20221213 20:50:10 @agent_ppo2.py:145][0m 2985984 total steps have happened
[32m[20221213 20:50:10 @agent_ppo2.py:121][0m #------------------------ Iteration 729 --------------------------#
[32m[20221213 20:50:11 @agent_ppo2.py:127][0m Sampling time: 0.43 s by 5 slaves
[32m[20221213 20:50:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:50:11 @agent_ppo2.py:185][0m |           0.0620 |           0.0170 |           0.0000 |
[32m[20221213 20:50:11 @agent_ppo2.py:185][0m |           0.0233 |           0.0116 |           0.0000 |
[32m[20221213 20:50:11 @agent_ppo2.py:185][0m |           0.0254 |           0.0110 |           0.0000 |
[32m[20221213 20:50:11 @agent_ppo2.py:185][0m |           0.0040 |           0.0107 |           0.0000 |
[32m[20221213 20:50:11 @agent_ppo2.py:185][0m |          -0.0116 |           0.0107 |           0.0000 |
[32m[20221213 20:50:12 @agent_ppo2.py:185][0m |          -0.0223 |           0.0105 |           0.0000 |
[32m[20221213 20:50:12 @agent_ppo2.py:185][0m |          -0.0289 |           0.0103 |           0.0000 |
[32m[20221213 20:50:12 @agent_ppo2.py:185][0m |          -0.0281 |           0.0103 |           0.0000 |
[32m[20221213 20:50:12 @agent_ppo2.py:185][0m |          -0.0153 |           0.0104 |           0.0000 |
[32m[20221213 20:50:12 @agent_ppo2.py:185][0m |          -0.0344 |           0.0103 |           0.0000 |
[32m[20221213 20:50:12 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221213 20:50:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.38
[32m[20221213 20:50:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.84
[32m[20221213 20:50:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.00
[32m[20221213 20:50:12 @agent_ppo2.py:143][0m Total time:      21.05 min
[32m[20221213 20:50:12 @agent_ppo2.py:145][0m 2990080 total steps have happened
[32m[20221213 20:50:12 @agent_ppo2.py:121][0m #------------------------ Iteration 730 --------------------------#
[32m[20221213 20:50:13 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 5 slaves
[32m[20221213 20:50:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:50:13 @agent_ppo2.py:185][0m |           0.0002 |           0.0361 |           0.0000 |
[32m[20221213 20:50:13 @agent_ppo2.py:185][0m |           0.0212 |           0.0193 |           0.0000 |
[32m[20221213 20:50:13 @agent_ppo2.py:185][0m |           0.0035 |           0.0170 |           0.0000 |
[32m[20221213 20:50:13 @agent_ppo2.py:185][0m |          -0.0076 |           0.0164 |           0.0000 |
[32m[20221213 20:50:14 @agent_ppo2.py:185][0m |          -0.0156 |           0.0160 |           0.0000 |
[32m[20221213 20:50:14 @agent_ppo2.py:185][0m |          -0.0207 |           0.0151 |           0.0000 |
[32m[20221213 20:50:14 @agent_ppo2.py:185][0m |          -0.0226 |           0.0149 |           0.0000 |
[32m[20221213 20:50:14 @agent_ppo2.py:185][0m |          -0.0230 |           0.0148 |           0.0000 |
[32m[20221213 20:50:14 @agent_ppo2.py:185][0m |          -0.0481 |           0.0151 |           0.0000 |
[32m[20221213 20:50:14 @agent_ppo2.py:185][0m |          -0.0290 |           0.0154 |           0.0000 |
[32m[20221213 20:50:14 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 20:50:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.52
[32m[20221213 20:50:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.37
[32m[20221213 20:50:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.54
[32m[20221213 20:50:15 @agent_ppo2.py:143][0m Total time:      21.09 min
[32m[20221213 20:50:15 @agent_ppo2.py:145][0m 2994176 total steps have happened
[32m[20221213 20:50:15 @agent_ppo2.py:121][0m #------------------------ Iteration 731 --------------------------#
[32m[20221213 20:50:15 @agent_ppo2.py:127][0m Sampling time: 0.43 s by 5 slaves
[32m[20221213 20:50:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:50:15 @agent_ppo2.py:185][0m |           0.0359 |           0.0254 |           0.0000 |
[32m[20221213 20:50:15 @agent_ppo2.py:185][0m |           0.0007 |           0.0181 |           0.0000 |
[32m[20221213 20:50:15 @agent_ppo2.py:185][0m |          -0.0219 |           0.0163 |           0.0000 |
[32m[20221213 20:50:16 @agent_ppo2.py:185][0m |          -0.0375 |           0.0160 |           0.0000 |
[32m[20221213 20:50:16 @agent_ppo2.py:185][0m |          -0.0454 |           0.0159 |           0.0000 |
[32m[20221213 20:50:16 @agent_ppo2.py:185][0m |          -0.0349 |           0.0157 |           0.0000 |
[32m[20221213 20:50:16 @agent_ppo2.py:185][0m |          -0.0464 |           0.0155 |           0.0000 |
[32m[20221213 20:50:16 @agent_ppo2.py:185][0m |          -0.0403 |           0.0148 |           0.0000 |
[32m[20221213 20:50:16 @agent_ppo2.py:185][0m |          -0.0451 |           0.0143 |           0.0000 |
[32m[20221213 20:50:16 @agent_ppo2.py:185][0m |          -0.0479 |           0.0142 |           0.0000 |
[32m[20221213 20:50:16 @agent_ppo2.py:130][0m Policy update time: 1.28 s
[32m[20221213 20:50:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.63
[32m[20221213 20:50:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.52
[32m[20221213 20:50:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.00
[32m[20221213 20:50:17 @agent_ppo2.py:143][0m Total time:      21.13 min
[32m[20221213 20:50:17 @agent_ppo2.py:145][0m 2998272 total steps have happened
[32m[20221213 20:50:17 @agent_ppo2.py:121][0m #------------------------ Iteration 732 --------------------------#
[32m[20221213 20:50:17 @agent_ppo2.py:127][0m Sampling time: 0.44 s by 5 slaves
[32m[20221213 20:50:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:50:17 @agent_ppo2.py:185][0m |           0.0335 |           0.0175 |           0.0000 |
[32m[20221213 20:50:17 @agent_ppo2.py:185][0m |           0.0225 |           0.0153 |           0.0000 |
[32m[20221213 20:50:18 @agent_ppo2.py:185][0m |          -0.0060 |           0.0141 |           0.0000 |
[32m[20221213 20:50:18 @agent_ppo2.py:185][0m |          -0.0314 |           0.0136 |           0.0000 |
[32m[20221213 20:50:18 @agent_ppo2.py:185][0m |          -0.0516 |           0.0132 |           0.0000 |
[32m[20221213 20:50:18 @agent_ppo2.py:185][0m |          -0.0580 |           0.0131 |           0.0000 |
[32m[20221213 20:50:18 @agent_ppo2.py:185][0m |          -0.0625 |           0.0129 |           0.0000 |
[32m[20221213 20:50:18 @agent_ppo2.py:185][0m |          -0.0739 |           0.0128 |           0.0000 |
[32m[20221213 20:50:18 @agent_ppo2.py:185][0m |          -0.0749 |           0.0129 |           0.0000 |
[32m[20221213 20:50:18 @agent_ppo2.py:185][0m |          -0.0712 |           0.0128 |           0.0000 |
[32m[20221213 20:50:18 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221213 20:50:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.25
[32m[20221213 20:50:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.52
[32m[20221213 20:50:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.51
[32m[20221213 20:50:19 @agent_ppo2.py:143][0m Total time:      21.16 min
[32m[20221213 20:50:19 @agent_ppo2.py:145][0m 3002368 total steps have happened
[32m[20221213 20:50:19 @agent_ppo2.py:121][0m #------------------------ Iteration 733 --------------------------#
[32m[20221213 20:50:19 @agent_ppo2.py:127][0m Sampling time: 0.44 s by 5 slaves
[32m[20221213 20:50:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:50:19 @agent_ppo2.py:185][0m |           0.0542 |           0.0289 |           0.0000 |
[32m[20221213 20:50:20 @agent_ppo2.py:185][0m |           0.0196 |           0.0126 |           0.0000 |
[32m[20221213 20:50:20 @agent_ppo2.py:185][0m |          -0.0060 |           0.0119 |           0.0000 |
[32m[20221213 20:50:20 @agent_ppo2.py:185][0m |          -0.0169 |           0.0116 |           0.0000 |
[32m[20221213 20:50:20 @agent_ppo2.py:185][0m |          -0.0233 |           0.0114 |           0.0000 |
[32m[20221213 20:50:20 @agent_ppo2.py:185][0m |          -0.0314 |           0.0112 |           0.0000 |
[32m[20221213 20:50:20 @agent_ppo2.py:185][0m |          -0.0352 |           0.0110 |           0.0000 |
[32m[20221213 20:50:20 @agent_ppo2.py:185][0m |          -0.0352 |           0.0110 |           0.0000 |
[32m[20221213 20:50:20 @agent_ppo2.py:185][0m |          -0.0418 |           0.0108 |           0.0000 |
[32m[20221213 20:50:21 @agent_ppo2.py:185][0m |          -0.0445 |           0.0107 |           0.0000 |
[32m[20221213 20:50:21 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221213 20:50:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.20
[32m[20221213 20:50:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.16
[32m[20221213 20:50:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.99
[32m[20221213 20:50:21 @agent_ppo2.py:143][0m Total time:      21.20 min
[32m[20221213 20:50:21 @agent_ppo2.py:145][0m 3006464 total steps have happened
[32m[20221213 20:50:21 @agent_ppo2.py:121][0m #------------------------ Iteration 734 --------------------------#
[32m[20221213 20:50:21 @agent_ppo2.py:127][0m Sampling time: 0.43 s by 5 slaves
[32m[20221213 20:50:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:50:22 @agent_ppo2.py:185][0m |           0.0254 |           0.0083 |           0.0000 |
[32m[20221213 20:50:22 @agent_ppo2.py:185][0m |          -0.0113 |           0.0074 |           0.0000 |
[32m[20221213 20:50:22 @agent_ppo2.py:185][0m |          -0.0486 |           0.0072 |           0.0000 |
[32m[20221213 20:50:22 @agent_ppo2.py:185][0m |          -0.0609 |           0.0071 |           0.0000 |
[32m[20221213 20:50:22 @agent_ppo2.py:185][0m |          -0.0631 |           0.0070 |           0.0000 |
[32m[20221213 20:50:22 @agent_ppo2.py:185][0m |          -0.0744 |           0.0070 |           0.0000 |
[32m[20221213 20:50:22 @agent_ppo2.py:185][0m |          -0.0839 |           0.0069 |           0.0000 |
[32m[20221213 20:50:22 @agent_ppo2.py:185][0m |          -0.0585 |           0.0069 |           0.0000 |
[32m[20221213 20:50:23 @agent_ppo2.py:185][0m |          -0.0797 |           0.0068 |           0.0000 |
[32m[20221213 20:50:23 @agent_ppo2.py:185][0m |          -0.0901 |           0.0067 |           0.0000 |
[32m[20221213 20:50:23 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221213 20:50:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.08
[32m[20221213 20:50:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.25
[32m[20221213 20:50:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.98
[32m[20221213 20:50:23 @agent_ppo2.py:143][0m Total time:      21.23 min
[32m[20221213 20:50:23 @agent_ppo2.py:145][0m 3010560 total steps have happened
[32m[20221213 20:50:23 @agent_ppo2.py:121][0m #------------------------ Iteration 735 --------------------------#
[32m[20221213 20:50:24 @agent_ppo2.py:127][0m Sampling time: 0.44 s by 5 slaves
[32m[20221213 20:50:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:50:24 @agent_ppo2.py:185][0m |           0.0204 |           0.0621 |           0.0000 |
[32m[20221213 20:50:24 @agent_ppo2.py:185][0m |          -0.0063 |           0.0194 |           0.0000 |
[32m[20221213 20:50:24 @agent_ppo2.py:185][0m |          -0.0105 |           0.0153 |           0.0000 |
[32m[20221213 20:50:24 @agent_ppo2.py:185][0m |          -0.0194 |           0.0142 |           0.0000 |
[32m[20221213 20:50:24 @agent_ppo2.py:185][0m |          -0.0571 |           0.0138 |           0.0000 |
[32m[20221213 20:50:24 @agent_ppo2.py:185][0m |          -0.0211 |           0.0141 |           0.0000 |
[32m[20221213 20:50:24 @agent_ppo2.py:185][0m |          -0.0228 |           0.0132 |           0.0000 |
[32m[20221213 20:50:25 @agent_ppo2.py:185][0m |          -0.0243 |           0.0132 |           0.0000 |
[32m[20221213 20:50:25 @agent_ppo2.py:185][0m |          -0.0294 |           0.0131 |           0.0000 |
[32m[20221213 20:50:25 @agent_ppo2.py:185][0m |          -0.0314 |           0.0129 |           0.0000 |
[32m[20221213 20:50:25 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 20:50:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.18
[32m[20221213 20:50:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.79
[32m[20221213 20:50:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.78
[32m[20221213 20:50:25 @agent_ppo2.py:143][0m Total time:      21.27 min
[32m[20221213 20:50:25 @agent_ppo2.py:145][0m 3014656 total steps have happened
[32m[20221213 20:50:25 @agent_ppo2.py:121][0m #------------------------ Iteration 736 --------------------------#
[32m[20221213 20:50:26 @agent_ppo2.py:127][0m Sampling time: 0.44 s by 5 slaves
[32m[20221213 20:50:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:50:26 @agent_ppo2.py:185][0m |           0.0229 |           0.0127 |           0.0000 |
[32m[20221213 20:50:26 @agent_ppo2.py:185][0m |          -0.0038 |           0.0122 |           0.0000 |
[32m[20221213 20:50:26 @agent_ppo2.py:185][0m |          -0.0344 |           0.0117 |           0.0000 |
[32m[20221213 20:50:26 @agent_ppo2.py:185][0m |          -0.0475 |           0.0116 |           0.0000 |
[32m[20221213 20:50:26 @agent_ppo2.py:185][0m |          -0.0530 |           0.0114 |           0.0000 |
[32m[20221213 20:50:26 @agent_ppo2.py:185][0m |          -0.0603 |           0.0113 |           0.0000 |
[32m[20221213 20:50:27 @agent_ppo2.py:185][0m |          -0.0632 |           0.0111 |           0.0000 |
[32m[20221213 20:50:27 @agent_ppo2.py:185][0m |          -0.0664 |           0.0113 |           0.0000 |
[32m[20221213 20:50:27 @agent_ppo2.py:185][0m |          -0.0678 |           0.0112 |           0.0000 |
[32m[20221213 20:50:27 @agent_ppo2.py:185][0m |          -0.0678 |           0.0110 |           0.0000 |
[32m[20221213 20:50:27 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221213 20:50:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.67
[32m[20221213 20:50:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.44
[32m[20221213 20:50:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.08
[32m[20221213 20:50:27 @agent_ppo2.py:143][0m Total time:      21.30 min
[32m[20221213 20:50:27 @agent_ppo2.py:145][0m 3018752 total steps have happened
[32m[20221213 20:50:27 @agent_ppo2.py:121][0m #------------------------ Iteration 737 --------------------------#
[32m[20221213 20:50:28 @agent_ppo2.py:127][0m Sampling time: 0.39 s by 5 slaves
[32m[20221213 20:50:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:50:28 @agent_ppo2.py:185][0m |           0.0267 |           0.0162 |           0.0000 |
[32m[20221213 20:50:28 @agent_ppo2.py:185][0m |           0.0048 |           0.0151 |           0.0000 |
[32m[20221213 20:50:28 @agent_ppo2.py:185][0m |          -0.0289 |           0.0146 |           0.0000 |
[32m[20221213 20:50:28 @agent_ppo2.py:185][0m |          -0.0320 |           0.0138 |           0.0000 |
[32m[20221213 20:50:28 @agent_ppo2.py:185][0m |          -0.0462 |           0.0135 |           0.0000 |
[32m[20221213 20:50:28 @agent_ppo2.py:185][0m |          -0.0555 |           0.0132 |           0.0000 |
[32m[20221213 20:50:28 @agent_ppo2.py:185][0m |          -0.0587 |           0.0129 |           0.0000 |
[32m[20221213 20:50:28 @agent_ppo2.py:185][0m |          -0.0637 |           0.0130 |           0.0000 |
[32m[20221213 20:50:29 @agent_ppo2.py:185][0m |          -0.0712 |           0.0125 |           0.0000 |
[32m[20221213 20:50:29 @agent_ppo2.py:185][0m |          -0.0619 |           0.0126 |           0.0000 |
[32m[20221213 20:50:29 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:50:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.55
[32m[20221213 20:50:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.88
[32m[20221213 20:50:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.90
[32m[20221213 20:50:29 @agent_ppo2.py:143][0m Total time:      21.33 min
[32m[20221213 20:50:29 @agent_ppo2.py:145][0m 3022848 total steps have happened
[32m[20221213 20:50:29 @agent_ppo2.py:121][0m #------------------------ Iteration 738 --------------------------#
