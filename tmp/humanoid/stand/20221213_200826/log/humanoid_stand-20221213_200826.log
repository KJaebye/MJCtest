[32m[20221213 20:08:26 @logger.py:105][0m Log file set to ./tmp/humanoid/stand/20221213_200826/log/humanoid_stand-20221213_200826.log
[32m[20221213 20:08:26 @agent_ppo2.py:121][0m #------------------------ Iteration 0 --------------------------#
[32m[20221213 20:08:27 @agent_ppo2.py:127][0m Sampling time: 0.53 s by 10 slaves
[32m[20221213 20:08:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:08:27 @agent_ppo2.py:185][0m |           0.0005 |           0.1130 |           0.0000 |
[32m[20221213 20:08:28 @agent_ppo2.py:185][0m |          -0.0014 |           0.0762 |           0.0000 |
[32m[20221213 20:08:28 @agent_ppo2.py:185][0m |          -0.0036 |           0.0651 |           0.0000 |
[32m[20221213 20:08:28 @agent_ppo2.py:185][0m |          -0.0058 |           0.0613 |           0.0000 |
[32m[20221213 20:08:28 @agent_ppo2.py:185][0m |          -0.0081 |           0.0599 |           0.0000 |
[32m[20221213 20:08:29 @agent_ppo2.py:185][0m |          -0.0125 |           0.0601 |           0.0000 |
[32m[20221213 20:08:29 @agent_ppo2.py:185][0m |          -0.0115 |           0.0587 |           0.0000 |
[32m[20221213 20:08:29 @agent_ppo2.py:185][0m |          -0.0127 |           0.0583 |           0.0000 |
[32m[20221213 20:08:30 @agent_ppo2.py:185][0m |          -0.0135 |           0.0580 |           0.0000 |
[32m[20221213 20:08:30 @agent_ppo2.py:185][0m |          -0.0142 |           0.0581 |           0.0000 |
[32m[20221213 20:08:30 @agent_ppo2.py:130][0m Policy update time: 2.96 s
[32m[20221213 20:08:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.51
[32m[20221213 20:08:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.80
[32m[20221213 20:08:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.69
[32m[20221213 20:08:30 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 2.69
[32m[20221213 20:08:30 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 2.69
[32m[20221213 20:08:30 @agent_ppo2.py:143][0m Total time:       0.07 min
[32m[20221213 20:08:30 @agent_ppo2.py:145][0m 2048 total steps have happened
[32m[20221213 20:08:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1 --------------------------#
[32m[20221213 20:08:31 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:08:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:08:31 @agent_ppo2.py:185][0m |           0.0001 |           0.0423 |           0.0000 |
[32m[20221213 20:08:32 @agent_ppo2.py:185][0m |          -0.0045 |           0.0288 |           0.0000 |
[32m[20221213 20:08:32 @agent_ppo2.py:185][0m |          -0.0070 |           0.0276 |           0.0000 |
[32m[20221213 20:08:32 @agent_ppo2.py:185][0m |          -0.0087 |           0.0254 |           0.0000 |
[32m[20221213 20:08:32 @agent_ppo2.py:185][0m |          -0.0108 |           0.0226 |           0.0000 |
[32m[20221213 20:08:33 @agent_ppo2.py:185][0m |          -0.0121 |           0.0203 |           0.0000 |
[32m[20221213 20:08:33 @agent_ppo2.py:185][0m |          -0.0151 |           0.0191 |           0.0000 |
[32m[20221213 20:08:33 @agent_ppo2.py:185][0m |          -0.0198 |           0.0197 |           0.0000 |
[32m[20221213 20:08:33 @agent_ppo2.py:185][0m |          -0.0155 |           0.0184 |           0.0000 |
[32m[20221213 20:08:34 @agent_ppo2.py:185][0m |          -0.0159 |           0.0173 |           0.0000 |
[32m[20221213 20:08:34 @agent_ppo2.py:130][0m Policy update time: 2.99 s
[32m[20221213 20:08:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.16
[32m[20221213 20:08:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.34
[32m[20221213 20:08:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.57
[32m[20221213 20:08:34 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 7.57
[32m[20221213 20:08:34 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 7.57
[32m[20221213 20:08:34 @agent_ppo2.py:143][0m Total time:       0.13 min
[32m[20221213 20:08:34 @agent_ppo2.py:145][0m 4096 total steps have happened
[32m[20221213 20:08:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2 --------------------------#
[32m[20221213 20:08:35 @agent_ppo2.py:127][0m Sampling time: 0.56 s by 10 slaves
[32m[20221213 20:08:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:08:35 @agent_ppo2.py:185][0m |           0.0008 |           0.1989 |           0.0000 |
[32m[20221213 20:08:36 @agent_ppo2.py:185][0m |          -0.0026 |           0.1724 |           0.0000 |
[32m[20221213 20:08:36 @agent_ppo2.py:185][0m |          -0.0066 |           0.1502 |           0.0000 |
[32m[20221213 20:08:36 @agent_ppo2.py:185][0m |          -0.0093 |           0.1375 |           0.0000 |
[32m[20221213 20:08:37 @agent_ppo2.py:185][0m |          -0.0109 |           0.1261 |           0.0000 |
[32m[20221213 20:08:37 @agent_ppo2.py:185][0m |          -0.0121 |           0.1152 |           0.0000 |
[32m[20221213 20:08:37 @agent_ppo2.py:185][0m |          -0.0134 |           0.1051 |           0.0000 |
[32m[20221213 20:08:37 @agent_ppo2.py:185][0m |          -0.0143 |           0.0992 |           0.0000 |
[32m[20221213 20:08:38 @agent_ppo2.py:185][0m |          -0.0175 |           0.0929 |           0.0000 |
[32m[20221213 20:08:38 @agent_ppo2.py:185][0m |          -0.0158 |           0.0891 |           0.0000 |
[32m[20221213 20:08:38 @agent_ppo2.py:130][0m Policy update time: 3.18 s
[32m[20221213 20:08:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.95
[32m[20221213 20:08:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.03
[32m[20221213 20:08:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.15
[32m[20221213 20:08:39 @agent_ppo2.py:143][0m Total time:       0.20 min
[32m[20221213 20:08:39 @agent_ppo2.py:145][0m 6144 total steps have happened
[32m[20221213 20:08:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3 --------------------------#
[32m[20221213 20:08:39 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:08:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:08:39 @agent_ppo2.py:185][0m |           0.0024 |           0.0321 |           0.0000 |
[32m[20221213 20:08:40 @agent_ppo2.py:185][0m |          -0.0075 |           0.0200 |           0.0000 |
[32m[20221213 20:08:40 @agent_ppo2.py:185][0m |          -0.0098 |           0.0194 |           0.0000 |
[32m[20221213 20:08:40 @agent_ppo2.py:185][0m |          -0.0142 |           0.0188 |           0.0000 |
[32m[20221213 20:08:41 @agent_ppo2.py:185][0m |          -0.0167 |           0.0185 |           0.0000 |
[32m[20221213 20:08:41 @agent_ppo2.py:185][0m |          -0.0182 |           0.0181 |           0.0000 |
[32m[20221213 20:08:41 @agent_ppo2.py:185][0m |          -0.0197 |           0.0176 |           0.0000 |
[32m[20221213 20:08:41 @agent_ppo2.py:185][0m |          -0.0207 |           0.0178 |           0.0000 |
[32m[20221213 20:08:42 @agent_ppo2.py:185][0m |          -0.0231 |           0.0172 |           0.0000 |
[32m[20221213 20:08:42 @agent_ppo2.py:185][0m |          -0.0221 |           0.0171 |           0.0000 |
[32m[20221213 20:08:42 @agent_ppo2.py:130][0m Policy update time: 2.89 s
[32m[20221213 20:08:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.83
[32m[20221213 20:08:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.98
[32m[20221213 20:08:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.73
[32m[20221213 20:08:42 @agent_ppo2.py:143][0m Total time:       0.27 min
[32m[20221213 20:08:42 @agent_ppo2.py:145][0m 8192 total steps have happened
[32m[20221213 20:08:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4 --------------------------#
[32m[20221213 20:08:43 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:08:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:08:43 @agent_ppo2.py:185][0m |          -0.0010 |           0.0107 |           0.0000 |
[32m[20221213 20:08:44 @agent_ppo2.py:185][0m |          -0.0056 |           0.0097 |           0.0000 |
[32m[20221213 20:08:44 @agent_ppo2.py:185][0m |          -0.0109 |           0.0095 |           0.0000 |
[32m[20221213 20:08:44 @agent_ppo2.py:185][0m |          -0.0172 |           0.0092 |           0.0000 |
[32m[20221213 20:08:44 @agent_ppo2.py:185][0m |          -0.0191 |           0.0090 |           0.0000 |
[32m[20221213 20:08:45 @agent_ppo2.py:185][0m |          -0.0178 |           0.0088 |           0.0000 |
[32m[20221213 20:08:45 @agent_ppo2.py:185][0m |          -0.0208 |           0.0087 |           0.0000 |
[32m[20221213 20:08:45 @agent_ppo2.py:185][0m |          -0.0230 |           0.0086 |           0.0000 |
[32m[20221213 20:08:45 @agent_ppo2.py:185][0m |          -0.0233 |           0.0085 |           0.0000 |
[32m[20221213 20:08:46 @agent_ppo2.py:185][0m |          -0.0248 |           0.0083 |           0.0000 |
[32m[20221213 20:08:46 @agent_ppo2.py:130][0m Policy update time: 2.81 s
[32m[20221213 20:08:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.78
[32m[20221213 20:08:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.56
[32m[20221213 20:08:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.95
[32m[20221213 20:08:46 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 9.95
[32m[20221213 20:08:46 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 9.95
[32m[20221213 20:08:46 @agent_ppo2.py:143][0m Total time:       0.33 min
[32m[20221213 20:08:46 @agent_ppo2.py:145][0m 10240 total steps have happened
[32m[20221213 20:08:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5 --------------------------#
[32m[20221213 20:08:47 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:08:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:08:47 @agent_ppo2.py:185][0m |          -0.0018 |           0.0059 |           0.0000 |
[32m[20221213 20:08:47 @agent_ppo2.py:185][0m |          -0.0083 |           0.0055 |           0.0000 |
[32m[20221213 20:08:48 @agent_ppo2.py:185][0m |          -0.0140 |           0.0053 |           0.0000 |
[32m[20221213 20:08:48 @agent_ppo2.py:185][0m |          -0.0165 |           0.0051 |           0.0000 |
[32m[20221213 20:08:48 @agent_ppo2.py:185][0m |          -0.0170 |           0.0050 |           0.0000 |
[32m[20221213 20:08:48 @agent_ppo2.py:185][0m |          -0.0202 |           0.0048 |           0.0000 |
[32m[20221213 20:08:49 @agent_ppo2.py:185][0m |          -0.0231 |           0.0047 |           0.0000 |
[32m[20221213 20:08:49 @agent_ppo2.py:185][0m |          -0.0250 |           0.0046 |           0.0000 |
[32m[20221213 20:08:49 @agent_ppo2.py:185][0m |          -0.0250 |           0.0044 |           0.0000 |
[32m[20221213 20:08:49 @agent_ppo2.py:185][0m |          -0.0252 |           0.0044 |           0.0000 |
[32m[20221213 20:08:49 @agent_ppo2.py:130][0m Policy update time: 2.75 s
[32m[20221213 20:08:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.94
[32m[20221213 20:08:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.46
[32m[20221213 20:08:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.72
[32m[20221213 20:08:50 @agent_ppo2.py:143][0m Total time:       0.39 min
[32m[20221213 20:08:50 @agent_ppo2.py:145][0m 12288 total steps have happened
[32m[20221213 20:08:50 @agent_ppo2.py:121][0m #------------------------ Iteration 6 --------------------------#
[32m[20221213 20:08:50 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:08:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:08:51 @agent_ppo2.py:185][0m |          -0.0003 |           0.0030 |           0.0000 |
[32m[20221213 20:08:51 @agent_ppo2.py:185][0m |          -0.0081 |           0.0027 |           0.0000 |
[32m[20221213 20:08:51 @agent_ppo2.py:185][0m |          -0.0127 |           0.0026 |           0.0000 |
[32m[20221213 20:08:52 @agent_ppo2.py:185][0m |          -0.0163 |           0.0025 |           0.0000 |
[32m[20221213 20:08:52 @agent_ppo2.py:185][0m |          -0.0210 |           0.0023 |           0.0000 |
[32m[20221213 20:08:52 @agent_ppo2.py:185][0m |          -0.0198 |           0.0022 |           0.0000 |
[32m[20221213 20:08:52 @agent_ppo2.py:185][0m |          -0.0224 |           0.0021 |           0.0000 |
[32m[20221213 20:08:53 @agent_ppo2.py:185][0m |          -0.0244 |           0.0021 |           0.0000 |
[32m[20221213 20:08:53 @agent_ppo2.py:185][0m |          -0.0276 |           0.0020 |           0.0000 |
[32m[20221213 20:08:53 @agent_ppo2.py:185][0m |          -0.0258 |           0.0019 |           0.0000 |
[32m[20221213 20:08:53 @agent_ppo2.py:130][0m Policy update time: 2.91 s
[32m[20221213 20:08:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.40
[32m[20221213 20:08:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.49
[32m[20221213 20:08:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.81
[32m[20221213 20:08:54 @agent_ppo2.py:143][0m Total time:       0.46 min
[32m[20221213 20:08:54 @agent_ppo2.py:145][0m 14336 total steps have happened
[32m[20221213 20:08:54 @agent_ppo2.py:121][0m #------------------------ Iteration 7 --------------------------#
[32m[20221213 20:08:54 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:08:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:08:55 @agent_ppo2.py:185][0m |          -0.0019 |           0.0234 |           0.0000 |
[32m[20221213 20:08:55 @agent_ppo2.py:185][0m |          -0.0070 |           0.0195 |           0.0000 |
[32m[20221213 20:08:55 @agent_ppo2.py:185][0m |          -0.0092 |           0.0185 |           0.0000 |
[32m[20221213 20:08:56 @agent_ppo2.py:185][0m |          -0.0107 |           0.0185 |           0.0000 |
[32m[20221213 20:08:56 @agent_ppo2.py:185][0m |          -0.0120 |           0.0174 |           0.0000 |
[32m[20221213 20:08:56 @agent_ppo2.py:185][0m |          -0.0127 |           0.0163 |           0.0000 |
[32m[20221213 20:08:56 @agent_ppo2.py:185][0m |          -0.0133 |           0.0165 |           0.0000 |
[32m[20221213 20:08:57 @agent_ppo2.py:185][0m |          -0.0141 |           0.0161 |           0.0000 |
[32m[20221213 20:08:57 @agent_ppo2.py:185][0m |          -0.0148 |           0.0159 |           0.0000 |
[32m[20221213 20:08:57 @agent_ppo2.py:185][0m |          -0.0151 |           0.0157 |           0.0000 |
[32m[20221213 20:08:57 @agent_ppo2.py:130][0m Policy update time: 2.87 s
[32m[20221213 20:08:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.63
[32m[20221213 20:08:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.10
[32m[20221213 20:08:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.11
[32m[20221213 20:08:58 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 11.11
[32m[20221213 20:08:58 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 11.11
[32m[20221213 20:08:58 @agent_ppo2.py:143][0m Total time:       0.52 min
[32m[20221213 20:08:58 @agent_ppo2.py:145][0m 16384 total steps have happened
[32m[20221213 20:08:58 @agent_ppo2.py:121][0m #------------------------ Iteration 8 --------------------------#
[32m[20221213 20:08:58 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:08:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:08:59 @agent_ppo2.py:185][0m |           0.0008 |           0.0083 |           0.0000 |
[32m[20221213 20:08:59 @agent_ppo2.py:185][0m |          -0.0060 |           0.0077 |           0.0000 |
[32m[20221213 20:08:59 @agent_ppo2.py:185][0m |          -0.0141 |           0.0075 |           0.0000 |
[32m[20221213 20:08:59 @agent_ppo2.py:185][0m |          -0.0162 |           0.0074 |           0.0000 |
[32m[20221213 20:09:00 @agent_ppo2.py:185][0m |          -0.0218 |           0.0072 |           0.0000 |
[32m[20221213 20:09:00 @agent_ppo2.py:185][0m |          -0.0202 |           0.0072 |           0.0000 |
[32m[20221213 20:09:00 @agent_ppo2.py:185][0m |          -0.0228 |           0.0072 |           0.0000 |
[32m[20221213 20:09:01 @agent_ppo2.py:185][0m |          -0.0245 |           0.0070 |           0.0000 |
[32m[20221213 20:09:01 @agent_ppo2.py:185][0m |          -0.0262 |           0.0070 |           0.0000 |
[32m[20221213 20:09:01 @agent_ppo2.py:185][0m |          -0.0278 |           0.0069 |           0.0000 |
[32m[20221213 20:09:01 @agent_ppo2.py:130][0m Policy update time: 2.85 s
[32m[20221213 20:09:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.12
[32m[20221213 20:09:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.70
[32m[20221213 20:09:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.74
[32m[20221213 20:09:02 @agent_ppo2.py:143][0m Total time:       0.59 min
[32m[20221213 20:09:02 @agent_ppo2.py:145][0m 18432 total steps have happened
[32m[20221213 20:09:02 @agent_ppo2.py:121][0m #------------------------ Iteration 9 --------------------------#
[32m[20221213 20:09:02 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:09:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:03 @agent_ppo2.py:185][0m |           0.0020 |           0.0698 |           0.0000 |
[32m[20221213 20:09:03 @agent_ppo2.py:185][0m |          -0.0039 |           0.0487 |           0.0000 |
[32m[20221213 20:09:03 @agent_ppo2.py:185][0m |          -0.0068 |           0.0424 |           0.0000 |
[32m[20221213 20:09:03 @agent_ppo2.py:185][0m |          -0.0126 |           0.0404 |           0.0000 |
[32m[20221213 20:09:04 @agent_ppo2.py:185][0m |          -0.0107 |           0.0378 |           0.0000 |
[32m[20221213 20:09:04 @agent_ppo2.py:185][0m |          -0.0122 |           0.0361 |           0.0000 |
[32m[20221213 20:09:04 @agent_ppo2.py:185][0m |          -0.0132 |           0.0353 |           0.0000 |
[32m[20221213 20:09:05 @agent_ppo2.py:185][0m |          -0.0141 |           0.0341 |           0.0000 |
[32m[20221213 20:09:05 @agent_ppo2.py:185][0m |          -0.0141 |           0.0333 |           0.0000 |
[32m[20221213 20:09:05 @agent_ppo2.py:185][0m |          -0.0152 |           0.0332 |           0.0000 |
[32m[20221213 20:09:05 @agent_ppo2.py:130][0m Policy update time: 2.93 s
[32m[20221213 20:09:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.60
[32m[20221213 20:09:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.23
[32m[20221213 20:09:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.43
[32m[20221213 20:09:06 @agent_ppo2.py:143][0m Total time:       0.66 min
[32m[20221213 20:09:06 @agent_ppo2.py:145][0m 20480 total steps have happened
[32m[20221213 20:09:06 @agent_ppo2.py:121][0m #------------------------ Iteration 10 --------------------------#
[32m[20221213 20:09:06 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:09:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:07 @agent_ppo2.py:185][0m |          -0.0002 |           0.0213 |           0.0000 |
[32m[20221213 20:09:07 @agent_ppo2.py:185][0m |          -0.0061 |           0.0153 |           0.0000 |
[32m[20221213 20:09:07 @agent_ppo2.py:185][0m |          -0.0130 |           0.0149 |           0.0000 |
[32m[20221213 20:09:07 @agent_ppo2.py:185][0m |          -0.0157 |           0.0147 |           0.0000 |
[32m[20221213 20:09:08 @agent_ppo2.py:185][0m |          -0.0176 |           0.0144 |           0.0000 |
[32m[20221213 20:09:08 @agent_ppo2.py:185][0m |          -0.0214 |           0.0142 |           0.0000 |
[32m[20221213 20:09:08 @agent_ppo2.py:185][0m |          -0.0212 |           0.0141 |           0.0000 |
[32m[20221213 20:09:08 @agent_ppo2.py:185][0m |          -0.0238 |           0.0140 |           0.0000 |
[32m[20221213 20:09:09 @agent_ppo2.py:185][0m |          -0.0251 |           0.0139 |           0.0000 |
[32m[20221213 20:09:09 @agent_ppo2.py:185][0m |          -0.0272 |           0.0138 |           0.0000 |
[32m[20221213 20:09:09 @agent_ppo2.py:130][0m Policy update time: 2.83 s
[32m[20221213 20:09:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.03
[32m[20221213 20:09:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.55
[32m[20221213 20:09:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.34
[32m[20221213 20:09:09 @agent_ppo2.py:143][0m Total time:       0.72 min
[32m[20221213 20:09:09 @agent_ppo2.py:145][0m 22528 total steps have happened
[32m[20221213 20:09:09 @agent_ppo2.py:121][0m #------------------------ Iteration 11 --------------------------#
[32m[20221213 20:09:10 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:09:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:10 @agent_ppo2.py:185][0m |          -0.0045 |           0.0617 |           0.0000 |
[32m[20221213 20:09:11 @agent_ppo2.py:185][0m |          -0.0067 |           0.0502 |           0.0000 |
[32m[20221213 20:09:11 @agent_ppo2.py:185][0m |          -0.0100 |           0.0455 |           0.0000 |
[32m[20221213 20:09:11 @agent_ppo2.py:185][0m |          -0.0122 |           0.0433 |           0.0000 |
[32m[20221213 20:09:11 @agent_ppo2.py:185][0m |          -0.0141 |           0.0424 |           0.0000 |
[32m[20221213 20:09:12 @agent_ppo2.py:185][0m |          -0.0155 |           0.0415 |           0.0000 |
[32m[20221213 20:09:12 @agent_ppo2.py:185][0m |          -0.0163 |           0.0405 |           0.0000 |
[32m[20221213 20:09:12 @agent_ppo2.py:185][0m |          -0.0167 |           0.0398 |           0.0000 |
[32m[20221213 20:09:12 @agent_ppo2.py:185][0m |          -0.0179 |           0.0385 |           0.0000 |
[32m[20221213 20:09:13 @agent_ppo2.py:185][0m |          -0.0181 |           0.0383 |           0.0000 |
[32m[20221213 20:09:13 @agent_ppo2.py:130][0m Policy update time: 2.76 s
[32m[20221213 20:09:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.98
[32m[20221213 20:09:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.56
[32m[20221213 20:09:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.09
[32m[20221213 20:09:13 @agent_ppo2.py:143][0m Total time:       0.78 min
[32m[20221213 20:09:13 @agent_ppo2.py:145][0m 24576 total steps have happened
[32m[20221213 20:09:13 @agent_ppo2.py:121][0m #------------------------ Iteration 12 --------------------------#
[32m[20221213 20:09:14 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:09:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:14 @agent_ppo2.py:185][0m |           0.0003 |           0.0668 |           0.0000 |
[32m[20221213 20:09:14 @agent_ppo2.py:185][0m |          -0.0076 |           0.0524 |           0.0000 |
[32m[20221213 20:09:15 @agent_ppo2.py:185][0m |          -0.0116 |           0.0484 |           0.0000 |
[32m[20221213 20:09:15 @agent_ppo2.py:185][0m |          -0.0165 |           0.0460 |           0.0000 |
[32m[20221213 20:09:15 @agent_ppo2.py:185][0m |          -0.0170 |           0.0450 |           0.0000 |
[32m[20221213 20:09:15 @agent_ppo2.py:185][0m |          -0.0207 |           0.0428 |           0.0000 |
[32m[20221213 20:09:16 @agent_ppo2.py:185][0m |          -0.0201 |           0.0417 |           0.0000 |
[32m[20221213 20:09:16 @agent_ppo2.py:185][0m |          -0.0216 |           0.0406 |           0.0000 |
[32m[20221213 20:09:16 @agent_ppo2.py:185][0m |          -0.0219 |           0.0398 |           0.0000 |
[32m[20221213 20:09:16 @agent_ppo2.py:185][0m |          -0.0223 |           0.0391 |           0.0000 |
[32m[20221213 20:09:16 @agent_ppo2.py:130][0m Policy update time: 2.77 s
[32m[20221213 20:09:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.51
[32m[20221213 20:09:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.94
[32m[20221213 20:09:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.54
[32m[20221213 20:09:17 @agent_ppo2.py:143][0m Total time:       0.84 min
[32m[20221213 20:09:17 @agent_ppo2.py:145][0m 26624 total steps have happened
[32m[20221213 20:09:17 @agent_ppo2.py:121][0m #------------------------ Iteration 13 --------------------------#
[32m[20221213 20:09:17 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:09:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:18 @agent_ppo2.py:185][0m |          -0.0002 |           0.0519 |           0.0000 |
[32m[20221213 20:09:18 @agent_ppo2.py:185][0m |          -0.0097 |           0.0452 |           0.0000 |
[32m[20221213 20:09:18 @agent_ppo2.py:185][0m |          -0.0143 |           0.0412 |           0.0000 |
[32m[20221213 20:09:19 @agent_ppo2.py:185][0m |          -0.0170 |           0.0397 |           0.0000 |
[32m[20221213 20:09:19 @agent_ppo2.py:185][0m |          -0.0187 |           0.0377 |           0.0000 |
[32m[20221213 20:09:19 @agent_ppo2.py:185][0m |          -0.0201 |           0.0364 |           0.0000 |
[32m[20221213 20:09:20 @agent_ppo2.py:185][0m |          -0.0223 |           0.0358 |           0.0000 |
[32m[20221213 20:09:20 @agent_ppo2.py:185][0m |          -0.0262 |           0.0352 |           0.0000 |
[32m[20221213 20:09:20 @agent_ppo2.py:185][0m |          -0.0245 |           0.0350 |           0.0000 |
[32m[20221213 20:09:20 @agent_ppo2.py:185][0m |          -0.0253 |           0.0329 |           0.0000 |
[32m[20221213 20:09:20 @agent_ppo2.py:130][0m Policy update time: 2.84 s
[32m[20221213 20:09:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.48
[32m[20221213 20:09:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.83
[32m[20221213 20:09:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.69
[32m[20221213 20:09:21 @agent_ppo2.py:143][0m Total time:       0.91 min
[32m[20221213 20:09:21 @agent_ppo2.py:145][0m 28672 total steps have happened
[32m[20221213 20:09:21 @agent_ppo2.py:121][0m #------------------------ Iteration 14 --------------------------#
[32m[20221213 20:09:21 @agent_ppo2.py:127][0m Sampling time: 0.59 s by 10 slaves
[32m[20221213 20:09:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:22 @agent_ppo2.py:185][0m |          -0.0008 |           0.0416 |           0.0000 |
[32m[20221213 20:09:22 @agent_ppo2.py:185][0m |          -0.0091 |           0.0235 |           0.0000 |
[32m[20221213 20:09:22 @agent_ppo2.py:185][0m |          -0.0130 |           0.0229 |           0.0000 |
[32m[20221213 20:09:23 @agent_ppo2.py:185][0m |          -0.0168 |           0.0222 |           0.0000 |
[32m[20221213 20:09:23 @agent_ppo2.py:185][0m |          -0.0149 |           0.0221 |           0.0000 |
[32m[20221213 20:09:23 @agent_ppo2.py:185][0m |          -0.0213 |           0.0217 |           0.0000 |
[32m[20221213 20:09:23 @agent_ppo2.py:185][0m |          -0.0215 |           0.0215 |           0.0000 |
[32m[20221213 20:09:24 @agent_ppo2.py:185][0m |          -0.0210 |           0.0215 |           0.0000 |
[32m[20221213 20:09:24 @agent_ppo2.py:185][0m |          -0.0243 |           0.0210 |           0.0000 |
[32m[20221213 20:09:24 @agent_ppo2.py:185][0m |          -0.0249 |           0.0207 |           0.0000 |
[32m[20221213 20:09:24 @agent_ppo2.py:130][0m Policy update time: 2.77 s
[32m[20221213 20:09:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.74
[32m[20221213 20:09:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.50
[32m[20221213 20:09:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.30
[32m[20221213 20:09:25 @agent_ppo2.py:143][0m Total time:       0.97 min
[32m[20221213 20:09:25 @agent_ppo2.py:145][0m 30720 total steps have happened
[32m[20221213 20:09:25 @agent_ppo2.py:121][0m #------------------------ Iteration 15 --------------------------#
[32m[20221213 20:09:25 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:09:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:26 @agent_ppo2.py:185][0m |           0.0018 |           0.0641 |           0.0000 |
[32m[20221213 20:09:26 @agent_ppo2.py:185][0m |          -0.0078 |           0.0451 |           0.0000 |
[32m[20221213 20:09:26 @agent_ppo2.py:185][0m |          -0.0098 |           0.0412 |           0.0000 |
[32m[20221213 20:09:26 @agent_ppo2.py:185][0m |          -0.0124 |           0.0379 |           0.0000 |
[32m[20221213 20:09:27 @agent_ppo2.py:185][0m |          -0.0142 |           0.0365 |           0.0000 |
[32m[20221213 20:09:27 @agent_ppo2.py:185][0m |          -0.0186 |           0.0351 |           0.0000 |
[32m[20221213 20:09:27 @agent_ppo2.py:185][0m |          -0.0166 |           0.0337 |           0.0000 |
[32m[20221213 20:09:27 @agent_ppo2.py:185][0m |          -0.0178 |           0.0324 |           0.0000 |
[32m[20221213 20:09:28 @agent_ppo2.py:185][0m |          -0.0210 |           0.0320 |           0.0000 |
[32m[20221213 20:09:28 @agent_ppo2.py:185][0m |          -0.0186 |           0.0319 |           0.0000 |
[32m[20221213 20:09:28 @agent_ppo2.py:130][0m Policy update time: 2.75 s
[32m[20221213 20:09:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.16
[32m[20221213 20:09:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.03
[32m[20221213 20:09:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.52
[32m[20221213 20:09:28 @agent_ppo2.py:143][0m Total time:       1.03 min
[32m[20221213 20:09:28 @agent_ppo2.py:145][0m 32768 total steps have happened
[32m[20221213 20:09:28 @agent_ppo2.py:121][0m #------------------------ Iteration 16 --------------------------#
[32m[20221213 20:09:29 @agent_ppo2.py:127][0m Sampling time: 0.56 s by 10 slaves
[32m[20221213 20:09:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:29 @agent_ppo2.py:185][0m |           0.0008 |           0.0468 |           0.0000 |
[32m[20221213 20:09:30 @agent_ppo2.py:185][0m |          -0.0131 |           0.0359 |           0.0000 |
[32m[20221213 20:09:30 @agent_ppo2.py:185][0m |          -0.0190 |           0.0341 |           0.0000 |
[32m[20221213 20:09:30 @agent_ppo2.py:185][0m |          -0.0206 |           0.0333 |           0.0000 |
[32m[20221213 20:09:30 @agent_ppo2.py:185][0m |          -0.0237 |           0.0325 |           0.0000 |
[32m[20221213 20:09:31 @agent_ppo2.py:185][0m |          -0.0264 |           0.0320 |           0.0000 |
[32m[20221213 20:09:31 @agent_ppo2.py:185][0m |          -0.0283 |           0.0315 |           0.0000 |
[32m[20221213 20:09:31 @agent_ppo2.py:185][0m |          -0.0294 |           0.0311 |           0.0000 |
[32m[20221213 20:09:31 @agent_ppo2.py:185][0m |          -0.0318 |           0.0308 |           0.0000 |
[32m[20221213 20:09:32 @agent_ppo2.py:185][0m |          -0.0313 |           0.0306 |           0.0000 |
[32m[20221213 20:09:32 @agent_ppo2.py:130][0m Policy update time: 2.75 s
[32m[20221213 20:09:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.25
[32m[20221213 20:09:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.55
[32m[20221213 20:09:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.56
[32m[20221213 20:09:32 @agent_ppo2.py:143][0m Total time:       1.10 min
[32m[20221213 20:09:32 @agent_ppo2.py:145][0m 34816 total steps have happened
[32m[20221213 20:09:32 @agent_ppo2.py:121][0m #------------------------ Iteration 17 --------------------------#
[32m[20221213 20:09:33 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:09:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:33 @agent_ppo2.py:185][0m |          -0.0018 |           0.0744 |           0.0000 |
[32m[20221213 20:09:33 @agent_ppo2.py:185][0m |          -0.0079 |           0.0564 |           0.0000 |
[32m[20221213 20:09:34 @agent_ppo2.py:185][0m |          -0.0104 |           0.0525 |           0.0000 |
[32m[20221213 20:09:34 @agent_ppo2.py:185][0m |          -0.0124 |           0.0498 |           0.0000 |
[32m[20221213 20:09:34 @agent_ppo2.py:185][0m |          -0.0145 |           0.0478 |           0.0000 |
[32m[20221213 20:09:34 @agent_ppo2.py:185][0m |          -0.0156 |           0.0459 |           0.0000 |
[32m[20221213 20:09:35 @agent_ppo2.py:185][0m |          -0.0165 |           0.0440 |           0.0000 |
[32m[20221213 20:09:35 @agent_ppo2.py:185][0m |          -0.0182 |           0.0411 |           0.0000 |
[32m[20221213 20:09:35 @agent_ppo2.py:185][0m |          -0.0199 |           0.0394 |           0.0000 |
[32m[20221213 20:09:36 @agent_ppo2.py:185][0m |          -0.0228 |           0.0385 |           0.0000 |
[32m[20221213 20:09:36 @agent_ppo2.py:130][0m Policy update time: 2.94 s
[32m[20221213 20:09:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.11
[32m[20221213 20:09:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.85
[32m[20221213 20:09:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.21
[32m[20221213 20:09:36 @agent_ppo2.py:143][0m Total time:       1.16 min
[32m[20221213 20:09:36 @agent_ppo2.py:145][0m 36864 total steps have happened
[32m[20221213 20:09:36 @agent_ppo2.py:121][0m #------------------------ Iteration 18 --------------------------#
[32m[20221213 20:09:36 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:09:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:37 @agent_ppo2.py:185][0m |          -0.0029 |           0.0492 |           0.0000 |
[32m[20221213 20:09:37 @agent_ppo2.py:185][0m |          -0.0111 |           0.0361 |           0.0000 |
[32m[20221213 20:09:38 @agent_ppo2.py:185][0m |          -0.0170 |           0.0316 |           0.0000 |
[32m[20221213 20:09:38 @agent_ppo2.py:185][0m |          -0.0205 |           0.0293 |           0.0000 |
[32m[20221213 20:09:38 @agent_ppo2.py:185][0m |          -0.0215 |           0.0280 |           0.0000 |
[32m[20221213 20:09:38 @agent_ppo2.py:185][0m |          -0.0234 |           0.0273 |           0.0000 |
[32m[20221213 20:09:39 @agent_ppo2.py:185][0m |          -0.0258 |           0.0263 |           0.0000 |
[32m[20221213 20:09:39 @agent_ppo2.py:185][0m |          -0.0253 |           0.0263 |           0.0000 |
[32m[20221213 20:09:39 @agent_ppo2.py:185][0m |          -0.0299 |           0.0254 |           0.0000 |
[32m[20221213 20:09:39 @agent_ppo2.py:185][0m |          -0.0285 |           0.0252 |           0.0000 |
[32m[20221213 20:09:39 @agent_ppo2.py:130][0m Policy update time: 2.93 s
[32m[20221213 20:09:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.83
[32m[20221213 20:09:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.50
[32m[20221213 20:09:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.22
[32m[20221213 20:09:40 @agent_ppo2.py:143][0m Total time:       1.23 min
[32m[20221213 20:09:40 @agent_ppo2.py:145][0m 38912 total steps have happened
[32m[20221213 20:09:40 @agent_ppo2.py:121][0m #------------------------ Iteration 19 --------------------------#
[32m[20221213 20:09:40 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:09:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:41 @agent_ppo2.py:185][0m |          -0.0018 |           0.0191 |           0.0000 |
[32m[20221213 20:09:41 @agent_ppo2.py:185][0m |          -0.0136 |           0.0169 |           0.0000 |
[32m[20221213 20:09:41 @agent_ppo2.py:185][0m |          -0.0182 |           0.0165 |           0.0000 |
[32m[20221213 20:09:42 @agent_ppo2.py:185][0m |          -0.0200 |           0.0160 |           0.0000 |
[32m[20221213 20:09:42 @agent_ppo2.py:185][0m |          -0.0242 |           0.0156 |           0.0000 |
[32m[20221213 20:09:42 @agent_ppo2.py:185][0m |          -0.0262 |           0.0153 |           0.0000 |
[32m[20221213 20:09:42 @agent_ppo2.py:185][0m |          -0.0273 |           0.0151 |           0.0000 |
[32m[20221213 20:09:43 @agent_ppo2.py:185][0m |          -0.0296 |           0.0150 |           0.0000 |
[32m[20221213 20:09:43 @agent_ppo2.py:185][0m |          -0.0300 |           0.0148 |           0.0000 |
[32m[20221213 20:09:43 @agent_ppo2.py:185][0m |          -0.0295 |           0.0145 |           0.0000 |
[32m[20221213 20:09:43 @agent_ppo2.py:130][0m Policy update time: 2.77 s
[32m[20221213 20:09:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.47
[32m[20221213 20:09:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.44
[32m[20221213 20:09:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.81
[32m[20221213 20:09:44 @agent_ppo2.py:143][0m Total time:       1.29 min
[32m[20221213 20:09:44 @agent_ppo2.py:145][0m 40960 total steps have happened
[32m[20221213 20:09:44 @agent_ppo2.py:121][0m #------------------------ Iteration 20 --------------------------#
[32m[20221213 20:09:44 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:09:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:45 @agent_ppo2.py:185][0m |          -0.0003 |           0.0600 |           0.0000 |
[32m[20221213 20:09:45 @agent_ppo2.py:185][0m |          -0.0070 |           0.0414 |           0.0000 |
[32m[20221213 20:09:45 @agent_ppo2.py:185][0m |          -0.0110 |           0.0349 |           0.0000 |
[32m[20221213 20:09:45 @agent_ppo2.py:185][0m |          -0.0122 |           0.0317 |           0.0000 |
[32m[20221213 20:09:46 @agent_ppo2.py:185][0m |          -0.0159 |           0.0303 |           0.0000 |
[32m[20221213 20:09:46 @agent_ppo2.py:185][0m |          -0.0169 |           0.0290 |           0.0000 |
[32m[20221213 20:09:46 @agent_ppo2.py:185][0m |          -0.0171 |           0.0276 |           0.0000 |
[32m[20221213 20:09:47 @agent_ppo2.py:185][0m |          -0.0186 |           0.0269 |           0.0000 |
[32m[20221213 20:09:47 @agent_ppo2.py:185][0m |          -0.0197 |           0.0261 |           0.0000 |
[32m[20221213 20:09:47 @agent_ppo2.py:185][0m |          -0.0203 |           0.0261 |           0.0000 |
[32m[20221213 20:09:47 @agent_ppo2.py:130][0m Policy update time: 2.98 s
[32m[20221213 20:09:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.55
[32m[20221213 20:09:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.48
[32m[20221213 20:09:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.80
[32m[20221213 20:09:48 @agent_ppo2.py:143][0m Total time:       1.36 min
[32m[20221213 20:09:48 @agent_ppo2.py:145][0m 43008 total steps have happened
[32m[20221213 20:09:48 @agent_ppo2.py:121][0m #------------------------ Iteration 21 --------------------------#
[32m[20221213 20:09:48 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:09:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:49 @agent_ppo2.py:185][0m |           0.0004 |           0.0172 |           0.0000 |
[32m[20221213 20:09:49 @agent_ppo2.py:185][0m |          -0.0097 |           0.0117 |           0.0000 |
[32m[20221213 20:09:49 @agent_ppo2.py:185][0m |          -0.0136 |           0.0113 |           0.0000 |
[32m[20221213 20:09:50 @agent_ppo2.py:185][0m |          -0.0177 |           0.0110 |           0.0000 |
[32m[20221213 20:09:50 @agent_ppo2.py:185][0m |          -0.0196 |           0.0108 |           0.0000 |
[32m[20221213 20:09:50 @agent_ppo2.py:185][0m |          -0.0202 |           0.0106 |           0.0000 |
[32m[20221213 20:09:50 @agent_ppo2.py:185][0m |          -0.0225 |           0.0104 |           0.0000 |
[32m[20221213 20:09:51 @agent_ppo2.py:185][0m |          -0.0233 |           0.0102 |           0.0000 |
[32m[20221213 20:09:51 @agent_ppo2.py:185][0m |          -0.0239 |           0.0100 |           0.0000 |
[32m[20221213 20:09:51 @agent_ppo2.py:185][0m |          -0.0245 |           0.0098 |           0.0000 |
[32m[20221213 20:09:51 @agent_ppo2.py:130][0m Policy update time: 2.98 s
[32m[20221213 20:09:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.50
[32m[20221213 20:09:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.36
[32m[20221213 20:09:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.15
[32m[20221213 20:09:52 @agent_ppo2.py:143][0m Total time:       1.42 min
[32m[20221213 20:09:52 @agent_ppo2.py:145][0m 45056 total steps have happened
[32m[20221213 20:09:52 @agent_ppo2.py:121][0m #------------------------ Iteration 22 --------------------------#
[32m[20221213 20:09:52 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:09:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:53 @agent_ppo2.py:185][0m |           0.0019 |           0.0169 |           0.0000 |
[32m[20221213 20:09:53 @agent_ppo2.py:185][0m |          -0.0055 |           0.0120 |           0.0000 |
[32m[20221213 20:09:53 @agent_ppo2.py:185][0m |          -0.0090 |           0.0115 |           0.0000 |
[32m[20221213 20:09:53 @agent_ppo2.py:185][0m |          -0.0158 |           0.0110 |           0.0000 |
[32m[20221213 20:09:54 @agent_ppo2.py:185][0m |          -0.0124 |           0.0109 |           0.0000 |
[32m[20221213 20:09:54 @agent_ppo2.py:185][0m |          -0.0141 |           0.0108 |           0.0000 |
[32m[20221213 20:09:54 @agent_ppo2.py:185][0m |          -0.0144 |           0.0106 |           0.0000 |
[32m[20221213 20:09:54 @agent_ppo2.py:185][0m |          -0.0160 |           0.0106 |           0.0000 |
[32m[20221213 20:09:55 @agent_ppo2.py:185][0m |          -0.0160 |           0.0104 |           0.0000 |
[32m[20221213 20:09:55 @agent_ppo2.py:185][0m |          -0.0164 |           0.0103 |           0.0000 |
[32m[20221213 20:09:55 @agent_ppo2.py:130][0m Policy update time: 2.91 s
[32m[20221213 20:09:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.72
[32m[20221213 20:09:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.34
[32m[20221213 20:09:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.50
[32m[20221213 20:09:56 @agent_ppo2.py:143][0m Total time:       1.49 min
[32m[20221213 20:09:56 @agent_ppo2.py:145][0m 47104 total steps have happened
[32m[20221213 20:09:56 @agent_ppo2.py:121][0m #------------------------ Iteration 23 --------------------------#
[32m[20221213 20:09:56 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:09:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:09:57 @agent_ppo2.py:185][0m |           0.0007 |           0.0286 |           0.0000 |
[32m[20221213 20:09:57 @agent_ppo2.py:185][0m |          -0.0071 |           0.0222 |           0.0000 |
[32m[20221213 20:09:57 @agent_ppo2.py:185][0m |          -0.0109 |           0.0208 |           0.0000 |
[32m[20221213 20:09:57 @agent_ppo2.py:185][0m |          -0.0137 |           0.0210 |           0.0000 |
[32m[20221213 20:09:58 @agent_ppo2.py:185][0m |          -0.0174 |           0.0193 |           0.0000 |
[32m[20221213 20:09:58 @agent_ppo2.py:185][0m |          -0.0167 |           0.0193 |           0.0000 |
[32m[20221213 20:09:58 @agent_ppo2.py:185][0m |          -0.0174 |           0.0188 |           0.0000 |
[32m[20221213 20:09:59 @agent_ppo2.py:185][0m |          -0.0195 |           0.0184 |           0.0000 |
[32m[20221213 20:09:59 @agent_ppo2.py:185][0m |          -0.0197 |           0.0181 |           0.0000 |
[32m[20221213 20:09:59 @agent_ppo2.py:185][0m |          -0.0215 |           0.0179 |           0.0000 |
[32m[20221213 20:09:59 @agent_ppo2.py:130][0m Policy update time: 3.01 s
[32m[20221213 20:10:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.10
[32m[20221213 20:10:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.20
[32m[20221213 20:10:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.16
[32m[20221213 20:10:00 @agent_ppo2.py:143][0m Total time:       1.55 min
[32m[20221213 20:10:00 @agent_ppo2.py:145][0m 49152 total steps have happened
[32m[20221213 20:10:00 @agent_ppo2.py:121][0m #------------------------ Iteration 24 --------------------------#
[32m[20221213 20:10:00 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:10:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:01 @agent_ppo2.py:185][0m |          -0.0013 |           0.0146 |           0.0000 |
[32m[20221213 20:10:01 @agent_ppo2.py:185][0m |          -0.0134 |           0.0123 |           0.0000 |
[32m[20221213 20:10:01 @agent_ppo2.py:185][0m |          -0.0198 |           0.0119 |           0.0000 |
[32m[20221213 20:10:01 @agent_ppo2.py:185][0m |          -0.0220 |           0.0113 |           0.0000 |
[32m[20221213 20:10:02 @agent_ppo2.py:185][0m |          -0.0234 |           0.0108 |           0.0000 |
[32m[20221213 20:10:02 @agent_ppo2.py:185][0m |          -0.0249 |           0.0106 |           0.0000 |
[32m[20221213 20:10:02 @agent_ppo2.py:185][0m |          -0.0264 |           0.0105 |           0.0000 |
[32m[20221213 20:10:02 @agent_ppo2.py:185][0m |          -0.0267 |           0.0103 |           0.0000 |
[32m[20221213 20:10:03 @agent_ppo2.py:185][0m |          -0.0296 |           0.0102 |           0.0000 |
[32m[20221213 20:10:03 @agent_ppo2.py:185][0m |          -0.0321 |           0.0101 |           0.0000 |
[32m[20221213 20:10:03 @agent_ppo2.py:130][0m Policy update time: 2.93 s
[32m[20221213 20:10:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.93
[32m[20221213 20:10:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.14
[32m[20221213 20:10:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.66
[32m[20221213 20:10:04 @agent_ppo2.py:143][0m Total time:       1.62 min
[32m[20221213 20:10:04 @agent_ppo2.py:145][0m 51200 total steps have happened
[32m[20221213 20:10:04 @agent_ppo2.py:121][0m #------------------------ Iteration 25 --------------------------#
[32m[20221213 20:10:04 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:10:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:04 @agent_ppo2.py:185][0m |          -0.0070 |           0.0130 |           0.0000 |
[32m[20221213 20:10:05 @agent_ppo2.py:185][0m |          -0.0053 |           0.0120 |           0.0000 |
[32m[20221213 20:10:05 @agent_ppo2.py:185][0m |          -0.0093 |           0.0113 |           0.0000 |
[32m[20221213 20:10:05 @agent_ppo2.py:185][0m |          -0.0127 |           0.0111 |           0.0000 |
[32m[20221213 20:10:06 @agent_ppo2.py:185][0m |          -0.0143 |           0.0107 |           0.0000 |
[32m[20221213 20:10:06 @agent_ppo2.py:185][0m |          -0.0161 |           0.0105 |           0.0000 |
[32m[20221213 20:10:06 @agent_ppo2.py:185][0m |          -0.0179 |           0.0105 |           0.0000 |
[32m[20221213 20:10:06 @agent_ppo2.py:185][0m |          -0.0187 |           0.0104 |           0.0000 |
[32m[20221213 20:10:07 @agent_ppo2.py:185][0m |          -0.0198 |           0.0103 |           0.0000 |
[32m[20221213 20:10:07 @agent_ppo2.py:185][0m |          -0.0206 |           0.0102 |           0.0000 |
[32m[20221213 20:10:07 @agent_ppo2.py:130][0m Policy update time: 2.91 s
[32m[20221213 20:10:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.09
[32m[20221213 20:10:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.37
[32m[20221213 20:10:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.78
[32m[20221213 20:10:08 @agent_ppo2.py:143][0m Total time:       1.69 min
[32m[20221213 20:10:08 @agent_ppo2.py:145][0m 53248 total steps have happened
[32m[20221213 20:10:08 @agent_ppo2.py:121][0m #------------------------ Iteration 26 --------------------------#
[32m[20221213 20:10:08 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:10:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:09 @agent_ppo2.py:185][0m |           0.0008 |           0.0238 |           0.0000 |
[32m[20221213 20:10:09 @agent_ppo2.py:185][0m |          -0.0077 |           0.0176 |           0.0000 |
[32m[20221213 20:10:09 @agent_ppo2.py:185][0m |          -0.0107 |           0.0165 |           0.0000 |
[32m[20221213 20:10:09 @agent_ppo2.py:185][0m |          -0.0128 |           0.0154 |           0.0000 |
[32m[20221213 20:10:10 @agent_ppo2.py:185][0m |          -0.0141 |           0.0150 |           0.0000 |
[32m[20221213 20:10:10 @agent_ppo2.py:185][0m |          -0.0155 |           0.0149 |           0.0000 |
[32m[20221213 20:10:10 @agent_ppo2.py:185][0m |          -0.0158 |           0.0147 |           0.0000 |
[32m[20221213 20:10:10 @agent_ppo2.py:185][0m |          -0.0166 |           0.0145 |           0.0000 |
[32m[20221213 20:10:11 @agent_ppo2.py:185][0m |          -0.0170 |           0.0143 |           0.0000 |
[32m[20221213 20:10:11 @agent_ppo2.py:185][0m |          -0.0171 |           0.0142 |           0.0000 |
[32m[20221213 20:10:11 @agent_ppo2.py:130][0m Policy update time: 2.78 s
[32m[20221213 20:10:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.19
[32m[20221213 20:10:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.37
[32m[20221213 20:10:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.72
[32m[20221213 20:10:11 @agent_ppo2.py:143][0m Total time:       1.75 min
[32m[20221213 20:10:11 @agent_ppo2.py:145][0m 55296 total steps have happened
[32m[20221213 20:10:11 @agent_ppo2.py:121][0m #------------------------ Iteration 27 --------------------------#
[32m[20221213 20:10:12 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:10:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:12 @agent_ppo2.py:185][0m |          -0.0009 |           0.0128 |           0.0000 |
[32m[20221213 20:10:13 @agent_ppo2.py:185][0m |          -0.0099 |           0.0080 |           0.0000 |
[32m[20221213 20:10:13 @agent_ppo2.py:185][0m |          -0.0136 |           0.0078 |           0.0000 |
[32m[20221213 20:10:13 @agent_ppo2.py:185][0m |          -0.0157 |           0.0075 |           0.0000 |
[32m[20221213 20:10:13 @agent_ppo2.py:185][0m |          -0.0170 |           0.0074 |           0.0000 |
[32m[20221213 20:10:14 @agent_ppo2.py:185][0m |          -0.0155 |           0.0071 |           0.0000 |
[32m[20221213 20:10:14 @agent_ppo2.py:185][0m |          -0.0197 |           0.0069 |           0.0000 |
[32m[20221213 20:10:14 @agent_ppo2.py:185][0m |          -0.0202 |           0.0067 |           0.0000 |
[32m[20221213 20:10:14 @agent_ppo2.py:185][0m |          -0.0188 |           0.0066 |           0.0000 |
[32m[20221213 20:10:15 @agent_ppo2.py:185][0m |          -0.0216 |           0.0065 |           0.0000 |
[32m[20221213 20:10:15 @agent_ppo2.py:130][0m Policy update time: 2.82 s
[32m[20221213 20:10:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.80
[32m[20221213 20:10:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.55
[32m[20221213 20:10:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.59
[32m[20221213 20:10:15 @agent_ppo2.py:143][0m Total time:       1.81 min
[32m[20221213 20:10:15 @agent_ppo2.py:145][0m 57344 total steps have happened
[32m[20221213 20:10:15 @agent_ppo2.py:121][0m #------------------------ Iteration 28 --------------------------#
[32m[20221213 20:10:16 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:10:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:16 @agent_ppo2.py:185][0m |          -0.0007 |           0.0137 |           0.0000 |
[32m[20221213 20:10:16 @agent_ppo2.py:185][0m |          -0.0086 |           0.0109 |           0.0000 |
[32m[20221213 20:10:17 @agent_ppo2.py:185][0m |          -0.0133 |           0.0104 |           0.0000 |
[32m[20221213 20:10:17 @agent_ppo2.py:185][0m |          -0.0128 |           0.0103 |           0.0000 |
[32m[20221213 20:10:17 @agent_ppo2.py:185][0m |          -0.0137 |           0.0101 |           0.0000 |
[32m[20221213 20:10:18 @agent_ppo2.py:185][0m |          -0.0151 |           0.0100 |           0.0000 |
[32m[20221213 20:10:18 @agent_ppo2.py:185][0m |          -0.0157 |           0.0099 |           0.0000 |
[32m[20221213 20:10:18 @agent_ppo2.py:185][0m |          -0.0164 |           0.0098 |           0.0000 |
[32m[20221213 20:10:18 @agent_ppo2.py:185][0m |          -0.0169 |           0.0096 |           0.0000 |
[32m[20221213 20:10:19 @agent_ppo2.py:185][0m |          -0.0176 |           0.0096 |           0.0000 |
[32m[20221213 20:10:19 @agent_ppo2.py:130][0m Policy update time: 3.09 s
[32m[20221213 20:10:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.76
[32m[20221213 20:10:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.90
[32m[20221213 20:10:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.24
[32m[20221213 20:10:19 @agent_ppo2.py:143][0m Total time:       1.88 min
[32m[20221213 20:10:19 @agent_ppo2.py:145][0m 59392 total steps have happened
[32m[20221213 20:10:19 @agent_ppo2.py:121][0m #------------------------ Iteration 29 --------------------------#
[32m[20221213 20:10:20 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:10:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:20 @agent_ppo2.py:185][0m |          -0.0004 |           0.1197 |           0.0000 |
[32m[20221213 20:10:20 @agent_ppo2.py:185][0m |          -0.0072 |           0.0566 |           0.0000 |
[32m[20221213 20:10:21 @agent_ppo2.py:185][0m |          -0.0108 |           0.0453 |           0.0000 |
[32m[20221213 20:10:21 @agent_ppo2.py:185][0m |          -0.0139 |           0.0411 |           0.0000 |
[32m[20221213 20:10:21 @agent_ppo2.py:185][0m |          -0.0144 |           0.0378 |           0.0000 |
[32m[20221213 20:10:21 @agent_ppo2.py:185][0m |          -0.0157 |           0.0367 |           0.0000 |
[32m[20221213 20:10:22 @agent_ppo2.py:185][0m |          -0.0165 |           0.0339 |           0.0000 |
[32m[20221213 20:10:22 @agent_ppo2.py:185][0m |          -0.0167 |           0.0330 |           0.0000 |
[32m[20221213 20:10:22 @agent_ppo2.py:185][0m |          -0.0177 |           0.0319 |           0.0000 |
[32m[20221213 20:10:22 @agent_ppo2.py:185][0m |          -0.0183 |           0.0315 |           0.0000 |
[32m[20221213 20:10:22 @agent_ppo2.py:130][0m Policy update time: 2.77 s
[32m[20221213 20:10:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.53
[32m[20221213 20:10:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.38
[32m[20221213 20:10:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.77
[32m[20221213 20:10:23 @agent_ppo2.py:143][0m Total time:       1.94 min
[32m[20221213 20:10:23 @agent_ppo2.py:145][0m 61440 total steps have happened
[32m[20221213 20:10:23 @agent_ppo2.py:121][0m #------------------------ Iteration 30 --------------------------#
[32m[20221213 20:10:23 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:10:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:24 @agent_ppo2.py:185][0m |          -0.0004 |           0.0328 |           0.0000 |
[32m[20221213 20:10:24 @agent_ppo2.py:185][0m |          -0.0113 |           0.0274 |           0.0000 |
[32m[20221213 20:10:24 @agent_ppo2.py:185][0m |          -0.0168 |           0.0254 |           0.0000 |
[32m[20221213 20:10:25 @agent_ppo2.py:185][0m |          -0.0187 |           0.0242 |           0.0000 |
[32m[20221213 20:10:25 @agent_ppo2.py:185][0m |          -0.0229 |           0.0237 |           0.0000 |
[32m[20221213 20:10:25 @agent_ppo2.py:185][0m |          -0.0223 |           0.0225 |           0.0000 |
[32m[20221213 20:10:25 @agent_ppo2.py:185][0m |          -0.0254 |           0.0223 |           0.0000 |
[32m[20221213 20:10:26 @agent_ppo2.py:185][0m |          -0.0276 |           0.0217 |           0.0000 |
[32m[20221213 20:10:26 @agent_ppo2.py:185][0m |          -0.0286 |           0.0213 |           0.0000 |
[32m[20221213 20:10:26 @agent_ppo2.py:185][0m |          -0.0301 |           0.0210 |           0.0000 |
[32m[20221213 20:10:26 @agent_ppo2.py:130][0m Policy update time: 2.83 s
[32m[20221213 20:10:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.19
[32m[20221213 20:10:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.04
[32m[20221213 20:10:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.47
[32m[20221213 20:10:27 @agent_ppo2.py:143][0m Total time:       2.01 min
[32m[20221213 20:10:27 @agent_ppo2.py:145][0m 63488 total steps have happened
[32m[20221213 20:10:27 @agent_ppo2.py:121][0m #------------------------ Iteration 31 --------------------------#
[32m[20221213 20:10:27 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:10:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:28 @agent_ppo2.py:185][0m |           0.0012 |           0.0492 |           0.0000 |
[32m[20221213 20:10:28 @agent_ppo2.py:185][0m |          -0.0054 |           0.0338 |           0.0000 |
[32m[20221213 20:10:28 @agent_ppo2.py:185][0m |          -0.0114 |           0.0316 |           0.0000 |
[32m[20221213 20:10:29 @agent_ppo2.py:185][0m |          -0.0126 |           0.0283 |           0.0000 |
[32m[20221213 20:10:29 @agent_ppo2.py:185][0m |          -0.0152 |           0.0277 |           0.0000 |
[32m[20221213 20:10:29 @agent_ppo2.py:185][0m |          -0.0163 |           0.0271 |           0.0000 |
[32m[20221213 20:10:29 @agent_ppo2.py:185][0m |          -0.0190 |           0.0253 |           0.0000 |
[32m[20221213 20:10:30 @agent_ppo2.py:185][0m |          -0.0217 |           0.0251 |           0.0000 |
[32m[20221213 20:10:30 @agent_ppo2.py:185][0m |          -0.0183 |           0.0239 |           0.0000 |
[32m[20221213 20:10:30 @agent_ppo2.py:185][0m |          -0.0196 |           0.0239 |           0.0000 |
[32m[20221213 20:10:30 @agent_ppo2.py:130][0m Policy update time: 2.83 s
[32m[20221213 20:10:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.43
[32m[20221213 20:10:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.90
[32m[20221213 20:10:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.16
[32m[20221213 20:10:31 @agent_ppo2.py:143][0m Total time:       2.07 min
[32m[20221213 20:10:31 @agent_ppo2.py:145][0m 65536 total steps have happened
[32m[20221213 20:10:31 @agent_ppo2.py:121][0m #------------------------ Iteration 32 --------------------------#
[32m[20221213 20:10:31 @agent_ppo2.py:127][0m Sampling time: 0.53 s by 10 slaves
[32m[20221213 20:10:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:32 @agent_ppo2.py:185][0m |          -0.0003 |           0.0224 |           0.0000 |
[32m[20221213 20:10:32 @agent_ppo2.py:185][0m |          -0.0126 |           0.0175 |           0.0000 |
[32m[20221213 20:10:32 @agent_ppo2.py:185][0m |          -0.0176 |           0.0168 |           0.0000 |
[32m[20221213 20:10:32 @agent_ppo2.py:185][0m |          -0.0208 |           0.0162 |           0.0000 |
[32m[20221213 20:10:33 @agent_ppo2.py:185][0m |          -0.0229 |           0.0157 |           0.0000 |
[32m[20221213 20:10:33 @agent_ppo2.py:185][0m |          -0.0238 |           0.0154 |           0.0000 |
[32m[20221213 20:10:33 @agent_ppo2.py:185][0m |          -0.0270 |           0.0152 |           0.0000 |
[32m[20221213 20:10:34 @agent_ppo2.py:185][0m |          -0.0285 |           0.0151 |           0.0000 |
[32m[20221213 20:10:34 @agent_ppo2.py:185][0m |          -0.0284 |           0.0147 |           0.0000 |
[32m[20221213 20:10:34 @agent_ppo2.py:185][0m |          -0.0291 |           0.0144 |           0.0000 |
[32m[20221213 20:10:34 @agent_ppo2.py:130][0m Policy update time: 2.87 s
[32m[20221213 20:10:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.45
[32m[20221213 20:10:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.57
[32m[20221213 20:10:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.63
[32m[20221213 20:10:35 @agent_ppo2.py:143][0m Total time:       2.14 min
[32m[20221213 20:10:35 @agent_ppo2.py:145][0m 67584 total steps have happened
[32m[20221213 20:10:35 @agent_ppo2.py:121][0m #------------------------ Iteration 33 --------------------------#
[32m[20221213 20:10:35 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:10:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:36 @agent_ppo2.py:185][0m |          -0.0010 |           0.0136 |           0.0000 |
[32m[20221213 20:10:36 @agent_ppo2.py:185][0m |          -0.0110 |           0.0122 |           0.0000 |
[32m[20221213 20:10:36 @agent_ppo2.py:185][0m |          -0.0144 |           0.0117 |           0.0000 |
[32m[20221213 20:10:36 @agent_ppo2.py:185][0m |          -0.0203 |           0.0114 |           0.0000 |
[32m[20221213 20:10:37 @agent_ppo2.py:185][0m |          -0.0219 |           0.0110 |           0.0000 |
[32m[20221213 20:10:37 @agent_ppo2.py:185][0m |          -0.0251 |           0.0108 |           0.0000 |
[32m[20221213 20:10:37 @agent_ppo2.py:185][0m |          -0.0261 |           0.0106 |           0.0000 |
[32m[20221213 20:10:37 @agent_ppo2.py:185][0m |          -0.0273 |           0.0106 |           0.0000 |
[32m[20221213 20:10:38 @agent_ppo2.py:185][0m |          -0.0293 |           0.0104 |           0.0000 |
[32m[20221213 20:10:38 @agent_ppo2.py:185][0m |          -0.0312 |           0.0103 |           0.0000 |
[32m[20221213 20:10:38 @agent_ppo2.py:130][0m Policy update time: 2.79 s
[32m[20221213 20:10:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.07
[32m[20221213 20:10:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.78
[32m[20221213 20:10:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.69
[32m[20221213 20:10:38 @agent_ppo2.py:143][0m Total time:       2.20 min
[32m[20221213 20:10:38 @agent_ppo2.py:145][0m 69632 total steps have happened
[32m[20221213 20:10:38 @agent_ppo2.py:121][0m #------------------------ Iteration 34 --------------------------#
[32m[20221213 20:10:39 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:10:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:39 @agent_ppo2.py:185][0m |          -0.0017 |           0.0380 |           0.0000 |
[32m[20221213 20:10:40 @agent_ppo2.py:185][0m |          -0.0081 |           0.0245 |           0.0000 |
[32m[20221213 20:10:40 @agent_ppo2.py:185][0m |          -0.0121 |           0.0213 |           0.0000 |
[32m[20221213 20:10:40 @agent_ppo2.py:185][0m |          -0.0144 |           0.0197 |           0.0000 |
[32m[20221213 20:10:40 @agent_ppo2.py:185][0m |          -0.0160 |           0.0195 |           0.0000 |
[32m[20221213 20:10:41 @agent_ppo2.py:185][0m |          -0.0167 |           0.0191 |           0.0000 |
[32m[20221213 20:10:41 @agent_ppo2.py:185][0m |          -0.0174 |           0.0182 |           0.0000 |
[32m[20221213 20:10:41 @agent_ppo2.py:185][0m |          -0.0176 |           0.0181 |           0.0000 |
[32m[20221213 20:10:41 @agent_ppo2.py:185][0m |          -0.0184 |           0.0178 |           0.0000 |
[32m[20221213 20:10:42 @agent_ppo2.py:185][0m |          -0.0188 |           0.0175 |           0.0000 |
[32m[20221213 20:10:42 @agent_ppo2.py:130][0m Policy update time: 2.80 s
[32m[20221213 20:10:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.10
[32m[20221213 20:10:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.05
[32m[20221213 20:10:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.89
[32m[20221213 20:10:42 @agent_ppo2.py:143][0m Total time:       2.26 min
[32m[20221213 20:10:42 @agent_ppo2.py:145][0m 71680 total steps have happened
[32m[20221213 20:10:42 @agent_ppo2.py:121][0m #------------------------ Iteration 35 --------------------------#
[32m[20221213 20:10:43 @agent_ppo2.py:127][0m Sampling time: 0.53 s by 10 slaves
[32m[20221213 20:10:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:43 @agent_ppo2.py:185][0m |           0.0018 |           0.0204 |           0.0000 |
[32m[20221213 20:10:43 @agent_ppo2.py:185][0m |          -0.0111 |           0.0183 |           0.0000 |
[32m[20221213 20:10:44 @agent_ppo2.py:185][0m |          -0.0155 |           0.0169 |           0.0000 |
[32m[20221213 20:10:44 @agent_ppo2.py:185][0m |          -0.0206 |           0.0163 |           0.0000 |
[32m[20221213 20:10:44 @agent_ppo2.py:185][0m |          -0.0222 |           0.0161 |           0.0000 |
[32m[20221213 20:10:44 @agent_ppo2.py:185][0m |          -0.0204 |           0.0157 |           0.0000 |
[32m[20221213 20:10:45 @agent_ppo2.py:185][0m |          -0.0260 |           0.0157 |           0.0000 |
[32m[20221213 20:10:45 @agent_ppo2.py:185][0m |          -0.0260 |           0.0152 |           0.0000 |
[32m[20221213 20:10:45 @agent_ppo2.py:185][0m |          -0.0271 |           0.0148 |           0.0000 |
[32m[20221213 20:10:46 @agent_ppo2.py:185][0m |          -0.0294 |           0.0148 |           0.0000 |
[32m[20221213 20:10:46 @agent_ppo2.py:130][0m Policy update time: 2.99 s
[32m[20221213 20:10:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.67
[32m[20221213 20:10:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.52
[32m[20221213 20:10:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.91
[32m[20221213 20:10:46 @agent_ppo2.py:143][0m Total time:       2.33 min
[32m[20221213 20:10:46 @agent_ppo2.py:145][0m 73728 total steps have happened
[32m[20221213 20:10:46 @agent_ppo2.py:121][0m #------------------------ Iteration 36 --------------------------#
[32m[20221213 20:10:47 @agent_ppo2.py:127][0m Sampling time: 0.61 s by 10 slaves
[32m[20221213 20:10:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:47 @agent_ppo2.py:185][0m |           0.0002 |           0.0449 |           0.0000 |
[32m[20221213 20:10:48 @agent_ppo2.py:185][0m |          -0.0141 |           0.0315 |           0.0000 |
[32m[20221213 20:10:48 @agent_ppo2.py:185][0m |          -0.0128 |           0.0277 |           0.0000 |
[32m[20221213 20:10:49 @agent_ppo2.py:185][0m |          -0.0188 |           0.0256 |           0.0000 |
[32m[20221213 20:10:49 @agent_ppo2.py:185][0m |          -0.0167 |           0.0237 |           0.0000 |
[32m[20221213 20:10:49 @agent_ppo2.py:185][0m |          -0.0206 |           0.0231 |           0.0000 |
[32m[20221213 20:10:49 @agent_ppo2.py:185][0m |          -0.0188 |           0.0226 |           0.0000 |
[32m[20221213 20:10:50 @agent_ppo2.py:185][0m |          -0.0209 |           0.0217 |           0.0000 |
[32m[20221213 20:10:50 @agent_ppo2.py:185][0m |          -0.0200 |           0.0219 |           0.0000 |
[32m[20221213 20:10:50 @agent_ppo2.py:185][0m |          -0.0201 |           0.0213 |           0.0000 |
[32m[20221213 20:10:50 @agent_ppo2.py:130][0m Policy update time: 3.48 s
[32m[20221213 20:10:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.93
[32m[20221213 20:10:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.50
[32m[20221213 20:10:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.96
[32m[20221213 20:10:51 @agent_ppo2.py:143][0m Total time:       2.41 min
[32m[20221213 20:10:51 @agent_ppo2.py:145][0m 75776 total steps have happened
[32m[20221213 20:10:51 @agent_ppo2.py:121][0m #------------------------ Iteration 37 --------------------------#
[32m[20221213 20:10:51 @agent_ppo2.py:127][0m Sampling time: 0.59 s by 10 slaves
[32m[20221213 20:10:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:52 @agent_ppo2.py:185][0m |          -0.0001 |           0.0193 |           0.0000 |
[32m[20221213 20:10:52 @agent_ppo2.py:185][0m |          -0.0101 |           0.0120 |           0.0000 |
[32m[20221213 20:10:52 @agent_ppo2.py:185][0m |          -0.0108 |           0.0115 |           0.0000 |
[32m[20221213 20:10:53 @agent_ppo2.py:185][0m |          -0.0167 |           0.0112 |           0.0000 |
[32m[20221213 20:10:53 @agent_ppo2.py:185][0m |          -0.0186 |           0.0109 |           0.0000 |
[32m[20221213 20:10:53 @agent_ppo2.py:185][0m |          -0.0194 |           0.0108 |           0.0000 |
[32m[20221213 20:10:54 @agent_ppo2.py:185][0m |          -0.0203 |           0.0104 |           0.0000 |
[32m[20221213 20:10:54 @agent_ppo2.py:185][0m |          -0.0215 |           0.0102 |           0.0000 |
[32m[20221213 20:10:54 @agent_ppo2.py:185][0m |          -0.0222 |           0.0100 |           0.0000 |
[32m[20221213 20:10:54 @agent_ppo2.py:185][0m |          -0.0226 |           0.0098 |           0.0000 |
[32m[20221213 20:10:54 @agent_ppo2.py:130][0m Policy update time: 3.05 s
[32m[20221213 20:10:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.72
[32m[20221213 20:10:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.72
[32m[20221213 20:10:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.97
[32m[20221213 20:10:55 @agent_ppo2.py:143][0m Total time:       2.48 min
[32m[20221213 20:10:55 @agent_ppo2.py:145][0m 77824 total steps have happened
[32m[20221213 20:10:55 @agent_ppo2.py:121][0m #------------------------ Iteration 38 --------------------------#
[32m[20221213 20:10:56 @agent_ppo2.py:127][0m Sampling time: 0.71 s by 10 slaves
[32m[20221213 20:10:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:10:56 @agent_ppo2.py:185][0m |          -0.0001 |           0.0401 |           0.0000 |
[32m[20221213 20:10:56 @agent_ppo2.py:185][0m |          -0.0087 |           0.0245 |           0.0000 |
[32m[20221213 20:10:57 @agent_ppo2.py:185][0m |          -0.0176 |           0.0227 |           0.0000 |
[32m[20221213 20:10:57 @agent_ppo2.py:185][0m |          -0.0149 |           0.0208 |           0.0000 |
[32m[20221213 20:10:57 @agent_ppo2.py:185][0m |          -0.0161 |           0.0195 |           0.0000 |
[32m[20221213 20:10:58 @agent_ppo2.py:185][0m |          -0.0165 |           0.0189 |           0.0000 |
[32m[20221213 20:10:58 @agent_ppo2.py:185][0m |          -0.0192 |           0.0181 |           0.0000 |
[32m[20221213 20:10:58 @agent_ppo2.py:185][0m |          -0.0179 |           0.0180 |           0.0000 |
[32m[20221213 20:10:59 @agent_ppo2.py:185][0m |          -0.0178 |           0.0176 |           0.0000 |
[32m[20221213 20:10:59 @agent_ppo2.py:185][0m |          -0.0183 |           0.0174 |           0.0000 |
[32m[20221213 20:10:59 @agent_ppo2.py:130][0m Policy update time: 3.14 s
[32m[20221213 20:10:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.23
[32m[20221213 20:10:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.06
[32m[20221213 20:10:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.55
[32m[20221213 20:10:59 @agent_ppo2.py:143][0m Total time:       2.55 min
[32m[20221213 20:10:59 @agent_ppo2.py:145][0m 79872 total steps have happened
[32m[20221213 20:10:59 @agent_ppo2.py:121][0m #------------------------ Iteration 39 --------------------------#
[32m[20221213 20:11:00 @agent_ppo2.py:127][0m Sampling time: 0.62 s by 10 slaves
[32m[20221213 20:11:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:00 @agent_ppo2.py:185][0m |           0.0008 |           0.0267 |           0.0000 |
[32m[20221213 20:11:01 @agent_ppo2.py:185][0m |          -0.0152 |           0.0218 |           0.0000 |
[32m[20221213 20:11:01 @agent_ppo2.py:185][0m |          -0.0141 |           0.0198 |           0.0000 |
[32m[20221213 20:11:01 @agent_ppo2.py:185][0m |          -0.0190 |           0.0191 |           0.0000 |
[32m[20221213 20:11:02 @agent_ppo2.py:185][0m |          -0.0212 |           0.0182 |           0.0000 |
[32m[20221213 20:11:02 @agent_ppo2.py:185][0m |          -0.0242 |           0.0182 |           0.0000 |
[32m[20221213 20:11:02 @agent_ppo2.py:185][0m |          -0.0247 |           0.0179 |           0.0000 |
[32m[20221213 20:11:02 @agent_ppo2.py:185][0m |          -0.0246 |           0.0176 |           0.0000 |
[32m[20221213 20:11:03 @agent_ppo2.py:185][0m |          -0.0257 |           0.0176 |           0.0000 |
[32m[20221213 20:11:03 @agent_ppo2.py:185][0m |          -0.0270 |           0.0174 |           0.0000 |
[32m[20221213 20:11:03 @agent_ppo2.py:130][0m Policy update time: 2.94 s
[32m[20221213 20:11:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.99
[32m[20221213 20:11:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.10
[32m[20221213 20:11:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.28
[32m[20221213 20:11:03 @agent_ppo2.py:143][0m Total time:       2.62 min
[32m[20221213 20:11:03 @agent_ppo2.py:145][0m 81920 total steps have happened
[32m[20221213 20:11:03 @agent_ppo2.py:121][0m #------------------------ Iteration 40 --------------------------#
[32m[20221213 20:11:04 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:11:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:04 @agent_ppo2.py:185][0m |          -0.0019 |           0.0142 |           0.0000 |
[32m[20221213 20:11:05 @agent_ppo2.py:185][0m |          -0.0078 |           0.0099 |           0.0000 |
[32m[20221213 20:11:05 @agent_ppo2.py:185][0m |          -0.0161 |           0.0095 |           0.0000 |
[32m[20221213 20:11:05 @agent_ppo2.py:185][0m |          -0.0195 |           0.0091 |           0.0000 |
[32m[20221213 20:11:05 @agent_ppo2.py:185][0m |          -0.0200 |           0.0089 |           0.0000 |
[32m[20221213 20:11:06 @agent_ppo2.py:185][0m |          -0.0222 |           0.0086 |           0.0000 |
[32m[20221213 20:11:06 @agent_ppo2.py:185][0m |          -0.0236 |           0.0084 |           0.0000 |
[32m[20221213 20:11:06 @agent_ppo2.py:185][0m |          -0.0215 |           0.0082 |           0.0000 |
[32m[20221213 20:11:06 @agent_ppo2.py:185][0m |          -0.0268 |           0.0080 |           0.0000 |
[32m[20221213 20:11:07 @agent_ppo2.py:185][0m |          -0.0222 |           0.0080 |           0.0000 |
[32m[20221213 20:11:07 @agent_ppo2.py:130][0m Policy update time: 2.85 s
[32m[20221213 20:11:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.12
[32m[20221213 20:11:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.59
[32m[20221213 20:11:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.19
[32m[20221213 20:11:07 @agent_ppo2.py:143][0m Total time:       2.68 min
[32m[20221213 20:11:07 @agent_ppo2.py:145][0m 83968 total steps have happened
[32m[20221213 20:11:07 @agent_ppo2.py:121][0m #------------------------ Iteration 41 --------------------------#
[32m[20221213 20:11:08 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:11:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:08 @agent_ppo2.py:185][0m |           0.0008 |           0.0159 |           0.0000 |
[32m[20221213 20:11:08 @agent_ppo2.py:185][0m |          -0.0062 |           0.0129 |           0.0000 |
[32m[20221213 20:11:09 @agent_ppo2.py:185][0m |          -0.0111 |           0.0122 |           0.0000 |
[32m[20221213 20:11:09 @agent_ppo2.py:185][0m |          -0.0127 |           0.0117 |           0.0000 |
[32m[20221213 20:11:09 @agent_ppo2.py:185][0m |          -0.0134 |           0.0116 |           0.0000 |
[32m[20221213 20:11:10 @agent_ppo2.py:185][0m |          -0.0154 |           0.0113 |           0.0000 |
[32m[20221213 20:11:10 @agent_ppo2.py:185][0m |          -0.0208 |           0.0113 |           0.0000 |
[32m[20221213 20:11:10 @agent_ppo2.py:185][0m |          -0.0162 |           0.0111 |           0.0000 |
[32m[20221213 20:11:10 @agent_ppo2.py:185][0m |          -0.0193 |           0.0111 |           0.0000 |
[32m[20221213 20:11:11 @agent_ppo2.py:185][0m |          -0.0169 |           0.0111 |           0.0000 |
[32m[20221213 20:11:11 @agent_ppo2.py:130][0m Policy update time: 2.85 s
[32m[20221213 20:11:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.50
[32m[20221213 20:11:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.46
[32m[20221213 20:11:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.80
[32m[20221213 20:11:11 @agent_ppo2.py:143][0m Total time:       2.75 min
[32m[20221213 20:11:11 @agent_ppo2.py:145][0m 86016 total steps have happened
[32m[20221213 20:11:11 @agent_ppo2.py:121][0m #------------------------ Iteration 42 --------------------------#
[32m[20221213 20:11:12 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:11:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:12 @agent_ppo2.py:185][0m |           0.0001 |           0.0108 |           0.0000 |
[32m[20221213 20:11:12 @agent_ppo2.py:185][0m |          -0.0114 |           0.0086 |           0.0000 |
[32m[20221213 20:11:13 @agent_ppo2.py:185][0m |          -0.0158 |           0.0083 |           0.0000 |
[32m[20221213 20:11:13 @agent_ppo2.py:185][0m |          -0.0178 |           0.0081 |           0.0000 |
[32m[20221213 20:11:13 @agent_ppo2.py:185][0m |          -0.0196 |           0.0080 |           0.0000 |
[32m[20221213 20:11:13 @agent_ppo2.py:185][0m |          -0.0208 |           0.0079 |           0.0000 |
[32m[20221213 20:11:14 @agent_ppo2.py:185][0m |          -0.0224 |           0.0078 |           0.0000 |
[32m[20221213 20:11:14 @agent_ppo2.py:185][0m |          -0.0229 |           0.0076 |           0.0000 |
[32m[20221213 20:11:14 @agent_ppo2.py:185][0m |          -0.0230 |           0.0075 |           0.0000 |
[32m[20221213 20:11:14 @agent_ppo2.py:185][0m |          -0.0238 |           0.0074 |           0.0000 |
[32m[20221213 20:11:14 @agent_ppo2.py:130][0m Policy update time: 2.81 s
[32m[20221213 20:11:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.23
[32m[20221213 20:11:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.73
[32m[20221213 20:11:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.46
[32m[20221213 20:11:15 @agent_ppo2.py:143][0m Total time:       2.81 min
[32m[20221213 20:11:15 @agent_ppo2.py:145][0m 88064 total steps have happened
[32m[20221213 20:11:15 @agent_ppo2.py:121][0m #------------------------ Iteration 43 --------------------------#
[32m[20221213 20:11:15 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:11:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:16 @agent_ppo2.py:185][0m |           0.0011 |           0.0390 |           0.0000 |
[32m[20221213 20:11:16 @agent_ppo2.py:185][0m |          -0.0091 |           0.0226 |           0.0000 |
[32m[20221213 20:11:16 @agent_ppo2.py:185][0m |          -0.0121 |           0.0205 |           0.0000 |
[32m[20221213 20:11:17 @agent_ppo2.py:185][0m |          -0.0164 |           0.0198 |           0.0000 |
[32m[20221213 20:11:17 @agent_ppo2.py:185][0m |          -0.0163 |           0.0192 |           0.0000 |
[32m[20221213 20:11:17 @agent_ppo2.py:185][0m |          -0.0203 |           0.0184 |           0.0000 |
[32m[20221213 20:11:17 @agent_ppo2.py:185][0m |          -0.0177 |           0.0176 |           0.0000 |
[32m[20221213 20:11:18 @agent_ppo2.py:185][0m |          -0.0209 |           0.0173 |           0.0000 |
[32m[20221213 20:11:18 @agent_ppo2.py:185][0m |          -0.0237 |           0.0173 |           0.0000 |
[32m[20221213 20:11:18 @agent_ppo2.py:185][0m |          -0.0177 |           0.0171 |           0.0000 |
[32m[20221213 20:11:18 @agent_ppo2.py:130][0m Policy update time: 2.82 s
[32m[20221213 20:11:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.63
[32m[20221213 20:11:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.97
[32m[20221213 20:11:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.18
[32m[20221213 20:11:19 @agent_ppo2.py:143][0m Total time:       2.88 min
[32m[20221213 20:11:19 @agent_ppo2.py:145][0m 90112 total steps have happened
[32m[20221213 20:11:19 @agent_ppo2.py:121][0m #------------------------ Iteration 44 --------------------------#
[32m[20221213 20:11:19 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:11:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:20 @agent_ppo2.py:185][0m |          -0.0003 |           0.0164 |           0.0000 |
[32m[20221213 20:11:20 @agent_ppo2.py:185][0m |          -0.0109 |           0.0078 |           0.0000 |
[32m[20221213 20:11:20 @agent_ppo2.py:185][0m |          -0.0163 |           0.0076 |           0.0000 |
[32m[20221213 20:11:21 @agent_ppo2.py:185][0m |          -0.0164 |           0.0074 |           0.0000 |
[32m[20221213 20:11:21 @agent_ppo2.py:185][0m |          -0.0193 |           0.0072 |           0.0000 |
[32m[20221213 20:11:21 @agent_ppo2.py:185][0m |          -0.0191 |           0.0070 |           0.0000 |
[32m[20221213 20:11:21 @agent_ppo2.py:185][0m |          -0.0207 |           0.0068 |           0.0000 |
[32m[20221213 20:11:22 @agent_ppo2.py:185][0m |          -0.0215 |           0.0068 |           0.0000 |
[32m[20221213 20:11:22 @agent_ppo2.py:185][0m |          -0.0182 |           0.0065 |           0.0000 |
[32m[20221213 20:11:22 @agent_ppo2.py:185][0m |          -0.0222 |           0.0064 |           0.0000 |
[32m[20221213 20:11:22 @agent_ppo2.py:130][0m Policy update time: 2.93 s
[32m[20221213 20:11:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.55
[32m[20221213 20:11:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.72
[32m[20221213 20:11:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 12.33
[32m[20221213 20:11:23 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 12.33
[32m[20221213 20:11:23 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 12.33
[32m[20221213 20:11:23 @agent_ppo2.py:143][0m Total time:       2.94 min
[32m[20221213 20:11:23 @agent_ppo2.py:145][0m 92160 total steps have happened
[32m[20221213 20:11:23 @agent_ppo2.py:121][0m #------------------------ Iteration 45 --------------------------#
[32m[20221213 20:11:23 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:11:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:24 @agent_ppo2.py:185][0m |           0.0023 |           0.0055 |           0.0000 |
[32m[20221213 20:11:24 @agent_ppo2.py:185][0m |          -0.0168 |           0.0052 |           0.0000 |
[32m[20221213 20:11:24 @agent_ppo2.py:185][0m |          -0.0202 |           0.0050 |           0.0000 |
[32m[20221213 20:11:25 @agent_ppo2.py:185][0m |          -0.0224 |           0.0048 |           0.0000 |
[32m[20221213 20:11:25 @agent_ppo2.py:185][0m |          -0.0322 |           0.0047 |           0.0000 |
[32m[20221213 20:11:25 @agent_ppo2.py:185][0m |          -0.0280 |           0.0045 |           0.0000 |
[32m[20221213 20:11:26 @agent_ppo2.py:185][0m |          -0.0300 |           0.0043 |           0.0000 |
[32m[20221213 20:11:26 @agent_ppo2.py:185][0m |          -0.0315 |           0.0042 |           0.0000 |
[32m[20221213 20:11:26 @agent_ppo2.py:185][0m |          -0.0304 |           0.0041 |           0.0000 |
[32m[20221213 20:11:27 @agent_ppo2.py:185][0m |          -0.0325 |           0.0040 |           0.0000 |
[32m[20221213 20:11:27 @agent_ppo2.py:130][0m Policy update time: 3.28 s
[32m[20221213 20:11:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.95
[32m[20221213 20:11:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.54
[32m[20221213 20:11:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.22
[32m[20221213 20:11:27 @agent_ppo2.py:143][0m Total time:       3.01 min
[32m[20221213 20:11:27 @agent_ppo2.py:145][0m 94208 total steps have happened
[32m[20221213 20:11:27 @agent_ppo2.py:121][0m #------------------------ Iteration 46 --------------------------#
[32m[20221213 20:11:28 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:11:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:28 @agent_ppo2.py:185][0m |           0.0031 |           0.1230 |           0.0000 |
[32m[20221213 20:11:28 @agent_ppo2.py:185][0m |          -0.0058 |           0.0394 |           0.0000 |
[32m[20221213 20:11:29 @agent_ppo2.py:185][0m |          -0.0102 |           0.0294 |           0.0000 |
[32m[20221213 20:11:29 @agent_ppo2.py:185][0m |          -0.0165 |           0.0264 |           0.0000 |
[32m[20221213 20:11:29 @agent_ppo2.py:185][0m |          -0.0136 |           0.0235 |           0.0000 |
[32m[20221213 20:11:29 @agent_ppo2.py:185][0m |          -0.0187 |           0.0222 |           0.0000 |
[32m[20221213 20:11:30 @agent_ppo2.py:185][0m |          -0.0163 |           0.0213 |           0.0000 |
[32m[20221213 20:11:30 @agent_ppo2.py:185][0m |          -0.0168 |           0.0209 |           0.0000 |
[32m[20221213 20:11:30 @agent_ppo2.py:185][0m |          -0.0172 |           0.0205 |           0.0000 |
[32m[20221213 20:11:31 @agent_ppo2.py:185][0m |          -0.0174 |           0.0202 |           0.0000 |
[32m[20221213 20:11:31 @agent_ppo2.py:130][0m Policy update time: 3.22 s
[32m[20221213 20:11:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.79
[32m[20221213 20:11:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 12.11
[32m[20221213 20:11:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.76
[32m[20221213 20:11:31 @agent_ppo2.py:143][0m Total time:       3.08 min
[32m[20221213 20:11:31 @agent_ppo2.py:145][0m 96256 total steps have happened
[32m[20221213 20:11:31 @agent_ppo2.py:121][0m #------------------------ Iteration 47 --------------------------#
[32m[20221213 20:11:32 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:11:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:32 @agent_ppo2.py:185][0m |           0.0014 |           0.0293 |           0.0000 |
[32m[20221213 20:11:33 @agent_ppo2.py:185][0m |          -0.0111 |           0.0246 |           0.0000 |
[32m[20221213 20:11:33 @agent_ppo2.py:185][0m |          -0.0179 |           0.0220 |           0.0000 |
[32m[20221213 20:11:33 @agent_ppo2.py:185][0m |          -0.0189 |           0.0206 |           0.0000 |
[32m[20221213 20:11:33 @agent_ppo2.py:185][0m |          -0.0219 |           0.0201 |           0.0000 |
[32m[20221213 20:11:34 @agent_ppo2.py:185][0m |          -0.0243 |           0.0193 |           0.0000 |
[32m[20221213 20:11:34 @agent_ppo2.py:185][0m |          -0.0239 |           0.0190 |           0.0000 |
[32m[20221213 20:11:34 @agent_ppo2.py:185][0m |          -0.0263 |           0.0188 |           0.0000 |
[32m[20221213 20:11:34 @agent_ppo2.py:185][0m |          -0.0270 |           0.0183 |           0.0000 |
[32m[20221213 20:11:35 @agent_ppo2.py:185][0m |          -0.0277 |           0.0180 |           0.0000 |
[32m[20221213 20:11:35 @agent_ppo2.py:130][0m Policy update time: 2.92 s
[32m[20221213 20:11:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.62
[32m[20221213 20:11:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.35
[32m[20221213 20:11:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.58
[32m[20221213 20:11:35 @agent_ppo2.py:143][0m Total time:       3.15 min
[32m[20221213 20:11:35 @agent_ppo2.py:145][0m 98304 total steps have happened
[32m[20221213 20:11:35 @agent_ppo2.py:121][0m #------------------------ Iteration 48 --------------------------#
[32m[20221213 20:11:36 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:11:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:36 @agent_ppo2.py:185][0m |           0.0001 |           0.0201 |           0.0000 |
[32m[20221213 20:11:37 @agent_ppo2.py:185][0m |          -0.0107 |           0.0158 |           0.0000 |
[32m[20221213 20:11:37 @agent_ppo2.py:185][0m |          -0.0132 |           0.0152 |           0.0000 |
[32m[20221213 20:11:37 @agent_ppo2.py:185][0m |          -0.0170 |           0.0146 |           0.0000 |
[32m[20221213 20:11:37 @agent_ppo2.py:185][0m |          -0.0189 |           0.0144 |           0.0000 |
[32m[20221213 20:11:38 @agent_ppo2.py:185][0m |          -0.0199 |           0.0140 |           0.0000 |
[32m[20221213 20:11:38 @agent_ppo2.py:185][0m |          -0.0211 |           0.0140 |           0.0000 |
[32m[20221213 20:11:38 @agent_ppo2.py:185][0m |          -0.0229 |           0.0135 |           0.0000 |
[32m[20221213 20:11:38 @agent_ppo2.py:185][0m |          -0.0227 |           0.0132 |           0.0000 |
[32m[20221213 20:11:39 @agent_ppo2.py:185][0m |          -0.0227 |           0.0128 |           0.0000 |
[32m[20221213 20:11:39 @agent_ppo2.py:130][0m Policy update time: 2.88 s
[32m[20221213 20:11:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.92
[32m[20221213 20:11:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.22
[32m[20221213 20:11:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.25
[32m[20221213 20:11:39 @agent_ppo2.py:143][0m Total time:       3.22 min
[32m[20221213 20:11:39 @agent_ppo2.py:145][0m 100352 total steps have happened
[32m[20221213 20:11:39 @agent_ppo2.py:121][0m #------------------------ Iteration 49 --------------------------#
[32m[20221213 20:11:40 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:11:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:40 @agent_ppo2.py:185][0m |           0.0008 |           0.0113 |           0.0000 |
[32m[20221213 20:11:40 @agent_ppo2.py:185][0m |          -0.0109 |           0.0107 |           0.0000 |
[32m[20221213 20:11:41 @agent_ppo2.py:185][0m |          -0.0156 |           0.0103 |           0.0000 |
[32m[20221213 20:11:41 @agent_ppo2.py:185][0m |          -0.0192 |           0.0099 |           0.0000 |
[32m[20221213 20:11:41 @agent_ppo2.py:185][0m |          -0.0197 |           0.0096 |           0.0000 |
[32m[20221213 20:11:42 @agent_ppo2.py:185][0m |          -0.0217 |           0.0096 |           0.0000 |
[32m[20221213 20:11:42 @agent_ppo2.py:185][0m |          -0.0222 |           0.0095 |           0.0000 |
[32m[20221213 20:11:42 @agent_ppo2.py:185][0m |          -0.0230 |           0.0094 |           0.0000 |
[32m[20221213 20:11:42 @agent_ppo2.py:185][0m |          -0.0241 |           0.0094 |           0.0000 |
[32m[20221213 20:11:43 @agent_ppo2.py:185][0m |          -0.0250 |           0.0090 |           0.0000 |
[32m[20221213 20:11:43 @agent_ppo2.py:130][0m Policy update time: 2.81 s
[32m[20221213 20:11:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.35
[32m[20221213 20:11:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.71
[32m[20221213 20:11:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.80
[32m[20221213 20:11:43 @agent_ppo2.py:143][0m Total time:       3.28 min
[32m[20221213 20:11:43 @agent_ppo2.py:145][0m 102400 total steps have happened
[32m[20221213 20:11:43 @agent_ppo2.py:121][0m #------------------------ Iteration 50 --------------------------#
[32m[20221213 20:11:44 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:11:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:44 @agent_ppo2.py:185][0m |          -0.0016 |           0.0283 |           0.0000 |
[32m[20221213 20:11:44 @agent_ppo2.py:185][0m |          -0.0093 |           0.0190 |           0.0000 |
[32m[20221213 20:11:45 @agent_ppo2.py:185][0m |          -0.0127 |           0.0183 |           0.0000 |
[32m[20221213 20:11:45 @agent_ppo2.py:185][0m |          -0.0147 |           0.0168 |           0.0000 |
[32m[20221213 20:11:45 @agent_ppo2.py:185][0m |          -0.0162 |           0.0160 |           0.0000 |
[32m[20221213 20:11:45 @agent_ppo2.py:185][0m |          -0.0173 |           0.0157 |           0.0000 |
[32m[20221213 20:11:46 @agent_ppo2.py:185][0m |          -0.0179 |           0.0164 |           0.0000 |
[32m[20221213 20:11:46 @agent_ppo2.py:185][0m |          -0.0185 |           0.0159 |           0.0000 |
[32m[20221213 20:11:46 @agent_ppo2.py:185][0m |          -0.0187 |           0.0155 |           0.0000 |
[32m[20221213 20:11:46 @agent_ppo2.py:185][0m |          -0.0189 |           0.0151 |           0.0000 |
[32m[20221213 20:11:46 @agent_ppo2.py:130][0m Policy update time: 2.87 s
[32m[20221213 20:11:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.17
[32m[20221213 20:11:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.49
[32m[20221213 20:11:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.06
[32m[20221213 20:11:47 @agent_ppo2.py:143][0m Total time:       3.34 min
[32m[20221213 20:11:47 @agent_ppo2.py:145][0m 104448 total steps have happened
[32m[20221213 20:11:47 @agent_ppo2.py:121][0m #------------------------ Iteration 51 --------------------------#
[32m[20221213 20:11:47 @agent_ppo2.py:127][0m Sampling time: 0.57 s by 10 slaves
[32m[20221213 20:11:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:48 @agent_ppo2.py:185][0m |          -0.0002 |           0.0147 |           0.0000 |
[32m[20221213 20:11:48 @agent_ppo2.py:185][0m |          -0.0092 |           0.0067 |           0.0000 |
[32m[20221213 20:11:49 @agent_ppo2.py:185][0m |          -0.0104 |           0.0065 |           0.0000 |
[32m[20221213 20:11:49 @agent_ppo2.py:185][0m |          -0.0154 |           0.0063 |           0.0000 |
[32m[20221213 20:11:49 @agent_ppo2.py:185][0m |          -0.0180 |           0.0061 |           0.0000 |
[32m[20221213 20:11:49 @agent_ppo2.py:185][0m |          -0.0180 |           0.0059 |           0.0000 |
[32m[20221213 20:11:50 @agent_ppo2.py:185][0m |          -0.0178 |           0.0057 |           0.0000 |
[32m[20221213 20:11:50 @agent_ppo2.py:185][0m |          -0.0193 |           0.0056 |           0.0000 |
[32m[20221213 20:11:50 @agent_ppo2.py:185][0m |          -0.0202 |           0.0054 |           0.0000 |
[32m[20221213 20:11:50 @agent_ppo2.py:185][0m |          -0.0205 |           0.0053 |           0.0000 |
[32m[20221213 20:11:50 @agent_ppo2.py:130][0m Policy update time: 2.97 s
[32m[20221213 20:11:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.14
[32m[20221213 20:11:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.66
[32m[20221213 20:11:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.38
[32m[20221213 20:11:51 @agent_ppo2.py:143][0m Total time:       3.41 min
[32m[20221213 20:11:51 @agent_ppo2.py:145][0m 106496 total steps have happened
[32m[20221213 20:11:51 @agent_ppo2.py:121][0m #------------------------ Iteration 52 --------------------------#
[32m[20221213 20:11:51 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:11:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:52 @agent_ppo2.py:185][0m |          -0.0017 |           0.0056 |           0.0000 |
[32m[20221213 20:11:52 @agent_ppo2.py:185][0m |          -0.0097 |           0.0053 |           0.0000 |
[32m[20221213 20:11:52 @agent_ppo2.py:185][0m |          -0.0126 |           0.0051 |           0.0000 |
[32m[20221213 20:11:53 @agent_ppo2.py:185][0m |          -0.0148 |           0.0051 |           0.0000 |
[32m[20221213 20:11:53 @agent_ppo2.py:185][0m |          -0.0164 |           0.0049 |           0.0000 |
[32m[20221213 20:11:53 @agent_ppo2.py:185][0m |          -0.0184 |           0.0048 |           0.0000 |
[32m[20221213 20:11:54 @agent_ppo2.py:185][0m |          -0.0193 |           0.0048 |           0.0000 |
[32m[20221213 20:11:54 @agent_ppo2.py:185][0m |          -0.0193 |           0.0047 |           0.0000 |
[32m[20221213 20:11:54 @agent_ppo2.py:185][0m |          -0.0242 |           0.0047 |           0.0000 |
[32m[20221213 20:11:54 @agent_ppo2.py:185][0m |          -0.0208 |           0.0046 |           0.0000 |
[32m[20221213 20:11:54 @agent_ppo2.py:130][0m Policy update time: 2.89 s
[32m[20221213 20:11:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.67
[32m[20221213 20:11:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.10
[32m[20221213 20:11:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.56
[32m[20221213 20:11:55 @agent_ppo2.py:143][0m Total time:       3.48 min
[32m[20221213 20:11:55 @agent_ppo2.py:145][0m 108544 total steps have happened
[32m[20221213 20:11:55 @agent_ppo2.py:121][0m #------------------------ Iteration 53 --------------------------#
[32m[20221213 20:11:55 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:11:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:11:56 @agent_ppo2.py:185][0m |           0.0006 |           0.0632 |           0.0000 |
[32m[20221213 20:11:56 @agent_ppo2.py:185][0m |          -0.0099 |           0.0287 |           0.0000 |
[32m[20221213 20:11:56 @agent_ppo2.py:185][0m |          -0.0129 |           0.0236 |           0.0000 |
[32m[20221213 20:11:57 @agent_ppo2.py:185][0m |          -0.0149 |           0.0211 |           0.0000 |
[32m[20221213 20:11:57 @agent_ppo2.py:185][0m |          -0.0199 |           0.0205 |           0.0000 |
[32m[20221213 20:11:57 @agent_ppo2.py:185][0m |          -0.0165 |           0.0193 |           0.0000 |
[32m[20221213 20:11:57 @agent_ppo2.py:185][0m |          -0.0169 |           0.0191 |           0.0000 |
[32m[20221213 20:11:58 @agent_ppo2.py:185][0m |          -0.0179 |           0.0188 |           0.0000 |
[32m[20221213 20:11:58 @agent_ppo2.py:185][0m |          -0.0182 |           0.0181 |           0.0000 |
[32m[20221213 20:11:58 @agent_ppo2.py:185][0m |          -0.0185 |           0.0179 |           0.0000 |
[32m[20221213 20:11:58 @agent_ppo2.py:130][0m Policy update time: 2.81 s
[32m[20221213 20:11:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.66
[32m[20221213 20:11:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.43
[32m[20221213 20:11:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.99
[32m[20221213 20:11:59 @agent_ppo2.py:143][0m Total time:       3.54 min
[32m[20221213 20:11:59 @agent_ppo2.py:145][0m 110592 total steps have happened
[32m[20221213 20:11:59 @agent_ppo2.py:121][0m #------------------------ Iteration 54 --------------------------#
[32m[20221213 20:11:59 @agent_ppo2.py:127][0m Sampling time: 0.56 s by 10 slaves
[32m[20221213 20:11:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:12:00 @agent_ppo2.py:185][0m |           0.0020 |           0.0154 |           0.0000 |
[32m[20221213 20:12:00 @agent_ppo2.py:185][0m |          -0.0114 |           0.0093 |           0.0000 |
[32m[20221213 20:12:00 @agent_ppo2.py:185][0m |          -0.0167 |           0.0090 |           0.0000 |
[32m[20221213 20:12:01 @agent_ppo2.py:185][0m |          -0.0183 |           0.0088 |           0.0000 |
[32m[20221213 20:12:01 @agent_ppo2.py:185][0m |          -0.0170 |           0.0087 |           0.0000 |
[32m[20221213 20:12:01 @agent_ppo2.py:185][0m |          -0.0222 |           0.0085 |           0.0000 |
[32m[20221213 20:12:01 @agent_ppo2.py:185][0m |          -0.0217 |           0.0083 |           0.0000 |
[32m[20221213 20:12:02 @agent_ppo2.py:185][0m |          -0.0218 |           0.0082 |           0.0000 |
[32m[20221213 20:12:02 @agent_ppo2.py:185][0m |          -0.0238 |           0.0082 |           0.0000 |
[32m[20221213 20:12:02 @agent_ppo2.py:185][0m |          -0.0238 |           0.0080 |           0.0000 |
[32m[20221213 20:12:02 @agent_ppo2.py:130][0m Policy update time: 2.81 s
[32m[20221213 20:12:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.99
[32m[20221213 20:12:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.09
[32m[20221213 20:12:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.00
[32m[20221213 20:12:03 @agent_ppo2.py:143][0m Total time:       3.61 min
[32m[20221213 20:12:03 @agent_ppo2.py:145][0m 112640 total steps have happened
[32m[20221213 20:12:03 @agent_ppo2.py:121][0m #------------------------ Iteration 55 --------------------------#
[32m[20221213 20:12:03 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:12:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:12:04 @agent_ppo2.py:185][0m |          -0.0004 |           0.0084 |           0.0000 |
[32m[20221213 20:12:04 @agent_ppo2.py:185][0m |          -0.0123 |           0.0081 |           0.0000 |
[32m[20221213 20:12:04 @agent_ppo2.py:185][0m |          -0.0179 |           0.0078 |           0.0000 |
[32m[20221213 20:12:04 @agent_ppo2.py:185][0m |          -0.0210 |           0.0076 |           0.0000 |
[32m[20221213 20:12:05 @agent_ppo2.py:185][0m |          -0.0224 |           0.0075 |           0.0000 |
[32m[20221213 20:12:05 @agent_ppo2.py:185][0m |          -0.0229 |           0.0076 |           0.0000 |
[32m[20221213 20:12:05 @agent_ppo2.py:185][0m |          -0.0271 |           0.0073 |           0.0000 |
[32m[20221213 20:12:05 @agent_ppo2.py:185][0m |          -0.0280 |           0.0072 |           0.0000 |
[32m[20221213 20:12:06 @agent_ppo2.py:185][0m |          -0.0282 |           0.0074 |           0.0000 |
[32m[20221213 20:12:06 @agent_ppo2.py:185][0m |          -0.0295 |           0.0071 |           0.0000 |
[32m[20221213 20:12:06 @agent_ppo2.py:130][0m Policy update time: 2.88 s
[32m[20221213 20:12:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.20
[32m[20221213 20:12:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.53
[32m[20221213 20:12:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.55
[32m[20221213 20:12:06 @agent_ppo2.py:143][0m Total time:       3.67 min
[32m[20221213 20:12:06 @agent_ppo2.py:145][0m 114688 total steps have happened
[32m[20221213 20:12:06 @agent_ppo2.py:121][0m #------------------------ Iteration 56 --------------------------#
[32m[20221213 20:12:07 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:12:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:12:07 @agent_ppo2.py:185][0m |          -0.0018 |           0.0055 |           0.0000 |
[32m[20221213 20:12:08 @agent_ppo2.py:185][0m |          -0.0153 |           0.0053 |           0.0000 |
[32m[20221213 20:12:08 @agent_ppo2.py:185][0m |          -0.0179 |           0.0050 |           0.0000 |
[32m[20221213 20:12:08 @agent_ppo2.py:185][0m |          -0.0191 |           0.0049 |           0.0000 |
[32m[20221213 20:12:08 @agent_ppo2.py:185][0m |          -0.0228 |           0.0048 |           0.0000 |
[32m[20221213 20:12:09 @agent_ppo2.py:185][0m |          -0.0236 |           0.0047 |           0.0000 |
[32m[20221213 20:12:09 @agent_ppo2.py:185][0m |          -0.0246 |           0.0046 |           0.0000 |
[32m[20221213 20:12:09 @agent_ppo2.py:185][0m |          -0.0263 |           0.0046 |           0.0000 |
[32m[20221213 20:12:10 @agent_ppo2.py:185][0m |          -0.0222 |           0.0045 |           0.0000 |
[32m[20221213 20:12:10 @agent_ppo2.py:185][0m |          -0.0265 |           0.0044 |           0.0000 |
[32m[20221213 20:12:10 @agent_ppo2.py:130][0m Policy update time: 2.84 s
[32m[20221213 20:12:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.53
[32m[20221213 20:12:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.06
[32m[20221213 20:12:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.94
[32m[20221213 20:12:10 @agent_ppo2.py:143][0m Total time:       3.73 min
[32m[20221213 20:12:10 @agent_ppo2.py:145][0m 116736 total steps have happened
[32m[20221213 20:12:10 @agent_ppo2.py:121][0m #------------------------ Iteration 57 --------------------------#
[32m[20221213 20:12:11 @agent_ppo2.py:127][0m Sampling time: 0.60 s by 10 slaves
[32m[20221213 20:12:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:12:11 @agent_ppo2.py:185][0m |           0.0003 |           0.0344 |           0.0000 |
[32m[20221213 20:12:12 @agent_ppo2.py:185][0m |          -0.0081 |           0.0179 |           0.0000 |
[32m[20221213 20:12:12 @agent_ppo2.py:185][0m |          -0.0121 |           0.0167 |           0.0000 |
[32m[20221213 20:12:12 @agent_ppo2.py:185][0m |          -0.0144 |           0.0160 |           0.0000 |
[32m[20221213 20:12:12 @agent_ppo2.py:185][0m |          -0.0155 |           0.0148 |           0.0000 |
[32m[20221213 20:12:13 @agent_ppo2.py:185][0m |          -0.0166 |           0.0147 |           0.0000 |
[32m[20221213 20:12:13 @agent_ppo2.py:185][0m |          -0.0166 |           0.0140 |           0.0000 |
[32m[20221213 20:12:13 @agent_ppo2.py:185][0m |          -0.0176 |           0.0139 |           0.0000 |
[32m[20221213 20:12:13 @agent_ppo2.py:185][0m |          -0.0182 |           0.0139 |           0.0000 |
[32m[20221213 20:12:14 @agent_ppo2.py:185][0m |          -0.0192 |           0.0132 |           0.0000 |
[32m[20221213 20:12:14 @agent_ppo2.py:130][0m Policy update time: 2.76 s
[32m[20221213 20:12:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.22
[32m[20221213 20:12:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.48
[32m[20221213 20:12:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.81
[32m[20221213 20:12:14 @agent_ppo2.py:143][0m Total time:       3.80 min
[32m[20221213 20:12:14 @agent_ppo2.py:145][0m 118784 total steps have happened
[32m[20221213 20:12:14 @agent_ppo2.py:121][0m #------------------------ Iteration 58 --------------------------#
[32m[20221213 20:12:15 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:12:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:12:15 @agent_ppo2.py:185][0m |           0.0001 |           0.0119 |           0.0000 |
[32m[20221213 20:12:15 @agent_ppo2.py:185][0m |          -0.0112 |           0.0058 |           0.0000 |
[32m[20221213 20:12:16 @agent_ppo2.py:185][0m |          -0.0107 |           0.0056 |           0.0000 |
[32m[20221213 20:12:16 @agent_ppo2.py:185][0m |          -0.0170 |           0.0055 |           0.0000 |
[32m[20221213 20:12:16 @agent_ppo2.py:185][0m |          -0.0179 |           0.0053 |           0.0000 |
[32m[20221213 20:12:17 @agent_ppo2.py:185][0m |          -0.0194 |           0.0051 |           0.0000 |
[32m[20221213 20:12:17 @agent_ppo2.py:185][0m |          -0.0213 |           0.0050 |           0.0000 |
[32m[20221213 20:12:17 @agent_ppo2.py:185][0m |          -0.0201 |           0.0049 |           0.0000 |
[32m[20221213 20:12:17 @agent_ppo2.py:185][0m |          -0.0214 |           0.0048 |           0.0000 |
[32m[20221213 20:12:18 @agent_ppo2.py:185][0m |          -0.0185 |           0.0046 |           0.0000 |
[32m[20221213 20:12:18 @agent_ppo2.py:130][0m Policy update time: 2.90 s
[32m[20221213 20:12:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.57
[32m[20221213 20:12:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.97
[32m[20221213 20:12:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.20
[32m[20221213 20:12:18 @agent_ppo2.py:143][0m Total time:       3.86 min
[32m[20221213 20:12:18 @agent_ppo2.py:145][0m 120832 total steps have happened
[32m[20221213 20:12:18 @agent_ppo2.py:121][0m #------------------------ Iteration 59 --------------------------#
[32m[20221213 20:12:19 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:12:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:12:19 @agent_ppo2.py:185][0m |           0.0023 |           0.0235 |           0.0000 |
[32m[20221213 20:12:19 @agent_ppo2.py:185][0m |          -0.0097 |           0.0136 |           0.0000 |
[32m[20221213 20:12:20 @agent_ppo2.py:185][0m |          -0.0128 |           0.0115 |           0.0000 |
[32m[20221213 20:12:20 @agent_ppo2.py:185][0m |          -0.0146 |           0.0110 |           0.0000 |
[32m[20221213 20:12:20 @agent_ppo2.py:185][0m |          -0.0166 |           0.0102 |           0.0000 |
[32m[20221213 20:12:20 @agent_ppo2.py:185][0m |          -0.0170 |           0.0104 |           0.0000 |
[32m[20221213 20:12:21 @agent_ppo2.py:185][0m |          -0.0172 |           0.0102 |           0.0000 |
[32m[20221213 20:12:21 @agent_ppo2.py:185][0m |          -0.0171 |           0.0097 |           0.0000 |
[32m[20221213 20:12:21 @agent_ppo2.py:185][0m |          -0.0204 |           0.0097 |           0.0000 |
[32m[20221213 20:12:21 @agent_ppo2.py:185][0m |          -0.0177 |           0.0099 |           0.0000 |
[32m[20221213 20:12:21 @agent_ppo2.py:130][0m Policy update time: 2.82 s
[32m[20221213 20:12:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.45
[32m[20221213 20:12:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.76
[32m[20221213 20:12:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.41
[32m[20221213 20:12:22 @agent_ppo2.py:143][0m Total time:       3.93 min
[32m[20221213 20:12:22 @agent_ppo2.py:145][0m 122880 total steps have happened
[32m[20221213 20:12:22 @agent_ppo2.py:121][0m #------------------------ Iteration 60 --------------------------#
[32m[20221213 20:12:23 @agent_ppo2.py:127][0m Sampling time: 0.61 s by 10 slaves
[32m[20221213 20:12:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:12:23 @agent_ppo2.py:185][0m |           0.0014 |           0.0255 |           0.0000 |
[32m[20221213 20:12:23 @agent_ppo2.py:185][0m |          -0.0098 |           0.0175 |           0.0000 |
[32m[20221213 20:12:24 @agent_ppo2.py:185][0m |          -0.0130 |           0.0156 |           0.0000 |
[32m[20221213 20:12:24 @agent_ppo2.py:185][0m |          -0.0155 |           0.0152 |           0.0000 |
[32m[20221213 20:12:24 @agent_ppo2.py:185][0m |          -0.0165 |           0.0146 |           0.0000 |
[32m[20221213 20:12:24 @agent_ppo2.py:185][0m |          -0.0177 |           0.0144 |           0.0000 |
[32m[20221213 20:12:25 @agent_ppo2.py:185][0m |          -0.0182 |           0.0144 |           0.0000 |
[32m[20221213 20:12:25 @agent_ppo2.py:185][0m |          -0.0184 |           0.0139 |           0.0000 |
[32m[20221213 20:12:25 @agent_ppo2.py:185][0m |          -0.0194 |           0.0139 |           0.0000 |
[32m[20221213 20:12:25 @agent_ppo2.py:185][0m |          -0.0200 |           0.0133 |           0.0000 |
[32m[20221213 20:12:25 @agent_ppo2.py:130][0m Policy update time: 2.81 s
[32m[20221213 20:12:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.89
[32m[20221213 20:12:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.60
[32m[20221213 20:12:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.07
[32m[20221213 20:12:26 @agent_ppo2.py:143][0m Total time:       3.99 min
[32m[20221213 20:12:26 @agent_ppo2.py:145][0m 124928 total steps have happened
[32m[20221213 20:12:26 @agent_ppo2.py:121][0m #------------------------ Iteration 61 --------------------------#
[32m[20221213 20:12:26 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:12:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:12:27 @agent_ppo2.py:185][0m |           0.0013 |           0.0350 |           0.0000 |
[32m[20221213 20:12:27 @agent_ppo2.py:185][0m |          -0.0090 |           0.0236 |           0.0000 |
[32m[20221213 20:12:27 @agent_ppo2.py:185][0m |          -0.0142 |           0.0208 |           0.0000 |
[32m[20221213 20:12:28 @agent_ppo2.py:185][0m |          -0.0168 |           0.0192 |           0.0000 |
[32m[20221213 20:12:28 @agent_ppo2.py:185][0m |          -0.0221 |           0.0186 |           0.0000 |
[32m[20221213 20:12:28 @agent_ppo2.py:185][0m |          -0.0188 |           0.0178 |           0.0000 |
[32m[20221213 20:12:28 @agent_ppo2.py:185][0m |          -0.0201 |           0.0176 |           0.0000 |
[32m[20221213 20:12:29 @agent_ppo2.py:185][0m |          -0.0219 |           0.0174 |           0.0000 |
[32m[20221213 20:12:29 @agent_ppo2.py:185][0m |          -0.0221 |           0.0169 |           0.0000 |
[32m[20221213 20:12:29 @agent_ppo2.py:185][0m |          -0.0268 |           0.0169 |           0.0000 |
[32m[20221213 20:12:29 @agent_ppo2.py:130][0m Policy update time: 2.67 s
[32m[20221213 20:12:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.08
[32m[20221213 20:12:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.94
[32m[20221213 20:12:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.74
[32m[20221213 20:12:30 @agent_ppo2.py:143][0m Total time:       4.05 min
[32m[20221213 20:12:30 @agent_ppo2.py:145][0m 126976 total steps have happened
[32m[20221213 20:12:30 @agent_ppo2.py:121][0m #------------------------ Iteration 62 --------------------------#
[32m[20221213 20:12:30 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:12:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:12:30 @agent_ppo2.py:185][0m |          -0.0002 |           0.0217 |           0.0000 |
[32m[20221213 20:12:31 @agent_ppo2.py:185][0m |          -0.0146 |           0.0192 |           0.0000 |
[32m[20221213 20:12:31 @agent_ppo2.py:185][0m |          -0.0179 |           0.0177 |           0.0000 |
[32m[20221213 20:12:31 @agent_ppo2.py:185][0m |          -0.0203 |           0.0177 |           0.0000 |
[32m[20221213 20:12:32 @agent_ppo2.py:185][0m |          -0.0261 |           0.0169 |           0.0000 |
[32m[20221213 20:12:32 @agent_ppo2.py:185][0m |          -0.0276 |           0.0162 |           0.0000 |
[32m[20221213 20:12:33 @agent_ppo2.py:185][0m |          -0.0290 |           0.0159 |           0.0000 |
[32m[20221213 20:12:33 @agent_ppo2.py:185][0m |          -0.0310 |           0.0158 |           0.0000 |
[32m[20221213 20:12:34 @agent_ppo2.py:185][0m |          -0.0340 |           0.0157 |           0.0000 |
[32m[20221213 20:12:34 @agent_ppo2.py:185][0m |          -0.0327 |           0.0155 |           0.0000 |
[32m[20221213 20:12:34 @agent_ppo2.py:130][0m Policy update time: 4.04 s
[32m[20221213 20:12:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.90
[32m[20221213 20:12:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.16
[32m[20221213 20:12:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.64
[32m[20221213 20:12:35 @agent_ppo2.py:143][0m Total time:       4.14 min
[32m[20221213 20:12:35 @agent_ppo2.py:145][0m 129024 total steps have happened
[32m[20221213 20:12:35 @agent_ppo2.py:121][0m #------------------------ Iteration 63 --------------------------#
[32m[20221213 20:12:35 @agent_ppo2.py:127][0m Sampling time: 0.59 s by 10 slaves
[32m[20221213 20:12:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:12:36 @agent_ppo2.py:185][0m |          -0.0020 |           0.0125 |           0.0000 |
[32m[20221213 20:12:36 @agent_ppo2.py:185][0m |          -0.0135 |           0.0105 |           0.0000 |
[32m[20221213 20:12:36 @agent_ppo2.py:185][0m |          -0.0138 |           0.0104 |           0.0000 |
[32m[20221213 20:12:37 @agent_ppo2.py:185][0m |          -0.0190 |           0.0101 |           0.0000 |
[32m[20221213 20:12:37 @agent_ppo2.py:185][0m |          -0.0204 |           0.0098 |           0.0000 |
[32m[20221213 20:12:37 @agent_ppo2.py:185][0m |          -0.0213 |           0.0096 |           0.0000 |
[32m[20221213 20:12:38 @agent_ppo2.py:185][0m |          -0.0236 |           0.0096 |           0.0000 |
[32m[20221213 20:12:38 @agent_ppo2.py:185][0m |          -0.0240 |           0.0093 |           0.0000 |
[32m[20221213 20:12:38 @agent_ppo2.py:185][0m |          -0.0253 |           0.0092 |           0.0000 |
[32m[20221213 20:12:39 @agent_ppo2.py:185][0m |          -0.0248 |           0.0091 |           0.0000 |
[32m[20221213 20:12:39 @agent_ppo2.py:130][0m Policy update time: 3.57 s
[32m[20221213 20:12:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.46
[32m[20221213 20:12:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.26
[32m[20221213 20:12:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.98
[32m[20221213 20:12:40 @agent_ppo2.py:143][0m Total time:       4.22 min
[32m[20221213 20:12:40 @agent_ppo2.py:145][0m 131072 total steps have happened
[32m[20221213 20:12:40 @agent_ppo2.py:121][0m #------------------------ Iteration 64 --------------------------#
[32m[20221213 20:12:40 @agent_ppo2.py:127][0m Sampling time: 0.62 s by 10 slaves
[32m[20221213 20:12:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:12:41 @agent_ppo2.py:185][0m |           0.0026 |           0.0424 |           0.0000 |
[32m[20221213 20:12:41 @agent_ppo2.py:185][0m |          -0.0062 |           0.0241 |           0.0000 |
[32m[20221213 20:12:41 @agent_ppo2.py:185][0m |          -0.0111 |           0.0209 |           0.0000 |
[32m[20221213 20:12:42 @agent_ppo2.py:185][0m |          -0.0131 |           0.0199 |           0.0000 |
[32m[20221213 20:12:42 @agent_ppo2.py:185][0m |          -0.0187 |           0.0194 |           0.0000 |
[32m[20221213 20:12:42 @agent_ppo2.py:185][0m |          -0.0156 |           0.0191 |           0.0000 |
[32m[20221213 20:12:43 @agent_ppo2.py:185][0m |          -0.0219 |           0.0186 |           0.0000 |
[32m[20221213 20:12:43 @agent_ppo2.py:185][0m |          -0.0176 |           0.0184 |           0.0000 |
[32m[20221213 20:12:43 @agent_ppo2.py:185][0m |          -0.0182 |           0.0178 |           0.0000 |
[32m[20221213 20:12:44 @agent_ppo2.py:185][0m |          -0.0183 |           0.0177 |           0.0000 |
[32m[20221213 20:12:44 @agent_ppo2.py:130][0m Policy update time: 3.56 s
[32m[20221213 20:12:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.99
[32m[20221213 20:12:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.48
[32m[20221213 20:12:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.79
[32m[20221213 20:12:44 @agent_ppo2.py:143][0m Total time:       4.30 min
[32m[20221213 20:12:44 @agent_ppo2.py:145][0m 133120 total steps have happened
[32m[20221213 20:12:44 @agent_ppo2.py:121][0m #------------------------ Iteration 65 --------------------------#
[32m[20221213 20:12:45 @agent_ppo2.py:127][0m Sampling time: 0.66 s by 10 slaves
[32m[20221213 20:12:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:12:46 @agent_ppo2.py:185][0m |           0.0021 |           0.0177 |           0.0000 |
[32m[20221213 20:12:46 @agent_ppo2.py:185][0m |          -0.0116 |           0.0092 |           0.0000 |
[32m[20221213 20:12:46 @agent_ppo2.py:185][0m |          -0.0153 |           0.0089 |           0.0000 |
[32m[20221213 20:12:47 @agent_ppo2.py:185][0m |          -0.0172 |           0.0087 |           0.0000 |
[32m[20221213 20:12:47 @agent_ppo2.py:185][0m |          -0.0164 |           0.0085 |           0.0000 |
[32m[20221213 20:12:47 @agent_ppo2.py:185][0m |          -0.0193 |           0.0082 |           0.0000 |
[32m[20221213 20:12:48 @agent_ppo2.py:185][0m |          -0.0206 |           0.0080 |           0.0000 |
[32m[20221213 20:12:48 @agent_ppo2.py:185][0m |          -0.0216 |           0.0077 |           0.0000 |
[32m[20221213 20:12:48 @agent_ppo2.py:185][0m |          -0.0222 |           0.0075 |           0.0000 |
[32m[20221213 20:12:49 @agent_ppo2.py:185][0m |          -0.0224 |           0.0073 |           0.0000 |
[32m[20221213 20:12:49 @agent_ppo2.py:130][0m Policy update time: 3.74 s
[32m[20221213 20:12:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.66
[32m[20221213 20:12:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.18
[32m[20221213 20:12:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.14
[32m[20221213 20:12:49 @agent_ppo2.py:143][0m Total time:       4.39 min
[32m[20221213 20:12:49 @agent_ppo2.py:145][0m 135168 total steps have happened
[32m[20221213 20:12:49 @agent_ppo2.py:121][0m #------------------------ Iteration 66 --------------------------#
[32m[20221213 20:12:50 @agent_ppo2.py:127][0m Sampling time: 0.58 s by 10 slaves
[32m[20221213 20:12:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:12:51 @agent_ppo2.py:185][0m |          -0.0003 |           0.0062 |           0.0000 |
[32m[20221213 20:12:51 @agent_ppo2.py:185][0m |          -0.0149 |           0.0059 |           0.0000 |
[32m[20221213 20:12:51 @agent_ppo2.py:185][0m |          -0.0200 |           0.0057 |           0.0000 |
[32m[20221213 20:12:52 @agent_ppo2.py:185][0m |          -0.0233 |           0.0056 |           0.0000 |
[32m[20221213 20:12:52 @agent_ppo2.py:185][0m |          -0.0255 |           0.0054 |           0.0000 |
[32m[20221213 20:12:52 @agent_ppo2.py:185][0m |          -0.0277 |           0.0053 |           0.0000 |
[32m[20221213 20:12:53 @agent_ppo2.py:185][0m |          -0.0259 |           0.0051 |           0.0000 |
[32m[20221213 20:12:53 @agent_ppo2.py:185][0m |          -0.0298 |           0.0050 |           0.0000 |
[32m[20221213 20:12:53 @agent_ppo2.py:185][0m |          -0.0307 |           0.0049 |           0.0000 |
[32m[20221213 20:12:54 @agent_ppo2.py:185][0m |          -0.0306 |           0.0048 |           0.0000 |
[32m[20221213 20:12:54 @agent_ppo2.py:130][0m Policy update time: 3.61 s
[32m[20221213 20:12:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.40
[32m[20221213 20:12:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.98
[32m[20221213 20:12:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.35
[32m[20221213 20:12:54 @agent_ppo2.py:143][0m Total time:       4.46 min
[32m[20221213 20:12:54 @agent_ppo2.py:145][0m 137216 total steps have happened
[32m[20221213 20:12:54 @agent_ppo2.py:121][0m #------------------------ Iteration 67 --------------------------#
[32m[20221213 20:12:55 @agent_ppo2.py:127][0m Sampling time: 0.62 s by 10 slaves
[32m[20221213 20:12:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:12:55 @agent_ppo2.py:185][0m |           0.0007 |           0.0539 |           0.0000 |
[32m[20221213 20:12:56 @agent_ppo2.py:185][0m |          -0.0086 |           0.0229 |           0.0000 |
[32m[20221213 20:12:56 @agent_ppo2.py:185][0m |          -0.0132 |           0.0204 |           0.0000 |
[32m[20221213 20:12:56 @agent_ppo2.py:185][0m |          -0.0162 |           0.0182 |           0.0000 |
[32m[20221213 20:12:57 @agent_ppo2.py:185][0m |          -0.0152 |           0.0178 |           0.0000 |
[32m[20221213 20:12:57 @agent_ppo2.py:185][0m |          -0.0163 |           0.0168 |           0.0000 |
[32m[20221213 20:12:57 @agent_ppo2.py:185][0m |          -0.0163 |           0.0169 |           0.0000 |
[32m[20221213 20:12:58 @agent_ppo2.py:185][0m |          -0.0172 |           0.0166 |           0.0000 |
[32m[20221213 20:12:58 @agent_ppo2.py:185][0m |          -0.0174 |           0.0158 |           0.0000 |
[32m[20221213 20:12:58 @agent_ppo2.py:185][0m |          -0.0174 |           0.0157 |           0.0000 |
[32m[20221213 20:12:58 @agent_ppo2.py:130][0m Policy update time: 3.68 s
[32m[20221213 20:12:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.32
[32m[20221213 20:12:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.58
[32m[20221213 20:12:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.10
[32m[20221213 20:12:59 @agent_ppo2.py:143][0m Total time:       4.55 min
[32m[20221213 20:12:59 @agent_ppo2.py:145][0m 139264 total steps have happened
[32m[20221213 20:12:59 @agent_ppo2.py:121][0m #------------------------ Iteration 68 --------------------------#
[32m[20221213 20:13:00 @agent_ppo2.py:127][0m Sampling time: 0.59 s by 10 slaves
[32m[20221213 20:13:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:00 @agent_ppo2.py:185][0m |           0.0017 |           0.0167 |           0.0000 |
[32m[20221213 20:13:01 @agent_ppo2.py:185][0m |          -0.0117 |           0.0138 |           0.0000 |
[32m[20221213 20:13:01 @agent_ppo2.py:185][0m |          -0.0161 |           0.0132 |           0.0000 |
[32m[20221213 20:13:01 @agent_ppo2.py:185][0m |          -0.0189 |           0.0124 |           0.0000 |
[32m[20221213 20:13:02 @agent_ppo2.py:185][0m |          -0.0220 |           0.0121 |           0.0000 |
[32m[20221213 20:13:02 @agent_ppo2.py:185][0m |          -0.0218 |           0.0117 |           0.0000 |
[32m[20221213 20:13:02 @agent_ppo2.py:185][0m |          -0.0232 |           0.0115 |           0.0000 |
[32m[20221213 20:13:03 @agent_ppo2.py:185][0m |          -0.0236 |           0.0113 |           0.0000 |
[32m[20221213 20:13:03 @agent_ppo2.py:185][0m |          -0.0246 |           0.0112 |           0.0000 |
[32m[20221213 20:13:03 @agent_ppo2.py:185][0m |          -0.0254 |           0.0108 |           0.0000 |
[32m[20221213 20:13:03 @agent_ppo2.py:130][0m Policy update time: 3.53 s
[32m[20221213 20:13:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.05
[32m[20221213 20:13:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.69
[32m[20221213 20:13:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.13
[32m[20221213 20:13:04 @agent_ppo2.py:143][0m Total time:       4.63 min
[32m[20221213 20:13:04 @agent_ppo2.py:145][0m 141312 total steps have happened
[32m[20221213 20:13:04 @agent_ppo2.py:121][0m #------------------------ Iteration 69 --------------------------#
[32m[20221213 20:13:04 @agent_ppo2.py:127][0m Sampling time: 0.57 s by 10 slaves
[32m[20221213 20:13:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:05 @agent_ppo2.py:185][0m |           0.0018 |           0.0261 |           0.0000 |
[32m[20221213 20:13:05 @agent_ppo2.py:185][0m |          -0.0091 |           0.0193 |           0.0000 |
[32m[20221213 20:13:06 @agent_ppo2.py:185][0m |          -0.0121 |           0.0176 |           0.0000 |
[32m[20221213 20:13:06 @agent_ppo2.py:185][0m |          -0.0137 |           0.0166 |           0.0000 |
[32m[20221213 20:13:06 @agent_ppo2.py:185][0m |          -0.0150 |           0.0158 |           0.0000 |
[32m[20221213 20:13:07 @agent_ppo2.py:185][0m |          -0.0162 |           0.0161 |           0.0000 |
[32m[20221213 20:13:07 @agent_ppo2.py:185][0m |          -0.0171 |           0.0154 |           0.0000 |
[32m[20221213 20:13:07 @agent_ppo2.py:185][0m |          -0.0170 |           0.0149 |           0.0000 |
[32m[20221213 20:13:08 @agent_ppo2.py:185][0m |          -0.0185 |           0.0153 |           0.0000 |
[32m[20221213 20:13:08 @agent_ppo2.py:185][0m |          -0.0189 |           0.0150 |           0.0000 |
[32m[20221213 20:13:08 @agent_ppo2.py:130][0m Policy update time: 3.60 s
[32m[20221213 20:13:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.11
[32m[20221213 20:13:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.03
[32m[20221213 20:13:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.88
[32m[20221213 20:13:09 @agent_ppo2.py:143][0m Total time:       4.71 min
[32m[20221213 20:13:09 @agent_ppo2.py:145][0m 143360 total steps have happened
[32m[20221213 20:13:09 @agent_ppo2.py:121][0m #------------------------ Iteration 70 --------------------------#
[32m[20221213 20:13:09 @agent_ppo2.py:127][0m Sampling time: 0.58 s by 10 slaves
[32m[20221213 20:13:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:10 @agent_ppo2.py:185][0m |          -0.0004 |           0.0217 |           0.0000 |
[32m[20221213 20:13:10 @agent_ppo2.py:185][0m |          -0.0117 |           0.0179 |           0.0000 |
[32m[20221213 20:13:10 @agent_ppo2.py:185][0m |          -0.0164 |           0.0165 |           0.0000 |
[32m[20221213 20:13:11 @agent_ppo2.py:185][0m |          -0.0196 |           0.0159 |           0.0000 |
[32m[20221213 20:13:11 @agent_ppo2.py:185][0m |          -0.0212 |           0.0156 |           0.0000 |
[32m[20221213 20:13:11 @agent_ppo2.py:185][0m |          -0.0209 |           0.0152 |           0.0000 |
[32m[20221213 20:13:11 @agent_ppo2.py:185][0m |          -0.0223 |           0.0153 |           0.0000 |
[32m[20221213 20:13:12 @agent_ppo2.py:185][0m |          -0.0250 |           0.0147 |           0.0000 |
[32m[20221213 20:13:12 @agent_ppo2.py:185][0m |          -0.0247 |           0.0146 |           0.0000 |
[32m[20221213 20:13:12 @agent_ppo2.py:185][0m |          -0.0263 |           0.0143 |           0.0000 |
[32m[20221213 20:13:12 @agent_ppo2.py:130][0m Policy update time: 3.06 s
[32m[20221213 20:13:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.19
[32m[20221213 20:13:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.81
[32m[20221213 20:13:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.53
[32m[20221213 20:13:13 @agent_ppo2.py:143][0m Total time:       4.78 min
[32m[20221213 20:13:13 @agent_ppo2.py:145][0m 145408 total steps have happened
[32m[20221213 20:13:13 @agent_ppo2.py:121][0m #------------------------ Iteration 71 --------------------------#
[32m[20221213 20:13:13 @agent_ppo2.py:127][0m Sampling time: 0.56 s by 10 slaves
[32m[20221213 20:13:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:14 @agent_ppo2.py:185][0m |          -0.0007 |           0.0169 |           0.0000 |
[32m[20221213 20:13:14 @agent_ppo2.py:185][0m |          -0.0117 |           0.0155 |           0.0000 |
[32m[20221213 20:13:14 @agent_ppo2.py:185][0m |          -0.0179 |           0.0146 |           0.0000 |
[32m[20221213 20:13:15 @agent_ppo2.py:185][0m |          -0.0204 |           0.0141 |           0.0000 |
[32m[20221213 20:13:15 @agent_ppo2.py:185][0m |          -0.0222 |           0.0132 |           0.0000 |
[32m[20221213 20:13:15 @agent_ppo2.py:185][0m |          -0.0232 |           0.0130 |           0.0000 |
[32m[20221213 20:13:15 @agent_ppo2.py:185][0m |          -0.0239 |           0.0127 |           0.0000 |
[32m[20221213 20:13:16 @agent_ppo2.py:185][0m |          -0.0257 |           0.0125 |           0.0000 |
[32m[20221213 20:13:16 @agent_ppo2.py:185][0m |          -0.0258 |           0.0122 |           0.0000 |
[32m[20221213 20:13:16 @agent_ppo2.py:185][0m |          -0.0278 |           0.0124 |           0.0000 |
[32m[20221213 20:13:16 @agent_ppo2.py:130][0m Policy update time: 2.84 s
[32m[20221213 20:13:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.61
[32m[20221213 20:13:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.63
[32m[20221213 20:13:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.77
[32m[20221213 20:13:17 @agent_ppo2.py:143][0m Total time:       4.84 min
[32m[20221213 20:13:17 @agent_ppo2.py:145][0m 147456 total steps have happened
[32m[20221213 20:13:17 @agent_ppo2.py:121][0m #------------------------ Iteration 72 --------------------------#
[32m[20221213 20:13:17 @agent_ppo2.py:127][0m Sampling time: 0.64 s by 10 slaves
[32m[20221213 20:13:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:18 @agent_ppo2.py:185][0m |          -0.0004 |           0.0922 |           0.0000 |
[32m[20221213 20:13:18 @agent_ppo2.py:185][0m |          -0.0132 |           0.0462 |           0.0000 |
[32m[20221213 20:13:18 @agent_ppo2.py:185][0m |          -0.0155 |           0.0396 |           0.0000 |
[32m[20221213 20:13:19 @agent_ppo2.py:185][0m |          -0.0167 |           0.0363 |           0.0000 |
[32m[20221213 20:13:19 @agent_ppo2.py:185][0m |          -0.0192 |           0.0340 |           0.0000 |
[32m[20221213 20:13:19 @agent_ppo2.py:185][0m |          -0.0205 |           0.0319 |           0.0000 |
[32m[20221213 20:13:19 @agent_ppo2.py:185][0m |          -0.0225 |           0.0311 |           0.0000 |
[32m[20221213 20:13:20 @agent_ppo2.py:185][0m |          -0.0218 |           0.0305 |           0.0000 |
[32m[20221213 20:13:20 @agent_ppo2.py:185][0m |          -0.0274 |           0.0294 |           0.0000 |
[32m[20221213 20:13:20 @agent_ppo2.py:185][0m |          -0.0211 |           0.0300 |           0.0000 |
[32m[20221213 20:13:20 @agent_ppo2.py:130][0m Policy update time: 2.80 s
[32m[20221213 20:13:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.93
[32m[20221213 20:13:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.49
[32m[20221213 20:13:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.83
[32m[20221213 20:13:21 @agent_ppo2.py:143][0m Total time:       4.91 min
[32m[20221213 20:13:21 @agent_ppo2.py:145][0m 149504 total steps have happened
[32m[20221213 20:13:21 @agent_ppo2.py:121][0m #------------------------ Iteration 73 --------------------------#
[32m[20221213 20:13:21 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:13:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:22 @agent_ppo2.py:185][0m |           0.0018 |           0.0231 |           0.0000 |
[32m[20221213 20:13:22 @agent_ppo2.py:185][0m |          -0.0107 |           0.0160 |           0.0000 |
[32m[20221213 20:13:22 @agent_ppo2.py:185][0m |          -0.0159 |           0.0156 |           0.0000 |
[32m[20221213 20:13:22 @agent_ppo2.py:185][0m |          -0.0201 |           0.0147 |           0.0000 |
[32m[20221213 20:13:23 @agent_ppo2.py:185][0m |          -0.0232 |           0.0143 |           0.0000 |
[32m[20221213 20:13:23 @agent_ppo2.py:185][0m |          -0.0235 |           0.0140 |           0.0000 |
[32m[20221213 20:13:23 @agent_ppo2.py:185][0m |          -0.0244 |           0.0138 |           0.0000 |
[32m[20221213 20:13:24 @agent_ppo2.py:185][0m |          -0.0257 |           0.0137 |           0.0000 |
[32m[20221213 20:13:24 @agent_ppo2.py:185][0m |          -0.0269 |           0.0134 |           0.0000 |
[32m[20221213 20:13:24 @agent_ppo2.py:185][0m |          -0.0276 |           0.0133 |           0.0000 |
[32m[20221213 20:13:24 @agent_ppo2.py:130][0m Policy update time: 2.79 s
[32m[20221213 20:13:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.63
[32m[20221213 20:13:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.37
[32m[20221213 20:13:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.15
[32m[20221213 20:13:25 @agent_ppo2.py:143][0m Total time:       4.97 min
[32m[20221213 20:13:25 @agent_ppo2.py:145][0m 151552 total steps have happened
[32m[20221213 20:13:25 @agent_ppo2.py:121][0m #------------------------ Iteration 74 --------------------------#
[32m[20221213 20:13:25 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:13:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:25 @agent_ppo2.py:185][0m |           0.0021 |           0.0267 |           0.0000 |
[32m[20221213 20:13:26 @agent_ppo2.py:185][0m |          -0.0100 |           0.0218 |           0.0000 |
[32m[20221213 20:13:26 @agent_ppo2.py:185][0m |          -0.0143 |           0.0197 |           0.0000 |
[32m[20221213 20:13:26 @agent_ppo2.py:185][0m |          -0.0150 |           0.0178 |           0.0000 |
[32m[20221213 20:13:27 @agent_ppo2.py:185][0m |          -0.0171 |           0.0172 |           0.0000 |
[32m[20221213 20:13:27 @agent_ppo2.py:185][0m |          -0.0194 |           0.0164 |           0.0000 |
[32m[20221213 20:13:27 @agent_ppo2.py:185][0m |          -0.0211 |           0.0163 |           0.0000 |
[32m[20221213 20:13:27 @agent_ppo2.py:185][0m |          -0.0215 |           0.0156 |           0.0000 |
[32m[20221213 20:13:28 @agent_ppo2.py:185][0m |          -0.0221 |           0.0161 |           0.0000 |
[32m[20221213 20:13:28 @agent_ppo2.py:185][0m |          -0.0228 |           0.0150 |           0.0000 |
[32m[20221213 20:13:28 @agent_ppo2.py:130][0m Policy update time: 2.96 s
[32m[20221213 20:13:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.88
[32m[20221213 20:13:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.19
[32m[20221213 20:13:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.80
[32m[20221213 20:13:29 @agent_ppo2.py:143][0m Total time:       5.04 min
[32m[20221213 20:13:29 @agent_ppo2.py:145][0m 153600 total steps have happened
[32m[20221213 20:13:29 @agent_ppo2.py:121][0m #------------------------ Iteration 75 --------------------------#
[32m[20221213 20:13:29 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:13:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:30 @agent_ppo2.py:185][0m |           0.0014 |           0.0329 |           0.0000 |
[32m[20221213 20:13:30 @agent_ppo2.py:185][0m |          -0.0087 |           0.0251 |           0.0000 |
[32m[20221213 20:13:30 @agent_ppo2.py:185][0m |          -0.0131 |           0.0224 |           0.0000 |
[32m[20221213 20:13:30 @agent_ppo2.py:185][0m |          -0.0147 |           0.0204 |           0.0000 |
[32m[20221213 20:13:31 @agent_ppo2.py:185][0m |          -0.0163 |           0.0199 |           0.0000 |
[32m[20221213 20:13:31 @agent_ppo2.py:185][0m |          -0.0175 |           0.0194 |           0.0000 |
[32m[20221213 20:13:31 @agent_ppo2.py:185][0m |          -0.0238 |           0.0186 |           0.0000 |
[32m[20221213 20:13:31 @agent_ppo2.py:185][0m |          -0.0183 |           0.0183 |           0.0000 |
[32m[20221213 20:13:32 @agent_ppo2.py:185][0m |          -0.0193 |           0.0178 |           0.0000 |
[32m[20221213 20:13:32 @agent_ppo2.py:185][0m |          -0.0196 |           0.0178 |           0.0000 |
[32m[20221213 20:13:32 @agent_ppo2.py:130][0m Policy update time: 2.81 s
[32m[20221213 20:13:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.78
[32m[20221213 20:13:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.78
[32m[20221213 20:13:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.67
[32m[20221213 20:13:32 @agent_ppo2.py:143][0m Total time:       5.10 min
[32m[20221213 20:13:32 @agent_ppo2.py:145][0m 155648 total steps have happened
[32m[20221213 20:13:32 @agent_ppo2.py:121][0m #------------------------ Iteration 76 --------------------------#
[32m[20221213 20:13:33 @agent_ppo2.py:127][0m Sampling time: 0.46 s by 10 slaves
[32m[20221213 20:13:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:33 @agent_ppo2.py:185][0m |           0.0017 |           0.0180 |           0.0000 |
[32m[20221213 20:13:34 @agent_ppo2.py:185][0m |          -0.0118 |           0.0100 |           0.0000 |
[32m[20221213 20:13:34 @agent_ppo2.py:185][0m |          -0.0150 |           0.0096 |           0.0000 |
[32m[20221213 20:13:34 @agent_ppo2.py:185][0m |          -0.0176 |           0.0094 |           0.0000 |
[32m[20221213 20:13:34 @agent_ppo2.py:185][0m |          -0.0188 |           0.0091 |           0.0000 |
[32m[20221213 20:13:35 @agent_ppo2.py:185][0m |          -0.0208 |           0.0090 |           0.0000 |
[32m[20221213 20:13:35 @agent_ppo2.py:185][0m |          -0.0210 |           0.0087 |           0.0000 |
[32m[20221213 20:13:35 @agent_ppo2.py:185][0m |          -0.0223 |           0.0085 |           0.0000 |
[32m[20221213 20:13:35 @agent_ppo2.py:185][0m |          -0.0224 |           0.0083 |           0.0000 |
[32m[20221213 20:13:36 @agent_ppo2.py:185][0m |          -0.0229 |           0.0081 |           0.0000 |
[32m[20221213 20:13:36 @agent_ppo2.py:130][0m Policy update time: 2.91 s
[32m[20221213 20:13:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.24
[32m[20221213 20:13:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.86
[32m[20221213 20:13:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.82
[32m[20221213 20:13:36 @agent_ppo2.py:143][0m Total time:       5.17 min
[32m[20221213 20:13:36 @agent_ppo2.py:145][0m 157696 total steps have happened
[32m[20221213 20:13:36 @agent_ppo2.py:121][0m #------------------------ Iteration 77 --------------------------#
[32m[20221213 20:13:37 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:13:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:37 @agent_ppo2.py:185][0m |           0.0009 |           0.0148 |           0.0000 |
[32m[20221213 20:13:37 @agent_ppo2.py:185][0m |          -0.0107 |           0.0119 |           0.0000 |
[32m[20221213 20:13:38 @agent_ppo2.py:185][0m |          -0.0130 |           0.0112 |           0.0000 |
[32m[20221213 20:13:38 @agent_ppo2.py:185][0m |          -0.0150 |           0.0109 |           0.0000 |
[32m[20221213 20:13:38 @agent_ppo2.py:185][0m |          -0.0169 |           0.0107 |           0.0000 |
[32m[20221213 20:13:39 @agent_ppo2.py:185][0m |          -0.0178 |           0.0106 |           0.0000 |
[32m[20221213 20:13:39 @agent_ppo2.py:185][0m |          -0.0182 |           0.0106 |           0.0000 |
[32m[20221213 20:13:39 @agent_ppo2.py:185][0m |          -0.0188 |           0.0106 |           0.0000 |
[32m[20221213 20:13:39 @agent_ppo2.py:185][0m |          -0.0198 |           0.0103 |           0.0000 |
[32m[20221213 20:13:40 @agent_ppo2.py:185][0m |          -0.0196 |           0.0103 |           0.0000 |
[32m[20221213 20:13:40 @agent_ppo2.py:130][0m Policy update time: 2.88 s
[32m[20221213 20:13:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.68
[32m[20221213 20:13:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.04
[32m[20221213 20:13:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.75
[32m[20221213 20:13:40 @agent_ppo2.py:143][0m Total time:       5.23 min
[32m[20221213 20:13:40 @agent_ppo2.py:145][0m 159744 total steps have happened
[32m[20221213 20:13:40 @agent_ppo2.py:121][0m #------------------------ Iteration 78 --------------------------#
[32m[20221213 20:13:41 @agent_ppo2.py:127][0m Sampling time: 0.56 s by 10 slaves
[32m[20221213 20:13:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:41 @agent_ppo2.py:185][0m |          -0.0000 |           0.0210 |           0.0000 |
[32m[20221213 20:13:41 @agent_ppo2.py:185][0m |          -0.0112 |           0.0161 |           0.0000 |
[32m[20221213 20:13:42 @agent_ppo2.py:185][0m |          -0.0165 |           0.0148 |           0.0000 |
[32m[20221213 20:13:42 @agent_ppo2.py:185][0m |          -0.0139 |           0.0149 |           0.0000 |
[32m[20221213 20:13:42 @agent_ppo2.py:185][0m |          -0.0196 |           0.0143 |           0.0000 |
[32m[20221213 20:13:42 @agent_ppo2.py:185][0m |          -0.0152 |           0.0138 |           0.0000 |
[32m[20221213 20:13:43 @agent_ppo2.py:185][0m |          -0.0168 |           0.0139 |           0.0000 |
[32m[20221213 20:13:43 @agent_ppo2.py:185][0m |          -0.0170 |           0.0132 |           0.0000 |
[32m[20221213 20:13:43 @agent_ppo2.py:185][0m |          -0.0183 |           0.0135 |           0.0000 |
[32m[20221213 20:13:44 @agent_ppo2.py:185][0m |          -0.0186 |           0.0131 |           0.0000 |
[32m[20221213 20:13:44 @agent_ppo2.py:130][0m Policy update time: 2.86 s
[32m[20221213 20:13:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.72
[32m[20221213 20:13:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.70
[32m[20221213 20:13:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.25
[32m[20221213 20:13:44 @agent_ppo2.py:143][0m Total time:       5.30 min
[32m[20221213 20:13:44 @agent_ppo2.py:145][0m 161792 total steps have happened
[32m[20221213 20:13:44 @agent_ppo2.py:121][0m #------------------------ Iteration 79 --------------------------#
[32m[20221213 20:13:45 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:13:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:45 @agent_ppo2.py:185][0m |           0.0027 |           0.0115 |           0.0000 |
[32m[20221213 20:13:45 @agent_ppo2.py:185][0m |          -0.0120 |           0.0100 |           0.0000 |
[32m[20221213 20:13:45 @agent_ppo2.py:185][0m |          -0.0164 |           0.0098 |           0.0000 |
[32m[20221213 20:13:46 @agent_ppo2.py:185][0m |          -0.0177 |           0.0096 |           0.0000 |
[32m[20221213 20:13:46 @agent_ppo2.py:185][0m |          -0.0197 |           0.0095 |           0.0000 |
[32m[20221213 20:13:46 @agent_ppo2.py:185][0m |          -0.0211 |           0.0093 |           0.0000 |
[32m[20221213 20:13:47 @agent_ppo2.py:185][0m |          -0.0221 |           0.0092 |           0.0000 |
[32m[20221213 20:13:47 @agent_ppo2.py:185][0m |          -0.0235 |           0.0092 |           0.0000 |
[32m[20221213 20:13:47 @agent_ppo2.py:185][0m |          -0.0242 |           0.0091 |           0.0000 |
[32m[20221213 20:13:47 @agent_ppo2.py:185][0m |          -0.0245 |           0.0091 |           0.0000 |
[32m[20221213 20:13:47 @agent_ppo2.py:130][0m Policy update time: 2.76 s
[32m[20221213 20:13:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.01
[32m[20221213 20:13:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.28
[32m[20221213 20:13:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.26
[32m[20221213 20:13:48 @agent_ppo2.py:143][0m Total time:       5.36 min
[32m[20221213 20:13:48 @agent_ppo2.py:145][0m 163840 total steps have happened
[32m[20221213 20:13:48 @agent_ppo2.py:121][0m #------------------------ Iteration 80 --------------------------#
[32m[20221213 20:13:48 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:13:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:49 @agent_ppo2.py:185][0m |           0.0021 |           0.0527 |           0.0000 |
[32m[20221213 20:13:49 @agent_ppo2.py:185][0m |          -0.0076 |           0.0295 |           0.0000 |
[32m[20221213 20:13:49 @agent_ppo2.py:185][0m |          -0.0129 |           0.0233 |           0.0000 |
[32m[20221213 20:13:50 @agent_ppo2.py:185][0m |          -0.0143 |           0.0226 |           0.0000 |
[32m[20221213 20:13:50 @agent_ppo2.py:185][0m |          -0.0152 |           0.0211 |           0.0000 |
[32m[20221213 20:13:50 @agent_ppo2.py:185][0m |          -0.0161 |           0.0203 |           0.0000 |
[32m[20221213 20:13:50 @agent_ppo2.py:185][0m |          -0.0174 |           0.0199 |           0.0000 |
[32m[20221213 20:13:51 @agent_ppo2.py:185][0m |          -0.0176 |           0.0197 |           0.0000 |
[32m[20221213 20:13:51 @agent_ppo2.py:185][0m |          -0.0179 |           0.0197 |           0.0000 |
[32m[20221213 20:13:51 @agent_ppo2.py:185][0m |          -0.0188 |           0.0194 |           0.0000 |
[32m[20221213 20:13:51 @agent_ppo2.py:130][0m Policy update time: 2.96 s
[32m[20221213 20:13:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.09
[32m[20221213 20:13:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.28
[32m[20221213 20:13:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.09
[32m[20221213 20:13:52 @agent_ppo2.py:143][0m Total time:       5.42 min
[32m[20221213 20:13:52 @agent_ppo2.py:145][0m 165888 total steps have happened
[32m[20221213 20:13:52 @agent_ppo2.py:121][0m #------------------------ Iteration 81 --------------------------#
[32m[20221213 20:13:52 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:13:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:53 @agent_ppo2.py:185][0m |           0.0028 |           0.0189 |           0.0000 |
[32m[20221213 20:13:53 @agent_ppo2.py:185][0m |          -0.0110 |           0.0094 |           0.0000 |
[32m[20221213 20:13:53 @agent_ppo2.py:185][0m |          -0.0154 |           0.0089 |           0.0000 |
[32m[20221213 20:13:54 @agent_ppo2.py:185][0m |          -0.0178 |           0.0087 |           0.0000 |
[32m[20221213 20:13:54 @agent_ppo2.py:185][0m |          -0.0185 |           0.0085 |           0.0000 |
[32m[20221213 20:13:54 @agent_ppo2.py:185][0m |          -0.0182 |           0.0083 |           0.0000 |
[32m[20221213 20:13:54 @agent_ppo2.py:185][0m |          -0.0200 |           0.0081 |           0.0000 |
[32m[20221213 20:13:55 @agent_ppo2.py:185][0m |          -0.0197 |           0.0078 |           0.0000 |
[32m[20221213 20:13:55 @agent_ppo2.py:185][0m |          -0.0207 |           0.0076 |           0.0000 |
[32m[20221213 20:13:55 @agent_ppo2.py:185][0m |          -0.0218 |           0.0074 |           0.0000 |
[32m[20221213 20:13:55 @agent_ppo2.py:130][0m Policy update time: 2.99 s
[32m[20221213 20:13:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.76
[32m[20221213 20:13:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.46
[32m[20221213 20:13:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.31
[32m[20221213 20:13:56 @agent_ppo2.py:143][0m Total time:       5.49 min
[32m[20221213 20:13:56 @agent_ppo2.py:145][0m 167936 total steps have happened
[32m[20221213 20:13:56 @agent_ppo2.py:121][0m #------------------------ Iteration 82 --------------------------#
[32m[20221213 20:13:56 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:13:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:13:57 @agent_ppo2.py:185][0m |           0.0017 |           0.1079 |           0.0000 |
[32m[20221213 20:13:57 @agent_ppo2.py:185][0m |          -0.0082 |           0.0511 |           0.0000 |
[32m[20221213 20:13:57 @agent_ppo2.py:185][0m |          -0.0131 |           0.0426 |           0.0000 |
[32m[20221213 20:13:57 @agent_ppo2.py:185][0m |          -0.0155 |           0.0386 |           0.0000 |
[32m[20221213 20:13:58 @agent_ppo2.py:185][0m |          -0.0169 |           0.0359 |           0.0000 |
[32m[20221213 20:13:58 @agent_ppo2.py:185][0m |          -0.0173 |           0.0338 |           0.0000 |
[32m[20221213 20:13:58 @agent_ppo2.py:185][0m |          -0.0213 |           0.0329 |           0.0000 |
[32m[20221213 20:13:58 @agent_ppo2.py:185][0m |          -0.0197 |           0.0320 |           0.0000 |
[32m[20221213 20:13:59 @agent_ppo2.py:185][0m |          -0.0184 |           0.0299 |           0.0000 |
[32m[20221213 20:13:59 @agent_ppo2.py:185][0m |          -0.0191 |           0.0300 |           0.0000 |
[32m[20221213 20:13:59 @agent_ppo2.py:130][0m Policy update time: 2.78 s
[32m[20221213 20:13:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.68
[32m[20221213 20:13:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 15.86
[32m[20221213 20:13:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.23
[32m[20221213 20:13:59 @agent_ppo2.py:143][0m Total time:       5.55 min
[32m[20221213 20:13:59 @agent_ppo2.py:145][0m 169984 total steps have happened
[32m[20221213 20:13:59 @agent_ppo2.py:121][0m #------------------------ Iteration 83 --------------------------#
[32m[20221213 20:14:00 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:14:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:00 @agent_ppo2.py:185][0m |          -0.0009 |           0.0345 |           0.0000 |
[32m[20221213 20:14:01 @agent_ppo2.py:185][0m |          -0.0140 |           0.0288 |           0.0000 |
[32m[20221213 20:14:01 @agent_ppo2.py:185][0m |          -0.0177 |           0.0277 |           0.0000 |
[32m[20221213 20:14:01 @agent_ppo2.py:185][0m |          -0.0210 |           0.0253 |           0.0000 |
[32m[20221213 20:14:01 @agent_ppo2.py:185][0m |          -0.0235 |           0.0241 |           0.0000 |
[32m[20221213 20:14:02 @agent_ppo2.py:185][0m |          -0.0268 |           0.0240 |           0.0000 |
[32m[20221213 20:14:02 @agent_ppo2.py:185][0m |          -0.0267 |           0.0229 |           0.0000 |
[32m[20221213 20:14:02 @agent_ppo2.py:185][0m |          -0.0286 |           0.0220 |           0.0000 |
[32m[20221213 20:14:02 @agent_ppo2.py:185][0m |          -0.0287 |           0.0221 |           0.0000 |
[32m[20221213 20:14:03 @agent_ppo2.py:185][0m |          -0.0325 |           0.0216 |           0.0000 |
[32m[20221213 20:14:03 @agent_ppo2.py:130][0m Policy update time: 2.79 s
[32m[20221213 20:14:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.51
[32m[20221213 20:14:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.48
[32m[20221213 20:14:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.12
[32m[20221213 20:14:03 @agent_ppo2.py:143][0m Total time:       5.62 min
[32m[20221213 20:14:03 @agent_ppo2.py:145][0m 172032 total steps have happened
[32m[20221213 20:14:03 @agent_ppo2.py:121][0m #------------------------ Iteration 84 --------------------------#
[32m[20221213 20:14:04 @agent_ppo2.py:127][0m Sampling time: 0.56 s by 10 slaves
[32m[20221213 20:14:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:04 @agent_ppo2.py:185][0m |          -0.0001 |           0.0226 |           0.0000 |
[32m[20221213 20:14:04 @agent_ppo2.py:185][0m |          -0.0134 |           0.0151 |           0.0000 |
[32m[20221213 20:14:05 @agent_ppo2.py:185][0m |          -0.0159 |           0.0142 |           0.0000 |
[32m[20221213 20:14:05 @agent_ppo2.py:185][0m |          -0.0187 |           0.0139 |           0.0000 |
[32m[20221213 20:14:05 @agent_ppo2.py:185][0m |          -0.0195 |           0.0135 |           0.0000 |
[32m[20221213 20:14:06 @agent_ppo2.py:185][0m |          -0.0199 |           0.0132 |           0.0000 |
[32m[20221213 20:14:06 @agent_ppo2.py:185][0m |          -0.0226 |           0.0127 |           0.0000 |
[32m[20221213 20:14:06 @agent_ppo2.py:185][0m |          -0.0233 |           0.0129 |           0.0000 |
[32m[20221213 20:14:06 @agent_ppo2.py:185][0m |          -0.0220 |           0.0127 |           0.0000 |
[32m[20221213 20:14:07 @agent_ppo2.py:185][0m |          -0.0242 |           0.0123 |           0.0000 |
[32m[20221213 20:14:07 @agent_ppo2.py:130][0m Policy update time: 2.79 s
[32m[20221213 20:14:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.59
[32m[20221213 20:14:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.03
[32m[20221213 20:14:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.96
[32m[20221213 20:14:07 @agent_ppo2.py:143][0m Total time:       5.68 min
[32m[20221213 20:14:07 @agent_ppo2.py:145][0m 174080 total steps have happened
[32m[20221213 20:14:07 @agent_ppo2.py:121][0m #------------------------ Iteration 85 --------------------------#
[32m[20221213 20:14:08 @agent_ppo2.py:127][0m Sampling time: 0.46 s by 10 slaves
[32m[20221213 20:14:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:08 @agent_ppo2.py:185][0m |          -0.0014 |           0.0111 |           0.0000 |
[32m[20221213 20:14:08 @agent_ppo2.py:185][0m |          -0.0160 |           0.0107 |           0.0000 |
[32m[20221213 20:14:08 @agent_ppo2.py:185][0m |          -0.0217 |           0.0105 |           0.0000 |
[32m[20221213 20:14:09 @agent_ppo2.py:185][0m |          -0.0240 |           0.0103 |           0.0000 |
[32m[20221213 20:14:09 @agent_ppo2.py:185][0m |          -0.0250 |           0.0101 |           0.0000 |
[32m[20221213 20:14:09 @agent_ppo2.py:185][0m |          -0.0302 |           0.0100 |           0.0000 |
[32m[20221213 20:14:10 @agent_ppo2.py:185][0m |          -0.0302 |           0.0099 |           0.0000 |
[32m[20221213 20:14:10 @agent_ppo2.py:185][0m |          -0.0321 |           0.0099 |           0.0000 |
[32m[20221213 20:14:10 @agent_ppo2.py:185][0m |          -0.0343 |           0.0097 |           0.0000 |
[32m[20221213 20:14:10 @agent_ppo2.py:185][0m |          -0.0332 |           0.0097 |           0.0000 |
[32m[20221213 20:14:10 @agent_ppo2.py:130][0m Policy update time: 2.83 s
[32m[20221213 20:14:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.26
[32m[20221213 20:14:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.16
[32m[20221213 20:14:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.23
[32m[20221213 20:14:11 @agent_ppo2.py:143][0m Total time:       5.74 min
[32m[20221213 20:14:11 @agent_ppo2.py:145][0m 176128 total steps have happened
[32m[20221213 20:14:11 @agent_ppo2.py:121][0m #------------------------ Iteration 86 --------------------------#
[32m[20221213 20:14:11 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:14:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:12 @agent_ppo2.py:185][0m |           0.0039 |           0.0126 |           0.0000 |
[32m[20221213 20:14:12 @agent_ppo2.py:185][0m |          -0.0078 |           0.0116 |           0.0000 |
[32m[20221213 20:14:12 @agent_ppo2.py:185][0m |          -0.0125 |           0.0111 |           0.0000 |
[32m[20221213 20:14:13 @agent_ppo2.py:185][0m |          -0.0216 |           0.0113 |           0.0000 |
[32m[20221213 20:14:13 @agent_ppo2.py:185][0m |          -0.0172 |           0.0112 |           0.0000 |
[32m[20221213 20:14:13 @agent_ppo2.py:185][0m |          -0.0242 |           0.0110 |           0.0000 |
[32m[20221213 20:14:13 @agent_ppo2.py:185][0m |          -0.0178 |           0.0107 |           0.0000 |
[32m[20221213 20:14:14 @agent_ppo2.py:185][0m |          -0.0198 |           0.0107 |           0.0000 |
[32m[20221213 20:14:14 @agent_ppo2.py:185][0m |          -0.0200 |           0.0103 |           0.0000 |
[32m[20221213 20:14:14 @agent_ppo2.py:185][0m |          -0.0205 |           0.0103 |           0.0000 |
[32m[20221213 20:14:14 @agent_ppo2.py:130][0m Policy update time: 2.73 s
[32m[20221213 20:14:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.84
[32m[20221213 20:14:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.55
[32m[20221213 20:14:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.10
[32m[20221213 20:14:15 @agent_ppo2.py:143][0m Total time:       5.81 min
[32m[20221213 20:14:15 @agent_ppo2.py:145][0m 178176 total steps have happened
[32m[20221213 20:14:15 @agent_ppo2.py:121][0m #------------------------ Iteration 87 --------------------------#
[32m[20221213 20:14:15 @agent_ppo2.py:127][0m Sampling time: 0.55 s by 10 slaves
[32m[20221213 20:14:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:16 @agent_ppo2.py:185][0m |           0.0012 |           0.0178 |           0.0000 |
[32m[20221213 20:14:16 @agent_ppo2.py:185][0m |          -0.0101 |           0.0140 |           0.0000 |
[32m[20221213 20:14:16 @agent_ppo2.py:185][0m |          -0.0136 |           0.0134 |           0.0000 |
[32m[20221213 20:14:16 @agent_ppo2.py:185][0m |          -0.0156 |           0.0128 |           0.0000 |
[32m[20221213 20:14:17 @agent_ppo2.py:185][0m |          -0.0187 |           0.0126 |           0.0000 |
[32m[20221213 20:14:17 @agent_ppo2.py:185][0m |          -0.0183 |           0.0127 |           0.0000 |
[32m[20221213 20:14:17 @agent_ppo2.py:185][0m |          -0.0190 |           0.0120 |           0.0000 |
[32m[20221213 20:14:17 @agent_ppo2.py:185][0m |          -0.0188 |           0.0121 |           0.0000 |
[32m[20221213 20:14:18 @agent_ppo2.py:185][0m |          -0.0201 |           0.0122 |           0.0000 |
[32m[20221213 20:14:18 @agent_ppo2.py:185][0m |          -0.0196 |           0.0119 |           0.0000 |
[32m[20221213 20:14:18 @agent_ppo2.py:130][0m Policy update time: 2.81 s
[32m[20221213 20:14:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.79
[32m[20221213 20:14:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.87
[32m[20221213 20:14:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.97
[32m[20221213 20:14:19 @agent_ppo2.py:143][0m Total time:       5.87 min
[32m[20221213 20:14:19 @agent_ppo2.py:145][0m 180224 total steps have happened
[32m[20221213 20:14:19 @agent_ppo2.py:121][0m #------------------------ Iteration 88 --------------------------#
[32m[20221213 20:14:19 @agent_ppo2.py:127][0m Sampling time: 0.46 s by 10 slaves
[32m[20221213 20:14:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:19 @agent_ppo2.py:185][0m |           0.0022 |           0.0108 |           0.0000 |
[32m[20221213 20:14:20 @agent_ppo2.py:185][0m |          -0.0117 |           0.0097 |           0.0000 |
[32m[20221213 20:14:20 @agent_ppo2.py:185][0m |          -0.0166 |           0.0094 |           0.0000 |
[32m[20221213 20:14:20 @agent_ppo2.py:185][0m |          -0.0195 |           0.0091 |           0.0000 |
[32m[20221213 20:14:21 @agent_ppo2.py:185][0m |          -0.0211 |           0.0090 |           0.0000 |
[32m[20221213 20:14:21 @agent_ppo2.py:185][0m |          -0.0209 |           0.0088 |           0.0000 |
[32m[20221213 20:14:21 @agent_ppo2.py:185][0m |          -0.0228 |           0.0087 |           0.0000 |
[32m[20221213 20:14:21 @agent_ppo2.py:185][0m |          -0.0239 |           0.0085 |           0.0000 |
[32m[20221213 20:14:22 @agent_ppo2.py:185][0m |          -0.0249 |           0.0085 |           0.0000 |
[32m[20221213 20:14:22 @agent_ppo2.py:185][0m |          -0.0257 |           0.0083 |           0.0000 |
[32m[20221213 20:14:22 @agent_ppo2.py:130][0m Policy update time: 2.79 s
[32m[20221213 20:14:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.19
[32m[20221213 20:14:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.96
[32m[20221213 20:14:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.69
[32m[20221213 20:14:22 @agent_ppo2.py:143][0m Total time:       5.93 min
[32m[20221213 20:14:22 @agent_ppo2.py:145][0m 182272 total steps have happened
[32m[20221213 20:14:22 @agent_ppo2.py:121][0m #------------------------ Iteration 89 --------------------------#
[32m[20221213 20:14:23 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:14:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:23 @agent_ppo2.py:185][0m |           0.0000 |           0.0101 |           0.0000 |
[32m[20221213 20:14:23 @agent_ppo2.py:185][0m |          -0.0118 |           0.0094 |           0.0000 |
[32m[20221213 20:14:24 @agent_ppo2.py:185][0m |          -0.0162 |           0.0091 |           0.0000 |
[32m[20221213 20:14:24 @agent_ppo2.py:185][0m |          -0.0183 |           0.0086 |           0.0000 |
[32m[20221213 20:14:24 @agent_ppo2.py:185][0m |          -0.0184 |           0.0086 |           0.0000 |
[32m[20221213 20:14:24 @agent_ppo2.py:185][0m |          -0.0184 |           0.0083 |           0.0000 |
[32m[20221213 20:14:25 @agent_ppo2.py:185][0m |          -0.0217 |           0.0082 |           0.0000 |
[32m[20221213 20:14:25 @agent_ppo2.py:185][0m |          -0.0223 |           0.0082 |           0.0000 |
[32m[20221213 20:14:25 @agent_ppo2.py:185][0m |          -0.0225 |           0.0082 |           0.0000 |
[32m[20221213 20:14:25 @agent_ppo2.py:185][0m |          -0.0227 |           0.0079 |           0.0000 |
[32m[20221213 20:14:25 @agent_ppo2.py:130][0m Policy update time: 2.73 s
[32m[20221213 20:14:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.90
[32m[20221213 20:14:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.81
[32m[20221213 20:14:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.38
[32m[20221213 20:14:26 @agent_ppo2.py:143][0m Total time:       5.99 min
[32m[20221213 20:14:26 @agent_ppo2.py:145][0m 184320 total steps have happened
[32m[20221213 20:14:26 @agent_ppo2.py:121][0m #------------------------ Iteration 90 --------------------------#
[32m[20221213 20:14:27 @agent_ppo2.py:127][0m Sampling time: 0.57 s by 10 slaves
[32m[20221213 20:14:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:27 @agent_ppo2.py:185][0m |           0.0015 |           0.0074 |           0.0000 |
[32m[20221213 20:14:27 @agent_ppo2.py:185][0m |          -0.0138 |           0.0070 |           0.0000 |
[32m[20221213 20:14:27 @agent_ppo2.py:185][0m |          -0.0142 |           0.0068 |           0.0000 |
[32m[20221213 20:14:28 @agent_ppo2.py:185][0m |          -0.0157 |           0.0067 |           0.0000 |
[32m[20221213 20:14:28 @agent_ppo2.py:185][0m |          -0.0178 |           0.0065 |           0.0000 |
[32m[20221213 20:14:28 @agent_ppo2.py:185][0m |          -0.0188 |           0.0063 |           0.0000 |
[32m[20221213 20:14:28 @agent_ppo2.py:185][0m |          -0.0192 |           0.0063 |           0.0000 |
[32m[20221213 20:14:29 @agent_ppo2.py:185][0m |          -0.0198 |           0.0063 |           0.0000 |
[32m[20221213 20:14:29 @agent_ppo2.py:185][0m |          -0.0207 |           0.0062 |           0.0000 |
[32m[20221213 20:14:29 @agent_ppo2.py:185][0m |          -0.0213 |           0.0062 |           0.0000 |
[32m[20221213 20:14:29 @agent_ppo2.py:130][0m Policy update time: 2.77 s
[32m[20221213 20:14:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.49
[32m[20221213 20:14:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.39
[32m[20221213 20:14:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.44
[32m[20221213 20:14:30 @agent_ppo2.py:143][0m Total time:       6.06 min
[32m[20221213 20:14:30 @agent_ppo2.py:145][0m 186368 total steps have happened
[32m[20221213 20:14:30 @agent_ppo2.py:121][0m #------------------------ Iteration 91 --------------------------#
[32m[20221213 20:14:30 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:14:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:31 @agent_ppo2.py:185][0m |           0.0019 |           0.0271 |           0.0000 |
[32m[20221213 20:14:31 @agent_ppo2.py:185][0m |          -0.0152 |           0.0180 |           0.0000 |
[32m[20221213 20:14:31 @agent_ppo2.py:185][0m |          -0.0136 |           0.0160 |           0.0000 |
[32m[20221213 20:14:31 @agent_ppo2.py:185][0m |          -0.0145 |           0.0149 |           0.0000 |
[32m[20221213 20:14:32 @agent_ppo2.py:185][0m |          -0.0166 |           0.0146 |           0.0000 |
[32m[20221213 20:14:32 @agent_ppo2.py:185][0m |          -0.0175 |           0.0141 |           0.0000 |
[32m[20221213 20:14:32 @agent_ppo2.py:185][0m |          -0.0179 |           0.0140 |           0.0000 |
[32m[20221213 20:14:32 @agent_ppo2.py:185][0m |          -0.0179 |           0.0136 |           0.0000 |
[32m[20221213 20:14:33 @agent_ppo2.py:185][0m |          -0.0182 |           0.0137 |           0.0000 |
[32m[20221213 20:14:33 @agent_ppo2.py:185][0m |          -0.0187 |           0.0138 |           0.0000 |
[32m[20221213 20:14:33 @agent_ppo2.py:130][0m Policy update time: 2.79 s
[32m[20221213 20:14:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.69
[32m[20221213 20:14:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.54
[32m[20221213 20:14:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.76
[32m[20221213 20:14:34 @agent_ppo2.py:143][0m Total time:       6.12 min
[32m[20221213 20:14:34 @agent_ppo2.py:145][0m 188416 total steps have happened
[32m[20221213 20:14:34 @agent_ppo2.py:121][0m #------------------------ Iteration 92 --------------------------#
[32m[20221213 20:14:34 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:14:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:34 @agent_ppo2.py:185][0m |          -0.0015 |           0.0192 |           0.0000 |
[32m[20221213 20:14:35 @agent_ppo2.py:185][0m |          -0.0126 |           0.0161 |           0.0000 |
[32m[20221213 20:14:35 @agent_ppo2.py:185][0m |          -0.0171 |           0.0149 |           0.0000 |
[32m[20221213 20:14:35 @agent_ppo2.py:185][0m |          -0.0193 |           0.0141 |           0.0000 |
[32m[20221213 20:14:35 @agent_ppo2.py:185][0m |          -0.0219 |           0.0139 |           0.0000 |
[32m[20221213 20:14:36 @agent_ppo2.py:185][0m |          -0.0230 |           0.0137 |           0.0000 |
[32m[20221213 20:14:36 @agent_ppo2.py:185][0m |          -0.0250 |           0.0135 |           0.0000 |
[32m[20221213 20:14:36 @agent_ppo2.py:185][0m |          -0.0245 |           0.0131 |           0.0000 |
[32m[20221213 20:14:36 @agent_ppo2.py:185][0m |          -0.0293 |           0.0135 |           0.0000 |
[32m[20221213 20:14:37 @agent_ppo2.py:185][0m |          -0.0262 |           0.0132 |           0.0000 |
[32m[20221213 20:14:37 @agent_ppo2.py:130][0m Policy update time: 2.75 s
[32m[20221213 20:14:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.98
[32m[20221213 20:14:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.17
[32m[20221213 20:14:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.09
[32m[20221213 20:14:37 @agent_ppo2.py:143][0m Total time:       6.18 min
[32m[20221213 20:14:37 @agent_ppo2.py:145][0m 190464 total steps have happened
[32m[20221213 20:14:37 @agent_ppo2.py:121][0m #------------------------ Iteration 93 --------------------------#
[32m[20221213 20:14:38 @agent_ppo2.py:127][0m Sampling time: 0.55 s by 10 slaves
[32m[20221213 20:14:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:38 @agent_ppo2.py:185][0m |           0.0021 |           0.0104 |           0.0000 |
[32m[20221213 20:14:39 @agent_ppo2.py:185][0m |          -0.0082 |           0.0064 |           0.0000 |
[32m[20221213 20:14:39 @agent_ppo2.py:185][0m |          -0.0139 |           0.0061 |           0.0000 |
[32m[20221213 20:14:39 @agent_ppo2.py:185][0m |          -0.0160 |           0.0059 |           0.0000 |
[32m[20221213 20:14:39 @agent_ppo2.py:185][0m |          -0.0180 |           0.0057 |           0.0000 |
[32m[20221213 20:14:40 @agent_ppo2.py:185][0m |          -0.0188 |           0.0055 |           0.0000 |
[32m[20221213 20:14:40 @agent_ppo2.py:185][0m |          -0.0193 |           0.0053 |           0.0000 |
[32m[20221213 20:14:40 @agent_ppo2.py:185][0m |          -0.0204 |           0.0051 |           0.0000 |
[32m[20221213 20:14:41 @agent_ppo2.py:185][0m |          -0.0205 |           0.0049 |           0.0000 |
[32m[20221213 20:14:41 @agent_ppo2.py:185][0m |          -0.0212 |           0.0048 |           0.0000 |
[32m[20221213 20:14:41 @agent_ppo2.py:130][0m Policy update time: 3.15 s
[32m[20221213 20:14:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.65
[32m[20221213 20:14:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.21
[32m[20221213 20:14:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.75
[32m[20221213 20:14:42 @agent_ppo2.py:143][0m Total time:       6.26 min
[32m[20221213 20:14:42 @agent_ppo2.py:145][0m 192512 total steps have happened
[32m[20221213 20:14:42 @agent_ppo2.py:121][0m #------------------------ Iteration 94 --------------------------#
[32m[20221213 20:14:42 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:14:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:43 @agent_ppo2.py:185][0m |           0.0016 |           0.0597 |           0.0000 |
[32m[20221213 20:14:43 @agent_ppo2.py:185][0m |          -0.0095 |           0.0230 |           0.0000 |
[32m[20221213 20:14:43 @agent_ppo2.py:185][0m |          -0.0127 |           0.0193 |           0.0000 |
[32m[20221213 20:14:43 @agent_ppo2.py:185][0m |          -0.0154 |           0.0179 |           0.0000 |
[32m[20221213 20:14:44 @agent_ppo2.py:185][0m |          -0.0158 |           0.0176 |           0.0000 |
[32m[20221213 20:14:44 @agent_ppo2.py:185][0m |          -0.0211 |           0.0168 |           0.0000 |
[32m[20221213 20:14:44 @agent_ppo2.py:185][0m |          -0.0170 |           0.0164 |           0.0000 |
[32m[20221213 20:14:45 @agent_ppo2.py:185][0m |          -0.0181 |           0.0158 |           0.0000 |
[32m[20221213 20:14:45 @agent_ppo2.py:185][0m |          -0.0188 |           0.0155 |           0.0000 |
[32m[20221213 20:14:45 @agent_ppo2.py:185][0m |          -0.0194 |           0.0155 |           0.0000 |
[32m[20221213 20:14:45 @agent_ppo2.py:130][0m Policy update time: 2.98 s
[32m[20221213 20:14:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.21
[32m[20221213 20:14:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.42
[32m[20221213 20:14:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.65
[32m[20221213 20:14:46 @agent_ppo2.py:143][0m Total time:       6.32 min
[32m[20221213 20:14:46 @agent_ppo2.py:145][0m 194560 total steps have happened
[32m[20221213 20:14:46 @agent_ppo2.py:121][0m #------------------------ Iteration 95 --------------------------#
[32m[20221213 20:14:46 @agent_ppo2.py:127][0m Sampling time: 0.53 s by 10 slaves
[32m[20221213 20:14:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:47 @agent_ppo2.py:185][0m |          -0.0001 |           0.0127 |           0.0000 |
[32m[20221213 20:14:47 @agent_ppo2.py:185][0m |          -0.0124 |           0.0102 |           0.0000 |
[32m[20221213 20:14:47 @agent_ppo2.py:185][0m |          -0.0183 |           0.0098 |           0.0000 |
[32m[20221213 20:14:47 @agent_ppo2.py:185][0m |          -0.0187 |           0.0096 |           0.0000 |
[32m[20221213 20:14:48 @agent_ppo2.py:185][0m |          -0.0214 |           0.0093 |           0.0000 |
[32m[20221213 20:14:48 @agent_ppo2.py:185][0m |          -0.0234 |           0.0091 |           0.0000 |
[32m[20221213 20:14:48 @agent_ppo2.py:185][0m |          -0.0243 |           0.0090 |           0.0000 |
[32m[20221213 20:14:48 @agent_ppo2.py:185][0m |          -0.0264 |           0.0089 |           0.0000 |
[32m[20221213 20:14:49 @agent_ppo2.py:185][0m |          -0.0273 |           0.0087 |           0.0000 |
[32m[20221213 20:14:49 @agent_ppo2.py:185][0m |          -0.0250 |           0.0087 |           0.0000 |
[32m[20221213 20:14:49 @agent_ppo2.py:130][0m Policy update time: 2.88 s
[32m[20221213 20:14:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.65
[32m[20221213 20:14:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.30
[32m[20221213 20:14:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.34
[32m[20221213 20:14:50 @agent_ppo2.py:143][0m Total time:       6.39 min
[32m[20221213 20:14:50 @agent_ppo2.py:145][0m 196608 total steps have happened
[32m[20221213 20:14:50 @agent_ppo2.py:121][0m #------------------------ Iteration 96 --------------------------#
[32m[20221213 20:14:50 @agent_ppo2.py:127][0m Sampling time: 0.46 s by 10 slaves
[32m[20221213 20:14:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:50 @agent_ppo2.py:185][0m |           0.0029 |           0.0390 |           0.0000 |
[32m[20221213 20:14:51 @agent_ppo2.py:185][0m |          -0.0087 |           0.0217 |           0.0000 |
[32m[20221213 20:14:51 @agent_ppo2.py:185][0m |          -0.0131 |           0.0202 |           0.0000 |
[32m[20221213 20:14:51 @agent_ppo2.py:185][0m |          -0.0153 |           0.0190 |           0.0000 |
[32m[20221213 20:14:51 @agent_ppo2.py:185][0m |          -0.0214 |           0.0184 |           0.0000 |
[32m[20221213 20:14:52 @agent_ppo2.py:185][0m |          -0.0171 |           0.0180 |           0.0000 |
[32m[20221213 20:14:52 @agent_ppo2.py:185][0m |          -0.0236 |           0.0172 |           0.0000 |
[32m[20221213 20:14:52 @agent_ppo2.py:185][0m |          -0.0177 |           0.0173 |           0.0000 |
[32m[20221213 20:14:53 @agent_ppo2.py:185][0m |          -0.0186 |           0.0168 |           0.0000 |
[32m[20221213 20:14:53 @agent_ppo2.py:185][0m |          -0.0186 |           0.0166 |           0.0000 |
[32m[20221213 20:14:53 @agent_ppo2.py:130][0m Policy update time: 2.86 s
[32m[20221213 20:14:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.65
[32m[20221213 20:14:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.76
[32m[20221213 20:14:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.07
[32m[20221213 20:14:54 @agent_ppo2.py:143][0m Total time:       6.45 min
[32m[20221213 20:14:54 @agent_ppo2.py:145][0m 198656 total steps have happened
[32m[20221213 20:14:54 @agent_ppo2.py:121][0m #------------------------ Iteration 97 --------------------------#
[32m[20221213 20:14:54 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:14:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:55 @agent_ppo2.py:185][0m |           0.0011 |           0.0170 |           0.0000 |
[32m[20221213 20:14:55 @agent_ppo2.py:185][0m |          -0.0114 |           0.0082 |           0.0000 |
[32m[20221213 20:14:55 @agent_ppo2.py:185][0m |          -0.0159 |           0.0079 |           0.0000 |
[32m[20221213 20:14:55 @agent_ppo2.py:185][0m |          -0.0191 |           0.0077 |           0.0000 |
[32m[20221213 20:14:56 @agent_ppo2.py:185][0m |          -0.0182 |           0.0076 |           0.0000 |
[32m[20221213 20:14:56 @agent_ppo2.py:185][0m |          -0.0208 |           0.0074 |           0.0000 |
[32m[20221213 20:14:56 @agent_ppo2.py:185][0m |          -0.0213 |           0.0072 |           0.0000 |
[32m[20221213 20:14:57 @agent_ppo2.py:185][0m |          -0.0214 |           0.0070 |           0.0000 |
[32m[20221213 20:14:57 @agent_ppo2.py:185][0m |          -0.0230 |           0.0068 |           0.0000 |
[32m[20221213 20:14:57 @agent_ppo2.py:185][0m |          -0.0226 |           0.0067 |           0.0000 |
[32m[20221213 20:14:57 @agent_ppo2.py:130][0m Policy update time: 3.13 s
[32m[20221213 20:14:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.55
[32m[20221213 20:14:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.85
[32m[20221213 20:14:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.94
[32m[20221213 20:14:58 @agent_ppo2.py:143][0m Total time:       6.52 min
[32m[20221213 20:14:58 @agent_ppo2.py:145][0m 200704 total steps have happened
[32m[20221213 20:14:58 @agent_ppo2.py:121][0m #------------------------ Iteration 98 --------------------------#
[32m[20221213 20:14:58 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:14:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:14:59 @agent_ppo2.py:185][0m |           0.0033 |           0.0124 |           0.0000 |
[32m[20221213 20:14:59 @agent_ppo2.py:185][0m |          -0.0102 |           0.0101 |           0.0000 |
[32m[20221213 20:14:59 @agent_ppo2.py:185][0m |          -0.0119 |           0.0098 |           0.0000 |
[32m[20221213 20:15:00 @agent_ppo2.py:185][0m |          -0.0140 |           0.0097 |           0.0000 |
[32m[20221213 20:15:00 @agent_ppo2.py:185][0m |          -0.0141 |           0.0093 |           0.0000 |
[32m[20221213 20:15:00 @agent_ppo2.py:185][0m |          -0.0147 |           0.0092 |           0.0000 |
[32m[20221213 20:15:00 @agent_ppo2.py:185][0m |          -0.0156 |           0.0090 |           0.0000 |
[32m[20221213 20:15:01 @agent_ppo2.py:185][0m |          -0.0164 |           0.0090 |           0.0000 |
[32m[20221213 20:15:01 @agent_ppo2.py:185][0m |          -0.0169 |           0.0089 |           0.0000 |
[32m[20221213 20:15:01 @agent_ppo2.py:185][0m |          -0.0164 |           0.0086 |           0.0000 |
[32m[20221213 20:15:01 @agent_ppo2.py:130][0m Policy update time: 3.07 s
[32m[20221213 20:15:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.78
[32m[20221213 20:15:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.18
[32m[20221213 20:15:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.84
[32m[20221213 20:15:02 @agent_ppo2.py:143][0m Total time:       6.59 min
[32m[20221213 20:15:02 @agent_ppo2.py:145][0m 202752 total steps have happened
[32m[20221213 20:15:02 @agent_ppo2.py:121][0m #------------------------ Iteration 99 --------------------------#
[32m[20221213 20:15:02 @agent_ppo2.py:127][0m Sampling time: 0.58 s by 10 slaves
[32m[20221213 20:15:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:03 @agent_ppo2.py:185][0m |           0.0032 |           0.0105 |           0.0000 |
[32m[20221213 20:15:03 @agent_ppo2.py:185][0m |          -0.0134 |           0.0100 |           0.0000 |
[32m[20221213 20:15:03 @agent_ppo2.py:185][0m |          -0.0194 |           0.0096 |           0.0000 |
[32m[20221213 20:15:04 @agent_ppo2.py:185][0m |          -0.0201 |           0.0094 |           0.0000 |
[32m[20221213 20:15:04 @agent_ppo2.py:185][0m |          -0.0242 |           0.0093 |           0.0000 |
[32m[20221213 20:15:04 @agent_ppo2.py:185][0m |          -0.0250 |           0.0092 |           0.0000 |
[32m[20221213 20:15:04 @agent_ppo2.py:185][0m |          -0.0259 |           0.0090 |           0.0000 |
[32m[20221213 20:15:05 @agent_ppo2.py:185][0m |          -0.0227 |           0.0091 |           0.0000 |
[32m[20221213 20:15:05 @agent_ppo2.py:185][0m |          -0.0266 |           0.0089 |           0.0000 |
[32m[20221213 20:15:05 @agent_ppo2.py:185][0m |          -0.0297 |           0.0088 |           0.0000 |
[32m[20221213 20:15:05 @agent_ppo2.py:130][0m Policy update time: 2.83 s
[32m[20221213 20:15:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.07
[32m[20221213 20:15:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.28
[32m[20221213 20:15:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.66
[32m[20221213 20:15:06 @agent_ppo2.py:143][0m Total time:       6.66 min
[32m[20221213 20:15:06 @agent_ppo2.py:145][0m 204800 total steps have happened
[32m[20221213 20:15:06 @agent_ppo2.py:121][0m #------------------------ Iteration 100 --------------------------#
[32m[20221213 20:15:06 @agent_ppo2.py:127][0m Sampling time: 0.62 s by 10 slaves
[32m[20221213 20:15:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:07 @agent_ppo2.py:185][0m |           0.0011 |           0.0228 |           0.0000 |
[32m[20221213 20:15:07 @agent_ppo2.py:185][0m |          -0.0085 |           0.0173 |           0.0000 |
[32m[20221213 20:15:07 @agent_ppo2.py:185][0m |          -0.0134 |           0.0158 |           0.0000 |
[32m[20221213 20:15:08 @agent_ppo2.py:185][0m |          -0.0145 |           0.0150 |           0.0000 |
[32m[20221213 20:15:08 @agent_ppo2.py:185][0m |          -0.0161 |           0.0148 |           0.0000 |
[32m[20221213 20:15:08 @agent_ppo2.py:185][0m |          -0.0175 |           0.0144 |           0.0000 |
[32m[20221213 20:15:08 @agent_ppo2.py:185][0m |          -0.0178 |           0.0141 |           0.0000 |
[32m[20221213 20:15:09 @agent_ppo2.py:185][0m |          -0.0188 |           0.0139 |           0.0000 |
[32m[20221213 20:15:09 @agent_ppo2.py:185][0m |          -0.0182 |           0.0140 |           0.0000 |
[32m[20221213 20:15:09 @agent_ppo2.py:185][0m |          -0.0189 |           0.0139 |           0.0000 |
[32m[20221213 20:15:09 @agent_ppo2.py:130][0m Policy update time: 2.80 s
[32m[20221213 20:15:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.77
[32m[20221213 20:15:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.18
[32m[20221213 20:15:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.25
[32m[20221213 20:15:10 @agent_ppo2.py:143][0m Total time:       6.72 min
[32m[20221213 20:15:10 @agent_ppo2.py:145][0m 206848 total steps have happened
[32m[20221213 20:15:10 @agent_ppo2.py:121][0m #------------------------ Iteration 101 --------------------------#
[32m[20221213 20:15:10 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:15:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:11 @agent_ppo2.py:185][0m |           0.0013 |           0.0244 |           0.0000 |
[32m[20221213 20:15:11 @agent_ppo2.py:185][0m |          -0.0081 |           0.0183 |           0.0000 |
[32m[20221213 20:15:11 @agent_ppo2.py:185][0m |          -0.0128 |           0.0172 |           0.0000 |
[32m[20221213 20:15:11 @agent_ppo2.py:185][0m |          -0.0149 |           0.0168 |           0.0000 |
[32m[20221213 20:15:12 @agent_ppo2.py:185][0m |          -0.0161 |           0.0159 |           0.0000 |
[32m[20221213 20:15:12 @agent_ppo2.py:185][0m |          -0.0176 |           0.0156 |           0.0000 |
[32m[20221213 20:15:12 @agent_ppo2.py:185][0m |          -0.0213 |           0.0160 |           0.0000 |
[32m[20221213 20:15:12 @agent_ppo2.py:185][0m |          -0.0192 |           0.0153 |           0.0000 |
[32m[20221213 20:15:13 @agent_ppo2.py:185][0m |          -0.0254 |           0.0151 |           0.0000 |
[32m[20221213 20:15:13 @agent_ppo2.py:185][0m |          -0.0194 |           0.0153 |           0.0000 |
[32m[20221213 20:15:13 @agent_ppo2.py:130][0m Policy update time: 2.81 s
[32m[20221213 20:15:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.92
[32m[20221213 20:15:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.96
[32m[20221213 20:15:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.00
[32m[20221213 20:15:13 @agent_ppo2.py:143][0m Total time:       6.79 min
[32m[20221213 20:15:13 @agent_ppo2.py:145][0m 208896 total steps have happened
[32m[20221213 20:15:13 @agent_ppo2.py:121][0m #------------------------ Iteration 102 --------------------------#
[32m[20221213 20:15:14 @agent_ppo2.py:127][0m Sampling time: 0.46 s by 10 slaves
[32m[20221213 20:15:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:14 @agent_ppo2.py:185][0m |           0.0017 |           0.0176 |           0.0000 |
[32m[20221213 20:15:15 @agent_ppo2.py:185][0m |          -0.0113 |           0.0148 |           0.0000 |
[32m[20221213 20:15:15 @agent_ppo2.py:185][0m |          -0.0161 |           0.0139 |           0.0000 |
[32m[20221213 20:15:15 @agent_ppo2.py:185][0m |          -0.0184 |           0.0138 |           0.0000 |
[32m[20221213 20:15:15 @agent_ppo2.py:185][0m |          -0.0196 |           0.0136 |           0.0000 |
[32m[20221213 20:15:16 @agent_ppo2.py:185][0m |          -0.0215 |           0.0132 |           0.0000 |
[32m[20221213 20:15:16 @agent_ppo2.py:185][0m |          -0.0225 |           0.0130 |           0.0000 |
[32m[20221213 20:15:16 @agent_ppo2.py:185][0m |          -0.0236 |           0.0131 |           0.0000 |
[32m[20221213 20:15:16 @agent_ppo2.py:185][0m |          -0.0253 |           0.0129 |           0.0000 |
[32m[20221213 20:15:17 @agent_ppo2.py:185][0m |          -0.0250 |           0.0127 |           0.0000 |
[32m[20221213 20:15:17 @agent_ppo2.py:130][0m Policy update time: 2.85 s
[32m[20221213 20:15:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.19
[32m[20221213 20:15:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.00
[32m[20221213 20:15:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.14
[32m[20221213 20:15:17 @agent_ppo2.py:143][0m Total time:       6.85 min
[32m[20221213 20:15:17 @agent_ppo2.py:145][0m 210944 total steps have happened
[32m[20221213 20:15:17 @agent_ppo2.py:121][0m #------------------------ Iteration 103 --------------------------#
[32m[20221213 20:15:18 @agent_ppo2.py:127][0m Sampling time: 0.56 s by 10 slaves
[32m[20221213 20:15:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:18 @agent_ppo2.py:185][0m |           0.0027 |           0.0127 |           0.0000 |
[32m[20221213 20:15:18 @agent_ppo2.py:185][0m |          -0.0117 |           0.0118 |           0.0000 |
[32m[20221213 20:15:19 @agent_ppo2.py:185][0m |          -0.0169 |           0.0112 |           0.0000 |
[32m[20221213 20:15:19 @agent_ppo2.py:185][0m |          -0.0192 |           0.0111 |           0.0000 |
[32m[20221213 20:15:19 @agent_ppo2.py:185][0m |          -0.0227 |           0.0110 |           0.0000 |
[32m[20221213 20:15:20 @agent_ppo2.py:185][0m |          -0.0244 |           0.0107 |           0.0000 |
[32m[20221213 20:15:20 @agent_ppo2.py:185][0m |          -0.0259 |           0.0109 |           0.0000 |
[32m[20221213 20:15:20 @agent_ppo2.py:185][0m |          -0.0282 |           0.0108 |           0.0000 |
[32m[20221213 20:15:20 @agent_ppo2.py:185][0m |          -0.0296 |           0.0107 |           0.0000 |
[32m[20221213 20:15:21 @agent_ppo2.py:185][0m |          -0.0310 |           0.0105 |           0.0000 |
[32m[20221213 20:15:21 @agent_ppo2.py:130][0m Policy update time: 2.75 s
[32m[20221213 20:15:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.46
[32m[20221213 20:15:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.43
[32m[20221213 20:15:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.55
[32m[20221213 20:15:21 @agent_ppo2.py:143][0m Total time:       6.91 min
[32m[20221213 20:15:21 @agent_ppo2.py:145][0m 212992 total steps have happened
[32m[20221213 20:15:21 @agent_ppo2.py:121][0m #------------------------ Iteration 104 --------------------------#
[32m[20221213 20:15:22 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:15:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:22 @agent_ppo2.py:185][0m |          -0.0003 |           0.0099 |           0.0000 |
[32m[20221213 20:15:22 @agent_ppo2.py:185][0m |          -0.0128 |           0.0067 |           0.0000 |
[32m[20221213 20:15:23 @agent_ppo2.py:185][0m |          -0.0178 |           0.0064 |           0.0000 |
[32m[20221213 20:15:23 @agent_ppo2.py:185][0m |          -0.0193 |           0.0061 |           0.0000 |
[32m[20221213 20:15:23 @agent_ppo2.py:185][0m |          -0.0204 |           0.0058 |           0.0000 |
[32m[20221213 20:15:23 @agent_ppo2.py:185][0m |          -0.0219 |           0.0056 |           0.0000 |
[32m[20221213 20:15:24 @agent_ppo2.py:185][0m |          -0.0174 |           0.0054 |           0.0000 |
[32m[20221213 20:15:24 @agent_ppo2.py:185][0m |          -0.0226 |           0.0052 |           0.0000 |
[32m[20221213 20:15:24 @agent_ppo2.py:185][0m |          -0.0234 |           0.0050 |           0.0000 |
[32m[20221213 20:15:24 @agent_ppo2.py:185][0m |          -0.0238 |           0.0049 |           0.0000 |
[32m[20221213 20:15:24 @agent_ppo2.py:130][0m Policy update time: 2.86 s
[32m[20221213 20:15:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.29
[32m[20221213 20:15:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.64
[32m[20221213 20:15:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.20
[32m[20221213 20:15:25 @agent_ppo2.py:143][0m Total time:       6.98 min
[32m[20221213 20:15:25 @agent_ppo2.py:145][0m 215040 total steps have happened
[32m[20221213 20:15:25 @agent_ppo2.py:121][0m #------------------------ Iteration 105 --------------------------#
[32m[20221213 20:15:25 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:15:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:26 @agent_ppo2.py:185][0m |           0.0011 |           0.0163 |           0.0000 |
[32m[20221213 20:15:26 @agent_ppo2.py:185][0m |          -0.0087 |           0.0105 |           0.0000 |
[32m[20221213 20:15:26 @agent_ppo2.py:185][0m |          -0.0119 |           0.0098 |           0.0000 |
[32m[20221213 20:15:27 @agent_ppo2.py:185][0m |          -0.0139 |           0.0094 |           0.0000 |
[32m[20221213 20:15:27 @agent_ppo2.py:185][0m |          -0.0160 |           0.0091 |           0.0000 |
[32m[20221213 20:15:27 @agent_ppo2.py:185][0m |          -0.0164 |           0.0093 |           0.0000 |
[32m[20221213 20:15:27 @agent_ppo2.py:185][0m |          -0.0220 |           0.0089 |           0.0000 |
[32m[20221213 20:15:28 @agent_ppo2.py:185][0m |          -0.0168 |           0.0092 |           0.0000 |
[32m[20221213 20:15:28 @agent_ppo2.py:185][0m |          -0.0240 |           0.0091 |           0.0000 |
[32m[20221213 20:15:28 @agent_ppo2.py:185][0m |          -0.0179 |           0.0091 |           0.0000 |
[32m[20221213 20:15:28 @agent_ppo2.py:130][0m Policy update time: 2.75 s
[32m[20221213 20:15:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.03
[32m[20221213 20:15:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.33
[32m[20221213 20:15:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.33
[32m[20221213 20:15:29 @agent_ppo2.py:143][0m Total time:       7.04 min
[32m[20221213 20:15:29 @agent_ppo2.py:145][0m 217088 total steps have happened
[32m[20221213 20:15:29 @agent_ppo2.py:121][0m #------------------------ Iteration 106 --------------------------#
[32m[20221213 20:15:29 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:15:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:30 @agent_ppo2.py:185][0m |           0.0009 |           0.0399 |           0.0000 |
[32m[20221213 20:15:30 @agent_ppo2.py:185][0m |          -0.0112 |           0.0218 |           0.0000 |
[32m[20221213 20:15:30 @agent_ppo2.py:185][0m |          -0.0136 |           0.0191 |           0.0000 |
[32m[20221213 20:15:30 @agent_ppo2.py:185][0m |          -0.0172 |           0.0178 |           0.0000 |
[32m[20221213 20:15:31 @agent_ppo2.py:185][0m |          -0.0159 |           0.0167 |           0.0000 |
[32m[20221213 20:15:31 @agent_ppo2.py:185][0m |          -0.0177 |           0.0163 |           0.0000 |
[32m[20221213 20:15:31 @agent_ppo2.py:185][0m |          -0.0247 |           0.0163 |           0.0000 |
[32m[20221213 20:15:31 @agent_ppo2.py:185][0m |          -0.0186 |           0.0164 |           0.0000 |
[32m[20221213 20:15:32 @agent_ppo2.py:185][0m |          -0.0189 |           0.0152 |           0.0000 |
[32m[20221213 20:15:32 @agent_ppo2.py:185][0m |          -0.0193 |           0.0150 |           0.0000 |
[32m[20221213 20:15:32 @agent_ppo2.py:130][0m Policy update time: 2.84 s
[32m[20221213 20:15:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.15
[32m[20221213 20:15:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.11
[32m[20221213 20:15:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.81
[32m[20221213 20:15:33 @agent_ppo2.py:143][0m Total time:       7.11 min
[32m[20221213 20:15:33 @agent_ppo2.py:145][0m 219136 total steps have happened
[32m[20221213 20:15:33 @agent_ppo2.py:121][0m #------------------------ Iteration 107 --------------------------#
[32m[20221213 20:15:33 @agent_ppo2.py:127][0m Sampling time: 0.72 s by 10 slaves
[32m[20221213 20:15:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:34 @agent_ppo2.py:185][0m |           0.0029 |           0.0294 |           0.0000 |
[32m[20221213 20:15:34 @agent_ppo2.py:185][0m |          -0.0115 |           0.0221 |           0.0000 |
[32m[20221213 20:15:34 @agent_ppo2.py:185][0m |          -0.0164 |           0.0203 |           0.0000 |
[32m[20221213 20:15:35 @agent_ppo2.py:185][0m |          -0.0195 |           0.0195 |           0.0000 |
[32m[20221213 20:15:35 @agent_ppo2.py:185][0m |          -0.0208 |           0.0189 |           0.0000 |
[32m[20221213 20:15:35 @agent_ppo2.py:185][0m |          -0.0227 |           0.0189 |           0.0000 |
[32m[20221213 20:15:35 @agent_ppo2.py:185][0m |          -0.0232 |           0.0183 |           0.0000 |
[32m[20221213 20:15:36 @agent_ppo2.py:185][0m |          -0.0243 |           0.0178 |           0.0000 |
[32m[20221213 20:15:36 @agent_ppo2.py:185][0m |          -0.0268 |           0.0176 |           0.0000 |
[32m[20221213 20:15:36 @agent_ppo2.py:185][0m |          -0.0257 |           0.0173 |           0.0000 |
[32m[20221213 20:15:36 @agent_ppo2.py:130][0m Policy update time: 2.78 s
[32m[20221213 20:15:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.41
[32m[20221213 20:15:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.57
[32m[20221213 20:15:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.61
[32m[20221213 20:15:37 @agent_ppo2.py:143][0m Total time:       7.17 min
[32m[20221213 20:15:37 @agent_ppo2.py:145][0m 221184 total steps have happened
[32m[20221213 20:15:37 @agent_ppo2.py:121][0m #------------------------ Iteration 108 --------------------------#
[32m[20221213 20:15:37 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:15:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:38 @agent_ppo2.py:185][0m |           0.0014 |           0.0866 |           0.0000 |
[32m[20221213 20:15:38 @agent_ppo2.py:185][0m |          -0.0084 |           0.0466 |           0.0000 |
[32m[20221213 20:15:38 @agent_ppo2.py:185][0m |          -0.0124 |           0.0404 |           0.0000 |
[32m[20221213 20:15:39 @agent_ppo2.py:185][0m |          -0.0147 |           0.0363 |           0.0000 |
[32m[20221213 20:15:39 @agent_ppo2.py:185][0m |          -0.0167 |           0.0341 |           0.0000 |
[32m[20221213 20:15:39 @agent_ppo2.py:185][0m |          -0.0179 |           0.0328 |           0.0000 |
[32m[20221213 20:15:39 @agent_ppo2.py:185][0m |          -0.0220 |           0.0310 |           0.0000 |
[32m[20221213 20:15:40 @agent_ppo2.py:185][0m |          -0.0194 |           0.0305 |           0.0000 |
[32m[20221213 20:15:40 @agent_ppo2.py:185][0m |          -0.0214 |           0.0299 |           0.0000 |
[32m[20221213 20:15:40 @agent_ppo2.py:185][0m |          -0.0199 |           0.0296 |           0.0000 |
[32m[20221213 20:15:40 @agent_ppo2.py:130][0m Policy update time: 2.97 s
[32m[20221213 20:15:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 11.33
[32m[20221213 20:15:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 12.50
[32m[20221213 20:15:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.72
[32m[20221213 20:15:41 @agent_ppo2.py:143][0m Total time:       7.24 min
[32m[20221213 20:15:41 @agent_ppo2.py:145][0m 223232 total steps have happened
[32m[20221213 20:15:41 @agent_ppo2.py:121][0m #------------------------ Iteration 109 --------------------------#
[32m[20221213 20:15:41 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:15:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:42 @agent_ppo2.py:185][0m |           0.0012 |           0.0459 |           0.0000 |
[32m[20221213 20:15:42 @agent_ppo2.py:185][0m |          -0.0145 |           0.0377 |           0.0000 |
[32m[20221213 20:15:42 @agent_ppo2.py:185][0m |          -0.0190 |           0.0337 |           0.0000 |
[32m[20221213 20:15:42 @agent_ppo2.py:185][0m |          -0.0208 |           0.0312 |           0.0000 |
[32m[20221213 20:15:43 @agent_ppo2.py:185][0m |          -0.0239 |           0.0301 |           0.0000 |
[32m[20221213 20:15:43 @agent_ppo2.py:185][0m |          -0.0251 |           0.0291 |           0.0000 |
[32m[20221213 20:15:43 @agent_ppo2.py:185][0m |          -0.0260 |           0.0285 |           0.0000 |
[32m[20221213 20:15:43 @agent_ppo2.py:185][0m |          -0.0287 |           0.0273 |           0.0000 |
[32m[20221213 20:15:44 @agent_ppo2.py:185][0m |          -0.0296 |           0.0269 |           0.0000 |
[32m[20221213 20:15:44 @agent_ppo2.py:185][0m |          -0.0303 |           0.0264 |           0.0000 |
[32m[20221213 20:15:44 @agent_ppo2.py:130][0m Policy update time: 2.74 s
[32m[20221213 20:15:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.87
[32m[20221213 20:15:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.10
[32m[20221213 20:15:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 12.66
[32m[20221213 20:15:44 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 12.66
[32m[20221213 20:15:44 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 12.66
[32m[20221213 20:15:44 @agent_ppo2.py:143][0m Total time:       7.30 min
[32m[20221213 20:15:44 @agent_ppo2.py:145][0m 225280 total steps have happened
[32m[20221213 20:15:44 @agent_ppo2.py:121][0m #------------------------ Iteration 110 --------------------------#
[32m[20221213 20:15:45 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:15:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:45 @agent_ppo2.py:185][0m |           0.0012 |           0.0404 |           0.0000 |
[32m[20221213 20:15:46 @agent_ppo2.py:185][0m |          -0.0101 |           0.0117 |           0.0000 |
[32m[20221213 20:15:46 @agent_ppo2.py:185][0m |          -0.0140 |           0.0113 |           0.0000 |
[32m[20221213 20:15:46 @agent_ppo2.py:185][0m |          -0.0159 |           0.0111 |           0.0000 |
[32m[20221213 20:15:46 @agent_ppo2.py:185][0m |          -0.0139 |           0.0109 |           0.0000 |
[32m[20221213 20:15:47 @agent_ppo2.py:185][0m |          -0.0186 |           0.0107 |           0.0000 |
[32m[20221213 20:15:47 @agent_ppo2.py:185][0m |          -0.0192 |           0.0105 |           0.0000 |
[32m[20221213 20:15:47 @agent_ppo2.py:185][0m |          -0.0207 |           0.0103 |           0.0000 |
[32m[20221213 20:15:47 @agent_ppo2.py:185][0m |          -0.0191 |           0.0101 |           0.0000 |
[32m[20221213 20:15:48 @agent_ppo2.py:185][0m |          -0.0209 |           0.0099 |           0.0000 |
[32m[20221213 20:15:48 @agent_ppo2.py:130][0m Policy update time: 2.73 s
[32m[20221213 20:15:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.07
[32m[20221213 20:15:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.11
[32m[20221213 20:15:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.44
[32m[20221213 20:15:48 @agent_ppo2.py:143][0m Total time:       7.36 min
[32m[20221213 20:15:48 @agent_ppo2.py:145][0m 227328 total steps have happened
[32m[20221213 20:15:48 @agent_ppo2.py:121][0m #------------------------ Iteration 111 --------------------------#
[32m[20221213 20:15:49 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:15:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:49 @agent_ppo2.py:185][0m |           0.0012 |           0.0215 |           0.0000 |
[32m[20221213 20:15:49 @agent_ppo2.py:185][0m |          -0.0095 |           0.0143 |           0.0000 |
[32m[20221213 20:15:50 @agent_ppo2.py:185][0m |          -0.0120 |           0.0138 |           0.0000 |
[32m[20221213 20:15:50 @agent_ppo2.py:185][0m |          -0.0147 |           0.0136 |           0.0000 |
[32m[20221213 20:15:50 @agent_ppo2.py:185][0m |          -0.0145 |           0.0130 |           0.0000 |
[32m[20221213 20:15:50 @agent_ppo2.py:185][0m |          -0.0160 |           0.0130 |           0.0000 |
[32m[20221213 20:15:51 @agent_ppo2.py:185][0m |          -0.0166 |           0.0126 |           0.0000 |
[32m[20221213 20:15:51 @agent_ppo2.py:185][0m |          -0.0157 |           0.0125 |           0.0000 |
[32m[20221213 20:15:51 @agent_ppo2.py:185][0m |          -0.0204 |           0.0124 |           0.0000 |
[32m[20221213 20:15:52 @agent_ppo2.py:185][0m |          -0.0161 |           0.0121 |           0.0000 |
[32m[20221213 20:15:52 @agent_ppo2.py:130][0m Policy update time: 3.06 s
[32m[20221213 20:15:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.51
[32m[20221213 20:15:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.57
[32m[20221213 20:15:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.97
[32m[20221213 20:15:52 @agent_ppo2.py:143][0m Total time:       7.43 min
[32m[20221213 20:15:52 @agent_ppo2.py:145][0m 229376 total steps have happened
[32m[20221213 20:15:52 @agent_ppo2.py:121][0m #------------------------ Iteration 112 --------------------------#
[32m[20221213 20:15:53 @agent_ppo2.py:127][0m Sampling time: 0.58 s by 10 slaves
[32m[20221213 20:15:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:54 @agent_ppo2.py:185][0m |          -0.0001 |           0.0139 |           0.0000 |
[32m[20221213 20:15:54 @agent_ppo2.py:185][0m |          -0.0144 |           0.0123 |           0.0000 |
[32m[20221213 20:15:54 @agent_ppo2.py:185][0m |          -0.0175 |           0.0117 |           0.0000 |
[32m[20221213 20:15:55 @agent_ppo2.py:185][0m |          -0.0200 |           0.0113 |           0.0000 |
[32m[20221213 20:15:55 @agent_ppo2.py:185][0m |          -0.0215 |           0.0108 |           0.0000 |
[32m[20221213 20:15:55 @agent_ppo2.py:185][0m |          -0.0225 |           0.0107 |           0.0000 |
[32m[20221213 20:15:56 @agent_ppo2.py:185][0m |          -0.0243 |           0.0108 |           0.0000 |
[32m[20221213 20:15:56 @agent_ppo2.py:185][0m |          -0.0252 |           0.0107 |           0.0000 |
[32m[20221213 20:15:56 @agent_ppo2.py:185][0m |          -0.0264 |           0.0105 |           0.0000 |
[32m[20221213 20:15:57 @agent_ppo2.py:185][0m |          -0.0266 |           0.0105 |           0.0000 |
[32m[20221213 20:15:57 @agent_ppo2.py:130][0m Policy update time: 3.94 s
[32m[20221213 20:15:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.90
[32m[20221213 20:15:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.94
[32m[20221213 20:15:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.75
[32m[20221213 20:15:58 @agent_ppo2.py:143][0m Total time:       7.52 min
[32m[20221213 20:15:58 @agent_ppo2.py:145][0m 231424 total steps have happened
[32m[20221213 20:15:58 @agent_ppo2.py:121][0m #------------------------ Iteration 113 --------------------------#
[32m[20221213 20:15:58 @agent_ppo2.py:127][0m Sampling time: 0.62 s by 10 slaves
[32m[20221213 20:15:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:15:59 @agent_ppo2.py:185][0m |          -0.0009 |           0.0497 |           0.0000 |
[32m[20221213 20:15:59 @agent_ppo2.py:185][0m |          -0.0096 |           0.0269 |           0.0000 |
[32m[20221213 20:15:59 @agent_ppo2.py:185][0m |          -0.0143 |           0.0239 |           0.0000 |
[32m[20221213 20:16:00 @agent_ppo2.py:185][0m |          -0.0158 |           0.0231 |           0.0000 |
[32m[20221213 20:16:00 @agent_ppo2.py:185][0m |          -0.0169 |           0.0219 |           0.0000 |
[32m[20221213 20:16:00 @agent_ppo2.py:185][0m |          -0.0178 |           0.0212 |           0.0000 |
[32m[20221213 20:16:01 @agent_ppo2.py:185][0m |          -0.0180 |           0.0207 |           0.0000 |
[32m[20221213 20:16:01 @agent_ppo2.py:185][0m |          -0.0187 |           0.0204 |           0.0000 |
[32m[20221213 20:16:01 @agent_ppo2.py:185][0m |          -0.0186 |           0.0199 |           0.0000 |
[32m[20221213 20:16:02 @agent_ppo2.py:185][0m |          -0.0202 |           0.0201 |           0.0000 |
[32m[20221213 20:16:02 @agent_ppo2.py:130][0m Policy update time: 3.46 s
[32m[20221213 20:16:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.46
[32m[20221213 20:16:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.89
[32m[20221213 20:16:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.78
[32m[20221213 20:16:02 @agent_ppo2.py:143][0m Total time:       7.60 min
[32m[20221213 20:16:02 @agent_ppo2.py:145][0m 233472 total steps have happened
[32m[20221213 20:16:02 @agent_ppo2.py:121][0m #------------------------ Iteration 114 --------------------------#
[32m[20221213 20:16:03 @agent_ppo2.py:127][0m Sampling time: 0.59 s by 10 slaves
[32m[20221213 20:16:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:16:03 @agent_ppo2.py:185][0m |           0.0028 |           0.0565 |           0.0000 |
[32m[20221213 20:16:04 @agent_ppo2.py:185][0m |          -0.0110 |           0.0413 |           0.0000 |
[32m[20221213 20:16:04 @agent_ppo2.py:185][0m |          -0.0145 |           0.0373 |           0.0000 |
[32m[20221213 20:16:04 @agent_ppo2.py:185][0m |          -0.0185 |           0.0338 |           0.0000 |
[32m[20221213 20:16:04 @agent_ppo2.py:185][0m |          -0.0206 |           0.0322 |           0.0000 |
[32m[20221213 20:16:05 @agent_ppo2.py:185][0m |          -0.0219 |           0.0311 |           0.0000 |
[32m[20221213 20:16:05 @agent_ppo2.py:185][0m |          -0.0227 |           0.0300 |           0.0000 |
[32m[20221213 20:16:05 @agent_ppo2.py:185][0m |          -0.0244 |           0.0295 |           0.0000 |
[32m[20221213 20:16:06 @agent_ppo2.py:185][0m |          -0.0244 |           0.0295 |           0.0000 |
[32m[20221213 20:16:06 @agent_ppo2.py:185][0m |          -0.0254 |           0.0292 |           0.0000 |
[32m[20221213 20:16:06 @agent_ppo2.py:130][0m Policy update time: 3.02 s
[32m[20221213 20:16:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.56
[32m[20221213 20:16:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.53
[32m[20221213 20:16:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 12.54
[32m[20221213 20:16:06 @agent_ppo2.py:143][0m Total time:       7.67 min
[32m[20221213 20:16:06 @agent_ppo2.py:145][0m 235520 total steps have happened
[32m[20221213 20:16:06 @agent_ppo2.py:121][0m #------------------------ Iteration 115 --------------------------#
[32m[20221213 20:16:07 @agent_ppo2.py:127][0m Sampling time: 0.64 s by 10 slaves
[32m[20221213 20:16:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:16:08 @agent_ppo2.py:185][0m |          -0.0009 |           0.0345 |           0.0000 |
[32m[20221213 20:16:08 @agent_ppo2.py:185][0m |          -0.0126 |           0.0119 |           0.0000 |
[32m[20221213 20:16:08 @agent_ppo2.py:185][0m |          -0.0149 |           0.0115 |           0.0000 |
[32m[20221213 20:16:08 @agent_ppo2.py:185][0m |          -0.0177 |           0.0113 |           0.0000 |
[32m[20221213 20:16:09 @agent_ppo2.py:185][0m |          -0.0191 |           0.0111 |           0.0000 |
[32m[20221213 20:16:09 @agent_ppo2.py:185][0m |          -0.0194 |           0.0109 |           0.0000 |
[32m[20221213 20:16:09 @agent_ppo2.py:185][0m |          -0.0201 |           0.0107 |           0.0000 |
[32m[20221213 20:16:10 @agent_ppo2.py:185][0m |          -0.0215 |           0.0105 |           0.0000 |
[32m[20221213 20:16:10 @agent_ppo2.py:185][0m |          -0.0212 |           0.0103 |           0.0000 |
[32m[20221213 20:16:10 @agent_ppo2.py:185][0m |          -0.0216 |           0.0100 |           0.0000 |
[32m[20221213 20:16:10 @agent_ppo2.py:130][0m Policy update time: 3.20 s
[32m[20221213 20:16:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.12
[32m[20221213 20:16:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.50
[32m[20221213 20:16:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.03
[32m[20221213 20:16:11 @agent_ppo2.py:143][0m Total time:       7.74 min
[32m[20221213 20:16:11 @agent_ppo2.py:145][0m 237568 total steps have happened
[32m[20221213 20:16:11 @agent_ppo2.py:121][0m #------------------------ Iteration 116 --------------------------#
[32m[20221213 20:16:11 @agent_ppo2.py:127][0m Sampling time: 0.56 s by 10 slaves
[32m[20221213 20:16:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:16:12 @agent_ppo2.py:185][0m |           0.0035 |           0.0128 |           0.0000 |
[32m[20221213 20:16:12 @agent_ppo2.py:185][0m |          -0.0090 |           0.0105 |           0.0000 |
[32m[20221213 20:16:12 @agent_ppo2.py:185][0m |          -0.0144 |           0.0102 |           0.0000 |
[32m[20221213 20:16:13 @agent_ppo2.py:185][0m |          -0.0152 |           0.0100 |           0.0000 |
[32m[20221213 20:16:13 @agent_ppo2.py:185][0m |          -0.0146 |           0.0098 |           0.0000 |
[32m[20221213 20:16:13 @agent_ppo2.py:185][0m |          -0.0176 |           0.0096 |           0.0000 |
[32m[20221213 20:16:14 @agent_ppo2.py:185][0m |          -0.0188 |           0.0095 |           0.0000 |
[32m[20221213 20:16:14 @agent_ppo2.py:185][0m |          -0.0191 |           0.0093 |           0.0000 |
[32m[20221213 20:16:14 @agent_ppo2.py:185][0m |          -0.0200 |           0.0093 |           0.0000 |
[32m[20221213 20:16:14 @agent_ppo2.py:185][0m |          -0.0199 |           0.0092 |           0.0000 |
[32m[20221213 20:16:14 @agent_ppo2.py:130][0m Policy update time: 3.00 s
[32m[20221213 20:16:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.86
[32m[20221213 20:16:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.77
[32m[20221213 20:16:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.40
[32m[20221213 20:16:15 @agent_ppo2.py:143][0m Total time:       7.81 min
[32m[20221213 20:16:15 @agent_ppo2.py:145][0m 239616 total steps have happened
[32m[20221213 20:16:15 @agent_ppo2.py:121][0m #------------------------ Iteration 117 --------------------------#
[32m[20221213 20:16:16 @agent_ppo2.py:127][0m Sampling time: 0.62 s by 10 slaves
[32m[20221213 20:16:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:16:16 @agent_ppo2.py:185][0m |           0.0012 |           0.0208 |           0.0000 |
[32m[20221213 20:16:16 @agent_ppo2.py:185][0m |          -0.0141 |           0.0167 |           0.0000 |
[32m[20221213 20:16:17 @agent_ppo2.py:185][0m |          -0.0130 |           0.0151 |           0.0000 |
[32m[20221213 20:16:17 @agent_ppo2.py:185][0m |          -0.0147 |           0.0146 |           0.0000 |
[32m[20221213 20:16:17 @agent_ppo2.py:185][0m |          -0.0162 |           0.0142 |           0.0000 |
[32m[20221213 20:16:18 @agent_ppo2.py:185][0m |          -0.0177 |           0.0139 |           0.0000 |
[32m[20221213 20:16:18 @agent_ppo2.py:185][0m |          -0.0187 |           0.0138 |           0.0000 |
[32m[20221213 20:16:18 @agent_ppo2.py:185][0m |          -0.0189 |           0.0137 |           0.0000 |
[32m[20221213 20:16:19 @agent_ppo2.py:185][0m |          -0.0228 |           0.0137 |           0.0000 |
[32m[20221213 20:16:19 @agent_ppo2.py:185][0m |          -0.0204 |           0.0135 |           0.0000 |
[32m[20221213 20:16:19 @agent_ppo2.py:130][0m Policy update time: 3.17 s
[32m[20221213 20:16:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.93
[32m[20221213 20:16:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.65
[32m[20221213 20:16:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.35
[32m[20221213 20:16:19 @agent_ppo2.py:143][0m Total time:       7.89 min
[32m[20221213 20:16:19 @agent_ppo2.py:145][0m 241664 total steps have happened
[32m[20221213 20:16:19 @agent_ppo2.py:121][0m #------------------------ Iteration 118 --------------------------#
[32m[20221213 20:16:20 @agent_ppo2.py:127][0m Sampling time: 0.62 s by 10 slaves
[32m[20221213 20:16:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:16:21 @agent_ppo2.py:185][0m |           0.0027 |           0.0261 |           0.0000 |
[32m[20221213 20:16:21 @agent_ppo2.py:185][0m |          -0.0101 |           0.0206 |           0.0000 |
[32m[20221213 20:16:21 @agent_ppo2.py:185][0m |          -0.0124 |           0.0186 |           0.0000 |
[32m[20221213 20:16:21 @agent_ppo2.py:185][0m |          -0.0146 |           0.0183 |           0.0000 |
[32m[20221213 20:16:22 @agent_ppo2.py:185][0m |          -0.0156 |           0.0171 |           0.0000 |
[32m[20221213 20:16:22 @agent_ppo2.py:185][0m |          -0.0165 |           0.0170 |           0.0000 |
[32m[20221213 20:16:22 @agent_ppo2.py:185][0m |          -0.0174 |           0.0165 |           0.0000 |
[32m[20221213 20:16:23 @agent_ppo2.py:185][0m |          -0.0181 |           0.0162 |           0.0000 |
[32m[20221213 20:16:23 @agent_ppo2.py:185][0m |          -0.0189 |           0.0166 |           0.0000 |
[32m[20221213 20:16:23 @agent_ppo2.py:185][0m |          -0.0215 |           0.0164 |           0.0000 |
[32m[20221213 20:16:23 @agent_ppo2.py:130][0m Policy update time: 3.04 s
[32m[20221213 20:16:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.91
[32m[20221213 20:16:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.14
[32m[20221213 20:16:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.38
[32m[20221213 20:16:24 @agent_ppo2.py:143][0m Total time:       7.96 min
[32m[20221213 20:16:24 @agent_ppo2.py:145][0m 243712 total steps have happened
[32m[20221213 20:16:24 @agent_ppo2.py:121][0m #------------------------ Iteration 119 --------------------------#
[32m[20221213 20:16:24 @agent_ppo2.py:127][0m Sampling time: 0.55 s by 10 slaves
[32m[20221213 20:16:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:16:25 @agent_ppo2.py:185][0m |          -0.0021 |           0.0182 |           0.0000 |
[32m[20221213 20:16:25 @agent_ppo2.py:185][0m |          -0.0166 |           0.0160 |           0.0000 |
[32m[20221213 20:16:25 @agent_ppo2.py:185][0m |          -0.0214 |           0.0146 |           0.0000 |
[32m[20221213 20:16:26 @agent_ppo2.py:185][0m |          -0.0225 |           0.0141 |           0.0000 |
[32m[20221213 20:16:26 @agent_ppo2.py:185][0m |          -0.0260 |           0.0140 |           0.0000 |
[32m[20221213 20:16:26 @agent_ppo2.py:185][0m |          -0.0272 |           0.0135 |           0.0000 |
[32m[20221213 20:16:27 @agent_ppo2.py:185][0m |          -0.0295 |           0.0140 |           0.0000 |
[32m[20221213 20:16:27 @agent_ppo2.py:185][0m |          -0.0307 |           0.0135 |           0.0000 |
[32m[20221213 20:16:27 @agent_ppo2.py:185][0m |          -0.0318 |           0.0134 |           0.0000 |
[32m[20221213 20:16:27 @agent_ppo2.py:185][0m |          -0.0330 |           0.0134 |           0.0000 |
[32m[20221213 20:16:27 @agent_ppo2.py:130][0m Policy update time: 3.04 s
[32m[20221213 20:16:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.00
[32m[20221213 20:16:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.05
[32m[20221213 20:16:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.09
[32m[20221213 20:16:28 @agent_ppo2.py:143][0m Total time:       8.03 min
[32m[20221213 20:16:28 @agent_ppo2.py:145][0m 245760 total steps have happened
[32m[20221213 20:16:28 @agent_ppo2.py:121][0m #------------------------ Iteration 120 --------------------------#
[32m[20221213 20:16:29 @agent_ppo2.py:127][0m Sampling time: 0.57 s by 10 slaves
[32m[20221213 20:16:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:16:29 @agent_ppo2.py:185][0m |           0.0030 |           0.1321 |           0.0000 |
[32m[20221213 20:16:29 @agent_ppo2.py:185][0m |          -0.0102 |           0.0670 |           0.0000 |
[32m[20221213 20:16:30 @agent_ppo2.py:185][0m |          -0.0137 |           0.0561 |           0.0000 |
[32m[20221213 20:16:30 @agent_ppo2.py:185][0m |          -0.0169 |           0.0498 |           0.0000 |
[32m[20221213 20:16:30 @agent_ppo2.py:185][0m |          -0.0194 |           0.0481 |           0.0000 |
[32m[20221213 20:16:31 @agent_ppo2.py:185][0m |          -0.0184 |           0.0459 |           0.0000 |
[32m[20221213 20:16:31 @agent_ppo2.py:185][0m |          -0.0255 |           0.0436 |           0.0000 |
[32m[20221213 20:16:31 @agent_ppo2.py:185][0m |          -0.0223 |           0.0424 |           0.0000 |
[32m[20221213 20:16:31 @agent_ppo2.py:185][0m |          -0.0248 |           0.0398 |           0.0000 |
[32m[20221213 20:16:32 @agent_ppo2.py:185][0m |          -0.0229 |           0.0385 |           0.0000 |
[32m[20221213 20:16:32 @agent_ppo2.py:130][0m Policy update time: 3.09 s
[32m[20221213 20:16:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.89
[32m[20221213 20:16:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 13.29
[32m[20221213 20:16:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.89
[32m[20221213 20:16:32 @agent_ppo2.py:143][0m Total time:       8.10 min
[32m[20221213 20:16:32 @agent_ppo2.py:145][0m 247808 total steps have happened
[32m[20221213 20:16:32 @agent_ppo2.py:121][0m #------------------------ Iteration 121 --------------------------#
[32m[20221213 20:16:33 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:16:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:16:33 @agent_ppo2.py:185][0m |           0.0050 |           0.0276 |           0.0000 |
[32m[20221213 20:16:33 @agent_ppo2.py:185][0m |          -0.0106 |           0.0156 |           0.0000 |
[32m[20221213 20:16:34 @agent_ppo2.py:185][0m |          -0.0184 |           0.0149 |           0.0000 |
[32m[20221213 20:16:34 @agent_ppo2.py:185][0m |          -0.0226 |           0.0144 |           0.0000 |
[32m[20221213 20:16:34 @agent_ppo2.py:185][0m |          -0.0242 |           0.0141 |           0.0000 |
[32m[20221213 20:16:34 @agent_ppo2.py:185][0m |          -0.0255 |           0.0139 |           0.0000 |
[32m[20221213 20:16:35 @agent_ppo2.py:185][0m |          -0.0271 |           0.0137 |           0.0000 |
[32m[20221213 20:16:35 @agent_ppo2.py:185][0m |          -0.0274 |           0.0135 |           0.0000 |
[32m[20221213 20:16:35 @agent_ppo2.py:185][0m |          -0.0265 |           0.0133 |           0.0000 |
[32m[20221213 20:16:36 @agent_ppo2.py:185][0m |          -0.0286 |           0.0131 |           0.0000 |
[32m[20221213 20:16:36 @agent_ppo2.py:130][0m Policy update time: 2.82 s
[32m[20221213 20:16:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.61
[32m[20221213 20:16:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.47
[32m[20221213 20:16:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.17
[32m[20221213 20:16:36 @agent_ppo2.py:143][0m Total time:       8.16 min
[32m[20221213 20:16:36 @agent_ppo2.py:145][0m 249856 total steps have happened
[32m[20221213 20:16:36 @agent_ppo2.py:121][0m #------------------------ Iteration 122 --------------------------#
[32m[20221213 20:16:37 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:16:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:16:37 @agent_ppo2.py:185][0m |           0.0024 |           0.0241 |           0.0000 |
[32m[20221213 20:16:37 @agent_ppo2.py:185][0m |          -0.0095 |           0.0189 |           0.0000 |
[32m[20221213 20:16:38 @agent_ppo2.py:185][0m |          -0.0129 |           0.0180 |           0.0000 |
[32m[20221213 20:16:38 @agent_ppo2.py:185][0m |          -0.0137 |           0.0171 |           0.0000 |
[32m[20221213 20:16:38 @agent_ppo2.py:185][0m |          -0.0172 |           0.0163 |           0.0000 |
[32m[20221213 20:16:38 @agent_ppo2.py:185][0m |          -0.0178 |           0.0166 |           0.0000 |
[32m[20221213 20:16:39 @agent_ppo2.py:185][0m |          -0.0182 |           0.0159 |           0.0000 |
[32m[20221213 20:16:39 @agent_ppo2.py:185][0m |          -0.0194 |           0.0163 |           0.0000 |
[32m[20221213 20:16:39 @agent_ppo2.py:185][0m |          -0.0204 |           0.0155 |           0.0000 |
[32m[20221213 20:16:39 @agent_ppo2.py:185][0m |          -0.0205 |           0.0155 |           0.0000 |
[32m[20221213 20:16:39 @agent_ppo2.py:130][0m Policy update time: 2.92 s
[32m[20221213 20:16:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.87
[32m[20221213 20:16:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.84
[32m[20221213 20:16:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.07
[32m[20221213 20:16:40 @agent_ppo2.py:143][0m Total time:       8.23 min
[32m[20221213 20:16:40 @agent_ppo2.py:145][0m 251904 total steps have happened
[32m[20221213 20:16:40 @agent_ppo2.py:121][0m #------------------------ Iteration 123 --------------------------#
[32m[20221213 20:16:41 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:16:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:16:41 @agent_ppo2.py:185][0m |           0.0004 |           0.0268 |           0.0000 |
[32m[20221213 20:16:41 @agent_ppo2.py:185][0m |          -0.0065 |           0.0210 |           0.0000 |
[32m[20221213 20:16:42 @agent_ppo2.py:185][0m |          -0.0104 |           0.0194 |           0.0000 |
[32m[20221213 20:16:42 @agent_ppo2.py:185][0m |          -0.0148 |           0.0189 |           0.0000 |
[32m[20221213 20:16:42 @agent_ppo2.py:185][0m |          -0.0167 |           0.0185 |           0.0000 |
[32m[20221213 20:16:42 @agent_ppo2.py:185][0m |          -0.0173 |           0.0183 |           0.0000 |
[32m[20221213 20:16:43 @agent_ppo2.py:185][0m |          -0.0183 |           0.0181 |           0.0000 |
[32m[20221213 20:16:43 @agent_ppo2.py:185][0m |          -0.0197 |           0.0177 |           0.0000 |
[32m[20221213 20:16:43 @agent_ppo2.py:185][0m |          -0.0203 |           0.0177 |           0.0000 |
[32m[20221213 20:16:43 @agent_ppo2.py:185][0m |          -0.0213 |           0.0175 |           0.0000 |
[32m[20221213 20:16:43 @agent_ppo2.py:130][0m Policy update time: 2.81 s
[32m[20221213 20:16:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.08
[32m[20221213 20:16:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.92
[32m[20221213 20:16:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.45
[32m[20221213 20:16:44 @agent_ppo2.py:143][0m Total time:       8.30 min
[32m[20221213 20:16:44 @agent_ppo2.py:145][0m 253952 total steps have happened
[32m[20221213 20:16:44 @agent_ppo2.py:121][0m #------------------------ Iteration 124 --------------------------#
[32m[20221213 20:16:45 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:16:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:16:45 @agent_ppo2.py:185][0m |           0.0031 |           0.0265 |           0.0000 |
[32m[20221213 20:16:45 @agent_ppo2.py:185][0m |          -0.0147 |           0.0216 |           0.0000 |
[32m[20221213 20:16:45 @agent_ppo2.py:185][0m |          -0.0197 |           0.0201 |           0.0000 |
[32m[20221213 20:16:46 @agent_ppo2.py:185][0m |          -0.0209 |           0.0193 |           0.0000 |
[32m[20221213 20:16:46 @agent_ppo2.py:185][0m |          -0.0239 |           0.0190 |           0.0000 |
[32m[20221213 20:16:46 @agent_ppo2.py:185][0m |          -0.0268 |           0.0184 |           0.0000 |
[32m[20221213 20:16:46 @agent_ppo2.py:185][0m |          -0.0273 |           0.0180 |           0.0000 |
[32m[20221213 20:16:47 @agent_ppo2.py:185][0m |          -0.0290 |           0.0177 |           0.0000 |
[32m[20221213 20:16:47 @agent_ppo2.py:185][0m |          -0.0290 |           0.0179 |           0.0000 |
[32m[20221213 20:16:47 @agent_ppo2.py:185][0m |          -0.0299 |           0.0178 |           0.0000 |
[32m[20221213 20:16:47 @agent_ppo2.py:130][0m Policy update time: 2.75 s
[32m[20221213 20:16:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.72
[32m[20221213 20:16:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.49
[32m[20221213 20:16:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.11
[32m[20221213 20:16:48 @agent_ppo2.py:143][0m Total time:       8.36 min
[32m[20221213 20:16:48 @agent_ppo2.py:145][0m 256000 total steps have happened
[32m[20221213 20:16:48 @agent_ppo2.py:121][0m #------------------------ Iteration 125 --------------------------#
[32m[20221213 20:16:48 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:16:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:16:49 @agent_ppo2.py:185][0m |           0.0011 |           0.0242 |           0.0000 |
[32m[20221213 20:16:49 @agent_ppo2.py:185][0m |          -0.0133 |           0.0206 |           0.0000 |
[32m[20221213 20:16:49 @agent_ppo2.py:185][0m |          -0.0179 |           0.0195 |           0.0000 |
[32m[20221213 20:16:50 @agent_ppo2.py:185][0m |          -0.0207 |           0.0190 |           0.0000 |
[32m[20221213 20:16:50 @agent_ppo2.py:185][0m |          -0.0234 |           0.0185 |           0.0000 |
[32m[20221213 20:16:50 @agent_ppo2.py:185][0m |          -0.0253 |           0.0179 |           0.0000 |
[32m[20221213 20:16:50 @agent_ppo2.py:185][0m |          -0.0259 |           0.0180 |           0.0000 |
[32m[20221213 20:16:51 @agent_ppo2.py:185][0m |          -0.0292 |           0.0173 |           0.0000 |
[32m[20221213 20:16:51 @agent_ppo2.py:185][0m |          -0.0284 |           0.0175 |           0.0000 |
[32m[20221213 20:16:51 @agent_ppo2.py:185][0m |          -0.0334 |           0.0176 |           0.0000 |
[32m[20221213 20:16:51 @agent_ppo2.py:130][0m Policy update time: 2.73 s
[32m[20221213 20:16:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.03
[32m[20221213 20:16:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.76
[32m[20221213 20:16:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.26
[32m[20221213 20:16:52 @agent_ppo2.py:143][0m Total time:       8.42 min
[32m[20221213 20:16:52 @agent_ppo2.py:145][0m 258048 total steps have happened
[32m[20221213 20:16:52 @agent_ppo2.py:121][0m #------------------------ Iteration 126 --------------------------#
[32m[20221213 20:16:52 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:16:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:16:53 @agent_ppo2.py:185][0m |          -0.0011 |           0.0189 |           0.0000 |
[32m[20221213 20:16:53 @agent_ppo2.py:185][0m |          -0.0119 |           0.0154 |           0.0000 |
[32m[20221213 20:16:53 @agent_ppo2.py:185][0m |          -0.0195 |           0.0151 |           0.0000 |
[32m[20221213 20:16:53 @agent_ppo2.py:185][0m |          -0.0223 |           0.0144 |           0.0000 |
[32m[20221213 20:16:54 @agent_ppo2.py:185][0m |          -0.0242 |           0.0142 |           0.0000 |
[32m[20221213 20:16:54 @agent_ppo2.py:185][0m |          -0.0278 |           0.0138 |           0.0000 |
[32m[20221213 20:16:54 @agent_ppo2.py:185][0m |          -0.0287 |           0.0139 |           0.0000 |
[32m[20221213 20:16:55 @agent_ppo2.py:185][0m |          -0.0314 |           0.0138 |           0.0000 |
[32m[20221213 20:16:55 @agent_ppo2.py:185][0m |          -0.0295 |           0.0136 |           0.0000 |
[32m[20221213 20:16:55 @agent_ppo2.py:185][0m |          -0.0288 |           0.0134 |           0.0000 |
[32m[20221213 20:16:55 @agent_ppo2.py:130][0m Policy update time: 3.12 s
[32m[20221213 20:16:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.16
[32m[20221213 20:16:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.92
[32m[20221213 20:16:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.22
[32m[20221213 20:16:56 @agent_ppo2.py:143][0m Total time:       8.49 min
[32m[20221213 20:16:56 @agent_ppo2.py:145][0m 260096 total steps have happened
[32m[20221213 20:16:56 @agent_ppo2.py:121][0m #------------------------ Iteration 127 --------------------------#
[32m[20221213 20:16:56 @agent_ppo2.py:127][0m Sampling time: 0.61 s by 10 slaves
[32m[20221213 20:16:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:16:57 @agent_ppo2.py:185][0m |           0.0016 |           0.0128 |           0.0000 |
[32m[20221213 20:16:57 @agent_ppo2.py:185][0m |          -0.0141 |           0.0123 |           0.0000 |
[32m[20221213 20:16:58 @agent_ppo2.py:185][0m |          -0.0187 |           0.0120 |           0.0000 |
[32m[20221213 20:16:58 @agent_ppo2.py:185][0m |          -0.0210 |           0.0117 |           0.0000 |
[32m[20221213 20:16:58 @agent_ppo2.py:185][0m |          -0.0240 |           0.0118 |           0.0000 |
[32m[20221213 20:16:59 @agent_ppo2.py:185][0m |          -0.0268 |           0.0115 |           0.0000 |
[32m[20221213 20:16:59 @agent_ppo2.py:185][0m |          -0.0278 |           0.0116 |           0.0000 |
[32m[20221213 20:16:59 @agent_ppo2.py:185][0m |          -0.0290 |           0.0116 |           0.0000 |
[32m[20221213 20:17:00 @agent_ppo2.py:185][0m |          -0.0296 |           0.0115 |           0.0000 |
[32m[20221213 20:17:00 @agent_ppo2.py:185][0m |          -0.0303 |           0.0113 |           0.0000 |
[32m[20221213 20:17:00 @agent_ppo2.py:130][0m Policy update time: 3.48 s
[32m[20221213 20:17:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.50
[32m[20221213 20:17:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.55
[32m[20221213 20:17:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.38
[32m[20221213 20:17:01 @agent_ppo2.py:143][0m Total time:       8.57 min
[32m[20221213 20:17:01 @agent_ppo2.py:145][0m 262144 total steps have happened
[32m[20221213 20:17:01 @agent_ppo2.py:121][0m #------------------------ Iteration 128 --------------------------#
[32m[20221213 20:17:01 @agent_ppo2.py:127][0m Sampling time: 0.70 s by 10 slaves
[32m[20221213 20:17:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:02 @agent_ppo2.py:185][0m |           0.0029 |           0.0499 |           0.0000 |
[32m[20221213 20:17:02 @agent_ppo2.py:185][0m |          -0.0076 |           0.0273 |           0.0000 |
[32m[20221213 20:17:02 @agent_ppo2.py:185][0m |          -0.0113 |           0.0241 |           0.0000 |
[32m[20221213 20:17:03 @agent_ppo2.py:185][0m |          -0.0144 |           0.0226 |           0.0000 |
[32m[20221213 20:17:03 @agent_ppo2.py:185][0m |          -0.0161 |           0.0217 |           0.0000 |
[32m[20221213 20:17:03 @agent_ppo2.py:185][0m |          -0.0161 |           0.0214 |           0.0000 |
[32m[20221213 20:17:04 @agent_ppo2.py:185][0m |          -0.0207 |           0.0212 |           0.0000 |
[32m[20221213 20:17:04 @agent_ppo2.py:185][0m |          -0.0181 |           0.0207 |           0.0000 |
[32m[20221213 20:17:04 @agent_ppo2.py:185][0m |          -0.0179 |           0.0202 |           0.0000 |
[32m[20221213 20:17:05 @agent_ppo2.py:185][0m |          -0.0198 |           0.0200 |           0.0000 |
[32m[20221213 20:17:05 @agent_ppo2.py:130][0m Policy update time: 3.35 s
[32m[20221213 20:17:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.36
[32m[20221213 20:17:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.98
[32m[20221213 20:17:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.95
[32m[20221213 20:17:05 @agent_ppo2.py:143][0m Total time:       8.65 min
[32m[20221213 20:17:05 @agent_ppo2.py:145][0m 264192 total steps have happened
[32m[20221213 20:17:05 @agent_ppo2.py:121][0m #------------------------ Iteration 129 --------------------------#
[32m[20221213 20:17:06 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:17:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:06 @agent_ppo2.py:185][0m |           0.0015 |           0.0194 |           0.0000 |
[32m[20221213 20:17:07 @agent_ppo2.py:185][0m |          -0.0124 |           0.0109 |           0.0000 |
[32m[20221213 20:17:07 @agent_ppo2.py:185][0m |          -0.0149 |           0.0106 |           0.0000 |
[32m[20221213 20:17:07 @agent_ppo2.py:185][0m |          -0.0187 |           0.0103 |           0.0000 |
[32m[20221213 20:17:07 @agent_ppo2.py:185][0m |          -0.0193 |           0.0101 |           0.0000 |
[32m[20221213 20:17:08 @agent_ppo2.py:185][0m |          -0.0217 |           0.0100 |           0.0000 |
[32m[20221213 20:17:08 @agent_ppo2.py:185][0m |          -0.0224 |           0.0098 |           0.0000 |
[32m[20221213 20:17:08 @agent_ppo2.py:185][0m |          -0.0237 |           0.0096 |           0.0000 |
[32m[20221213 20:17:08 @agent_ppo2.py:185][0m |          -0.0227 |           0.0095 |           0.0000 |
[32m[20221213 20:17:09 @agent_ppo2.py:185][0m |          -0.0230 |           0.0093 |           0.0000 |
[32m[20221213 20:17:09 @agent_ppo2.py:130][0m Policy update time: 2.85 s
[32m[20221213 20:17:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.46
[32m[20221213 20:17:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.16
[32m[20221213 20:17:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.20
[32m[20221213 20:17:09 @agent_ppo2.py:143][0m Total time:       8.72 min
[32m[20221213 20:17:09 @agent_ppo2.py:145][0m 266240 total steps have happened
[32m[20221213 20:17:09 @agent_ppo2.py:121][0m #------------------------ Iteration 130 --------------------------#
[32m[20221213 20:17:10 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:17:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:10 @agent_ppo2.py:185][0m |          -0.0011 |           0.0138 |           0.0000 |
[32m[20221213 20:17:11 @agent_ppo2.py:185][0m |          -0.0095 |           0.0120 |           0.0000 |
[32m[20221213 20:17:11 @agent_ppo2.py:185][0m |          -0.0115 |           0.0117 |           0.0000 |
[32m[20221213 20:17:11 @agent_ppo2.py:185][0m |          -0.0145 |           0.0115 |           0.0000 |
[32m[20221213 20:17:11 @agent_ppo2.py:185][0m |          -0.0158 |           0.0112 |           0.0000 |
[32m[20221213 20:17:12 @agent_ppo2.py:185][0m |          -0.0166 |           0.0110 |           0.0000 |
[32m[20221213 20:17:12 @agent_ppo2.py:185][0m |          -0.0179 |           0.0112 |           0.0000 |
[32m[20221213 20:17:12 @agent_ppo2.py:185][0m |          -0.0182 |           0.0109 |           0.0000 |
[32m[20221213 20:17:12 @agent_ppo2.py:185][0m |          -0.0188 |           0.0110 |           0.0000 |
[32m[20221213 20:17:13 @agent_ppo2.py:185][0m |          -0.0251 |           0.0108 |           0.0000 |
[32m[20221213 20:17:13 @agent_ppo2.py:130][0m Policy update time: 2.82 s
[32m[20221213 20:17:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.96
[32m[20221213 20:17:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.70
[32m[20221213 20:17:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.42
[32m[20221213 20:17:14 @agent_ppo2.py:143][0m Total time:       8.79 min
[32m[20221213 20:17:14 @agent_ppo2.py:145][0m 268288 total steps have happened
[32m[20221213 20:17:14 @agent_ppo2.py:121][0m #------------------------ Iteration 131 --------------------------#
[32m[20221213 20:17:14 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:17:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:14 @agent_ppo2.py:185][0m |          -0.0005 |           0.0405 |           0.0000 |
[32m[20221213 20:17:15 @agent_ppo2.py:185][0m |          -0.0094 |           0.0272 |           0.0000 |
[32m[20221213 20:17:15 @agent_ppo2.py:185][0m |          -0.0148 |           0.0257 |           0.0000 |
[32m[20221213 20:17:15 @agent_ppo2.py:185][0m |          -0.0148 |           0.0232 |           0.0000 |
[32m[20221213 20:17:15 @agent_ppo2.py:185][0m |          -0.0169 |           0.0225 |           0.0000 |
[32m[20221213 20:17:16 @agent_ppo2.py:185][0m |          -0.0174 |           0.0220 |           0.0000 |
[32m[20221213 20:17:16 @agent_ppo2.py:185][0m |          -0.0183 |           0.0211 |           0.0000 |
[32m[20221213 20:17:16 @agent_ppo2.py:185][0m |          -0.0189 |           0.0207 |           0.0000 |
[32m[20221213 20:17:16 @agent_ppo2.py:185][0m |          -0.0228 |           0.0208 |           0.0000 |
[32m[20221213 20:17:17 @agent_ppo2.py:185][0m |          -0.0195 |           0.0206 |           0.0000 |
[32m[20221213 20:17:17 @agent_ppo2.py:130][0m Policy update time: 2.70 s
[32m[20221213 20:17:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.91
[32m[20221213 20:17:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.32
[32m[20221213 20:17:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.09
[32m[20221213 20:17:17 @agent_ppo2.py:143][0m Total time:       8.85 min
[32m[20221213 20:17:17 @agent_ppo2.py:145][0m 270336 total steps have happened
[32m[20221213 20:17:17 @agent_ppo2.py:121][0m #------------------------ Iteration 132 --------------------------#
[32m[20221213 20:17:18 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:17:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:18 @agent_ppo2.py:185][0m |           0.0020 |           0.0577 |           0.0000 |
[32m[20221213 20:17:18 @agent_ppo2.py:185][0m |          -0.0130 |           0.0407 |           0.0000 |
[32m[20221213 20:17:19 @agent_ppo2.py:185][0m |          -0.0170 |           0.0351 |           0.0000 |
[32m[20221213 20:17:19 @agent_ppo2.py:185][0m |          -0.0208 |           0.0320 |           0.0000 |
[32m[20221213 20:17:19 @agent_ppo2.py:185][0m |          -0.0245 |           0.0306 |           0.0000 |
[32m[20221213 20:17:19 @agent_ppo2.py:185][0m |          -0.0239 |           0.0292 |           0.0000 |
[32m[20221213 20:17:20 @agent_ppo2.py:185][0m |          -0.0247 |           0.0285 |           0.0000 |
[32m[20221213 20:17:20 @agent_ppo2.py:185][0m |          -0.0269 |           0.0277 |           0.0000 |
[32m[20221213 20:17:20 @agent_ppo2.py:185][0m |          -0.0271 |           0.0268 |           0.0000 |
[32m[20221213 20:17:20 @agent_ppo2.py:185][0m |          -0.0276 |           0.0261 |           0.0000 |
[32m[20221213 20:17:20 @agent_ppo2.py:130][0m Policy update time: 2.69 s
[32m[20221213 20:17:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.81
[32m[20221213 20:17:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.80
[32m[20221213 20:17:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.84
[32m[20221213 20:17:21 @agent_ppo2.py:143][0m Total time:       8.91 min
[32m[20221213 20:17:21 @agent_ppo2.py:145][0m 272384 total steps have happened
[32m[20221213 20:17:21 @agent_ppo2.py:121][0m #------------------------ Iteration 133 --------------------------#
[32m[20221213 20:17:21 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:17:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:22 @agent_ppo2.py:185][0m |           0.0003 |           0.0253 |           0.0000 |
[32m[20221213 20:17:22 @agent_ppo2.py:185][0m |          -0.0099 |           0.0103 |           0.0000 |
[32m[20221213 20:17:22 @agent_ppo2.py:185][0m |          -0.0118 |           0.0100 |           0.0000 |
[32m[20221213 20:17:23 @agent_ppo2.py:185][0m |          -0.0159 |           0.0098 |           0.0000 |
[32m[20221213 20:17:23 @agent_ppo2.py:185][0m |          -0.0185 |           0.0096 |           0.0000 |
[32m[20221213 20:17:23 @agent_ppo2.py:185][0m |          -0.0196 |           0.0094 |           0.0000 |
[32m[20221213 20:17:23 @agent_ppo2.py:185][0m |          -0.0205 |           0.0092 |           0.0000 |
[32m[20221213 20:17:24 @agent_ppo2.py:185][0m |          -0.0199 |           0.0090 |           0.0000 |
[32m[20221213 20:17:24 @agent_ppo2.py:185][0m |          -0.0215 |           0.0088 |           0.0000 |
[32m[20221213 20:17:24 @agent_ppo2.py:185][0m |          -0.0223 |           0.0086 |           0.0000 |
[32m[20221213 20:17:24 @agent_ppo2.py:130][0m Policy update time: 2.72 s
[32m[20221213 20:17:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.22
[32m[20221213 20:17:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.37
[32m[20221213 20:17:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.68
[32m[20221213 20:17:25 @agent_ppo2.py:143][0m Total time:       8.97 min
[32m[20221213 20:17:25 @agent_ppo2.py:145][0m 274432 total steps have happened
[32m[20221213 20:17:25 @agent_ppo2.py:121][0m #------------------------ Iteration 134 --------------------------#
[32m[20221213 20:17:25 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:17:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:26 @agent_ppo2.py:185][0m |           0.0039 |           0.0756 |           0.0000 |
[32m[20221213 20:17:26 @agent_ppo2.py:185][0m |          -0.0114 |           0.0322 |           0.0000 |
[32m[20221213 20:17:26 @agent_ppo2.py:185][0m |          -0.0131 |           0.0273 |           0.0000 |
[32m[20221213 20:17:26 @agent_ppo2.py:185][0m |          -0.0158 |           0.0245 |           0.0000 |
[32m[20221213 20:17:27 @agent_ppo2.py:185][0m |          -0.0161 |           0.0238 |           0.0000 |
[32m[20221213 20:17:27 @agent_ppo2.py:185][0m |          -0.0172 |           0.0226 |           0.0000 |
[32m[20221213 20:17:27 @agent_ppo2.py:185][0m |          -0.0185 |           0.0217 |           0.0000 |
[32m[20221213 20:17:27 @agent_ppo2.py:185][0m |          -0.0187 |           0.0215 |           0.0000 |
[32m[20221213 20:17:28 @agent_ppo2.py:185][0m |          -0.0230 |           0.0208 |           0.0000 |
[32m[20221213 20:17:28 @agent_ppo2.py:185][0m |          -0.0191 |           0.0204 |           0.0000 |
[32m[20221213 20:17:28 @agent_ppo2.py:130][0m Policy update time: 2.69 s
[32m[20221213 20:17:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.92
[32m[20221213 20:17:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.71
[32m[20221213 20:17:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.53
[32m[20221213 20:17:28 @agent_ppo2.py:143][0m Total time:       9.04 min
[32m[20221213 20:17:28 @agent_ppo2.py:145][0m 276480 total steps have happened
[32m[20221213 20:17:28 @agent_ppo2.py:121][0m #------------------------ Iteration 135 --------------------------#
[32m[20221213 20:17:29 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:17:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:29 @agent_ppo2.py:185][0m |          -0.0004 |           0.0188 |           0.0000 |
[32m[20221213 20:17:30 @agent_ppo2.py:185][0m |          -0.0132 |           0.0112 |           0.0000 |
[32m[20221213 20:17:30 @agent_ppo2.py:185][0m |          -0.0179 |           0.0107 |           0.0000 |
[32m[20221213 20:17:30 @agent_ppo2.py:185][0m |          -0.0193 |           0.0104 |           0.0000 |
[32m[20221213 20:17:30 @agent_ppo2.py:185][0m |          -0.0207 |           0.0102 |           0.0000 |
[32m[20221213 20:17:31 @agent_ppo2.py:185][0m |          -0.0185 |           0.0099 |           0.0000 |
[32m[20221213 20:17:31 @agent_ppo2.py:185][0m |          -0.0223 |           0.0098 |           0.0000 |
[32m[20221213 20:17:31 @agent_ppo2.py:185][0m |          -0.0237 |           0.0095 |           0.0000 |
[32m[20221213 20:17:32 @agent_ppo2.py:185][0m |          -0.0241 |           0.0094 |           0.0000 |
[32m[20221213 20:17:32 @agent_ppo2.py:185][0m |          -0.0242 |           0.0092 |           0.0000 |
[32m[20221213 20:17:32 @agent_ppo2.py:130][0m Policy update time: 2.79 s
[32m[20221213 20:17:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.84
[32m[20221213 20:17:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.80
[32m[20221213 20:17:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.24
[32m[20221213 20:17:32 @agent_ppo2.py:143][0m Total time:       9.10 min
[32m[20221213 20:17:32 @agent_ppo2.py:145][0m 278528 total steps have happened
[32m[20221213 20:17:32 @agent_ppo2.py:121][0m #------------------------ Iteration 136 --------------------------#
[32m[20221213 20:17:33 @agent_ppo2.py:127][0m Sampling time: 0.55 s by 10 slaves
[32m[20221213 20:17:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:33 @agent_ppo2.py:185][0m |          -0.0014 |           0.0083 |           0.0000 |
[32m[20221213 20:17:34 @agent_ppo2.py:185][0m |          -0.0190 |           0.0079 |           0.0000 |
[32m[20221213 20:17:34 @agent_ppo2.py:185][0m |          -0.0213 |           0.0077 |           0.0000 |
[32m[20221213 20:17:34 @agent_ppo2.py:185][0m |          -0.0246 |           0.0075 |           0.0000 |
[32m[20221213 20:17:34 @agent_ppo2.py:185][0m |          -0.0270 |           0.0073 |           0.0000 |
[32m[20221213 20:17:35 @agent_ppo2.py:185][0m |          -0.0296 |           0.0071 |           0.0000 |
[32m[20221213 20:17:35 @agent_ppo2.py:185][0m |          -0.0305 |           0.0069 |           0.0000 |
[32m[20221213 20:17:35 @agent_ppo2.py:185][0m |          -0.0335 |           0.0068 |           0.0000 |
[32m[20221213 20:17:35 @agent_ppo2.py:185][0m |          -0.0344 |           0.0066 |           0.0000 |
[32m[20221213 20:17:36 @agent_ppo2.py:185][0m |          -0.0353 |           0.0065 |           0.0000 |
[32m[20221213 20:17:36 @agent_ppo2.py:130][0m Policy update time: 2.67 s
[32m[20221213 20:17:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.59
[32m[20221213 20:17:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.13
[32m[20221213 20:17:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.73
[32m[20221213 20:17:36 @agent_ppo2.py:143][0m Total time:       9.16 min
[32m[20221213 20:17:36 @agent_ppo2.py:145][0m 280576 total steps have happened
[32m[20221213 20:17:36 @agent_ppo2.py:121][0m #------------------------ Iteration 137 --------------------------#
[32m[20221213 20:17:37 @agent_ppo2.py:127][0m Sampling time: 0.58 s by 10 slaves
[32m[20221213 20:17:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:37 @agent_ppo2.py:185][0m |          -0.0048 |           0.0179 |           0.0000 |
[32m[20221213 20:17:37 @agent_ppo2.py:185][0m |          -0.0022 |           0.0132 |           0.0000 |
[32m[20221213 20:17:38 @agent_ppo2.py:185][0m |          -0.0096 |           0.0115 |           0.0000 |
[32m[20221213 20:17:38 @agent_ppo2.py:185][0m |          -0.0127 |           0.0113 |           0.0000 |
[32m[20221213 20:17:38 @agent_ppo2.py:185][0m |          -0.0134 |           0.0112 |           0.0000 |
[32m[20221213 20:17:38 @agent_ppo2.py:185][0m |          -0.0153 |           0.0108 |           0.0000 |
[32m[20221213 20:17:39 @agent_ppo2.py:185][0m |          -0.0162 |           0.0106 |           0.0000 |
[32m[20221213 20:17:39 @agent_ppo2.py:185][0m |          -0.0168 |           0.0107 |           0.0000 |
[32m[20221213 20:17:39 @agent_ppo2.py:185][0m |          -0.0171 |           0.0105 |           0.0000 |
[32m[20221213 20:17:39 @agent_ppo2.py:185][0m |          -0.0180 |           0.0103 |           0.0000 |
[32m[20221213 20:17:39 @agent_ppo2.py:130][0m Policy update time: 2.74 s
[32m[20221213 20:17:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.56
[32m[20221213 20:17:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.13
[32m[20221213 20:17:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.69
[32m[20221213 20:17:40 @agent_ppo2.py:143][0m Total time:       9.23 min
[32m[20221213 20:17:40 @agent_ppo2.py:145][0m 282624 total steps have happened
[32m[20221213 20:17:40 @agent_ppo2.py:121][0m #------------------------ Iteration 138 --------------------------#
[32m[20221213 20:17:40 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:17:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:41 @agent_ppo2.py:185][0m |          -0.0020 |           0.0097 |           0.0000 |
[32m[20221213 20:17:41 @agent_ppo2.py:185][0m |          -0.0149 |           0.0089 |           0.0000 |
[32m[20221213 20:17:41 @agent_ppo2.py:185][0m |          -0.0185 |           0.0086 |           0.0000 |
[32m[20221213 20:17:42 @agent_ppo2.py:185][0m |          -0.0214 |           0.0084 |           0.0000 |
[32m[20221213 20:17:42 @agent_ppo2.py:185][0m |          -0.0239 |           0.0083 |           0.0000 |
[32m[20221213 20:17:42 @agent_ppo2.py:185][0m |          -0.0266 |           0.0080 |           0.0000 |
[32m[20221213 20:17:42 @agent_ppo2.py:185][0m |          -0.0248 |           0.0080 |           0.0000 |
[32m[20221213 20:17:43 @agent_ppo2.py:185][0m |          -0.0276 |           0.0080 |           0.0000 |
[32m[20221213 20:17:43 @agent_ppo2.py:185][0m |          -0.0278 |           0.0078 |           0.0000 |
[32m[20221213 20:17:43 @agent_ppo2.py:185][0m |          -0.0280 |           0.0077 |           0.0000 |
[32m[20221213 20:17:43 @agent_ppo2.py:130][0m Policy update time: 2.76 s
[32m[20221213 20:17:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.35
[32m[20221213 20:17:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.51
[32m[20221213 20:17:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.74
[32m[20221213 20:17:44 @agent_ppo2.py:143][0m Total time:       9.29 min
[32m[20221213 20:17:44 @agent_ppo2.py:145][0m 284672 total steps have happened
[32m[20221213 20:17:44 @agent_ppo2.py:121][0m #------------------------ Iteration 139 --------------------------#
[32m[20221213 20:17:44 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:17:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:45 @agent_ppo2.py:185][0m |           0.0018 |           0.0138 |           0.0000 |
[32m[20221213 20:17:45 @agent_ppo2.py:185][0m |          -0.0114 |           0.0111 |           0.0000 |
[32m[20221213 20:17:45 @agent_ppo2.py:185][0m |          -0.0117 |           0.0105 |           0.0000 |
[32m[20221213 20:17:45 @agent_ppo2.py:185][0m |          -0.0156 |           0.0103 |           0.0000 |
[32m[20221213 20:17:46 @agent_ppo2.py:185][0m |          -0.0166 |           0.0104 |           0.0000 |
[32m[20221213 20:17:46 @agent_ppo2.py:185][0m |          -0.0176 |           0.0103 |           0.0000 |
[32m[20221213 20:17:46 @agent_ppo2.py:185][0m |          -0.0177 |           0.0102 |           0.0000 |
[32m[20221213 20:17:46 @agent_ppo2.py:185][0m |          -0.0181 |           0.0100 |           0.0000 |
[32m[20221213 20:17:47 @agent_ppo2.py:185][0m |          -0.0190 |           0.0102 |           0.0000 |
[32m[20221213 20:17:47 @agent_ppo2.py:185][0m |          -0.0195 |           0.0098 |           0.0000 |
[32m[20221213 20:17:47 @agent_ppo2.py:130][0m Policy update time: 2.74 s
[32m[20221213 20:17:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.45
[32m[20221213 20:17:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.42
[32m[20221213 20:17:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.69
[32m[20221213 20:17:47 @agent_ppo2.py:143][0m Total time:       9.35 min
[32m[20221213 20:17:47 @agent_ppo2.py:145][0m 286720 total steps have happened
[32m[20221213 20:17:47 @agent_ppo2.py:121][0m #------------------------ Iteration 140 --------------------------#
[32m[20221213 20:17:48 @agent_ppo2.py:127][0m Sampling time: 0.60 s by 10 slaves
[32m[20221213 20:17:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:49 @agent_ppo2.py:185][0m |           0.0014 |           0.0175 |           0.0000 |
[32m[20221213 20:17:49 @agent_ppo2.py:185][0m |          -0.0104 |           0.0148 |           0.0000 |
[32m[20221213 20:17:49 @agent_ppo2.py:185][0m |          -0.0147 |           0.0142 |           0.0000 |
[32m[20221213 20:17:49 @agent_ppo2.py:185][0m |          -0.0173 |           0.0138 |           0.0000 |
[32m[20221213 20:17:50 @agent_ppo2.py:185][0m |          -0.0180 |           0.0135 |           0.0000 |
[32m[20221213 20:17:50 @agent_ppo2.py:185][0m |          -0.0198 |           0.0134 |           0.0000 |
[32m[20221213 20:17:50 @agent_ppo2.py:185][0m |          -0.0206 |           0.0129 |           0.0000 |
[32m[20221213 20:17:50 @agent_ppo2.py:185][0m |          -0.0209 |           0.0132 |           0.0000 |
[32m[20221213 20:17:51 @agent_ppo2.py:185][0m |          -0.0219 |           0.0127 |           0.0000 |
[32m[20221213 20:17:51 @agent_ppo2.py:185][0m |          -0.0220 |           0.0128 |           0.0000 |
[32m[20221213 20:17:51 @agent_ppo2.py:130][0m Policy update time: 2.79 s
[32m[20221213 20:17:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.44
[32m[20221213 20:17:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.38
[32m[20221213 20:17:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.57
[32m[20221213 20:17:51 @agent_ppo2.py:143][0m Total time:       9.42 min
[32m[20221213 20:17:51 @agent_ppo2.py:145][0m 288768 total steps have happened
[32m[20221213 20:17:51 @agent_ppo2.py:121][0m #------------------------ Iteration 141 --------------------------#
[32m[20221213 20:17:52 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:17:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:52 @agent_ppo2.py:185][0m |          -0.0019 |           0.0137 |           0.0000 |
[32m[20221213 20:17:53 @agent_ppo2.py:185][0m |          -0.0131 |           0.0130 |           0.0000 |
[32m[20221213 20:17:53 @agent_ppo2.py:185][0m |          -0.0173 |           0.0125 |           0.0000 |
[32m[20221213 20:17:53 @agent_ppo2.py:185][0m |          -0.0201 |           0.0118 |           0.0000 |
[32m[20221213 20:17:53 @agent_ppo2.py:185][0m |          -0.0212 |           0.0116 |           0.0000 |
[32m[20221213 20:17:54 @agent_ppo2.py:185][0m |          -0.0284 |           0.0118 |           0.0000 |
[32m[20221213 20:17:54 @agent_ppo2.py:185][0m |          -0.0242 |           0.0113 |           0.0000 |
[32m[20221213 20:17:54 @agent_ppo2.py:185][0m |          -0.0329 |           0.0114 |           0.0000 |
[32m[20221213 20:17:54 @agent_ppo2.py:185][0m |          -0.0257 |           0.0115 |           0.0000 |
[32m[20221213 20:17:55 @agent_ppo2.py:185][0m |          -0.0260 |           0.0110 |           0.0000 |
[32m[20221213 20:17:55 @agent_ppo2.py:130][0m Policy update time: 2.74 s
[32m[20221213 20:17:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.99
[32m[20221213 20:17:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.60
[32m[20221213 20:17:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.91
[32m[20221213 20:17:55 @agent_ppo2.py:143][0m Total time:       9.48 min
[32m[20221213 20:17:55 @agent_ppo2.py:145][0m 290816 total steps have happened
[32m[20221213 20:17:55 @agent_ppo2.py:121][0m #------------------------ Iteration 142 --------------------------#
[32m[20221213 20:17:56 @agent_ppo2.py:127][0m Sampling time: 0.53 s by 10 slaves
[32m[20221213 20:17:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:17:56 @agent_ppo2.py:185][0m |          -0.0026 |           0.0084 |           0.0000 |
[32m[20221213 20:17:57 @agent_ppo2.py:185][0m |          -0.0124 |           0.0065 |           0.0000 |
[32m[20221213 20:17:57 @agent_ppo2.py:185][0m |          -0.0169 |           0.0063 |           0.0000 |
[32m[20221213 20:17:57 @agent_ppo2.py:185][0m |          -0.0182 |           0.0061 |           0.0000 |
[32m[20221213 20:17:57 @agent_ppo2.py:185][0m |          -0.0195 |           0.0059 |           0.0000 |
[32m[20221213 20:17:58 @agent_ppo2.py:185][0m |          -0.0211 |           0.0058 |           0.0000 |
[32m[20221213 20:17:58 @agent_ppo2.py:185][0m |          -0.0225 |           0.0056 |           0.0000 |
[32m[20221213 20:17:58 @agent_ppo2.py:185][0m |          -0.0225 |           0.0055 |           0.0000 |
[32m[20221213 20:17:58 @agent_ppo2.py:185][0m |          -0.0232 |           0.0053 |           0.0000 |
[32m[20221213 20:17:59 @agent_ppo2.py:185][0m |          -0.0247 |           0.0052 |           0.0000 |
[32m[20221213 20:17:59 @agent_ppo2.py:130][0m Policy update time: 2.90 s
[32m[20221213 20:17:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.16
[32m[20221213 20:17:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.97
[32m[20221213 20:17:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.23
[32m[20221213 20:17:59 @agent_ppo2.py:143][0m Total time:       9.55 min
[32m[20221213 20:17:59 @agent_ppo2.py:145][0m 292864 total steps have happened
[32m[20221213 20:17:59 @agent_ppo2.py:121][0m #------------------------ Iteration 143 --------------------------#
[32m[20221213 20:18:00 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:18:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:18:00 @agent_ppo2.py:185][0m |           0.0031 |           0.0781 |           0.0000 |
[32m[20221213 20:18:00 @agent_ppo2.py:185][0m |          -0.0122 |           0.0298 |           0.0000 |
[32m[20221213 20:18:01 @agent_ppo2.py:185][0m |          -0.0138 |           0.0242 |           0.0000 |
[32m[20221213 20:18:01 @agent_ppo2.py:185][0m |          -0.0145 |           0.0214 |           0.0000 |
[32m[20221213 20:18:01 @agent_ppo2.py:185][0m |          -0.0205 |           0.0196 |           0.0000 |
[32m[20221213 20:18:02 @agent_ppo2.py:185][0m |          -0.0166 |           0.0187 |           0.0000 |
[32m[20221213 20:18:02 @agent_ppo2.py:185][0m |          -0.0179 |           0.0185 |           0.0000 |
[32m[20221213 20:18:02 @agent_ppo2.py:185][0m |          -0.0175 |           0.0175 |           0.0000 |
[32m[20221213 20:18:02 @agent_ppo2.py:185][0m |          -0.0185 |           0.0173 |           0.0000 |
[32m[20221213 20:18:03 @agent_ppo2.py:185][0m |          -0.0195 |           0.0173 |           0.0000 |
[32m[20221213 20:18:03 @agent_ppo2.py:130][0m Policy update time: 2.93 s
[32m[20221213 20:18:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.52
[32m[20221213 20:18:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.04
[32m[20221213 20:18:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.68
[32m[20221213 20:18:03 @agent_ppo2.py:143][0m Total time:       9.61 min
[32m[20221213 20:18:03 @agent_ppo2.py:145][0m 294912 total steps have happened
[32m[20221213 20:18:03 @agent_ppo2.py:121][0m #------------------------ Iteration 144 --------------------------#
[32m[20221213 20:18:04 @agent_ppo2.py:127][0m Sampling time: 0.57 s by 10 slaves
[32m[20221213 20:18:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:18:04 @agent_ppo2.py:185][0m |          -0.0019 |           0.0481 |           0.0000 |
[32m[20221213 20:18:05 @agent_ppo2.py:185][0m |          -0.0119 |           0.0344 |           0.0000 |
[32m[20221213 20:18:05 @agent_ppo2.py:185][0m |          -0.0164 |           0.0293 |           0.0000 |
[32m[20221213 20:18:05 @agent_ppo2.py:185][0m |          -0.0204 |           0.0272 |           0.0000 |
[32m[20221213 20:18:05 @agent_ppo2.py:185][0m |          -0.0201 |           0.0263 |           0.0000 |
[32m[20221213 20:18:06 @agent_ppo2.py:185][0m |          -0.0220 |           0.0256 |           0.0000 |
[32m[20221213 20:18:06 @agent_ppo2.py:185][0m |          -0.0225 |           0.0246 |           0.0000 |
[32m[20221213 20:18:06 @agent_ppo2.py:185][0m |          -0.0240 |           0.0242 |           0.0000 |
[32m[20221213 20:18:07 @agent_ppo2.py:185][0m |          -0.0245 |           0.0243 |           0.0000 |
[32m[20221213 20:18:07 @agent_ppo2.py:185][0m |          -0.0238 |           0.0230 |           0.0000 |
[32m[20221213 20:18:07 @agent_ppo2.py:130][0m Policy update time: 3.17 s
[32m[20221213 20:18:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.18
[32m[20221213 20:18:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.96
[32m[20221213 20:18:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.59
[32m[20221213 20:18:07 @agent_ppo2.py:143][0m Total time:       9.69 min
[32m[20221213 20:18:07 @agent_ppo2.py:145][0m 296960 total steps have happened
[32m[20221213 20:18:07 @agent_ppo2.py:121][0m #------------------------ Iteration 145 --------------------------#
[32m[20221213 20:18:08 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:18:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:18:08 @agent_ppo2.py:185][0m |           0.0003 |           0.0581 |           0.0000 |
[32m[20221213 20:18:09 @agent_ppo2.py:185][0m |          -0.0144 |           0.0385 |           0.0000 |
[32m[20221213 20:18:09 @agent_ppo2.py:185][0m |          -0.0189 |           0.0344 |           0.0000 |
[32m[20221213 20:18:09 @agent_ppo2.py:185][0m |          -0.0221 |           0.0324 |           0.0000 |
[32m[20221213 20:18:10 @agent_ppo2.py:185][0m |          -0.0251 |           0.0302 |           0.0000 |
[32m[20221213 20:18:10 @agent_ppo2.py:185][0m |          -0.0268 |           0.0298 |           0.0000 |
[32m[20221213 20:18:10 @agent_ppo2.py:185][0m |          -0.0277 |           0.0285 |           0.0000 |
[32m[20221213 20:18:11 @agent_ppo2.py:185][0m |          -0.0279 |           0.0278 |           0.0000 |
[32m[20221213 20:18:11 @agent_ppo2.py:185][0m |          -0.0294 |           0.0278 |           0.0000 |
[32m[20221213 20:18:11 @agent_ppo2.py:185][0m |          -0.0278 |           0.0272 |           0.0000 |
[32m[20221213 20:18:11 @agent_ppo2.py:130][0m Policy update time: 3.12 s
[32m[20221213 20:18:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.54
[32m[20221213 20:18:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.50
[32m[20221213 20:18:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.60
[32m[20221213 20:18:12 @agent_ppo2.py:143][0m Total time:       9.76 min
[32m[20221213 20:18:12 @agent_ppo2.py:145][0m 299008 total steps have happened
[32m[20221213 20:18:12 @agent_ppo2.py:121][0m #------------------------ Iteration 146 --------------------------#
[32m[20221213 20:18:12 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:18:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:18:13 @agent_ppo2.py:185][0m |           0.0009 |           0.0261 |           0.0000 |
[32m[20221213 20:18:13 @agent_ppo2.py:185][0m |          -0.0124 |           0.0184 |           0.0000 |
[32m[20221213 20:18:13 @agent_ppo2.py:185][0m |          -0.0185 |           0.0178 |           0.0000 |
[32m[20221213 20:18:14 @agent_ppo2.py:185][0m |          -0.0197 |           0.0174 |           0.0000 |
[32m[20221213 20:18:14 @agent_ppo2.py:185][0m |          -0.0227 |           0.0169 |           0.0000 |
[32m[20221213 20:18:14 @agent_ppo2.py:185][0m |          -0.0231 |           0.0168 |           0.0000 |
[32m[20221213 20:18:14 @agent_ppo2.py:185][0m |          -0.0261 |           0.0164 |           0.0000 |
[32m[20221213 20:18:15 @agent_ppo2.py:185][0m |          -0.0262 |           0.0165 |           0.0000 |
[32m[20221213 20:18:15 @agent_ppo2.py:185][0m |          -0.0281 |           0.0162 |           0.0000 |
[32m[20221213 20:18:15 @agent_ppo2.py:185][0m |          -0.0283 |           0.0158 |           0.0000 |
[32m[20221213 20:18:15 @agent_ppo2.py:130][0m Policy update time: 3.12 s
[32m[20221213 20:18:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.70
[32m[20221213 20:18:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.06
[32m[20221213 20:18:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.73
[32m[20221213 20:18:16 @agent_ppo2.py:143][0m Total time:       9.83 min
[32m[20221213 20:18:16 @agent_ppo2.py:145][0m 301056 total steps have happened
[32m[20221213 20:18:16 @agent_ppo2.py:121][0m #------------------------ Iteration 147 --------------------------#
[32m[20221213 20:18:17 @agent_ppo2.py:127][0m Sampling time: 0.64 s by 10 slaves
[32m[20221213 20:18:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:18:17 @agent_ppo2.py:185][0m |          -0.0002 |           0.0330 |           0.0000 |
[32m[20221213 20:18:17 @agent_ppo2.py:185][0m |          -0.0071 |           0.0260 |           0.0000 |
[32m[20221213 20:18:18 @agent_ppo2.py:185][0m |          -0.0126 |           0.0236 |           0.0000 |
[32m[20221213 20:18:18 @agent_ppo2.py:185][0m |          -0.0147 |           0.0223 |           0.0000 |
[32m[20221213 20:18:18 @agent_ppo2.py:185][0m |          -0.0161 |           0.0219 |           0.0000 |
[32m[20221213 20:18:19 @agent_ppo2.py:185][0m |          -0.0172 |           0.0207 |           0.0000 |
[32m[20221213 20:18:19 @agent_ppo2.py:185][0m |          -0.0179 |           0.0206 |           0.0000 |
[32m[20221213 20:18:19 @agent_ppo2.py:185][0m |          -0.0184 |           0.0202 |           0.0000 |
[32m[20221213 20:18:20 @agent_ppo2.py:185][0m |          -0.0267 |           0.0208 |           0.0000 |
[32m[20221213 20:18:20 @agent_ppo2.py:185][0m |          -0.0244 |           0.0212 |           0.0000 |
[32m[20221213 20:18:20 @agent_ppo2.py:130][0m Policy update time: 3.59 s
[32m[20221213 20:18:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.97
[32m[20221213 20:18:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.74
[32m[20221213 20:18:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.53
[32m[20221213 20:18:21 @agent_ppo2.py:143][0m Total time:       9.91 min
[32m[20221213 20:18:21 @agent_ppo2.py:145][0m 303104 total steps have happened
[32m[20221213 20:18:21 @agent_ppo2.py:121][0m #------------------------ Iteration 148 --------------------------#
[32m[20221213 20:18:21 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:18:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:18:22 @agent_ppo2.py:185][0m |           0.0004 |           0.0344 |           0.0000 |
[32m[20221213 20:18:22 @agent_ppo2.py:185][0m |          -0.0148 |           0.0280 |           0.0000 |
[32m[20221213 20:18:22 @agent_ppo2.py:185][0m |          -0.0197 |           0.0249 |           0.0000 |
[32m[20221213 20:18:23 @agent_ppo2.py:185][0m |          -0.0222 |           0.0241 |           0.0000 |
[32m[20221213 20:18:23 @agent_ppo2.py:185][0m |          -0.0247 |           0.0229 |           0.0000 |
[32m[20221213 20:18:23 @agent_ppo2.py:185][0m |          -0.0261 |           0.0222 |           0.0000 |
[32m[20221213 20:18:24 @agent_ppo2.py:185][0m |          -0.0304 |           0.0218 |           0.0000 |
[32m[20221213 20:18:24 @agent_ppo2.py:185][0m |          -0.0347 |           0.0216 |           0.0000 |
[32m[20221213 20:18:24 @agent_ppo2.py:185][0m |          -0.0300 |           0.0213 |           0.0000 |
[32m[20221213 20:18:24 @agent_ppo2.py:185][0m |          -0.0314 |           0.0208 |           0.0000 |
[32m[20221213 20:18:24 @agent_ppo2.py:130][0m Policy update time: 3.06 s
[32m[20221213 20:18:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.59
[32m[20221213 20:18:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.10
[32m[20221213 20:18:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.96
[32m[20221213 20:18:25 @agent_ppo2.py:143][0m Total time:       9.98 min
[32m[20221213 20:18:25 @agent_ppo2.py:145][0m 305152 total steps have happened
[32m[20221213 20:18:25 @agent_ppo2.py:121][0m #------------------------ Iteration 149 --------------------------#
[32m[20221213 20:18:25 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:18:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:18:26 @agent_ppo2.py:185][0m |          -0.0002 |           0.0219 |           0.0000 |
[32m[20221213 20:18:26 @agent_ppo2.py:185][0m |          -0.0143 |           0.0175 |           0.0000 |
[32m[20221213 20:18:26 @agent_ppo2.py:185][0m |          -0.0181 |           0.0166 |           0.0000 |
[32m[20221213 20:18:27 @agent_ppo2.py:185][0m |          -0.0208 |           0.0160 |           0.0000 |
[32m[20221213 20:18:27 @agent_ppo2.py:185][0m |          -0.0229 |           0.0158 |           0.0000 |
[32m[20221213 20:18:27 @agent_ppo2.py:185][0m |          -0.0244 |           0.0154 |           0.0000 |
[32m[20221213 20:18:28 @agent_ppo2.py:185][0m |          -0.0252 |           0.0153 |           0.0000 |
[32m[20221213 20:18:28 @agent_ppo2.py:185][0m |          -0.0277 |           0.0150 |           0.0000 |
[32m[20221213 20:18:28 @agent_ppo2.py:185][0m |          -0.0280 |           0.0147 |           0.0000 |
[32m[20221213 20:18:29 @agent_ppo2.py:185][0m |          -0.0293 |           0.0148 |           0.0000 |
[32m[20221213 20:18:29 @agent_ppo2.py:130][0m Policy update time: 3.48 s
[32m[20221213 20:18:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.63
[32m[20221213 20:18:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.84
[32m[20221213 20:18:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.60
[32m[20221213 20:18:29 @agent_ppo2.py:143][0m Total time:      10.05 min
[32m[20221213 20:18:29 @agent_ppo2.py:145][0m 307200 total steps have happened
[32m[20221213 20:18:30 @agent_ppo2.py:121][0m #------------------------ Iteration 150 --------------------------#
[32m[20221213 20:18:30 @agent_ppo2.py:127][0m Sampling time: 0.58 s by 10 slaves
[32m[20221213 20:18:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:18:31 @agent_ppo2.py:185][0m |           0.0005 |           0.0163 |           0.0000 |
[32m[20221213 20:18:31 @agent_ppo2.py:185][0m |          -0.0148 |           0.0162 |           0.0000 |
[32m[20221213 20:18:31 @agent_ppo2.py:185][0m |          -0.0177 |           0.0153 |           0.0000 |
[32m[20221213 20:18:32 @agent_ppo2.py:185][0m |          -0.0178 |           0.0139 |           0.0000 |
[32m[20221213 20:18:32 @agent_ppo2.py:185][0m |          -0.0217 |           0.0137 |           0.0000 |
[32m[20221213 20:18:32 @agent_ppo2.py:185][0m |          -0.0230 |           0.0133 |           0.0000 |
[32m[20221213 20:18:32 @agent_ppo2.py:185][0m |          -0.0254 |           0.0131 |           0.0000 |
[32m[20221213 20:18:33 @agent_ppo2.py:185][0m |          -0.0259 |           0.0128 |           0.0000 |
[32m[20221213 20:18:33 @agent_ppo2.py:185][0m |          -0.0270 |           0.0126 |           0.0000 |
[32m[20221213 20:18:33 @agent_ppo2.py:185][0m |          -0.0281 |           0.0127 |           0.0000 |
[32m[20221213 20:18:33 @agent_ppo2.py:130][0m Policy update time: 3.31 s
[32m[20221213 20:18:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.84
[32m[20221213 20:18:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.03
[32m[20221213 20:18:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.83
[32m[20221213 20:18:34 @agent_ppo2.py:143][0m Total time:      10.13 min
[32m[20221213 20:18:34 @agent_ppo2.py:145][0m 309248 total steps have happened
[32m[20221213 20:18:34 @agent_ppo2.py:121][0m #------------------------ Iteration 151 --------------------------#
[32m[20221213 20:18:35 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:18:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:18:35 @agent_ppo2.py:185][0m |           0.0034 |           0.0494 |           0.0000 |
[32m[20221213 20:18:35 @agent_ppo2.py:185][0m |          -0.0091 |           0.0243 |           0.0000 |
[32m[20221213 20:18:36 @agent_ppo2.py:185][0m |          -0.0125 |           0.0220 |           0.0000 |
[32m[20221213 20:18:36 @agent_ppo2.py:185][0m |          -0.0146 |           0.0216 |           0.0000 |
[32m[20221213 20:18:36 @agent_ppo2.py:185][0m |          -0.0167 |           0.0203 |           0.0000 |
[32m[20221213 20:18:37 @agent_ppo2.py:185][0m |          -0.0175 |           0.0203 |           0.0000 |
[32m[20221213 20:18:37 @agent_ppo2.py:185][0m |          -0.0172 |           0.0198 |           0.0000 |
[32m[20221213 20:18:37 @agent_ppo2.py:185][0m |          -0.0182 |           0.0194 |           0.0000 |
[32m[20221213 20:18:38 @agent_ppo2.py:185][0m |          -0.0215 |           0.0194 |           0.0000 |
[32m[20221213 20:18:38 @agent_ppo2.py:185][0m |          -0.0189 |           0.0192 |           0.0000 |
[32m[20221213 20:18:38 @agent_ppo2.py:130][0m Policy update time: 3.50 s
[32m[20221213 20:18:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.17
[32m[20221213 20:18:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.91
[32m[20221213 20:18:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.20
[32m[20221213 20:18:39 @agent_ppo2.py:143][0m Total time:      10.21 min
[32m[20221213 20:18:39 @agent_ppo2.py:145][0m 311296 total steps have happened
[32m[20221213 20:18:39 @agent_ppo2.py:121][0m #------------------------ Iteration 152 --------------------------#
[32m[20221213 20:18:39 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:18:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:18:40 @agent_ppo2.py:185][0m |           0.0001 |           0.0274 |           0.0000 |
[32m[20221213 20:18:40 @agent_ppo2.py:185][0m |          -0.0144 |           0.0220 |           0.0000 |
[32m[20221213 20:18:40 @agent_ppo2.py:185][0m |          -0.0190 |           0.0211 |           0.0000 |
[32m[20221213 20:18:40 @agent_ppo2.py:185][0m |          -0.0214 |           0.0195 |           0.0000 |
[32m[20221213 20:18:41 @agent_ppo2.py:185][0m |          -0.0250 |           0.0187 |           0.0000 |
[32m[20221213 20:18:41 @agent_ppo2.py:185][0m |          -0.0277 |           0.0184 |           0.0000 |
[32m[20221213 20:18:41 @agent_ppo2.py:185][0m |          -0.0276 |           0.0178 |           0.0000 |
[32m[20221213 20:18:42 @agent_ppo2.py:185][0m |          -0.0291 |           0.0176 |           0.0000 |
[32m[20221213 20:18:42 @agent_ppo2.py:185][0m |          -0.0305 |           0.0174 |           0.0000 |
[32m[20221213 20:18:42 @agent_ppo2.py:185][0m |          -0.0314 |           0.0171 |           0.0000 |
[32m[20221213 20:18:42 @agent_ppo2.py:130][0m Policy update time: 3.03 s
[32m[20221213 20:18:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.38
[32m[20221213 20:18:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.14
[32m[20221213 20:18:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.37
[32m[20221213 20:18:43 @agent_ppo2.py:143][0m Total time:      10.27 min
[32m[20221213 20:18:43 @agent_ppo2.py:145][0m 313344 total steps have happened
[32m[20221213 20:18:43 @agent_ppo2.py:121][0m #------------------------ Iteration 153 --------------------------#
[32m[20221213 20:18:43 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:18:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:18:44 @agent_ppo2.py:185][0m |           0.0012 |           0.0274 |           0.0000 |
[32m[20221213 20:18:44 @agent_ppo2.py:185][0m |          -0.0129 |           0.0224 |           0.0000 |
[32m[20221213 20:18:44 @agent_ppo2.py:185][0m |          -0.0171 |           0.0211 |           0.0000 |
[32m[20221213 20:18:45 @agent_ppo2.py:185][0m |          -0.0199 |           0.0198 |           0.0000 |
[32m[20221213 20:18:45 @agent_ppo2.py:185][0m |          -0.0261 |           0.0190 |           0.0000 |
[32m[20221213 20:18:45 @agent_ppo2.py:185][0m |          -0.0244 |           0.0188 |           0.0000 |
[32m[20221213 20:18:45 @agent_ppo2.py:185][0m |          -0.0231 |           0.0186 |           0.0000 |
[32m[20221213 20:18:46 @agent_ppo2.py:185][0m |          -0.0254 |           0.0180 |           0.0000 |
[32m[20221213 20:18:46 @agent_ppo2.py:185][0m |          -0.0255 |           0.0179 |           0.0000 |
[32m[20221213 20:18:46 @agent_ppo2.py:185][0m |          -0.0262 |           0.0178 |           0.0000 |
[32m[20221213 20:18:46 @agent_ppo2.py:130][0m Policy update time: 3.01 s
[32m[20221213 20:18:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.43
[32m[20221213 20:18:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.84
[32m[20221213 20:18:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.89
[32m[20221213 20:18:47 @agent_ppo2.py:143][0m Total time:      10.34 min
[32m[20221213 20:18:47 @agent_ppo2.py:145][0m 315392 total steps have happened
[32m[20221213 20:18:47 @agent_ppo2.py:121][0m #------------------------ Iteration 154 --------------------------#
[32m[20221213 20:18:47 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:18:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:18:48 @agent_ppo2.py:185][0m |           0.0010 |           0.0190 |           0.0000 |
[32m[20221213 20:18:48 @agent_ppo2.py:185][0m |          -0.0131 |           0.0168 |           0.0000 |
[32m[20221213 20:18:48 @agent_ppo2.py:185][0m |          -0.0176 |           0.0159 |           0.0000 |
[32m[20221213 20:18:49 @agent_ppo2.py:185][0m |          -0.0204 |           0.0151 |           0.0000 |
[32m[20221213 20:18:49 @agent_ppo2.py:185][0m |          -0.0221 |           0.0154 |           0.0000 |
[32m[20221213 20:18:49 @agent_ppo2.py:185][0m |          -0.0240 |           0.0148 |           0.0000 |
[32m[20221213 20:18:49 @agent_ppo2.py:185][0m |          -0.0256 |           0.0149 |           0.0000 |
[32m[20221213 20:18:50 @agent_ppo2.py:185][0m |          -0.0237 |           0.0149 |           0.0000 |
[32m[20221213 20:18:50 @agent_ppo2.py:185][0m |          -0.0268 |           0.0147 |           0.0000 |
[32m[20221213 20:18:50 @agent_ppo2.py:185][0m |          -0.0282 |           0.0144 |           0.0000 |
[32m[20221213 20:18:50 @agent_ppo2.py:130][0m Policy update time: 2.97 s
[32m[20221213 20:18:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.55
[32m[20221213 20:18:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.72
[32m[20221213 20:18:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.65
[32m[20221213 20:18:51 @agent_ppo2.py:143][0m Total time:      10.41 min
[32m[20221213 20:18:51 @agent_ppo2.py:145][0m 317440 total steps have happened
[32m[20221213 20:18:51 @agent_ppo2.py:121][0m #------------------------ Iteration 155 --------------------------#
[32m[20221213 20:18:51 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:18:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:18:52 @agent_ppo2.py:185][0m |           0.0028 |           0.0133 |           0.0000 |
[32m[20221213 20:18:52 @agent_ppo2.py:185][0m |          -0.0129 |           0.0124 |           0.0000 |
[32m[20221213 20:18:52 @agent_ppo2.py:185][0m |          -0.0187 |           0.0118 |           0.0000 |
[32m[20221213 20:18:53 @agent_ppo2.py:185][0m |          -0.0201 |           0.0114 |           0.0000 |
[32m[20221213 20:18:53 @agent_ppo2.py:185][0m |          -0.0178 |           0.0114 |           0.0000 |
[32m[20221213 20:18:53 @agent_ppo2.py:185][0m |          -0.0197 |           0.0115 |           0.0000 |
[32m[20221213 20:18:53 @agent_ppo2.py:185][0m |          -0.0248 |           0.0109 |           0.0000 |
[32m[20221213 20:18:54 @agent_ppo2.py:185][0m |          -0.0245 |           0.0108 |           0.0000 |
[32m[20221213 20:18:54 @agent_ppo2.py:185][0m |          -0.0262 |           0.0106 |           0.0000 |
[32m[20221213 20:18:54 @agent_ppo2.py:185][0m |          -0.0279 |           0.0104 |           0.0000 |
[32m[20221213 20:18:54 @agent_ppo2.py:130][0m Policy update time: 3.02 s
[32m[20221213 20:18:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.11
[32m[20221213 20:18:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.30
[32m[20221213 20:18:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.97
[32m[20221213 20:18:55 @agent_ppo2.py:143][0m Total time:      10.47 min
[32m[20221213 20:18:55 @agent_ppo2.py:145][0m 319488 total steps have happened
[32m[20221213 20:18:55 @agent_ppo2.py:121][0m #------------------------ Iteration 156 --------------------------#
[32m[20221213 20:18:55 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:18:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:18:56 @agent_ppo2.py:185][0m |           0.0013 |           0.0124 |           0.0000 |
[32m[20221213 20:18:56 @agent_ppo2.py:185][0m |          -0.0142 |           0.0113 |           0.0000 |
[32m[20221213 20:18:56 @agent_ppo2.py:185][0m |          -0.0175 |           0.0108 |           0.0000 |
[32m[20221213 20:18:57 @agent_ppo2.py:185][0m |          -0.0207 |           0.0106 |           0.0000 |
[32m[20221213 20:18:57 @agent_ppo2.py:185][0m |          -0.0232 |           0.0105 |           0.0000 |
[32m[20221213 20:18:57 @agent_ppo2.py:185][0m |          -0.0237 |           0.0101 |           0.0000 |
[32m[20221213 20:18:58 @agent_ppo2.py:185][0m |          -0.0248 |           0.0102 |           0.0000 |
[32m[20221213 20:18:58 @agent_ppo2.py:185][0m |          -0.0261 |           0.0099 |           0.0000 |
[32m[20221213 20:18:59 @agent_ppo2.py:185][0m |          -0.0263 |           0.0098 |           0.0000 |
[32m[20221213 20:18:59 @agent_ppo2.py:185][0m |          -0.0268 |           0.0099 |           0.0000 |
[32m[20221213 20:18:59 @agent_ppo2.py:130][0m Policy update time: 3.61 s
[32m[20221213 20:18:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.42
[32m[20221213 20:18:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.41
[32m[20221213 20:18:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.35
[32m[20221213 20:18:59 @agent_ppo2.py:143][0m Total time:      10.55 min
[32m[20221213 20:18:59 @agent_ppo2.py:145][0m 321536 total steps have happened
[32m[20221213 20:18:59 @agent_ppo2.py:121][0m #------------------------ Iteration 157 --------------------------#
[32m[20221213 20:19:00 @agent_ppo2.py:127][0m Sampling time: 0.58 s by 10 slaves
[32m[20221213 20:19:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:19:01 @agent_ppo2.py:185][0m |          -0.0022 |           0.0229 |           0.0000 |
[32m[20221213 20:19:01 @agent_ppo2.py:185][0m |          -0.0152 |           0.0172 |           0.0000 |
[32m[20221213 20:19:01 @agent_ppo2.py:185][0m |          -0.0157 |           0.0158 |           0.0000 |
[32m[20221213 20:19:02 @agent_ppo2.py:185][0m |          -0.0155 |           0.0147 |           0.0000 |
[32m[20221213 20:19:02 @agent_ppo2.py:185][0m |          -0.0166 |           0.0147 |           0.0000 |
[32m[20221213 20:19:02 @agent_ppo2.py:185][0m |          -0.0175 |           0.0143 |           0.0000 |
[32m[20221213 20:19:02 @agent_ppo2.py:185][0m |          -0.0182 |           0.0140 |           0.0000 |
[32m[20221213 20:19:03 @agent_ppo2.py:185][0m |          -0.0188 |           0.0140 |           0.0000 |
[32m[20221213 20:19:03 @agent_ppo2.py:185][0m |          -0.0194 |           0.0140 |           0.0000 |
[32m[20221213 20:19:04 @agent_ppo2.py:185][0m |          -0.0193 |           0.0138 |           0.0000 |
[32m[20221213 20:19:04 @agent_ppo2.py:130][0m Policy update time: 3.63 s
[32m[20221213 20:19:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.60
[32m[20221213 20:19:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.97
[32m[20221213 20:19:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.20
[32m[20221213 20:19:04 @agent_ppo2.py:143][0m Total time:      10.63 min
[32m[20221213 20:19:04 @agent_ppo2.py:145][0m 323584 total steps have happened
[32m[20221213 20:19:04 @agent_ppo2.py:121][0m #------------------------ Iteration 158 --------------------------#
[32m[20221213 20:19:05 @agent_ppo2.py:127][0m Sampling time: 0.55 s by 10 slaves
[32m[20221213 20:19:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:19:05 @agent_ppo2.py:185][0m |           0.0045 |           0.0320 |           0.0000 |
[32m[20221213 20:19:06 @agent_ppo2.py:185][0m |          -0.0118 |           0.0202 |           0.0000 |
[32m[20221213 20:19:06 @agent_ppo2.py:185][0m |          -0.0142 |           0.0186 |           0.0000 |
[32m[20221213 20:19:06 @agent_ppo2.py:185][0m |          -0.0148 |           0.0176 |           0.0000 |
[32m[20221213 20:19:07 @agent_ppo2.py:185][0m |          -0.0168 |           0.0169 |           0.0000 |
[32m[20221213 20:19:07 @agent_ppo2.py:185][0m |          -0.0184 |           0.0165 |           0.0000 |
[32m[20221213 20:19:07 @agent_ppo2.py:185][0m |          -0.0194 |           0.0165 |           0.0000 |
[32m[20221213 20:19:08 @agent_ppo2.py:185][0m |          -0.0202 |           0.0157 |           0.0000 |
[32m[20221213 20:19:08 @agent_ppo2.py:185][0m |          -0.0207 |           0.0157 |           0.0000 |
[32m[20221213 20:19:09 @agent_ppo2.py:185][0m |          -0.0219 |           0.0156 |           0.0000 |
[32m[20221213 20:19:09 @agent_ppo2.py:130][0m Policy update time: 3.81 s
[32m[20221213 20:19:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.14
[32m[20221213 20:19:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.27
[32m[20221213 20:19:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.07
[32m[20221213 20:19:09 @agent_ppo2.py:143][0m Total time:      10.71 min
[32m[20221213 20:19:09 @agent_ppo2.py:145][0m 325632 total steps have happened
[32m[20221213 20:19:09 @agent_ppo2.py:121][0m #------------------------ Iteration 159 --------------------------#
[32m[20221213 20:19:10 @agent_ppo2.py:127][0m Sampling time: 0.60 s by 10 slaves
[32m[20221213 20:19:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:19:10 @agent_ppo2.py:185][0m |           0.0036 |           0.0394 |           0.0000 |
[32m[20221213 20:19:11 @agent_ppo2.py:185][0m |          -0.0076 |           0.0283 |           0.0000 |
[32m[20221213 20:19:11 @agent_ppo2.py:185][0m |          -0.0121 |           0.0247 |           0.0000 |
[32m[20221213 20:19:11 @agent_ppo2.py:185][0m |          -0.0132 |           0.0233 |           0.0000 |
[32m[20221213 20:19:12 @agent_ppo2.py:185][0m |          -0.0163 |           0.0224 |           0.0000 |
[32m[20221213 20:19:12 @agent_ppo2.py:185][0m |          -0.0181 |           0.0215 |           0.0000 |
[32m[20221213 20:19:13 @agent_ppo2.py:185][0m |          -0.0192 |           0.0210 |           0.0000 |
[32m[20221213 20:19:13 @agent_ppo2.py:185][0m |          -0.0200 |           0.0210 |           0.0000 |
[32m[20221213 20:19:13 @agent_ppo2.py:185][0m |          -0.0209 |           0.0202 |           0.0000 |
[32m[20221213 20:19:14 @agent_ppo2.py:185][0m |          -0.0227 |           0.0202 |           0.0000 |
[32m[20221213 20:19:14 @agent_ppo2.py:130][0m Policy update time: 3.91 s
[32m[20221213 20:19:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.49
[32m[20221213 20:19:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.44
[32m[20221213 20:19:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.66
[32m[20221213 20:19:14 @agent_ppo2.py:143][0m Total time:      10.80 min
[32m[20221213 20:19:14 @agent_ppo2.py:145][0m 327680 total steps have happened
[32m[20221213 20:19:14 @agent_ppo2.py:121][0m #------------------------ Iteration 160 --------------------------#
[32m[20221213 20:19:15 @agent_ppo2.py:127][0m Sampling time: 0.73 s by 10 slaves
[32m[20221213 20:19:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:19:16 @agent_ppo2.py:185][0m |           0.0016 |           0.0216 |           0.0000 |
[32m[20221213 20:19:16 @agent_ppo2.py:185][0m |          -0.0141 |           0.0147 |           0.0000 |
[32m[20221213 20:19:16 @agent_ppo2.py:185][0m |          -0.0164 |           0.0146 |           0.0000 |
[32m[20221213 20:19:16 @agent_ppo2.py:185][0m |          -0.0209 |           0.0141 |           0.0000 |
[32m[20221213 20:19:17 @agent_ppo2.py:185][0m |          -0.0231 |           0.0139 |           0.0000 |
[32m[20221213 20:19:17 @agent_ppo2.py:185][0m |          -0.0247 |           0.0138 |           0.0000 |
[32m[20221213 20:19:17 @agent_ppo2.py:185][0m |          -0.0261 |           0.0136 |           0.0000 |
[32m[20221213 20:19:18 @agent_ppo2.py:185][0m |          -0.0265 |           0.0133 |           0.0000 |
[32m[20221213 20:19:18 @agent_ppo2.py:185][0m |          -0.0277 |           0.0132 |           0.0000 |
[32m[20221213 20:19:18 @agent_ppo2.py:185][0m |          -0.0281 |           0.0131 |           0.0000 |
[32m[20221213 20:19:18 @agent_ppo2.py:130][0m Policy update time: 3.17 s
[32m[20221213 20:19:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.56
[32m[20221213 20:19:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.95
[32m[20221213 20:19:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.92
[32m[20221213 20:19:19 @agent_ppo2.py:143][0m Total time:      10.87 min
[32m[20221213 20:19:19 @agent_ppo2.py:145][0m 329728 total steps have happened
[32m[20221213 20:19:19 @agent_ppo2.py:121][0m #------------------------ Iteration 161 --------------------------#
[32m[20221213 20:19:19 @agent_ppo2.py:127][0m Sampling time: 0.58 s by 10 slaves
[32m[20221213 20:19:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:19:20 @agent_ppo2.py:185][0m |           0.0012 |           0.0264 |           0.0000 |
[32m[20221213 20:19:20 @agent_ppo2.py:185][0m |          -0.0111 |           0.0194 |           0.0000 |
[32m[20221213 20:19:20 @agent_ppo2.py:185][0m |          -0.0148 |           0.0178 |           0.0000 |
[32m[20221213 20:19:21 @agent_ppo2.py:185][0m |          -0.0179 |           0.0171 |           0.0000 |
[32m[20221213 20:19:21 @agent_ppo2.py:185][0m |          -0.0189 |           0.0164 |           0.0000 |
[32m[20221213 20:19:21 @agent_ppo2.py:185][0m |          -0.0202 |           0.0159 |           0.0000 |
[32m[20221213 20:19:21 @agent_ppo2.py:185][0m |          -0.0204 |           0.0155 |           0.0000 |
[32m[20221213 20:19:22 @agent_ppo2.py:185][0m |          -0.0217 |           0.0157 |           0.0000 |
[32m[20221213 20:19:22 @agent_ppo2.py:185][0m |          -0.0223 |           0.0158 |           0.0000 |
[32m[20221213 20:19:22 @agent_ppo2.py:185][0m |          -0.0227 |           0.0151 |           0.0000 |
[32m[20221213 20:19:22 @agent_ppo2.py:130][0m Policy update time: 2.87 s
[32m[20221213 20:19:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.29
[32m[20221213 20:19:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.11
[32m[20221213 20:19:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.26
[32m[20221213 20:19:23 @agent_ppo2.py:143][0m Total time:      10.94 min
[32m[20221213 20:19:23 @agent_ppo2.py:145][0m 331776 total steps have happened
[32m[20221213 20:19:23 @agent_ppo2.py:121][0m #------------------------ Iteration 162 --------------------------#
[32m[20221213 20:19:23 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:19:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:19:24 @agent_ppo2.py:185][0m |           0.0056 |           0.0159 |           0.0000 |
[32m[20221213 20:19:24 @agent_ppo2.py:185][0m |          -0.0124 |           0.0126 |           0.0000 |
[32m[20221213 20:19:24 @agent_ppo2.py:185][0m |          -0.0178 |           0.0122 |           0.0000 |
[32m[20221213 20:19:24 @agent_ppo2.py:185][0m |          -0.0168 |           0.0120 |           0.0000 |
[32m[20221213 20:19:25 @agent_ppo2.py:185][0m |          -0.0211 |           0.0118 |           0.0000 |
[32m[20221213 20:19:25 @agent_ppo2.py:185][0m |          -0.0204 |           0.0116 |           0.0000 |
[32m[20221213 20:19:25 @agent_ppo2.py:185][0m |          -0.0237 |           0.0116 |           0.0000 |
[32m[20221213 20:19:26 @agent_ppo2.py:185][0m |          -0.0249 |           0.0114 |           0.0000 |
[32m[20221213 20:19:26 @agent_ppo2.py:185][0m |          -0.0219 |           0.0115 |           0.0000 |
[32m[20221213 20:19:26 @agent_ppo2.py:185][0m |          -0.0242 |           0.0112 |           0.0000 |
[32m[20221213 20:19:26 @agent_ppo2.py:130][0m Policy update time: 3.18 s
[32m[20221213 20:19:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.23
[32m[20221213 20:19:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.76
[32m[20221213 20:19:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.77
[32m[20221213 20:19:27 @agent_ppo2.py:143][0m Total time:      11.01 min
[32m[20221213 20:19:27 @agent_ppo2.py:145][0m 333824 total steps have happened
[32m[20221213 20:19:27 @agent_ppo2.py:121][0m #------------------------ Iteration 163 --------------------------#
[32m[20221213 20:19:28 @agent_ppo2.py:127][0m Sampling time: 0.55 s by 10 slaves
[32m[20221213 20:19:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:19:28 @agent_ppo2.py:185][0m |          -0.0022 |           0.0205 |           0.0000 |
[32m[20221213 20:19:28 @agent_ppo2.py:185][0m |          -0.0091 |           0.0167 |           0.0000 |
[32m[20221213 20:19:29 @agent_ppo2.py:185][0m |          -0.0129 |           0.0156 |           0.0000 |
[32m[20221213 20:19:29 @agent_ppo2.py:185][0m |          -0.0163 |           0.0149 |           0.0000 |
[32m[20221213 20:19:29 @agent_ppo2.py:185][0m |          -0.0171 |           0.0145 |           0.0000 |
[32m[20221213 20:19:30 @agent_ppo2.py:185][0m |          -0.0207 |           0.0146 |           0.0000 |
[32m[20221213 20:19:30 @agent_ppo2.py:185][0m |          -0.0194 |           0.0140 |           0.0000 |
[32m[20221213 20:19:31 @agent_ppo2.py:185][0m |          -0.0200 |           0.0144 |           0.0000 |
[32m[20221213 20:19:31 @agent_ppo2.py:185][0m |          -0.0202 |           0.0140 |           0.0000 |
[32m[20221213 20:19:31 @agent_ppo2.py:185][0m |          -0.0223 |           0.0138 |           0.0000 |
[32m[20221213 20:19:31 @agent_ppo2.py:130][0m Policy update time: 3.59 s
[32m[20221213 20:19:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.61
[32m[20221213 20:19:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.25
[32m[20221213 20:19:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.78
[32m[20221213 20:19:32 @agent_ppo2.py:143][0m Total time:      11.09 min
[32m[20221213 20:19:32 @agent_ppo2.py:145][0m 335872 total steps have happened
[32m[20221213 20:19:32 @agent_ppo2.py:121][0m #------------------------ Iteration 164 --------------------------#
[32m[20221213 20:19:32 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:19:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:19:33 @agent_ppo2.py:185][0m |           0.0011 |           0.0133 |           0.0000 |
[32m[20221213 20:19:33 @agent_ppo2.py:185][0m |          -0.0136 |           0.0124 |           0.0000 |
[32m[20221213 20:19:34 @agent_ppo2.py:185][0m |          -0.0191 |           0.0118 |           0.0000 |
[32m[20221213 20:19:34 @agent_ppo2.py:185][0m |          -0.0225 |           0.0117 |           0.0000 |
[32m[20221213 20:19:34 @agent_ppo2.py:185][0m |          -0.0229 |           0.0116 |           0.0000 |
[32m[20221213 20:19:34 @agent_ppo2.py:185][0m |          -0.0255 |           0.0113 |           0.0000 |
[32m[20221213 20:19:35 @agent_ppo2.py:185][0m |          -0.0283 |           0.0110 |           0.0000 |
[32m[20221213 20:19:35 @agent_ppo2.py:185][0m |          -0.0287 |           0.0110 |           0.0000 |
[32m[20221213 20:19:36 @agent_ppo2.py:185][0m |          -0.0308 |           0.0112 |           0.0000 |
[32m[20221213 20:19:36 @agent_ppo2.py:185][0m |          -0.0303 |           0.0110 |           0.0000 |
[32m[20221213 20:19:36 @agent_ppo2.py:130][0m Policy update time: 3.75 s
[32m[20221213 20:19:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.81
[32m[20221213 20:19:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.44
[32m[20221213 20:19:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.29
[32m[20221213 20:19:37 @agent_ppo2.py:143][0m Total time:      11.17 min
[32m[20221213 20:19:37 @agent_ppo2.py:145][0m 337920 total steps have happened
[32m[20221213 20:19:37 @agent_ppo2.py:121][0m #------------------------ Iteration 165 --------------------------#
[32m[20221213 20:19:37 @agent_ppo2.py:127][0m Sampling time: 0.57 s by 10 slaves
[32m[20221213 20:19:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:19:38 @agent_ppo2.py:185][0m |           0.0017 |           0.0113 |           0.0000 |
[32m[20221213 20:19:38 @agent_ppo2.py:185][0m |          -0.0129 |           0.0065 |           0.0000 |
[32m[20221213 20:19:38 @agent_ppo2.py:185][0m |          -0.0161 |           0.0061 |           0.0000 |
[32m[20221213 20:19:39 @agent_ppo2.py:185][0m |          -0.0183 |           0.0059 |           0.0000 |
[32m[20221213 20:19:39 @agent_ppo2.py:185][0m |          -0.0195 |           0.0056 |           0.0000 |
[32m[20221213 20:19:39 @agent_ppo2.py:185][0m |          -0.0191 |           0.0053 |           0.0000 |
[32m[20221213 20:19:40 @agent_ppo2.py:185][0m |          -0.0204 |           0.0051 |           0.0000 |
[32m[20221213 20:19:40 @agent_ppo2.py:185][0m |          -0.0215 |           0.0049 |           0.0000 |
[32m[20221213 20:19:40 @agent_ppo2.py:185][0m |          -0.0207 |           0.0047 |           0.0000 |
[32m[20221213 20:19:41 @agent_ppo2.py:185][0m |          -0.0224 |           0.0045 |           0.0000 |
[32m[20221213 20:19:41 @agent_ppo2.py:130][0m Policy update time: 3.54 s
[32m[20221213 20:19:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.10
[32m[20221213 20:19:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.17
[32m[20221213 20:19:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.19
[32m[20221213 20:19:41 @agent_ppo2.py:143][0m Total time:      11.25 min
[32m[20221213 20:19:41 @agent_ppo2.py:145][0m 339968 total steps have happened
[32m[20221213 20:19:41 @agent_ppo2.py:121][0m #------------------------ Iteration 166 --------------------------#
[32m[20221213 20:19:42 @agent_ppo2.py:127][0m Sampling time: 0.64 s by 10 slaves
[32m[20221213 20:19:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:19:42 @agent_ppo2.py:185][0m |           0.0038 |           0.0557 |           0.0000 |
[32m[20221213 20:19:43 @agent_ppo2.py:185][0m |          -0.0066 |           0.0254 |           0.0000 |
[32m[20221213 20:19:43 @agent_ppo2.py:185][0m |          -0.0106 |           0.0213 |           0.0000 |
[32m[20221213 20:19:43 @agent_ppo2.py:185][0m |          -0.0143 |           0.0202 |           0.0000 |
[32m[20221213 20:19:44 @agent_ppo2.py:185][0m |          -0.0143 |           0.0192 |           0.0000 |
[32m[20221213 20:19:44 @agent_ppo2.py:185][0m |          -0.0157 |           0.0184 |           0.0000 |
[32m[20221213 20:19:44 @agent_ppo2.py:185][0m |          -0.0160 |           0.0182 |           0.0000 |
[32m[20221213 20:19:44 @agent_ppo2.py:185][0m |          -0.0167 |           0.0176 |           0.0000 |
[32m[20221213 20:19:45 @agent_ppo2.py:185][0m |          -0.0180 |           0.0174 |           0.0000 |
[32m[20221213 20:19:45 @agent_ppo2.py:185][0m |          -0.0182 |           0.0170 |           0.0000 |
[32m[20221213 20:19:45 @agent_ppo2.py:130][0m Policy update time: 3.21 s
[32m[20221213 20:19:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.81
[32m[20221213 20:19:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.77
[32m[20221213 20:19:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.07
[32m[20221213 20:19:45 @agent_ppo2.py:143][0m Total time:      11.32 min
[32m[20221213 20:19:45 @agent_ppo2.py:145][0m 342016 total steps have happened
[32m[20221213 20:19:45 @agent_ppo2.py:121][0m #------------------------ Iteration 167 --------------------------#
[32m[20221213 20:19:46 @agent_ppo2.py:127][0m Sampling time: 0.55 s by 10 slaves
[32m[20221213 20:19:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:19:46 @agent_ppo2.py:185][0m |           0.0013 |           0.0405 |           0.0000 |
[32m[20221213 20:19:47 @agent_ppo2.py:185][0m |          -0.0116 |           0.0277 |           0.0000 |
[32m[20221213 20:19:47 @agent_ppo2.py:185][0m |          -0.0154 |           0.0251 |           0.0000 |
[32m[20221213 20:19:47 @agent_ppo2.py:185][0m |          -0.0183 |           0.0234 |           0.0000 |
[32m[20221213 20:19:48 @agent_ppo2.py:185][0m |          -0.0185 |           0.0222 |           0.0000 |
[32m[20221213 20:19:48 @agent_ppo2.py:185][0m |          -0.0254 |           0.0220 |           0.0000 |
[32m[20221213 20:19:48 @agent_ppo2.py:185][0m |          -0.0214 |           0.0214 |           0.0000 |
[32m[20221213 20:19:48 @agent_ppo2.py:185][0m |          -0.0224 |           0.0207 |           0.0000 |
[32m[20221213 20:19:49 @agent_ppo2.py:185][0m |          -0.0224 |           0.0202 |           0.0000 |
[32m[20221213 20:19:49 @agent_ppo2.py:185][0m |          -0.0229 |           0.0200 |           0.0000 |
[32m[20221213 20:19:49 @agent_ppo2.py:130][0m Policy update time: 2.92 s
[32m[20221213 20:19:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.80
[32m[20221213 20:19:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.39
[32m[20221213 20:19:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.75
[32m[20221213 20:19:49 @agent_ppo2.py:143][0m Total time:      11.38 min
[32m[20221213 20:19:49 @agent_ppo2.py:145][0m 344064 total steps have happened
[32m[20221213 20:19:49 @agent_ppo2.py:121][0m #------------------------ Iteration 168 --------------------------#
[32m[20221213 20:19:50 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:19:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:19:50 @agent_ppo2.py:185][0m |           0.0011 |           0.0163 |           0.0000 |
[32m[20221213 20:19:51 @agent_ppo2.py:185][0m |          -0.0138 |           0.0127 |           0.0000 |
[32m[20221213 20:19:51 @agent_ppo2.py:185][0m |          -0.0206 |           0.0123 |           0.0000 |
[32m[20221213 20:19:51 @agent_ppo2.py:185][0m |          -0.0223 |           0.0120 |           0.0000 |
[32m[20221213 20:19:51 @agent_ppo2.py:185][0m |          -0.0239 |           0.0117 |           0.0000 |
[32m[20221213 20:19:52 @agent_ppo2.py:185][0m |          -0.0257 |           0.0115 |           0.0000 |
[32m[20221213 20:19:52 @agent_ppo2.py:185][0m |          -0.0263 |           0.0112 |           0.0000 |
[32m[20221213 20:19:52 @agent_ppo2.py:185][0m |          -0.0282 |           0.0111 |           0.0000 |
[32m[20221213 20:19:53 @agent_ppo2.py:185][0m |          -0.0289 |           0.0110 |           0.0000 |
[32m[20221213 20:19:53 @agent_ppo2.py:185][0m |          -0.0312 |           0.0108 |           0.0000 |
[32m[20221213 20:19:53 @agent_ppo2.py:130][0m Policy update time: 2.95 s
[32m[20221213 20:19:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.16
[32m[20221213 20:19:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.07
[32m[20221213 20:19:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.34
[32m[20221213 20:19:53 @agent_ppo2.py:143][0m Total time:      11.45 min
[32m[20221213 20:19:53 @agent_ppo2.py:145][0m 346112 total steps have happened
[32m[20221213 20:19:53 @agent_ppo2.py:121][0m #------------------------ Iteration 169 --------------------------#
[32m[20221213 20:19:54 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:19:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:19:54 @agent_ppo2.py:185][0m |           0.0007 |           0.0152 |           0.0000 |
[32m[20221213 20:19:55 @agent_ppo2.py:185][0m |          -0.0086 |           0.0140 |           0.0000 |
[32m[20221213 20:19:55 @agent_ppo2.py:185][0m |          -0.0115 |           0.0137 |           0.0000 |
[32m[20221213 20:19:55 @agent_ppo2.py:185][0m |          -0.0152 |           0.0133 |           0.0000 |
[32m[20221213 20:19:56 @agent_ppo2.py:185][0m |          -0.0160 |           0.0133 |           0.0000 |
[32m[20221213 20:19:56 @agent_ppo2.py:185][0m |          -0.0179 |           0.0127 |           0.0000 |
[32m[20221213 20:19:56 @agent_ppo2.py:185][0m |          -0.0200 |           0.0127 |           0.0000 |
[32m[20221213 20:19:57 @agent_ppo2.py:185][0m |          -0.0197 |           0.0125 |           0.0000 |
[32m[20221213 20:19:57 @agent_ppo2.py:185][0m |          -0.0201 |           0.0124 |           0.0000 |
[32m[20221213 20:19:57 @agent_ppo2.py:185][0m |          -0.0208 |           0.0123 |           0.0000 |
[32m[20221213 20:19:57 @agent_ppo2.py:130][0m Policy update time: 3.46 s
[32m[20221213 20:19:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.22
[32m[20221213 20:19:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.29
[32m[20221213 20:19:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.01
[32m[20221213 20:19:58 @agent_ppo2.py:143][0m Total time:      11.53 min
[32m[20221213 20:19:58 @agent_ppo2.py:145][0m 348160 total steps have happened
[32m[20221213 20:19:58 @agent_ppo2.py:121][0m #------------------------ Iteration 170 --------------------------#
[32m[20221213 20:19:59 @agent_ppo2.py:127][0m Sampling time: 0.76 s by 10 slaves
[32m[20221213 20:19:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:19:59 @agent_ppo2.py:185][0m |           0.0023 |           0.0111 |           0.0000 |
[32m[20221213 20:19:59 @agent_ppo2.py:185][0m |          -0.0144 |           0.0101 |           0.0000 |
[32m[20221213 20:20:00 @agent_ppo2.py:185][0m |          -0.0198 |           0.0096 |           0.0000 |
[32m[20221213 20:20:00 @agent_ppo2.py:185][0m |          -0.0217 |           0.0094 |           0.0000 |
[32m[20221213 20:20:00 @agent_ppo2.py:185][0m |          -0.0266 |           0.0092 |           0.0000 |
[32m[20221213 20:20:00 @agent_ppo2.py:185][0m |          -0.0285 |           0.0090 |           0.0000 |
[32m[20221213 20:20:01 @agent_ppo2.py:185][0m |          -0.0298 |           0.0088 |           0.0000 |
[32m[20221213 20:20:01 @agent_ppo2.py:185][0m |          -0.0309 |           0.0087 |           0.0000 |
[32m[20221213 20:20:01 @agent_ppo2.py:185][0m |          -0.0339 |           0.0088 |           0.0000 |
[32m[20221213 20:20:01 @agent_ppo2.py:185][0m |          -0.0345 |           0.0088 |           0.0000 |
[32m[20221213 20:20:01 @agent_ppo2.py:130][0m Policy update time: 2.78 s
[32m[20221213 20:20:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.10
[32m[20221213 20:20:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.88
[32m[20221213 20:20:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.17
[32m[20221213 20:20:02 @agent_ppo2.py:143][0m Total time:      11.59 min
[32m[20221213 20:20:02 @agent_ppo2.py:145][0m 350208 total steps have happened
[32m[20221213 20:20:02 @agent_ppo2.py:121][0m #------------------------ Iteration 171 --------------------------#
[32m[20221213 20:20:02 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:20:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:03 @agent_ppo2.py:185][0m |          -0.0002 |           0.0082 |           0.0000 |
[32m[20221213 20:20:03 @agent_ppo2.py:185][0m |          -0.0098 |           0.0075 |           0.0000 |
[32m[20221213 20:20:03 @agent_ppo2.py:185][0m |          -0.0163 |           0.0072 |           0.0000 |
[32m[20221213 20:20:04 @agent_ppo2.py:185][0m |          -0.0194 |           0.0071 |           0.0000 |
[32m[20221213 20:20:04 @agent_ppo2.py:185][0m |          -0.0197 |           0.0070 |           0.0000 |
[32m[20221213 20:20:04 @agent_ppo2.py:185][0m |          -0.0206 |           0.0067 |           0.0000 |
[32m[20221213 20:20:05 @agent_ppo2.py:185][0m |          -0.0236 |           0.0067 |           0.0000 |
[32m[20221213 20:20:05 @agent_ppo2.py:185][0m |          -0.0239 |           0.0066 |           0.0000 |
[32m[20221213 20:20:05 @agent_ppo2.py:185][0m |          -0.0252 |           0.0065 |           0.0000 |
[32m[20221213 20:20:06 @agent_ppo2.py:185][0m |          -0.0250 |           0.0063 |           0.0000 |
[32m[20221213 20:20:06 @agent_ppo2.py:130][0m Policy update time: 3.26 s
[32m[20221213 20:20:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.46
[32m[20221213 20:20:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.82
[32m[20221213 20:20:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.44
[32m[20221213 20:20:06 @agent_ppo2.py:143][0m Total time:      11.67 min
[32m[20221213 20:20:06 @agent_ppo2.py:145][0m 352256 total steps have happened
[32m[20221213 20:20:06 @agent_ppo2.py:121][0m #------------------------ Iteration 172 --------------------------#
[32m[20221213 20:20:07 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:20:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:07 @agent_ppo2.py:185][0m |          -0.0013 |           0.0126 |           0.0000 |
[32m[20221213 20:20:08 @agent_ppo2.py:185][0m |          -0.0080 |           0.0094 |           0.0000 |
[32m[20221213 20:20:08 @agent_ppo2.py:185][0m |          -0.0124 |           0.0089 |           0.0000 |
[32m[20221213 20:20:08 @agent_ppo2.py:185][0m |          -0.0142 |           0.0083 |           0.0000 |
[32m[20221213 20:20:08 @agent_ppo2.py:185][0m |          -0.0155 |           0.0082 |           0.0000 |
[32m[20221213 20:20:09 @agent_ppo2.py:185][0m |          -0.0157 |           0.0080 |           0.0000 |
[32m[20221213 20:20:09 @agent_ppo2.py:185][0m |          -0.0168 |           0.0079 |           0.0000 |
[32m[20221213 20:20:09 @agent_ppo2.py:185][0m |          -0.0173 |           0.0079 |           0.0000 |
[32m[20221213 20:20:09 @agent_ppo2.py:185][0m |          -0.0180 |           0.0078 |           0.0000 |
[32m[20221213 20:20:10 @agent_ppo2.py:185][0m |          -0.0184 |           0.0079 |           0.0000 |
[32m[20221213 20:20:10 @agent_ppo2.py:130][0m Policy update time: 3.01 s
[32m[20221213 20:20:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.55
[32m[20221213 20:20:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.50
[32m[20221213 20:20:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.41
[32m[20221213 20:20:10 @agent_ppo2.py:143][0m Total time:      11.73 min
[32m[20221213 20:20:10 @agent_ppo2.py:145][0m 354304 total steps have happened
[32m[20221213 20:20:10 @agent_ppo2.py:121][0m #------------------------ Iteration 173 --------------------------#
[32m[20221213 20:20:11 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:20:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:11 @agent_ppo2.py:185][0m |           0.0014 |           0.0241 |           0.0000 |
[32m[20221213 20:20:11 @agent_ppo2.py:185][0m |          -0.0122 |           0.0166 |           0.0000 |
[32m[20221213 20:20:12 @agent_ppo2.py:185][0m |          -0.0142 |           0.0154 |           0.0000 |
[32m[20221213 20:20:12 @agent_ppo2.py:185][0m |          -0.0139 |           0.0142 |           0.0000 |
[32m[20221213 20:20:12 @agent_ppo2.py:185][0m |          -0.0147 |           0.0139 |           0.0000 |
[32m[20221213 20:20:12 @agent_ppo2.py:185][0m |          -0.0171 |           0.0136 |           0.0000 |
[32m[20221213 20:20:13 @agent_ppo2.py:185][0m |          -0.0250 |           0.0130 |           0.0000 |
[32m[20221213 20:20:13 @agent_ppo2.py:185][0m |          -0.0177 |           0.0133 |           0.0000 |
[32m[20221213 20:20:13 @agent_ppo2.py:185][0m |          -0.0177 |           0.0140 |           0.0000 |
[32m[20221213 20:20:13 @agent_ppo2.py:185][0m |          -0.0176 |           0.0130 |           0.0000 |
[32m[20221213 20:20:13 @agent_ppo2.py:130][0m Policy update time: 2.68 s
[32m[20221213 20:20:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.42
[32m[20221213 20:20:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.54
[32m[20221213 20:20:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.07
[32m[20221213 20:20:14 @agent_ppo2.py:143][0m Total time:      11.80 min
[32m[20221213 20:20:14 @agent_ppo2.py:145][0m 356352 total steps have happened
[32m[20221213 20:20:14 @agent_ppo2.py:121][0m #------------------------ Iteration 174 --------------------------#
[32m[20221213 20:20:15 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:20:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:15 @agent_ppo2.py:185][0m |           0.0031 |           0.0106 |           0.0000 |
[32m[20221213 20:20:15 @agent_ppo2.py:185][0m |          -0.0143 |           0.0073 |           0.0000 |
[32m[20221213 20:20:16 @agent_ppo2.py:185][0m |          -0.0184 |           0.0071 |           0.0000 |
[32m[20221213 20:20:16 @agent_ppo2.py:185][0m |          -0.0167 |           0.0069 |           0.0000 |
[32m[20221213 20:20:16 @agent_ppo2.py:185][0m |          -0.0222 |           0.0067 |           0.0000 |
[32m[20221213 20:20:17 @agent_ppo2.py:185][0m |          -0.0212 |           0.0066 |           0.0000 |
[32m[20221213 20:20:17 @agent_ppo2.py:185][0m |          -0.0234 |           0.0065 |           0.0000 |
[32m[20221213 20:20:17 @agent_ppo2.py:185][0m |          -0.0241 |           0.0063 |           0.0000 |
[32m[20221213 20:20:17 @agent_ppo2.py:185][0m |          -0.0255 |           0.0062 |           0.0000 |
[32m[20221213 20:20:18 @agent_ppo2.py:185][0m |          -0.0263 |           0.0061 |           0.0000 |
[32m[20221213 20:20:18 @agent_ppo2.py:130][0m Policy update time: 3.05 s
[32m[20221213 20:20:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.18
[32m[20221213 20:20:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.52
[32m[20221213 20:20:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.55
[32m[20221213 20:20:18 @agent_ppo2.py:143][0m Total time:      11.86 min
[32m[20221213 20:20:18 @agent_ppo2.py:145][0m 358400 total steps have happened
[32m[20221213 20:20:18 @agent_ppo2.py:121][0m #------------------------ Iteration 175 --------------------------#
[32m[20221213 20:20:19 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:20:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:19 @agent_ppo2.py:185][0m |           0.0006 |           0.0228 |           0.0000 |
[32m[20221213 20:20:19 @agent_ppo2.py:185][0m |          -0.0091 |           0.0153 |           0.0000 |
[32m[20221213 20:20:19 @agent_ppo2.py:185][0m |          -0.0138 |           0.0138 |           0.0000 |
[32m[20221213 20:20:20 @agent_ppo2.py:185][0m |          -0.0160 |           0.0131 |           0.0000 |
[32m[20221213 20:20:20 @agent_ppo2.py:185][0m |          -0.0171 |           0.0132 |           0.0000 |
[32m[20221213 20:20:20 @agent_ppo2.py:185][0m |          -0.0177 |           0.0127 |           0.0000 |
[32m[20221213 20:20:20 @agent_ppo2.py:185][0m |          -0.0208 |           0.0126 |           0.0000 |
[32m[20221213 20:20:21 @agent_ppo2.py:185][0m |          -0.0189 |           0.0125 |           0.0000 |
[32m[20221213 20:20:21 @agent_ppo2.py:185][0m |          -0.0194 |           0.0124 |           0.0000 |
[32m[20221213 20:20:21 @agent_ppo2.py:185][0m |          -0.0195 |           0.0121 |           0.0000 |
[32m[20221213 20:20:21 @agent_ppo2.py:130][0m Policy update time: 2.75 s
[32m[20221213 20:20:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.91
[32m[20221213 20:20:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.93
[32m[20221213 20:20:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.53
[32m[20221213 20:20:22 @agent_ppo2.py:143][0m Total time:      11.92 min
[32m[20221213 20:20:22 @agent_ppo2.py:145][0m 360448 total steps have happened
[32m[20221213 20:20:22 @agent_ppo2.py:121][0m #------------------------ Iteration 176 --------------------------#
[32m[20221213 20:20:22 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:20:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:23 @agent_ppo2.py:185][0m |           0.0027 |           0.0308 |           0.0000 |
[32m[20221213 20:20:23 @agent_ppo2.py:185][0m |          -0.0097 |           0.0210 |           0.0000 |
[32m[20221213 20:20:23 @agent_ppo2.py:185][0m |          -0.0171 |           0.0189 |           0.0000 |
[32m[20221213 20:20:24 @agent_ppo2.py:185][0m |          -0.0163 |           0.0182 |           0.0000 |
[32m[20221213 20:20:24 @agent_ppo2.py:185][0m |          -0.0178 |           0.0182 |           0.0000 |
[32m[20221213 20:20:24 @agent_ppo2.py:185][0m |          -0.0188 |           0.0174 |           0.0000 |
[32m[20221213 20:20:24 @agent_ppo2.py:185][0m |          -0.0198 |           0.0167 |           0.0000 |
[32m[20221213 20:20:25 @agent_ppo2.py:185][0m |          -0.0211 |           0.0166 |           0.0000 |
[32m[20221213 20:20:25 @agent_ppo2.py:185][0m |          -0.0202 |           0.0161 |           0.0000 |
[32m[20221213 20:20:25 @agent_ppo2.py:185][0m |          -0.0213 |           0.0162 |           0.0000 |
[32m[20221213 20:20:25 @agent_ppo2.py:130][0m Policy update time: 2.94 s
[32m[20221213 20:20:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.57
[32m[20221213 20:20:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.87
[32m[20221213 20:20:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.39
[32m[20221213 20:20:26 @agent_ppo2.py:143][0m Total time:      11.99 min
[32m[20221213 20:20:26 @agent_ppo2.py:145][0m 362496 total steps have happened
[32m[20221213 20:20:26 @agent_ppo2.py:121][0m #------------------------ Iteration 177 --------------------------#
[32m[20221213 20:20:26 @agent_ppo2.py:127][0m Sampling time: 0.53 s by 10 slaves
[32m[20221213 20:20:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:27 @agent_ppo2.py:185][0m |           0.0013 |           0.0163 |           0.0000 |
[32m[20221213 20:20:27 @agent_ppo2.py:185][0m |          -0.0128 |           0.0098 |           0.0000 |
[32m[20221213 20:20:27 @agent_ppo2.py:185][0m |          -0.0168 |           0.0095 |           0.0000 |
[32m[20221213 20:20:28 @agent_ppo2.py:185][0m |          -0.0198 |           0.0092 |           0.0000 |
[32m[20221213 20:20:28 @agent_ppo2.py:185][0m |          -0.0202 |           0.0090 |           0.0000 |
[32m[20221213 20:20:28 @agent_ppo2.py:185][0m |          -0.0227 |           0.0089 |           0.0000 |
[32m[20221213 20:20:29 @agent_ppo2.py:185][0m |          -0.0229 |           0.0088 |           0.0000 |
[32m[20221213 20:20:29 @agent_ppo2.py:185][0m |          -0.0237 |           0.0086 |           0.0000 |
[32m[20221213 20:20:29 @agent_ppo2.py:185][0m |          -0.0239 |           0.0085 |           0.0000 |
[32m[20221213 20:20:30 @agent_ppo2.py:185][0m |          -0.0243 |           0.0085 |           0.0000 |
[32m[20221213 20:20:30 @agent_ppo2.py:130][0m Policy update time: 3.29 s
[32m[20221213 20:20:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.87
[32m[20221213 20:20:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.36
[32m[20221213 20:20:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.98
[32m[20221213 20:20:30 @agent_ppo2.py:143][0m Total time:      12.06 min
[32m[20221213 20:20:30 @agent_ppo2.py:145][0m 364544 total steps have happened
[32m[20221213 20:20:30 @agent_ppo2.py:121][0m #------------------------ Iteration 178 --------------------------#
[32m[20221213 20:20:31 @agent_ppo2.py:127][0m Sampling time: 0.53 s by 10 slaves
[32m[20221213 20:20:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:31 @agent_ppo2.py:185][0m |           0.0015 |           0.0569 |           0.0000 |
[32m[20221213 20:20:31 @agent_ppo2.py:185][0m |          -0.0071 |           0.0292 |           0.0000 |
[32m[20221213 20:20:31 @agent_ppo2.py:185][0m |          -0.0109 |           0.0255 |           0.0000 |
[32m[20221213 20:20:32 @agent_ppo2.py:185][0m |          -0.0131 |           0.0239 |           0.0000 |
[32m[20221213 20:20:32 @agent_ppo2.py:185][0m |          -0.0145 |           0.0226 |           0.0000 |
[32m[20221213 20:20:32 @agent_ppo2.py:185][0m |          -0.0205 |           0.0222 |           0.0000 |
[32m[20221213 20:20:33 @agent_ppo2.py:185][0m |          -0.0168 |           0.0214 |           0.0000 |
[32m[20221213 20:20:33 @agent_ppo2.py:185][0m |          -0.0183 |           0.0208 |           0.0000 |
[32m[20221213 20:20:33 @agent_ppo2.py:185][0m |          -0.0179 |           0.0201 |           0.0000 |
[32m[20221213 20:20:34 @agent_ppo2.py:185][0m |          -0.0177 |           0.0200 |           0.0000 |
[32m[20221213 20:20:34 @agent_ppo2.py:130][0m Policy update time: 3.01 s
[32m[20221213 20:20:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.03
[32m[20221213 20:20:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.84
[32m[20221213 20:20:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.45
[32m[20221213 20:20:34 @agent_ppo2.py:143][0m Total time:      12.13 min
[32m[20221213 20:20:34 @agent_ppo2.py:145][0m 366592 total steps have happened
[32m[20221213 20:20:34 @agent_ppo2.py:121][0m #------------------------ Iteration 179 --------------------------#
[32m[20221213 20:20:35 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:20:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:35 @agent_ppo2.py:185][0m |           0.0032 |           0.0408 |           0.0000 |
[32m[20221213 20:20:35 @agent_ppo2.py:185][0m |          -0.0096 |           0.0277 |           0.0000 |
[32m[20221213 20:20:36 @agent_ppo2.py:185][0m |          -0.0139 |           0.0255 |           0.0000 |
[32m[20221213 20:20:36 @agent_ppo2.py:185][0m |          -0.0171 |           0.0238 |           0.0000 |
[32m[20221213 20:20:36 @agent_ppo2.py:185][0m |          -0.0170 |           0.0223 |           0.0000 |
[32m[20221213 20:20:36 @agent_ppo2.py:185][0m |          -0.0188 |           0.0219 |           0.0000 |
[32m[20221213 20:20:37 @agent_ppo2.py:185][0m |          -0.0195 |           0.0211 |           0.0000 |
[32m[20221213 20:20:37 @agent_ppo2.py:185][0m |          -0.0233 |           0.0211 |           0.0000 |
[32m[20221213 20:20:37 @agent_ppo2.py:185][0m |          -0.0211 |           0.0209 |           0.0000 |
[32m[20221213 20:20:37 @agent_ppo2.py:185][0m |          -0.0232 |           0.0208 |           0.0000 |
[32m[20221213 20:20:37 @agent_ppo2.py:130][0m Policy update time: 2.88 s
[32m[20221213 20:20:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.13
[32m[20221213 20:20:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.93
[32m[20221213 20:20:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.37
[32m[20221213 20:20:38 @agent_ppo2.py:143][0m Total time:      12.19 min
[32m[20221213 20:20:38 @agent_ppo2.py:145][0m 368640 total steps have happened
[32m[20221213 20:20:38 @agent_ppo2.py:121][0m #------------------------ Iteration 180 --------------------------#
[32m[20221213 20:20:38 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:20:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:39 @agent_ppo2.py:185][0m |           0.0005 |           0.0335 |           0.0000 |
[32m[20221213 20:20:39 @agent_ppo2.py:185][0m |          -0.0157 |           0.0280 |           0.0000 |
[32m[20221213 20:20:39 @agent_ppo2.py:185][0m |          -0.0239 |           0.0265 |           0.0000 |
[32m[20221213 20:20:40 @agent_ppo2.py:185][0m |          -0.0259 |           0.0249 |           0.0000 |
[32m[20221213 20:20:40 @agent_ppo2.py:185][0m |          -0.0282 |           0.0243 |           0.0000 |
[32m[20221213 20:20:40 @agent_ppo2.py:185][0m |          -0.0272 |           0.0240 |           0.0000 |
[32m[20221213 20:20:41 @agent_ppo2.py:185][0m |          -0.0283 |           0.0235 |           0.0000 |
[32m[20221213 20:20:41 @agent_ppo2.py:185][0m |          -0.0311 |           0.0228 |           0.0000 |
[32m[20221213 20:20:41 @agent_ppo2.py:185][0m |          -0.0329 |           0.0222 |           0.0000 |
[32m[20221213 20:20:41 @agent_ppo2.py:185][0m |          -0.0343 |           0.0219 |           0.0000 |
[32m[20221213 20:20:41 @agent_ppo2.py:130][0m Policy update time: 3.01 s
[32m[20221213 20:20:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.98
[32m[20221213 20:20:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.79
[32m[20221213 20:20:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.25
[32m[20221213 20:20:42 @agent_ppo2.py:143][0m Total time:      12.26 min
[32m[20221213 20:20:42 @agent_ppo2.py:145][0m 370688 total steps have happened
[32m[20221213 20:20:42 @agent_ppo2.py:121][0m #------------------------ Iteration 181 --------------------------#
[32m[20221213 20:20:42 @agent_ppo2.py:127][0m Sampling time: 0.56 s by 10 slaves
[32m[20221213 20:20:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:43 @agent_ppo2.py:185][0m |           0.0035 |           0.0172 |           0.0000 |
[32m[20221213 20:20:43 @agent_ppo2.py:185][0m |          -0.0099 |           0.0097 |           0.0000 |
[32m[20221213 20:20:44 @agent_ppo2.py:185][0m |          -0.0157 |           0.0093 |           0.0000 |
[32m[20221213 20:20:44 @agent_ppo2.py:185][0m |          -0.0175 |           0.0090 |           0.0000 |
[32m[20221213 20:20:44 @agent_ppo2.py:185][0m |          -0.0197 |           0.0088 |           0.0000 |
[32m[20221213 20:20:44 @agent_ppo2.py:185][0m |          -0.0211 |           0.0086 |           0.0000 |
[32m[20221213 20:20:45 @agent_ppo2.py:185][0m |          -0.0196 |           0.0084 |           0.0000 |
[32m[20221213 20:20:45 @agent_ppo2.py:185][0m |          -0.0233 |           0.0081 |           0.0000 |
[32m[20221213 20:20:45 @agent_ppo2.py:185][0m |          -0.0245 |           0.0080 |           0.0000 |
[32m[20221213 20:20:46 @agent_ppo2.py:185][0m |          -0.0208 |           0.0077 |           0.0000 |
[32m[20221213 20:20:46 @agent_ppo2.py:130][0m Policy update time: 3.10 s
[32m[20221213 20:20:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.21
[32m[20221213 20:20:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.47
[32m[20221213 20:20:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.59
[32m[20221213 20:20:46 @agent_ppo2.py:143][0m Total time:      12.33 min
[32m[20221213 20:20:46 @agent_ppo2.py:145][0m 372736 total steps have happened
[32m[20221213 20:20:46 @agent_ppo2.py:121][0m #------------------------ Iteration 182 --------------------------#
[32m[20221213 20:20:47 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:20:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:47 @agent_ppo2.py:185][0m |           0.0015 |           0.0132 |           0.0000 |
[32m[20221213 20:20:47 @agent_ppo2.py:185][0m |          -0.0086 |           0.0114 |           0.0000 |
[32m[20221213 20:20:48 @agent_ppo2.py:185][0m |          -0.0126 |           0.0109 |           0.0000 |
[32m[20221213 20:20:48 @agent_ppo2.py:185][0m |          -0.0158 |           0.0105 |           0.0000 |
[32m[20221213 20:20:48 @agent_ppo2.py:185][0m |          -0.0156 |           0.0104 |           0.0000 |
[32m[20221213 20:20:48 @agent_ppo2.py:185][0m |          -0.0155 |           0.0102 |           0.0000 |
[32m[20221213 20:20:49 @agent_ppo2.py:185][0m |          -0.0163 |           0.0099 |           0.0000 |
[32m[20221213 20:20:49 @agent_ppo2.py:185][0m |          -0.0166 |           0.0097 |           0.0000 |
[32m[20221213 20:20:49 @agent_ppo2.py:185][0m |          -0.0172 |           0.0097 |           0.0000 |
[32m[20221213 20:20:50 @agent_ppo2.py:185][0m |          -0.0175 |           0.0097 |           0.0000 |
[32m[20221213 20:20:50 @agent_ppo2.py:130][0m Policy update time: 3.00 s
[32m[20221213 20:20:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.52
[32m[20221213 20:20:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.71
[32m[20221213 20:20:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.61
[32m[20221213 20:20:50 @agent_ppo2.py:143][0m Total time:      12.40 min
[32m[20221213 20:20:50 @agent_ppo2.py:145][0m 374784 total steps have happened
[32m[20221213 20:20:50 @agent_ppo2.py:121][0m #------------------------ Iteration 183 --------------------------#
[32m[20221213 20:20:51 @agent_ppo2.py:127][0m Sampling time: 0.53 s by 10 slaves
[32m[20221213 20:20:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:51 @agent_ppo2.py:185][0m |           0.0005 |           0.0177 |           0.0000 |
[32m[20221213 20:20:51 @agent_ppo2.py:185][0m |          -0.0147 |           0.0130 |           0.0000 |
[32m[20221213 20:20:52 @agent_ppo2.py:185][0m |          -0.0115 |           0.0120 |           0.0000 |
[32m[20221213 20:20:52 @agent_ppo2.py:185][0m |          -0.0163 |           0.0115 |           0.0000 |
[32m[20221213 20:20:52 @agent_ppo2.py:185][0m |          -0.0177 |           0.0114 |           0.0000 |
[32m[20221213 20:20:52 @agent_ppo2.py:185][0m |          -0.0192 |           0.0111 |           0.0000 |
[32m[20221213 20:20:53 @agent_ppo2.py:185][0m |          -0.0193 |           0.0110 |           0.0000 |
[32m[20221213 20:20:53 @agent_ppo2.py:185][0m |          -0.0200 |           0.0109 |           0.0000 |
[32m[20221213 20:20:53 @agent_ppo2.py:185][0m |          -0.0203 |           0.0109 |           0.0000 |
[32m[20221213 20:20:53 @agent_ppo2.py:185][0m |          -0.0204 |           0.0109 |           0.0000 |
[32m[20221213 20:20:53 @agent_ppo2.py:130][0m Policy update time: 2.82 s
[32m[20221213 20:20:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.69
[32m[20221213 20:20:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.41
[32m[20221213 20:20:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.67
[32m[20221213 20:20:54 @agent_ppo2.py:143][0m Total time:      12.46 min
[32m[20221213 20:20:54 @agent_ppo2.py:145][0m 376832 total steps have happened
[32m[20221213 20:20:54 @agent_ppo2.py:121][0m #------------------------ Iteration 184 --------------------------#
[32m[20221213 20:20:54 @agent_ppo2.py:127][0m Sampling time: 0.56 s by 10 slaves
[32m[20221213 20:20:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:55 @agent_ppo2.py:185][0m |          -0.0018 |           0.0933 |           0.0000 |
[32m[20221213 20:20:55 @agent_ppo2.py:185][0m |          -0.0104 |           0.0454 |           0.0000 |
[32m[20221213 20:20:55 @agent_ppo2.py:185][0m |          -0.0180 |           0.0400 |           0.0000 |
[32m[20221213 20:20:56 @agent_ppo2.py:185][0m |          -0.0168 |           0.0359 |           0.0000 |
[32m[20221213 20:20:56 @agent_ppo2.py:185][0m |          -0.0181 |           0.0333 |           0.0000 |
[32m[20221213 20:20:56 @agent_ppo2.py:185][0m |          -0.0271 |           0.0324 |           0.0000 |
[32m[20221213 20:20:56 @agent_ppo2.py:185][0m |          -0.0185 |           0.0309 |           0.0000 |
[32m[20221213 20:20:57 @agent_ppo2.py:185][0m |          -0.0252 |           0.0303 |           0.0000 |
[32m[20221213 20:20:57 @agent_ppo2.py:185][0m |          -0.0205 |           0.0294 |           0.0000 |
[32m[20221213 20:20:57 @agent_ppo2.py:185][0m |          -0.0212 |           0.0279 |           0.0000 |
[32m[20221213 20:20:57 @agent_ppo2.py:130][0m Policy update time: 2.75 s
[32m[20221213 20:20:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.85
[32m[20221213 20:20:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 12.04
[32m[20221213 20:20:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.47
[32m[20221213 20:20:58 @agent_ppo2.py:143][0m Total time:      12.52 min
[32m[20221213 20:20:58 @agent_ppo2.py:145][0m 378880 total steps have happened
[32m[20221213 20:20:58 @agent_ppo2.py:121][0m #------------------------ Iteration 185 --------------------------#
[32m[20221213 20:20:58 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:20:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:20:59 @agent_ppo2.py:185][0m |           0.0037 |           0.0294 |           0.0000 |
[32m[20221213 20:20:59 @agent_ppo2.py:185][0m |          -0.0121 |           0.0221 |           0.0000 |
[32m[20221213 20:20:59 @agent_ppo2.py:185][0m |          -0.0177 |           0.0211 |           0.0000 |
[32m[20221213 20:20:59 @agent_ppo2.py:185][0m |          -0.0209 |           0.0203 |           0.0000 |
[32m[20221213 20:21:00 @agent_ppo2.py:185][0m |          -0.0232 |           0.0196 |           0.0000 |
[32m[20221213 20:21:00 @agent_ppo2.py:185][0m |          -0.0260 |           0.0193 |           0.0000 |
[32m[20221213 20:21:00 @agent_ppo2.py:185][0m |          -0.0264 |           0.0187 |           0.0000 |
[32m[20221213 20:21:01 @agent_ppo2.py:185][0m |          -0.0277 |           0.0188 |           0.0000 |
[32m[20221213 20:21:01 @agent_ppo2.py:185][0m |          -0.0284 |           0.0184 |           0.0000 |
[32m[20221213 20:21:01 @agent_ppo2.py:185][0m |          -0.0280 |           0.0184 |           0.0000 |
[32m[20221213 20:21:01 @agent_ppo2.py:130][0m Policy update time: 3.03 s
[32m[20221213 20:21:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.39
[32m[20221213 20:21:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.36
[32m[20221213 20:21:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.70
[32m[20221213 20:21:02 @agent_ppo2.py:143][0m Total time:      12.59 min
[32m[20221213 20:21:02 @agent_ppo2.py:145][0m 380928 total steps have happened
[32m[20221213 20:21:02 @agent_ppo2.py:121][0m #------------------------ Iteration 186 --------------------------#
[32m[20221213 20:21:02 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:21:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:03 @agent_ppo2.py:185][0m |          -0.0008 |           0.0191 |           0.0000 |
[32m[20221213 20:21:03 @agent_ppo2.py:185][0m |          -0.0163 |           0.0171 |           0.0000 |
[32m[20221213 20:21:03 @agent_ppo2.py:185][0m |          -0.0186 |           0.0159 |           0.0000 |
[32m[20221213 20:21:04 @agent_ppo2.py:185][0m |          -0.0220 |           0.0153 |           0.0000 |
[32m[20221213 20:21:04 @agent_ppo2.py:185][0m |          -0.0249 |           0.0151 |           0.0000 |
[32m[20221213 20:21:04 @agent_ppo2.py:185][0m |          -0.0252 |           0.0146 |           0.0000 |
[32m[20221213 20:21:04 @agent_ppo2.py:185][0m |          -0.0278 |           0.0143 |           0.0000 |
[32m[20221213 20:21:05 @agent_ppo2.py:185][0m |          -0.0329 |           0.0142 |           0.0000 |
[32m[20221213 20:21:05 @agent_ppo2.py:185][0m |          -0.0310 |           0.0138 |           0.0000 |
[32m[20221213 20:21:05 @agent_ppo2.py:185][0m |          -0.0308 |           0.0138 |           0.0000 |
[32m[20221213 20:21:05 @agent_ppo2.py:130][0m Policy update time: 2.81 s
[32m[20221213 20:21:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.19
[32m[20221213 20:21:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.91
[32m[20221213 20:21:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.94
[32m[20221213 20:21:06 @agent_ppo2.py:143][0m Total time:      12.65 min
[32m[20221213 20:21:06 @agent_ppo2.py:145][0m 382976 total steps have happened
[32m[20221213 20:21:06 @agent_ppo2.py:121][0m #------------------------ Iteration 187 --------------------------#
[32m[20221213 20:21:06 @agent_ppo2.py:127][0m Sampling time: 0.57 s by 10 slaves
[32m[20221213 20:21:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:07 @agent_ppo2.py:185][0m |           0.0001 |           0.0157 |           0.0000 |
[32m[20221213 20:21:07 @agent_ppo2.py:185][0m |          -0.0143 |           0.0136 |           0.0000 |
[32m[20221213 20:21:07 @agent_ppo2.py:185][0m |          -0.0207 |           0.0129 |           0.0000 |
[32m[20221213 20:21:07 @agent_ppo2.py:185][0m |          -0.0217 |           0.0124 |           0.0000 |
[32m[20221213 20:21:08 @agent_ppo2.py:185][0m |          -0.0235 |           0.0122 |           0.0000 |
[32m[20221213 20:21:08 @agent_ppo2.py:185][0m |          -0.0259 |           0.0119 |           0.0000 |
[32m[20221213 20:21:08 @agent_ppo2.py:185][0m |          -0.0274 |           0.0118 |           0.0000 |
[32m[20221213 20:21:08 @agent_ppo2.py:185][0m |          -0.0278 |           0.0116 |           0.0000 |
[32m[20221213 20:21:09 @agent_ppo2.py:185][0m |          -0.0289 |           0.0116 |           0.0000 |
[32m[20221213 20:21:09 @agent_ppo2.py:185][0m |          -0.0304 |           0.0114 |           0.0000 |
[32m[20221213 20:21:09 @agent_ppo2.py:130][0m Policy update time: 2.95 s
[32m[20221213 20:21:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.74
[32m[20221213 20:21:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.25
[32m[20221213 20:21:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.42
[32m[20221213 20:21:10 @agent_ppo2.py:143][0m Total time:      12.72 min
[32m[20221213 20:21:10 @agent_ppo2.py:145][0m 385024 total steps have happened
[32m[20221213 20:21:10 @agent_ppo2.py:121][0m #------------------------ Iteration 188 --------------------------#
[32m[20221213 20:21:10 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:21:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:10 @agent_ppo2.py:185][0m |           0.0004 |           0.0121 |           0.0000 |
[32m[20221213 20:21:11 @agent_ppo2.py:185][0m |          -0.0155 |           0.0105 |           0.0000 |
[32m[20221213 20:21:11 @agent_ppo2.py:185][0m |          -0.0186 |           0.0100 |           0.0000 |
[32m[20221213 20:21:11 @agent_ppo2.py:185][0m |          -0.0207 |           0.0097 |           0.0000 |
[32m[20221213 20:21:12 @agent_ppo2.py:185][0m |          -0.0229 |           0.0094 |           0.0000 |
[32m[20221213 20:21:12 @agent_ppo2.py:185][0m |          -0.0228 |           0.0091 |           0.0000 |
[32m[20221213 20:21:12 @agent_ppo2.py:185][0m |          -0.0243 |           0.0091 |           0.0000 |
[32m[20221213 20:21:12 @agent_ppo2.py:185][0m |          -0.0258 |           0.0090 |           0.0000 |
[32m[20221213 20:21:13 @agent_ppo2.py:185][0m |          -0.0259 |           0.0088 |           0.0000 |
[32m[20221213 20:21:13 @agent_ppo2.py:185][0m |          -0.0258 |           0.0089 |           0.0000 |
[32m[20221213 20:21:13 @agent_ppo2.py:130][0m Policy update time: 2.84 s
[32m[20221213 20:21:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.67
[32m[20221213 20:21:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.34
[32m[20221213 20:21:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.30
[32m[20221213 20:21:13 @agent_ppo2.py:143][0m Total time:      12.78 min
[32m[20221213 20:21:13 @agent_ppo2.py:145][0m 387072 total steps have happened
[32m[20221213 20:21:13 @agent_ppo2.py:121][0m #------------------------ Iteration 189 --------------------------#
[32m[20221213 20:21:14 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:21:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:14 @agent_ppo2.py:185][0m |          -0.0012 |           0.0207 |           0.0000 |
[32m[20221213 20:21:15 @agent_ppo2.py:185][0m |          -0.0080 |           0.0163 |           0.0000 |
[32m[20221213 20:21:15 @agent_ppo2.py:185][0m |          -0.0127 |           0.0139 |           0.0000 |
[32m[20221213 20:21:15 @agent_ppo2.py:185][0m |          -0.0147 |           0.0136 |           0.0000 |
[32m[20221213 20:21:15 @agent_ppo2.py:185][0m |          -0.0147 |           0.0132 |           0.0000 |
[32m[20221213 20:21:16 @agent_ppo2.py:185][0m |          -0.0164 |           0.0132 |           0.0000 |
[32m[20221213 20:21:16 @agent_ppo2.py:185][0m |          -0.0167 |           0.0128 |           0.0000 |
[32m[20221213 20:21:16 @agent_ppo2.py:185][0m |          -0.0205 |           0.0126 |           0.0000 |
[32m[20221213 20:21:16 @agent_ppo2.py:185][0m |          -0.0181 |           0.0125 |           0.0000 |
[32m[20221213 20:21:17 @agent_ppo2.py:185][0m |          -0.0196 |           0.0123 |           0.0000 |
[32m[20221213 20:21:17 @agent_ppo2.py:130][0m Policy update time: 2.78 s
[32m[20221213 20:21:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.34
[32m[20221213 20:21:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.60
[32m[20221213 20:21:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.10
[32m[20221213 20:21:17 @agent_ppo2.py:143][0m Total time:      12.85 min
[32m[20221213 20:21:17 @agent_ppo2.py:145][0m 389120 total steps have happened
[32m[20221213 20:21:17 @agent_ppo2.py:121][0m #------------------------ Iteration 190 --------------------------#
[32m[20221213 20:21:18 @agent_ppo2.py:127][0m Sampling time: 0.57 s by 10 slaves
[32m[20221213 20:21:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:18 @agent_ppo2.py:185][0m |           0.0049 |           0.0299 |           0.0000 |
[32m[20221213 20:21:18 @agent_ppo2.py:185][0m |          -0.0101 |           0.0217 |           0.0000 |
[32m[20221213 20:21:19 @agent_ppo2.py:185][0m |          -0.0138 |           0.0193 |           0.0000 |
[32m[20221213 20:21:19 @agent_ppo2.py:185][0m |          -0.0167 |           0.0182 |           0.0000 |
[32m[20221213 20:21:19 @agent_ppo2.py:185][0m |          -0.0184 |           0.0181 |           0.0000 |
[32m[20221213 20:21:19 @agent_ppo2.py:185][0m |          -0.0193 |           0.0174 |           0.0000 |
[32m[20221213 20:21:20 @agent_ppo2.py:185][0m |          -0.0210 |           0.0168 |           0.0000 |
[32m[20221213 20:21:20 @agent_ppo2.py:185][0m |          -0.0222 |           0.0172 |           0.0000 |
[32m[20221213 20:21:20 @agent_ppo2.py:185][0m |          -0.0221 |           0.0167 |           0.0000 |
[32m[20221213 20:21:20 @agent_ppo2.py:185][0m |          -0.0222 |           0.0167 |           0.0000 |
[32m[20221213 20:21:20 @agent_ppo2.py:130][0m Policy update time: 2.74 s
[32m[20221213 20:21:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.72
[32m[20221213 20:21:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.73
[32m[20221213 20:21:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.14
[32m[20221213 20:21:21 @agent_ppo2.py:143][0m Total time:      12.91 min
[32m[20221213 20:21:21 @agent_ppo2.py:145][0m 391168 total steps have happened
[32m[20221213 20:21:21 @agent_ppo2.py:121][0m #------------------------ Iteration 191 --------------------------#
[32m[20221213 20:21:21 @agent_ppo2.py:127][0m Sampling time: 0.53 s by 10 slaves
[32m[20221213 20:21:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:22 @agent_ppo2.py:185][0m |           0.0037 |           0.0321 |           0.0000 |
[32m[20221213 20:21:22 @agent_ppo2.py:185][0m |          -0.0096 |           0.0237 |           0.0000 |
[32m[20221213 20:21:22 @agent_ppo2.py:185][0m |          -0.0136 |           0.0226 |           0.0000 |
[32m[20221213 20:21:23 @agent_ppo2.py:185][0m |          -0.0155 |           0.0211 |           0.0000 |
[32m[20221213 20:21:23 @agent_ppo2.py:185][0m |          -0.0174 |           0.0205 |           0.0000 |
[32m[20221213 20:21:23 @agent_ppo2.py:185][0m |          -0.0204 |           0.0197 |           0.0000 |
[32m[20221213 20:21:23 @agent_ppo2.py:185][0m |          -0.0210 |           0.0200 |           0.0000 |
[32m[20221213 20:21:24 @agent_ppo2.py:185][0m |          -0.0254 |           0.0195 |           0.0000 |
[32m[20221213 20:21:24 @agent_ppo2.py:185][0m |          -0.0229 |           0.0192 |           0.0000 |
[32m[20221213 20:21:24 @agent_ppo2.py:185][0m |          -0.0247 |           0.0190 |           0.0000 |
[32m[20221213 20:21:24 @agent_ppo2.py:130][0m Policy update time: 2.78 s
[32m[20221213 20:21:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.42
[32m[20221213 20:21:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.59
[32m[20221213 20:21:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.05
[32m[20221213 20:21:25 @agent_ppo2.py:143][0m Total time:      12.97 min
[32m[20221213 20:21:25 @agent_ppo2.py:145][0m 393216 total steps have happened
[32m[20221213 20:21:25 @agent_ppo2.py:121][0m #------------------------ Iteration 192 --------------------------#
[32m[20221213 20:21:25 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:21:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:26 @agent_ppo2.py:185][0m |           0.0018 |           0.0182 |           0.0000 |
[32m[20221213 20:21:26 @agent_ppo2.py:185][0m |          -0.0113 |           0.0132 |           0.0000 |
[32m[20221213 20:21:26 @agent_ppo2.py:185][0m |          -0.0157 |           0.0128 |           0.0000 |
[32m[20221213 20:21:26 @agent_ppo2.py:185][0m |          -0.0132 |           0.0127 |           0.0000 |
[32m[20221213 20:21:27 @agent_ppo2.py:185][0m |          -0.0201 |           0.0124 |           0.0000 |
[32m[20221213 20:21:27 @agent_ppo2.py:185][0m |          -0.0211 |           0.0121 |           0.0000 |
[32m[20221213 20:21:27 @agent_ppo2.py:185][0m |          -0.0227 |           0.0119 |           0.0000 |
[32m[20221213 20:21:27 @agent_ppo2.py:185][0m |          -0.0233 |           0.0118 |           0.0000 |
[32m[20221213 20:21:28 @agent_ppo2.py:185][0m |          -0.0239 |           0.0117 |           0.0000 |
[32m[20221213 20:21:28 @agent_ppo2.py:185][0m |          -0.0244 |           0.0115 |           0.0000 |
[32m[20221213 20:21:28 @agent_ppo2.py:130][0m Policy update time: 2.79 s
[32m[20221213 20:21:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.70
[32m[20221213 20:21:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.81
[32m[20221213 20:21:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.58
[32m[20221213 20:21:28 @agent_ppo2.py:143][0m Total time:      13.04 min
[32m[20221213 20:21:28 @agent_ppo2.py:145][0m 395264 total steps have happened
[32m[20221213 20:21:28 @agent_ppo2.py:121][0m #------------------------ Iteration 193 --------------------------#
[32m[20221213 20:21:29 @agent_ppo2.py:127][0m Sampling time: 0.69 s by 10 slaves
[32m[20221213 20:21:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:30 @agent_ppo2.py:185][0m |           0.0015 |           0.0143 |           0.0000 |
[32m[20221213 20:21:30 @agent_ppo2.py:185][0m |          -0.0123 |           0.0128 |           0.0000 |
[32m[20221213 20:21:30 @agent_ppo2.py:185][0m |          -0.0200 |           0.0122 |           0.0000 |
[32m[20221213 20:21:30 @agent_ppo2.py:185][0m |          -0.0220 |           0.0116 |           0.0000 |
[32m[20221213 20:21:31 @agent_ppo2.py:185][0m |          -0.0227 |           0.0114 |           0.0000 |
[32m[20221213 20:21:31 @agent_ppo2.py:185][0m |          -0.0248 |           0.0112 |           0.0000 |
[32m[20221213 20:21:31 @agent_ppo2.py:185][0m |          -0.0265 |           0.0111 |           0.0000 |
[32m[20221213 20:21:32 @agent_ppo2.py:185][0m |          -0.0270 |           0.0109 |           0.0000 |
[32m[20221213 20:21:32 @agent_ppo2.py:185][0m |          -0.0283 |           0.0108 |           0.0000 |
[32m[20221213 20:21:32 @agent_ppo2.py:185][0m |          -0.0292 |           0.0107 |           0.0000 |
[32m[20221213 20:21:32 @agent_ppo2.py:130][0m Policy update time: 2.94 s
[32m[20221213 20:21:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.35
[32m[20221213 20:21:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.88
[32m[20221213 20:21:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.50
[32m[20221213 20:21:33 @agent_ppo2.py:143][0m Total time:      13.11 min
[32m[20221213 20:21:33 @agent_ppo2.py:145][0m 397312 total steps have happened
[32m[20221213 20:21:33 @agent_ppo2.py:121][0m #------------------------ Iteration 194 --------------------------#
[32m[20221213 20:21:33 @agent_ppo2.py:127][0m Sampling time: 0.45 s by 10 slaves
[32m[20221213 20:21:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:34 @agent_ppo2.py:185][0m |           0.0012 |           0.0230 |           0.0000 |
[32m[20221213 20:21:34 @agent_ppo2.py:185][0m |          -0.0089 |           0.0183 |           0.0000 |
[32m[20221213 20:21:34 @agent_ppo2.py:185][0m |          -0.0136 |           0.0172 |           0.0000 |
[32m[20221213 20:21:34 @agent_ppo2.py:185][0m |          -0.0222 |           0.0165 |           0.0000 |
[32m[20221213 20:21:35 @agent_ppo2.py:185][0m |          -0.0173 |           0.0166 |           0.0000 |
[32m[20221213 20:21:35 @agent_ppo2.py:185][0m |          -0.0179 |           0.0157 |           0.0000 |
[32m[20221213 20:21:35 @agent_ppo2.py:185][0m |          -0.0196 |           0.0156 |           0.0000 |
[32m[20221213 20:21:36 @agent_ppo2.py:185][0m |          -0.0233 |           0.0155 |           0.0000 |
[32m[20221213 20:21:36 @agent_ppo2.py:185][0m |          -0.0181 |           0.0155 |           0.0000 |
[32m[20221213 20:21:36 @agent_ppo2.py:185][0m |          -0.0208 |           0.0152 |           0.0000 |
[32m[20221213 20:21:36 @agent_ppo2.py:130][0m Policy update time: 3.08 s
[32m[20221213 20:21:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.40
[32m[20221213 20:21:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.72
[32m[20221213 20:21:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.51
[32m[20221213 20:21:37 @agent_ppo2.py:143][0m Total time:      13.17 min
[32m[20221213 20:21:37 @agent_ppo2.py:145][0m 399360 total steps have happened
[32m[20221213 20:21:37 @agent_ppo2.py:121][0m #------------------------ Iteration 195 --------------------------#
[32m[20221213 20:21:37 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:21:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:38 @agent_ppo2.py:185][0m |           0.0020 |           0.0282 |           0.0000 |
[32m[20221213 20:21:38 @agent_ppo2.py:185][0m |          -0.0095 |           0.0234 |           0.0000 |
[32m[20221213 20:21:38 @agent_ppo2.py:185][0m |          -0.0145 |           0.0209 |           0.0000 |
[32m[20221213 20:21:38 @agent_ppo2.py:185][0m |          -0.0171 |           0.0202 |           0.0000 |
[32m[20221213 20:21:39 @agent_ppo2.py:185][0m |          -0.0195 |           0.0196 |           0.0000 |
[32m[20221213 20:21:39 @agent_ppo2.py:185][0m |          -0.0195 |           0.0195 |           0.0000 |
[32m[20221213 20:21:39 @agent_ppo2.py:185][0m |          -0.0217 |           0.0191 |           0.0000 |
[32m[20221213 20:21:40 @agent_ppo2.py:185][0m |          -0.0236 |           0.0185 |           0.0000 |
[32m[20221213 20:21:40 @agent_ppo2.py:185][0m |          -0.0225 |           0.0181 |           0.0000 |
[32m[20221213 20:21:40 @agent_ppo2.py:185][0m |          -0.0258 |           0.0179 |           0.0000 |
[32m[20221213 20:21:40 @agent_ppo2.py:130][0m Policy update time: 2.92 s
[32m[20221213 20:21:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.74
[32m[20221213 20:21:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.02
[32m[20221213 20:21:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.93
[32m[20221213 20:21:41 @agent_ppo2.py:143][0m Total time:      13.24 min
[32m[20221213 20:21:41 @agent_ppo2.py:145][0m 401408 total steps have happened
[32m[20221213 20:21:41 @agent_ppo2.py:121][0m #------------------------ Iteration 196 --------------------------#
[32m[20221213 20:21:41 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:21:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:42 @agent_ppo2.py:185][0m |           0.0034 |           0.0310 |           0.0000 |
[32m[20221213 20:21:42 @agent_ppo2.py:185][0m |          -0.0103 |           0.0240 |           0.0000 |
[32m[20221213 20:21:42 @agent_ppo2.py:185][0m |          -0.0141 |           0.0224 |           0.0000 |
[32m[20221213 20:21:42 @agent_ppo2.py:185][0m |          -0.0178 |           0.0213 |           0.0000 |
[32m[20221213 20:21:43 @agent_ppo2.py:185][0m |          -0.0176 |           0.0215 |           0.0000 |
[32m[20221213 20:21:43 @agent_ppo2.py:185][0m |          -0.0210 |           0.0203 |           0.0000 |
[32m[20221213 20:21:43 @agent_ppo2.py:185][0m |          -0.0220 |           0.0201 |           0.0000 |
[32m[20221213 20:21:44 @agent_ppo2.py:185][0m |          -0.0227 |           0.0198 |           0.0000 |
[32m[20221213 20:21:44 @agent_ppo2.py:185][0m |          -0.0242 |           0.0197 |           0.0000 |
[32m[20221213 20:21:44 @agent_ppo2.py:185][0m |          -0.0239 |           0.0197 |           0.0000 |
[32m[20221213 20:21:44 @agent_ppo2.py:130][0m Policy update time: 2.92 s
[32m[20221213 20:21:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.19
[32m[20221213 20:21:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.30
[32m[20221213 20:21:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.19
[32m[20221213 20:21:45 @agent_ppo2.py:143][0m Total time:      13.31 min
[32m[20221213 20:21:45 @agent_ppo2.py:145][0m 403456 total steps have happened
[32m[20221213 20:21:45 @agent_ppo2.py:121][0m #------------------------ Iteration 197 --------------------------#
[32m[20221213 20:21:45 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:21:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:46 @agent_ppo2.py:185][0m |          -0.0014 |           0.0281 |           0.0000 |
[32m[20221213 20:21:46 @agent_ppo2.py:185][0m |          -0.0118 |           0.0236 |           0.0000 |
[32m[20221213 20:21:46 @agent_ppo2.py:185][0m |          -0.0212 |           0.0216 |           0.0000 |
[32m[20221213 20:21:46 @agent_ppo2.py:185][0m |          -0.0238 |           0.0204 |           0.0000 |
[32m[20221213 20:21:47 @agent_ppo2.py:185][0m |          -0.0255 |           0.0200 |           0.0000 |
[32m[20221213 20:21:47 @agent_ppo2.py:185][0m |          -0.0278 |           0.0202 |           0.0000 |
[32m[20221213 20:21:47 @agent_ppo2.py:185][0m |          -0.0253 |           0.0190 |           0.0000 |
[32m[20221213 20:21:47 @agent_ppo2.py:185][0m |          -0.0307 |           0.0188 |           0.0000 |
[32m[20221213 20:21:48 @agent_ppo2.py:185][0m |          -0.0298 |           0.0185 |           0.0000 |
[32m[20221213 20:21:48 @agent_ppo2.py:185][0m |          -0.0328 |           0.0181 |           0.0000 |
[32m[20221213 20:21:48 @agent_ppo2.py:130][0m Policy update time: 2.88 s
[32m[20221213 20:21:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.60
[32m[20221213 20:21:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.47
[32m[20221213 20:21:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.07
[32m[20221213 20:21:49 @agent_ppo2.py:143][0m Total time:      13.37 min
[32m[20221213 20:21:49 @agent_ppo2.py:145][0m 405504 total steps have happened
[32m[20221213 20:21:49 @agent_ppo2.py:121][0m #------------------------ Iteration 198 --------------------------#
[32m[20221213 20:21:49 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:21:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:49 @agent_ppo2.py:185][0m |           0.0028 |           0.0203 |           0.0000 |
[32m[20221213 20:21:50 @agent_ppo2.py:185][0m |          -0.0122 |           0.0190 |           0.0000 |
[32m[20221213 20:21:50 @agent_ppo2.py:185][0m |          -0.0167 |           0.0181 |           0.0000 |
[32m[20221213 20:21:50 @agent_ppo2.py:185][0m |          -0.0195 |           0.0179 |           0.0000 |
[32m[20221213 20:21:50 @agent_ppo2.py:185][0m |          -0.0228 |           0.0173 |           0.0000 |
[32m[20221213 20:21:51 @agent_ppo2.py:185][0m |          -0.0234 |           0.0168 |           0.0000 |
[32m[20221213 20:21:51 @agent_ppo2.py:185][0m |          -0.0270 |           0.0168 |           0.0000 |
[32m[20221213 20:21:51 @agent_ppo2.py:185][0m |          -0.0279 |           0.0163 |           0.0000 |
[32m[20221213 20:21:52 @agent_ppo2.py:185][0m |          -0.0279 |           0.0164 |           0.0000 |
[32m[20221213 20:21:52 @agent_ppo2.py:185][0m |          -0.0295 |           0.0163 |           0.0000 |
[32m[20221213 20:21:52 @agent_ppo2.py:130][0m Policy update time: 2.87 s
[32m[20221213 20:21:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.11
[32m[20221213 20:21:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.03
[32m[20221213 20:21:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.33
[32m[20221213 20:21:52 @agent_ppo2.py:143][0m Total time:      13.43 min
[32m[20221213 20:21:52 @agent_ppo2.py:145][0m 407552 total steps have happened
[32m[20221213 20:21:52 @agent_ppo2.py:121][0m #------------------------ Iteration 199 --------------------------#
[32m[20221213 20:21:53 @agent_ppo2.py:127][0m Sampling time: 0.59 s by 10 slaves
[32m[20221213 20:21:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:54 @agent_ppo2.py:185][0m |           0.0008 |           0.0203 |           0.0000 |
[32m[20221213 20:21:54 @agent_ppo2.py:185][0m |          -0.0156 |           0.0180 |           0.0000 |
[32m[20221213 20:21:54 @agent_ppo2.py:185][0m |          -0.0146 |           0.0171 |           0.0000 |
[32m[20221213 20:21:54 @agent_ppo2.py:185][0m |          -0.0203 |           0.0167 |           0.0000 |
[32m[20221213 20:21:55 @agent_ppo2.py:185][0m |          -0.0230 |           0.0162 |           0.0000 |
[32m[20221213 20:21:55 @agent_ppo2.py:185][0m |          -0.0265 |           0.0162 |           0.0000 |
[32m[20221213 20:21:55 @agent_ppo2.py:185][0m |          -0.0264 |           0.0163 |           0.0000 |
[32m[20221213 20:21:56 @agent_ppo2.py:185][0m |          -0.0301 |           0.0158 |           0.0000 |
[32m[20221213 20:21:56 @agent_ppo2.py:185][0m |          -0.0292 |           0.0158 |           0.0000 |
[32m[20221213 20:21:56 @agent_ppo2.py:185][0m |          -0.0304 |           0.0155 |           0.0000 |
[32m[20221213 20:21:56 @agent_ppo2.py:130][0m Policy update time: 3.11 s
[32m[20221213 20:21:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.28
[32m[20221213 20:21:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.63
[32m[20221213 20:21:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.03
[32m[20221213 20:21:57 @agent_ppo2.py:143][0m Total time:      13.51 min
[32m[20221213 20:21:57 @agent_ppo2.py:145][0m 409600 total steps have happened
[32m[20221213 20:21:57 @agent_ppo2.py:121][0m #------------------------ Iteration 200 --------------------------#
[32m[20221213 20:21:57 @agent_ppo2.py:127][0m Sampling time: 0.65 s by 10 slaves
[32m[20221213 20:21:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:21:58 @agent_ppo2.py:185][0m |           0.0018 |           0.0203 |           0.0000 |
[32m[20221213 20:21:58 @agent_ppo2.py:185][0m |          -0.0139 |           0.0180 |           0.0000 |
[32m[20221213 20:21:58 @agent_ppo2.py:185][0m |          -0.0155 |           0.0169 |           0.0000 |
[32m[20221213 20:21:59 @agent_ppo2.py:185][0m |          -0.0181 |           0.0164 |           0.0000 |
[32m[20221213 20:21:59 @agent_ppo2.py:185][0m |          -0.0197 |           0.0160 |           0.0000 |
[32m[20221213 20:21:59 @agent_ppo2.py:185][0m |          -0.0210 |           0.0159 |           0.0000 |
[32m[20221213 20:22:00 @agent_ppo2.py:185][0m |          -0.0236 |           0.0156 |           0.0000 |
[32m[20221213 20:22:00 @agent_ppo2.py:185][0m |          -0.0220 |           0.0154 |           0.0000 |
[32m[20221213 20:22:00 @agent_ppo2.py:185][0m |          -0.0244 |           0.0155 |           0.0000 |
[32m[20221213 20:22:00 @agent_ppo2.py:185][0m |          -0.0248 |           0.0154 |           0.0000 |
[32m[20221213 20:22:00 @agent_ppo2.py:130][0m Policy update time: 3.15 s
[32m[20221213 20:22:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.18
[32m[20221213 20:22:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.68
[32m[20221213 20:22:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.49
[32m[20221213 20:22:01 @agent_ppo2.py:143][0m Total time:      13.58 min
[32m[20221213 20:22:01 @agent_ppo2.py:145][0m 411648 total steps have happened
[32m[20221213 20:22:01 @agent_ppo2.py:121][0m #------------------------ Iteration 201 --------------------------#
[32m[20221213 20:22:01 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:22:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:02 @agent_ppo2.py:185][0m |           0.0018 |           0.0179 |           0.0000 |
[32m[20221213 20:22:02 @agent_ppo2.py:185][0m |          -0.0127 |           0.0088 |           0.0000 |
[32m[20221213 20:22:02 @agent_ppo2.py:185][0m |          -0.0164 |           0.0084 |           0.0000 |
[32m[20221213 20:22:03 @agent_ppo2.py:185][0m |          -0.0190 |           0.0082 |           0.0000 |
[32m[20221213 20:22:03 @agent_ppo2.py:185][0m |          -0.0198 |           0.0079 |           0.0000 |
[32m[20221213 20:22:03 @agent_ppo2.py:185][0m |          -0.0208 |           0.0077 |           0.0000 |
[32m[20221213 20:22:04 @agent_ppo2.py:185][0m |          -0.0213 |           0.0074 |           0.0000 |
[32m[20221213 20:22:04 @agent_ppo2.py:185][0m |          -0.0200 |           0.0072 |           0.0000 |
[32m[20221213 20:22:04 @agent_ppo2.py:185][0m |          -0.0232 |           0.0070 |           0.0000 |
[32m[20221213 20:22:05 @agent_ppo2.py:185][0m |          -0.0209 |           0.0068 |           0.0000 |
[32m[20221213 20:22:05 @agent_ppo2.py:130][0m Policy update time: 3.07 s
[32m[20221213 20:22:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.33
[32m[20221213 20:22:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.63
[32m[20221213 20:22:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.19
[32m[20221213 20:22:05 @agent_ppo2.py:143][0m Total time:      13.65 min
[32m[20221213 20:22:05 @agent_ppo2.py:145][0m 413696 total steps have happened
[32m[20221213 20:22:05 @agent_ppo2.py:121][0m #------------------------ Iteration 202 --------------------------#
[32m[20221213 20:22:06 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:22:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:06 @agent_ppo2.py:185][0m |           0.0036 |           0.0145 |           0.0000 |
[32m[20221213 20:22:06 @agent_ppo2.py:185][0m |          -0.0082 |           0.0102 |           0.0000 |
[32m[20221213 20:22:07 @agent_ppo2.py:185][0m |          -0.0134 |           0.0098 |           0.0000 |
[32m[20221213 20:22:07 @agent_ppo2.py:185][0m |          -0.0158 |           0.0096 |           0.0000 |
[32m[20221213 20:22:07 @agent_ppo2.py:185][0m |          -0.0159 |           0.0094 |           0.0000 |
[32m[20221213 20:22:08 @agent_ppo2.py:185][0m |          -0.0222 |           0.0092 |           0.0000 |
[32m[20221213 20:22:08 @agent_ppo2.py:185][0m |          -0.0191 |           0.0095 |           0.0000 |
[32m[20221213 20:22:08 @agent_ppo2.py:185][0m |          -0.0185 |           0.0090 |           0.0000 |
[32m[20221213 20:22:08 @agent_ppo2.py:185][0m |          -0.0191 |           0.0091 |           0.0000 |
[32m[20221213 20:22:09 @agent_ppo2.py:185][0m |          -0.0197 |           0.0089 |           0.0000 |
[32m[20221213 20:22:09 @agent_ppo2.py:130][0m Policy update time: 3.08 s
[32m[20221213 20:22:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.20
[32m[20221213 20:22:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.15
[32m[20221213 20:22:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.83
[32m[20221213 20:22:09 @agent_ppo2.py:143][0m Total time:      13.71 min
[32m[20221213 20:22:09 @agent_ppo2.py:145][0m 415744 total steps have happened
[32m[20221213 20:22:09 @agent_ppo2.py:121][0m #------------------------ Iteration 203 --------------------------#
[32m[20221213 20:22:10 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:22:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:10 @agent_ppo2.py:185][0m |           0.0033 |           0.0949 |           0.0000 |
[32m[20221213 20:22:10 @agent_ppo2.py:185][0m |          -0.0096 |           0.0477 |           0.0000 |
[32m[20221213 20:22:11 @agent_ppo2.py:185][0m |          -0.0157 |           0.0383 |           0.0000 |
[32m[20221213 20:22:11 @agent_ppo2.py:185][0m |          -0.0165 |           0.0344 |           0.0000 |
[32m[20221213 20:22:11 @agent_ppo2.py:185][0m |          -0.0180 |           0.0326 |           0.0000 |
[32m[20221213 20:22:11 @agent_ppo2.py:185][0m |          -0.0190 |           0.0304 |           0.0000 |
[32m[20221213 20:22:12 @agent_ppo2.py:185][0m |          -0.0199 |           0.0301 |           0.0000 |
[32m[20221213 20:22:12 @agent_ppo2.py:185][0m |          -0.0199 |           0.0290 |           0.0000 |
[32m[20221213 20:22:12 @agent_ppo2.py:185][0m |          -0.0213 |           0.0283 |           0.0000 |
[32m[20221213 20:22:12 @agent_ppo2.py:185][0m |          -0.0216 |           0.0278 |           0.0000 |
[32m[20221213 20:22:12 @agent_ppo2.py:130][0m Policy update time: 2.87 s
[32m[20221213 20:22:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 11.04
[32m[20221213 20:22:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 12.25
[32m[20221213 20:22:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.20
[32m[20221213 20:22:13 @agent_ppo2.py:143][0m Total time:      13.78 min
[32m[20221213 20:22:13 @agent_ppo2.py:145][0m 417792 total steps have happened
[32m[20221213 20:22:13 @agent_ppo2.py:121][0m #------------------------ Iteration 204 --------------------------#
[32m[20221213 20:22:13 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:22:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:14 @agent_ppo2.py:185][0m |           0.0003 |           0.0281 |           0.0000 |
[32m[20221213 20:22:14 @agent_ppo2.py:185][0m |          -0.0169 |           0.0223 |           0.0000 |
[32m[20221213 20:22:14 @agent_ppo2.py:185][0m |          -0.0237 |           0.0209 |           0.0000 |
[32m[20221213 20:22:15 @agent_ppo2.py:185][0m |          -0.0299 |           0.0199 |           0.0000 |
[32m[20221213 20:22:15 @agent_ppo2.py:185][0m |          -0.0309 |           0.0192 |           0.0000 |
[32m[20221213 20:22:15 @agent_ppo2.py:185][0m |          -0.0330 |           0.0188 |           0.0000 |
[32m[20221213 20:22:16 @agent_ppo2.py:185][0m |          -0.0351 |           0.0187 |           0.0000 |
[32m[20221213 20:22:16 @agent_ppo2.py:185][0m |          -0.0364 |           0.0189 |           0.0000 |
[32m[20221213 20:22:16 @agent_ppo2.py:185][0m |          -0.0385 |           0.0180 |           0.0000 |
[32m[20221213 20:22:17 @agent_ppo2.py:185][0m |          -0.0388 |           0.0178 |           0.0000 |
[32m[20221213 20:22:17 @agent_ppo2.py:130][0m Policy update time: 3.28 s
[32m[20221213 20:22:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.86
[32m[20221213 20:22:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.95
[32m[20221213 20:22:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.37
[32m[20221213 20:22:17 @agent_ppo2.py:143][0m Total time:      13.85 min
[32m[20221213 20:22:17 @agent_ppo2.py:145][0m 419840 total steps have happened
[32m[20221213 20:22:17 @agent_ppo2.py:121][0m #------------------------ Iteration 205 --------------------------#
[32m[20221213 20:22:18 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:22:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:18 @agent_ppo2.py:185][0m |           0.0015 |           0.0571 |           0.0000 |
[32m[20221213 20:22:19 @agent_ppo2.py:185][0m |          -0.0100 |           0.0364 |           0.0000 |
[32m[20221213 20:22:19 @agent_ppo2.py:185][0m |          -0.0154 |           0.0327 |           0.0000 |
[32m[20221213 20:22:19 @agent_ppo2.py:185][0m |          -0.0168 |           0.0300 |           0.0000 |
[32m[20221213 20:22:19 @agent_ppo2.py:185][0m |          -0.0186 |           0.0293 |           0.0000 |
[32m[20221213 20:22:20 @agent_ppo2.py:185][0m |          -0.0197 |           0.0282 |           0.0000 |
[32m[20221213 20:22:20 @agent_ppo2.py:185][0m |          -0.0237 |           0.0271 |           0.0000 |
[32m[20221213 20:22:20 @agent_ppo2.py:185][0m |          -0.0214 |           0.0265 |           0.0000 |
[32m[20221213 20:22:21 @agent_ppo2.py:185][0m |          -0.0220 |           0.0264 |           0.0000 |
[32m[20221213 20:22:21 @agent_ppo2.py:185][0m |          -0.0233 |           0.0252 |           0.0000 |
[32m[20221213 20:22:21 @agent_ppo2.py:130][0m Policy update time: 3.05 s
[32m[20221213 20:22:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.56
[32m[20221213 20:22:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.19
[32m[20221213 20:22:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.92
[32m[20221213 20:22:21 @agent_ppo2.py:143][0m Total time:      13.92 min
[32m[20221213 20:22:21 @agent_ppo2.py:145][0m 421888 total steps have happened
[32m[20221213 20:22:21 @agent_ppo2.py:121][0m #------------------------ Iteration 206 --------------------------#
[32m[20221213 20:22:22 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:22:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:22 @agent_ppo2.py:185][0m |           0.0028 |           0.0271 |           0.0000 |
[32m[20221213 20:22:22 @agent_ppo2.py:185][0m |          -0.0140 |           0.0161 |           0.0000 |
[32m[20221213 20:22:23 @agent_ppo2.py:185][0m |          -0.0187 |           0.0154 |           0.0000 |
[32m[20221213 20:22:23 @agent_ppo2.py:185][0m |          -0.0202 |           0.0151 |           0.0000 |
[32m[20221213 20:22:23 @agent_ppo2.py:185][0m |          -0.0225 |           0.0147 |           0.0000 |
[32m[20221213 20:22:24 @agent_ppo2.py:185][0m |          -0.0231 |           0.0143 |           0.0000 |
[32m[20221213 20:22:24 @agent_ppo2.py:185][0m |          -0.0247 |           0.0143 |           0.0000 |
[32m[20221213 20:22:24 @agent_ppo2.py:185][0m |          -0.0245 |           0.0140 |           0.0000 |
[32m[20221213 20:22:24 @agent_ppo2.py:185][0m |          -0.0251 |           0.0140 |           0.0000 |
[32m[20221213 20:22:25 @agent_ppo2.py:185][0m |          -0.0261 |           0.0135 |           0.0000 |
[32m[20221213 20:22:25 @agent_ppo2.py:130][0m Policy update time: 2.93 s
[32m[20221213 20:22:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.05
[32m[20221213 20:22:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.24
[32m[20221213 20:22:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.87
[32m[20221213 20:22:25 @agent_ppo2.py:143][0m Total time:      13.98 min
[32m[20221213 20:22:25 @agent_ppo2.py:145][0m 423936 total steps have happened
[32m[20221213 20:22:25 @agent_ppo2.py:121][0m #------------------------ Iteration 207 --------------------------#
[32m[20221213 20:22:26 @agent_ppo2.py:127][0m Sampling time: 0.60 s by 10 slaves
[32m[20221213 20:22:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:26 @agent_ppo2.py:185][0m |           0.0041 |           0.0488 |           0.0000 |
[32m[20221213 20:22:27 @agent_ppo2.py:185][0m |          -0.0097 |           0.0251 |           0.0000 |
[32m[20221213 20:22:27 @agent_ppo2.py:185][0m |          -0.0121 |           0.0227 |           0.0000 |
[32m[20221213 20:22:27 @agent_ppo2.py:185][0m |          -0.0153 |           0.0217 |           0.0000 |
[32m[20221213 20:22:27 @agent_ppo2.py:185][0m |          -0.0166 |           0.0213 |           0.0000 |
[32m[20221213 20:22:28 @agent_ppo2.py:185][0m |          -0.0178 |           0.0206 |           0.0000 |
[32m[20221213 20:22:28 @agent_ppo2.py:185][0m |          -0.0182 |           0.0203 |           0.0000 |
[32m[20221213 20:22:28 @agent_ppo2.py:185][0m |          -0.0180 |           0.0201 |           0.0000 |
[32m[20221213 20:22:29 @agent_ppo2.py:185][0m |          -0.0217 |           0.0200 |           0.0000 |
[32m[20221213 20:22:29 @agent_ppo2.py:185][0m |          -0.0184 |           0.0197 |           0.0000 |
[32m[20221213 20:22:29 @agent_ppo2.py:130][0m Policy update time: 2.91 s
[32m[20221213 20:22:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.19
[32m[20221213 20:22:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.31
[32m[20221213 20:22:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.08
[32m[20221213 20:22:29 @agent_ppo2.py:143][0m Total time:      14.05 min
[32m[20221213 20:22:29 @agent_ppo2.py:145][0m 425984 total steps have happened
[32m[20221213 20:22:29 @agent_ppo2.py:121][0m #------------------------ Iteration 208 --------------------------#
[32m[20221213 20:22:30 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:22:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:30 @agent_ppo2.py:185][0m |           0.0030 |           0.0206 |           0.0000 |
[32m[20221213 20:22:31 @agent_ppo2.py:185][0m |          -0.0148 |           0.0137 |           0.0000 |
[32m[20221213 20:22:31 @agent_ppo2.py:185][0m |          -0.0188 |           0.0134 |           0.0000 |
[32m[20221213 20:22:31 @agent_ppo2.py:185][0m |          -0.0207 |           0.0131 |           0.0000 |
[32m[20221213 20:22:31 @agent_ppo2.py:185][0m |          -0.0215 |           0.0129 |           0.0000 |
[32m[20221213 20:22:32 @agent_ppo2.py:185][0m |          -0.0233 |           0.0125 |           0.0000 |
[32m[20221213 20:22:32 @agent_ppo2.py:185][0m |          -0.0204 |           0.0126 |           0.0000 |
[32m[20221213 20:22:32 @agent_ppo2.py:185][0m |          -0.0237 |           0.0122 |           0.0000 |
[32m[20221213 20:22:32 @agent_ppo2.py:185][0m |          -0.0248 |           0.0120 |           0.0000 |
[32m[20221213 20:22:33 @agent_ppo2.py:185][0m |          -0.0263 |           0.0120 |           0.0000 |
[32m[20221213 20:22:33 @agent_ppo2.py:130][0m Policy update time: 2.83 s
[32m[20221213 20:22:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.48
[32m[20221213 20:22:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.46
[32m[20221213 20:22:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 12.10
[32m[20221213 20:22:33 @agent_ppo2.py:143][0m Total time:      14.12 min
[32m[20221213 20:22:33 @agent_ppo2.py:145][0m 428032 total steps have happened
[32m[20221213 20:22:33 @agent_ppo2.py:121][0m #------------------------ Iteration 209 --------------------------#
[32m[20221213 20:22:34 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:22:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:34 @agent_ppo2.py:185][0m |           0.0037 |           0.0207 |           0.0000 |
[32m[20221213 20:22:34 @agent_ppo2.py:185][0m |          -0.0119 |           0.0185 |           0.0000 |
[32m[20221213 20:22:35 @agent_ppo2.py:185][0m |          -0.0167 |           0.0173 |           0.0000 |
[32m[20221213 20:22:35 @agent_ppo2.py:185][0m |          -0.0196 |           0.0165 |           0.0000 |
[32m[20221213 20:22:35 @agent_ppo2.py:185][0m |          -0.0213 |           0.0162 |           0.0000 |
[32m[20221213 20:22:35 @agent_ppo2.py:185][0m |          -0.0237 |           0.0161 |           0.0000 |
[32m[20221213 20:22:36 @agent_ppo2.py:185][0m |          -0.0255 |           0.0158 |           0.0000 |
[32m[20221213 20:22:36 @agent_ppo2.py:185][0m |          -0.0257 |           0.0157 |           0.0000 |
[32m[20221213 20:22:36 @agent_ppo2.py:185][0m |          -0.0263 |           0.0154 |           0.0000 |
[32m[20221213 20:22:36 @agent_ppo2.py:185][0m |          -0.0283 |           0.0158 |           0.0000 |
[32m[20221213 20:22:36 @agent_ppo2.py:130][0m Policy update time: 2.74 s
[32m[20221213 20:22:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.82
[32m[20221213 20:22:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.21
[32m[20221213 20:22:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.22
[32m[20221213 20:22:37 @agent_ppo2.py:143][0m Total time:      14.18 min
[32m[20221213 20:22:37 @agent_ppo2.py:145][0m 430080 total steps have happened
[32m[20221213 20:22:37 @agent_ppo2.py:121][0m #------------------------ Iteration 210 --------------------------#
[32m[20221213 20:22:38 @agent_ppo2.py:127][0m Sampling time: 0.59 s by 10 slaves
[32m[20221213 20:22:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:38 @agent_ppo2.py:185][0m |          -0.0006 |           0.0678 |           0.0000 |
[32m[20221213 20:22:38 @agent_ppo2.py:185][0m |          -0.0137 |           0.0390 |           0.0000 |
[32m[20221213 20:22:39 @agent_ppo2.py:185][0m |          -0.0134 |           0.0343 |           0.0000 |
[32m[20221213 20:22:39 @agent_ppo2.py:185][0m |          -0.0157 |           0.0314 |           0.0000 |
[32m[20221213 20:22:39 @agent_ppo2.py:185][0m |          -0.0184 |           0.0313 |           0.0000 |
[32m[20221213 20:22:39 @agent_ppo2.py:185][0m |          -0.0230 |           0.0297 |           0.0000 |
[32m[20221213 20:22:40 @agent_ppo2.py:185][0m |          -0.0194 |           0.0285 |           0.0000 |
[32m[20221213 20:22:40 @agent_ppo2.py:185][0m |          -0.0203 |           0.0278 |           0.0000 |
[32m[20221213 20:22:40 @agent_ppo2.py:185][0m |          -0.0218 |           0.0276 |           0.0000 |
[32m[20221213 20:22:40 @agent_ppo2.py:185][0m |          -0.0243 |           0.0268 |           0.0000 |
[32m[20221213 20:22:40 @agent_ppo2.py:130][0m Policy update time: 2.71 s
[32m[20221213 20:22:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.86
[32m[20221213 20:22:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 12.14
[32m[20221213 20:22:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.15
[32m[20221213 20:22:41 @agent_ppo2.py:143][0m Total time:      14.24 min
[32m[20221213 20:22:41 @agent_ppo2.py:145][0m 432128 total steps have happened
[32m[20221213 20:22:41 @agent_ppo2.py:121][0m #------------------------ Iteration 211 --------------------------#
[32m[20221213 20:22:41 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:22:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:42 @agent_ppo2.py:185][0m |           0.0007 |           0.0359 |           0.0000 |
[32m[20221213 20:22:42 @agent_ppo2.py:185][0m |          -0.0148 |           0.0116 |           0.0000 |
[32m[20221213 20:22:42 @agent_ppo2.py:185][0m |          -0.0159 |           0.0113 |           0.0000 |
[32m[20221213 20:22:43 @agent_ppo2.py:185][0m |          -0.0208 |           0.0111 |           0.0000 |
[32m[20221213 20:22:43 @agent_ppo2.py:185][0m |          -0.0217 |           0.0109 |           0.0000 |
[32m[20221213 20:22:43 @agent_ppo2.py:185][0m |          -0.0230 |           0.0107 |           0.0000 |
[32m[20221213 20:22:43 @agent_ppo2.py:185][0m |          -0.0237 |           0.0106 |           0.0000 |
[32m[20221213 20:22:44 @agent_ppo2.py:185][0m |          -0.0245 |           0.0104 |           0.0000 |
[32m[20221213 20:22:44 @agent_ppo2.py:185][0m |          -0.0250 |           0.0102 |           0.0000 |
[32m[20221213 20:22:44 @agent_ppo2.py:185][0m |          -0.0251 |           0.0099 |           0.0000 |
[32m[20221213 20:22:44 @agent_ppo2.py:130][0m Policy update time: 3.08 s
[32m[20221213 20:22:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.15
[32m[20221213 20:22:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.25
[32m[20221213 20:22:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.66
[32m[20221213 20:22:45 @agent_ppo2.py:143][0m Total time:      14.31 min
[32m[20221213 20:22:45 @agent_ppo2.py:145][0m 434176 total steps have happened
[32m[20221213 20:22:45 @agent_ppo2.py:121][0m #------------------------ Iteration 212 --------------------------#
[32m[20221213 20:22:45 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:22:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:46 @agent_ppo2.py:185][0m |           0.0043 |           0.0497 |           0.0000 |
[32m[20221213 20:22:46 @agent_ppo2.py:185][0m |          -0.0098 |           0.0271 |           0.0000 |
[32m[20221213 20:22:47 @agent_ppo2.py:185][0m |          -0.0141 |           0.0234 |           0.0000 |
[32m[20221213 20:22:47 @agent_ppo2.py:185][0m |          -0.0169 |           0.0215 |           0.0000 |
[32m[20221213 20:22:47 @agent_ppo2.py:185][0m |          -0.0193 |           0.0209 |           0.0000 |
[32m[20221213 20:22:47 @agent_ppo2.py:185][0m |          -0.0187 |           0.0195 |           0.0000 |
[32m[20221213 20:22:48 @agent_ppo2.py:185][0m |          -0.0191 |           0.0190 |           0.0000 |
[32m[20221213 20:22:48 @agent_ppo2.py:185][0m |          -0.0194 |           0.0186 |           0.0000 |
[32m[20221213 20:22:48 @agent_ppo2.py:185][0m |          -0.0202 |           0.0184 |           0.0000 |
[32m[20221213 20:22:49 @agent_ppo2.py:185][0m |          -0.0230 |           0.0182 |           0.0000 |
[32m[20221213 20:22:49 @agent_ppo2.py:130][0m Policy update time: 3.42 s
[32m[20221213 20:22:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.42
[32m[20221213 20:22:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.63
[32m[20221213 20:22:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.56
[32m[20221213 20:22:49 @agent_ppo2.py:143][0m Total time:      14.38 min
[32m[20221213 20:22:49 @agent_ppo2.py:145][0m 436224 total steps have happened
[32m[20221213 20:22:49 @agent_ppo2.py:121][0m #------------------------ Iteration 213 --------------------------#
[32m[20221213 20:22:50 @agent_ppo2.py:127][0m Sampling time: 0.62 s by 10 slaves
[32m[20221213 20:22:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:50 @agent_ppo2.py:185][0m |           0.0036 |           0.0298 |           0.0000 |
[32m[20221213 20:22:51 @agent_ppo2.py:185][0m |          -0.0081 |           0.0216 |           0.0000 |
[32m[20221213 20:22:51 @agent_ppo2.py:185][0m |          -0.0121 |           0.0204 |           0.0000 |
[32m[20221213 20:22:51 @agent_ppo2.py:185][0m |          -0.0155 |           0.0198 |           0.0000 |
[32m[20221213 20:22:52 @agent_ppo2.py:185][0m |          -0.0168 |           0.0194 |           0.0000 |
[32m[20221213 20:22:52 @agent_ppo2.py:185][0m |          -0.0225 |           0.0190 |           0.0000 |
[32m[20221213 20:22:52 @agent_ppo2.py:185][0m |          -0.0188 |           0.0189 |           0.0000 |
[32m[20221213 20:22:53 @agent_ppo2.py:185][0m |          -0.0240 |           0.0182 |           0.0000 |
[32m[20221213 20:22:53 @agent_ppo2.py:185][0m |          -0.0221 |           0.0180 |           0.0000 |
[32m[20221213 20:22:53 @agent_ppo2.py:185][0m |          -0.0219 |           0.0180 |           0.0000 |
[32m[20221213 20:22:53 @agent_ppo2.py:130][0m Policy update time: 3.41 s
[32m[20221213 20:22:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.65
[32m[20221213 20:22:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.65
[32m[20221213 20:22:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.12
[32m[20221213 20:22:54 @agent_ppo2.py:143][0m Total time:      14.46 min
[32m[20221213 20:22:54 @agent_ppo2.py:145][0m 438272 total steps have happened
[32m[20221213 20:22:54 @agent_ppo2.py:121][0m #------------------------ Iteration 214 --------------------------#
[32m[20221213 20:22:54 @agent_ppo2.py:127][0m Sampling time: 0.55 s by 10 slaves
[32m[20221213 20:22:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:55 @agent_ppo2.py:185][0m |           0.0019 |           0.0619 |           0.0000 |
[32m[20221213 20:22:55 @agent_ppo2.py:185][0m |          -0.0124 |           0.0380 |           0.0000 |
[32m[20221213 20:22:55 @agent_ppo2.py:185][0m |          -0.0179 |           0.0335 |           0.0000 |
[32m[20221213 20:22:56 @agent_ppo2.py:185][0m |          -0.0193 |           0.0315 |           0.0000 |
[32m[20221213 20:22:56 @agent_ppo2.py:185][0m |          -0.0209 |           0.0293 |           0.0000 |
[32m[20221213 20:22:56 @agent_ppo2.py:185][0m |          -0.0221 |           0.0285 |           0.0000 |
[32m[20221213 20:22:57 @agent_ppo2.py:185][0m |          -0.0232 |           0.0273 |           0.0000 |
[32m[20221213 20:22:57 @agent_ppo2.py:185][0m |          -0.0256 |           0.0276 |           0.0000 |
[32m[20221213 20:22:57 @agent_ppo2.py:185][0m |          -0.0255 |           0.0260 |           0.0000 |
[32m[20221213 20:22:57 @agent_ppo2.py:185][0m |          -0.0259 |           0.0263 |           0.0000 |
[32m[20221213 20:22:57 @agent_ppo2.py:130][0m Policy update time: 2.91 s
[32m[20221213 20:22:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.53
[32m[20221213 20:22:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.40
[32m[20221213 20:22:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.32
[32m[20221213 20:22:58 @agent_ppo2.py:143][0m Total time:      14.53 min
[32m[20221213 20:22:58 @agent_ppo2.py:145][0m 440320 total steps have happened
[32m[20221213 20:22:58 @agent_ppo2.py:121][0m #------------------------ Iteration 215 --------------------------#
[32m[20221213 20:22:58 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:22:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:22:59 @agent_ppo2.py:185][0m |          -0.0001 |           0.0256 |           0.0000 |
[32m[20221213 20:22:59 @agent_ppo2.py:185][0m |          -0.0117 |           0.0168 |           0.0000 |
[32m[20221213 20:22:59 @agent_ppo2.py:185][0m |          -0.0168 |           0.0160 |           0.0000 |
[32m[20221213 20:23:00 @agent_ppo2.py:185][0m |          -0.0207 |           0.0154 |           0.0000 |
[32m[20221213 20:23:00 @agent_ppo2.py:185][0m |          -0.0233 |           0.0149 |           0.0000 |
[32m[20221213 20:23:00 @agent_ppo2.py:185][0m |          -0.0244 |           0.0147 |           0.0000 |
[32m[20221213 20:23:00 @agent_ppo2.py:185][0m |          -0.0257 |           0.0145 |           0.0000 |
[32m[20221213 20:23:01 @agent_ppo2.py:185][0m |          -0.0265 |           0.0141 |           0.0000 |
[32m[20221213 20:23:01 @agent_ppo2.py:185][0m |          -0.0260 |           0.0139 |           0.0000 |
[32m[20221213 20:23:01 @agent_ppo2.py:185][0m |          -0.0280 |           0.0137 |           0.0000 |
[32m[20221213 20:23:01 @agent_ppo2.py:130][0m Policy update time: 2.95 s
[32m[20221213 20:23:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.52
[32m[20221213 20:23:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.30
[32m[20221213 20:23:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.92
[32m[20221213 20:23:02 @agent_ppo2.py:143][0m Total time:      14.59 min
[32m[20221213 20:23:02 @agent_ppo2.py:145][0m 442368 total steps have happened
[32m[20221213 20:23:02 @agent_ppo2.py:121][0m #------------------------ Iteration 216 --------------------------#
[32m[20221213 20:23:02 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:23:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:23:03 @agent_ppo2.py:185][0m |           0.0033 |           0.0164 |           0.0000 |
[32m[20221213 20:23:03 @agent_ppo2.py:185][0m |          -0.0132 |           0.0152 |           0.0000 |
[32m[20221213 20:23:04 @agent_ppo2.py:185][0m |          -0.0131 |           0.0147 |           0.0000 |
[32m[20221213 20:23:04 @agent_ppo2.py:185][0m |          -0.0177 |           0.0144 |           0.0000 |
[32m[20221213 20:23:04 @agent_ppo2.py:185][0m |          -0.0190 |           0.0139 |           0.0000 |
[32m[20221213 20:23:05 @agent_ppo2.py:185][0m |          -0.0206 |           0.0137 |           0.0000 |
[32m[20221213 20:23:05 @agent_ppo2.py:185][0m |          -0.0219 |           0.0136 |           0.0000 |
[32m[20221213 20:23:05 @agent_ppo2.py:185][0m |          -0.0229 |           0.0136 |           0.0000 |
[32m[20221213 20:23:06 @agent_ppo2.py:185][0m |          -0.0233 |           0.0132 |           0.0000 |
[32m[20221213 20:23:06 @agent_ppo2.py:185][0m |          -0.0244 |           0.0133 |           0.0000 |
[32m[20221213 20:23:06 @agent_ppo2.py:130][0m Policy update time: 3.46 s
[32m[20221213 20:23:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.80
[32m[20221213 20:23:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.95
[32m[20221213 20:23:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.31
[32m[20221213 20:23:06 @agent_ppo2.py:143][0m Total time:      14.67 min
[32m[20221213 20:23:06 @agent_ppo2.py:145][0m 444416 total steps have happened
[32m[20221213 20:23:06 @agent_ppo2.py:121][0m #------------------------ Iteration 217 --------------------------#
[32m[20221213 20:23:07 @agent_ppo2.py:127][0m Sampling time: 0.62 s by 10 slaves
[32m[20221213 20:23:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:23:07 @agent_ppo2.py:185][0m |           0.0036 |           0.0242 |           0.0000 |
[32m[20221213 20:23:08 @agent_ppo2.py:185][0m |          -0.0081 |           0.0204 |           0.0000 |
[32m[20221213 20:23:08 @agent_ppo2.py:185][0m |          -0.0126 |           0.0192 |           0.0000 |
[32m[20221213 20:23:08 @agent_ppo2.py:185][0m |          -0.0164 |           0.0187 |           0.0000 |
[32m[20221213 20:23:08 @agent_ppo2.py:185][0m |          -0.0187 |           0.0180 |           0.0000 |
[32m[20221213 20:23:09 @agent_ppo2.py:185][0m |          -0.0197 |           0.0171 |           0.0000 |
[32m[20221213 20:23:09 @agent_ppo2.py:185][0m |          -0.0212 |           0.0171 |           0.0000 |
[32m[20221213 20:23:09 @agent_ppo2.py:185][0m |          -0.0228 |           0.0171 |           0.0000 |
[32m[20221213 20:23:10 @agent_ppo2.py:185][0m |          -0.0228 |           0.0168 |           0.0000 |
[32m[20221213 20:23:10 @agent_ppo2.py:185][0m |          -0.0222 |           0.0167 |           0.0000 |
[32m[20221213 20:23:10 @agent_ppo2.py:130][0m Policy update time: 2.80 s
[32m[20221213 20:23:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.37
[32m[20221213 20:23:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.81
[32m[20221213 20:23:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.81
[32m[20221213 20:23:10 @agent_ppo2.py:143][0m Total time:      14.74 min
[32m[20221213 20:23:10 @agent_ppo2.py:145][0m 446464 total steps have happened
[32m[20221213 20:23:10 @agent_ppo2.py:121][0m #------------------------ Iteration 218 --------------------------#
[32m[20221213 20:23:11 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:23:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:23:11 @agent_ppo2.py:185][0m |           0.0041 |           0.0157 |           0.0000 |
[32m[20221213 20:23:12 @agent_ppo2.py:185][0m |          -0.0098 |           0.0092 |           0.0000 |
[32m[20221213 20:23:12 @agent_ppo2.py:185][0m |          -0.0161 |           0.0089 |           0.0000 |
[32m[20221213 20:23:12 @agent_ppo2.py:185][0m |          -0.0189 |           0.0086 |           0.0000 |
[32m[20221213 20:23:12 @agent_ppo2.py:185][0m |          -0.0205 |           0.0084 |           0.0000 |
[32m[20221213 20:23:13 @agent_ppo2.py:185][0m |          -0.0215 |           0.0082 |           0.0000 |
[32m[20221213 20:23:13 @agent_ppo2.py:185][0m |          -0.0210 |           0.0080 |           0.0000 |
[32m[20221213 20:23:13 @agent_ppo2.py:185][0m |          -0.0226 |           0.0077 |           0.0000 |
[32m[20221213 20:23:13 @agent_ppo2.py:185][0m |          -0.0230 |           0.0076 |           0.0000 |
[32m[20221213 20:23:14 @agent_ppo2.py:185][0m |          -0.0241 |           0.0074 |           0.0000 |
[32m[20221213 20:23:14 @agent_ppo2.py:130][0m Policy update time: 2.76 s
[32m[20221213 20:23:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.72
[32m[20221213 20:23:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.12
[32m[20221213 20:23:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.30
[32m[20221213 20:23:14 @agent_ppo2.py:143][0m Total time:      14.80 min
[32m[20221213 20:23:14 @agent_ppo2.py:145][0m 448512 total steps have happened
[32m[20221213 20:23:14 @agent_ppo2.py:121][0m #------------------------ Iteration 219 --------------------------#
[32m[20221213 20:23:15 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:23:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:23:15 @agent_ppo2.py:185][0m |           0.0016 |           0.0640 |           0.0000 |
[32m[20221213 20:23:16 @agent_ppo2.py:185][0m |          -0.0142 |           0.0228 |           0.0000 |
[32m[20221213 20:23:16 @agent_ppo2.py:185][0m |          -0.0136 |           0.0207 |           0.0000 |
[32m[20221213 20:23:16 @agent_ppo2.py:185][0m |          -0.0150 |           0.0191 |           0.0000 |
[32m[20221213 20:23:16 @agent_ppo2.py:185][0m |          -0.0182 |           0.0186 |           0.0000 |
[32m[20221213 20:23:17 @agent_ppo2.py:185][0m |          -0.0174 |           0.0184 |           0.0000 |
[32m[20221213 20:23:17 @agent_ppo2.py:185][0m |          -0.0184 |           0.0180 |           0.0000 |
[32m[20221213 20:23:17 @agent_ppo2.py:185][0m |          -0.0190 |           0.0179 |           0.0000 |
[32m[20221213 20:23:18 @agent_ppo2.py:185][0m |          -0.0195 |           0.0173 |           0.0000 |
[32m[20221213 20:23:18 @agent_ppo2.py:185][0m |          -0.0195 |           0.0172 |           0.0000 |
[32m[20221213 20:23:18 @agent_ppo2.py:130][0m Policy update time: 3.12 s
[32m[20221213 20:23:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.76
[32m[20221213 20:23:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.07
[32m[20221213 20:23:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.06
[32m[20221213 20:23:18 @agent_ppo2.py:143][0m Total time:      14.87 min
[32m[20221213 20:23:18 @agent_ppo2.py:145][0m 450560 total steps have happened
[32m[20221213 20:23:18 @agent_ppo2.py:121][0m #------------------------ Iteration 220 --------------------------#
[32m[20221213 20:23:19 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:23:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:23:19 @agent_ppo2.py:185][0m |           0.0019 |           0.0300 |           0.0000 |
[32m[20221213 20:23:20 @agent_ppo2.py:185][0m |          -0.0153 |           0.0236 |           0.0000 |
[32m[20221213 20:23:20 @agent_ppo2.py:185][0m |          -0.0157 |           0.0220 |           0.0000 |
[32m[20221213 20:23:20 @agent_ppo2.py:185][0m |          -0.0192 |           0.0217 |           0.0000 |
[32m[20221213 20:23:20 @agent_ppo2.py:185][0m |          -0.0193 |           0.0208 |           0.0000 |
[32m[20221213 20:23:21 @agent_ppo2.py:185][0m |          -0.0209 |           0.0197 |           0.0000 |
[32m[20221213 20:23:21 @agent_ppo2.py:185][0m |          -0.0225 |           0.0201 |           0.0000 |
[32m[20221213 20:23:21 @agent_ppo2.py:185][0m |          -0.0239 |           0.0194 |           0.0000 |
[32m[20221213 20:23:22 @agent_ppo2.py:185][0m |          -0.0262 |           0.0190 |           0.0000 |
[32m[20221213 20:23:22 @agent_ppo2.py:185][0m |          -0.0311 |           0.0195 |           0.0000 |
[32m[20221213 20:23:22 @agent_ppo2.py:130][0m Policy update time: 3.15 s
[32m[20221213 20:23:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.66
[32m[20221213 20:23:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.68
[32m[20221213 20:23:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.48
[32m[20221213 20:23:23 @agent_ppo2.py:143][0m Total time:      14.94 min
[32m[20221213 20:23:23 @agent_ppo2.py:145][0m 452608 total steps have happened
[32m[20221213 20:23:23 @agent_ppo2.py:121][0m #------------------------ Iteration 221 --------------------------#
[32m[20221213 20:23:23 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:23:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:23:24 @agent_ppo2.py:185][0m |           0.0014 |           0.0366 |           0.0000 |
[32m[20221213 20:23:24 @agent_ppo2.py:185][0m |          -0.0104 |           0.0296 |           0.0000 |
[32m[20221213 20:23:24 @agent_ppo2.py:185][0m |          -0.0152 |           0.0268 |           0.0000 |
[32m[20221213 20:23:24 @agent_ppo2.py:185][0m |          -0.0186 |           0.0261 |           0.0000 |
[32m[20221213 20:23:25 @agent_ppo2.py:185][0m |          -0.0195 |           0.0241 |           0.0000 |
[32m[20221213 20:23:25 @agent_ppo2.py:185][0m |          -0.0213 |           0.0239 |           0.0000 |
[32m[20221213 20:23:25 @agent_ppo2.py:185][0m |          -0.0231 |           0.0233 |           0.0000 |
[32m[20221213 20:23:26 @agent_ppo2.py:185][0m |          -0.0231 |           0.0235 |           0.0000 |
[32m[20221213 20:23:26 @agent_ppo2.py:185][0m |          -0.0256 |           0.0226 |           0.0000 |
[32m[20221213 20:23:26 @agent_ppo2.py:185][0m |          -0.0265 |           0.0226 |           0.0000 |
[32m[20221213 20:23:26 @agent_ppo2.py:130][0m Policy update time: 3.16 s
[32m[20221213 20:23:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.54
[32m[20221213 20:23:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.12
[32m[20221213 20:23:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.62
[32m[20221213 20:23:27 @agent_ppo2.py:143][0m Total time:      15.01 min
[32m[20221213 20:23:27 @agent_ppo2.py:145][0m 454656 total steps have happened
[32m[20221213 20:23:27 @agent_ppo2.py:121][0m #------------------------ Iteration 222 --------------------------#
[32m[20221213 20:23:28 @agent_ppo2.py:127][0m Sampling time: 0.70 s by 10 slaves
[32m[20221213 20:23:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:23:28 @agent_ppo2.py:185][0m |           0.0025 |           0.0271 |           0.0000 |
[32m[20221213 20:23:28 @agent_ppo2.py:185][0m |          -0.0143 |           0.0241 |           0.0000 |
[32m[20221213 20:23:29 @agent_ppo2.py:185][0m |          -0.0212 |           0.0226 |           0.0000 |
[32m[20221213 20:23:29 @agent_ppo2.py:185][0m |          -0.0247 |           0.0216 |           0.0000 |
[32m[20221213 20:23:29 @agent_ppo2.py:185][0m |          -0.0272 |           0.0209 |           0.0000 |
[32m[20221213 20:23:30 @agent_ppo2.py:185][0m |          -0.0297 |           0.0203 |           0.0000 |
[32m[20221213 20:23:30 @agent_ppo2.py:185][0m |          -0.0311 |           0.0201 |           0.0000 |
[32m[20221213 20:23:30 @agent_ppo2.py:185][0m |          -0.0334 |           0.0195 |           0.0000 |
[32m[20221213 20:23:31 @agent_ppo2.py:185][0m |          -0.0354 |           0.0195 |           0.0000 |
[32m[20221213 20:23:31 @agent_ppo2.py:185][0m |          -0.0369 |           0.0194 |           0.0000 |
[32m[20221213 20:23:31 @agent_ppo2.py:130][0m Policy update time: 3.41 s
[32m[20221213 20:23:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.09
[32m[20221213 20:23:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.44
[32m[20221213 20:23:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.30
[32m[20221213 20:23:31 @agent_ppo2.py:143][0m Total time:      15.09 min
[32m[20221213 20:23:31 @agent_ppo2.py:145][0m 456704 total steps have happened
[32m[20221213 20:23:31 @agent_ppo2.py:121][0m #------------------------ Iteration 223 --------------------------#
[32m[20221213 20:23:32 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:23:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:23:32 @agent_ppo2.py:185][0m |          -0.0006 |           0.0378 |           0.0000 |
[32m[20221213 20:23:33 @agent_ppo2.py:185][0m |          -0.0113 |           0.0272 |           0.0000 |
[32m[20221213 20:23:33 @agent_ppo2.py:185][0m |          -0.0159 |           0.0246 |           0.0000 |
[32m[20221213 20:23:33 @agent_ppo2.py:185][0m |          -0.0176 |           0.0228 |           0.0000 |
[32m[20221213 20:23:33 @agent_ppo2.py:185][0m |          -0.0195 |           0.0223 |           0.0000 |
[32m[20221213 20:23:34 @agent_ppo2.py:185][0m |          -0.0219 |           0.0219 |           0.0000 |
[32m[20221213 20:23:34 @agent_ppo2.py:185][0m |          -0.0225 |           0.0215 |           0.0000 |
[32m[20221213 20:23:34 @agent_ppo2.py:185][0m |          -0.0240 |           0.0212 |           0.0000 |
[32m[20221213 20:23:35 @agent_ppo2.py:185][0m |          -0.0273 |           0.0207 |           0.0000 |
[32m[20221213 20:23:35 @agent_ppo2.py:185][0m |          -0.0252 |           0.0205 |           0.0000 |
[32m[20221213 20:23:35 @agent_ppo2.py:130][0m Policy update time: 2.90 s
[32m[20221213 20:23:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.95
[32m[20221213 20:23:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.88
[32m[20221213 20:23:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.16
[32m[20221213 20:23:35 @agent_ppo2.py:143][0m Total time:      15.15 min
[32m[20221213 20:23:35 @agent_ppo2.py:145][0m 458752 total steps have happened
[32m[20221213 20:23:35 @agent_ppo2.py:121][0m #------------------------ Iteration 224 --------------------------#
[32m[20221213 20:23:36 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:23:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:23:36 @agent_ppo2.py:185][0m |           0.0012 |           0.0249 |           0.0000 |
[32m[20221213 20:23:37 @agent_ppo2.py:185][0m |          -0.0176 |           0.0221 |           0.0000 |
[32m[20221213 20:23:37 @agent_ppo2.py:185][0m |          -0.0225 |           0.0205 |           0.0000 |
[32m[20221213 20:23:37 @agent_ppo2.py:185][0m |          -0.0264 |           0.0196 |           0.0000 |
[32m[20221213 20:23:37 @agent_ppo2.py:185][0m |          -0.0319 |           0.0193 |           0.0000 |
[32m[20221213 20:23:38 @agent_ppo2.py:185][0m |          -0.0322 |           0.0187 |           0.0000 |
[32m[20221213 20:23:38 @agent_ppo2.py:185][0m |          -0.0334 |           0.0183 |           0.0000 |
[32m[20221213 20:23:38 @agent_ppo2.py:185][0m |          -0.0347 |           0.0181 |           0.0000 |
[32m[20221213 20:23:38 @agent_ppo2.py:185][0m |          -0.0358 |           0.0180 |           0.0000 |
[32m[20221213 20:23:39 @agent_ppo2.py:185][0m |          -0.0377 |           0.0182 |           0.0000 |
[32m[20221213 20:23:39 @agent_ppo2.py:130][0m Policy update time: 2.96 s
[32m[20221213 20:23:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.24
[32m[20221213 20:23:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.87
[32m[20221213 20:23:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.64
[32m[20221213 20:23:39 @agent_ppo2.py:143][0m Total time:      15.22 min
[32m[20221213 20:23:39 @agent_ppo2.py:145][0m 460800 total steps have happened
[32m[20221213 20:23:39 @agent_ppo2.py:121][0m #------------------------ Iteration 225 --------------------------#
[32m[20221213 20:23:40 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:23:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:23:40 @agent_ppo2.py:185][0m |           0.0040 |           0.0720 |           0.0000 |
[32m[20221213 20:23:41 @agent_ppo2.py:185][0m |          -0.0054 |           0.0348 |           0.0000 |
[32m[20221213 20:23:41 @agent_ppo2.py:185][0m |          -0.0129 |           0.0309 |           0.0000 |
[32m[20221213 20:23:41 @agent_ppo2.py:185][0m |          -0.0162 |           0.0288 |           0.0000 |
[32m[20221213 20:23:41 @agent_ppo2.py:185][0m |          -0.0173 |           0.0280 |           0.0000 |
[32m[20221213 20:23:42 @agent_ppo2.py:185][0m |          -0.0189 |           0.0271 |           0.0000 |
[32m[20221213 20:23:42 @agent_ppo2.py:185][0m |          -0.0192 |           0.0261 |           0.0000 |
[32m[20221213 20:23:42 @agent_ppo2.py:185][0m |          -0.0202 |           0.0256 |           0.0000 |
[32m[20221213 20:23:42 @agent_ppo2.py:185][0m |          -0.0227 |           0.0248 |           0.0000 |
[32m[20221213 20:23:43 @agent_ppo2.py:185][0m |          -0.0216 |           0.0254 |           0.0000 |
[32m[20221213 20:23:43 @agent_ppo2.py:130][0m Policy update time: 2.79 s
[32m[20221213 20:23:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.04
[32m[20221213 20:23:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.43
[32m[20221213 20:23:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.26
[32m[20221213 20:23:43 @agent_ppo2.py:143][0m Total time:      15.28 min
[32m[20221213 20:23:43 @agent_ppo2.py:145][0m 462848 total steps have happened
[32m[20221213 20:23:43 @agent_ppo2.py:121][0m #------------------------ Iteration 226 --------------------------#
[32m[20221213 20:23:44 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:23:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:23:44 @agent_ppo2.py:185][0m |           0.0035 |           0.0305 |           0.0000 |
[32m[20221213 20:23:44 @agent_ppo2.py:185][0m |          -0.0162 |           0.0250 |           0.0000 |
[32m[20221213 20:23:45 @agent_ppo2.py:185][0m |          -0.0210 |           0.0231 |           0.0000 |
[32m[20221213 20:23:45 @agent_ppo2.py:185][0m |          -0.0245 |           0.0223 |           0.0000 |
[32m[20221213 20:23:45 @agent_ppo2.py:185][0m |          -0.0270 |           0.0211 |           0.0000 |
[32m[20221213 20:23:46 @agent_ppo2.py:185][0m |          -0.0298 |           0.0205 |           0.0000 |
[32m[20221213 20:23:46 @agent_ppo2.py:185][0m |          -0.0325 |           0.0199 |           0.0000 |
[32m[20221213 20:23:46 @agent_ppo2.py:185][0m |          -0.0329 |           0.0196 |           0.0000 |
[32m[20221213 20:23:46 @agent_ppo2.py:185][0m |          -0.0337 |           0.0194 |           0.0000 |
[32m[20221213 20:23:47 @agent_ppo2.py:185][0m |          -0.0386 |           0.0192 |           0.0000 |
[32m[20221213 20:23:47 @agent_ppo2.py:130][0m Policy update time: 3.07 s
[32m[20221213 20:23:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.14
[32m[20221213 20:23:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.71
[32m[20221213 20:23:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.88
[32m[20221213 20:23:47 @agent_ppo2.py:143][0m Total time:      15.35 min
[32m[20221213 20:23:47 @agent_ppo2.py:145][0m 464896 total steps have happened
[32m[20221213 20:23:47 @agent_ppo2.py:121][0m #------------------------ Iteration 227 --------------------------#
[32m[20221213 20:23:48 @agent_ppo2.py:127][0m Sampling time: 0.59 s by 10 slaves
[32m[20221213 20:23:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:23:48 @agent_ppo2.py:185][0m |           0.0022 |           0.0452 |           0.0000 |
[32m[20221213 20:23:49 @agent_ppo2.py:185][0m |          -0.0101 |           0.0275 |           0.0000 |
[32m[20221213 20:23:49 @agent_ppo2.py:185][0m |          -0.0153 |           0.0250 |           0.0000 |
[32m[20221213 20:23:49 @agent_ppo2.py:185][0m |          -0.0186 |           0.0241 |           0.0000 |
[32m[20221213 20:23:49 @agent_ppo2.py:185][0m |          -0.0195 |           0.0231 |           0.0000 |
[32m[20221213 20:23:50 @agent_ppo2.py:185][0m |          -0.0220 |           0.0231 |           0.0000 |
[32m[20221213 20:23:50 @agent_ppo2.py:185][0m |          -0.0234 |           0.0219 |           0.0000 |
[32m[20221213 20:23:50 @agent_ppo2.py:185][0m |          -0.0227 |           0.0215 |           0.0000 |
[32m[20221213 20:23:50 @agent_ppo2.py:185][0m |          -0.0235 |           0.0216 |           0.0000 |
[32m[20221213 20:23:51 @agent_ppo2.py:185][0m |          -0.0248 |           0.0210 |           0.0000 |
[32m[20221213 20:23:51 @agent_ppo2.py:130][0m Policy update time: 2.86 s
[32m[20221213 20:23:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.47
[32m[20221213 20:23:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.46
[32m[20221213 20:23:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.26
[32m[20221213 20:23:51 @agent_ppo2.py:143][0m Total time:      15.42 min
[32m[20221213 20:23:51 @agent_ppo2.py:145][0m 466944 total steps have happened
[32m[20221213 20:23:51 @agent_ppo2.py:121][0m #------------------------ Iteration 228 --------------------------#
[32m[20221213 20:23:52 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:23:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:23:52 @agent_ppo2.py:185][0m |          -0.0006 |           0.0229 |           0.0000 |
[32m[20221213 20:23:52 @agent_ppo2.py:185][0m |          -0.0156 |           0.0160 |           0.0000 |
[32m[20221213 20:23:53 @agent_ppo2.py:185][0m |          -0.0195 |           0.0152 |           0.0000 |
[32m[20221213 20:23:53 @agent_ppo2.py:185][0m |          -0.0230 |           0.0147 |           0.0000 |
[32m[20221213 20:23:53 @agent_ppo2.py:185][0m |          -0.0244 |           0.0145 |           0.0000 |
[32m[20221213 20:23:54 @agent_ppo2.py:185][0m |          -0.0260 |           0.0141 |           0.0000 |
[32m[20221213 20:23:54 @agent_ppo2.py:185][0m |          -0.0277 |           0.0136 |           0.0000 |
[32m[20221213 20:23:54 @agent_ppo2.py:185][0m |          -0.0285 |           0.0135 |           0.0000 |
[32m[20221213 20:23:54 @agent_ppo2.py:185][0m |          -0.0247 |           0.0133 |           0.0000 |
[32m[20221213 20:23:55 @agent_ppo2.py:185][0m |          -0.0289 |           0.0129 |           0.0000 |
[32m[20221213 20:23:55 @agent_ppo2.py:130][0m Policy update time: 2.97 s
[32m[20221213 20:23:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.02
[32m[20221213 20:23:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.70
[32m[20221213 20:23:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.72
[32m[20221213 20:23:55 @agent_ppo2.py:143][0m Total time:      15.48 min
[32m[20221213 20:23:55 @agent_ppo2.py:145][0m 468992 total steps have happened
[32m[20221213 20:23:55 @agent_ppo2.py:121][0m #------------------------ Iteration 229 --------------------------#
[32m[20221213 20:23:56 @agent_ppo2.py:127][0m Sampling time: 0.57 s by 10 slaves
[32m[20221213 20:23:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:23:56 @agent_ppo2.py:185][0m |           0.0056 |           0.0115 |           0.0000 |
[32m[20221213 20:23:57 @agent_ppo2.py:185][0m |          -0.0095 |           0.0092 |           0.0000 |
[32m[20221213 20:23:57 @agent_ppo2.py:185][0m |          -0.0161 |           0.0088 |           0.0000 |
[32m[20221213 20:23:57 @agent_ppo2.py:185][0m |          -0.0154 |           0.0086 |           0.0000 |
[32m[20221213 20:23:57 @agent_ppo2.py:185][0m |          -0.0191 |           0.0082 |           0.0000 |
[32m[20221213 20:23:58 @agent_ppo2.py:185][0m |          -0.0224 |           0.0080 |           0.0000 |
[32m[20221213 20:23:58 @agent_ppo2.py:185][0m |          -0.0239 |           0.0077 |           0.0000 |
[32m[20221213 20:23:58 @agent_ppo2.py:185][0m |          -0.0241 |           0.0075 |           0.0000 |
[32m[20221213 20:23:59 @agent_ppo2.py:185][0m |          -0.0257 |           0.0072 |           0.0000 |
[32m[20221213 20:23:59 @agent_ppo2.py:185][0m |          -0.0242 |           0.0071 |           0.0000 |
[32m[20221213 20:23:59 @agent_ppo2.py:130][0m Policy update time: 3.02 s
[32m[20221213 20:23:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.84
[32m[20221213 20:23:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.58
[32m[20221213 20:23:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.35
[32m[20221213 20:23:59 @agent_ppo2.py:143][0m Total time:      15.55 min
[32m[20221213 20:23:59 @agent_ppo2.py:145][0m 471040 total steps have happened
[32m[20221213 20:24:00 @agent_ppo2.py:121][0m #------------------------ Iteration 230 --------------------------#
[32m[20221213 20:24:00 @agent_ppo2.py:127][0m Sampling time: 0.62 s by 10 slaves
[32m[20221213 20:24:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:24:01 @agent_ppo2.py:185][0m |           0.0036 |           0.0594 |           0.0000 |
[32m[20221213 20:24:01 @agent_ppo2.py:185][0m |          -0.0118 |           0.0221 |           0.0000 |
[32m[20221213 20:24:01 @agent_ppo2.py:185][0m |          -0.0134 |           0.0201 |           0.0000 |
[32m[20221213 20:24:02 @agent_ppo2.py:185][0m |          -0.0206 |           0.0187 |           0.0000 |
[32m[20221213 20:24:02 @agent_ppo2.py:185][0m |          -0.0175 |           0.0180 |           0.0000 |
[32m[20221213 20:24:02 @agent_ppo2.py:185][0m |          -0.0184 |           0.0175 |           0.0000 |
[32m[20221213 20:24:02 @agent_ppo2.py:185][0m |          -0.0194 |           0.0174 |           0.0000 |
[32m[20221213 20:24:03 @agent_ppo2.py:185][0m |          -0.0208 |           0.0169 |           0.0000 |
[32m[20221213 20:24:03 @agent_ppo2.py:185][0m |          -0.0192 |           0.0169 |           0.0000 |
[32m[20221213 20:24:03 @agent_ppo2.py:185][0m |          -0.0254 |           0.0168 |           0.0000 |
[32m[20221213 20:24:03 @agent_ppo2.py:130][0m Policy update time: 3.16 s
[32m[20221213 20:24:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.16
[32m[20221213 20:24:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.29
[32m[20221213 20:24:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.15
[32m[20221213 20:24:04 @agent_ppo2.py:143][0m Total time:      15.63 min
[32m[20221213 20:24:04 @agent_ppo2.py:145][0m 473088 total steps have happened
[32m[20221213 20:24:04 @agent_ppo2.py:121][0m #------------------------ Iteration 231 --------------------------#
[32m[20221213 20:24:05 @agent_ppo2.py:127][0m Sampling time: 0.63 s by 10 slaves
[32m[20221213 20:24:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:24:05 @agent_ppo2.py:185][0m |          -0.0004 |           0.0172 |           0.0000 |
[32m[20221213 20:24:05 @agent_ppo2.py:185][0m |          -0.0131 |           0.0124 |           0.0000 |
[32m[20221213 20:24:06 @agent_ppo2.py:185][0m |          -0.0198 |           0.0119 |           0.0000 |
[32m[20221213 20:24:06 @agent_ppo2.py:185][0m |          -0.0218 |           0.0116 |           0.0000 |
[32m[20221213 20:24:07 @agent_ppo2.py:185][0m |          -0.0230 |           0.0115 |           0.0000 |
[32m[20221213 20:24:07 @agent_ppo2.py:185][0m |          -0.0246 |           0.0113 |           0.0000 |
[32m[20221213 20:24:07 @agent_ppo2.py:185][0m |          -0.0250 |           0.0111 |           0.0000 |
[32m[20221213 20:24:07 @agent_ppo2.py:185][0m |          -0.0263 |           0.0109 |           0.0000 |
[32m[20221213 20:24:08 @agent_ppo2.py:185][0m |          -0.0265 |           0.0109 |           0.0000 |
[32m[20221213 20:24:08 @agent_ppo2.py:185][0m |          -0.0263 |           0.0109 |           0.0000 |
[32m[20221213 20:24:08 @agent_ppo2.py:130][0m Policy update time: 3.44 s
[32m[20221213 20:24:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.35
[32m[20221213 20:24:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.28
[32m[20221213 20:24:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.07
[32m[20221213 20:24:08 @agent_ppo2.py:143][0m Total time:      15.70 min
[32m[20221213 20:24:08 @agent_ppo2.py:145][0m 475136 total steps have happened
[32m[20221213 20:24:08 @agent_ppo2.py:121][0m #------------------------ Iteration 232 --------------------------#
[32m[20221213 20:24:09 @agent_ppo2.py:127][0m Sampling time: 0.62 s by 10 slaves
[32m[20221213 20:24:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:24:10 @agent_ppo2.py:185][0m |           0.0012 |           0.0116 |           0.0000 |
[32m[20221213 20:24:10 @agent_ppo2.py:185][0m |          -0.0160 |           0.0107 |           0.0000 |
[32m[20221213 20:24:10 @agent_ppo2.py:185][0m |          -0.0213 |           0.0103 |           0.0000 |
[32m[20221213 20:24:11 @agent_ppo2.py:185][0m |          -0.0273 |           0.0101 |           0.0000 |
[32m[20221213 20:24:11 @agent_ppo2.py:185][0m |          -0.0273 |           0.0098 |           0.0000 |
[32m[20221213 20:24:11 @agent_ppo2.py:185][0m |          -0.0293 |           0.0097 |           0.0000 |
[32m[20221213 20:24:12 @agent_ppo2.py:185][0m |          -0.0302 |           0.0095 |           0.0000 |
[32m[20221213 20:24:12 @agent_ppo2.py:185][0m |          -0.0320 |           0.0093 |           0.0000 |
[32m[20221213 20:24:12 @agent_ppo2.py:185][0m |          -0.0329 |           0.0093 |           0.0000 |
[32m[20221213 20:24:13 @agent_ppo2.py:185][0m |          -0.0354 |           0.0093 |           0.0000 |
[32m[20221213 20:24:13 @agent_ppo2.py:130][0m Policy update time: 3.65 s
[32m[20221213 20:24:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.76
[32m[20221213 20:24:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.34
[32m[20221213 20:24:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.60
[32m[20221213 20:24:13 @agent_ppo2.py:143][0m Total time:      15.78 min
[32m[20221213 20:24:13 @agent_ppo2.py:145][0m 477184 total steps have happened
[32m[20221213 20:24:13 @agent_ppo2.py:121][0m #------------------------ Iteration 233 --------------------------#
[32m[20221213 20:24:14 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:24:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:24:14 @agent_ppo2.py:185][0m |           0.0030 |           0.0536 |           0.0000 |
[32m[20221213 20:24:15 @agent_ppo2.py:185][0m |          -0.0111 |           0.0290 |           0.0000 |
[32m[20221213 20:24:15 @agent_ppo2.py:185][0m |          -0.0144 |           0.0244 |           0.0000 |
[32m[20221213 20:24:15 @agent_ppo2.py:185][0m |          -0.0163 |           0.0222 |           0.0000 |
[32m[20221213 20:24:16 @agent_ppo2.py:185][0m |          -0.0174 |           0.0216 |           0.0000 |
[32m[20221213 20:24:16 @agent_ppo2.py:185][0m |          -0.0197 |           0.0210 |           0.0000 |
[32m[20221213 20:24:16 @agent_ppo2.py:185][0m |          -0.0192 |           0.0201 |           0.0000 |
[32m[20221213 20:24:16 @agent_ppo2.py:185][0m |          -0.0181 |           0.0201 |           0.0000 |
[32m[20221213 20:24:17 @agent_ppo2.py:185][0m |          -0.0197 |           0.0197 |           0.0000 |
[32m[20221213 20:24:17 @agent_ppo2.py:185][0m |          -0.0244 |           0.0190 |           0.0000 |
[32m[20221213 20:24:17 @agent_ppo2.py:130][0m Policy update time: 3.13 s
[32m[20221213 20:24:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.68
[32m[20221213 20:24:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.76
[32m[20221213 20:24:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.14
[32m[20221213 20:24:17 @agent_ppo2.py:143][0m Total time:      15.85 min
[32m[20221213 20:24:17 @agent_ppo2.py:145][0m 479232 total steps have happened
[32m[20221213 20:24:17 @agent_ppo2.py:121][0m #------------------------ Iteration 234 --------------------------#
[32m[20221213 20:24:18 @agent_ppo2.py:127][0m Sampling time: 0.72 s by 10 slaves
[32m[20221213 20:24:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:24:19 @agent_ppo2.py:185][0m |           0.0042 |           0.0444 |           0.0000 |
[32m[20221213 20:24:19 @agent_ppo2.py:185][0m |          -0.0121 |           0.0320 |           0.0000 |
[32m[20221213 20:24:19 @agent_ppo2.py:185][0m |          -0.0190 |           0.0288 |           0.0000 |
[32m[20221213 20:24:20 @agent_ppo2.py:185][0m |          -0.0200 |           0.0271 |           0.0000 |
[32m[20221213 20:24:20 @agent_ppo2.py:185][0m |          -0.0232 |           0.0254 |           0.0000 |
[32m[20221213 20:24:20 @agent_ppo2.py:185][0m |          -0.0246 |           0.0253 |           0.0000 |
[32m[20221213 20:24:21 @agent_ppo2.py:185][0m |          -0.0258 |           0.0244 |           0.0000 |
[32m[20221213 20:24:21 @agent_ppo2.py:185][0m |          -0.0260 |           0.0239 |           0.0000 |
[32m[20221213 20:24:21 @agent_ppo2.py:185][0m |          -0.0275 |           0.0246 |           0.0000 |
[32m[20221213 20:24:22 @agent_ppo2.py:185][0m |          -0.0283 |           0.0231 |           0.0000 |
[32m[20221213 20:24:22 @agent_ppo2.py:130][0m Policy update time: 3.46 s
[32m[20221213 20:24:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.31
[32m[20221213 20:24:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.87
[32m[20221213 20:24:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.10
[32m[20221213 20:24:22 @agent_ppo2.py:143][0m Total time:      15.93 min
[32m[20221213 20:24:22 @agent_ppo2.py:145][0m 481280 total steps have happened
[32m[20221213 20:24:22 @agent_ppo2.py:121][0m #------------------------ Iteration 235 --------------------------#
[32m[20221213 20:24:23 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:24:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:24:23 @agent_ppo2.py:185][0m |           0.0008 |           0.0230 |           0.0000 |
[32m[20221213 20:24:23 @agent_ppo2.py:185][0m |          -0.0139 |           0.0192 |           0.0000 |
[32m[20221213 20:24:24 @agent_ppo2.py:185][0m |          -0.0167 |           0.0178 |           0.0000 |
[32m[20221213 20:24:24 @agent_ppo2.py:185][0m |          -0.0209 |           0.0174 |           0.0000 |
[32m[20221213 20:24:24 @agent_ppo2.py:185][0m |          -0.0238 |           0.0170 |           0.0000 |
[32m[20221213 20:24:24 @agent_ppo2.py:185][0m |          -0.0252 |           0.0166 |           0.0000 |
[32m[20221213 20:24:25 @agent_ppo2.py:185][0m |          -0.0268 |           0.0159 |           0.0000 |
[32m[20221213 20:24:25 @agent_ppo2.py:185][0m |          -0.0278 |           0.0156 |           0.0000 |
[32m[20221213 20:24:25 @agent_ppo2.py:185][0m |          -0.0300 |           0.0160 |           0.0000 |
[32m[20221213 20:24:26 @agent_ppo2.py:185][0m |          -0.0248 |           0.0159 |           0.0000 |
[32m[20221213 20:24:26 @agent_ppo2.py:130][0m Policy update time: 2.92 s
[32m[20221213 20:24:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.22
[32m[20221213 20:24:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.41
[32m[20221213 20:24:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.02
[32m[20221213 20:24:26 @agent_ppo2.py:143][0m Total time:      16.00 min
[32m[20221213 20:24:26 @agent_ppo2.py:145][0m 483328 total steps have happened
[32m[20221213 20:24:26 @agent_ppo2.py:121][0m #------------------------ Iteration 236 --------------------------#
[32m[20221213 20:24:27 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:24:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:24:27 @agent_ppo2.py:185][0m |          -0.0002 |           0.0236 |           0.0000 |
[32m[20221213 20:24:28 @agent_ppo2.py:185][0m |          -0.0109 |           0.0186 |           0.0000 |
[32m[20221213 20:24:28 @agent_ppo2.py:185][0m |          -0.0152 |           0.0173 |           0.0000 |
[32m[20221213 20:24:28 @agent_ppo2.py:185][0m |          -0.0177 |           0.0168 |           0.0000 |
[32m[20221213 20:24:29 @agent_ppo2.py:185][0m |          -0.0197 |           0.0164 |           0.0000 |
[32m[20221213 20:24:29 @agent_ppo2.py:185][0m |          -0.0212 |           0.0159 |           0.0000 |
[32m[20221213 20:24:29 @agent_ppo2.py:185][0m |          -0.0212 |           0.0157 |           0.0000 |
[32m[20221213 20:24:30 @agent_ppo2.py:185][0m |          -0.0289 |           0.0156 |           0.0000 |
[32m[20221213 20:24:30 @agent_ppo2.py:185][0m |          -0.0242 |           0.0154 |           0.0000 |
[32m[20221213 20:24:30 @agent_ppo2.py:185][0m |          -0.0245 |           0.0154 |           0.0000 |
[32m[20221213 20:24:30 @agent_ppo2.py:130][0m Policy update time: 3.68 s
[32m[20221213 20:24:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.47
[32m[20221213 20:24:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.64
[32m[20221213 20:24:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.97
[32m[20221213 20:24:31 @agent_ppo2.py:143][0m Total time:      16.08 min
[32m[20221213 20:24:31 @agent_ppo2.py:145][0m 485376 total steps have happened
[32m[20221213 20:24:31 @agent_ppo2.py:121][0m #------------------------ Iteration 237 --------------------------#
[32m[20221213 20:24:31 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:24:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:24:32 @agent_ppo2.py:185][0m |           0.0036 |           0.0269 |           0.0000 |
[32m[20221213 20:24:32 @agent_ppo2.py:185][0m |          -0.0096 |           0.0185 |           0.0000 |
[32m[20221213 20:24:32 @agent_ppo2.py:185][0m |          -0.0156 |           0.0172 |           0.0000 |
[32m[20221213 20:24:33 @agent_ppo2.py:185][0m |          -0.0191 |           0.0166 |           0.0000 |
[32m[20221213 20:24:33 @agent_ppo2.py:185][0m |          -0.0205 |           0.0162 |           0.0000 |
[32m[20221213 20:24:33 @agent_ppo2.py:185][0m |          -0.0218 |           0.0160 |           0.0000 |
[32m[20221213 20:24:34 @agent_ppo2.py:185][0m |          -0.0235 |           0.0159 |           0.0000 |
[32m[20221213 20:24:34 @agent_ppo2.py:185][0m |          -0.0239 |           0.0154 |           0.0000 |
[32m[20221213 20:24:34 @agent_ppo2.py:185][0m |          -0.0248 |           0.0156 |           0.0000 |
[32m[20221213 20:24:34 @agent_ppo2.py:185][0m |          -0.0316 |           0.0153 |           0.0000 |
[32m[20221213 20:24:34 @agent_ppo2.py:130][0m Policy update time: 2.92 s
[32m[20221213 20:24:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.25
[32m[20221213 20:24:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.74
[32m[20221213 20:24:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.79
[32m[20221213 20:24:35 @agent_ppo2.py:143][0m Total time:      16.14 min
[32m[20221213 20:24:35 @agent_ppo2.py:145][0m 487424 total steps have happened
[32m[20221213 20:24:35 @agent_ppo2.py:121][0m #------------------------ Iteration 238 --------------------------#
[32m[20221213 20:24:35 @agent_ppo2.py:127][0m Sampling time: 0.67 s by 10 slaves
[32m[20221213 20:24:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:24:36 @agent_ppo2.py:185][0m |           0.0030 |           0.0331 |           0.0000 |
[32m[20221213 20:24:36 @agent_ppo2.py:185][0m |          -0.0045 |           0.0238 |           0.0000 |
[32m[20221213 20:24:37 @agent_ppo2.py:185][0m |          -0.0115 |           0.0227 |           0.0000 |
[32m[20221213 20:24:37 @agent_ppo2.py:185][0m |          -0.0165 |           0.0210 |           0.0000 |
[32m[20221213 20:24:37 @agent_ppo2.py:185][0m |          -0.0167 |           0.0207 |           0.0000 |
[32m[20221213 20:24:38 @agent_ppo2.py:185][0m |          -0.0224 |           0.0203 |           0.0000 |
[32m[20221213 20:24:38 @agent_ppo2.py:185][0m |          -0.0190 |           0.0201 |           0.0000 |
[32m[20221213 20:24:38 @agent_ppo2.py:185][0m |          -0.0227 |           0.0199 |           0.0000 |
[32m[20221213 20:24:39 @agent_ppo2.py:185][0m |          -0.0209 |           0.0197 |           0.0000 |
[32m[20221213 20:24:39 @agent_ppo2.py:185][0m |          -0.0231 |           0.0193 |           0.0000 |
[32m[20221213 20:24:39 @agent_ppo2.py:130][0m Policy update time: 3.34 s
[32m[20221213 20:24:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.07
[32m[20221213 20:24:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.92
[32m[20221213 20:24:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.87
[32m[20221213 20:24:39 @agent_ppo2.py:143][0m Total time:      16.22 min
[32m[20221213 20:24:39 @agent_ppo2.py:145][0m 489472 total steps have happened
[32m[20221213 20:24:39 @agent_ppo2.py:121][0m #------------------------ Iteration 239 --------------------------#
[32m[20221213 20:24:40 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:24:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:24:40 @agent_ppo2.py:185][0m |          -0.0001 |           0.0230 |           0.0000 |
[32m[20221213 20:24:41 @agent_ppo2.py:185][0m |          -0.0155 |           0.0203 |           0.0000 |
[32m[20221213 20:24:41 @agent_ppo2.py:185][0m |          -0.0217 |           0.0195 |           0.0000 |
[32m[20221213 20:24:41 @agent_ppo2.py:185][0m |          -0.0253 |           0.0188 |           0.0000 |
[32m[20221213 20:24:42 @agent_ppo2.py:185][0m |          -0.0281 |           0.0181 |           0.0000 |
[32m[20221213 20:24:42 @agent_ppo2.py:185][0m |          -0.0300 |           0.0178 |           0.0000 |
[32m[20221213 20:24:42 @agent_ppo2.py:185][0m |          -0.0321 |           0.0177 |           0.0000 |
[32m[20221213 20:24:43 @agent_ppo2.py:185][0m |          -0.0342 |           0.0177 |           0.0000 |
[32m[20221213 20:24:43 @agent_ppo2.py:185][0m |          -0.0346 |           0.0175 |           0.0000 |
[32m[20221213 20:24:43 @agent_ppo2.py:185][0m |          -0.0364 |           0.0173 |           0.0000 |
[32m[20221213 20:24:43 @agent_ppo2.py:130][0m Policy update time: 3.60 s
[32m[20221213 20:24:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.54
[32m[20221213 20:24:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.71
[32m[20221213 20:24:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.04
[32m[20221213 20:24:44 @agent_ppo2.py:143][0m Total time:      16.30 min
[32m[20221213 20:24:44 @agent_ppo2.py:145][0m 491520 total steps have happened
[32m[20221213 20:24:44 @agent_ppo2.py:121][0m #------------------------ Iteration 240 --------------------------#
[32m[20221213 20:24:45 @agent_ppo2.py:127][0m Sampling time: 0.60 s by 10 slaves
[32m[20221213 20:24:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:24:45 @agent_ppo2.py:185][0m |           0.0013 |           0.0334 |           0.0000 |
[32m[20221213 20:24:46 @agent_ppo2.py:185][0m |          -0.0109 |           0.0245 |           0.0000 |
[32m[20221213 20:24:46 @agent_ppo2.py:185][0m |          -0.0172 |           0.0221 |           0.0000 |
[32m[20221213 20:24:47 @agent_ppo2.py:185][0m |          -0.0199 |           0.0210 |           0.0000 |
[32m[20221213 20:24:47 @agent_ppo2.py:185][0m |          -0.0245 |           0.0209 |           0.0000 |
[32m[20221213 20:24:47 @agent_ppo2.py:185][0m |          -0.0323 |           0.0199 |           0.0000 |
[32m[20221213 20:24:48 @agent_ppo2.py:185][0m |          -0.0253 |           0.0199 |           0.0000 |
[32m[20221213 20:24:48 @agent_ppo2.py:185][0m |          -0.0288 |           0.0191 |           0.0000 |
[32m[20221213 20:24:48 @agent_ppo2.py:185][0m |          -0.0283 |           0.0185 |           0.0000 |
[32m[20221213 20:24:49 @agent_ppo2.py:185][0m |          -0.0277 |           0.0185 |           0.0000 |
[32m[20221213 20:24:49 @agent_ppo2.py:130][0m Policy update time: 3.68 s
[32m[20221213 20:24:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.29
[32m[20221213 20:24:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.45
[32m[20221213 20:24:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.47
[32m[20221213 20:24:49 @agent_ppo2.py:143][0m Total time:      16.38 min
[32m[20221213 20:24:49 @agent_ppo2.py:145][0m 493568 total steps have happened
[32m[20221213 20:24:49 @agent_ppo2.py:121][0m #------------------------ Iteration 241 --------------------------#
[32m[20221213 20:24:50 @agent_ppo2.py:127][0m Sampling time: 0.64 s by 10 slaves
[32m[20221213 20:24:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:24:50 @agent_ppo2.py:185][0m |           0.0037 |           0.0236 |           0.0000 |
[32m[20221213 20:24:50 @agent_ppo2.py:185][0m |          -0.0124 |           0.0126 |           0.0000 |
[32m[20221213 20:24:51 @agent_ppo2.py:185][0m |          -0.0173 |           0.0120 |           0.0000 |
[32m[20221213 20:24:51 @agent_ppo2.py:185][0m |          -0.0202 |           0.0117 |           0.0000 |
[32m[20221213 20:24:51 @agent_ppo2.py:185][0m |          -0.0190 |           0.0114 |           0.0000 |
[32m[20221213 20:24:51 @agent_ppo2.py:185][0m |          -0.0185 |           0.0113 |           0.0000 |
[32m[20221213 20:24:52 @agent_ppo2.py:185][0m |          -0.0224 |           0.0111 |           0.0000 |
[32m[20221213 20:24:52 @agent_ppo2.py:185][0m |          -0.0210 |           0.0107 |           0.0000 |
[32m[20221213 20:24:52 @agent_ppo2.py:185][0m |          -0.0243 |           0.0105 |           0.0000 |
[32m[20221213 20:24:53 @agent_ppo2.py:185][0m |          -0.0248 |           0.0103 |           0.0000 |
[32m[20221213 20:24:53 @agent_ppo2.py:130][0m Policy update time: 3.12 s
[32m[20221213 20:24:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.68
[32m[20221213 20:24:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.19
[32m[20221213 20:24:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.51
[32m[20221213 20:24:54 @agent_ppo2.py:143][0m Total time:      16.45 min
[32m[20221213 20:24:54 @agent_ppo2.py:145][0m 495616 total steps have happened
[32m[20221213 20:24:54 @agent_ppo2.py:121][0m #------------------------ Iteration 242 --------------------------#
[32m[20221213 20:24:54 @agent_ppo2.py:127][0m Sampling time: 0.64 s by 10 slaves
[32m[20221213 20:24:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:24:55 @agent_ppo2.py:185][0m |           0.0002 |           0.0374 |           0.0000 |
[32m[20221213 20:24:55 @agent_ppo2.py:185][0m |          -0.0100 |           0.0245 |           0.0000 |
[32m[20221213 20:24:55 @agent_ppo2.py:185][0m |          -0.0146 |           0.0226 |           0.0000 |
[32m[20221213 20:24:56 @agent_ppo2.py:185][0m |          -0.0222 |           0.0218 |           0.0000 |
[32m[20221213 20:24:56 @agent_ppo2.py:185][0m |          -0.0216 |           0.0212 |           0.0000 |
[32m[20221213 20:24:57 @agent_ppo2.py:185][0m |          -0.0181 |           0.0205 |           0.0000 |
[32m[20221213 20:24:57 @agent_ppo2.py:185][0m |          -0.0196 |           0.0199 |           0.0000 |
[32m[20221213 20:24:57 @agent_ppo2.py:185][0m |          -0.0216 |           0.0199 |           0.0000 |
[32m[20221213 20:24:58 @agent_ppo2.py:185][0m |          -0.0203 |           0.0192 |           0.0000 |
[32m[20221213 20:24:58 @agent_ppo2.py:185][0m |          -0.0213 |           0.0197 |           0.0000 |
[32m[20221213 20:24:58 @agent_ppo2.py:130][0m Policy update time: 3.77 s
[32m[20221213 20:24:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.71
[32m[20221213 20:24:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.63
[32m[20221213 20:24:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.30
[32m[20221213 20:24:59 @agent_ppo2.py:143][0m Total time:      16.54 min
[32m[20221213 20:24:59 @agent_ppo2.py:145][0m 497664 total steps have happened
[32m[20221213 20:24:59 @agent_ppo2.py:121][0m #------------------------ Iteration 243 --------------------------#
[32m[20221213 20:24:59 @agent_ppo2.py:127][0m Sampling time: 0.68 s by 10 slaves
[32m[20221213 20:24:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:00 @agent_ppo2.py:185][0m |           0.0019 |           0.0228 |           0.0000 |
[32m[20221213 20:25:00 @agent_ppo2.py:185][0m |          -0.0171 |           0.0200 |           0.0000 |
[32m[20221213 20:25:01 @agent_ppo2.py:185][0m |          -0.0232 |           0.0192 |           0.0000 |
[32m[20221213 20:25:01 @agent_ppo2.py:185][0m |          -0.0311 |           0.0190 |           0.0000 |
[32m[20221213 20:25:01 @agent_ppo2.py:185][0m |          -0.0290 |           0.0190 |           0.0000 |
[32m[20221213 20:25:02 @agent_ppo2.py:185][0m |          -0.0305 |           0.0183 |           0.0000 |
[32m[20221213 20:25:02 @agent_ppo2.py:185][0m |          -0.0341 |           0.0178 |           0.0000 |
[32m[20221213 20:25:02 @agent_ppo2.py:185][0m |          -0.0335 |           0.0179 |           0.0000 |
[32m[20221213 20:25:03 @agent_ppo2.py:185][0m |          -0.0365 |           0.0177 |           0.0000 |
[32m[20221213 20:25:03 @agent_ppo2.py:185][0m |          -0.0366 |           0.0179 |           0.0000 |
[32m[20221213 20:25:03 @agent_ppo2.py:130][0m Policy update time: 3.99 s
[32m[20221213 20:25:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.69
[32m[20221213 20:25:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.87
[32m[20221213 20:25:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.80
[32m[20221213 20:25:04 @agent_ppo2.py:143][0m Total time:      16.62 min
[32m[20221213 20:25:04 @agent_ppo2.py:145][0m 499712 total steps have happened
[32m[20221213 20:25:04 @agent_ppo2.py:121][0m #------------------------ Iteration 244 --------------------------#
[32m[20221213 20:25:04 @agent_ppo2.py:127][0m Sampling time: 0.69 s by 10 slaves
[32m[20221213 20:25:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:05 @agent_ppo2.py:185][0m |           0.0028 |           0.0275 |           0.0000 |
[32m[20221213 20:25:05 @agent_ppo2.py:185][0m |          -0.0117 |           0.0219 |           0.0000 |
[32m[20221213 20:25:06 @agent_ppo2.py:185][0m |          -0.0211 |           0.0207 |           0.0000 |
[32m[20221213 20:25:06 @agent_ppo2.py:185][0m |          -0.0155 |           0.0189 |           0.0000 |
[32m[20221213 20:25:06 @agent_ppo2.py:185][0m |          -0.0249 |           0.0181 |           0.0000 |
[32m[20221213 20:25:07 @agent_ppo2.py:185][0m |          -0.0217 |           0.0178 |           0.0000 |
[32m[20221213 20:25:07 @agent_ppo2.py:185][0m |          -0.0222 |           0.0174 |           0.0000 |
[32m[20221213 20:25:07 @agent_ppo2.py:185][0m |          -0.0238 |           0.0173 |           0.0000 |
[32m[20221213 20:25:08 @agent_ppo2.py:185][0m |          -0.0272 |           0.0171 |           0.0000 |
[32m[20221213 20:25:08 @agent_ppo2.py:185][0m |          -0.0247 |           0.0167 |           0.0000 |
[32m[20221213 20:25:08 @agent_ppo2.py:130][0m Policy update time: 3.47 s
[32m[20221213 20:25:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.70
[32m[20221213 20:25:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.92
[32m[20221213 20:25:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.76
[32m[20221213 20:25:09 @agent_ppo2.py:143][0m Total time:      16.71 min
[32m[20221213 20:25:09 @agent_ppo2.py:145][0m 501760 total steps have happened
[32m[20221213 20:25:09 @agent_ppo2.py:121][0m #------------------------ Iteration 245 --------------------------#
[32m[20221213 20:25:09 @agent_ppo2.py:127][0m Sampling time: 0.60 s by 10 slaves
[32m[20221213 20:25:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:10 @agent_ppo2.py:185][0m |           0.0011 |           0.0174 |           0.0000 |
[32m[20221213 20:25:10 @agent_ppo2.py:185][0m |          -0.0169 |           0.0152 |           0.0000 |
[32m[20221213 20:25:11 @agent_ppo2.py:185][0m |          -0.0229 |           0.0145 |           0.0000 |
[32m[20221213 20:25:11 @agent_ppo2.py:185][0m |          -0.0259 |           0.0141 |           0.0000 |
[32m[20221213 20:25:11 @agent_ppo2.py:185][0m |          -0.0279 |           0.0138 |           0.0000 |
[32m[20221213 20:25:11 @agent_ppo2.py:185][0m |          -0.0315 |           0.0136 |           0.0000 |
[32m[20221213 20:25:12 @agent_ppo2.py:185][0m |          -0.0330 |           0.0133 |           0.0000 |
[32m[20221213 20:25:12 @agent_ppo2.py:185][0m |          -0.0350 |           0.0130 |           0.0000 |
[32m[20221213 20:25:12 @agent_ppo2.py:185][0m |          -0.0343 |           0.0129 |           0.0000 |
[32m[20221213 20:25:12 @agent_ppo2.py:185][0m |          -0.0378 |           0.0128 |           0.0000 |
[32m[20221213 20:25:12 @agent_ppo2.py:130][0m Policy update time: 3.11 s
[32m[20221213 20:25:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.32
[32m[20221213 20:25:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.37
[32m[20221213 20:25:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.92
[32m[20221213 20:25:13 @agent_ppo2.py:143][0m Total time:      16.78 min
[32m[20221213 20:25:13 @agent_ppo2.py:145][0m 503808 total steps have happened
[32m[20221213 20:25:13 @agent_ppo2.py:121][0m #------------------------ Iteration 246 --------------------------#
[32m[20221213 20:25:13 @agent_ppo2.py:127][0m Sampling time: 0.60 s by 10 slaves
[32m[20221213 20:25:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:14 @agent_ppo2.py:185][0m |           0.0026 |           0.0657 |           0.0000 |
[32m[20221213 20:25:14 @agent_ppo2.py:185][0m |          -0.0091 |           0.0360 |           0.0000 |
[32m[20221213 20:25:15 @agent_ppo2.py:185][0m |          -0.0143 |           0.0314 |           0.0000 |
[32m[20221213 20:25:15 @agent_ppo2.py:185][0m |          -0.0198 |           0.0288 |           0.0000 |
[32m[20221213 20:25:15 @agent_ppo2.py:185][0m |          -0.0163 |           0.0271 |           0.0000 |
[32m[20221213 20:25:16 @agent_ppo2.py:185][0m |          -0.0234 |           0.0284 |           0.0000 |
[32m[20221213 20:25:16 @agent_ppo2.py:185][0m |          -0.0189 |           0.0264 |           0.0000 |
[32m[20221213 20:25:16 @agent_ppo2.py:185][0m |          -0.0233 |           0.0259 |           0.0000 |
[32m[20221213 20:25:17 @agent_ppo2.py:185][0m |          -0.0210 |           0.0251 |           0.0000 |
[32m[20221213 20:25:17 @agent_ppo2.py:185][0m |          -0.0209 |           0.0243 |           0.0000 |
[32m[20221213 20:25:17 @agent_ppo2.py:130][0m Policy update time: 3.56 s
[32m[20221213 20:25:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.90
[32m[20221213 20:25:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 12.91
[32m[20221213 20:25:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.65
[32m[20221213 20:25:17 @agent_ppo2.py:143][0m Total time:      16.85 min
[32m[20221213 20:25:17 @agent_ppo2.py:145][0m 505856 total steps have happened
[32m[20221213 20:25:17 @agent_ppo2.py:121][0m #------------------------ Iteration 247 --------------------------#
[32m[20221213 20:25:18 @agent_ppo2.py:127][0m Sampling time: 0.64 s by 10 slaves
[32m[20221213 20:25:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:19 @agent_ppo2.py:185][0m |           0.0028 |           0.0331 |           0.0000 |
[32m[20221213 20:25:19 @agent_ppo2.py:185][0m |          -0.0132 |           0.0254 |           0.0000 |
[32m[20221213 20:25:19 @agent_ppo2.py:185][0m |          -0.0163 |           0.0226 |           0.0000 |
[32m[20221213 20:25:19 @agent_ppo2.py:185][0m |          -0.0198 |           0.0213 |           0.0000 |
[32m[20221213 20:25:20 @agent_ppo2.py:185][0m |          -0.0231 |           0.0202 |           0.0000 |
[32m[20221213 20:25:20 @agent_ppo2.py:185][0m |          -0.0261 |           0.0196 |           0.0000 |
[32m[20221213 20:25:20 @agent_ppo2.py:185][0m |          -0.0263 |           0.0190 |           0.0000 |
[32m[20221213 20:25:20 @agent_ppo2.py:185][0m |          -0.0278 |           0.0187 |           0.0000 |
[32m[20221213 20:25:21 @agent_ppo2.py:185][0m |          -0.0290 |           0.0185 |           0.0000 |
[32m[20221213 20:25:21 @agent_ppo2.py:185][0m |          -0.0304 |           0.0181 |           0.0000 |
[32m[20221213 20:25:21 @agent_ppo2.py:130][0m Policy update time: 2.92 s
[32m[20221213 20:25:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.56
[32m[20221213 20:25:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.10
[32m[20221213 20:25:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.86
[32m[20221213 20:25:21 @agent_ppo2.py:143][0m Total time:      16.92 min
[32m[20221213 20:25:21 @agent_ppo2.py:145][0m 507904 total steps have happened
[32m[20221213 20:25:21 @agent_ppo2.py:121][0m #------------------------ Iteration 248 --------------------------#
[32m[20221213 20:25:22 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:25:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:22 @agent_ppo2.py:185][0m |           0.0024 |           0.0185 |           0.0000 |
[32m[20221213 20:25:23 @agent_ppo2.py:185][0m |          -0.0142 |           0.0145 |           0.0000 |
[32m[20221213 20:25:23 @agent_ppo2.py:185][0m |          -0.0184 |           0.0137 |           0.0000 |
[32m[20221213 20:25:23 @agent_ppo2.py:185][0m |          -0.0211 |           0.0134 |           0.0000 |
[32m[20221213 20:25:23 @agent_ppo2.py:185][0m |          -0.0235 |           0.0131 |           0.0000 |
[32m[20221213 20:25:24 @agent_ppo2.py:185][0m |          -0.0251 |           0.0129 |           0.0000 |
[32m[20221213 20:25:24 @agent_ppo2.py:185][0m |          -0.0261 |           0.0125 |           0.0000 |
[32m[20221213 20:25:24 @agent_ppo2.py:185][0m |          -0.0264 |           0.0126 |           0.0000 |
[32m[20221213 20:25:25 @agent_ppo2.py:185][0m |          -0.0277 |           0.0125 |           0.0000 |
[32m[20221213 20:25:25 @agent_ppo2.py:185][0m |          -0.0282 |           0.0123 |           0.0000 |
[32m[20221213 20:25:25 @agent_ppo2.py:130][0m Policy update time: 2.90 s
[32m[20221213 20:25:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.69
[32m[20221213 20:25:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.18
[32m[20221213 20:25:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.87
[32m[20221213 20:25:25 @agent_ppo2.py:143][0m Total time:      16.98 min
[32m[20221213 20:25:25 @agent_ppo2.py:145][0m 509952 total steps have happened
[32m[20221213 20:25:25 @agent_ppo2.py:121][0m #------------------------ Iteration 249 --------------------------#
[32m[20221213 20:25:26 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:25:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:26 @agent_ppo2.py:185][0m |          -0.0006 |           0.0356 |           0.0000 |
[32m[20221213 20:25:27 @agent_ppo2.py:185][0m |          -0.0147 |           0.0254 |           0.0000 |
[32m[20221213 20:25:27 @agent_ppo2.py:185][0m |          -0.0142 |           0.0235 |           0.0000 |
[32m[20221213 20:25:27 @agent_ppo2.py:185][0m |          -0.0174 |           0.0218 |           0.0000 |
[32m[20221213 20:25:28 @agent_ppo2.py:185][0m |          -0.0199 |           0.0215 |           0.0000 |
[32m[20221213 20:25:28 @agent_ppo2.py:185][0m |          -0.0194 |           0.0207 |           0.0000 |
[32m[20221213 20:25:28 @agent_ppo2.py:185][0m |          -0.0199 |           0.0203 |           0.0000 |
[32m[20221213 20:25:28 @agent_ppo2.py:185][0m |          -0.0215 |           0.0203 |           0.0000 |
[32m[20221213 20:25:29 @agent_ppo2.py:185][0m |          -0.0214 |           0.0197 |           0.0000 |
[32m[20221213 20:25:29 @agent_ppo2.py:185][0m |          -0.0226 |           0.0200 |           0.0000 |
[32m[20221213 20:25:29 @agent_ppo2.py:130][0m Policy update time: 3.12 s
[32m[20221213 20:25:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.23
[32m[20221213 20:25:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.84
[32m[20221213 20:25:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.91
[32m[20221213 20:25:30 @agent_ppo2.py:143][0m Total time:      17.05 min
[32m[20221213 20:25:30 @agent_ppo2.py:145][0m 512000 total steps have happened
[32m[20221213 20:25:30 @agent_ppo2.py:121][0m #------------------------ Iteration 250 --------------------------#
[32m[20221213 20:25:30 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:25:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:31 @agent_ppo2.py:185][0m |           0.0028 |           0.0156 |           0.0000 |
[32m[20221213 20:25:31 @agent_ppo2.py:185][0m |          -0.0140 |           0.0129 |           0.0000 |
[32m[20221213 20:25:31 @agent_ppo2.py:185][0m |          -0.0196 |           0.0127 |           0.0000 |
[32m[20221213 20:25:31 @agent_ppo2.py:185][0m |          -0.0207 |           0.0126 |           0.0000 |
[32m[20221213 20:25:32 @agent_ppo2.py:185][0m |          -0.0250 |           0.0122 |           0.0000 |
[32m[20221213 20:25:32 @agent_ppo2.py:185][0m |          -0.0243 |           0.0121 |           0.0000 |
[32m[20221213 20:25:32 @agent_ppo2.py:185][0m |          -0.0275 |           0.0120 |           0.0000 |
[32m[20221213 20:25:32 @agent_ppo2.py:185][0m |          -0.0284 |           0.0118 |           0.0000 |
[32m[20221213 20:25:33 @agent_ppo2.py:185][0m |          -0.0299 |           0.0116 |           0.0000 |
[32m[20221213 20:25:33 @agent_ppo2.py:185][0m |          -0.0308 |           0.0115 |           0.0000 |
[32m[20221213 20:25:33 @agent_ppo2.py:130][0m Policy update time: 2.88 s
[32m[20221213 20:25:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.44
[32m[20221213 20:25:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.32
[32m[20221213 20:25:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.31
[32m[20221213 20:25:34 @agent_ppo2.py:143][0m Total time:      17.12 min
[32m[20221213 20:25:34 @agent_ppo2.py:145][0m 514048 total steps have happened
[32m[20221213 20:25:34 @agent_ppo2.py:121][0m #------------------------ Iteration 251 --------------------------#
[32m[20221213 20:25:34 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:25:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:34 @agent_ppo2.py:185][0m |           0.0039 |           0.0688 |           0.0000 |
[32m[20221213 20:25:35 @agent_ppo2.py:185][0m |          -0.0112 |           0.0339 |           0.0000 |
[32m[20221213 20:25:35 @agent_ppo2.py:185][0m |          -0.0131 |           0.0302 |           0.0000 |
[32m[20221213 20:25:35 @agent_ppo2.py:185][0m |          -0.0172 |           0.0281 |           0.0000 |
[32m[20221213 20:25:36 @agent_ppo2.py:185][0m |          -0.0171 |           0.0274 |           0.0000 |
[32m[20221213 20:25:36 @agent_ppo2.py:185][0m |          -0.0175 |           0.0259 |           0.0000 |
[32m[20221213 20:25:36 @agent_ppo2.py:185][0m |          -0.0227 |           0.0259 |           0.0000 |
[32m[20221213 20:25:36 @agent_ppo2.py:185][0m |          -0.0195 |           0.0252 |           0.0000 |
[32m[20221213 20:25:37 @agent_ppo2.py:185][0m |          -0.0196 |           0.0255 |           0.0000 |
[32m[20221213 20:25:37 @agent_ppo2.py:185][0m |          -0.0195 |           0.0246 |           0.0000 |
[32m[20221213 20:25:37 @agent_ppo2.py:130][0m Policy update time: 2.95 s
[32m[20221213 20:25:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.13
[32m[20221213 20:25:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.12
[32m[20221213 20:25:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.12
[32m[20221213 20:25:37 @agent_ppo2.py:143][0m Total time:      17.19 min
[32m[20221213 20:25:37 @agent_ppo2.py:145][0m 516096 total steps have happened
[32m[20221213 20:25:37 @agent_ppo2.py:121][0m #------------------------ Iteration 252 --------------------------#
[32m[20221213 20:25:38 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:25:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:38 @agent_ppo2.py:185][0m |           0.0008 |           0.0256 |           0.0000 |
[32m[20221213 20:25:39 @agent_ppo2.py:185][0m |          -0.0140 |           0.0159 |           0.0000 |
[32m[20221213 20:25:39 @agent_ppo2.py:185][0m |          -0.0189 |           0.0155 |           0.0000 |
[32m[20221213 20:25:39 @agent_ppo2.py:185][0m |          -0.0192 |           0.0161 |           0.0000 |
[32m[20221213 20:25:40 @agent_ppo2.py:185][0m |          -0.0230 |           0.0151 |           0.0000 |
[32m[20221213 20:25:40 @agent_ppo2.py:185][0m |          -0.0229 |           0.0146 |           0.0000 |
[32m[20221213 20:25:40 @agent_ppo2.py:185][0m |          -0.0249 |           0.0143 |           0.0000 |
[32m[20221213 20:25:40 @agent_ppo2.py:185][0m |          -0.0230 |           0.0142 |           0.0000 |
[32m[20221213 20:25:41 @agent_ppo2.py:185][0m |          -0.0240 |           0.0140 |           0.0000 |
[32m[20221213 20:25:41 @agent_ppo2.py:185][0m |          -0.0269 |           0.0136 |           0.0000 |
[32m[20221213 20:25:41 @agent_ppo2.py:130][0m Policy update time: 3.08 s
[32m[20221213 20:25:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.78
[32m[20221213 20:25:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.87
[32m[20221213 20:25:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.61
[32m[20221213 20:25:42 @agent_ppo2.py:143][0m Total time:      17.26 min
[32m[20221213 20:25:42 @agent_ppo2.py:145][0m 518144 total steps have happened
[32m[20221213 20:25:42 @agent_ppo2.py:121][0m #------------------------ Iteration 253 --------------------------#
[32m[20221213 20:25:42 @agent_ppo2.py:127][0m Sampling time: 0.57 s by 10 slaves
[32m[20221213 20:25:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:43 @agent_ppo2.py:185][0m |           0.0055 |           0.0252 |           0.0000 |
[32m[20221213 20:25:43 @agent_ppo2.py:185][0m |          -0.0088 |           0.0191 |           0.0000 |
[32m[20221213 20:25:43 @agent_ppo2.py:185][0m |          -0.0123 |           0.0179 |           0.0000 |
[32m[20221213 20:25:44 @agent_ppo2.py:185][0m |          -0.0147 |           0.0174 |           0.0000 |
[32m[20221213 20:25:44 @agent_ppo2.py:185][0m |          -0.0168 |           0.0170 |           0.0000 |
[32m[20221213 20:25:44 @agent_ppo2.py:185][0m |          -0.0182 |           0.0169 |           0.0000 |
[32m[20221213 20:25:45 @agent_ppo2.py:185][0m |          -0.0188 |           0.0167 |           0.0000 |
[32m[20221213 20:25:45 @agent_ppo2.py:185][0m |          -0.0187 |           0.0163 |           0.0000 |
[32m[20221213 20:25:45 @agent_ppo2.py:185][0m |          -0.0186 |           0.0165 |           0.0000 |
[32m[20221213 20:25:45 @agent_ppo2.py:185][0m |          -0.0228 |           0.0164 |           0.0000 |
[32m[20221213 20:25:45 @agent_ppo2.py:130][0m Policy update time: 3.00 s
[32m[20221213 20:25:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.80
[32m[20221213 20:25:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.80
[32m[20221213 20:25:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.40
[32m[20221213 20:25:46 @agent_ppo2.py:143][0m Total time:      17.33 min
[32m[20221213 20:25:46 @agent_ppo2.py:145][0m 520192 total steps have happened
[32m[20221213 20:25:46 @agent_ppo2.py:121][0m #------------------------ Iteration 254 --------------------------#
[32m[20221213 20:25:46 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:25:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:47 @agent_ppo2.py:185][0m |           0.0012 |           0.0518 |           0.0000 |
[32m[20221213 20:25:47 @agent_ppo2.py:185][0m |          -0.0090 |           0.0367 |           0.0000 |
[32m[20221213 20:25:47 @agent_ppo2.py:185][0m |          -0.0126 |           0.0319 |           0.0000 |
[32m[20221213 20:25:48 @agent_ppo2.py:185][0m |          -0.0188 |           0.0307 |           0.0000 |
[32m[20221213 20:25:48 @agent_ppo2.py:185][0m |          -0.0183 |           0.0288 |           0.0000 |
[32m[20221213 20:25:48 @agent_ppo2.py:185][0m |          -0.0195 |           0.0284 |           0.0000 |
[32m[20221213 20:25:48 @agent_ppo2.py:185][0m |          -0.0194 |           0.0273 |           0.0000 |
[32m[20221213 20:25:49 @agent_ppo2.py:185][0m |          -0.0212 |           0.0266 |           0.0000 |
[32m[20221213 20:25:49 @agent_ppo2.py:185][0m |          -0.0223 |           0.0269 |           0.0000 |
[32m[20221213 20:25:49 @agent_ppo2.py:185][0m |          -0.0230 |           0.0262 |           0.0000 |
[32m[20221213 20:25:49 @agent_ppo2.py:130][0m Policy update time: 2.76 s
[32m[20221213 20:25:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.70
[32m[20221213 20:25:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.59
[32m[20221213 20:25:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.46
[32m[20221213 20:25:50 @agent_ppo2.py:143][0m Total time:      17.39 min
[32m[20221213 20:25:50 @agent_ppo2.py:145][0m 522240 total steps have happened
[32m[20221213 20:25:50 @agent_ppo2.py:121][0m #------------------------ Iteration 255 --------------------------#
[32m[20221213 20:25:50 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:25:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:51 @agent_ppo2.py:185][0m |           0.0003 |           0.0259 |           0.0000 |
[32m[20221213 20:25:51 @agent_ppo2.py:185][0m |          -0.0174 |           0.0204 |           0.0000 |
[32m[20221213 20:25:51 @agent_ppo2.py:185][0m |          -0.0200 |           0.0190 |           0.0000 |
[32m[20221213 20:25:51 @agent_ppo2.py:185][0m |          -0.0246 |           0.0186 |           0.0000 |
[32m[20221213 20:25:52 @agent_ppo2.py:185][0m |          -0.0277 |           0.0181 |           0.0000 |
[32m[20221213 20:25:52 @agent_ppo2.py:185][0m |          -0.0296 |           0.0180 |           0.0000 |
[32m[20221213 20:25:52 @agent_ppo2.py:185][0m |          -0.0309 |           0.0175 |           0.0000 |
[32m[20221213 20:25:52 @agent_ppo2.py:185][0m |          -0.0316 |           0.0171 |           0.0000 |
[32m[20221213 20:25:53 @agent_ppo2.py:185][0m |          -0.0305 |           0.0167 |           0.0000 |
[32m[20221213 20:25:53 @agent_ppo2.py:185][0m |          -0.0334 |           0.0169 |           0.0000 |
[32m[20221213 20:25:53 @agent_ppo2.py:130][0m Policy update time: 2.88 s
[32m[20221213 20:25:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.98
[32m[20221213 20:25:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.47
[32m[20221213 20:25:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 13.68
[32m[20221213 20:25:53 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 13.68
[32m[20221213 20:25:53 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 13.68
[32m[20221213 20:25:53 @agent_ppo2.py:143][0m Total time:      17.45 min
[32m[20221213 20:25:53 @agent_ppo2.py:145][0m 524288 total steps have happened
[32m[20221213 20:25:53 @agent_ppo2.py:121][0m #------------------------ Iteration 256 --------------------------#
[32m[20221213 20:25:54 @agent_ppo2.py:127][0m Sampling time: 0.56 s by 10 slaves
[32m[20221213 20:25:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:54 @agent_ppo2.py:185][0m |           0.0019 |           0.0182 |           0.0000 |
[32m[20221213 20:25:55 @agent_ppo2.py:185][0m |          -0.0119 |           0.0100 |           0.0000 |
[32m[20221213 20:25:55 @agent_ppo2.py:185][0m |          -0.0160 |           0.0096 |           0.0000 |
[32m[20221213 20:25:55 @agent_ppo2.py:185][0m |          -0.0187 |           0.0094 |           0.0000 |
[32m[20221213 20:25:55 @agent_ppo2.py:185][0m |          -0.0167 |           0.0092 |           0.0000 |
[32m[20221213 20:25:56 @agent_ppo2.py:185][0m |          -0.0203 |           0.0089 |           0.0000 |
[32m[20221213 20:25:56 @agent_ppo2.py:185][0m |          -0.0190 |           0.0087 |           0.0000 |
[32m[20221213 20:25:56 @agent_ppo2.py:185][0m |          -0.0221 |           0.0084 |           0.0000 |
[32m[20221213 20:25:57 @agent_ppo2.py:185][0m |          -0.0237 |           0.0081 |           0.0000 |
[32m[20221213 20:25:57 @agent_ppo2.py:185][0m |          -0.0244 |           0.0079 |           0.0000 |
[32m[20221213 20:25:57 @agent_ppo2.py:130][0m Policy update time: 2.79 s
[32m[20221213 20:25:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.38
[32m[20221213 20:25:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.78
[32m[20221213 20:25:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.60
[32m[20221213 20:25:57 @agent_ppo2.py:143][0m Total time:      17.52 min
[32m[20221213 20:25:57 @agent_ppo2.py:145][0m 526336 total steps have happened
[32m[20221213 20:25:57 @agent_ppo2.py:121][0m #------------------------ Iteration 257 --------------------------#
[32m[20221213 20:25:58 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:25:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:25:58 @agent_ppo2.py:185][0m |           0.0018 |           0.0165 |           0.0000 |
[32m[20221213 20:25:59 @agent_ppo2.py:185][0m |          -0.0142 |           0.0124 |           0.0000 |
[32m[20221213 20:25:59 @agent_ppo2.py:185][0m |          -0.0148 |           0.0118 |           0.0000 |
[32m[20221213 20:25:59 @agent_ppo2.py:185][0m |          -0.0161 |           0.0116 |           0.0000 |
[32m[20221213 20:25:59 @agent_ppo2.py:185][0m |          -0.0180 |           0.0113 |           0.0000 |
[32m[20221213 20:26:00 @agent_ppo2.py:185][0m |          -0.0188 |           0.0111 |           0.0000 |
[32m[20221213 20:26:00 @agent_ppo2.py:185][0m |          -0.0196 |           0.0111 |           0.0000 |
[32m[20221213 20:26:00 @agent_ppo2.py:185][0m |          -0.0206 |           0.0108 |           0.0000 |
[32m[20221213 20:26:01 @agent_ppo2.py:185][0m |          -0.0205 |           0.0107 |           0.0000 |
[32m[20221213 20:26:01 @agent_ppo2.py:185][0m |          -0.0241 |           0.0107 |           0.0000 |
[32m[20221213 20:26:01 @agent_ppo2.py:130][0m Policy update time: 3.25 s
[32m[20221213 20:26:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.44
[32m[20221213 20:26:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.30
[32m[20221213 20:26:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.44
[32m[20221213 20:26:02 @agent_ppo2.py:143][0m Total time:      17.59 min
[32m[20221213 20:26:02 @agent_ppo2.py:145][0m 528384 total steps have happened
[32m[20221213 20:26:02 @agent_ppo2.py:121][0m #------------------------ Iteration 258 --------------------------#
[32m[20221213 20:26:02 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:26:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:03 @agent_ppo2.py:185][0m |           0.0039 |           0.0941 |           0.0000 |
[32m[20221213 20:26:03 @agent_ppo2.py:185][0m |          -0.0105 |           0.0444 |           0.0000 |
[32m[20221213 20:26:03 @agent_ppo2.py:185][0m |          -0.0148 |           0.0373 |           0.0000 |
[32m[20221213 20:26:03 @agent_ppo2.py:185][0m |          -0.0152 |           0.0350 |           0.0000 |
[32m[20221213 20:26:04 @agent_ppo2.py:185][0m |          -0.0189 |           0.0339 |           0.0000 |
[32m[20221213 20:26:04 @agent_ppo2.py:185][0m |          -0.0194 |           0.0320 |           0.0000 |
[32m[20221213 20:26:04 @agent_ppo2.py:185][0m |          -0.0201 |           0.0307 |           0.0000 |
[32m[20221213 20:26:05 @agent_ppo2.py:185][0m |          -0.0252 |           0.0300 |           0.0000 |
[32m[20221213 20:26:05 @agent_ppo2.py:185][0m |          -0.0202 |           0.0296 |           0.0000 |
[32m[20221213 20:26:05 @agent_ppo2.py:185][0m |          -0.0216 |           0.0295 |           0.0000 |
[32m[20221213 20:26:05 @agent_ppo2.py:130][0m Policy update time: 2.99 s
[32m[20221213 20:26:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.10
[32m[20221213 20:26:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.85
[32m[20221213 20:26:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.77
[32m[20221213 20:26:06 @agent_ppo2.py:143][0m Total time:      17.66 min
[32m[20221213 20:26:06 @agent_ppo2.py:145][0m 530432 total steps have happened
[32m[20221213 20:26:06 @agent_ppo2.py:121][0m #------------------------ Iteration 259 --------------------------#
[32m[20221213 20:26:06 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:26:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:07 @agent_ppo2.py:185][0m |           0.0024 |           0.0227 |           0.0000 |
[32m[20221213 20:26:07 @agent_ppo2.py:185][0m |          -0.0155 |           0.0179 |           0.0000 |
[32m[20221213 20:26:07 @agent_ppo2.py:185][0m |          -0.0211 |           0.0169 |           0.0000 |
[32m[20221213 20:26:08 @agent_ppo2.py:185][0m |          -0.0249 |           0.0166 |           0.0000 |
[32m[20221213 20:26:08 @agent_ppo2.py:185][0m |          -0.0272 |           0.0161 |           0.0000 |
[32m[20221213 20:26:08 @agent_ppo2.py:185][0m |          -0.0297 |           0.0155 |           0.0000 |
[32m[20221213 20:26:08 @agent_ppo2.py:185][0m |          -0.0310 |           0.0153 |           0.0000 |
[32m[20221213 20:26:09 @agent_ppo2.py:185][0m |          -0.0310 |           0.0149 |           0.0000 |
[32m[20221213 20:26:09 @agent_ppo2.py:185][0m |          -0.0336 |           0.0148 |           0.0000 |
[32m[20221213 20:26:09 @agent_ppo2.py:185][0m |          -0.0325 |           0.0148 |           0.0000 |
[32m[20221213 20:26:09 @agent_ppo2.py:130][0m Policy update time: 3.06 s
[32m[20221213 20:26:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.89
[32m[20221213 20:26:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.64
[32m[20221213 20:26:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.68
[32m[20221213 20:26:10 @agent_ppo2.py:143][0m Total time:      17.73 min
[32m[20221213 20:26:10 @agent_ppo2.py:145][0m 532480 total steps have happened
[32m[20221213 20:26:10 @agent_ppo2.py:121][0m #------------------------ Iteration 260 --------------------------#
[32m[20221213 20:26:10 @agent_ppo2.py:127][0m Sampling time: 0.53 s by 10 slaves
[32m[20221213 20:26:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:11 @agent_ppo2.py:185][0m |           0.0018 |           0.0373 |           0.0000 |
[32m[20221213 20:26:11 @agent_ppo2.py:185][0m |          -0.0124 |           0.0264 |           0.0000 |
[32m[20221213 20:26:11 @agent_ppo2.py:185][0m |          -0.0154 |           0.0236 |           0.0000 |
[32m[20221213 20:26:12 @agent_ppo2.py:185][0m |          -0.0183 |           0.0229 |           0.0000 |
[32m[20221213 20:26:12 @agent_ppo2.py:185][0m |          -0.0202 |           0.0215 |           0.0000 |
[32m[20221213 20:26:12 @agent_ppo2.py:185][0m |          -0.0208 |           0.0209 |           0.0000 |
[32m[20221213 20:26:12 @agent_ppo2.py:185][0m |          -0.0222 |           0.0205 |           0.0000 |
[32m[20221213 20:26:13 @agent_ppo2.py:185][0m |          -0.0233 |           0.0202 |           0.0000 |
[32m[20221213 20:26:13 @agent_ppo2.py:185][0m |          -0.0228 |           0.0197 |           0.0000 |
[32m[20221213 20:26:13 @agent_ppo2.py:185][0m |          -0.0243 |           0.0196 |           0.0000 |
[32m[20221213 20:26:13 @agent_ppo2.py:130][0m Policy update time: 2.88 s
[32m[20221213 20:26:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.22
[32m[20221213 20:26:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.55
[32m[20221213 20:26:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.05
[32m[20221213 20:26:14 @agent_ppo2.py:143][0m Total time:      17.79 min
[32m[20221213 20:26:14 @agent_ppo2.py:145][0m 534528 total steps have happened
[32m[20221213 20:26:14 @agent_ppo2.py:121][0m #------------------------ Iteration 261 --------------------------#
[32m[20221213 20:26:14 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:26:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:15 @agent_ppo2.py:185][0m |           0.0029 |           0.0214 |           0.0000 |
[32m[20221213 20:26:15 @agent_ppo2.py:185][0m |          -0.0126 |           0.0160 |           0.0000 |
[32m[20221213 20:26:15 @agent_ppo2.py:185][0m |          -0.0183 |           0.0150 |           0.0000 |
[32m[20221213 20:26:15 @agent_ppo2.py:185][0m |          -0.0205 |           0.0145 |           0.0000 |
[32m[20221213 20:26:16 @agent_ppo2.py:185][0m |          -0.0196 |           0.0142 |           0.0000 |
[32m[20221213 20:26:16 @agent_ppo2.py:185][0m |          -0.0252 |           0.0139 |           0.0000 |
[32m[20221213 20:26:16 @agent_ppo2.py:185][0m |          -0.0264 |           0.0134 |           0.0000 |
[32m[20221213 20:26:17 @agent_ppo2.py:185][0m |          -0.0249 |           0.0134 |           0.0000 |
[32m[20221213 20:26:17 @agent_ppo2.py:185][0m |          -0.0266 |           0.0131 |           0.0000 |
[32m[20221213 20:26:17 @agent_ppo2.py:185][0m |          -0.0287 |           0.0130 |           0.0000 |
[32m[20221213 20:26:17 @agent_ppo2.py:130][0m Policy update time: 2.90 s
[32m[20221213 20:26:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.25
[32m[20221213 20:26:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.14
[32m[20221213 20:26:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.91
[32m[20221213 20:26:18 @agent_ppo2.py:143][0m Total time:      17.86 min
[32m[20221213 20:26:18 @agent_ppo2.py:145][0m 536576 total steps have happened
[32m[20221213 20:26:18 @agent_ppo2.py:121][0m #------------------------ Iteration 262 --------------------------#
[32m[20221213 20:26:18 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:26:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:19 @agent_ppo2.py:185][0m |           0.0017 |           0.0310 |           0.0000 |
[32m[20221213 20:26:19 @agent_ppo2.py:185][0m |          -0.0081 |           0.0222 |           0.0000 |
[32m[20221213 20:26:19 @agent_ppo2.py:185][0m |          -0.0121 |           0.0203 |           0.0000 |
[32m[20221213 20:26:19 @agent_ppo2.py:185][0m |          -0.0158 |           0.0197 |           0.0000 |
[32m[20221213 20:26:20 @agent_ppo2.py:185][0m |          -0.0173 |           0.0197 |           0.0000 |
[32m[20221213 20:26:20 @agent_ppo2.py:185][0m |          -0.0195 |           0.0186 |           0.0000 |
[32m[20221213 20:26:21 @agent_ppo2.py:185][0m |          -0.0235 |           0.0184 |           0.0000 |
[32m[20221213 20:26:21 @agent_ppo2.py:185][0m |          -0.0203 |           0.0185 |           0.0000 |
[32m[20221213 20:26:21 @agent_ppo2.py:185][0m |          -0.0205 |           0.0182 |           0.0000 |
[32m[20221213 20:26:21 @agent_ppo2.py:185][0m |          -0.0214 |           0.0181 |           0.0000 |
[32m[20221213 20:26:21 @agent_ppo2.py:130][0m Policy update time: 3.36 s
[32m[20221213 20:26:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.42
[32m[20221213 20:26:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.10
[32m[20221213 20:26:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.31
[32m[20221213 20:26:22 @agent_ppo2.py:143][0m Total time:      17.93 min
[32m[20221213 20:26:22 @agent_ppo2.py:145][0m 538624 total steps have happened
[32m[20221213 20:26:22 @agent_ppo2.py:121][0m #------------------------ Iteration 263 --------------------------#
[32m[20221213 20:26:23 @agent_ppo2.py:127][0m Sampling time: 0.55 s by 10 slaves
[32m[20221213 20:26:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:23 @agent_ppo2.py:185][0m |           0.0029 |           0.0174 |           0.0000 |
[32m[20221213 20:26:24 @agent_ppo2.py:185][0m |          -0.0149 |           0.0164 |           0.0000 |
[32m[20221213 20:26:24 @agent_ppo2.py:185][0m |          -0.0242 |           0.0159 |           0.0000 |
[32m[20221213 20:26:24 @agent_ppo2.py:185][0m |          -0.0268 |           0.0156 |           0.0000 |
[32m[20221213 20:26:24 @agent_ppo2.py:185][0m |          -0.0310 |           0.0152 |           0.0000 |
[32m[20221213 20:26:25 @agent_ppo2.py:185][0m |          -0.0325 |           0.0152 |           0.0000 |
[32m[20221213 20:26:25 @agent_ppo2.py:185][0m |          -0.0357 |           0.0150 |           0.0000 |
[32m[20221213 20:26:25 @agent_ppo2.py:185][0m |          -0.0412 |           0.0149 |           0.0000 |
[32m[20221213 20:26:26 @agent_ppo2.py:185][0m |          -0.0383 |           0.0146 |           0.0000 |
[32m[20221213 20:26:26 @agent_ppo2.py:185][0m |          -0.0411 |           0.0146 |           0.0000 |
[32m[20221213 20:26:26 @agent_ppo2.py:130][0m Policy update time: 3.15 s
[32m[20221213 20:26:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.54
[32m[20221213 20:26:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.18
[32m[20221213 20:26:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.14
[32m[20221213 20:26:26 @agent_ppo2.py:143][0m Total time:      18.00 min
[32m[20221213 20:26:26 @agent_ppo2.py:145][0m 540672 total steps have happened
[32m[20221213 20:26:26 @agent_ppo2.py:121][0m #------------------------ Iteration 264 --------------------------#
[32m[20221213 20:26:27 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:26:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:27 @agent_ppo2.py:185][0m |           0.0046 |           0.0328 |           0.0000 |
[32m[20221213 20:26:28 @agent_ppo2.py:185][0m |          -0.0104 |           0.0261 |           0.0000 |
[32m[20221213 20:26:28 @agent_ppo2.py:185][0m |          -0.0145 |           0.0242 |           0.0000 |
[32m[20221213 20:26:28 @agent_ppo2.py:185][0m |          -0.0183 |           0.0238 |           0.0000 |
[32m[20221213 20:26:28 @agent_ppo2.py:185][0m |          -0.0247 |           0.0223 |           0.0000 |
[32m[20221213 20:26:29 @agent_ppo2.py:185][0m |          -0.0205 |           0.0221 |           0.0000 |
[32m[20221213 20:26:29 @agent_ppo2.py:185][0m |          -0.0218 |           0.0212 |           0.0000 |
[32m[20221213 20:26:29 @agent_ppo2.py:185][0m |          -0.0227 |           0.0208 |           0.0000 |
[32m[20221213 20:26:29 @agent_ppo2.py:185][0m |          -0.0261 |           0.0206 |           0.0000 |
[32m[20221213 20:26:30 @agent_ppo2.py:185][0m |          -0.0256 |           0.0206 |           0.0000 |
[32m[20221213 20:26:30 @agent_ppo2.py:130][0m Policy update time: 2.83 s
[32m[20221213 20:26:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.09
[32m[20221213 20:26:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.88
[32m[20221213 20:26:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.99
[32m[20221213 20:26:30 @agent_ppo2.py:143][0m Total time:      18.06 min
[32m[20221213 20:26:30 @agent_ppo2.py:145][0m 542720 total steps have happened
[32m[20221213 20:26:30 @agent_ppo2.py:121][0m #------------------------ Iteration 265 --------------------------#
[32m[20221213 20:26:31 @agent_ppo2.py:127][0m Sampling time: 0.53 s by 10 slaves
[32m[20221213 20:26:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:31 @agent_ppo2.py:185][0m |           0.0023 |           0.0159 |           0.0000 |
[32m[20221213 20:26:31 @agent_ppo2.py:185][0m |          -0.0101 |           0.0120 |           0.0000 |
[32m[20221213 20:26:32 @agent_ppo2.py:185][0m |          -0.0178 |           0.0116 |           0.0000 |
[32m[20221213 20:26:32 @agent_ppo2.py:185][0m |          -0.0219 |           0.0112 |           0.0000 |
[32m[20221213 20:26:32 @agent_ppo2.py:185][0m |          -0.0231 |           0.0110 |           0.0000 |
[32m[20221213 20:26:33 @agent_ppo2.py:185][0m |          -0.0218 |           0.0108 |           0.0000 |
[32m[20221213 20:26:33 @agent_ppo2.py:185][0m |          -0.0268 |           0.0106 |           0.0000 |
[32m[20221213 20:26:33 @agent_ppo2.py:185][0m |          -0.0241 |           0.0105 |           0.0000 |
[32m[20221213 20:26:33 @agent_ppo2.py:185][0m |          -0.0271 |           0.0103 |           0.0000 |
[32m[20221213 20:26:34 @agent_ppo2.py:185][0m |          -0.0288 |           0.0102 |           0.0000 |
[32m[20221213 20:26:34 @agent_ppo2.py:130][0m Policy update time: 3.02 s
[32m[20221213 20:26:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.71
[32m[20221213 20:26:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.54
[32m[20221213 20:26:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.12
[32m[20221213 20:26:34 @agent_ppo2.py:143][0m Total time:      18.13 min
[32m[20221213 20:26:34 @agent_ppo2.py:145][0m 544768 total steps have happened
[32m[20221213 20:26:34 @agent_ppo2.py:121][0m #------------------------ Iteration 266 --------------------------#
[32m[20221213 20:26:35 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:26:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:35 @agent_ppo2.py:185][0m |           0.0024 |           0.0143 |           0.0000 |
[32m[20221213 20:26:35 @agent_ppo2.py:185][0m |          -0.0088 |           0.0129 |           0.0000 |
[32m[20221213 20:26:36 @agent_ppo2.py:185][0m |          -0.0121 |           0.0126 |           0.0000 |
[32m[20221213 20:26:36 @agent_ppo2.py:185][0m |          -0.0155 |           0.0123 |           0.0000 |
[32m[20221213 20:26:36 @agent_ppo2.py:185][0m |          -0.0174 |           0.0121 |           0.0000 |
[32m[20221213 20:26:37 @agent_ppo2.py:185][0m |          -0.0196 |           0.0119 |           0.0000 |
[32m[20221213 20:26:37 @agent_ppo2.py:185][0m |          -0.0208 |           0.0118 |           0.0000 |
[32m[20221213 20:26:37 @agent_ppo2.py:185][0m |          -0.0216 |           0.0118 |           0.0000 |
[32m[20221213 20:26:37 @agent_ppo2.py:185][0m |          -0.0226 |           0.0116 |           0.0000 |
[32m[20221213 20:26:38 @agent_ppo2.py:185][0m |          -0.0231 |           0.0116 |           0.0000 |
[32m[20221213 20:26:38 @agent_ppo2.py:130][0m Policy update time: 2.85 s
[32m[20221213 20:26:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.87
[32m[20221213 20:26:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.72
[32m[20221213 20:26:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.93
[32m[20221213 20:26:38 @agent_ppo2.py:143][0m Total time:      18.20 min
[32m[20221213 20:26:38 @agent_ppo2.py:145][0m 546816 total steps have happened
[32m[20221213 20:26:38 @agent_ppo2.py:121][0m #------------------------ Iteration 267 --------------------------#
[32m[20221213 20:26:39 @agent_ppo2.py:127][0m Sampling time: 0.53 s by 10 slaves
[32m[20221213 20:26:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:39 @agent_ppo2.py:185][0m |           0.0030 |           0.0107 |           0.0000 |
[32m[20221213 20:26:39 @agent_ppo2.py:185][0m |          -0.0145 |           0.0094 |           0.0000 |
[32m[20221213 20:26:40 @agent_ppo2.py:185][0m |          -0.0153 |           0.0091 |           0.0000 |
[32m[20221213 20:26:40 @agent_ppo2.py:185][0m |          -0.0196 |           0.0089 |           0.0000 |
[32m[20221213 20:26:40 @agent_ppo2.py:185][0m |          -0.0233 |           0.0088 |           0.0000 |
[32m[20221213 20:26:41 @agent_ppo2.py:185][0m |          -0.0261 |           0.0086 |           0.0000 |
[32m[20221213 20:26:41 @agent_ppo2.py:185][0m |          -0.0273 |           0.0085 |           0.0000 |
[32m[20221213 20:26:41 @agent_ppo2.py:185][0m |          -0.0280 |           0.0085 |           0.0000 |
[32m[20221213 20:26:41 @agent_ppo2.py:185][0m |          -0.0297 |           0.0084 |           0.0000 |
[32m[20221213 20:26:42 @agent_ppo2.py:185][0m |          -0.0311 |           0.0083 |           0.0000 |
[32m[20221213 20:26:42 @agent_ppo2.py:130][0m Policy update time: 3.25 s
[32m[20221213 20:26:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.51
[32m[20221213 20:26:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.38
[32m[20221213 20:26:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.59
[32m[20221213 20:26:42 @agent_ppo2.py:143][0m Total time:      18.27 min
[32m[20221213 20:26:42 @agent_ppo2.py:145][0m 548864 total steps have happened
[32m[20221213 20:26:42 @agent_ppo2.py:121][0m #------------------------ Iteration 268 --------------------------#
[32m[20221213 20:26:43 @agent_ppo2.py:127][0m Sampling time: 0.63 s by 10 slaves
[32m[20221213 20:26:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:43 @agent_ppo2.py:185][0m |           0.0037 |           0.0781 |           0.0000 |
[32m[20221213 20:26:44 @agent_ppo2.py:185][0m |          -0.0061 |           0.0277 |           0.0000 |
[32m[20221213 20:26:44 @agent_ppo2.py:185][0m |          -0.0136 |           0.0245 |           0.0000 |
[32m[20221213 20:26:44 @agent_ppo2.py:185][0m |          -0.0152 |           0.0227 |           0.0000 |
[32m[20221213 20:26:45 @agent_ppo2.py:185][0m |          -0.0180 |           0.0215 |           0.0000 |
[32m[20221213 20:26:45 @agent_ppo2.py:185][0m |          -0.0225 |           0.0209 |           0.0000 |
[32m[20221213 20:26:45 @agent_ppo2.py:185][0m |          -0.0207 |           0.0212 |           0.0000 |
[32m[20221213 20:26:45 @agent_ppo2.py:185][0m |          -0.0197 |           0.0200 |           0.0000 |
[32m[20221213 20:26:46 @agent_ppo2.py:185][0m |          -0.0206 |           0.0198 |           0.0000 |
[32m[20221213 20:26:46 @agent_ppo2.py:185][0m |          -0.0206 |           0.0189 |           0.0000 |
[32m[20221213 20:26:46 @agent_ppo2.py:130][0m Policy update time: 2.99 s
[32m[20221213 20:26:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.74
[32m[20221213 20:26:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.98
[32m[20221213 20:26:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.81
[32m[20221213 20:26:47 @agent_ppo2.py:143][0m Total time:      18.34 min
[32m[20221213 20:26:47 @agent_ppo2.py:145][0m 550912 total steps have happened
[32m[20221213 20:26:47 @agent_ppo2.py:121][0m #------------------------ Iteration 269 --------------------------#
[32m[20221213 20:26:47 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:26:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:48 @agent_ppo2.py:185][0m |           0.0002 |           0.0232 |           0.0000 |
[32m[20221213 20:26:48 @agent_ppo2.py:185][0m |          -0.0152 |           0.0186 |           0.0000 |
[32m[20221213 20:26:48 @agent_ppo2.py:185][0m |          -0.0226 |           0.0176 |           0.0000 |
[32m[20221213 20:26:48 @agent_ppo2.py:185][0m |          -0.0271 |           0.0169 |           0.0000 |
[32m[20221213 20:26:49 @agent_ppo2.py:185][0m |          -0.0308 |           0.0165 |           0.0000 |
[32m[20221213 20:26:49 @agent_ppo2.py:185][0m |          -0.0301 |           0.0163 |           0.0000 |
[32m[20221213 20:26:49 @agent_ppo2.py:185][0m |          -0.0329 |           0.0160 |           0.0000 |
[32m[20221213 20:26:49 @agent_ppo2.py:185][0m |          -0.0364 |           0.0159 |           0.0000 |
[32m[20221213 20:26:50 @agent_ppo2.py:185][0m |          -0.0358 |           0.0157 |           0.0000 |
[32m[20221213 20:26:50 @agent_ppo2.py:185][0m |          -0.0375 |           0.0155 |           0.0000 |
[32m[20221213 20:26:50 @agent_ppo2.py:130][0m Policy update time: 2.96 s
[32m[20221213 20:26:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.13
[32m[20221213 20:26:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.18
[32m[20221213 20:26:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.60
[32m[20221213 20:26:50 @agent_ppo2.py:143][0m Total time:      18.40 min
[32m[20221213 20:26:50 @agent_ppo2.py:145][0m 552960 total steps have happened
[32m[20221213 20:26:50 @agent_ppo2.py:121][0m #------------------------ Iteration 270 --------------------------#
[32m[20221213 20:26:51 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:26:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:51 @agent_ppo2.py:185][0m |           0.0031 |           0.0360 |           0.0000 |
[32m[20221213 20:26:52 @agent_ppo2.py:185][0m |          -0.0116 |           0.0272 |           0.0000 |
[32m[20221213 20:26:52 @agent_ppo2.py:185][0m |          -0.0211 |           0.0251 |           0.0000 |
[32m[20221213 20:26:52 @agent_ppo2.py:185][0m |          -0.0203 |           0.0238 |           0.0000 |
[32m[20221213 20:26:53 @agent_ppo2.py:185][0m |          -0.0225 |           0.0227 |           0.0000 |
[32m[20221213 20:26:53 @agent_ppo2.py:185][0m |          -0.0242 |           0.0218 |           0.0000 |
[32m[20221213 20:26:53 @agent_ppo2.py:185][0m |          -0.0264 |           0.0212 |           0.0000 |
[32m[20221213 20:26:53 @agent_ppo2.py:185][0m |          -0.0314 |           0.0215 |           0.0000 |
[32m[20221213 20:26:54 @agent_ppo2.py:185][0m |          -0.0280 |           0.0217 |           0.0000 |
[32m[20221213 20:26:54 @agent_ppo2.py:185][0m |          -0.0289 |           0.0205 |           0.0000 |
[32m[20221213 20:26:54 @agent_ppo2.py:130][0m Policy update time: 2.96 s
[32m[20221213 20:26:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.62
[32m[20221213 20:26:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.10
[32m[20221213 20:26:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.29
[32m[20221213 20:26:54 @agent_ppo2.py:143][0m Total time:      18.47 min
[32m[20221213 20:26:54 @agent_ppo2.py:145][0m 555008 total steps have happened
[32m[20221213 20:26:54 @agent_ppo2.py:121][0m #------------------------ Iteration 271 --------------------------#
[32m[20221213 20:26:55 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:26:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:55 @agent_ppo2.py:185][0m |           0.0020 |           0.0248 |           0.0000 |
[32m[20221213 20:26:56 @agent_ppo2.py:185][0m |          -0.0148 |           0.0192 |           0.0000 |
[32m[20221213 20:26:56 @agent_ppo2.py:185][0m |          -0.0176 |           0.0177 |           0.0000 |
[32m[20221213 20:26:56 @agent_ppo2.py:185][0m |          -0.0168 |           0.0171 |           0.0000 |
[32m[20221213 20:26:56 @agent_ppo2.py:185][0m |          -0.0210 |           0.0161 |           0.0000 |
[32m[20221213 20:26:57 @agent_ppo2.py:185][0m |          -0.0242 |           0.0158 |           0.0000 |
[32m[20221213 20:26:57 @agent_ppo2.py:185][0m |          -0.0253 |           0.0156 |           0.0000 |
[32m[20221213 20:26:57 @agent_ppo2.py:185][0m |          -0.0255 |           0.0152 |           0.0000 |
[32m[20221213 20:26:57 @agent_ppo2.py:185][0m |          -0.0275 |           0.0150 |           0.0000 |
[32m[20221213 20:26:58 @agent_ppo2.py:185][0m |          -0.0276 |           0.0151 |           0.0000 |
[32m[20221213 20:26:58 @agent_ppo2.py:130][0m Policy update time: 2.80 s
[32m[20221213 20:26:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.32
[32m[20221213 20:26:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.56
[32m[20221213 20:26:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.26
[32m[20221213 20:26:58 @agent_ppo2.py:143][0m Total time:      18.53 min
[32m[20221213 20:26:58 @agent_ppo2.py:145][0m 557056 total steps have happened
[32m[20221213 20:26:58 @agent_ppo2.py:121][0m #------------------------ Iteration 272 --------------------------#
[32m[20221213 20:26:59 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:26:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:26:59 @agent_ppo2.py:185][0m |           0.0012 |           0.0252 |           0.0000 |
[32m[20221213 20:26:59 @agent_ppo2.py:185][0m |          -0.0103 |           0.0197 |           0.0000 |
[32m[20221213 20:27:00 @agent_ppo2.py:185][0m |          -0.0135 |           0.0182 |           0.0000 |
[32m[20221213 20:27:00 @agent_ppo2.py:185][0m |          -0.0189 |           0.0174 |           0.0000 |
[32m[20221213 20:27:00 @agent_ppo2.py:185][0m |          -0.0212 |           0.0171 |           0.0000 |
[32m[20221213 20:27:00 @agent_ppo2.py:185][0m |          -0.0217 |           0.0163 |           0.0000 |
[32m[20221213 20:27:01 @agent_ppo2.py:185][0m |          -0.0240 |           0.0163 |           0.0000 |
[32m[20221213 20:27:01 @agent_ppo2.py:185][0m |          -0.0255 |           0.0161 |           0.0000 |
[32m[20221213 20:27:01 @agent_ppo2.py:185][0m |          -0.0261 |           0.0168 |           0.0000 |
[32m[20221213 20:27:01 @agent_ppo2.py:185][0m |          -0.0272 |           0.0154 |           0.0000 |
[32m[20221213 20:27:01 @agent_ppo2.py:130][0m Policy update time: 2.79 s
[32m[20221213 20:27:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.74
[32m[20221213 20:27:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.05
[32m[20221213 20:27:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.80
[32m[20221213 20:27:02 @agent_ppo2.py:143][0m Total time:      18.60 min
[32m[20221213 20:27:02 @agent_ppo2.py:145][0m 559104 total steps have happened
[32m[20221213 20:27:02 @agent_ppo2.py:121][0m #------------------------ Iteration 273 --------------------------#
[32m[20221213 20:27:03 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:27:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:03 @agent_ppo2.py:185][0m |           0.0042 |           0.0133 |           0.0000 |
[32m[20221213 20:27:03 @agent_ppo2.py:185][0m |          -0.0119 |           0.0087 |           0.0000 |
[32m[20221213 20:27:04 @agent_ppo2.py:185][0m |          -0.0160 |           0.0085 |           0.0000 |
[32m[20221213 20:27:04 @agent_ppo2.py:185][0m |          -0.0185 |           0.0083 |           0.0000 |
[32m[20221213 20:27:04 @agent_ppo2.py:185][0m |          -0.0204 |           0.0081 |           0.0000 |
[32m[20221213 20:27:04 @agent_ppo2.py:185][0m |          -0.0210 |           0.0079 |           0.0000 |
[32m[20221213 20:27:05 @agent_ppo2.py:185][0m |          -0.0221 |           0.0077 |           0.0000 |
[32m[20221213 20:27:05 @agent_ppo2.py:185][0m |          -0.0227 |           0.0076 |           0.0000 |
[32m[20221213 20:27:05 @agent_ppo2.py:185][0m |          -0.0235 |           0.0074 |           0.0000 |
[32m[20221213 20:27:05 @agent_ppo2.py:185][0m |          -0.0239 |           0.0073 |           0.0000 |
[32m[20221213 20:27:05 @agent_ppo2.py:130][0m Policy update time: 2.86 s
[32m[20221213 20:27:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.29
[32m[20221213 20:27:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.93
[32m[20221213 20:27:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.26
[32m[20221213 20:27:06 @agent_ppo2.py:143][0m Total time:      18.66 min
[32m[20221213 20:27:06 @agent_ppo2.py:145][0m 561152 total steps have happened
[32m[20221213 20:27:06 @agent_ppo2.py:121][0m #------------------------ Iteration 274 --------------------------#
[32m[20221213 20:27:06 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:27:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:07 @agent_ppo2.py:185][0m |           0.0025 |           0.0126 |           0.0000 |
[32m[20221213 20:27:07 @agent_ppo2.py:185][0m |          -0.0076 |           0.0111 |           0.0000 |
[32m[20221213 20:27:07 @agent_ppo2.py:185][0m |          -0.0106 |           0.0105 |           0.0000 |
[32m[20221213 20:27:08 @agent_ppo2.py:185][0m |          -0.0149 |           0.0108 |           0.0000 |
[32m[20221213 20:27:08 @agent_ppo2.py:185][0m |          -0.0158 |           0.0103 |           0.0000 |
[32m[20221213 20:27:08 @agent_ppo2.py:185][0m |          -0.0171 |           0.0102 |           0.0000 |
[32m[20221213 20:27:09 @agent_ppo2.py:185][0m |          -0.0224 |           0.0102 |           0.0000 |
[32m[20221213 20:27:09 @agent_ppo2.py:185][0m |          -0.0179 |           0.0099 |           0.0000 |
[32m[20221213 20:27:09 @agent_ppo2.py:185][0m |          -0.0197 |           0.0100 |           0.0000 |
[32m[20221213 20:27:09 @agent_ppo2.py:185][0m |          -0.0207 |           0.0098 |           0.0000 |
[32m[20221213 20:27:09 @agent_ppo2.py:130][0m Policy update time: 2.97 s
[32m[20221213 20:27:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.70
[32m[20221213 20:27:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.82
[32m[20221213 20:27:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.62
[32m[20221213 20:27:10 @agent_ppo2.py:143][0m Total time:      18.73 min
[32m[20221213 20:27:10 @agent_ppo2.py:145][0m 563200 total steps have happened
[32m[20221213 20:27:10 @agent_ppo2.py:121][0m #------------------------ Iteration 275 --------------------------#
[32m[20221213 20:27:11 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:27:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:11 @agent_ppo2.py:185][0m |           0.0015 |           0.0337 |           0.0000 |
[32m[20221213 20:27:11 @agent_ppo2.py:185][0m |          -0.0096 |           0.0192 |           0.0000 |
[32m[20221213 20:27:12 @agent_ppo2.py:185][0m |          -0.0140 |           0.0182 |           0.0000 |
[32m[20221213 20:27:12 @agent_ppo2.py:185][0m |          -0.0152 |           0.0174 |           0.0000 |
[32m[20221213 20:27:12 @agent_ppo2.py:185][0m |          -0.0149 |           0.0165 |           0.0000 |
[32m[20221213 20:27:12 @agent_ppo2.py:185][0m |          -0.0170 |           0.0163 |           0.0000 |
[32m[20221213 20:27:13 @agent_ppo2.py:185][0m |          -0.0179 |           0.0159 |           0.0000 |
[32m[20221213 20:27:13 @agent_ppo2.py:185][0m |          -0.0183 |           0.0159 |           0.0000 |
[32m[20221213 20:27:13 @agent_ppo2.py:185][0m |          -0.0241 |           0.0156 |           0.0000 |
[32m[20221213 20:27:14 @agent_ppo2.py:185][0m |          -0.0190 |           0.0156 |           0.0000 |
[32m[20221213 20:27:14 @agent_ppo2.py:130][0m Policy update time: 3.00 s
[32m[20221213 20:27:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.08
[32m[20221213 20:27:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.13
[32m[20221213 20:27:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.57
[32m[20221213 20:27:14 @agent_ppo2.py:143][0m Total time:      18.80 min
[32m[20221213 20:27:14 @agent_ppo2.py:145][0m 565248 total steps have happened
[32m[20221213 20:27:14 @agent_ppo2.py:121][0m #------------------------ Iteration 276 --------------------------#
[32m[20221213 20:27:15 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:27:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:15 @agent_ppo2.py:185][0m |           0.0048 |           0.0577 |           0.0000 |
[32m[20221213 20:27:15 @agent_ppo2.py:185][0m |          -0.0074 |           0.0368 |           0.0000 |
[32m[20221213 20:27:16 @agent_ppo2.py:185][0m |          -0.0124 |           0.0328 |           0.0000 |
[32m[20221213 20:27:16 @agent_ppo2.py:185][0m |          -0.0137 |           0.0313 |           0.0000 |
[32m[20221213 20:27:16 @agent_ppo2.py:185][0m |          -0.0170 |           0.0288 |           0.0000 |
[32m[20221213 20:27:17 @agent_ppo2.py:185][0m |          -0.0185 |           0.0274 |           0.0000 |
[32m[20221213 20:27:17 @agent_ppo2.py:185][0m |          -0.0193 |           0.0266 |           0.0000 |
[32m[20221213 20:27:17 @agent_ppo2.py:185][0m |          -0.0260 |           0.0272 |           0.0000 |
[32m[20221213 20:27:17 @agent_ppo2.py:185][0m |          -0.0186 |           0.0263 |           0.0000 |
[32m[20221213 20:27:18 @agent_ppo2.py:185][0m |          -0.0214 |           0.0252 |           0.0000 |
[32m[20221213 20:27:18 @agent_ppo2.py:130][0m Policy update time: 2.98 s
[32m[20221213 20:27:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.03
[32m[20221213 20:27:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.22
[32m[20221213 20:27:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.45
[32m[20221213 20:27:18 @agent_ppo2.py:143][0m Total time:      18.86 min
[32m[20221213 20:27:18 @agent_ppo2.py:145][0m 567296 total steps have happened
[32m[20221213 20:27:18 @agent_ppo2.py:121][0m #------------------------ Iteration 277 --------------------------#
[32m[20221213 20:27:19 @agent_ppo2.py:127][0m Sampling time: 0.64 s by 10 slaves
[32m[20221213 20:27:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:19 @agent_ppo2.py:185][0m |           0.0096 |           0.0305 |           0.0000 |
[32m[20221213 20:27:19 @agent_ppo2.py:185][0m |          -0.0102 |           0.0135 |           0.0000 |
[32m[20221213 20:27:20 @agent_ppo2.py:185][0m |          -0.0182 |           0.0130 |           0.0000 |
[32m[20221213 20:27:20 @agent_ppo2.py:185][0m |          -0.0185 |           0.0127 |           0.0000 |
[32m[20221213 20:27:20 @agent_ppo2.py:185][0m |          -0.0220 |           0.0124 |           0.0000 |
[32m[20221213 20:27:20 @agent_ppo2.py:185][0m |          -0.0245 |           0.0121 |           0.0000 |
[32m[20221213 20:27:21 @agent_ppo2.py:185][0m |          -0.0215 |           0.0119 |           0.0000 |
[32m[20221213 20:27:21 @agent_ppo2.py:185][0m |          -0.0239 |           0.0117 |           0.0000 |
[32m[20221213 20:27:21 @agent_ppo2.py:185][0m |          -0.0266 |           0.0116 |           0.0000 |
[32m[20221213 20:27:21 @agent_ppo2.py:185][0m |          -0.0274 |           0.0114 |           0.0000 |
[32m[20221213 20:27:21 @agent_ppo2.py:130][0m Policy update time: 2.76 s
[32m[20221213 20:27:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.42
[32m[20221213 20:27:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.66
[32m[20221213 20:27:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.04
[32m[20221213 20:27:22 @agent_ppo2.py:143][0m Total time:      18.93 min
[32m[20221213 20:27:22 @agent_ppo2.py:145][0m 569344 total steps have happened
[32m[20221213 20:27:22 @agent_ppo2.py:121][0m #------------------------ Iteration 278 --------------------------#
[32m[20221213 20:27:22 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:27:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:23 @agent_ppo2.py:185][0m |           0.0040 |           0.0116 |           0.0000 |
[32m[20221213 20:27:23 @agent_ppo2.py:185][0m |          -0.0128 |           0.0080 |           0.0000 |
[32m[20221213 20:27:23 @agent_ppo2.py:185][0m |          -0.0164 |           0.0077 |           0.0000 |
[32m[20221213 20:27:24 @agent_ppo2.py:185][0m |          -0.0200 |           0.0075 |           0.0000 |
[32m[20221213 20:27:24 @agent_ppo2.py:185][0m |          -0.0179 |           0.0073 |           0.0000 |
[32m[20221213 20:27:24 @agent_ppo2.py:185][0m |          -0.0216 |           0.0070 |           0.0000 |
[32m[20221213 20:27:25 @agent_ppo2.py:185][0m |          -0.0239 |           0.0068 |           0.0000 |
[32m[20221213 20:27:25 @agent_ppo2.py:185][0m |          -0.0236 |           0.0066 |           0.0000 |
[32m[20221213 20:27:25 @agent_ppo2.py:185][0m |          -0.0234 |           0.0063 |           0.0000 |
[32m[20221213 20:27:26 @agent_ppo2.py:185][0m |          -0.0262 |           0.0061 |           0.0000 |
[32m[20221213 20:27:26 @agent_ppo2.py:130][0m Policy update time: 3.28 s
[32m[20221213 20:27:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.34
[32m[20221213 20:27:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.66
[32m[20221213 20:27:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.28
[32m[20221213 20:27:26 @agent_ppo2.py:143][0m Total time:      19.00 min
[32m[20221213 20:27:26 @agent_ppo2.py:145][0m 571392 total steps have happened
[32m[20221213 20:27:26 @agent_ppo2.py:121][0m #------------------------ Iteration 279 --------------------------#
[32m[20221213 20:27:27 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:27:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:27 @agent_ppo2.py:185][0m |           0.0033 |           0.1000 |           0.0000 |
[32m[20221213 20:27:27 @agent_ppo2.py:185][0m |          -0.0083 |           0.0349 |           0.0000 |
[32m[20221213 20:27:28 @agent_ppo2.py:185][0m |          -0.0133 |           0.0301 |           0.0000 |
[32m[20221213 20:27:28 @agent_ppo2.py:185][0m |          -0.0169 |           0.0278 |           0.0000 |
[32m[20221213 20:27:28 @agent_ppo2.py:185][0m |          -0.0170 |           0.0265 |           0.0000 |
[32m[20221213 20:27:28 @agent_ppo2.py:185][0m |          -0.0219 |           0.0247 |           0.0000 |
[32m[20221213 20:27:29 @agent_ppo2.py:185][0m |          -0.0192 |           0.0240 |           0.0000 |
[32m[20221213 20:27:29 @agent_ppo2.py:185][0m |          -0.0176 |           0.0232 |           0.0000 |
[32m[20221213 20:27:29 @agent_ppo2.py:185][0m |          -0.0196 |           0.0227 |           0.0000 |
[32m[20221213 20:27:30 @agent_ppo2.py:185][0m |          -0.0200 |           0.0224 |           0.0000 |
[32m[20221213 20:27:30 @agent_ppo2.py:130][0m Policy update time: 2.92 s
[32m[20221213 20:27:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.73
[32m[20221213 20:27:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.53
[32m[20221213 20:27:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 2.36
[32m[20221213 20:27:30 @agent_ppo2.py:143][0m Total time:      19.06 min
[32m[20221213 20:27:30 @agent_ppo2.py:145][0m 573440 total steps have happened
[32m[20221213 20:27:30 @agent_ppo2.py:121][0m #------------------------ Iteration 280 --------------------------#
[32m[20221213 20:27:31 @agent_ppo2.py:127][0m Sampling time: 0.66 s by 10 slaves
[32m[20221213 20:27:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:31 @agent_ppo2.py:185][0m |           0.0024 |           0.0187 |           0.0000 |
[32m[20221213 20:27:31 @agent_ppo2.py:185][0m |          -0.0156 |           0.0131 |           0.0000 |
[32m[20221213 20:27:32 @agent_ppo2.py:185][0m |          -0.0206 |           0.0128 |           0.0000 |
[32m[20221213 20:27:32 @agent_ppo2.py:185][0m |          -0.0233 |           0.0125 |           0.0000 |
[32m[20221213 20:27:32 @agent_ppo2.py:185][0m |          -0.0258 |           0.0122 |           0.0000 |
[32m[20221213 20:27:33 @agent_ppo2.py:185][0m |          -0.0260 |           0.0119 |           0.0000 |
[32m[20221213 20:27:33 @agent_ppo2.py:185][0m |          -0.0281 |           0.0120 |           0.0000 |
[32m[20221213 20:27:33 @agent_ppo2.py:185][0m |          -0.0291 |           0.0119 |           0.0000 |
[32m[20221213 20:27:33 @agent_ppo2.py:185][0m |          -0.0300 |           0.0117 |           0.0000 |
[32m[20221213 20:27:34 @agent_ppo2.py:185][0m |          -0.0318 |           0.0115 |           0.0000 |
[32m[20221213 20:27:34 @agent_ppo2.py:130][0m Policy update time: 2.89 s
[32m[20221213 20:27:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.62
[32m[20221213 20:27:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.53
[32m[20221213 20:27:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.76
[32m[20221213 20:27:34 @agent_ppo2.py:143][0m Total time:      19.13 min
[32m[20221213 20:27:34 @agent_ppo2.py:145][0m 575488 total steps have happened
[32m[20221213 20:27:34 @agent_ppo2.py:121][0m #------------------------ Iteration 281 --------------------------#
[32m[20221213 20:27:35 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:27:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:35 @agent_ppo2.py:185][0m |           0.0039 |           0.0368 |           0.0000 |
[32m[20221213 20:27:35 @agent_ppo2.py:185][0m |          -0.0098 |           0.0280 |           0.0000 |
[32m[20221213 20:27:36 @agent_ppo2.py:185][0m |          -0.0139 |           0.0239 |           0.0000 |
[32m[20221213 20:27:36 @agent_ppo2.py:185][0m |          -0.0182 |           0.0224 |           0.0000 |
[32m[20221213 20:27:36 @agent_ppo2.py:185][0m |          -0.0185 |           0.0217 |           0.0000 |
[32m[20221213 20:27:36 @agent_ppo2.py:185][0m |          -0.0201 |           0.0208 |           0.0000 |
[32m[20221213 20:27:37 @agent_ppo2.py:185][0m |          -0.0213 |           0.0205 |           0.0000 |
[32m[20221213 20:27:37 @agent_ppo2.py:185][0m |          -0.0224 |           0.0201 |           0.0000 |
[32m[20221213 20:27:37 @agent_ppo2.py:185][0m |          -0.0239 |           0.0198 |           0.0000 |
[32m[20221213 20:27:37 @agent_ppo2.py:185][0m |          -0.0236 |           0.0197 |           0.0000 |
[32m[20221213 20:27:37 @agent_ppo2.py:130][0m Policy update time: 2.81 s
[32m[20221213 20:27:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.71
[32m[20221213 20:27:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.42
[32m[20221213 20:27:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.54
[32m[20221213 20:27:38 @agent_ppo2.py:143][0m Total time:      19.19 min
[32m[20221213 20:27:38 @agent_ppo2.py:145][0m 577536 total steps have happened
[32m[20221213 20:27:38 @agent_ppo2.py:121][0m #------------------------ Iteration 282 --------------------------#
[32m[20221213 20:27:38 @agent_ppo2.py:127][0m Sampling time: 0.47 s by 10 slaves
[32m[20221213 20:27:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:39 @agent_ppo2.py:185][0m |           0.0043 |           0.0400 |           0.0000 |
[32m[20221213 20:27:39 @agent_ppo2.py:185][0m |          -0.0095 |           0.0289 |           0.0000 |
[32m[20221213 20:27:39 @agent_ppo2.py:185][0m |          -0.0157 |           0.0257 |           0.0000 |
[32m[20221213 20:27:40 @agent_ppo2.py:185][0m |          -0.0180 |           0.0247 |           0.0000 |
[32m[20221213 20:27:40 @agent_ppo2.py:185][0m |          -0.0196 |           0.0234 |           0.0000 |
[32m[20221213 20:27:40 @agent_ppo2.py:185][0m |          -0.0220 |           0.0229 |           0.0000 |
[32m[20221213 20:27:40 @agent_ppo2.py:185][0m |          -0.0228 |           0.0224 |           0.0000 |
[32m[20221213 20:27:41 @agent_ppo2.py:185][0m |          -0.0238 |           0.0221 |           0.0000 |
[32m[20221213 20:27:41 @agent_ppo2.py:185][0m |          -0.0238 |           0.0217 |           0.0000 |
[32m[20221213 20:27:41 @agent_ppo2.py:185][0m |          -0.0240 |           0.0220 |           0.0000 |
[32m[20221213 20:27:41 @agent_ppo2.py:130][0m Policy update time: 2.73 s
[32m[20221213 20:27:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.82
[32m[20221213 20:27:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.47
[32m[20221213 20:27:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.28
[32m[20221213 20:27:42 @agent_ppo2.py:143][0m Total time:      19.26 min
[32m[20221213 20:27:42 @agent_ppo2.py:145][0m 579584 total steps have happened
[32m[20221213 20:27:42 @agent_ppo2.py:121][0m #------------------------ Iteration 283 --------------------------#
[32m[20221213 20:27:42 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:27:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:43 @agent_ppo2.py:185][0m |           0.0034 |           0.0339 |           0.0000 |
[32m[20221213 20:27:43 @agent_ppo2.py:185][0m |          -0.0162 |           0.0195 |           0.0000 |
[32m[20221213 20:27:43 @agent_ppo2.py:185][0m |          -0.0203 |           0.0183 |           0.0000 |
[32m[20221213 20:27:43 @agent_ppo2.py:185][0m |          -0.0229 |           0.0175 |           0.0000 |
[32m[20221213 20:27:44 @agent_ppo2.py:185][0m |          -0.0247 |           0.0169 |           0.0000 |
[32m[20221213 20:27:44 @agent_ppo2.py:185][0m |          -0.0265 |           0.0168 |           0.0000 |
[32m[20221213 20:27:44 @agent_ppo2.py:185][0m |          -0.0243 |           0.0163 |           0.0000 |
[32m[20221213 20:27:44 @agent_ppo2.py:185][0m |          -0.0277 |           0.0158 |           0.0000 |
[32m[20221213 20:27:45 @agent_ppo2.py:185][0m |          -0.0281 |           0.0156 |           0.0000 |
[32m[20221213 20:27:45 @agent_ppo2.py:185][0m |          -0.0273 |           0.0153 |           0.0000 |
[32m[20221213 20:27:45 @agent_ppo2.py:130][0m Policy update time: 2.69 s
[32m[20221213 20:27:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.49
[32m[20221213 20:27:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.28
[32m[20221213 20:27:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.55
[32m[20221213 20:27:45 @agent_ppo2.py:143][0m Total time:      19.32 min
[32m[20221213 20:27:45 @agent_ppo2.py:145][0m 581632 total steps have happened
[32m[20221213 20:27:45 @agent_ppo2.py:121][0m #------------------------ Iteration 284 --------------------------#
[32m[20221213 20:27:46 @agent_ppo2.py:127][0m Sampling time: 0.61 s by 10 slaves
[32m[20221213 20:27:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:47 @agent_ppo2.py:185][0m |           0.0041 |           0.0119 |           0.0000 |
[32m[20221213 20:27:47 @agent_ppo2.py:185][0m |          -0.0132 |           0.0093 |           0.0000 |
[32m[20221213 20:27:47 @agent_ppo2.py:185][0m |          -0.0204 |           0.0090 |           0.0000 |
[32m[20221213 20:27:47 @agent_ppo2.py:185][0m |          -0.0245 |           0.0087 |           0.0000 |
[32m[20221213 20:27:48 @agent_ppo2.py:185][0m |          -0.0250 |           0.0084 |           0.0000 |
[32m[20221213 20:27:48 @agent_ppo2.py:185][0m |          -0.0275 |           0.0082 |           0.0000 |
[32m[20221213 20:27:48 @agent_ppo2.py:185][0m |          -0.0279 |           0.0080 |           0.0000 |
[32m[20221213 20:27:48 @agent_ppo2.py:185][0m |          -0.0304 |           0.0078 |           0.0000 |
[32m[20221213 20:27:49 @agent_ppo2.py:185][0m |          -0.0316 |           0.0076 |           0.0000 |
[32m[20221213 20:27:49 @agent_ppo2.py:185][0m |          -0.0327 |           0.0074 |           0.0000 |
[32m[20221213 20:27:49 @agent_ppo2.py:130][0m Policy update time: 2.94 s
[32m[20221213 20:27:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.95
[32m[20221213 20:27:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.59
[32m[20221213 20:27:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.65
[32m[20221213 20:27:49 @agent_ppo2.py:143][0m Total time:      19.39 min
[32m[20221213 20:27:49 @agent_ppo2.py:145][0m 583680 total steps have happened
[32m[20221213 20:27:49 @agent_ppo2.py:121][0m #------------------------ Iteration 285 --------------------------#
[32m[20221213 20:27:50 @agent_ppo2.py:127][0m Sampling time: 0.48 s by 10 slaves
[32m[20221213 20:27:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:50 @agent_ppo2.py:185][0m |           0.0054 |           0.0487 |           0.0000 |
[32m[20221213 20:27:51 @agent_ppo2.py:185][0m |          -0.0107 |           0.0204 |           0.0000 |
[32m[20221213 20:27:51 @agent_ppo2.py:185][0m |          -0.0142 |           0.0176 |           0.0000 |
[32m[20221213 20:27:51 @agent_ppo2.py:185][0m |          -0.0160 |           0.0168 |           0.0000 |
[32m[20221213 20:27:52 @agent_ppo2.py:185][0m |          -0.0184 |           0.0160 |           0.0000 |
[32m[20221213 20:27:52 @agent_ppo2.py:185][0m |          -0.0182 |           0.0157 |           0.0000 |
[32m[20221213 20:27:52 @agent_ppo2.py:185][0m |          -0.0196 |           0.0154 |           0.0000 |
[32m[20221213 20:27:52 @agent_ppo2.py:185][0m |          -0.0223 |           0.0151 |           0.0000 |
[32m[20221213 20:27:53 @agent_ppo2.py:185][0m |          -0.0193 |           0.0150 |           0.0000 |
[32m[20221213 20:27:53 @agent_ppo2.py:185][0m |          -0.0205 |           0.0148 |           0.0000 |
[32m[20221213 20:27:53 @agent_ppo2.py:130][0m Policy update time: 2.96 s
[32m[20221213 20:27:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.87
[32m[20221213 20:27:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.82
[32m[20221213 20:27:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.28
[32m[20221213 20:27:53 @agent_ppo2.py:143][0m Total time:      19.45 min
[32m[20221213 20:27:53 @agent_ppo2.py:145][0m 585728 total steps have happened
[32m[20221213 20:27:53 @agent_ppo2.py:121][0m #------------------------ Iteration 286 --------------------------#
[32m[20221213 20:27:54 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:27:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:54 @agent_ppo2.py:185][0m |           0.0033 |           0.0320 |           0.0000 |
[32m[20221213 20:27:55 @agent_ppo2.py:185][0m |          -0.0102 |           0.0217 |           0.0000 |
[32m[20221213 20:27:55 @agent_ppo2.py:185][0m |          -0.0165 |           0.0203 |           0.0000 |
[32m[20221213 20:27:55 @agent_ppo2.py:185][0m |          -0.0195 |           0.0193 |           0.0000 |
[32m[20221213 20:27:55 @agent_ppo2.py:185][0m |          -0.0207 |           0.0190 |           0.0000 |
[32m[20221213 20:27:56 @agent_ppo2.py:185][0m |          -0.0220 |           0.0179 |           0.0000 |
[32m[20221213 20:27:56 @agent_ppo2.py:185][0m |          -0.0221 |           0.0181 |           0.0000 |
[32m[20221213 20:27:56 @agent_ppo2.py:185][0m |          -0.0228 |           0.0175 |           0.0000 |
[32m[20221213 20:27:57 @agent_ppo2.py:185][0m |          -0.0237 |           0.0173 |           0.0000 |
[32m[20221213 20:27:57 @agent_ppo2.py:185][0m |          -0.0247 |           0.0171 |           0.0000 |
[32m[20221213 20:27:57 @agent_ppo2.py:130][0m Policy update time: 3.01 s
[32m[20221213 20:27:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.66
[32m[20221213 20:27:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.07
[32m[20221213 20:27:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.77
[32m[20221213 20:27:58 @agent_ppo2.py:143][0m Total time:      19.52 min
[32m[20221213 20:27:58 @agent_ppo2.py:145][0m 587776 total steps have happened
[32m[20221213 20:27:58 @agent_ppo2.py:121][0m #------------------------ Iteration 287 --------------------------#
[32m[20221213 20:27:58 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:27:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:27:59 @agent_ppo2.py:185][0m |           0.0041 |           0.0177 |           0.0000 |
[32m[20221213 20:27:59 @agent_ppo2.py:185][0m |          -0.0138 |           0.0152 |           0.0000 |
[32m[20221213 20:27:59 @agent_ppo2.py:185][0m |          -0.0202 |           0.0145 |           0.0000 |
[32m[20221213 20:27:59 @agent_ppo2.py:185][0m |          -0.0254 |           0.0140 |           0.0000 |
[32m[20221213 20:28:00 @agent_ppo2.py:185][0m |          -0.0295 |           0.0137 |           0.0000 |
[32m[20221213 20:28:00 @agent_ppo2.py:185][0m |          -0.0312 |           0.0136 |           0.0000 |
[32m[20221213 20:28:00 @agent_ppo2.py:185][0m |          -0.0323 |           0.0134 |           0.0000 |
[32m[20221213 20:28:00 @agent_ppo2.py:185][0m |          -0.0348 |           0.0133 |           0.0000 |
[32m[20221213 20:28:01 @agent_ppo2.py:185][0m |          -0.0366 |           0.0133 |           0.0000 |
[32m[20221213 20:28:01 @agent_ppo2.py:185][0m |          -0.0336 |           0.0132 |           0.0000 |
[32m[20221213 20:28:01 @agent_ppo2.py:130][0m Policy update time: 2.98 s
[32m[20221213 20:28:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.32
[32m[20221213 20:28:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.93
[32m[20221213 20:28:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.97
[32m[20221213 20:28:01 @agent_ppo2.py:143][0m Total time:      19.59 min
[32m[20221213 20:28:01 @agent_ppo2.py:145][0m 589824 total steps have happened
[32m[20221213 20:28:02 @agent_ppo2.py:121][0m #------------------------ Iteration 288 --------------------------#
[32m[20221213 20:28:02 @agent_ppo2.py:127][0m Sampling time: 0.52 s by 10 slaves
[32m[20221213 20:28:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:03 @agent_ppo2.py:185][0m |          -0.0026 |           0.0127 |           0.0000 |
[32m[20221213 20:28:03 @agent_ppo2.py:185][0m |          -0.0152 |           0.0119 |           0.0000 |
[32m[20221213 20:28:03 @agent_ppo2.py:185][0m |          -0.0224 |           0.0115 |           0.0000 |
[32m[20221213 20:28:03 @agent_ppo2.py:185][0m |          -0.0240 |           0.0112 |           0.0000 |
[32m[20221213 20:28:04 @agent_ppo2.py:185][0m |          -0.0256 |           0.0110 |           0.0000 |
[32m[20221213 20:28:04 @agent_ppo2.py:185][0m |          -0.0278 |           0.0108 |           0.0000 |
[32m[20221213 20:28:04 @agent_ppo2.py:185][0m |          -0.0298 |           0.0108 |           0.0000 |
[32m[20221213 20:28:05 @agent_ppo2.py:185][0m |          -0.0304 |           0.0107 |           0.0000 |
[32m[20221213 20:28:05 @agent_ppo2.py:185][0m |          -0.0339 |           0.0106 |           0.0000 |
[32m[20221213 20:28:05 @agent_ppo2.py:185][0m |          -0.0319 |           0.0105 |           0.0000 |
[32m[20221213 20:28:05 @agent_ppo2.py:130][0m Policy update time: 3.17 s
[32m[20221213 20:28:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.98
[32m[20221213 20:28:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.64
[32m[20221213 20:28:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.71
[32m[20221213 20:28:06 @agent_ppo2.py:143][0m Total time:      19.66 min
[32m[20221213 20:28:06 @agent_ppo2.py:145][0m 591872 total steps have happened
[32m[20221213 20:28:06 @agent_ppo2.py:121][0m #------------------------ Iteration 289 --------------------------#
[32m[20221213 20:28:06 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:28:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:07 @agent_ppo2.py:185][0m |           0.0051 |           0.0242 |           0.0000 |
[32m[20221213 20:28:07 @agent_ppo2.py:185][0m |          -0.0123 |           0.0170 |           0.0000 |
[32m[20221213 20:28:07 @agent_ppo2.py:185][0m |          -0.0154 |           0.0160 |           0.0000 |
[32m[20221213 20:28:08 @agent_ppo2.py:185][0m |          -0.0175 |           0.0152 |           0.0000 |
[32m[20221213 20:28:08 @agent_ppo2.py:185][0m |          -0.0188 |           0.0146 |           0.0000 |
[32m[20221213 20:28:08 @agent_ppo2.py:185][0m |          -0.0193 |           0.0148 |           0.0000 |
[32m[20221213 20:28:08 @agent_ppo2.py:185][0m |          -0.0211 |           0.0143 |           0.0000 |
[32m[20221213 20:28:09 @agent_ppo2.py:185][0m |          -0.0214 |           0.0141 |           0.0000 |
[32m[20221213 20:28:09 @agent_ppo2.py:185][0m |          -0.0223 |           0.0142 |           0.0000 |
[32m[20221213 20:28:09 @agent_ppo2.py:185][0m |          -0.0231 |           0.0140 |           0.0000 |
[32m[20221213 20:28:09 @agent_ppo2.py:130][0m Policy update time: 2.97 s
[32m[20221213 20:28:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.99
[32m[20221213 20:28:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.94
[32m[20221213 20:28:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.97
[32m[20221213 20:28:10 @agent_ppo2.py:143][0m Total time:      19.72 min
[32m[20221213 20:28:10 @agent_ppo2.py:145][0m 593920 total steps have happened
[32m[20221213 20:28:10 @agent_ppo2.py:121][0m #------------------------ Iteration 290 --------------------------#
[32m[20221213 20:28:10 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:28:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:11 @agent_ppo2.py:185][0m |           0.0042 |           0.0182 |           0.0000 |
[32m[20221213 20:28:11 @agent_ppo2.py:185][0m |          -0.0106 |           0.0150 |           0.0000 |
[32m[20221213 20:28:11 @agent_ppo2.py:185][0m |          -0.0140 |           0.0142 |           0.0000 |
[32m[20221213 20:28:12 @agent_ppo2.py:185][0m |          -0.0188 |           0.0142 |           0.0000 |
[32m[20221213 20:28:12 @agent_ppo2.py:185][0m |          -0.0192 |           0.0137 |           0.0000 |
[32m[20221213 20:28:12 @agent_ppo2.py:185][0m |          -0.0231 |           0.0134 |           0.0000 |
[32m[20221213 20:28:13 @agent_ppo2.py:185][0m |          -0.0229 |           0.0132 |           0.0000 |
[32m[20221213 20:28:13 @agent_ppo2.py:185][0m |          -0.0250 |           0.0132 |           0.0000 |
[32m[20221213 20:28:13 @agent_ppo2.py:185][0m |          -0.0258 |           0.0129 |           0.0000 |
[32m[20221213 20:28:13 @agent_ppo2.py:185][0m |          -0.0256 |           0.0130 |           0.0000 |
[32m[20221213 20:28:13 @agent_ppo2.py:130][0m Policy update time: 3.08 s
[32m[20221213 20:28:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.00
[32m[20221213 20:28:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.58
[32m[20221213 20:28:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.14
[32m[20221213 20:28:14 @agent_ppo2.py:143][0m Total time:      19.79 min
[32m[20221213 20:28:14 @agent_ppo2.py:145][0m 595968 total steps have happened
[32m[20221213 20:28:14 @agent_ppo2.py:121][0m #------------------------ Iteration 291 --------------------------#
[32m[20221213 20:28:14 @agent_ppo2.py:127][0m Sampling time: 0.54 s by 10 slaves
[32m[20221213 20:28:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:15 @agent_ppo2.py:185][0m |           0.0018 |           0.0227 |           0.0000 |
[32m[20221213 20:28:15 @agent_ppo2.py:185][0m |          -0.0131 |           0.0188 |           0.0000 |
[32m[20221213 20:28:15 @agent_ppo2.py:185][0m |          -0.0226 |           0.0176 |           0.0000 |
[32m[20221213 20:28:16 @agent_ppo2.py:185][0m |          -0.0294 |           0.0171 |           0.0000 |
[32m[20221213 20:28:16 @agent_ppo2.py:185][0m |          -0.0312 |           0.0165 |           0.0000 |
[32m[20221213 20:28:16 @agent_ppo2.py:185][0m |          -0.0325 |           0.0162 |           0.0000 |
[32m[20221213 20:28:17 @agent_ppo2.py:185][0m |          -0.0342 |           0.0163 |           0.0000 |
[32m[20221213 20:28:17 @agent_ppo2.py:185][0m |          -0.0393 |           0.0161 |           0.0000 |
[32m[20221213 20:28:17 @agent_ppo2.py:185][0m |          -0.0374 |           0.0160 |           0.0000 |
[32m[20221213 20:28:17 @agent_ppo2.py:185][0m |          -0.0400 |           0.0158 |           0.0000 |
[32m[20221213 20:28:17 @agent_ppo2.py:130][0m Policy update time: 3.07 s
[32m[20221213 20:28:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.82
[32m[20221213 20:28:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.00
[32m[20221213 20:28:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.00
[32m[20221213 20:28:18 @agent_ppo2.py:143][0m Total time:      19.86 min
[32m[20221213 20:28:18 @agent_ppo2.py:145][0m 598016 total steps have happened
[32m[20221213 20:28:18 @agent_ppo2.py:121][0m #------------------------ Iteration 292 --------------------------#
[32m[20221213 20:28:18 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:28:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:19 @agent_ppo2.py:185][0m |           0.0031 |           0.0270 |           0.0000 |
[32m[20221213 20:28:19 @agent_ppo2.py:185][0m |          -0.0107 |           0.0200 |           0.0000 |
[32m[20221213 20:28:19 @agent_ppo2.py:185][0m |          -0.0154 |           0.0184 |           0.0000 |
[32m[20221213 20:28:20 @agent_ppo2.py:185][0m |          -0.0168 |           0.0179 |           0.0000 |
[32m[20221213 20:28:20 @agent_ppo2.py:185][0m |          -0.0200 |           0.0168 |           0.0000 |
[32m[20221213 20:28:20 @agent_ppo2.py:185][0m |          -0.0221 |           0.0167 |           0.0000 |
[32m[20221213 20:28:20 @agent_ppo2.py:185][0m |          -0.0223 |           0.0161 |           0.0000 |
[32m[20221213 20:28:21 @agent_ppo2.py:185][0m |          -0.0227 |           0.0160 |           0.0000 |
[32m[20221213 20:28:21 @agent_ppo2.py:185][0m |          -0.0247 |           0.0157 |           0.0000 |
[32m[20221213 20:28:21 @agent_ppo2.py:185][0m |          -0.0254 |           0.0156 |           0.0000 |
[32m[20221213 20:28:21 @agent_ppo2.py:130][0m Policy update time: 2.86 s
[32m[20221213 20:28:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.69
[32m[20221213 20:28:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.74
[32m[20221213 20:28:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.63
[32m[20221213 20:28:22 @agent_ppo2.py:143][0m Total time:      19.92 min
[32m[20221213 20:28:22 @agent_ppo2.py:145][0m 600064 total steps have happened
[32m[20221213 20:28:22 @agent_ppo2.py:121][0m #------------------------ Iteration 293 --------------------------#
[32m[20221213 20:28:22 @agent_ppo2.py:127][0m Sampling time: 0.53 s by 10 slaves
[32m[20221213 20:28:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:23 @agent_ppo2.py:185][0m |          -0.0000 |           0.0209 |           0.0000 |
[32m[20221213 20:28:23 @agent_ppo2.py:185][0m |          -0.0152 |           0.0169 |           0.0000 |
[32m[20221213 20:28:23 @agent_ppo2.py:185][0m |          -0.0234 |           0.0157 |           0.0000 |
[32m[20221213 20:28:24 @agent_ppo2.py:185][0m |          -0.0259 |           0.0150 |           0.0000 |
[32m[20221213 20:28:24 @agent_ppo2.py:185][0m |          -0.0281 |           0.0148 |           0.0000 |
[32m[20221213 20:28:24 @agent_ppo2.py:185][0m |          -0.0297 |           0.0145 |           0.0000 |
[32m[20221213 20:28:24 @agent_ppo2.py:185][0m |          -0.0318 |           0.0144 |           0.0000 |
[32m[20221213 20:28:25 @agent_ppo2.py:185][0m |          -0.0337 |           0.0141 |           0.0000 |
[32m[20221213 20:28:25 @agent_ppo2.py:185][0m |          -0.0337 |           0.0139 |           0.0000 |
[32m[20221213 20:28:25 @agent_ppo2.py:185][0m |          -0.0346 |           0.0137 |           0.0000 |
[32m[20221213 20:28:25 @agent_ppo2.py:130][0m Policy update time: 2.88 s
[32m[20221213 20:28:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.65
[32m[20221213 20:28:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.30
[32m[20221213 20:28:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.97
[32m[20221213 20:28:26 @agent_ppo2.py:143][0m Total time:      19.99 min
[32m[20221213 20:28:26 @agent_ppo2.py:145][0m 602112 total steps have happened
[32m[20221213 20:28:26 @agent_ppo2.py:121][0m #------------------------ Iteration 294 --------------------------#
[32m[20221213 20:28:26 @agent_ppo2.py:127][0m Sampling time: 0.55 s by 10 slaves
[32m[20221213 20:28:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:27 @agent_ppo2.py:185][0m |           0.0022 |           0.0747 |           0.0000 |
[32m[20221213 20:28:27 @agent_ppo2.py:185][0m |          -0.0095 |           0.0341 |           0.0000 |
[32m[20221213 20:28:27 @agent_ppo2.py:185][0m |          -0.0133 |           0.0282 |           0.0000 |
[32m[20221213 20:28:28 @agent_ppo2.py:185][0m |          -0.0167 |           0.0255 |           0.0000 |
[32m[20221213 20:28:28 @agent_ppo2.py:185][0m |          -0.0177 |           0.0243 |           0.0000 |
[32m[20221213 20:28:28 @agent_ppo2.py:185][0m |          -0.0186 |           0.0235 |           0.0000 |
[32m[20221213 20:28:28 @agent_ppo2.py:185][0m |          -0.0199 |           0.0228 |           0.0000 |
[32m[20221213 20:28:29 @agent_ppo2.py:185][0m |          -0.0190 |           0.0221 |           0.0000 |
[32m[20221213 20:28:29 @agent_ppo2.py:185][0m |          -0.0250 |           0.0218 |           0.0000 |
[32m[20221213 20:28:29 @agent_ppo2.py:185][0m |          -0.0212 |           0.0216 |           0.0000 |
[32m[20221213 20:28:29 @agent_ppo2.py:130][0m Policy update time: 2.97 s
[32m[20221213 20:28:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.00
[32m[20221213 20:28:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.07
[32m[20221213 20:28:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.89
[32m[20221213 20:28:30 @agent_ppo2.py:143][0m Total time:      20.06 min
[32m[20221213 20:28:30 @agent_ppo2.py:145][0m 604160 total steps have happened
[32m[20221213 20:28:30 @agent_ppo2.py:121][0m #------------------------ Iteration 295 --------------------------#
[32m[20221213 20:28:30 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:28:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:31 @agent_ppo2.py:185][0m |           0.0039 |           0.0332 |           0.0000 |
[32m[20221213 20:28:31 @agent_ppo2.py:185][0m |          -0.0175 |           0.0218 |           0.0000 |
[32m[20221213 20:28:31 @agent_ppo2.py:185][0m |          -0.0209 |           0.0210 |           0.0000 |
[32m[20221213 20:28:32 @agent_ppo2.py:185][0m |          -0.0252 |           0.0203 |           0.0000 |
[32m[20221213 20:28:32 @agent_ppo2.py:185][0m |          -0.0273 |           0.0198 |           0.0000 |
[32m[20221213 20:28:32 @agent_ppo2.py:185][0m |          -0.0295 |           0.0195 |           0.0000 |
[32m[20221213 20:28:32 @agent_ppo2.py:185][0m |          -0.0300 |           0.0190 |           0.0000 |
[32m[20221213 20:28:33 @agent_ppo2.py:185][0m |          -0.0311 |           0.0187 |           0.0000 |
[32m[20221213 20:28:33 @agent_ppo2.py:185][0m |          -0.0304 |           0.0185 |           0.0000 |
[32m[20221213 20:28:33 @agent_ppo2.py:185][0m |          -0.0333 |           0.0183 |           0.0000 |
[32m[20221213 20:28:33 @agent_ppo2.py:130][0m Policy update time: 2.84 s
[32m[20221213 20:28:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.84
[32m[20221213 20:28:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.29
[32m[20221213 20:28:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.87
[32m[20221213 20:28:34 @agent_ppo2.py:143][0m Total time:      20.12 min
[32m[20221213 20:28:34 @agent_ppo2.py:145][0m 606208 total steps have happened
[32m[20221213 20:28:34 @agent_ppo2.py:121][0m #------------------------ Iteration 296 --------------------------#
[32m[20221213 20:28:34 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:28:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:35 @agent_ppo2.py:185][0m |           0.0018 |           0.0143 |           0.0000 |
[32m[20221213 20:28:35 @agent_ppo2.py:185][0m |          -0.0114 |           0.0127 |           0.0000 |
[32m[20221213 20:28:35 @agent_ppo2.py:185][0m |          -0.0173 |           0.0121 |           0.0000 |
[32m[20221213 20:28:35 @agent_ppo2.py:185][0m |          -0.0231 |           0.0117 |           0.0000 |
[32m[20221213 20:28:36 @agent_ppo2.py:185][0m |          -0.0311 |           0.0115 |           0.0000 |
[32m[20221213 20:28:36 @agent_ppo2.py:185][0m |          -0.0270 |           0.0113 |           0.0000 |
[32m[20221213 20:28:36 @agent_ppo2.py:185][0m |          -0.0301 |           0.0110 |           0.0000 |
[32m[20221213 20:28:37 @agent_ppo2.py:185][0m |          -0.0306 |           0.0108 |           0.0000 |
[32m[20221213 20:28:37 @agent_ppo2.py:185][0m |          -0.0305 |           0.0107 |           0.0000 |
[32m[20221213 20:28:37 @agent_ppo2.py:185][0m |          -0.0322 |           0.0106 |           0.0000 |
[32m[20221213 20:28:37 @agent_ppo2.py:130][0m Policy update time: 2.82 s
[32m[20221213 20:28:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.55
[32m[20221213 20:28:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.35
[32m[20221213 20:28:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.25
[32m[20221213 20:28:38 @agent_ppo2.py:143][0m Total time:      20.19 min
[32m[20221213 20:28:38 @agent_ppo2.py:145][0m 608256 total steps have happened
[32m[20221213 20:28:38 @agent_ppo2.py:121][0m #------------------------ Iteration 297 --------------------------#
[32m[20221213 20:28:38 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:28:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:39 @agent_ppo2.py:185][0m |          -0.0013 |           0.0266 |           0.0000 |
[32m[20221213 20:28:39 @agent_ppo2.py:185][0m |          -0.0098 |           0.0157 |           0.0000 |
[32m[20221213 20:28:39 @agent_ppo2.py:185][0m |          -0.0142 |           0.0149 |           0.0000 |
[32m[20221213 20:28:39 @agent_ppo2.py:185][0m |          -0.0169 |           0.0145 |           0.0000 |
[32m[20221213 20:28:40 @agent_ppo2.py:185][0m |          -0.0182 |           0.0140 |           0.0000 |
[32m[20221213 20:28:40 @agent_ppo2.py:185][0m |          -0.0197 |           0.0137 |           0.0000 |
[32m[20221213 20:28:40 @agent_ppo2.py:185][0m |          -0.0205 |           0.0137 |           0.0000 |
[32m[20221213 20:28:40 @agent_ppo2.py:185][0m |          -0.0207 |           0.0137 |           0.0000 |
[32m[20221213 20:28:41 @agent_ppo2.py:185][0m |          -0.0218 |           0.0137 |           0.0000 |
[32m[20221213 20:28:41 @agent_ppo2.py:185][0m |          -0.0225 |           0.0134 |           0.0000 |
[32m[20221213 20:28:41 @agent_ppo2.py:130][0m Policy update time: 2.90 s
[32m[20221213 20:28:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.89
[32m[20221213 20:28:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.67
[32m[20221213 20:28:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.44
[32m[20221213 20:28:41 @agent_ppo2.py:143][0m Total time:      20.25 min
[32m[20221213 20:28:41 @agent_ppo2.py:145][0m 610304 total steps have happened
[32m[20221213 20:28:41 @agent_ppo2.py:121][0m #------------------------ Iteration 298 --------------------------#
[32m[20221213 20:28:42 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:28:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:42 @agent_ppo2.py:185][0m |           0.0029 |           0.0129 |           0.0000 |
[32m[20221213 20:28:43 @agent_ppo2.py:185][0m |          -0.0150 |           0.0120 |           0.0000 |
[32m[20221213 20:28:43 @agent_ppo2.py:185][0m |          -0.0209 |           0.0118 |           0.0000 |
[32m[20221213 20:28:43 @agent_ppo2.py:185][0m |          -0.0249 |           0.0115 |           0.0000 |
[32m[20221213 20:28:44 @agent_ppo2.py:185][0m |          -0.0279 |           0.0115 |           0.0000 |
[32m[20221213 20:28:44 @agent_ppo2.py:185][0m |          -0.0289 |           0.0113 |           0.0000 |
[32m[20221213 20:28:44 @agent_ppo2.py:185][0m |          -0.0318 |           0.0112 |           0.0000 |
[32m[20221213 20:28:44 @agent_ppo2.py:185][0m |          -0.0333 |           0.0112 |           0.0000 |
[32m[20221213 20:28:45 @agent_ppo2.py:185][0m |          -0.0343 |           0.0112 |           0.0000 |
[32m[20221213 20:28:45 @agent_ppo2.py:185][0m |          -0.0368 |           0.0110 |           0.0000 |
[32m[20221213 20:28:45 @agent_ppo2.py:130][0m Policy update time: 2.85 s
[32m[20221213 20:28:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.35
[32m[20221213 20:28:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.11
[32m[20221213 20:28:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.25
[32m[20221213 20:28:45 @agent_ppo2.py:143][0m Total time:      20.32 min
[32m[20221213 20:28:45 @agent_ppo2.py:145][0m 612352 total steps have happened
[32m[20221213 20:28:45 @agent_ppo2.py:121][0m #------------------------ Iteration 299 --------------------------#
[32m[20221213 20:28:46 @agent_ppo2.py:127][0m Sampling time: 0.51 s by 10 slaves
[32m[20221213 20:28:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:46 @agent_ppo2.py:185][0m |           0.0028 |           0.0153 |           0.0000 |
[32m[20221213 20:28:47 @agent_ppo2.py:185][0m |          -0.0145 |           0.0131 |           0.0000 |
[32m[20221213 20:28:47 @agent_ppo2.py:185][0m |          -0.0204 |           0.0128 |           0.0000 |
[32m[20221213 20:28:47 @agent_ppo2.py:185][0m |          -0.0200 |           0.0124 |           0.0000 |
[32m[20221213 20:28:47 @agent_ppo2.py:185][0m |          -0.0248 |           0.0122 |           0.0000 |
[32m[20221213 20:28:48 @agent_ppo2.py:185][0m |          -0.0242 |           0.0118 |           0.0000 |
[32m[20221213 20:28:48 @agent_ppo2.py:185][0m |          -0.0277 |           0.0118 |           0.0000 |
[32m[20221213 20:28:48 @agent_ppo2.py:185][0m |          -0.0266 |           0.0117 |           0.0000 |
[32m[20221213 20:28:48 @agent_ppo2.py:185][0m |          -0.0299 |           0.0117 |           0.0000 |
[32m[20221213 20:28:49 @agent_ppo2.py:185][0m |          -0.0298 |           0.0117 |           0.0000 |
[32m[20221213 20:28:49 @agent_ppo2.py:130][0m Policy update time: 2.83 s
[32m[20221213 20:28:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.17
[32m[20221213 20:28:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.99
[32m[20221213 20:28:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.78
[32m[20221213 20:28:49 @agent_ppo2.py:143][0m Total time:      20.38 min
[32m[20221213 20:28:49 @agent_ppo2.py:145][0m 614400 total steps have happened
[32m[20221213 20:28:49 @agent_ppo2.py:121][0m #------------------------ Iteration 300 --------------------------#
[32m[20221213 20:28:50 @agent_ppo2.py:127][0m Sampling time: 0.49 s by 10 slaves
[32m[20221213 20:28:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:50 @agent_ppo2.py:185][0m |           0.0007 |           0.0176 |           0.0000 |
[32m[20221213 20:28:50 @agent_ppo2.py:185][0m |          -0.0138 |           0.0155 |           0.0000 |
[32m[20221213 20:28:51 @agent_ppo2.py:185][0m |          -0.0197 |           0.0150 |           0.0000 |
[32m[20221213 20:28:51 @agent_ppo2.py:185][0m |          -0.0231 |           0.0142 |           0.0000 |
[32m[20221213 20:28:51 @agent_ppo2.py:185][0m |          -0.0248 |           0.0140 |           0.0000 |
[32m[20221213 20:28:51 @agent_ppo2.py:185][0m |          -0.0268 |           0.0136 |           0.0000 |
[32m[20221213 20:28:52 @agent_ppo2.py:185][0m |          -0.0279 |           0.0134 |           0.0000 |
[32m[20221213 20:28:52 @agent_ppo2.py:185][0m |          -0.0296 |           0.0131 |           0.0000 |
[32m[20221213 20:28:52 @agent_ppo2.py:185][0m |          -0.0306 |           0.0132 |           0.0000 |
[32m[20221213 20:28:52 @agent_ppo2.py:185][0m |          -0.0303 |           0.0132 |           0.0000 |
[32m[20221213 20:28:52 @agent_ppo2.py:130][0m Policy update time: 2.73 s
[32m[20221213 20:28:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.18
[32m[20221213 20:28:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.22
[32m[20221213 20:28:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.55
[32m[20221213 20:28:53 @agent_ppo2.py:143][0m Total time:      20.44 min
[32m[20221213 20:28:53 @agent_ppo2.py:145][0m 616448 total steps have happened
[32m[20221213 20:28:53 @agent_ppo2.py:121][0m #------------------------ Iteration 301 --------------------------#
[32m[20221213 20:28:53 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:28:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:54 @agent_ppo2.py:185][0m |           0.0022 |           0.0106 |           0.0000 |
[32m[20221213 20:28:54 @agent_ppo2.py:185][0m |          -0.0189 |           0.0098 |           0.0000 |
[32m[20221213 20:28:54 @agent_ppo2.py:185][0m |          -0.0245 |           0.0094 |           0.0000 |
[32m[20221213 20:28:55 @agent_ppo2.py:185][0m |          -0.0287 |           0.0093 |           0.0000 |
[32m[20221213 20:28:55 @agent_ppo2.py:185][0m |          -0.0323 |           0.0092 |           0.0000 |
[32m[20221213 20:28:55 @agent_ppo2.py:185][0m |          -0.0326 |           0.0090 |           0.0000 |
[32m[20221213 20:28:55 @agent_ppo2.py:185][0m |          -0.0354 |           0.0089 |           0.0000 |
[32m[20221213 20:28:56 @agent_ppo2.py:185][0m |          -0.0318 |           0.0088 |           0.0000 |
[32m[20221213 20:28:56 @agent_ppo2.py:185][0m |          -0.0364 |           0.0087 |           0.0000 |
[32m[20221213 20:28:56 @agent_ppo2.py:185][0m |          -0.0406 |           0.0086 |           0.0000 |
[32m[20221213 20:28:56 @agent_ppo2.py:130][0m Policy update time: 2.80 s
[32m[20221213 20:28:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.92
[32m[20221213 20:28:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.55
[32m[20221213 20:28:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.41
[32m[20221213 20:28:57 @agent_ppo2.py:143][0m Total time:      20.51 min
[32m[20221213 20:28:57 @agent_ppo2.py:145][0m 618496 total steps have happened
[32m[20221213 20:28:57 @agent_ppo2.py:121][0m #------------------------ Iteration 302 --------------------------#
[32m[20221213 20:28:57 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:28:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:28:58 @agent_ppo2.py:185][0m |           0.0041 |           0.0599 |           0.0000 |
[32m[20221213 20:28:58 @agent_ppo2.py:185][0m |          -0.0092 |           0.0303 |           0.0000 |
[32m[20221213 20:28:58 @agent_ppo2.py:185][0m |          -0.0137 |           0.0261 |           0.0000 |
[32m[20221213 20:28:58 @agent_ppo2.py:185][0m |          -0.0157 |           0.0243 |           0.0000 |
[32m[20221213 20:28:59 @agent_ppo2.py:185][0m |          -0.0182 |           0.0233 |           0.0000 |
[32m[20221213 20:28:59 @agent_ppo2.py:185][0m |          -0.0190 |           0.0225 |           0.0000 |
[32m[20221213 20:28:59 @agent_ppo2.py:185][0m |          -0.0200 |           0.0220 |           0.0000 |
[32m[20221213 20:28:59 @agent_ppo2.py:185][0m |          -0.0202 |           0.0215 |           0.0000 |
[32m[20221213 20:29:00 @agent_ppo2.py:185][0m |          -0.0264 |           0.0219 |           0.0000 |
[32m[20221213 20:29:00 @agent_ppo2.py:185][0m |          -0.0214 |           0.0211 |           0.0000 |
[32m[20221213 20:29:00 @agent_ppo2.py:130][0m Policy update time: 2.85 s
[32m[20221213 20:29:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.53
[32m[20221213 20:29:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.54
[32m[20221213 20:29:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.39
[32m[20221213 20:29:01 @agent_ppo2.py:143][0m Total time:      20.57 min
[32m[20221213 20:29:01 @agent_ppo2.py:145][0m 620544 total steps have happened
[32m[20221213 20:29:01 @agent_ppo2.py:121][0m #------------------------ Iteration 303 --------------------------#
[32m[20221213 20:29:01 @agent_ppo2.py:127][0m Sampling time: 0.50 s by 10 slaves
[32m[20221213 20:29:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:29:01 @agent_ppo2.py:185][0m |           0.0037 |           0.0231 |           0.0000 |
[32m[20221213 20:29:02 @agent_ppo2.py:185][0m |          -0.0135 |           0.0163 |           0.0000 |
[32m[20221213 20:29:02 @agent_ppo2.py:185][0m |          -0.0192 |           0.0157 |           0.0000 |
[32m[20221213 20:29:02 @agent_ppo2.py:185][0m |          -0.0220 |           0.0152 |           0.0000 |
[32m[20221213 20:29:03 @agent_ppo2.py:185][0m |          -0.0236 |           0.0147 |           0.0000 |
[32m[20221213 20:29:03 @agent_ppo2.py:185][0m |          -0.0259 |           0.0144 |           0.0000 |
[32m[20221213 20:29:03 @agent_ppo2.py:185][0m |          -0.0260 |           0.0142 |           0.0000 |
[32m[20221213 20:29:03 @agent_ppo2.py:185][0m |          -0.0265 |           0.0138 |           0.0000 |
[32m[20221213 20:29:04 @agent_ppo2.py:185][0m |          -0.0281 |           0.0137 |           0.0000 |
[32m[20221213 20:29:04 @agent_ppo2.py:185][0m |          -0.0299 |           0.0134 |           0.0000 |
[32m[20221213 20:29:04 @agent_ppo2.py:130][0m Policy update time: 2.83 s
[32m[20221213 20:29:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.39
[32m[20221213 20:29:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.08
[32m[20221213 20:29:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.33
[32m[20221213 20:29:04 @agent_ppo2.py:143][0m Total time:      20.63 min
[32m[20221213 20:29:04 @agent_ppo2.py:145][0m 622592 total steps have happened
