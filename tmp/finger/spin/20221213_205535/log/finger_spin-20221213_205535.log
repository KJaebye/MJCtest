[32m[20221213 20:55:35 @logger.py:105][0m Log file set to ./tmp/finger/spin/20221213_205535/log/finger_spin-20221213_205535.log
[32m[20221213 20:55:35 @agent_ppo2.py:121][0m #------------------------ Iteration 0 --------------------------#
[32m[20221213 20:55:35 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:55:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:35 @agent_ppo2.py:185][0m |           0.0022 |           0.2652 |          11.6768 |
[32m[20221213 20:55:35 @agent_ppo2.py:185][0m |           0.0012 |           0.2526 |          11.6789 |
[32m[20221213 20:55:35 @agent_ppo2.py:185][0m |           0.0006 |           0.2450 |          11.6805 |
[32m[20221213 20:55:35 @agent_ppo2.py:185][0m |           0.0001 |           0.2407 |          11.6830 |
[32m[20221213 20:55:35 @agent_ppo2.py:185][0m |          -0.0007 |           0.2370 |          11.6834 |
[32m[20221213 20:55:35 @agent_ppo2.py:185][0m |          -0.0028 |           0.2346 |          11.6831 |
[32m[20221213 20:55:36 @agent_ppo2.py:185][0m |          -0.0179 |           0.2626 |          11.6846 |
[32m[20221213 20:55:36 @agent_ppo2.py:185][0m |          -0.0025 |           0.2303 |          11.6850 |
[32m[20221213 20:55:36 @agent_ppo2.py:185][0m |          -0.0072 |           0.2305 |          11.6853 |
[32m[20221213 20:55:36 @agent_ppo2.py:185][0m |          -0.0051 |           0.2288 |          11.6863 |
[32m[20221213 20:55:36 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:55:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.60
[32m[20221213 20:55:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 12.00
[32m[20221213 20:55:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 20:55:36 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 0.00
[32m[20221213 20:55:36 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 0.00
[32m[20221213 20:55:36 @agent_ppo2.py:143][0m Total time:       0.02 min
[32m[20221213 20:55:36 @agent_ppo2.py:145][0m 2048 total steps have happened
[32m[20221213 20:55:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1 --------------------------#
[32m[20221213 20:55:36 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:55:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:36 @agent_ppo2.py:185][0m |          -0.0024 |           0.0670 |          11.8208 |
[32m[20221213 20:55:36 @agent_ppo2.py:185][0m |          -0.0012 |           0.0645 |          11.8233 |
[32m[20221213 20:55:36 @agent_ppo2.py:185][0m |          -0.0064 |           0.0646 |          11.8268 |
[32m[20221213 20:55:37 @agent_ppo2.py:185][0m |          -0.0144 |           0.0695 |          11.8294 |
[32m[20221213 20:55:37 @agent_ppo2.py:185][0m |          -0.0020 |           0.0614 |          11.8301 |
[32m[20221213 20:55:37 @agent_ppo2.py:185][0m |          -0.0023 |           0.0606 |          11.8319 |
[32m[20221213 20:55:37 @agent_ppo2.py:185][0m |          -0.0024 |           0.0599 |          11.8334 |
[32m[20221213 20:55:37 @agent_ppo2.py:185][0m |          -0.0015 |           0.0593 |          11.8362 |
[32m[20221213 20:55:37 @agent_ppo2.py:185][0m |          -0.0041 |           0.0585 |          11.8383 |
[32m[20221213 20:55:37 @agent_ppo2.py:185][0m |          -0.0089 |           0.0595 |          11.8398 |
[32m[20221213 20:55:37 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:55:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.40
[32m[20221213 20:55:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.00
[32m[20221213 20:55:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 20:55:37 @agent_ppo2.py:143][0m Total time:       0.04 min
[32m[20221213 20:55:37 @agent_ppo2.py:145][0m 4096 total steps have happened
[32m[20221213 20:55:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2 --------------------------#
[32m[20221213 20:55:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:38 @agent_ppo2.py:185][0m |           0.0008 |           0.0373 |          11.9264 |
[32m[20221213 20:55:38 @agent_ppo2.py:185][0m |           0.0017 |           0.0358 |          11.9273 |
[32m[20221213 20:55:38 @agent_ppo2.py:185][0m |           0.0004 |           0.0350 |          11.9272 |
[32m[20221213 20:55:38 @agent_ppo2.py:185][0m |          -0.0021 |           0.0344 |          11.9264 |
[32m[20221213 20:55:38 @agent_ppo2.py:185][0m |          -0.0018 |           0.0338 |          11.9257 |
[32m[20221213 20:55:38 @agent_ppo2.py:185][0m |          -0.0023 |           0.0335 |          11.9241 |
[32m[20221213 20:55:38 @agent_ppo2.py:185][0m |          -0.0030 |           0.0331 |          11.9237 |
[32m[20221213 20:55:38 @agent_ppo2.py:185][0m |          -0.0025 |           0.0327 |          11.9216 |
[32m[20221213 20:55:38 @agent_ppo2.py:185][0m |          -0.0030 |           0.0324 |          11.9200 |
[32m[20221213 20:55:38 @agent_ppo2.py:185][0m |          -0.0036 |           0.0321 |          11.9191 |
[32m[20221213 20:55:38 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:55:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.80
[32m[20221213 20:55:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 4.00
[32m[20221213 20:55:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 20:55:38 @agent_ppo2.py:143][0m Total time:       0.06 min
[32m[20221213 20:55:38 @agent_ppo2.py:145][0m 6144 total steps have happened
[32m[20221213 20:55:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3 --------------------------#
[32m[20221213 20:55:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:39 @agent_ppo2.py:185][0m |           0.0010 |           0.1943 |          11.8435 |
[32m[20221213 20:55:39 @agent_ppo2.py:185][0m |          -0.0012 |           0.1894 |          11.8423 |
[32m[20221213 20:55:39 @agent_ppo2.py:185][0m |          -0.0041 |           0.1885 |          11.8405 |
[32m[20221213 20:55:39 @agent_ppo2.py:185][0m |          -0.0008 |           0.1872 |          11.8390 |
[32m[20221213 20:55:39 @agent_ppo2.py:185][0m |          -0.0013 |           0.1873 |          11.8369 |
[32m[20221213 20:55:39 @agent_ppo2.py:185][0m |          -0.0026 |           0.1871 |          11.8357 |
[32m[20221213 20:55:39 @agent_ppo2.py:185][0m |          -0.0024 |           0.1865 |          11.8326 |
[32m[20221213 20:55:39 @agent_ppo2.py:185][0m |          -0.0031 |           0.1864 |          11.8323 |
[32m[20221213 20:55:39 @agent_ppo2.py:185][0m |          -0.0027 |           0.1862 |          11.8326 |
[32m[20221213 20:55:40 @agent_ppo2.py:185][0m |          -0.0033 |           0.1862 |          11.8316 |
[32m[20221213 20:55:40 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:55:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.40
[32m[20221213 20:55:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.00
[32m[20221213 20:55:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 20:55:40 @agent_ppo2.py:143][0m Total time:       0.08 min
[32m[20221213 20:55:40 @agent_ppo2.py:145][0m 8192 total steps have happened
[32m[20221213 20:55:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4 --------------------------#
[32m[20221213 20:55:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:40 @agent_ppo2.py:185][0m |           0.0018 |           0.3986 |          11.9043 |
[32m[20221213 20:55:40 @agent_ppo2.py:185][0m |          -0.0032 |           0.3936 |          11.9052 |
[32m[20221213 20:55:40 @agent_ppo2.py:185][0m |           0.0002 |           0.3889 |          11.9025 |
[32m[20221213 20:55:40 @agent_ppo2.py:185][0m |          -0.0004 |           0.3895 |          11.9028 |
[32m[20221213 20:55:40 @agent_ppo2.py:185][0m |          -0.0004 |           0.3893 |          11.9008 |
[32m[20221213 20:55:40 @agent_ppo2.py:185][0m |          -0.0007 |           0.3884 |          11.9004 |
[32m[20221213 20:55:41 @agent_ppo2.py:185][0m |          -0.0009 |           0.3875 |          11.8976 |
[32m[20221213 20:55:41 @agent_ppo2.py:185][0m |          -0.0011 |           0.3881 |          11.8985 |
[32m[20221213 20:55:41 @agent_ppo2.py:185][0m |          -0.0009 |           0.3882 |          11.8985 |
[32m[20221213 20:55:41 @agent_ppo2.py:185][0m |          -0.0013 |           0.3877 |          11.9009 |
[32m[20221213 20:55:41 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:55:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.40
[32m[20221213 20:55:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 21.00
[32m[20221213 20:55:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 20:55:41 @agent_ppo2.py:143][0m Total time:       0.10 min
[32m[20221213 20:55:41 @agent_ppo2.py:145][0m 10240 total steps have happened
[32m[20221213 20:55:41 @agent_ppo2.py:121][0m #------------------------ Iteration 5 --------------------------#
[32m[20221213 20:55:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:41 @agent_ppo2.py:185][0m |           0.0025 |           0.1786 |          11.9964 |
[32m[20221213 20:55:41 @agent_ppo2.py:185][0m |           0.0004 |           0.1749 |          11.9946 |
[32m[20221213 20:55:41 @agent_ppo2.py:185][0m |          -0.0011 |           0.1741 |          11.9918 |
[32m[20221213 20:55:42 @agent_ppo2.py:185][0m |          -0.0030 |           0.1742 |          11.9873 |
[32m[20221213 20:55:42 @agent_ppo2.py:185][0m |          -0.0046 |           0.1738 |          11.9843 |
[32m[20221213 20:55:42 @agent_ppo2.py:185][0m |          -0.0033 |           0.1736 |          11.9828 |
[32m[20221213 20:55:42 @agent_ppo2.py:185][0m |          -0.0031 |           0.1743 |          11.9790 |
[32m[20221213 20:55:42 @agent_ppo2.py:185][0m |          -0.0040 |           0.1736 |          11.9761 |
[32m[20221213 20:55:42 @agent_ppo2.py:185][0m |          -0.0048 |           0.1733 |          11.9738 |
[32m[20221213 20:55:42 @agent_ppo2.py:185][0m |          -0.0048 |           0.1733 |          11.9725 |
[32m[20221213 20:55:42 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:55:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.80
[32m[20221213 20:55:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.00
[32m[20221213 20:55:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 20:55:42 @agent_ppo2.py:143][0m Total time:       0.13 min
[32m[20221213 20:55:42 @agent_ppo2.py:145][0m 12288 total steps have happened
[32m[20221213 20:55:42 @agent_ppo2.py:121][0m #------------------------ Iteration 6 --------------------------#
[32m[20221213 20:55:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:43 @agent_ppo2.py:185][0m |          -0.0015 |           0.2726 |          11.8725 |
[32m[20221213 20:55:43 @agent_ppo2.py:185][0m |          -0.0015 |           0.2676 |          11.8691 |
[32m[20221213 20:55:43 @agent_ppo2.py:185][0m |          -0.0014 |           0.2675 |          11.8640 |
[32m[20221213 20:55:43 @agent_ppo2.py:185][0m |          -0.0062 |           0.2680 |          11.8582 |
[32m[20221213 20:55:43 @agent_ppo2.py:185][0m |          -0.0025 |           0.2679 |          11.8549 |
[32m[20221213 20:55:43 @agent_ppo2.py:185][0m |          -0.0146 |           0.2794 |          11.8523 |
[32m[20221213 20:55:43 @agent_ppo2.py:185][0m |          -0.0055 |           0.2675 |          11.8520 |
[32m[20221213 20:55:43 @agent_ppo2.py:185][0m |          -0.0062 |           0.2676 |          11.8521 |
[32m[20221213 20:55:43 @agent_ppo2.py:185][0m |          -0.0049 |           0.2668 |          11.8506 |
[32m[20221213 20:55:43 @agent_ppo2.py:185][0m |          -0.0047 |           0.2669 |          11.8526 |
[32m[20221213 20:55:43 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:55:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.80
[32m[20221213 20:55:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.00
[32m[20221213 20:55:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 20:55:43 @agent_ppo2.py:143][0m Total time:       0.15 min
[32m[20221213 20:55:43 @agent_ppo2.py:145][0m 14336 total steps have happened
[32m[20221213 20:55:43 @agent_ppo2.py:121][0m #------------------------ Iteration 7 --------------------------#
[32m[20221213 20:55:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:44 @agent_ppo2.py:185][0m |          -0.0142 |           0.5848 |          12.1224 |
[32m[20221213 20:55:44 @agent_ppo2.py:185][0m |          -0.0031 |           0.4962 |          12.1261 |
[32m[20221213 20:55:44 @agent_ppo2.py:185][0m |          -0.0034 |           0.4940 |          12.1278 |
[32m[20221213 20:55:44 @agent_ppo2.py:185][0m |          -0.0007 |           0.4897 |          12.1282 |
[32m[20221213 20:55:44 @agent_ppo2.py:185][0m |          -0.0029 |           0.4890 |          12.1305 |
[32m[20221213 20:55:44 @agent_ppo2.py:185][0m |           0.0001 |           0.4891 |          12.1315 |
[32m[20221213 20:55:45 @agent_ppo2.py:185][0m |          -0.0012 |           0.4896 |          12.1322 |
[32m[20221213 20:55:45 @agent_ppo2.py:185][0m |          -0.0084 |           0.4942 |          12.1317 |
[32m[20221213 20:55:45 @agent_ppo2.py:185][0m |          -0.0033 |           0.4893 |          12.1332 |
[32m[20221213 20:55:45 @agent_ppo2.py:185][0m |          -0.0019 |           0.4898 |          12.1344 |
[32m[20221213 20:55:45 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 20:55:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.80
[32m[20221213 20:55:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 18.00
[32m[20221213 20:55:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 20:55:45 @agent_ppo2.py:143][0m Total time:       0.17 min
[32m[20221213 20:55:45 @agent_ppo2.py:145][0m 16384 total steps have happened
[32m[20221213 20:55:45 @agent_ppo2.py:121][0m #------------------------ Iteration 8 --------------------------#
[32m[20221213 20:55:45 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:55:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:45 @agent_ppo2.py:185][0m |           0.0008 |           1.2220 |          12.0933 |
[32m[20221213 20:55:45 @agent_ppo2.py:185][0m |          -0.0056 |           1.2221 |          12.0926 |
[32m[20221213 20:55:45 @agent_ppo2.py:185][0m |          -0.0006 |           1.1917 |          12.0902 |
[32m[20221213 20:55:45 @agent_ppo2.py:185][0m |          -0.0048 |           1.1944 |          12.0907 |
[32m[20221213 20:55:46 @agent_ppo2.py:185][0m |          -0.0066 |           1.2010 |          12.0915 |
[32m[20221213 20:55:46 @agent_ppo2.py:185][0m |          -0.0023 |           1.1903 |          12.0921 |
[32m[20221213 20:55:46 @agent_ppo2.py:185][0m |          -0.0099 |           1.2137 |          12.0920 |
[32m[20221213 20:55:46 @agent_ppo2.py:185][0m |          -0.0030 |           1.1878 |          12.0939 |
[32m[20221213 20:55:46 @agent_ppo2.py:185][0m |          -0.0031 |           1.1875 |          12.0957 |
[32m[20221213 20:55:46 @agent_ppo2.py:185][0m |          -0.0019 |           1.1906 |          12.0964 |
[32m[20221213 20:55:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:55:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 19.40
[32m[20221213 20:55:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 35.00
[32m[20221213 20:55:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 20:55:46 @agent_ppo2.py:143][0m Total time:       0.19 min
[32m[20221213 20:55:46 @agent_ppo2.py:145][0m 18432 total steps have happened
[32m[20221213 20:55:46 @agent_ppo2.py:121][0m #------------------------ Iteration 9 --------------------------#
[32m[20221213 20:55:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:46 @agent_ppo2.py:185][0m |           0.0011 |           0.7196 |          12.2438 |
[32m[20221213 20:55:46 @agent_ppo2.py:185][0m |          -0.0012 |           0.6938 |          12.2411 |
[32m[20221213 20:55:47 @agent_ppo2.py:185][0m |           0.0045 |           0.6932 |          12.2358 |
[32m[20221213 20:55:47 @agent_ppo2.py:185][0m |           0.0002 |           0.6917 |          12.2321 |
[32m[20221213 20:55:47 @agent_ppo2.py:185][0m |          -0.0026 |           0.6940 |          12.2274 |
[32m[20221213 20:55:47 @agent_ppo2.py:185][0m |          -0.0031 |           0.6934 |          12.2248 |
[32m[20221213 20:55:47 @agent_ppo2.py:185][0m |          -0.0015 |           0.6939 |          12.2196 |
[32m[20221213 20:55:47 @agent_ppo2.py:185][0m |          -0.0018 |           0.6935 |          12.2185 |
[32m[20221213 20:55:47 @agent_ppo2.py:185][0m |          -0.0035 |           0.6943 |          12.2163 |
[32m[20221213 20:55:47 @agent_ppo2.py:185][0m |          -0.0070 |           0.7016 |          12.2153 |
[32m[20221213 20:55:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:55:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 16.00
[32m[20221213 20:55:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 28.00
[32m[20221213 20:55:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.00
[32m[20221213 20:55:47 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 1.00
[32m[20221213 20:55:47 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 1.00
[32m[20221213 20:55:47 @agent_ppo2.py:143][0m Total time:       0.21 min
[32m[20221213 20:55:47 @agent_ppo2.py:145][0m 20480 total steps have happened
[32m[20221213 20:55:47 @agent_ppo2.py:121][0m #------------------------ Iteration 10 --------------------------#
[32m[20221213 20:55:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:55:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:48 @agent_ppo2.py:185][0m |          -0.0107 |           0.9820 |          12.1964 |
[32m[20221213 20:55:48 @agent_ppo2.py:185][0m |          -0.0001 |           0.8929 |          12.1864 |
[32m[20221213 20:55:48 @agent_ppo2.py:185][0m |          -0.0019 |           0.8877 |          12.1729 |
[32m[20221213 20:55:48 @agent_ppo2.py:185][0m |          -0.0032 |           0.8863 |          12.1643 |
[32m[20221213 20:55:48 @agent_ppo2.py:185][0m |          -0.0010 |           0.8895 |          12.1595 |
[32m[20221213 20:55:48 @agent_ppo2.py:185][0m |          -0.0115 |           0.9139 |          12.1555 |
[32m[20221213 20:55:48 @agent_ppo2.py:185][0m |          -0.0069 |           0.8879 |          12.1567 |
[32m[20221213 20:55:48 @agent_ppo2.py:185][0m |          -0.0059 |           0.8878 |          12.1551 |
[32m[20221213 20:55:48 @agent_ppo2.py:185][0m |          -0.0075 |           0.8886 |          12.1552 |
[32m[20221213 20:55:48 @agent_ppo2.py:185][0m |          -0.0070 |           0.8856 |          12.1544 |
[32m[20221213 20:55:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:55:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 17.40
[32m[20221213 20:55:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 24.00
[32m[20221213 20:55:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 32.00
[32m[20221213 20:55:48 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 32.00
[32m[20221213 20:55:48 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 32.00
[32m[20221213 20:55:48 @agent_ppo2.py:143][0m Total time:       0.23 min
[32m[20221213 20:55:48 @agent_ppo2.py:145][0m 22528 total steps have happened
[32m[20221213 20:55:48 @agent_ppo2.py:121][0m #------------------------ Iteration 11 --------------------------#
[32m[20221213 20:55:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:49 @agent_ppo2.py:185][0m |          -0.0010 |           1.2201 |          12.1233 |
[32m[20221213 20:55:49 @agent_ppo2.py:185][0m |          -0.0056 |           1.2447 |          12.1257 |
[32m[20221213 20:55:49 @agent_ppo2.py:185][0m |          -0.0027 |           1.1917 |          12.1237 |
[32m[20221213 20:55:49 @agent_ppo2.py:185][0m |          -0.0092 |           1.2349 |          12.1234 |
[32m[20221213 20:55:49 @agent_ppo2.py:185][0m |          -0.0085 |           1.1992 |          12.1242 |
[32m[20221213 20:55:49 @agent_ppo2.py:185][0m |           0.0001 |           1.1780 |          12.1235 |
[32m[20221213 20:55:49 @agent_ppo2.py:185][0m |          -0.0015 |           1.1797 |          12.1241 |
[32m[20221213 20:55:49 @agent_ppo2.py:185][0m |          -0.0038 |           1.1785 |          12.1238 |
[32m[20221213 20:55:49 @agent_ppo2.py:185][0m |          -0.0058 |           1.1772 |          12.1215 |
[32m[20221213 20:55:50 @agent_ppo2.py:185][0m |          -0.0045 |           1.1780 |          12.1225 |
[32m[20221213 20:55:50 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:55:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 25.20
[32m[20221213 20:55:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 30.00
[32m[20221213 20:55:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.00
[32m[20221213 20:55:50 @agent_ppo2.py:143][0m Total time:       0.25 min
[32m[20221213 20:55:50 @agent_ppo2.py:145][0m 24576 total steps have happened
[32m[20221213 20:55:50 @agent_ppo2.py:121][0m #------------------------ Iteration 12 --------------------------#
[32m[20221213 20:55:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:50 @agent_ppo2.py:185][0m |           0.0047 |           1.0381 |          12.2816 |
[32m[20221213 20:55:50 @agent_ppo2.py:185][0m |          -0.0082 |           0.9834 |          12.2764 |
[32m[20221213 20:55:50 @agent_ppo2.py:185][0m |          -0.0011 |           0.9718 |          12.2701 |
[32m[20221213 20:55:50 @agent_ppo2.py:185][0m |          -0.0073 |           0.9815 |          12.2668 |
[32m[20221213 20:55:50 @agent_ppo2.py:185][0m |          -0.0074 |           0.9939 |          12.2618 |
[32m[20221213 20:55:50 @agent_ppo2.py:185][0m |          -0.0045 |           0.9664 |          12.2593 |
[32m[20221213 20:55:50 @agent_ppo2.py:185][0m |          -0.0037 |           0.9662 |          12.2561 |
[32m[20221213 20:55:51 @agent_ppo2.py:185][0m |          -0.0029 |           0.9688 |          12.2505 |
[32m[20221213 20:55:51 @agent_ppo2.py:185][0m |          -0.0044 |           0.9681 |          12.2506 |
[32m[20221213 20:55:51 @agent_ppo2.py:185][0m |          -0.0122 |           0.9741 |          12.2480 |
[32m[20221213 20:55:51 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:55:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 20.00
[32m[20221213 20:55:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 41.00
[32m[20221213 20:55:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 15.00
[32m[20221213 20:55:51 @agent_ppo2.py:143][0m Total time:       0.27 min
[32m[20221213 20:55:51 @agent_ppo2.py:145][0m 26624 total steps have happened
[32m[20221213 20:55:51 @agent_ppo2.py:121][0m #------------------------ Iteration 13 --------------------------#
[32m[20221213 20:55:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:55:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:51 @agent_ppo2.py:185][0m |           0.0022 |           1.7525 |          12.2862 |
[32m[20221213 20:55:51 @agent_ppo2.py:185][0m |          -0.0219 |           1.9227 |          12.2853 |
[32m[20221213 20:55:51 @agent_ppo2.py:185][0m |          -0.0003 |           1.6980 |          12.2817 |
[32m[20221213 20:55:51 @agent_ppo2.py:185][0m |          -0.0008 |           1.6899 |          12.2787 |
[32m[20221213 20:55:52 @agent_ppo2.py:185][0m |          -0.0014 |           1.6747 |          12.2778 |
[32m[20221213 20:55:52 @agent_ppo2.py:185][0m |          -0.0058 |           1.6772 |          12.2780 |
[32m[20221213 20:55:52 @agent_ppo2.py:185][0m |          -0.0158 |           1.7718 |          12.2778 |
[32m[20221213 20:55:52 @agent_ppo2.py:185][0m |          -0.0011 |           1.6591 |          12.2820 |
[32m[20221213 20:55:52 @agent_ppo2.py:185][0m |          -0.0047 |           1.6529 |          12.2824 |
[32m[20221213 20:55:52 @agent_ppo2.py:185][0m |          -0.0044 |           1.6471 |          12.2827 |
[32m[20221213 20:55:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:55:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 33.20
[32m[20221213 20:55:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 46.00
[32m[20221213 20:55:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 65.00
[32m[20221213 20:55:52 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 65.00
[32m[20221213 20:55:52 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 65.00
[32m[20221213 20:55:52 @agent_ppo2.py:143][0m Total time:       0.29 min
[32m[20221213 20:55:52 @agent_ppo2.py:145][0m 28672 total steps have happened
[32m[20221213 20:55:52 @agent_ppo2.py:121][0m #------------------------ Iteration 14 --------------------------#
[32m[20221213 20:55:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:52 @agent_ppo2.py:185][0m |           0.0018 |           2.1991 |          12.2433 |
[32m[20221213 20:55:52 @agent_ppo2.py:185][0m |           0.0023 |           2.1240 |          12.2417 |
[32m[20221213 20:55:53 @agent_ppo2.py:185][0m |          -0.0021 |           2.1281 |          12.2372 |
[32m[20221213 20:55:53 @agent_ppo2.py:185][0m |          -0.0000 |           2.1131 |          12.2344 |
[32m[20221213 20:55:53 @agent_ppo2.py:185][0m |          -0.0021 |           2.1107 |          12.2331 |
[32m[20221213 20:55:53 @agent_ppo2.py:185][0m |          -0.0066 |           2.1448 |          12.2299 |
[32m[20221213 20:55:53 @agent_ppo2.py:185][0m |          -0.0015 |           2.0991 |          12.2308 |
[32m[20221213 20:55:53 @agent_ppo2.py:185][0m |          -0.0088 |           2.1320 |          12.2304 |
[32m[20221213 20:55:53 @agent_ppo2.py:185][0m |          -0.0040 |           2.1089 |          12.2279 |
[32m[20221213 20:55:53 @agent_ppo2.py:185][0m |          -0.0011 |           2.0857 |          12.2299 |
[32m[20221213 20:55:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:55:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 41.20
[32m[20221213 20:55:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.00
[32m[20221213 20:55:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 77.00
[32m[20221213 20:55:53 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 77.00
[32m[20221213 20:55:53 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 77.00
[32m[20221213 20:55:53 @agent_ppo2.py:143][0m Total time:       0.31 min
[32m[20221213 20:55:53 @agent_ppo2.py:145][0m 30720 total steps have happened
[32m[20221213 20:55:53 @agent_ppo2.py:121][0m #------------------------ Iteration 15 --------------------------#
[32m[20221213 20:55:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:54 @agent_ppo2.py:185][0m |          -0.0131 |           2.4865 |          12.3525 |
[32m[20221213 20:55:54 @agent_ppo2.py:185][0m |          -0.0085 |           2.4210 |          12.3417 |
[32m[20221213 20:55:54 @agent_ppo2.py:185][0m |          -0.0060 |           2.3470 |          12.3387 |
[32m[20221213 20:55:54 @agent_ppo2.py:185][0m |           0.0005 |           2.3381 |          12.3417 |
[32m[20221213 20:55:54 @agent_ppo2.py:185][0m |          -0.0020 |           2.3133 |          12.3432 |
[32m[20221213 20:55:54 @agent_ppo2.py:185][0m |          -0.0083 |           2.4196 |          12.3444 |
[32m[20221213 20:55:54 @agent_ppo2.py:185][0m |          -0.0074 |           2.3461 |          12.3446 |
[32m[20221213 20:55:54 @agent_ppo2.py:185][0m |          -0.0062 |           2.3355 |          12.3449 |
[32m[20221213 20:55:54 @agent_ppo2.py:185][0m |          -0.0045 |           2.3157 |          12.3474 |
[32m[20221213 20:55:54 @agent_ppo2.py:185][0m |          -0.0089 |           2.3286 |          12.3494 |
[32m[20221213 20:55:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:55:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 45.80
[32m[20221213 20:55:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 20:55:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.00
[32m[20221213 20:55:54 @agent_ppo2.py:143][0m Total time:       0.33 min
[32m[20221213 20:55:54 @agent_ppo2.py:145][0m 32768 total steps have happened
[32m[20221213 20:55:54 @agent_ppo2.py:121][0m #------------------------ Iteration 16 --------------------------#
[32m[20221213 20:55:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:55 @agent_ppo2.py:185][0m |          -0.0013 |           2.1420 |          12.3663 |
[32m[20221213 20:55:55 @agent_ppo2.py:185][0m |          -0.0005 |           2.0943 |          12.3645 |
[32m[20221213 20:55:55 @agent_ppo2.py:185][0m |          -0.0023 |           2.0873 |          12.3574 |
[32m[20221213 20:55:55 @agent_ppo2.py:185][0m |          -0.0025 |           2.0824 |          12.3525 |
[32m[20221213 20:55:55 @agent_ppo2.py:185][0m |          -0.0115 |           2.1496 |          12.3546 |
[32m[20221213 20:55:55 @agent_ppo2.py:185][0m |           0.0011 |           2.1009 |          12.3508 |
[32m[20221213 20:55:55 @agent_ppo2.py:185][0m |          -0.0120 |           2.1152 |          12.3532 |
[32m[20221213 20:55:55 @agent_ppo2.py:185][0m |          -0.0095 |           2.1508 |          12.3553 |
[32m[20221213 20:55:55 @agent_ppo2.py:185][0m |          -0.0001 |           2.0730 |          12.3591 |
[32m[20221213 20:55:56 @agent_ppo2.py:185][0m |          -0.0049 |           2.0664 |          12.3624 |
[32m[20221213 20:55:56 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:55:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 38.60
[32m[20221213 20:55:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.00
[32m[20221213 20:55:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 63.00
[32m[20221213 20:55:56 @agent_ppo2.py:143][0m Total time:       0.35 min
[32m[20221213 20:55:56 @agent_ppo2.py:145][0m 34816 total steps have happened
[32m[20221213 20:55:56 @agent_ppo2.py:121][0m #------------------------ Iteration 17 --------------------------#
[32m[20221213 20:55:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:56 @agent_ppo2.py:185][0m |          -0.0130 |           1.9639 |          12.4046 |
[32m[20221213 20:55:56 @agent_ppo2.py:185][0m |          -0.0105 |           1.9319 |          12.3985 |
[32m[20221213 20:55:56 @agent_ppo2.py:185][0m |          -0.0045 |           1.8752 |          12.3916 |
[32m[20221213 20:55:56 @agent_ppo2.py:185][0m |          -0.0103 |           1.9159 |          12.3918 |
[32m[20221213 20:55:56 @agent_ppo2.py:185][0m |          -0.0004 |           1.8814 |          12.3888 |
[32m[20221213 20:55:56 @agent_ppo2.py:185][0m |          -0.0146 |           1.9054 |          12.3905 |
[32m[20221213 20:55:56 @agent_ppo2.py:185][0m |          -0.0111 |           1.9008 |          12.3937 |
[32m[20221213 20:55:57 @agent_ppo2.py:185][0m |           0.0001 |           1.8762 |          12.3917 |
[32m[20221213 20:55:57 @agent_ppo2.py:185][0m |          -0.0065 |           1.8905 |          12.3970 |
[32m[20221213 20:55:57 @agent_ppo2.py:185][0m |          -0.0059 |           1.8725 |          12.3966 |
[32m[20221213 20:55:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:55:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 40.40
[32m[20221213 20:55:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.00
[32m[20221213 20:55:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 95.00
[32m[20221213 20:55:57 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 95.00
[32m[20221213 20:55:57 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 95.00
[32m[20221213 20:55:57 @agent_ppo2.py:143][0m Total time:       0.37 min
[32m[20221213 20:55:57 @agent_ppo2.py:145][0m 36864 total steps have happened
[32m[20221213 20:55:57 @agent_ppo2.py:121][0m #------------------------ Iteration 18 --------------------------#
[32m[20221213 20:55:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:57 @agent_ppo2.py:185][0m |           0.0008 |           3.1530 |          12.5974 |
[32m[20221213 20:55:57 @agent_ppo2.py:185][0m |          -0.0019 |           3.0741 |          12.5961 |
[32m[20221213 20:55:57 @agent_ppo2.py:185][0m |          -0.0030 |           3.0644 |          12.5938 |
[32m[20221213 20:55:57 @agent_ppo2.py:185][0m |          -0.0021 |           3.0430 |          12.5923 |
[32m[20221213 20:55:57 @agent_ppo2.py:185][0m |          -0.0024 |           3.0325 |          12.5898 |
[32m[20221213 20:55:58 @agent_ppo2.py:185][0m |          -0.0020 |           3.0370 |          12.5905 |
[32m[20221213 20:55:58 @agent_ppo2.py:185][0m |          -0.0095 |           3.0811 |          12.5891 |
[32m[20221213 20:55:58 @agent_ppo2.py:185][0m |          -0.0055 |           3.0311 |          12.5909 |
[32m[20221213 20:55:58 @agent_ppo2.py:185][0m |          -0.0024 |           3.0176 |          12.5863 |
[32m[20221213 20:55:58 @agent_ppo2.py:185][0m |          -0.0035 |           3.0177 |          12.5884 |
[32m[20221213 20:55:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:55:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 62.80
[32m[20221213 20:55:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 96.00
[32m[20221213 20:55:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 109.00
[32m[20221213 20:55:58 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 109.00
[32m[20221213 20:55:58 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 109.00
[32m[20221213 20:55:58 @agent_ppo2.py:143][0m Total time:       0.39 min
[32m[20221213 20:55:58 @agent_ppo2.py:145][0m 38912 total steps have happened
[32m[20221213 20:55:58 @agent_ppo2.py:121][0m #------------------------ Iteration 19 --------------------------#
[32m[20221213 20:55:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:55:58 @agent_ppo2.py:185][0m |          -0.0061 |           3.3052 |          12.5163 |
[32m[20221213 20:55:58 @agent_ppo2.py:185][0m |          -0.0066 |           3.1980 |          12.5149 |
[32m[20221213 20:55:58 @agent_ppo2.py:185][0m |          -0.0047 |           3.1631 |          12.5081 |
[32m[20221213 20:55:59 @agent_ppo2.py:185][0m |           0.0003 |           3.1368 |          12.5055 |
[32m[20221213 20:55:59 @agent_ppo2.py:185][0m |           0.0005 |           3.1331 |          12.5047 |
[32m[20221213 20:55:59 @agent_ppo2.py:185][0m |          -0.0046 |           3.1128 |          12.5039 |
[32m[20221213 20:55:59 @agent_ppo2.py:185][0m |          -0.0057 |           3.1213 |          12.5055 |
[32m[20221213 20:55:59 @agent_ppo2.py:185][0m |          -0.0041 |           3.1185 |          12.5050 |
[32m[20221213 20:55:59 @agent_ppo2.py:185][0m |          -0.0091 |           3.1143 |          12.5059 |
[32m[20221213 20:55:59 @agent_ppo2.py:185][0m |          -0.0053 |           3.1071 |          12.5069 |
[32m[20221213 20:55:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:55:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 72.00
[32m[20221213 20:55:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 92.00
[32m[20221213 20:55:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 100.00
[32m[20221213 20:55:59 @agent_ppo2.py:143][0m Total time:       0.41 min
[32m[20221213 20:55:59 @agent_ppo2.py:145][0m 40960 total steps have happened
[32m[20221213 20:55:59 @agent_ppo2.py:121][0m #------------------------ Iteration 20 --------------------------#
[32m[20221213 20:55:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:55:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:00 @agent_ppo2.py:185][0m |          -0.0051 |           4.6920 |          12.5802 |
[32m[20221213 20:56:00 @agent_ppo2.py:185][0m |           0.0050 |           4.4331 |          12.5759 |
[32m[20221213 20:56:00 @agent_ppo2.py:185][0m |           0.0037 |           4.3997 |          12.5734 |
[32m[20221213 20:56:00 @agent_ppo2.py:185][0m |          -0.0100 |           4.3723 |          12.5750 |
[32m[20221213 20:56:00 @agent_ppo2.py:185][0m |          -0.0022 |           4.3545 |          12.5723 |
[32m[20221213 20:56:00 @agent_ppo2.py:185][0m |          -0.0052 |           4.3363 |          12.5751 |
[32m[20221213 20:56:00 @agent_ppo2.py:185][0m |          -0.0092 |           4.3750 |          12.5793 |
[32m[20221213 20:56:00 @agent_ppo2.py:185][0m |          -0.0127 |           4.4448 |          12.5824 |
[32m[20221213 20:56:00 @agent_ppo2.py:185][0m |          -0.0067 |           4.4256 |          12.5824 |
[32m[20221213 20:56:00 @agent_ppo2.py:185][0m |          -0.0021 |           4.2962 |          12.5829 |
[32m[20221213 20:56:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:56:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.60
[32m[20221213 20:56:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 119.00
[32m[20221213 20:56:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.00
[32m[20221213 20:56:00 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 136.00
[32m[20221213 20:56:00 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 136.00
[32m[20221213 20:56:00 @agent_ppo2.py:143][0m Total time:       0.43 min
[32m[20221213 20:56:00 @agent_ppo2.py:145][0m 43008 total steps have happened
[32m[20221213 20:56:00 @agent_ppo2.py:121][0m #------------------------ Iteration 21 --------------------------#
[32m[20221213 20:56:01 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:56:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:01 @agent_ppo2.py:185][0m |           0.0003 |           3.8145 |          12.6872 |
[32m[20221213 20:56:01 @agent_ppo2.py:185][0m |          -0.0056 |           3.7343 |          12.6788 |
[32m[20221213 20:56:01 @agent_ppo2.py:185][0m |          -0.0001 |           3.7259 |          12.6725 |
[32m[20221213 20:56:01 @agent_ppo2.py:185][0m |          -0.0042 |           3.7188 |          12.6718 |
[32m[20221213 20:56:01 @agent_ppo2.py:185][0m |          -0.0064 |           3.7221 |          12.6708 |
[32m[20221213 20:56:01 @agent_ppo2.py:185][0m |           0.0037 |           3.7318 |          12.6674 |
[32m[20221213 20:56:01 @agent_ppo2.py:185][0m |          -0.0038 |           3.6946 |          12.6678 |
[32m[20221213 20:56:01 @agent_ppo2.py:185][0m |          -0.0009 |           3.7071 |          12.6719 |
[32m[20221213 20:56:01 @agent_ppo2.py:185][0m |          -0.0050 |           3.6921 |          12.6717 |
[32m[20221213 20:56:02 @agent_ppo2.py:185][0m |          -0.0057 |           3.7049 |          12.6763 |
[32m[20221213 20:56:02 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:56:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 78.40
[32m[20221213 20:56:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 98.00
[32m[20221213 20:56:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 148.00
[32m[20221213 20:56:02 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 148.00
[32m[20221213 20:56:02 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 148.00
[32m[20221213 20:56:02 @agent_ppo2.py:143][0m Total time:       0.45 min
[32m[20221213 20:56:02 @agent_ppo2.py:145][0m 45056 total steps have happened
[32m[20221213 20:56:02 @agent_ppo2.py:121][0m #------------------------ Iteration 22 --------------------------#
[32m[20221213 20:56:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:02 @agent_ppo2.py:185][0m |           0.0043 |           4.7540 |          12.8062 |
[32m[20221213 20:56:02 @agent_ppo2.py:185][0m |          -0.0004 |           4.6667 |          12.8049 |
[32m[20221213 20:56:02 @agent_ppo2.py:185][0m |          -0.0025 |           4.6658 |          12.7983 |
[32m[20221213 20:56:02 @agent_ppo2.py:185][0m |          -0.0045 |           4.6635 |          12.7933 |
[32m[20221213 20:56:02 @agent_ppo2.py:185][0m |          -0.0056 |           4.6684 |          12.7922 |
[32m[20221213 20:56:02 @agent_ppo2.py:185][0m |          -0.0018 |           4.6418 |          12.7945 |
[32m[20221213 20:56:03 @agent_ppo2.py:185][0m |          -0.0219 |           4.8334 |          12.7959 |
[32m[20221213 20:56:03 @agent_ppo2.py:185][0m |           0.0009 |           4.6664 |          12.7964 |
[32m[20221213 20:56:03 @agent_ppo2.py:185][0m |          -0.0082 |           4.6874 |          12.7988 |
[32m[20221213 20:56:03 @agent_ppo2.py:185][0m |          -0.0060 |           4.6396 |          12.7989 |
[32m[20221213 20:56:03 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:56:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.20
[32m[20221213 20:56:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 137.00
[32m[20221213 20:56:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 20:56:03 @agent_ppo2.py:143][0m Total time:       0.47 min
[32m[20221213 20:56:03 @agent_ppo2.py:145][0m 47104 total steps have happened
[32m[20221213 20:56:03 @agent_ppo2.py:121][0m #------------------------ Iteration 23 --------------------------#
[32m[20221213 20:56:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 20:56:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:03 @agent_ppo2.py:185][0m |          -0.0011 |           5.5099 |          12.9850 |
[32m[20221213 20:56:03 @agent_ppo2.py:185][0m |          -0.0042 |           5.4609 |          12.9852 |
[32m[20221213 20:56:03 @agent_ppo2.py:185][0m |           0.0036 |           5.4194 |          12.9814 |
[32m[20221213 20:56:04 @agent_ppo2.py:185][0m |          -0.0044 |           5.3343 |          12.9825 |
[32m[20221213 20:56:04 @agent_ppo2.py:185][0m |          -0.0069 |           5.3119 |          12.9790 |
[32m[20221213 20:56:04 @agent_ppo2.py:185][0m |          -0.0003 |           5.3092 |          12.9822 |
[32m[20221213 20:56:04 @agent_ppo2.py:185][0m |          -0.0123 |           5.3333 |          12.9834 |
[32m[20221213 20:56:04 @agent_ppo2.py:185][0m |          -0.0046 |           5.2808 |          12.9842 |
[32m[20221213 20:56:04 @agent_ppo2.py:185][0m |          -0.0123 |           5.3691 |          12.9867 |
[32m[20221213 20:56:04 @agent_ppo2.py:185][0m |          -0.0038 |           5.3037 |          12.9918 |
[32m[20221213 20:56:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:56:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 114.80
[32m[20221213 20:56:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 134.00
[32m[20221213 20:56:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 148.00
[32m[20221213 20:56:04 @agent_ppo2.py:143][0m Total time:       0.49 min
[32m[20221213 20:56:04 @agent_ppo2.py:145][0m 49152 total steps have happened
[32m[20221213 20:56:04 @agent_ppo2.py:121][0m #------------------------ Iteration 24 --------------------------#
[32m[20221213 20:56:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:04 @agent_ppo2.py:185][0m |           0.0012 |           5.9682 |          12.9362 |
[32m[20221213 20:56:05 @agent_ppo2.py:185][0m |          -0.0026 |           5.8683 |          12.9359 |
[32m[20221213 20:56:05 @agent_ppo2.py:185][0m |          -0.0019 |           5.8037 |          12.9325 |
[32m[20221213 20:56:05 @agent_ppo2.py:185][0m |          -0.0116 |           5.9380 |          12.9317 |
[32m[20221213 20:56:05 @agent_ppo2.py:185][0m |          -0.0030 |           5.7981 |          12.9310 |
[32m[20221213 20:56:05 @agent_ppo2.py:185][0m |          -0.0065 |           5.7551 |          12.9327 |
[32m[20221213 20:56:05 @agent_ppo2.py:185][0m |          -0.0056 |           5.7775 |          12.9327 |
[32m[20221213 20:56:05 @agent_ppo2.py:185][0m |          -0.0054 |           5.7335 |          12.9319 |
[32m[20221213 20:56:05 @agent_ppo2.py:185][0m |          -0.0046 |           5.7437 |          12.9307 |
[32m[20221213 20:56:05 @agent_ppo2.py:185][0m |          -0.0060 |           5.7560 |          12.9335 |
[32m[20221213 20:56:05 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:56:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 105.20
[32m[20221213 20:56:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 132.00
[32m[20221213 20:56:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 231.00
[32m[20221213 20:56:05 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 231.00
[32m[20221213 20:56:05 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 231.00
[32m[20221213 20:56:05 @agent_ppo2.py:143][0m Total time:       0.51 min
[32m[20221213 20:56:05 @agent_ppo2.py:145][0m 51200 total steps have happened
[32m[20221213 20:56:05 @agent_ppo2.py:121][0m #------------------------ Iteration 25 --------------------------#
[32m[20221213 20:56:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:06 @agent_ppo2.py:185][0m |          -0.0048 |           4.5025 |          13.0147 |
[32m[20221213 20:56:06 @agent_ppo2.py:185][0m |           0.0010 |           4.4145 |          13.0155 |
[32m[20221213 20:56:06 @agent_ppo2.py:185][0m |          -0.0047 |           4.4293 |          13.0110 |
[32m[20221213 20:56:06 @agent_ppo2.py:185][0m |          -0.0003 |           4.3855 |          13.0043 |
[32m[20221213 20:56:06 @agent_ppo2.py:185][0m |           0.0022 |           4.3677 |          13.0045 |
[32m[20221213 20:56:06 @agent_ppo2.py:185][0m |          -0.0062 |           4.3878 |          13.0029 |
[32m[20221213 20:56:06 @agent_ppo2.py:185][0m |           0.0034 |           4.4465 |          13.0016 |
[32m[20221213 20:56:06 @agent_ppo2.py:185][0m |          -0.0062 |           4.3751 |          12.9997 |
[32m[20221213 20:56:06 @agent_ppo2.py:185][0m |          -0.0042 |           4.3513 |          12.9989 |
[32m[20221213 20:56:07 @agent_ppo2.py:185][0m |          -0.0052 |           4.4045 |          13.0010 |
[32m[20221213 20:56:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:56:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 90.00
[32m[20221213 20:56:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 106.00
[32m[20221213 20:56:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.00
[32m[20221213 20:56:07 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 245.00
[32m[20221213 20:56:07 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 245.00
[32m[20221213 20:56:07 @agent_ppo2.py:143][0m Total time:       0.53 min
[32m[20221213 20:56:07 @agent_ppo2.py:145][0m 53248 total steps have happened
[32m[20221213 20:56:07 @agent_ppo2.py:121][0m #------------------------ Iteration 26 --------------------------#
[32m[20221213 20:56:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:56:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:07 @agent_ppo2.py:185][0m |          -0.0002 |           6.8033 |          12.8988 |
[32m[20221213 20:56:07 @agent_ppo2.py:185][0m |          -0.0008 |           6.6143 |          12.8958 |
[32m[20221213 20:56:07 @agent_ppo2.py:185][0m |          -0.0082 |           6.6185 |          12.8890 |
[32m[20221213 20:56:07 @agent_ppo2.py:185][0m |          -0.0032 |           6.5601 |          12.8877 |
[32m[20221213 20:56:07 @agent_ppo2.py:185][0m |          -0.0019 |           6.4885 |          12.8892 |
[32m[20221213 20:56:07 @agent_ppo2.py:185][0m |           0.0010 |           6.6670 |          12.8901 |
[32m[20221213 20:56:07 @agent_ppo2.py:185][0m |          -0.0035 |           6.4498 |          12.8892 |
[32m[20221213 20:56:08 @agent_ppo2.py:185][0m |          -0.0096 |           6.4395 |          12.8946 |
[32m[20221213 20:56:08 @agent_ppo2.py:185][0m |          -0.0069 |           6.4441 |          12.8928 |
[32m[20221213 20:56:08 @agent_ppo2.py:185][0m |          -0.0070 |           6.3880 |          12.8914 |
[32m[20221213 20:56:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:56:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 111.40
[32m[20221213 20:56:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 155.00
[32m[20221213 20:56:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 215.00
[32m[20221213 20:56:08 @agent_ppo2.py:143][0m Total time:       0.55 min
[32m[20221213 20:56:08 @agent_ppo2.py:145][0m 55296 total steps have happened
[32m[20221213 20:56:08 @agent_ppo2.py:121][0m #------------------------ Iteration 27 --------------------------#
[32m[20221213 20:56:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:08 @agent_ppo2.py:185][0m |          -0.0050 |           8.2913 |          13.1948 |
[32m[20221213 20:56:08 @agent_ppo2.py:185][0m |          -0.0014 |           8.0184 |          13.1874 |
[32m[20221213 20:56:08 @agent_ppo2.py:185][0m |          -0.0060 |           7.9299 |          13.1785 |
[32m[20221213 20:56:08 @agent_ppo2.py:185][0m |          -0.0089 |           8.1358 |          13.1777 |
[32m[20221213 20:56:08 @agent_ppo2.py:185][0m |          -0.0072 |           7.9142 |          13.1729 |
[32m[20221213 20:56:09 @agent_ppo2.py:185][0m |          -0.0003 |           8.0213 |          13.1748 |
[32m[20221213 20:56:09 @agent_ppo2.py:185][0m |          -0.0012 |           7.8381 |          13.1760 |
[32m[20221213 20:56:09 @agent_ppo2.py:185][0m |          -0.0118 |           7.8546 |          13.1792 |
[32m[20221213 20:56:09 @agent_ppo2.py:185][0m |          -0.0060 |           7.8111 |          13.1835 |
[32m[20221213 20:56:09 @agent_ppo2.py:185][0m |          -0.0010 |           7.7551 |          13.1826 |
[32m[20221213 20:56:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:56:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 134.20
[32m[20221213 20:56:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 164.00
[32m[20221213 20:56:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.00
[32m[20221213 20:56:09 @agent_ppo2.py:143][0m Total time:       0.57 min
[32m[20221213 20:56:09 @agent_ppo2.py:145][0m 57344 total steps have happened
[32m[20221213 20:56:09 @agent_ppo2.py:121][0m #------------------------ Iteration 28 --------------------------#
[32m[20221213 20:56:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:09 @agent_ppo2.py:185][0m |          -0.0024 |           7.9089 |          13.1417 |
[32m[20221213 20:56:09 @agent_ppo2.py:185][0m |          -0.0005 |           7.8361 |          13.1362 |
[32m[20221213 20:56:10 @agent_ppo2.py:185][0m |          -0.0056 |           7.7594 |          13.1329 |
[32m[20221213 20:56:10 @agent_ppo2.py:185][0m |          -0.0109 |           7.9115 |          13.1319 |
[32m[20221213 20:56:10 @agent_ppo2.py:185][0m |          -0.0079 |           7.7510 |          13.1375 |
[32m[20221213 20:56:10 @agent_ppo2.py:185][0m |          -0.0120 |           7.7967 |          13.1353 |
[32m[20221213 20:56:10 @agent_ppo2.py:185][0m |          -0.0080 |           7.7828 |          13.1396 |
[32m[20221213 20:56:10 @agent_ppo2.py:185][0m |          -0.0035 |           7.6218 |          13.1400 |
[32m[20221213 20:56:10 @agent_ppo2.py:185][0m |          -0.0127 |           7.6942 |          13.1428 |
[32m[20221213 20:56:10 @agent_ppo2.py:185][0m |          -0.0048 |           7.6172 |          13.1444 |
[32m[20221213 20:56:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:56:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 124.40
[32m[20221213 20:56:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.00
[32m[20221213 20:56:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 235.00
[32m[20221213 20:56:10 @agent_ppo2.py:143][0m Total time:       0.59 min
[32m[20221213 20:56:10 @agent_ppo2.py:145][0m 59392 total steps have happened
[32m[20221213 20:56:10 @agent_ppo2.py:121][0m #------------------------ Iteration 29 --------------------------#
[32m[20221213 20:56:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:11 @agent_ppo2.py:185][0m |          -0.0064 |           8.4440 |          13.3530 |
[32m[20221213 20:56:11 @agent_ppo2.py:185][0m |           0.0089 |           8.5193 |          13.3518 |
[32m[20221213 20:56:11 @agent_ppo2.py:185][0m |          -0.0011 |           8.2759 |          13.3455 |
[32m[20221213 20:56:11 @agent_ppo2.py:185][0m |          -0.0055 |           8.3778 |          13.3450 |
[32m[20221213 20:56:11 @agent_ppo2.py:185][0m |           0.0041 |           8.3461 |          13.3400 |
[32m[20221213 20:56:11 @agent_ppo2.py:185][0m |           0.0000 |           8.2934 |          13.3443 |
[32m[20221213 20:56:11 @agent_ppo2.py:185][0m |          -0.0035 |           8.2532 |          13.3431 |
[32m[20221213 20:56:11 @agent_ppo2.py:185][0m |          -0.0016 |           8.2083 |          13.3419 |
[32m[20221213 20:56:11 @agent_ppo2.py:185][0m |          -0.0163 |           8.3959 |          13.3448 |
[32m[20221213 20:56:11 @agent_ppo2.py:185][0m |          -0.0049 |           8.2867 |          13.3410 |
[32m[20221213 20:56:11 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:56:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 132.40
[32m[20221213 20:56:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 155.00
[32m[20221213 20:56:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.00
[32m[20221213 20:56:11 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 259.00
[32m[20221213 20:56:11 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 259.00
[32m[20221213 20:56:11 @agent_ppo2.py:143][0m Total time:       0.61 min
[32m[20221213 20:56:11 @agent_ppo2.py:145][0m 61440 total steps have happened
[32m[20221213 20:56:11 @agent_ppo2.py:121][0m #------------------------ Iteration 30 --------------------------#
[32m[20221213 20:56:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:12 @agent_ppo2.py:185][0m |           0.0012 |           9.5351 |          13.2287 |
[32m[20221213 20:56:12 @agent_ppo2.py:185][0m |           0.0017 |           9.3603 |          13.2257 |
[32m[20221213 20:56:12 @agent_ppo2.py:185][0m |          -0.0055 |           9.3140 |          13.2175 |
[32m[20221213 20:56:12 @agent_ppo2.py:185][0m |          -0.0052 |           9.3086 |          13.2140 |
[32m[20221213 20:56:12 @agent_ppo2.py:185][0m |          -0.0067 |           9.2976 |          13.2180 |
[32m[20221213 20:56:12 @agent_ppo2.py:185][0m |          -0.0002 |           9.2974 |          13.2228 |
[32m[20221213 20:56:12 @agent_ppo2.py:185][0m |          -0.0114 |           9.3983 |          13.2177 |
[32m[20221213 20:56:12 @agent_ppo2.py:185][0m |          -0.0084 |           9.2259 |          13.2195 |
[32m[20221213 20:56:12 @agent_ppo2.py:185][0m |          -0.0063 |           9.2071 |          13.2245 |
[32m[20221213 20:56:12 @agent_ppo2.py:185][0m |          -0.0089 |           9.2661 |          13.2275 |
[32m[20221213 20:56:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:56:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 150.20
[32m[20221213 20:56:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 184.00
[32m[20221213 20:56:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 227.00
[32m[20221213 20:56:13 @agent_ppo2.py:143][0m Total time:       0.63 min
[32m[20221213 20:56:13 @agent_ppo2.py:145][0m 63488 total steps have happened
[32m[20221213 20:56:13 @agent_ppo2.py:121][0m #------------------------ Iteration 31 --------------------------#
[32m[20221213 20:56:13 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:56:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:13 @agent_ppo2.py:185][0m |           0.0028 |           8.9697 |          13.2369 |
[32m[20221213 20:56:13 @agent_ppo2.py:185][0m |           0.0027 |           8.9499 |          13.2305 |
[32m[20221213 20:56:13 @agent_ppo2.py:185][0m |          -0.0088 |           8.7583 |          13.2263 |
[32m[20221213 20:56:13 @agent_ppo2.py:185][0m |          -0.0038 |           8.6439 |          13.2251 |
[32m[20221213 20:56:13 @agent_ppo2.py:185][0m |          -0.0011 |           9.0290 |          13.2246 |
[32m[20221213 20:56:13 @agent_ppo2.py:185][0m |          -0.0059 |           8.7336 |          13.2263 |
[32m[20221213 20:56:13 @agent_ppo2.py:185][0m |          -0.0093 |           8.6654 |          13.2263 |
[32m[20221213 20:56:13 @agent_ppo2.py:185][0m |          -0.0064 |           8.6177 |          13.2323 |
[32m[20221213 20:56:14 @agent_ppo2.py:185][0m |          -0.0075 |           8.6383 |          13.2301 |
[32m[20221213 20:56:14 @agent_ppo2.py:185][0m |          -0.0029 |           8.8423 |          13.2332 |
[32m[20221213 20:56:14 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:56:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 169.40
[32m[20221213 20:56:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 187.00
[32m[20221213 20:56:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.00
[32m[20221213 20:56:14 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 301.00
[32m[20221213 20:56:14 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 301.00
[32m[20221213 20:56:14 @agent_ppo2.py:143][0m Total time:       0.65 min
[32m[20221213 20:56:14 @agent_ppo2.py:145][0m 65536 total steps have happened
[32m[20221213 20:56:14 @agent_ppo2.py:121][0m #------------------------ Iteration 32 --------------------------#
[32m[20221213 20:56:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:14 @agent_ppo2.py:185][0m |          -0.0038 |           9.7597 |          13.4329 |
[32m[20221213 20:56:14 @agent_ppo2.py:185][0m |          -0.0051 |           9.4254 |          13.4208 |
[32m[20221213 20:56:14 @agent_ppo2.py:185][0m |          -0.0053 |           9.4156 |          13.4151 |
[32m[20221213 20:56:14 @agent_ppo2.py:185][0m |          -0.0108 |           9.4385 |          13.4120 |
[32m[20221213 20:56:14 @agent_ppo2.py:185][0m |          -0.0120 |           9.4563 |          13.4067 |
[32m[20221213 20:56:14 @agent_ppo2.py:185][0m |          -0.0029 |           9.4010 |          13.4094 |
[32m[20221213 20:56:15 @agent_ppo2.py:185][0m |          -0.0091 |           9.3805 |          13.4077 |
[32m[20221213 20:56:15 @agent_ppo2.py:185][0m |          -0.0028 |           9.3130 |          13.4048 |
[32m[20221213 20:56:15 @agent_ppo2.py:185][0m |          -0.0045 |           9.3092 |          13.4122 |
[32m[20221213 20:56:15 @agent_ppo2.py:185][0m |          -0.0082 |           9.3036 |          13.4089 |
[32m[20221213 20:56:15 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:56:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 172.00
[32m[20221213 20:56:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 218.00
[32m[20221213 20:56:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 211.00
[32m[20221213 20:56:15 @agent_ppo2.py:143][0m Total time:       0.67 min
[32m[20221213 20:56:15 @agent_ppo2.py:145][0m 67584 total steps have happened
[32m[20221213 20:56:15 @agent_ppo2.py:121][0m #------------------------ Iteration 33 --------------------------#
[32m[20221213 20:56:15 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:56:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:15 @agent_ppo2.py:185][0m |          -0.0072 |          11.6654 |          13.3500 |
[32m[20221213 20:56:15 @agent_ppo2.py:185][0m |          -0.0057 |          11.5184 |          13.3456 |
[32m[20221213 20:56:16 @agent_ppo2.py:185][0m |          -0.0112 |          11.5407 |          13.3425 |
[32m[20221213 20:56:16 @agent_ppo2.py:185][0m |          -0.0077 |          11.4223 |          13.3430 |
[32m[20221213 20:56:16 @agent_ppo2.py:185][0m |          -0.0050 |          11.4544 |          13.3466 |
[32m[20221213 20:56:16 @agent_ppo2.py:185][0m |          -0.0069 |          11.3025 |          13.3497 |
[32m[20221213 20:56:16 @agent_ppo2.py:185][0m |          -0.0049 |          11.2568 |          13.3459 |
[32m[20221213 20:56:16 @agent_ppo2.py:185][0m |           0.0003 |          11.5571 |          13.3501 |
[32m[20221213 20:56:16 @agent_ppo2.py:185][0m |          -0.0159 |          11.3747 |          13.3534 |
[32m[20221213 20:56:16 @agent_ppo2.py:185][0m |          -0.0099 |          11.6560 |          13.3556 |
[32m[20221213 20:56:16 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 20:56:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 183.00
[32m[20221213 20:56:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 216.00
[32m[20221213 20:56:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.00
[32m[20221213 20:56:16 @agent_ppo2.py:143][0m Total time:       0.69 min
[32m[20221213 20:56:16 @agent_ppo2.py:145][0m 69632 total steps have happened
[32m[20221213 20:56:16 @agent_ppo2.py:121][0m #------------------------ Iteration 34 --------------------------#
[32m[20221213 20:56:16 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:56:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:17 @agent_ppo2.py:185][0m |           0.0018 |          10.2989 |          13.5634 |
[32m[20221213 20:56:17 @agent_ppo2.py:185][0m |          -0.0018 |          10.2096 |          13.5554 |
[32m[20221213 20:56:17 @agent_ppo2.py:185][0m |          -0.0063 |          10.2368 |          13.5483 |
[32m[20221213 20:56:17 @agent_ppo2.py:185][0m |          -0.0036 |          10.1053 |          13.5415 |
[32m[20221213 20:56:17 @agent_ppo2.py:185][0m |           0.0014 |          10.2146 |          13.5370 |
[32m[20221213 20:56:17 @agent_ppo2.py:185][0m |          -0.0092 |          10.2325 |          13.5319 |
[32m[20221213 20:56:18 @agent_ppo2.py:185][0m |          -0.0094 |          10.0646 |          13.5367 |
[32m[20221213 20:56:18 @agent_ppo2.py:185][0m |          -0.0072 |          10.0181 |          13.5347 |
[32m[20221213 20:56:18 @agent_ppo2.py:185][0m |          -0.0045 |          10.0362 |          13.5375 |
[32m[20221213 20:56:18 @agent_ppo2.py:185][0m |          -0.0062 |          10.1179 |          13.5376 |
[32m[20221213 20:56:18 @agent_ppo2.py:130][0m Policy update time: 1.53 s
[32m[20221213 20:56:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 180.40
[32m[20221213 20:56:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 214.00
[32m[20221213 20:56:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.00
[32m[20221213 20:56:18 @agent_ppo2.py:143][0m Total time:       0.72 min
[32m[20221213 20:56:18 @agent_ppo2.py:145][0m 71680 total steps have happened
[32m[20221213 20:56:18 @agent_ppo2.py:121][0m #------------------------ Iteration 35 --------------------------#
[32m[20221213 20:56:18 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:56:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:18 @agent_ppo2.py:185][0m |           0.0010 |          11.5341 |          13.7074 |
[32m[20221213 20:56:19 @agent_ppo2.py:185][0m |          -0.0051 |          11.4107 |          13.6989 |
[32m[20221213 20:56:19 @agent_ppo2.py:185][0m |          -0.0016 |          11.1968 |          13.6993 |
[32m[20221213 20:56:19 @agent_ppo2.py:185][0m |          -0.0057 |          11.2860 |          13.6892 |
[32m[20221213 20:56:19 @agent_ppo2.py:185][0m |          -0.0084 |          11.1190 |          13.6887 |
[32m[20221213 20:56:19 @agent_ppo2.py:185][0m |          -0.0079 |          11.2336 |          13.6871 |
[32m[20221213 20:56:19 @agent_ppo2.py:185][0m |          -0.0080 |          11.1697 |          13.6856 |
[32m[20221213 20:56:19 @agent_ppo2.py:185][0m |          -0.0071 |          11.4356 |          13.6832 |
[32m[20221213 20:56:19 @agent_ppo2.py:185][0m |          -0.0127 |          11.2078 |          13.6840 |
[32m[20221213 20:56:19 @agent_ppo2.py:185][0m |          -0.0058 |          11.1277 |          13.6836 |
[32m[20221213 20:56:19 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 20:56:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 189.20
[32m[20221213 20:56:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 208.00
[32m[20221213 20:56:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 322.00
[32m[20221213 20:56:20 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 322.00
[32m[20221213 20:56:20 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 322.00
[32m[20221213 20:56:20 @agent_ppo2.py:143][0m Total time:       0.75 min
[32m[20221213 20:56:20 @agent_ppo2.py:145][0m 73728 total steps have happened
[32m[20221213 20:56:20 @agent_ppo2.py:121][0m #------------------------ Iteration 36 --------------------------#
[32m[20221213 20:56:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 20:56:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:20 @agent_ppo2.py:185][0m |           0.0059 |          11.9714 |          13.5560 |
[32m[20221213 20:56:20 @agent_ppo2.py:185][0m |          -0.0058 |          11.7242 |          13.5510 |
[32m[20221213 20:56:20 @agent_ppo2.py:185][0m |          -0.0046 |          11.5623 |          13.5537 |
[32m[20221213 20:56:20 @agent_ppo2.py:185][0m |          -0.0057 |          11.5098 |          13.5533 |
[32m[20221213 20:56:20 @agent_ppo2.py:185][0m |          -0.0056 |          11.4992 |          13.5567 |
[32m[20221213 20:56:20 @agent_ppo2.py:185][0m |          -0.0035 |          11.5630 |          13.5579 |
[32m[20221213 20:56:21 @agent_ppo2.py:185][0m |          -0.0075 |          11.4761 |          13.5621 |
[32m[20221213 20:56:21 @agent_ppo2.py:185][0m |          -0.0044 |          11.4111 |          13.5604 |
[32m[20221213 20:56:21 @agent_ppo2.py:185][0m |          -0.0069 |          11.4421 |          13.5588 |
[32m[20221213 20:56:21 @agent_ppo2.py:185][0m |          -0.0052 |          11.5076 |          13.5636 |
[32m[20221213 20:56:21 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 20:56:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 186.00
[32m[20221213 20:56:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 222.00
[32m[20221213 20:56:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.00
[32m[20221213 20:56:21 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 367.00
[32m[20221213 20:56:21 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 367.00
[32m[20221213 20:56:21 @agent_ppo2.py:143][0m Total time:       0.77 min
[32m[20221213 20:56:21 @agent_ppo2.py:145][0m 75776 total steps have happened
[32m[20221213 20:56:21 @agent_ppo2.py:121][0m #------------------------ Iteration 37 --------------------------#
[32m[20221213 20:56:21 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:56:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:21 @agent_ppo2.py:185][0m |          -0.0036 |          12.5719 |          13.5811 |
[32m[20221213 20:56:21 @agent_ppo2.py:185][0m |          -0.0040 |          12.4193 |          13.5736 |
[32m[20221213 20:56:22 @agent_ppo2.py:185][0m |          -0.0072 |          12.3436 |          13.5656 |
[32m[20221213 20:56:22 @agent_ppo2.py:185][0m |          -0.0076 |          12.3251 |          13.5625 |
[32m[20221213 20:56:22 @agent_ppo2.py:185][0m |          -0.0084 |          12.4186 |          13.5611 |
[32m[20221213 20:56:22 @agent_ppo2.py:185][0m |          -0.0041 |          12.3276 |          13.5608 |
[32m[20221213 20:56:22 @agent_ppo2.py:185][0m |          -0.0101 |          12.3034 |          13.5592 |
[32m[20221213 20:56:22 @agent_ppo2.py:185][0m |          -0.0077 |          12.4743 |          13.5597 |
[32m[20221213 20:56:22 @agent_ppo2.py:185][0m |          -0.0081 |          12.3152 |          13.5624 |
[32m[20221213 20:56:22 @agent_ppo2.py:185][0m |          -0.0067 |          12.2455 |          13.5631 |
[32m[20221213 20:56:22 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 20:56:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 193.40
[32m[20221213 20:56:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.00
[32m[20221213 20:56:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.00
[32m[20221213 20:56:22 @agent_ppo2.py:143][0m Total time:       0.80 min
[32m[20221213 20:56:22 @agent_ppo2.py:145][0m 77824 total steps have happened
[32m[20221213 20:56:22 @agent_ppo2.py:121][0m #------------------------ Iteration 38 --------------------------#
[32m[20221213 20:56:23 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:56:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:23 @agent_ppo2.py:185][0m |          -0.0083 |          12.2896 |          13.6782 |
[32m[20221213 20:56:23 @agent_ppo2.py:185][0m |          -0.0020 |          11.9551 |          13.6701 |
[32m[20221213 20:56:23 @agent_ppo2.py:185][0m |          -0.0050 |          11.9419 |          13.6618 |
[32m[20221213 20:56:23 @agent_ppo2.py:185][0m |          -0.0017 |          11.8573 |          13.6616 |
[32m[20221213 20:56:23 @agent_ppo2.py:185][0m |          -0.0044 |          11.9425 |          13.6617 |
[32m[20221213 20:56:23 @agent_ppo2.py:185][0m |          -0.0096 |          11.9458 |          13.6590 |
[32m[20221213 20:56:23 @agent_ppo2.py:185][0m |          -0.0119 |          11.8358 |          13.6578 |
[32m[20221213 20:56:23 @agent_ppo2.py:185][0m |          -0.0028 |          11.9201 |          13.6603 |
[32m[20221213 20:56:24 @agent_ppo2.py:185][0m |          -0.0053 |          11.7853 |          13.6590 |
[32m[20221213 20:56:24 @agent_ppo2.py:185][0m |          -0.0101 |          11.8527 |          13.6635 |
[32m[20221213 20:56:24 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 20:56:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 180.40
[32m[20221213 20:56:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 207.00
[32m[20221213 20:56:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.00
[32m[20221213 20:56:24 @agent_ppo2.py:143][0m Total time:       0.82 min
[32m[20221213 20:56:24 @agent_ppo2.py:145][0m 79872 total steps have happened
[32m[20221213 20:56:24 @agent_ppo2.py:121][0m #------------------------ Iteration 39 --------------------------#
[32m[20221213 20:56:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 20:56:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:24 @agent_ppo2.py:185][0m |          -0.0009 |          13.3635 |          13.8503 |
[32m[20221213 20:56:24 @agent_ppo2.py:185][0m |          -0.0039 |          13.1990 |          13.8370 |
[32m[20221213 20:56:24 @agent_ppo2.py:185][0m |          -0.0077 |          13.0430 |          13.8348 |
[32m[20221213 20:56:25 @agent_ppo2.py:185][0m |          -0.0080 |          13.0721 |          13.8317 |
[32m[20221213 20:56:25 @agent_ppo2.py:185][0m |          -0.0083 |          13.0202 |          13.8319 |
[32m[20221213 20:56:25 @agent_ppo2.py:185][0m |          -0.0020 |          12.9047 |          13.8335 |
[32m[20221213 20:56:25 @agent_ppo2.py:185][0m |          -0.0079 |          12.9732 |          13.8337 |
[32m[20221213 20:56:25 @agent_ppo2.py:185][0m |          -0.0089 |          12.9133 |          13.8330 |
[32m[20221213 20:56:25 @agent_ppo2.py:185][0m |          -0.0058 |          12.8375 |          13.8367 |
[32m[20221213 20:56:25 @agent_ppo2.py:185][0m |          -0.0043 |          12.8302 |          13.8420 |
[32m[20221213 20:56:25 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 20:56:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 197.00
[32m[20221213 20:56:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 217.00
[32m[20221213 20:56:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.00
[32m[20221213 20:56:25 @agent_ppo2.py:143][0m Total time:       0.84 min
[32m[20221213 20:56:25 @agent_ppo2.py:145][0m 81920 total steps have happened
[32m[20221213 20:56:25 @agent_ppo2.py:121][0m #------------------------ Iteration 40 --------------------------#
[32m[20221213 20:56:25 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:56:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:26 @agent_ppo2.py:185][0m |           0.0049 |          14.3201 |          13.5592 |
[32m[20221213 20:56:26 @agent_ppo2.py:185][0m |          -0.0035 |          14.0704 |          13.5516 |
[32m[20221213 20:56:26 @agent_ppo2.py:185][0m |          -0.0049 |          14.0843 |          13.5518 |
[32m[20221213 20:56:26 @agent_ppo2.py:185][0m |          -0.0069 |          14.1009 |          13.5448 |
[32m[20221213 20:56:26 @agent_ppo2.py:185][0m |          -0.0119 |          14.0665 |          13.5485 |
[32m[20221213 20:56:26 @agent_ppo2.py:185][0m |          -0.0011 |          14.4588 |          13.5489 |
[32m[20221213 20:56:26 @agent_ppo2.py:185][0m |          -0.0098 |          14.0540 |          13.5479 |
[32m[20221213 20:56:26 @agent_ppo2.py:185][0m |          -0.0105 |          14.0062 |          13.5465 |
[32m[20221213 20:56:26 @agent_ppo2.py:185][0m |          -0.0031 |          14.0976 |          13.5493 |
[32m[20221213 20:56:27 @agent_ppo2.py:185][0m |          -0.0017 |          13.9818 |          13.5466 |
[32m[20221213 20:56:27 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 20:56:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 200.80
[32m[20221213 20:56:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 221.00
[32m[20221213 20:56:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.00
[32m[20221213 20:56:27 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 380.00
[32m[20221213 20:56:27 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 380.00
[32m[20221213 20:56:27 @agent_ppo2.py:143][0m Total time:       0.87 min
[32m[20221213 20:56:27 @agent_ppo2.py:145][0m 83968 total steps have happened
[32m[20221213 20:56:27 @agent_ppo2.py:121][0m #------------------------ Iteration 41 --------------------------#
[32m[20221213 20:56:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:56:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:27 @agent_ppo2.py:185][0m |          -0.0075 |          18.0343 |          13.8479 |
[32m[20221213 20:56:27 @agent_ppo2.py:185][0m |          -0.0012 |          17.4556 |          13.8437 |
[32m[20221213 20:56:27 @agent_ppo2.py:185][0m |           0.0014 |          17.6534 |          13.8297 |
[32m[20221213 20:56:27 @agent_ppo2.py:185][0m |          -0.0077 |          17.4248 |          13.8193 |
[32m[20221213 20:56:27 @agent_ppo2.py:185][0m |          -0.0015 |          17.3837 |          13.8277 |
[32m[20221213 20:56:27 @agent_ppo2.py:185][0m |          -0.0024 |          17.3229 |          13.8225 |
[32m[20221213 20:56:28 @agent_ppo2.py:185][0m |          -0.0069 |          17.3794 |          13.8285 |
[32m[20221213 20:56:28 @agent_ppo2.py:185][0m |          -0.0066 |          17.3188 |          13.8281 |
[32m[20221213 20:56:28 @agent_ppo2.py:185][0m |          -0.0086 |          17.4714 |          13.8258 |
[32m[20221213 20:56:28 @agent_ppo2.py:185][0m |          -0.0083 |          17.3756 |          13.8313 |
[32m[20221213 20:56:28 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:56:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.00
[32m[20221213 20:56:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 303.00
[32m[20221213 20:56:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.00
[32m[20221213 20:56:28 @agent_ppo2.py:143][0m Total time:       0.89 min
[32m[20221213 20:56:28 @agent_ppo2.py:145][0m 86016 total steps have happened
[32m[20221213 20:56:28 @agent_ppo2.py:121][0m #------------------------ Iteration 42 --------------------------#
[32m[20221213 20:56:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:28 @agent_ppo2.py:185][0m |          -0.0026 |          16.7201 |          13.8489 |
[32m[20221213 20:56:28 @agent_ppo2.py:185][0m |          -0.0046 |          16.3469 |          13.8318 |
[32m[20221213 20:56:28 @agent_ppo2.py:185][0m |           0.0034 |          16.5359 |          13.8276 |
[32m[20221213 20:56:29 @agent_ppo2.py:185][0m |          -0.0007 |          17.0577 |          13.8296 |
[32m[20221213 20:56:29 @agent_ppo2.py:185][0m |          -0.0081 |          16.3181 |          13.8277 |
[32m[20221213 20:56:29 @agent_ppo2.py:185][0m |          -0.0044 |          16.2546 |          13.8318 |
[32m[20221213 20:56:29 @agent_ppo2.py:185][0m |          -0.0069 |          16.2276 |          13.8332 |
[32m[20221213 20:56:29 @agent_ppo2.py:185][0m |          -0.0096 |          16.2395 |          13.8321 |
[32m[20221213 20:56:29 @agent_ppo2.py:185][0m |          -0.0077 |          16.1716 |          13.8338 |
[32m[20221213 20:56:29 @agent_ppo2.py:185][0m |          -0.0062 |          16.1770 |          13.8360 |
[32m[20221213 20:56:29 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:56:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 216.80
[32m[20221213 20:56:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.00
[32m[20221213 20:56:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.00
[32m[20221213 20:56:29 @agent_ppo2.py:143][0m Total time:       0.91 min
[32m[20221213 20:56:29 @agent_ppo2.py:145][0m 88064 total steps have happened
[32m[20221213 20:56:29 @agent_ppo2.py:121][0m #------------------------ Iteration 43 --------------------------#
[32m[20221213 20:56:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:30 @agent_ppo2.py:185][0m |          -0.0050 |          17.9450 |          13.9164 |
[32m[20221213 20:56:30 @agent_ppo2.py:185][0m |          -0.0077 |          17.9403 |          13.9107 |
[32m[20221213 20:56:30 @agent_ppo2.py:185][0m |          -0.0081 |          17.8985 |          13.9036 |
[32m[20221213 20:56:30 @agent_ppo2.py:185][0m |          -0.0070 |          17.9279 |          13.9080 |
[32m[20221213 20:56:30 @agent_ppo2.py:185][0m |          -0.0107 |          17.9152 |          13.9052 |
[32m[20221213 20:56:30 @agent_ppo2.py:185][0m |          -0.0097 |          17.9925 |          13.9034 |
[32m[20221213 20:56:30 @agent_ppo2.py:185][0m |          -0.0089 |          17.8049 |          13.8996 |
[32m[20221213 20:56:30 @agent_ppo2.py:185][0m |          -0.0038 |          17.7653 |          13.9012 |
[32m[20221213 20:56:30 @agent_ppo2.py:185][0m |          -0.0083 |          17.8048 |          13.8979 |
[32m[20221213 20:56:30 @agent_ppo2.py:185][0m |          -0.0076 |          17.7660 |          13.8996 |
[32m[20221213 20:56:30 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:56:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.40
[32m[20221213 20:56:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.00
[32m[20221213 20:56:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 222.00
[32m[20221213 20:56:30 @agent_ppo2.py:143][0m Total time:       0.93 min
[32m[20221213 20:56:30 @agent_ppo2.py:145][0m 90112 total steps have happened
[32m[20221213 20:56:30 @agent_ppo2.py:121][0m #------------------------ Iteration 44 --------------------------#
[32m[20221213 20:56:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:31 @agent_ppo2.py:185][0m |          -0.0069 |          17.4106 |          14.0002 |
[32m[20221213 20:56:31 @agent_ppo2.py:185][0m |          -0.0019 |          17.1206 |          13.9995 |
[32m[20221213 20:56:31 @agent_ppo2.py:185][0m |          -0.0031 |          17.1138 |          13.9912 |
[32m[20221213 20:56:31 @agent_ppo2.py:185][0m |          -0.0005 |          16.9821 |          13.9964 |
[32m[20221213 20:56:31 @agent_ppo2.py:185][0m |          -0.0046 |          16.9873 |          13.9926 |
[32m[20221213 20:56:31 @agent_ppo2.py:185][0m |          -0.0086 |          17.1559 |          13.9965 |
[32m[20221213 20:56:31 @agent_ppo2.py:185][0m |          -0.0032 |          17.0212 |          13.9954 |
[32m[20221213 20:56:31 @agent_ppo2.py:185][0m |          -0.0042 |          16.8828 |          13.9998 |
[32m[20221213 20:56:32 @agent_ppo2.py:185][0m |          -0.0029 |          17.1054 |          13.9985 |
[32m[20221213 20:56:32 @agent_ppo2.py:185][0m |           0.0073 |          17.8358 |          14.0013 |
[32m[20221213 20:56:32 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 20:56:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.20
[32m[20221213 20:56:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.00
[32m[20221213 20:56:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 340.00
[32m[20221213 20:56:32 @agent_ppo2.py:143][0m Total time:       0.95 min
[32m[20221213 20:56:32 @agent_ppo2.py:145][0m 92160 total steps have happened
[32m[20221213 20:56:32 @agent_ppo2.py:121][0m #------------------------ Iteration 45 --------------------------#
[32m[20221213 20:56:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:32 @agent_ppo2.py:185][0m |          -0.0019 |          16.9882 |          14.0934 |
[32m[20221213 20:56:32 @agent_ppo2.py:185][0m |           0.0015 |          17.0483 |          14.0909 |
[32m[20221213 20:56:32 @agent_ppo2.py:185][0m |          -0.0023 |          16.7650 |          14.0832 |
[32m[20221213 20:56:32 @agent_ppo2.py:185][0m |           0.0002 |          16.7561 |          14.0822 |
[32m[20221213 20:56:32 @agent_ppo2.py:185][0m |          -0.0053 |          16.7101 |          14.0821 |
[32m[20221213 20:56:32 @agent_ppo2.py:185][0m |          -0.0071 |          16.9572 |          14.0842 |
[32m[20221213 20:56:33 @agent_ppo2.py:185][0m |          -0.0034 |          16.7187 |          14.0854 |
[32m[20221213 20:56:33 @agent_ppo2.py:185][0m |          -0.0069 |          16.6674 |          14.0864 |
[32m[20221213 20:56:33 @agent_ppo2.py:185][0m |          -0.0101 |          16.6984 |          14.0887 |
[32m[20221213 20:56:33 @agent_ppo2.py:185][0m |          -0.0044 |          16.6388 |          14.0943 |
[32m[20221213 20:56:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:56:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 227.00
[32m[20221213 20:56:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.00
[32m[20221213 20:56:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.00
[32m[20221213 20:56:33 @agent_ppo2.py:143][0m Total time:       0.97 min
[32m[20221213 20:56:33 @agent_ppo2.py:145][0m 94208 total steps have happened
[32m[20221213 20:56:33 @agent_ppo2.py:121][0m #------------------------ Iteration 46 --------------------------#
[32m[20221213 20:56:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:33 @agent_ppo2.py:185][0m |           0.0011 |          20.9971 |          14.0977 |
[32m[20221213 20:56:33 @agent_ppo2.py:185][0m |          -0.0050 |          20.4537 |          14.0875 |
[32m[20221213 20:56:33 @agent_ppo2.py:185][0m |           0.0028 |          20.8000 |          14.0789 |
[32m[20221213 20:56:34 @agent_ppo2.py:185][0m |           0.0003 |          20.1632 |          14.0765 |
[32m[20221213 20:56:34 @agent_ppo2.py:185][0m |           0.0020 |          20.1044 |          14.0699 |
[32m[20221213 20:56:34 @agent_ppo2.py:185][0m |           0.0029 |          20.3076 |          14.0695 |
[32m[20221213 20:56:34 @agent_ppo2.py:185][0m |          -0.0011 |          19.9468 |          14.0638 |
[32m[20221213 20:56:34 @agent_ppo2.py:185][0m |          -0.0037 |          19.8624 |          14.0663 |
[32m[20221213 20:56:34 @agent_ppo2.py:185][0m |          -0.0069 |          19.9234 |          14.0662 |
[32m[20221213 20:56:34 @agent_ppo2.py:185][0m |           0.0018 |          20.4110 |          14.0645 |
[32m[20221213 20:56:34 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:56:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.60
[32m[20221213 20:56:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.00
[32m[20221213 20:56:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.00
[32m[20221213 20:56:34 @agent_ppo2.py:143][0m Total time:       0.99 min
[32m[20221213 20:56:34 @agent_ppo2.py:145][0m 96256 total steps have happened
[32m[20221213 20:56:34 @agent_ppo2.py:121][0m #------------------------ Iteration 47 --------------------------#
[32m[20221213 20:56:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:34 @agent_ppo2.py:185][0m |          -0.0027 |          21.0524 |          14.1075 |
[32m[20221213 20:56:35 @agent_ppo2.py:185][0m |          -0.0052 |          20.6739 |          14.0927 |
[32m[20221213 20:56:35 @agent_ppo2.py:185][0m |          -0.0051 |          20.5612 |          14.0876 |
[32m[20221213 20:56:35 @agent_ppo2.py:185][0m |          -0.0041 |          20.4513 |          14.0865 |
[32m[20221213 20:56:35 @agent_ppo2.py:185][0m |          -0.0072 |          20.3732 |          14.0816 |
[32m[20221213 20:56:35 @agent_ppo2.py:185][0m |          -0.0090 |          20.3554 |          14.0853 |
[32m[20221213 20:56:35 @agent_ppo2.py:185][0m |          -0.0044 |          20.4346 |          14.0834 |
[32m[20221213 20:56:35 @agent_ppo2.py:185][0m |          -0.0073 |          20.3070 |          14.0828 |
[32m[20221213 20:56:35 @agent_ppo2.py:185][0m |          -0.0050 |          20.2415 |          14.0892 |
[32m[20221213 20:56:35 @agent_ppo2.py:185][0m |          -0.0080 |          20.1157 |          14.0857 |
[32m[20221213 20:56:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:56:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.20
[32m[20221213 20:56:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.00
[32m[20221213 20:56:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 351.00
[32m[20221213 20:56:35 @agent_ppo2.py:143][0m Total time:       1.01 min
[32m[20221213 20:56:35 @agent_ppo2.py:145][0m 98304 total steps have happened
[32m[20221213 20:56:35 @agent_ppo2.py:121][0m #------------------------ Iteration 48 --------------------------#
[32m[20221213 20:56:35 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:56:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:36 @agent_ppo2.py:185][0m |          -0.0008 |          20.5642 |          14.0630 |
[32m[20221213 20:56:36 @agent_ppo2.py:185][0m |          -0.0035 |          20.4405 |          14.0485 |
[32m[20221213 20:56:36 @agent_ppo2.py:185][0m |          -0.0015 |          20.2654 |          14.0504 |
[32m[20221213 20:56:36 @agent_ppo2.py:185][0m |          -0.0122 |          20.3213 |          14.0443 |
[32m[20221213 20:56:36 @agent_ppo2.py:185][0m |          -0.0065 |          20.1353 |          14.0463 |
[32m[20221213 20:56:36 @agent_ppo2.py:185][0m |          -0.0047 |          20.0958 |          14.0460 |
[32m[20221213 20:56:36 @agent_ppo2.py:185][0m |          -0.0033 |          19.9822 |          14.0410 |
[32m[20221213 20:56:36 @agent_ppo2.py:185][0m |          -0.0023 |          20.0388 |          14.0394 |
[32m[20221213 20:56:36 @agent_ppo2.py:185][0m |          -0.0054 |          20.0638 |          14.0467 |
[32m[20221213 20:56:37 @agent_ppo2.py:185][0m |          -0.0060 |          19.9353 |          14.0440 |
[32m[20221213 20:56:37 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 20:56:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.00
[32m[20221213 20:56:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.00
[32m[20221213 20:56:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.00
[32m[20221213 20:56:37 @agent_ppo2.py:143][0m Total time:       1.03 min
[32m[20221213 20:56:37 @agent_ppo2.py:145][0m 100352 total steps have happened
[32m[20221213 20:56:37 @agent_ppo2.py:121][0m #------------------------ Iteration 49 --------------------------#
[32m[20221213 20:56:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 20:56:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:37 @agent_ppo2.py:185][0m |           0.0002 |          21.5036 |          14.2000 |
[32m[20221213 20:56:37 @agent_ppo2.py:185][0m |          -0.0035 |          21.1975 |          14.1958 |
[32m[20221213 20:56:37 @agent_ppo2.py:185][0m |          -0.0076 |          21.1588 |          14.1934 |
[32m[20221213 20:56:37 @agent_ppo2.py:185][0m |          -0.0091 |          21.2458 |          14.1908 |
[32m[20221213 20:56:37 @agent_ppo2.py:185][0m |          -0.0008 |          21.0370 |          14.1907 |
[32m[20221213 20:56:38 @agent_ppo2.py:185][0m |           0.0009 |          21.4345 |          14.1968 |
[32m[20221213 20:56:38 @agent_ppo2.py:185][0m |          -0.0059 |          21.0686 |          14.1978 |
[32m[20221213 20:56:38 @agent_ppo2.py:185][0m |          -0.0078 |          21.0125 |          14.1985 |
[32m[20221213 20:56:38 @agent_ppo2.py:185][0m |          -0.0039 |          20.9683 |          14.2006 |
[32m[20221213 20:56:38 @agent_ppo2.py:185][0m |          -0.0037 |          20.9273 |          14.2034 |
[32m[20221213 20:56:38 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:56:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.40
[32m[20221213 20:56:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 324.00
[32m[20221213 20:56:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 346.00
[32m[20221213 20:56:38 @agent_ppo2.py:143][0m Total time:       1.06 min
[32m[20221213 20:56:38 @agent_ppo2.py:145][0m 102400 total steps have happened
[32m[20221213 20:56:38 @agent_ppo2.py:121][0m #------------------------ Iteration 50 --------------------------#
[32m[20221213 20:56:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:38 @agent_ppo2.py:185][0m |           0.0056 |          22.5839 |          14.1023 |
[32m[20221213 20:56:38 @agent_ppo2.py:185][0m |          -0.0055 |          21.9527 |          14.0929 |
[32m[20221213 20:56:38 @agent_ppo2.py:185][0m |          -0.0072 |          21.9403 |          14.0941 |
[32m[20221213 20:56:39 @agent_ppo2.py:185][0m |          -0.0088 |          21.8776 |          14.0871 |
[32m[20221213 20:56:39 @agent_ppo2.py:185][0m |          -0.0037 |          21.7628 |          14.0895 |
[32m[20221213 20:56:39 @agent_ppo2.py:185][0m |          -0.0077 |          21.6877 |          14.0887 |
[32m[20221213 20:56:39 @agent_ppo2.py:185][0m |          -0.0034 |          21.7477 |          14.0906 |
[32m[20221213 20:56:39 @agent_ppo2.py:185][0m |          -0.0052 |          21.7054 |          14.0829 |
[32m[20221213 20:56:39 @agent_ppo2.py:185][0m |          -0.0055 |          21.6106 |          14.0896 |
[32m[20221213 20:56:39 @agent_ppo2.py:185][0m |          -0.0049 |          21.5807 |          14.0883 |
[32m[20221213 20:56:39 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:56:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.40
[32m[20221213 20:56:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 332.00
[32m[20221213 20:56:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.00
[32m[20221213 20:56:39 @agent_ppo2.py:143][0m Total time:       1.08 min
[32m[20221213 20:56:39 @agent_ppo2.py:145][0m 104448 total steps have happened
[32m[20221213 20:56:39 @agent_ppo2.py:121][0m #------------------------ Iteration 51 --------------------------#
[32m[20221213 20:56:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:40 @agent_ppo2.py:185][0m |          -0.0044 |          23.2982 |          14.2502 |
[32m[20221213 20:56:40 @agent_ppo2.py:185][0m |          -0.0068 |          23.1842 |          14.2410 |
[32m[20221213 20:56:40 @agent_ppo2.py:185][0m |          -0.0011 |          22.9655 |          14.2335 |
[32m[20221213 20:56:40 @agent_ppo2.py:185][0m |          -0.0034 |          22.8960 |          14.2271 |
[32m[20221213 20:56:40 @agent_ppo2.py:185][0m |           0.0006 |          23.7696 |          14.2240 |
[32m[20221213 20:56:40 @agent_ppo2.py:185][0m |          -0.0055 |          22.8111 |          14.2219 |
[32m[20221213 20:56:40 @agent_ppo2.py:185][0m |          -0.0044 |          22.7125 |          14.2222 |
[32m[20221213 20:56:40 @agent_ppo2.py:185][0m |          -0.0000 |          23.6318 |          14.2250 |
[32m[20221213 20:56:40 @agent_ppo2.py:185][0m |          -0.0071 |          22.6960 |          14.2232 |
[32m[20221213 20:56:40 @agent_ppo2.py:185][0m |          -0.0081 |          22.5727 |          14.2231 |
[32m[20221213 20:56:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:56:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.60
[32m[20221213 20:56:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 308.00
[32m[20221213 20:56:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.00
[32m[20221213 20:56:40 @agent_ppo2.py:143][0m Total time:       1.10 min
[32m[20221213 20:56:40 @agent_ppo2.py:145][0m 106496 total steps have happened
[32m[20221213 20:56:40 @agent_ppo2.py:121][0m #------------------------ Iteration 52 --------------------------#
[32m[20221213 20:56:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:56:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:41 @agent_ppo2.py:185][0m |           0.0002 |          23.2817 |          14.3913 |
[32m[20221213 20:56:41 @agent_ppo2.py:185][0m |          -0.0019 |          23.1080 |          14.3834 |
[32m[20221213 20:56:41 @agent_ppo2.py:185][0m |          -0.0023 |          22.9143 |          14.3764 |
[32m[20221213 20:56:41 @agent_ppo2.py:185][0m |          -0.0018 |          22.8633 |          14.3753 |
[32m[20221213 20:56:41 @agent_ppo2.py:185][0m |          -0.0028 |          22.7577 |          14.3740 |
[32m[20221213 20:56:41 @agent_ppo2.py:185][0m |           0.0040 |          24.3339 |          14.3706 |
[32m[20221213 20:56:41 @agent_ppo2.py:185][0m |          -0.0035 |          22.7388 |          14.3682 |
[32m[20221213 20:56:41 @agent_ppo2.py:185][0m |           0.0181 |          25.6797 |          14.3749 |
[32m[20221213 20:56:42 @agent_ppo2.py:185][0m |           0.0019 |          23.3245 |          14.3656 |
[32m[20221213 20:56:42 @agent_ppo2.py:185][0m |          -0.0051 |          22.6302 |          14.3688 |
[32m[20221213 20:56:42 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:56:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.60
[32m[20221213 20:56:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 313.00
[32m[20221213 20:56:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.00
[32m[20221213 20:56:42 @agent_ppo2.py:143][0m Total time:       1.12 min
[32m[20221213 20:56:42 @agent_ppo2.py:145][0m 108544 total steps have happened
[32m[20221213 20:56:42 @agent_ppo2.py:121][0m #------------------------ Iteration 53 --------------------------#
[32m[20221213 20:56:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:42 @agent_ppo2.py:185][0m |          -0.0009 |          23.2771 |          14.3795 |
[32m[20221213 20:56:42 @agent_ppo2.py:185][0m |          -0.0092 |          22.9100 |          14.3647 |
[32m[20221213 20:56:42 @agent_ppo2.py:185][0m |          -0.0019 |          23.2880 |          14.3568 |
[32m[20221213 20:56:42 @agent_ppo2.py:185][0m |          -0.0089 |          22.7206 |          14.3525 |
[32m[20221213 20:56:42 @agent_ppo2.py:185][0m |          -0.0048 |          22.6840 |          14.3431 |
[32m[20221213 20:56:42 @agent_ppo2.py:185][0m |          -0.0069 |          22.6739 |          14.3478 |
[32m[20221213 20:56:43 @agent_ppo2.py:185][0m |          -0.0131 |          22.6972 |          14.3493 |
[32m[20221213 20:56:43 @agent_ppo2.py:185][0m |          -0.0065 |          22.5944 |          14.3505 |
[32m[20221213 20:56:43 @agent_ppo2.py:185][0m |          -0.0046 |          22.4953 |          14.3465 |
[32m[20221213 20:56:43 @agent_ppo2.py:185][0m |          -0.0053 |          22.5552 |          14.3477 |
[32m[20221213 20:56:43 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:56:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.00
[32m[20221213 20:56:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.00
[32m[20221213 20:56:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.00
[32m[20221213 20:56:43 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 393.00
[32m[20221213 20:56:43 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 393.00
[32m[20221213 20:56:43 @agent_ppo2.py:143][0m Total time:       1.14 min
[32m[20221213 20:56:43 @agent_ppo2.py:145][0m 110592 total steps have happened
[32m[20221213 20:56:43 @agent_ppo2.py:121][0m #------------------------ Iteration 54 --------------------------#
[32m[20221213 20:56:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:43 @agent_ppo2.py:185][0m |          -0.0001 |          22.9905 |          14.3203 |
[32m[20221213 20:56:43 @agent_ppo2.py:185][0m |           0.0015 |          22.6123 |          14.3170 |
[32m[20221213 20:56:43 @agent_ppo2.py:185][0m |          -0.0052 |          22.4371 |          14.3083 |
[32m[20221213 20:56:44 @agent_ppo2.py:185][0m |          -0.0041 |          22.3586 |          14.3038 |
[32m[20221213 20:56:44 @agent_ppo2.py:185][0m |          -0.0010 |          22.2782 |          14.3016 |
[32m[20221213 20:56:44 @agent_ppo2.py:185][0m |          -0.0063 |          22.3418 |          14.3009 |
[32m[20221213 20:56:44 @agent_ppo2.py:185][0m |          -0.0033 |          22.3090 |          14.2977 |
[32m[20221213 20:56:44 @agent_ppo2.py:185][0m |          -0.0054 |          22.2093 |          14.2945 |
[32m[20221213 20:56:44 @agent_ppo2.py:185][0m |          -0.0031 |          22.3244 |          14.2949 |
[32m[20221213 20:56:44 @agent_ppo2.py:185][0m |          -0.0055 |          22.1439 |          14.2918 |
[32m[20221213 20:56:44 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:56:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.80
[32m[20221213 20:56:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.00
[32m[20221213 20:56:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.00
[32m[20221213 20:56:44 @agent_ppo2.py:143][0m Total time:       1.16 min
[32m[20221213 20:56:44 @agent_ppo2.py:145][0m 112640 total steps have happened
[32m[20221213 20:56:44 @agent_ppo2.py:121][0m #------------------------ Iteration 55 --------------------------#
[32m[20221213 20:56:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:45 @agent_ppo2.py:185][0m |           0.0099 |          28.6046 |          14.3938 |
[32m[20221213 20:56:45 @agent_ppo2.py:185][0m |          -0.0046 |          25.9523 |          14.3854 |
[32m[20221213 20:56:45 @agent_ppo2.py:185][0m |          -0.0060 |          25.8653 |          14.3778 |
[32m[20221213 20:56:45 @agent_ppo2.py:185][0m |          -0.0037 |          25.8521 |          14.3792 |
[32m[20221213 20:56:45 @agent_ppo2.py:185][0m |          -0.0060 |          25.8949 |          14.3800 |
[32m[20221213 20:56:45 @agent_ppo2.py:185][0m |          -0.0051 |          25.8265 |          14.3767 |
[32m[20221213 20:56:45 @agent_ppo2.py:185][0m |          -0.0046 |          25.7431 |          14.3800 |
[32m[20221213 20:56:45 @agent_ppo2.py:185][0m |          -0.0049 |          25.6894 |          14.3785 |
[32m[20221213 20:56:45 @agent_ppo2.py:185][0m |          -0.0056 |          25.6668 |          14.3852 |
[32m[20221213 20:56:45 @agent_ppo2.py:185][0m |          -0.0045 |          25.7833 |          14.3839 |
[32m[20221213 20:56:45 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:56:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.40
[32m[20221213 20:56:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 322.00
[32m[20221213 20:56:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.00
[32m[20221213 20:56:45 @agent_ppo2.py:143][0m Total time:       1.18 min
[32m[20221213 20:56:45 @agent_ppo2.py:145][0m 114688 total steps have happened
[32m[20221213 20:56:45 @agent_ppo2.py:121][0m #------------------------ Iteration 56 --------------------------#
[32m[20221213 20:56:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:46 @agent_ppo2.py:185][0m |           0.0006 |          28.2150 |          14.3587 |
[32m[20221213 20:56:46 @agent_ppo2.py:185][0m |          -0.0026 |          27.7790 |          14.3553 |
[32m[20221213 20:56:46 @agent_ppo2.py:185][0m |          -0.0080 |          27.6366 |          14.3501 |
[32m[20221213 20:56:46 @agent_ppo2.py:185][0m |          -0.0085 |          27.6461 |          14.3490 |
[32m[20221213 20:56:46 @agent_ppo2.py:185][0m |          -0.0080 |          27.5308 |          14.3479 |
[32m[20221213 20:56:46 @agent_ppo2.py:185][0m |          -0.0024 |          27.3755 |          14.3500 |
[32m[20221213 20:56:46 @agent_ppo2.py:185][0m |          -0.0072 |          27.4157 |          14.3474 |
[32m[20221213 20:56:46 @agent_ppo2.py:185][0m |          -0.0009 |          27.3787 |          14.3494 |
[32m[20221213 20:56:46 @agent_ppo2.py:185][0m |          -0.0071 |          27.3530 |          14.3478 |
[32m[20221213 20:56:47 @agent_ppo2.py:185][0m |          -0.0049 |          27.2245 |          14.3466 |
[32m[20221213 20:56:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:56:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 297.60
[32m[20221213 20:56:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 331.00
[32m[20221213 20:56:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.00
[32m[20221213 20:56:47 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 415.00
[32m[20221213 20:56:47 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 415.00
[32m[20221213 20:56:47 @agent_ppo2.py:143][0m Total time:       1.20 min
[32m[20221213 20:56:47 @agent_ppo2.py:145][0m 116736 total steps have happened
[32m[20221213 20:56:47 @agent_ppo2.py:121][0m #------------------------ Iteration 57 --------------------------#
[32m[20221213 20:56:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:47 @agent_ppo2.py:185][0m |           0.0014 |          26.8250 |          14.4289 |
[32m[20221213 20:56:47 @agent_ppo2.py:185][0m |           0.0012 |          26.4856 |          14.4229 |
[32m[20221213 20:56:47 @agent_ppo2.py:185][0m |          -0.0064 |          26.4669 |          14.4141 |
[32m[20221213 20:56:47 @agent_ppo2.py:185][0m |          -0.0059 |          26.2489 |          14.4110 |
[32m[20221213 20:56:47 @agent_ppo2.py:185][0m |          -0.0054 |          26.2336 |          14.4122 |
[32m[20221213 20:56:47 @agent_ppo2.py:185][0m |          -0.0021 |          26.2216 |          14.4119 |
[32m[20221213 20:56:47 @agent_ppo2.py:185][0m |          -0.0018 |          26.1099 |          14.4130 |
[32m[20221213 20:56:48 @agent_ppo2.py:185][0m |          -0.0067 |          26.0720 |          14.4121 |
[32m[20221213 20:56:48 @agent_ppo2.py:185][0m |          -0.0026 |          26.0820 |          14.4157 |
[32m[20221213 20:56:48 @agent_ppo2.py:185][0m |          -0.0033 |          25.8911 |          14.4110 |
[32m[20221213 20:56:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:56:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.40
[32m[20221213 20:56:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 325.00
[32m[20221213 20:56:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 365.00
[32m[20221213 20:56:48 @agent_ppo2.py:143][0m Total time:       1.22 min
[32m[20221213 20:56:48 @agent_ppo2.py:145][0m 118784 total steps have happened
[32m[20221213 20:56:48 @agent_ppo2.py:121][0m #------------------------ Iteration 58 --------------------------#
[32m[20221213 20:56:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:48 @agent_ppo2.py:185][0m |          -0.0026 |          27.6607 |          14.4801 |
[32m[20221213 20:56:48 @agent_ppo2.py:185][0m |          -0.0018 |          27.2236 |          14.4673 |
[32m[20221213 20:56:48 @agent_ppo2.py:185][0m |          -0.0045 |          27.2164 |          14.4545 |
[32m[20221213 20:56:48 @agent_ppo2.py:185][0m |          -0.0106 |          27.2040 |          14.4607 |
[32m[20221213 20:56:48 @agent_ppo2.py:185][0m |          -0.0048 |          27.4289 |          14.4628 |
[32m[20221213 20:56:49 @agent_ppo2.py:185][0m |          -0.0048 |          26.9505 |          14.4561 |
[32m[20221213 20:56:49 @agent_ppo2.py:185][0m |          -0.0078 |          26.9220 |          14.4651 |
[32m[20221213 20:56:49 @agent_ppo2.py:185][0m |          -0.0094 |          26.8959 |          14.4670 |
[32m[20221213 20:56:49 @agent_ppo2.py:185][0m |          -0.0042 |          26.9359 |          14.4721 |
[32m[20221213 20:56:49 @agent_ppo2.py:185][0m |          -0.0094 |          26.8550 |          14.4701 |
[32m[20221213 20:56:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:56:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 318.80
[32m[20221213 20:56:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 324.00
[32m[20221213 20:56:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 375.00
[32m[20221213 20:56:49 @agent_ppo2.py:143][0m Total time:       1.24 min
[32m[20221213 20:56:49 @agent_ppo2.py:145][0m 120832 total steps have happened
[32m[20221213 20:56:49 @agent_ppo2.py:121][0m #------------------------ Iteration 59 --------------------------#
[32m[20221213 20:56:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:49 @agent_ppo2.py:185][0m |           0.0015 |          29.2068 |          14.5280 |
[32m[20221213 20:56:49 @agent_ppo2.py:185][0m |           0.0001 |          28.8917 |          14.5219 |
[32m[20221213 20:56:50 @agent_ppo2.py:185][0m |          -0.0053 |          28.7765 |          14.5133 |
[32m[20221213 20:56:50 @agent_ppo2.py:185][0m |          -0.0015 |          28.7585 |          14.5158 |
[32m[20221213 20:56:50 @agent_ppo2.py:185][0m |          -0.0086 |          28.7078 |          14.5147 |
[32m[20221213 20:56:50 @agent_ppo2.py:185][0m |          -0.0072 |          28.7489 |          14.5162 |
[32m[20221213 20:56:50 @agent_ppo2.py:185][0m |          -0.0079 |          28.6699 |          14.5148 |
[32m[20221213 20:56:50 @agent_ppo2.py:185][0m |          -0.0058 |          28.6017 |          14.5157 |
[32m[20221213 20:56:50 @agent_ppo2.py:185][0m |          -0.0049 |          28.6333 |          14.5149 |
[32m[20221213 20:56:50 @agent_ppo2.py:185][0m |          -0.0074 |          28.5448 |          14.5158 |
[32m[20221213 20:56:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:56:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.00
[32m[20221213 20:56:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 359.00
[32m[20221213 20:56:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.00
[32m[20221213 20:56:50 @agent_ppo2.py:143][0m Total time:       1.26 min
[32m[20221213 20:56:50 @agent_ppo2.py:145][0m 122880 total steps have happened
[32m[20221213 20:56:50 @agent_ppo2.py:121][0m #------------------------ Iteration 60 --------------------------#
[32m[20221213 20:56:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:51 @agent_ppo2.py:185][0m |          -0.0034 |          30.4055 |          14.5850 |
[32m[20221213 20:56:51 @agent_ppo2.py:185][0m |          -0.0024 |          30.1261 |          14.5757 |
[32m[20221213 20:56:51 @agent_ppo2.py:185][0m |          -0.0044 |          29.8764 |          14.5711 |
[32m[20221213 20:56:51 @agent_ppo2.py:185][0m |          -0.0060 |          29.7437 |          14.5685 |
[32m[20221213 20:56:51 @agent_ppo2.py:185][0m |          -0.0058 |          29.7855 |          14.5729 |
[32m[20221213 20:56:51 @agent_ppo2.py:185][0m |          -0.0098 |          29.7533 |          14.5716 |
[32m[20221213 20:56:51 @agent_ppo2.py:185][0m |          -0.0042 |          29.5455 |          14.5688 |
[32m[20221213 20:56:51 @agent_ppo2.py:185][0m |          -0.0065 |          29.5130 |          14.5722 |
[32m[20221213 20:56:51 @agent_ppo2.py:185][0m |          -0.0073 |          29.3895 |          14.5729 |
[32m[20221213 20:56:51 @agent_ppo2.py:185][0m |          -0.0060 |          29.5789 |          14.5738 |
[32m[20221213 20:56:51 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:56:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.00
[32m[20221213 20:56:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.00
[32m[20221213 20:56:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 359.00
[32m[20221213 20:56:51 @agent_ppo2.py:143][0m Total time:       1.28 min
[32m[20221213 20:56:51 @agent_ppo2.py:145][0m 124928 total steps have happened
[32m[20221213 20:56:51 @agent_ppo2.py:121][0m #------------------------ Iteration 61 --------------------------#
[32m[20221213 20:56:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:52 @agent_ppo2.py:185][0m |          -0.0024 |          27.9818 |          14.6904 |
[32m[20221213 20:56:52 @agent_ppo2.py:185][0m |          -0.0044 |          27.5006 |          14.6920 |
[32m[20221213 20:56:52 @agent_ppo2.py:185][0m |          -0.0058 |          27.4019 |          14.6889 |
[32m[20221213 20:56:52 @agent_ppo2.py:185][0m |          -0.0045 |          27.4586 |          14.6863 |
[32m[20221213 20:56:52 @agent_ppo2.py:185][0m |           0.0136 |          29.4230 |          14.6967 |
[32m[20221213 20:56:52 @agent_ppo2.py:185][0m |           0.0010 |          28.6365 |          14.6904 |
[32m[20221213 20:56:52 @agent_ppo2.py:185][0m |          -0.0037 |          27.2059 |          14.6930 |
[32m[20221213 20:56:52 @agent_ppo2.py:185][0m |          -0.0035 |          27.2349 |          14.6902 |
[32m[20221213 20:56:52 @agent_ppo2.py:185][0m |          -0.0014 |          27.4229 |          14.6921 |
[32m[20221213 20:56:52 @agent_ppo2.py:185][0m |          -0.0076 |          27.1607 |          14.6936 |
[32m[20221213 20:56:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:56:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.60
[32m[20221213 20:56:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 362.00
[32m[20221213 20:56:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.00
[32m[20221213 20:56:53 @agent_ppo2.py:143][0m Total time:       1.30 min
[32m[20221213 20:56:53 @agent_ppo2.py:145][0m 126976 total steps have happened
[32m[20221213 20:56:53 @agent_ppo2.py:121][0m #------------------------ Iteration 62 --------------------------#
[32m[20221213 20:56:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 20:56:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:53 @agent_ppo2.py:185][0m |          -0.0012 |          29.5978 |          14.7875 |
[32m[20221213 20:56:53 @agent_ppo2.py:185][0m |           0.0073 |          31.3318 |          14.7826 |
[32m[20221213 20:56:53 @agent_ppo2.py:185][0m |          -0.0023 |          29.2696 |          14.7808 |
[32m[20221213 20:56:53 @agent_ppo2.py:185][0m |          -0.0052 |          29.3745 |          14.7745 |
[32m[20221213 20:56:53 @agent_ppo2.py:185][0m |           0.0056 |          31.0246 |          14.7781 |
[32m[20221213 20:56:53 @agent_ppo2.py:185][0m |          -0.0049 |          29.2201 |          14.7799 |
[32m[20221213 20:56:53 @agent_ppo2.py:185][0m |          -0.0063 |          29.1545 |          14.7847 |
[32m[20221213 20:56:53 @agent_ppo2.py:185][0m |          -0.0081 |          29.1695 |          14.7817 |
[32m[20221213 20:56:54 @agent_ppo2.py:185][0m |          -0.0065 |          29.1634 |          14.7823 |
[32m[20221213 20:56:54 @agent_ppo2.py:185][0m |          -0.0079 |          29.1103 |          14.7824 |
[32m[20221213 20:56:54 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:56:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.40
[32m[20221213 20:56:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.00
[32m[20221213 20:56:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.00
[32m[20221213 20:56:54 @agent_ppo2.py:143][0m Total time:       1.32 min
[32m[20221213 20:56:54 @agent_ppo2.py:145][0m 129024 total steps have happened
[32m[20221213 20:56:54 @agent_ppo2.py:121][0m #------------------------ Iteration 63 --------------------------#
[32m[20221213 20:56:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:54 @agent_ppo2.py:185][0m |          -0.0029 |          31.7245 |          14.8370 |
[32m[20221213 20:56:54 @agent_ppo2.py:185][0m |          -0.0046 |          31.3510 |          14.8207 |
[32m[20221213 20:56:54 @agent_ppo2.py:185][0m |          -0.0051 |          30.9606 |          14.8072 |
[32m[20221213 20:56:54 @agent_ppo2.py:185][0m |          -0.0060 |          30.8166 |          14.8042 |
[32m[20221213 20:56:54 @agent_ppo2.py:185][0m |          -0.0069 |          30.6277 |          14.8062 |
[32m[20221213 20:56:55 @agent_ppo2.py:185][0m |          -0.0113 |          30.7145 |          14.7952 |
[32m[20221213 20:56:55 @agent_ppo2.py:185][0m |          -0.0076 |          30.5294 |          14.8019 |
[32m[20221213 20:56:55 @agent_ppo2.py:185][0m |          -0.0064 |          30.4685 |          14.8011 |
[32m[20221213 20:56:55 @agent_ppo2.py:185][0m |          -0.0038 |          30.4201 |          14.8041 |
[32m[20221213 20:56:55 @agent_ppo2.py:185][0m |          -0.0071 |          30.3628 |          14.8046 |
[32m[20221213 20:56:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:56:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 345.00
[32m[20221213 20:56:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.00
[32m[20221213 20:56:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 383.00
[32m[20221213 20:56:55 @agent_ppo2.py:143][0m Total time:       1.34 min
[32m[20221213 20:56:55 @agent_ppo2.py:145][0m 131072 total steps have happened
[32m[20221213 20:56:55 @agent_ppo2.py:121][0m #------------------------ Iteration 64 --------------------------#
[32m[20221213 20:56:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:56:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:55 @agent_ppo2.py:185][0m |          -0.0032 |          34.8529 |          14.8814 |
[32m[20221213 20:56:55 @agent_ppo2.py:185][0m |           0.0014 |          34.2351 |          14.8648 |
[32m[20221213 20:56:55 @agent_ppo2.py:185][0m |           0.0020 |          34.4298 |          14.8661 |
[32m[20221213 20:56:56 @agent_ppo2.py:185][0m |          -0.0031 |          33.8921 |          14.8659 |
[32m[20221213 20:56:56 @agent_ppo2.py:185][0m |          -0.0052 |          33.7038 |          14.8647 |
[32m[20221213 20:56:56 @agent_ppo2.py:185][0m |          -0.0050 |          33.6754 |          14.8600 |
[32m[20221213 20:56:56 @agent_ppo2.py:185][0m |          -0.0029 |          33.6088 |          14.8623 |
[32m[20221213 20:56:56 @agent_ppo2.py:185][0m |          -0.0051 |          33.4394 |          14.8624 |
[32m[20221213 20:56:56 @agent_ppo2.py:185][0m |          -0.0069 |          33.4406 |          14.8622 |
[32m[20221213 20:56:56 @agent_ppo2.py:185][0m |          -0.0062 |          33.3095 |          14.8594 |
[32m[20221213 20:56:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:56:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.00
[32m[20221213 20:56:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 401.00
[32m[20221213 20:56:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.00
[32m[20221213 20:56:56 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 428.00
[32m[20221213 20:56:56 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 428.00
[32m[20221213 20:56:56 @agent_ppo2.py:143][0m Total time:       1.36 min
[32m[20221213 20:56:56 @agent_ppo2.py:145][0m 133120 total steps have happened
[32m[20221213 20:56:56 @agent_ppo2.py:121][0m #------------------------ Iteration 65 --------------------------#
[32m[20221213 20:56:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 20:56:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:56 @agent_ppo2.py:185][0m |          -0.0017 |          36.3309 |          14.7970 |
[32m[20221213 20:56:57 @agent_ppo2.py:185][0m |           0.0026 |          36.2531 |          14.7938 |
[32m[20221213 20:56:57 @agent_ppo2.py:185][0m |          -0.0018 |          35.6329 |          14.7904 |
[32m[20221213 20:56:57 @agent_ppo2.py:185][0m |          -0.0030 |          35.3367 |          14.7904 |
[32m[20221213 20:56:57 @agent_ppo2.py:185][0m |           0.0046 |          35.6387 |          14.7891 |
[32m[20221213 20:56:57 @agent_ppo2.py:185][0m |          -0.0032 |          35.2533 |          14.7901 |
[32m[20221213 20:56:57 @agent_ppo2.py:185][0m |          -0.0032 |          35.0459 |          14.7888 |
[32m[20221213 20:56:57 @agent_ppo2.py:185][0m |           0.0112 |          38.4854 |          14.7881 |
[32m[20221213 20:56:57 @agent_ppo2.py:185][0m |          -0.0003 |          35.0587 |          14.7783 |
[32m[20221213 20:56:57 @agent_ppo2.py:185][0m |          -0.0070 |          34.8817 |          14.7872 |
[32m[20221213 20:56:57 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:56:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.40
[32m[20221213 20:56:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.00
[32m[20221213 20:56:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.00
[32m[20221213 20:56:57 @agent_ppo2.py:143][0m Total time:       1.38 min
[32m[20221213 20:56:57 @agent_ppo2.py:145][0m 135168 total steps have happened
[32m[20221213 20:56:57 @agent_ppo2.py:121][0m #------------------------ Iteration 66 --------------------------#
[32m[20221213 20:56:57 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:56:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:58 @agent_ppo2.py:185][0m |          -0.0035 |          36.6008 |          14.8911 |
[32m[20221213 20:56:58 @agent_ppo2.py:185][0m |          -0.0058 |          35.9311 |          14.8752 |
[32m[20221213 20:56:58 @agent_ppo2.py:185][0m |           0.0039 |          37.7717 |          14.8641 |
[32m[20221213 20:56:58 @agent_ppo2.py:185][0m |          -0.0033 |          35.4481 |          14.8628 |
[32m[20221213 20:56:58 @agent_ppo2.py:185][0m |          -0.0062 |          35.3042 |          14.8622 |
[32m[20221213 20:56:58 @agent_ppo2.py:185][0m |           0.0063 |          38.4621 |          14.8650 |
[32m[20221213 20:56:58 @agent_ppo2.py:185][0m |          -0.0032 |          35.3151 |          14.8586 |
[32m[20221213 20:56:58 @agent_ppo2.py:185][0m |          -0.0065 |          34.9625 |          14.8619 |
[32m[20221213 20:56:58 @agent_ppo2.py:185][0m |          -0.0089 |          35.0052 |          14.8579 |
[32m[20221213 20:56:58 @agent_ppo2.py:185][0m |          -0.0031 |          35.3013 |          14.8622 |
[32m[20221213 20:56:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:56:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.20
[32m[20221213 20:56:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 379.00
[32m[20221213 20:56:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.00
[32m[20221213 20:56:59 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 440.00
[32m[20221213 20:56:59 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 440.00
[32m[20221213 20:56:59 @agent_ppo2.py:143][0m Total time:       1.40 min
[32m[20221213 20:56:59 @agent_ppo2.py:145][0m 137216 total steps have happened
[32m[20221213 20:56:59 @agent_ppo2.py:121][0m #------------------------ Iteration 67 --------------------------#
[32m[20221213 20:56:59 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:56:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:56:59 @agent_ppo2.py:185][0m |          -0.0058 |          34.8685 |          14.8090 |
[32m[20221213 20:56:59 @agent_ppo2.py:185][0m |          -0.0012 |          34.0109 |          14.8019 |
[32m[20221213 20:56:59 @agent_ppo2.py:185][0m |          -0.0050 |          33.9052 |          14.7970 |
[32m[20221213 20:56:59 @agent_ppo2.py:185][0m |          -0.0014 |          33.8797 |          14.7911 |
[32m[20221213 20:56:59 @agent_ppo2.py:185][0m |          -0.0053 |          33.7395 |          14.7920 |
[32m[20221213 20:56:59 @agent_ppo2.py:185][0m |          -0.0000 |          34.5304 |          14.7910 |
[32m[20221213 20:56:59 @agent_ppo2.py:185][0m |           0.0030 |          35.4515 |          14.7915 |
[32m[20221213 20:56:59 @agent_ppo2.py:185][0m |          -0.0063 |          33.5734 |          14.7842 |
[32m[20221213 20:57:00 @agent_ppo2.py:185][0m |          -0.0007 |          33.9159 |          14.7911 |
[32m[20221213 20:57:00 @agent_ppo2.py:185][0m |          -0.0072 |          33.5472 |          14.7934 |
[32m[20221213 20:57:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:57:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.20
[32m[20221213 20:57:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 371.00
[32m[20221213 20:57:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.00
[32m[20221213 20:57:00 @agent_ppo2.py:143][0m Total time:       1.42 min
[32m[20221213 20:57:00 @agent_ppo2.py:145][0m 139264 total steps have happened
[32m[20221213 20:57:00 @agent_ppo2.py:121][0m #------------------------ Iteration 68 --------------------------#
[32m[20221213 20:57:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:00 @agent_ppo2.py:185][0m |          -0.0046 |          35.6152 |          15.1323 |
[32m[20221213 20:57:00 @agent_ppo2.py:185][0m |          -0.0022 |          35.2799 |          15.1309 |
[32m[20221213 20:57:00 @agent_ppo2.py:185][0m |          -0.0051 |          35.2357 |          15.1194 |
[32m[20221213 20:57:00 @agent_ppo2.py:185][0m |          -0.0055 |          35.1447 |          15.1198 |
[32m[20221213 20:57:00 @agent_ppo2.py:185][0m |          -0.0013 |          35.3343 |          15.1154 |
[32m[20221213 20:57:00 @agent_ppo2.py:185][0m |          -0.0029 |          35.0177 |          15.1186 |
[32m[20221213 20:57:01 @agent_ppo2.py:185][0m |          -0.0038 |          34.9685 |          15.1196 |
[32m[20221213 20:57:01 @agent_ppo2.py:185][0m |          -0.0038 |          34.9056 |          15.1168 |
[32m[20221213 20:57:01 @agent_ppo2.py:185][0m |           0.0053 |          36.9461 |          15.1172 |
[32m[20221213 20:57:01 @agent_ppo2.py:185][0m |          -0.0063 |          34.8503 |          15.1184 |
[32m[20221213 20:57:01 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 20:57:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.80
[32m[20221213 20:57:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 401.00
[32m[20221213 20:57:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.00
[32m[20221213 20:57:01 @agent_ppo2.py:143][0m Total time:       1.44 min
[32m[20221213 20:57:01 @agent_ppo2.py:145][0m 141312 total steps have happened
[32m[20221213 20:57:01 @agent_ppo2.py:121][0m #------------------------ Iteration 69 --------------------------#
[32m[20221213 20:57:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:01 @agent_ppo2.py:185][0m |           0.0087 |          39.3640 |          15.1040 |
[32m[20221213 20:57:01 @agent_ppo2.py:185][0m |          -0.0063 |          37.4993 |          15.0580 |
[32m[20221213 20:57:01 @agent_ppo2.py:185][0m |          -0.0005 |          38.7217 |          15.0702 |
[32m[20221213 20:57:01 @agent_ppo2.py:185][0m |          -0.0062 |          37.4168 |          15.0675 |
[32m[20221213 20:57:02 @agent_ppo2.py:185][0m |           0.0098 |          40.0604 |          15.0737 |
[32m[20221213 20:57:02 @agent_ppo2.py:185][0m |          -0.0012 |          39.0989 |          15.0689 |
[32m[20221213 20:57:02 @agent_ppo2.py:185][0m |          -0.0053 |          37.1995 |          15.0694 |
[32m[20221213 20:57:02 @agent_ppo2.py:185][0m |          -0.0027 |          37.4420 |          15.0808 |
[32m[20221213 20:57:02 @agent_ppo2.py:185][0m |          -0.0029 |          36.9660 |          15.0772 |
[32m[20221213 20:57:02 @agent_ppo2.py:185][0m |          -0.0068 |          36.9257 |          15.0824 |
[32m[20221213 20:57:02 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:57:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.80
[32m[20221213 20:57:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.00
[32m[20221213 20:57:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.00
[32m[20221213 20:57:02 @agent_ppo2.py:143][0m Total time:       1.46 min
[32m[20221213 20:57:02 @agent_ppo2.py:145][0m 143360 total steps have happened
[32m[20221213 20:57:02 @agent_ppo2.py:121][0m #------------------------ Iteration 70 --------------------------#
[32m[20221213 20:57:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:02 @agent_ppo2.py:185][0m |          -0.0003 |          39.3012 |          14.9966 |
[32m[20221213 20:57:02 @agent_ppo2.py:185][0m |          -0.0052 |          39.0666 |          14.9843 |
[32m[20221213 20:57:03 @agent_ppo2.py:185][0m |          -0.0102 |          38.9291 |          14.9780 |
[32m[20221213 20:57:03 @agent_ppo2.py:185][0m |          -0.0055 |          38.8530 |          14.9805 |
[32m[20221213 20:57:03 @agent_ppo2.py:185][0m |          -0.0051 |          38.8880 |          14.9785 |
[32m[20221213 20:57:03 @agent_ppo2.py:185][0m |          -0.0023 |          38.6921 |          14.9742 |
[32m[20221213 20:57:03 @agent_ppo2.py:185][0m |          -0.0064 |          38.5877 |          14.9801 |
[32m[20221213 20:57:03 @agent_ppo2.py:185][0m |          -0.0062 |          38.5637 |          14.9734 |
[32m[20221213 20:57:03 @agent_ppo2.py:185][0m |          -0.0015 |          38.8982 |          14.9812 |
[32m[20221213 20:57:03 @agent_ppo2.py:185][0m |          -0.0054 |          38.4846 |          14.9865 |
[32m[20221213 20:57:03 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:57:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.80
[32m[20221213 20:57:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.00
[32m[20221213 20:57:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.00
[32m[20221213 20:57:03 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 441.00
[32m[20221213 20:57:03 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 441.00
[32m[20221213 20:57:03 @agent_ppo2.py:143][0m Total time:       1.48 min
[32m[20221213 20:57:03 @agent_ppo2.py:145][0m 145408 total steps have happened
[32m[20221213 20:57:03 @agent_ppo2.py:121][0m #------------------------ Iteration 71 --------------------------#
[32m[20221213 20:57:03 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:57:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:04 @agent_ppo2.py:185][0m |          -0.0060 |          39.1616 |          14.9626 |
[32m[20221213 20:57:04 @agent_ppo2.py:185][0m |          -0.0088 |          38.8851 |          14.9374 |
[32m[20221213 20:57:04 @agent_ppo2.py:185][0m |          -0.0031 |          38.8880 |          14.9304 |
[32m[20221213 20:57:04 @agent_ppo2.py:185][0m |          -0.0115 |          38.7811 |          14.9253 |
[32m[20221213 20:57:04 @agent_ppo2.py:185][0m |          -0.0067 |          38.6118 |          14.9257 |
[32m[20221213 20:57:04 @agent_ppo2.py:185][0m |          -0.0039 |          38.7284 |          14.9212 |
[32m[20221213 20:57:04 @agent_ppo2.py:185][0m |          -0.0012 |          39.3885 |          14.9213 |
[32m[20221213 20:57:04 @agent_ppo2.py:185][0m |          -0.0069 |          38.5379 |          14.9251 |
[32m[20221213 20:57:04 @agent_ppo2.py:185][0m |          -0.0040 |          38.4908 |          14.9293 |
[32m[20221213 20:57:04 @agent_ppo2.py:185][0m |          -0.0051 |          38.3699 |          14.9190 |
[32m[20221213 20:57:04 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:57:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.80
[32m[20221213 20:57:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.00
[32m[20221213 20:57:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.00
[32m[20221213 20:57:05 @agent_ppo2.py:143][0m Total time:       1.50 min
[32m[20221213 20:57:05 @agent_ppo2.py:145][0m 147456 total steps have happened
[32m[20221213 20:57:05 @agent_ppo2.py:121][0m #------------------------ Iteration 72 --------------------------#
[32m[20221213 20:57:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:05 @agent_ppo2.py:185][0m |           0.0068 |          38.9116 |          15.0778 |
[32m[20221213 20:57:05 @agent_ppo2.py:185][0m |          -0.0053 |          38.4656 |          15.0653 |
[32m[20221213 20:57:05 @agent_ppo2.py:185][0m |           0.0058 |          41.5929 |          15.0573 |
[32m[20221213 20:57:05 @agent_ppo2.py:185][0m |          -0.0058 |          38.4145 |          15.0483 |
[32m[20221213 20:57:05 @agent_ppo2.py:185][0m |          -0.0063 |          38.2939 |          15.0464 |
[32m[20221213 20:57:05 @agent_ppo2.py:185][0m |          -0.0052 |          38.1067 |          15.0499 |
[32m[20221213 20:57:05 @agent_ppo2.py:185][0m |           0.0006 |          38.0644 |          15.0475 |
[32m[20221213 20:57:06 @agent_ppo2.py:185][0m |          -0.0052 |          38.0946 |          15.0441 |
[32m[20221213 20:57:06 @agent_ppo2.py:185][0m |          -0.0076 |          37.9968 |          15.0484 |
[32m[20221213 20:57:06 @agent_ppo2.py:185][0m |          -0.0071 |          38.1831 |          15.0490 |
[32m[20221213 20:57:06 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:57:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.80
[32m[20221213 20:57:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.00
[32m[20221213 20:57:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.00
[32m[20221213 20:57:06 @agent_ppo2.py:143][0m Total time:       1.52 min
[32m[20221213 20:57:06 @agent_ppo2.py:145][0m 149504 total steps have happened
[32m[20221213 20:57:06 @agent_ppo2.py:121][0m #------------------------ Iteration 73 --------------------------#
[32m[20221213 20:57:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:06 @agent_ppo2.py:185][0m |          -0.0004 |          41.1760 |          14.9943 |
[32m[20221213 20:57:06 @agent_ppo2.py:185][0m |          -0.0041 |          41.0126 |          14.9844 |
[32m[20221213 20:57:06 @agent_ppo2.py:185][0m |           0.0084 |          44.2130 |          14.9847 |
[32m[20221213 20:57:06 @agent_ppo2.py:185][0m |          -0.0010 |          40.8264 |          14.9820 |
[32m[20221213 20:57:07 @agent_ppo2.py:185][0m |          -0.0019 |          40.8130 |          14.9732 |
[32m[20221213 20:57:07 @agent_ppo2.py:185][0m |          -0.0015 |          40.6719 |          14.9795 |
[32m[20221213 20:57:07 @agent_ppo2.py:185][0m |           0.0001 |          42.0817 |          14.9847 |
[32m[20221213 20:57:07 @agent_ppo2.py:185][0m |           0.0030 |          41.2250 |          14.9780 |
[32m[20221213 20:57:07 @agent_ppo2.py:185][0m |          -0.0072 |          40.6836 |          14.9747 |
[32m[20221213 20:57:07 @agent_ppo2.py:185][0m |          -0.0047 |          40.6257 |          14.9836 |
[32m[20221213 20:57:07 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:57:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 358.80
[32m[20221213 20:57:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.00
[32m[20221213 20:57:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.00
[32m[20221213 20:57:07 @agent_ppo2.py:143][0m Total time:       1.54 min
[32m[20221213 20:57:07 @agent_ppo2.py:145][0m 151552 total steps have happened
[32m[20221213 20:57:07 @agent_ppo2.py:121][0m #------------------------ Iteration 74 --------------------------#
[32m[20221213 20:57:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:07 @agent_ppo2.py:185][0m |          -0.0030 |          40.3353 |          14.9720 |
[32m[20221213 20:57:08 @agent_ppo2.py:185][0m |          -0.0061 |          39.6730 |          14.9563 |
[32m[20221213 20:57:08 @agent_ppo2.py:185][0m |          -0.0073 |          39.3688 |          14.9385 |
[32m[20221213 20:57:08 @agent_ppo2.py:185][0m |          -0.0013 |          41.7674 |          14.9464 |
[32m[20221213 20:57:08 @agent_ppo2.py:185][0m |          -0.0060 |          39.0885 |          14.9378 |
[32m[20221213 20:57:08 @agent_ppo2.py:185][0m |          -0.0097 |          39.0756 |          14.9460 |
[32m[20221213 20:57:08 @agent_ppo2.py:185][0m |          -0.0097 |          39.0218 |          14.9485 |
[32m[20221213 20:57:08 @agent_ppo2.py:185][0m |          -0.0071 |          38.8977 |          14.9491 |
[32m[20221213 20:57:08 @agent_ppo2.py:185][0m |          -0.0052 |          38.7301 |          14.9527 |
[32m[20221213 20:57:08 @agent_ppo2.py:185][0m |          -0.0053 |          38.7827 |          14.9487 |
[32m[20221213 20:57:08 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:57:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.40
[32m[20221213 20:57:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.00
[32m[20221213 20:57:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.00
[32m[20221213 20:57:08 @agent_ppo2.py:143][0m Total time:       1.56 min
[32m[20221213 20:57:08 @agent_ppo2.py:145][0m 153600 total steps have happened
[32m[20221213 20:57:08 @agent_ppo2.py:121][0m #------------------------ Iteration 75 --------------------------#
[32m[20221213 20:57:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:57:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:09 @agent_ppo2.py:185][0m |           0.0082 |          40.1130 |          15.0940 |
[32m[20221213 20:57:09 @agent_ppo2.py:185][0m |          -0.0058 |          38.5652 |          15.0787 |
[32m[20221213 20:57:09 @agent_ppo2.py:185][0m |           0.0064 |          40.2584 |          15.0806 |
[32m[20221213 20:57:09 @agent_ppo2.py:185][0m |          -0.0079 |          38.6142 |          15.0717 |
[32m[20221213 20:57:09 @agent_ppo2.py:185][0m |           0.0050 |          39.6156 |          15.0731 |
[32m[20221213 20:57:09 @agent_ppo2.py:185][0m |          -0.0032 |          38.3233 |          15.0728 |
[32m[20221213 20:57:10 @agent_ppo2.py:185][0m |          -0.0099 |          38.3586 |          15.0766 |
[32m[20221213 20:57:10 @agent_ppo2.py:185][0m |          -0.0067 |          38.3277 |          15.0725 |
[32m[20221213 20:57:10 @agent_ppo2.py:185][0m |          -0.0061 |          38.3090 |          15.0784 |
[32m[20221213 20:57:10 @agent_ppo2.py:185][0m |          -0.0096 |          38.2272 |          15.0788 |
[32m[20221213 20:57:10 @agent_ppo2.py:130][0m Policy update time: 1.50 s
[32m[20221213 20:57:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.20
[32m[20221213 20:57:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.00
[32m[20221213 20:57:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.00
[32m[20221213 20:57:10 @agent_ppo2.py:143][0m Total time:       1.59 min
[32m[20221213 20:57:10 @agent_ppo2.py:145][0m 155648 total steps have happened
[32m[20221213 20:57:10 @agent_ppo2.py:121][0m #------------------------ Iteration 76 --------------------------#
[32m[20221213 20:57:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:10 @agent_ppo2.py:185][0m |           0.0024 |          40.2272 |          15.1309 |
[32m[20221213 20:57:11 @agent_ppo2.py:185][0m |          -0.0051 |          39.8952 |          15.1164 |
[32m[20221213 20:57:11 @agent_ppo2.py:185][0m |          -0.0017 |          39.9137 |          15.1091 |
[32m[20221213 20:57:11 @agent_ppo2.py:185][0m |          -0.0080 |          39.8515 |          15.1180 |
[32m[20221213 20:57:11 @agent_ppo2.py:185][0m |          -0.0031 |          40.4380 |          15.1160 |
[32m[20221213 20:57:11 @agent_ppo2.py:185][0m |           0.0065 |          42.5004 |          15.1138 |
[32m[20221213 20:57:11 @agent_ppo2.py:185][0m |          -0.0043 |          39.8705 |          15.1143 |
[32m[20221213 20:57:11 @agent_ppo2.py:185][0m |           0.0008 |          40.9210 |          15.1146 |
[32m[20221213 20:57:11 @agent_ppo2.py:185][0m |          -0.0071 |          39.8562 |          15.1137 |
[32m[20221213 20:57:11 @agent_ppo2.py:185][0m |          -0.0017 |          39.7823 |          15.1147 |
[32m[20221213 20:57:11 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:57:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.00
[32m[20221213 20:57:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.00
[32m[20221213 20:57:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 365.00
[32m[20221213 20:57:11 @agent_ppo2.py:143][0m Total time:       1.61 min
[32m[20221213 20:57:11 @agent_ppo2.py:145][0m 157696 total steps have happened
[32m[20221213 20:57:11 @agent_ppo2.py:121][0m #------------------------ Iteration 77 --------------------------#
[32m[20221213 20:57:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:12 @agent_ppo2.py:185][0m |          -0.0021 |          42.8256 |          15.2438 |
[32m[20221213 20:57:12 @agent_ppo2.py:185][0m |          -0.0030 |          42.5406 |          15.2229 |
[32m[20221213 20:57:12 @agent_ppo2.py:185][0m |          -0.0005 |          42.6919 |          15.2245 |
[32m[20221213 20:57:12 @agent_ppo2.py:185][0m |           0.0032 |          44.5395 |          15.2210 |
[32m[20221213 20:57:12 @agent_ppo2.py:185][0m |          -0.0036 |          42.4209 |          15.2205 |
[32m[20221213 20:57:12 @agent_ppo2.py:185][0m |          -0.0063 |          42.1344 |          15.2198 |
[32m[20221213 20:57:12 @agent_ppo2.py:185][0m |          -0.0053 |          42.2816 |          15.2210 |
[32m[20221213 20:57:12 @agent_ppo2.py:185][0m |           0.0021 |          43.5862 |          15.2193 |
[32m[20221213 20:57:12 @agent_ppo2.py:185][0m |          -0.0069 |          42.1187 |          15.2208 |
[32m[20221213 20:57:13 @agent_ppo2.py:185][0m |          -0.0025 |          42.2764 |          15.2227 |
[32m[20221213 20:57:13 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:57:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.20
[32m[20221213 20:57:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.00
[32m[20221213 20:57:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.00
[32m[20221213 20:57:13 @agent_ppo2.py:143][0m Total time:       1.63 min
[32m[20221213 20:57:13 @agent_ppo2.py:145][0m 159744 total steps have happened
[32m[20221213 20:57:13 @agent_ppo2.py:121][0m #------------------------ Iteration 78 --------------------------#
[32m[20221213 20:57:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 20:57:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:13 @agent_ppo2.py:185][0m |           0.0005 |          45.0572 |          15.0278 |
[32m[20221213 20:57:13 @agent_ppo2.py:185][0m |           0.0005 |          44.7536 |          15.0086 |
[32m[20221213 20:57:13 @agent_ppo2.py:185][0m |          -0.0057 |          44.6070 |          14.9989 |
[32m[20221213 20:57:13 @agent_ppo2.py:185][0m |          -0.0041 |          44.7767 |          14.9929 |
[32m[20221213 20:57:13 @agent_ppo2.py:185][0m |          -0.0039 |          44.4555 |          14.9866 |
[32m[20221213 20:57:13 @agent_ppo2.py:185][0m |          -0.0064 |          44.6160 |          14.9887 |
[32m[20221213 20:57:14 @agent_ppo2.py:185][0m |          -0.0029 |          44.4942 |          14.9906 |
[32m[20221213 20:57:14 @agent_ppo2.py:185][0m |           0.0010 |          44.9226 |          14.9870 |
[32m[20221213 20:57:14 @agent_ppo2.py:185][0m |          -0.0011 |          44.4070 |          14.9941 |
[32m[20221213 20:57:14 @agent_ppo2.py:185][0m |          -0.0055 |          44.3258 |          14.9854 |
[32m[20221213 20:57:14 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:57:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.40
[32m[20221213 20:57:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.00
[32m[20221213 20:57:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.00
[32m[20221213 20:57:14 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 450.00
[32m[20221213 20:57:14 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 450.00
[32m[20221213 20:57:14 @agent_ppo2.py:143][0m Total time:       1.66 min
[32m[20221213 20:57:14 @agent_ppo2.py:145][0m 161792 total steps have happened
[32m[20221213 20:57:14 @agent_ppo2.py:121][0m #------------------------ Iteration 79 --------------------------#
[32m[20221213 20:57:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:14 @agent_ppo2.py:185][0m |          -0.0003 |          44.8226 |          15.2665 |
[32m[20221213 20:57:14 @agent_ppo2.py:185][0m |          -0.0054 |          44.1024 |          15.2516 |
[32m[20221213 20:57:15 @agent_ppo2.py:185][0m |          -0.0068 |          43.8884 |          15.2453 |
[32m[20221213 20:57:15 @agent_ppo2.py:185][0m |          -0.0047 |          43.5783 |          15.2427 |
[32m[20221213 20:57:15 @agent_ppo2.py:185][0m |          -0.0094 |          43.4979 |          15.2381 |
[32m[20221213 20:57:15 @agent_ppo2.py:185][0m |          -0.0084 |          43.3955 |          15.2390 |
[32m[20221213 20:57:15 @agent_ppo2.py:185][0m |          -0.0016 |          43.5336 |          15.2387 |
[32m[20221213 20:57:15 @agent_ppo2.py:185][0m |          -0.0050 |          43.5308 |          15.2390 |
[32m[20221213 20:57:15 @agent_ppo2.py:185][0m |          -0.0054 |          43.1570 |          15.2401 |
[32m[20221213 20:57:15 @agent_ppo2.py:185][0m |          -0.0065 |          43.0220 |          15.2350 |
[32m[20221213 20:57:15 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 20:57:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.80
[32m[20221213 20:57:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.00
[32m[20221213 20:57:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.00
[32m[20221213 20:57:15 @agent_ppo2.py:143][0m Total time:       1.68 min
[32m[20221213 20:57:15 @agent_ppo2.py:145][0m 163840 total steps have happened
[32m[20221213 20:57:15 @agent_ppo2.py:121][0m #------------------------ Iteration 80 --------------------------#
[32m[20221213 20:57:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 20:57:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:16 @agent_ppo2.py:185][0m |          -0.0029 |          48.2217 |          15.1926 |
[32m[20221213 20:57:16 @agent_ppo2.py:185][0m |          -0.0075 |          47.7153 |          15.1790 |
[32m[20221213 20:57:16 @agent_ppo2.py:185][0m |          -0.0033 |          47.6272 |          15.1756 |
[32m[20221213 20:57:16 @agent_ppo2.py:185][0m |          -0.0021 |          47.5591 |          15.1743 |
[32m[20221213 20:57:16 @agent_ppo2.py:185][0m |          -0.0077 |          47.3517 |          15.1687 |
[32m[20221213 20:57:16 @agent_ppo2.py:185][0m |          -0.0059 |          47.2809 |          15.1734 |
[32m[20221213 20:57:16 @agent_ppo2.py:185][0m |          -0.0071 |          47.2688 |          15.1749 |
[32m[20221213 20:57:16 @agent_ppo2.py:185][0m |          -0.0116 |          47.1867 |          15.1717 |
[32m[20221213 20:57:17 @agent_ppo2.py:185][0m |          -0.0078 |          46.8844 |          15.1759 |
[32m[20221213 20:57:17 @agent_ppo2.py:185][0m |          -0.0086 |          47.0622 |          15.1697 |
[32m[20221213 20:57:17 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 20:57:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.20
[32m[20221213 20:57:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.00
[32m[20221213 20:57:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.00
[32m[20221213 20:57:17 @agent_ppo2.py:143][0m Total time:       1.70 min
[32m[20221213 20:57:17 @agent_ppo2.py:145][0m 165888 total steps have happened
[32m[20221213 20:57:17 @agent_ppo2.py:121][0m #------------------------ Iteration 81 --------------------------#
[32m[20221213 20:57:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:17 @agent_ppo2.py:185][0m |          -0.0011 |          47.6662 |          15.3096 |
[32m[20221213 20:57:17 @agent_ppo2.py:185][0m |          -0.0024 |          47.3458 |          15.2904 |
[32m[20221213 20:57:17 @agent_ppo2.py:185][0m |          -0.0073 |          47.3958 |          15.2834 |
[32m[20221213 20:57:17 @agent_ppo2.py:185][0m |          -0.0032 |          47.0876 |          15.2781 |
[32m[20221213 20:57:18 @agent_ppo2.py:185][0m |          -0.0044 |          47.0558 |          15.2786 |
[32m[20221213 20:57:18 @agent_ppo2.py:185][0m |           0.0000 |          48.1693 |          15.2707 |
[32m[20221213 20:57:18 @agent_ppo2.py:185][0m |          -0.0047 |          47.7912 |          15.2731 |
[32m[20221213 20:57:18 @agent_ppo2.py:185][0m |          -0.0031 |          47.2540 |          15.2717 |
[32m[20221213 20:57:18 @agent_ppo2.py:185][0m |          -0.0050 |          46.8319 |          15.2672 |
[32m[20221213 20:57:18 @agent_ppo2.py:185][0m |          -0.0095 |          46.7592 |          15.2708 |
[32m[20221213 20:57:18 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 20:57:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.20
[32m[20221213 20:57:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.00
[32m[20221213 20:57:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.00
[32m[20221213 20:57:18 @agent_ppo2.py:143][0m Total time:       1.73 min
[32m[20221213 20:57:18 @agent_ppo2.py:145][0m 167936 total steps have happened
[32m[20221213 20:57:18 @agent_ppo2.py:121][0m #------------------------ Iteration 82 --------------------------#
[32m[20221213 20:57:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:19 @agent_ppo2.py:185][0m |          -0.0061 |          49.6302 |          15.2674 |
[32m[20221213 20:57:19 @agent_ppo2.py:185][0m |          -0.0078 |          49.2978 |          15.2477 |
[32m[20221213 20:57:19 @agent_ppo2.py:185][0m |          -0.0039 |          49.2289 |          15.2490 |
[32m[20221213 20:57:19 @agent_ppo2.py:185][0m |          -0.0033 |          49.0476 |          15.2534 |
[32m[20221213 20:57:19 @agent_ppo2.py:185][0m |          -0.0053 |          49.0966 |          15.2511 |
[32m[20221213 20:57:19 @agent_ppo2.py:185][0m |          -0.0049 |          48.9142 |          15.2447 |
[32m[20221213 20:57:19 @agent_ppo2.py:185][0m |          -0.0028 |          48.9748 |          15.2547 |
[32m[20221213 20:57:19 @agent_ppo2.py:185][0m |          -0.0051 |          48.6803 |          15.2517 |
[32m[20221213 20:57:19 @agent_ppo2.py:185][0m |          -0.0014 |          48.6589 |          15.2522 |
[32m[20221213 20:57:20 @agent_ppo2.py:185][0m |          -0.0075 |          48.6967 |          15.2598 |
[32m[20221213 20:57:20 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 20:57:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.20
[32m[20221213 20:57:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.00
[32m[20221213 20:57:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.00
[32m[20221213 20:57:20 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 476.00
[32m[20221213 20:57:20 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 476.00
[32m[20221213 20:57:20 @agent_ppo2.py:143][0m Total time:       1.75 min
[32m[20221213 20:57:20 @agent_ppo2.py:145][0m 169984 total steps have happened
[32m[20221213 20:57:20 @agent_ppo2.py:121][0m #------------------------ Iteration 83 --------------------------#
[32m[20221213 20:57:20 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:57:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:20 @agent_ppo2.py:185][0m |           0.0078 |          48.9628 |          15.2746 |
[32m[20221213 20:57:20 @agent_ppo2.py:185][0m |          -0.0035 |          46.4802 |          15.2595 |
[32m[20221213 20:57:20 @agent_ppo2.py:185][0m |          -0.0021 |          46.4385 |          15.2615 |
[32m[20221213 20:57:20 @agent_ppo2.py:185][0m |          -0.0023 |          46.3060 |          15.2606 |
[32m[20221213 20:57:21 @agent_ppo2.py:185][0m |          -0.0050 |          46.1754 |          15.2569 |
[32m[20221213 20:57:21 @agent_ppo2.py:185][0m |          -0.0073 |          46.1288 |          15.2607 |
[32m[20221213 20:57:21 @agent_ppo2.py:185][0m |          -0.0077 |          46.1465 |          15.2581 |
[32m[20221213 20:57:21 @agent_ppo2.py:185][0m |          -0.0043 |          46.1245 |          15.2571 |
[32m[20221213 20:57:21 @agent_ppo2.py:185][0m |          -0.0056 |          46.0354 |          15.2576 |
[32m[20221213 20:57:21 @agent_ppo2.py:185][0m |          -0.0019 |          46.6673 |          15.2591 |
[32m[20221213 20:57:21 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 20:57:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.20
[32m[20221213 20:57:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.00
[32m[20221213 20:57:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.00
[32m[20221213 20:57:21 @agent_ppo2.py:143][0m Total time:       1.77 min
[32m[20221213 20:57:21 @agent_ppo2.py:145][0m 172032 total steps have happened
[32m[20221213 20:57:21 @agent_ppo2.py:121][0m #------------------------ Iteration 84 --------------------------#
[32m[20221213 20:57:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:57:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:22 @agent_ppo2.py:185][0m |           0.0001 |          47.7722 |          15.2901 |
[32m[20221213 20:57:22 @agent_ppo2.py:185][0m |          -0.0018 |          47.3502 |          15.2876 |
[32m[20221213 20:57:22 @agent_ppo2.py:185][0m |          -0.0009 |          47.6153 |          15.2820 |
[32m[20221213 20:57:22 @agent_ppo2.py:185][0m |          -0.0040 |          47.0267 |          15.2832 |
[32m[20221213 20:57:22 @agent_ppo2.py:185][0m |          -0.0047 |          46.9196 |          15.2838 |
[32m[20221213 20:57:22 @agent_ppo2.py:185][0m |          -0.0084 |          46.8293 |          15.2846 |
[32m[20221213 20:57:22 @agent_ppo2.py:185][0m |          -0.0058 |          46.8390 |          15.2864 |
[32m[20221213 20:57:22 @agent_ppo2.py:185][0m |          -0.0008 |          46.6820 |          15.2848 |
[32m[20221213 20:57:22 @agent_ppo2.py:185][0m |          -0.0054 |          46.5976 |          15.2869 |
[32m[20221213 20:57:23 @agent_ppo2.py:185][0m |          -0.0054 |          46.6493 |          15.2876 |
[32m[20221213 20:57:23 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 20:57:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.20
[32m[20221213 20:57:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.00
[32m[20221213 20:57:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.00
[32m[20221213 20:57:23 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 482.00
[32m[20221213 20:57:23 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 482.00
[32m[20221213 20:57:23 @agent_ppo2.py:143][0m Total time:       1.80 min
[32m[20221213 20:57:23 @agent_ppo2.py:145][0m 174080 total steps have happened
[32m[20221213 20:57:23 @agent_ppo2.py:121][0m #------------------------ Iteration 85 --------------------------#
[32m[20221213 20:57:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 20:57:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:23 @agent_ppo2.py:185][0m |           0.0127 |          50.1218 |          15.4064 |
[32m[20221213 20:57:23 @agent_ppo2.py:185][0m |           0.0014 |          46.4571 |          15.4019 |
[32m[20221213 20:57:23 @agent_ppo2.py:185][0m |          -0.0034 |          45.5419 |          15.3928 |
[32m[20221213 20:57:23 @agent_ppo2.py:185][0m |          -0.0046 |          45.4012 |          15.3924 |
[32m[20221213 20:57:24 @agent_ppo2.py:185][0m |          -0.0077 |          45.3676 |          15.3953 |
[32m[20221213 20:57:24 @agent_ppo2.py:185][0m |          -0.0019 |          45.5862 |          15.3943 |
[32m[20221213 20:57:24 @agent_ppo2.py:185][0m |          -0.0078 |          45.2987 |          15.3873 |
[32m[20221213 20:57:24 @agent_ppo2.py:185][0m |           0.0100 |          49.9319 |          15.3883 |
[32m[20221213 20:57:24 @agent_ppo2.py:185][0m |          -0.0061 |          45.2711 |          15.3823 |
[32m[20221213 20:57:24 @agent_ppo2.py:185][0m |          -0.0004 |          46.1389 |          15.3859 |
[32m[20221213 20:57:24 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 20:57:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.20
[32m[20221213 20:57:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.00
[32m[20221213 20:57:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 442.00
[32m[20221213 20:57:24 @agent_ppo2.py:143][0m Total time:       1.83 min
[32m[20221213 20:57:24 @agent_ppo2.py:145][0m 176128 total steps have happened
[32m[20221213 20:57:24 @agent_ppo2.py:121][0m #------------------------ Iteration 86 --------------------------#
[32m[20221213 20:57:24 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:57:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:25 @agent_ppo2.py:185][0m |          -0.0008 |          46.2519 |          15.4210 |
[32m[20221213 20:57:25 @agent_ppo2.py:185][0m |          -0.0047 |          46.0371 |          15.4030 |
[32m[20221213 20:57:25 @agent_ppo2.py:185][0m |          -0.0056 |          45.8447 |          15.4029 |
[32m[20221213 20:57:25 @agent_ppo2.py:185][0m |           0.0111 |          50.0950 |          15.4027 |
[32m[20221213 20:57:25 @agent_ppo2.py:185][0m |          -0.0099 |          45.7119 |          15.4026 |
[32m[20221213 20:57:25 @agent_ppo2.py:185][0m |          -0.0037 |          45.5114 |          15.3991 |
[32m[20221213 20:57:25 @agent_ppo2.py:185][0m |          -0.0077 |          45.4706 |          15.3979 |
[32m[20221213 20:57:25 @agent_ppo2.py:185][0m |          -0.0097 |          45.4589 |          15.4034 |
[32m[20221213 20:57:25 @agent_ppo2.py:185][0m |          -0.0067 |          45.2868 |          15.4023 |
[32m[20221213 20:57:25 @agent_ppo2.py:185][0m |          -0.0054 |          45.4523 |          15.4086 |
[32m[20221213 20:57:25 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 20:57:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.00
[32m[20221213 20:57:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 394.00
[32m[20221213 20:57:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.00
[32m[20221213 20:57:26 @agent_ppo2.py:143][0m Total time:       1.85 min
[32m[20221213 20:57:26 @agent_ppo2.py:145][0m 178176 total steps have happened
[32m[20221213 20:57:26 @agent_ppo2.py:121][0m #------------------------ Iteration 87 --------------------------#
[32m[20221213 20:57:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 20:57:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:26 @agent_ppo2.py:185][0m |           0.0067 |          49.3241 |          15.5469 |
[32m[20221213 20:57:26 @agent_ppo2.py:185][0m |          -0.0070 |          48.2990 |          15.5168 |
[32m[20221213 20:57:26 @agent_ppo2.py:185][0m |          -0.0066 |          48.1967 |          15.5053 |
[32m[20221213 20:57:26 @agent_ppo2.py:185][0m |          -0.0051 |          48.1000 |          15.5084 |
[32m[20221213 20:57:26 @agent_ppo2.py:185][0m |          -0.0104 |          48.0687 |          15.5071 |
[32m[20221213 20:57:26 @agent_ppo2.py:185][0m |           0.0022 |          53.0676 |          15.5057 |
[32m[20221213 20:57:27 @agent_ppo2.py:185][0m |          -0.0086 |          48.0616 |          15.4861 |
[32m[20221213 20:57:27 @agent_ppo2.py:185][0m |          -0.0080 |          47.7704 |          15.5080 |
[32m[20221213 20:57:27 @agent_ppo2.py:185][0m |          -0.0114 |          47.8516 |          15.5031 |
[32m[20221213 20:57:27 @agent_ppo2.py:185][0m |          -0.0112 |          47.8219 |          15.5040 |
[32m[20221213 20:57:27 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:57:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.80
[32m[20221213 20:57:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.00
[32m[20221213 20:57:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.00
[32m[20221213 20:57:27 @agent_ppo2.py:143][0m Total time:       1.87 min
[32m[20221213 20:57:27 @agent_ppo2.py:145][0m 180224 total steps have happened
[32m[20221213 20:57:27 @agent_ppo2.py:121][0m #------------------------ Iteration 88 --------------------------#
[32m[20221213 20:57:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 20:57:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:27 @agent_ppo2.py:185][0m |          -0.0020 |          46.8508 |          15.5647 |
[32m[20221213 20:57:27 @agent_ppo2.py:185][0m |          -0.0022 |          46.3205 |          15.5563 |
[32m[20221213 20:57:27 @agent_ppo2.py:185][0m |          -0.0034 |          46.0863 |          15.5508 |
[32m[20221213 20:57:28 @agent_ppo2.py:185][0m |          -0.0046 |          46.0315 |          15.5472 |
[32m[20221213 20:57:28 @agent_ppo2.py:185][0m |           0.0015 |          47.4231 |          15.5531 |
[32m[20221213 20:57:28 @agent_ppo2.py:185][0m |          -0.0033 |          45.6518 |          15.5503 |
[32m[20221213 20:57:28 @agent_ppo2.py:185][0m |           0.0003 |          47.2904 |          15.5498 |
[32m[20221213 20:57:28 @agent_ppo2.py:185][0m |          -0.0049 |          45.2818 |          15.5522 |
[32m[20221213 20:57:28 @agent_ppo2.py:185][0m |           0.0046 |          50.7445 |          15.5497 |
[32m[20221213 20:57:28 @agent_ppo2.py:185][0m |          -0.0044 |          45.2980 |          15.5457 |
[32m[20221213 20:57:28 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:57:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.60
[32m[20221213 20:57:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.00
[32m[20221213 20:57:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.00
[32m[20221213 20:57:28 @agent_ppo2.py:143][0m Total time:       1.89 min
[32m[20221213 20:57:28 @agent_ppo2.py:145][0m 182272 total steps have happened
[32m[20221213 20:57:28 @agent_ppo2.py:121][0m #------------------------ Iteration 89 --------------------------#
[32m[20221213 20:57:28 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:57:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:29 @agent_ppo2.py:185][0m |           0.0010 |          50.0326 |          15.7154 |
[32m[20221213 20:57:29 @agent_ppo2.py:185][0m |          -0.0007 |          49.4614 |          15.6869 |
[32m[20221213 20:57:29 @agent_ppo2.py:185][0m |          -0.0054 |          48.5954 |          15.6713 |
[32m[20221213 20:57:29 @agent_ppo2.py:185][0m |          -0.0035 |          48.3036 |          15.6787 |
[32m[20221213 20:57:29 @agent_ppo2.py:185][0m |          -0.0080 |          47.9442 |          15.6756 |
[32m[20221213 20:57:29 @agent_ppo2.py:185][0m |          -0.0059 |          47.8177 |          15.6696 |
[32m[20221213 20:57:29 @agent_ppo2.py:185][0m |          -0.0022 |          48.4880 |          15.6748 |
[32m[20221213 20:57:29 @agent_ppo2.py:185][0m |          -0.0064 |          47.5064 |          15.6681 |
[32m[20221213 20:57:29 @agent_ppo2.py:185][0m |          -0.0031 |          48.0930 |          15.6761 |
[32m[20221213 20:57:29 @agent_ppo2.py:185][0m |          -0.0064 |          47.0236 |          15.6734 |
[32m[20221213 20:57:29 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 20:57:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.00
[32m[20221213 20:57:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.00
[32m[20221213 20:57:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.00
[32m[20221213 20:57:30 @agent_ppo2.py:143][0m Total time:       1.91 min
[32m[20221213 20:57:30 @agent_ppo2.py:145][0m 184320 total steps have happened
[32m[20221213 20:57:30 @agent_ppo2.py:121][0m #------------------------ Iteration 90 --------------------------#
[32m[20221213 20:57:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:30 @agent_ppo2.py:185][0m |          -0.0028 |          51.9264 |          15.6035 |
[32m[20221213 20:57:30 @agent_ppo2.py:185][0m |          -0.0043 |          50.9007 |          15.5840 |
[32m[20221213 20:57:30 @agent_ppo2.py:185][0m |          -0.0017 |          50.5674 |          15.5800 |
[32m[20221213 20:57:30 @agent_ppo2.py:185][0m |          -0.0018 |          50.7133 |          15.5763 |
[32m[20221213 20:57:30 @agent_ppo2.py:185][0m |          -0.0060 |          50.4648 |          15.5754 |
[32m[20221213 20:57:30 @agent_ppo2.py:185][0m |          -0.0019 |          50.4296 |          15.5750 |
[32m[20221213 20:57:30 @agent_ppo2.py:185][0m |           0.0026 |          55.3531 |          15.5672 |
[32m[20221213 20:57:30 @agent_ppo2.py:185][0m |          -0.0038 |          50.2194 |          15.5603 |
[32m[20221213 20:57:31 @agent_ppo2.py:185][0m |          -0.0040 |          50.1798 |          15.5626 |
[32m[20221213 20:57:31 @agent_ppo2.py:185][0m |          -0.0041 |          50.0015 |          15.5732 |
[32m[20221213 20:57:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:57:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.00
[32m[20221213 20:57:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.00
[32m[20221213 20:57:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.00
[32m[20221213 20:57:31 @agent_ppo2.py:143][0m Total time:       1.93 min
[32m[20221213 20:57:31 @agent_ppo2.py:145][0m 186368 total steps have happened
[32m[20221213 20:57:31 @agent_ppo2.py:121][0m #------------------------ Iteration 91 --------------------------#
[32m[20221213 20:57:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:57:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:31 @agent_ppo2.py:185][0m |           0.0041 |          50.8142 |          15.6647 |
[32m[20221213 20:57:31 @agent_ppo2.py:185][0m |          -0.0025 |          52.3916 |          15.6521 |
[32m[20221213 20:57:31 @agent_ppo2.py:185][0m |          -0.0067 |          49.3454 |          15.6293 |
[32m[20221213 20:57:31 @agent_ppo2.py:185][0m |          -0.0066 |          49.2773 |          15.6396 |
[32m[20221213 20:57:31 @agent_ppo2.py:185][0m |          -0.0052 |          49.2350 |          15.6440 |
[32m[20221213 20:57:32 @agent_ppo2.py:185][0m |          -0.0034 |          49.5983 |          15.6435 |
[32m[20221213 20:57:32 @agent_ppo2.py:185][0m |          -0.0089 |          49.0960 |          15.6437 |
[32m[20221213 20:57:32 @agent_ppo2.py:185][0m |          -0.0074 |          49.5810 |          15.6479 |
[32m[20221213 20:57:32 @agent_ppo2.py:185][0m |          -0.0001 |          51.1840 |          15.6483 |
[32m[20221213 20:57:32 @agent_ppo2.py:185][0m |          -0.0082 |          49.0733 |          15.6460 |
[32m[20221213 20:57:32 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:57:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.20
[32m[20221213 20:57:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.00
[32m[20221213 20:57:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.00
[32m[20221213 20:57:32 @agent_ppo2.py:143][0m Total time:       1.95 min
[32m[20221213 20:57:32 @agent_ppo2.py:145][0m 188416 total steps have happened
[32m[20221213 20:57:32 @agent_ppo2.py:121][0m #------------------------ Iteration 92 --------------------------#
[32m[20221213 20:57:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:32 @agent_ppo2.py:185][0m |          -0.0027 |          49.1017 |          15.6814 |
[32m[20221213 20:57:32 @agent_ppo2.py:185][0m |          -0.0053 |          48.5587 |          15.6809 |
[32m[20221213 20:57:32 @agent_ppo2.py:185][0m |          -0.0046 |          48.3817 |          15.6671 |
[32m[20221213 20:57:33 @agent_ppo2.py:185][0m |          -0.0033 |          48.2646 |          15.6691 |
[32m[20221213 20:57:33 @agent_ppo2.py:185][0m |          -0.0037 |          48.0790 |          15.6731 |
[32m[20221213 20:57:33 @agent_ppo2.py:185][0m |          -0.0086 |          48.1628 |          15.6738 |
[32m[20221213 20:57:33 @agent_ppo2.py:185][0m |           0.0077 |          54.5003 |          15.6732 |
[32m[20221213 20:57:33 @agent_ppo2.py:185][0m |          -0.0043 |          47.8513 |          15.6703 |
[32m[20221213 20:57:33 @agent_ppo2.py:185][0m |          -0.0090 |          47.9724 |          15.6684 |
[32m[20221213 20:57:33 @agent_ppo2.py:185][0m |          -0.0077 |          47.7056 |          15.6694 |
[32m[20221213 20:57:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:57:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.40
[32m[20221213 20:57:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.00
[32m[20221213 20:57:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.00
[32m[20221213 20:57:33 @agent_ppo2.py:143][0m Total time:       1.97 min
[32m[20221213 20:57:33 @agent_ppo2.py:145][0m 190464 total steps have happened
[32m[20221213 20:57:33 @agent_ppo2.py:121][0m #------------------------ Iteration 93 --------------------------#
[32m[20221213 20:57:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:33 @agent_ppo2.py:185][0m |          -0.0032 |          51.1281 |          15.6858 |
[32m[20221213 20:57:34 @agent_ppo2.py:185][0m |          -0.0046 |          50.4098 |          15.6699 |
[32m[20221213 20:57:34 @agent_ppo2.py:185][0m |          -0.0043 |          50.1655 |          15.6583 |
[32m[20221213 20:57:34 @agent_ppo2.py:185][0m |          -0.0075 |          50.0176 |          15.6537 |
[32m[20221213 20:57:34 @agent_ppo2.py:185][0m |          -0.0061 |          49.9210 |          15.6577 |
[32m[20221213 20:57:34 @agent_ppo2.py:185][0m |          -0.0003 |          50.3736 |          15.6519 |
[32m[20221213 20:57:34 @agent_ppo2.py:185][0m |          -0.0040 |          49.7874 |          15.6579 |
[32m[20221213 20:57:34 @agent_ppo2.py:185][0m |           0.0044 |          50.8338 |          15.6517 |
[32m[20221213 20:57:34 @agent_ppo2.py:185][0m |          -0.0057 |          49.3410 |          15.6443 |
[32m[20221213 20:57:34 @agent_ppo2.py:185][0m |          -0.0060 |          49.2793 |          15.6505 |
[32m[20221213 20:57:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:57:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.60
[32m[20221213 20:57:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.00
[32m[20221213 20:57:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.00
[32m[20221213 20:57:34 @agent_ppo2.py:143][0m Total time:       1.99 min
[32m[20221213 20:57:34 @agent_ppo2.py:145][0m 192512 total steps have happened
[32m[20221213 20:57:34 @agent_ppo2.py:121][0m #------------------------ Iteration 94 --------------------------#
[32m[20221213 20:57:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:35 @agent_ppo2.py:185][0m |          -0.0037 |          52.9589 |          15.6541 |
[32m[20221213 20:57:35 @agent_ppo2.py:185][0m |          -0.0075 |          52.5292 |          15.6464 |
[32m[20221213 20:57:35 @agent_ppo2.py:185][0m |          -0.0077 |          52.3234 |          15.6333 |
[32m[20221213 20:57:35 @agent_ppo2.py:185][0m |          -0.0052 |          51.9963 |          15.6321 |
[32m[20221213 20:57:35 @agent_ppo2.py:185][0m |          -0.0061 |          52.0428 |          15.6309 |
[32m[20221213 20:57:35 @agent_ppo2.py:185][0m |          -0.0066 |          51.7834 |          15.6239 |
[32m[20221213 20:57:35 @agent_ppo2.py:185][0m |          -0.0075 |          51.7079 |          15.6254 |
[32m[20221213 20:57:35 @agent_ppo2.py:185][0m |          -0.0075 |          51.5730 |          15.6259 |
[32m[20221213 20:57:35 @agent_ppo2.py:185][0m |          -0.0062 |          51.3249 |          15.6253 |
[32m[20221213 20:57:35 @agent_ppo2.py:185][0m |          -0.0063 |          51.4167 |          15.6217 |
[32m[20221213 20:57:35 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:57:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.60
[32m[20221213 20:57:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.00
[32m[20221213 20:57:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 402.00
[32m[20221213 20:57:36 @agent_ppo2.py:143][0m Total time:       2.01 min
[32m[20221213 20:57:36 @agent_ppo2.py:145][0m 194560 total steps have happened
[32m[20221213 20:57:36 @agent_ppo2.py:121][0m #------------------------ Iteration 95 --------------------------#
[32m[20221213 20:57:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:36 @agent_ppo2.py:185][0m |          -0.0034 |          53.7753 |          15.7581 |
[32m[20221213 20:57:36 @agent_ppo2.py:185][0m |           0.0076 |          54.5906 |          15.7516 |
[32m[20221213 20:57:36 @agent_ppo2.py:185][0m |          -0.0061 |          53.0662 |          15.7500 |
[32m[20221213 20:57:36 @agent_ppo2.py:185][0m |          -0.0037 |          52.8849 |          15.7449 |
[32m[20221213 20:57:36 @agent_ppo2.py:185][0m |          -0.0078 |          52.7444 |          15.7459 |
[32m[20221213 20:57:36 @agent_ppo2.py:185][0m |          -0.0063 |          52.6608 |          15.7441 |
[32m[20221213 20:57:36 @agent_ppo2.py:185][0m |          -0.0046 |          52.3821 |          15.7399 |
[32m[20221213 20:57:36 @agent_ppo2.py:185][0m |          -0.0028 |          52.4550 |          15.7416 |
[32m[20221213 20:57:37 @agent_ppo2.py:185][0m |          -0.0060 |          52.4539 |          15.7359 |
[32m[20221213 20:57:37 @agent_ppo2.py:185][0m |          -0.0057 |          52.4055 |          15.7331 |
[32m[20221213 20:57:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:57:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.20
[32m[20221213 20:57:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.00
[32m[20221213 20:57:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.00
[32m[20221213 20:57:37 @agent_ppo2.py:143][0m Total time:       2.03 min
[32m[20221213 20:57:37 @agent_ppo2.py:145][0m 196608 total steps have happened
[32m[20221213 20:57:37 @agent_ppo2.py:121][0m #------------------------ Iteration 96 --------------------------#
[32m[20221213 20:57:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:37 @agent_ppo2.py:185][0m |          -0.0005 |          51.6346 |          15.5788 |
[32m[20221213 20:57:37 @agent_ppo2.py:185][0m |           0.0035 |          52.9922 |          15.5609 |
[32m[20221213 20:57:37 @agent_ppo2.py:185][0m |           0.0205 |          58.4244 |          15.5637 |
[32m[20221213 20:57:37 @agent_ppo2.py:185][0m |          -0.0013 |          50.9962 |          15.5501 |
[32m[20221213 20:57:37 @agent_ppo2.py:185][0m |          -0.0086 |          50.8056 |          15.5513 |
[32m[20221213 20:57:37 @agent_ppo2.py:185][0m |          -0.0072 |          50.6823 |          15.5546 |
[32m[20221213 20:57:38 @agent_ppo2.py:185][0m |          -0.0066 |          50.7722 |          15.5543 |
[32m[20221213 20:57:38 @agent_ppo2.py:185][0m |          -0.0069 |          50.6315 |          15.5473 |
[32m[20221213 20:57:38 @agent_ppo2.py:185][0m |          -0.0084 |          50.5247 |          15.5499 |
[32m[20221213 20:57:38 @agent_ppo2.py:185][0m |          -0.0062 |          50.5206 |          15.5532 |
[32m[20221213 20:57:38 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:57:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.20
[32m[20221213 20:57:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.00
[32m[20221213 20:57:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.00
[32m[20221213 20:57:38 @agent_ppo2.py:143][0m Total time:       2.05 min
[32m[20221213 20:57:38 @agent_ppo2.py:145][0m 198656 total steps have happened
[32m[20221213 20:57:38 @agent_ppo2.py:121][0m #------------------------ Iteration 97 --------------------------#
[32m[20221213 20:57:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:38 @agent_ppo2.py:185][0m |          -0.0017 |          51.6451 |          15.7466 |
[32m[20221213 20:57:38 @agent_ppo2.py:185][0m |           0.0032 |          51.8160 |          15.7356 |
[32m[20221213 20:57:38 @agent_ppo2.py:185][0m |          -0.0030 |          51.1023 |          15.7211 |
[32m[20221213 20:57:39 @agent_ppo2.py:185][0m |          -0.0025 |          51.5010 |          15.7222 |
[32m[20221213 20:57:39 @agent_ppo2.py:185][0m |          -0.0060 |          51.2703 |          15.7181 |
[32m[20221213 20:57:39 @agent_ppo2.py:185][0m |          -0.0036 |          50.6727 |          15.7209 |
[32m[20221213 20:57:39 @agent_ppo2.py:185][0m |           0.0067 |          55.8583 |          15.7140 |
[32m[20221213 20:57:39 @agent_ppo2.py:185][0m |          -0.0092 |          50.7166 |          15.7164 |
[32m[20221213 20:57:39 @agent_ppo2.py:185][0m |          -0.0077 |          50.7428 |          15.7195 |
[32m[20221213 20:57:39 @agent_ppo2.py:185][0m |          -0.0076 |          50.6064 |          15.7193 |
[32m[20221213 20:57:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:57:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.60
[32m[20221213 20:57:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.00
[32m[20221213 20:57:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.00
[32m[20221213 20:57:39 @agent_ppo2.py:143][0m Total time:       2.07 min
[32m[20221213 20:57:39 @agent_ppo2.py:145][0m 200704 total steps have happened
[32m[20221213 20:57:39 @agent_ppo2.py:121][0m #------------------------ Iteration 98 --------------------------#
[32m[20221213 20:57:39 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:57:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:39 @agent_ppo2.py:185][0m |          -0.0021 |          48.0015 |          15.7845 |
[32m[20221213 20:57:40 @agent_ppo2.py:185][0m |          -0.0032 |          47.5619 |          15.7729 |
[32m[20221213 20:57:40 @agent_ppo2.py:185][0m |          -0.0049 |          47.3275 |          15.7654 |
[32m[20221213 20:57:40 @agent_ppo2.py:185][0m |           0.0048 |          48.2434 |          15.7620 |
[32m[20221213 20:57:40 @agent_ppo2.py:185][0m |          -0.0059 |          47.2030 |          15.7614 |
[32m[20221213 20:57:40 @agent_ppo2.py:185][0m |          -0.0036 |          47.1293 |          15.7546 |
[32m[20221213 20:57:40 @agent_ppo2.py:185][0m |          -0.0066 |          46.9887 |          15.7670 |
[32m[20221213 20:57:40 @agent_ppo2.py:185][0m |          -0.0058 |          47.1236 |          15.7637 |
[32m[20221213 20:57:40 @agent_ppo2.py:185][0m |          -0.0053 |          46.9710 |          15.7606 |
[32m[20221213 20:57:40 @agent_ppo2.py:185][0m |          -0.0062 |          46.8669 |          15.7629 |
[32m[20221213 20:57:40 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:57:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.00
[32m[20221213 20:57:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.00
[32m[20221213 20:57:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.00
[32m[20221213 20:57:40 @agent_ppo2.py:143][0m Total time:       2.09 min
[32m[20221213 20:57:40 @agent_ppo2.py:145][0m 202752 total steps have happened
[32m[20221213 20:57:40 @agent_ppo2.py:121][0m #------------------------ Iteration 99 --------------------------#
[32m[20221213 20:57:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:41 @agent_ppo2.py:185][0m |           0.0001 |          51.6790 |          15.8171 |
[32m[20221213 20:57:41 @agent_ppo2.py:185][0m |          -0.0003 |          51.3002 |          15.8186 |
[32m[20221213 20:57:41 @agent_ppo2.py:185][0m |          -0.0023 |          51.0983 |          15.8147 |
[32m[20221213 20:57:41 @agent_ppo2.py:185][0m |          -0.0021 |          50.8923 |          15.8049 |
[32m[20221213 20:57:41 @agent_ppo2.py:185][0m |          -0.0064 |          50.9723 |          15.8030 |
[32m[20221213 20:57:41 @agent_ppo2.py:185][0m |          -0.0006 |          51.1261 |          15.8076 |
[32m[20221213 20:57:41 @agent_ppo2.py:185][0m |          -0.0072 |          50.7319 |          15.8082 |
[32m[20221213 20:57:41 @agent_ppo2.py:185][0m |          -0.0047 |          50.7047 |          15.8065 |
[32m[20221213 20:57:41 @agent_ppo2.py:185][0m |          -0.0049 |          50.6001 |          15.8101 |
[32m[20221213 20:57:42 @agent_ppo2.py:185][0m |           0.0114 |          54.3085 |          15.8140 |
[32m[20221213 20:57:42 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:57:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.40
[32m[20221213 20:57:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.00
[32m[20221213 20:57:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.00
[32m[20221213 20:57:42 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 484.00
[32m[20221213 20:57:42 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 484.00
[32m[20221213 20:57:42 @agent_ppo2.py:143][0m Total time:       2.12 min
[32m[20221213 20:57:42 @agent_ppo2.py:145][0m 204800 total steps have happened
[32m[20221213 20:57:42 @agent_ppo2.py:121][0m #------------------------ Iteration 100 --------------------------#
[32m[20221213 20:57:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:42 @agent_ppo2.py:185][0m |           0.0016 |          49.6260 |          15.8126 |
[32m[20221213 20:57:42 @agent_ppo2.py:185][0m |          -0.0042 |          48.8613 |          15.8015 |
[32m[20221213 20:57:42 @agent_ppo2.py:185][0m |           0.0017 |          49.8666 |          15.7875 |
[32m[20221213 20:57:42 @agent_ppo2.py:185][0m |          -0.0016 |          48.7763 |          15.7881 |
[32m[20221213 20:57:42 @agent_ppo2.py:185][0m |           0.0044 |          51.7175 |          15.7852 |
[32m[20221213 20:57:42 @agent_ppo2.py:185][0m |          -0.0058 |          48.4935 |          15.7833 |
[32m[20221213 20:57:43 @agent_ppo2.py:185][0m |          -0.0051 |          48.3173 |          15.7804 |
[32m[20221213 20:57:43 @agent_ppo2.py:185][0m |          -0.0012 |          48.2176 |          15.7808 |
[32m[20221213 20:57:43 @agent_ppo2.py:185][0m |          -0.0070 |          48.2906 |          15.7908 |
[32m[20221213 20:57:43 @agent_ppo2.py:185][0m |          -0.0080 |          48.2475 |          15.7889 |
[32m[20221213 20:57:43 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:57:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.00
[32m[20221213 20:57:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.00
[32m[20221213 20:57:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.00
[32m[20221213 20:57:43 @agent_ppo2.py:143][0m Total time:       2.14 min
[32m[20221213 20:57:43 @agent_ppo2.py:145][0m 206848 total steps have happened
[32m[20221213 20:57:43 @agent_ppo2.py:121][0m #------------------------ Iteration 101 --------------------------#
[32m[20221213 20:57:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:57:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:43 @agent_ppo2.py:185][0m |           0.0011 |          49.5407 |          15.9796 |
[32m[20221213 20:57:43 @agent_ppo2.py:185][0m |          -0.0055 |          49.1905 |          15.9722 |
[32m[20221213 20:57:43 @agent_ppo2.py:185][0m |          -0.0045 |          49.1233 |          15.9639 |
[32m[20221213 20:57:44 @agent_ppo2.py:185][0m |           0.0032 |          52.1548 |          15.9632 |
[32m[20221213 20:57:44 @agent_ppo2.py:185][0m |          -0.0049 |          48.8505 |          15.9549 |
[32m[20221213 20:57:44 @agent_ppo2.py:185][0m |          -0.0055 |          48.8641 |          15.9625 |
[32m[20221213 20:57:44 @agent_ppo2.py:185][0m |          -0.0058 |          48.7640 |          15.9564 |
[32m[20221213 20:57:44 @agent_ppo2.py:185][0m |           0.0054 |          52.1360 |          15.9613 |
[32m[20221213 20:57:44 @agent_ppo2.py:185][0m |           0.0071 |          52.8937 |          15.9538 |
[32m[20221213 20:57:44 @agent_ppo2.py:185][0m |          -0.0073 |          48.5816 |          15.9618 |
[32m[20221213 20:57:44 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:57:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.80
[32m[20221213 20:57:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.00
[32m[20221213 20:57:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.00
[32m[20221213 20:57:44 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 540.00
[32m[20221213 20:57:44 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 540.00
[32m[20221213 20:57:44 @agent_ppo2.py:143][0m Total time:       2.16 min
[32m[20221213 20:57:44 @agent_ppo2.py:145][0m 208896 total steps have happened
[32m[20221213 20:57:44 @agent_ppo2.py:121][0m #------------------------ Iteration 102 --------------------------#
[32m[20221213 20:57:44 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:57:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:44 @agent_ppo2.py:185][0m |          -0.0030 |          51.2154 |          15.9744 |
[32m[20221213 20:57:45 @agent_ppo2.py:185][0m |          -0.0033 |          50.8487 |          15.9655 |
[32m[20221213 20:57:45 @agent_ppo2.py:185][0m |          -0.0058 |          50.7841 |          15.9445 |
[32m[20221213 20:57:45 @agent_ppo2.py:185][0m |          -0.0039 |          50.6561 |          15.9409 |
[32m[20221213 20:57:45 @agent_ppo2.py:185][0m |          -0.0053 |          50.6005 |          15.9403 |
[32m[20221213 20:57:45 @agent_ppo2.py:185][0m |          -0.0037 |          50.6459 |          15.9396 |
[32m[20221213 20:57:45 @agent_ppo2.py:185][0m |          -0.0039 |          50.4952 |          15.9269 |
[32m[20221213 20:57:45 @agent_ppo2.py:185][0m |          -0.0067 |          50.5658 |          15.9385 |
[32m[20221213 20:57:45 @agent_ppo2.py:185][0m |          -0.0038 |          50.5216 |          15.9321 |
[32m[20221213 20:57:45 @agent_ppo2.py:185][0m |          -0.0054 |          50.3779 |          15.9294 |
[32m[20221213 20:57:45 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:57:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.20
[32m[20221213 20:57:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.00
[32m[20221213 20:57:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.00
[32m[20221213 20:57:45 @agent_ppo2.py:143][0m Total time:       2.18 min
[32m[20221213 20:57:45 @agent_ppo2.py:145][0m 210944 total steps have happened
[32m[20221213 20:57:45 @agent_ppo2.py:121][0m #------------------------ Iteration 103 --------------------------#
[32m[20221213 20:57:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:46 @agent_ppo2.py:185][0m |          -0.0036 |          51.3876 |          15.9134 |
[32m[20221213 20:57:46 @agent_ppo2.py:185][0m |          -0.0086 |          51.1253 |          15.9031 |
[32m[20221213 20:57:46 @agent_ppo2.py:185][0m |          -0.0071 |          51.0392 |          15.8914 |
[32m[20221213 20:57:46 @agent_ppo2.py:185][0m |          -0.0031 |          50.6890 |          15.8976 |
[32m[20221213 20:57:46 @agent_ppo2.py:185][0m |          -0.0025 |          50.8848 |          15.8964 |
[32m[20221213 20:57:46 @agent_ppo2.py:185][0m |          -0.0046 |          50.4896 |          15.8933 |
[32m[20221213 20:57:46 @agent_ppo2.py:185][0m |           0.0013 |          51.7057 |          15.8934 |
[32m[20221213 20:57:46 @agent_ppo2.py:185][0m |           0.0014 |          51.4826 |          15.8934 |
[32m[20221213 20:57:46 @agent_ppo2.py:185][0m |          -0.0075 |          50.4024 |          15.8924 |
[32m[20221213 20:57:46 @agent_ppo2.py:185][0m |           0.0028 |          51.4747 |          15.8984 |
[32m[20221213 20:57:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:57:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.40
[32m[20221213 20:57:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.00
[32m[20221213 20:57:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.00
[32m[20221213 20:57:47 @agent_ppo2.py:143][0m Total time:       2.20 min
[32m[20221213 20:57:47 @agent_ppo2.py:145][0m 212992 total steps have happened
[32m[20221213 20:57:47 @agent_ppo2.py:121][0m #------------------------ Iteration 104 --------------------------#
[32m[20221213 20:57:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:57:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:47 @agent_ppo2.py:185][0m |          -0.0008 |          51.5021 |          16.0963 |
[32m[20221213 20:57:47 @agent_ppo2.py:185][0m |           0.0034 |          54.4930 |          16.0791 |
[32m[20221213 20:57:47 @agent_ppo2.py:185][0m |          -0.0040 |          51.1599 |          16.0555 |
[32m[20221213 20:57:47 @agent_ppo2.py:185][0m |          -0.0068 |          51.1473 |          16.0611 |
[32m[20221213 20:57:47 @agent_ppo2.py:185][0m |          -0.0049 |          51.0365 |          16.0622 |
[32m[20221213 20:57:47 @agent_ppo2.py:185][0m |          -0.0015 |          51.3059 |          16.0627 |
[32m[20221213 20:57:47 @agent_ppo2.py:185][0m |           0.0040 |          53.7538 |          16.0559 |
[32m[20221213 20:57:48 @agent_ppo2.py:185][0m |          -0.0053 |          51.0357 |          16.0593 |
[32m[20221213 20:57:48 @agent_ppo2.py:185][0m |          -0.0053 |          50.9929 |          16.0643 |
[32m[20221213 20:57:48 @agent_ppo2.py:185][0m |          -0.0046 |          50.9511 |          16.0555 |
[32m[20221213 20:57:48 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:57:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.20
[32m[20221213 20:57:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.00
[32m[20221213 20:57:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.00
[32m[20221213 20:57:48 @agent_ppo2.py:143][0m Total time:       2.22 min
[32m[20221213 20:57:48 @agent_ppo2.py:145][0m 215040 total steps have happened
[32m[20221213 20:57:48 @agent_ppo2.py:121][0m #------------------------ Iteration 105 --------------------------#
[32m[20221213 20:57:48 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 20:57:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:48 @agent_ppo2.py:185][0m |           0.0014 |          49.6460 |          16.0075 |
[32m[20221213 20:57:48 @agent_ppo2.py:185][0m |          -0.0051 |          49.4693 |          15.9791 |
[32m[20221213 20:57:49 @agent_ppo2.py:185][0m |          -0.0061 |          49.3218 |          15.9810 |
[32m[20221213 20:57:49 @agent_ppo2.py:185][0m |          -0.0016 |          49.6309 |          15.9701 |
[32m[20221213 20:57:49 @agent_ppo2.py:185][0m |          -0.0068 |          49.2547 |          15.9885 |
[32m[20221213 20:57:49 @agent_ppo2.py:185][0m |          -0.0075 |          49.1448 |          15.9810 |
[32m[20221213 20:57:49 @agent_ppo2.py:185][0m |          -0.0065 |          49.0826 |          15.9736 |
[32m[20221213 20:57:49 @agent_ppo2.py:185][0m |          -0.0058 |          49.0312 |          15.9819 |
[32m[20221213 20:57:49 @agent_ppo2.py:185][0m |          -0.0065 |          48.9931 |          15.9833 |
[32m[20221213 20:57:49 @agent_ppo2.py:185][0m |           0.0056 |          51.3865 |          15.9860 |
[32m[20221213 20:57:49 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:57:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.60
[32m[20221213 20:57:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.00
[32m[20221213 20:57:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 524.00
[32m[20221213 20:57:49 @agent_ppo2.py:143][0m Total time:       2.24 min
[32m[20221213 20:57:49 @agent_ppo2.py:145][0m 217088 total steps have happened
[32m[20221213 20:57:49 @agent_ppo2.py:121][0m #------------------------ Iteration 106 --------------------------#
[32m[20221213 20:57:49 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:57:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:50 @agent_ppo2.py:185][0m |          -0.0000 |          53.2924 |          16.0314 |
[32m[20221213 20:57:50 @agent_ppo2.py:185][0m |          -0.0055 |          52.7784 |          16.0214 |
[32m[20221213 20:57:50 @agent_ppo2.py:185][0m |          -0.0082 |          52.7224 |          16.0203 |
[32m[20221213 20:57:50 @agent_ppo2.py:185][0m |          -0.0065 |          52.6111 |          16.0075 |
[32m[20221213 20:57:50 @agent_ppo2.py:185][0m |          -0.0009 |          55.5298 |          16.0120 |
[32m[20221213 20:57:50 @agent_ppo2.py:185][0m |           0.0026 |          54.9416 |          16.0090 |
[32m[20221213 20:57:50 @agent_ppo2.py:185][0m |          -0.0079 |          52.4695 |          16.0072 |
[32m[20221213 20:57:50 @agent_ppo2.py:185][0m |          -0.0006 |          56.9509 |          16.0061 |
[32m[20221213 20:57:50 @agent_ppo2.py:185][0m |          -0.0055 |          52.3595 |          16.0085 |
[32m[20221213 20:57:51 @agent_ppo2.py:185][0m |          -0.0090 |          52.2370 |          16.0073 |
[32m[20221213 20:57:51 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 20:57:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.00
[32m[20221213 20:57:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.00
[32m[20221213 20:57:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.00
[32m[20221213 20:57:51 @agent_ppo2.py:143][0m Total time:       2.27 min
[32m[20221213 20:57:51 @agent_ppo2.py:145][0m 219136 total steps have happened
[32m[20221213 20:57:51 @agent_ppo2.py:121][0m #------------------------ Iteration 107 --------------------------#
[32m[20221213 20:57:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:51 @agent_ppo2.py:185][0m |           0.0035 |          53.1286 |          16.2161 |
[32m[20221213 20:57:51 @agent_ppo2.py:185][0m |           0.0072 |          54.4360 |          16.2005 |
[32m[20221213 20:57:51 @agent_ppo2.py:185][0m |          -0.0017 |          51.9952 |          16.1873 |
[32m[20221213 20:57:51 @agent_ppo2.py:185][0m |          -0.0047 |          51.6300 |          16.1852 |
[32m[20221213 20:57:51 @agent_ppo2.py:185][0m |          -0.0074 |          51.6044 |          16.1890 |
[32m[20221213 20:57:51 @agent_ppo2.py:185][0m |          -0.0046 |          51.3856 |          16.1810 |
[32m[20221213 20:57:51 @agent_ppo2.py:185][0m |          -0.0060 |          51.2560 |          16.1801 |
[32m[20221213 20:57:52 @agent_ppo2.py:185][0m |          -0.0061 |          51.1063 |          16.1785 |
[32m[20221213 20:57:52 @agent_ppo2.py:185][0m |          -0.0025 |          51.2617 |          16.1791 |
[32m[20221213 20:57:52 @agent_ppo2.py:185][0m |          -0.0064 |          50.9059 |          16.1818 |
[32m[20221213 20:57:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:57:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.00
[32m[20221213 20:57:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.00
[32m[20221213 20:57:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.00
[32m[20221213 20:57:52 @agent_ppo2.py:143][0m Total time:       2.29 min
[32m[20221213 20:57:52 @agent_ppo2.py:145][0m 221184 total steps have happened
[32m[20221213 20:57:52 @agent_ppo2.py:121][0m #------------------------ Iteration 108 --------------------------#
[32m[20221213 20:57:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 20:57:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:52 @agent_ppo2.py:185][0m |          -0.0019 |          52.7146 |          16.2300 |
[32m[20221213 20:57:52 @agent_ppo2.py:185][0m |          -0.0022 |          52.7757 |          16.2144 |
[32m[20221213 20:57:52 @agent_ppo2.py:185][0m |          -0.0055 |          52.2597 |          16.2081 |
[32m[20221213 20:57:53 @agent_ppo2.py:185][0m |          -0.0029 |          52.1058 |          16.2058 |
[32m[20221213 20:57:53 @agent_ppo2.py:185][0m |          -0.0034 |          52.0806 |          16.2084 |
[32m[20221213 20:57:53 @agent_ppo2.py:185][0m |          -0.0021 |          52.5162 |          16.2024 |
[32m[20221213 20:57:53 @agent_ppo2.py:185][0m |          -0.0091 |          51.9126 |          16.2044 |
[32m[20221213 20:57:53 @agent_ppo2.py:185][0m |          -0.0103 |          51.9647 |          16.1996 |
[32m[20221213 20:57:53 @agent_ppo2.py:185][0m |          -0.0055 |          51.6997 |          16.2033 |
[32m[20221213 20:57:53 @agent_ppo2.py:185][0m |          -0.0046 |          51.7551 |          16.2015 |
[32m[20221213 20:57:53 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:57:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.20
[32m[20221213 20:57:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.00
[32m[20221213 20:57:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.00
[32m[20221213 20:57:53 @agent_ppo2.py:143][0m Total time:       2.31 min
[32m[20221213 20:57:53 @agent_ppo2.py:145][0m 223232 total steps have happened
[32m[20221213 20:57:53 @agent_ppo2.py:121][0m #------------------------ Iteration 109 --------------------------#
[32m[20221213 20:57:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:53 @agent_ppo2.py:185][0m |          -0.0007 |          54.6022 |          16.1329 |
[32m[20221213 20:57:54 @agent_ppo2.py:185][0m |          -0.0047 |          53.9732 |          16.1187 |
[32m[20221213 20:57:54 @agent_ppo2.py:185][0m |          -0.0026 |          54.3600 |          16.1199 |
[32m[20221213 20:57:54 @agent_ppo2.py:185][0m |          -0.0090 |          53.6654 |          16.1114 |
[32m[20221213 20:57:54 @agent_ppo2.py:185][0m |          -0.0036 |          54.0612 |          16.1163 |
[32m[20221213 20:57:54 @agent_ppo2.py:185][0m |          -0.0067 |          53.5700 |          16.1161 |
[32m[20221213 20:57:54 @agent_ppo2.py:185][0m |          -0.0017 |          54.5155 |          16.1166 |
[32m[20221213 20:57:54 @agent_ppo2.py:185][0m |          -0.0050 |          53.4473 |          16.1097 |
[32m[20221213 20:57:54 @agent_ppo2.py:185][0m |          -0.0135 |          53.5639 |          16.1222 |
[32m[20221213 20:57:54 @agent_ppo2.py:185][0m |          -0.0076 |          53.3706 |          16.1190 |
[32m[20221213 20:57:54 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:57:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.80
[32m[20221213 20:57:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.00
[32m[20221213 20:57:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.00
[32m[20221213 20:57:54 @agent_ppo2.py:143][0m Total time:       2.33 min
[32m[20221213 20:57:54 @agent_ppo2.py:145][0m 225280 total steps have happened
[32m[20221213 20:57:54 @agent_ppo2.py:121][0m #------------------------ Iteration 110 --------------------------#
[32m[20221213 20:57:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:55 @agent_ppo2.py:185][0m |          -0.0027 |          55.6986 |          16.2964 |
[32m[20221213 20:57:55 @agent_ppo2.py:185][0m |          -0.0049 |          55.4711 |          16.2916 |
[32m[20221213 20:57:55 @agent_ppo2.py:185][0m |          -0.0098 |          55.3008 |          16.2842 |
[32m[20221213 20:57:55 @agent_ppo2.py:185][0m |          -0.0047 |          56.0861 |          16.2758 |
[32m[20221213 20:57:55 @agent_ppo2.py:185][0m |          -0.0080 |          55.1356 |          16.2764 |
[32m[20221213 20:57:55 @agent_ppo2.py:185][0m |          -0.0077 |          54.9417 |          16.2775 |
[32m[20221213 20:57:55 @agent_ppo2.py:185][0m |          -0.0059 |          55.1574 |          16.2756 |
[32m[20221213 20:57:55 @agent_ppo2.py:185][0m |           0.0077 |          60.8816 |          16.2791 |
[32m[20221213 20:57:55 @agent_ppo2.py:185][0m |          -0.0096 |          54.9133 |          16.2769 |
[32m[20221213 20:57:56 @agent_ppo2.py:185][0m |          -0.0068 |          54.9157 |          16.2780 |
[32m[20221213 20:57:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:57:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.40
[32m[20221213 20:57:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.00
[32m[20221213 20:57:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.00
[32m[20221213 20:57:56 @agent_ppo2.py:143][0m Total time:       2.35 min
[32m[20221213 20:57:56 @agent_ppo2.py:145][0m 227328 total steps have happened
[32m[20221213 20:57:56 @agent_ppo2.py:121][0m #------------------------ Iteration 111 --------------------------#
[32m[20221213 20:57:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:56 @agent_ppo2.py:185][0m |          -0.0059 |          57.3162 |          16.4430 |
[32m[20221213 20:57:56 @agent_ppo2.py:185][0m |          -0.0047 |          56.8603 |          16.4319 |
[32m[20221213 20:57:56 @agent_ppo2.py:185][0m |          -0.0040 |          56.5603 |          16.4223 |
[32m[20221213 20:57:56 @agent_ppo2.py:185][0m |          -0.0055 |          56.3675 |          16.4200 |
[32m[20221213 20:57:56 @agent_ppo2.py:185][0m |           0.0068 |          60.8105 |          16.4204 |
[32m[20221213 20:57:56 @agent_ppo2.py:185][0m |          -0.0070 |          56.3720 |          16.4124 |
[32m[20221213 20:57:57 @agent_ppo2.py:185][0m |          -0.0075 |          56.2610 |          16.4220 |
[32m[20221213 20:57:57 @agent_ppo2.py:185][0m |          -0.0054 |          56.2562 |          16.4129 |
[32m[20221213 20:57:57 @agent_ppo2.py:185][0m |          -0.0071 |          56.0770 |          16.4199 |
[32m[20221213 20:57:57 @agent_ppo2.py:185][0m |          -0.0086 |          56.2081 |          16.4182 |
[32m[20221213 20:57:57 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:57:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.80
[32m[20221213 20:57:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.00
[32m[20221213 20:57:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.00
[32m[20221213 20:57:57 @agent_ppo2.py:143][0m Total time:       2.37 min
[32m[20221213 20:57:57 @agent_ppo2.py:145][0m 229376 total steps have happened
[32m[20221213 20:57:57 @agent_ppo2.py:121][0m #------------------------ Iteration 112 --------------------------#
[32m[20221213 20:57:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:57 @agent_ppo2.py:185][0m |           0.0005 |          53.8975 |          16.1849 |
[32m[20221213 20:57:57 @agent_ppo2.py:185][0m |          -0.0082 |          53.3418 |          16.1740 |
[32m[20221213 20:57:57 @agent_ppo2.py:185][0m |          -0.0027 |          53.0218 |          16.1765 |
[32m[20221213 20:57:58 @agent_ppo2.py:185][0m |          -0.0063 |          52.8511 |          16.1727 |
[32m[20221213 20:57:58 @agent_ppo2.py:185][0m |           0.0005 |          54.3089 |          16.1725 |
[32m[20221213 20:57:58 @agent_ppo2.py:185][0m |          -0.0042 |          52.5686 |          16.1671 |
[32m[20221213 20:57:58 @agent_ppo2.py:185][0m |          -0.0067 |          52.6973 |          16.1719 |
[32m[20221213 20:57:58 @agent_ppo2.py:185][0m |          -0.0023 |          52.6227 |          16.1681 |
[32m[20221213 20:57:58 @agent_ppo2.py:185][0m |          -0.0050 |          52.4984 |          16.1744 |
[32m[20221213 20:57:58 @agent_ppo2.py:185][0m |          -0.0023 |          52.6231 |          16.1689 |
[32m[20221213 20:57:58 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:57:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.40
[32m[20221213 20:57:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.00
[32m[20221213 20:57:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.00
[32m[20221213 20:57:58 @agent_ppo2.py:143][0m Total time:       2.39 min
[32m[20221213 20:57:58 @agent_ppo2.py:145][0m 231424 total steps have happened
[32m[20221213 20:57:58 @agent_ppo2.py:121][0m #------------------------ Iteration 113 --------------------------#
[32m[20221213 20:57:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:57:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:57:59 @agent_ppo2.py:185][0m |          -0.0019 |          54.9321 |          16.4506 |
[32m[20221213 20:57:59 @agent_ppo2.py:185][0m |          -0.0060 |          54.3504 |          16.4372 |
[32m[20221213 20:57:59 @agent_ppo2.py:185][0m |          -0.0050 |          54.1156 |          16.4331 |
[32m[20221213 20:57:59 @agent_ppo2.py:185][0m |          -0.0040 |          53.8565 |          16.4285 |
[32m[20221213 20:57:59 @agent_ppo2.py:185][0m |          -0.0036 |          53.7527 |          16.4259 |
[32m[20221213 20:57:59 @agent_ppo2.py:185][0m |          -0.0090 |          53.6584 |          16.4280 |
[32m[20221213 20:57:59 @agent_ppo2.py:185][0m |          -0.0052 |          53.6605 |          16.4237 |
[32m[20221213 20:57:59 @agent_ppo2.py:185][0m |          -0.0051 |          53.3891 |          16.4232 |
[32m[20221213 20:57:59 @agent_ppo2.py:185][0m |          -0.0073 |          53.3589 |          16.4211 |
[32m[20221213 20:57:59 @agent_ppo2.py:185][0m |          -0.0077 |          53.3323 |          16.4204 |
[32m[20221213 20:57:59 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:57:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.20
[32m[20221213 20:57:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.00
[32m[20221213 20:57:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.00
[32m[20221213 20:57:59 @agent_ppo2.py:143][0m Total time:       2.41 min
[32m[20221213 20:57:59 @agent_ppo2.py:145][0m 233472 total steps have happened
[32m[20221213 20:57:59 @agent_ppo2.py:121][0m #------------------------ Iteration 114 --------------------------#
[32m[20221213 20:58:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:58:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:00 @agent_ppo2.py:185][0m |          -0.0024 |          60.6095 |          16.3532 |
[32m[20221213 20:58:00 @agent_ppo2.py:185][0m |          -0.0014 |          60.2546 |          16.3412 |
[32m[20221213 20:58:00 @agent_ppo2.py:185][0m |          -0.0019 |          59.9276 |          16.3366 |
[32m[20221213 20:58:00 @agent_ppo2.py:185][0m |          -0.0017 |          59.9362 |          16.3312 |
[32m[20221213 20:58:00 @agent_ppo2.py:185][0m |          -0.0036 |          59.6742 |          16.3285 |
[32m[20221213 20:58:00 @agent_ppo2.py:185][0m |          -0.0004 |          60.2707 |          16.3274 |
[32m[20221213 20:58:00 @agent_ppo2.py:185][0m |           0.0041 |          63.8623 |          16.3337 |
[32m[20221213 20:58:00 @agent_ppo2.py:185][0m |          -0.0041 |          59.3359 |          16.3329 |
[32m[20221213 20:58:00 @agent_ppo2.py:185][0m |          -0.0082 |          59.4805 |          16.3182 |
[32m[20221213 20:58:01 @agent_ppo2.py:185][0m |          -0.0062 |          59.2112 |          16.3192 |
[32m[20221213 20:58:01 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:58:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.60
[32m[20221213 20:58:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.00
[32m[20221213 20:58:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.00
[32m[20221213 20:58:01 @agent_ppo2.py:143][0m Total time:       2.43 min
[32m[20221213 20:58:01 @agent_ppo2.py:145][0m 235520 total steps have happened
[32m[20221213 20:58:01 @agent_ppo2.py:121][0m #------------------------ Iteration 115 --------------------------#
[32m[20221213 20:58:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:01 @agent_ppo2.py:185][0m |          -0.0004 |          59.0292 |          16.2350 |
[32m[20221213 20:58:01 @agent_ppo2.py:185][0m |          -0.0039 |          58.3339 |          16.2164 |
[32m[20221213 20:58:01 @agent_ppo2.py:185][0m |          -0.0020 |          58.0693 |          16.2082 |
[32m[20221213 20:58:01 @agent_ppo2.py:185][0m |          -0.0046 |          57.8352 |          16.2116 |
[32m[20221213 20:58:01 @agent_ppo2.py:185][0m |          -0.0088 |          57.9239 |          16.2107 |
[32m[20221213 20:58:01 @agent_ppo2.py:185][0m |          -0.0073 |          57.5940 |          16.2131 |
[32m[20221213 20:58:02 @agent_ppo2.py:185][0m |           0.0016 |          60.8552 |          16.2120 |
[32m[20221213 20:58:02 @agent_ppo2.py:185][0m |          -0.0077 |          57.4121 |          16.2123 |
[32m[20221213 20:58:02 @agent_ppo2.py:185][0m |          -0.0076 |          57.3117 |          16.2127 |
[32m[20221213 20:58:02 @agent_ppo2.py:185][0m |          -0.0013 |          58.9290 |          16.2086 |
[32m[20221213 20:58:02 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:58:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.80
[32m[20221213 20:58:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.00
[32m[20221213 20:58:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.00
[32m[20221213 20:58:02 @agent_ppo2.py:143][0m Total time:       2.45 min
[32m[20221213 20:58:02 @agent_ppo2.py:145][0m 237568 total steps have happened
[32m[20221213 20:58:02 @agent_ppo2.py:121][0m #------------------------ Iteration 116 --------------------------#
[32m[20221213 20:58:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:02 @agent_ppo2.py:185][0m |          -0.0020 |          57.2389 |          16.4111 |
[32m[20221213 20:58:02 @agent_ppo2.py:185][0m |           0.0024 |          59.7655 |          16.3943 |
[32m[20221213 20:58:02 @agent_ppo2.py:185][0m |          -0.0058 |          55.8809 |          16.3786 |
[32m[20221213 20:58:02 @agent_ppo2.py:185][0m |           0.0022 |          59.8266 |          16.3736 |
[32m[20221213 20:58:03 @agent_ppo2.py:185][0m |          -0.0063 |          55.7110 |          16.3585 |
[32m[20221213 20:58:03 @agent_ppo2.py:185][0m |          -0.0075 |          55.1774 |          16.3692 |
[32m[20221213 20:58:03 @agent_ppo2.py:185][0m |           0.0060 |          61.1337 |          16.3675 |
[32m[20221213 20:58:03 @agent_ppo2.py:185][0m |          -0.0060 |          54.9112 |          16.3699 |
[32m[20221213 20:58:03 @agent_ppo2.py:185][0m |          -0.0074 |          54.7387 |          16.3705 |
[32m[20221213 20:58:03 @agent_ppo2.py:185][0m |          -0.0087 |          54.5254 |          16.3701 |
[32m[20221213 20:58:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:58:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.00
[32m[20221213 20:58:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.00
[32m[20221213 20:58:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.00
[32m[20221213 20:58:03 @agent_ppo2.py:143][0m Total time:       2.47 min
[32m[20221213 20:58:03 @agent_ppo2.py:145][0m 239616 total steps have happened
[32m[20221213 20:58:03 @agent_ppo2.py:121][0m #------------------------ Iteration 117 --------------------------#
[32m[20221213 20:58:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:58:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:03 @agent_ppo2.py:185][0m |           0.0005 |          59.0906 |          16.4135 |
[32m[20221213 20:58:04 @agent_ppo2.py:185][0m |          -0.0055 |          58.6661 |          16.3967 |
[32m[20221213 20:58:04 @agent_ppo2.py:185][0m |          -0.0039 |          58.8681 |          16.3956 |
[32m[20221213 20:58:04 @agent_ppo2.py:185][0m |          -0.0061 |          58.3547 |          16.3873 |
[32m[20221213 20:58:04 @agent_ppo2.py:185][0m |           0.0003 |          58.7889 |          16.3861 |
[32m[20221213 20:58:04 @agent_ppo2.py:185][0m |          -0.0065 |          58.1590 |          16.3821 |
[32m[20221213 20:58:04 @agent_ppo2.py:185][0m |           0.0023 |          59.6336 |          16.3829 |
[32m[20221213 20:58:04 @agent_ppo2.py:185][0m |          -0.0043 |          57.8838 |          16.3822 |
[32m[20221213 20:58:04 @agent_ppo2.py:185][0m |          -0.0015 |          57.8641 |          16.3824 |
[32m[20221213 20:58:04 @agent_ppo2.py:185][0m |          -0.0046 |          57.7094 |          16.3794 |
[32m[20221213 20:58:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:58:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.80
[32m[20221213 20:58:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.00
[32m[20221213 20:58:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.00
[32m[20221213 20:58:04 @agent_ppo2.py:143][0m Total time:       2.49 min
[32m[20221213 20:58:04 @agent_ppo2.py:145][0m 241664 total steps have happened
[32m[20221213 20:58:04 @agent_ppo2.py:121][0m #------------------------ Iteration 118 --------------------------#
[32m[20221213 20:58:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:05 @agent_ppo2.py:185][0m |           0.0061 |          57.3550 |          16.4277 |
[32m[20221213 20:58:05 @agent_ppo2.py:185][0m |           0.0049 |          58.7229 |          16.4150 |
[32m[20221213 20:58:05 @agent_ppo2.py:185][0m |           0.0043 |          59.2826 |          16.4115 |
[32m[20221213 20:58:05 @agent_ppo2.py:185][0m |          -0.0017 |          55.6607 |          16.4046 |
[32m[20221213 20:58:05 @agent_ppo2.py:185][0m |          -0.0063 |          55.5734 |          16.4004 |
[32m[20221213 20:58:05 @agent_ppo2.py:185][0m |          -0.0045 |          56.2393 |          16.4033 |
[32m[20221213 20:58:05 @agent_ppo2.py:185][0m |          -0.0064 |          55.4248 |          16.3974 |
[32m[20221213 20:58:05 @agent_ppo2.py:185][0m |          -0.0035 |          55.3971 |          16.4035 |
[32m[20221213 20:58:05 @agent_ppo2.py:185][0m |          -0.0086 |          55.4027 |          16.3948 |
[32m[20221213 20:58:05 @agent_ppo2.py:185][0m |          -0.0078 |          55.3539 |          16.3970 |
[32m[20221213 20:58:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:58:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.60
[32m[20221213 20:58:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.00
[32m[20221213 20:58:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.00
[32m[20221213 20:58:06 @agent_ppo2.py:143][0m Total time:       2.51 min
[32m[20221213 20:58:06 @agent_ppo2.py:145][0m 243712 total steps have happened
[32m[20221213 20:58:06 @agent_ppo2.py:121][0m #------------------------ Iteration 119 --------------------------#
[32m[20221213 20:58:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:06 @agent_ppo2.py:185][0m |          -0.0018 |          55.3555 |          16.4019 |
[32m[20221213 20:58:06 @agent_ppo2.py:185][0m |          -0.0049 |          54.8784 |          16.3958 |
[32m[20221213 20:58:06 @agent_ppo2.py:185][0m |          -0.0023 |          54.8452 |          16.3795 |
[32m[20221213 20:58:06 @agent_ppo2.py:185][0m |          -0.0050 |          54.6113 |          16.3815 |
[32m[20221213 20:58:06 @agent_ppo2.py:185][0m |          -0.0043 |          54.5067 |          16.3767 |
[32m[20221213 20:58:06 @agent_ppo2.py:185][0m |          -0.0062 |          54.4737 |          16.3785 |
[32m[20221213 20:58:06 @agent_ppo2.py:185][0m |           0.0065 |          58.3785 |          16.3732 |
[32m[20221213 20:58:06 @agent_ppo2.py:185][0m |          -0.0062 |          54.5379 |          16.3753 |
[32m[20221213 20:58:07 @agent_ppo2.py:185][0m |          -0.0082 |          54.2100 |          16.3833 |
[32m[20221213 20:58:07 @agent_ppo2.py:185][0m |           0.0008 |          56.7200 |          16.3804 |
[32m[20221213 20:58:07 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:58:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.00
[32m[20221213 20:58:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.00
[32m[20221213 20:58:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.00
[32m[20221213 20:58:07 @agent_ppo2.py:143][0m Total time:       2.54 min
[32m[20221213 20:58:07 @agent_ppo2.py:145][0m 245760 total steps have happened
[32m[20221213 20:58:07 @agent_ppo2.py:121][0m #------------------------ Iteration 120 --------------------------#
[32m[20221213 20:58:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:07 @agent_ppo2.py:185][0m |          -0.0002 |          54.9772 |          16.5141 |
[32m[20221213 20:58:07 @agent_ppo2.py:185][0m |          -0.0012 |          54.4552 |          16.5057 |
[32m[20221213 20:58:07 @agent_ppo2.py:185][0m |          -0.0071 |          54.3532 |          16.4977 |
[32m[20221213 20:58:07 @agent_ppo2.py:185][0m |          -0.0050 |          54.0689 |          16.4936 |
[32m[20221213 20:58:07 @agent_ppo2.py:185][0m |          -0.0083 |          54.0040 |          16.4966 |
[32m[20221213 20:58:08 @agent_ppo2.py:185][0m |          -0.0066 |          53.9683 |          16.4999 |
[32m[20221213 20:58:08 @agent_ppo2.py:185][0m |          -0.0059 |          53.7333 |          16.4923 |
[32m[20221213 20:58:08 @agent_ppo2.py:185][0m |           0.0018 |          54.7145 |          16.4943 |
[32m[20221213 20:58:08 @agent_ppo2.py:185][0m |          -0.0053 |          53.5757 |          16.4952 |
[32m[20221213 20:58:08 @agent_ppo2.py:185][0m |          -0.0074 |          53.4969 |          16.4936 |
[32m[20221213 20:58:08 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 20:58:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.00
[32m[20221213 20:58:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.00
[32m[20221213 20:58:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 533.00
[32m[20221213 20:58:08 @agent_ppo2.py:143][0m Total time:       2.56 min
[32m[20221213 20:58:08 @agent_ppo2.py:145][0m 247808 total steps have happened
[32m[20221213 20:58:08 @agent_ppo2.py:121][0m #------------------------ Iteration 121 --------------------------#
[32m[20221213 20:58:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:09 @agent_ppo2.py:185][0m |          -0.0014 |          57.5370 |          16.5977 |
[32m[20221213 20:58:09 @agent_ppo2.py:185][0m |           0.0091 |          62.3261 |          16.5987 |
[32m[20221213 20:58:09 @agent_ppo2.py:185][0m |          -0.0037 |          56.7903 |          16.5820 |
[32m[20221213 20:58:09 @agent_ppo2.py:185][0m |          -0.0016 |          56.5032 |          16.5862 |
[32m[20221213 20:58:09 @agent_ppo2.py:185][0m |          -0.0040 |          56.3902 |          16.5850 |
[32m[20221213 20:58:09 @agent_ppo2.py:185][0m |           0.0012 |          57.7895 |          16.5818 |
[32m[20221213 20:58:09 @agent_ppo2.py:185][0m |          -0.0037 |          56.0184 |          16.5842 |
[32m[20221213 20:58:09 @agent_ppo2.py:185][0m |          -0.0072 |          56.0408 |          16.5743 |
[32m[20221213 20:58:09 @agent_ppo2.py:185][0m |          -0.0052 |          55.9397 |          16.5833 |
[32m[20221213 20:58:09 @agent_ppo2.py:185][0m |          -0.0049 |          55.7729 |          16.5799 |
[32m[20221213 20:58:09 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:58:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.80
[32m[20221213 20:58:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.00
[32m[20221213 20:58:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.00
[32m[20221213 20:58:09 @agent_ppo2.py:143][0m Total time:       2.58 min
[32m[20221213 20:58:09 @agent_ppo2.py:145][0m 249856 total steps have happened
[32m[20221213 20:58:09 @agent_ppo2.py:121][0m #------------------------ Iteration 122 --------------------------#
[32m[20221213 20:58:10 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:58:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:10 @agent_ppo2.py:185][0m |          -0.0033 |          57.0866 |          16.6529 |
[32m[20221213 20:58:10 @agent_ppo2.py:185][0m |          -0.0059 |          56.6008 |          16.6255 |
[32m[20221213 20:58:10 @agent_ppo2.py:185][0m |          -0.0053 |          56.1843 |          16.6079 |
[32m[20221213 20:58:10 @agent_ppo2.py:185][0m |          -0.0066 |          56.0193 |          16.6181 |
[32m[20221213 20:58:10 @agent_ppo2.py:185][0m |           0.0075 |          61.5092 |          16.6072 |
[32m[20221213 20:58:10 @agent_ppo2.py:185][0m |          -0.0049 |          55.9455 |          16.6144 |
[32m[20221213 20:58:10 @agent_ppo2.py:185][0m |          -0.0076 |          55.7356 |          16.6050 |
[32m[20221213 20:58:10 @agent_ppo2.py:185][0m |          -0.0088 |          55.7239 |          16.6057 |
[32m[20221213 20:58:10 @agent_ppo2.py:185][0m |          -0.0056 |          55.6221 |          16.6057 |
[32m[20221213 20:58:11 @agent_ppo2.py:185][0m |          -0.0068 |          55.5372 |          16.6060 |
[32m[20221213 20:58:11 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:58:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.60
[32m[20221213 20:58:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.00
[32m[20221213 20:58:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.00
[32m[20221213 20:58:11 @agent_ppo2.py:143][0m Total time:       2.60 min
[32m[20221213 20:58:11 @agent_ppo2.py:145][0m 251904 total steps have happened
[32m[20221213 20:58:11 @agent_ppo2.py:121][0m #------------------------ Iteration 123 --------------------------#
[32m[20221213 20:58:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:58:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:11 @agent_ppo2.py:185][0m |          -0.0007 |          56.1956 |          16.5846 |
[32m[20221213 20:58:11 @agent_ppo2.py:185][0m |           0.0052 |          58.2670 |          16.5700 |
[32m[20221213 20:58:11 @agent_ppo2.py:185][0m |           0.0039 |          56.2962 |          16.5544 |
[32m[20221213 20:58:11 @agent_ppo2.py:185][0m |          -0.0037 |          55.4156 |          16.5612 |
[32m[20221213 20:58:11 @agent_ppo2.py:185][0m |           0.0007 |          56.1275 |          16.5554 |
[32m[20221213 20:58:11 @agent_ppo2.py:185][0m |          -0.0051 |          54.9189 |          16.5543 |
[32m[20221213 20:58:12 @agent_ppo2.py:185][0m |          -0.0081 |          54.8800 |          16.5475 |
[32m[20221213 20:58:12 @agent_ppo2.py:185][0m |          -0.0058 |          54.7886 |          16.5507 |
[32m[20221213 20:58:12 @agent_ppo2.py:185][0m |          -0.0026 |          55.3397 |          16.5487 |
[32m[20221213 20:58:12 @agent_ppo2.py:185][0m |          -0.0044 |          54.4165 |          16.5381 |
[32m[20221213 20:58:12 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:58:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.20
[32m[20221213 20:58:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.00
[32m[20221213 20:58:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.00
[32m[20221213 20:58:12 @agent_ppo2.py:143][0m Total time:       2.62 min
[32m[20221213 20:58:12 @agent_ppo2.py:145][0m 253952 total steps have happened
[32m[20221213 20:58:12 @agent_ppo2.py:121][0m #------------------------ Iteration 124 --------------------------#
[32m[20221213 20:58:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:12 @agent_ppo2.py:185][0m |           0.0070 |          59.5754 |          16.6517 |
[32m[20221213 20:58:12 @agent_ppo2.py:185][0m |          -0.0037 |          54.6536 |          16.6285 |
[32m[20221213 20:58:12 @agent_ppo2.py:185][0m |          -0.0068 |          54.3685 |          16.6135 |
[32m[20221213 20:58:13 @agent_ppo2.py:185][0m |          -0.0081 |          54.3553 |          16.6124 |
[32m[20221213 20:58:13 @agent_ppo2.py:185][0m |          -0.0061 |          54.2274 |          16.6139 |
[32m[20221213 20:58:13 @agent_ppo2.py:185][0m |          -0.0075 |          54.4484 |          16.6124 |
[32m[20221213 20:58:13 @agent_ppo2.py:185][0m |          -0.0057 |          54.0491 |          16.6091 |
[32m[20221213 20:58:13 @agent_ppo2.py:185][0m |          -0.0027 |          55.2270 |          16.6129 |
[32m[20221213 20:58:13 @agent_ppo2.py:185][0m |          -0.0091 |          54.0314 |          16.6107 |
[32m[20221213 20:58:13 @agent_ppo2.py:185][0m |          -0.0084 |          53.8624 |          16.6083 |
[32m[20221213 20:58:13 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:58:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.40
[32m[20221213 20:58:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.00
[32m[20221213 20:58:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.00
[32m[20221213 20:58:13 @agent_ppo2.py:143][0m Total time:       2.64 min
[32m[20221213 20:58:13 @agent_ppo2.py:145][0m 256000 total steps have happened
[32m[20221213 20:58:13 @agent_ppo2.py:121][0m #------------------------ Iteration 125 --------------------------#
[32m[20221213 20:58:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:14 @agent_ppo2.py:185][0m |          -0.0011 |          57.2374 |          16.5771 |
[32m[20221213 20:58:14 @agent_ppo2.py:185][0m |          -0.0045 |          57.1110 |          16.5729 |
[32m[20221213 20:58:14 @agent_ppo2.py:185][0m |          -0.0024 |          58.0388 |          16.5703 |
[32m[20221213 20:58:14 @agent_ppo2.py:185][0m |          -0.0077 |          56.8594 |          16.5600 |
[32m[20221213 20:58:14 @agent_ppo2.py:185][0m |          -0.0071 |          56.8599 |          16.5567 |
[32m[20221213 20:58:14 @agent_ppo2.py:185][0m |          -0.0058 |          56.7649 |          16.5589 |
[32m[20221213 20:58:14 @agent_ppo2.py:185][0m |          -0.0058 |          57.1970 |          16.5553 |
[32m[20221213 20:58:14 @agent_ppo2.py:185][0m |          -0.0088 |          56.6394 |          16.5566 |
[32m[20221213 20:58:14 @agent_ppo2.py:185][0m |           0.0091 |          66.2417 |          16.5610 |
[32m[20221213 20:58:14 @agent_ppo2.py:185][0m |          -0.0110 |          56.6892 |          16.5556 |
[32m[20221213 20:58:14 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:58:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.80
[32m[20221213 20:58:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.00
[32m[20221213 20:58:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.00
[32m[20221213 20:58:14 @agent_ppo2.py:143][0m Total time:       2.66 min
[32m[20221213 20:58:14 @agent_ppo2.py:145][0m 258048 total steps have happened
[32m[20221213 20:58:14 @agent_ppo2.py:121][0m #------------------------ Iteration 126 --------------------------#
[32m[20221213 20:58:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:15 @agent_ppo2.py:185][0m |          -0.0025 |          58.2638 |          16.6403 |
[32m[20221213 20:58:15 @agent_ppo2.py:185][0m |          -0.0052 |          57.5033 |          16.6306 |
[32m[20221213 20:58:15 @agent_ppo2.py:185][0m |          -0.0035 |          56.8834 |          16.6201 |
[32m[20221213 20:58:15 @agent_ppo2.py:185][0m |          -0.0002 |          56.4039 |          16.6168 |
[32m[20221213 20:58:15 @agent_ppo2.py:185][0m |          -0.0037 |          55.7371 |          16.6179 |
[32m[20221213 20:58:15 @agent_ppo2.py:185][0m |          -0.0029 |          55.3647 |          16.6153 |
[32m[20221213 20:58:15 @agent_ppo2.py:185][0m |          -0.0006 |          55.6460 |          16.6140 |
[32m[20221213 20:58:15 @agent_ppo2.py:185][0m |          -0.0037 |          54.7107 |          16.6167 |
[32m[20221213 20:58:15 @agent_ppo2.py:185][0m |          -0.0022 |          55.3212 |          16.6134 |
[32m[20221213 20:58:16 @agent_ppo2.py:185][0m |          -0.0042 |          54.4196 |          16.6104 |
[32m[20221213 20:58:16 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:58:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.00
[32m[20221213 20:58:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.00
[32m[20221213 20:58:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.00
[32m[20221213 20:58:16 @agent_ppo2.py:143][0m Total time:       2.68 min
[32m[20221213 20:58:16 @agent_ppo2.py:145][0m 260096 total steps have happened
[32m[20221213 20:58:16 @agent_ppo2.py:121][0m #------------------------ Iteration 127 --------------------------#
[32m[20221213 20:58:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:58:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:16 @agent_ppo2.py:185][0m |          -0.0031 |          60.0247 |          16.7865 |
[32m[20221213 20:58:16 @agent_ppo2.py:185][0m |          -0.0048 |          59.1233 |          16.7843 |
[32m[20221213 20:58:16 @agent_ppo2.py:185][0m |          -0.0065 |          58.7647 |          16.7723 |
[32m[20221213 20:58:16 @agent_ppo2.py:185][0m |          -0.0042 |          58.4887 |          16.7682 |
[32m[20221213 20:58:16 @agent_ppo2.py:185][0m |          -0.0032 |          58.2400 |          16.7775 |
[32m[20221213 20:58:16 @agent_ppo2.py:185][0m |          -0.0072 |          58.1242 |          16.7676 |
[32m[20221213 20:58:17 @agent_ppo2.py:185][0m |          -0.0082 |          58.1187 |          16.7692 |
[32m[20221213 20:58:17 @agent_ppo2.py:185][0m |          -0.0058 |          57.9370 |          16.7763 |
[32m[20221213 20:58:17 @agent_ppo2.py:185][0m |          -0.0042 |          57.7910 |          16.7698 |
[32m[20221213 20:58:17 @agent_ppo2.py:185][0m |          -0.0040 |          57.6706 |          16.7780 |
[32m[20221213 20:58:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:58:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.20
[32m[20221213 20:58:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.00
[32m[20221213 20:58:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.00
[32m[20221213 20:58:17 @agent_ppo2.py:143][0m Total time:       2.70 min
[32m[20221213 20:58:17 @agent_ppo2.py:145][0m 262144 total steps have happened
[32m[20221213 20:58:17 @agent_ppo2.py:121][0m #------------------------ Iteration 128 --------------------------#
[32m[20221213 20:58:17 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:58:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:17 @agent_ppo2.py:185][0m |           0.0004 |          60.6407 |          16.6153 |
[32m[20221213 20:58:17 @agent_ppo2.py:185][0m |           0.0007 |          60.8118 |          16.6036 |
[32m[20221213 20:58:17 @agent_ppo2.py:185][0m |          -0.0035 |          60.2235 |          16.6047 |
[32m[20221213 20:58:17 @agent_ppo2.py:185][0m |          -0.0012 |          60.1770 |          16.6045 |
[32m[20221213 20:58:18 @agent_ppo2.py:185][0m |          -0.0032 |          60.0074 |          16.6010 |
[32m[20221213 20:58:18 @agent_ppo2.py:185][0m |          -0.0075 |          59.8266 |          16.6012 |
[32m[20221213 20:58:18 @agent_ppo2.py:185][0m |          -0.0052 |          59.8171 |          16.6047 |
[32m[20221213 20:58:18 @agent_ppo2.py:185][0m |           0.0075 |          63.1587 |          16.6065 |
[32m[20221213 20:58:18 @agent_ppo2.py:185][0m |          -0.0018 |          59.6676 |          16.6057 |
[32m[20221213 20:58:18 @agent_ppo2.py:185][0m |          -0.0062 |          59.5974 |          16.6057 |
[32m[20221213 20:58:18 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:58:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.40
[32m[20221213 20:58:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.00
[32m[20221213 20:58:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.00
[32m[20221213 20:58:18 @agent_ppo2.py:143][0m Total time:       2.73 min
[32m[20221213 20:58:18 @agent_ppo2.py:145][0m 264192 total steps have happened
[32m[20221213 20:58:18 @agent_ppo2.py:121][0m #------------------------ Iteration 129 --------------------------#
[32m[20221213 20:58:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:19 @agent_ppo2.py:185][0m |           0.0017 |          57.8927 |          16.7195 |
[32m[20221213 20:58:19 @agent_ppo2.py:185][0m |          -0.0068 |          56.6955 |          16.7100 |
[32m[20221213 20:58:19 @agent_ppo2.py:185][0m |          -0.0059 |          56.6178 |          16.6984 |
[32m[20221213 20:58:19 @agent_ppo2.py:185][0m |          -0.0095 |          56.4792 |          16.7041 |
[32m[20221213 20:58:19 @agent_ppo2.py:185][0m |          -0.0078 |          56.0760 |          16.6965 |
[32m[20221213 20:58:19 @agent_ppo2.py:185][0m |          -0.0053 |          56.1428 |          16.6917 |
[32m[20221213 20:58:19 @agent_ppo2.py:185][0m |           0.0029 |          58.6283 |          16.7000 |
[32m[20221213 20:58:19 @agent_ppo2.py:185][0m |          -0.0053 |          56.4947 |          16.6963 |
[32m[20221213 20:58:19 @agent_ppo2.py:185][0m |          -0.0098 |          55.8721 |          16.7027 |
[32m[20221213 20:58:19 @agent_ppo2.py:185][0m |          -0.0070 |          55.7742 |          16.7027 |
[32m[20221213 20:58:19 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:58:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.60
[32m[20221213 20:58:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.00
[32m[20221213 20:58:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.00
[32m[20221213 20:58:19 @agent_ppo2.py:143][0m Total time:       2.75 min
[32m[20221213 20:58:19 @agent_ppo2.py:145][0m 266240 total steps have happened
[32m[20221213 20:58:19 @agent_ppo2.py:121][0m #------------------------ Iteration 130 --------------------------#
[32m[20221213 20:58:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:58:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:20 @agent_ppo2.py:185][0m |           0.0005 |          57.2754 |          16.8489 |
[32m[20221213 20:58:20 @agent_ppo2.py:185][0m |          -0.0021 |          56.6702 |          16.8421 |
[32m[20221213 20:58:20 @agent_ppo2.py:185][0m |          -0.0043 |          56.3717 |          16.8357 |
[32m[20221213 20:58:20 @agent_ppo2.py:185][0m |          -0.0007 |          56.3425 |          16.8349 |
[32m[20221213 20:58:20 @agent_ppo2.py:185][0m |           0.0076 |          63.8833 |          16.8291 |
[32m[20221213 20:58:20 @agent_ppo2.py:185][0m |           0.0041 |          58.6791 |          16.8248 |
[32m[20221213 20:58:20 @agent_ppo2.py:185][0m |          -0.0053 |          55.8419 |          16.8231 |
[32m[20221213 20:58:20 @agent_ppo2.py:185][0m |          -0.0015 |          56.7843 |          16.8305 |
[32m[20221213 20:58:20 @agent_ppo2.py:185][0m |           0.0146 |          59.3165 |          16.8256 |
[32m[20221213 20:58:21 @agent_ppo2.py:185][0m |          -0.0054 |          55.7590 |          16.8343 |
[32m[20221213 20:58:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:58:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.20
[32m[20221213 20:58:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.00
[32m[20221213 20:58:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.00
[32m[20221213 20:58:21 @agent_ppo2.py:143][0m Total time:       2.77 min
[32m[20221213 20:58:21 @agent_ppo2.py:145][0m 268288 total steps have happened
[32m[20221213 20:58:21 @agent_ppo2.py:121][0m #------------------------ Iteration 131 --------------------------#
[32m[20221213 20:58:21 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:58:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:21 @agent_ppo2.py:185][0m |           0.0074 |          65.5126 |          16.8620 |
[32m[20221213 20:58:21 @agent_ppo2.py:185][0m |          -0.0036 |          61.3149 |          16.8511 |
[32m[20221213 20:58:21 @agent_ppo2.py:185][0m |          -0.0046 |          61.1393 |          16.8490 |
[32m[20221213 20:58:21 @agent_ppo2.py:185][0m |          -0.0095 |          61.0368 |          16.8482 |
[32m[20221213 20:58:21 @agent_ppo2.py:185][0m |          -0.0073 |          60.9989 |          16.8441 |
[32m[20221213 20:58:21 @agent_ppo2.py:185][0m |          -0.0032 |          61.1481 |          16.8437 |
[32m[20221213 20:58:21 @agent_ppo2.py:185][0m |          -0.0046 |          60.6427 |          16.8450 |
[32m[20221213 20:58:22 @agent_ppo2.py:185][0m |          -0.0076 |          60.7559 |          16.8483 |
[32m[20221213 20:58:22 @agent_ppo2.py:185][0m |          -0.0077 |          60.5093 |          16.8480 |
[32m[20221213 20:58:22 @agent_ppo2.py:185][0m |          -0.0043 |          61.8748 |          16.8440 |
[32m[20221213 20:58:22 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:58:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.20
[32m[20221213 20:58:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.00
[32m[20221213 20:58:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.00
[32m[20221213 20:58:22 @agent_ppo2.py:143][0m Total time:       2.79 min
[32m[20221213 20:58:22 @agent_ppo2.py:145][0m 270336 total steps have happened
[32m[20221213 20:58:22 @agent_ppo2.py:121][0m #------------------------ Iteration 132 --------------------------#
[32m[20221213 20:58:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:22 @agent_ppo2.py:185][0m |          -0.0017 |          58.0931 |          16.7816 |
[32m[20221213 20:58:22 @agent_ppo2.py:185][0m |          -0.0030 |          57.6223 |          16.7747 |
[32m[20221213 20:58:22 @agent_ppo2.py:185][0m |          -0.0053 |          57.4054 |          16.7709 |
[32m[20221213 20:58:22 @agent_ppo2.py:185][0m |          -0.0044 |          57.2046 |          16.7587 |
[32m[20221213 20:58:23 @agent_ppo2.py:185][0m |          -0.0037 |          57.1169 |          16.7583 |
[32m[20221213 20:58:23 @agent_ppo2.py:185][0m |          -0.0037 |          57.1092 |          16.7559 |
[32m[20221213 20:58:23 @agent_ppo2.py:185][0m |          -0.0061 |          57.1178 |          16.7488 |
[32m[20221213 20:58:23 @agent_ppo2.py:185][0m |          -0.0009 |          57.8511 |          16.7495 |
[32m[20221213 20:58:23 @agent_ppo2.py:185][0m |          -0.0049 |          57.0190 |          16.7566 |
[32m[20221213 20:58:23 @agent_ppo2.py:185][0m |          -0.0078 |          56.9991 |          16.7531 |
[32m[20221213 20:58:23 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:58:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.00
[32m[20221213 20:58:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.00
[32m[20221213 20:58:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 409.00
[32m[20221213 20:58:23 @agent_ppo2.py:143][0m Total time:       2.81 min
[32m[20221213 20:58:23 @agent_ppo2.py:145][0m 272384 total steps have happened
[32m[20221213 20:58:23 @agent_ppo2.py:121][0m #------------------------ Iteration 133 --------------------------#
[32m[20221213 20:58:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:23 @agent_ppo2.py:185][0m |           0.0003 |          61.2959 |          16.8660 |
[32m[20221213 20:58:23 @agent_ppo2.py:185][0m |          -0.0005 |          60.7725 |          16.8529 |
[32m[20221213 20:58:24 @agent_ppo2.py:185][0m |          -0.0045 |          60.5439 |          16.8461 |
[32m[20221213 20:58:24 @agent_ppo2.py:185][0m |          -0.0049 |          60.4562 |          16.8434 |
[32m[20221213 20:58:24 @agent_ppo2.py:185][0m |          -0.0027 |          60.3029 |          16.8398 |
[32m[20221213 20:58:24 @agent_ppo2.py:185][0m |          -0.0048 |          60.3032 |          16.8369 |
[32m[20221213 20:58:24 @agent_ppo2.py:185][0m |           0.0039 |          64.0513 |          16.8345 |
[32m[20221213 20:58:24 @agent_ppo2.py:185][0m |          -0.0058 |          60.2392 |          16.8353 |
[32m[20221213 20:58:24 @agent_ppo2.py:185][0m |          -0.0044 |          60.1777 |          16.8383 |
[32m[20221213 20:58:24 @agent_ppo2.py:185][0m |          -0.0032 |          59.9850 |          16.8313 |
[32m[20221213 20:58:24 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:58:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.40
[32m[20221213 20:58:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.00
[32m[20221213 20:58:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.00
[32m[20221213 20:58:24 @agent_ppo2.py:143][0m Total time:       2.83 min
[32m[20221213 20:58:24 @agent_ppo2.py:145][0m 274432 total steps have happened
[32m[20221213 20:58:24 @agent_ppo2.py:121][0m #------------------------ Iteration 134 --------------------------#
[32m[20221213 20:58:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:25 @agent_ppo2.py:185][0m |          -0.0030 |          60.5040 |          16.8128 |
[32m[20221213 20:58:25 @agent_ppo2.py:185][0m |          -0.0057 |          60.1242 |          16.7926 |
[32m[20221213 20:58:25 @agent_ppo2.py:185][0m |          -0.0044 |          59.8485 |          16.7748 |
[32m[20221213 20:58:25 @agent_ppo2.py:185][0m |          -0.0038 |          59.7386 |          16.7621 |
[32m[20221213 20:58:25 @agent_ppo2.py:185][0m |          -0.0034 |          59.4073 |          16.7606 |
[32m[20221213 20:58:25 @agent_ppo2.py:185][0m |          -0.0086 |          59.4044 |          16.7669 |
[32m[20221213 20:58:25 @agent_ppo2.py:185][0m |          -0.0056 |          59.3105 |          16.7608 |
[32m[20221213 20:58:25 @agent_ppo2.py:185][0m |          -0.0075 |          59.1485 |          16.7665 |
[32m[20221213 20:58:25 @agent_ppo2.py:185][0m |          -0.0065 |          59.0606 |          16.7722 |
[32m[20221213 20:58:25 @agent_ppo2.py:185][0m |          -0.0000 |          61.0559 |          16.7718 |
[32m[20221213 20:58:25 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:58:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.20
[32m[20221213 20:58:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.00
[32m[20221213 20:58:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.00
[32m[20221213 20:58:26 @agent_ppo2.py:143][0m Total time:       2.85 min
[32m[20221213 20:58:26 @agent_ppo2.py:145][0m 276480 total steps have happened
[32m[20221213 20:58:26 @agent_ppo2.py:121][0m #------------------------ Iteration 135 --------------------------#
[32m[20221213 20:58:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:26 @agent_ppo2.py:185][0m |          -0.0016 |          61.2759 |          17.0225 |
[32m[20221213 20:58:26 @agent_ppo2.py:185][0m |           0.0000 |          60.9180 |          17.0022 |
[32m[20221213 20:58:26 @agent_ppo2.py:185][0m |          -0.0065 |          60.5653 |          16.9907 |
[32m[20221213 20:58:26 @agent_ppo2.py:185][0m |          -0.0004 |          61.9458 |          16.9814 |
[32m[20221213 20:58:26 @agent_ppo2.py:185][0m |          -0.0007 |          61.6171 |          16.9750 |
[32m[20221213 20:58:26 @agent_ppo2.py:185][0m |           0.0021 |          66.3616 |          16.9736 |
[32m[20221213 20:58:26 @agent_ppo2.py:185][0m |          -0.0064 |          60.3194 |          16.9495 |
[32m[20221213 20:58:27 @agent_ppo2.py:185][0m |          -0.0011 |          62.0513 |          16.9741 |
[32m[20221213 20:58:27 @agent_ppo2.py:185][0m |          -0.0011 |          60.8172 |          16.9713 |
[32m[20221213 20:58:27 @agent_ppo2.py:185][0m |          -0.0034 |          60.4546 |          16.9603 |
[32m[20221213 20:58:27 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 20:58:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.20
[32m[20221213 20:58:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.00
[32m[20221213 20:58:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.00
[32m[20221213 20:58:27 @agent_ppo2.py:143][0m Total time:       2.87 min
[32m[20221213 20:58:27 @agent_ppo2.py:145][0m 278528 total steps have happened
[32m[20221213 20:58:27 @agent_ppo2.py:121][0m #------------------------ Iteration 136 --------------------------#
[32m[20221213 20:58:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:27 @agent_ppo2.py:185][0m |          -0.0021 |          58.6854 |          16.9707 |
[32m[20221213 20:58:27 @agent_ppo2.py:185][0m |          -0.0009 |          58.0268 |          16.9617 |
[32m[20221213 20:58:27 @agent_ppo2.py:185][0m |          -0.0032 |          56.8840 |          16.9547 |
[32m[20221213 20:58:28 @agent_ppo2.py:185][0m |          -0.0061 |          56.4953 |          16.9521 |
[32m[20221213 20:58:28 @agent_ppo2.py:185][0m |          -0.0038 |          56.0906 |          16.9459 |
[32m[20221213 20:58:28 @agent_ppo2.py:185][0m |          -0.0049 |          55.8005 |          16.9421 |
[32m[20221213 20:58:28 @agent_ppo2.py:185][0m |          -0.0069 |          55.7179 |          16.9380 |
[32m[20221213 20:58:28 @agent_ppo2.py:185][0m |           0.0010 |          58.6952 |          16.9398 |
[32m[20221213 20:58:28 @agent_ppo2.py:185][0m |          -0.0076 |          55.4511 |          16.9409 |
[32m[20221213 20:58:28 @agent_ppo2.py:185][0m |          -0.0056 |          55.2786 |          16.9380 |
[32m[20221213 20:58:28 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:58:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.20
[32m[20221213 20:58:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.00
[32m[20221213 20:58:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.00
[32m[20221213 20:58:28 @agent_ppo2.py:143][0m Total time:       2.89 min
[32m[20221213 20:58:28 @agent_ppo2.py:145][0m 280576 total steps have happened
[32m[20221213 20:58:28 @agent_ppo2.py:121][0m #------------------------ Iteration 137 --------------------------#
[32m[20221213 20:58:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:29 @agent_ppo2.py:185][0m |           0.0063 |          62.0483 |          17.0445 |
[32m[20221213 20:58:29 @agent_ppo2.py:185][0m |          -0.0030 |          60.6725 |          17.0312 |
[32m[20221213 20:58:29 @agent_ppo2.py:185][0m |          -0.0058 |          60.2929 |          17.0215 |
[32m[20221213 20:58:29 @agent_ppo2.py:185][0m |          -0.0059 |          60.1057 |          17.0131 |
[32m[20221213 20:58:29 @agent_ppo2.py:185][0m |          -0.0061 |          59.9314 |          17.0145 |
[32m[20221213 20:58:29 @agent_ppo2.py:185][0m |          -0.0078 |          59.8212 |          17.0117 |
[32m[20221213 20:58:29 @agent_ppo2.py:185][0m |          -0.0090 |          59.7093 |          17.0128 |
[32m[20221213 20:58:29 @agent_ppo2.py:185][0m |          -0.0070 |          59.5984 |          17.0072 |
[32m[20221213 20:58:29 @agent_ppo2.py:185][0m |          -0.0046 |          59.4889 |          17.0065 |
[32m[20221213 20:58:29 @agent_ppo2.py:185][0m |          -0.0066 |          59.4105 |          17.0057 |
[32m[20221213 20:58:29 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 20:58:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.80
[32m[20221213 20:58:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.00
[32m[20221213 20:58:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.00
[32m[20221213 20:58:30 @agent_ppo2.py:143][0m Total time:       2.91 min
[32m[20221213 20:58:30 @agent_ppo2.py:145][0m 282624 total steps have happened
[32m[20221213 20:58:30 @agent_ppo2.py:121][0m #------------------------ Iteration 138 --------------------------#
[32m[20221213 20:58:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:30 @agent_ppo2.py:185][0m |          -0.0025 |          60.7155 |          17.0399 |
[32m[20221213 20:58:30 @agent_ppo2.py:185][0m |          -0.0047 |          60.3625 |          17.0317 |
[32m[20221213 20:58:30 @agent_ppo2.py:185][0m |           0.0047 |          61.8322 |          17.0218 |
[32m[20221213 20:58:30 @agent_ppo2.py:185][0m |          -0.0060 |          60.1132 |          17.0147 |
[32m[20221213 20:58:30 @agent_ppo2.py:185][0m |          -0.0029 |          60.0195 |          17.0184 |
[32m[20221213 20:58:30 @agent_ppo2.py:185][0m |          -0.0039 |          59.9682 |          17.0163 |
[32m[20221213 20:58:30 @agent_ppo2.py:185][0m |          -0.0028 |          59.8722 |          17.0204 |
[32m[20221213 20:58:31 @agent_ppo2.py:185][0m |           0.0052 |          61.9556 |          17.0135 |
[32m[20221213 20:58:31 @agent_ppo2.py:185][0m |           0.0011 |          61.5234 |          17.0149 |
[32m[20221213 20:58:31 @agent_ppo2.py:185][0m |          -0.0046 |          59.7193 |          17.0140 |
[32m[20221213 20:58:31 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:58:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.00
[32m[20221213 20:58:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.00
[32m[20221213 20:58:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.00
[32m[20221213 20:58:31 @agent_ppo2.py:143][0m Total time:       2.94 min
[32m[20221213 20:58:31 @agent_ppo2.py:145][0m 284672 total steps have happened
[32m[20221213 20:58:31 @agent_ppo2.py:121][0m #------------------------ Iteration 139 --------------------------#
[32m[20221213 20:58:31 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:58:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:31 @agent_ppo2.py:185][0m |          -0.0027 |          63.3291 |          17.0265 |
[32m[20221213 20:58:31 @agent_ppo2.py:185][0m |          -0.0044 |          62.6327 |          17.0094 |
[32m[20221213 20:58:31 @agent_ppo2.py:185][0m |          -0.0017 |          62.4719 |          16.9978 |
[32m[20221213 20:58:31 @agent_ppo2.py:185][0m |          -0.0060 |          62.3637 |          16.9986 |
[32m[20221213 20:58:32 @agent_ppo2.py:185][0m |           0.0012 |          64.5785 |          16.9858 |
[32m[20221213 20:58:32 @agent_ppo2.py:185][0m |          -0.0064 |          61.9714 |          16.9782 |
[32m[20221213 20:58:32 @agent_ppo2.py:185][0m |          -0.0001 |          62.5586 |          16.9806 |
[32m[20221213 20:58:32 @agent_ppo2.py:185][0m |           0.0019 |          66.0017 |          16.9868 |
[32m[20221213 20:58:32 @agent_ppo2.py:185][0m |          -0.0057 |          61.6482 |          16.9812 |
[32m[20221213 20:58:32 @agent_ppo2.py:185][0m |          -0.0052 |          61.4636 |          16.9800 |
[32m[20221213 20:58:32 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:58:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.60
[32m[20221213 20:58:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.00
[32m[20221213 20:58:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.00
[32m[20221213 20:58:32 @agent_ppo2.py:143][0m Total time:       2.96 min
[32m[20221213 20:58:32 @agent_ppo2.py:145][0m 286720 total steps have happened
[32m[20221213 20:58:32 @agent_ppo2.py:121][0m #------------------------ Iteration 140 --------------------------#
[32m[20221213 20:58:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 20:58:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:32 @agent_ppo2.py:185][0m |          -0.0027 |          60.4974 |          17.0581 |
[32m[20221213 20:58:33 @agent_ppo2.py:185][0m |          -0.0053 |          59.7976 |          17.0417 |
[32m[20221213 20:58:33 @agent_ppo2.py:185][0m |          -0.0059 |          59.7357 |          17.0329 |
[32m[20221213 20:58:33 @agent_ppo2.py:185][0m |          -0.0037 |          59.5503 |          17.0302 |
[32m[20221213 20:58:33 @agent_ppo2.py:185][0m |          -0.0069 |          59.6482 |          17.0282 |
[32m[20221213 20:58:33 @agent_ppo2.py:185][0m |          -0.0055 |          59.2654 |          17.0292 |
[32m[20221213 20:58:33 @agent_ppo2.py:185][0m |          -0.0102 |          59.1208 |          17.0280 |
[32m[20221213 20:58:33 @agent_ppo2.py:185][0m |          -0.0042 |          58.9967 |          17.0296 |
[32m[20221213 20:58:33 @agent_ppo2.py:185][0m |          -0.0060 |          59.3408 |          17.0319 |
[32m[20221213 20:58:33 @agent_ppo2.py:185][0m |          -0.0072 |          58.9580 |          17.0324 |
[32m[20221213 20:58:33 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:58:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.40
[32m[20221213 20:58:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.00
[32m[20221213 20:58:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.00
[32m[20221213 20:58:33 @agent_ppo2.py:143][0m Total time:       2.98 min
[32m[20221213 20:58:33 @agent_ppo2.py:145][0m 288768 total steps have happened
[32m[20221213 20:58:33 @agent_ppo2.py:121][0m #------------------------ Iteration 141 --------------------------#
[32m[20221213 20:58:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:34 @agent_ppo2.py:185][0m |           0.0005 |          59.2690 |          16.9806 |
[32m[20221213 20:58:34 @agent_ppo2.py:185][0m |          -0.0050 |          58.6878 |          16.9461 |
[32m[20221213 20:58:34 @agent_ppo2.py:185][0m |          -0.0024 |          58.7362 |          16.9504 |
[32m[20221213 20:58:34 @agent_ppo2.py:185][0m |          -0.0046 |          58.3350 |          16.9359 |
[32m[20221213 20:58:34 @agent_ppo2.py:185][0m |           0.0037 |          63.2227 |          16.9360 |
[32m[20221213 20:58:34 @agent_ppo2.py:185][0m |          -0.0029 |          58.2076 |          16.9165 |
[32m[20221213 20:58:34 @agent_ppo2.py:185][0m |          -0.0096 |          58.1493 |          16.9298 |
[32m[20221213 20:58:34 @agent_ppo2.py:185][0m |          -0.0071 |          58.0121 |          16.9364 |
[32m[20221213 20:58:34 @agent_ppo2.py:185][0m |          -0.0062 |          58.0736 |          16.9362 |
[32m[20221213 20:58:34 @agent_ppo2.py:185][0m |          -0.0065 |          58.0171 |          16.9341 |
[32m[20221213 20:58:34 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:58:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.40
[32m[20221213 20:58:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.00
[32m[20221213 20:58:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.00
[32m[20221213 20:58:35 @agent_ppo2.py:143][0m Total time:       3.00 min
[32m[20221213 20:58:35 @agent_ppo2.py:145][0m 290816 total steps have happened
[32m[20221213 20:58:35 @agent_ppo2.py:121][0m #------------------------ Iteration 142 --------------------------#
[32m[20221213 20:58:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:35 @agent_ppo2.py:185][0m |          -0.0014 |          59.5919 |          17.0520 |
[32m[20221213 20:58:35 @agent_ppo2.py:185][0m |           0.0010 |          59.3498 |          17.0555 |
[32m[20221213 20:58:35 @agent_ppo2.py:185][0m |          -0.0044 |          59.1341 |          17.0532 |
[32m[20221213 20:58:35 @agent_ppo2.py:185][0m |          -0.0019 |          59.1887 |          17.0461 |
[32m[20221213 20:58:35 @agent_ppo2.py:185][0m |          -0.0002 |          59.5121 |          17.0481 |
[32m[20221213 20:58:35 @agent_ppo2.py:185][0m |          -0.0052 |          58.7933 |          17.0425 |
[32m[20221213 20:58:35 @agent_ppo2.py:185][0m |           0.0070 |          64.6045 |          17.0463 |
[32m[20221213 20:58:36 @agent_ppo2.py:185][0m |          -0.0036 |          58.7091 |          17.0387 |
[32m[20221213 20:58:36 @agent_ppo2.py:185][0m |          -0.0054 |          58.6379 |          17.0442 |
[32m[20221213 20:58:36 @agent_ppo2.py:185][0m |           0.0051 |          64.7857 |          17.0419 |
[32m[20221213 20:58:36 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:58:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.40
[32m[20221213 20:58:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.00
[32m[20221213 20:58:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.00
[32m[20221213 20:58:36 @agent_ppo2.py:143][0m Total time:       3.02 min
[32m[20221213 20:58:36 @agent_ppo2.py:145][0m 292864 total steps have happened
[32m[20221213 20:58:36 @agent_ppo2.py:121][0m #------------------------ Iteration 143 --------------------------#
[32m[20221213 20:58:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 20:58:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:36 @agent_ppo2.py:185][0m |           0.0005 |          60.8553 |          17.0980 |
[32m[20221213 20:58:36 @agent_ppo2.py:185][0m |          -0.0047 |          60.2231 |          17.0869 |
[32m[20221213 20:58:36 @agent_ppo2.py:185][0m |          -0.0044 |          59.9995 |          17.0848 |
[32m[20221213 20:58:37 @agent_ppo2.py:185][0m |          -0.0021 |          60.6998 |          17.0819 |
[32m[20221213 20:58:37 @agent_ppo2.py:185][0m |          -0.0016 |          60.9013 |          17.0878 |
[32m[20221213 20:58:37 @agent_ppo2.py:185][0m |          -0.0042 |          59.9696 |          17.0815 |
[32m[20221213 20:58:37 @agent_ppo2.py:185][0m |          -0.0052 |          59.8539 |          17.0846 |
[32m[20221213 20:58:37 @agent_ppo2.py:185][0m |          -0.0051 |          59.8098 |          17.0872 |
[32m[20221213 20:58:37 @agent_ppo2.py:185][0m |          -0.0080 |          59.8730 |          17.0827 |
[32m[20221213 20:58:37 @agent_ppo2.py:185][0m |           0.0064 |          66.6467 |          17.0784 |
[32m[20221213 20:58:37 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:58:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.40
[32m[20221213 20:58:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.00
[32m[20221213 20:58:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.00
[32m[20221213 20:58:37 @agent_ppo2.py:143][0m Total time:       3.04 min
[32m[20221213 20:58:37 @agent_ppo2.py:145][0m 294912 total steps have happened
[32m[20221213 20:58:37 @agent_ppo2.py:121][0m #------------------------ Iteration 144 --------------------------#
[32m[20221213 20:58:37 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:58:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:37 @agent_ppo2.py:185][0m |           0.0109 |          68.0300 |          16.9026 |
[32m[20221213 20:58:38 @agent_ppo2.py:185][0m |          -0.0024 |          61.9637 |          16.9010 |
[32m[20221213 20:58:38 @agent_ppo2.py:185][0m |          -0.0051 |          61.7089 |          16.8942 |
[32m[20221213 20:58:38 @agent_ppo2.py:185][0m |          -0.0044 |          61.5229 |          16.8923 |
[32m[20221213 20:58:38 @agent_ppo2.py:185][0m |          -0.0003 |          64.1558 |          16.8873 |
[32m[20221213 20:58:38 @agent_ppo2.py:185][0m |          -0.0078 |          61.3291 |          16.8798 |
[32m[20221213 20:58:38 @agent_ppo2.py:185][0m |           0.0004 |          64.8911 |          16.8878 |
[32m[20221213 20:58:38 @agent_ppo2.py:185][0m |          -0.0059 |          61.1876 |          16.8788 |
[32m[20221213 20:58:38 @agent_ppo2.py:185][0m |          -0.0102 |          61.1465 |          16.8863 |
[32m[20221213 20:58:38 @agent_ppo2.py:185][0m |          -0.0070 |          61.0756 |          16.8845 |
[32m[20221213 20:58:38 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:58:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.60
[32m[20221213 20:58:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.00
[32m[20221213 20:58:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.00
[32m[20221213 20:58:38 @agent_ppo2.py:143][0m Total time:       3.06 min
[32m[20221213 20:58:38 @agent_ppo2.py:145][0m 296960 total steps have happened
[32m[20221213 20:58:38 @agent_ppo2.py:121][0m #------------------------ Iteration 145 --------------------------#
[32m[20221213 20:58:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:58:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:39 @agent_ppo2.py:185][0m |          -0.0031 |          62.5681 |          16.9683 |
[32m[20221213 20:58:39 @agent_ppo2.py:185][0m |          -0.0020 |          62.1765 |          16.9497 |
[32m[20221213 20:58:39 @agent_ppo2.py:185][0m |          -0.0020 |          62.9715 |          16.9423 |
[32m[20221213 20:58:39 @agent_ppo2.py:185][0m |          -0.0083 |          62.0798 |          16.9426 |
[32m[20221213 20:58:39 @agent_ppo2.py:185][0m |          -0.0068 |          61.9126 |          16.9451 |
[32m[20221213 20:58:39 @agent_ppo2.py:185][0m |          -0.0047 |          61.7851 |          16.9487 |
[32m[20221213 20:58:39 @agent_ppo2.py:185][0m |          -0.0060 |          61.6961 |          16.9447 |
[32m[20221213 20:58:39 @agent_ppo2.py:185][0m |          -0.0074 |          61.7395 |          16.9413 |
[32m[20221213 20:58:39 @agent_ppo2.py:185][0m |          -0.0077 |          61.5545 |          16.9468 |
[32m[20221213 20:58:40 @agent_ppo2.py:185][0m |          -0.0054 |          61.6387 |          16.9454 |
[32m[20221213 20:58:40 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:58:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.20
[32m[20221213 20:58:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.00
[32m[20221213 20:58:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.00
[32m[20221213 20:58:40 @agent_ppo2.py:143][0m Total time:       3.08 min
[32m[20221213 20:58:40 @agent_ppo2.py:145][0m 299008 total steps have happened
[32m[20221213 20:58:40 @agent_ppo2.py:121][0m #------------------------ Iteration 146 --------------------------#
[32m[20221213 20:58:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:40 @agent_ppo2.py:185][0m |          -0.0025 |          63.5443 |          17.2469 |
[32m[20221213 20:58:40 @agent_ppo2.py:185][0m |          -0.0060 |          63.1152 |          17.2262 |
[32m[20221213 20:58:40 @agent_ppo2.py:185][0m |          -0.0058 |          62.9417 |          17.2190 |
[32m[20221213 20:58:40 @agent_ppo2.py:185][0m |          -0.0055 |          62.8923 |          17.2193 |
[32m[20221213 20:58:40 @agent_ppo2.py:185][0m |          -0.0076 |          62.8691 |          17.2070 |
[32m[20221213 20:58:40 @agent_ppo2.py:185][0m |          -0.0023 |          62.6965 |          17.2096 |
[32m[20221213 20:58:40 @agent_ppo2.py:185][0m |          -0.0094 |          62.6143 |          17.2079 |
[32m[20221213 20:58:41 @agent_ppo2.py:185][0m |          -0.0086 |          62.6220 |          17.2069 |
[32m[20221213 20:58:41 @agent_ppo2.py:185][0m |          -0.0082 |          62.4521 |          17.2109 |
[32m[20221213 20:58:41 @agent_ppo2.py:185][0m |          -0.0073 |          62.3662 |          17.2041 |
[32m[20221213 20:58:41 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:58:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.60
[32m[20221213 20:58:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.00
[32m[20221213 20:58:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.00
[32m[20221213 20:58:41 @agent_ppo2.py:143][0m Total time:       3.10 min
[32m[20221213 20:58:41 @agent_ppo2.py:145][0m 301056 total steps have happened
[32m[20221213 20:58:41 @agent_ppo2.py:121][0m #------------------------ Iteration 147 --------------------------#
[32m[20221213 20:58:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:41 @agent_ppo2.py:185][0m |          -0.0021 |          59.2416 |          16.9698 |
[32m[20221213 20:58:41 @agent_ppo2.py:185][0m |          -0.0022 |          58.7576 |          16.9473 |
[32m[20221213 20:58:41 @agent_ppo2.py:185][0m |          -0.0048 |          58.5134 |          16.9438 |
[32m[20221213 20:58:41 @agent_ppo2.py:185][0m |          -0.0053 |          58.3513 |          16.9426 |
[32m[20221213 20:58:42 @agent_ppo2.py:185][0m |          -0.0046 |          58.2234 |          16.9387 |
[32m[20221213 20:58:42 @agent_ppo2.py:185][0m |          -0.0062 |          58.0879 |          16.9376 |
[32m[20221213 20:58:42 @agent_ppo2.py:185][0m |          -0.0027 |          57.9981 |          16.9328 |
[32m[20221213 20:58:42 @agent_ppo2.py:185][0m |          -0.0062 |          57.9847 |          16.9364 |
[32m[20221213 20:58:42 @agent_ppo2.py:185][0m |           0.0006 |          60.4884 |          16.9406 |
[32m[20221213 20:58:42 @agent_ppo2.py:185][0m |          -0.0051 |          57.7662 |          16.9342 |
[32m[20221213 20:58:42 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:58:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.00
[32m[20221213 20:58:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.00
[32m[20221213 20:58:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.00
[32m[20221213 20:58:42 @agent_ppo2.py:143][0m Total time:       3.12 min
[32m[20221213 20:58:42 @agent_ppo2.py:145][0m 303104 total steps have happened
[32m[20221213 20:58:42 @agent_ppo2.py:121][0m #------------------------ Iteration 148 --------------------------#
[32m[20221213 20:58:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:42 @agent_ppo2.py:185][0m |           0.0005 |          61.2791 |          17.0469 |
[32m[20221213 20:58:43 @agent_ppo2.py:185][0m |           0.0011 |          61.4345 |          17.0195 |
[32m[20221213 20:58:43 @agent_ppo2.py:185][0m |          -0.0041 |          60.7457 |          17.0098 |
[32m[20221213 20:58:43 @agent_ppo2.py:185][0m |          -0.0080 |          60.5566 |          17.0021 |
[32m[20221213 20:58:43 @agent_ppo2.py:185][0m |          -0.0070 |          60.5832 |          17.0102 |
[32m[20221213 20:58:43 @agent_ppo2.py:185][0m |          -0.0065 |          60.4127 |          16.9968 |
[32m[20221213 20:58:43 @agent_ppo2.py:185][0m |          -0.0066 |          60.2045 |          17.0034 |
[32m[20221213 20:58:43 @agent_ppo2.py:185][0m |          -0.0090 |          60.2583 |          17.0001 |
[32m[20221213 20:58:43 @agent_ppo2.py:185][0m |          -0.0087 |          60.2886 |          17.0075 |
[32m[20221213 20:58:43 @agent_ppo2.py:185][0m |          -0.0118 |          60.2163 |          17.0025 |
[32m[20221213 20:58:43 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:58:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.60
[32m[20221213 20:58:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.00
[32m[20221213 20:58:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.00
[32m[20221213 20:58:43 @agent_ppo2.py:143][0m Total time:       3.14 min
[32m[20221213 20:58:43 @agent_ppo2.py:145][0m 305152 total steps have happened
[32m[20221213 20:58:43 @agent_ppo2.py:121][0m #------------------------ Iteration 149 --------------------------#
[32m[20221213 20:58:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:44 @agent_ppo2.py:185][0m |          -0.0033 |          59.3773 |          17.1163 |
[32m[20221213 20:58:44 @agent_ppo2.py:185][0m |          -0.0063 |          58.8747 |          17.1171 |
[32m[20221213 20:58:44 @agent_ppo2.py:185][0m |          -0.0034 |          58.8329 |          17.1175 |
[32m[20221213 20:58:44 @agent_ppo2.py:185][0m |          -0.0022 |          58.7566 |          17.1169 |
[32m[20221213 20:58:44 @agent_ppo2.py:185][0m |          -0.0075 |          58.6387 |          17.1159 |
[32m[20221213 20:58:44 @agent_ppo2.py:185][0m |          -0.0006 |          59.6966 |          17.1115 |
[32m[20221213 20:58:44 @agent_ppo2.py:185][0m |          -0.0088 |          58.4536 |          17.1098 |
[32m[20221213 20:58:44 @agent_ppo2.py:185][0m |          -0.0081 |          58.3890 |          17.1092 |
[32m[20221213 20:58:44 @agent_ppo2.py:185][0m |          -0.0067 |          58.3535 |          17.1057 |
[32m[20221213 20:58:44 @agent_ppo2.py:185][0m |          -0.0063 |          58.3736 |          17.1072 |
[32m[20221213 20:58:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:58:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.40
[32m[20221213 20:58:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.00
[32m[20221213 20:58:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.00
[32m[20221213 20:58:45 @agent_ppo2.py:143][0m Total time:       3.16 min
[32m[20221213 20:58:45 @agent_ppo2.py:145][0m 307200 total steps have happened
[32m[20221213 20:58:45 @agent_ppo2.py:121][0m #------------------------ Iteration 150 --------------------------#
[32m[20221213 20:58:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:45 @agent_ppo2.py:185][0m |          -0.0010 |          59.0281 |          17.0241 |
[32m[20221213 20:58:45 @agent_ppo2.py:185][0m |          -0.0036 |          58.6342 |          17.0108 |
[32m[20221213 20:58:45 @agent_ppo2.py:185][0m |          -0.0055 |          58.4769 |          17.0043 |
[32m[20221213 20:58:45 @agent_ppo2.py:185][0m |          -0.0048 |          58.4103 |          16.9973 |
[32m[20221213 20:58:45 @agent_ppo2.py:185][0m |          -0.0037 |          58.2452 |          16.9984 |
[32m[20221213 20:58:45 @agent_ppo2.py:185][0m |          -0.0071 |          58.3746 |          16.9958 |
[32m[20221213 20:58:45 @agent_ppo2.py:185][0m |          -0.0077 |          58.1633 |          16.9961 |
[32m[20221213 20:58:45 @agent_ppo2.py:185][0m |          -0.0052 |          58.2573 |          16.9987 |
[32m[20221213 20:58:46 @agent_ppo2.py:185][0m |          -0.0077 |          58.0749 |          16.9962 |
[32m[20221213 20:58:46 @agent_ppo2.py:185][0m |          -0.0076 |          57.9716 |          16.9888 |
[32m[20221213 20:58:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:58:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.40
[32m[20221213 20:58:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.00
[32m[20221213 20:58:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.00
[32m[20221213 20:58:46 @agent_ppo2.py:143][0m Total time:       3.18 min
[32m[20221213 20:58:46 @agent_ppo2.py:145][0m 309248 total steps have happened
[32m[20221213 20:58:46 @agent_ppo2.py:121][0m #------------------------ Iteration 151 --------------------------#
[32m[20221213 20:58:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:46 @agent_ppo2.py:185][0m |          -0.0021 |          58.6712 |          17.1761 |
[32m[20221213 20:58:46 @agent_ppo2.py:185][0m |           0.0115 |          66.9990 |          17.1679 |
[32m[20221213 20:58:46 @agent_ppo2.py:185][0m |          -0.0052 |          58.0777 |          17.1448 |
[32m[20221213 20:58:46 @agent_ppo2.py:185][0m |          -0.0041 |          58.0793 |          17.1433 |
[32m[20221213 20:58:46 @agent_ppo2.py:185][0m |          -0.0058 |          57.8397 |          17.1380 |
[32m[20221213 20:58:46 @agent_ppo2.py:185][0m |          -0.0050 |          57.8152 |          17.1337 |
[32m[20221213 20:58:47 @agent_ppo2.py:185][0m |          -0.0039 |          57.6290 |          17.1347 |
[32m[20221213 20:58:47 @agent_ppo2.py:185][0m |          -0.0039 |          57.7962 |          17.1323 |
[32m[20221213 20:58:47 @agent_ppo2.py:185][0m |          -0.0086 |          57.6814 |          17.1396 |
[32m[20221213 20:58:47 @agent_ppo2.py:185][0m |          -0.0071 |          57.6165 |          17.1350 |
[32m[20221213 20:58:47 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:58:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.60
[32m[20221213 20:58:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.00
[32m[20221213 20:58:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.00
[32m[20221213 20:58:47 @agent_ppo2.py:143][0m Total time:       3.20 min
[32m[20221213 20:58:47 @agent_ppo2.py:145][0m 311296 total steps have happened
[32m[20221213 20:58:47 @agent_ppo2.py:121][0m #------------------------ Iteration 152 --------------------------#
[32m[20221213 20:58:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:47 @agent_ppo2.py:185][0m |          -0.0020 |          60.6997 |          17.2279 |
[32m[20221213 20:58:47 @agent_ppo2.py:185][0m |          -0.0033 |          59.4694 |          17.2261 |
[32m[20221213 20:58:47 @agent_ppo2.py:185][0m |          -0.0019 |          59.1587 |          17.2209 |
[32m[20221213 20:58:48 @agent_ppo2.py:185][0m |          -0.0040 |          58.9082 |          17.2150 |
[32m[20221213 20:58:48 @agent_ppo2.py:185][0m |          -0.0028 |          58.8380 |          17.2170 |
[32m[20221213 20:58:48 @agent_ppo2.py:185][0m |          -0.0056 |          58.5165 |          17.2101 |
[32m[20221213 20:58:48 @agent_ppo2.py:185][0m |           0.0037 |          63.7362 |          17.2167 |
[32m[20221213 20:58:48 @agent_ppo2.py:185][0m |           0.0049 |          64.5613 |          17.2196 |
[32m[20221213 20:58:48 @agent_ppo2.py:185][0m |          -0.0065 |          58.2452 |          17.2150 |
[32m[20221213 20:58:48 @agent_ppo2.py:185][0m |          -0.0054 |          58.2701 |          17.2196 |
[32m[20221213 20:58:48 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:58:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.60
[32m[20221213 20:58:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.00
[32m[20221213 20:58:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.00
[32m[20221213 20:58:48 @agent_ppo2.py:143][0m Total time:       3.23 min
[32m[20221213 20:58:48 @agent_ppo2.py:145][0m 313344 total steps have happened
[32m[20221213 20:58:48 @agent_ppo2.py:121][0m #------------------------ Iteration 153 --------------------------#
[32m[20221213 20:58:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 20:58:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:49 @agent_ppo2.py:185][0m |          -0.0005 |          63.1102 |          17.0830 |
[32m[20221213 20:58:49 @agent_ppo2.py:185][0m |          -0.0011 |          62.6346 |          17.0696 |
[32m[20221213 20:58:49 @agent_ppo2.py:185][0m |          -0.0052 |          62.6135 |          17.0647 |
[32m[20221213 20:58:49 @agent_ppo2.py:185][0m |          -0.0050 |          62.3925 |          17.0677 |
[32m[20221213 20:58:49 @agent_ppo2.py:185][0m |          -0.0044 |          62.5055 |          17.0556 |
[32m[20221213 20:58:49 @agent_ppo2.py:185][0m |          -0.0036 |          62.4527 |          17.0711 |
[32m[20221213 20:58:49 @agent_ppo2.py:185][0m |          -0.0075 |          62.2783 |          17.0640 |
[32m[20221213 20:58:49 @agent_ppo2.py:185][0m |          -0.0081 |          62.2231 |          17.0631 |
[32m[20221213 20:58:49 @agent_ppo2.py:185][0m |          -0.0102 |          62.2198 |          17.0611 |
[32m[20221213 20:58:49 @agent_ppo2.py:185][0m |          -0.0033 |          62.5876 |          17.0637 |
[32m[20221213 20:58:49 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:58:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.60
[32m[20221213 20:58:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.00
[32m[20221213 20:58:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.00
[32m[20221213 20:58:50 @agent_ppo2.py:143][0m Total time:       3.25 min
[32m[20221213 20:58:50 @agent_ppo2.py:145][0m 315392 total steps have happened
[32m[20221213 20:58:50 @agent_ppo2.py:121][0m #------------------------ Iteration 154 --------------------------#
[32m[20221213 20:58:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:50 @agent_ppo2.py:185][0m |           0.0056 |          64.3443 |          17.2612 |
[32m[20221213 20:58:50 @agent_ppo2.py:185][0m |          -0.0032 |          60.7113 |          17.2476 |
[32m[20221213 20:58:50 @agent_ppo2.py:185][0m |          -0.0060 |          60.5651 |          17.2417 |
[32m[20221213 20:58:50 @agent_ppo2.py:185][0m |          -0.0034 |          60.5538 |          17.2420 |
[32m[20221213 20:58:50 @agent_ppo2.py:185][0m |          -0.0015 |          60.3972 |          17.2394 |
[32m[20221213 20:58:50 @agent_ppo2.py:185][0m |          -0.0076 |          60.1436 |          17.2343 |
[32m[20221213 20:58:50 @agent_ppo2.py:185][0m |          -0.0048 |          60.0535 |          17.2385 |
[32m[20221213 20:58:50 @agent_ppo2.py:185][0m |          -0.0054 |          60.0233 |          17.2355 |
[32m[20221213 20:58:51 @agent_ppo2.py:185][0m |          -0.0045 |          60.2692 |          17.2374 |
[32m[20221213 20:58:51 @agent_ppo2.py:185][0m |          -0.0067 |          60.0086 |          17.2392 |
[32m[20221213 20:58:51 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:58:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.60
[32m[20221213 20:58:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.00
[32m[20221213 20:58:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.00
[32m[20221213 20:58:51 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 541.00
[32m[20221213 20:58:51 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 541.00
[32m[20221213 20:58:51 @agent_ppo2.py:143][0m Total time:       3.27 min
[32m[20221213 20:58:51 @agent_ppo2.py:145][0m 317440 total steps have happened
[32m[20221213 20:58:51 @agent_ppo2.py:121][0m #------------------------ Iteration 155 --------------------------#
[32m[20221213 20:58:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:51 @agent_ppo2.py:185][0m |          -0.0041 |          61.7427 |          17.2864 |
[32m[20221213 20:58:51 @agent_ppo2.py:185][0m |          -0.0073 |          61.4012 |          17.2724 |
[32m[20221213 20:58:51 @agent_ppo2.py:185][0m |          -0.0047 |          61.2199 |          17.2655 |
[32m[20221213 20:58:51 @agent_ppo2.py:185][0m |           0.0029 |          67.9601 |          17.2643 |
[32m[20221213 20:58:51 @agent_ppo2.py:185][0m |          -0.0073 |          61.0292 |          17.2624 |
[32m[20221213 20:58:51 @agent_ppo2.py:185][0m |          -0.0061 |          60.8268 |          17.2620 |
[32m[20221213 20:58:52 @agent_ppo2.py:185][0m |          -0.0081 |          60.7069 |          17.2659 |
[32m[20221213 20:58:52 @agent_ppo2.py:185][0m |          -0.0087 |          60.7876 |          17.2582 |
[32m[20221213 20:58:52 @agent_ppo2.py:185][0m |          -0.0081 |          60.6932 |          17.2629 |
[32m[20221213 20:58:52 @agent_ppo2.py:185][0m |          -0.0059 |          60.9100 |          17.2613 |
[32m[20221213 20:58:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:58:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.20
[32m[20221213 20:58:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.00
[32m[20221213 20:58:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.00
[32m[20221213 20:58:52 @agent_ppo2.py:143][0m Total time:       3.29 min
[32m[20221213 20:58:52 @agent_ppo2.py:145][0m 319488 total steps have happened
[32m[20221213 20:58:52 @agent_ppo2.py:121][0m #------------------------ Iteration 156 --------------------------#
[32m[20221213 20:58:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:58:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:52 @agent_ppo2.py:185][0m |           0.0041 |          63.1270 |          17.2265 |
[32m[20221213 20:58:52 @agent_ppo2.py:185][0m |          -0.0046 |          60.9452 |          17.2075 |
[32m[20221213 20:58:52 @agent_ppo2.py:185][0m |           0.0000 |          61.4913 |          17.1964 |
[32m[20221213 20:58:52 @agent_ppo2.py:185][0m |          -0.0063 |          60.3454 |          17.1938 |
[32m[20221213 20:58:53 @agent_ppo2.py:185][0m |          -0.0023 |          59.9197 |          17.1951 |
[32m[20221213 20:58:53 @agent_ppo2.py:185][0m |          -0.0041 |          60.2562 |          17.2076 |
[32m[20221213 20:58:53 @agent_ppo2.py:185][0m |          -0.0056 |          59.3063 |          17.1854 |
[32m[20221213 20:58:53 @agent_ppo2.py:185][0m |          -0.0094 |          59.1412 |          17.1960 |
[32m[20221213 20:58:53 @agent_ppo2.py:185][0m |          -0.0074 |          58.9521 |          17.1936 |
[32m[20221213 20:58:53 @agent_ppo2.py:185][0m |          -0.0025 |          58.9846 |          17.1888 |
[32m[20221213 20:58:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:58:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.60
[32m[20221213 20:58:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.00
[32m[20221213 20:58:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.00
[32m[20221213 20:58:53 @agent_ppo2.py:143][0m Total time:       3.31 min
[32m[20221213 20:58:53 @agent_ppo2.py:145][0m 321536 total steps have happened
[32m[20221213 20:58:53 @agent_ppo2.py:121][0m #------------------------ Iteration 157 --------------------------#
[32m[20221213 20:58:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:53 @agent_ppo2.py:185][0m |           0.0068 |          67.2585 |          17.3686 |
[32m[20221213 20:58:54 @agent_ppo2.py:185][0m |          -0.0002 |          63.4004 |          17.3688 |
[32m[20221213 20:58:54 @agent_ppo2.py:185][0m |          -0.0033 |          63.1605 |          17.3650 |
[32m[20221213 20:58:54 @agent_ppo2.py:185][0m |           0.0011 |          63.8872 |          17.3595 |
[32m[20221213 20:58:54 @agent_ppo2.py:185][0m |          -0.0033 |          62.7620 |          17.3590 |
[32m[20221213 20:58:54 @agent_ppo2.py:185][0m |          -0.0004 |          62.6163 |          17.3499 |
[32m[20221213 20:58:54 @agent_ppo2.py:185][0m |          -0.0055 |          62.6143 |          17.3574 |
[32m[20221213 20:58:54 @agent_ppo2.py:185][0m |          -0.0036 |          62.4545 |          17.3530 |
[32m[20221213 20:58:54 @agent_ppo2.py:185][0m |          -0.0040 |          62.4918 |          17.3535 |
[32m[20221213 20:58:54 @agent_ppo2.py:185][0m |          -0.0063 |          62.3693 |          17.3557 |
[32m[20221213 20:58:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:58:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.20
[32m[20221213 20:58:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.00
[32m[20221213 20:58:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.00
[32m[20221213 20:58:54 @agent_ppo2.py:143][0m Total time:       3.33 min
[32m[20221213 20:58:54 @agent_ppo2.py:145][0m 323584 total steps have happened
[32m[20221213 20:58:54 @agent_ppo2.py:121][0m #------------------------ Iteration 158 --------------------------#
[32m[20221213 20:58:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:55 @agent_ppo2.py:185][0m |           0.0008 |          61.1398 |          17.3379 |
[32m[20221213 20:58:55 @agent_ppo2.py:185][0m |          -0.0009 |          60.7293 |          17.3287 |
[32m[20221213 20:58:55 @agent_ppo2.py:185][0m |          -0.0047 |          60.5867 |          17.3156 |
[32m[20221213 20:58:55 @agent_ppo2.py:185][0m |          -0.0033 |          60.4741 |          17.3197 |
[32m[20221213 20:58:55 @agent_ppo2.py:185][0m |           0.0035 |          63.0706 |          17.3199 |
[32m[20221213 20:58:55 @agent_ppo2.py:185][0m |           0.0012 |          61.5271 |          17.3135 |
[32m[20221213 20:58:55 @agent_ppo2.py:185][0m |          -0.0065 |          60.2595 |          17.3098 |
[32m[20221213 20:58:55 @agent_ppo2.py:185][0m |           0.0079 |          67.0947 |          17.3028 |
[32m[20221213 20:58:55 @agent_ppo2.py:185][0m |          -0.0074 |          60.2230 |          17.3019 |
[32m[20221213 20:58:55 @agent_ppo2.py:185][0m |          -0.0020 |          60.7952 |          17.3105 |
[32m[20221213 20:58:55 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:58:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.40
[32m[20221213 20:58:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.00
[32m[20221213 20:58:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.00
[32m[20221213 20:58:55 @agent_ppo2.py:143][0m Total time:       3.35 min
[32m[20221213 20:58:55 @agent_ppo2.py:145][0m 325632 total steps have happened
[32m[20221213 20:58:55 @agent_ppo2.py:121][0m #------------------------ Iteration 159 --------------------------#
[32m[20221213 20:58:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:56 @agent_ppo2.py:185][0m |          -0.0019 |          62.1968 |          17.3501 |
[32m[20221213 20:58:56 @agent_ppo2.py:185][0m |          -0.0061 |          60.9475 |          17.3379 |
[32m[20221213 20:58:56 @agent_ppo2.py:185][0m |           0.0164 |          65.6746 |          17.3292 |
[32m[20221213 20:58:56 @agent_ppo2.py:185][0m |          -0.0033 |          60.0721 |          17.3124 |
[32m[20221213 20:58:56 @agent_ppo2.py:185][0m |          -0.0072 |          59.2764 |          17.3224 |
[32m[20221213 20:58:56 @agent_ppo2.py:185][0m |          -0.0057 |          58.9677 |          17.3248 |
[32m[20221213 20:58:56 @agent_ppo2.py:185][0m |          -0.0024 |          58.9755 |          17.3324 |
[32m[20221213 20:58:56 @agent_ppo2.py:185][0m |          -0.0037 |          60.2909 |          17.3228 |
[32m[20221213 20:58:56 @agent_ppo2.py:185][0m |          -0.0053 |          58.4060 |          17.3209 |
[32m[20221213 20:58:57 @agent_ppo2.py:185][0m |          -0.0070 |          58.3273 |          17.3244 |
[32m[20221213 20:58:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:58:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.00
[32m[20221213 20:58:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.00
[32m[20221213 20:58:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.00
[32m[20221213 20:58:57 @agent_ppo2.py:143][0m Total time:       3.37 min
[32m[20221213 20:58:57 @agent_ppo2.py:145][0m 327680 total steps have happened
[32m[20221213 20:58:57 @agent_ppo2.py:121][0m #------------------------ Iteration 160 --------------------------#
[32m[20221213 20:58:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:57 @agent_ppo2.py:185][0m |          -0.0030 |          61.4614 |          17.2994 |
[32m[20221213 20:58:57 @agent_ppo2.py:185][0m |          -0.0016 |          61.0077 |          17.2843 |
[32m[20221213 20:58:57 @agent_ppo2.py:185][0m |          -0.0034 |          60.8380 |          17.2778 |
[32m[20221213 20:58:57 @agent_ppo2.py:185][0m |          -0.0027 |          60.7915 |          17.2811 |
[32m[20221213 20:58:57 @agent_ppo2.py:185][0m |          -0.0034 |          60.6525 |          17.2703 |
[32m[20221213 20:58:57 @agent_ppo2.py:185][0m |          -0.0057 |          60.5918 |          17.2751 |
[32m[20221213 20:58:58 @agent_ppo2.py:185][0m |          -0.0038 |          60.5229 |          17.2761 |
[32m[20221213 20:58:58 @agent_ppo2.py:185][0m |          -0.0066 |          60.4778 |          17.2762 |
[32m[20221213 20:58:58 @agent_ppo2.py:185][0m |          -0.0057 |          60.5321 |          17.2741 |
[32m[20221213 20:58:58 @agent_ppo2.py:185][0m |          -0.0030 |          60.6651 |          17.2728 |
[32m[20221213 20:58:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:58:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.80
[32m[20221213 20:58:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.00
[32m[20221213 20:58:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.00
[32m[20221213 20:58:58 @agent_ppo2.py:143][0m Total time:       3.39 min
[32m[20221213 20:58:58 @agent_ppo2.py:145][0m 329728 total steps have happened
[32m[20221213 20:58:58 @agent_ppo2.py:121][0m #------------------------ Iteration 161 --------------------------#
[32m[20221213 20:58:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:58 @agent_ppo2.py:185][0m |          -0.0020 |          64.7115 |          17.3643 |
[32m[20221213 20:58:58 @agent_ppo2.py:185][0m |           0.0015 |          63.7712 |          17.3586 |
[32m[20221213 20:58:58 @agent_ppo2.py:185][0m |          -0.0040 |          62.8503 |          17.3526 |
[32m[20221213 20:58:58 @agent_ppo2.py:185][0m |          -0.0030 |          62.6254 |          17.3417 |
[32m[20221213 20:58:59 @agent_ppo2.py:185][0m |          -0.0054 |          62.2378 |          17.3444 |
[32m[20221213 20:58:59 @agent_ppo2.py:185][0m |          -0.0064 |          61.9064 |          17.3445 |
[32m[20221213 20:58:59 @agent_ppo2.py:185][0m |          -0.0036 |          62.2646 |          17.3421 |
[32m[20221213 20:58:59 @agent_ppo2.py:185][0m |          -0.0065 |          61.6838 |          17.3396 |
[32m[20221213 20:58:59 @agent_ppo2.py:185][0m |          -0.0039 |          61.9842 |          17.3393 |
[32m[20221213 20:58:59 @agent_ppo2.py:185][0m |          -0.0072 |          61.4531 |          17.3396 |
[32m[20221213 20:58:59 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:58:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.40
[32m[20221213 20:58:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.00
[32m[20221213 20:58:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.00
[32m[20221213 20:58:59 @agent_ppo2.py:143][0m Total time:       3.41 min
[32m[20221213 20:58:59 @agent_ppo2.py:145][0m 331776 total steps have happened
[32m[20221213 20:58:59 @agent_ppo2.py:121][0m #------------------------ Iteration 162 --------------------------#
[32m[20221213 20:58:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:58:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:58:59 @agent_ppo2.py:185][0m |          -0.0030 |          62.6699 |          17.2936 |
[32m[20221213 20:58:59 @agent_ppo2.py:185][0m |          -0.0103 |          62.2535 |          17.2895 |
[32m[20221213 20:59:00 @agent_ppo2.py:185][0m |          -0.0031 |          61.7625 |          17.2680 |
[32m[20221213 20:59:00 @agent_ppo2.py:185][0m |          -0.0046 |          61.5756 |          17.2724 |
[32m[20221213 20:59:00 @agent_ppo2.py:185][0m |          -0.0074 |          61.4863 |          17.2765 |
[32m[20221213 20:59:00 @agent_ppo2.py:185][0m |          -0.0065 |          61.4009 |          17.2722 |
[32m[20221213 20:59:00 @agent_ppo2.py:185][0m |          -0.0023 |          62.5902 |          17.2695 |
[32m[20221213 20:59:00 @agent_ppo2.py:185][0m |          -0.0048 |          61.3463 |          17.2665 |
[32m[20221213 20:59:00 @agent_ppo2.py:185][0m |          -0.0077 |          61.2380 |          17.2668 |
[32m[20221213 20:59:00 @agent_ppo2.py:185][0m |          -0.0036 |          61.0463 |          17.2677 |
[32m[20221213 20:59:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:59:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.20
[32m[20221213 20:59:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.00
[32m[20221213 20:59:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.00
[32m[20221213 20:59:00 @agent_ppo2.py:143][0m Total time:       3.43 min
[32m[20221213 20:59:00 @agent_ppo2.py:145][0m 333824 total steps have happened
[32m[20221213 20:59:00 @agent_ppo2.py:121][0m #------------------------ Iteration 163 --------------------------#
[32m[20221213 20:59:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:01 @agent_ppo2.py:185][0m |           0.0004 |          61.8808 |          17.2902 |
[32m[20221213 20:59:01 @agent_ppo2.py:185][0m |          -0.0046 |          61.2406 |          17.2866 |
[32m[20221213 20:59:01 @agent_ppo2.py:185][0m |          -0.0031 |          61.1613 |          17.2875 |
[32m[20221213 20:59:01 @agent_ppo2.py:185][0m |          -0.0043 |          60.8300 |          17.2834 |
[32m[20221213 20:59:01 @agent_ppo2.py:185][0m |          -0.0077 |          60.8041 |          17.2738 |
[32m[20221213 20:59:01 @agent_ppo2.py:185][0m |          -0.0051 |          60.6610 |          17.2733 |
[32m[20221213 20:59:01 @agent_ppo2.py:185][0m |          -0.0073 |          60.5567 |          17.2728 |
[32m[20221213 20:59:01 @agent_ppo2.py:185][0m |          -0.0035 |          60.4402 |          17.2723 |
[32m[20221213 20:59:01 @agent_ppo2.py:185][0m |           0.0211 |          75.6101 |          17.2753 |
[32m[20221213 20:59:01 @agent_ppo2.py:185][0m |          -0.0046 |          60.7627 |          17.2546 |
[32m[20221213 20:59:01 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:59:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.00
[32m[20221213 20:59:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.00
[32m[20221213 20:59:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 533.00
[32m[20221213 20:59:01 @agent_ppo2.py:143][0m Total time:       3.45 min
[32m[20221213 20:59:01 @agent_ppo2.py:145][0m 335872 total steps have happened
[32m[20221213 20:59:01 @agent_ppo2.py:121][0m #------------------------ Iteration 164 --------------------------#
[32m[20221213 20:59:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:02 @agent_ppo2.py:185][0m |           0.0028 |          62.8937 |          17.3113 |
[32m[20221213 20:59:02 @agent_ppo2.py:185][0m |          -0.0036 |          60.7757 |          17.2957 |
[32m[20221213 20:59:02 @agent_ppo2.py:185][0m |          -0.0045 |          60.5174 |          17.2921 |
[32m[20221213 20:59:02 @agent_ppo2.py:185][0m |          -0.0069 |          60.4027 |          17.2846 |
[32m[20221213 20:59:02 @agent_ppo2.py:185][0m |           0.0052 |          66.6205 |          17.2829 |
[32m[20221213 20:59:02 @agent_ppo2.py:185][0m |          -0.0053 |          60.3112 |          17.2814 |
[32m[20221213 20:59:02 @agent_ppo2.py:185][0m |          -0.0022 |          60.3129 |          17.2850 |
[32m[20221213 20:59:02 @agent_ppo2.py:185][0m |          -0.0045 |          60.5090 |          17.2775 |
[32m[20221213 20:59:02 @agent_ppo2.py:185][0m |          -0.0034 |          60.2656 |          17.2802 |
[32m[20221213 20:59:03 @agent_ppo2.py:185][0m |          -0.0031 |          59.7732 |          17.2784 |
[32m[20221213 20:59:03 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:59:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.20
[32m[20221213 20:59:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.00
[32m[20221213 20:59:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.00
[32m[20221213 20:59:03 @agent_ppo2.py:143][0m Total time:       3.47 min
[32m[20221213 20:59:03 @agent_ppo2.py:145][0m 337920 total steps have happened
[32m[20221213 20:59:03 @agent_ppo2.py:121][0m #------------------------ Iteration 165 --------------------------#
[32m[20221213 20:59:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:03 @agent_ppo2.py:185][0m |           0.0050 |          60.3041 |          17.4448 |
[32m[20221213 20:59:03 @agent_ppo2.py:185][0m |          -0.0047 |          58.5967 |          17.4229 |
[32m[20221213 20:59:03 @agent_ppo2.py:185][0m |          -0.0045 |          58.2329 |          17.4148 |
[32m[20221213 20:59:03 @agent_ppo2.py:185][0m |          -0.0030 |          58.6690 |          17.4090 |
[32m[20221213 20:59:03 @agent_ppo2.py:185][0m |          -0.0056 |          58.0778 |          17.4089 |
[32m[20221213 20:59:03 @agent_ppo2.py:185][0m |          -0.0084 |          58.0514 |          17.4083 |
[32m[20221213 20:59:03 @agent_ppo2.py:185][0m |          -0.0067 |          57.8324 |          17.4050 |
[32m[20221213 20:59:04 @agent_ppo2.py:185][0m |          -0.0067 |          57.7696 |          17.4096 |
[32m[20221213 20:59:04 @agent_ppo2.py:185][0m |          -0.0076 |          57.7357 |          17.4092 |
[32m[20221213 20:59:04 @agent_ppo2.py:185][0m |          -0.0053 |          57.7878 |          17.4042 |
[32m[20221213 20:59:04 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:59:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.00
[32m[20221213 20:59:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.00
[32m[20221213 20:59:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.00
[32m[20221213 20:59:04 @agent_ppo2.py:143][0m Total time:       3.49 min
[32m[20221213 20:59:04 @agent_ppo2.py:145][0m 339968 total steps have happened
[32m[20221213 20:59:04 @agent_ppo2.py:121][0m #------------------------ Iteration 166 --------------------------#
[32m[20221213 20:59:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:59:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:04 @agent_ppo2.py:185][0m |           0.0204 |          73.0686 |          17.4266 |
[32m[20221213 20:59:04 @agent_ppo2.py:185][0m |          -0.0044 |          62.4900 |          17.4025 |
[32m[20221213 20:59:04 @agent_ppo2.py:185][0m |          -0.0063 |          62.1608 |          17.3991 |
[32m[20221213 20:59:04 @agent_ppo2.py:185][0m |          -0.0030 |          62.5539 |          17.3929 |
[32m[20221213 20:59:05 @agent_ppo2.py:185][0m |          -0.0089 |          62.0347 |          17.3957 |
[32m[20221213 20:59:05 @agent_ppo2.py:185][0m |          -0.0093 |          61.8707 |          17.3936 |
[32m[20221213 20:59:05 @agent_ppo2.py:185][0m |          -0.0026 |          63.4964 |          17.3939 |
[32m[20221213 20:59:05 @agent_ppo2.py:185][0m |          -0.0071 |          61.6510 |          17.3906 |
[32m[20221213 20:59:05 @agent_ppo2.py:185][0m |          -0.0081 |          61.5904 |          17.3913 |
[32m[20221213 20:59:05 @agent_ppo2.py:185][0m |          -0.0055 |          61.6582 |          17.3988 |
[32m[20221213 20:59:05 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:59:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.20
[32m[20221213 20:59:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.00
[32m[20221213 20:59:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.00
[32m[20221213 20:59:05 @agent_ppo2.py:143][0m Total time:       3.51 min
[32m[20221213 20:59:05 @agent_ppo2.py:145][0m 342016 total steps have happened
[32m[20221213 20:59:05 @agent_ppo2.py:121][0m #------------------------ Iteration 167 --------------------------#
[32m[20221213 20:59:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:05 @agent_ppo2.py:185][0m |           0.0002 |          62.0825 |          17.4821 |
[32m[20221213 20:59:05 @agent_ppo2.py:185][0m |          -0.0036 |          61.7571 |          17.4606 |
[32m[20221213 20:59:06 @agent_ppo2.py:185][0m |          -0.0045 |          61.6505 |          17.4498 |
[32m[20221213 20:59:06 @agent_ppo2.py:185][0m |          -0.0090 |          61.6459 |          17.4393 |
[32m[20221213 20:59:06 @agent_ppo2.py:185][0m |          -0.0053 |          61.3303 |          17.4400 |
[32m[20221213 20:59:06 @agent_ppo2.py:185][0m |          -0.0046 |          61.1847 |          17.4398 |
[32m[20221213 20:59:06 @agent_ppo2.py:185][0m |          -0.0078 |          61.1965 |          17.4430 |
[32m[20221213 20:59:06 @agent_ppo2.py:185][0m |          -0.0073 |          61.1188 |          17.4361 |
[32m[20221213 20:59:06 @agent_ppo2.py:185][0m |          -0.0085 |          61.1384 |          17.4418 |
[32m[20221213 20:59:06 @agent_ppo2.py:185][0m |           0.0016 |          64.5277 |          17.4340 |
[32m[20221213 20:59:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:59:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.00
[32m[20221213 20:59:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.00
[32m[20221213 20:59:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.00
[32m[20221213 20:59:06 @agent_ppo2.py:143][0m Total time:       3.53 min
[32m[20221213 20:59:06 @agent_ppo2.py:145][0m 344064 total steps have happened
[32m[20221213 20:59:06 @agent_ppo2.py:121][0m #------------------------ Iteration 168 --------------------------#
[32m[20221213 20:59:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:07 @agent_ppo2.py:185][0m |           0.0007 |          62.0863 |          17.5780 |
[32m[20221213 20:59:07 @agent_ppo2.py:185][0m |          -0.0017 |          61.4075 |          17.5777 |
[32m[20221213 20:59:07 @agent_ppo2.py:185][0m |          -0.0002 |          61.7336 |          17.5716 |
[32m[20221213 20:59:07 @agent_ppo2.py:185][0m |          -0.0059 |          61.1126 |          17.5679 |
[32m[20221213 20:59:07 @agent_ppo2.py:185][0m |          -0.0012 |          60.9025 |          17.5699 |
[32m[20221213 20:59:07 @agent_ppo2.py:185][0m |          -0.0023 |          61.0364 |          17.5630 |
[32m[20221213 20:59:07 @agent_ppo2.py:185][0m |          -0.0043 |          60.7775 |          17.5638 |
[32m[20221213 20:59:07 @agent_ppo2.py:185][0m |          -0.0051 |          60.7132 |          17.5631 |
[32m[20221213 20:59:07 @agent_ppo2.py:185][0m |          -0.0062 |          60.6535 |          17.5635 |
[32m[20221213 20:59:07 @agent_ppo2.py:185][0m |          -0.0037 |          60.8721 |          17.5610 |
[32m[20221213 20:59:07 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:59:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.60
[32m[20221213 20:59:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.00
[32m[20221213 20:59:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.00
[32m[20221213 20:59:07 @agent_ppo2.py:143][0m Total time:       3.55 min
[32m[20221213 20:59:07 @agent_ppo2.py:145][0m 346112 total steps have happened
[32m[20221213 20:59:07 @agent_ppo2.py:121][0m #------------------------ Iteration 169 --------------------------#
[32m[20221213 20:59:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:59:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:08 @agent_ppo2.py:185][0m |          -0.0015 |          63.5032 |          17.6278 |
[32m[20221213 20:59:08 @agent_ppo2.py:185][0m |          -0.0069 |          63.0127 |          17.6109 |
[32m[20221213 20:59:08 @agent_ppo2.py:185][0m |          -0.0082 |          62.7387 |          17.5940 |
[32m[20221213 20:59:08 @agent_ppo2.py:185][0m |          -0.0071 |          62.5009 |          17.5928 |
[32m[20221213 20:59:08 @agent_ppo2.py:185][0m |          -0.0057 |          62.3441 |          17.5797 |
[32m[20221213 20:59:08 @agent_ppo2.py:185][0m |          -0.0009 |          63.9618 |          17.5826 |
[32m[20221213 20:59:08 @agent_ppo2.py:185][0m |          -0.0064 |          62.1109 |          17.5669 |
[32m[20221213 20:59:08 @agent_ppo2.py:185][0m |          -0.0078 |          62.0959 |          17.5824 |
[32m[20221213 20:59:08 @agent_ppo2.py:185][0m |          -0.0097 |          62.2051 |          17.5743 |
[32m[20221213 20:59:09 @agent_ppo2.py:185][0m |          -0.0101 |          61.9816 |          17.5761 |
[32m[20221213 20:59:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 20:59:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.60
[32m[20221213 20:59:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.00
[32m[20221213 20:59:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.00
[32m[20221213 20:59:09 @agent_ppo2.py:143][0m Total time:       3.57 min
[32m[20221213 20:59:09 @agent_ppo2.py:145][0m 348160 total steps have happened
[32m[20221213 20:59:09 @agent_ppo2.py:121][0m #------------------------ Iteration 170 --------------------------#
[32m[20221213 20:59:09 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:59:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:09 @agent_ppo2.py:185][0m |          -0.0021 |          64.1071 |          17.6472 |
[32m[20221213 20:59:09 @agent_ppo2.py:185][0m |          -0.0007 |          63.7013 |          17.6285 |
[32m[20221213 20:59:09 @agent_ppo2.py:185][0m |           0.0022 |          64.2124 |          17.6253 |
[32m[20221213 20:59:09 @agent_ppo2.py:185][0m |          -0.0068 |          63.2216 |          17.6153 |
[32m[20221213 20:59:09 @agent_ppo2.py:185][0m |          -0.0033 |          63.5058 |          17.6193 |
[32m[20221213 20:59:09 @agent_ppo2.py:185][0m |          -0.0054 |          62.9624 |          17.6196 |
[32m[20221213 20:59:09 @agent_ppo2.py:185][0m |           0.0001 |          63.3443 |          17.6174 |
[32m[20221213 20:59:10 @agent_ppo2.py:185][0m |          -0.0079 |          62.8311 |          17.6241 |
[32m[20221213 20:59:10 @agent_ppo2.py:185][0m |          -0.0071 |          62.6670 |          17.6151 |
[32m[20221213 20:59:10 @agent_ppo2.py:185][0m |          -0.0062 |          62.5167 |          17.6165 |
[32m[20221213 20:59:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:59:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.80
[32m[20221213 20:59:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.00
[32m[20221213 20:59:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.00
[32m[20221213 20:59:10 @agent_ppo2.py:143][0m Total time:       3.59 min
[32m[20221213 20:59:10 @agent_ppo2.py:145][0m 350208 total steps have happened
[32m[20221213 20:59:10 @agent_ppo2.py:121][0m #------------------------ Iteration 171 --------------------------#
[32m[20221213 20:59:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:10 @agent_ppo2.py:185][0m |          -0.0026 |          65.0254 |          17.4956 |
[32m[20221213 20:59:10 @agent_ppo2.py:185][0m |          -0.0020 |          63.8516 |          17.4854 |
[32m[20221213 20:59:10 @agent_ppo2.py:185][0m |          -0.0055 |          63.4207 |          17.4783 |
[32m[20221213 20:59:10 @agent_ppo2.py:185][0m |          -0.0034 |          64.2291 |          17.4661 |
[32m[20221213 20:59:10 @agent_ppo2.py:185][0m |          -0.0032 |          62.3028 |          17.4716 |
[32m[20221213 20:59:11 @agent_ppo2.py:185][0m |          -0.0066 |          61.9492 |          17.4685 |
[32m[20221213 20:59:11 @agent_ppo2.py:185][0m |          -0.0078 |          61.6731 |          17.4724 |
[32m[20221213 20:59:11 @agent_ppo2.py:185][0m |          -0.0044 |          61.2497 |          17.4701 |
[32m[20221213 20:59:11 @agent_ppo2.py:185][0m |          -0.0051 |          60.9712 |          17.4756 |
[32m[20221213 20:59:11 @agent_ppo2.py:185][0m |          -0.0059 |          60.6742 |          17.4711 |
[32m[20221213 20:59:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:59:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.60
[32m[20221213 20:59:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.00
[32m[20221213 20:59:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.00
[32m[20221213 20:59:11 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 543.00
[32m[20221213 20:59:11 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 543.00
[32m[20221213 20:59:11 @agent_ppo2.py:143][0m Total time:       3.61 min
[32m[20221213 20:59:11 @agent_ppo2.py:145][0m 352256 total steps have happened
[32m[20221213 20:59:11 @agent_ppo2.py:121][0m #------------------------ Iteration 172 --------------------------#
[32m[20221213 20:59:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:11 @agent_ppo2.py:185][0m |          -0.0016 |          68.3918 |          17.6123 |
[32m[20221213 20:59:11 @agent_ppo2.py:185][0m |          -0.0019 |          67.4191 |          17.6044 |
[32m[20221213 20:59:12 @agent_ppo2.py:185][0m |          -0.0018 |          68.2765 |          17.5999 |
[32m[20221213 20:59:12 @agent_ppo2.py:185][0m |          -0.0040 |          66.5180 |          17.5927 |
[32m[20221213 20:59:12 @agent_ppo2.py:185][0m |          -0.0036 |          66.1286 |          17.5908 |
[32m[20221213 20:59:12 @agent_ppo2.py:185][0m |          -0.0043 |          65.8313 |          17.5912 |
[32m[20221213 20:59:12 @agent_ppo2.py:185][0m |          -0.0090 |          65.9746 |          17.5955 |
[32m[20221213 20:59:12 @agent_ppo2.py:185][0m |          -0.0049 |          65.6535 |          17.5907 |
[32m[20221213 20:59:12 @agent_ppo2.py:185][0m |          -0.0040 |          65.4547 |          17.5944 |
[32m[20221213 20:59:12 @agent_ppo2.py:185][0m |          -0.0071 |          65.3658 |          17.5900 |
[32m[20221213 20:59:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:59:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.60
[32m[20221213 20:59:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.00
[32m[20221213 20:59:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 524.00
[32m[20221213 20:59:12 @agent_ppo2.py:143][0m Total time:       3.63 min
[32m[20221213 20:59:12 @agent_ppo2.py:145][0m 354304 total steps have happened
[32m[20221213 20:59:12 @agent_ppo2.py:121][0m #------------------------ Iteration 173 --------------------------#
[32m[20221213 20:59:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:13 @agent_ppo2.py:185][0m |          -0.0011 |          66.2351 |          17.6056 |
[32m[20221213 20:59:13 @agent_ppo2.py:185][0m |          -0.0056 |          65.8450 |          17.5850 |
[32m[20221213 20:59:13 @agent_ppo2.py:185][0m |          -0.0027 |          65.5430 |          17.5882 |
[32m[20221213 20:59:13 @agent_ppo2.py:185][0m |          -0.0024 |          65.4945 |          17.5859 |
[32m[20221213 20:59:13 @agent_ppo2.py:185][0m |           0.0013 |          66.7436 |          17.5806 |
[32m[20221213 20:59:13 @agent_ppo2.py:185][0m |          -0.0054 |          65.2311 |          17.5895 |
[32m[20221213 20:59:13 @agent_ppo2.py:185][0m |          -0.0026 |          65.0709 |          17.5881 |
[32m[20221213 20:59:13 @agent_ppo2.py:185][0m |          -0.0049 |          65.1613 |          17.5807 |
[32m[20221213 20:59:13 @agent_ppo2.py:185][0m |          -0.0071 |          65.0588 |          17.5813 |
[32m[20221213 20:59:13 @agent_ppo2.py:185][0m |          -0.0021 |          65.7738 |          17.5814 |
[32m[20221213 20:59:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:59:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.00
[32m[20221213 20:59:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.00
[32m[20221213 20:59:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 534.00
[32m[20221213 20:59:13 @agent_ppo2.py:143][0m Total time:       3.65 min
[32m[20221213 20:59:13 @agent_ppo2.py:145][0m 356352 total steps have happened
[32m[20221213 20:59:13 @agent_ppo2.py:121][0m #------------------------ Iteration 174 --------------------------#
[32m[20221213 20:59:14 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:59:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:14 @agent_ppo2.py:185][0m |           0.0026 |          64.5150 |          17.5987 |
[32m[20221213 20:59:14 @agent_ppo2.py:185][0m |           0.0065 |          67.2129 |          17.5840 |
[32m[20221213 20:59:14 @agent_ppo2.py:185][0m |          -0.0051 |          63.1595 |          17.5608 |
[32m[20221213 20:59:14 @agent_ppo2.py:185][0m |          -0.0042 |          62.9623 |          17.5610 |
[32m[20221213 20:59:14 @agent_ppo2.py:185][0m |          -0.0060 |          62.7589 |          17.5535 |
[32m[20221213 20:59:14 @agent_ppo2.py:185][0m |          -0.0056 |          62.4721 |          17.5483 |
[32m[20221213 20:59:14 @agent_ppo2.py:185][0m |          -0.0057 |          62.3898 |          17.5499 |
[32m[20221213 20:59:14 @agent_ppo2.py:185][0m |          -0.0055 |          62.3408 |          17.5555 |
[32m[20221213 20:59:14 @agent_ppo2.py:185][0m |          -0.0060 |          62.1686 |          17.5524 |
[32m[20221213 20:59:15 @agent_ppo2.py:185][0m |          -0.0083 |          62.1267 |          17.5459 |
[32m[20221213 20:59:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:59:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.40
[32m[20221213 20:59:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.00
[32m[20221213 20:59:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.00
[32m[20221213 20:59:15 @agent_ppo2.py:143][0m Total time:       3.67 min
[32m[20221213 20:59:15 @agent_ppo2.py:145][0m 358400 total steps have happened
[32m[20221213 20:59:15 @agent_ppo2.py:121][0m #------------------------ Iteration 175 --------------------------#
[32m[20221213 20:59:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:15 @agent_ppo2.py:185][0m |           0.0001 |          68.0543 |          17.5755 |
[32m[20221213 20:59:15 @agent_ppo2.py:185][0m |           0.0104 |          75.3320 |          17.5661 |
[32m[20221213 20:59:15 @agent_ppo2.py:185][0m |           0.0001 |          67.6305 |          17.5488 |
[32m[20221213 20:59:15 @agent_ppo2.py:185][0m |          -0.0056 |          67.1466 |          17.5534 |
[32m[20221213 20:59:15 @agent_ppo2.py:185][0m |          -0.0017 |          66.9635 |          17.5470 |
[32m[20221213 20:59:15 @agent_ppo2.py:185][0m |          -0.0045 |          67.1663 |          17.5495 |
[32m[20221213 20:59:16 @agent_ppo2.py:185][0m |          -0.0051 |          66.6352 |          17.5460 |
[32m[20221213 20:59:16 @agent_ppo2.py:185][0m |          -0.0062 |          66.4776 |          17.5479 |
[32m[20221213 20:59:16 @agent_ppo2.py:185][0m |          -0.0052 |          66.3413 |          17.5465 |
[32m[20221213 20:59:16 @agent_ppo2.py:185][0m |          -0.0088 |          66.4124 |          17.5479 |
[32m[20221213 20:59:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:59:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.80
[32m[20221213 20:59:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.00
[32m[20221213 20:59:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.00
[32m[20221213 20:59:16 @agent_ppo2.py:143][0m Total time:       3.69 min
[32m[20221213 20:59:16 @agent_ppo2.py:145][0m 360448 total steps have happened
[32m[20221213 20:59:16 @agent_ppo2.py:121][0m #------------------------ Iteration 176 --------------------------#
[32m[20221213 20:59:16 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:59:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:16 @agent_ppo2.py:185][0m |           0.0001 |          69.0756 |          17.5254 |
[32m[20221213 20:59:16 @agent_ppo2.py:185][0m |          -0.0013 |          68.5472 |          17.4978 |
[32m[20221213 20:59:16 @agent_ppo2.py:185][0m |          -0.0071 |          68.2337 |          17.4975 |
[32m[20221213 20:59:17 @agent_ppo2.py:185][0m |           0.0083 |          73.0542 |          17.5026 |
[32m[20221213 20:59:17 @agent_ppo2.py:185][0m |          -0.0041 |          67.8174 |          17.4926 |
[32m[20221213 20:59:17 @agent_ppo2.py:185][0m |          -0.0038 |          68.1769 |          17.4950 |
[32m[20221213 20:59:17 @agent_ppo2.py:185][0m |          -0.0065 |          67.6759 |          17.4834 |
[32m[20221213 20:59:17 @agent_ppo2.py:185][0m |           0.0005 |          69.3938 |          17.4850 |
[32m[20221213 20:59:17 @agent_ppo2.py:185][0m |          -0.0064 |          67.4209 |          17.4793 |
[32m[20221213 20:59:17 @agent_ppo2.py:185][0m |           0.0062 |          76.1682 |          17.4878 |
[32m[20221213 20:59:17 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 20:59:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.20
[32m[20221213 20:59:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.00
[32m[20221213 20:59:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.00
[32m[20221213 20:59:17 @agent_ppo2.py:143][0m Total time:       3.71 min
[32m[20221213 20:59:17 @agent_ppo2.py:145][0m 362496 total steps have happened
[32m[20221213 20:59:17 @agent_ppo2.py:121][0m #------------------------ Iteration 177 --------------------------#
[32m[20221213 20:59:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:18 @agent_ppo2.py:185][0m |          -0.0033 |          68.4193 |          17.5636 |
[32m[20221213 20:59:18 @agent_ppo2.py:185][0m |          -0.0044 |          67.7723 |          17.5534 |
[32m[20221213 20:59:18 @agent_ppo2.py:185][0m |          -0.0042 |          67.4368 |          17.5571 |
[32m[20221213 20:59:18 @agent_ppo2.py:185][0m |           0.0033 |          69.5551 |          17.5480 |
[32m[20221213 20:59:18 @agent_ppo2.py:185][0m |           0.0031 |          69.6507 |          17.5399 |
[32m[20221213 20:59:18 @agent_ppo2.py:185][0m |          -0.0001 |          67.7368 |          17.5374 |
[32m[20221213 20:59:18 @agent_ppo2.py:185][0m |          -0.0055 |          66.8960 |          17.5391 |
[32m[20221213 20:59:18 @agent_ppo2.py:185][0m |          -0.0076 |          66.8662 |          17.5429 |
[32m[20221213 20:59:18 @agent_ppo2.py:185][0m |          -0.0056 |          66.8671 |          17.5400 |
[32m[20221213 20:59:18 @agent_ppo2.py:185][0m |          -0.0042 |          66.7224 |          17.5442 |
[32m[20221213 20:59:18 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:59:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.40
[32m[20221213 20:59:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.00
[32m[20221213 20:59:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 566.00
[32m[20221213 20:59:19 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 566.00
[32m[20221213 20:59:19 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 566.00
[32m[20221213 20:59:19 @agent_ppo2.py:143][0m Total time:       3.73 min
[32m[20221213 20:59:19 @agent_ppo2.py:145][0m 364544 total steps have happened
[32m[20221213 20:59:19 @agent_ppo2.py:121][0m #------------------------ Iteration 178 --------------------------#
[32m[20221213 20:59:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:19 @agent_ppo2.py:185][0m |          -0.0023 |          65.8847 |          17.5307 |
[32m[20221213 20:59:19 @agent_ppo2.py:185][0m |          -0.0037 |          65.4015 |          17.4979 |
[32m[20221213 20:59:19 @agent_ppo2.py:185][0m |          -0.0008 |          65.5432 |          17.4907 |
[32m[20221213 20:59:19 @agent_ppo2.py:185][0m |          -0.0029 |          65.1041 |          17.4868 |
[32m[20221213 20:59:19 @agent_ppo2.py:185][0m |          -0.0090 |          64.8630 |          17.4876 |
[32m[20221213 20:59:19 @agent_ppo2.py:185][0m |          -0.0058 |          64.6375 |          17.4910 |
[32m[20221213 20:59:19 @agent_ppo2.py:185][0m |          -0.0080 |          64.5592 |          17.4870 |
[32m[20221213 20:59:19 @agent_ppo2.py:185][0m |          -0.0061 |          64.4703 |          17.4936 |
[32m[20221213 20:59:20 @agent_ppo2.py:185][0m |          -0.0077 |          64.5545 |          17.4864 |
[32m[20221213 20:59:20 @agent_ppo2.py:185][0m |          -0.0091 |          64.4548 |          17.4974 |
[32m[20221213 20:59:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:59:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.60
[32m[20221213 20:59:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.00
[32m[20221213 20:59:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.00
[32m[20221213 20:59:20 @agent_ppo2.py:143][0m Total time:       3.75 min
[32m[20221213 20:59:20 @agent_ppo2.py:145][0m 366592 total steps have happened
[32m[20221213 20:59:20 @agent_ppo2.py:121][0m #------------------------ Iteration 179 --------------------------#
[32m[20221213 20:59:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:59:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:20 @agent_ppo2.py:185][0m |          -0.0030 |          65.7069 |          17.7255 |
[32m[20221213 20:59:20 @agent_ppo2.py:185][0m |           0.0002 |          65.4387 |          17.7098 |
[32m[20221213 20:59:20 @agent_ppo2.py:185][0m |          -0.0056 |          64.0788 |          17.6987 |
[32m[20221213 20:59:20 @agent_ppo2.py:185][0m |          -0.0041 |          63.8709 |          17.6989 |
[32m[20221213 20:59:20 @agent_ppo2.py:185][0m |           0.0046 |          64.6537 |          17.6959 |
[32m[20221213 20:59:20 @agent_ppo2.py:185][0m |          -0.0068 |          63.2427 |          17.6923 |
[32m[20221213 20:59:21 @agent_ppo2.py:185][0m |          -0.0075 |          63.0096 |          17.6936 |
[32m[20221213 20:59:21 @agent_ppo2.py:185][0m |          -0.0068 |          62.9802 |          17.6888 |
[32m[20221213 20:59:21 @agent_ppo2.py:185][0m |           0.0005 |          64.2027 |          17.6895 |
[32m[20221213 20:59:21 @agent_ppo2.py:185][0m |          -0.0076 |          62.5790 |          17.6967 |
[32m[20221213 20:59:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:59:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.60
[32m[20221213 20:59:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.00
[32m[20221213 20:59:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.00
[32m[20221213 20:59:21 @agent_ppo2.py:143][0m Total time:       3.77 min
[32m[20221213 20:59:21 @agent_ppo2.py:145][0m 368640 total steps have happened
[32m[20221213 20:59:21 @agent_ppo2.py:121][0m #------------------------ Iteration 180 --------------------------#
[32m[20221213 20:59:21 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:59:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:21 @agent_ppo2.py:185][0m |           0.0021 |          67.0681 |          17.8265 |
[32m[20221213 20:59:21 @agent_ppo2.py:185][0m |          -0.0069 |          65.9676 |          17.7998 |
[32m[20221213 20:59:21 @agent_ppo2.py:185][0m |          -0.0024 |          65.9949 |          17.7905 |
[32m[20221213 20:59:22 @agent_ppo2.py:185][0m |          -0.0054 |          65.4233 |          17.7886 |
[32m[20221213 20:59:22 @agent_ppo2.py:185][0m |          -0.0078 |          65.5172 |          17.7917 |
[32m[20221213 20:59:22 @agent_ppo2.py:185][0m |          -0.0085 |          65.2661 |          17.7875 |
[32m[20221213 20:59:22 @agent_ppo2.py:185][0m |          -0.0084 |          65.3169 |          17.7869 |
[32m[20221213 20:59:22 @agent_ppo2.py:185][0m |          -0.0047 |          65.3612 |          17.7852 |
[32m[20221213 20:59:22 @agent_ppo2.py:185][0m |          -0.0059 |          65.0396 |          17.7739 |
[32m[20221213 20:59:22 @agent_ppo2.py:185][0m |          -0.0030 |          65.1551 |          17.7931 |
[32m[20221213 20:59:22 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 20:59:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.40
[32m[20221213 20:59:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.00
[32m[20221213 20:59:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.00
[32m[20221213 20:59:22 @agent_ppo2.py:143][0m Total time:       3.79 min
[32m[20221213 20:59:22 @agent_ppo2.py:145][0m 370688 total steps have happened
[32m[20221213 20:59:22 @agent_ppo2.py:121][0m #------------------------ Iteration 181 --------------------------#
[32m[20221213 20:59:22 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:59:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:23 @agent_ppo2.py:185][0m |          -0.0001 |          67.0850 |          17.6599 |
[32m[20221213 20:59:23 @agent_ppo2.py:185][0m |          -0.0041 |          66.5723 |          17.6537 |
[32m[20221213 20:59:23 @agent_ppo2.py:185][0m |          -0.0042 |          66.2781 |          17.6363 |
[32m[20221213 20:59:23 @agent_ppo2.py:185][0m |          -0.0042 |          66.2859 |          17.6345 |
[32m[20221213 20:59:23 @agent_ppo2.py:185][0m |          -0.0051 |          66.0701 |          17.6233 |
[32m[20221213 20:59:23 @agent_ppo2.py:185][0m |          -0.0047 |          65.9888 |          17.6346 |
[32m[20221213 20:59:23 @agent_ppo2.py:185][0m |          -0.0077 |          66.0131 |          17.6315 |
[32m[20221213 20:59:23 @agent_ppo2.py:185][0m |          -0.0039 |          65.8096 |          17.6375 |
[32m[20221213 20:59:23 @agent_ppo2.py:185][0m |          -0.0064 |          65.9168 |          17.6298 |
[32m[20221213 20:59:23 @agent_ppo2.py:185][0m |          -0.0041 |          65.7418 |          17.6311 |
[32m[20221213 20:59:23 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:59:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.80
[32m[20221213 20:59:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.00
[32m[20221213 20:59:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.00
[32m[20221213 20:59:24 @agent_ppo2.py:143][0m Total time:       3.81 min
[32m[20221213 20:59:24 @agent_ppo2.py:145][0m 372736 total steps have happened
[32m[20221213 20:59:24 @agent_ppo2.py:121][0m #------------------------ Iteration 182 --------------------------#
[32m[20221213 20:59:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 20:59:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:24 @agent_ppo2.py:185][0m |          -0.0010 |          67.6690 |          17.6178 |
[32m[20221213 20:59:24 @agent_ppo2.py:185][0m |          -0.0022 |          67.0996 |          17.5814 |
[32m[20221213 20:59:24 @agent_ppo2.py:185][0m |           0.0035 |          68.9284 |          17.5762 |
[32m[20221213 20:59:24 @agent_ppo2.py:185][0m |          -0.0040 |          66.5873 |          17.5830 |
[32m[20221213 20:59:24 @agent_ppo2.py:185][0m |          -0.0043 |          66.4117 |          17.5776 |
[32m[20221213 20:59:24 @agent_ppo2.py:185][0m |          -0.0056 |          66.2886 |          17.5757 |
[32m[20221213 20:59:24 @agent_ppo2.py:185][0m |          -0.0050 |          66.3737 |          17.5775 |
[32m[20221213 20:59:25 @agent_ppo2.py:185][0m |          -0.0054 |          66.0449 |          17.5702 |
[32m[20221213 20:59:25 @agent_ppo2.py:185][0m |          -0.0066 |          65.9906 |          17.5790 |
[32m[20221213 20:59:25 @agent_ppo2.py:185][0m |          -0.0035 |          66.0544 |          17.5692 |
[32m[20221213 20:59:25 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 20:59:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.80
[32m[20221213 20:59:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.00
[32m[20221213 20:59:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.00
[32m[20221213 20:59:25 @agent_ppo2.py:143][0m Total time:       3.84 min
[32m[20221213 20:59:25 @agent_ppo2.py:145][0m 374784 total steps have happened
[32m[20221213 20:59:25 @agent_ppo2.py:121][0m #------------------------ Iteration 183 --------------------------#
[32m[20221213 20:59:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:25 @agent_ppo2.py:185][0m |          -0.0023 |          66.9395 |          17.5483 |
[32m[20221213 20:59:25 @agent_ppo2.py:185][0m |          -0.0025 |          66.0790 |          17.5360 |
[32m[20221213 20:59:25 @agent_ppo2.py:185][0m |          -0.0026 |          65.6976 |          17.5313 |
[32m[20221213 20:59:26 @agent_ppo2.py:185][0m |          -0.0024 |          65.1358 |          17.5348 |
[32m[20221213 20:59:26 @agent_ppo2.py:185][0m |          -0.0015 |          64.6829 |          17.5326 |
[32m[20221213 20:59:26 @agent_ppo2.py:185][0m |           0.0002 |          64.6145 |          17.5230 |
[32m[20221213 20:59:26 @agent_ppo2.py:185][0m |           0.0051 |          69.4901 |          17.5323 |
[32m[20221213 20:59:26 @agent_ppo2.py:185][0m |          -0.0060 |          63.8928 |          17.5203 |
[32m[20221213 20:59:26 @agent_ppo2.py:185][0m |           0.0002 |          64.4439 |          17.5203 |
[32m[20221213 20:59:26 @agent_ppo2.py:185][0m |          -0.0038 |          63.3637 |          17.5352 |
[32m[20221213 20:59:26 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:59:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.60
[32m[20221213 20:59:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.00
[32m[20221213 20:59:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.00
[32m[20221213 20:59:26 @agent_ppo2.py:143][0m Total time:       3.86 min
[32m[20221213 20:59:26 @agent_ppo2.py:145][0m 376832 total steps have happened
[32m[20221213 20:59:26 @agent_ppo2.py:121][0m #------------------------ Iteration 184 --------------------------#
[32m[20221213 20:59:26 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:59:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:27 @agent_ppo2.py:185][0m |          -0.0020 |          65.2297 |          17.7138 |
[32m[20221213 20:59:27 @agent_ppo2.py:185][0m |          -0.0069 |          63.9559 |          17.6918 |
[32m[20221213 20:59:27 @agent_ppo2.py:185][0m |          -0.0066 |          63.4548 |          17.6805 |
[32m[20221213 20:59:27 @agent_ppo2.py:185][0m |          -0.0040 |          63.1836 |          17.6714 |
[32m[20221213 20:59:27 @agent_ppo2.py:185][0m |          -0.0082 |          62.9677 |          17.6718 |
[32m[20221213 20:59:27 @agent_ppo2.py:185][0m |          -0.0061 |          62.8216 |          17.6745 |
[32m[20221213 20:59:27 @agent_ppo2.py:185][0m |          -0.0072 |          62.7390 |          17.6798 |
[32m[20221213 20:59:27 @agent_ppo2.py:185][0m |          -0.0052 |          62.6252 |          17.6767 |
[32m[20221213 20:59:27 @agent_ppo2.py:185][0m |          -0.0068 |          62.6026 |          17.6682 |
[32m[20221213 20:59:27 @agent_ppo2.py:185][0m |           0.0059 |          69.8562 |          17.6797 |
[32m[20221213 20:59:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:59:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.20
[32m[20221213 20:59:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.00
[32m[20221213 20:59:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.00
[32m[20221213 20:59:27 @agent_ppo2.py:143][0m Total time:       3.88 min
[32m[20221213 20:59:27 @agent_ppo2.py:145][0m 378880 total steps have happened
[32m[20221213 20:59:27 @agent_ppo2.py:121][0m #------------------------ Iteration 185 --------------------------#
[32m[20221213 20:59:28 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:59:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:28 @agent_ppo2.py:185][0m |           0.0001 |          69.8168 |          17.5860 |
[32m[20221213 20:59:28 @agent_ppo2.py:185][0m |           0.0064 |          73.1490 |          17.5857 |
[32m[20221213 20:59:28 @agent_ppo2.py:185][0m |          -0.0059 |          69.2657 |          17.5701 |
[32m[20221213 20:59:28 @agent_ppo2.py:185][0m |          -0.0063 |          69.1304 |          17.5675 |
[32m[20221213 20:59:28 @agent_ppo2.py:185][0m |          -0.0058 |          68.9370 |          17.5630 |
[32m[20221213 20:59:28 @agent_ppo2.py:185][0m |          -0.0032 |          68.9918 |          17.5577 |
[32m[20221213 20:59:28 @agent_ppo2.py:185][0m |          -0.0002 |          70.4379 |          17.5566 |
[32m[20221213 20:59:28 @agent_ppo2.py:185][0m |          -0.0064 |          68.6532 |          17.5552 |
[32m[20221213 20:59:28 @agent_ppo2.py:185][0m |          -0.0031 |          68.5973 |          17.5553 |
[32m[20221213 20:59:28 @agent_ppo2.py:185][0m |          -0.0045 |          68.5231 |          17.5559 |
[32m[20221213 20:59:28 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:59:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.40
[32m[20221213 20:59:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.00
[32m[20221213 20:59:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.00
[32m[20221213 20:59:29 @agent_ppo2.py:143][0m Total time:       3.90 min
[32m[20221213 20:59:29 @agent_ppo2.py:145][0m 380928 total steps have happened
[32m[20221213 20:59:29 @agent_ppo2.py:121][0m #------------------------ Iteration 186 --------------------------#
[32m[20221213 20:59:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:29 @agent_ppo2.py:185][0m |          -0.0010 |          63.4736 |          17.7524 |
[32m[20221213 20:59:29 @agent_ppo2.py:185][0m |          -0.0020 |          62.7860 |          17.7447 |
[32m[20221213 20:59:29 @agent_ppo2.py:185][0m |          -0.0018 |          62.4988 |          17.7357 |
[32m[20221213 20:59:29 @agent_ppo2.py:185][0m |          -0.0024 |          62.3659 |          17.7326 |
[32m[20221213 20:59:29 @agent_ppo2.py:185][0m |           0.0041 |          64.9244 |          17.7317 |
[32m[20221213 20:59:29 @agent_ppo2.py:185][0m |          -0.0022 |          62.2283 |          17.7283 |
[32m[20221213 20:59:29 @agent_ppo2.py:185][0m |          -0.0034 |          61.8666 |          17.7303 |
[32m[20221213 20:59:30 @agent_ppo2.py:185][0m |          -0.0014 |          61.7201 |          17.7261 |
[32m[20221213 20:59:30 @agent_ppo2.py:185][0m |          -0.0046 |          61.6402 |          17.7204 |
[32m[20221213 20:59:30 @agent_ppo2.py:185][0m |          -0.0051 |          61.7645 |          17.7242 |
[32m[20221213 20:59:30 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:59:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.80
[32m[20221213 20:59:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.00
[32m[20221213 20:59:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.00
[32m[20221213 20:59:30 @agent_ppo2.py:143][0m Total time:       3.92 min
[32m[20221213 20:59:30 @agent_ppo2.py:145][0m 382976 total steps have happened
[32m[20221213 20:59:30 @agent_ppo2.py:121][0m #------------------------ Iteration 187 --------------------------#
[32m[20221213 20:59:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:30 @agent_ppo2.py:185][0m |          -0.0020 |          66.5679 |          17.7093 |
[32m[20221213 20:59:30 @agent_ppo2.py:185][0m |          -0.0050 |          66.1319 |          17.6967 |
[32m[20221213 20:59:30 @agent_ppo2.py:185][0m |          -0.0040 |          66.3655 |          17.6946 |
[32m[20221213 20:59:30 @agent_ppo2.py:185][0m |          -0.0040 |          65.8474 |          17.6981 |
[32m[20221213 20:59:31 @agent_ppo2.py:185][0m |          -0.0030 |          67.3162 |          17.6942 |
[32m[20221213 20:59:31 @agent_ppo2.py:185][0m |           0.0011 |          68.9837 |          17.6855 |
[32m[20221213 20:59:31 @agent_ppo2.py:185][0m |           0.0064 |          71.5159 |          17.6826 |
[32m[20221213 20:59:31 @agent_ppo2.py:185][0m |          -0.0061 |          65.5158 |          17.6808 |
[32m[20221213 20:59:31 @agent_ppo2.py:185][0m |          -0.0075 |          65.4007 |          17.6895 |
[32m[20221213 20:59:31 @agent_ppo2.py:185][0m |          -0.0057 |          65.3042 |          17.6872 |
[32m[20221213 20:59:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 20:59:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.00
[32m[20221213 20:59:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.00
[32m[20221213 20:59:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.00
[32m[20221213 20:59:31 @agent_ppo2.py:143][0m Total time:       3.94 min
[32m[20221213 20:59:31 @agent_ppo2.py:145][0m 385024 total steps have happened
[32m[20221213 20:59:31 @agent_ppo2.py:121][0m #------------------------ Iteration 188 --------------------------#
[32m[20221213 20:59:31 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 20:59:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:31 @agent_ppo2.py:185][0m |          -0.0007 |          66.7054 |          17.6511 |
[32m[20221213 20:59:31 @agent_ppo2.py:185][0m |           0.0015 |          68.1111 |          17.6375 |
[32m[20221213 20:59:32 @agent_ppo2.py:185][0m |          -0.0026 |          66.0399 |          17.6220 |
[32m[20221213 20:59:32 @agent_ppo2.py:185][0m |          -0.0057 |          65.7984 |          17.6287 |
[32m[20221213 20:59:32 @agent_ppo2.py:185][0m |          -0.0056 |          65.6676 |          17.6210 |
[32m[20221213 20:59:32 @agent_ppo2.py:185][0m |           0.0050 |          71.0189 |          17.6222 |
[32m[20221213 20:59:32 @agent_ppo2.py:185][0m |           0.0028 |          69.2066 |          17.6166 |
[32m[20221213 20:59:32 @agent_ppo2.py:185][0m |          -0.0055 |          65.4929 |          17.6202 |
[32m[20221213 20:59:32 @agent_ppo2.py:185][0m |          -0.0056 |          65.4073 |          17.6194 |
[32m[20221213 20:59:32 @agent_ppo2.py:185][0m |          -0.0023 |          65.4455 |          17.6180 |
[32m[20221213 20:59:32 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:59:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.60
[32m[20221213 20:59:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.00
[32m[20221213 20:59:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.00
[32m[20221213 20:59:32 @agent_ppo2.py:143][0m Total time:       3.96 min
[32m[20221213 20:59:32 @agent_ppo2.py:145][0m 387072 total steps have happened
[32m[20221213 20:59:32 @agent_ppo2.py:121][0m #------------------------ Iteration 189 --------------------------#
[32m[20221213 20:59:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:33 @agent_ppo2.py:185][0m |          -0.0019 |          68.6625 |          17.7722 |
[32m[20221213 20:59:33 @agent_ppo2.py:185][0m |          -0.0062 |          68.2723 |          17.7444 |
[32m[20221213 20:59:33 @agent_ppo2.py:185][0m |          -0.0059 |          68.1912 |          17.7400 |
[32m[20221213 20:59:33 @agent_ppo2.py:185][0m |           0.0012 |          69.8099 |          17.7298 |
[32m[20221213 20:59:33 @agent_ppo2.py:185][0m |          -0.0067 |          67.8161 |          17.7396 |
[32m[20221213 20:59:33 @agent_ppo2.py:185][0m |          -0.0062 |          67.7343 |          17.7195 |
[32m[20221213 20:59:33 @agent_ppo2.py:185][0m |          -0.0109 |          67.8076 |          17.7369 |
[32m[20221213 20:59:33 @agent_ppo2.py:185][0m |          -0.0061 |          67.6346 |          17.7338 |
[32m[20221213 20:59:33 @agent_ppo2.py:185][0m |          -0.0095 |          67.5662 |          17.7444 |
[32m[20221213 20:59:33 @agent_ppo2.py:185][0m |          -0.0067 |          67.5673 |          17.7344 |
[32m[20221213 20:59:33 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 20:59:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.40
[32m[20221213 20:59:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.00
[32m[20221213 20:59:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 532.00
[32m[20221213 20:59:34 @agent_ppo2.py:143][0m Total time:       3.98 min
[32m[20221213 20:59:34 @agent_ppo2.py:145][0m 389120 total steps have happened
[32m[20221213 20:59:34 @agent_ppo2.py:121][0m #------------------------ Iteration 190 --------------------------#
[32m[20221213 20:59:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:34 @agent_ppo2.py:185][0m |          -0.0050 |          66.0807 |          17.6265 |
[32m[20221213 20:59:34 @agent_ppo2.py:185][0m |          -0.0058 |          65.4738 |          17.6041 |
[32m[20221213 20:59:34 @agent_ppo2.py:185][0m |          -0.0049 |          64.9252 |          17.5969 |
[32m[20221213 20:59:34 @agent_ppo2.py:185][0m |          -0.0072 |          64.6182 |          17.5936 |
[32m[20221213 20:59:34 @agent_ppo2.py:185][0m |          -0.0010 |          65.7240 |          17.5941 |
[32m[20221213 20:59:34 @agent_ppo2.py:185][0m |          -0.0058 |          64.3530 |          17.5910 |
[32m[20221213 20:59:35 @agent_ppo2.py:185][0m |          -0.0076 |          64.0762 |          17.5913 |
[32m[20221213 20:59:35 @agent_ppo2.py:185][0m |          -0.0071 |          64.1013 |          17.5915 |
[32m[20221213 20:59:35 @agent_ppo2.py:185][0m |           0.0009 |          65.4474 |          17.5956 |
[32m[20221213 20:59:35 @agent_ppo2.py:185][0m |          -0.0063 |          63.7110 |          17.5914 |
[32m[20221213 20:59:35 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221213 20:59:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.00
[32m[20221213 20:59:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.00
[32m[20221213 20:59:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.00
[32m[20221213 20:59:35 @agent_ppo2.py:143][0m Total time:       4.01 min
[32m[20221213 20:59:35 @agent_ppo2.py:145][0m 391168 total steps have happened
[32m[20221213 20:59:35 @agent_ppo2.py:121][0m #------------------------ Iteration 191 --------------------------#
[32m[20221213 20:59:35 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:59:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:35 @agent_ppo2.py:185][0m |          -0.0037 |          65.0083 |          17.5694 |
[32m[20221213 20:59:35 @agent_ppo2.py:185][0m |          -0.0030 |          64.6168 |          17.5478 |
[32m[20221213 20:59:36 @agent_ppo2.py:185][0m |          -0.0052 |          64.4728 |          17.5512 |
[32m[20221213 20:59:36 @agent_ppo2.py:185][0m |          -0.0031 |          64.2781 |          17.5403 |
[32m[20221213 20:59:36 @agent_ppo2.py:185][0m |          -0.0063 |          64.2423 |          17.5405 |
[32m[20221213 20:59:36 @agent_ppo2.py:185][0m |          -0.0069 |          64.2403 |          17.5368 |
[32m[20221213 20:59:36 @agent_ppo2.py:185][0m |          -0.0066 |          64.1393 |          17.5437 |
[32m[20221213 20:59:36 @agent_ppo2.py:185][0m |          -0.0062 |          64.1297 |          17.5384 |
[32m[20221213 20:59:36 @agent_ppo2.py:185][0m |          -0.0065 |          64.0036 |          17.5308 |
[32m[20221213 20:59:36 @agent_ppo2.py:185][0m |           0.0151 |          72.0977 |          17.5338 |
[32m[20221213 20:59:36 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 20:59:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.60
[32m[20221213 20:59:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.00
[32m[20221213 20:59:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 553.00
[32m[20221213 20:59:36 @agent_ppo2.py:143][0m Total time:       4.03 min
[32m[20221213 20:59:36 @agent_ppo2.py:145][0m 393216 total steps have happened
[32m[20221213 20:59:36 @agent_ppo2.py:121][0m #------------------------ Iteration 192 --------------------------#
[32m[20221213 20:59:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 20:59:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:37 @agent_ppo2.py:185][0m |          -0.0021 |          65.2962 |          17.6702 |
[32m[20221213 20:59:37 @agent_ppo2.py:185][0m |          -0.0064 |          64.5967 |          17.6506 |
[32m[20221213 20:59:37 @agent_ppo2.py:185][0m |           0.0001 |          64.9616 |          17.6396 |
[32m[20221213 20:59:37 @agent_ppo2.py:185][0m |          -0.0022 |          64.1448 |          17.6414 |
[32m[20221213 20:59:37 @agent_ppo2.py:185][0m |          -0.0081 |          64.0770 |          17.6269 |
[32m[20221213 20:59:37 @agent_ppo2.py:185][0m |          -0.0008 |          65.9739 |          17.6387 |
[32m[20221213 20:59:37 @agent_ppo2.py:185][0m |          -0.0021 |          64.5606 |          17.6258 |
[32m[20221213 20:59:37 @agent_ppo2.py:185][0m |          -0.0075 |          63.6091 |          17.6355 |
[32m[20221213 20:59:37 @agent_ppo2.py:185][0m |           0.0008 |          67.7714 |          17.6251 |
[32m[20221213 20:59:37 @agent_ppo2.py:185][0m |          -0.0090 |          63.5522 |          17.6265 |
[32m[20221213 20:59:37 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:59:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.60
[32m[20221213 20:59:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.00
[32m[20221213 20:59:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.00
[32m[20221213 20:59:38 @agent_ppo2.py:143][0m Total time:       4.05 min
[32m[20221213 20:59:38 @agent_ppo2.py:145][0m 395264 total steps have happened
[32m[20221213 20:59:38 @agent_ppo2.py:121][0m #------------------------ Iteration 193 --------------------------#
[32m[20221213 20:59:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:38 @agent_ppo2.py:185][0m |          -0.0006 |          66.3348 |          17.6310 |
[32m[20221213 20:59:38 @agent_ppo2.py:185][0m |          -0.0034 |          65.7316 |          17.6144 |
[32m[20221213 20:59:38 @agent_ppo2.py:185][0m |          -0.0018 |          65.4808 |          17.6095 |
[32m[20221213 20:59:38 @agent_ppo2.py:185][0m |          -0.0045 |          65.1931 |          17.6013 |
[32m[20221213 20:59:38 @agent_ppo2.py:185][0m |          -0.0070 |          65.0861 |          17.6055 |
[32m[20221213 20:59:38 @agent_ppo2.py:185][0m |          -0.0069 |          64.9856 |          17.6005 |
[32m[20221213 20:59:38 @agent_ppo2.py:185][0m |          -0.0037 |          65.4164 |          17.5994 |
[32m[20221213 20:59:39 @agent_ppo2.py:185][0m |          -0.0054 |          64.8358 |          17.5950 |
[32m[20221213 20:59:39 @agent_ppo2.py:185][0m |          -0.0081 |          64.6972 |          17.5871 |
[32m[20221213 20:59:39 @agent_ppo2.py:185][0m |          -0.0055 |          64.6411 |          17.5894 |
[32m[20221213 20:59:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:59:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.60
[32m[20221213 20:59:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.00
[32m[20221213 20:59:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.00
[32m[20221213 20:59:39 @agent_ppo2.py:143][0m Total time:       4.07 min
[32m[20221213 20:59:39 @agent_ppo2.py:145][0m 397312 total steps have happened
[32m[20221213 20:59:39 @agent_ppo2.py:121][0m #------------------------ Iteration 194 --------------------------#
[32m[20221213 20:59:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:39 @agent_ppo2.py:185][0m |          -0.0008 |          70.0257 |          17.5231 |
[32m[20221213 20:59:39 @agent_ppo2.py:185][0m |           0.0055 |          75.4799 |          17.5103 |
[32m[20221213 20:59:39 @agent_ppo2.py:185][0m |           0.0027 |          70.6996 |          17.5118 |
[32m[20221213 20:59:39 @agent_ppo2.py:185][0m |          -0.0078 |          68.0700 |          17.5108 |
[32m[20221213 20:59:39 @agent_ppo2.py:185][0m |          -0.0044 |          67.8922 |          17.5091 |
[32m[20221213 20:59:40 @agent_ppo2.py:185][0m |          -0.0052 |          67.7604 |          17.5064 |
[32m[20221213 20:59:40 @agent_ppo2.py:185][0m |          -0.0052 |          67.8385 |          17.5061 |
[32m[20221213 20:59:40 @agent_ppo2.py:185][0m |          -0.0061 |          67.5735 |          17.5023 |
[32m[20221213 20:59:40 @agent_ppo2.py:185][0m |          -0.0072 |          67.5778 |          17.5067 |
[32m[20221213 20:59:40 @agent_ppo2.py:185][0m |          -0.0085 |          67.3929 |          17.5087 |
[32m[20221213 20:59:40 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 20:59:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.00
[32m[20221213 20:59:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.00
[32m[20221213 20:59:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.00
[32m[20221213 20:59:40 @agent_ppo2.py:143][0m Total time:       4.09 min
[32m[20221213 20:59:40 @agent_ppo2.py:145][0m 399360 total steps have happened
[32m[20221213 20:59:40 @agent_ppo2.py:121][0m #------------------------ Iteration 195 --------------------------#
[32m[20221213 20:59:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 20:59:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:40 @agent_ppo2.py:185][0m |           0.0050 |          69.6587 |          17.6720 |
[32m[20221213 20:59:40 @agent_ppo2.py:185][0m |          -0.0061 |          66.3830 |          17.6403 |
[32m[20221213 20:59:41 @agent_ppo2.py:185][0m |          -0.0017 |          65.9671 |          17.6459 |
[32m[20221213 20:59:41 @agent_ppo2.py:185][0m |          -0.0016 |          65.9705 |          17.6371 |
[32m[20221213 20:59:41 @agent_ppo2.py:185][0m |          -0.0047 |          65.6130 |          17.6435 |
[32m[20221213 20:59:41 @agent_ppo2.py:185][0m |          -0.0040 |          65.3361 |          17.6358 |
[32m[20221213 20:59:41 @agent_ppo2.py:185][0m |          -0.0050 |          65.1731 |          17.6341 |
[32m[20221213 20:59:41 @agent_ppo2.py:185][0m |          -0.0068 |          65.0514 |          17.6390 |
[32m[20221213 20:59:41 @agent_ppo2.py:185][0m |           0.0029 |          68.0760 |          17.6320 |
[32m[20221213 20:59:41 @agent_ppo2.py:185][0m |          -0.0022 |          65.2935 |          17.6171 |
[32m[20221213 20:59:41 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:59:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.80
[32m[20221213 20:59:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.00
[32m[20221213 20:59:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.00
[32m[20221213 20:59:41 @agent_ppo2.py:143][0m Total time:       4.11 min
[32m[20221213 20:59:41 @agent_ppo2.py:145][0m 401408 total steps have happened
[32m[20221213 20:59:41 @agent_ppo2.py:121][0m #------------------------ Iteration 196 --------------------------#
[32m[20221213 20:59:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:42 @agent_ppo2.py:185][0m |          -0.0027 |          66.3800 |          17.5725 |
[32m[20221213 20:59:42 @agent_ppo2.py:185][0m |           0.0040 |          68.6865 |          17.5527 |
[32m[20221213 20:59:42 @agent_ppo2.py:185][0m |          -0.0034 |          65.5941 |          17.5543 |
[32m[20221213 20:59:42 @agent_ppo2.py:185][0m |          -0.0044 |          65.4983 |          17.5409 |
[32m[20221213 20:59:42 @agent_ppo2.py:185][0m |          -0.0029 |          67.2946 |          17.5415 |
[32m[20221213 20:59:42 @agent_ppo2.py:185][0m |           0.0003 |          72.0678 |          17.5410 |
[32m[20221213 20:59:42 @agent_ppo2.py:185][0m |          -0.0048 |          65.5010 |          17.5321 |
[32m[20221213 20:59:42 @agent_ppo2.py:185][0m |          -0.0061 |          65.0430 |          17.5353 |
[32m[20221213 20:59:42 @agent_ppo2.py:185][0m |          -0.0072 |          64.8299 |          17.5380 |
[32m[20221213 20:59:42 @agent_ppo2.py:185][0m |          -0.0075 |          64.9002 |          17.5328 |
[32m[20221213 20:59:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 20:59:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.40
[32m[20221213 20:59:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.00
[32m[20221213 20:59:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.00
[32m[20221213 20:59:43 @agent_ppo2.py:143][0m Total time:       4.13 min
[32m[20221213 20:59:43 @agent_ppo2.py:145][0m 403456 total steps have happened
[32m[20221213 20:59:43 @agent_ppo2.py:121][0m #------------------------ Iteration 197 --------------------------#
[32m[20221213 20:59:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:43 @agent_ppo2.py:185][0m |           0.0017 |          66.6375 |          17.5792 |
[32m[20221213 20:59:43 @agent_ppo2.py:185][0m |          -0.0003 |          68.4635 |          17.5679 |
[32m[20221213 20:59:43 @agent_ppo2.py:185][0m |          -0.0083 |          65.5253 |          17.5596 |
[32m[20221213 20:59:43 @agent_ppo2.py:185][0m |          -0.0018 |          65.6768 |          17.5560 |
[32m[20221213 20:59:43 @agent_ppo2.py:185][0m |          -0.0073 |          65.2820 |          17.5507 |
[32m[20221213 20:59:44 @agent_ppo2.py:185][0m |          -0.0070 |          65.2173 |          17.5560 |
[32m[20221213 20:59:44 @agent_ppo2.py:185][0m |          -0.0077 |          65.0848 |          17.5538 |
[32m[20221213 20:59:44 @agent_ppo2.py:185][0m |          -0.0095 |          65.0236 |          17.5496 |
[32m[20221213 20:59:44 @agent_ppo2.py:185][0m |          -0.0083 |          64.9421 |          17.5490 |
[32m[20221213 20:59:44 @agent_ppo2.py:185][0m |          -0.0042 |          65.2611 |          17.5525 |
[32m[20221213 20:59:44 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221213 20:59:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.60
[32m[20221213 20:59:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.00
[32m[20221213 20:59:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.00
[32m[20221213 20:59:44 @agent_ppo2.py:143][0m Total time:       4.16 min
[32m[20221213 20:59:44 @agent_ppo2.py:145][0m 405504 total steps have happened
[32m[20221213 20:59:44 @agent_ppo2.py:121][0m #------------------------ Iteration 198 --------------------------#
[32m[20221213 20:59:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:44 @agent_ppo2.py:185][0m |          -0.0038 |          65.8067 |          17.7285 |
[32m[20221213 20:59:45 @agent_ppo2.py:185][0m |          -0.0024 |          65.0999 |          17.7119 |
[32m[20221213 20:59:45 @agent_ppo2.py:185][0m |           0.0025 |          67.5571 |          17.7122 |
[32m[20221213 20:59:45 @agent_ppo2.py:185][0m |          -0.0069 |          64.5876 |          17.7116 |
[32m[20221213 20:59:45 @agent_ppo2.py:185][0m |          -0.0045 |          64.4325 |          17.7077 |
[32m[20221213 20:59:45 @agent_ppo2.py:185][0m |          -0.0075 |          64.3727 |          17.7116 |
[32m[20221213 20:59:45 @agent_ppo2.py:185][0m |          -0.0069 |          64.3072 |          17.7002 |
[32m[20221213 20:59:45 @agent_ppo2.py:185][0m |          -0.0063 |          64.2449 |          17.7094 |
[32m[20221213 20:59:45 @agent_ppo2.py:185][0m |          -0.0041 |          64.8293 |          17.7124 |
[32m[20221213 20:59:45 @agent_ppo2.py:185][0m |          -0.0069 |          63.9745 |          17.7063 |
[32m[20221213 20:59:45 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 20:59:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 449.80
[32m[20221213 20:59:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.00
[32m[20221213 20:59:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.00
[32m[20221213 20:59:46 @agent_ppo2.py:143][0m Total time:       4.18 min
[32m[20221213 20:59:46 @agent_ppo2.py:145][0m 407552 total steps have happened
[32m[20221213 20:59:46 @agent_ppo2.py:121][0m #------------------------ Iteration 199 --------------------------#
[32m[20221213 20:59:46 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 20:59:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:46 @agent_ppo2.py:185][0m |           0.0005 |          65.1542 |          17.8112 |
[32m[20221213 20:59:46 @agent_ppo2.py:185][0m |          -0.0018 |          64.4930 |          17.8077 |
[32m[20221213 20:59:46 @agent_ppo2.py:185][0m |          -0.0051 |          64.3530 |          17.7998 |
[32m[20221213 20:59:46 @agent_ppo2.py:185][0m |          -0.0026 |          63.9920 |          17.8071 |
[32m[20221213 20:59:46 @agent_ppo2.py:185][0m |           0.0005 |          65.0438 |          17.7987 |
[32m[20221213 20:59:46 @agent_ppo2.py:185][0m |          -0.0034 |          63.7347 |          17.7981 |
[32m[20221213 20:59:46 @agent_ppo2.py:185][0m |          -0.0046 |          63.5628 |          17.7975 |
[32m[20221213 20:59:46 @agent_ppo2.py:185][0m |          -0.0016 |          64.5112 |          17.7941 |
[32m[20221213 20:59:47 @agent_ppo2.py:185][0m |          -0.0055 |          63.4353 |          17.7998 |
[32m[20221213 20:59:47 @agent_ppo2.py:185][0m |          -0.0031 |          63.2985 |          17.7998 |
[32m[20221213 20:59:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 20:59:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.60
[32m[20221213 20:59:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.00
[32m[20221213 20:59:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.00
[32m[20221213 20:59:47 @agent_ppo2.py:143][0m Total time:       4.20 min
[32m[20221213 20:59:47 @agent_ppo2.py:145][0m 409600 total steps have happened
[32m[20221213 20:59:47 @agent_ppo2.py:121][0m #------------------------ Iteration 200 --------------------------#
[32m[20221213 20:59:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:59:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:47 @agent_ppo2.py:185][0m |           0.0129 |          74.4195 |          17.7739 |
[32m[20221213 20:59:47 @agent_ppo2.py:185][0m |          -0.0029 |          65.2850 |          17.7438 |
[32m[20221213 20:59:47 @agent_ppo2.py:185][0m |           0.0029 |          65.3418 |          17.7535 |
[32m[20221213 20:59:47 @agent_ppo2.py:185][0m |          -0.0035 |          64.6260 |          17.7558 |
[32m[20221213 20:59:48 @agent_ppo2.py:185][0m |          -0.0052 |          64.5644 |          17.7551 |
[32m[20221213 20:59:48 @agent_ppo2.py:185][0m |          -0.0041 |          64.3153 |          17.7464 |
[32m[20221213 20:59:48 @agent_ppo2.py:185][0m |          -0.0043 |          64.1819 |          17.7503 |
[32m[20221213 20:59:48 @agent_ppo2.py:185][0m |          -0.0054 |          64.1342 |          17.7513 |
[32m[20221213 20:59:48 @agent_ppo2.py:185][0m |          -0.0075 |          64.0949 |          17.7542 |
[32m[20221213 20:59:48 @agent_ppo2.py:185][0m |          -0.0052 |          63.9740 |          17.7445 |
[32m[20221213 20:59:48 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 20:59:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.60
[32m[20221213 20:59:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 541.00
[32m[20221213 20:59:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.00
[32m[20221213 20:59:48 @agent_ppo2.py:143][0m Total time:       4.23 min
[32m[20221213 20:59:48 @agent_ppo2.py:145][0m 411648 total steps have happened
[32m[20221213 20:59:48 @agent_ppo2.py:121][0m #------------------------ Iteration 201 --------------------------#
[32m[20221213 20:59:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:49 @agent_ppo2.py:185][0m |          -0.0004 |          65.8191 |          17.7809 |
[32m[20221213 20:59:49 @agent_ppo2.py:185][0m |          -0.0035 |          64.8564 |          17.7604 |
[32m[20221213 20:59:49 @agent_ppo2.py:185][0m |          -0.0040 |          64.4280 |          17.7585 |
[32m[20221213 20:59:49 @agent_ppo2.py:185][0m |          -0.0045 |          64.3343 |          17.7537 |
[32m[20221213 20:59:49 @agent_ppo2.py:185][0m |          -0.0044 |          64.0935 |          17.7484 |
[32m[20221213 20:59:49 @agent_ppo2.py:185][0m |          -0.0063 |          63.9960 |          17.7487 |
[32m[20221213 20:59:49 @agent_ppo2.py:185][0m |          -0.0065 |          63.8680 |          17.7497 |
[32m[20221213 20:59:49 @agent_ppo2.py:185][0m |          -0.0076 |          64.2200 |          17.7439 |
[32m[20221213 20:59:49 @agent_ppo2.py:185][0m |          -0.0083 |          63.6886 |          17.7490 |
[32m[20221213 20:59:49 @agent_ppo2.py:185][0m |          -0.0089 |          63.6644 |          17.7512 |
[32m[20221213 20:59:49 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 20:59:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.80
[32m[20221213 20:59:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.00
[32m[20221213 20:59:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.00
[32m[20221213 20:59:49 @agent_ppo2.py:143][0m Total time:       4.25 min
[32m[20221213 20:59:49 @agent_ppo2.py:145][0m 413696 total steps have happened
[32m[20221213 20:59:49 @agent_ppo2.py:121][0m #------------------------ Iteration 202 --------------------------#
[32m[20221213 20:59:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:50 @agent_ppo2.py:185][0m |           0.0053 |          61.5397 |          17.8470 |
[32m[20221213 20:59:50 @agent_ppo2.py:185][0m |          -0.0041 |          60.6838 |          17.8260 |
[32m[20221213 20:59:50 @agent_ppo2.py:185][0m |           0.0080 |          63.2258 |          17.8234 |
[32m[20221213 20:59:50 @agent_ppo2.py:185][0m |          -0.0007 |          60.9323 |          17.8360 |
[32m[20221213 20:59:50 @agent_ppo2.py:185][0m |          -0.0073 |          60.3112 |          17.8205 |
[32m[20221213 20:59:50 @agent_ppo2.py:185][0m |          -0.0029 |          60.1054 |          17.8233 |
[32m[20221213 20:59:50 @agent_ppo2.py:185][0m |          -0.0079 |          60.0647 |          17.8184 |
[32m[20221213 20:59:50 @agent_ppo2.py:185][0m |          -0.0036 |          60.2334 |          17.8177 |
[32m[20221213 20:59:50 @agent_ppo2.py:185][0m |          -0.0044 |          59.9118 |          17.8292 |
[32m[20221213 20:59:51 @agent_ppo2.py:185][0m |          -0.0054 |          59.8365 |          17.8310 |
[32m[20221213 20:59:51 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:59:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.80
[32m[20221213 20:59:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.00
[32m[20221213 20:59:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.00
[32m[20221213 20:59:51 @agent_ppo2.py:143][0m Total time:       4.27 min
[32m[20221213 20:59:51 @agent_ppo2.py:145][0m 415744 total steps have happened
[32m[20221213 20:59:51 @agent_ppo2.py:121][0m #------------------------ Iteration 203 --------------------------#
[32m[20221213 20:59:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:51 @agent_ppo2.py:185][0m |          -0.0019 |          65.6186 |          17.6799 |
[32m[20221213 20:59:51 @agent_ppo2.py:185][0m |          -0.0055 |          64.9711 |          17.6581 |
[32m[20221213 20:59:51 @agent_ppo2.py:185][0m |          -0.0085 |          64.8379 |          17.6526 |
[32m[20221213 20:59:51 @agent_ppo2.py:185][0m |          -0.0077 |          64.7262 |          17.6488 |
[32m[20221213 20:59:51 @agent_ppo2.py:185][0m |          -0.0073 |          64.5694 |          17.6425 |
[32m[20221213 20:59:51 @agent_ppo2.py:185][0m |          -0.0055 |          64.3363 |          17.6424 |
[32m[20221213 20:59:52 @agent_ppo2.py:185][0m |          -0.0063 |          64.3089 |          17.6456 |
[32m[20221213 20:59:52 @agent_ppo2.py:185][0m |          -0.0058 |          64.1735 |          17.6429 |
[32m[20221213 20:59:52 @agent_ppo2.py:185][0m |          -0.0020 |          64.3681 |          17.6409 |
[32m[20221213 20:59:52 @agent_ppo2.py:185][0m |          -0.0099 |          64.0286 |          17.6410 |
[32m[20221213 20:59:52 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 20:59:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.00
[32m[20221213 20:59:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.00
[32m[20221213 20:59:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.00
[32m[20221213 20:59:52 @agent_ppo2.py:143][0m Total time:       4.29 min
[32m[20221213 20:59:52 @agent_ppo2.py:145][0m 417792 total steps have happened
[32m[20221213 20:59:52 @agent_ppo2.py:121][0m #------------------------ Iteration 204 --------------------------#
[32m[20221213 20:59:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:52 @agent_ppo2.py:185][0m |          -0.0022 |          65.5706 |          17.7053 |
[32m[20221213 20:59:52 @agent_ppo2.py:185][0m |          -0.0074 |          65.1025 |          17.6855 |
[32m[20221213 20:59:52 @agent_ppo2.py:185][0m |          -0.0055 |          64.7922 |          17.6890 |
[32m[20221213 20:59:52 @agent_ppo2.py:185][0m |          -0.0014 |          67.9917 |          17.6884 |
[32m[20221213 20:59:53 @agent_ppo2.py:185][0m |          -0.0074 |          64.6172 |          17.6843 |
[32m[20221213 20:59:53 @agent_ppo2.py:185][0m |          -0.0096 |          64.4783 |          17.6848 |
[32m[20221213 20:59:53 @agent_ppo2.py:185][0m |          -0.0068 |          64.4213 |          17.6860 |
[32m[20221213 20:59:53 @agent_ppo2.py:185][0m |          -0.0091 |          64.2787 |          17.6808 |
[32m[20221213 20:59:53 @agent_ppo2.py:185][0m |           0.0042 |          68.8677 |          17.6792 |
[32m[20221213 20:59:53 @agent_ppo2.py:185][0m |          -0.0057 |          64.5196 |          17.6906 |
[32m[20221213 20:59:53 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:59:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.80
[32m[20221213 20:59:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.00
[32m[20221213 20:59:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.00
[32m[20221213 20:59:53 @agent_ppo2.py:143][0m Total time:       4.31 min
[32m[20221213 20:59:53 @agent_ppo2.py:145][0m 419840 total steps have happened
[32m[20221213 20:59:53 @agent_ppo2.py:121][0m #------------------------ Iteration 205 --------------------------#
[32m[20221213 20:59:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 20:59:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:53 @agent_ppo2.py:185][0m |          -0.0038 |          64.0748 |          17.7394 |
[32m[20221213 20:59:54 @agent_ppo2.py:185][0m |          -0.0055 |          63.6457 |          17.7246 |
[32m[20221213 20:59:54 @agent_ppo2.py:185][0m |          -0.0010 |          63.5999 |          17.7235 |
[32m[20221213 20:59:54 @agent_ppo2.py:185][0m |          -0.0064 |          63.4249 |          17.7165 |
[32m[20221213 20:59:54 @agent_ppo2.py:185][0m |          -0.0061 |          63.3021 |          17.7118 |
[32m[20221213 20:59:54 @agent_ppo2.py:185][0m |          -0.0072 |          63.2576 |          17.7193 |
[32m[20221213 20:59:54 @agent_ppo2.py:185][0m |          -0.0037 |          63.2497 |          17.7167 |
[32m[20221213 20:59:54 @agent_ppo2.py:185][0m |          -0.0054 |          63.1355 |          17.7114 |
[32m[20221213 20:59:54 @agent_ppo2.py:185][0m |          -0.0085 |          63.1148 |          17.7230 |
[32m[20221213 20:59:54 @agent_ppo2.py:185][0m |          -0.0076 |          62.9981 |          17.7164 |
[32m[20221213 20:59:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 20:59:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.60
[32m[20221213 20:59:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.00
[32m[20221213 20:59:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.00
[32m[20221213 20:59:54 @agent_ppo2.py:143][0m Total time:       4.33 min
[32m[20221213 20:59:54 @agent_ppo2.py:145][0m 421888 total steps have happened
[32m[20221213 20:59:54 @agent_ppo2.py:121][0m #------------------------ Iteration 206 --------------------------#
[32m[20221213 20:59:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:55 @agent_ppo2.py:185][0m |           0.0011 |          63.0884 |          17.8635 |
[32m[20221213 20:59:55 @agent_ppo2.py:185][0m |          -0.0050 |          62.5594 |          17.8439 |
[32m[20221213 20:59:55 @agent_ppo2.py:185][0m |          -0.0049 |          62.4530 |          17.8379 |
[32m[20221213 20:59:55 @agent_ppo2.py:185][0m |          -0.0020 |          63.0875 |          17.8310 |
[32m[20221213 20:59:55 @agent_ppo2.py:185][0m |          -0.0043 |          62.1530 |          17.8318 |
[32m[20221213 20:59:55 @agent_ppo2.py:185][0m |          -0.0066 |          62.0308 |          17.8354 |
[32m[20221213 20:59:55 @agent_ppo2.py:185][0m |          -0.0056 |          62.0027 |          17.8349 |
[32m[20221213 20:59:55 @agent_ppo2.py:185][0m |          -0.0073 |          61.8927 |          17.8265 |
[32m[20221213 20:59:55 @agent_ppo2.py:185][0m |          -0.0025 |          63.1769 |          17.8334 |
[32m[20221213 20:59:55 @agent_ppo2.py:185][0m |           0.0044 |          64.4488 |          17.8230 |
[32m[20221213 20:59:55 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 20:59:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.20
[32m[20221213 20:59:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.00
[32m[20221213 20:59:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.00
[32m[20221213 20:59:56 @agent_ppo2.py:143][0m Total time:       4.35 min
[32m[20221213 20:59:56 @agent_ppo2.py:145][0m 423936 total steps have happened
[32m[20221213 20:59:56 @agent_ppo2.py:121][0m #------------------------ Iteration 207 --------------------------#
[32m[20221213 20:59:56 @agent_ppo2.py:127][0m Sampling time: 0.40 s by 5 slaves
[32m[20221213 20:59:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:56 @agent_ppo2.py:185][0m |           0.0002 |          62.1151 |          17.8896 |
[32m[20221213 20:59:56 @agent_ppo2.py:185][0m |          -0.0026 |          62.0064 |          17.8735 |
[32m[20221213 20:59:56 @agent_ppo2.py:185][0m |          -0.0000 |          61.7280 |          17.8666 |
[32m[20221213 20:59:57 @agent_ppo2.py:185][0m |          -0.0046 |          61.6509 |          17.8742 |
[32m[20221213 20:59:57 @agent_ppo2.py:185][0m |          -0.0030 |          61.7580 |          17.8669 |
[32m[20221213 20:59:57 @agent_ppo2.py:185][0m |          -0.0050 |          61.2974 |          17.8644 |
[32m[20221213 20:59:57 @agent_ppo2.py:185][0m |          -0.0027 |          61.3327 |          17.8739 |
[32m[20221213 20:59:57 @agent_ppo2.py:185][0m |          -0.0047 |          61.0666 |          17.8577 |
[32m[20221213 20:59:57 @agent_ppo2.py:185][0m |          -0.0058 |          61.1261 |          17.8613 |
[32m[20221213 20:59:57 @agent_ppo2.py:185][0m |           0.0055 |          67.1192 |          17.8614 |
[32m[20221213 20:59:57 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 20:59:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.20
[32m[20221213 20:59:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.00
[32m[20221213 20:59:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.00
[32m[20221213 20:59:57 @agent_ppo2.py:143][0m Total time:       4.38 min
[32m[20221213 20:59:57 @agent_ppo2.py:145][0m 425984 total steps have happened
[32m[20221213 20:59:57 @agent_ppo2.py:121][0m #------------------------ Iteration 208 --------------------------#
[32m[20221213 20:59:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 20:59:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:58 @agent_ppo2.py:185][0m |          -0.0021 |          63.6836 |          17.7678 |
[32m[20221213 20:59:58 @agent_ppo2.py:185][0m |          -0.0009 |          64.1571 |          17.7660 |
[32m[20221213 20:59:58 @agent_ppo2.py:185][0m |          -0.0052 |          63.0292 |          17.7637 |
[32m[20221213 20:59:58 @agent_ppo2.py:185][0m |          -0.0052 |          62.8389 |          17.7671 |
[32m[20221213 20:59:58 @agent_ppo2.py:185][0m |          -0.0087 |          62.8454 |          17.7583 |
[32m[20221213 20:59:58 @agent_ppo2.py:185][0m |          -0.0083 |          62.6412 |          17.7558 |
[32m[20221213 20:59:58 @agent_ppo2.py:185][0m |          -0.0070 |          62.6613 |          17.7632 |
[32m[20221213 20:59:58 @agent_ppo2.py:185][0m |          -0.0060 |          62.4662 |          17.7538 |
[32m[20221213 20:59:58 @agent_ppo2.py:185][0m |          -0.0086 |          62.5248 |          17.7522 |
[32m[20221213 20:59:58 @agent_ppo2.py:185][0m |          -0.0051 |          62.4417 |          17.7600 |
[32m[20221213 20:59:58 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 20:59:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.20
[32m[20221213 20:59:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.00
[32m[20221213 20:59:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 20:59:59 @agent_ppo2.py:143][0m Total time:       4.40 min
[32m[20221213 20:59:59 @agent_ppo2.py:145][0m 428032 total steps have happened
[32m[20221213 20:59:59 @agent_ppo2.py:121][0m #------------------------ Iteration 209 --------------------------#
[32m[20221213 20:59:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 20:59:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 20:59:59 @agent_ppo2.py:185][0m |          -0.0003 |          63.3560 |          18.0148 |
[32m[20221213 20:59:59 @agent_ppo2.py:185][0m |           0.0014 |          66.5151 |          18.0019 |
[32m[20221213 20:59:59 @agent_ppo2.py:185][0m |          -0.0046 |          62.9141 |          17.9751 |
[32m[20221213 20:59:59 @agent_ppo2.py:185][0m |          -0.0050 |          62.5895 |          17.9833 |
[32m[20221213 20:59:59 @agent_ppo2.py:185][0m |           0.0015 |          69.7866 |          17.9821 |
[32m[20221213 20:59:59 @agent_ppo2.py:185][0m |          -0.0073 |          62.4723 |          17.9879 |
[32m[20221213 21:00:00 @agent_ppo2.py:185][0m |          -0.0085 |          62.2480 |          17.9761 |
[32m[20221213 21:00:00 @agent_ppo2.py:185][0m |          -0.0084 |          62.2307 |          17.9856 |
[32m[20221213 21:00:00 @agent_ppo2.py:185][0m |          -0.0089 |          62.0824 |          17.9865 |
[32m[20221213 21:00:00 @agent_ppo2.py:185][0m |          -0.0101 |          62.1754 |          17.9818 |
[32m[20221213 21:00:00 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 21:00:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.00
[32m[20221213 21:00:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.00
[32m[20221213 21:00:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 520.00
[32m[20221213 21:00:00 @agent_ppo2.py:143][0m Total time:       4.42 min
[32m[20221213 21:00:00 @agent_ppo2.py:145][0m 430080 total steps have happened
[32m[20221213 21:00:00 @agent_ppo2.py:121][0m #------------------------ Iteration 210 --------------------------#
[32m[20221213 21:00:00 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:00:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:00 @agent_ppo2.py:185][0m |          -0.0026 |          63.0516 |          17.8351 |
[32m[20221213 21:00:00 @agent_ppo2.py:185][0m |          -0.0039 |          62.4152 |          17.8130 |
[32m[20221213 21:00:01 @agent_ppo2.py:185][0m |          -0.0006 |          62.4053 |          17.8141 |
[32m[20221213 21:00:01 @agent_ppo2.py:185][0m |          -0.0029 |          61.9426 |          17.8096 |
[32m[20221213 21:00:01 @agent_ppo2.py:185][0m |          -0.0059 |          61.8489 |          17.8078 |
[32m[20221213 21:00:01 @agent_ppo2.py:185][0m |          -0.0072 |          61.7832 |          17.8020 |
[32m[20221213 21:00:01 @agent_ppo2.py:185][0m |          -0.0117 |          61.6241 |          17.8037 |
[32m[20221213 21:00:01 @agent_ppo2.py:185][0m |           0.0081 |          69.5733 |          17.8020 |
[32m[20221213 21:00:01 @agent_ppo2.py:185][0m |          -0.0081 |          61.6318 |          17.7972 |
[32m[20221213 21:00:01 @agent_ppo2.py:185][0m |          -0.0045 |          61.6074 |          17.7972 |
[32m[20221213 21:00:01 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:00:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.60
[32m[20221213 21:00:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.00
[32m[20221213 21:00:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.00
[32m[20221213 21:00:01 @agent_ppo2.py:143][0m Total time:       4.44 min
[32m[20221213 21:00:01 @agent_ppo2.py:145][0m 432128 total steps have happened
[32m[20221213 21:00:01 @agent_ppo2.py:121][0m #------------------------ Iteration 211 --------------------------#
[32m[20221213 21:00:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:02 @agent_ppo2.py:185][0m |           0.0038 |          65.7184 |          18.0010 |
[32m[20221213 21:00:02 @agent_ppo2.py:185][0m |           0.0084 |          70.4340 |          17.9814 |
[32m[20221213 21:00:02 @agent_ppo2.py:185][0m |           0.0016 |          65.3290 |          17.9732 |
[32m[20221213 21:00:02 @agent_ppo2.py:185][0m |          -0.0040 |          63.7900 |          17.9811 |
[32m[20221213 21:00:02 @agent_ppo2.py:185][0m |          -0.0090 |          63.8703 |          17.9698 |
[32m[20221213 21:00:02 @agent_ppo2.py:185][0m |          -0.0041 |          63.5479 |          17.9730 |
[32m[20221213 21:00:02 @agent_ppo2.py:185][0m |          -0.0027 |          63.5329 |          17.9811 |
[32m[20221213 21:00:02 @agent_ppo2.py:185][0m |          -0.0048 |          63.8381 |          17.9671 |
[32m[20221213 21:00:02 @agent_ppo2.py:185][0m |          -0.0056 |          63.6578 |          17.9744 |
[32m[20221213 21:00:03 @agent_ppo2.py:185][0m |          -0.0077 |          63.3259 |          17.9716 |
[32m[20221213 21:00:03 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:00:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 449.00
[32m[20221213 21:00:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.00
[32m[20221213 21:00:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.00
[32m[20221213 21:00:03 @agent_ppo2.py:143][0m Total time:       4.47 min
[32m[20221213 21:00:03 @agent_ppo2.py:145][0m 434176 total steps have happened
[32m[20221213 21:00:03 @agent_ppo2.py:121][0m #------------------------ Iteration 212 --------------------------#
[32m[20221213 21:00:03 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:00:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:03 @agent_ppo2.py:185][0m |           0.0113 |          69.6908 |          17.8512 |
[32m[20221213 21:00:03 @agent_ppo2.py:185][0m |          -0.0061 |          63.9167 |          17.8192 |
[32m[20221213 21:00:03 @agent_ppo2.py:185][0m |          -0.0036 |          63.5163 |          17.8306 |
[32m[20221213 21:00:03 @agent_ppo2.py:185][0m |          -0.0062 |          63.3773 |          17.8257 |
[32m[20221213 21:00:03 @agent_ppo2.py:185][0m |          -0.0069 |          63.3333 |          17.8301 |
[32m[20221213 21:00:03 @agent_ppo2.py:185][0m |          -0.0058 |          63.1486 |          17.8294 |
[32m[20221213 21:00:04 @agent_ppo2.py:185][0m |          -0.0060 |          63.1933 |          17.8233 |
[32m[20221213 21:00:04 @agent_ppo2.py:185][0m |          -0.0018 |          65.0223 |          17.8208 |
[32m[20221213 21:00:04 @agent_ppo2.py:185][0m |          -0.0068 |          62.9245 |          17.8217 |
[32m[20221213 21:00:04 @agent_ppo2.py:185][0m |          -0.0098 |          62.9380 |          17.8228 |
[32m[20221213 21:00:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:00:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.60
[32m[20221213 21:00:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.00
[32m[20221213 21:00:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.00
[32m[20221213 21:00:04 @agent_ppo2.py:143][0m Total time:       4.49 min
[32m[20221213 21:00:04 @agent_ppo2.py:145][0m 436224 total steps have happened
[32m[20221213 21:00:04 @agent_ppo2.py:121][0m #------------------------ Iteration 213 --------------------------#
[32m[20221213 21:00:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:04 @agent_ppo2.py:185][0m |          -0.0028 |          66.6977 |          18.1062 |
[32m[20221213 21:00:04 @agent_ppo2.py:185][0m |          -0.0079 |          66.2098 |          18.0832 |
[32m[20221213 21:00:04 @agent_ppo2.py:185][0m |          -0.0059 |          65.6452 |          18.0891 |
[32m[20221213 21:00:04 @agent_ppo2.py:185][0m |          -0.0061 |          65.5876 |          18.0790 |
[32m[20221213 21:00:05 @agent_ppo2.py:185][0m |          -0.0073 |          65.3934 |          18.0822 |
[32m[20221213 21:00:05 @agent_ppo2.py:185][0m |          -0.0030 |          65.2443 |          18.0855 |
[32m[20221213 21:00:05 @agent_ppo2.py:185][0m |          -0.0058 |          65.1455 |          18.0793 |
[32m[20221213 21:00:05 @agent_ppo2.py:185][0m |          -0.0060 |          65.0790 |          18.0809 |
[32m[20221213 21:00:05 @agent_ppo2.py:185][0m |          -0.0101 |          65.0105 |          18.0739 |
[32m[20221213 21:00:05 @agent_ppo2.py:185][0m |          -0.0076 |          64.8773 |          18.0729 |
[32m[20221213 21:00:05 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:00:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.40
[32m[20221213 21:00:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.00
[32m[20221213 21:00:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.00
[32m[20221213 21:00:05 @agent_ppo2.py:143][0m Total time:       4.51 min
[32m[20221213 21:00:05 @agent_ppo2.py:145][0m 438272 total steps have happened
[32m[20221213 21:00:05 @agent_ppo2.py:121][0m #------------------------ Iteration 214 --------------------------#
[32m[20221213 21:00:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:05 @agent_ppo2.py:185][0m |           0.0004 |          66.2724 |          18.0485 |
[32m[20221213 21:00:05 @agent_ppo2.py:185][0m |          -0.0018 |          65.5886 |          18.0376 |
[32m[20221213 21:00:06 @agent_ppo2.py:185][0m |          -0.0036 |          65.4729 |          18.0325 |
[32m[20221213 21:00:06 @agent_ppo2.py:185][0m |          -0.0037 |          65.2561 |          18.0252 |
[32m[20221213 21:00:06 @agent_ppo2.py:185][0m |          -0.0044 |          65.1887 |          18.0291 |
[32m[20221213 21:00:06 @agent_ppo2.py:185][0m |          -0.0037 |          65.1283 |          18.0214 |
[32m[20221213 21:00:06 @agent_ppo2.py:185][0m |          -0.0056 |          64.9762 |          18.0258 |
[32m[20221213 21:00:06 @agent_ppo2.py:185][0m |          -0.0039 |          65.3818 |          18.0219 |
[32m[20221213 21:00:06 @agent_ppo2.py:185][0m |          -0.0052 |          64.6990 |          18.0243 |
[32m[20221213 21:00:06 @agent_ppo2.py:185][0m |          -0.0064 |          64.7791 |          18.0198 |
[32m[20221213 21:00:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:00:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.80
[32m[20221213 21:00:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 547.00
[32m[20221213 21:00:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.00
[32m[20221213 21:00:06 @agent_ppo2.py:143][0m Total time:       4.53 min
[32m[20221213 21:00:06 @agent_ppo2.py:145][0m 440320 total steps have happened
[32m[20221213 21:00:06 @agent_ppo2.py:121][0m #------------------------ Iteration 215 --------------------------#
[32m[20221213 21:00:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:07 @agent_ppo2.py:185][0m |           0.0071 |          67.9311 |          18.0504 |
[32m[20221213 21:00:07 @agent_ppo2.py:185][0m |          -0.0032 |          64.8534 |          18.0347 |
[32m[20221213 21:00:07 @agent_ppo2.py:185][0m |          -0.0064 |          64.3886 |          18.0144 |
[32m[20221213 21:00:07 @agent_ppo2.py:185][0m |          -0.0005 |          65.4125 |          18.0241 |
[32m[20221213 21:00:07 @agent_ppo2.py:185][0m |          -0.0017 |          64.0264 |          18.0175 |
[32m[20221213 21:00:07 @agent_ppo2.py:185][0m |           0.0072 |          70.9743 |          18.0167 |
[32m[20221213 21:00:07 @agent_ppo2.py:185][0m |          -0.0060 |          63.4350 |          18.0014 |
[32m[20221213 21:00:07 @agent_ppo2.py:185][0m |          -0.0094 |          63.1939 |          18.0089 |
[32m[20221213 21:00:07 @agent_ppo2.py:185][0m |          -0.0049 |          63.2465 |          18.0156 |
[32m[20221213 21:00:07 @agent_ppo2.py:185][0m |          -0.0083 |          62.8407 |          18.0090 |
[32m[20221213 21:00:07 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:00:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 486.60
[32m[20221213 21:00:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.00
[32m[20221213 21:00:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.00
[32m[20221213 21:00:08 @agent_ppo2.py:143][0m Total time:       4.55 min
[32m[20221213 21:00:08 @agent_ppo2.py:145][0m 442368 total steps have happened
[32m[20221213 21:00:08 @agent_ppo2.py:121][0m #------------------------ Iteration 216 --------------------------#
[32m[20221213 21:00:08 @agent_ppo2.py:127][0m Sampling time: 0.27 s by 5 slaves
[32m[20221213 21:00:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:08 @agent_ppo2.py:185][0m |          -0.0005 |          66.6454 |          17.9886 |
[32m[20221213 21:00:08 @agent_ppo2.py:185][0m |          -0.0059 |          65.4832 |          17.9796 |
[32m[20221213 21:00:08 @agent_ppo2.py:185][0m |          -0.0012 |          64.8597 |          17.9763 |
[32m[20221213 21:00:08 @agent_ppo2.py:185][0m |          -0.0065 |          64.5791 |          17.9647 |
[32m[20221213 21:00:08 @agent_ppo2.py:185][0m |          -0.0060 |          64.3310 |          17.9679 |
[32m[20221213 21:00:08 @agent_ppo2.py:185][0m |          -0.0038 |          64.1631 |          17.9622 |
[32m[20221213 21:00:09 @agent_ppo2.py:185][0m |          -0.0076 |          63.9110 |          17.9652 |
[32m[20221213 21:00:09 @agent_ppo2.py:185][0m |          -0.0082 |          63.8283 |          17.9703 |
[32m[20221213 21:00:09 @agent_ppo2.py:185][0m |           0.0016 |          67.2327 |          17.9705 |
[32m[20221213 21:00:09 @agent_ppo2.py:185][0m |          -0.0044 |          63.7222 |          17.9578 |
[32m[20221213 21:00:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:00:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.00
[32m[20221213 21:00:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.00
[32m[20221213 21:00:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.00
[32m[20221213 21:00:09 @agent_ppo2.py:143][0m Total time:       4.57 min
[32m[20221213 21:00:09 @agent_ppo2.py:145][0m 444416 total steps have happened
[32m[20221213 21:00:09 @agent_ppo2.py:121][0m #------------------------ Iteration 217 --------------------------#
[32m[20221213 21:00:09 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:00:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:09 @agent_ppo2.py:185][0m |           0.0029 |          64.9652 |          18.0472 |
[32m[20221213 21:00:09 @agent_ppo2.py:185][0m |          -0.0042 |          63.7516 |          18.0343 |
[32m[20221213 21:00:09 @agent_ppo2.py:185][0m |           0.0072 |          64.3065 |          18.0249 |
[32m[20221213 21:00:10 @agent_ppo2.py:185][0m |           0.0011 |          63.3269 |          18.0252 |
[32m[20221213 21:00:10 @agent_ppo2.py:185][0m |          -0.0044 |          62.1919 |          18.0161 |
[32m[20221213 21:00:10 @agent_ppo2.py:185][0m |          -0.0017 |          62.9594 |          18.0248 |
[32m[20221213 21:00:10 @agent_ppo2.py:185][0m |          -0.0037 |          61.8182 |          18.0204 |
[32m[20221213 21:00:10 @agent_ppo2.py:185][0m |          -0.0028 |          61.7615 |          18.0258 |
[32m[20221213 21:00:10 @agent_ppo2.py:185][0m |          -0.0025 |          61.6297 |          18.0209 |
[32m[20221213 21:00:10 @agent_ppo2.py:185][0m |          -0.0065 |          61.3199 |          18.0217 |
[32m[20221213 21:00:10 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:00:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.60
[32m[20221213 21:00:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.00
[32m[20221213 21:00:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.00
[32m[20221213 21:00:10 @agent_ppo2.py:143][0m Total time:       4.59 min
[32m[20221213 21:00:10 @agent_ppo2.py:145][0m 446464 total steps have happened
[32m[20221213 21:00:10 @agent_ppo2.py:121][0m #------------------------ Iteration 218 --------------------------#
[32m[20221213 21:00:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:00:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:11 @agent_ppo2.py:185][0m |          -0.0002 |          67.5476 |          17.8360 |
[32m[20221213 21:00:11 @agent_ppo2.py:185][0m |          -0.0066 |          66.7649 |          17.8103 |
[32m[20221213 21:00:11 @agent_ppo2.py:185][0m |          -0.0030 |          66.6112 |          17.7980 |
[32m[20221213 21:00:11 @agent_ppo2.py:185][0m |          -0.0003 |          67.3855 |          17.8141 |
[32m[20221213 21:00:11 @agent_ppo2.py:185][0m |          -0.0085 |          66.1433 |          17.8177 |
[32m[20221213 21:00:11 @agent_ppo2.py:185][0m |           0.0045 |          72.3589 |          17.8112 |
[32m[20221213 21:00:11 @agent_ppo2.py:185][0m |          -0.0066 |          65.7708 |          17.7987 |
[32m[20221213 21:00:11 @agent_ppo2.py:185][0m |          -0.0078 |          65.7346 |          17.8094 |
[32m[20221213 21:00:11 @agent_ppo2.py:185][0m |          -0.0075 |          65.5322 |          17.8102 |
[32m[20221213 21:00:11 @agent_ppo2.py:185][0m |          -0.0087 |          65.4654 |          17.8160 |
[32m[20221213 21:00:11 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:00:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.20
[32m[20221213 21:00:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.00
[32m[20221213 21:00:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.00
[32m[20221213 21:00:12 @agent_ppo2.py:143][0m Total time:       4.61 min
[32m[20221213 21:00:12 @agent_ppo2.py:145][0m 448512 total steps have happened
[32m[20221213 21:00:12 @agent_ppo2.py:121][0m #------------------------ Iteration 219 --------------------------#
[32m[20221213 21:00:12 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:00:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:12 @agent_ppo2.py:185][0m |          -0.0008 |          68.4906 |          18.1031 |
[32m[20221213 21:00:12 @agent_ppo2.py:185][0m |          -0.0022 |          67.5515 |          18.0869 |
[32m[20221213 21:00:12 @agent_ppo2.py:185][0m |          -0.0041 |          67.4258 |          18.0816 |
[32m[20221213 21:00:12 @agent_ppo2.py:185][0m |           0.0047 |          71.2536 |          18.0818 |
[32m[20221213 21:00:12 @agent_ppo2.py:185][0m |          -0.0029 |          67.0239 |          18.0798 |
[32m[20221213 21:00:12 @agent_ppo2.py:185][0m |          -0.0071 |          67.1196 |          18.0743 |
[32m[20221213 21:00:12 @agent_ppo2.py:185][0m |          -0.0015 |          67.2587 |          18.0755 |
[32m[20221213 21:00:12 @agent_ppo2.py:185][0m |          -0.0049 |          66.8586 |          18.0774 |
[32m[20221213 21:00:13 @agent_ppo2.py:185][0m |          -0.0054 |          66.6623 |          18.0778 |
[32m[20221213 21:00:13 @agent_ppo2.py:185][0m |          -0.0039 |          67.1033 |          18.0735 |
[32m[20221213 21:00:13 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:00:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.60
[32m[20221213 21:00:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.00
[32m[20221213 21:00:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.00
[32m[20221213 21:00:13 @agent_ppo2.py:143][0m Total time:       4.63 min
[32m[20221213 21:00:13 @agent_ppo2.py:145][0m 450560 total steps have happened
[32m[20221213 21:00:13 @agent_ppo2.py:121][0m #------------------------ Iteration 220 --------------------------#
[32m[20221213 21:00:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:13 @agent_ppo2.py:185][0m |           0.0045 |          67.0597 |          17.9885 |
[32m[20221213 21:00:13 @agent_ppo2.py:185][0m |          -0.0051 |          64.1608 |          17.9741 |
[32m[20221213 21:00:13 @agent_ppo2.py:185][0m |          -0.0006 |          65.2254 |          17.9609 |
[32m[20221213 21:00:13 @agent_ppo2.py:185][0m |          -0.0054 |          63.9375 |          17.9556 |
[32m[20221213 21:00:13 @agent_ppo2.py:185][0m |           0.0004 |          67.8611 |          17.9625 |
[32m[20221213 21:00:14 @agent_ppo2.py:185][0m |          -0.0045 |          63.6689 |          17.9454 |
[32m[20221213 21:00:14 @agent_ppo2.py:185][0m |           0.0196 |          77.5365 |          17.9561 |
[32m[20221213 21:00:14 @agent_ppo2.py:185][0m |          -0.0086 |          63.3789 |          17.9480 |
[32m[20221213 21:00:14 @agent_ppo2.py:185][0m |           0.0039 |          67.0728 |          17.9644 |
[32m[20221213 21:00:14 @agent_ppo2.py:185][0m |          -0.0092 |          63.1049 |          17.9459 |
[32m[20221213 21:00:14 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:00:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.60
[32m[20221213 21:00:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.00
[32m[20221213 21:00:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.00
[32m[20221213 21:00:14 @agent_ppo2.py:143][0m Total time:       4.66 min
[32m[20221213 21:00:14 @agent_ppo2.py:145][0m 452608 total steps have happened
[32m[20221213 21:00:14 @agent_ppo2.py:121][0m #------------------------ Iteration 221 --------------------------#
[32m[20221213 21:00:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:00:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:14 @agent_ppo2.py:185][0m |          -0.0022 |          67.5256 |          18.0366 |
[32m[20221213 21:00:15 @agent_ppo2.py:185][0m |          -0.0031 |          66.7568 |          18.0240 |
[32m[20221213 21:00:15 @agent_ppo2.py:185][0m |          -0.0044 |          66.5383 |          18.0248 |
[32m[20221213 21:00:15 @agent_ppo2.py:185][0m |          -0.0031 |          66.4142 |          18.0290 |
[32m[20221213 21:00:15 @agent_ppo2.py:185][0m |          -0.0048 |          66.2016 |          18.0194 |
[32m[20221213 21:00:15 @agent_ppo2.py:185][0m |          -0.0031 |          66.1419 |          18.0218 |
[32m[20221213 21:00:15 @agent_ppo2.py:185][0m |          -0.0019 |          66.7187 |          18.0149 |
[32m[20221213 21:00:15 @agent_ppo2.py:185][0m |          -0.0035 |          66.0416 |          18.0161 |
[32m[20221213 21:00:15 @agent_ppo2.py:185][0m |          -0.0080 |          65.8545 |          18.0114 |
[32m[20221213 21:00:15 @agent_ppo2.py:185][0m |          -0.0027 |          65.7589 |          18.0247 |
[32m[20221213 21:00:15 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:00:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.80
[32m[20221213 21:00:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.00
[32m[20221213 21:00:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.00
[32m[20221213 21:00:15 @agent_ppo2.py:143][0m Total time:       4.68 min
[32m[20221213 21:00:15 @agent_ppo2.py:145][0m 454656 total steps have happened
[32m[20221213 21:00:15 @agent_ppo2.py:121][0m #------------------------ Iteration 222 --------------------------#
[32m[20221213 21:00:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:16 @agent_ppo2.py:185][0m |          -0.0016 |          69.0253 |          18.1599 |
[32m[20221213 21:00:16 @agent_ppo2.py:185][0m |          -0.0046 |          68.4976 |          18.1526 |
[32m[20221213 21:00:16 @agent_ppo2.py:185][0m |           0.0023 |          70.3574 |          18.1446 |
[32m[20221213 21:00:16 @agent_ppo2.py:185][0m |          -0.0048 |          68.1212 |          18.1189 |
[32m[20221213 21:00:16 @agent_ppo2.py:185][0m |          -0.0052 |          67.9104 |          18.1333 |
[32m[20221213 21:00:16 @agent_ppo2.py:185][0m |          -0.0078 |          67.9385 |          18.1355 |
[32m[20221213 21:00:16 @agent_ppo2.py:185][0m |          -0.0052 |          67.7637 |          18.1294 |
[32m[20221213 21:00:16 @agent_ppo2.py:185][0m |          -0.0090 |          67.7029 |          18.1364 |
[32m[20221213 21:00:16 @agent_ppo2.py:185][0m |          -0.0060 |          67.5450 |          18.1319 |
[32m[20221213 21:00:17 @agent_ppo2.py:185][0m |          -0.0098 |          67.5896 |          18.1322 |
[32m[20221213 21:00:17 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:00:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.80
[32m[20221213 21:00:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.00
[32m[20221213 21:00:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.00
[32m[20221213 21:00:17 @agent_ppo2.py:143][0m Total time:       4.70 min
[32m[20221213 21:00:17 @agent_ppo2.py:145][0m 456704 total steps have happened
[32m[20221213 21:00:17 @agent_ppo2.py:121][0m #------------------------ Iteration 223 --------------------------#
[32m[20221213 21:00:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:17 @agent_ppo2.py:185][0m |          -0.0025 |          67.6412 |          18.1871 |
[32m[20221213 21:00:17 @agent_ppo2.py:185][0m |          -0.0048 |          67.0903 |          18.1785 |
[32m[20221213 21:00:17 @agent_ppo2.py:185][0m |          -0.0010 |          66.6873 |          18.1627 |
[32m[20221213 21:00:17 @agent_ppo2.py:185][0m |          -0.0028 |          66.6096 |          18.1677 |
[32m[20221213 21:00:17 @agent_ppo2.py:185][0m |          -0.0025 |          66.3178 |          18.1577 |
[32m[20221213 21:00:17 @agent_ppo2.py:185][0m |          -0.0040 |          66.1955 |          18.1642 |
[32m[20221213 21:00:18 @agent_ppo2.py:185][0m |           0.0024 |          73.7515 |          18.1560 |
[32m[20221213 21:00:18 @agent_ppo2.py:185][0m |          -0.0037 |          65.9866 |          18.1582 |
[32m[20221213 21:00:18 @agent_ppo2.py:185][0m |          -0.0026 |          66.5252 |          18.1527 |
[32m[20221213 21:00:18 @agent_ppo2.py:185][0m |          -0.0076 |          65.8234 |          18.1501 |
[32m[20221213 21:00:18 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:00:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.20
[32m[20221213 21:00:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.00
[32m[20221213 21:00:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.00
[32m[20221213 21:00:18 @agent_ppo2.py:143][0m Total time:       4.72 min
[32m[20221213 21:00:18 @agent_ppo2.py:145][0m 458752 total steps have happened
[32m[20221213 21:00:18 @agent_ppo2.py:121][0m #------------------------ Iteration 224 --------------------------#
[32m[20221213 21:00:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:18 @agent_ppo2.py:185][0m |          -0.0013 |          66.7668 |          18.0959 |
[32m[20221213 21:00:18 @agent_ppo2.py:185][0m |          -0.0049 |          66.1994 |          18.0780 |
[32m[20221213 21:00:18 @agent_ppo2.py:185][0m |          -0.0051 |          66.0247 |          18.0744 |
[32m[20221213 21:00:18 @agent_ppo2.py:185][0m |          -0.0077 |          66.0191 |          18.0729 |
[32m[20221213 21:00:19 @agent_ppo2.py:185][0m |          -0.0039 |          65.8117 |          18.0669 |
[32m[20221213 21:00:19 @agent_ppo2.py:185][0m |           0.0092 |          71.1568 |          18.0575 |
[32m[20221213 21:00:19 @agent_ppo2.py:185][0m |          -0.0013 |          66.2861 |          18.0632 |
[32m[20221213 21:00:19 @agent_ppo2.py:185][0m |          -0.0062 |          65.6574 |          18.0566 |
[32m[20221213 21:00:19 @agent_ppo2.py:185][0m |          -0.0053 |          65.8522 |          18.0570 |
[32m[20221213 21:00:19 @agent_ppo2.py:185][0m |          -0.0048 |          65.4749 |          18.0532 |
[32m[20221213 21:00:19 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:00:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.80
[32m[20221213 21:00:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.00
[32m[20221213 21:00:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.00
[32m[20221213 21:00:19 @agent_ppo2.py:143][0m Total time:       4.74 min
[32m[20221213 21:00:19 @agent_ppo2.py:145][0m 460800 total steps have happened
[32m[20221213 21:00:19 @agent_ppo2.py:121][0m #------------------------ Iteration 225 --------------------------#
[32m[20221213 21:00:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:19 @agent_ppo2.py:185][0m |          -0.0032 |          66.2030 |          18.0952 |
[32m[20221213 21:00:20 @agent_ppo2.py:185][0m |          -0.0059 |          65.9737 |          18.0753 |
[32m[20221213 21:00:20 @agent_ppo2.py:185][0m |          -0.0081 |          65.9352 |          18.0757 |
[32m[20221213 21:00:20 @agent_ppo2.py:185][0m |          -0.0047 |          65.6216 |          18.0732 |
[32m[20221213 21:00:20 @agent_ppo2.py:185][0m |          -0.0003 |          67.2042 |          18.0763 |
[32m[20221213 21:00:20 @agent_ppo2.py:185][0m |          -0.0064 |          65.5771 |          18.0667 |
[32m[20221213 21:00:20 @agent_ppo2.py:185][0m |          -0.0059 |          65.4595 |          18.0695 |
[32m[20221213 21:00:20 @agent_ppo2.py:185][0m |          -0.0070 |          65.4709 |          18.0726 |
[32m[20221213 21:00:20 @agent_ppo2.py:185][0m |          -0.0054 |          65.3435 |          18.0804 |
[32m[20221213 21:00:20 @agent_ppo2.py:185][0m |          -0.0026 |          65.4091 |          18.0784 |
[32m[20221213 21:00:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:00:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.40
[32m[20221213 21:00:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.00
[32m[20221213 21:00:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.00
[32m[20221213 21:00:20 @agent_ppo2.py:143][0m Total time:       4.76 min
[32m[20221213 21:00:20 @agent_ppo2.py:145][0m 462848 total steps have happened
[32m[20221213 21:00:20 @agent_ppo2.py:121][0m #------------------------ Iteration 226 --------------------------#
[32m[20221213 21:00:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:21 @agent_ppo2.py:185][0m |          -0.0025 |          66.2818 |          18.1373 |
[32m[20221213 21:00:21 @agent_ppo2.py:185][0m |          -0.0032 |          65.5726 |          18.1193 |
[32m[20221213 21:00:21 @agent_ppo2.py:185][0m |          -0.0034 |          65.7201 |          18.1077 |
[32m[20221213 21:00:21 @agent_ppo2.py:185][0m |          -0.0030 |          65.1940 |          18.1198 |
[32m[20221213 21:00:21 @agent_ppo2.py:185][0m |          -0.0040 |          65.1725 |          18.1050 |
[32m[20221213 21:00:21 @agent_ppo2.py:185][0m |           0.0039 |          68.5736 |          18.1035 |
[32m[20221213 21:00:21 @agent_ppo2.py:185][0m |          -0.0045 |          64.9522 |          18.1135 |
[32m[20221213 21:00:21 @agent_ppo2.py:185][0m |          -0.0037 |          64.8337 |          18.1092 |
[32m[20221213 21:00:21 @agent_ppo2.py:185][0m |           0.0013 |          68.9258 |          18.1039 |
[32m[20221213 21:00:21 @agent_ppo2.py:185][0m |          -0.0060 |          64.9578 |          18.0985 |
[32m[20221213 21:00:21 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:00:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.80
[32m[20221213 21:00:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.00
[32m[20221213 21:00:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.00
[32m[20221213 21:00:22 @agent_ppo2.py:143][0m Total time:       4.78 min
[32m[20221213 21:00:22 @agent_ppo2.py:145][0m 464896 total steps have happened
[32m[20221213 21:00:22 @agent_ppo2.py:121][0m #------------------------ Iteration 227 --------------------------#
[32m[20221213 21:00:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:22 @agent_ppo2.py:185][0m |          -0.0016 |          71.0218 |          18.2689 |
[32m[20221213 21:00:22 @agent_ppo2.py:185][0m |          -0.0039 |          69.2801 |          18.2504 |
[32m[20221213 21:00:22 @agent_ppo2.py:185][0m |           0.0023 |          73.2479 |          18.2437 |
[32m[20221213 21:00:22 @agent_ppo2.py:185][0m |          -0.0076 |          68.3462 |          18.2262 |
[32m[20221213 21:00:22 @agent_ppo2.py:185][0m |          -0.0078 |          67.9763 |          18.2437 |
[32m[20221213 21:00:22 @agent_ppo2.py:185][0m |          -0.0071 |          67.6306 |          18.2314 |
[32m[20221213 21:00:23 @agent_ppo2.py:185][0m |          -0.0086 |          67.3106 |          18.2292 |
[32m[20221213 21:00:23 @agent_ppo2.py:185][0m |          -0.0082 |          67.1618 |          18.2272 |
[32m[20221213 21:00:23 @agent_ppo2.py:185][0m |          -0.0082 |          66.8409 |          18.2349 |
[32m[20221213 21:00:23 @agent_ppo2.py:185][0m |          -0.0094 |          66.5790 |          18.2241 |
[32m[20221213 21:00:23 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 21:00:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.80
[32m[20221213 21:00:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.00
[32m[20221213 21:00:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.00
[32m[20221213 21:00:23 @agent_ppo2.py:143][0m Total time:       4.80 min
[32m[20221213 21:00:23 @agent_ppo2.py:145][0m 466944 total steps have happened
[32m[20221213 21:00:23 @agent_ppo2.py:121][0m #------------------------ Iteration 228 --------------------------#
[32m[20221213 21:00:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:23 @agent_ppo2.py:185][0m |          -0.0036 |          68.6048 |          18.2142 |
[32m[20221213 21:00:23 @agent_ppo2.py:185][0m |          -0.0008 |          68.1506 |          18.2008 |
[32m[20221213 21:00:23 @agent_ppo2.py:185][0m |          -0.0010 |          67.5537 |          18.1818 |
[32m[20221213 21:00:24 @agent_ppo2.py:185][0m |          -0.0066 |          67.2197 |          18.1876 |
[32m[20221213 21:00:24 @agent_ppo2.py:185][0m |          -0.0060 |          67.0028 |          18.1793 |
[32m[20221213 21:00:24 @agent_ppo2.py:185][0m |          -0.0063 |          66.9127 |          18.1880 |
[32m[20221213 21:00:24 @agent_ppo2.py:185][0m |          -0.0054 |          66.8589 |          18.1889 |
[32m[20221213 21:00:24 @agent_ppo2.py:185][0m |          -0.0075 |          66.6270 |          18.1852 |
[32m[20221213 21:00:24 @agent_ppo2.py:185][0m |          -0.0057 |          66.4527 |          18.1837 |
[32m[20221213 21:00:24 @agent_ppo2.py:185][0m |          -0.0078 |          66.3996 |          18.1783 |
[32m[20221213 21:00:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:00:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.40
[32m[20221213 21:00:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.00
[32m[20221213 21:00:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.00
[32m[20221213 21:00:24 @agent_ppo2.py:143][0m Total time:       4.82 min
[32m[20221213 21:00:24 @agent_ppo2.py:145][0m 468992 total steps have happened
[32m[20221213 21:00:24 @agent_ppo2.py:121][0m #------------------------ Iteration 229 --------------------------#
[32m[20221213 21:00:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:24 @agent_ppo2.py:185][0m |          -0.0017 |          68.5029 |          18.1413 |
[32m[20221213 21:00:25 @agent_ppo2.py:185][0m |          -0.0023 |          67.8863 |          18.1043 |
[32m[20221213 21:00:25 @agent_ppo2.py:185][0m |          -0.0058 |          67.8725 |          18.1046 |
[32m[20221213 21:00:25 @agent_ppo2.py:185][0m |          -0.0064 |          67.5745 |          18.1047 |
[32m[20221213 21:00:25 @agent_ppo2.py:185][0m |          -0.0078 |          67.3528 |          18.0874 |
[32m[20221213 21:00:25 @agent_ppo2.py:185][0m |          -0.0058 |          67.3270 |          18.0964 |
[32m[20221213 21:00:25 @agent_ppo2.py:185][0m |           0.0046 |          69.5166 |          18.0960 |
[32m[20221213 21:00:25 @agent_ppo2.py:185][0m |          -0.0055 |          67.1371 |          18.1067 |
[32m[20221213 21:00:25 @agent_ppo2.py:185][0m |          -0.0075 |          67.1018 |          18.0982 |
[32m[20221213 21:00:25 @agent_ppo2.py:185][0m |          -0.0028 |          67.2146 |          18.0892 |
[32m[20221213 21:00:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:00:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.60
[32m[20221213 21:00:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.00
[32m[20221213 21:00:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.00
[32m[20221213 21:00:25 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 567.00
[32m[20221213 21:00:25 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 567.00
[32m[20221213 21:00:25 @agent_ppo2.py:143][0m Total time:       4.84 min
[32m[20221213 21:00:25 @agent_ppo2.py:145][0m 471040 total steps have happened
[32m[20221213 21:00:25 @agent_ppo2.py:121][0m #------------------------ Iteration 230 --------------------------#
[32m[20221213 21:00:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:26 @agent_ppo2.py:185][0m |          -0.0024 |          68.7235 |          18.1466 |
[32m[20221213 21:00:26 @agent_ppo2.py:185][0m |          -0.0051 |          68.3553 |          18.1313 |
[32m[20221213 21:00:26 @agent_ppo2.py:185][0m |          -0.0054 |          68.1645 |          18.1233 |
[32m[20221213 21:00:26 @agent_ppo2.py:185][0m |           0.0042 |          72.7717 |          18.1338 |
[32m[20221213 21:00:26 @agent_ppo2.py:185][0m |          -0.0040 |          67.9293 |          18.1123 |
[32m[20221213 21:00:26 @agent_ppo2.py:185][0m |          -0.0062 |          67.8473 |          18.1316 |
[32m[20221213 21:00:26 @agent_ppo2.py:185][0m |          -0.0064 |          67.7512 |          18.1316 |
[32m[20221213 21:00:26 @agent_ppo2.py:185][0m |          -0.0034 |          67.9302 |          18.1330 |
[32m[20221213 21:00:26 @agent_ppo2.py:185][0m |          -0.0073 |          67.6656 |          18.1265 |
[32m[20221213 21:00:26 @agent_ppo2.py:185][0m |          -0.0046 |          67.6305 |          18.1304 |
[32m[20221213 21:00:26 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:00:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.00
[32m[20221213 21:00:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 547.00
[32m[20221213 21:00:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:00:27 @agent_ppo2.py:143][0m Total time:       4.86 min
[32m[20221213 21:00:27 @agent_ppo2.py:145][0m 473088 total steps have happened
[32m[20221213 21:00:27 @agent_ppo2.py:121][0m #------------------------ Iteration 231 --------------------------#
[32m[20221213 21:00:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:00:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:27 @agent_ppo2.py:185][0m |          -0.0003 |          65.0708 |          18.0832 |
[32m[20221213 21:00:27 @agent_ppo2.py:185][0m |          -0.0015 |          64.5513 |          18.0748 |
[32m[20221213 21:00:27 @agent_ppo2.py:185][0m |          -0.0031 |          64.2904 |          18.0637 |
[32m[20221213 21:00:27 @agent_ppo2.py:185][0m |          -0.0024 |          64.4456 |          18.0594 |
[32m[20221213 21:00:27 @agent_ppo2.py:185][0m |          -0.0016 |          64.0365 |          18.0534 |
[32m[20221213 21:00:27 @agent_ppo2.py:185][0m |          -0.0050 |          63.8444 |          18.0587 |
[32m[20221213 21:00:27 @agent_ppo2.py:185][0m |          -0.0071 |          63.8704 |          18.0559 |
[32m[20221213 21:00:27 @agent_ppo2.py:185][0m |          -0.0051 |          64.0129 |          18.0587 |
[32m[20221213 21:00:28 @agent_ppo2.py:185][0m |           0.0038 |          66.9404 |          18.0539 |
[32m[20221213 21:00:28 @agent_ppo2.py:185][0m |          -0.0027 |          63.8896 |          18.0583 |
[32m[20221213 21:00:28 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:00:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.20
[32m[20221213 21:00:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.00
[32m[20221213 21:00:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.00
[32m[20221213 21:00:28 @agent_ppo2.py:143][0m Total time:       4.88 min
[32m[20221213 21:00:28 @agent_ppo2.py:145][0m 475136 total steps have happened
[32m[20221213 21:00:28 @agent_ppo2.py:121][0m #------------------------ Iteration 232 --------------------------#
[32m[20221213 21:00:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:28 @agent_ppo2.py:185][0m |          -0.0000 |          67.3082 |          18.2021 |
[32m[20221213 21:00:28 @agent_ppo2.py:185][0m |          -0.0030 |          66.9435 |          18.1925 |
[32m[20221213 21:00:28 @agent_ppo2.py:185][0m |          -0.0060 |          66.6245 |          18.1875 |
[32m[20221213 21:00:28 @agent_ppo2.py:185][0m |          -0.0023 |          67.0202 |          18.1798 |
[32m[20221213 21:00:28 @agent_ppo2.py:185][0m |          -0.0074 |          66.2894 |          18.1822 |
[32m[20221213 21:00:28 @agent_ppo2.py:185][0m |          -0.0042 |          66.1771 |          18.1807 |
[32m[20221213 21:00:29 @agent_ppo2.py:185][0m |          -0.0053 |          66.1843 |          18.1743 |
[32m[20221213 21:00:29 @agent_ppo2.py:185][0m |          -0.0069 |          66.1126 |          18.1772 |
[32m[20221213 21:00:29 @agent_ppo2.py:185][0m |          -0.0076 |          66.0106 |          18.1760 |
[32m[20221213 21:00:29 @agent_ppo2.py:185][0m |          -0.0079 |          66.0135 |          18.1767 |
[32m[20221213 21:00:29 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:00:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.00
[32m[20221213 21:00:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.00
[32m[20221213 21:00:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.00
[32m[20221213 21:00:29 @agent_ppo2.py:143][0m Total time:       4.90 min
[32m[20221213 21:00:29 @agent_ppo2.py:145][0m 477184 total steps have happened
[32m[20221213 21:00:29 @agent_ppo2.py:121][0m #------------------------ Iteration 233 --------------------------#
[32m[20221213 21:00:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:29 @agent_ppo2.py:185][0m |          -0.0047 |          70.0644 |          18.1417 |
[32m[20221213 21:00:29 @agent_ppo2.py:185][0m |          -0.0046 |          69.7087 |          18.0976 |
[32m[20221213 21:00:29 @agent_ppo2.py:185][0m |          -0.0027 |          68.9324 |          18.1063 |
[32m[20221213 21:00:29 @agent_ppo2.py:185][0m |          -0.0075 |          68.5860 |          18.1004 |
[32m[20221213 21:00:30 @agent_ppo2.py:185][0m |          -0.0041 |          68.4812 |          18.1040 |
[32m[20221213 21:00:30 @agent_ppo2.py:185][0m |          -0.0067 |          68.1864 |          18.0993 |
[32m[20221213 21:00:30 @agent_ppo2.py:185][0m |          -0.0069 |          68.0267 |          18.0953 |
[32m[20221213 21:00:30 @agent_ppo2.py:185][0m |          -0.0059 |          68.1199 |          18.1081 |
[32m[20221213 21:00:30 @agent_ppo2.py:185][0m |          -0.0116 |          67.9582 |          18.1023 |
[32m[20221213 21:00:30 @agent_ppo2.py:185][0m |          -0.0056 |          67.8269 |          18.0961 |
[32m[20221213 21:00:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:00:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.00
[32m[20221213 21:00:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.00
[32m[20221213 21:00:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.00
[32m[20221213 21:00:30 @agent_ppo2.py:143][0m Total time:       4.92 min
[32m[20221213 21:00:30 @agent_ppo2.py:145][0m 479232 total steps have happened
[32m[20221213 21:00:30 @agent_ppo2.py:121][0m #------------------------ Iteration 234 --------------------------#
[32m[20221213 21:00:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:00:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:30 @agent_ppo2.py:185][0m |           0.0058 |          70.2995 |          17.9972 |
[32m[20221213 21:00:31 @agent_ppo2.py:185][0m |          -0.0047 |          68.0563 |          17.9829 |
[32m[20221213 21:00:31 @agent_ppo2.py:185][0m |          -0.0010 |          68.6740 |          17.9754 |
[32m[20221213 21:00:31 @agent_ppo2.py:185][0m |          -0.0031 |          67.5733 |          17.9891 |
[32m[20221213 21:00:31 @agent_ppo2.py:185][0m |          -0.0057 |          67.5949 |          17.9729 |
[32m[20221213 21:00:31 @agent_ppo2.py:185][0m |          -0.0046 |          67.5746 |          17.9824 |
[32m[20221213 21:00:31 @agent_ppo2.py:185][0m |           0.0028 |          70.5817 |          17.9867 |
[32m[20221213 21:00:31 @agent_ppo2.py:185][0m |          -0.0057 |          67.2492 |          17.9778 |
[32m[20221213 21:00:31 @agent_ppo2.py:185][0m |          -0.0051 |          67.0084 |          17.9910 |
[32m[20221213 21:00:31 @agent_ppo2.py:185][0m |          -0.0076 |          66.8800 |          17.9783 |
[32m[20221213 21:00:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:00:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.40
[32m[20221213 21:00:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.00
[32m[20221213 21:00:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.00
[32m[20221213 21:00:31 @agent_ppo2.py:143][0m Total time:       4.94 min
[32m[20221213 21:00:31 @agent_ppo2.py:145][0m 481280 total steps have happened
[32m[20221213 21:00:31 @agent_ppo2.py:121][0m #------------------------ Iteration 235 --------------------------#
[32m[20221213 21:00:31 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:00:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:32 @agent_ppo2.py:185][0m |          -0.0027 |          70.0623 |          18.0812 |
[32m[20221213 21:00:32 @agent_ppo2.py:185][0m |          -0.0048 |          69.4009 |          18.0629 |
[32m[20221213 21:00:32 @agent_ppo2.py:185][0m |           0.0006 |          72.2340 |          18.0502 |
[32m[20221213 21:00:32 @agent_ppo2.py:185][0m |          -0.0064 |          69.0843 |          18.0375 |
[32m[20221213 21:00:32 @agent_ppo2.py:185][0m |          -0.0096 |          69.0504 |          18.0322 |
[32m[20221213 21:00:32 @agent_ppo2.py:185][0m |          -0.0093 |          68.8985 |          18.0382 |
[32m[20221213 21:00:32 @agent_ppo2.py:185][0m |          -0.0093 |          68.8221 |          18.0342 |
[32m[20221213 21:00:32 @agent_ppo2.py:185][0m |          -0.0057 |          68.7467 |          18.0351 |
[32m[20221213 21:00:32 @agent_ppo2.py:185][0m |          -0.0105 |          68.7943 |          18.0321 |
[32m[20221213 21:00:32 @agent_ppo2.py:185][0m |          -0.0096 |          68.6450 |          18.0363 |
[32m[20221213 21:00:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:00:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.40
[32m[20221213 21:00:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.00
[32m[20221213 21:00:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.00
[32m[20221213 21:00:33 @agent_ppo2.py:143][0m Total time:       4.96 min
[32m[20221213 21:00:33 @agent_ppo2.py:145][0m 483328 total steps have happened
[32m[20221213 21:00:33 @agent_ppo2.py:121][0m #------------------------ Iteration 236 --------------------------#
[32m[20221213 21:00:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:33 @agent_ppo2.py:185][0m |           0.0032 |          70.4235 |          17.9920 |
[32m[20221213 21:00:33 @agent_ppo2.py:185][0m |           0.0027 |          72.8653 |          17.9761 |
[32m[20221213 21:00:33 @agent_ppo2.py:185][0m |           0.0089 |          76.9450 |          17.9510 |
[32m[20221213 21:00:33 @agent_ppo2.py:185][0m |          -0.0061 |          68.7592 |          17.9469 |
[32m[20221213 21:00:33 @agent_ppo2.py:185][0m |          -0.0033 |          68.5151 |          17.9501 |
[32m[20221213 21:00:33 @agent_ppo2.py:185][0m |          -0.0054 |          68.2505 |          17.9605 |
[32m[20221213 21:00:33 @agent_ppo2.py:185][0m |          -0.0069 |          68.0739 |          17.9608 |
[32m[20221213 21:00:33 @agent_ppo2.py:185][0m |          -0.0050 |          68.1797 |          17.9594 |
[32m[20221213 21:00:34 @agent_ppo2.py:185][0m |          -0.0079 |          67.7720 |          17.9529 |
[32m[20221213 21:00:34 @agent_ppo2.py:185][0m |          -0.0049 |          67.6585 |          17.9530 |
[32m[20221213 21:00:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:00:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.80
[32m[20221213 21:00:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.00
[32m[20221213 21:00:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.00
[32m[20221213 21:00:34 @agent_ppo2.py:143][0m Total time:       4.98 min
[32m[20221213 21:00:34 @agent_ppo2.py:145][0m 485376 total steps have happened
[32m[20221213 21:00:34 @agent_ppo2.py:121][0m #------------------------ Iteration 237 --------------------------#
[32m[20221213 21:00:34 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:00:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:34 @agent_ppo2.py:185][0m |          -0.0002 |          68.2965 |          17.9596 |
[32m[20221213 21:00:34 @agent_ppo2.py:185][0m |          -0.0019 |          68.0351 |          17.9456 |
[32m[20221213 21:00:34 @agent_ppo2.py:185][0m |           0.0106 |          72.5374 |          17.9425 |
[32m[20221213 21:00:34 @agent_ppo2.py:185][0m |          -0.0056 |          67.5489 |          17.9304 |
[32m[20221213 21:00:34 @agent_ppo2.py:185][0m |          -0.0055 |          67.3272 |          17.9214 |
[32m[20221213 21:00:34 @agent_ppo2.py:185][0m |          -0.0056 |          67.2193 |          17.9264 |
[32m[20221213 21:00:35 @agent_ppo2.py:185][0m |           0.0002 |          68.3085 |          17.9282 |
[32m[20221213 21:00:35 @agent_ppo2.py:185][0m |          -0.0031 |          67.0065 |          17.9255 |
[32m[20221213 21:00:35 @agent_ppo2.py:185][0m |          -0.0077 |          66.9513 |          17.9166 |
[32m[20221213 21:00:35 @agent_ppo2.py:185][0m |          -0.0066 |          66.8356 |          17.9148 |
[32m[20221213 21:00:35 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:00:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 486.40
[32m[20221213 21:00:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.00
[32m[20221213 21:00:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.00
[32m[20221213 21:00:35 @agent_ppo2.py:143][0m Total time:       5.00 min
[32m[20221213 21:00:35 @agent_ppo2.py:145][0m 487424 total steps have happened
[32m[20221213 21:00:35 @agent_ppo2.py:121][0m #------------------------ Iteration 238 --------------------------#
[32m[20221213 21:00:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:35 @agent_ppo2.py:185][0m |          -0.0020 |          69.0812 |          18.0373 |
[32m[20221213 21:00:35 @agent_ppo2.py:185][0m |          -0.0029 |          68.4580 |          18.0277 |
[32m[20221213 21:00:35 @agent_ppo2.py:185][0m |          -0.0036 |          68.0484 |          18.0327 |
[32m[20221213 21:00:36 @agent_ppo2.py:185][0m |          -0.0046 |          67.6787 |          18.0269 |
[32m[20221213 21:00:36 @agent_ppo2.py:185][0m |           0.0072 |          74.2103 |          18.0292 |
[32m[20221213 21:00:36 @agent_ppo2.py:185][0m |          -0.0048 |          67.5650 |          18.0243 |
[32m[20221213 21:00:36 @agent_ppo2.py:185][0m |          -0.0019 |          68.5478 |          18.0205 |
[32m[20221213 21:00:36 @agent_ppo2.py:185][0m |           0.0006 |          67.1623 |          18.0262 |
[32m[20221213 21:00:36 @agent_ppo2.py:185][0m |          -0.0019 |          67.1600 |          18.0281 |
[32m[20221213 21:00:36 @agent_ppo2.py:185][0m |          -0.0037 |          66.8510 |          18.0202 |
[32m[20221213 21:00:36 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:00:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.20
[32m[20221213 21:00:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.00
[32m[20221213 21:00:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 556.00
[32m[20221213 21:00:36 @agent_ppo2.py:143][0m Total time:       5.03 min
[32m[20221213 21:00:36 @agent_ppo2.py:145][0m 489472 total steps have happened
[32m[20221213 21:00:36 @agent_ppo2.py:121][0m #------------------------ Iteration 239 --------------------------#
[32m[20221213 21:00:36 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:00:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:37 @agent_ppo2.py:185][0m |          -0.0036 |          72.8681 |          17.9590 |
[32m[20221213 21:00:37 @agent_ppo2.py:185][0m |           0.0020 |          72.4193 |          17.9380 |
[32m[20221213 21:00:37 @agent_ppo2.py:185][0m |          -0.0033 |          71.8115 |          17.9363 |
[32m[20221213 21:00:37 @agent_ppo2.py:185][0m |          -0.0050 |          71.4469 |          17.9293 |
[32m[20221213 21:00:37 @agent_ppo2.py:185][0m |          -0.0015 |          72.2828 |          17.9264 |
[32m[20221213 21:00:37 @agent_ppo2.py:185][0m |          -0.0072 |          71.1619 |          17.9272 |
[32m[20221213 21:00:37 @agent_ppo2.py:185][0m |          -0.0031 |          71.2297 |          17.9282 |
[32m[20221213 21:00:37 @agent_ppo2.py:185][0m |          -0.0070 |          70.9045 |          17.9215 |
[32m[20221213 21:00:37 @agent_ppo2.py:185][0m |          -0.0059 |          70.9190 |          17.9244 |
[32m[20221213 21:00:37 @agent_ppo2.py:185][0m |          -0.0069 |          70.7622 |          17.9207 |
[32m[20221213 21:00:37 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:00:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.00
[32m[20221213 21:00:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.00
[32m[20221213 21:00:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.00
[32m[20221213 21:00:38 @agent_ppo2.py:143][0m Total time:       5.05 min
[32m[20221213 21:00:38 @agent_ppo2.py:145][0m 491520 total steps have happened
[32m[20221213 21:00:38 @agent_ppo2.py:121][0m #------------------------ Iteration 240 --------------------------#
[32m[20221213 21:00:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:38 @agent_ppo2.py:185][0m |          -0.0024 |          70.3761 |          18.1118 |
[32m[20221213 21:00:38 @agent_ppo2.py:185][0m |           0.0039 |          72.0503 |          18.1108 |
[32m[20221213 21:00:38 @agent_ppo2.py:185][0m |          -0.0066 |          69.8214 |          18.0851 |
[32m[20221213 21:00:38 @agent_ppo2.py:185][0m |          -0.0034 |          69.5551 |          18.0893 |
[32m[20221213 21:00:38 @agent_ppo2.py:185][0m |          -0.0048 |          69.5730 |          18.0785 |
[32m[20221213 21:00:38 @agent_ppo2.py:185][0m |           0.0002 |          69.9070 |          18.0827 |
[32m[20221213 21:00:39 @agent_ppo2.py:185][0m |          -0.0054 |          69.0971 |          18.0799 |
[32m[20221213 21:00:39 @agent_ppo2.py:185][0m |          -0.0038 |          69.1111 |          18.0785 |
[32m[20221213 21:00:39 @agent_ppo2.py:185][0m |          -0.0067 |          68.8737 |          18.0783 |
[32m[20221213 21:00:39 @agent_ppo2.py:185][0m |          -0.0072 |          68.7861 |          18.0823 |
[32m[20221213 21:00:39 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 21:00:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.40
[32m[20221213 21:00:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.00
[32m[20221213 21:00:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.00
[32m[20221213 21:00:39 @agent_ppo2.py:143][0m Total time:       5.07 min
[32m[20221213 21:00:39 @agent_ppo2.py:145][0m 493568 total steps have happened
[32m[20221213 21:00:39 @agent_ppo2.py:121][0m #------------------------ Iteration 241 --------------------------#
[32m[20221213 21:00:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:39 @agent_ppo2.py:185][0m |          -0.0015 |          71.3901 |          18.1246 |
[32m[20221213 21:00:39 @agent_ppo2.py:185][0m |          -0.0038 |          70.4697 |          18.1047 |
[32m[20221213 21:00:39 @agent_ppo2.py:185][0m |          -0.0031 |          70.2270 |          18.0961 |
[32m[20221213 21:00:40 @agent_ppo2.py:185][0m |          -0.0033 |          71.1690 |          18.1011 |
[32m[20221213 21:00:40 @agent_ppo2.py:185][0m |           0.0129 |          80.9141 |          18.0967 |
[32m[20221213 21:00:40 @agent_ppo2.py:185][0m |           0.0094 |          79.5329 |          18.0941 |
[32m[20221213 21:00:40 @agent_ppo2.py:185][0m |          -0.0047 |          69.9097 |          18.0964 |
[32m[20221213 21:00:40 @agent_ppo2.py:185][0m |          -0.0041 |          69.7609 |          18.0922 |
[32m[20221213 21:00:40 @agent_ppo2.py:185][0m |           0.0005 |          71.1708 |          18.0953 |
[32m[20221213 21:00:40 @agent_ppo2.py:185][0m |          -0.0030 |          69.6367 |          18.0877 |
[32m[20221213 21:00:40 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:00:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.80
[32m[20221213 21:00:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.00
[32m[20221213 21:00:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.00
[32m[20221213 21:00:40 @agent_ppo2.py:143][0m Total time:       5.09 min
[32m[20221213 21:00:40 @agent_ppo2.py:145][0m 495616 total steps have happened
[32m[20221213 21:00:40 @agent_ppo2.py:121][0m #------------------------ Iteration 242 --------------------------#
[32m[20221213 21:00:40 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:00:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:41 @agent_ppo2.py:185][0m |           0.0003 |          72.1685 |          17.9871 |
[32m[20221213 21:00:41 @agent_ppo2.py:185][0m |          -0.0062 |          71.4354 |          17.9725 |
[32m[20221213 21:00:41 @agent_ppo2.py:185][0m |          -0.0078 |          71.2538 |          17.9580 |
[32m[20221213 21:00:41 @agent_ppo2.py:185][0m |          -0.0051 |          70.9312 |          17.9675 |
[32m[20221213 21:00:41 @agent_ppo2.py:185][0m |           0.0082 |          76.5217 |          17.9528 |
[32m[20221213 21:00:41 @agent_ppo2.py:185][0m |          -0.0047 |          70.7390 |          17.9645 |
[32m[20221213 21:00:41 @agent_ppo2.py:185][0m |          -0.0068 |          70.6717 |          17.9456 |
[32m[20221213 21:00:41 @agent_ppo2.py:185][0m |          -0.0073 |          70.5751 |          17.9487 |
[32m[20221213 21:00:41 @agent_ppo2.py:185][0m |          -0.0082 |          70.4803 |          17.9481 |
[32m[20221213 21:00:41 @agent_ppo2.py:185][0m |          -0.0098 |          70.7748 |          17.9434 |
[32m[20221213 21:00:41 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:00:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.40
[32m[20221213 21:00:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.00
[32m[20221213 21:00:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.00
[32m[20221213 21:00:42 @agent_ppo2.py:143][0m Total time:       5.12 min
[32m[20221213 21:00:42 @agent_ppo2.py:145][0m 497664 total steps have happened
[32m[20221213 21:00:42 @agent_ppo2.py:121][0m #------------------------ Iteration 243 --------------------------#
[32m[20221213 21:00:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:42 @agent_ppo2.py:185][0m |          -0.0025 |          71.1637 |          17.8960 |
[32m[20221213 21:00:42 @agent_ppo2.py:185][0m |          -0.0063 |          70.4198 |          17.8813 |
[32m[20221213 21:00:42 @agent_ppo2.py:185][0m |          -0.0069 |          70.1111 |          17.8785 |
[32m[20221213 21:00:42 @agent_ppo2.py:185][0m |          -0.0094 |          69.8564 |          17.8740 |
[32m[20221213 21:00:42 @agent_ppo2.py:185][0m |          -0.0080 |          69.5655 |          17.8760 |
[32m[20221213 21:00:42 @agent_ppo2.py:185][0m |          -0.0074 |          69.4564 |          17.8734 |
[32m[20221213 21:00:43 @agent_ppo2.py:185][0m |          -0.0002 |          72.5874 |          17.8733 |
[32m[20221213 21:00:43 @agent_ppo2.py:185][0m |          -0.0099 |          69.2288 |          17.8743 |
[32m[20221213 21:00:43 @agent_ppo2.py:185][0m |          -0.0087 |          69.1714 |          17.8767 |
[32m[20221213 21:00:43 @agent_ppo2.py:185][0m |          -0.0094 |          68.9788 |          17.8744 |
[32m[20221213 21:00:43 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:00:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.80
[32m[20221213 21:00:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.00
[32m[20221213 21:00:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 653.00
[32m[20221213 21:00:43 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 653.00
[32m[20221213 21:00:43 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 653.00
[32m[20221213 21:00:43 @agent_ppo2.py:143][0m Total time:       5.14 min
[32m[20221213 21:00:43 @agent_ppo2.py:145][0m 499712 total steps have happened
[32m[20221213 21:00:43 @agent_ppo2.py:121][0m #------------------------ Iteration 244 --------------------------#
[32m[20221213 21:00:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:00:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:43 @agent_ppo2.py:185][0m |           0.0018 |          72.0486 |          18.0643 |
[32m[20221213 21:00:43 @agent_ppo2.py:185][0m |          -0.0059 |          70.7205 |          18.0526 |
[32m[20221213 21:00:43 @agent_ppo2.py:185][0m |          -0.0030 |          70.2155 |          18.0511 |
[32m[20221213 21:00:44 @agent_ppo2.py:185][0m |           0.0092 |          75.2018 |          18.0464 |
[32m[20221213 21:00:44 @agent_ppo2.py:185][0m |          -0.0040 |          69.7666 |          18.0433 |
[32m[20221213 21:00:44 @agent_ppo2.py:185][0m |          -0.0046 |          69.4912 |          18.0429 |
[32m[20221213 21:00:44 @agent_ppo2.py:185][0m |          -0.0063 |          69.4408 |          18.0414 |
[32m[20221213 21:00:44 @agent_ppo2.py:185][0m |          -0.0055 |          69.1757 |          18.0459 |
[32m[20221213 21:00:44 @agent_ppo2.py:185][0m |          -0.0067 |          69.0080 |          18.0348 |
[32m[20221213 21:00:44 @agent_ppo2.py:185][0m |          -0.0063 |          69.0488 |          18.0392 |
[32m[20221213 21:00:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:00:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.60
[32m[20221213 21:00:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.00
[32m[20221213 21:00:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.00
[32m[20221213 21:00:44 @agent_ppo2.py:143][0m Total time:       5.16 min
[32m[20221213 21:00:44 @agent_ppo2.py:145][0m 501760 total steps have happened
[32m[20221213 21:00:44 @agent_ppo2.py:121][0m #------------------------ Iteration 245 --------------------------#
[32m[20221213 21:00:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:44 @agent_ppo2.py:185][0m |          -0.0018 |          74.0524 |          18.0999 |
[32m[20221213 21:00:45 @agent_ppo2.py:185][0m |           0.0028 |          75.7785 |          18.0946 |
[32m[20221213 21:00:45 @agent_ppo2.py:185][0m |          -0.0019 |          73.1255 |          18.0839 |
[32m[20221213 21:00:45 @agent_ppo2.py:185][0m |          -0.0011 |          71.8138 |          18.0694 |
[32m[20221213 21:00:45 @agent_ppo2.py:185][0m |          -0.0071 |          70.7096 |          18.0763 |
[32m[20221213 21:00:45 @agent_ppo2.py:185][0m |          -0.0091 |          70.3831 |          18.0705 |
[32m[20221213 21:00:45 @agent_ppo2.py:185][0m |          -0.0073 |          70.0876 |          18.0749 |
[32m[20221213 21:00:45 @agent_ppo2.py:185][0m |          -0.0066 |          69.8087 |          18.0753 |
[32m[20221213 21:00:45 @agent_ppo2.py:185][0m |          -0.0074 |          69.4466 |          18.0759 |
[32m[20221213 21:00:45 @agent_ppo2.py:185][0m |          -0.0050 |          69.2812 |          18.0749 |
[32m[20221213 21:00:45 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:00:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.60
[32m[20221213 21:00:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.00
[32m[20221213 21:00:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 601.00
[32m[20221213 21:00:45 @agent_ppo2.py:143][0m Total time:       5.18 min
[32m[20221213 21:00:45 @agent_ppo2.py:145][0m 503808 total steps have happened
[32m[20221213 21:00:45 @agent_ppo2.py:121][0m #------------------------ Iteration 246 --------------------------#
[32m[20221213 21:00:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:46 @agent_ppo2.py:185][0m |          -0.0010 |          69.2557 |          18.0037 |
[32m[20221213 21:00:46 @agent_ppo2.py:185][0m |          -0.0031 |          67.2228 |          17.9896 |
[32m[20221213 21:00:46 @agent_ppo2.py:185][0m |          -0.0077 |          66.4883 |          17.9851 |
[32m[20221213 21:00:46 @agent_ppo2.py:185][0m |          -0.0068 |          66.1695 |          17.9833 |
[32m[20221213 21:00:46 @agent_ppo2.py:185][0m |          -0.0008 |          67.1593 |          17.9779 |
[32m[20221213 21:00:46 @agent_ppo2.py:185][0m |          -0.0064 |          65.6551 |          17.9773 |
[32m[20221213 21:00:46 @agent_ppo2.py:185][0m |          -0.0035 |          65.3314 |          17.9749 |
[32m[20221213 21:00:46 @agent_ppo2.py:185][0m |          -0.0045 |          65.1026 |          17.9722 |
[32m[20221213 21:00:46 @agent_ppo2.py:185][0m |          -0.0085 |          64.8671 |          17.9741 |
[32m[20221213 21:00:47 @agent_ppo2.py:185][0m |          -0.0097 |          64.7923 |          17.9730 |
[32m[20221213 21:00:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:00:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.20
[32m[20221213 21:00:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.00
[32m[20221213 21:00:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.00
[32m[20221213 21:00:47 @agent_ppo2.py:143][0m Total time:       5.20 min
[32m[20221213 21:00:47 @agent_ppo2.py:145][0m 505856 total steps have happened
[32m[20221213 21:00:47 @agent_ppo2.py:121][0m #------------------------ Iteration 247 --------------------------#
[32m[20221213 21:00:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:00:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:47 @agent_ppo2.py:185][0m |           0.0048 |          71.4298 |          18.0974 |
[32m[20221213 21:00:47 @agent_ppo2.py:185][0m |          -0.0075 |          68.3211 |          18.0925 |
[32m[20221213 21:00:47 @agent_ppo2.py:185][0m |          -0.0095 |          67.6668 |          18.0872 |
[32m[20221213 21:00:47 @agent_ppo2.py:185][0m |          -0.0095 |          67.3485 |          18.0752 |
[32m[20221213 21:00:47 @agent_ppo2.py:185][0m |          -0.0067 |          66.9072 |          18.0825 |
[32m[20221213 21:00:47 @agent_ppo2.py:185][0m |          -0.0099 |          66.6804 |          18.0819 |
[32m[20221213 21:00:47 @agent_ppo2.py:185][0m |          -0.0070 |          66.5863 |          18.0787 |
[32m[20221213 21:00:48 @agent_ppo2.py:185][0m |          -0.0074 |          66.3785 |          18.0762 |
[32m[20221213 21:00:48 @agent_ppo2.py:185][0m |          -0.0031 |          68.0601 |          18.0804 |
[32m[20221213 21:00:48 @agent_ppo2.py:185][0m |          -0.0095 |          66.2785 |          18.0686 |
[32m[20221213 21:00:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:00:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.60
[32m[20221213 21:00:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.00
[32m[20221213 21:00:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 544.00
[32m[20221213 21:00:48 @agent_ppo2.py:143][0m Total time:       5.22 min
[32m[20221213 21:00:48 @agent_ppo2.py:145][0m 507904 total steps have happened
[32m[20221213 21:00:48 @agent_ppo2.py:121][0m #------------------------ Iteration 248 --------------------------#
[32m[20221213 21:00:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:48 @agent_ppo2.py:185][0m |           0.0078 |          72.0766 |          18.0824 |
[32m[20221213 21:00:48 @agent_ppo2.py:185][0m |          -0.0013 |          69.3234 |          18.0766 |
[32m[20221213 21:00:48 @agent_ppo2.py:185][0m |          -0.0073 |          68.6060 |          18.0703 |
[32m[20221213 21:00:48 @agent_ppo2.py:185][0m |           0.0033 |          69.6089 |          18.0719 |
[32m[20221213 21:00:49 @agent_ppo2.py:185][0m |          -0.0039 |          67.8816 |          18.0673 |
[32m[20221213 21:00:49 @agent_ppo2.py:185][0m |          -0.0030 |          68.0251 |          18.0678 |
[32m[20221213 21:00:49 @agent_ppo2.py:185][0m |          -0.0073 |          67.4024 |          18.0665 |
[32m[20221213 21:00:49 @agent_ppo2.py:185][0m |          -0.0075 |          67.4246 |          18.0658 |
[32m[20221213 21:00:49 @agent_ppo2.py:185][0m |          -0.0025 |          68.4262 |          18.0597 |
[32m[20221213 21:00:49 @agent_ppo2.py:185][0m |          -0.0033 |          67.1122 |          18.0635 |
[32m[20221213 21:00:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:00:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.00
[32m[20221213 21:00:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.00
[32m[20221213 21:00:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.00
[32m[20221213 21:00:49 @agent_ppo2.py:143][0m Total time:       5.24 min
[32m[20221213 21:00:49 @agent_ppo2.py:145][0m 509952 total steps have happened
[32m[20221213 21:00:49 @agent_ppo2.py:121][0m #------------------------ Iteration 249 --------------------------#
[32m[20221213 21:00:49 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:00:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:49 @agent_ppo2.py:185][0m |          -0.0008 |          66.3392 |          18.1062 |
[32m[20221213 21:00:49 @agent_ppo2.py:185][0m |           0.0095 |          75.6335 |          18.0939 |
[32m[20221213 21:00:50 @agent_ppo2.py:185][0m |          -0.0082 |          65.5232 |          18.0685 |
[32m[20221213 21:00:50 @agent_ppo2.py:185][0m |          -0.0031 |          65.6742 |          18.0765 |
[32m[20221213 21:00:50 @agent_ppo2.py:185][0m |          -0.0063 |          64.9952 |          18.0752 |
[32m[20221213 21:00:50 @agent_ppo2.py:185][0m |          -0.0065 |          64.8389 |          18.0816 |
[32m[20221213 21:00:50 @agent_ppo2.py:185][0m |          -0.0074 |          64.7953 |          18.0784 |
[32m[20221213 21:00:50 @agent_ppo2.py:185][0m |          -0.0074 |          64.5526 |          18.0755 |
[32m[20221213 21:00:50 @agent_ppo2.py:185][0m |           0.0013 |          68.4838 |          18.0837 |
[32m[20221213 21:00:50 @agent_ppo2.py:185][0m |          -0.0005 |          65.5444 |          18.0780 |
[32m[20221213 21:00:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:00:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.80
[32m[20221213 21:00:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.00
[32m[20221213 21:00:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.00
[32m[20221213 21:00:50 @agent_ppo2.py:143][0m Total time:       5.26 min
[32m[20221213 21:00:50 @agent_ppo2.py:145][0m 512000 total steps have happened
[32m[20221213 21:00:50 @agent_ppo2.py:121][0m #------------------------ Iteration 250 --------------------------#
[32m[20221213 21:00:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:51 @agent_ppo2.py:185][0m |          -0.0014 |          67.6740 |          18.1154 |
[32m[20221213 21:00:51 @agent_ppo2.py:185][0m |          -0.0020 |          67.1259 |          18.0994 |
[32m[20221213 21:00:51 @agent_ppo2.py:185][0m |          -0.0027 |          66.8739 |          18.0947 |
[32m[20221213 21:00:51 @agent_ppo2.py:185][0m |          -0.0044 |          66.7331 |          18.0836 |
[32m[20221213 21:00:51 @agent_ppo2.py:185][0m |          -0.0086 |          66.5952 |          18.0850 |
[32m[20221213 21:00:51 @agent_ppo2.py:185][0m |          -0.0053 |          66.3596 |          18.0828 |
[32m[20221213 21:00:51 @agent_ppo2.py:185][0m |          -0.0056 |          66.2409 |          18.0735 |
[32m[20221213 21:00:51 @agent_ppo2.py:185][0m |          -0.0078 |          66.2730 |          18.0745 |
[32m[20221213 21:00:51 @agent_ppo2.py:185][0m |          -0.0060 |          66.8157 |          18.0778 |
[32m[20221213 21:00:51 @agent_ppo2.py:185][0m |          -0.0079 |          66.0400 |          18.0626 |
[32m[20221213 21:00:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:00:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.60
[32m[20221213 21:00:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.00
[32m[20221213 21:00:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.00
[32m[20221213 21:00:51 @agent_ppo2.py:143][0m Total time:       5.28 min
[32m[20221213 21:00:51 @agent_ppo2.py:145][0m 514048 total steps have happened
[32m[20221213 21:00:51 @agent_ppo2.py:121][0m #------------------------ Iteration 251 --------------------------#
[32m[20221213 21:00:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:52 @agent_ppo2.py:185][0m |          -0.0017 |          71.6192 |          18.0323 |
[32m[20221213 21:00:52 @agent_ppo2.py:185][0m |          -0.0060 |          71.0396 |          18.0091 |
[32m[20221213 21:00:52 @agent_ppo2.py:185][0m |          -0.0063 |          70.6808 |          18.0204 |
[32m[20221213 21:00:52 @agent_ppo2.py:185][0m |          -0.0072 |          70.4669 |          18.0158 |
[32m[20221213 21:00:52 @agent_ppo2.py:185][0m |           0.0053 |          73.2678 |          18.0093 |
[32m[20221213 21:00:52 @agent_ppo2.py:185][0m |           0.0089 |          76.7263 |          18.0150 |
[32m[20221213 21:00:52 @agent_ppo2.py:185][0m |          -0.0106 |          70.2254 |          18.0196 |
[32m[20221213 21:00:52 @agent_ppo2.py:185][0m |          -0.0080 |          69.8785 |          18.0118 |
[32m[20221213 21:00:52 @agent_ppo2.py:185][0m |          -0.0035 |          69.8751 |          18.0144 |
[32m[20221213 21:00:53 @agent_ppo2.py:185][0m |          -0.0086 |          69.6415 |          18.0053 |
[32m[20221213 21:00:53 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:00:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.60
[32m[20221213 21:00:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.00
[32m[20221213 21:00:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.00
[32m[20221213 21:00:53 @agent_ppo2.py:143][0m Total time:       5.30 min
[32m[20221213 21:00:53 @agent_ppo2.py:145][0m 516096 total steps have happened
[32m[20221213 21:00:53 @agent_ppo2.py:121][0m #------------------------ Iteration 252 --------------------------#
[32m[20221213 21:00:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:53 @agent_ppo2.py:185][0m |          -0.0015 |          69.6643 |          18.1142 |
[32m[20221213 21:00:53 @agent_ppo2.py:185][0m |           0.0046 |          70.6289 |          18.0945 |
[32m[20221213 21:00:53 @agent_ppo2.py:185][0m |          -0.0059 |          68.8993 |          18.0867 |
[32m[20221213 21:00:53 @agent_ppo2.py:185][0m |          -0.0062 |          68.7193 |          18.0797 |
[32m[20221213 21:00:53 @agent_ppo2.py:185][0m |          -0.0058 |          68.4382 |          18.0861 |
[32m[20221213 21:00:53 @agent_ppo2.py:185][0m |          -0.0060 |          68.3264 |          18.0765 |
[32m[20221213 21:00:54 @agent_ppo2.py:185][0m |          -0.0050 |          68.1943 |          18.0756 |
[32m[20221213 21:00:54 @agent_ppo2.py:185][0m |          -0.0072 |          68.0790 |          18.0741 |
[32m[20221213 21:00:54 @agent_ppo2.py:185][0m |          -0.0082 |          67.9367 |          18.0716 |
[32m[20221213 21:00:54 @agent_ppo2.py:185][0m |          -0.0084 |          67.8849 |          18.0638 |
[32m[20221213 21:00:54 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:00:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.40
[32m[20221213 21:00:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.00
[32m[20221213 21:00:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 565.00
[32m[20221213 21:00:54 @agent_ppo2.py:143][0m Total time:       5.32 min
[32m[20221213 21:00:54 @agent_ppo2.py:145][0m 518144 total steps have happened
[32m[20221213 21:00:54 @agent_ppo2.py:121][0m #------------------------ Iteration 253 --------------------------#
[32m[20221213 21:00:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:54 @agent_ppo2.py:185][0m |          -0.0015 |          69.7064 |          18.0405 |
[32m[20221213 21:00:54 @agent_ppo2.py:185][0m |          -0.0033 |          65.0307 |          18.0221 |
[32m[20221213 21:00:54 @agent_ppo2.py:185][0m |           0.0069 |          69.3801 |          18.0192 |
[32m[20221213 21:00:54 @agent_ppo2.py:185][0m |          -0.0052 |          61.9654 |          18.0149 |
[32m[20221213 21:00:55 @agent_ppo2.py:185][0m |          -0.0050 |          61.2818 |          18.0132 |
[32m[20221213 21:00:55 @agent_ppo2.py:185][0m |          -0.0054 |          60.7294 |          18.0203 |
[32m[20221213 21:00:55 @agent_ppo2.py:185][0m |          -0.0075 |          60.3912 |          18.0124 |
[32m[20221213 21:00:55 @agent_ppo2.py:185][0m |          -0.0077 |          60.0939 |          18.0118 |
[32m[20221213 21:00:55 @agent_ppo2.py:185][0m |          -0.0070 |          59.8843 |          18.0153 |
[32m[20221213 21:00:55 @agent_ppo2.py:185][0m |          -0.0068 |          59.5736 |          18.0135 |
[32m[20221213 21:00:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:00:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.40
[32m[20221213 21:00:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.00
[32m[20221213 21:00:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 585.00
[32m[20221213 21:00:55 @agent_ppo2.py:143][0m Total time:       5.34 min
[32m[20221213 21:00:55 @agent_ppo2.py:145][0m 520192 total steps have happened
[32m[20221213 21:00:55 @agent_ppo2.py:121][0m #------------------------ Iteration 254 --------------------------#
[32m[20221213 21:00:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:55 @agent_ppo2.py:185][0m |          -0.0032 |          76.9595 |          18.0253 |
[32m[20221213 21:00:56 @agent_ppo2.py:185][0m |          -0.0024 |          73.4947 |          18.0102 |
[32m[20221213 21:00:56 @agent_ppo2.py:185][0m |           0.0039 |          75.4200 |          18.0137 |
[32m[20221213 21:00:56 @agent_ppo2.py:185][0m |          -0.0051 |          72.3112 |          18.0094 |
[32m[20221213 21:00:56 @agent_ppo2.py:185][0m |          -0.0046 |          72.0694 |          18.0083 |
[32m[20221213 21:00:56 @agent_ppo2.py:185][0m |           0.0010 |          72.9891 |          18.0023 |
[32m[20221213 21:00:56 @agent_ppo2.py:185][0m |          -0.0033 |          71.5657 |          18.0036 |
[32m[20221213 21:00:56 @agent_ppo2.py:185][0m |          -0.0052 |          71.4507 |          18.0048 |
[32m[20221213 21:00:56 @agent_ppo2.py:185][0m |          -0.0004 |          71.7430 |          18.0070 |
[32m[20221213 21:00:56 @agent_ppo2.py:185][0m |          -0.0074 |          71.4323 |          18.0007 |
[32m[20221213 21:00:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:00:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.00
[32m[20221213 21:00:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.00
[32m[20221213 21:00:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 520.00
[32m[20221213 21:00:56 @agent_ppo2.py:143][0m Total time:       5.36 min
[32m[20221213 21:00:56 @agent_ppo2.py:145][0m 522240 total steps have happened
[32m[20221213 21:00:56 @agent_ppo2.py:121][0m #------------------------ Iteration 255 --------------------------#
[32m[20221213 21:00:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:57 @agent_ppo2.py:185][0m |           0.0019 |          69.5560 |          18.1999 |
[32m[20221213 21:00:57 @agent_ppo2.py:185][0m |          -0.0002 |          69.6372 |          18.1841 |
[32m[20221213 21:00:57 @agent_ppo2.py:185][0m |          -0.0024 |          68.9906 |          18.1872 |
[32m[20221213 21:00:57 @agent_ppo2.py:185][0m |           0.0112 |          74.3993 |          18.1781 |
[32m[20221213 21:00:57 @agent_ppo2.py:185][0m |          -0.0061 |          68.7217 |          18.1873 |
[32m[20221213 21:00:57 @agent_ppo2.py:185][0m |          -0.0050 |          68.5565 |          18.1866 |
[32m[20221213 21:00:57 @agent_ppo2.py:185][0m |          -0.0048 |          68.4037 |          18.1761 |
[32m[20221213 21:00:57 @agent_ppo2.py:185][0m |          -0.0080 |          68.4403 |          18.1758 |
[32m[20221213 21:00:57 @agent_ppo2.py:185][0m |          -0.0066 |          68.3476 |          18.1766 |
[32m[20221213 21:00:57 @agent_ppo2.py:185][0m |          -0.0046 |          68.1853 |          18.1667 |
[32m[20221213 21:00:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:00:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.60
[32m[20221213 21:00:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.00
[32m[20221213 21:00:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 609.00
[32m[20221213 21:00:58 @agent_ppo2.py:143][0m Total time:       5.38 min
[32m[20221213 21:00:58 @agent_ppo2.py:145][0m 524288 total steps have happened
[32m[20221213 21:00:58 @agent_ppo2.py:121][0m #------------------------ Iteration 256 --------------------------#
[32m[20221213 21:00:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:00:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:58 @agent_ppo2.py:185][0m |          -0.0032 |          69.9450 |          17.9847 |
[32m[20221213 21:00:58 @agent_ppo2.py:185][0m |          -0.0056 |          68.5114 |          17.9777 |
[32m[20221213 21:00:58 @agent_ppo2.py:185][0m |          -0.0045 |          68.2616 |          17.9669 |
[32m[20221213 21:00:58 @agent_ppo2.py:185][0m |          -0.0035 |          68.2824 |          17.9604 |
[32m[20221213 21:00:58 @agent_ppo2.py:185][0m |          -0.0087 |          67.8181 |          17.9605 |
[32m[20221213 21:00:58 @agent_ppo2.py:185][0m |           0.0034 |          71.1427 |          17.9663 |
[32m[20221213 21:00:58 @agent_ppo2.py:185][0m |          -0.0068 |          67.8362 |          17.9593 |
[32m[20221213 21:00:58 @agent_ppo2.py:185][0m |           0.0148 |          77.3675 |          17.9505 |
[32m[20221213 21:00:58 @agent_ppo2.py:185][0m |          -0.0078 |          67.4745 |          17.9604 |
[32m[20221213 21:00:59 @agent_ppo2.py:185][0m |           0.0012 |          72.0219 |          17.9517 |
[32m[20221213 21:00:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:00:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.80
[32m[20221213 21:00:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.00
[32m[20221213 21:00:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.00
[32m[20221213 21:00:59 @agent_ppo2.py:143][0m Total time:       5.40 min
[32m[20221213 21:00:59 @agent_ppo2.py:145][0m 526336 total steps have happened
[32m[20221213 21:00:59 @agent_ppo2.py:121][0m #------------------------ Iteration 257 --------------------------#
[32m[20221213 21:00:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:00:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:00:59 @agent_ppo2.py:185][0m |          -0.0023 |          72.0382 |          18.2162 |
[32m[20221213 21:00:59 @agent_ppo2.py:185][0m |          -0.0030 |          71.4831 |          18.2088 |
[32m[20221213 21:00:59 @agent_ppo2.py:185][0m |          -0.0043 |          71.1994 |          18.2101 |
[32m[20221213 21:00:59 @agent_ppo2.py:185][0m |           0.0055 |          75.8538 |          18.2031 |
[32m[20221213 21:00:59 @agent_ppo2.py:185][0m |          -0.0038 |          70.9268 |          18.1926 |
[32m[20221213 21:00:59 @agent_ppo2.py:185][0m |          -0.0066 |          70.7711 |          18.1936 |
[32m[20221213 21:01:00 @agent_ppo2.py:185][0m |          -0.0065 |          70.6165 |          18.1938 |
[32m[20221213 21:01:00 @agent_ppo2.py:185][0m |          -0.0053 |          70.5901 |          18.1842 |
[32m[20221213 21:01:00 @agent_ppo2.py:185][0m |          -0.0060 |          70.5419 |          18.1896 |
[32m[20221213 21:01:00 @agent_ppo2.py:185][0m |          -0.0076 |          70.4551 |          18.1844 |
[32m[20221213 21:01:00 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:01:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 517.00
[32m[20221213 21:01:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 603.00
[32m[20221213 21:01:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 588.00
[32m[20221213 21:01:00 @agent_ppo2.py:143][0m Total time:       5.42 min
[32m[20221213 21:01:00 @agent_ppo2.py:145][0m 528384 total steps have happened
[32m[20221213 21:01:00 @agent_ppo2.py:121][0m #------------------------ Iteration 258 --------------------------#
[32m[20221213 21:01:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:00 @agent_ppo2.py:185][0m |          -0.0033 |          69.9860 |          18.3043 |
[32m[20221213 21:01:00 @agent_ppo2.py:185][0m |          -0.0056 |          69.1135 |          18.2839 |
[32m[20221213 21:01:00 @agent_ppo2.py:185][0m |           0.0047 |          74.9572 |          18.2732 |
[32m[20221213 21:01:01 @agent_ppo2.py:185][0m |          -0.0083 |          68.5226 |          18.2701 |
[32m[20221213 21:01:01 @agent_ppo2.py:185][0m |          -0.0062 |          68.2907 |          18.2632 |
[32m[20221213 21:01:01 @agent_ppo2.py:185][0m |          -0.0070 |          68.1234 |          18.2703 |
[32m[20221213 21:01:01 @agent_ppo2.py:185][0m |          -0.0049 |          68.4555 |          18.2739 |
[32m[20221213 21:01:01 @agent_ppo2.py:185][0m |          -0.0076 |          67.8092 |          18.2690 |
[32m[20221213 21:01:01 @agent_ppo2.py:185][0m |          -0.0065 |          67.6809 |          18.2769 |
[32m[20221213 21:01:01 @agent_ppo2.py:185][0m |           0.0021 |          72.6199 |          18.2807 |
[32m[20221213 21:01:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:01:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.60
[32m[20221213 21:01:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.00
[32m[20221213 21:01:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.00
[32m[20221213 21:01:01 @agent_ppo2.py:143][0m Total time:       5.44 min
[32m[20221213 21:01:01 @agent_ppo2.py:145][0m 530432 total steps have happened
[32m[20221213 21:01:01 @agent_ppo2.py:121][0m #------------------------ Iteration 259 --------------------------#
[32m[20221213 21:01:01 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:01:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:01 @agent_ppo2.py:185][0m |          -0.0015 |          71.6362 |          18.2344 |
[32m[20221213 21:01:02 @agent_ppo2.py:185][0m |           0.0059 |          77.5573 |          18.2156 |
[32m[20221213 21:01:02 @agent_ppo2.py:185][0m |          -0.0057 |          70.6786 |          18.1901 |
[32m[20221213 21:01:02 @agent_ppo2.py:185][0m |          -0.0055 |          70.5618 |          18.1854 |
[32m[20221213 21:01:02 @agent_ppo2.py:185][0m |          -0.0061 |          70.5248 |          18.1869 |
[32m[20221213 21:01:02 @agent_ppo2.py:185][0m |          -0.0064 |          70.3584 |          18.1850 |
[32m[20221213 21:01:02 @agent_ppo2.py:185][0m |          -0.0069 |          70.2983 |          18.1871 |
[32m[20221213 21:01:02 @agent_ppo2.py:185][0m |           0.0017 |          74.8957 |          18.1937 |
[32m[20221213 21:01:02 @agent_ppo2.py:185][0m |          -0.0064 |          70.2497 |          18.1945 |
[32m[20221213 21:01:02 @agent_ppo2.py:185][0m |          -0.0105 |          70.2899 |          18.1853 |
[32m[20221213 21:01:02 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:01:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.20
[32m[20221213 21:01:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.00
[32m[20221213 21:01:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.00
[32m[20221213 21:01:02 @agent_ppo2.py:143][0m Total time:       5.46 min
[32m[20221213 21:01:02 @agent_ppo2.py:145][0m 532480 total steps have happened
[32m[20221213 21:01:02 @agent_ppo2.py:121][0m #------------------------ Iteration 260 --------------------------#
[32m[20221213 21:01:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:01:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:03 @agent_ppo2.py:185][0m |          -0.0023 |          69.2741 |          18.1310 |
[32m[20221213 21:01:03 @agent_ppo2.py:185][0m |          -0.0007 |          69.3530 |          18.1202 |
[32m[20221213 21:01:03 @agent_ppo2.py:185][0m |          -0.0026 |          69.0811 |          18.1155 |
[32m[20221213 21:01:03 @agent_ppo2.py:185][0m |          -0.0026 |          68.7235 |          18.1126 |
[32m[20221213 21:01:03 @agent_ppo2.py:185][0m |          -0.0033 |          68.6793 |          18.1117 |
[32m[20221213 21:01:03 @agent_ppo2.py:185][0m |           0.0049 |          70.9746 |          18.1058 |
[32m[20221213 21:01:03 @agent_ppo2.py:185][0m |          -0.0033 |          68.4086 |          18.1117 |
[32m[20221213 21:01:03 @agent_ppo2.py:185][0m |          -0.0042 |          68.3117 |          18.1124 |
[32m[20221213 21:01:03 @agent_ppo2.py:185][0m |          -0.0033 |          68.8628 |          18.1030 |
[32m[20221213 21:01:03 @agent_ppo2.py:185][0m |          -0.0047 |          68.1245 |          18.1021 |
[32m[20221213 21:01:03 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:01:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.60
[32m[20221213 21:01:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.00
[32m[20221213 21:01:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.00
[32m[20221213 21:01:04 @agent_ppo2.py:143][0m Total time:       5.48 min
[32m[20221213 21:01:04 @agent_ppo2.py:145][0m 534528 total steps have happened
[32m[20221213 21:01:04 @agent_ppo2.py:121][0m #------------------------ Iteration 261 --------------------------#
[32m[20221213 21:01:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:04 @agent_ppo2.py:185][0m |          -0.0015 |          71.8809 |          18.2826 |
[32m[20221213 21:01:04 @agent_ppo2.py:185][0m |           0.0005 |          71.1294 |          18.2606 |
[32m[20221213 21:01:04 @agent_ppo2.py:185][0m |          -0.0051 |          70.7992 |          18.2453 |
[32m[20221213 21:01:04 @agent_ppo2.py:185][0m |          -0.0028 |          70.6524 |          18.2515 |
[32m[20221213 21:01:04 @agent_ppo2.py:185][0m |          -0.0072 |          70.2520 |          18.2412 |
[32m[20221213 21:01:04 @agent_ppo2.py:185][0m |          -0.0076 |          70.0767 |          18.2467 |
[32m[20221213 21:01:04 @agent_ppo2.py:185][0m |          -0.0003 |          71.5344 |          18.2350 |
[32m[20221213 21:01:05 @agent_ppo2.py:185][0m |          -0.0080 |          69.7690 |          18.2386 |
[32m[20221213 21:01:05 @agent_ppo2.py:185][0m |          -0.0046 |          70.0046 |          18.2385 |
[32m[20221213 21:01:05 @agent_ppo2.py:185][0m |          -0.0020 |          70.7862 |          18.2336 |
[32m[20221213 21:01:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:01:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.80
[32m[20221213 21:01:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.00
[32m[20221213 21:01:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.00
[32m[20221213 21:01:05 @agent_ppo2.py:143][0m Total time:       5.50 min
[32m[20221213 21:01:05 @agent_ppo2.py:145][0m 536576 total steps have happened
[32m[20221213 21:01:05 @agent_ppo2.py:121][0m #------------------------ Iteration 262 --------------------------#
[32m[20221213 21:01:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:05 @agent_ppo2.py:185][0m |          -0.0003 |          69.5071 |          18.2190 |
[32m[20221213 21:01:05 @agent_ppo2.py:185][0m |          -0.0052 |          68.9637 |          18.2086 |
[32m[20221213 21:01:05 @agent_ppo2.py:185][0m |          -0.0024 |          68.6881 |          18.1961 |
[32m[20221213 21:01:05 @agent_ppo2.py:185][0m |          -0.0055 |          68.3753 |          18.1968 |
[32m[20221213 21:01:05 @agent_ppo2.py:185][0m |          -0.0057 |          68.3323 |          18.1901 |
[32m[20221213 21:01:06 @agent_ppo2.py:185][0m |          -0.0051 |          68.1959 |          18.1977 |
[32m[20221213 21:01:06 @agent_ppo2.py:185][0m |          -0.0060 |          67.9673 |          18.1948 |
[32m[20221213 21:01:06 @agent_ppo2.py:185][0m |          -0.0064 |          67.8310 |          18.1903 |
[32m[20221213 21:01:06 @agent_ppo2.py:185][0m |          -0.0090 |          67.7337 |          18.1916 |
[32m[20221213 21:01:06 @agent_ppo2.py:185][0m |          -0.0064 |          67.5909 |          18.1963 |
[32m[20221213 21:01:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:01:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.00
[32m[20221213 21:01:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.00
[32m[20221213 21:01:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 553.00
[32m[20221213 21:01:06 @agent_ppo2.py:143][0m Total time:       5.52 min
[32m[20221213 21:01:06 @agent_ppo2.py:145][0m 538624 total steps have happened
[32m[20221213 21:01:06 @agent_ppo2.py:121][0m #------------------------ Iteration 263 --------------------------#
[32m[20221213 21:01:06 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:01:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:06 @agent_ppo2.py:185][0m |          -0.0004 |          73.7663 |          18.1479 |
[32m[20221213 21:01:06 @agent_ppo2.py:185][0m |           0.0028 |          73.7005 |          18.1321 |
[32m[20221213 21:01:06 @agent_ppo2.py:185][0m |           0.0005 |          74.0152 |          18.1248 |
[32m[20221213 21:01:07 @agent_ppo2.py:185][0m |          -0.0025 |          71.9934 |          18.1222 |
[32m[20221213 21:01:07 @agent_ppo2.py:185][0m |          -0.0039 |          71.6497 |          18.1283 |
[32m[20221213 21:01:07 @agent_ppo2.py:185][0m |          -0.0078 |          71.5321 |          18.1305 |
[32m[20221213 21:01:07 @agent_ppo2.py:185][0m |          -0.0084 |          71.2971 |          18.1319 |
[32m[20221213 21:01:07 @agent_ppo2.py:185][0m |          -0.0065 |          71.2484 |          18.1316 |
[32m[20221213 21:01:07 @agent_ppo2.py:185][0m |          -0.0056 |          71.1379 |          18.1274 |
[32m[20221213 21:01:07 @agent_ppo2.py:185][0m |          -0.0058 |          71.0634 |          18.1292 |
[32m[20221213 21:01:07 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:01:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.00
[32m[20221213 21:01:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.00
[32m[20221213 21:01:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 576.00
[32m[20221213 21:01:07 @agent_ppo2.py:143][0m Total time:       5.54 min
[32m[20221213 21:01:07 @agent_ppo2.py:145][0m 540672 total steps have happened
[32m[20221213 21:01:07 @agent_ppo2.py:121][0m #------------------------ Iteration 264 --------------------------#
[32m[20221213 21:01:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:08 @agent_ppo2.py:185][0m |           0.0020 |          73.3793 |          18.2541 |
[32m[20221213 21:01:08 @agent_ppo2.py:185][0m |           0.0065 |          73.5822 |          18.2175 |
[32m[20221213 21:01:08 @agent_ppo2.py:185][0m |          -0.0009 |          72.8338 |          18.2184 |
[32m[20221213 21:01:08 @agent_ppo2.py:185][0m |          -0.0056 |          70.9558 |          18.2013 |
[32m[20221213 21:01:08 @agent_ppo2.py:185][0m |          -0.0051 |          71.4754 |          18.2103 |
[32m[20221213 21:01:08 @agent_ppo2.py:185][0m |           0.0017 |          78.3492 |          18.2145 |
[32m[20221213 21:01:08 @agent_ppo2.py:185][0m |          -0.0065 |          70.7251 |          18.1899 |
[32m[20221213 21:01:08 @agent_ppo2.py:185][0m |          -0.0068 |          70.4449 |          18.2130 |
[32m[20221213 21:01:08 @agent_ppo2.py:185][0m |          -0.0076 |          70.3122 |          18.2142 |
[32m[20221213 21:01:08 @agent_ppo2.py:185][0m |           0.0036 |          76.8764 |          18.2126 |
[32m[20221213 21:01:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:01:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 526.20
[32m[20221213 21:01:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.00
[32m[20221213 21:01:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.00
[32m[20221213 21:01:08 @agent_ppo2.py:143][0m Total time:       5.56 min
[32m[20221213 21:01:08 @agent_ppo2.py:145][0m 542720 total steps have happened
[32m[20221213 21:01:08 @agent_ppo2.py:121][0m #------------------------ Iteration 265 --------------------------#
[32m[20221213 21:01:09 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:01:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:09 @agent_ppo2.py:185][0m |          -0.0047 |          71.6507 |          18.1284 |
[32m[20221213 21:01:09 @agent_ppo2.py:185][0m |           0.0057 |          75.1117 |          18.1166 |
[32m[20221213 21:01:09 @agent_ppo2.py:185][0m |          -0.0038 |          70.2865 |          18.1129 |
[32m[20221213 21:01:09 @agent_ppo2.py:185][0m |          -0.0054 |          70.1076 |          18.1121 |
[32m[20221213 21:01:09 @agent_ppo2.py:185][0m |          -0.0048 |          69.6892 |          18.1102 |
[32m[20221213 21:01:09 @agent_ppo2.py:185][0m |          -0.0081 |          69.4919 |          18.1118 |
[32m[20221213 21:01:09 @agent_ppo2.py:185][0m |          -0.0074 |          69.3166 |          18.1065 |
[32m[20221213 21:01:09 @agent_ppo2.py:185][0m |          -0.0071 |          69.0252 |          18.1100 |
[32m[20221213 21:01:09 @agent_ppo2.py:185][0m |          -0.0039 |          69.0709 |          18.1111 |
[32m[20221213 21:01:09 @agent_ppo2.py:185][0m |          -0.0077 |          68.8380 |          18.1066 |
[32m[20221213 21:01:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:01:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.60
[32m[20221213 21:01:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.00
[32m[20221213 21:01:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 577.00
[32m[20221213 21:01:10 @agent_ppo2.py:143][0m Total time:       5.58 min
[32m[20221213 21:01:10 @agent_ppo2.py:145][0m 544768 total steps have happened
[32m[20221213 21:01:10 @agent_ppo2.py:121][0m #------------------------ Iteration 266 --------------------------#
[32m[20221213 21:01:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:10 @agent_ppo2.py:185][0m |          -0.0021 |          73.5070 |          18.2902 |
[32m[20221213 21:01:10 @agent_ppo2.py:185][0m |          -0.0034 |          73.0262 |          18.2801 |
[32m[20221213 21:01:10 @agent_ppo2.py:185][0m |          -0.0044 |          72.7548 |          18.2754 |
[32m[20221213 21:01:10 @agent_ppo2.py:185][0m |          -0.0091 |          72.7464 |          18.2728 |
[32m[20221213 21:01:10 @agent_ppo2.py:185][0m |          -0.0060 |          72.4700 |          18.2641 |
[32m[20221213 21:01:10 @agent_ppo2.py:185][0m |           0.0075 |          80.8797 |          18.2670 |
[32m[20221213 21:01:10 @agent_ppo2.py:185][0m |          -0.0034 |          72.3052 |          18.2601 |
[32m[20221213 21:01:10 @agent_ppo2.py:185][0m |          -0.0078 |          72.2075 |          18.2743 |
[32m[20221213 21:01:11 @agent_ppo2.py:185][0m |          -0.0067 |          72.6532 |          18.2701 |
[32m[20221213 21:01:11 @agent_ppo2.py:185][0m |          -0.0069 |          72.1202 |          18.2722 |
[32m[20221213 21:01:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:01:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.00
[32m[20221213 21:01:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.00
[32m[20221213 21:01:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.00
[32m[20221213 21:01:11 @agent_ppo2.py:143][0m Total time:       5.60 min
[32m[20221213 21:01:11 @agent_ppo2.py:145][0m 546816 total steps have happened
[32m[20221213 21:01:11 @agent_ppo2.py:121][0m #------------------------ Iteration 267 --------------------------#
[32m[20221213 21:01:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:11 @agent_ppo2.py:185][0m |           0.0032 |          73.6125 |          18.2885 |
[32m[20221213 21:01:11 @agent_ppo2.py:185][0m |          -0.0068 |          72.1282 |          18.2682 |
[32m[20221213 21:01:11 @agent_ppo2.py:185][0m |          -0.0061 |          71.8991 |          18.2598 |
[32m[20221213 21:01:11 @agent_ppo2.py:185][0m |          -0.0061 |          71.6217 |          18.2660 |
[32m[20221213 21:01:11 @agent_ppo2.py:185][0m |          -0.0067 |          71.3761 |          18.2613 |
[32m[20221213 21:01:12 @agent_ppo2.py:185][0m |          -0.0063 |          71.4368 |          18.2629 |
[32m[20221213 21:01:12 @agent_ppo2.py:185][0m |          -0.0098 |          71.0907 |          18.2634 |
[32m[20221213 21:01:12 @agent_ppo2.py:185][0m |          -0.0099 |          70.9937 |          18.2607 |
[32m[20221213 21:01:12 @agent_ppo2.py:185][0m |          -0.0059 |          70.8323 |          18.2611 |
[32m[20221213 21:01:12 @agent_ppo2.py:185][0m |          -0.0077 |          70.8374 |          18.2617 |
[32m[20221213 21:01:12 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:01:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.80
[32m[20221213 21:01:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.00
[32m[20221213 21:01:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.00
[32m[20221213 21:01:12 @agent_ppo2.py:143][0m Total time:       5.62 min
[32m[20221213 21:01:12 @agent_ppo2.py:145][0m 548864 total steps have happened
[32m[20221213 21:01:12 @agent_ppo2.py:121][0m #------------------------ Iteration 268 --------------------------#
[32m[20221213 21:01:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:12 @agent_ppo2.py:185][0m |          -0.0019 |          71.3878 |          18.1227 |
[32m[20221213 21:01:12 @agent_ppo2.py:185][0m |          -0.0012 |          71.1498 |          18.1176 |
[32m[20221213 21:01:13 @agent_ppo2.py:185][0m |           0.0181 |          80.9937 |          18.1114 |
[32m[20221213 21:01:13 @agent_ppo2.py:185][0m |          -0.0069 |          70.5754 |          18.1089 |
[32m[20221213 21:01:13 @agent_ppo2.py:185][0m |           0.0024 |          70.8381 |          18.1081 |
[32m[20221213 21:01:13 @agent_ppo2.py:185][0m |          -0.0029 |          70.1591 |          18.1041 |
[32m[20221213 21:01:13 @agent_ppo2.py:185][0m |          -0.0054 |          70.1185 |          18.1070 |
[32m[20221213 21:01:13 @agent_ppo2.py:185][0m |          -0.0034 |          70.0725 |          18.1063 |
[32m[20221213 21:01:13 @agent_ppo2.py:185][0m |          -0.0036 |          69.8362 |          18.0985 |
[32m[20221213 21:01:13 @agent_ppo2.py:185][0m |           0.0031 |          72.9197 |          18.1058 |
[32m[20221213 21:01:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:01:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.20
[32m[20221213 21:01:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.00
[32m[20221213 21:01:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.00
[32m[20221213 21:01:13 @agent_ppo2.py:143][0m Total time:       5.64 min
[32m[20221213 21:01:13 @agent_ppo2.py:145][0m 550912 total steps have happened
[32m[20221213 21:01:13 @agent_ppo2.py:121][0m #------------------------ Iteration 269 --------------------------#
[32m[20221213 21:01:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:14 @agent_ppo2.py:185][0m |          -0.0024 |          70.1456 |          18.2833 |
[32m[20221213 21:01:14 @agent_ppo2.py:185][0m |          -0.0045 |          69.4265 |          18.2669 |
[32m[20221213 21:01:14 @agent_ppo2.py:185][0m |          -0.0075 |          69.2354 |          18.2617 |
[32m[20221213 21:01:14 @agent_ppo2.py:185][0m |          -0.0084 |          69.0692 |          18.2595 |
[32m[20221213 21:01:14 @agent_ppo2.py:185][0m |          -0.0076 |          68.8732 |          18.2632 |
[32m[20221213 21:01:14 @agent_ppo2.py:185][0m |          -0.0039 |          69.1721 |          18.2598 |
[32m[20221213 21:01:14 @agent_ppo2.py:185][0m |          -0.0084 |          68.6493 |          18.2683 |
[32m[20221213 21:01:14 @agent_ppo2.py:185][0m |           0.0010 |          69.8999 |          18.2669 |
[32m[20221213 21:01:14 @agent_ppo2.py:185][0m |          -0.0078 |          68.4034 |          18.2537 |
[32m[20221213 21:01:14 @agent_ppo2.py:185][0m |          -0.0037 |          68.7796 |          18.2660 |
[32m[20221213 21:01:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:01:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 500.80
[32m[20221213 21:01:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 543.00
[32m[20221213 21:01:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.00
[32m[20221213 21:01:14 @agent_ppo2.py:143][0m Total time:       5.66 min
[32m[20221213 21:01:14 @agent_ppo2.py:145][0m 552960 total steps have happened
[32m[20221213 21:01:14 @agent_ppo2.py:121][0m #------------------------ Iteration 270 --------------------------#
[32m[20221213 21:01:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:01:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:15 @agent_ppo2.py:185][0m |           0.0072 |          79.9352 |          18.2156 |
[32m[20221213 21:01:15 @agent_ppo2.py:185][0m |          -0.0052 |          74.1299 |          18.1963 |
[32m[20221213 21:01:15 @agent_ppo2.py:185][0m |          -0.0047 |          73.8594 |          18.1952 |
[32m[20221213 21:01:15 @agent_ppo2.py:185][0m |          -0.0079 |          73.8433 |          18.1856 |
[32m[20221213 21:01:15 @agent_ppo2.py:185][0m |          -0.0062 |          73.5337 |          18.1904 |
[32m[20221213 21:01:15 @agent_ppo2.py:185][0m |          -0.0016 |          73.6196 |          18.1842 |
[32m[20221213 21:01:15 @agent_ppo2.py:185][0m |          -0.0053 |          73.3104 |          18.1838 |
[32m[20221213 21:01:15 @agent_ppo2.py:185][0m |          -0.0013 |          73.7892 |          18.1839 |
[32m[20221213 21:01:15 @agent_ppo2.py:185][0m |          -0.0083 |          73.1455 |          18.1803 |
[32m[20221213 21:01:16 @agent_ppo2.py:185][0m |          -0.0043 |          73.2002 |          18.1850 |
[32m[20221213 21:01:16 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:01:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.40
[32m[20221213 21:01:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 589.00
[32m[20221213 21:01:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.00
[32m[20221213 21:01:16 @agent_ppo2.py:143][0m Total time:       5.68 min
[32m[20221213 21:01:16 @agent_ppo2.py:145][0m 555008 total steps have happened
[32m[20221213 21:01:16 @agent_ppo2.py:121][0m #------------------------ Iteration 271 --------------------------#
[32m[20221213 21:01:16 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:01:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:16 @agent_ppo2.py:185][0m |          -0.0009 |          71.2648 |          18.1723 |
[32m[20221213 21:01:16 @agent_ppo2.py:185][0m |          -0.0063 |          70.3779 |          18.1442 |
[32m[20221213 21:01:16 @agent_ppo2.py:185][0m |          -0.0048 |          70.0000 |          18.1317 |
[32m[20221213 21:01:16 @agent_ppo2.py:185][0m |          -0.0053 |          69.6305 |          18.1436 |
[32m[20221213 21:01:16 @agent_ppo2.py:185][0m |           0.0053 |          78.0813 |          18.1447 |
[32m[20221213 21:01:16 @agent_ppo2.py:185][0m |          -0.0059 |          69.3524 |          18.1366 |
[32m[20221213 21:01:17 @agent_ppo2.py:185][0m |           0.0145 |          80.4000 |          18.1290 |
[32m[20221213 21:01:17 @agent_ppo2.py:185][0m |          -0.0070 |          69.2293 |          18.1433 |
[32m[20221213 21:01:17 @agent_ppo2.py:185][0m |          -0.0053 |          68.9765 |          18.1462 |
[32m[20221213 21:01:17 @agent_ppo2.py:185][0m |          -0.0086 |          68.9387 |          18.1358 |
[32m[20221213 21:01:17 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:01:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.20
[32m[20221213 21:01:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.00
[32m[20221213 21:01:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.00
[32m[20221213 21:01:17 @agent_ppo2.py:143][0m Total time:       5.70 min
[32m[20221213 21:01:17 @agent_ppo2.py:145][0m 557056 total steps have happened
[32m[20221213 21:01:17 @agent_ppo2.py:121][0m #------------------------ Iteration 272 --------------------------#
[32m[20221213 21:01:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:17 @agent_ppo2.py:185][0m |           0.0023 |          74.4916 |          18.1778 |
[32m[20221213 21:01:17 @agent_ppo2.py:185][0m |          -0.0016 |          73.1933 |          18.1652 |
[32m[20221213 21:01:17 @agent_ppo2.py:185][0m |          -0.0051 |          73.0062 |          18.1596 |
[32m[20221213 21:01:17 @agent_ppo2.py:185][0m |          -0.0041 |          72.7496 |          18.1579 |
[32m[20221213 21:01:18 @agent_ppo2.py:185][0m |          -0.0091 |          72.6966 |          18.1559 |
[32m[20221213 21:01:18 @agent_ppo2.py:185][0m |          -0.0051 |          72.5314 |          18.1513 |
[32m[20221213 21:01:18 @agent_ppo2.py:185][0m |           0.0053 |          76.7654 |          18.1449 |
[32m[20221213 21:01:18 @agent_ppo2.py:185][0m |          -0.0019 |          75.1873 |          18.1496 |
[32m[20221213 21:01:18 @agent_ppo2.py:185][0m |           0.0049 |          75.1247 |          18.1444 |
[32m[20221213 21:01:18 @agent_ppo2.py:185][0m |          -0.0062 |          72.3448 |          18.1548 |
[32m[20221213 21:01:18 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:01:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.60
[32m[20221213 21:01:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.00
[32m[20221213 21:01:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.00
[32m[20221213 21:01:18 @agent_ppo2.py:143][0m Total time:       5.72 min
[32m[20221213 21:01:18 @agent_ppo2.py:145][0m 559104 total steps have happened
[32m[20221213 21:01:18 @agent_ppo2.py:121][0m #------------------------ Iteration 273 --------------------------#
[32m[20221213 21:01:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:01:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:18 @agent_ppo2.py:185][0m |          -0.0014 |          72.1514 |          18.1548 |
[32m[20221213 21:01:19 @agent_ppo2.py:185][0m |          -0.0020 |          71.5938 |          18.1372 |
[32m[20221213 21:01:19 @agent_ppo2.py:185][0m |          -0.0048 |          71.1494 |          18.1337 |
[32m[20221213 21:01:19 @agent_ppo2.py:185][0m |          -0.0079 |          71.0876 |          18.1279 |
[32m[20221213 21:01:19 @agent_ppo2.py:185][0m |          -0.0036 |          70.8338 |          18.1245 |
[32m[20221213 21:01:19 @agent_ppo2.py:185][0m |          -0.0032 |          70.6669 |          18.1208 |
[32m[20221213 21:01:19 @agent_ppo2.py:185][0m |          -0.0031 |          70.5996 |          18.1171 |
[32m[20221213 21:01:19 @agent_ppo2.py:185][0m |          -0.0013 |          70.4951 |          18.1266 |
[32m[20221213 21:01:19 @agent_ppo2.py:185][0m |          -0.0079 |          70.4491 |          18.1224 |
[32m[20221213 21:01:19 @agent_ppo2.py:185][0m |          -0.0048 |          70.3413 |          18.1128 |
[32m[20221213 21:01:19 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:01:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.00
[32m[20221213 21:01:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.00
[32m[20221213 21:01:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.00
[32m[20221213 21:01:19 @agent_ppo2.py:143][0m Total time:       5.74 min
[32m[20221213 21:01:19 @agent_ppo2.py:145][0m 561152 total steps have happened
[32m[20221213 21:01:19 @agent_ppo2.py:121][0m #------------------------ Iteration 274 --------------------------#
[32m[20221213 21:01:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:20 @agent_ppo2.py:185][0m |           0.0008 |          72.8876 |          18.1043 |
[32m[20221213 21:01:20 @agent_ppo2.py:185][0m |          -0.0019 |          72.3181 |          18.0929 |
[32m[20221213 21:01:20 @agent_ppo2.py:185][0m |          -0.0064 |          72.0545 |          18.0869 |
[32m[20221213 21:01:20 @agent_ppo2.py:185][0m |          -0.0051 |          71.8831 |          18.0841 |
[32m[20221213 21:01:20 @agent_ppo2.py:185][0m |          -0.0069 |          71.7236 |          18.0891 |
[32m[20221213 21:01:20 @agent_ppo2.py:185][0m |          -0.0048 |          71.4431 |          18.0910 |
[32m[20221213 21:01:20 @agent_ppo2.py:185][0m |          -0.0084 |          71.5575 |          18.0837 |
[32m[20221213 21:01:20 @agent_ppo2.py:185][0m |          -0.0070 |          71.3421 |          18.0846 |
[32m[20221213 21:01:20 @agent_ppo2.py:185][0m |          -0.0061 |          71.3526 |          18.0905 |
[32m[20221213 21:01:20 @agent_ppo2.py:185][0m |          -0.0059 |          71.2295 |          18.0842 |
[32m[20221213 21:01:20 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:01:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.80
[32m[20221213 21:01:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.00
[32m[20221213 21:01:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.00
[32m[20221213 21:01:21 @agent_ppo2.py:143][0m Total time:       5.76 min
[32m[20221213 21:01:21 @agent_ppo2.py:145][0m 563200 total steps have happened
[32m[20221213 21:01:21 @agent_ppo2.py:121][0m #------------------------ Iteration 275 --------------------------#
[32m[20221213 21:01:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:21 @agent_ppo2.py:185][0m |           0.0025 |          70.9529 |          18.1750 |
[32m[20221213 21:01:21 @agent_ppo2.py:185][0m |          -0.0045 |          69.6681 |          18.1615 |
[32m[20221213 21:01:21 @agent_ppo2.py:185][0m |          -0.0032 |          69.2735 |          18.1612 |
[32m[20221213 21:01:21 @agent_ppo2.py:185][0m |          -0.0028 |          69.0365 |          18.1524 |
[32m[20221213 21:01:21 @agent_ppo2.py:185][0m |          -0.0065 |          68.9536 |          18.1561 |
[32m[20221213 21:01:21 @agent_ppo2.py:185][0m |          -0.0054 |          68.8948 |          18.1488 |
[32m[20221213 21:01:21 @agent_ppo2.py:185][0m |          -0.0076 |          68.8300 |          18.1502 |
[32m[20221213 21:01:22 @agent_ppo2.py:185][0m |          -0.0075 |          68.7396 |          18.1568 |
[32m[20221213 21:01:22 @agent_ppo2.py:185][0m |          -0.0090 |          68.6703 |          18.1565 |
[32m[20221213 21:01:22 @agent_ppo2.py:185][0m |          -0.0086 |          68.5945 |          18.1443 |
[32m[20221213 21:01:22 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:01:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 486.20
[32m[20221213 21:01:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.00
[32m[20221213 21:01:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.00
[32m[20221213 21:01:22 @agent_ppo2.py:143][0m Total time:       5.79 min
[32m[20221213 21:01:22 @agent_ppo2.py:145][0m 565248 total steps have happened
[32m[20221213 21:01:22 @agent_ppo2.py:121][0m #------------------------ Iteration 276 --------------------------#
[32m[20221213 21:01:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:22 @agent_ppo2.py:185][0m |           0.0021 |          73.2829 |          18.0479 |
[32m[20221213 21:01:22 @agent_ppo2.py:185][0m |          -0.0051 |          70.6590 |          18.0290 |
[32m[20221213 21:01:22 @agent_ppo2.py:185][0m |           0.0045 |          74.0646 |          18.0054 |
[32m[20221213 21:01:22 @agent_ppo2.py:185][0m |          -0.0060 |          70.4557 |          18.0180 |
[32m[20221213 21:01:22 @agent_ppo2.py:185][0m |          -0.0082 |          70.1844 |          18.0082 |
[32m[20221213 21:01:23 @agent_ppo2.py:185][0m |          -0.0080 |          70.1210 |          18.0159 |
[32m[20221213 21:01:23 @agent_ppo2.py:185][0m |          -0.0017 |          70.9302 |          18.0165 |
[32m[20221213 21:01:23 @agent_ppo2.py:185][0m |          -0.0077 |          69.9451 |          18.0058 |
[32m[20221213 21:01:23 @agent_ppo2.py:185][0m |          -0.0068 |          69.8652 |          18.0119 |
[32m[20221213 21:01:23 @agent_ppo2.py:185][0m |          -0.0050 |          69.8120 |          18.0091 |
[32m[20221213 21:01:23 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:01:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.80
[32m[20221213 21:01:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.00
[32m[20221213 21:01:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.00
[32m[20221213 21:01:23 @agent_ppo2.py:143][0m Total time:       5.81 min
[32m[20221213 21:01:23 @agent_ppo2.py:145][0m 567296 total steps have happened
[32m[20221213 21:01:23 @agent_ppo2.py:121][0m #------------------------ Iteration 277 --------------------------#
[32m[20221213 21:01:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:23 @agent_ppo2.py:185][0m |           0.0002 |          72.9863 |          18.1377 |
[32m[20221213 21:01:23 @agent_ppo2.py:185][0m |          -0.0002 |          72.6408 |          18.1294 |
[32m[20221213 21:01:24 @agent_ppo2.py:185][0m |          -0.0048 |          72.3829 |          18.1178 |
[32m[20221213 21:01:24 @agent_ppo2.py:185][0m |          -0.0021 |          72.2415 |          18.1115 |
[32m[20221213 21:01:24 @agent_ppo2.py:185][0m |           0.0035 |          76.5459 |          18.1117 |
[32m[20221213 21:01:24 @agent_ppo2.py:185][0m |          -0.0054 |          72.0658 |          18.1037 |
[32m[20221213 21:01:24 @agent_ppo2.py:185][0m |          -0.0047 |          71.8330 |          18.1142 |
[32m[20221213 21:01:24 @agent_ppo2.py:185][0m |          -0.0061 |          71.8707 |          18.1101 |
[32m[20221213 21:01:24 @agent_ppo2.py:185][0m |          -0.0064 |          71.7549 |          18.1139 |
[32m[20221213 21:01:24 @agent_ppo2.py:185][0m |          -0.0046 |          71.6761 |          18.0982 |
[32m[20221213 21:01:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:01:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 500.20
[32m[20221213 21:01:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.00
[32m[20221213 21:01:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.00
[32m[20221213 21:01:24 @agent_ppo2.py:143][0m Total time:       5.83 min
[32m[20221213 21:01:24 @agent_ppo2.py:145][0m 569344 total steps have happened
[32m[20221213 21:01:24 @agent_ppo2.py:121][0m #------------------------ Iteration 278 --------------------------#
[32m[20221213 21:01:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:25 @agent_ppo2.py:185][0m |           0.0027 |          72.8004 |          18.1354 |
[32m[20221213 21:01:25 @agent_ppo2.py:185][0m |          -0.0010 |          71.7229 |          18.1273 |
[32m[20221213 21:01:25 @agent_ppo2.py:185][0m |          -0.0053 |          71.2499 |          18.1232 |
[32m[20221213 21:01:25 @agent_ppo2.py:185][0m |          -0.0046 |          71.1194 |          18.1115 |
[32m[20221213 21:01:25 @agent_ppo2.py:185][0m |          -0.0055 |          70.9780 |          18.1266 |
[32m[20221213 21:01:25 @agent_ppo2.py:185][0m |          -0.0072 |          71.0280 |          18.1171 |
[32m[20221213 21:01:25 @agent_ppo2.py:185][0m |          -0.0068 |          71.0378 |          18.1209 |
[32m[20221213 21:01:25 @agent_ppo2.py:185][0m |          -0.0071 |          70.9326 |          18.1245 |
[32m[20221213 21:01:25 @agent_ppo2.py:185][0m |          -0.0064 |          70.8238 |          18.1194 |
[32m[20221213 21:01:25 @agent_ppo2.py:185][0m |          -0.0078 |          70.7067 |          18.1269 |
[32m[20221213 21:01:25 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:01:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 508.60
[32m[20221213 21:01:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.00
[32m[20221213 21:01:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.00
[32m[20221213 21:01:25 @agent_ppo2.py:143][0m Total time:       5.85 min
[32m[20221213 21:01:25 @agent_ppo2.py:145][0m 571392 total steps have happened
[32m[20221213 21:01:25 @agent_ppo2.py:121][0m #------------------------ Iteration 279 --------------------------#
[32m[20221213 21:01:26 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:01:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:26 @agent_ppo2.py:185][0m |          -0.0023 |          74.1301 |          18.1249 |
[32m[20221213 21:01:26 @agent_ppo2.py:185][0m |          -0.0029 |          72.6316 |          18.1123 |
[32m[20221213 21:01:26 @agent_ppo2.py:185][0m |          -0.0054 |          72.1496 |          18.1038 |
[32m[20221213 21:01:26 @agent_ppo2.py:185][0m |          -0.0036 |          71.7922 |          18.1022 |
[32m[20221213 21:01:26 @agent_ppo2.py:185][0m |          -0.0060 |          71.2332 |          18.1018 |
[32m[20221213 21:01:26 @agent_ppo2.py:185][0m |          -0.0058 |          70.9275 |          18.0917 |
[32m[20221213 21:01:26 @agent_ppo2.py:185][0m |          -0.0043 |          71.0184 |          18.0949 |
[32m[20221213 21:01:26 @agent_ppo2.py:185][0m |          -0.0030 |          70.5475 |          18.0978 |
[32m[20221213 21:01:26 @agent_ppo2.py:185][0m |          -0.0045 |          70.0193 |          18.1040 |
[32m[20221213 21:01:27 @agent_ppo2.py:185][0m |          -0.0083 |          69.7957 |          18.1003 |
[32m[20221213 21:01:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:01:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.60
[32m[20221213 21:01:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.00
[32m[20221213 21:01:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.00
[32m[20221213 21:01:27 @agent_ppo2.py:143][0m Total time:       5.87 min
[32m[20221213 21:01:27 @agent_ppo2.py:145][0m 573440 total steps have happened
[32m[20221213 21:01:27 @agent_ppo2.py:121][0m #------------------------ Iteration 280 --------------------------#
[32m[20221213 21:01:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:27 @agent_ppo2.py:185][0m |          -0.0005 |          76.5503 |          18.1368 |
[32m[20221213 21:01:27 @agent_ppo2.py:185][0m |          -0.0017 |          75.9963 |          18.1120 |
[32m[20221213 21:01:27 @agent_ppo2.py:185][0m |          -0.0039 |          75.4603 |          18.1012 |
[32m[20221213 21:01:27 @agent_ppo2.py:185][0m |          -0.0062 |          75.4022 |          18.1062 |
[32m[20221213 21:01:27 @agent_ppo2.py:185][0m |          -0.0039 |          74.9005 |          18.0996 |
[32m[20221213 21:01:27 @agent_ppo2.py:185][0m |          -0.0038 |          74.8624 |          18.0969 |
[32m[20221213 21:01:27 @agent_ppo2.py:185][0m |          -0.0051 |          74.6460 |          18.0914 |
[32m[20221213 21:01:28 @agent_ppo2.py:185][0m |          -0.0069 |          74.5973 |          18.0913 |
[32m[20221213 21:01:28 @agent_ppo2.py:185][0m |          -0.0029 |          74.4091 |          18.1011 |
[32m[20221213 21:01:28 @agent_ppo2.py:185][0m |          -0.0083 |          74.5236 |          18.1010 |
[32m[20221213 21:01:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:01:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.80
[32m[20221213 21:01:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.00
[32m[20221213 21:01:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.00
[32m[20221213 21:01:28 @agent_ppo2.py:143][0m Total time:       5.89 min
[32m[20221213 21:01:28 @agent_ppo2.py:145][0m 575488 total steps have happened
[32m[20221213 21:01:28 @agent_ppo2.py:121][0m #------------------------ Iteration 281 --------------------------#
[32m[20221213 21:01:28 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:01:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:28 @agent_ppo2.py:185][0m |          -0.0013 |          74.4645 |          18.1518 |
[32m[20221213 21:01:28 @agent_ppo2.py:185][0m |          -0.0050 |          73.6407 |          18.1454 |
[32m[20221213 21:01:28 @agent_ppo2.py:185][0m |          -0.0061 |          73.3057 |          18.1349 |
[32m[20221213 21:01:28 @agent_ppo2.py:185][0m |          -0.0027 |          73.2856 |          18.1328 |
[32m[20221213 21:01:29 @agent_ppo2.py:185][0m |          -0.0018 |          73.5600 |          18.1376 |
[32m[20221213 21:01:29 @agent_ppo2.py:185][0m |          -0.0042 |          72.6699 |          18.1310 |
[32m[20221213 21:01:29 @agent_ppo2.py:185][0m |          -0.0004 |          74.0109 |          18.1321 |
[32m[20221213 21:01:29 @agent_ppo2.py:185][0m |          -0.0043 |          72.3415 |          18.1368 |
[32m[20221213 21:01:29 @agent_ppo2.py:185][0m |          -0.0082 |          72.3233 |          18.1294 |
[32m[20221213 21:01:29 @agent_ppo2.py:185][0m |          -0.0080 |          72.0994 |          18.1339 |
[32m[20221213 21:01:29 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:01:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 512.20
[32m[20221213 21:01:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.00
[32m[20221213 21:01:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.00
[32m[20221213 21:01:29 @agent_ppo2.py:143][0m Total time:       5.91 min
[32m[20221213 21:01:29 @agent_ppo2.py:145][0m 577536 total steps have happened
[32m[20221213 21:01:29 @agent_ppo2.py:121][0m #------------------------ Iteration 282 --------------------------#
[32m[20221213 21:01:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:29 @agent_ppo2.py:185][0m |           0.0003 |          74.1129 |          18.1370 |
[32m[20221213 21:01:29 @agent_ppo2.py:185][0m |          -0.0031 |          73.4971 |          18.1137 |
[32m[20221213 21:01:30 @agent_ppo2.py:185][0m |          -0.0067 |          73.1450 |          18.1021 |
[32m[20221213 21:01:30 @agent_ppo2.py:185][0m |          -0.0074 |          72.9804 |          18.0933 |
[32m[20221213 21:01:30 @agent_ppo2.py:185][0m |          -0.0028 |          72.8132 |          18.0889 |
[32m[20221213 21:01:30 @agent_ppo2.py:185][0m |          -0.0025 |          72.8267 |          18.0905 |
[32m[20221213 21:01:30 @agent_ppo2.py:185][0m |           0.0058 |          78.3152 |          18.1029 |
[32m[20221213 21:01:30 @agent_ppo2.py:185][0m |          -0.0069 |          72.4546 |          18.0952 |
[32m[20221213 21:01:30 @agent_ppo2.py:185][0m |          -0.0048 |          72.2738 |          18.0887 |
[32m[20221213 21:01:30 @agent_ppo2.py:185][0m |          -0.0081 |          72.3120 |          18.0878 |
[32m[20221213 21:01:30 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:01:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.60
[32m[20221213 21:01:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 565.00
[32m[20221213 21:01:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.00
[32m[20221213 21:01:30 @agent_ppo2.py:143][0m Total time:       5.93 min
[32m[20221213 21:01:30 @agent_ppo2.py:145][0m 579584 total steps have happened
[32m[20221213 21:01:30 @agent_ppo2.py:121][0m #------------------------ Iteration 283 --------------------------#
[32m[20221213 21:01:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:01:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:31 @agent_ppo2.py:185][0m |          -0.0018 |          71.1966 |          18.1322 |
[32m[20221213 21:01:31 @agent_ppo2.py:185][0m |          -0.0008 |          71.3112 |          18.1252 |
[32m[20221213 21:01:31 @agent_ppo2.py:185][0m |           0.0025 |          72.1755 |          18.1106 |
[32m[20221213 21:01:31 @agent_ppo2.py:185][0m |          -0.0060 |          70.4417 |          18.1129 |
[32m[20221213 21:01:31 @agent_ppo2.py:185][0m |          -0.0045 |          70.3957 |          18.1054 |
[32m[20221213 21:01:31 @agent_ppo2.py:185][0m |          -0.0000 |          71.6222 |          18.1033 |
[32m[20221213 21:01:31 @agent_ppo2.py:185][0m |           0.0073 |          78.7398 |          18.0949 |
[32m[20221213 21:01:31 @agent_ppo2.py:185][0m |          -0.0049 |          70.1232 |          18.0968 |
[32m[20221213 21:01:31 @agent_ppo2.py:185][0m |          -0.0078 |          70.0611 |          18.0991 |
[32m[20221213 21:01:31 @agent_ppo2.py:185][0m |          -0.0066 |          69.9416 |          18.1003 |
[32m[20221213 21:01:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:01:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 514.00
[32m[20221213 21:01:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.00
[32m[20221213 21:01:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.00
[32m[20221213 21:01:31 @agent_ppo2.py:143][0m Total time:       5.95 min
[32m[20221213 21:01:31 @agent_ppo2.py:145][0m 581632 total steps have happened
[32m[20221213 21:01:31 @agent_ppo2.py:121][0m #------------------------ Iteration 284 --------------------------#
[32m[20221213 21:01:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:32 @agent_ppo2.py:185][0m |          -0.0004 |          75.3234 |          18.0963 |
[32m[20221213 21:01:32 @agent_ppo2.py:185][0m |          -0.0030 |          74.1966 |          18.0769 |
[32m[20221213 21:01:32 @agent_ppo2.py:185][0m |          -0.0033 |          73.8109 |          18.0700 |
[32m[20221213 21:01:32 @agent_ppo2.py:185][0m |          -0.0065 |          73.4311 |          18.0657 |
[32m[20221213 21:01:32 @agent_ppo2.py:185][0m |          -0.0073 |          73.2420 |          18.0658 |
[32m[20221213 21:01:32 @agent_ppo2.py:185][0m |          -0.0032 |          73.5009 |          18.0610 |
[32m[20221213 21:01:32 @agent_ppo2.py:185][0m |          -0.0055 |          72.8062 |          18.0542 |
[32m[20221213 21:01:32 @agent_ppo2.py:185][0m |          -0.0087 |          72.6793 |          18.0564 |
[32m[20221213 21:01:32 @agent_ppo2.py:185][0m |          -0.0070 |          72.4547 |          18.0515 |
[32m[20221213 21:01:33 @agent_ppo2.py:185][0m |           0.0153 |          85.5862 |          18.0442 |
[32m[20221213 21:01:33 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:01:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.80
[32m[20221213 21:01:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.00
[32m[20221213 21:01:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.00
[32m[20221213 21:01:33 @agent_ppo2.py:143][0m Total time:       5.97 min
[32m[20221213 21:01:33 @agent_ppo2.py:145][0m 583680 total steps have happened
[32m[20221213 21:01:33 @agent_ppo2.py:121][0m #------------------------ Iteration 285 --------------------------#
[32m[20221213 21:01:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:33 @agent_ppo2.py:185][0m |          -0.0012 |          78.7947 |          18.1403 |
[32m[20221213 21:01:33 @agent_ppo2.py:185][0m |           0.0080 |          84.7282 |          18.1397 |
[32m[20221213 21:01:33 @agent_ppo2.py:185][0m |          -0.0045 |          77.5137 |          18.1234 |
[32m[20221213 21:01:33 @agent_ppo2.py:185][0m |          -0.0086 |          77.3403 |          18.1211 |
[32m[20221213 21:01:33 @agent_ppo2.py:185][0m |          -0.0095 |          76.8888 |          18.1238 |
[32m[20221213 21:01:33 @agent_ppo2.py:185][0m |          -0.0059 |          76.5562 |          18.1252 |
[32m[20221213 21:01:33 @agent_ppo2.py:185][0m |          -0.0046 |          76.8259 |          18.1145 |
[32m[20221213 21:01:34 @agent_ppo2.py:185][0m |          -0.0077 |          76.7411 |          18.1204 |
[32m[20221213 21:01:34 @agent_ppo2.py:185][0m |          -0.0089 |          76.3626 |          18.1097 |
[32m[20221213 21:01:34 @agent_ppo2.py:185][0m |          -0.0093 |          76.1971 |          18.1070 |
[32m[20221213 21:01:34 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:01:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.40
[32m[20221213 21:01:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.00
[32m[20221213 21:01:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.00
[32m[20221213 21:01:34 @agent_ppo2.py:143][0m Total time:       5.99 min
[32m[20221213 21:01:34 @agent_ppo2.py:145][0m 585728 total steps have happened
[32m[20221213 21:01:34 @agent_ppo2.py:121][0m #------------------------ Iteration 286 --------------------------#
[32m[20221213 21:01:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:01:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:34 @agent_ppo2.py:185][0m |          -0.0024 |          79.9586 |          18.1662 |
[32m[20221213 21:01:34 @agent_ppo2.py:185][0m |          -0.0017 |          79.9886 |          18.1573 |
[32m[20221213 21:01:34 @agent_ppo2.py:185][0m |           0.0051 |          86.0860 |          18.1527 |
[32m[20221213 21:01:34 @agent_ppo2.py:185][0m |           0.0068 |          86.3889 |          18.1242 |
[32m[20221213 21:01:35 @agent_ppo2.py:185][0m |          -0.0055 |          78.5303 |          18.1332 |
[32m[20221213 21:01:35 @agent_ppo2.py:185][0m |           0.0006 |          80.1141 |          18.1440 |
[32m[20221213 21:01:35 @agent_ppo2.py:185][0m |          -0.0066 |          78.1941 |          18.1421 |
[32m[20221213 21:01:35 @agent_ppo2.py:185][0m |          -0.0052 |          77.9377 |          18.1386 |
[32m[20221213 21:01:35 @agent_ppo2.py:185][0m |          -0.0069 |          77.8119 |          18.1441 |
[32m[20221213 21:01:35 @agent_ppo2.py:185][0m |          -0.0080 |          77.7371 |          18.1354 |
[32m[20221213 21:01:35 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:01:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.20
[32m[20221213 21:01:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.00
[32m[20221213 21:01:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.00
[32m[20221213 21:01:35 @agent_ppo2.py:143][0m Total time:       6.01 min
[32m[20221213 21:01:35 @agent_ppo2.py:145][0m 587776 total steps have happened
[32m[20221213 21:01:35 @agent_ppo2.py:121][0m #------------------------ Iteration 287 --------------------------#
[32m[20221213 21:01:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:35 @agent_ppo2.py:185][0m |          -0.0018 |          79.1882 |          18.2569 |
[32m[20221213 21:01:35 @agent_ppo2.py:185][0m |          -0.0054 |          78.2027 |          18.2409 |
[32m[20221213 21:01:36 @agent_ppo2.py:185][0m |          -0.0075 |          77.7283 |          18.2212 |
[32m[20221213 21:01:36 @agent_ppo2.py:185][0m |          -0.0013 |          78.3893 |          18.2186 |
[32m[20221213 21:01:36 @agent_ppo2.py:185][0m |          -0.0087 |          77.2620 |          18.2145 |
[32m[20221213 21:01:36 @agent_ppo2.py:185][0m |          -0.0084 |          77.2193 |          18.2206 |
[32m[20221213 21:01:36 @agent_ppo2.py:185][0m |          -0.0093 |          77.0900 |          18.2214 |
[32m[20221213 21:01:36 @agent_ppo2.py:185][0m |          -0.0086 |          77.0469 |          18.2238 |
[32m[20221213 21:01:36 @agent_ppo2.py:185][0m |          -0.0069 |          77.6893 |          18.2146 |
[32m[20221213 21:01:36 @agent_ppo2.py:185][0m |          -0.0080 |          76.5988 |          18.2214 |
[32m[20221213 21:01:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:01:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.40
[32m[20221213 21:01:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.00
[32m[20221213 21:01:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 547.00
[32m[20221213 21:01:36 @agent_ppo2.py:143][0m Total time:       6.03 min
[32m[20221213 21:01:36 @agent_ppo2.py:145][0m 589824 total steps have happened
[32m[20221213 21:01:36 @agent_ppo2.py:121][0m #------------------------ Iteration 288 --------------------------#
[32m[20221213 21:01:36 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:01:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:37 @agent_ppo2.py:185][0m |          -0.0012 |          75.1872 |          18.2271 |
[32m[20221213 21:01:37 @agent_ppo2.py:185][0m |          -0.0038 |          73.4573 |          18.2162 |
[32m[20221213 21:01:37 @agent_ppo2.py:185][0m |           0.0229 |          87.9892 |          18.2002 |
[32m[20221213 21:01:37 @agent_ppo2.py:185][0m |          -0.0048 |          72.8256 |          18.1810 |
[32m[20221213 21:01:37 @agent_ppo2.py:185][0m |          -0.0062 |          72.4450 |          18.1891 |
[32m[20221213 21:01:37 @agent_ppo2.py:185][0m |          -0.0072 |          72.2497 |          18.1941 |
[32m[20221213 21:01:37 @agent_ppo2.py:185][0m |          -0.0057 |          72.1766 |          18.1930 |
[32m[20221213 21:01:37 @agent_ppo2.py:185][0m |          -0.0059 |          71.9876 |          18.1854 |
[32m[20221213 21:01:37 @agent_ppo2.py:185][0m |           0.0011 |          75.1629 |          18.1952 |
[32m[20221213 21:01:37 @agent_ppo2.py:185][0m |          -0.0094 |          71.8682 |          18.1885 |
[32m[20221213 21:01:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:01:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.80
[32m[20221213 21:01:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.00
[32m[20221213 21:01:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.00
[32m[20221213 21:01:37 @agent_ppo2.py:143][0m Total time:       6.05 min
[32m[20221213 21:01:37 @agent_ppo2.py:145][0m 591872 total steps have happened
[32m[20221213 21:01:37 @agent_ppo2.py:121][0m #------------------------ Iteration 289 --------------------------#
[32m[20221213 21:01:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:38 @agent_ppo2.py:185][0m |          -0.0006 |          75.0466 |          18.3001 |
[32m[20221213 21:01:38 @agent_ppo2.py:185][0m |          -0.0037 |          74.0961 |          18.2837 |
[32m[20221213 21:01:38 @agent_ppo2.py:185][0m |          -0.0041 |          73.7718 |          18.2747 |
[32m[20221213 21:01:38 @agent_ppo2.py:185][0m |          -0.0079 |          73.4845 |          18.2795 |
[32m[20221213 21:01:38 @agent_ppo2.py:185][0m |           0.0017 |          78.2855 |          18.2702 |
[32m[20221213 21:01:38 @agent_ppo2.py:185][0m |          -0.0061 |          72.9043 |          18.2671 |
[32m[20221213 21:01:38 @agent_ppo2.py:185][0m |          -0.0053 |          72.9855 |          18.2701 |
[32m[20221213 21:01:38 @agent_ppo2.py:185][0m |          -0.0064 |          72.5998 |          18.2724 |
[32m[20221213 21:01:38 @agent_ppo2.py:185][0m |          -0.0073 |          72.4618 |          18.2668 |
[32m[20221213 21:01:39 @agent_ppo2.py:185][0m |          -0.0075 |          72.2653 |          18.2719 |
[32m[20221213 21:01:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:01:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 523.60
[32m[20221213 21:01:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 570.00
[32m[20221213 21:01:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.00
[32m[20221213 21:01:39 @agent_ppo2.py:143][0m Total time:       6.07 min
[32m[20221213 21:01:39 @agent_ppo2.py:145][0m 593920 total steps have happened
[32m[20221213 21:01:39 @agent_ppo2.py:121][0m #------------------------ Iteration 290 --------------------------#
[32m[20221213 21:01:39 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:01:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:39 @agent_ppo2.py:185][0m |          -0.0006 |          71.6950 |          18.0538 |
[32m[20221213 21:01:39 @agent_ppo2.py:185][0m |          -0.0063 |          71.0392 |          18.0476 |
[32m[20221213 21:01:39 @agent_ppo2.py:185][0m |          -0.0030 |          70.7614 |          18.0361 |
[32m[20221213 21:01:39 @agent_ppo2.py:185][0m |          -0.0051 |          70.4970 |          18.0251 |
[32m[20221213 21:01:39 @agent_ppo2.py:185][0m |          -0.0037 |          70.4974 |          18.0359 |
[32m[20221213 21:01:39 @agent_ppo2.py:185][0m |          -0.0084 |          70.3328 |          18.0297 |
[32m[20221213 21:01:40 @agent_ppo2.py:185][0m |          -0.0062 |          70.2253 |          18.0252 |
[32m[20221213 21:01:40 @agent_ppo2.py:185][0m |          -0.0028 |          70.2690 |          18.0277 |
[32m[20221213 21:01:40 @agent_ppo2.py:185][0m |          -0.0075 |          70.1045 |          18.0264 |
[32m[20221213 21:01:40 @agent_ppo2.py:185][0m |          -0.0057 |          69.9347 |          18.0286 |
[32m[20221213 21:01:40 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:01:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.80
[32m[20221213 21:01:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.00
[32m[20221213 21:01:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.00
[32m[20221213 21:01:40 @agent_ppo2.py:143][0m Total time:       6.09 min
[32m[20221213 21:01:40 @agent_ppo2.py:145][0m 595968 total steps have happened
[32m[20221213 21:01:40 @agent_ppo2.py:121][0m #------------------------ Iteration 291 --------------------------#
[32m[20221213 21:01:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:40 @agent_ppo2.py:185][0m |           0.0020 |          72.1100 |          18.2009 |
[32m[20221213 21:01:40 @agent_ppo2.py:185][0m |          -0.0051 |          71.7156 |          18.1744 |
[32m[20221213 21:01:40 @agent_ppo2.py:185][0m |          -0.0018 |          71.4155 |          18.1770 |
[32m[20221213 21:01:40 @agent_ppo2.py:185][0m |          -0.0067 |          71.3125 |          18.1796 |
[32m[20221213 21:01:41 @agent_ppo2.py:185][0m |          -0.0049 |          71.1948 |          18.1730 |
[32m[20221213 21:01:41 @agent_ppo2.py:185][0m |          -0.0039 |          70.9668 |          18.1851 |
[32m[20221213 21:01:41 @agent_ppo2.py:185][0m |          -0.0056 |          70.9248 |          18.1688 |
[32m[20221213 21:01:41 @agent_ppo2.py:185][0m |          -0.0063 |          70.6814 |          18.1739 |
[32m[20221213 21:01:41 @agent_ppo2.py:185][0m |          -0.0068 |          70.6503 |          18.1699 |
[32m[20221213 21:01:41 @agent_ppo2.py:185][0m |          -0.0031 |          70.8360 |          18.1696 |
[32m[20221213 21:01:41 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:01:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 500.80
[32m[20221213 21:01:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.00
[32m[20221213 21:01:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.00
[32m[20221213 21:01:41 @agent_ppo2.py:143][0m Total time:       6.11 min
[32m[20221213 21:01:41 @agent_ppo2.py:145][0m 598016 total steps have happened
[32m[20221213 21:01:41 @agent_ppo2.py:121][0m #------------------------ Iteration 292 --------------------------#
[32m[20221213 21:01:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:41 @agent_ppo2.py:185][0m |          -0.0027 |          76.2447 |          18.2883 |
[32m[20221213 21:01:42 @agent_ppo2.py:185][0m |          -0.0066 |          75.5204 |          18.2721 |
[32m[20221213 21:01:42 @agent_ppo2.py:185][0m |          -0.0041 |          75.1386 |          18.2589 |
[32m[20221213 21:01:42 @agent_ppo2.py:185][0m |           0.0015 |          78.0747 |          18.2577 |
[32m[20221213 21:01:42 @agent_ppo2.py:185][0m |          -0.0031 |          74.7572 |          18.2493 |
[32m[20221213 21:01:42 @agent_ppo2.py:185][0m |          -0.0088 |          74.6431 |          18.2488 |
[32m[20221213 21:01:42 @agent_ppo2.py:185][0m |          -0.0065 |          74.5034 |          18.2530 |
[32m[20221213 21:01:42 @agent_ppo2.py:185][0m |           0.0013 |          75.6625 |          18.2494 |
[32m[20221213 21:01:42 @agent_ppo2.py:185][0m |          -0.0069 |          74.2900 |          18.2561 |
[32m[20221213 21:01:42 @agent_ppo2.py:185][0m |          -0.0074 |          74.1169 |          18.2489 |
[32m[20221213 21:01:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:01:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.20
[32m[20221213 21:01:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.00
[32m[20221213 21:01:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.00
[32m[20221213 21:01:42 @agent_ppo2.py:143][0m Total time:       6.13 min
[32m[20221213 21:01:42 @agent_ppo2.py:145][0m 600064 total steps have happened
[32m[20221213 21:01:42 @agent_ppo2.py:121][0m #------------------------ Iteration 293 --------------------------#
[32m[20221213 21:01:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:43 @agent_ppo2.py:185][0m |           0.0053 |          79.8855 |          18.3140 |
[32m[20221213 21:01:43 @agent_ppo2.py:185][0m |          -0.0042 |          77.5672 |          18.2928 |
[32m[20221213 21:01:43 @agent_ppo2.py:185][0m |          -0.0036 |          77.3022 |          18.2982 |
[32m[20221213 21:01:43 @agent_ppo2.py:185][0m |          -0.0053 |          77.0699 |          18.2968 |
[32m[20221213 21:01:43 @agent_ppo2.py:185][0m |          -0.0047 |          76.9150 |          18.2911 |
[32m[20221213 21:01:43 @agent_ppo2.py:185][0m |          -0.0053 |          76.7298 |          18.2957 |
[32m[20221213 21:01:43 @agent_ppo2.py:185][0m |          -0.0059 |          76.6933 |          18.2925 |
[32m[20221213 21:01:43 @agent_ppo2.py:185][0m |          -0.0023 |          77.1656 |          18.2929 |
[32m[20221213 21:01:43 @agent_ppo2.py:185][0m |          -0.0047 |          76.8174 |          18.2900 |
[32m[20221213 21:01:43 @agent_ppo2.py:185][0m |          -0.0022 |          76.9373 |          18.2939 |
[32m[20221213 21:01:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:01:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.00
[32m[20221213 21:01:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.00
[32m[20221213 21:01:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.00
[32m[20221213 21:01:44 @agent_ppo2.py:143][0m Total time:       6.15 min
[32m[20221213 21:01:44 @agent_ppo2.py:145][0m 602112 total steps have happened
[32m[20221213 21:01:44 @agent_ppo2.py:121][0m #------------------------ Iteration 294 --------------------------#
[32m[20221213 21:01:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:44 @agent_ppo2.py:185][0m |          -0.0042 |          74.5832 |          18.2953 |
[32m[20221213 21:01:44 @agent_ppo2.py:185][0m |           0.0036 |          76.4666 |          18.2726 |
[32m[20221213 21:01:44 @agent_ppo2.py:185][0m |          -0.0064 |          73.3340 |          18.2578 |
[32m[20221213 21:01:44 @agent_ppo2.py:185][0m |          -0.0066 |          72.9273 |          18.2646 |
[32m[20221213 21:01:44 @agent_ppo2.py:185][0m |          -0.0037 |          72.7027 |          18.2651 |
[32m[20221213 21:01:44 @agent_ppo2.py:185][0m |          -0.0068 |          72.4501 |          18.2633 |
[32m[20221213 21:01:44 @agent_ppo2.py:185][0m |          -0.0068 |          72.3504 |          18.2669 |
[32m[20221213 21:01:45 @agent_ppo2.py:185][0m |          -0.0058 |          72.2808 |          18.2498 |
[32m[20221213 21:01:45 @agent_ppo2.py:185][0m |          -0.0019 |          74.2550 |          18.2490 |
[32m[20221213 21:01:45 @agent_ppo2.py:185][0m |          -0.0039 |          72.6466 |          18.2547 |
[32m[20221213 21:01:45 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:01:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.80
[32m[20221213 21:01:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.00
[32m[20221213 21:01:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 589.00
[32m[20221213 21:01:45 @agent_ppo2.py:143][0m Total time:       6.17 min
[32m[20221213 21:01:45 @agent_ppo2.py:145][0m 604160 total steps have happened
[32m[20221213 21:01:45 @agent_ppo2.py:121][0m #------------------------ Iteration 295 --------------------------#
[32m[20221213 21:01:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:45 @agent_ppo2.py:185][0m |          -0.0024 |          74.5140 |          18.1767 |
[32m[20221213 21:01:45 @agent_ppo2.py:185][0m |          -0.0029 |          74.1185 |          18.1538 |
[32m[20221213 21:01:45 @agent_ppo2.py:185][0m |          -0.0035 |          73.9615 |          18.1580 |
[32m[20221213 21:01:45 @agent_ppo2.py:185][0m |          -0.0067 |          73.8084 |          18.1493 |
[32m[20221213 21:01:45 @agent_ppo2.py:185][0m |          -0.0033 |          74.0210 |          18.1346 |
[32m[20221213 21:01:46 @agent_ppo2.py:185][0m |          -0.0070 |          73.6488 |          18.1488 |
[32m[20221213 21:01:46 @agent_ppo2.py:185][0m |          -0.0011 |          75.7671 |          18.1351 |
[32m[20221213 21:01:46 @agent_ppo2.py:185][0m |          -0.0071 |          73.6831 |          18.1342 |
[32m[20221213 21:01:46 @agent_ppo2.py:185][0m |          -0.0001 |          79.1558 |          18.1303 |
[32m[20221213 21:01:46 @agent_ppo2.py:185][0m |          -0.0059 |          73.7916 |          18.1463 |
[32m[20221213 21:01:46 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:01:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.60
[32m[20221213 21:01:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.00
[32m[20221213 21:01:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 524.00
[32m[20221213 21:01:46 @agent_ppo2.py:143][0m Total time:       6.19 min
[32m[20221213 21:01:46 @agent_ppo2.py:145][0m 606208 total steps have happened
[32m[20221213 21:01:46 @agent_ppo2.py:121][0m #------------------------ Iteration 296 --------------------------#
[32m[20221213 21:01:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:01:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:46 @agent_ppo2.py:185][0m |           0.0002 |          79.1348 |          18.2167 |
[32m[20221213 21:01:46 @agent_ppo2.py:185][0m |          -0.0030 |          78.6440 |          18.2029 |
[32m[20221213 21:01:47 @agent_ppo2.py:185][0m |           0.0042 |          80.5785 |          18.2043 |
[32m[20221213 21:01:47 @agent_ppo2.py:185][0m |          -0.0047 |          77.5940 |          18.1992 |
[32m[20221213 21:01:47 @agent_ppo2.py:185][0m |          -0.0042 |          77.3472 |          18.1979 |
[32m[20221213 21:01:47 @agent_ppo2.py:185][0m |          -0.0050 |          77.1487 |          18.1961 |
[32m[20221213 21:01:47 @agent_ppo2.py:185][0m |          -0.0023 |          78.1825 |          18.1984 |
[32m[20221213 21:01:47 @agent_ppo2.py:185][0m |           0.0032 |          81.8361 |          18.1957 |
[32m[20221213 21:01:47 @agent_ppo2.py:185][0m |          -0.0055 |          76.3516 |          18.1888 |
[32m[20221213 21:01:47 @agent_ppo2.py:185][0m |          -0.0065 |          76.1229 |          18.1948 |
[32m[20221213 21:01:47 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:01:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.40
[32m[20221213 21:01:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.00
[32m[20221213 21:01:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.00
[32m[20221213 21:01:47 @agent_ppo2.py:143][0m Total time:       6.21 min
[32m[20221213 21:01:47 @agent_ppo2.py:145][0m 608256 total steps have happened
[32m[20221213 21:01:47 @agent_ppo2.py:121][0m #------------------------ Iteration 297 --------------------------#
[32m[20221213 21:01:48 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:01:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:48 @agent_ppo2.py:185][0m |           0.0003 |          77.3952 |          18.2996 |
[32m[20221213 21:01:48 @agent_ppo2.py:185][0m |          -0.0032 |          76.7481 |          18.2827 |
[32m[20221213 21:01:48 @agent_ppo2.py:185][0m |          -0.0017 |          76.5722 |          18.2803 |
[32m[20221213 21:01:48 @agent_ppo2.py:185][0m |          -0.0061 |          76.1969 |          18.2705 |
[32m[20221213 21:01:48 @agent_ppo2.py:185][0m |           0.0032 |          79.1847 |          18.2691 |
[32m[20221213 21:01:48 @agent_ppo2.py:185][0m |          -0.0030 |          75.9086 |          18.2726 |
[32m[20221213 21:01:48 @agent_ppo2.py:185][0m |          -0.0003 |          76.8236 |          18.2756 |
[32m[20221213 21:01:48 @agent_ppo2.py:185][0m |          -0.0046 |          75.5013 |          18.2635 |
[32m[20221213 21:01:48 @agent_ppo2.py:185][0m |          -0.0078 |          75.3210 |          18.2667 |
[32m[20221213 21:01:49 @agent_ppo2.py:185][0m |          -0.0035 |          75.1733 |          18.2756 |
[32m[20221213 21:01:49 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:01:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 524.60
[32m[20221213 21:01:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.00
[32m[20221213 21:01:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.00
[32m[20221213 21:01:49 @agent_ppo2.py:143][0m Total time:       6.23 min
[32m[20221213 21:01:49 @agent_ppo2.py:145][0m 610304 total steps have happened
[32m[20221213 21:01:49 @agent_ppo2.py:121][0m #------------------------ Iteration 298 --------------------------#
[32m[20221213 21:01:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:49 @agent_ppo2.py:185][0m |           0.0022 |          75.5674 |          18.3101 |
[32m[20221213 21:01:49 @agent_ppo2.py:185][0m |          -0.0084 |          74.3417 |          18.2877 |
[32m[20221213 21:01:49 @agent_ppo2.py:185][0m |          -0.0028 |          73.6947 |          18.2700 |
[32m[20221213 21:01:49 @agent_ppo2.py:185][0m |          -0.0083 |          73.2110 |          18.2659 |
[32m[20221213 21:01:49 @agent_ppo2.py:185][0m |          -0.0082 |          72.7495 |          18.2732 |
[32m[20221213 21:01:49 @agent_ppo2.py:185][0m |          -0.0090 |          72.5974 |          18.2725 |
[32m[20221213 21:01:50 @agent_ppo2.py:185][0m |          -0.0035 |          72.3365 |          18.2754 |
[32m[20221213 21:01:50 @agent_ppo2.py:185][0m |          -0.0081 |          72.1642 |          18.2722 |
[32m[20221213 21:01:50 @agent_ppo2.py:185][0m |          -0.0096 |          71.9143 |          18.2697 |
[32m[20221213 21:01:50 @agent_ppo2.py:185][0m |          -0.0071 |          71.8327 |          18.2656 |
[32m[20221213 21:01:50 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:01:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.60
[32m[20221213 21:01:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 537.00
[32m[20221213 21:01:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.00
[32m[20221213 21:01:50 @agent_ppo2.py:143][0m Total time:       6.25 min
[32m[20221213 21:01:50 @agent_ppo2.py:145][0m 612352 total steps have happened
[32m[20221213 21:01:50 @agent_ppo2.py:121][0m #------------------------ Iteration 299 --------------------------#
[32m[20221213 21:01:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:01:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:50 @agent_ppo2.py:185][0m |          -0.0028 |          78.1399 |          18.2264 |
[32m[20221213 21:01:50 @agent_ppo2.py:185][0m |          -0.0055 |          77.4347 |          18.2100 |
[32m[20221213 21:01:50 @agent_ppo2.py:185][0m |          -0.0052 |          76.9899 |          18.1893 |
[32m[20221213 21:01:51 @agent_ppo2.py:185][0m |          -0.0092 |          76.8096 |          18.1754 |
[32m[20221213 21:01:51 @agent_ppo2.py:185][0m |          -0.0048 |          77.0965 |          18.1718 |
[32m[20221213 21:01:51 @agent_ppo2.py:185][0m |          -0.0015 |          78.2048 |          18.1728 |
[32m[20221213 21:01:51 @agent_ppo2.py:185][0m |           0.0000 |          78.8737 |          18.1672 |
[32m[20221213 21:01:51 @agent_ppo2.py:185][0m |          -0.0064 |          76.3795 |          18.1602 |
[32m[20221213 21:01:51 @agent_ppo2.py:185][0m |          -0.0103 |          76.2085 |          18.1606 |
[32m[20221213 21:01:51 @agent_ppo2.py:185][0m |          -0.0070 |          76.1314 |          18.1612 |
[32m[20221213 21:01:51 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:01:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 525.60
[32m[20221213 21:01:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.00
[32m[20221213 21:01:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.00
[32m[20221213 21:01:51 @agent_ppo2.py:143][0m Total time:       6.27 min
[32m[20221213 21:01:51 @agent_ppo2.py:145][0m 614400 total steps have happened
[32m[20221213 21:01:51 @agent_ppo2.py:121][0m #------------------------ Iteration 300 --------------------------#
[32m[20221213 21:01:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:51 @agent_ppo2.py:185][0m |           0.0002 |          75.8762 |          18.2856 |
[32m[20221213 21:01:52 @agent_ppo2.py:185][0m |          -0.0006 |          74.3988 |          18.2757 |
[32m[20221213 21:01:52 @agent_ppo2.py:185][0m |          -0.0040 |          74.0128 |          18.2683 |
[32m[20221213 21:01:52 @agent_ppo2.py:185][0m |          -0.0039 |          73.7454 |          18.2685 |
[32m[20221213 21:01:52 @agent_ppo2.py:185][0m |          -0.0017 |          73.4706 |          18.2650 |
[32m[20221213 21:01:52 @agent_ppo2.py:185][0m |          -0.0075 |          73.3663 |          18.2605 |
[32m[20221213 21:01:52 @agent_ppo2.py:185][0m |          -0.0070 |          73.0960 |          18.2585 |
[32m[20221213 21:01:52 @agent_ppo2.py:185][0m |          -0.0083 |          73.1198 |          18.2653 |
[32m[20221213 21:01:52 @agent_ppo2.py:185][0m |          -0.0010 |          73.8127 |          18.2688 |
[32m[20221213 21:01:52 @agent_ppo2.py:185][0m |          -0.0038 |          73.4098 |          18.2624 |
[32m[20221213 21:01:52 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:01:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.40
[32m[20221213 21:01:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 565.00
[32m[20221213 21:01:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.00
[32m[20221213 21:01:52 @agent_ppo2.py:143][0m Total time:       6.29 min
[32m[20221213 21:01:52 @agent_ppo2.py:145][0m 616448 total steps have happened
[32m[20221213 21:01:52 @agent_ppo2.py:121][0m #------------------------ Iteration 301 --------------------------#
[32m[20221213 21:01:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:53 @agent_ppo2.py:185][0m |          -0.0012 |          77.0774 |          18.3001 |
[32m[20221213 21:01:53 @agent_ppo2.py:185][0m |          -0.0060 |          76.3545 |          18.2795 |
[32m[20221213 21:01:53 @agent_ppo2.py:185][0m |          -0.0041 |          75.5742 |          18.2739 |
[32m[20221213 21:01:53 @agent_ppo2.py:185][0m |          -0.0054 |          75.2517 |          18.2638 |
[32m[20221213 21:01:53 @agent_ppo2.py:185][0m |          -0.0078 |          75.0568 |          18.2683 |
[32m[20221213 21:01:53 @agent_ppo2.py:185][0m |          -0.0056 |          74.6504 |          18.2574 |
[32m[20221213 21:01:53 @agent_ppo2.py:185][0m |          -0.0064 |          74.5488 |          18.2662 |
[32m[20221213 21:01:53 @agent_ppo2.py:185][0m |           0.0004 |          75.8427 |          18.2589 |
[32m[20221213 21:01:53 @agent_ppo2.py:185][0m |          -0.0070 |          74.2205 |          18.2677 |
[32m[20221213 21:01:54 @agent_ppo2.py:185][0m |          -0.0070 |          73.9836 |          18.2698 |
[32m[20221213 21:01:54 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:01:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.20
[32m[20221213 21:01:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.00
[32m[20221213 21:01:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 536.00
[32m[20221213 21:01:54 @agent_ppo2.py:143][0m Total time:       6.32 min
[32m[20221213 21:01:54 @agent_ppo2.py:145][0m 618496 total steps have happened
[32m[20221213 21:01:54 @agent_ppo2.py:121][0m #------------------------ Iteration 302 --------------------------#
[32m[20221213 21:01:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:54 @agent_ppo2.py:185][0m |          -0.0016 |          75.5156 |          18.2236 |
[32m[20221213 21:01:54 @agent_ppo2.py:185][0m |          -0.0029 |          74.8240 |          18.1901 |
[32m[20221213 21:01:54 @agent_ppo2.py:185][0m |          -0.0069 |          74.6058 |          18.1847 |
[32m[20221213 21:01:54 @agent_ppo2.py:185][0m |          -0.0035 |          74.3184 |          18.1889 |
[32m[20221213 21:01:54 @agent_ppo2.py:185][0m |           0.0086 |          82.6948 |          18.1781 |
[32m[20221213 21:01:54 @agent_ppo2.py:185][0m |          -0.0006 |          76.1912 |          18.1673 |
[32m[20221213 21:01:54 @agent_ppo2.py:185][0m |           0.0078 |          83.9157 |          18.1786 |
[32m[20221213 21:01:55 @agent_ppo2.py:185][0m |          -0.0034 |          74.0205 |          18.1888 |
[32m[20221213 21:01:55 @agent_ppo2.py:185][0m |          -0.0083 |          73.8784 |          18.1820 |
[32m[20221213 21:01:55 @agent_ppo2.py:185][0m |          -0.0084 |          73.9409 |          18.1702 |
[32m[20221213 21:01:55 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:01:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.40
[32m[20221213 21:01:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.00
[32m[20221213 21:01:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.00
[32m[20221213 21:01:55 @agent_ppo2.py:143][0m Total time:       6.34 min
[32m[20221213 21:01:55 @agent_ppo2.py:145][0m 620544 total steps have happened
[32m[20221213 21:01:55 @agent_ppo2.py:121][0m #------------------------ Iteration 303 --------------------------#
[32m[20221213 21:01:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:55 @agent_ppo2.py:185][0m |          -0.0024 |          74.6046 |          18.1210 |
[32m[20221213 21:01:55 @agent_ppo2.py:185][0m |           0.0013 |          75.1126 |          18.1066 |
[32m[20221213 21:01:55 @agent_ppo2.py:185][0m |          -0.0060 |          73.6852 |          18.0889 |
[32m[20221213 21:01:55 @agent_ppo2.py:185][0m |          -0.0051 |          73.4077 |          18.0881 |
[32m[20221213 21:01:56 @agent_ppo2.py:185][0m |          -0.0061 |          73.2098 |          18.0890 |
[32m[20221213 21:01:56 @agent_ppo2.py:185][0m |          -0.0073 |          72.9726 |          18.0832 |
[32m[20221213 21:01:56 @agent_ppo2.py:185][0m |          -0.0049 |          72.9065 |          18.0861 |
[32m[20221213 21:01:56 @agent_ppo2.py:185][0m |          -0.0080 |          72.7014 |          18.0873 |
[32m[20221213 21:01:56 @agent_ppo2.py:185][0m |          -0.0083 |          72.6365 |          18.0821 |
[32m[20221213 21:01:56 @agent_ppo2.py:185][0m |          -0.0092 |          72.4817 |          18.0888 |
[32m[20221213 21:01:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:01:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.00
[32m[20221213 21:01:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 543.00
[32m[20221213 21:01:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.00
[32m[20221213 21:01:56 @agent_ppo2.py:143][0m Total time:       6.36 min
[32m[20221213 21:01:56 @agent_ppo2.py:145][0m 622592 total steps have happened
[32m[20221213 21:01:56 @agent_ppo2.py:121][0m #------------------------ Iteration 304 --------------------------#
[32m[20221213 21:01:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:56 @agent_ppo2.py:185][0m |          -0.0043 |          78.1379 |          18.2176 |
[32m[20221213 21:01:56 @agent_ppo2.py:185][0m |          -0.0055 |          77.3608 |          18.1941 |
[32m[20221213 21:01:57 @agent_ppo2.py:185][0m |          -0.0028 |          77.0800 |          18.1765 |
[32m[20221213 21:01:57 @agent_ppo2.py:185][0m |           0.0052 |          84.9262 |          18.1885 |
[32m[20221213 21:01:57 @agent_ppo2.py:185][0m |          -0.0014 |          77.0846 |          18.1727 |
[32m[20221213 21:01:57 @agent_ppo2.py:185][0m |          -0.0070 |          76.5958 |          18.1754 |
[32m[20221213 21:01:57 @agent_ppo2.py:185][0m |          -0.0034 |          77.7721 |          18.1768 |
[32m[20221213 21:01:57 @agent_ppo2.py:185][0m |          -0.0054 |          76.3217 |          18.1781 |
[32m[20221213 21:01:57 @agent_ppo2.py:185][0m |          -0.0083 |          76.2261 |          18.1694 |
[32m[20221213 21:01:57 @agent_ppo2.py:185][0m |          -0.0025 |          79.5483 |          18.1773 |
[32m[20221213 21:01:57 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:01:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.20
[32m[20221213 21:01:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.00
[32m[20221213 21:01:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.00
[32m[20221213 21:01:57 @agent_ppo2.py:143][0m Total time:       6.38 min
[32m[20221213 21:01:57 @agent_ppo2.py:145][0m 624640 total steps have happened
[32m[20221213 21:01:57 @agent_ppo2.py:121][0m #------------------------ Iteration 305 --------------------------#
[32m[20221213 21:01:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:58 @agent_ppo2.py:185][0m |           0.0090 |          82.1336 |          18.2342 |
[32m[20221213 21:01:58 @agent_ppo2.py:185][0m |          -0.0022 |          76.9358 |          18.2315 |
[32m[20221213 21:01:58 @agent_ppo2.py:185][0m |          -0.0046 |          76.6862 |          18.2142 |
[32m[20221213 21:01:58 @agent_ppo2.py:185][0m |          -0.0004 |          76.9264 |          18.2135 |
[32m[20221213 21:01:58 @agent_ppo2.py:185][0m |          -0.0059 |          76.1430 |          18.2161 |
[32m[20221213 21:01:58 @agent_ppo2.py:185][0m |           0.0090 |          81.7537 |          18.2107 |
[32m[20221213 21:01:58 @agent_ppo2.py:185][0m |          -0.0055 |          75.8841 |          18.2005 |
[32m[20221213 21:01:58 @agent_ppo2.py:185][0m |          -0.0054 |          75.7886 |          18.2059 |
[32m[20221213 21:01:58 @agent_ppo2.py:185][0m |          -0.0017 |          76.4397 |          18.2074 |
[32m[20221213 21:01:58 @agent_ppo2.py:185][0m |          -0.0018 |          78.1987 |          18.2006 |
[32m[20221213 21:01:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:01:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.20
[32m[20221213 21:01:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 547.00
[32m[20221213 21:01:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 534.00
[32m[20221213 21:01:59 @agent_ppo2.py:143][0m Total time:       6.40 min
[32m[20221213 21:01:59 @agent_ppo2.py:145][0m 626688 total steps have happened
[32m[20221213 21:01:59 @agent_ppo2.py:121][0m #------------------------ Iteration 306 --------------------------#
[32m[20221213 21:01:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:01:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:01:59 @agent_ppo2.py:185][0m |           0.0003 |          77.4974 |          18.2817 |
[32m[20221213 21:01:59 @agent_ppo2.py:185][0m |          -0.0050 |          76.7672 |          18.2688 |
[32m[20221213 21:01:59 @agent_ppo2.py:185][0m |          -0.0027 |          76.5699 |          18.2602 |
[32m[20221213 21:01:59 @agent_ppo2.py:185][0m |          -0.0045 |          76.3165 |          18.2622 |
[32m[20221213 21:01:59 @agent_ppo2.py:185][0m |          -0.0061 |          75.9637 |          18.2563 |
[32m[20221213 21:01:59 @agent_ppo2.py:185][0m |          -0.0051 |          76.0183 |          18.2562 |
[32m[20221213 21:01:59 @agent_ppo2.py:185][0m |          -0.0068 |          75.7469 |          18.2624 |
[32m[20221213 21:01:59 @agent_ppo2.py:185][0m |          -0.0038 |          75.6837 |          18.2535 |
[32m[20221213 21:02:00 @agent_ppo2.py:185][0m |          -0.0037 |          76.5457 |          18.2626 |
[32m[20221213 21:02:00 @agent_ppo2.py:185][0m |          -0.0075 |          75.4041 |          18.2610 |
[32m[20221213 21:02:00 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:02:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 538.80
[32m[20221213 21:02:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.00
[32m[20221213 21:02:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.00
[32m[20221213 21:02:00 @agent_ppo2.py:143][0m Total time:       6.42 min
[32m[20221213 21:02:00 @agent_ppo2.py:145][0m 628736 total steps have happened
[32m[20221213 21:02:00 @agent_ppo2.py:121][0m #------------------------ Iteration 307 --------------------------#
[32m[20221213 21:02:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:00 @agent_ppo2.py:185][0m |           0.0082 |          82.5564 |          18.2727 |
[32m[20221213 21:02:00 @agent_ppo2.py:185][0m |          -0.0050 |          77.0554 |          18.2557 |
[32m[20221213 21:02:00 @agent_ppo2.py:185][0m |          -0.0045 |          76.7809 |          18.2522 |
[32m[20221213 21:02:00 @agent_ppo2.py:185][0m |          -0.0045 |          76.5279 |          18.2491 |
[32m[20221213 21:02:00 @agent_ppo2.py:185][0m |           0.0072 |          83.9544 |          18.2497 |
[32m[20221213 21:02:00 @agent_ppo2.py:185][0m |          -0.0071 |          76.2768 |          18.2462 |
[32m[20221213 21:02:01 @agent_ppo2.py:185][0m |          -0.0040 |          76.1101 |          18.2463 |
[32m[20221213 21:02:01 @agent_ppo2.py:185][0m |          -0.0058 |          75.9007 |          18.2528 |
[32m[20221213 21:02:01 @agent_ppo2.py:185][0m |          -0.0061 |          75.7832 |          18.2508 |
[32m[20221213 21:02:01 @agent_ppo2.py:185][0m |           0.0051 |          81.9491 |          18.2484 |
[32m[20221213 21:02:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:02:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.60
[32m[20221213 21:02:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.00
[32m[20221213 21:02:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.00
[32m[20221213 21:02:01 @agent_ppo2.py:143][0m Total time:       6.44 min
[32m[20221213 21:02:01 @agent_ppo2.py:145][0m 630784 total steps have happened
[32m[20221213 21:02:01 @agent_ppo2.py:121][0m #------------------------ Iteration 308 --------------------------#
[32m[20221213 21:02:01 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:02:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:01 @agent_ppo2.py:185][0m |          -0.0013 |          73.5357 |          18.2227 |
[32m[20221213 21:02:01 @agent_ppo2.py:185][0m |          -0.0036 |          72.8339 |          18.2130 |
[32m[20221213 21:02:01 @agent_ppo2.py:185][0m |          -0.0025 |          72.6165 |          18.2031 |
[32m[20221213 21:02:01 @agent_ppo2.py:185][0m |           0.0061 |          76.8105 |          18.1988 |
[32m[20221213 21:02:02 @agent_ppo2.py:185][0m |          -0.0055 |          72.3146 |          18.1993 |
[32m[20221213 21:02:02 @agent_ppo2.py:185][0m |          -0.0019 |          72.3840 |          18.1983 |
[32m[20221213 21:02:02 @agent_ppo2.py:185][0m |          -0.0048 |          72.0015 |          18.1946 |
[32m[20221213 21:02:02 @agent_ppo2.py:185][0m |          -0.0042 |          71.9792 |          18.1837 |
[32m[20221213 21:02:02 @agent_ppo2.py:185][0m |          -0.0069 |          71.7853 |          18.1835 |
[32m[20221213 21:02:02 @agent_ppo2.py:185][0m |          -0.0071 |          71.7685 |          18.1895 |
[32m[20221213 21:02:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:02:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.00
[32m[20221213 21:02:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.00
[32m[20221213 21:02:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.00
[32m[20221213 21:02:02 @agent_ppo2.py:143][0m Total time:       6.46 min
[32m[20221213 21:02:02 @agent_ppo2.py:145][0m 632832 total steps have happened
[32m[20221213 21:02:02 @agent_ppo2.py:121][0m #------------------------ Iteration 309 --------------------------#
[32m[20221213 21:02:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:02:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:02 @agent_ppo2.py:185][0m |          -0.0012 |          75.5176 |          18.3832 |
[32m[20221213 21:02:03 @agent_ppo2.py:185][0m |          -0.0054 |          75.0173 |          18.3810 |
[32m[20221213 21:02:03 @agent_ppo2.py:185][0m |          -0.0027 |          74.6063 |          18.3772 |
[32m[20221213 21:02:03 @agent_ppo2.py:185][0m |          -0.0050 |          74.5372 |          18.3802 |
[32m[20221213 21:02:03 @agent_ppo2.py:185][0m |          -0.0054 |          74.2581 |          18.3711 |
[32m[20221213 21:02:03 @agent_ppo2.py:185][0m |          -0.0058 |          74.1107 |          18.3796 |
[32m[20221213 21:02:03 @agent_ppo2.py:185][0m |          -0.0054 |          74.1299 |          18.3768 |
[32m[20221213 21:02:03 @agent_ppo2.py:185][0m |          -0.0044 |          73.9397 |          18.3810 |
[32m[20221213 21:02:03 @agent_ppo2.py:185][0m |          -0.0066 |          73.9876 |          18.3762 |
[32m[20221213 21:02:03 @agent_ppo2.py:185][0m |          -0.0059 |          73.8358 |          18.3784 |
[32m[20221213 21:02:03 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:02:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 525.00
[32m[20221213 21:02:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.00
[32m[20221213 21:02:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.00
[32m[20221213 21:02:03 @agent_ppo2.py:143][0m Total time:       6.48 min
[32m[20221213 21:02:03 @agent_ppo2.py:145][0m 634880 total steps have happened
[32m[20221213 21:02:03 @agent_ppo2.py:121][0m #------------------------ Iteration 310 --------------------------#
[32m[20221213 21:02:03 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:02:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:04 @agent_ppo2.py:185][0m |          -0.0019 |          78.6311 |          18.2149 |
[32m[20221213 21:02:04 @agent_ppo2.py:185][0m |          -0.0066 |          77.2922 |          18.1771 |
[32m[20221213 21:02:04 @agent_ppo2.py:185][0m |          -0.0056 |          76.6602 |          18.1645 |
[32m[20221213 21:02:04 @agent_ppo2.py:185][0m |          -0.0089 |          75.9309 |          18.1630 |
[32m[20221213 21:02:04 @agent_ppo2.py:185][0m |          -0.0072 |          75.5051 |          18.1529 |
[32m[20221213 21:02:04 @agent_ppo2.py:185][0m |          -0.0079 |          75.2415 |          18.1510 |
[32m[20221213 21:02:04 @agent_ppo2.py:185][0m |          -0.0101 |          74.9737 |          18.1507 |
[32m[20221213 21:02:04 @agent_ppo2.py:185][0m |          -0.0070 |          74.8403 |          18.1465 |
[32m[20221213 21:02:04 @agent_ppo2.py:185][0m |          -0.0081 |          74.7045 |          18.1424 |
[32m[20221213 21:02:04 @agent_ppo2.py:185][0m |          -0.0108 |          74.4840 |          18.1578 |
[32m[20221213 21:02:04 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:02:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 532.20
[32m[20221213 21:02:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 554.00
[32m[20221213 21:02:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 238.00
[32m[20221213 21:02:05 @agent_ppo2.py:143][0m Total time:       6.50 min
[32m[20221213 21:02:05 @agent_ppo2.py:145][0m 636928 total steps have happened
[32m[20221213 21:02:05 @agent_ppo2.py:121][0m #------------------------ Iteration 311 --------------------------#
[32m[20221213 21:02:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:05 @agent_ppo2.py:185][0m |          -0.0019 |          79.2302 |          18.2556 |
[32m[20221213 21:02:05 @agent_ppo2.py:185][0m |          -0.0055 |          77.7944 |          18.2324 |
[32m[20221213 21:02:05 @agent_ppo2.py:185][0m |           0.0055 |          87.1357 |          18.2308 |
[32m[20221213 21:02:05 @agent_ppo2.py:185][0m |          -0.0041 |          77.4435 |          18.2118 |
[32m[20221213 21:02:05 @agent_ppo2.py:185][0m |          -0.0116 |          77.0277 |          18.2225 |
[32m[20221213 21:02:05 @agent_ppo2.py:185][0m |          -0.0068 |          77.4749 |          18.2181 |
[32m[20221213 21:02:05 @agent_ppo2.py:185][0m |          -0.0077 |          76.6093 |          18.2157 |
[32m[20221213 21:02:05 @agent_ppo2.py:185][0m |          -0.0101 |          76.6233 |          18.2126 |
[32m[20221213 21:02:06 @agent_ppo2.py:185][0m |          -0.0053 |          78.0510 |          18.2130 |
[32m[20221213 21:02:06 @agent_ppo2.py:185][0m |          -0.0095 |          76.3740 |          18.2147 |
[32m[20221213 21:02:06 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:02:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.40
[32m[20221213 21:02:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.00
[32m[20221213 21:02:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.00
[32m[20221213 21:02:06 @agent_ppo2.py:143][0m Total time:       6.52 min
[32m[20221213 21:02:06 @agent_ppo2.py:145][0m 638976 total steps have happened
[32m[20221213 21:02:06 @agent_ppo2.py:121][0m #------------------------ Iteration 312 --------------------------#
[32m[20221213 21:02:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:02:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:06 @agent_ppo2.py:185][0m |           0.0006 |          78.3488 |          18.2812 |
[32m[20221213 21:02:06 @agent_ppo2.py:185][0m |          -0.0033 |          76.1633 |          18.2655 |
[32m[20221213 21:02:06 @agent_ppo2.py:185][0m |          -0.0056 |          75.4939 |          18.2630 |
[32m[20221213 21:02:06 @agent_ppo2.py:185][0m |          -0.0049 |          75.2230 |          18.2569 |
[32m[20221213 21:02:06 @agent_ppo2.py:185][0m |          -0.0068 |          75.1396 |          18.2548 |
[32m[20221213 21:02:06 @agent_ppo2.py:185][0m |          -0.0065 |          74.9358 |          18.2496 |
[32m[20221213 21:02:07 @agent_ppo2.py:185][0m |          -0.0067 |          74.8027 |          18.2595 |
[32m[20221213 21:02:07 @agent_ppo2.py:185][0m |          -0.0066 |          74.6607 |          18.2541 |
[32m[20221213 21:02:07 @agent_ppo2.py:185][0m |          -0.0064 |          74.6519 |          18.2613 |
[32m[20221213 21:02:07 @agent_ppo2.py:185][0m |          -0.0069 |          74.5832 |          18.2529 |
[32m[20221213 21:02:07 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:02:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.00
[32m[20221213 21:02:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.00
[32m[20221213 21:02:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.00
[32m[20221213 21:02:07 @agent_ppo2.py:143][0m Total time:       6.54 min
[32m[20221213 21:02:07 @agent_ppo2.py:145][0m 641024 total steps have happened
[32m[20221213 21:02:07 @agent_ppo2.py:121][0m #------------------------ Iteration 313 --------------------------#
[32m[20221213 21:02:07 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:02:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:07 @agent_ppo2.py:185][0m |          -0.0019 |          76.0905 |          18.0515 |
[32m[20221213 21:02:07 @agent_ppo2.py:185][0m |          -0.0024 |          75.7085 |          18.0371 |
[32m[20221213 21:02:07 @agent_ppo2.py:185][0m |          -0.0060 |          75.1615 |          18.0187 |
[32m[20221213 21:02:07 @agent_ppo2.py:185][0m |          -0.0072 |          75.0249 |          18.0165 |
[32m[20221213 21:02:08 @agent_ppo2.py:185][0m |          -0.0053 |          74.8620 |          18.0226 |
[32m[20221213 21:02:08 @agent_ppo2.py:185][0m |          -0.0070 |          74.6898 |          18.0120 |
[32m[20221213 21:02:08 @agent_ppo2.py:185][0m |           0.0121 |          85.3191 |          18.0123 |
[32m[20221213 21:02:08 @agent_ppo2.py:185][0m |          -0.0078 |          74.3475 |          17.9988 |
[32m[20221213 21:02:08 @agent_ppo2.py:185][0m |          -0.0086 |          74.0583 |          18.0088 |
[32m[20221213 21:02:08 @agent_ppo2.py:185][0m |          -0.0083 |          73.9359 |          18.0086 |
[32m[20221213 21:02:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:02:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.20
[32m[20221213 21:02:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.00
[32m[20221213 21:02:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 557.00
[32m[20221213 21:02:08 @agent_ppo2.py:143][0m Total time:       6.56 min
[32m[20221213 21:02:08 @agent_ppo2.py:145][0m 643072 total steps have happened
[32m[20221213 21:02:08 @agent_ppo2.py:121][0m #------------------------ Iteration 314 --------------------------#
[32m[20221213 21:02:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:08 @agent_ppo2.py:185][0m |          -0.0032 |          77.3802 |          18.3488 |
[32m[20221213 21:02:08 @agent_ppo2.py:185][0m |          -0.0057 |          76.8956 |          18.3349 |
[32m[20221213 21:02:09 @agent_ppo2.py:185][0m |          -0.0021 |          77.0336 |          18.3210 |
[32m[20221213 21:02:09 @agent_ppo2.py:185][0m |          -0.0061 |          76.4963 |          18.3177 |
[32m[20221213 21:02:09 @agent_ppo2.py:185][0m |          -0.0040 |          76.5303 |          18.3106 |
[32m[20221213 21:02:09 @agent_ppo2.py:185][0m |          -0.0040 |          76.1846 |          18.3102 |
[32m[20221213 21:02:09 @agent_ppo2.py:185][0m |          -0.0062 |          76.0860 |          18.3034 |
[32m[20221213 21:02:09 @agent_ppo2.py:185][0m |          -0.0038 |          75.9712 |          18.3100 |
[32m[20221213 21:02:09 @agent_ppo2.py:185][0m |          -0.0067 |          75.9581 |          18.3084 |
[32m[20221213 21:02:09 @agent_ppo2.py:185][0m |          -0.0057 |          75.8755 |          18.3116 |
[32m[20221213 21:02:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:02:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 507.00
[32m[20221213 21:02:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.00
[32m[20221213 21:02:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.00
[32m[20221213 21:02:09 @agent_ppo2.py:143][0m Total time:       6.58 min
[32m[20221213 21:02:09 @agent_ppo2.py:145][0m 645120 total steps have happened
[32m[20221213 21:02:09 @agent_ppo2.py:121][0m #------------------------ Iteration 315 --------------------------#
[32m[20221213 21:02:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:10 @agent_ppo2.py:185][0m |          -0.0021 |          78.1711 |          18.2168 |
[32m[20221213 21:02:10 @agent_ppo2.py:185][0m |          -0.0007 |          78.5154 |          18.1977 |
[32m[20221213 21:02:10 @agent_ppo2.py:185][0m |          -0.0043 |          77.4686 |          18.1802 |
[32m[20221213 21:02:10 @agent_ppo2.py:185][0m |          -0.0043 |          77.0447 |          18.1800 |
[32m[20221213 21:02:10 @agent_ppo2.py:185][0m |          -0.0068 |          76.8855 |          18.1740 |
[32m[20221213 21:02:10 @agent_ppo2.py:185][0m |          -0.0037 |          76.8096 |          18.1843 |
[32m[20221213 21:02:10 @agent_ppo2.py:185][0m |          -0.0036 |          76.9719 |          18.1743 |
[32m[20221213 21:02:10 @agent_ppo2.py:185][0m |          -0.0078 |          76.6237 |          18.1868 |
[32m[20221213 21:02:10 @agent_ppo2.py:185][0m |          -0.0075 |          76.5588 |          18.1802 |
[32m[20221213 21:02:10 @agent_ppo2.py:185][0m |          -0.0076 |          76.5796 |          18.1848 |
[32m[20221213 21:02:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:02:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 523.60
[32m[20221213 21:02:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.00
[32m[20221213 21:02:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 528.00
[32m[20221213 21:02:11 @agent_ppo2.py:143][0m Total time:       6.60 min
[32m[20221213 21:02:11 @agent_ppo2.py:145][0m 647168 total steps have happened
[32m[20221213 21:02:11 @agent_ppo2.py:121][0m #------------------------ Iteration 316 --------------------------#
[32m[20221213 21:02:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:11 @agent_ppo2.py:185][0m |          -0.0031 |          73.7143 |          18.3121 |
[32m[20221213 21:02:11 @agent_ppo2.py:185][0m |          -0.0024 |          73.0971 |          18.2951 |
[32m[20221213 21:02:11 @agent_ppo2.py:185][0m |          -0.0023 |          73.1212 |          18.2832 |
[32m[20221213 21:02:11 @agent_ppo2.py:185][0m |           0.0099 |          81.5862 |          18.2826 |
[32m[20221213 21:02:11 @agent_ppo2.py:185][0m |          -0.0078 |          73.0046 |          18.2671 |
[32m[20221213 21:02:11 @agent_ppo2.py:185][0m |          -0.0069 |          72.6409 |          18.2827 |
[32m[20221213 21:02:11 @agent_ppo2.py:185][0m |          -0.0064 |          72.6444 |          18.2718 |
[32m[20221213 21:02:11 @agent_ppo2.py:185][0m |          -0.0045 |          72.5072 |          18.2801 |
[32m[20221213 21:02:12 @agent_ppo2.py:185][0m |          -0.0086 |          72.4829 |          18.2714 |
[32m[20221213 21:02:12 @agent_ppo2.py:185][0m |          -0.0080 |          72.4162 |          18.2802 |
[32m[20221213 21:02:12 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:02:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 537.20
[32m[20221213 21:02:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 566.00
[32m[20221213 21:02:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 626.00
[32m[20221213 21:02:12 @agent_ppo2.py:143][0m Total time:       6.62 min
[32m[20221213 21:02:12 @agent_ppo2.py:145][0m 649216 total steps have happened
[32m[20221213 21:02:12 @agent_ppo2.py:121][0m #------------------------ Iteration 317 --------------------------#
[32m[20221213 21:02:12 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:02:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:12 @agent_ppo2.py:185][0m |          -0.0013 |          76.7120 |          18.2737 |
[32m[20221213 21:02:12 @agent_ppo2.py:185][0m |          -0.0054 |          76.1160 |          18.2562 |
[32m[20221213 21:02:12 @agent_ppo2.py:185][0m |          -0.0082 |          75.8663 |          18.2449 |
[32m[20221213 21:02:12 @agent_ppo2.py:185][0m |          -0.0071 |          75.6434 |          18.2351 |
[32m[20221213 21:02:12 @agent_ppo2.py:185][0m |           0.0033 |          82.2567 |          18.2367 |
[32m[20221213 21:02:13 @agent_ppo2.py:185][0m |          -0.0106 |          76.0987 |          18.2400 |
[32m[20221213 21:02:13 @agent_ppo2.py:185][0m |          -0.0079 |          76.1424 |          18.2362 |
[32m[20221213 21:02:13 @agent_ppo2.py:185][0m |          -0.0089 |          75.2610 |          18.2371 |
[32m[20221213 21:02:13 @agent_ppo2.py:185][0m |          -0.0109 |          75.1630 |          18.2407 |
[32m[20221213 21:02:13 @agent_ppo2.py:185][0m |          -0.0036 |          77.4839 |          18.2371 |
[32m[20221213 21:02:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:02:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.40
[32m[20221213 21:02:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.00
[32m[20221213 21:02:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.00
[32m[20221213 21:02:13 @agent_ppo2.py:143][0m Total time:       6.64 min
[32m[20221213 21:02:13 @agent_ppo2.py:145][0m 651264 total steps have happened
[32m[20221213 21:02:13 @agent_ppo2.py:121][0m #------------------------ Iteration 318 --------------------------#
[32m[20221213 21:02:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:13 @agent_ppo2.py:185][0m |          -0.0017 |          78.0933 |          18.2823 |
[32m[20221213 21:02:13 @agent_ppo2.py:185][0m |          -0.0058 |          77.4744 |          18.2609 |
[32m[20221213 21:02:13 @agent_ppo2.py:185][0m |           0.0010 |          78.5104 |          18.2670 |
[32m[20221213 21:02:14 @agent_ppo2.py:185][0m |          -0.0047 |          76.9828 |          18.2579 |
[32m[20221213 21:02:14 @agent_ppo2.py:185][0m |           0.0023 |          81.6013 |          18.2615 |
[32m[20221213 21:02:14 @agent_ppo2.py:185][0m |          -0.0044 |          76.6166 |          18.2571 |
[32m[20221213 21:02:14 @agent_ppo2.py:185][0m |          -0.0037 |          76.5996 |          18.2536 |
[32m[20221213 21:02:14 @agent_ppo2.py:185][0m |          -0.0098 |          76.3584 |          18.2596 |
[32m[20221213 21:02:14 @agent_ppo2.py:185][0m |          -0.0100 |          76.3539 |          18.2553 |
[32m[20221213 21:02:14 @agent_ppo2.py:185][0m |          -0.0082 |          76.1565 |          18.2534 |
[32m[20221213 21:02:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:02:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.80
[32m[20221213 21:02:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.00
[32m[20221213 21:02:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 584.00
[32m[20221213 21:02:14 @agent_ppo2.py:143][0m Total time:       6.66 min
[32m[20221213 21:02:14 @agent_ppo2.py:145][0m 653312 total steps have happened
[32m[20221213 21:02:14 @agent_ppo2.py:121][0m #------------------------ Iteration 319 --------------------------#
[32m[20221213 21:02:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:15 @agent_ppo2.py:185][0m |           0.0004 |          74.6136 |          18.1548 |
[32m[20221213 21:02:15 @agent_ppo2.py:185][0m |          -0.0056 |          73.8722 |          18.1358 |
[32m[20221213 21:02:15 @agent_ppo2.py:185][0m |          -0.0039 |          73.4002 |          18.1380 |
[32m[20221213 21:02:15 @agent_ppo2.py:185][0m |          -0.0057 |          73.0711 |          18.1333 |
[32m[20221213 21:02:15 @agent_ppo2.py:185][0m |          -0.0064 |          72.8421 |          18.1262 |
[32m[20221213 21:02:15 @agent_ppo2.py:185][0m |          -0.0056 |          72.5569 |          18.1320 |
[32m[20221213 21:02:15 @agent_ppo2.py:185][0m |          -0.0011 |          73.2945 |          18.1325 |
[32m[20221213 21:02:15 @agent_ppo2.py:185][0m |          -0.0071 |          72.4140 |          18.1267 |
[32m[20221213 21:02:15 @agent_ppo2.py:185][0m |          -0.0056 |          72.8240 |          18.1323 |
[32m[20221213 21:02:15 @agent_ppo2.py:185][0m |           0.0002 |          76.7891 |          18.1264 |
[32m[20221213 21:02:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:02:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.60
[32m[20221213 21:02:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.00
[32m[20221213 21:02:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.00
[32m[20221213 21:02:15 @agent_ppo2.py:143][0m Total time:       6.68 min
[32m[20221213 21:02:15 @agent_ppo2.py:145][0m 655360 total steps have happened
[32m[20221213 21:02:15 @agent_ppo2.py:121][0m #------------------------ Iteration 320 --------------------------#
[32m[20221213 21:02:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:16 @agent_ppo2.py:185][0m |           0.0004 |          74.1251 |          18.1996 |
[32m[20221213 21:02:16 @agent_ppo2.py:185][0m |          -0.0002 |          73.2248 |          18.1811 |
[32m[20221213 21:02:16 @agent_ppo2.py:185][0m |           0.0043 |          73.8041 |          18.1553 |
[32m[20221213 21:02:16 @agent_ppo2.py:185][0m |          -0.0010 |          72.9983 |          18.1745 |
[32m[20221213 21:02:16 @agent_ppo2.py:185][0m |          -0.0039 |          72.5485 |          18.1610 |
[32m[20221213 21:02:16 @agent_ppo2.py:185][0m |          -0.0040 |          72.3820 |          18.1655 |
[32m[20221213 21:02:16 @agent_ppo2.py:185][0m |          -0.0031 |          72.3162 |          18.1636 |
[32m[20221213 21:02:16 @agent_ppo2.py:185][0m |          -0.0017 |          72.4421 |          18.1559 |
[32m[20221213 21:02:16 @agent_ppo2.py:185][0m |           0.0012 |          73.5311 |          18.1635 |
[32m[20221213 21:02:17 @agent_ppo2.py:185][0m |          -0.0003 |          72.7248 |          18.1470 |
[32m[20221213 21:02:17 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:02:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 526.80
[32m[20221213 21:02:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.00
[32m[20221213 21:02:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.00
[32m[20221213 21:02:17 @agent_ppo2.py:143][0m Total time:       6.70 min
[32m[20221213 21:02:17 @agent_ppo2.py:145][0m 657408 total steps have happened
[32m[20221213 21:02:17 @agent_ppo2.py:121][0m #------------------------ Iteration 321 --------------------------#
[32m[20221213 21:02:17 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:02:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:17 @agent_ppo2.py:185][0m |          -0.0001 |          73.2823 |          18.2936 |
[32m[20221213 21:02:17 @agent_ppo2.py:185][0m |          -0.0035 |          72.2793 |          18.2871 |
[32m[20221213 21:02:17 @agent_ppo2.py:185][0m |           0.0026 |          74.2450 |          18.2808 |
[32m[20221213 21:02:17 @agent_ppo2.py:185][0m |          -0.0024 |          71.8882 |          18.2747 |
[32m[20221213 21:02:17 @agent_ppo2.py:185][0m |          -0.0050 |          71.5055 |          18.2736 |
[32m[20221213 21:02:17 @agent_ppo2.py:185][0m |          -0.0038 |          71.9448 |          18.2696 |
[32m[20221213 21:02:17 @agent_ppo2.py:185][0m |          -0.0042 |          71.4057 |          18.2712 |
[32m[20221213 21:02:18 @agent_ppo2.py:185][0m |          -0.0058 |          71.3223 |          18.2637 |
[32m[20221213 21:02:18 @agent_ppo2.py:185][0m |          -0.0055 |          71.1495 |          18.2675 |
[32m[20221213 21:02:18 @agent_ppo2.py:185][0m |          -0.0071 |          71.0055 |          18.2661 |
[32m[20221213 21:02:18 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:02:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.00
[32m[20221213 21:02:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.00
[32m[20221213 21:02:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 574.00
[32m[20221213 21:02:18 @agent_ppo2.py:143][0m Total time:       6.72 min
[32m[20221213 21:02:18 @agent_ppo2.py:145][0m 659456 total steps have happened
[32m[20221213 21:02:18 @agent_ppo2.py:121][0m #------------------------ Iteration 322 --------------------------#
[32m[20221213 21:02:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:02:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:18 @agent_ppo2.py:185][0m |          -0.0000 |          74.3121 |          18.4093 |
[32m[20221213 21:02:18 @agent_ppo2.py:185][0m |           0.0086 |          81.5894 |          18.3872 |
[32m[20221213 21:02:18 @agent_ppo2.py:185][0m |          -0.0041 |          72.6362 |          18.3718 |
[32m[20221213 21:02:18 @agent_ppo2.py:185][0m |          -0.0059 |          72.0675 |          18.3766 |
[32m[20221213 21:02:19 @agent_ppo2.py:185][0m |           0.0014 |          75.5488 |          18.3671 |
[32m[20221213 21:02:19 @agent_ppo2.py:185][0m |          -0.0081 |          71.6134 |          18.3671 |
[32m[20221213 21:02:19 @agent_ppo2.py:185][0m |           0.0053 |          77.1477 |          18.3657 |
[32m[20221213 21:02:19 @agent_ppo2.py:185][0m |          -0.0062 |          71.1145 |          18.3604 |
[32m[20221213 21:02:19 @agent_ppo2.py:185][0m |          -0.0004 |          74.3889 |          18.3587 |
[32m[20221213 21:02:19 @agent_ppo2.py:185][0m |          -0.0060 |          70.8105 |          18.3591 |
[32m[20221213 21:02:19 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:02:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 525.20
[32m[20221213 21:02:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.00
[32m[20221213 21:02:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 586.00
[32m[20221213 21:02:19 @agent_ppo2.py:143][0m Total time:       6.74 min
[32m[20221213 21:02:19 @agent_ppo2.py:145][0m 661504 total steps have happened
[32m[20221213 21:02:19 @agent_ppo2.py:121][0m #------------------------ Iteration 323 --------------------------#
[32m[20221213 21:02:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:19 @agent_ppo2.py:185][0m |          -0.0026 |          77.7482 |          18.3338 |
[32m[20221213 21:02:20 @agent_ppo2.py:185][0m |          -0.0040 |          77.2906 |          18.3215 |
[32m[20221213 21:02:20 @agent_ppo2.py:185][0m |          -0.0052 |          76.9639 |          18.2996 |
[32m[20221213 21:02:20 @agent_ppo2.py:185][0m |          -0.0058 |          76.6538 |          18.2927 |
[32m[20221213 21:02:20 @agent_ppo2.py:185][0m |           0.0061 |          84.5802 |          18.2966 |
[32m[20221213 21:02:20 @agent_ppo2.py:185][0m |          -0.0003 |          77.3051 |          18.2559 |
[32m[20221213 21:02:20 @agent_ppo2.py:185][0m |           0.0032 |          80.9788 |          18.2751 |
[32m[20221213 21:02:20 @agent_ppo2.py:185][0m |          -0.0063 |          76.0978 |          18.2881 |
[32m[20221213 21:02:20 @agent_ppo2.py:185][0m |          -0.0096 |          76.1364 |          18.2876 |
[32m[20221213 21:02:20 @agent_ppo2.py:185][0m |           0.0030 |          81.3943 |          18.2861 |
[32m[20221213 21:02:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:02:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.40
[32m[20221213 21:02:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.00
[32m[20221213 21:02:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 570.00
[32m[20221213 21:02:20 @agent_ppo2.py:143][0m Total time:       6.76 min
[32m[20221213 21:02:20 @agent_ppo2.py:145][0m 663552 total steps have happened
[32m[20221213 21:02:20 @agent_ppo2.py:121][0m #------------------------ Iteration 324 --------------------------#
[32m[20221213 21:02:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:21 @agent_ppo2.py:185][0m |          -0.0008 |          78.7690 |          18.5100 |
[32m[20221213 21:02:21 @agent_ppo2.py:185][0m |          -0.0038 |          77.6940 |          18.5078 |
[32m[20221213 21:02:21 @agent_ppo2.py:185][0m |          -0.0044 |          77.3208 |          18.4953 |
[32m[20221213 21:02:21 @agent_ppo2.py:185][0m |          -0.0071 |          77.3729 |          18.4947 |
[32m[20221213 21:02:21 @agent_ppo2.py:185][0m |          -0.0083 |          77.0510 |          18.4926 |
[32m[20221213 21:02:21 @agent_ppo2.py:185][0m |          -0.0055 |          76.9212 |          18.4934 |
[32m[20221213 21:02:21 @agent_ppo2.py:185][0m |          -0.0051 |          76.7667 |          18.4879 |
[32m[20221213 21:02:21 @agent_ppo2.py:185][0m |          -0.0090 |          76.6968 |          18.4957 |
[32m[20221213 21:02:21 @agent_ppo2.py:185][0m |          -0.0082 |          76.6098 |          18.4927 |
[32m[20221213 21:02:21 @agent_ppo2.py:185][0m |          -0.0065 |          76.4639 |          18.4941 |
[32m[20221213 21:02:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:02:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.20
[32m[20221213 21:02:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.00
[32m[20221213 21:02:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.00
[32m[20221213 21:02:22 @agent_ppo2.py:143][0m Total time:       6.78 min
[32m[20221213 21:02:22 @agent_ppo2.py:145][0m 665600 total steps have happened
[32m[20221213 21:02:22 @agent_ppo2.py:121][0m #------------------------ Iteration 325 --------------------------#
[32m[20221213 21:02:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:02:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:22 @agent_ppo2.py:185][0m |          -0.0006 |          77.7953 |          18.3095 |
[32m[20221213 21:02:22 @agent_ppo2.py:185][0m |          -0.0033 |          77.3974 |          18.3010 |
[32m[20221213 21:02:22 @agent_ppo2.py:185][0m |           0.0017 |          78.0352 |          18.2960 |
[32m[20221213 21:02:22 @agent_ppo2.py:185][0m |          -0.0074 |          76.8199 |          18.2920 |
[32m[20221213 21:02:22 @agent_ppo2.py:185][0m |          -0.0050 |          76.5701 |          18.2858 |
[32m[20221213 21:02:22 @agent_ppo2.py:185][0m |          -0.0068 |          77.0368 |          18.2846 |
[32m[20221213 21:02:22 @agent_ppo2.py:185][0m |          -0.0078 |          76.3111 |          18.2906 |
[32m[20221213 21:02:22 @agent_ppo2.py:185][0m |          -0.0057 |          76.2280 |          18.2869 |
[32m[20221213 21:02:23 @agent_ppo2.py:185][0m |          -0.0069 |          76.1361 |          18.2833 |
[32m[20221213 21:02:23 @agent_ppo2.py:185][0m |          -0.0076 |          75.9895 |          18.2818 |
[32m[20221213 21:02:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:02:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.20
[32m[20221213 21:02:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.00
[32m[20221213 21:02:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.00
[32m[20221213 21:02:23 @agent_ppo2.py:143][0m Total time:       6.80 min
[32m[20221213 21:02:23 @agent_ppo2.py:145][0m 667648 total steps have happened
[32m[20221213 21:02:23 @agent_ppo2.py:121][0m #------------------------ Iteration 326 --------------------------#
[32m[20221213 21:02:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:23 @agent_ppo2.py:185][0m |          -0.0009 |          75.6446 |          18.2411 |
[32m[20221213 21:02:23 @agent_ppo2.py:185][0m |          -0.0057 |          74.8845 |          18.2184 |
[32m[20221213 21:02:23 @agent_ppo2.py:185][0m |          -0.0074 |          74.7793 |          18.2161 |
[32m[20221213 21:02:23 @agent_ppo2.py:185][0m |          -0.0032 |          74.4901 |          18.2135 |
[32m[20221213 21:02:23 @agent_ppo2.py:185][0m |          -0.0068 |          74.3194 |          18.2123 |
[32m[20221213 21:02:23 @agent_ppo2.py:185][0m |          -0.0083 |          74.2844 |          18.1990 |
[32m[20221213 21:02:24 @agent_ppo2.py:185][0m |          -0.0049 |          74.6995 |          18.2154 |
[32m[20221213 21:02:24 @agent_ppo2.py:185][0m |          -0.0064 |          74.0100 |          18.2014 |
[32m[20221213 21:02:24 @agent_ppo2.py:185][0m |          -0.0070 |          74.0074 |          18.2036 |
[32m[20221213 21:02:24 @agent_ppo2.py:185][0m |          -0.0069 |          73.8315 |          18.2059 |
[32m[20221213 21:02:24 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:02:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.40
[32m[20221213 21:02:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 557.00
[32m[20221213 21:02:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 371.00
[32m[20221213 21:02:24 @agent_ppo2.py:143][0m Total time:       6.82 min
[32m[20221213 21:02:24 @agent_ppo2.py:145][0m 669696 total steps have happened
[32m[20221213 21:02:24 @agent_ppo2.py:121][0m #------------------------ Iteration 327 --------------------------#
[32m[20221213 21:02:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:24 @agent_ppo2.py:185][0m |           0.0023 |          75.9615 |          18.4225 |
[32m[20221213 21:02:24 @agent_ppo2.py:185][0m |          -0.0026 |          75.3661 |          18.3984 |
[32m[20221213 21:02:24 @agent_ppo2.py:185][0m |           0.0030 |          77.1655 |          18.3907 |
[32m[20221213 21:02:25 @agent_ppo2.py:185][0m |          -0.0049 |          74.7279 |          18.3914 |
[32m[20221213 21:02:25 @agent_ppo2.py:185][0m |          -0.0064 |          74.4892 |          18.3853 |
[32m[20221213 21:02:25 @agent_ppo2.py:185][0m |           0.0003 |          76.6119 |          18.3837 |
[32m[20221213 21:02:25 @agent_ppo2.py:185][0m |          -0.0064 |          74.1442 |          18.3828 |
[32m[20221213 21:02:25 @agent_ppo2.py:185][0m |          -0.0067 |          74.0820 |          18.3854 |
[32m[20221213 21:02:25 @agent_ppo2.py:185][0m |          -0.0051 |          73.7519 |          18.3762 |
[32m[20221213 21:02:25 @agent_ppo2.py:185][0m |          -0.0067 |          73.6447 |          18.3736 |
[32m[20221213 21:02:25 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:02:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.40
[32m[20221213 21:02:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.00
[32m[20221213 21:02:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 551.00
[32m[20221213 21:02:25 @agent_ppo2.py:143][0m Total time:       6.84 min
[32m[20221213 21:02:25 @agent_ppo2.py:145][0m 671744 total steps have happened
[32m[20221213 21:02:25 @agent_ppo2.py:121][0m #------------------------ Iteration 328 --------------------------#
[32m[20221213 21:02:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:25 @agent_ppo2.py:185][0m |          -0.0016 |          74.4387 |          18.4346 |
[32m[20221213 21:02:26 @agent_ppo2.py:185][0m |          -0.0010 |          74.4306 |          18.4123 |
[32m[20221213 21:02:26 @agent_ppo2.py:185][0m |          -0.0062 |          73.8220 |          18.4050 |
[32m[20221213 21:02:26 @agent_ppo2.py:185][0m |          -0.0043 |          73.7030 |          18.3959 |
[32m[20221213 21:02:26 @agent_ppo2.py:185][0m |           0.0071 |          77.0099 |          18.3985 |
[32m[20221213 21:02:26 @agent_ppo2.py:185][0m |          -0.0058 |          73.5663 |          18.3825 |
[32m[20221213 21:02:26 @agent_ppo2.py:185][0m |          -0.0040 |          73.5896 |          18.3961 |
[32m[20221213 21:02:26 @agent_ppo2.py:185][0m |          -0.0060 |          73.4237 |          18.3877 |
[32m[20221213 21:02:26 @agent_ppo2.py:185][0m |          -0.0079 |          73.3552 |          18.3948 |
[32m[20221213 21:02:26 @agent_ppo2.py:185][0m |          -0.0078 |          73.4176 |          18.3900 |
[32m[20221213 21:02:26 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:02:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.20
[32m[20221213 21:02:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.00
[32m[20221213 21:02:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.00
[32m[20221213 21:02:26 @agent_ppo2.py:143][0m Total time:       6.86 min
[32m[20221213 21:02:26 @agent_ppo2.py:145][0m 673792 total steps have happened
[32m[20221213 21:02:26 @agent_ppo2.py:121][0m #------------------------ Iteration 329 --------------------------#
[32m[20221213 21:02:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:27 @agent_ppo2.py:185][0m |          -0.0003 |          78.1050 |          18.2239 |
[32m[20221213 21:02:27 @agent_ppo2.py:185][0m |          -0.0016 |          77.5308 |          18.2025 |
[32m[20221213 21:02:27 @agent_ppo2.py:185][0m |          -0.0045 |          77.2977 |          18.1949 |
[32m[20221213 21:02:27 @agent_ppo2.py:185][0m |          -0.0024 |          77.8116 |          18.1900 |
[32m[20221213 21:02:27 @agent_ppo2.py:185][0m |          -0.0055 |          77.0119 |          18.1851 |
[32m[20221213 21:02:27 @agent_ppo2.py:185][0m |          -0.0029 |          77.4837 |          18.1906 |
[32m[20221213 21:02:27 @agent_ppo2.py:185][0m |          -0.0073 |          76.8033 |          18.1867 |
[32m[20221213 21:02:27 @agent_ppo2.py:185][0m |          -0.0051 |          76.6312 |          18.1871 |
[32m[20221213 21:02:27 @agent_ppo2.py:185][0m |           0.0001 |          78.1254 |          18.1909 |
[32m[20221213 21:02:27 @agent_ppo2.py:185][0m |          -0.0075 |          76.6072 |          18.1915 |
[32m[20221213 21:02:27 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:02:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.80
[32m[20221213 21:02:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 21:02:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 563.00
[32m[20221213 21:02:28 @agent_ppo2.py:143][0m Total time:       6.88 min
[32m[20221213 21:02:28 @agent_ppo2.py:145][0m 675840 total steps have happened
[32m[20221213 21:02:28 @agent_ppo2.py:121][0m #------------------------ Iteration 330 --------------------------#
[32m[20221213 21:02:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:28 @agent_ppo2.py:185][0m |          -0.0006 |          78.1927 |          18.4567 |
[32m[20221213 21:02:28 @agent_ppo2.py:185][0m |          -0.0046 |          77.7511 |          18.4310 |
[32m[20221213 21:02:28 @agent_ppo2.py:185][0m |          -0.0061 |          77.4129 |          18.4305 |
[32m[20221213 21:02:28 @agent_ppo2.py:185][0m |          -0.0057 |          77.3698 |          18.4186 |
[32m[20221213 21:02:28 @agent_ppo2.py:185][0m |          -0.0040 |          77.2262 |          18.4307 |
[32m[20221213 21:02:28 @agent_ppo2.py:185][0m |          -0.0069 |          77.1289 |          18.4260 |
[32m[20221213 21:02:28 @agent_ppo2.py:185][0m |          -0.0078 |          77.0378 |          18.4272 |
[32m[20221213 21:02:29 @agent_ppo2.py:185][0m |          -0.0069 |          76.8757 |          18.4322 |
[32m[20221213 21:02:29 @agent_ppo2.py:185][0m |          -0.0107 |          76.9903 |          18.4219 |
[32m[20221213 21:02:29 @agent_ppo2.py:185][0m |           0.0039 |          87.2648 |          18.4295 |
[32m[20221213 21:02:29 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:02:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.80
[32m[20221213 21:02:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 571.00
[32m[20221213 21:02:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.00
[32m[20221213 21:02:29 @agent_ppo2.py:143][0m Total time:       6.90 min
[32m[20221213 21:02:29 @agent_ppo2.py:145][0m 677888 total steps have happened
[32m[20221213 21:02:29 @agent_ppo2.py:121][0m #------------------------ Iteration 331 --------------------------#
[32m[20221213 21:02:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:29 @agent_ppo2.py:185][0m |           0.0044 |          79.0965 |          18.3840 |
[32m[20221213 21:02:29 @agent_ppo2.py:185][0m |          -0.0089 |          77.2598 |          18.3687 |
[32m[20221213 21:02:29 @agent_ppo2.py:185][0m |          -0.0044 |          76.7828 |          18.3608 |
[32m[20221213 21:02:29 @agent_ppo2.py:185][0m |          -0.0066 |          76.3983 |          18.3542 |
[32m[20221213 21:02:29 @agent_ppo2.py:185][0m |          -0.0033 |          77.1601 |          18.3495 |
[32m[20221213 21:02:30 @agent_ppo2.py:185][0m |          -0.0081 |          75.9070 |          18.3551 |
[32m[20221213 21:02:30 @agent_ppo2.py:185][0m |          -0.0052 |          76.0113 |          18.3537 |
[32m[20221213 21:02:30 @agent_ppo2.py:185][0m |          -0.0040 |          75.3787 |          18.3555 |
[32m[20221213 21:02:30 @agent_ppo2.py:185][0m |          -0.0089 |          75.4310 |          18.3528 |
[32m[20221213 21:02:30 @agent_ppo2.py:185][0m |          -0.0123 |          75.2310 |          18.3503 |
[32m[20221213 21:02:30 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:02:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.20
[32m[20221213 21:02:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.00
[32m[20221213 21:02:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.00
[32m[20221213 21:02:30 @agent_ppo2.py:143][0m Total time:       6.92 min
[32m[20221213 21:02:30 @agent_ppo2.py:145][0m 679936 total steps have happened
[32m[20221213 21:02:30 @agent_ppo2.py:121][0m #------------------------ Iteration 332 --------------------------#
[32m[20221213 21:02:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:30 @agent_ppo2.py:185][0m |          -0.0004 |          78.6488 |          18.2514 |
[32m[20221213 21:02:30 @agent_ppo2.py:185][0m |          -0.0025 |          77.9107 |          18.2448 |
[32m[20221213 21:02:31 @agent_ppo2.py:185][0m |          -0.0058 |          77.5500 |          18.2455 |
[32m[20221213 21:02:31 @agent_ppo2.py:185][0m |          -0.0036 |          77.2988 |          18.2364 |
[32m[20221213 21:02:31 @agent_ppo2.py:185][0m |           0.0001 |          78.0209 |          18.2428 |
[32m[20221213 21:02:31 @agent_ppo2.py:185][0m |          -0.0060 |          77.0060 |          18.2338 |
[32m[20221213 21:02:31 @agent_ppo2.py:185][0m |           0.0027 |          80.6944 |          18.2328 |
[32m[20221213 21:02:31 @agent_ppo2.py:185][0m |          -0.0045 |          76.7711 |          18.2244 |
[32m[20221213 21:02:31 @agent_ppo2.py:185][0m |          -0.0055 |          76.5688 |          18.2148 |
[32m[20221213 21:02:31 @agent_ppo2.py:185][0m |          -0.0084 |          76.5595 |          18.2160 |
[32m[20221213 21:02:31 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:02:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.00
[32m[20221213 21:02:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 593.00
[32m[20221213 21:02:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.00
[32m[20221213 21:02:31 @agent_ppo2.py:143][0m Total time:       6.94 min
[32m[20221213 21:02:31 @agent_ppo2.py:145][0m 681984 total steps have happened
[32m[20221213 21:02:31 @agent_ppo2.py:121][0m #------------------------ Iteration 333 --------------------------#
[32m[20221213 21:02:31 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:02:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:32 @agent_ppo2.py:185][0m |           0.0004 |          79.0614 |          18.4181 |
[32m[20221213 21:02:32 @agent_ppo2.py:185][0m |          -0.0023 |          78.4379 |          18.4028 |
[32m[20221213 21:02:32 @agent_ppo2.py:185][0m |          -0.0037 |          78.2595 |          18.3969 |
[32m[20221213 21:02:32 @agent_ppo2.py:185][0m |          -0.0041 |          78.0486 |          18.3877 |
[32m[20221213 21:02:32 @agent_ppo2.py:185][0m |          -0.0032 |          78.3945 |          18.3897 |
[32m[20221213 21:02:32 @agent_ppo2.py:185][0m |          -0.0046 |          77.9108 |          18.3865 |
[32m[20221213 21:02:32 @agent_ppo2.py:185][0m |          -0.0078 |          77.7969 |          18.3812 |
[32m[20221213 21:02:32 @agent_ppo2.py:185][0m |          -0.0048 |          77.7837 |          18.3778 |
[32m[20221213 21:02:32 @agent_ppo2.py:185][0m |          -0.0048 |          78.2069 |          18.3775 |
[32m[20221213 21:02:32 @agent_ppo2.py:185][0m |          -0.0055 |          78.5806 |          18.3735 |
[32m[20221213 21:02:32 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:02:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 537.00
[32m[20221213 21:02:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.00
[32m[20221213 21:02:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.00
[32m[20221213 21:02:33 @agent_ppo2.py:143][0m Total time:       6.96 min
[32m[20221213 21:02:33 @agent_ppo2.py:145][0m 684032 total steps have happened
[32m[20221213 21:02:33 @agent_ppo2.py:121][0m #------------------------ Iteration 334 --------------------------#
[32m[20221213 21:02:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:33 @agent_ppo2.py:185][0m |          -0.0013 |          76.0263 |          18.3022 |
[32m[20221213 21:02:33 @agent_ppo2.py:185][0m |          -0.0033 |          75.3011 |          18.2789 |
[32m[20221213 21:02:33 @agent_ppo2.py:185][0m |          -0.0043 |          75.1636 |          18.2937 |
[32m[20221213 21:02:33 @agent_ppo2.py:185][0m |           0.0020 |          76.1952 |          18.2749 |
[32m[20221213 21:02:33 @agent_ppo2.py:185][0m |           0.0020 |          78.3241 |          18.2818 |
[32m[20221213 21:02:33 @agent_ppo2.py:185][0m |          -0.0034 |          74.7630 |          18.2699 |
[32m[20221213 21:02:33 @agent_ppo2.py:185][0m |          -0.0006 |          75.3843 |          18.2745 |
[32m[20221213 21:02:33 @agent_ppo2.py:185][0m |           0.0055 |          81.1721 |          18.2818 |
[32m[20221213 21:02:34 @agent_ppo2.py:185][0m |           0.0002 |          76.3126 |          18.2712 |
[32m[20221213 21:02:34 @agent_ppo2.py:185][0m |          -0.0059 |          74.4724 |          18.2731 |
[32m[20221213 21:02:34 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:02:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 517.20
[32m[20221213 21:02:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 557.00
[32m[20221213 21:02:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.00
[32m[20221213 21:02:34 @agent_ppo2.py:143][0m Total time:       6.98 min
[32m[20221213 21:02:34 @agent_ppo2.py:145][0m 686080 total steps have happened
[32m[20221213 21:02:34 @agent_ppo2.py:121][0m #------------------------ Iteration 335 --------------------------#
[32m[20221213 21:02:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:02:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:34 @agent_ppo2.py:185][0m |           0.0112 |          85.4389 |          18.1614 |
[32m[20221213 21:02:34 @agent_ppo2.py:185][0m |          -0.0073 |          78.2365 |          18.1195 |
[32m[20221213 21:02:34 @agent_ppo2.py:185][0m |          -0.0061 |          77.0678 |          18.1160 |
[32m[20221213 21:02:34 @agent_ppo2.py:185][0m |          -0.0067 |          76.5406 |          18.1167 |
[32m[20221213 21:02:34 @agent_ppo2.py:185][0m |          -0.0057 |          76.0275 |          18.1225 |
[32m[20221213 21:02:35 @agent_ppo2.py:185][0m |          -0.0077 |          75.4807 |          18.1162 |
[32m[20221213 21:02:35 @agent_ppo2.py:185][0m |           0.0076 |          84.3047 |          18.1227 |
[32m[20221213 21:02:35 @agent_ppo2.py:185][0m |          -0.0093 |          74.9730 |          18.1220 |
[32m[20221213 21:02:35 @agent_ppo2.py:185][0m |          -0.0075 |          74.4132 |          18.1178 |
[32m[20221213 21:02:35 @agent_ppo2.py:185][0m |          -0.0075 |          74.2144 |          18.1174 |
[32m[20221213 21:02:35 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:02:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.00
[32m[20221213 21:02:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 541.00
[32m[20221213 21:02:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.00
[32m[20221213 21:02:35 @agent_ppo2.py:143][0m Total time:       7.01 min
[32m[20221213 21:02:35 @agent_ppo2.py:145][0m 688128 total steps have happened
[32m[20221213 21:02:35 @agent_ppo2.py:121][0m #------------------------ Iteration 336 --------------------------#
[32m[20221213 21:02:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:35 @agent_ppo2.py:185][0m |          -0.0020 |          78.8149 |          18.3236 |
[32m[20221213 21:02:35 @agent_ppo2.py:185][0m |          -0.0039 |          76.9718 |          18.3179 |
[32m[20221213 21:02:36 @agent_ppo2.py:185][0m |           0.0059 |          81.1722 |          18.3174 |
[32m[20221213 21:02:36 @agent_ppo2.py:185][0m |          -0.0042 |          76.3193 |          18.2938 |
[32m[20221213 21:02:36 @agent_ppo2.py:185][0m |          -0.0045 |          75.7758 |          18.3032 |
[32m[20221213 21:02:36 @agent_ppo2.py:185][0m |           0.0013 |          79.2425 |          18.2982 |
[32m[20221213 21:02:36 @agent_ppo2.py:185][0m |          -0.0003 |          76.8397 |          18.2875 |
[32m[20221213 21:02:36 @agent_ppo2.py:185][0m |          -0.0030 |          75.3668 |          18.2937 |
[32m[20221213 21:02:36 @agent_ppo2.py:185][0m |           0.0108 |          80.5992 |          18.2970 |
[32m[20221213 21:02:36 @agent_ppo2.py:185][0m |          -0.0058 |          75.2235 |          18.2963 |
[32m[20221213 21:02:36 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:02:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.40
[32m[20221213 21:02:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.00
[32m[20221213 21:02:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.00
[32m[20221213 21:02:36 @agent_ppo2.py:143][0m Total time:       7.03 min
[32m[20221213 21:02:36 @agent_ppo2.py:145][0m 690176 total steps have happened
[32m[20221213 21:02:36 @agent_ppo2.py:121][0m #------------------------ Iteration 337 --------------------------#
[32m[20221213 21:02:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:37 @agent_ppo2.py:185][0m |          -0.0020 |          78.4193 |          18.3061 |
[32m[20221213 21:02:37 @agent_ppo2.py:185][0m |          -0.0022 |          77.8022 |          18.2957 |
[32m[20221213 21:02:37 @agent_ppo2.py:185][0m |          -0.0062 |          77.4892 |          18.2974 |
[32m[20221213 21:02:37 @agent_ppo2.py:185][0m |          -0.0056 |          77.2019 |          18.2946 |
[32m[20221213 21:02:37 @agent_ppo2.py:185][0m |          -0.0013 |          77.2654 |          18.2764 |
[32m[20221213 21:02:37 @agent_ppo2.py:185][0m |          -0.0011 |          77.6939 |          18.2867 |
[32m[20221213 21:02:37 @agent_ppo2.py:185][0m |          -0.0053 |          76.6575 |          18.2852 |
[32m[20221213 21:02:37 @agent_ppo2.py:185][0m |          -0.0059 |          76.3850 |          18.2823 |
[32m[20221213 21:02:37 @agent_ppo2.py:185][0m |          -0.0069 |          76.2596 |          18.2765 |
[32m[20221213 21:02:37 @agent_ppo2.py:185][0m |          -0.0069 |          76.1830 |          18.2743 |
[32m[20221213 21:02:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:02:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.00
[32m[20221213 21:02:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.00
[32m[20221213 21:02:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 635.00
[32m[20221213 21:02:37 @agent_ppo2.py:143][0m Total time:       7.05 min
[32m[20221213 21:02:37 @agent_ppo2.py:145][0m 692224 total steps have happened
[32m[20221213 21:02:37 @agent_ppo2.py:121][0m #------------------------ Iteration 338 --------------------------#
[32m[20221213 21:02:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:02:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:38 @agent_ppo2.py:185][0m |          -0.0043 |          82.0616 |          18.4632 |
[32m[20221213 21:02:38 @agent_ppo2.py:185][0m |           0.0001 |          79.6558 |          18.4522 |
[32m[20221213 21:02:38 @agent_ppo2.py:185][0m |          -0.0055 |          78.1518 |          18.4529 |
[32m[20221213 21:02:38 @agent_ppo2.py:185][0m |          -0.0076 |          77.2102 |          18.4491 |
[32m[20221213 21:02:38 @agent_ppo2.py:185][0m |          -0.0071 |          76.4828 |          18.4506 |
[32m[20221213 21:02:38 @agent_ppo2.py:185][0m |          -0.0055 |          75.6030 |          18.4460 |
[32m[20221213 21:02:38 @agent_ppo2.py:185][0m |          -0.0012 |          76.5273 |          18.4441 |
[32m[20221213 21:02:38 @agent_ppo2.py:185][0m |          -0.0039 |          74.2863 |          18.4471 |
[32m[20221213 21:02:38 @agent_ppo2.py:185][0m |          -0.0087 |          73.7792 |          18.4486 |
[32m[20221213 21:02:39 @agent_ppo2.py:185][0m |          -0.0074 |          72.9079 |          18.4405 |
[32m[20221213 21:02:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:02:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.20
[32m[20221213 21:02:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 585.00
[32m[20221213 21:02:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.00
[32m[20221213 21:02:39 @agent_ppo2.py:143][0m Total time:       7.07 min
[32m[20221213 21:02:39 @agent_ppo2.py:145][0m 694272 total steps have happened
[32m[20221213 21:02:39 @agent_ppo2.py:121][0m #------------------------ Iteration 339 --------------------------#
[32m[20221213 21:02:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:39 @agent_ppo2.py:185][0m |          -0.0029 |          87.5571 |          18.4559 |
[32m[20221213 21:02:39 @agent_ppo2.py:185][0m |          -0.0043 |          86.2202 |          18.4430 |
[32m[20221213 21:02:39 @agent_ppo2.py:185][0m |          -0.0052 |          85.8741 |          18.4338 |
[32m[20221213 21:02:39 @agent_ppo2.py:185][0m |           0.0032 |          92.0936 |          18.4338 |
[32m[20221213 21:02:39 @agent_ppo2.py:185][0m |          -0.0031 |          85.4728 |          18.4230 |
[32m[20221213 21:02:39 @agent_ppo2.py:185][0m |           0.0024 |          89.2709 |          18.4232 |
[32m[20221213 21:02:40 @agent_ppo2.py:185][0m |          -0.0043 |          84.5880 |          18.4270 |
[32m[20221213 21:02:40 @agent_ppo2.py:185][0m |          -0.0018 |          86.2724 |          18.4295 |
[32m[20221213 21:02:40 @agent_ppo2.py:185][0m |          -0.0073 |          84.5007 |          18.4066 |
[32m[20221213 21:02:40 @agent_ppo2.py:185][0m |          -0.0052 |          84.2133 |          18.4185 |
[32m[20221213 21:02:40 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:02:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.80
[32m[20221213 21:02:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.00
[32m[20221213 21:02:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 195.00
[32m[20221213 21:02:40 @agent_ppo2.py:143][0m Total time:       7.09 min
[32m[20221213 21:02:40 @agent_ppo2.py:145][0m 696320 total steps have happened
[32m[20221213 21:02:40 @agent_ppo2.py:121][0m #------------------------ Iteration 340 --------------------------#
[32m[20221213 21:02:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:40 @agent_ppo2.py:185][0m |          -0.0010 |          80.9746 |          18.4295 |
[32m[20221213 21:02:40 @agent_ppo2.py:185][0m |          -0.0009 |          80.0042 |          18.3935 |
[32m[20221213 21:02:40 @agent_ppo2.py:185][0m |          -0.0004 |          81.2995 |          18.3802 |
[32m[20221213 21:02:40 @agent_ppo2.py:185][0m |           0.0051 |          85.7656 |          18.3822 |
[32m[20221213 21:02:41 @agent_ppo2.py:185][0m |          -0.0077 |          78.5032 |          18.3745 |
[32m[20221213 21:02:41 @agent_ppo2.py:185][0m |          -0.0053 |          78.1465 |          18.3751 |
[32m[20221213 21:02:41 @agent_ppo2.py:185][0m |          -0.0074 |          77.9506 |          18.3812 |
[32m[20221213 21:02:41 @agent_ppo2.py:185][0m |          -0.0099 |          77.8838 |          18.3910 |
[32m[20221213 21:02:41 @agent_ppo2.py:185][0m |          -0.0026 |          79.9715 |          18.3829 |
[32m[20221213 21:02:41 @agent_ppo2.py:185][0m |          -0.0097 |          77.7046 |          18.3921 |
[32m[20221213 21:02:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:02:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.40
[32m[20221213 21:02:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.00
[32m[20221213 21:02:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 585.00
[32m[20221213 21:02:41 @agent_ppo2.py:143][0m Total time:       7.11 min
[32m[20221213 21:02:41 @agent_ppo2.py:145][0m 698368 total steps have happened
[32m[20221213 21:02:41 @agent_ppo2.py:121][0m #------------------------ Iteration 341 --------------------------#
[32m[20221213 21:02:41 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:02:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:41 @agent_ppo2.py:185][0m |          -0.0008 |          81.3725 |          18.4914 |
[32m[20221213 21:02:41 @agent_ppo2.py:185][0m |          -0.0022 |          80.4477 |          18.4789 |
[32m[20221213 21:02:42 @agent_ppo2.py:185][0m |          -0.0053 |          79.7850 |          18.4651 |
[32m[20221213 21:02:42 @agent_ppo2.py:185][0m |          -0.0045 |          79.4160 |          18.4734 |
[32m[20221213 21:02:42 @agent_ppo2.py:185][0m |          -0.0051 |          79.0622 |          18.4693 |
[32m[20221213 21:02:42 @agent_ppo2.py:185][0m |           0.0012 |          79.5037 |          18.4715 |
[32m[20221213 21:02:42 @agent_ppo2.py:185][0m |          -0.0051 |          78.6064 |          18.4698 |
[32m[20221213 21:02:42 @agent_ppo2.py:185][0m |           0.0021 |          83.5656 |          18.4653 |
[32m[20221213 21:02:42 @agent_ppo2.py:185][0m |          -0.0031 |          78.9441 |          18.4567 |
[32m[20221213 21:02:42 @agent_ppo2.py:185][0m |          -0.0084 |          78.4835 |          18.4620 |
[32m[20221213 21:02:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:02:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 552.80
[32m[20221213 21:02:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 569.00
[32m[20221213 21:02:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.00
[32m[20221213 21:02:42 @agent_ppo2.py:143][0m Total time:       7.13 min
[32m[20221213 21:02:42 @agent_ppo2.py:145][0m 700416 total steps have happened
[32m[20221213 21:02:42 @agent_ppo2.py:121][0m #------------------------ Iteration 342 --------------------------#
[32m[20221213 21:02:42 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:02:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:43 @agent_ppo2.py:185][0m |           0.0022 |          81.5627 |          18.3416 |
[32m[20221213 21:02:43 @agent_ppo2.py:185][0m |          -0.0024 |          80.3266 |          18.3348 |
[32m[20221213 21:02:43 @agent_ppo2.py:185][0m |          -0.0086 |          79.9261 |          18.3243 |
[32m[20221213 21:02:43 @agent_ppo2.py:185][0m |          -0.0082 |          79.5588 |          18.3142 |
[32m[20221213 21:02:43 @agent_ppo2.py:185][0m |          -0.0083 |          79.3576 |          18.3113 |
[32m[20221213 21:02:43 @agent_ppo2.py:185][0m |          -0.0088 |          79.1035 |          18.3080 |
[32m[20221213 21:02:43 @agent_ppo2.py:185][0m |          -0.0029 |          81.2785 |          18.3079 |
[32m[20221213 21:02:43 @agent_ppo2.py:185][0m |          -0.0046 |          79.6845 |          18.2971 |
[32m[20221213 21:02:43 @agent_ppo2.py:185][0m |          -0.0094 |          78.7716 |          18.3000 |
[32m[20221213 21:02:43 @agent_ppo2.py:185][0m |          -0.0034 |          79.4896 |          18.3030 |
[32m[20221213 21:02:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:02:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.20
[32m[20221213 21:02:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.00
[32m[20221213 21:02:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.00
[32m[20221213 21:02:43 @agent_ppo2.py:143][0m Total time:       7.15 min
[32m[20221213 21:02:43 @agent_ppo2.py:145][0m 702464 total steps have happened
[32m[20221213 21:02:43 @agent_ppo2.py:121][0m #------------------------ Iteration 343 --------------------------#
[32m[20221213 21:02:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:44 @agent_ppo2.py:185][0m |          -0.0030 |          80.7492 |          18.3539 |
[32m[20221213 21:02:44 @agent_ppo2.py:185][0m |          -0.0017 |          80.5880 |          18.3444 |
[32m[20221213 21:02:44 @agent_ppo2.py:185][0m |           0.0014 |          82.5964 |          18.3485 |
[32m[20221213 21:02:44 @agent_ppo2.py:185][0m |          -0.0067 |          79.9434 |          18.3523 |
[32m[20221213 21:02:44 @agent_ppo2.py:185][0m |          -0.0047 |          80.0242 |          18.3448 |
[32m[20221213 21:02:44 @agent_ppo2.py:185][0m |          -0.0067 |          79.4862 |          18.3529 |
[32m[20221213 21:02:44 @agent_ppo2.py:185][0m |          -0.0087 |          79.4423 |          18.3514 |
[32m[20221213 21:02:44 @agent_ppo2.py:185][0m |           0.0132 |          90.4439 |          18.3436 |
[32m[20221213 21:02:44 @agent_ppo2.py:185][0m |          -0.0075 |          79.4106 |          18.3430 |
[32m[20221213 21:02:45 @agent_ppo2.py:185][0m |          -0.0055 |          79.2096 |          18.3407 |
[32m[20221213 21:02:45 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:02:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 530.60
[32m[20221213 21:02:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 601.00
[32m[20221213 21:02:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.00
[32m[20221213 21:02:45 @agent_ppo2.py:143][0m Total time:       7.17 min
[32m[20221213 21:02:45 @agent_ppo2.py:145][0m 704512 total steps have happened
[32m[20221213 21:02:45 @agent_ppo2.py:121][0m #------------------------ Iteration 344 --------------------------#
[32m[20221213 21:02:45 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:02:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:45 @agent_ppo2.py:185][0m |          -0.0023 |          79.4047 |          18.4068 |
[32m[20221213 21:02:45 @agent_ppo2.py:185][0m |           0.0015 |          81.3771 |          18.4006 |
[32m[20221213 21:02:45 @agent_ppo2.py:185][0m |          -0.0059 |          78.9117 |          18.3921 |
[32m[20221213 21:02:45 @agent_ppo2.py:185][0m |          -0.0067 |          78.3927 |          18.3953 |
[32m[20221213 21:02:45 @agent_ppo2.py:185][0m |          -0.0104 |          78.5198 |          18.3951 |
[32m[20221213 21:02:45 @agent_ppo2.py:185][0m |          -0.0070 |          78.1175 |          18.3901 |
[32m[20221213 21:02:46 @agent_ppo2.py:185][0m |           0.0018 |          82.6503 |          18.3915 |
[32m[20221213 21:02:46 @agent_ppo2.py:185][0m |          -0.0091 |          78.0658 |          18.3730 |
[32m[20221213 21:02:46 @agent_ppo2.py:185][0m |          -0.0027 |          80.2850 |          18.3760 |
[32m[20221213 21:02:46 @agent_ppo2.py:185][0m |          -0.0043 |          77.8976 |          18.3774 |
[32m[20221213 21:02:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:02:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.20
[32m[20221213 21:02:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.00
[32m[20221213 21:02:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 578.00
[32m[20221213 21:02:46 @agent_ppo2.py:143][0m Total time:       7.19 min
[32m[20221213 21:02:46 @agent_ppo2.py:145][0m 706560 total steps have happened
[32m[20221213 21:02:46 @agent_ppo2.py:121][0m #------------------------ Iteration 345 --------------------------#
[32m[20221213 21:02:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:46 @agent_ppo2.py:185][0m |          -0.0001 |          79.1742 |          18.4177 |
[32m[20221213 21:02:46 @agent_ppo2.py:185][0m |           0.0002 |          79.1608 |          18.4044 |
[32m[20221213 21:02:46 @agent_ppo2.py:185][0m |          -0.0042 |          77.8748 |          18.3931 |
[32m[20221213 21:02:46 @agent_ppo2.py:185][0m |          -0.0040 |          77.6411 |          18.3949 |
[32m[20221213 21:02:47 @agent_ppo2.py:185][0m |          -0.0023 |          77.8361 |          18.3907 |
[32m[20221213 21:02:47 @agent_ppo2.py:185][0m |          -0.0033 |          77.2526 |          18.3877 |
[32m[20221213 21:02:47 @agent_ppo2.py:185][0m |           0.0060 |          80.6560 |          18.3889 |
[32m[20221213 21:02:47 @agent_ppo2.py:185][0m |          -0.0066 |          77.0728 |          18.3842 |
[32m[20221213 21:02:47 @agent_ppo2.py:185][0m |          -0.0071 |          77.0267 |          18.3884 |
[32m[20221213 21:02:47 @agent_ppo2.py:185][0m |          -0.0055 |          77.0069 |          18.3800 |
[32m[20221213 21:02:47 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:02:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.80
[32m[20221213 21:02:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.00
[32m[20221213 21:02:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.00
[32m[20221213 21:02:47 @agent_ppo2.py:143][0m Total time:       7.21 min
[32m[20221213 21:02:47 @agent_ppo2.py:145][0m 708608 total steps have happened
[32m[20221213 21:02:47 @agent_ppo2.py:121][0m #------------------------ Iteration 346 --------------------------#
[32m[20221213 21:02:47 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:02:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:47 @agent_ppo2.py:185][0m |          -0.0008 |          82.4599 |          18.4379 |
[32m[20221213 21:02:48 @agent_ppo2.py:185][0m |          -0.0039 |          81.5530 |          18.4228 |
[32m[20221213 21:02:48 @agent_ppo2.py:185][0m |           0.0005 |          81.2273 |          18.4223 |
[32m[20221213 21:02:48 @agent_ppo2.py:185][0m |           0.0055 |          85.3284 |          18.4234 |
[32m[20221213 21:02:48 @agent_ppo2.py:185][0m |          -0.0060 |          80.6965 |          18.4141 |
[32m[20221213 21:02:48 @agent_ppo2.py:185][0m |          -0.0068 |          80.2697 |          18.4147 |
[32m[20221213 21:02:48 @agent_ppo2.py:185][0m |          -0.0003 |          84.3068 |          18.4064 |
[32m[20221213 21:02:48 @agent_ppo2.py:185][0m |          -0.0059 |          79.8747 |          18.4039 |
[32m[20221213 21:02:48 @agent_ppo2.py:185][0m |          -0.0075 |          79.8177 |          18.4131 |
[32m[20221213 21:02:48 @agent_ppo2.py:185][0m |          -0.0077 |          79.6744 |          18.4087 |
[32m[20221213 21:02:48 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:02:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 507.40
[32m[20221213 21:02:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.00
[32m[20221213 21:02:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.00
[32m[20221213 21:02:48 @agent_ppo2.py:143][0m Total time:       7.23 min
[32m[20221213 21:02:48 @agent_ppo2.py:145][0m 710656 total steps have happened
[32m[20221213 21:02:48 @agent_ppo2.py:121][0m #------------------------ Iteration 347 --------------------------#
[32m[20221213 21:02:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:49 @agent_ppo2.py:185][0m |           0.0010 |          81.8828 |          18.4247 |
[32m[20221213 21:02:49 @agent_ppo2.py:185][0m |          -0.0034 |          80.8103 |          18.4168 |
[32m[20221213 21:02:49 @agent_ppo2.py:185][0m |          -0.0036 |          80.4211 |          18.4152 |
[32m[20221213 21:02:49 @agent_ppo2.py:185][0m |          -0.0029 |          80.0882 |          18.4074 |
[32m[20221213 21:02:49 @agent_ppo2.py:185][0m |           0.0040 |          85.6932 |          18.4106 |
[32m[20221213 21:02:49 @agent_ppo2.py:185][0m |          -0.0089 |          79.8772 |          18.4192 |
[32m[20221213 21:02:49 @agent_ppo2.py:185][0m |          -0.0064 |          79.6782 |          18.4138 |
[32m[20221213 21:02:49 @agent_ppo2.py:185][0m |          -0.0065 |          79.6624 |          18.4179 |
[32m[20221213 21:02:49 @agent_ppo2.py:185][0m |          -0.0066 |          79.5346 |          18.4137 |
[32m[20221213 21:02:49 @agent_ppo2.py:185][0m |          -0.0041 |          79.3584 |          18.4139 |
[32m[20221213 21:02:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:02:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.40
[32m[20221213 21:02:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.00
[32m[20221213 21:02:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.00
[32m[20221213 21:02:50 @agent_ppo2.py:143][0m Total time:       7.25 min
[32m[20221213 21:02:50 @agent_ppo2.py:145][0m 712704 total steps have happened
[32m[20221213 21:02:50 @agent_ppo2.py:121][0m #------------------------ Iteration 348 --------------------------#
[32m[20221213 21:02:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:02:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:50 @agent_ppo2.py:185][0m |           0.0028 |          79.1988 |          18.1919 |
[32m[20221213 21:02:50 @agent_ppo2.py:185][0m |          -0.0014 |          77.6001 |          18.1809 |
[32m[20221213 21:02:50 @agent_ppo2.py:185][0m |          -0.0031 |          77.7133 |          18.1693 |
[32m[20221213 21:02:50 @agent_ppo2.py:185][0m |          -0.0057 |          76.8315 |          18.1648 |
[32m[20221213 21:02:50 @agent_ppo2.py:185][0m |          -0.0051 |          76.7286 |          18.1702 |
[32m[20221213 21:02:50 @agent_ppo2.py:185][0m |          -0.0083 |          76.6684 |          18.1644 |
[32m[20221213 21:02:50 @agent_ppo2.py:185][0m |          -0.0095 |          76.5662 |          18.1611 |
[32m[20221213 21:02:50 @agent_ppo2.py:185][0m |          -0.0061 |          76.2787 |          18.1740 |
[32m[20221213 21:02:51 @agent_ppo2.py:185][0m |          -0.0078 |          76.4002 |          18.1710 |
[32m[20221213 21:02:51 @agent_ppo2.py:185][0m |          -0.0081 |          76.3174 |          18.1623 |
[32m[20221213 21:02:51 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:02:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.40
[32m[20221213 21:02:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 572.00
[32m[20221213 21:02:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 547.00
[32m[20221213 21:02:51 @agent_ppo2.py:143][0m Total time:       7.27 min
[32m[20221213 21:02:51 @agent_ppo2.py:145][0m 714752 total steps have happened
[32m[20221213 21:02:51 @agent_ppo2.py:121][0m #------------------------ Iteration 349 --------------------------#
[32m[20221213 21:02:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:51 @agent_ppo2.py:185][0m |           0.0100 |          91.1331 |          18.4105 |
[32m[20221213 21:02:51 @agent_ppo2.py:185][0m |          -0.0050 |          81.2505 |          18.3916 |
[32m[20221213 21:02:51 @agent_ppo2.py:185][0m |          -0.0057 |          80.8373 |          18.3875 |
[32m[20221213 21:02:51 @agent_ppo2.py:185][0m |          -0.0056 |          80.6560 |          18.3911 |
[32m[20221213 21:02:51 @agent_ppo2.py:185][0m |          -0.0051 |          80.5517 |          18.3889 |
[32m[20221213 21:02:52 @agent_ppo2.py:185][0m |          -0.0019 |          80.0666 |          18.3887 |
[32m[20221213 21:02:52 @agent_ppo2.py:185][0m |          -0.0053 |          79.9413 |          18.3868 |
[32m[20221213 21:02:52 @agent_ppo2.py:185][0m |          -0.0016 |          79.9572 |          18.3887 |
[32m[20221213 21:02:52 @agent_ppo2.py:185][0m |          -0.0086 |          79.6661 |          18.3899 |
[32m[20221213 21:02:52 @agent_ppo2.py:185][0m |           0.0016 |          82.6839 |          18.3857 |
[32m[20221213 21:02:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:02:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.40
[32m[20221213 21:02:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.00
[32m[20221213 21:02:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.00
[32m[20221213 21:02:52 @agent_ppo2.py:143][0m Total time:       7.29 min
[32m[20221213 21:02:52 @agent_ppo2.py:145][0m 716800 total steps have happened
[32m[20221213 21:02:52 @agent_ppo2.py:121][0m #------------------------ Iteration 350 --------------------------#
[32m[20221213 21:02:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:52 @agent_ppo2.py:185][0m |           0.0003 |          78.6287 |          18.3183 |
[32m[20221213 21:02:52 @agent_ppo2.py:185][0m |          -0.0007 |          78.0559 |          18.2931 |
[32m[20221213 21:02:52 @agent_ppo2.py:185][0m |          -0.0007 |          78.3724 |          18.2791 |
[32m[20221213 21:02:53 @agent_ppo2.py:185][0m |          -0.0039 |          76.9543 |          18.2727 |
[32m[20221213 21:02:53 @agent_ppo2.py:185][0m |          -0.0045 |          76.5979 |          18.2800 |
[32m[20221213 21:02:53 @agent_ppo2.py:185][0m |          -0.0059 |          76.5491 |          18.2730 |
[32m[20221213 21:02:53 @agent_ppo2.py:185][0m |          -0.0066 |          76.3202 |          18.2725 |
[32m[20221213 21:02:53 @agent_ppo2.py:185][0m |          -0.0047 |          76.1004 |          18.2636 |
[32m[20221213 21:02:53 @agent_ppo2.py:185][0m |          -0.0003 |          79.5023 |          18.2700 |
[32m[20221213 21:02:53 @agent_ppo2.py:185][0m |          -0.0065 |          75.9388 |          18.2731 |
[32m[20221213 21:02:53 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:02:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.40
[32m[20221213 21:02:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.00
[32m[20221213 21:02:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 565.00
[32m[20221213 21:02:53 @agent_ppo2.py:143][0m Total time:       7.31 min
[32m[20221213 21:02:53 @agent_ppo2.py:145][0m 718848 total steps have happened
[32m[20221213 21:02:53 @agent_ppo2.py:121][0m #------------------------ Iteration 351 --------------------------#
[32m[20221213 21:02:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:02:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:54 @agent_ppo2.py:185][0m |          -0.0010 |          79.7944 |          18.3042 |
[32m[20221213 21:02:54 @agent_ppo2.py:185][0m |           0.0017 |          80.4967 |          18.2963 |
[32m[20221213 21:02:54 @agent_ppo2.py:185][0m |          -0.0045 |          78.8466 |          18.2934 |
[32m[20221213 21:02:54 @agent_ppo2.py:185][0m |          -0.0051 |          78.5898 |          18.2906 |
[32m[20221213 21:02:54 @agent_ppo2.py:185][0m |          -0.0046 |          78.4571 |          18.2849 |
[32m[20221213 21:02:54 @agent_ppo2.py:185][0m |          -0.0057 |          78.3768 |          18.2872 |
[32m[20221213 21:02:54 @agent_ppo2.py:185][0m |          -0.0041 |          78.1610 |          18.2887 |
[32m[20221213 21:02:54 @agent_ppo2.py:185][0m |          -0.0022 |          78.0836 |          18.2815 |
[32m[20221213 21:02:54 @agent_ppo2.py:185][0m |          -0.0035 |          78.5350 |          18.2800 |
[32m[20221213 21:02:54 @agent_ppo2.py:185][0m |          -0.0055 |          77.8428 |          18.2709 |
[32m[20221213 21:02:54 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:02:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.80
[32m[20221213 21:02:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.00
[32m[20221213 21:02:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.00
[32m[20221213 21:02:54 @agent_ppo2.py:143][0m Total time:       7.33 min
[32m[20221213 21:02:54 @agent_ppo2.py:145][0m 720896 total steps have happened
[32m[20221213 21:02:54 @agent_ppo2.py:121][0m #------------------------ Iteration 352 --------------------------#
[32m[20221213 21:02:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:55 @agent_ppo2.py:185][0m |          -0.0009 |          79.2964 |          18.6080 |
[32m[20221213 21:02:55 @agent_ppo2.py:185][0m |          -0.0026 |          78.7586 |          18.6014 |
[32m[20221213 21:02:55 @agent_ppo2.py:185][0m |          -0.0060 |          78.6245 |          18.5961 |
[32m[20221213 21:02:55 @agent_ppo2.py:185][0m |          -0.0027 |          78.5894 |          18.5983 |
[32m[20221213 21:02:55 @agent_ppo2.py:185][0m |          -0.0059 |          78.2366 |          18.5923 |
[32m[20221213 21:02:55 @agent_ppo2.py:185][0m |          -0.0056 |          78.0866 |          18.5995 |
[32m[20221213 21:02:55 @agent_ppo2.py:185][0m |          -0.0059 |          78.0040 |          18.5869 |
[32m[20221213 21:02:55 @agent_ppo2.py:185][0m |          -0.0071 |          77.9779 |          18.5970 |
[32m[20221213 21:02:55 @agent_ppo2.py:185][0m |           0.0038 |          81.3277 |          18.5932 |
[32m[20221213 21:02:56 @agent_ppo2.py:185][0m |          -0.0019 |          77.8770 |          18.5965 |
[32m[20221213 21:02:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:02:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.40
[32m[20221213 21:02:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.00
[32m[20221213 21:02:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.00
[32m[20221213 21:02:56 @agent_ppo2.py:143][0m Total time:       7.35 min
[32m[20221213 21:02:56 @agent_ppo2.py:145][0m 722944 total steps have happened
[32m[20221213 21:02:56 @agent_ppo2.py:121][0m #------------------------ Iteration 353 --------------------------#
[32m[20221213 21:02:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:56 @agent_ppo2.py:185][0m |          -0.0015 |          78.0669 |          18.3982 |
[32m[20221213 21:02:56 @agent_ppo2.py:185][0m |          -0.0032 |          77.4508 |          18.4033 |
[32m[20221213 21:02:56 @agent_ppo2.py:185][0m |          -0.0064 |          77.1895 |          18.3979 |
[32m[20221213 21:02:56 @agent_ppo2.py:185][0m |          -0.0032 |          76.8021 |          18.3965 |
[32m[20221213 21:02:56 @agent_ppo2.py:185][0m |          -0.0044 |          76.5595 |          18.3977 |
[32m[20221213 21:02:56 @agent_ppo2.py:185][0m |          -0.0056 |          76.4642 |          18.3897 |
[32m[20221213 21:02:56 @agent_ppo2.py:185][0m |          -0.0059 |          76.2707 |          18.4038 |
[32m[20221213 21:02:57 @agent_ppo2.py:185][0m |          -0.0049 |          76.1046 |          18.3953 |
[32m[20221213 21:02:57 @agent_ppo2.py:185][0m |          -0.0042 |          76.1317 |          18.3938 |
[32m[20221213 21:02:57 @agent_ppo2.py:185][0m |          -0.0055 |          75.9495 |          18.3927 |
[32m[20221213 21:02:57 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:02:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.80
[32m[20221213 21:02:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 572.00
[32m[20221213 21:02:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.00
[32m[20221213 21:02:57 @agent_ppo2.py:143][0m Total time:       7.37 min
[32m[20221213 21:02:57 @agent_ppo2.py:145][0m 724992 total steps have happened
[32m[20221213 21:02:57 @agent_ppo2.py:121][0m #------------------------ Iteration 354 --------------------------#
[32m[20221213 21:02:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:57 @agent_ppo2.py:185][0m |          -0.0039 |          81.0341 |          18.1900 |
[32m[20221213 21:02:57 @agent_ppo2.py:185][0m |           0.0004 |          82.4613 |          18.1678 |
[32m[20221213 21:02:57 @agent_ppo2.py:185][0m |          -0.0031 |          80.1691 |          18.1653 |
[32m[20221213 21:02:57 @agent_ppo2.py:185][0m |          -0.0061 |          80.0113 |          18.1702 |
[32m[20221213 21:02:58 @agent_ppo2.py:185][0m |          -0.0025 |          79.8943 |          18.1649 |
[32m[20221213 21:02:58 @agent_ppo2.py:185][0m |           0.0056 |          88.9418 |          18.1644 |
[32m[20221213 21:02:58 @agent_ppo2.py:185][0m |          -0.0082 |          79.6459 |          18.1683 |
[32m[20221213 21:02:58 @agent_ppo2.py:185][0m |          -0.0076 |          79.4572 |          18.1581 |
[32m[20221213 21:02:58 @agent_ppo2.py:185][0m |          -0.0113 |          79.4279 |          18.1593 |
[32m[20221213 21:02:58 @agent_ppo2.py:185][0m |          -0.0081 |          79.0753 |          18.1615 |
[32m[20221213 21:02:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:02:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.00
[32m[20221213 21:02:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 543.00
[32m[20221213 21:02:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 524.00
[32m[20221213 21:02:58 @agent_ppo2.py:143][0m Total time:       7.39 min
[32m[20221213 21:02:58 @agent_ppo2.py:145][0m 727040 total steps have happened
[32m[20221213 21:02:58 @agent_ppo2.py:121][0m #------------------------ Iteration 355 --------------------------#
[32m[20221213 21:02:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:02:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:02:58 @agent_ppo2.py:185][0m |          -0.0009 |          79.4286 |          18.3618 |
[32m[20221213 21:02:58 @agent_ppo2.py:185][0m |          -0.0058 |          78.9035 |          18.3461 |
[32m[20221213 21:02:59 @agent_ppo2.py:185][0m |          -0.0075 |          78.6372 |          18.3352 |
[32m[20221213 21:02:59 @agent_ppo2.py:185][0m |          -0.0056 |          78.2996 |          18.3343 |
[32m[20221213 21:02:59 @agent_ppo2.py:185][0m |          -0.0066 |          78.1544 |          18.3215 |
[32m[20221213 21:02:59 @agent_ppo2.py:185][0m |          -0.0069 |          78.0528 |          18.3290 |
[32m[20221213 21:02:59 @agent_ppo2.py:185][0m |          -0.0096 |          77.9597 |          18.3282 |
[32m[20221213 21:02:59 @agent_ppo2.py:185][0m |          -0.0101 |          77.7143 |          18.3228 |
[32m[20221213 21:02:59 @agent_ppo2.py:185][0m |          -0.0103 |          77.6221 |          18.3192 |
[32m[20221213 21:02:59 @agent_ppo2.py:185][0m |          -0.0079 |          77.5209 |          18.3202 |
[32m[20221213 21:02:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:02:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.00
[32m[20221213 21:02:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.00
[32m[20221213 21:02:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 589.00
[32m[20221213 21:02:59 @agent_ppo2.py:143][0m Total time:       7.41 min
[32m[20221213 21:02:59 @agent_ppo2.py:145][0m 729088 total steps have happened
[32m[20221213 21:02:59 @agent_ppo2.py:121][0m #------------------------ Iteration 356 --------------------------#
[32m[20221213 21:02:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:00 @agent_ppo2.py:185][0m |          -0.0035 |          84.8114 |          18.4494 |
[32m[20221213 21:03:00 @agent_ppo2.py:185][0m |          -0.0061 |          83.0128 |          18.4302 |
[32m[20221213 21:03:00 @agent_ppo2.py:185][0m |          -0.0084 |          82.2787 |          18.4149 |
[32m[20221213 21:03:00 @agent_ppo2.py:185][0m |          -0.0084 |          81.8168 |          18.4280 |
[32m[20221213 21:03:00 @agent_ppo2.py:185][0m |          -0.0090 |          81.4387 |          18.4155 |
[32m[20221213 21:03:00 @agent_ppo2.py:185][0m |          -0.0097 |          81.3060 |          18.4177 |
[32m[20221213 21:03:00 @agent_ppo2.py:185][0m |          -0.0076 |          81.2233 |          18.4167 |
[32m[20221213 21:03:00 @agent_ppo2.py:185][0m |          -0.0056 |          81.0412 |          18.4186 |
[32m[20221213 21:03:00 @agent_ppo2.py:185][0m |          -0.0078 |          80.7412 |          18.4185 |
[32m[20221213 21:03:00 @agent_ppo2.py:185][0m |          -0.0075 |          80.4910 |          18.4148 |
[32m[20221213 21:03:00 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:03:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.00
[32m[20221213 21:03:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.00
[32m[20221213 21:03:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 570.00
[32m[20221213 21:03:01 @agent_ppo2.py:143][0m Total time:       7.43 min
[32m[20221213 21:03:01 @agent_ppo2.py:145][0m 731136 total steps have happened
[32m[20221213 21:03:01 @agent_ppo2.py:121][0m #------------------------ Iteration 357 --------------------------#
[32m[20221213 21:03:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:01 @agent_ppo2.py:185][0m |          -0.0001 |          83.3828 |          18.4105 |
[32m[20221213 21:03:01 @agent_ppo2.py:185][0m |          -0.0021 |          82.3107 |          18.4012 |
[32m[20221213 21:03:01 @agent_ppo2.py:185][0m |          -0.0023 |          82.7627 |          18.4000 |
[32m[20221213 21:03:01 @agent_ppo2.py:185][0m |          -0.0041 |          81.8815 |          18.3970 |
[32m[20221213 21:03:01 @agent_ppo2.py:185][0m |          -0.0023 |          81.6918 |          18.3982 |
[32m[20221213 21:03:01 @agent_ppo2.py:185][0m |          -0.0070 |          81.5292 |          18.3933 |
[32m[20221213 21:03:01 @agent_ppo2.py:185][0m |          -0.0037 |          81.2724 |          18.3978 |
[32m[20221213 21:03:01 @agent_ppo2.py:185][0m |          -0.0010 |          82.7778 |          18.3883 |
[32m[20221213 21:03:02 @agent_ppo2.py:185][0m |          -0.0073 |          81.2074 |          18.3890 |
[32m[20221213 21:03:02 @agent_ppo2.py:185][0m |           0.0003 |          84.7997 |          18.3877 |
[32m[20221213 21:03:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:03:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.60
[32m[20221213 21:03:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.00
[32m[20221213 21:03:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.00
[32m[20221213 21:03:02 @agent_ppo2.py:143][0m Total time:       7.45 min
[32m[20221213 21:03:02 @agent_ppo2.py:145][0m 733184 total steps have happened
[32m[20221213 21:03:02 @agent_ppo2.py:121][0m #------------------------ Iteration 358 --------------------------#
[32m[20221213 21:03:02 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:03:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:02 @agent_ppo2.py:185][0m |           0.0027 |          83.9455 |          18.3606 |
[32m[20221213 21:03:02 @agent_ppo2.py:185][0m |          -0.0021 |          80.8226 |          18.3498 |
[32m[20221213 21:03:02 @agent_ppo2.py:185][0m |          -0.0048 |          80.0965 |          18.3371 |
[32m[20221213 21:03:02 @agent_ppo2.py:185][0m |          -0.0029 |          79.9319 |          18.3382 |
[32m[20221213 21:03:02 @agent_ppo2.py:185][0m |          -0.0055 |          79.5943 |          18.3302 |
[32m[20221213 21:03:02 @agent_ppo2.py:185][0m |          -0.0072 |          79.4768 |          18.3391 |
[32m[20221213 21:03:03 @agent_ppo2.py:185][0m |           0.0039 |          83.6391 |          18.3368 |
[32m[20221213 21:03:03 @agent_ppo2.py:185][0m |          -0.0037 |          79.1217 |          18.3330 |
[32m[20221213 21:03:03 @agent_ppo2.py:185][0m |          -0.0084 |          79.0518 |          18.3274 |
[32m[20221213 21:03:03 @agent_ppo2.py:185][0m |          -0.0049 |          78.9755 |          18.3344 |
[32m[20221213 21:03:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:03:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.40
[32m[20221213 21:03:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.00
[32m[20221213 21:03:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.00
[32m[20221213 21:03:03 @agent_ppo2.py:143][0m Total time:       7.47 min
[32m[20221213 21:03:03 @agent_ppo2.py:145][0m 735232 total steps have happened
[32m[20221213 21:03:03 @agent_ppo2.py:121][0m #------------------------ Iteration 359 --------------------------#
[32m[20221213 21:03:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:03 @agent_ppo2.py:185][0m |          -0.0013 |          78.9694 |          18.5249 |
[32m[20221213 21:03:03 @agent_ppo2.py:185][0m |          -0.0031 |          78.6661 |          18.5120 |
[32m[20221213 21:03:03 @agent_ppo2.py:185][0m |           0.0040 |          83.8003 |          18.5043 |
[32m[20221213 21:03:03 @agent_ppo2.py:185][0m |          -0.0072 |          77.6484 |          18.4940 |
[32m[20221213 21:03:04 @agent_ppo2.py:185][0m |          -0.0071 |          77.3752 |          18.4958 |
[32m[20221213 21:03:04 @agent_ppo2.py:185][0m |           0.0047 |          84.2935 |          18.4981 |
[32m[20221213 21:03:04 @agent_ppo2.py:185][0m |          -0.0082 |          77.2990 |          18.4925 |
[32m[20221213 21:03:04 @agent_ppo2.py:185][0m |          -0.0092 |          76.9808 |          18.4934 |
[32m[20221213 21:03:04 @agent_ppo2.py:185][0m |          -0.0052 |          76.8510 |          18.4952 |
[32m[20221213 21:03:04 @agent_ppo2.py:185][0m |          -0.0057 |          77.2152 |          18.5002 |
[32m[20221213 21:03:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:03:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 528.60
[32m[20221213 21:03:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.00
[32m[20221213 21:03:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 555.00
[32m[20221213 21:03:04 @agent_ppo2.py:143][0m Total time:       7.49 min
[32m[20221213 21:03:04 @agent_ppo2.py:145][0m 737280 total steps have happened
[32m[20221213 21:03:04 @agent_ppo2.py:121][0m #------------------------ Iteration 360 --------------------------#
[32m[20221213 21:03:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:04 @agent_ppo2.py:185][0m |           0.0027 |          81.2192 |          18.3295 |
[32m[20221213 21:03:05 @agent_ppo2.py:185][0m |           0.0030 |          81.5568 |          18.3213 |
[32m[20221213 21:03:05 @agent_ppo2.py:185][0m |          -0.0063 |          79.7566 |          18.3064 |
[32m[20221213 21:03:05 @agent_ppo2.py:185][0m |          -0.0001 |          80.3098 |          18.3113 |
[32m[20221213 21:03:05 @agent_ppo2.py:185][0m |           0.0026 |          81.5430 |          18.3071 |
[32m[20221213 21:03:05 @agent_ppo2.py:185][0m |          -0.0079 |          79.0952 |          18.2920 |
[32m[20221213 21:03:05 @agent_ppo2.py:185][0m |          -0.0072 |          78.9035 |          18.3073 |
[32m[20221213 21:03:05 @agent_ppo2.py:185][0m |          -0.0070 |          78.9119 |          18.3120 |
[32m[20221213 21:03:05 @agent_ppo2.py:185][0m |          -0.0080 |          78.5837 |          18.3107 |
[32m[20221213 21:03:05 @agent_ppo2.py:185][0m |          -0.0093 |          78.6494 |          18.3138 |
[32m[20221213 21:03:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:03:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.40
[32m[20221213 21:03:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.00
[32m[20221213 21:03:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 610.00
[32m[20221213 21:03:05 @agent_ppo2.py:143][0m Total time:       7.51 min
[32m[20221213 21:03:05 @agent_ppo2.py:145][0m 739328 total steps have happened
[32m[20221213 21:03:05 @agent_ppo2.py:121][0m #------------------------ Iteration 361 --------------------------#
[32m[20221213 21:03:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:03:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:06 @agent_ppo2.py:185][0m |           0.0034 |          85.4564 |          18.3583 |
[32m[20221213 21:03:06 @agent_ppo2.py:185][0m |          -0.0073 |          81.8165 |          18.3409 |
[32m[20221213 21:03:06 @agent_ppo2.py:185][0m |           0.0021 |          82.8126 |          18.3367 |
[32m[20221213 21:03:06 @agent_ppo2.py:185][0m |          -0.0047 |          80.9435 |          18.3328 |
[32m[20221213 21:03:06 @agent_ppo2.py:185][0m |           0.0022 |          85.4720 |          18.3314 |
[32m[20221213 21:03:06 @agent_ppo2.py:185][0m |          -0.0078 |          80.6832 |          18.3249 |
[32m[20221213 21:03:06 @agent_ppo2.py:185][0m |          -0.0081 |          80.3509 |          18.3303 |
[32m[20221213 21:03:06 @agent_ppo2.py:185][0m |          -0.0061 |          80.1926 |          18.3285 |
[32m[20221213 21:03:06 @agent_ppo2.py:185][0m |          -0.0084 |          79.8248 |          18.3246 |
[32m[20221213 21:03:06 @agent_ppo2.py:185][0m |          -0.0105 |          79.8440 |          18.3235 |
[32m[20221213 21:03:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:03:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.40
[32m[20221213 21:03:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.00
[32m[20221213 21:03:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 547.00
[32m[20221213 21:03:07 @agent_ppo2.py:143][0m Total time:       7.53 min
[32m[20221213 21:03:07 @agent_ppo2.py:145][0m 741376 total steps have happened
[32m[20221213 21:03:07 @agent_ppo2.py:121][0m #------------------------ Iteration 362 --------------------------#
[32m[20221213 21:03:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:07 @agent_ppo2.py:185][0m |           0.0005 |          81.0887 |          18.3086 |
[32m[20221213 21:03:07 @agent_ppo2.py:185][0m |          -0.0008 |          80.4109 |          18.2955 |
[32m[20221213 21:03:07 @agent_ppo2.py:185][0m |          -0.0036 |          79.9948 |          18.2853 |
[32m[20221213 21:03:07 @agent_ppo2.py:185][0m |          -0.0027 |          79.7503 |          18.2900 |
[32m[20221213 21:03:07 @agent_ppo2.py:185][0m |          -0.0038 |          79.6822 |          18.2904 |
[32m[20221213 21:03:07 @agent_ppo2.py:185][0m |          -0.0055 |          79.4259 |          18.2827 |
[32m[20221213 21:03:07 @agent_ppo2.py:185][0m |          -0.0068 |          79.3221 |          18.2885 |
[32m[20221213 21:03:07 @agent_ppo2.py:185][0m |          -0.0027 |          79.2453 |          18.2854 |
[32m[20221213 21:03:08 @agent_ppo2.py:185][0m |          -0.0064 |          79.0117 |          18.2869 |
[32m[20221213 21:03:08 @agent_ppo2.py:185][0m |          -0.0080 |          79.0286 |          18.2788 |
[32m[20221213 21:03:08 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:03:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 524.60
[32m[20221213 21:03:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.00
[32m[20221213 21:03:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.00
[32m[20221213 21:03:08 @agent_ppo2.py:143][0m Total time:       7.55 min
[32m[20221213 21:03:08 @agent_ppo2.py:145][0m 743424 total steps have happened
[32m[20221213 21:03:08 @agent_ppo2.py:121][0m #------------------------ Iteration 363 --------------------------#
[32m[20221213 21:03:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:08 @agent_ppo2.py:185][0m |          -0.0008 |          81.0709 |          18.3075 |
[32m[20221213 21:03:08 @agent_ppo2.py:185][0m |          -0.0028 |          80.3193 |          18.2805 |
[32m[20221213 21:03:08 @agent_ppo2.py:185][0m |           0.0006 |          82.5477 |          18.2796 |
[32m[20221213 21:03:08 @agent_ppo2.py:185][0m |          -0.0032 |          79.6009 |          18.2711 |
[32m[20221213 21:03:08 @agent_ppo2.py:185][0m |          -0.0073 |          78.2583 |          18.2671 |
[32m[20221213 21:03:09 @agent_ppo2.py:185][0m |          -0.0088 |          77.8625 |          18.2599 |
[32m[20221213 21:03:09 @agent_ppo2.py:185][0m |           0.0117 |          91.9750 |          18.2714 |
[32m[20221213 21:03:09 @agent_ppo2.py:185][0m |          -0.0043 |          77.2356 |          18.2357 |
[32m[20221213 21:03:09 @agent_ppo2.py:185][0m |          -0.0072 |          76.9734 |          18.2507 |
[32m[20221213 21:03:09 @agent_ppo2.py:185][0m |           0.0016 |          81.9575 |          18.2556 |
[32m[20221213 21:03:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:03:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.80
[32m[20221213 21:03:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.00
[32m[20221213 21:03:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 582.00
[32m[20221213 21:03:09 @agent_ppo2.py:143][0m Total time:       7.57 min
[32m[20221213 21:03:09 @agent_ppo2.py:145][0m 745472 total steps have happened
[32m[20221213 21:03:09 @agent_ppo2.py:121][0m #------------------------ Iteration 364 --------------------------#
[32m[20221213 21:03:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:03:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:09 @agent_ppo2.py:185][0m |          -0.0001 |          80.0650 |          18.5191 |
[32m[20221213 21:03:09 @agent_ppo2.py:185][0m |          -0.0050 |          78.6517 |          18.5067 |
[32m[20221213 21:03:09 @agent_ppo2.py:185][0m |          -0.0037 |          77.9434 |          18.4997 |
[32m[20221213 21:03:10 @agent_ppo2.py:185][0m |          -0.0060 |          77.5746 |          18.5056 |
[32m[20221213 21:03:10 @agent_ppo2.py:185][0m |          -0.0059 |          77.3754 |          18.4958 |
[32m[20221213 21:03:10 @agent_ppo2.py:185][0m |          -0.0030 |          77.8844 |          18.5047 |
[32m[20221213 21:03:10 @agent_ppo2.py:185][0m |          -0.0075 |          76.9909 |          18.4973 |
[32m[20221213 21:03:10 @agent_ppo2.py:185][0m |          -0.0069 |          77.0989 |          18.4981 |
[32m[20221213 21:03:10 @agent_ppo2.py:185][0m |          -0.0072 |          76.8025 |          18.4927 |
[32m[20221213 21:03:10 @agent_ppo2.py:185][0m |          -0.0063 |          76.7510 |          18.4943 |
[32m[20221213 21:03:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:03:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.80
[32m[20221213 21:03:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.00
[32m[20221213 21:03:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.00
[32m[20221213 21:03:10 @agent_ppo2.py:143][0m Total time:       7.59 min
[32m[20221213 21:03:10 @agent_ppo2.py:145][0m 747520 total steps have happened
[32m[20221213 21:03:10 @agent_ppo2.py:121][0m #------------------------ Iteration 365 --------------------------#
[32m[20221213 21:03:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:11 @agent_ppo2.py:185][0m |          -0.0002 |          77.4121 |          18.4089 |
[32m[20221213 21:03:11 @agent_ppo2.py:185][0m |          -0.0048 |          76.0476 |          18.3934 |
[32m[20221213 21:03:11 @agent_ppo2.py:185][0m |          -0.0063 |          75.3158 |          18.3903 |
[32m[20221213 21:03:11 @agent_ppo2.py:185][0m |          -0.0007 |          75.2257 |          18.3878 |
[32m[20221213 21:03:11 @agent_ppo2.py:185][0m |          -0.0047 |          74.3823 |          18.3961 |
[32m[20221213 21:03:11 @agent_ppo2.py:185][0m |          -0.0091 |          74.0738 |          18.3891 |
[32m[20221213 21:03:11 @agent_ppo2.py:185][0m |          -0.0064 |          73.6435 |          18.3833 |
[32m[20221213 21:03:11 @agent_ppo2.py:185][0m |          -0.0075 |          73.3837 |          18.3882 |
[32m[20221213 21:03:11 @agent_ppo2.py:185][0m |          -0.0024 |          73.9236 |          18.3853 |
[32m[20221213 21:03:11 @agent_ppo2.py:185][0m |          -0.0082 |          72.9776 |          18.3854 |
[32m[20221213 21:03:11 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:03:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 514.80
[32m[20221213 21:03:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 560.00
[32m[20221213 21:03:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.00
[32m[20221213 21:03:11 @agent_ppo2.py:143][0m Total time:       7.61 min
[32m[20221213 21:03:11 @agent_ppo2.py:145][0m 749568 total steps have happened
[32m[20221213 21:03:11 @agent_ppo2.py:121][0m #------------------------ Iteration 366 --------------------------#
[32m[20221213 21:03:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:12 @agent_ppo2.py:185][0m |          -0.0001 |          79.4448 |          18.3632 |
[32m[20221213 21:03:12 @agent_ppo2.py:185][0m |           0.0010 |          79.9706 |          18.3490 |
[32m[20221213 21:03:12 @agent_ppo2.py:185][0m |          -0.0035 |          78.5424 |          18.3410 |
[32m[20221213 21:03:12 @agent_ppo2.py:185][0m |          -0.0041 |          78.4517 |          18.3372 |
[32m[20221213 21:03:12 @agent_ppo2.py:185][0m |          -0.0087 |          77.8292 |          18.3197 |
[32m[20221213 21:03:12 @agent_ppo2.py:185][0m |          -0.0062 |          77.5602 |          18.3279 |
[32m[20221213 21:03:12 @agent_ppo2.py:185][0m |          -0.0003 |          82.3537 |          18.3252 |
[32m[20221213 21:03:12 @agent_ppo2.py:185][0m |          -0.0054 |          77.3475 |          18.3281 |
[32m[20221213 21:03:12 @agent_ppo2.py:185][0m |          -0.0077 |          77.2835 |          18.3250 |
[32m[20221213 21:03:13 @agent_ppo2.py:185][0m |          -0.0010 |          79.2537 |          18.3211 |
[32m[20221213 21:03:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:03:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 534.20
[32m[20221213 21:03:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 566.00
[32m[20221213 21:03:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.00
[32m[20221213 21:03:13 @agent_ppo2.py:143][0m Total time:       7.63 min
[32m[20221213 21:03:13 @agent_ppo2.py:145][0m 751616 total steps have happened
[32m[20221213 21:03:13 @agent_ppo2.py:121][0m #------------------------ Iteration 367 --------------------------#
[32m[20221213 21:03:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:13 @agent_ppo2.py:185][0m |          -0.0035 |          79.7547 |          18.3472 |
[32m[20221213 21:03:13 @agent_ppo2.py:185][0m |           0.0021 |          82.8507 |          18.3252 |
[32m[20221213 21:03:13 @agent_ppo2.py:185][0m |          -0.0039 |          78.6563 |          18.3228 |
[32m[20221213 21:03:13 @agent_ppo2.py:185][0m |          -0.0082 |          78.3697 |          18.3243 |
[32m[20221213 21:03:13 @agent_ppo2.py:185][0m |          -0.0062 |          77.9986 |          18.3126 |
[32m[20221213 21:03:13 @agent_ppo2.py:185][0m |          -0.0066 |          77.9438 |          18.3182 |
[32m[20221213 21:03:13 @agent_ppo2.py:185][0m |          -0.0071 |          77.6546 |          18.3083 |
[32m[20221213 21:03:14 @agent_ppo2.py:185][0m |          -0.0063 |          77.4631 |          18.3101 |
[32m[20221213 21:03:14 @agent_ppo2.py:185][0m |          -0.0056 |          77.5030 |          18.3089 |
[32m[20221213 21:03:14 @agent_ppo2.py:185][0m |          -0.0067 |          77.3159 |          18.3086 |
[32m[20221213 21:03:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:03:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.00
[32m[20221213 21:03:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 21:03:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.00
[32m[20221213 21:03:14 @agent_ppo2.py:143][0m Total time:       7.65 min
[32m[20221213 21:03:14 @agent_ppo2.py:145][0m 753664 total steps have happened
[32m[20221213 21:03:14 @agent_ppo2.py:121][0m #------------------------ Iteration 368 --------------------------#
[32m[20221213 21:03:14 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:03:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:14 @agent_ppo2.py:185][0m |          -0.0010 |          77.5535 |          18.4351 |
[32m[20221213 21:03:14 @agent_ppo2.py:185][0m |          -0.0021 |          76.7195 |          18.4292 |
[32m[20221213 21:03:14 @agent_ppo2.py:185][0m |          -0.0033 |          76.3722 |          18.4119 |
[32m[20221213 21:03:14 @agent_ppo2.py:185][0m |          -0.0047 |          76.3442 |          18.4241 |
[32m[20221213 21:03:14 @agent_ppo2.py:185][0m |          -0.0052 |          76.1815 |          18.4043 |
[32m[20221213 21:03:15 @agent_ppo2.py:185][0m |          -0.0070 |          76.0304 |          18.4081 |
[32m[20221213 21:03:15 @agent_ppo2.py:185][0m |           0.0059 |          82.9936 |          18.4026 |
[32m[20221213 21:03:15 @agent_ppo2.py:185][0m |          -0.0078 |          75.9684 |          18.4052 |
[32m[20221213 21:03:15 @agent_ppo2.py:185][0m |          -0.0056 |          75.6549 |          18.4079 |
[32m[20221213 21:03:15 @agent_ppo2.py:185][0m |           0.0031 |          81.3154 |          18.4109 |
[32m[20221213 21:03:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:03:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.60
[32m[20221213 21:03:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.00
[32m[20221213 21:03:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 576.00
[32m[20221213 21:03:15 @agent_ppo2.py:143][0m Total time:       7.67 min
[32m[20221213 21:03:15 @agent_ppo2.py:145][0m 755712 total steps have happened
[32m[20221213 21:03:15 @agent_ppo2.py:121][0m #------------------------ Iteration 369 --------------------------#
[32m[20221213 21:03:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:15 @agent_ppo2.py:185][0m |          -0.0036 |          78.8968 |          18.4833 |
[32m[20221213 21:03:15 @agent_ppo2.py:185][0m |          -0.0013 |          78.1771 |          18.4925 |
[32m[20221213 21:03:16 @agent_ppo2.py:185][0m |          -0.0056 |          78.1674 |          18.4888 |
[32m[20221213 21:03:16 @agent_ppo2.py:185][0m |          -0.0033 |          77.9943 |          18.4841 |
[32m[20221213 21:03:16 @agent_ppo2.py:185][0m |          -0.0011 |          78.2631 |          18.4778 |
[32m[20221213 21:03:16 @agent_ppo2.py:185][0m |          -0.0059 |          77.8186 |          18.4807 |
[32m[20221213 21:03:16 @agent_ppo2.py:185][0m |          -0.0045 |          77.6790 |          18.4755 |
[32m[20221213 21:03:16 @agent_ppo2.py:185][0m |          -0.0062 |          77.5381 |          18.4715 |
[32m[20221213 21:03:16 @agent_ppo2.py:185][0m |          -0.0053 |          77.5645 |          18.4725 |
[32m[20221213 21:03:16 @agent_ppo2.py:185][0m |          -0.0098 |          77.5667 |          18.4786 |
[32m[20221213 21:03:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:03:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 521.80
[32m[20221213 21:03:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.00
[32m[20221213 21:03:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.00
[32m[20221213 21:03:16 @agent_ppo2.py:143][0m Total time:       7.69 min
[32m[20221213 21:03:16 @agent_ppo2.py:145][0m 757760 total steps have happened
[32m[20221213 21:03:16 @agent_ppo2.py:121][0m #------------------------ Iteration 370 --------------------------#
[32m[20221213 21:03:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:17 @agent_ppo2.py:185][0m |           0.0009 |          81.8144 |          18.5225 |
[32m[20221213 21:03:17 @agent_ppo2.py:185][0m |          -0.0029 |          80.8952 |          18.5048 |
[32m[20221213 21:03:17 @agent_ppo2.py:185][0m |          -0.0071 |          80.4822 |          18.5077 |
[32m[20221213 21:03:17 @agent_ppo2.py:185][0m |          -0.0056 |          79.9788 |          18.4969 |
[32m[20221213 21:03:17 @agent_ppo2.py:185][0m |          -0.0031 |          80.1307 |          18.4853 |
[32m[20221213 21:03:17 @agent_ppo2.py:185][0m |          -0.0070 |          79.5661 |          18.4974 |
[32m[20221213 21:03:17 @agent_ppo2.py:185][0m |          -0.0084 |          79.2407 |          18.4878 |
[32m[20221213 21:03:17 @agent_ppo2.py:185][0m |          -0.0069 |          79.0887 |          18.4831 |
[32m[20221213 21:03:17 @agent_ppo2.py:185][0m |          -0.0046 |          79.0980 |          18.4803 |
[32m[20221213 21:03:17 @agent_ppo2.py:185][0m |          -0.0067 |          78.6978 |          18.4828 |
[32m[20221213 21:03:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:03:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 541.80
[32m[20221213 21:03:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 574.00
[32m[20221213 21:03:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.00
[32m[20221213 21:03:17 @agent_ppo2.py:143][0m Total time:       7.71 min
[32m[20221213 21:03:17 @agent_ppo2.py:145][0m 759808 total steps have happened
[32m[20221213 21:03:17 @agent_ppo2.py:121][0m #------------------------ Iteration 371 --------------------------#
[32m[20221213 21:03:18 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:03:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:18 @agent_ppo2.py:185][0m |          -0.0026 |          82.7011 |          18.3873 |
[32m[20221213 21:03:18 @agent_ppo2.py:185][0m |          -0.0039 |          82.1246 |          18.3619 |
[32m[20221213 21:03:18 @agent_ppo2.py:185][0m |           0.0075 |          89.1676 |          18.3522 |
[32m[20221213 21:03:18 @agent_ppo2.py:185][0m |          -0.0065 |          81.6235 |          18.3504 |
[32m[20221213 21:03:18 @agent_ppo2.py:185][0m |           0.0061 |          88.0595 |          18.3458 |
[32m[20221213 21:03:18 @agent_ppo2.py:185][0m |          -0.0035 |          82.8632 |          18.3407 |
[32m[20221213 21:03:18 @agent_ppo2.py:185][0m |          -0.0096 |          81.1813 |          18.3502 |
[32m[20221213 21:03:18 @agent_ppo2.py:185][0m |          -0.0064 |          81.0456 |          18.3463 |
[32m[20221213 21:03:18 @agent_ppo2.py:185][0m |          -0.0063 |          80.9028 |          18.3470 |
[32m[20221213 21:03:18 @agent_ppo2.py:185][0m |          -0.0064 |          80.8869 |          18.3428 |
[32m[20221213 21:03:18 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:03:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.80
[32m[20221213 21:03:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.00
[32m[20221213 21:03:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 564.00
[32m[20221213 21:03:19 @agent_ppo2.py:143][0m Total time:       7.73 min
[32m[20221213 21:03:19 @agent_ppo2.py:145][0m 761856 total steps have happened
[32m[20221213 21:03:19 @agent_ppo2.py:121][0m #------------------------ Iteration 372 --------------------------#
[32m[20221213 21:03:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:19 @agent_ppo2.py:185][0m |          -0.0022 |          80.2868 |          18.5688 |
[32m[20221213 21:03:19 @agent_ppo2.py:185][0m |           0.0044 |          83.1419 |          18.5699 |
[32m[20221213 21:03:19 @agent_ppo2.py:185][0m |           0.0074 |          86.2493 |          18.5610 |
[32m[20221213 21:03:19 @agent_ppo2.py:185][0m |          -0.0008 |          78.9982 |          18.5591 |
[32m[20221213 21:03:19 @agent_ppo2.py:185][0m |           0.0029 |          83.4566 |          18.5577 |
[32m[20221213 21:03:19 @agent_ppo2.py:185][0m |          -0.0057 |          78.6469 |          18.5605 |
[32m[20221213 21:03:19 @agent_ppo2.py:185][0m |          -0.0060 |          78.4968 |          18.5503 |
[32m[20221213 21:03:20 @agent_ppo2.py:185][0m |          -0.0050 |          78.4043 |          18.5586 |
[32m[20221213 21:03:20 @agent_ppo2.py:185][0m |          -0.0045 |          78.2085 |          18.5576 |
[32m[20221213 21:03:20 @agent_ppo2.py:185][0m |          -0.0038 |          78.0962 |          18.5555 |
[32m[20221213 21:03:20 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:03:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 537.00
[32m[20221213 21:03:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 583.00
[32m[20221213 21:03:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.00
[32m[20221213 21:03:20 @agent_ppo2.py:143][0m Total time:       7.75 min
[32m[20221213 21:03:20 @agent_ppo2.py:145][0m 763904 total steps have happened
[32m[20221213 21:03:20 @agent_ppo2.py:121][0m #------------------------ Iteration 373 --------------------------#
[32m[20221213 21:03:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:20 @agent_ppo2.py:185][0m |           0.0089 |          93.7010 |          18.4952 |
[32m[20221213 21:03:20 @agent_ppo2.py:185][0m |           0.0019 |          83.1970 |          18.4499 |
[32m[20221213 21:03:20 @agent_ppo2.py:185][0m |          -0.0059 |          81.3666 |          18.4727 |
[32m[20221213 21:03:20 @agent_ppo2.py:185][0m |          -0.0044 |          81.0622 |          18.4646 |
[32m[20221213 21:03:20 @agent_ppo2.py:185][0m |          -0.0074 |          80.8647 |          18.4614 |
[32m[20221213 21:03:21 @agent_ppo2.py:185][0m |          -0.0044 |          84.7378 |          18.4690 |
[32m[20221213 21:03:21 @agent_ppo2.py:185][0m |           0.0048 |          90.3628 |          18.4591 |
[32m[20221213 21:03:21 @agent_ppo2.py:185][0m |          -0.0069 |          80.5565 |          18.4200 |
[32m[20221213 21:03:21 @agent_ppo2.py:185][0m |          -0.0074 |          80.3726 |          18.4607 |
[32m[20221213 21:03:21 @agent_ppo2.py:185][0m |           0.0020 |          84.2952 |          18.4627 |
[32m[20221213 21:03:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:03:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 545.20
[32m[20221213 21:03:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 21:03:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 536.00
[32m[20221213 21:03:21 @agent_ppo2.py:143][0m Total time:       7.77 min
[32m[20221213 21:03:21 @agent_ppo2.py:145][0m 765952 total steps have happened
[32m[20221213 21:03:21 @agent_ppo2.py:121][0m #------------------------ Iteration 374 --------------------------#
[32m[20221213 21:03:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:03:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:21 @agent_ppo2.py:185][0m |          -0.0014 |          82.3518 |          18.1731 |
[32m[20221213 21:03:21 @agent_ppo2.py:185][0m |          -0.0065 |          81.6746 |          18.1538 |
[32m[20221213 21:03:21 @agent_ppo2.py:185][0m |          -0.0025 |          81.3045 |          18.1609 |
[32m[20221213 21:03:22 @agent_ppo2.py:185][0m |          -0.0055 |          81.0311 |          18.1541 |
[32m[20221213 21:03:22 @agent_ppo2.py:185][0m |          -0.0040 |          80.7988 |          18.1485 |
[32m[20221213 21:03:22 @agent_ppo2.py:185][0m |          -0.0068 |          80.7345 |          18.1387 |
[32m[20221213 21:03:22 @agent_ppo2.py:185][0m |          -0.0078 |          80.6096 |          18.1535 |
[32m[20221213 21:03:22 @agent_ppo2.py:185][0m |           0.0153 |          94.7143 |          18.1495 |
[32m[20221213 21:03:22 @agent_ppo2.py:185][0m |          -0.0050 |          80.5520 |          18.1215 |
[32m[20221213 21:03:22 @agent_ppo2.py:185][0m |          -0.0049 |          80.9517 |          18.1431 |
[32m[20221213 21:03:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:03:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.40
[32m[20221213 21:03:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.00
[32m[20221213 21:03:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 570.00
[32m[20221213 21:03:22 @agent_ppo2.py:143][0m Total time:       7.79 min
[32m[20221213 21:03:22 @agent_ppo2.py:145][0m 768000 total steps have happened
[32m[20221213 21:03:22 @agent_ppo2.py:121][0m #------------------------ Iteration 375 --------------------------#
[32m[20221213 21:03:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:22 @agent_ppo2.py:185][0m |          -0.0012 |          80.3350 |          18.5009 |
[32m[20221213 21:03:23 @agent_ppo2.py:185][0m |          -0.0049 |          79.7845 |          18.4754 |
[32m[20221213 21:03:23 @agent_ppo2.py:185][0m |          -0.0054 |          79.2815 |          18.4669 |
[32m[20221213 21:03:23 @agent_ppo2.py:185][0m |          -0.0043 |          79.3643 |          18.4600 |
[32m[20221213 21:03:23 @agent_ppo2.py:185][0m |          -0.0053 |          78.6391 |          18.4612 |
[32m[20221213 21:03:23 @agent_ppo2.py:185][0m |          -0.0051 |          78.6575 |          18.4557 |
[32m[20221213 21:03:23 @agent_ppo2.py:185][0m |          -0.0073 |          78.2643 |          18.4507 |
[32m[20221213 21:03:23 @agent_ppo2.py:185][0m |          -0.0077 |          78.1007 |          18.4457 |
[32m[20221213 21:03:23 @agent_ppo2.py:185][0m |          -0.0083 |          77.9873 |          18.4529 |
[32m[20221213 21:03:23 @agent_ppo2.py:185][0m |          -0.0068 |          77.7460 |          18.4550 |
[32m[20221213 21:03:23 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:03:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.40
[32m[20221213 21:03:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 579.00
[32m[20221213 21:03:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.00
[32m[20221213 21:03:23 @agent_ppo2.py:143][0m Total time:       7.81 min
[32m[20221213 21:03:23 @agent_ppo2.py:145][0m 770048 total steps have happened
[32m[20221213 21:03:23 @agent_ppo2.py:121][0m #------------------------ Iteration 376 --------------------------#
[32m[20221213 21:03:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:24 @agent_ppo2.py:185][0m |          -0.0021 |          85.6398 |          18.4825 |
[32m[20221213 21:03:24 @agent_ppo2.py:185][0m |          -0.0031 |          84.5501 |          18.4846 |
[32m[20221213 21:03:24 @agent_ppo2.py:185][0m |          -0.0061 |          84.2965 |          18.4840 |
[32m[20221213 21:03:24 @agent_ppo2.py:185][0m |          -0.0060 |          83.8543 |          18.4817 |
[32m[20221213 21:03:24 @agent_ppo2.py:185][0m |          -0.0068 |          83.7014 |          18.4856 |
[32m[20221213 21:03:24 @agent_ppo2.py:185][0m |          -0.0083 |          83.4900 |          18.4713 |
[32m[20221213 21:03:24 @agent_ppo2.py:185][0m |          -0.0089 |          83.2532 |          18.4808 |
[32m[20221213 21:03:24 @agent_ppo2.py:185][0m |          -0.0071 |          83.3974 |          18.4796 |
[32m[20221213 21:03:24 @agent_ppo2.py:185][0m |          -0.0088 |          83.0548 |          18.4716 |
[32m[20221213 21:03:24 @agent_ppo2.py:185][0m |          -0.0097 |          82.8509 |          18.4733 |
[32m[20221213 21:03:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:03:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 541.60
[32m[20221213 21:03:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 566.00
[32m[20221213 21:03:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 583.00
[32m[20221213 21:03:25 @agent_ppo2.py:143][0m Total time:       7.83 min
[32m[20221213 21:03:25 @agent_ppo2.py:145][0m 772096 total steps have happened
[32m[20221213 21:03:25 @agent_ppo2.py:121][0m #------------------------ Iteration 377 --------------------------#
[32m[20221213 21:03:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:03:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:25 @agent_ppo2.py:185][0m |           0.0066 |          83.4654 |          18.3171 |
[32m[20221213 21:03:25 @agent_ppo2.py:185][0m |          -0.0010 |          80.6413 |          18.2891 |
[32m[20221213 21:03:25 @agent_ppo2.py:185][0m |           0.0061 |          83.8406 |          18.2843 |
[32m[20221213 21:03:25 @agent_ppo2.py:185][0m |          -0.0021 |          79.5050 |          18.2747 |
[32m[20221213 21:03:25 @agent_ppo2.py:185][0m |          -0.0041 |          79.0707 |          18.2781 |
[32m[20221213 21:03:25 @agent_ppo2.py:185][0m |           0.0081 |          89.6646 |          18.2710 |
[32m[20221213 21:03:25 @agent_ppo2.py:185][0m |           0.0016 |          80.4131 |          18.2617 |
[32m[20221213 21:03:26 @agent_ppo2.py:185][0m |          -0.0043 |          78.5945 |          18.2668 |
[32m[20221213 21:03:26 @agent_ppo2.py:185][0m |          -0.0036 |          79.5035 |          18.2633 |
[32m[20221213 21:03:26 @agent_ppo2.py:185][0m |          -0.0030 |          79.2820 |          18.2591 |
[32m[20221213 21:03:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:03:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 517.40
[32m[20221213 21:03:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 537.00
[32m[20221213 21:03:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.00
[32m[20221213 21:03:26 @agent_ppo2.py:143][0m Total time:       7.85 min
[32m[20221213 21:03:26 @agent_ppo2.py:145][0m 774144 total steps have happened
[32m[20221213 21:03:26 @agent_ppo2.py:121][0m #------------------------ Iteration 378 --------------------------#
[32m[20221213 21:03:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:26 @agent_ppo2.py:185][0m |           0.0009 |          85.9928 |          18.3785 |
[32m[20221213 21:03:26 @agent_ppo2.py:185][0m |          -0.0023 |          83.6551 |          18.3648 |
[32m[20221213 21:03:26 @agent_ppo2.py:185][0m |          -0.0055 |          82.8411 |          18.3615 |
[32m[20221213 21:03:26 @agent_ppo2.py:185][0m |          -0.0065 |          81.8732 |          18.3571 |
[32m[20221213 21:03:26 @agent_ppo2.py:185][0m |          -0.0068 |          81.3809 |          18.3478 |
[32m[20221213 21:03:27 @agent_ppo2.py:185][0m |          -0.0063 |          80.9139 |          18.3558 |
[32m[20221213 21:03:27 @agent_ppo2.py:185][0m |          -0.0091 |          80.4619 |          18.3512 |
[32m[20221213 21:03:27 @agent_ppo2.py:185][0m |          -0.0076 |          79.8993 |          18.3532 |
[32m[20221213 21:03:27 @agent_ppo2.py:185][0m |          -0.0099 |          79.6595 |          18.3525 |
[32m[20221213 21:03:27 @agent_ppo2.py:185][0m |          -0.0017 |          79.6858 |          18.3413 |
[32m[20221213 21:03:27 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:03:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.20
[32m[20221213 21:03:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.00
[32m[20221213 21:03:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 576.00
[32m[20221213 21:03:27 @agent_ppo2.py:143][0m Total time:       7.87 min
[32m[20221213 21:03:27 @agent_ppo2.py:145][0m 776192 total steps have happened
[32m[20221213 21:03:27 @agent_ppo2.py:121][0m #------------------------ Iteration 379 --------------------------#
[32m[20221213 21:03:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:27 @agent_ppo2.py:185][0m |          -0.0033 |          85.2306 |          18.5027 |
[32m[20221213 21:03:27 @agent_ppo2.py:185][0m |          -0.0060 |          83.9023 |          18.4863 |
[32m[20221213 21:03:28 @agent_ppo2.py:185][0m |          -0.0051 |          83.2278 |          18.4864 |
[32m[20221213 21:03:28 @agent_ppo2.py:185][0m |          -0.0069 |          83.0218 |          18.4869 |
[32m[20221213 21:03:28 @agent_ppo2.py:185][0m |          -0.0013 |          85.8164 |          18.4846 |
[32m[20221213 21:03:28 @agent_ppo2.py:185][0m |           0.0084 |          93.8844 |          18.4912 |
[32m[20221213 21:03:28 @agent_ppo2.py:185][0m |          -0.0082 |          82.6425 |          18.4869 |
[32m[20221213 21:03:28 @agent_ppo2.py:185][0m |           0.0000 |          83.6446 |          18.4871 |
[32m[20221213 21:03:28 @agent_ppo2.py:185][0m |           0.0023 |          86.9186 |          18.4919 |
[32m[20221213 21:03:28 @agent_ppo2.py:185][0m |          -0.0070 |          82.2749 |          18.4876 |
[32m[20221213 21:03:28 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:03:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 523.80
[32m[20221213 21:03:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.00
[32m[20221213 21:03:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 565.00
[32m[20221213 21:03:28 @agent_ppo2.py:143][0m Total time:       7.89 min
[32m[20221213 21:03:28 @agent_ppo2.py:145][0m 778240 total steps have happened
[32m[20221213 21:03:28 @agent_ppo2.py:121][0m #------------------------ Iteration 380 --------------------------#
[32m[20221213 21:03:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:29 @agent_ppo2.py:185][0m |          -0.0007 |          82.1035 |          18.5921 |
[32m[20221213 21:03:29 @agent_ppo2.py:185][0m |          -0.0042 |          81.3502 |          18.5788 |
[32m[20221213 21:03:29 @agent_ppo2.py:185][0m |          -0.0033 |          80.7856 |          18.5754 |
[32m[20221213 21:03:29 @agent_ppo2.py:185][0m |          -0.0036 |          80.4613 |          18.5611 |
[32m[20221213 21:03:29 @agent_ppo2.py:185][0m |          -0.0039 |          80.0994 |          18.5638 |
[32m[20221213 21:03:29 @agent_ppo2.py:185][0m |          -0.0054 |          80.0443 |          18.5647 |
[32m[20221213 21:03:29 @agent_ppo2.py:185][0m |          -0.0044 |          79.7942 |          18.5635 |
[32m[20221213 21:03:29 @agent_ppo2.py:185][0m |           0.0070 |          87.2081 |          18.5528 |
[32m[20221213 21:03:29 @agent_ppo2.py:185][0m |          -0.0054 |          79.5473 |          18.5497 |
[32m[20221213 21:03:29 @agent_ppo2.py:185][0m |          -0.0061 |          79.4383 |          18.5587 |
[32m[20221213 21:03:29 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 21:03:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 525.60
[32m[20221213 21:03:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 595.00
[32m[20221213 21:03:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.00
[32m[20221213 21:03:30 @agent_ppo2.py:143][0m Total time:       7.92 min
[32m[20221213 21:03:30 @agent_ppo2.py:145][0m 780288 total steps have happened
[32m[20221213 21:03:30 @agent_ppo2.py:121][0m #------------------------ Iteration 381 --------------------------#
[32m[20221213 21:03:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:30 @agent_ppo2.py:185][0m |          -0.0016 |          81.3589 |          18.5301 |
[32m[20221213 21:03:30 @agent_ppo2.py:185][0m |          -0.0024 |          80.9622 |          18.5069 |
[32m[20221213 21:03:30 @agent_ppo2.py:185][0m |          -0.0061 |          80.3877 |          18.4933 |
[32m[20221213 21:03:30 @agent_ppo2.py:185][0m |          -0.0071 |          80.1053 |          18.4962 |
[32m[20221213 21:03:30 @agent_ppo2.py:185][0m |           0.0007 |          81.8924 |          18.4961 |
[32m[20221213 21:03:30 @agent_ppo2.py:185][0m |          -0.0059 |          79.7552 |          18.4931 |
[32m[20221213 21:03:30 @agent_ppo2.py:185][0m |          -0.0092 |          79.6826 |          18.4875 |
[32m[20221213 21:03:31 @agent_ppo2.py:185][0m |          -0.0076 |          79.5040 |          18.4873 |
[32m[20221213 21:03:31 @agent_ppo2.py:185][0m |          -0.0057 |          79.3409 |          18.4838 |
[32m[20221213 21:03:31 @agent_ppo2.py:185][0m |          -0.0077 |          79.2447 |          18.4924 |
[32m[20221213 21:03:31 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:03:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.80
[32m[20221213 21:03:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 569.00
[32m[20221213 21:03:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.00
[32m[20221213 21:03:31 @agent_ppo2.py:143][0m Total time:       7.94 min
[32m[20221213 21:03:31 @agent_ppo2.py:145][0m 782336 total steps have happened
[32m[20221213 21:03:31 @agent_ppo2.py:121][0m #------------------------ Iteration 382 --------------------------#
[32m[20221213 21:03:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:31 @agent_ppo2.py:185][0m |          -0.0001 |          81.4163 |          18.4118 |
[32m[20221213 21:03:31 @agent_ppo2.py:185][0m |          -0.0036 |          80.6452 |          18.3985 |
[32m[20221213 21:03:31 @agent_ppo2.py:185][0m |          -0.0032 |          80.1388 |          18.3971 |
[32m[20221213 21:03:31 @agent_ppo2.py:185][0m |          -0.0038 |          80.2306 |          18.3944 |
[32m[20221213 21:03:32 @agent_ppo2.py:185][0m |          -0.0037 |          79.7403 |          18.3944 |
[32m[20221213 21:03:32 @agent_ppo2.py:185][0m |           0.0019 |          83.7928 |          18.3991 |
[32m[20221213 21:03:32 @agent_ppo2.py:185][0m |          -0.0036 |          79.5043 |          18.3860 |
[32m[20221213 21:03:32 @agent_ppo2.py:185][0m |          -0.0060 |          79.3767 |          18.4003 |
[32m[20221213 21:03:32 @agent_ppo2.py:185][0m |          -0.0048 |          79.2994 |          18.3937 |
[32m[20221213 21:03:32 @agent_ppo2.py:185][0m |          -0.0079 |          79.2810 |          18.3990 |
[32m[20221213 21:03:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:03:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.80
[32m[20221213 21:03:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.00
[32m[20221213 21:03:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.00
[32m[20221213 21:03:32 @agent_ppo2.py:143][0m Total time:       7.96 min
[32m[20221213 21:03:32 @agent_ppo2.py:145][0m 784384 total steps have happened
[32m[20221213 21:03:32 @agent_ppo2.py:121][0m #------------------------ Iteration 383 --------------------------#
[32m[20221213 21:03:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:32 @agent_ppo2.py:185][0m |           0.0078 |          84.4098 |          18.6534 |
[32m[20221213 21:03:32 @agent_ppo2.py:185][0m |          -0.0044 |          79.0468 |          18.6476 |
[32m[20221213 21:03:33 @agent_ppo2.py:185][0m |          -0.0058 |          78.4410 |          18.6407 |
[32m[20221213 21:03:33 @agent_ppo2.py:185][0m |          -0.0068 |          78.0662 |          18.6222 |
[32m[20221213 21:03:33 @agent_ppo2.py:185][0m |          -0.0007 |          79.8067 |          18.6436 |
[32m[20221213 21:03:33 @agent_ppo2.py:185][0m |          -0.0023 |          78.4997 |          18.6200 |
[32m[20221213 21:03:33 @agent_ppo2.py:185][0m |          -0.0070 |          77.5668 |          18.6323 |
[32m[20221213 21:03:33 @agent_ppo2.py:185][0m |           0.0026 |          80.1671 |          18.6380 |
[32m[20221213 21:03:33 @agent_ppo2.py:185][0m |          -0.0107 |          77.4984 |          18.6361 |
[32m[20221213 21:03:33 @agent_ppo2.py:185][0m |          -0.0053 |          77.4751 |          18.6260 |
[32m[20221213 21:03:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:03:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 560.80
[32m[20221213 21:03:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 21:03:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 582.00
[32m[20221213 21:03:33 @agent_ppo2.py:143][0m Total time:       7.98 min
[32m[20221213 21:03:33 @agent_ppo2.py:145][0m 786432 total steps have happened
[32m[20221213 21:03:33 @agent_ppo2.py:121][0m #------------------------ Iteration 384 --------------------------#
[32m[20221213 21:03:33 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:03:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:34 @agent_ppo2.py:185][0m |           0.0016 |          80.6477 |          18.5334 |
[32m[20221213 21:03:34 @agent_ppo2.py:185][0m |          -0.0028 |          79.9163 |          18.5027 |
[32m[20221213 21:03:34 @agent_ppo2.py:185][0m |          -0.0047 |          79.4806 |          18.5068 |
[32m[20221213 21:03:34 @agent_ppo2.py:185][0m |           0.0026 |          81.0365 |          18.5009 |
[32m[20221213 21:03:34 @agent_ppo2.py:185][0m |          -0.0029 |          79.1160 |          18.4882 |
[32m[20221213 21:03:34 @agent_ppo2.py:185][0m |          -0.0046 |          78.7659 |          18.4925 |
[32m[20221213 21:03:34 @agent_ppo2.py:185][0m |           0.0029 |          81.6688 |          18.4830 |
[32m[20221213 21:03:34 @agent_ppo2.py:185][0m |          -0.0046 |          78.4712 |          18.4922 |
[32m[20221213 21:03:34 @agent_ppo2.py:185][0m |          -0.0026 |          79.2074 |          18.4751 |
[32m[20221213 21:03:34 @agent_ppo2.py:185][0m |          -0.0084 |          78.1620 |          18.4871 |
[32m[20221213 21:03:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:03:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.00
[32m[20221213 21:03:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.00
[32m[20221213 21:03:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 609.00
[32m[20221213 21:03:34 @agent_ppo2.py:143][0m Total time:       8.00 min
[32m[20221213 21:03:34 @agent_ppo2.py:145][0m 788480 total steps have happened
[32m[20221213 21:03:34 @agent_ppo2.py:121][0m #------------------------ Iteration 385 --------------------------#
[32m[20221213 21:03:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:35 @agent_ppo2.py:185][0m |           0.0104 |          93.3636 |          18.3510 |
[32m[20221213 21:03:35 @agent_ppo2.py:185][0m |          -0.0041 |          84.5303 |          18.3329 |
[32m[20221213 21:03:35 @agent_ppo2.py:185][0m |          -0.0027 |          83.9534 |          18.3271 |
[32m[20221213 21:03:35 @agent_ppo2.py:185][0m |          -0.0001 |          85.5389 |          18.3241 |
[32m[20221213 21:03:35 @agent_ppo2.py:185][0m |          -0.0056 |          83.0562 |          18.3183 |
[32m[20221213 21:03:35 @agent_ppo2.py:185][0m |          -0.0030 |          83.2770 |          18.3226 |
[32m[20221213 21:03:35 @agent_ppo2.py:185][0m |           0.0046 |          86.3203 |          18.3199 |
[32m[20221213 21:03:35 @agent_ppo2.py:185][0m |          -0.0050 |          82.1613 |          18.3099 |
[32m[20221213 21:03:35 @agent_ppo2.py:185][0m |          -0.0049 |          82.3661 |          18.3124 |
[32m[20221213 21:03:36 @agent_ppo2.py:185][0m |          -0.0021 |          82.2135 |          18.3177 |
[32m[20221213 21:03:36 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:03:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 508.40
[32m[20221213 21:03:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.00
[32m[20221213 21:03:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.00
[32m[20221213 21:03:36 @agent_ppo2.py:143][0m Total time:       8.02 min
[32m[20221213 21:03:36 @agent_ppo2.py:145][0m 790528 total steps have happened
[32m[20221213 21:03:36 @agent_ppo2.py:121][0m #------------------------ Iteration 386 --------------------------#
[32m[20221213 21:03:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:36 @agent_ppo2.py:185][0m |          -0.0045 |          86.6814 |          18.5839 |
[32m[20221213 21:03:36 @agent_ppo2.py:185][0m |          -0.0061 |          85.8584 |          18.5682 |
[32m[20221213 21:03:36 @agent_ppo2.py:185][0m |          -0.0076 |          85.4213 |          18.5604 |
[32m[20221213 21:03:36 @agent_ppo2.py:185][0m |          -0.0066 |          84.9966 |          18.5554 |
[32m[20221213 21:03:36 @agent_ppo2.py:185][0m |          -0.0007 |          86.2944 |          18.5397 |
[32m[20221213 21:03:36 @agent_ppo2.py:185][0m |          -0.0060 |          84.4139 |          18.5470 |
[32m[20221213 21:03:37 @agent_ppo2.py:185][0m |          -0.0040 |          84.5180 |          18.5380 |
[32m[20221213 21:03:37 @agent_ppo2.py:185][0m |          -0.0060 |          84.2284 |          18.5510 |
[32m[20221213 21:03:37 @agent_ppo2.py:185][0m |          -0.0057 |          83.9925 |          18.5402 |
[32m[20221213 21:03:37 @agent_ppo2.py:185][0m |           0.0023 |          89.3351 |          18.5477 |
[32m[20221213 21:03:37 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:03:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 528.40
[32m[20221213 21:03:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 585.00
[32m[20221213 21:03:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.00
[32m[20221213 21:03:37 @agent_ppo2.py:143][0m Total time:       8.04 min
[32m[20221213 21:03:37 @agent_ppo2.py:145][0m 792576 total steps have happened
[32m[20221213 21:03:37 @agent_ppo2.py:121][0m #------------------------ Iteration 387 --------------------------#
[32m[20221213 21:03:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:03:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:37 @agent_ppo2.py:185][0m |           0.0012 |          83.3778 |          18.4781 |
[32m[20221213 21:03:37 @agent_ppo2.py:185][0m |          -0.0067 |          81.9947 |          18.4664 |
[32m[20221213 21:03:37 @agent_ppo2.py:185][0m |          -0.0064 |          81.3433 |          18.4613 |
[32m[20221213 21:03:38 @agent_ppo2.py:185][0m |          -0.0079 |          80.8517 |          18.4671 |
[32m[20221213 21:03:38 @agent_ppo2.py:185][0m |          -0.0076 |          80.3722 |          18.4489 |
[32m[20221213 21:03:38 @agent_ppo2.py:185][0m |          -0.0083 |          80.0652 |          18.4549 |
[32m[20221213 21:03:38 @agent_ppo2.py:185][0m |          -0.0080 |          79.8722 |          18.4485 |
[32m[20221213 21:03:38 @agent_ppo2.py:185][0m |          -0.0084 |          79.5096 |          18.4417 |
[32m[20221213 21:03:38 @agent_ppo2.py:185][0m |          -0.0099 |          79.3215 |          18.4516 |
[32m[20221213 21:03:38 @agent_ppo2.py:185][0m |          -0.0085 |          79.1633 |          18.4410 |
[32m[20221213 21:03:38 @agent_ppo2.py:130][0m Policy update time: 1.30 s
[32m[20221213 21:03:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.40
[32m[20221213 21:03:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 584.00
[32m[20221213 21:03:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 563.00
[32m[20221213 21:03:38 @agent_ppo2.py:143][0m Total time:       8.06 min
[32m[20221213 21:03:38 @agent_ppo2.py:145][0m 794624 total steps have happened
[32m[20221213 21:03:38 @agent_ppo2.py:121][0m #------------------------ Iteration 388 --------------------------#
[32m[20221213 21:03:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:03:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:39 @agent_ppo2.py:185][0m |          -0.0038 |          86.5540 |          18.4786 |
[32m[20221213 21:03:39 @agent_ppo2.py:185][0m |          -0.0050 |          84.5706 |          18.4447 |
[32m[20221213 21:03:39 @agent_ppo2.py:185][0m |          -0.0039 |          83.9768 |          18.4333 |
[32m[20221213 21:03:39 @agent_ppo2.py:185][0m |          -0.0082 |          83.4691 |          18.4312 |
[32m[20221213 21:03:39 @agent_ppo2.py:185][0m |          -0.0073 |          83.1108 |          18.4351 |
[32m[20221213 21:03:39 @agent_ppo2.py:185][0m |          -0.0013 |          84.1929 |          18.4283 |
[32m[20221213 21:03:39 @agent_ppo2.py:185][0m |          -0.0073 |          82.7530 |          18.4151 |
[32m[20221213 21:03:39 @agent_ppo2.py:185][0m |          -0.0084 |          82.5140 |          18.4174 |
[32m[20221213 21:03:40 @agent_ppo2.py:185][0m |          -0.0060 |          82.3305 |          18.4296 |
[32m[20221213 21:03:40 @agent_ppo2.py:185][0m |          -0.0097 |          82.2490 |          18.4202 |
[32m[20221213 21:03:40 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:03:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.40
[32m[20221213 21:03:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 580.00
[32m[20221213 21:03:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 574.00
[32m[20221213 21:03:40 @agent_ppo2.py:143][0m Total time:       8.08 min
[32m[20221213 21:03:40 @agent_ppo2.py:145][0m 796672 total steps have happened
[32m[20221213 21:03:40 @agent_ppo2.py:121][0m #------------------------ Iteration 389 --------------------------#
[32m[20221213 21:03:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:40 @agent_ppo2.py:185][0m |          -0.0011 |          80.3847 |          18.3744 |
[32m[20221213 21:03:40 @agent_ppo2.py:185][0m |          -0.0032 |          79.6460 |          18.3668 |
[32m[20221213 21:03:40 @agent_ppo2.py:185][0m |          -0.0051 |          79.2954 |          18.3569 |
[32m[20221213 21:03:40 @agent_ppo2.py:185][0m |          -0.0021 |          79.7996 |          18.3560 |
[32m[20221213 21:03:40 @agent_ppo2.py:185][0m |          -0.0010 |          79.3515 |          18.3515 |
[32m[20221213 21:03:41 @agent_ppo2.py:185][0m |          -0.0103 |          78.7092 |          18.3512 |
[32m[20221213 21:03:41 @agent_ppo2.py:185][0m |          -0.0069 |          78.3511 |          18.3450 |
[32m[20221213 21:03:41 @agent_ppo2.py:185][0m |          -0.0069 |          78.2068 |          18.3538 |
[32m[20221213 21:03:41 @agent_ppo2.py:185][0m |          -0.0074 |          78.0669 |          18.3449 |
[32m[20221213 21:03:41 @agent_ppo2.py:185][0m |          -0.0079 |          77.9793 |          18.3423 |
[32m[20221213 21:03:41 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:03:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 507.20
[32m[20221213 21:03:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 560.00
[32m[20221213 21:03:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 572.00
[32m[20221213 21:03:41 @agent_ppo2.py:143][0m Total time:       8.11 min
[32m[20221213 21:03:41 @agent_ppo2.py:145][0m 798720 total steps have happened
[32m[20221213 21:03:41 @agent_ppo2.py:121][0m #------------------------ Iteration 390 --------------------------#
[32m[20221213 21:03:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:03:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:41 @agent_ppo2.py:185][0m |          -0.0002 |          82.1068 |          18.3612 |
[32m[20221213 21:03:42 @agent_ppo2.py:185][0m |          -0.0047 |          81.7314 |          18.3234 |
[32m[20221213 21:03:42 @agent_ppo2.py:185][0m |          -0.0036 |          81.6015 |          18.3162 |
[32m[20221213 21:03:42 @agent_ppo2.py:185][0m |          -0.0063 |          81.1957 |          18.3093 |
[32m[20221213 21:03:42 @agent_ppo2.py:185][0m |          -0.0060 |          81.0192 |          18.2985 |
[32m[20221213 21:03:42 @agent_ppo2.py:185][0m |          -0.0092 |          80.8536 |          18.3053 |
[32m[20221213 21:03:42 @agent_ppo2.py:185][0m |          -0.0035 |          81.3212 |          18.2908 |
[32m[20221213 21:03:42 @agent_ppo2.py:185][0m |          -0.0067 |          80.6465 |          18.2943 |
[32m[20221213 21:03:42 @agent_ppo2.py:185][0m |          -0.0083 |          80.4468 |          18.2902 |
[32m[20221213 21:03:42 @agent_ppo2.py:185][0m |          -0.0088 |          80.4317 |          18.2912 |
[32m[20221213 21:03:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:03:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 528.00
[32m[20221213 21:03:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 569.00
[32m[20221213 21:03:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.00
[32m[20221213 21:03:42 @agent_ppo2.py:143][0m Total time:       8.13 min
[32m[20221213 21:03:42 @agent_ppo2.py:145][0m 800768 total steps have happened
[32m[20221213 21:03:42 @agent_ppo2.py:121][0m #------------------------ Iteration 391 --------------------------#
[32m[20221213 21:03:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:43 @agent_ppo2.py:185][0m |          -0.0017 |          84.2540 |          18.5622 |
[32m[20221213 21:03:43 @agent_ppo2.py:185][0m |          -0.0047 |          83.8439 |          18.5444 |
[32m[20221213 21:03:43 @agent_ppo2.py:185][0m |          -0.0051 |          83.4507 |          18.5472 |
[32m[20221213 21:03:43 @agent_ppo2.py:185][0m |          -0.0030 |          83.8560 |          18.5304 |
[32m[20221213 21:03:43 @agent_ppo2.py:185][0m |          -0.0068 |          83.0801 |          18.5310 |
[32m[20221213 21:03:43 @agent_ppo2.py:185][0m |          -0.0095 |          82.9062 |          18.5306 |
[32m[20221213 21:03:43 @agent_ppo2.py:185][0m |           0.0053 |          88.0991 |          18.5265 |
[32m[20221213 21:03:43 @agent_ppo2.py:185][0m |          -0.0059 |          83.6681 |          18.5318 |
[32m[20221213 21:03:43 @agent_ppo2.py:185][0m |          -0.0080 |          82.6767 |          18.5253 |
[32m[20221213 21:03:44 @agent_ppo2.py:185][0m |          -0.0072 |          83.1741 |          18.5255 |
[32m[20221213 21:03:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:03:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 514.20
[32m[20221213 21:03:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 560.00
[32m[20221213 21:03:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.00
[32m[20221213 21:03:44 @agent_ppo2.py:143][0m Total time:       8.15 min
[32m[20221213 21:03:44 @agent_ppo2.py:145][0m 802816 total steps have happened
[32m[20221213 21:03:44 @agent_ppo2.py:121][0m #------------------------ Iteration 392 --------------------------#
[32m[20221213 21:03:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:44 @agent_ppo2.py:185][0m |          -0.0019 |          78.0724 |          18.4125 |
[32m[20221213 21:03:44 @agent_ppo2.py:185][0m |          -0.0041 |          77.3515 |          18.3911 |
[32m[20221213 21:03:44 @agent_ppo2.py:185][0m |          -0.0084 |          77.1062 |          18.3792 |
[32m[20221213 21:03:44 @agent_ppo2.py:185][0m |          -0.0059 |          76.9587 |          18.3807 |
[32m[20221213 21:03:44 @agent_ppo2.py:185][0m |          -0.0083 |          76.4900 |          18.3698 |
[32m[20221213 21:03:44 @agent_ppo2.py:185][0m |          -0.0074 |          76.2820 |          18.3741 |
[32m[20221213 21:03:45 @agent_ppo2.py:185][0m |          -0.0059 |          76.0745 |          18.3785 |
[32m[20221213 21:03:45 @agent_ppo2.py:185][0m |          -0.0039 |          75.9466 |          18.3755 |
[32m[20221213 21:03:45 @agent_ppo2.py:185][0m |          -0.0058 |          75.8454 |          18.3710 |
[32m[20221213 21:03:45 @agent_ppo2.py:185][0m |          -0.0101 |          75.8949 |          18.3734 |
[32m[20221213 21:03:45 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:03:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 500.00
[32m[20221213 21:03:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.00
[32m[20221213 21:03:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 568.00
[32m[20221213 21:03:45 @agent_ppo2.py:143][0m Total time:       8.17 min
[32m[20221213 21:03:45 @agent_ppo2.py:145][0m 804864 total steps have happened
[32m[20221213 21:03:45 @agent_ppo2.py:121][0m #------------------------ Iteration 393 --------------------------#
[32m[20221213 21:03:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:45 @agent_ppo2.py:185][0m |           0.0000 |          79.7548 |          18.4718 |
[32m[20221213 21:03:45 @agent_ppo2.py:185][0m |          -0.0059 |          78.6281 |          18.4692 |
[32m[20221213 21:03:45 @agent_ppo2.py:185][0m |          -0.0044 |          78.2113 |          18.4711 |
[32m[20221213 21:03:45 @agent_ppo2.py:185][0m |          -0.0031 |          77.8269 |          18.4675 |
[32m[20221213 21:03:46 @agent_ppo2.py:185][0m |          -0.0006 |          78.1391 |          18.4638 |
[32m[20221213 21:03:46 @agent_ppo2.py:185][0m |          -0.0055 |          77.5485 |          18.4638 |
[32m[20221213 21:03:46 @agent_ppo2.py:185][0m |          -0.0043 |          77.5207 |          18.4626 |
[32m[20221213 21:03:46 @agent_ppo2.py:185][0m |          -0.0053 |          77.1712 |          18.4497 |
[32m[20221213 21:03:46 @agent_ppo2.py:185][0m |          -0.0078 |          77.3761 |          18.4588 |
[32m[20221213 21:03:46 @agent_ppo2.py:185][0m |          -0.0073 |          77.1353 |          18.4573 |
[32m[20221213 21:03:46 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:03:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.80
[32m[20221213 21:03:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.00
[32m[20221213 21:03:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 663.00
[32m[20221213 21:03:46 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 663.00
[32m[20221213 21:03:46 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 663.00
[32m[20221213 21:03:46 @agent_ppo2.py:143][0m Total time:       8.19 min
[32m[20221213 21:03:46 @agent_ppo2.py:145][0m 806912 total steps have happened
[32m[20221213 21:03:46 @agent_ppo2.py:121][0m #------------------------ Iteration 394 --------------------------#
[32m[20221213 21:03:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:03:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:47 @agent_ppo2.py:185][0m |          -0.0007 |          80.8304 |          18.5740 |
[32m[20221213 21:03:47 @agent_ppo2.py:185][0m |          -0.0032 |          79.0344 |          18.5646 |
[32m[20221213 21:03:47 @agent_ppo2.py:185][0m |          -0.0045 |          78.4134 |          18.5605 |
[32m[20221213 21:03:47 @agent_ppo2.py:185][0m |           0.0068 |          85.5482 |          18.5525 |
[32m[20221213 21:03:47 @agent_ppo2.py:185][0m |          -0.0038 |          77.7910 |          18.5496 |
[32m[20221213 21:03:47 @agent_ppo2.py:185][0m |          -0.0038 |          77.6417 |          18.5507 |
[32m[20221213 21:03:47 @agent_ppo2.py:185][0m |          -0.0084 |          77.6320 |          18.5471 |
[32m[20221213 21:03:47 @agent_ppo2.py:185][0m |          -0.0049 |          77.4964 |          18.5505 |
[32m[20221213 21:03:47 @agent_ppo2.py:185][0m |           0.0055 |          80.9440 |          18.5407 |
[32m[20221213 21:03:47 @agent_ppo2.py:185][0m |           0.0022 |          81.6066 |          18.5385 |
[32m[20221213 21:03:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:03:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.00
[32m[20221213 21:03:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 571.00
[32m[20221213 21:03:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.00
[32m[20221213 21:03:48 @agent_ppo2.py:143][0m Total time:       8.21 min
[32m[20221213 21:03:48 @agent_ppo2.py:145][0m 808960 total steps have happened
[32m[20221213 21:03:48 @agent_ppo2.py:121][0m #------------------------ Iteration 395 --------------------------#
[32m[20221213 21:03:48 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:03:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:48 @agent_ppo2.py:185][0m |          -0.0014 |          79.0349 |          18.5655 |
[32m[20221213 21:03:48 @agent_ppo2.py:185][0m |          -0.0054 |          78.0930 |          18.5564 |
[32m[20221213 21:03:48 @agent_ppo2.py:185][0m |          -0.0042 |          77.6882 |          18.5527 |
[32m[20221213 21:03:48 @agent_ppo2.py:185][0m |          -0.0042 |          77.2952 |          18.5630 |
[32m[20221213 21:03:48 @agent_ppo2.py:185][0m |          -0.0058 |          77.0295 |          18.5598 |
[32m[20221213 21:03:48 @agent_ppo2.py:185][0m |          -0.0039 |          76.9412 |          18.5598 |
[32m[20221213 21:03:48 @agent_ppo2.py:185][0m |          -0.0006 |          77.2876 |          18.5616 |
[32m[20221213 21:03:49 @agent_ppo2.py:185][0m |          -0.0004 |          78.1838 |          18.5617 |
[32m[20221213 21:03:49 @agent_ppo2.py:185][0m |          -0.0060 |          76.5671 |          18.5660 |
[32m[20221213 21:03:49 @agent_ppo2.py:185][0m |          -0.0042 |          76.4687 |          18.5612 |
[32m[20221213 21:03:49 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:03:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.20
[32m[20221213 21:03:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 572.00
[32m[20221213 21:03:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.00
[32m[20221213 21:03:49 @agent_ppo2.py:143][0m Total time:       8.24 min
[32m[20221213 21:03:49 @agent_ppo2.py:145][0m 811008 total steps have happened
[32m[20221213 21:03:49 @agent_ppo2.py:121][0m #------------------------ Iteration 396 --------------------------#
[32m[20221213 21:03:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:49 @agent_ppo2.py:185][0m |           0.0053 |          85.3924 |          18.5337 |
[32m[20221213 21:03:49 @agent_ppo2.py:185][0m |           0.0083 |          90.4117 |          18.5112 |
[32m[20221213 21:03:49 @agent_ppo2.py:185][0m |          -0.0045 |          82.4326 |          18.4959 |
[32m[20221213 21:03:49 @agent_ppo2.py:185][0m |          -0.0074 |          82.1875 |          18.4884 |
[32m[20221213 21:03:50 @agent_ppo2.py:185][0m |          -0.0056 |          82.0104 |          18.4832 |
[32m[20221213 21:03:50 @agent_ppo2.py:185][0m |          -0.0043 |          81.8215 |          18.4810 |
[32m[20221213 21:03:50 @agent_ppo2.py:185][0m |          -0.0021 |          84.6004 |          18.4679 |
[32m[20221213 21:03:50 @agent_ppo2.py:185][0m |          -0.0070 |          81.6388 |          18.4655 |
[32m[20221213 21:03:50 @agent_ppo2.py:185][0m |           0.0025 |          90.5728 |          18.4642 |
[32m[20221213 21:03:50 @agent_ppo2.py:185][0m |          -0.0020 |          82.2062 |          18.4680 |
[32m[20221213 21:03:50 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:03:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 512.40
[32m[20221213 21:03:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.00
[32m[20221213 21:03:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.00
[32m[20221213 21:03:50 @agent_ppo2.py:143][0m Total time:       8.26 min
[32m[20221213 21:03:50 @agent_ppo2.py:145][0m 813056 total steps have happened
[32m[20221213 21:03:50 @agent_ppo2.py:121][0m #------------------------ Iteration 397 --------------------------#
[32m[20221213 21:03:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:50 @agent_ppo2.py:185][0m |          -0.0026 |          83.0235 |          18.5986 |
[32m[20221213 21:03:51 @agent_ppo2.py:185][0m |          -0.0044 |          82.2354 |          18.5810 |
[32m[20221213 21:03:51 @agent_ppo2.py:185][0m |          -0.0062 |          81.8022 |          18.5934 |
[32m[20221213 21:03:51 @agent_ppo2.py:185][0m |          -0.0030 |          81.5616 |          18.5829 |
[32m[20221213 21:03:51 @agent_ppo2.py:185][0m |          -0.0028 |          82.1337 |          18.5849 |
[32m[20221213 21:03:51 @agent_ppo2.py:185][0m |          -0.0051 |          80.9332 |          18.5847 |
[32m[20221213 21:03:51 @agent_ppo2.py:185][0m |          -0.0059 |          80.6534 |          18.5763 |
[32m[20221213 21:03:51 @agent_ppo2.py:185][0m |          -0.0061 |          80.6337 |          18.5839 |
[32m[20221213 21:03:51 @agent_ppo2.py:185][0m |          -0.0053 |          80.4527 |          18.5801 |
[32m[20221213 21:03:51 @agent_ppo2.py:185][0m |          -0.0075 |          80.2493 |          18.5720 |
[32m[20221213 21:03:51 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:03:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 508.40
[32m[20221213 21:03:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 559.00
[32m[20221213 21:03:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.00
[32m[20221213 21:03:51 @agent_ppo2.py:143][0m Total time:       8.28 min
[32m[20221213 21:03:51 @agent_ppo2.py:145][0m 815104 total steps have happened
[32m[20221213 21:03:51 @agent_ppo2.py:121][0m #------------------------ Iteration 398 --------------------------#
[32m[20221213 21:03:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:52 @agent_ppo2.py:185][0m |          -0.0033 |          82.0893 |          18.5907 |
[32m[20221213 21:03:52 @agent_ppo2.py:185][0m |          -0.0007 |          82.2844 |          18.5816 |
[32m[20221213 21:03:52 @agent_ppo2.py:185][0m |          -0.0050 |          81.3419 |          18.5790 |
[32m[20221213 21:03:52 @agent_ppo2.py:185][0m |          -0.0035 |          81.1262 |          18.5719 |
[32m[20221213 21:03:52 @agent_ppo2.py:185][0m |          -0.0046 |          80.8298 |          18.5671 |
[32m[20221213 21:03:52 @agent_ppo2.py:185][0m |          -0.0034 |          80.8390 |          18.5736 |
[32m[20221213 21:03:52 @agent_ppo2.py:185][0m |           0.0012 |          83.4884 |          18.5608 |
[32m[20221213 21:03:52 @agent_ppo2.py:185][0m |          -0.0056 |          80.5052 |          18.5622 |
[32m[20221213 21:03:52 @agent_ppo2.py:185][0m |          -0.0058 |          80.2721 |          18.5600 |
[32m[20221213 21:03:52 @agent_ppo2.py:185][0m |          -0.0067 |          80.0565 |          18.5642 |
[32m[20221213 21:03:52 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:03:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 550.60
[32m[20221213 21:03:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.00
[32m[20221213 21:03:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.00
[32m[20221213 21:03:53 @agent_ppo2.py:143][0m Total time:       8.30 min
[32m[20221213 21:03:53 @agent_ppo2.py:145][0m 817152 total steps have happened
[32m[20221213 21:03:53 @agent_ppo2.py:121][0m #------------------------ Iteration 399 --------------------------#
[32m[20221213 21:03:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:53 @agent_ppo2.py:185][0m |          -0.0023 |          80.4090 |          18.6250 |
[32m[20221213 21:03:53 @agent_ppo2.py:185][0m |          -0.0035 |          79.8985 |          18.6102 |
[32m[20221213 21:03:53 @agent_ppo2.py:185][0m |          -0.0051 |          79.5880 |          18.6074 |
[32m[20221213 21:03:53 @agent_ppo2.py:185][0m |          -0.0056 |          79.2825 |          18.5936 |
[32m[20221213 21:03:53 @agent_ppo2.py:185][0m |          -0.0035 |          79.0977 |          18.5992 |
[32m[20221213 21:03:53 @agent_ppo2.py:185][0m |          -0.0030 |          78.6407 |          18.5901 |
[32m[20221213 21:03:53 @agent_ppo2.py:185][0m |          -0.0034 |          78.4793 |          18.5901 |
[32m[20221213 21:03:54 @agent_ppo2.py:185][0m |          -0.0050 |          78.3602 |          18.5945 |
[32m[20221213 21:03:54 @agent_ppo2.py:185][0m |           0.0075 |          85.2224 |          18.5887 |
[32m[20221213 21:03:54 @agent_ppo2.py:185][0m |          -0.0058 |          78.0504 |          18.5897 |
[32m[20221213 21:03:54 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:03:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.40
[32m[20221213 21:03:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.00
[32m[20221213 21:03:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 532.00
[32m[20221213 21:03:54 @agent_ppo2.py:143][0m Total time:       8.32 min
[32m[20221213 21:03:54 @agent_ppo2.py:145][0m 819200 total steps have happened
[32m[20221213 21:03:54 @agent_ppo2.py:121][0m #------------------------ Iteration 400 --------------------------#
[32m[20221213 21:03:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:03:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:54 @agent_ppo2.py:185][0m |          -0.0029 |          82.5623 |          18.5100 |
[32m[20221213 21:03:54 @agent_ppo2.py:185][0m |          -0.0054 |          81.8284 |          18.4871 |
[32m[20221213 21:03:54 @agent_ppo2.py:185][0m |           0.0065 |          91.6304 |          18.4845 |
[32m[20221213 21:03:55 @agent_ppo2.py:185][0m |          -0.0071 |          81.3465 |          18.4753 |
[32m[20221213 21:03:55 @agent_ppo2.py:185][0m |          -0.0068 |          81.1250 |          18.4806 |
[32m[20221213 21:03:55 @agent_ppo2.py:185][0m |          -0.0095 |          81.0034 |          18.4813 |
[32m[20221213 21:03:55 @agent_ppo2.py:185][0m |          -0.0062 |          81.6110 |          18.4818 |
[32m[20221213 21:03:55 @agent_ppo2.py:185][0m |          -0.0066 |          80.7859 |          18.4815 |
[32m[20221213 21:03:55 @agent_ppo2.py:185][0m |          -0.0066 |          80.7204 |          18.4781 |
[32m[20221213 21:03:55 @agent_ppo2.py:185][0m |          -0.0065 |          80.5097 |          18.4778 |
[32m[20221213 21:03:55 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:03:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.20
[32m[20221213 21:03:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.00
[32m[20221213 21:03:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.00
[32m[20221213 21:03:55 @agent_ppo2.py:143][0m Total time:       8.34 min
[32m[20221213 21:03:55 @agent_ppo2.py:145][0m 821248 total steps have happened
[32m[20221213 21:03:55 @agent_ppo2.py:121][0m #------------------------ Iteration 401 --------------------------#
[32m[20221213 21:03:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:56 @agent_ppo2.py:185][0m |           0.0068 |          85.5499 |          18.5490 |
[32m[20221213 21:03:56 @agent_ppo2.py:185][0m |          -0.0048 |          80.0591 |          18.5035 |
[32m[20221213 21:03:56 @agent_ppo2.py:185][0m |          -0.0039 |          79.8437 |          18.5142 |
[32m[20221213 21:03:56 @agent_ppo2.py:185][0m |          -0.0030 |          80.0565 |          18.5135 |
[32m[20221213 21:03:56 @agent_ppo2.py:185][0m |          -0.0083 |          79.3956 |          18.5108 |
[32m[20221213 21:03:56 @agent_ppo2.py:185][0m |          -0.0074 |          79.2786 |          18.5078 |
[32m[20221213 21:03:56 @agent_ppo2.py:185][0m |          -0.0083 |          79.0934 |          18.5079 |
[32m[20221213 21:03:56 @agent_ppo2.py:185][0m |          -0.0103 |          79.2237 |          18.5150 |
[32m[20221213 21:03:56 @agent_ppo2.py:185][0m |          -0.0067 |          79.0468 |          18.5030 |
[32m[20221213 21:03:56 @agent_ppo2.py:185][0m |          -0.0089 |          78.8951 |          18.5019 |
[32m[20221213 21:03:56 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 21:03:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 531.60
[32m[20221213 21:03:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 547.00
[32m[20221213 21:03:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 555.00
[32m[20221213 21:03:57 @agent_ppo2.py:143][0m Total time:       8.36 min
[32m[20221213 21:03:57 @agent_ppo2.py:145][0m 823296 total steps have happened
[32m[20221213 21:03:57 @agent_ppo2.py:121][0m #------------------------ Iteration 402 --------------------------#
[32m[20221213 21:03:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:03:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:57 @agent_ppo2.py:185][0m |          -0.0006 |          81.4600 |          18.7106 |
[32m[20221213 21:03:57 @agent_ppo2.py:185][0m |          -0.0047 |          80.4719 |          18.6780 |
[32m[20221213 21:03:57 @agent_ppo2.py:185][0m |          -0.0058 |          80.0211 |          18.6722 |
[32m[20221213 21:03:57 @agent_ppo2.py:185][0m |          -0.0042 |          79.6802 |          18.6710 |
[32m[20221213 21:03:57 @agent_ppo2.py:185][0m |          -0.0035 |          79.3905 |          18.6702 |
[32m[20221213 21:03:57 @agent_ppo2.py:185][0m |          -0.0051 |          79.3791 |          18.6588 |
[32m[20221213 21:03:57 @agent_ppo2.py:185][0m |          -0.0091 |          79.1422 |          18.6687 |
[32m[20221213 21:03:58 @agent_ppo2.py:185][0m |          -0.0060 |          78.9556 |          18.6545 |
[32m[20221213 21:03:58 @agent_ppo2.py:185][0m |          -0.0033 |          79.6877 |          18.6472 |
[32m[20221213 21:03:58 @agent_ppo2.py:185][0m |          -0.0058 |          78.7553 |          18.6580 |
[32m[20221213 21:03:58 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:03:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.40
[32m[20221213 21:03:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.00
[32m[20221213 21:03:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.00
[32m[20221213 21:03:58 @agent_ppo2.py:143][0m Total time:       8.39 min
[32m[20221213 21:03:58 @agent_ppo2.py:145][0m 825344 total steps have happened
[32m[20221213 21:03:58 @agent_ppo2.py:121][0m #------------------------ Iteration 403 --------------------------#
[32m[20221213 21:03:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 21:03:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:58 @agent_ppo2.py:185][0m |          -0.0022 |          82.1834 |          18.5465 |
[32m[20221213 21:03:58 @agent_ppo2.py:185][0m |          -0.0007 |          82.6962 |          18.5207 |
[32m[20221213 21:03:58 @agent_ppo2.py:185][0m |          -0.0054 |          80.8675 |          18.5313 |
[32m[20221213 21:03:58 @agent_ppo2.py:185][0m |          -0.0054 |          80.4957 |          18.5250 |
[32m[20221213 21:03:59 @agent_ppo2.py:185][0m |          -0.0032 |          80.1675 |          18.5251 |
[32m[20221213 21:03:59 @agent_ppo2.py:185][0m |          -0.0018 |          80.0807 |          18.5169 |
[32m[20221213 21:03:59 @agent_ppo2.py:185][0m |          -0.0074 |          79.8281 |          18.5262 |
[32m[20221213 21:03:59 @agent_ppo2.py:185][0m |          -0.0079 |          79.6709 |          18.5257 |
[32m[20221213 21:03:59 @agent_ppo2.py:185][0m |          -0.0041 |          79.5925 |          18.5207 |
[32m[20221213 21:03:59 @agent_ppo2.py:185][0m |          -0.0032 |          80.1356 |          18.5154 |
[32m[20221213 21:03:59 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:03:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 541.60
[32m[20221213 21:03:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 581.00
[32m[20221213 21:03:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 551.00
[32m[20221213 21:03:59 @agent_ppo2.py:143][0m Total time:       8.41 min
[32m[20221213 21:03:59 @agent_ppo2.py:145][0m 827392 total steps have happened
[32m[20221213 21:03:59 @agent_ppo2.py:121][0m #------------------------ Iteration 404 --------------------------#
[32m[20221213 21:03:59 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:03:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:03:59 @agent_ppo2.py:185][0m |          -0.0002 |          80.8793 |          18.6938 |
[32m[20221213 21:04:00 @agent_ppo2.py:185][0m |          -0.0039 |          79.9053 |          18.6904 |
[32m[20221213 21:04:00 @agent_ppo2.py:185][0m |          -0.0039 |          79.4776 |          18.6832 |
[32m[20221213 21:04:00 @agent_ppo2.py:185][0m |          -0.0070 |          79.4275 |          18.6698 |
[32m[20221213 21:04:00 @agent_ppo2.py:185][0m |          -0.0020 |          79.4424 |          18.6713 |
[32m[20221213 21:04:00 @agent_ppo2.py:185][0m |           0.0032 |          82.5780 |          18.6758 |
[32m[20221213 21:04:00 @agent_ppo2.py:185][0m |          -0.0046 |          78.8915 |          18.6733 |
[32m[20221213 21:04:00 @agent_ppo2.py:185][0m |           0.0005 |          81.0319 |          18.6753 |
[32m[20221213 21:04:00 @agent_ppo2.py:185][0m |          -0.0078 |          78.5458 |          18.6664 |
[32m[20221213 21:04:00 @agent_ppo2.py:185][0m |           0.0081 |          84.7696 |          18.6701 |
[32m[20221213 21:04:00 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:04:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.00
[32m[20221213 21:04:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.00
[32m[20221213 21:04:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 435.00
[32m[20221213 21:04:00 @agent_ppo2.py:143][0m Total time:       8.43 min
[32m[20221213 21:04:00 @agent_ppo2.py:145][0m 829440 total steps have happened
[32m[20221213 21:04:00 @agent_ppo2.py:121][0m #------------------------ Iteration 405 --------------------------#
[32m[20221213 21:04:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:01 @agent_ppo2.py:185][0m |           0.0003 |          81.2603 |          18.6835 |
[32m[20221213 21:04:01 @agent_ppo2.py:185][0m |          -0.0012 |          79.8074 |          18.6431 |
[32m[20221213 21:04:01 @agent_ppo2.py:185][0m |          -0.0049 |          79.0998 |          18.6405 |
[32m[20221213 21:04:01 @agent_ppo2.py:185][0m |           0.0009 |          81.9056 |          18.6308 |
[32m[20221213 21:04:01 @agent_ppo2.py:185][0m |           0.0024 |          82.8450 |          18.6276 |
[32m[20221213 21:04:01 @agent_ppo2.py:185][0m |          -0.0039 |          79.0446 |          18.6310 |
[32m[20221213 21:04:01 @agent_ppo2.py:185][0m |          -0.0050 |          78.7194 |          18.6288 |
[32m[20221213 21:04:01 @agent_ppo2.py:185][0m |          -0.0037 |          78.3101 |          18.6264 |
[32m[20221213 21:04:02 @agent_ppo2.py:185][0m |          -0.0061 |          77.9050 |          18.6255 |
[32m[20221213 21:04:02 @agent_ppo2.py:185][0m |          -0.0028 |          78.6397 |          18.6304 |
[32m[20221213 21:04:02 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:04:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 530.20
[32m[20221213 21:04:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.00
[32m[20221213 21:04:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 574.00
[32m[20221213 21:04:02 @agent_ppo2.py:143][0m Total time:       8.45 min
[32m[20221213 21:04:02 @agent_ppo2.py:145][0m 831488 total steps have happened
[32m[20221213 21:04:02 @agent_ppo2.py:121][0m #------------------------ Iteration 406 --------------------------#
[32m[20221213 21:04:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:02 @agent_ppo2.py:185][0m |          -0.0012 |          83.5554 |          18.6452 |
[32m[20221213 21:04:02 @agent_ppo2.py:185][0m |          -0.0065 |          83.0170 |          18.6287 |
[32m[20221213 21:04:02 @agent_ppo2.py:185][0m |          -0.0032 |          82.9077 |          18.6255 |
[32m[20221213 21:04:02 @agent_ppo2.py:185][0m |           0.0026 |          87.4439 |          18.6234 |
[32m[20221213 21:04:02 @agent_ppo2.py:185][0m |          -0.0067 |          82.1219 |          18.6102 |
[32m[20221213 21:04:02 @agent_ppo2.py:185][0m |           0.0056 |          91.6900 |          18.6201 |
[32m[20221213 21:04:03 @agent_ppo2.py:185][0m |          -0.0073 |          81.9262 |          18.6250 |
[32m[20221213 21:04:03 @agent_ppo2.py:185][0m |          -0.0121 |          82.1034 |          18.6217 |
[32m[20221213 21:04:03 @agent_ppo2.py:185][0m |          -0.0049 |          81.7719 |          18.6252 |
[32m[20221213 21:04:03 @agent_ppo2.py:185][0m |          -0.0057 |          81.7021 |          18.6174 |
[32m[20221213 21:04:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:04:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.00
[32m[20221213 21:04:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.00
[32m[20221213 21:04:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 598.00
[32m[20221213 21:04:03 @agent_ppo2.py:143][0m Total time:       8.47 min
[32m[20221213 21:04:03 @agent_ppo2.py:145][0m 833536 total steps have happened
[32m[20221213 21:04:03 @agent_ppo2.py:121][0m #------------------------ Iteration 407 --------------------------#
[32m[20221213 21:04:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:03 @agent_ppo2.py:185][0m |           0.0012 |          83.7957 |          18.6095 |
[32m[20221213 21:04:03 @agent_ppo2.py:185][0m |          -0.0058 |          82.0792 |          18.5823 |
[32m[20221213 21:04:03 @agent_ppo2.py:185][0m |          -0.0075 |          81.8684 |          18.5859 |
[32m[20221213 21:04:04 @agent_ppo2.py:185][0m |          -0.0045 |          81.7088 |          18.5809 |
[32m[20221213 21:04:04 @agent_ppo2.py:185][0m |           0.0016 |          86.3623 |          18.5724 |
[32m[20221213 21:04:04 @agent_ppo2.py:185][0m |          -0.0092 |          81.4452 |          18.5643 |
[32m[20221213 21:04:04 @agent_ppo2.py:185][0m |          -0.0062 |          81.2197 |          18.5742 |
[32m[20221213 21:04:04 @agent_ppo2.py:185][0m |          -0.0062 |          81.3118 |          18.5671 |
[32m[20221213 21:04:04 @agent_ppo2.py:185][0m |          -0.0110 |          81.1309 |          18.5723 |
[32m[20221213 21:04:04 @agent_ppo2.py:185][0m |          -0.0057 |          81.6404 |          18.5685 |
[32m[20221213 21:04:04 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:04:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.40
[32m[20221213 21:04:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 572.00
[32m[20221213 21:04:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 597.00
[32m[20221213 21:04:04 @agent_ppo2.py:143][0m Total time:       8.49 min
[32m[20221213 21:04:04 @agent_ppo2.py:145][0m 835584 total steps have happened
[32m[20221213 21:04:04 @agent_ppo2.py:121][0m #------------------------ Iteration 408 --------------------------#
[32m[20221213 21:04:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:05 @agent_ppo2.py:185][0m |          -0.0035 |          82.0601 |          18.6269 |
[32m[20221213 21:04:05 @agent_ppo2.py:185][0m |          -0.0019 |          82.1178 |          18.6009 |
[32m[20221213 21:04:05 @agent_ppo2.py:185][0m |          -0.0087 |          81.1063 |          18.5997 |
[32m[20221213 21:04:05 @agent_ppo2.py:185][0m |          -0.0099 |          80.8382 |          18.5964 |
[32m[20221213 21:04:05 @agent_ppo2.py:185][0m |          -0.0104 |          80.6317 |          18.5904 |
[32m[20221213 21:04:05 @agent_ppo2.py:185][0m |           0.0013 |          86.6067 |          18.5915 |
[32m[20221213 21:04:05 @agent_ppo2.py:185][0m |          -0.0089 |          80.3073 |          18.5847 |
[32m[20221213 21:04:05 @agent_ppo2.py:185][0m |          -0.0057 |          80.3838 |          18.5944 |
[32m[20221213 21:04:05 @agent_ppo2.py:185][0m |          -0.0029 |          81.1459 |          18.5879 |
[32m[20221213 21:04:05 @agent_ppo2.py:185][0m |          -0.0086 |          80.0648 |          18.5931 |
[32m[20221213 21:04:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:04:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.60
[32m[20221213 21:04:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 577.00
[32m[20221213 21:04:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.00
[32m[20221213 21:04:05 @agent_ppo2.py:143][0m Total time:       8.51 min
[32m[20221213 21:04:05 @agent_ppo2.py:145][0m 837632 total steps have happened
[32m[20221213 21:04:05 @agent_ppo2.py:121][0m #------------------------ Iteration 409 --------------------------#
[32m[20221213 21:04:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:06 @agent_ppo2.py:185][0m |          -0.0047 |          83.2864 |          18.6763 |
[32m[20221213 21:04:06 @agent_ppo2.py:185][0m |          -0.0061 |          82.7353 |          18.6714 |
[32m[20221213 21:04:06 @agent_ppo2.py:185][0m |          -0.0017 |          83.3116 |          18.6676 |
[32m[20221213 21:04:06 @agent_ppo2.py:185][0m |          -0.0059 |          82.3255 |          18.6570 |
[32m[20221213 21:04:06 @agent_ppo2.py:185][0m |          -0.0045 |          82.0257 |          18.6579 |
[32m[20221213 21:04:06 @agent_ppo2.py:185][0m |           0.0074 |          91.4978 |          18.6521 |
[32m[20221213 21:04:06 @agent_ppo2.py:185][0m |          -0.0059 |          81.9090 |          18.6458 |
[32m[20221213 21:04:06 @agent_ppo2.py:185][0m |          -0.0064 |          81.7196 |          18.6466 |
[32m[20221213 21:04:06 @agent_ppo2.py:185][0m |          -0.0056 |          81.6191 |          18.6510 |
[32m[20221213 21:04:07 @agent_ppo2.py:185][0m |          -0.0050 |          82.5930 |          18.6469 |
[32m[20221213 21:04:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:04:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.80
[32m[20221213 21:04:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.00
[32m[20221213 21:04:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.00
[32m[20221213 21:04:07 @agent_ppo2.py:143][0m Total time:       8.53 min
[32m[20221213 21:04:07 @agent_ppo2.py:145][0m 839680 total steps have happened
[32m[20221213 21:04:07 @agent_ppo2.py:121][0m #------------------------ Iteration 410 --------------------------#
[32m[20221213 21:04:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:07 @agent_ppo2.py:185][0m |          -0.0012 |          82.1510 |          18.7248 |
[32m[20221213 21:04:07 @agent_ppo2.py:185][0m |          -0.0050 |          81.4876 |          18.7111 |
[32m[20221213 21:04:07 @agent_ppo2.py:185][0m |          -0.0066 |          81.1405 |          18.6920 |
[32m[20221213 21:04:07 @agent_ppo2.py:185][0m |          -0.0082 |          80.9722 |          18.6935 |
[32m[20221213 21:04:07 @agent_ppo2.py:185][0m |          -0.0070 |          80.8615 |          18.7031 |
[32m[20221213 21:04:07 @agent_ppo2.py:185][0m |          -0.0090 |          80.8518 |          18.6944 |
[32m[20221213 21:04:07 @agent_ppo2.py:185][0m |          -0.0001 |          83.4757 |          18.6931 |
[32m[20221213 21:04:08 @agent_ppo2.py:185][0m |          -0.0036 |          82.3550 |          18.6916 |
[32m[20221213 21:04:08 @agent_ppo2.py:185][0m |          -0.0085 |          80.4362 |          18.6848 |
[32m[20221213 21:04:08 @agent_ppo2.py:185][0m |          -0.0062 |          80.4331 |          18.6956 |
[32m[20221213 21:04:08 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:04:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.80
[32m[20221213 21:04:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 577.00
[32m[20221213 21:04:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 556.00
[32m[20221213 21:04:08 @agent_ppo2.py:143][0m Total time:       8.55 min
[32m[20221213 21:04:08 @agent_ppo2.py:145][0m 841728 total steps have happened
[32m[20221213 21:04:08 @agent_ppo2.py:121][0m #------------------------ Iteration 411 --------------------------#
[32m[20221213 21:04:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:08 @agent_ppo2.py:185][0m |           0.0022 |          82.6918 |          18.7375 |
[32m[20221213 21:04:08 @agent_ppo2.py:185][0m |          -0.0021 |          82.1288 |          18.7261 |
[32m[20221213 21:04:08 @agent_ppo2.py:185][0m |          -0.0047 |          81.8578 |          18.7193 |
[32m[20221213 21:04:08 @agent_ppo2.py:185][0m |          -0.0038 |          81.5450 |          18.7052 |
[32m[20221213 21:04:09 @agent_ppo2.py:185][0m |          -0.0052 |          81.5172 |          18.7021 |
[32m[20221213 21:04:09 @agent_ppo2.py:185][0m |          -0.0066 |          81.2396 |          18.6922 |
[32m[20221213 21:04:09 @agent_ppo2.py:185][0m |          -0.0063 |          81.0012 |          18.6895 |
[32m[20221213 21:04:09 @agent_ppo2.py:185][0m |          -0.0065 |          80.8978 |          18.6939 |
[32m[20221213 21:04:09 @agent_ppo2.py:185][0m |          -0.0063 |          80.7199 |          18.6915 |
[32m[20221213 21:04:09 @agent_ppo2.py:185][0m |          -0.0051 |          80.7006 |          18.6893 |
[32m[20221213 21:04:09 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:04:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 545.40
[32m[20221213 21:04:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.00
[32m[20221213 21:04:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 528.00
[32m[20221213 21:04:09 @agent_ppo2.py:143][0m Total time:       8.57 min
[32m[20221213 21:04:09 @agent_ppo2.py:145][0m 843776 total steps have happened
[32m[20221213 21:04:09 @agent_ppo2.py:121][0m #------------------------ Iteration 412 --------------------------#
[32m[20221213 21:04:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:09 @agent_ppo2.py:185][0m |          -0.0011 |          85.8579 |          18.8036 |
[32m[20221213 21:04:10 @agent_ppo2.py:185][0m |          -0.0042 |          84.6346 |          18.7774 |
[32m[20221213 21:04:10 @agent_ppo2.py:185][0m |          -0.0061 |          83.7039 |          18.7701 |
[32m[20221213 21:04:10 @agent_ppo2.py:185][0m |          -0.0061 |          83.2257 |          18.7696 |
[32m[20221213 21:04:10 @agent_ppo2.py:185][0m |          -0.0047 |          83.2738 |          18.7641 |
[32m[20221213 21:04:10 @agent_ppo2.py:185][0m |          -0.0016 |          83.9302 |          18.7785 |
[32m[20221213 21:04:10 @agent_ppo2.py:185][0m |          -0.0071 |          82.2285 |          18.7603 |
[32m[20221213 21:04:10 @agent_ppo2.py:185][0m |          -0.0062 |          81.8960 |          18.7715 |
[32m[20221213 21:04:10 @agent_ppo2.py:185][0m |          -0.0094 |          81.6197 |          18.7612 |
[32m[20221213 21:04:10 @agent_ppo2.py:185][0m |          -0.0049 |          81.6826 |          18.7750 |
[32m[20221213 21:04:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:04:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.80
[32m[20221213 21:04:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.00
[32m[20221213 21:04:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 577.00
[32m[20221213 21:04:10 @agent_ppo2.py:143][0m Total time:       8.59 min
[32m[20221213 21:04:10 @agent_ppo2.py:145][0m 845824 total steps have happened
[32m[20221213 21:04:10 @agent_ppo2.py:121][0m #------------------------ Iteration 413 --------------------------#
[32m[20221213 21:04:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:04:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:11 @agent_ppo2.py:185][0m |           0.0132 |          96.0122 |          18.6633 |
[32m[20221213 21:04:11 @agent_ppo2.py:185][0m |          -0.0066 |          84.3320 |          18.6487 |
[32m[20221213 21:04:11 @agent_ppo2.py:185][0m |          -0.0040 |          83.6565 |          18.6402 |
[32m[20221213 21:04:11 @agent_ppo2.py:185][0m |          -0.0074 |          83.2780 |          18.6387 |
[32m[20221213 21:04:11 @agent_ppo2.py:185][0m |           0.0037 |          94.2121 |          18.6396 |
[32m[20221213 21:04:11 @agent_ppo2.py:185][0m |          -0.0040 |          82.8733 |          18.6285 |
[32m[20221213 21:04:11 @agent_ppo2.py:185][0m |          -0.0048 |          82.6113 |          18.6226 |
[32m[20221213 21:04:11 @agent_ppo2.py:185][0m |          -0.0052 |          82.4670 |          18.6388 |
[32m[20221213 21:04:11 @agent_ppo2.py:185][0m |          -0.0067 |          82.3437 |          18.6298 |
[32m[20221213 21:04:11 @agent_ppo2.py:185][0m |          -0.0059 |          82.3718 |          18.6369 |
[32m[20221213 21:04:11 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:04:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.00
[32m[20221213 21:04:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 577.00
[32m[20221213 21:04:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.00
[32m[20221213 21:04:12 @agent_ppo2.py:143][0m Total time:       8.61 min
[32m[20221213 21:04:12 @agent_ppo2.py:145][0m 847872 total steps have happened
[32m[20221213 21:04:12 @agent_ppo2.py:121][0m #------------------------ Iteration 414 --------------------------#
[32m[20221213 21:04:12 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:04:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:12 @agent_ppo2.py:185][0m |           0.0049 |          82.9534 |          18.5986 |
[32m[20221213 21:04:12 @agent_ppo2.py:185][0m |          -0.0034 |          80.6836 |          18.5993 |
[32m[20221213 21:04:12 @agent_ppo2.py:185][0m |          -0.0027 |          81.2028 |          18.5920 |
[32m[20221213 21:04:12 @agent_ppo2.py:185][0m |          -0.0071 |          79.8936 |          18.5962 |
[32m[20221213 21:04:12 @agent_ppo2.py:185][0m |          -0.0041 |          79.6444 |          18.6014 |
[32m[20221213 21:04:12 @agent_ppo2.py:185][0m |           0.0010 |          80.7063 |          18.5959 |
[32m[20221213 21:04:12 @agent_ppo2.py:185][0m |          -0.0047 |          79.2510 |          18.5996 |
[32m[20221213 21:04:13 @agent_ppo2.py:185][0m |          -0.0067 |          79.0996 |          18.5907 |
[32m[20221213 21:04:13 @agent_ppo2.py:185][0m |          -0.0072 |          78.9501 |          18.5990 |
[32m[20221213 21:04:13 @agent_ppo2.py:185][0m |          -0.0057 |          79.2818 |          18.5993 |
[32m[20221213 21:04:13 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:04:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.20
[32m[20221213 21:04:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.00
[32m[20221213 21:04:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 602.00
[32m[20221213 21:04:13 @agent_ppo2.py:143][0m Total time:       8.64 min
[32m[20221213 21:04:13 @agent_ppo2.py:145][0m 849920 total steps have happened
[32m[20221213 21:04:13 @agent_ppo2.py:121][0m #------------------------ Iteration 415 --------------------------#
[32m[20221213 21:04:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:13 @agent_ppo2.py:185][0m |          -0.0006 |          84.2247 |          18.6184 |
[32m[20221213 21:04:13 @agent_ppo2.py:185][0m |          -0.0048 |          83.5274 |          18.5840 |
[32m[20221213 21:04:13 @agent_ppo2.py:185][0m |          -0.0079 |          83.2203 |          18.5799 |
[32m[20221213 21:04:13 @agent_ppo2.py:185][0m |           0.0026 |          88.8810 |          18.5792 |
[32m[20221213 21:04:14 @agent_ppo2.py:185][0m |          -0.0105 |          82.8206 |          18.5701 |
[32m[20221213 21:04:14 @agent_ppo2.py:185][0m |          -0.0070 |          82.7049 |          18.5611 |
[32m[20221213 21:04:14 @agent_ppo2.py:185][0m |          -0.0079 |          82.3529 |          18.5677 |
[32m[20221213 21:04:14 @agent_ppo2.py:185][0m |          -0.0101 |          82.2521 |          18.5713 |
[32m[20221213 21:04:14 @agent_ppo2.py:185][0m |          -0.0070 |          82.2690 |          18.5692 |
[32m[20221213 21:04:14 @agent_ppo2.py:185][0m |          -0.0115 |          82.0538 |          18.5728 |
[32m[20221213 21:04:14 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:04:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 573.20
[32m[20221213 21:04:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.00
[32m[20221213 21:04:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:04:14 @agent_ppo2.py:143][0m Total time:       8.66 min
[32m[20221213 21:04:14 @agent_ppo2.py:145][0m 851968 total steps have happened
[32m[20221213 21:04:14 @agent_ppo2.py:121][0m #------------------------ Iteration 416 --------------------------#
[32m[20221213 21:04:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:04:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:14 @agent_ppo2.py:185][0m |          -0.0024 |          83.5274 |          18.6238 |
[32m[20221213 21:04:14 @agent_ppo2.py:185][0m |          -0.0042 |          82.9009 |          18.6120 |
[32m[20221213 21:04:15 @agent_ppo2.py:185][0m |          -0.0082 |          82.8517 |          18.5948 |
[32m[20221213 21:04:15 @agent_ppo2.py:185][0m |           0.0093 |          93.0487 |          18.5995 |
[32m[20221213 21:04:15 @agent_ppo2.py:185][0m |          -0.0100 |          82.6206 |          18.5998 |
[32m[20221213 21:04:15 @agent_ppo2.py:185][0m |          -0.0074 |          82.4010 |          18.6017 |
[32m[20221213 21:04:15 @agent_ppo2.py:185][0m |          -0.0079 |          82.0506 |          18.5953 |
[32m[20221213 21:04:15 @agent_ppo2.py:185][0m |          -0.0083 |          81.9315 |          18.6010 |
[32m[20221213 21:04:15 @agent_ppo2.py:185][0m |          -0.0071 |          81.8778 |          18.6000 |
[32m[20221213 21:04:15 @agent_ppo2.py:185][0m |          -0.0103 |          81.7441 |          18.6092 |
[32m[20221213 21:04:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:04:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.80
[32m[20221213 21:04:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 566.00
[32m[20221213 21:04:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.00
[32m[20221213 21:04:15 @agent_ppo2.py:143][0m Total time:       8.68 min
[32m[20221213 21:04:15 @agent_ppo2.py:145][0m 854016 total steps have happened
[32m[20221213 21:04:15 @agent_ppo2.py:121][0m #------------------------ Iteration 417 --------------------------#
[32m[20221213 21:04:15 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:04:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:16 @agent_ppo2.py:185][0m |           0.0006 |          81.4329 |          18.6674 |
[32m[20221213 21:04:16 @agent_ppo2.py:185][0m |           0.0035 |          82.2310 |          18.6389 |
[32m[20221213 21:04:16 @agent_ppo2.py:185][0m |          -0.0043 |          80.7804 |          18.6365 |
[32m[20221213 21:04:16 @agent_ppo2.py:185][0m |          -0.0019 |          80.8325 |          18.6342 |
[32m[20221213 21:04:16 @agent_ppo2.py:185][0m |          -0.0069 |          80.4497 |          18.6270 |
[32m[20221213 21:04:16 @agent_ppo2.py:185][0m |           0.0000 |          83.2472 |          18.6318 |
[32m[20221213 21:04:16 @agent_ppo2.py:185][0m |          -0.0056 |          80.2116 |          18.6298 |
[32m[20221213 21:04:16 @agent_ppo2.py:185][0m |          -0.0089 |          80.1841 |          18.6199 |
[32m[20221213 21:04:16 @agent_ppo2.py:185][0m |          -0.0061 |          80.0353 |          18.6174 |
[32m[20221213 21:04:16 @agent_ppo2.py:185][0m |          -0.0049 |          80.3929 |          18.6304 |
[32m[20221213 21:04:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:04:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 524.60
[32m[20221213 21:04:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.00
[32m[20221213 21:04:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.00
[32m[20221213 21:04:16 @agent_ppo2.py:143][0m Total time:       8.70 min
[32m[20221213 21:04:16 @agent_ppo2.py:145][0m 856064 total steps have happened
[32m[20221213 21:04:16 @agent_ppo2.py:121][0m #------------------------ Iteration 418 --------------------------#
[32m[20221213 21:04:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:17 @agent_ppo2.py:185][0m |          -0.0017 |          81.1355 |          18.6853 |
[32m[20221213 21:04:17 @agent_ppo2.py:185][0m |          -0.0026 |          80.7268 |          18.6598 |
[32m[20221213 21:04:17 @agent_ppo2.py:185][0m |          -0.0041 |          80.3961 |          18.6582 |
[32m[20221213 21:04:17 @agent_ppo2.py:185][0m |          -0.0021 |          80.0938 |          18.6520 |
[32m[20221213 21:04:17 @agent_ppo2.py:185][0m |          -0.0053 |          79.7206 |          18.6417 |
[32m[20221213 21:04:17 @agent_ppo2.py:185][0m |          -0.0042 |          79.4238 |          18.6553 |
[32m[20221213 21:04:17 @agent_ppo2.py:185][0m |          -0.0032 |          79.1513 |          18.6458 |
[32m[20221213 21:04:17 @agent_ppo2.py:185][0m |          -0.0059 |          78.9324 |          18.6484 |
[32m[20221213 21:04:18 @agent_ppo2.py:185][0m |           0.0004 |          80.5014 |          18.6374 |
[32m[20221213 21:04:18 @agent_ppo2.py:185][0m |           0.0036 |          85.6228 |          18.6482 |
[32m[20221213 21:04:18 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:04:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.60
[32m[20221213 21:04:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 575.00
[32m[20221213 21:04:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.00
[32m[20221213 21:04:18 @agent_ppo2.py:143][0m Total time:       8.72 min
[32m[20221213 21:04:18 @agent_ppo2.py:145][0m 858112 total steps have happened
[32m[20221213 21:04:18 @agent_ppo2.py:121][0m #------------------------ Iteration 419 --------------------------#
[32m[20221213 21:04:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:18 @agent_ppo2.py:185][0m |          -0.0010 |          84.6077 |          18.6292 |
[32m[20221213 21:04:18 @agent_ppo2.py:185][0m |          -0.0055 |          82.6073 |          18.6213 |
[32m[20221213 21:04:18 @agent_ppo2.py:185][0m |           0.0054 |          92.3559 |          18.6156 |
[32m[20221213 21:04:18 @agent_ppo2.py:185][0m |          -0.0079 |          81.6825 |          18.5831 |
[32m[20221213 21:04:18 @agent_ppo2.py:185][0m |          -0.0001 |          82.5410 |          18.6096 |
[32m[20221213 21:04:18 @agent_ppo2.py:185][0m |           0.0008 |          84.8331 |          18.6126 |
[32m[20221213 21:04:19 @agent_ppo2.py:185][0m |          -0.0070 |          80.9852 |          18.6033 |
[32m[20221213 21:04:19 @agent_ppo2.py:185][0m |          -0.0058 |          80.7033 |          18.6070 |
[32m[20221213 21:04:19 @agent_ppo2.py:185][0m |          -0.0067 |          80.5846 |          18.6045 |
[32m[20221213 21:04:19 @agent_ppo2.py:185][0m |          -0.0057 |          80.5026 |          18.6098 |
[32m[20221213 21:04:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:04:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.80
[32m[20221213 21:04:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 595.00
[32m[20221213 21:04:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 590.00
[32m[20221213 21:04:19 @agent_ppo2.py:143][0m Total time:       8.74 min
[32m[20221213 21:04:19 @agent_ppo2.py:145][0m 860160 total steps have happened
[32m[20221213 21:04:19 @agent_ppo2.py:121][0m #------------------------ Iteration 420 --------------------------#
[32m[20221213 21:04:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:19 @agent_ppo2.py:185][0m |          -0.0007 |          84.4905 |          18.6157 |
[32m[20221213 21:04:19 @agent_ppo2.py:185][0m |          -0.0052 |          83.4353 |          18.6035 |
[32m[20221213 21:04:19 @agent_ppo2.py:185][0m |          -0.0060 |          82.9767 |          18.5941 |
[32m[20221213 21:04:20 @agent_ppo2.py:185][0m |          -0.0068 |          82.6619 |          18.5803 |
[32m[20221213 21:04:20 @agent_ppo2.py:185][0m |          -0.0046 |          82.6287 |          18.5859 |
[32m[20221213 21:04:20 @agent_ppo2.py:185][0m |          -0.0075 |          82.3805 |          18.5918 |
[32m[20221213 21:04:20 @agent_ppo2.py:185][0m |          -0.0063 |          82.3867 |          18.5855 |
[32m[20221213 21:04:20 @agent_ppo2.py:185][0m |          -0.0081 |          82.1819 |          18.6011 |
[32m[20221213 21:04:20 @agent_ppo2.py:185][0m |          -0.0021 |          83.0309 |          18.5885 |
[32m[20221213 21:04:20 @agent_ppo2.py:185][0m |          -0.0038 |          82.3015 |          18.5920 |
[32m[20221213 21:04:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:04:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.00
[32m[20221213 21:04:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 583.00
[32m[20221213 21:04:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 601.00
[32m[20221213 21:04:20 @agent_ppo2.py:143][0m Total time:       8.76 min
[32m[20221213 21:04:20 @agent_ppo2.py:145][0m 862208 total steps have happened
[32m[20221213 21:04:20 @agent_ppo2.py:121][0m #------------------------ Iteration 421 --------------------------#
[32m[20221213 21:04:20 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:04:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:20 @agent_ppo2.py:185][0m |          -0.0027 |          83.4155 |          18.6819 |
[32m[20221213 21:04:21 @agent_ppo2.py:185][0m |          -0.0045 |          83.0184 |          18.6685 |
[32m[20221213 21:04:21 @agent_ppo2.py:185][0m |          -0.0033 |          82.9803 |          18.6603 |
[32m[20221213 21:04:21 @agent_ppo2.py:185][0m |          -0.0060 |          82.5057 |          18.6667 |
[32m[20221213 21:04:21 @agent_ppo2.py:185][0m |          -0.0059 |          82.4283 |          18.6686 |
[32m[20221213 21:04:21 @agent_ppo2.py:185][0m |          -0.0068 |          82.2060 |          18.6636 |
[32m[20221213 21:04:21 @agent_ppo2.py:185][0m |           0.0040 |          92.3502 |          18.6623 |
[32m[20221213 21:04:21 @agent_ppo2.py:185][0m |          -0.0094 |          82.1197 |          18.6682 |
[32m[20221213 21:04:21 @agent_ppo2.py:185][0m |          -0.0063 |          82.0899 |          18.6703 |
[32m[20221213 21:04:21 @agent_ppo2.py:185][0m |          -0.0014 |          82.7755 |          18.6611 |
[32m[20221213 21:04:21 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:04:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.20
[32m[20221213 21:04:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.00
[32m[20221213 21:04:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 579.00
[32m[20221213 21:04:21 @agent_ppo2.py:143][0m Total time:       8.78 min
[32m[20221213 21:04:21 @agent_ppo2.py:145][0m 864256 total steps have happened
[32m[20221213 21:04:21 @agent_ppo2.py:121][0m #------------------------ Iteration 422 --------------------------#
[32m[20221213 21:04:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:22 @agent_ppo2.py:185][0m |          -0.0022 |          82.7370 |          18.7068 |
[32m[20221213 21:04:22 @agent_ppo2.py:185][0m |           0.0003 |          84.2666 |          18.6914 |
[32m[20221213 21:04:22 @agent_ppo2.py:185][0m |          -0.0039 |          81.4397 |          18.6851 |
[32m[20221213 21:04:22 @agent_ppo2.py:185][0m |          -0.0014 |          83.3681 |          18.6862 |
[32m[20221213 21:04:22 @agent_ppo2.py:185][0m |          -0.0079 |          81.0368 |          18.6897 |
[32m[20221213 21:04:22 @agent_ppo2.py:185][0m |          -0.0070 |          80.8409 |          18.6844 |
[32m[20221213 21:04:22 @agent_ppo2.py:185][0m |           0.0120 |          95.4247 |          18.6827 |
[32m[20221213 21:04:22 @agent_ppo2.py:185][0m |           0.0051 |          89.1490 |          18.6569 |
[32m[20221213 21:04:22 @agent_ppo2.py:185][0m |          -0.0068 |          80.8211 |          18.6558 |
[32m[20221213 21:04:22 @agent_ppo2.py:185][0m |          -0.0057 |          80.8590 |          18.6809 |
[32m[20221213 21:04:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:04:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 559.40
[32m[20221213 21:04:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 21:04:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 617.00
[32m[20221213 21:04:23 @agent_ppo2.py:143][0m Total time:       8.80 min
[32m[20221213 21:04:23 @agent_ppo2.py:145][0m 866304 total steps have happened
[32m[20221213 21:04:23 @agent_ppo2.py:121][0m #------------------------ Iteration 423 --------------------------#
[32m[20221213 21:04:23 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:04:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:23 @agent_ppo2.py:185][0m |          -0.0006 |          84.3433 |          18.6230 |
[32m[20221213 21:04:23 @agent_ppo2.py:185][0m |          -0.0036 |          83.6115 |          18.6020 |
[32m[20221213 21:04:23 @agent_ppo2.py:185][0m |          -0.0044 |          83.1705 |          18.5899 |
[32m[20221213 21:04:23 @agent_ppo2.py:185][0m |          -0.0000 |          82.9865 |          18.5815 |
[32m[20221213 21:04:23 @agent_ppo2.py:185][0m |          -0.0069 |          82.7817 |          18.5886 |
[32m[20221213 21:04:23 @agent_ppo2.py:185][0m |          -0.0070 |          82.5946 |          18.5843 |
[32m[20221213 21:04:23 @agent_ppo2.py:185][0m |          -0.0045 |          82.3128 |          18.5895 |
[32m[20221213 21:04:23 @agent_ppo2.py:185][0m |          -0.0089 |          82.3692 |          18.5851 |
[32m[20221213 21:04:24 @agent_ppo2.py:185][0m |          -0.0082 |          82.2653 |          18.5869 |
[32m[20221213 21:04:24 @agent_ppo2.py:185][0m |          -0.0008 |          84.1538 |          18.5907 |
[32m[20221213 21:04:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:04:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.80
[32m[20221213 21:04:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.00
[32m[20221213 21:04:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.00
[32m[20221213 21:04:24 @agent_ppo2.py:143][0m Total time:       8.82 min
[32m[20221213 21:04:24 @agent_ppo2.py:145][0m 868352 total steps have happened
[32m[20221213 21:04:24 @agent_ppo2.py:121][0m #------------------------ Iteration 424 --------------------------#
[32m[20221213 21:04:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:24 @agent_ppo2.py:185][0m |          -0.0008 |          85.6681 |          18.6483 |
[32m[20221213 21:04:24 @agent_ppo2.py:185][0m |          -0.0045 |          85.1526 |          18.6247 |
[32m[20221213 21:04:24 @agent_ppo2.py:185][0m |           0.0005 |          87.5592 |          18.6181 |
[32m[20221213 21:04:24 @agent_ppo2.py:185][0m |          -0.0067 |          84.6690 |          18.6103 |
[32m[20221213 21:04:24 @agent_ppo2.py:185][0m |          -0.0071 |          84.5050 |          18.6267 |
[32m[20221213 21:04:24 @agent_ppo2.py:185][0m |           0.0051 |          92.3456 |          18.6215 |
[32m[20221213 21:04:25 @agent_ppo2.py:185][0m |          -0.0061 |          84.2022 |          18.6034 |
[32m[20221213 21:04:25 @agent_ppo2.py:185][0m |          -0.0031 |          85.2548 |          18.6095 |
[32m[20221213 21:04:25 @agent_ppo2.py:185][0m |          -0.0028 |          84.5369 |          18.6164 |
[32m[20221213 21:04:25 @agent_ppo2.py:185][0m |           0.0065 |          95.3913 |          18.6200 |
[32m[20221213 21:04:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:04:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 534.40
[32m[20221213 21:04:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.00
[32m[20221213 21:04:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 565.00
[32m[20221213 21:04:25 @agent_ppo2.py:143][0m Total time:       8.84 min
[32m[20221213 21:04:25 @agent_ppo2.py:145][0m 870400 total steps have happened
[32m[20221213 21:04:25 @agent_ppo2.py:121][0m #------------------------ Iteration 425 --------------------------#
[32m[20221213 21:04:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:25 @agent_ppo2.py:185][0m |           0.0036 |          89.8814 |          18.6518 |
[32m[20221213 21:04:25 @agent_ppo2.py:185][0m |           0.0025 |          87.6501 |          18.6322 |
[32m[20221213 21:04:25 @agent_ppo2.py:185][0m |           0.0012 |          86.2218 |          18.6258 |
[32m[20221213 21:04:26 @agent_ppo2.py:185][0m |          -0.0015 |          85.6366 |          18.6248 |
[32m[20221213 21:04:26 @agent_ppo2.py:185][0m |          -0.0058 |          85.0133 |          18.6214 |
[32m[20221213 21:04:26 @agent_ppo2.py:185][0m |          -0.0027 |          84.8958 |          18.6160 |
[32m[20221213 21:04:26 @agent_ppo2.py:185][0m |          -0.0065 |          84.6627 |          18.6221 |
[32m[20221213 21:04:26 @agent_ppo2.py:185][0m |          -0.0019 |          85.0349 |          18.6248 |
[32m[20221213 21:04:26 @agent_ppo2.py:185][0m |          -0.0088 |          84.4125 |          18.6193 |
[32m[20221213 21:04:26 @agent_ppo2.py:185][0m |          -0.0089 |          84.2315 |          18.6191 |
[32m[20221213 21:04:26 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:04:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.00
[32m[20221213 21:04:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.00
[32m[20221213 21:04:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.00
[32m[20221213 21:04:26 @agent_ppo2.py:143][0m Total time:       8.86 min
[32m[20221213 21:04:26 @agent_ppo2.py:145][0m 872448 total steps have happened
[32m[20221213 21:04:26 @agent_ppo2.py:121][0m #------------------------ Iteration 426 --------------------------#
[32m[20221213 21:04:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:04:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:26 @agent_ppo2.py:185][0m |           0.0088 |          85.1187 |          18.7845 |
[32m[20221213 21:04:27 @agent_ppo2.py:185][0m |           0.0010 |          82.3346 |          18.7736 |
[32m[20221213 21:04:27 @agent_ppo2.py:185][0m |           0.0084 |          88.6145 |          18.7659 |
[32m[20221213 21:04:27 @agent_ppo2.py:185][0m |          -0.0056 |          81.3153 |          18.7557 |
[32m[20221213 21:04:27 @agent_ppo2.py:185][0m |          -0.0069 |          81.1507 |          18.7631 |
[32m[20221213 21:04:27 @agent_ppo2.py:185][0m |          -0.0037 |          80.8788 |          18.7671 |
[32m[20221213 21:04:27 @agent_ppo2.py:185][0m |           0.0044 |          87.0354 |          18.7637 |
[32m[20221213 21:04:27 @agent_ppo2.py:185][0m |          -0.0061 |          80.9845 |          18.7665 |
[32m[20221213 21:04:27 @agent_ppo2.py:185][0m |          -0.0097 |          80.7038 |          18.7654 |
[32m[20221213 21:04:27 @agent_ppo2.py:185][0m |          -0.0070 |          80.7890 |          18.7609 |
[32m[20221213 21:04:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:04:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.00
[32m[20221213 21:04:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.00
[32m[20221213 21:04:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.00
[32m[20221213 21:04:27 @agent_ppo2.py:143][0m Total time:       8.88 min
[32m[20221213 21:04:27 @agent_ppo2.py:145][0m 874496 total steps have happened
[32m[20221213 21:04:27 @agent_ppo2.py:121][0m #------------------------ Iteration 427 --------------------------#
[32m[20221213 21:04:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:04:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:28 @agent_ppo2.py:185][0m |           0.0038 |          84.7629 |          18.5627 |
[32m[20221213 21:04:28 @agent_ppo2.py:185][0m |           0.0012 |          84.3763 |          18.5485 |
[32m[20221213 21:04:28 @agent_ppo2.py:185][0m |          -0.0013 |          84.1190 |          18.5422 |
[32m[20221213 21:04:28 @agent_ppo2.py:185][0m |          -0.0050 |          82.8176 |          18.5304 |
[32m[20221213 21:04:28 @agent_ppo2.py:185][0m |          -0.0045 |          82.5937 |          18.5265 |
[32m[20221213 21:04:28 @agent_ppo2.py:185][0m |          -0.0032 |          83.1277 |          18.5243 |
[32m[20221213 21:04:28 @agent_ppo2.py:185][0m |          -0.0050 |          82.1951 |          18.5219 |
[32m[20221213 21:04:28 @agent_ppo2.py:185][0m |          -0.0092 |          82.1678 |          18.5273 |
[32m[20221213 21:04:29 @agent_ppo2.py:185][0m |          -0.0032 |          81.9491 |          18.5216 |
[32m[20221213 21:04:29 @agent_ppo2.py:185][0m |          -0.0071 |          81.8621 |          18.5127 |
[32m[20221213 21:04:29 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 21:04:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 543.40
[32m[20221213 21:04:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.00
[32m[20221213 21:04:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 585.00
[32m[20221213 21:04:29 @agent_ppo2.py:143][0m Total time:       8.90 min
[32m[20221213 21:04:29 @agent_ppo2.py:145][0m 876544 total steps have happened
[32m[20221213 21:04:29 @agent_ppo2.py:121][0m #------------------------ Iteration 428 --------------------------#
[32m[20221213 21:04:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:29 @agent_ppo2.py:185][0m |          -0.0026 |          86.9274 |          18.4635 |
[32m[20221213 21:04:29 @agent_ppo2.py:185][0m |          -0.0060 |          86.1247 |          18.4590 |
[32m[20221213 21:04:29 @agent_ppo2.py:185][0m |          -0.0104 |          85.7987 |          18.4520 |
[32m[20221213 21:04:29 @agent_ppo2.py:185][0m |          -0.0114 |          85.4466 |          18.4437 |
[32m[20221213 21:04:30 @agent_ppo2.py:185][0m |          -0.0098 |          85.1198 |          18.4416 |
[32m[20221213 21:04:30 @agent_ppo2.py:185][0m |          -0.0106 |          84.9244 |          18.4365 |
[32m[20221213 21:04:30 @agent_ppo2.py:185][0m |          -0.0065 |          84.7506 |          18.4399 |
[32m[20221213 21:04:30 @agent_ppo2.py:185][0m |          -0.0115 |          84.6764 |          18.4455 |
[32m[20221213 21:04:30 @agent_ppo2.py:185][0m |          -0.0085 |          84.4776 |          18.4311 |
[32m[20221213 21:04:30 @agent_ppo2.py:185][0m |          -0.0116 |          84.4070 |          18.4297 |
[32m[20221213 21:04:30 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 21:04:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.80
[32m[20221213 21:04:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.00
[32m[20221213 21:04:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 547.00
[32m[20221213 21:04:30 @agent_ppo2.py:143][0m Total time:       8.92 min
[32m[20221213 21:04:30 @agent_ppo2.py:145][0m 878592 total steps have happened
[32m[20221213 21:04:30 @agent_ppo2.py:121][0m #------------------------ Iteration 429 --------------------------#
[32m[20221213 21:04:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:04:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:30 @agent_ppo2.py:185][0m |          -0.0020 |          85.1079 |          18.6541 |
[32m[20221213 21:04:31 @agent_ppo2.py:185][0m |          -0.0043 |          83.9886 |          18.6352 |
[32m[20221213 21:04:31 @agent_ppo2.py:185][0m |          -0.0078 |          83.6612 |          18.6383 |
[32m[20221213 21:04:31 @agent_ppo2.py:185][0m |          -0.0053 |          83.2541 |          18.6321 |
[32m[20221213 21:04:31 @agent_ppo2.py:185][0m |          -0.0074 |          82.9845 |          18.6317 |
[32m[20221213 21:04:31 @agent_ppo2.py:185][0m |          -0.0103 |          82.5917 |          18.6321 |
[32m[20221213 21:04:31 @agent_ppo2.py:185][0m |          -0.0044 |          82.8408 |          18.6353 |
[32m[20221213 21:04:31 @agent_ppo2.py:185][0m |          -0.0071 |          81.9364 |          18.6357 |
[32m[20221213 21:04:31 @agent_ppo2.py:185][0m |          -0.0015 |          84.2452 |          18.6419 |
[32m[20221213 21:04:31 @agent_ppo2.py:185][0m |          -0.0075 |          81.5602 |          18.6457 |
[32m[20221213 21:04:31 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:04:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.60
[32m[20221213 21:04:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.00
[32m[20221213 21:04:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.00
[32m[20221213 21:04:31 @agent_ppo2.py:143][0m Total time:       8.95 min
[32m[20221213 21:04:31 @agent_ppo2.py:145][0m 880640 total steps have happened
[32m[20221213 21:04:31 @agent_ppo2.py:121][0m #------------------------ Iteration 430 --------------------------#
[32m[20221213 21:04:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:32 @agent_ppo2.py:185][0m |          -0.0056 |          87.0756 |          18.7696 |
[32m[20221213 21:04:32 @agent_ppo2.py:185][0m |           0.0030 |          88.7944 |          18.7517 |
[32m[20221213 21:04:32 @agent_ppo2.py:185][0m |           0.0102 |          91.2588 |          18.7484 |
[32m[20221213 21:04:32 @agent_ppo2.py:185][0m |          -0.0029 |          85.6594 |          18.7400 |
[32m[20221213 21:04:32 @agent_ppo2.py:185][0m |          -0.0022 |          85.2530 |          18.7436 |
[32m[20221213 21:04:32 @agent_ppo2.py:185][0m |          -0.0070 |          83.5916 |          18.7347 |
[32m[20221213 21:04:32 @agent_ppo2.py:185][0m |          -0.0058 |          83.2687 |          18.7406 |
[32m[20221213 21:04:32 @agent_ppo2.py:185][0m |          -0.0105 |          82.9571 |          18.7437 |
[32m[20221213 21:04:32 @agent_ppo2.py:185][0m |          -0.0077 |          82.8302 |          18.7417 |
[32m[20221213 21:04:33 @agent_ppo2.py:185][0m |          -0.0057 |          82.4589 |          18.7452 |
[32m[20221213 21:04:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:04:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.00
[32m[20221213 21:04:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 562.00
[32m[20221213 21:04:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 630.00
[32m[20221213 21:04:33 @agent_ppo2.py:143][0m Total time:       8.97 min
[32m[20221213 21:04:33 @agent_ppo2.py:145][0m 882688 total steps have happened
[32m[20221213 21:04:33 @agent_ppo2.py:121][0m #------------------------ Iteration 431 --------------------------#
[32m[20221213 21:04:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:33 @agent_ppo2.py:185][0m |           0.0010 |          85.2932 |          18.6750 |
[32m[20221213 21:04:33 @agent_ppo2.py:185][0m |          -0.0023 |          83.9145 |          18.6757 |
[32m[20221213 21:04:33 @agent_ppo2.py:185][0m |          -0.0049 |          83.1201 |          18.6720 |
[32m[20221213 21:04:33 @agent_ppo2.py:185][0m |          -0.0045 |          82.7337 |          18.6684 |
[32m[20221213 21:04:33 @agent_ppo2.py:185][0m |           0.0007 |          85.4372 |          18.6607 |
[32m[20221213 21:04:33 @agent_ppo2.py:185][0m |          -0.0070 |          82.1887 |          18.6471 |
[32m[20221213 21:04:33 @agent_ppo2.py:185][0m |          -0.0083 |          82.0697 |          18.6589 |
[32m[20221213 21:04:34 @agent_ppo2.py:185][0m |          -0.0089 |          82.0028 |          18.6490 |
[32m[20221213 21:04:34 @agent_ppo2.py:185][0m |          -0.0068 |          81.5609 |          18.6492 |
[32m[20221213 21:04:34 @agent_ppo2.py:185][0m |          -0.0096 |          81.5931 |          18.6350 |
[32m[20221213 21:04:34 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:04:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.00
[32m[20221213 21:04:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.00
[32m[20221213 21:04:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.00
[32m[20221213 21:04:34 @agent_ppo2.py:143][0m Total time:       8.99 min
[32m[20221213 21:04:34 @agent_ppo2.py:145][0m 884736 total steps have happened
[32m[20221213 21:04:34 @agent_ppo2.py:121][0m #------------------------ Iteration 432 --------------------------#
[32m[20221213 21:04:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:34 @agent_ppo2.py:185][0m |           0.0002 |          83.3835 |          18.6264 |
[32m[20221213 21:04:34 @agent_ppo2.py:185][0m |          -0.0035 |          82.5704 |          18.6296 |
[32m[20221213 21:04:34 @agent_ppo2.py:185][0m |          -0.0054 |          82.1932 |          18.6173 |
[32m[20221213 21:04:34 @agent_ppo2.py:185][0m |          -0.0018 |          83.3790 |          18.6201 |
[32m[20221213 21:04:35 @agent_ppo2.py:185][0m |          -0.0094 |          81.9311 |          18.6137 |
[32m[20221213 21:04:35 @agent_ppo2.py:185][0m |          -0.0055 |          81.6594 |          18.6095 |
[32m[20221213 21:04:35 @agent_ppo2.py:185][0m |          -0.0020 |          83.7209 |          18.6035 |
[32m[20221213 21:04:35 @agent_ppo2.py:185][0m |          -0.0054 |          81.3632 |          18.6027 |
[32m[20221213 21:04:35 @agent_ppo2.py:185][0m |           0.0030 |          90.0752 |          18.6030 |
[32m[20221213 21:04:35 @agent_ppo2.py:185][0m |          -0.0099 |          81.4473 |          18.5956 |
[32m[20221213 21:04:35 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:04:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.20
[32m[20221213 21:04:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.00
[32m[20221213 21:04:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 559.00
[32m[20221213 21:04:35 @agent_ppo2.py:143][0m Total time:       9.01 min
[32m[20221213 21:04:35 @agent_ppo2.py:145][0m 886784 total steps have happened
[32m[20221213 21:04:35 @agent_ppo2.py:121][0m #------------------------ Iteration 433 --------------------------#
[32m[20221213 21:04:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:35 @agent_ppo2.py:185][0m |           0.0070 |          88.4026 |          18.7451 |
[32m[20221213 21:04:36 @agent_ppo2.py:185][0m |          -0.0045 |          82.8559 |          18.7274 |
[32m[20221213 21:04:36 @agent_ppo2.py:185][0m |          -0.0065 |          82.0211 |          18.7110 |
[32m[20221213 21:04:36 @agent_ppo2.py:185][0m |          -0.0053 |          81.6732 |          18.7217 |
[32m[20221213 21:04:36 @agent_ppo2.py:185][0m |          -0.0021 |          81.9222 |          18.7216 |
[32m[20221213 21:04:36 @agent_ppo2.py:185][0m |          -0.0019 |          82.4980 |          18.7183 |
[32m[20221213 21:04:36 @agent_ppo2.py:185][0m |          -0.0063 |          81.1409 |          18.7222 |
[32m[20221213 21:04:36 @agent_ppo2.py:185][0m |           0.0011 |          83.5726 |          18.7120 |
[32m[20221213 21:04:36 @agent_ppo2.py:185][0m |          -0.0064 |          80.9010 |          18.7067 |
[32m[20221213 21:04:36 @agent_ppo2.py:185][0m |          -0.0067 |          80.7461 |          18.7179 |
[32m[20221213 21:04:36 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:04:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 552.20
[32m[20221213 21:04:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.00
[32m[20221213 21:04:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 553.00
[32m[20221213 21:04:36 @agent_ppo2.py:143][0m Total time:       9.03 min
[32m[20221213 21:04:36 @agent_ppo2.py:145][0m 888832 total steps have happened
[32m[20221213 21:04:36 @agent_ppo2.py:121][0m #------------------------ Iteration 434 --------------------------#
[32m[20221213 21:04:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:37 @agent_ppo2.py:185][0m |          -0.0003 |          84.5341 |          18.5845 |
[32m[20221213 21:04:37 @agent_ppo2.py:185][0m |          -0.0052 |          83.6731 |          18.5659 |
[32m[20221213 21:04:37 @agent_ppo2.py:185][0m |          -0.0070 |          83.1251 |          18.5613 |
[32m[20221213 21:04:37 @agent_ppo2.py:185][0m |          -0.0078 |          82.8076 |          18.5494 |
[32m[20221213 21:04:37 @agent_ppo2.py:185][0m |          -0.0062 |          82.5757 |          18.5572 |
[32m[20221213 21:04:37 @agent_ppo2.py:185][0m |          -0.0086 |          82.2215 |          18.5509 |
[32m[20221213 21:04:37 @agent_ppo2.py:185][0m |          -0.0100 |          81.8829 |          18.5480 |
[32m[20221213 21:04:37 @agent_ppo2.py:185][0m |          -0.0100 |          81.5501 |          18.5588 |
[32m[20221213 21:04:37 @agent_ppo2.py:185][0m |           0.0084 |          90.2235 |          18.5552 |
[32m[20221213 21:04:38 @agent_ppo2.py:185][0m |          -0.0063 |          81.0015 |          18.5514 |
[32m[20221213 21:04:38 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:04:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.60
[32m[20221213 21:04:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 599.00
[32m[20221213 21:04:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 583.00
[32m[20221213 21:04:38 @agent_ppo2.py:143][0m Total time:       9.05 min
[32m[20221213 21:04:38 @agent_ppo2.py:145][0m 890880 total steps have happened
[32m[20221213 21:04:38 @agent_ppo2.py:121][0m #------------------------ Iteration 435 --------------------------#
[32m[20221213 21:04:38 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:04:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:38 @agent_ppo2.py:185][0m |          -0.0012 |          87.7871 |          18.7575 |
[32m[20221213 21:04:38 @agent_ppo2.py:185][0m |          -0.0035 |          87.7701 |          18.7384 |
[32m[20221213 21:04:38 @agent_ppo2.py:185][0m |          -0.0065 |          86.7423 |          18.7277 |
[32m[20221213 21:04:38 @agent_ppo2.py:185][0m |          -0.0070 |          86.4124 |          18.7309 |
[32m[20221213 21:04:38 @agent_ppo2.py:185][0m |          -0.0068 |          86.2942 |          18.7250 |
[32m[20221213 21:04:39 @agent_ppo2.py:185][0m |          -0.0031 |          86.9253 |          18.7175 |
[32m[20221213 21:04:39 @agent_ppo2.py:185][0m |          -0.0073 |          85.6194 |          18.7226 |
[32m[20221213 21:04:39 @agent_ppo2.py:185][0m |          -0.0072 |          85.5477 |          18.7104 |
[32m[20221213 21:04:39 @agent_ppo2.py:185][0m |          -0.0075 |          85.3814 |          18.7143 |
[32m[20221213 21:04:39 @agent_ppo2.py:185][0m |          -0.0036 |          86.1013 |          18.7152 |
[32m[20221213 21:04:39 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:04:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.40
[32m[20221213 21:04:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 580.00
[32m[20221213 21:04:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 544.00
[32m[20221213 21:04:39 @agent_ppo2.py:143][0m Total time:       9.07 min
[32m[20221213 21:04:39 @agent_ppo2.py:145][0m 892928 total steps have happened
[32m[20221213 21:04:39 @agent_ppo2.py:121][0m #------------------------ Iteration 436 --------------------------#
[32m[20221213 21:04:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:39 @agent_ppo2.py:185][0m |          -0.0006 |          87.4787 |          18.7331 |
[32m[20221213 21:04:40 @agent_ppo2.py:185][0m |          -0.0044 |          86.5604 |          18.7206 |
[32m[20221213 21:04:40 @agent_ppo2.py:185][0m |          -0.0057 |          86.1771 |          18.7080 |
[32m[20221213 21:04:40 @agent_ppo2.py:185][0m |           0.0013 |          92.2525 |          18.7079 |
[32m[20221213 21:04:40 @agent_ppo2.py:185][0m |          -0.0065 |          85.6017 |          18.6963 |
[32m[20221213 21:04:40 @agent_ppo2.py:185][0m |           0.0009 |          87.4418 |          18.7042 |
[32m[20221213 21:04:40 @agent_ppo2.py:185][0m |          -0.0079 |          85.1822 |          18.6942 |
[32m[20221213 21:04:41 @agent_ppo2.py:185][0m |          -0.0052 |          85.1933 |          18.7033 |
[32m[20221213 21:04:41 @agent_ppo2.py:185][0m |          -0.0078 |          84.7655 |          18.6920 |
[32m[20221213 21:04:41 @agent_ppo2.py:185][0m |          -0.0045 |          85.9508 |          18.6888 |
[32m[20221213 21:04:41 @agent_ppo2.py:130][0m Policy update time: 1.55 s
[32m[20221213 21:04:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 539.20
[32m[20221213 21:04:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.00
[32m[20221213 21:04:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.00
[32m[20221213 21:04:41 @agent_ppo2.py:143][0m Total time:       9.10 min
[32m[20221213 21:04:41 @agent_ppo2.py:145][0m 894976 total steps have happened
[32m[20221213 21:04:41 @agent_ppo2.py:121][0m #------------------------ Iteration 437 --------------------------#
[32m[20221213 21:04:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:04:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:41 @agent_ppo2.py:185][0m |          -0.0013 |          83.9534 |          18.8012 |
[32m[20221213 21:04:41 @agent_ppo2.py:185][0m |          -0.0033 |          83.2516 |          18.7961 |
[32m[20221213 21:04:41 @agent_ppo2.py:185][0m |          -0.0036 |          83.0459 |          18.7934 |
[32m[20221213 21:04:42 @agent_ppo2.py:185][0m |          -0.0046 |          82.8110 |          18.7947 |
[32m[20221213 21:04:42 @agent_ppo2.py:185][0m |          -0.0081 |          82.6913 |          18.7903 |
[32m[20221213 21:04:42 @agent_ppo2.py:185][0m |          -0.0053 |          82.5516 |          18.7898 |
[32m[20221213 21:04:42 @agent_ppo2.py:185][0m |          -0.0069 |          82.3895 |          18.7862 |
[32m[20221213 21:04:42 @agent_ppo2.py:185][0m |          -0.0048 |          82.5203 |          18.7866 |
[32m[20221213 21:04:42 @agent_ppo2.py:185][0m |          -0.0002 |          83.7364 |          18.7842 |
[32m[20221213 21:04:42 @agent_ppo2.py:185][0m |          -0.0056 |          82.3980 |          18.7880 |
[32m[20221213 21:04:42 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:04:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 559.00
[32m[20221213 21:04:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 582.00
[32m[20221213 21:04:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 608.00
[32m[20221213 21:04:42 @agent_ppo2.py:143][0m Total time:       9.13 min
[32m[20221213 21:04:42 @agent_ppo2.py:145][0m 897024 total steps have happened
[32m[20221213 21:04:42 @agent_ppo2.py:121][0m #------------------------ Iteration 438 --------------------------#
[32m[20221213 21:04:42 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:04:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:43 @agent_ppo2.py:185][0m |          -0.0019 |          85.7353 |          18.7547 |
[32m[20221213 21:04:43 @agent_ppo2.py:185][0m |          -0.0062 |          85.1593 |          18.7337 |
[32m[20221213 21:04:43 @agent_ppo2.py:185][0m |          -0.0042 |          84.8849 |          18.7163 |
[32m[20221213 21:04:43 @agent_ppo2.py:185][0m |          -0.0046 |          84.6209 |          18.7198 |
[32m[20221213 21:04:43 @agent_ppo2.py:185][0m |          -0.0050 |          84.5251 |          18.7172 |
[32m[20221213 21:04:43 @agent_ppo2.py:185][0m |          -0.0068 |          84.3752 |          18.7106 |
[32m[20221213 21:04:43 @agent_ppo2.py:185][0m |          -0.0036 |          84.4273 |          18.7054 |
[32m[20221213 21:04:43 @agent_ppo2.py:185][0m |          -0.0091 |          84.2563 |          18.7035 |
[32m[20221213 21:04:44 @agent_ppo2.py:185][0m |          -0.0014 |          85.6747 |          18.7030 |
[32m[20221213 21:04:44 @agent_ppo2.py:185][0m |          -0.0061 |          83.9952 |          18.7123 |
[32m[20221213 21:04:44 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:04:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.20
[32m[20221213 21:04:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 600.00
[32m[20221213 21:04:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.00
[32m[20221213 21:04:44 @agent_ppo2.py:143][0m Total time:       9.15 min
[32m[20221213 21:04:44 @agent_ppo2.py:145][0m 899072 total steps have happened
[32m[20221213 21:04:44 @agent_ppo2.py:121][0m #------------------------ Iteration 439 --------------------------#
[32m[20221213 21:04:44 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 21:04:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:44 @agent_ppo2.py:185][0m |          -0.0027 |          89.2102 |          18.7271 |
[32m[20221213 21:04:44 @agent_ppo2.py:185][0m |          -0.0041 |          89.2275 |          18.7140 |
[32m[20221213 21:04:44 @agent_ppo2.py:185][0m |          -0.0049 |          88.5384 |          18.7118 |
[32m[20221213 21:04:44 @agent_ppo2.py:185][0m |          -0.0073 |          88.1493 |          18.7138 |
[32m[20221213 21:04:45 @agent_ppo2.py:185][0m |          -0.0067 |          88.4556 |          18.7161 |
[32m[20221213 21:04:45 @agent_ppo2.py:185][0m |          -0.0084 |          87.8514 |          18.7062 |
[32m[20221213 21:04:45 @agent_ppo2.py:185][0m |          -0.0074 |          87.9109 |          18.7119 |
[32m[20221213 21:04:45 @agent_ppo2.py:185][0m |          -0.0074 |          87.4863 |          18.7132 |
[32m[20221213 21:04:45 @agent_ppo2.py:185][0m |          -0.0090 |          87.3819 |          18.7021 |
[32m[20221213 21:04:45 @agent_ppo2.py:185][0m |          -0.0089 |          87.2415 |          18.7046 |
[32m[20221213 21:04:45 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:04:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 544.00
[32m[20221213 21:04:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 589.00
[32m[20221213 21:04:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.00
[32m[20221213 21:04:45 @agent_ppo2.py:143][0m Total time:       9.18 min
[32m[20221213 21:04:45 @agent_ppo2.py:145][0m 901120 total steps have happened
[32m[20221213 21:04:45 @agent_ppo2.py:121][0m #------------------------ Iteration 440 --------------------------#
[32m[20221213 21:04:45 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:04:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:46 @agent_ppo2.py:185][0m |          -0.0002 |          84.1828 |          18.7803 |
[32m[20221213 21:04:46 @agent_ppo2.py:185][0m |          -0.0068 |          82.5071 |          18.7681 |
[32m[20221213 21:04:46 @agent_ppo2.py:185][0m |          -0.0067 |          81.6148 |          18.7598 |
[32m[20221213 21:04:46 @agent_ppo2.py:185][0m |          -0.0020 |          81.0573 |          18.7593 |
[32m[20221213 21:04:46 @agent_ppo2.py:185][0m |          -0.0047 |          80.6771 |          18.7691 |
[32m[20221213 21:04:46 @agent_ppo2.py:185][0m |          -0.0082 |          80.4338 |          18.7586 |
[32m[20221213 21:04:46 @agent_ppo2.py:185][0m |          -0.0063 |          79.9155 |          18.7547 |
[32m[20221213 21:04:46 @agent_ppo2.py:185][0m |          -0.0067 |          79.9752 |          18.7585 |
[32m[20221213 21:04:46 @agent_ppo2.py:185][0m |           0.0049 |          89.5105 |          18.7590 |
[32m[20221213 21:04:47 @agent_ppo2.py:185][0m |          -0.0103 |          79.3970 |          18.7638 |
[32m[20221213 21:04:47 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 21:04:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.60
[32m[20221213 21:04:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.00
[32m[20221213 21:04:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.00
[32m[20221213 21:04:47 @agent_ppo2.py:143][0m Total time:       9.20 min
[32m[20221213 21:04:47 @agent_ppo2.py:145][0m 903168 total steps have happened
[32m[20221213 21:04:47 @agent_ppo2.py:121][0m #------------------------ Iteration 441 --------------------------#
[32m[20221213 21:04:47 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:04:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:47 @agent_ppo2.py:185][0m |           0.0109 |          98.6946 |          18.8294 |
[32m[20221213 21:04:47 @agent_ppo2.py:185][0m |          -0.0012 |          87.4548 |          18.8057 |
[32m[20221213 21:04:47 @agent_ppo2.py:185][0m |           0.0010 |          91.1092 |          18.8128 |
[32m[20221213 21:04:47 @agent_ppo2.py:185][0m |           0.0036 |          93.0419 |          18.8117 |
[32m[20221213 21:04:48 @agent_ppo2.py:185][0m |          -0.0076 |          86.9007 |          18.8116 |
[32m[20221213 21:04:48 @agent_ppo2.py:185][0m |          -0.0054 |          86.3909 |          18.8109 |
[32m[20221213 21:04:48 @agent_ppo2.py:185][0m |          -0.0061 |          86.2353 |          18.8019 |
[32m[20221213 21:04:48 @agent_ppo2.py:185][0m |          -0.0077 |          86.0626 |          18.8024 |
[32m[20221213 21:04:48 @agent_ppo2.py:185][0m |          -0.0054 |          85.9608 |          18.8011 |
[32m[20221213 21:04:48 @agent_ppo2.py:185][0m |          -0.0014 |          87.4168 |          18.7962 |
[32m[20221213 21:04:48 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 21:04:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.20
[32m[20221213 21:04:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 560.00
[32m[20221213 21:04:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 589.00
[32m[20221213 21:04:48 @agent_ppo2.py:143][0m Total time:       9.23 min
[32m[20221213 21:04:48 @agent_ppo2.py:145][0m 905216 total steps have happened
[32m[20221213 21:04:48 @agent_ppo2.py:121][0m #------------------------ Iteration 442 --------------------------#
[32m[20221213 21:04:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:04:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:49 @agent_ppo2.py:185][0m |          -0.0013 |          87.7079 |          18.7933 |
[32m[20221213 21:04:49 @agent_ppo2.py:185][0m |          -0.0052 |          86.8621 |          18.7836 |
[32m[20221213 21:04:49 @agent_ppo2.py:185][0m |          -0.0083 |          86.3386 |          18.7866 |
[32m[20221213 21:04:49 @agent_ppo2.py:185][0m |          -0.0014 |          88.7590 |          18.7818 |
[32m[20221213 21:04:49 @agent_ppo2.py:185][0m |          -0.0068 |          85.7224 |          18.7815 |
[32m[20221213 21:04:49 @agent_ppo2.py:185][0m |          -0.0081 |          85.6119 |          18.7815 |
[32m[20221213 21:04:49 @agent_ppo2.py:185][0m |          -0.0085 |          85.3415 |          18.7725 |
[32m[20221213 21:04:49 @agent_ppo2.py:185][0m |          -0.0112 |          85.2060 |          18.7809 |
[32m[20221213 21:04:49 @agent_ppo2.py:185][0m |          -0.0104 |          84.9413 |          18.7793 |
[32m[20221213 21:04:50 @agent_ppo2.py:185][0m |          -0.0097 |          84.8362 |          18.7666 |
[32m[20221213 21:04:50 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 21:04:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 534.00
[32m[20221213 21:04:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.00
[32m[20221213 21:04:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 571.00
[32m[20221213 21:04:50 @agent_ppo2.py:143][0m Total time:       9.25 min
[32m[20221213 21:04:50 @agent_ppo2.py:145][0m 907264 total steps have happened
[32m[20221213 21:04:50 @agent_ppo2.py:121][0m #------------------------ Iteration 443 --------------------------#
[32m[20221213 21:04:50 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:04:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:50 @agent_ppo2.py:185][0m |          -0.0021 |          85.2098 |          18.8845 |
[32m[20221213 21:04:50 @agent_ppo2.py:185][0m |          -0.0028 |          84.1859 |          18.8678 |
[32m[20221213 21:04:50 @agent_ppo2.py:185][0m |          -0.0034 |          83.5927 |          18.8400 |
[32m[20221213 21:04:50 @agent_ppo2.py:185][0m |          -0.0040 |          83.3905 |          18.8496 |
[32m[20221213 21:04:50 @agent_ppo2.py:185][0m |           0.0050 |          87.2945 |          18.8470 |
[32m[20221213 21:04:51 @agent_ppo2.py:185][0m |          -0.0027 |          83.8467 |          18.8386 |
[32m[20221213 21:04:51 @agent_ppo2.py:185][0m |          -0.0069 |          83.0680 |          18.8423 |
[32m[20221213 21:04:51 @agent_ppo2.py:185][0m |           0.0040 |          89.2518 |          18.8369 |
[32m[20221213 21:04:51 @agent_ppo2.py:185][0m |          -0.0034 |          82.9689 |          18.8282 |
[32m[20221213 21:04:51 @agent_ppo2.py:185][0m |          -0.0053 |          82.7886 |          18.8485 |
[32m[20221213 21:04:51 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:04:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.20
[32m[20221213 21:04:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 562.00
[32m[20221213 21:04:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.00
[32m[20221213 21:04:51 @agent_ppo2.py:143][0m Total time:       9.28 min
[32m[20221213 21:04:51 @agent_ppo2.py:145][0m 909312 total steps have happened
[32m[20221213 21:04:51 @agent_ppo2.py:121][0m #------------------------ Iteration 444 --------------------------#
[32m[20221213 21:04:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:04:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:52 @agent_ppo2.py:185][0m |          -0.0014 |          87.2771 |          18.7312 |
[32m[20221213 21:04:52 @agent_ppo2.py:185][0m |           0.0024 |          86.1758 |          18.7221 |
[32m[20221213 21:04:52 @agent_ppo2.py:185][0m |          -0.0040 |          84.8950 |          18.7194 |
[32m[20221213 21:04:52 @agent_ppo2.py:185][0m |          -0.0037 |          84.8155 |          18.7142 |
[32m[20221213 21:04:52 @agent_ppo2.py:185][0m |          -0.0012 |          84.3315 |          18.7188 |
[32m[20221213 21:04:52 @agent_ppo2.py:185][0m |          -0.0052 |          84.1268 |          18.7089 |
[32m[20221213 21:04:52 @agent_ppo2.py:185][0m |          -0.0072 |          83.8563 |          18.7196 |
[32m[20221213 21:04:52 @agent_ppo2.py:185][0m |           0.0072 |          95.6169 |          18.7076 |
[32m[20221213 21:04:52 @agent_ppo2.py:185][0m |          -0.0029 |          83.7947 |          18.6916 |
[32m[20221213 21:04:52 @agent_ppo2.py:185][0m |          -0.0102 |          83.5833 |          18.6970 |
[32m[20221213 21:04:52 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 21:04:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.20
[32m[20221213 21:04:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 541.00
[32m[20221213 21:04:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 559.00
[32m[20221213 21:04:53 @agent_ppo2.py:143][0m Total time:       9.30 min
[32m[20221213 21:04:53 @agent_ppo2.py:145][0m 911360 total steps have happened
[32m[20221213 21:04:53 @agent_ppo2.py:121][0m #------------------------ Iteration 445 --------------------------#
[32m[20221213 21:04:53 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:04:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:53 @agent_ppo2.py:185][0m |           0.0042 |          84.4813 |          18.7076 |
[32m[20221213 21:04:53 @agent_ppo2.py:185][0m |          -0.0062 |          83.1889 |          18.6826 |
[32m[20221213 21:04:53 @agent_ppo2.py:185][0m |          -0.0056 |          82.4422 |          18.6745 |
[32m[20221213 21:04:53 @agent_ppo2.py:185][0m |          -0.0029 |          81.9637 |          18.6614 |
[32m[20221213 21:04:53 @agent_ppo2.py:185][0m |          -0.0070 |          81.4964 |          18.6493 |
[32m[20221213 21:04:53 @agent_ppo2.py:185][0m |          -0.0040 |          81.4627 |          18.6621 |
[32m[20221213 21:04:54 @agent_ppo2.py:185][0m |           0.0088 |          87.9562 |          18.6452 |
[32m[20221213 21:04:54 @agent_ppo2.py:185][0m |          -0.0094 |          80.8536 |          18.6438 |
[32m[20221213 21:04:54 @agent_ppo2.py:185][0m |          -0.0095 |          80.5586 |          18.6459 |
[32m[20221213 21:04:54 @agent_ppo2.py:185][0m |          -0.0072 |          80.2983 |          18.6495 |
[32m[20221213 21:04:54 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:04:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.40
[32m[20221213 21:04:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.00
[32m[20221213 21:04:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.00
[32m[20221213 21:04:54 @agent_ppo2.py:143][0m Total time:       9.32 min
[32m[20221213 21:04:54 @agent_ppo2.py:145][0m 913408 total steps have happened
[32m[20221213 21:04:54 @agent_ppo2.py:121][0m #------------------------ Iteration 446 --------------------------#
[32m[20221213 21:04:54 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:04:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:54 @agent_ppo2.py:185][0m |           0.0092 |          88.8093 |          18.7768 |
[32m[20221213 21:04:55 @agent_ppo2.py:185][0m |          -0.0030 |          82.9483 |          18.7700 |
[32m[20221213 21:04:55 @agent_ppo2.py:185][0m |          -0.0035 |          82.5458 |          18.7627 |
[32m[20221213 21:04:55 @agent_ppo2.py:185][0m |          -0.0035 |          82.2309 |          18.7650 |
[32m[20221213 21:04:55 @agent_ppo2.py:185][0m |           0.0050 |          88.6837 |          18.7488 |
[32m[20221213 21:04:55 @agent_ppo2.py:185][0m |          -0.0020 |          81.9097 |          18.7453 |
[32m[20221213 21:04:55 @agent_ppo2.py:185][0m |          -0.0039 |          81.7866 |          18.7478 |
[32m[20221213 21:04:55 @agent_ppo2.py:185][0m |          -0.0036 |          81.6127 |          18.7460 |
[32m[20221213 21:04:55 @agent_ppo2.py:185][0m |          -0.0008 |          81.6744 |          18.7416 |
[32m[20221213 21:04:55 @agent_ppo2.py:185][0m |          -0.0057 |          81.4312 |          18.7408 |
[32m[20221213 21:04:55 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:04:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.80
[32m[20221213 21:04:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.00
[32m[20221213 21:04:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.00
[32m[20221213 21:04:56 @agent_ppo2.py:143][0m Total time:       9.35 min
[32m[20221213 21:04:56 @agent_ppo2.py:145][0m 915456 total steps have happened
[32m[20221213 21:04:56 @agent_ppo2.py:121][0m #------------------------ Iteration 447 --------------------------#
[32m[20221213 21:04:56 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:04:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:56 @agent_ppo2.py:185][0m |           0.0095 |          96.0586 |          18.8334 |
[32m[20221213 21:04:56 @agent_ppo2.py:185][0m |          -0.0040 |          84.3067 |          18.8036 |
[32m[20221213 21:04:56 @agent_ppo2.py:185][0m |          -0.0056 |          83.8797 |          18.8011 |
[32m[20221213 21:04:56 @agent_ppo2.py:185][0m |          -0.0072 |          83.2334 |          18.8063 |
[32m[20221213 21:04:56 @agent_ppo2.py:185][0m |          -0.0030 |          85.8591 |          18.7972 |
[32m[20221213 21:04:56 @agent_ppo2.py:185][0m |          -0.0059 |          82.7070 |          18.8046 |
[32m[20221213 21:04:57 @agent_ppo2.py:185][0m |          -0.0087 |          82.6432 |          18.8127 |
[32m[20221213 21:04:57 @agent_ppo2.py:185][0m |          -0.0106 |          82.3765 |          18.8036 |
[32m[20221213 21:04:57 @agent_ppo2.py:185][0m |          -0.0072 |          82.2266 |          18.8071 |
[32m[20221213 21:04:57 @agent_ppo2.py:185][0m |          -0.0079 |          82.1316 |          18.8123 |
[32m[20221213 21:04:57 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:04:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.20
[32m[20221213 21:04:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 571.00
[32m[20221213 21:04:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.00
[32m[20221213 21:04:57 @agent_ppo2.py:143][0m Total time:       9.37 min
[32m[20221213 21:04:57 @agent_ppo2.py:145][0m 917504 total steps have happened
[32m[20221213 21:04:57 @agent_ppo2.py:121][0m #------------------------ Iteration 448 --------------------------#
[32m[20221213 21:04:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:04:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:57 @agent_ppo2.py:185][0m |          -0.0029 |          84.5270 |          18.9101 |
[32m[20221213 21:04:57 @agent_ppo2.py:185][0m |          -0.0021 |          83.9940 |          18.8923 |
[32m[20221213 21:04:58 @agent_ppo2.py:185][0m |          -0.0021 |          84.1293 |          18.8897 |
[32m[20221213 21:04:58 @agent_ppo2.py:185][0m |          -0.0063 |          83.5096 |          18.8849 |
[32m[20221213 21:04:58 @agent_ppo2.py:185][0m |          -0.0003 |          86.0948 |          18.8759 |
[32m[20221213 21:04:58 @agent_ppo2.py:185][0m |          -0.0050 |          82.9364 |          18.8768 |
[32m[20221213 21:04:58 @agent_ppo2.py:185][0m |           0.0025 |          88.5011 |          18.8701 |
[32m[20221213 21:04:58 @agent_ppo2.py:185][0m |          -0.0068 |          82.6291 |          18.8771 |
[32m[20221213 21:04:58 @agent_ppo2.py:185][0m |          -0.0009 |          85.6783 |          18.8691 |
[32m[20221213 21:04:58 @agent_ppo2.py:185][0m |          -0.0085 |          82.4111 |          18.8539 |
[32m[20221213 21:04:58 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:04:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 555.40
[32m[20221213 21:04:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 595.00
[32m[20221213 21:04:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.00
[32m[20221213 21:04:58 @agent_ppo2.py:143][0m Total time:       9.40 min
[32m[20221213 21:04:58 @agent_ppo2.py:145][0m 919552 total steps have happened
[32m[20221213 21:04:58 @agent_ppo2.py:121][0m #------------------------ Iteration 449 --------------------------#
[32m[20221213 21:04:59 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:04:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:04:59 @agent_ppo2.py:185][0m |          -0.0021 |          83.2392 |          18.8626 |
[32m[20221213 21:04:59 @agent_ppo2.py:185][0m |          -0.0071 |          81.5139 |          18.8576 |
[32m[20221213 21:04:59 @agent_ppo2.py:185][0m |          -0.0039 |          80.8743 |          18.8516 |
[32m[20221213 21:04:59 @agent_ppo2.py:185][0m |          -0.0022 |          80.8495 |          18.8457 |
[32m[20221213 21:04:59 @agent_ppo2.py:185][0m |           0.0038 |          91.5635 |          18.8454 |
[32m[20221213 21:04:59 @agent_ppo2.py:185][0m |          -0.0077 |          80.1292 |          18.8236 |
[32m[20221213 21:04:59 @agent_ppo2.py:185][0m |          -0.0097 |          79.7428 |          18.8419 |
[32m[20221213 21:05:00 @agent_ppo2.py:185][0m |          -0.0089 |          79.7735 |          18.8362 |
[32m[20221213 21:05:00 @agent_ppo2.py:185][0m |          -0.0047 |          80.5632 |          18.8351 |
[32m[20221213 21:05:00 @agent_ppo2.py:185][0m |          -0.0067 |          79.2443 |          18.8400 |
[32m[20221213 21:05:00 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:05:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 526.80
[32m[20221213 21:05:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 565.00
[32m[20221213 21:05:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 615.00
[32m[20221213 21:05:00 @agent_ppo2.py:143][0m Total time:       9.42 min
[32m[20221213 21:05:00 @agent_ppo2.py:145][0m 921600 total steps have happened
[32m[20221213 21:05:00 @agent_ppo2.py:121][0m #------------------------ Iteration 450 --------------------------#
[32m[20221213 21:05:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:05:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:00 @agent_ppo2.py:185][0m |           0.0007 |          84.7625 |          18.6809 |
[32m[20221213 21:05:00 @agent_ppo2.py:185][0m |          -0.0009 |          84.1695 |          18.6733 |
[32m[20221213 21:05:00 @agent_ppo2.py:185][0m |          -0.0042 |          83.3640 |          18.6730 |
[32m[20221213 21:05:01 @agent_ppo2.py:185][0m |          -0.0055 |          83.0012 |          18.6639 |
[32m[20221213 21:05:01 @agent_ppo2.py:185][0m |          -0.0047 |          82.7005 |          18.6619 |
[32m[20221213 21:05:01 @agent_ppo2.py:185][0m |          -0.0001 |          84.5840 |          18.6620 |
[32m[20221213 21:05:01 @agent_ppo2.py:185][0m |          -0.0060 |          82.4074 |          18.6501 |
[32m[20221213 21:05:01 @agent_ppo2.py:185][0m |          -0.0020 |          82.5542 |          18.6588 |
[32m[20221213 21:05:01 @agent_ppo2.py:185][0m |          -0.0036 |          82.2305 |          18.6593 |
[32m[20221213 21:05:01 @agent_ppo2.py:185][0m |          -0.0071 |          82.0113 |          18.6503 |
[32m[20221213 21:05:01 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:05:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 528.60
[32m[20221213 21:05:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.00
[32m[20221213 21:05:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.00
[32m[20221213 21:05:01 @agent_ppo2.py:143][0m Total time:       9.44 min
[32m[20221213 21:05:01 @agent_ppo2.py:145][0m 923648 total steps have happened
[32m[20221213 21:05:01 @agent_ppo2.py:121][0m #------------------------ Iteration 451 --------------------------#
[32m[20221213 21:05:01 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:02 @agent_ppo2.py:185][0m |           0.0067 |          88.6711 |          18.8500 |
[32m[20221213 21:05:02 @agent_ppo2.py:185][0m |          -0.0066 |          83.8524 |          18.8330 |
[32m[20221213 21:05:02 @agent_ppo2.py:185][0m |          -0.0055 |          83.5158 |          18.8285 |
[32m[20221213 21:05:02 @agent_ppo2.py:185][0m |          -0.0030 |          83.3820 |          18.8175 |
[32m[20221213 21:05:02 @agent_ppo2.py:185][0m |          -0.0072 |          83.0957 |          18.8130 |
[32m[20221213 21:05:02 @agent_ppo2.py:185][0m |          -0.0022 |          83.9997 |          18.8140 |
[32m[20221213 21:05:02 @agent_ppo2.py:185][0m |          -0.0081 |          82.9372 |          18.8149 |
[32m[20221213 21:05:02 @agent_ppo2.py:185][0m |          -0.0052 |          82.7567 |          18.8056 |
[32m[20221213 21:05:02 @agent_ppo2.py:185][0m |          -0.0084 |          82.6811 |          18.8088 |
[32m[20221213 21:05:03 @agent_ppo2.py:185][0m |          -0.0092 |          82.5808 |          18.8043 |
[32m[20221213 21:05:03 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:05:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 550.40
[32m[20221213 21:05:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 582.00
[32m[20221213 21:05:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.00
[32m[20221213 21:05:03 @agent_ppo2.py:143][0m Total time:       9.47 min
[32m[20221213 21:05:03 @agent_ppo2.py:145][0m 925696 total steps have happened
[32m[20221213 21:05:03 @agent_ppo2.py:121][0m #------------------------ Iteration 452 --------------------------#
[32m[20221213 21:05:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:05:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:03 @agent_ppo2.py:185][0m |          -0.0021 |          85.7462 |          18.7678 |
[32m[20221213 21:05:03 @agent_ppo2.py:185][0m |          -0.0068 |          85.2494 |          18.7332 |
[32m[20221213 21:05:03 @agent_ppo2.py:185][0m |          -0.0053 |          84.8569 |          18.7439 |
[32m[20221213 21:05:03 @agent_ppo2.py:185][0m |          -0.0060 |          84.5424 |          18.7243 |
[32m[20221213 21:05:03 @agent_ppo2.py:185][0m |          -0.0040 |          84.4770 |          18.7255 |
[32m[20221213 21:05:04 @agent_ppo2.py:185][0m |          -0.0049 |          84.1172 |          18.7175 |
[32m[20221213 21:05:04 @agent_ppo2.py:185][0m |          -0.0084 |          84.0749 |          18.7172 |
[32m[20221213 21:05:04 @agent_ppo2.py:185][0m |          -0.0089 |          83.8615 |          18.7219 |
[32m[20221213 21:05:04 @agent_ppo2.py:185][0m |          -0.0060 |          83.6459 |          18.7137 |
[32m[20221213 21:05:04 @agent_ppo2.py:185][0m |          -0.0076 |          83.5575 |          18.7053 |
[32m[20221213 21:05:04 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:05:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.40
[32m[20221213 21:05:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 585.00
[32m[20221213 21:05:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 614.00
[32m[20221213 21:05:04 @agent_ppo2.py:143][0m Total time:       9.49 min
[32m[20221213 21:05:04 @agent_ppo2.py:145][0m 927744 total steps have happened
[32m[20221213 21:05:04 @agent_ppo2.py:121][0m #------------------------ Iteration 453 --------------------------#
[32m[20221213 21:05:04 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:04 @agent_ppo2.py:185][0m |           0.0014 |          88.8150 |          19.0109 |
[32m[20221213 21:05:05 @agent_ppo2.py:185][0m |          -0.0041 |          87.6572 |          19.0018 |
[32m[20221213 21:05:05 @agent_ppo2.py:185][0m |          -0.0068 |          87.1917 |          18.9931 |
[32m[20221213 21:05:05 @agent_ppo2.py:185][0m |          -0.0049 |          86.8567 |          18.9888 |
[32m[20221213 21:05:05 @agent_ppo2.py:185][0m |          -0.0061 |          86.8226 |          18.9938 |
[32m[20221213 21:05:05 @agent_ppo2.py:185][0m |          -0.0062 |          86.4338 |          18.9785 |
[32m[20221213 21:05:05 @agent_ppo2.py:185][0m |          -0.0050 |          86.0724 |          18.9818 |
[32m[20221213 21:05:05 @agent_ppo2.py:185][0m |          -0.0060 |          86.5937 |          18.9839 |
[32m[20221213 21:05:05 @agent_ppo2.py:185][0m |          -0.0064 |          85.7970 |          18.9818 |
[32m[20221213 21:05:05 @agent_ppo2.py:185][0m |          -0.0075 |          85.6366 |          18.9770 |
[32m[20221213 21:05:05 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 21:05:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 558.20
[32m[20221213 21:05:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.00
[32m[20221213 21:05:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 528.00
[32m[20221213 21:05:06 @agent_ppo2.py:143][0m Total time:       9.51 min
[32m[20221213 21:05:06 @agent_ppo2.py:145][0m 929792 total steps have happened
[32m[20221213 21:05:06 @agent_ppo2.py:121][0m #------------------------ Iteration 454 --------------------------#
[32m[20221213 21:05:06 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:06 @agent_ppo2.py:185][0m |           0.0093 |          92.2680 |          18.8411 |
[32m[20221213 21:05:06 @agent_ppo2.py:185][0m |          -0.0037 |          88.4751 |          18.8329 |
[32m[20221213 21:05:06 @agent_ppo2.py:185][0m |          -0.0056 |          88.0246 |          18.8302 |
[32m[20221213 21:05:06 @agent_ppo2.py:185][0m |          -0.0078 |          87.7983 |          18.8302 |
[32m[20221213 21:05:06 @agent_ppo2.py:185][0m |          -0.0058 |          87.4365 |          18.8253 |
[32m[20221213 21:05:06 @agent_ppo2.py:185][0m |          -0.0020 |          88.2493 |          18.8245 |
[32m[20221213 21:05:06 @agent_ppo2.py:185][0m |          -0.0091 |          87.0648 |          18.8265 |
[32m[20221213 21:05:07 @agent_ppo2.py:185][0m |          -0.0057 |          86.7317 |          18.8209 |
[32m[20221213 21:05:07 @agent_ppo2.py:185][0m |          -0.0082 |          86.6341 |          18.8198 |
[32m[20221213 21:05:07 @agent_ppo2.py:185][0m |          -0.0060 |          86.5304 |          18.8282 |
[32m[20221213 21:05:07 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 21:05:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 549.40
[32m[20221213 21:05:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 614.00
[32m[20221213 21:05:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 572.00
[32m[20221213 21:05:07 @agent_ppo2.py:143][0m Total time:       9.54 min
[32m[20221213 21:05:07 @agent_ppo2.py:145][0m 931840 total steps have happened
[32m[20221213 21:05:07 @agent_ppo2.py:121][0m #------------------------ Iteration 455 --------------------------#
[32m[20221213 21:05:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:05:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:07 @agent_ppo2.py:185][0m |           0.0019 |          86.8604 |          18.8838 |
[32m[20221213 21:05:07 @agent_ppo2.py:185][0m |          -0.0019 |          85.0350 |          18.8682 |
[32m[20221213 21:05:08 @agent_ppo2.py:185][0m |          -0.0051 |          84.6798 |          18.8646 |
[32m[20221213 21:05:08 @agent_ppo2.py:185][0m |          -0.0052 |          84.1304 |          18.8662 |
[32m[20221213 21:05:08 @agent_ppo2.py:185][0m |          -0.0063 |          83.6099 |          18.8585 |
[32m[20221213 21:05:08 @agent_ppo2.py:185][0m |          -0.0031 |          83.4604 |          18.8567 |
[32m[20221213 21:05:08 @agent_ppo2.py:185][0m |          -0.0072 |          83.2114 |          18.8539 |
[32m[20221213 21:05:08 @agent_ppo2.py:185][0m |          -0.0067 |          83.0200 |          18.8510 |
[32m[20221213 21:05:08 @agent_ppo2.py:185][0m |          -0.0091 |          82.7571 |          18.8552 |
[32m[20221213 21:05:08 @agent_ppo2.py:185][0m |          -0.0088 |          82.6236 |          18.8492 |
[32m[20221213 21:05:08 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:05:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.80
[32m[20221213 21:05:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 611.00
[32m[20221213 21:05:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 578.00
[32m[20221213 21:05:08 @agent_ppo2.py:143][0m Total time:       9.56 min
[32m[20221213 21:05:08 @agent_ppo2.py:145][0m 933888 total steps have happened
[32m[20221213 21:05:08 @agent_ppo2.py:121][0m #------------------------ Iteration 456 --------------------------#
[32m[20221213 21:05:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:05:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:09 @agent_ppo2.py:185][0m |          -0.0014 |          85.9571 |          18.9588 |
[32m[20221213 21:05:09 @agent_ppo2.py:185][0m |          -0.0011 |          85.9893 |          18.9251 |
[32m[20221213 21:05:09 @agent_ppo2.py:185][0m |          -0.0042 |          85.0685 |          18.9230 |
[32m[20221213 21:05:09 @agent_ppo2.py:185][0m |          -0.0067 |          84.8222 |          18.9216 |
[32m[20221213 21:05:09 @agent_ppo2.py:185][0m |          -0.0075 |          84.6955 |          18.9135 |
[32m[20221213 21:05:09 @agent_ppo2.py:185][0m |          -0.0090 |          84.5413 |          18.9146 |
[32m[20221213 21:05:09 @agent_ppo2.py:185][0m |          -0.0071 |          84.4250 |          18.9170 |
[32m[20221213 21:05:09 @agent_ppo2.py:185][0m |          -0.0084 |          84.1681 |          18.9111 |
[32m[20221213 21:05:09 @agent_ppo2.py:185][0m |          -0.0080 |          84.0793 |          18.9082 |
[32m[20221213 21:05:10 @agent_ppo2.py:185][0m |          -0.0095 |          84.0074 |          18.9053 |
[32m[20221213 21:05:10 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 21:05:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 561.60
[32m[20221213 21:05:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.00
[32m[20221213 21:05:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 558.00
[32m[20221213 21:05:10 @agent_ppo2.py:143][0m Total time:       9.58 min
[32m[20221213 21:05:10 @agent_ppo2.py:145][0m 935936 total steps have happened
[32m[20221213 21:05:10 @agent_ppo2.py:121][0m #------------------------ Iteration 457 --------------------------#
[32m[20221213 21:05:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:05:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:10 @agent_ppo2.py:185][0m |           0.0033 |          86.1443 |          18.9316 |
[32m[20221213 21:05:10 @agent_ppo2.py:185][0m |          -0.0042 |          83.5627 |          18.9038 |
[32m[20221213 21:05:10 @agent_ppo2.py:185][0m |           0.0018 |          86.0041 |          18.8991 |
[32m[20221213 21:05:10 @agent_ppo2.py:185][0m |          -0.0084 |          82.4611 |          18.8931 |
[32m[20221213 21:05:10 @agent_ppo2.py:185][0m |          -0.0061 |          82.4223 |          18.8821 |
[32m[20221213 21:05:11 @agent_ppo2.py:185][0m |          -0.0102 |          82.1597 |          18.8886 |
[32m[20221213 21:05:11 @agent_ppo2.py:185][0m |          -0.0102 |          82.0995 |          18.8765 |
[32m[20221213 21:05:11 @agent_ppo2.py:185][0m |          -0.0100 |          81.9131 |          18.8820 |
[32m[20221213 21:05:11 @agent_ppo2.py:185][0m |          -0.0099 |          81.7708 |          18.8822 |
[32m[20221213 21:05:11 @agent_ppo2.py:185][0m |          -0.0105 |          81.7283 |          18.8799 |
[32m[20221213 21:05:11 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:05:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 528.20
[32m[20221213 21:05:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.00
[32m[20221213 21:05:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 593.00
[32m[20221213 21:05:11 @agent_ppo2.py:143][0m Total time:       9.61 min
[32m[20221213 21:05:11 @agent_ppo2.py:145][0m 937984 total steps have happened
[32m[20221213 21:05:11 @agent_ppo2.py:121][0m #------------------------ Iteration 458 --------------------------#
[32m[20221213 21:05:11 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:11 @agent_ppo2.py:185][0m |          -0.0026 |          87.1971 |          18.8676 |
[32m[20221213 21:05:12 @agent_ppo2.py:185][0m |          -0.0011 |          84.5989 |          18.8239 |
[32m[20221213 21:05:12 @agent_ppo2.py:185][0m |          -0.0036 |          82.0896 |          18.8328 |
[32m[20221213 21:05:12 @agent_ppo2.py:185][0m |          -0.0075 |          81.6598 |          18.8323 |
[32m[20221213 21:05:12 @agent_ppo2.py:185][0m |          -0.0038 |          81.5621 |          18.8180 |
[32m[20221213 21:05:12 @agent_ppo2.py:185][0m |          -0.0051 |          81.1746 |          18.8317 |
[32m[20221213 21:05:12 @agent_ppo2.py:185][0m |          -0.0070 |          81.4633 |          18.8295 |
[32m[20221213 21:05:12 @agent_ppo2.py:185][0m |           0.0015 |          83.7171 |          18.8248 |
[32m[20221213 21:05:12 @agent_ppo2.py:185][0m |          -0.0096 |          80.6179 |          18.8243 |
[32m[20221213 21:05:12 @agent_ppo2.py:185][0m |          -0.0086 |          80.2458 |          18.8290 |
[32m[20221213 21:05:12 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 21:05:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 558.40
[32m[20221213 21:05:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 577.00
[32m[20221213 21:05:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 583.00
[32m[20221213 21:05:13 @agent_ppo2.py:143][0m Total time:       9.63 min
[32m[20221213 21:05:13 @agent_ppo2.py:145][0m 940032 total steps have happened
[32m[20221213 21:05:13 @agent_ppo2.py:121][0m #------------------------ Iteration 459 --------------------------#
[32m[20221213 21:05:13 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:13 @agent_ppo2.py:185][0m |           0.0091 |          96.6012 |          19.0440 |
[32m[20221213 21:05:13 @agent_ppo2.py:185][0m |          -0.0049 |          87.4863 |          19.0253 |
[32m[20221213 21:05:13 @agent_ppo2.py:185][0m |           0.0013 |          90.2581 |          19.0282 |
[32m[20221213 21:05:13 @agent_ppo2.py:185][0m |          -0.0041 |          86.7208 |          19.0105 |
[32m[20221213 21:05:13 @agent_ppo2.py:185][0m |           0.0034 |          91.9978 |          19.0129 |
[32m[20221213 21:05:13 @agent_ppo2.py:185][0m |          -0.0064 |          86.3220 |          18.9967 |
[32m[20221213 21:05:13 @agent_ppo2.py:185][0m |          -0.0054 |          86.3045 |          19.0098 |
[32m[20221213 21:05:14 @agent_ppo2.py:185][0m |          -0.0046 |          86.5047 |          19.0050 |
[32m[20221213 21:05:14 @agent_ppo2.py:185][0m |          -0.0069 |          85.8591 |          19.0109 |
[32m[20221213 21:05:14 @agent_ppo2.py:185][0m |          -0.0099 |          85.7950 |          19.0055 |
[32m[20221213 21:05:14 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:05:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.80
[32m[20221213 21:05:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 575.00
[32m[20221213 21:05:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 588.00
[32m[20221213 21:05:14 @agent_ppo2.py:143][0m Total time:       9.65 min
[32m[20221213 21:05:14 @agent_ppo2.py:145][0m 942080 total steps have happened
[32m[20221213 21:05:14 @agent_ppo2.py:121][0m #------------------------ Iteration 460 --------------------------#
[32m[20221213 21:05:14 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:14 @agent_ppo2.py:185][0m |          -0.0037 |          90.2997 |          18.8283 |
[32m[20221213 21:05:14 @agent_ppo2.py:185][0m |          -0.0022 |          89.8415 |          18.8143 |
[32m[20221213 21:05:14 @agent_ppo2.py:185][0m |           0.0046 |         100.2846 |          18.7973 |
[32m[20221213 21:05:15 @agent_ppo2.py:185][0m |          -0.0045 |          89.3115 |          18.7794 |
[32m[20221213 21:05:15 @agent_ppo2.py:185][0m |          -0.0079 |          88.2890 |          18.7751 |
[32m[20221213 21:05:15 @agent_ppo2.py:185][0m |          -0.0082 |          88.1861 |          18.7688 |
[32m[20221213 21:05:15 @agent_ppo2.py:185][0m |          -0.0059 |          88.0407 |          18.7715 |
[32m[20221213 21:05:15 @agent_ppo2.py:185][0m |          -0.0100 |          87.7394 |          18.7690 |
[32m[20221213 21:05:15 @agent_ppo2.py:185][0m |          -0.0106 |          87.6445 |          18.7662 |
[32m[20221213 21:05:15 @agent_ppo2.py:185][0m |           0.0145 |         102.1200 |          18.7622 |
[32m[20221213 21:05:15 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:05:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.80
[32m[20221213 21:05:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 582.00
[32m[20221213 21:05:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 558.00
[32m[20221213 21:05:15 @agent_ppo2.py:143][0m Total time:       9.68 min
[32m[20221213 21:05:15 @agent_ppo2.py:145][0m 944128 total steps have happened
[32m[20221213 21:05:15 @agent_ppo2.py:121][0m #------------------------ Iteration 461 --------------------------#
[32m[20221213 21:05:15 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:16 @agent_ppo2.py:185][0m |          -0.0007 |          89.5722 |          18.8641 |
[32m[20221213 21:05:16 @agent_ppo2.py:185][0m |          -0.0031 |          89.1070 |          18.8582 |
[32m[20221213 21:05:16 @agent_ppo2.py:185][0m |          -0.0045 |          88.8146 |          18.8518 |
[32m[20221213 21:05:16 @agent_ppo2.py:185][0m |          -0.0024 |          89.2888 |          18.8556 |
[32m[20221213 21:05:16 @agent_ppo2.py:185][0m |          -0.0033 |          88.3828 |          18.8495 |
[32m[20221213 21:05:16 @agent_ppo2.py:185][0m |          -0.0055 |          88.1955 |          18.8502 |
[32m[20221213 21:05:16 @agent_ppo2.py:185][0m |          -0.0054 |          88.0028 |          18.8551 |
[32m[20221213 21:05:16 @agent_ppo2.py:185][0m |          -0.0042 |          88.0185 |          18.8543 |
[32m[20221213 21:05:17 @agent_ppo2.py:185][0m |          -0.0048 |          87.7282 |          18.8524 |
[32m[20221213 21:05:17 @agent_ppo2.py:185][0m |          -0.0032 |          87.7170 |          18.8561 |
[32m[20221213 21:05:17 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:05:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 531.00
[32m[20221213 21:05:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.00
[32m[20221213 21:05:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 612.00
[32m[20221213 21:05:17 @agent_ppo2.py:143][0m Total time:       9.70 min
[32m[20221213 21:05:17 @agent_ppo2.py:145][0m 946176 total steps have happened
[32m[20221213 21:05:17 @agent_ppo2.py:121][0m #------------------------ Iteration 462 --------------------------#
[32m[20221213 21:05:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:05:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:17 @agent_ppo2.py:185][0m |          -0.0039 |          87.2112 |          18.8706 |
[32m[20221213 21:05:17 @agent_ppo2.py:185][0m |          -0.0070 |          85.9428 |          18.8643 |
[32m[20221213 21:05:17 @agent_ppo2.py:185][0m |          -0.0063 |          85.2222 |          18.8515 |
[32m[20221213 21:05:17 @agent_ppo2.py:185][0m |          -0.0073 |          84.9185 |          18.8538 |
[32m[20221213 21:05:18 @agent_ppo2.py:185][0m |          -0.0076 |          84.3964 |          18.8471 |
[32m[20221213 21:05:18 @agent_ppo2.py:185][0m |          -0.0103 |          84.1768 |          18.8532 |
[32m[20221213 21:05:18 @agent_ppo2.py:185][0m |          -0.0085 |          84.2941 |          18.8423 |
[32m[20221213 21:05:18 @agent_ppo2.py:185][0m |          -0.0076 |          83.6101 |          18.8407 |
[32m[20221213 21:05:18 @agent_ppo2.py:185][0m |          -0.0111 |          83.4888 |          18.8482 |
[32m[20221213 21:05:18 @agent_ppo2.py:185][0m |          -0.0038 |          84.5208 |          18.8386 |
[32m[20221213 21:05:18 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:05:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 552.80
[32m[20221213 21:05:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 579.00
[32m[20221213 21:05:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 619.00
[32m[20221213 21:05:18 @agent_ppo2.py:143][0m Total time:       9.73 min
[32m[20221213 21:05:18 @agent_ppo2.py:145][0m 948224 total steps have happened
[32m[20221213 21:05:18 @agent_ppo2.py:121][0m #------------------------ Iteration 463 --------------------------#
[32m[20221213 21:05:18 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:19 @agent_ppo2.py:185][0m |           0.0054 |          87.7681 |          19.0714 |
[32m[20221213 21:05:19 @agent_ppo2.py:185][0m |          -0.0048 |          86.8435 |          19.0683 |
[32m[20221213 21:05:19 @agent_ppo2.py:185][0m |          -0.0060 |          86.3048 |          19.0620 |
[32m[20221213 21:05:19 @agent_ppo2.py:185][0m |          -0.0051 |          85.9735 |          19.0526 |
[32m[20221213 21:05:19 @agent_ppo2.py:185][0m |          -0.0056 |          85.7164 |          19.0508 |
[32m[20221213 21:05:19 @agent_ppo2.py:185][0m |          -0.0058 |          86.5040 |          19.0453 |
[32m[20221213 21:05:19 @agent_ppo2.py:185][0m |          -0.0067 |          85.4803 |          19.0434 |
[32m[20221213 21:05:19 @agent_ppo2.py:185][0m |          -0.0071 |          85.3775 |          19.0511 |
[32m[20221213 21:05:19 @agent_ppo2.py:185][0m |          -0.0099 |          85.2518 |          19.0517 |
[32m[20221213 21:05:20 @agent_ppo2.py:185][0m |          -0.0090 |          85.2230 |          19.0570 |
[32m[20221213 21:05:20 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 21:05:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.60
[32m[20221213 21:05:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.00
[32m[20221213 21:05:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 622.00
[32m[20221213 21:05:20 @agent_ppo2.py:143][0m Total time:       9.75 min
[32m[20221213 21:05:20 @agent_ppo2.py:145][0m 950272 total steps have happened
[32m[20221213 21:05:20 @agent_ppo2.py:121][0m #------------------------ Iteration 464 --------------------------#
[32m[20221213 21:05:20 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:20 @agent_ppo2.py:185][0m |           0.0006 |          89.8579 |          18.9300 |
[32m[20221213 21:05:20 @agent_ppo2.py:185][0m |           0.0045 |          97.4024 |          18.9144 |
[32m[20221213 21:05:20 @agent_ppo2.py:185][0m |          -0.0061 |          87.1226 |          18.8917 |
[32m[20221213 21:05:20 @agent_ppo2.py:185][0m |          -0.0017 |          87.2220 |          18.9026 |
[32m[20221213 21:05:20 @agent_ppo2.py:185][0m |          -0.0064 |          86.4508 |          18.8917 |
[32m[20221213 21:05:20 @agent_ppo2.py:185][0m |          -0.0006 |          89.4427 |          18.8885 |
[32m[20221213 21:05:21 @agent_ppo2.py:185][0m |          -0.0070 |          85.8968 |          18.8973 |
[32m[20221213 21:05:21 @agent_ppo2.py:185][0m |           0.0034 |          90.6946 |          18.8873 |
[32m[20221213 21:05:21 @agent_ppo2.py:185][0m |          -0.0117 |          85.5136 |          18.8763 |
[32m[20221213 21:05:21 @agent_ppo2.py:185][0m |          -0.0073 |          85.2198 |          18.8862 |
[32m[20221213 21:05:21 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:05:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.80
[32m[20221213 21:05:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.00
[32m[20221213 21:05:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 596.00
[32m[20221213 21:05:21 @agent_ppo2.py:143][0m Total time:       9.77 min
[32m[20221213 21:05:21 @agent_ppo2.py:145][0m 952320 total steps have happened
[32m[20221213 21:05:21 @agent_ppo2.py:121][0m #------------------------ Iteration 465 --------------------------#
[32m[20221213 21:05:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:05:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:21 @agent_ppo2.py:185][0m |          -0.0010 |          88.0812 |          19.0661 |
[32m[20221213 21:05:22 @agent_ppo2.py:185][0m |          -0.0053 |          86.5851 |          19.0441 |
[32m[20221213 21:05:22 @agent_ppo2.py:185][0m |          -0.0055 |          85.9912 |          19.0398 |
[32m[20221213 21:05:22 @agent_ppo2.py:185][0m |          -0.0099 |          85.5436 |          19.0434 |
[32m[20221213 21:05:22 @agent_ppo2.py:185][0m |          -0.0063 |          85.3185 |          19.0413 |
[32m[20221213 21:05:22 @agent_ppo2.py:185][0m |          -0.0047 |          84.9485 |          19.0375 |
[32m[20221213 21:05:22 @agent_ppo2.py:185][0m |          -0.0050 |          84.8005 |          19.0311 |
[32m[20221213 21:05:22 @agent_ppo2.py:185][0m |           0.0005 |          88.9925 |          19.0319 |
[32m[20221213 21:05:22 @agent_ppo2.py:185][0m |          -0.0060 |          84.5022 |          19.0242 |
[32m[20221213 21:05:22 @agent_ppo2.py:185][0m |          -0.0074 |          84.3863 |          19.0263 |
[32m[20221213 21:05:22 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:05:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 524.20
[32m[20221213 21:05:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.00
[32m[20221213 21:05:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 614.00
[32m[20221213 21:05:23 @agent_ppo2.py:143][0m Total time:       9.80 min
[32m[20221213 21:05:23 @agent_ppo2.py:145][0m 954368 total steps have happened
[32m[20221213 21:05:23 @agent_ppo2.py:121][0m #------------------------ Iteration 466 --------------------------#
[32m[20221213 21:05:23 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:23 @agent_ppo2.py:185][0m |           0.0008 |          89.8926 |          18.9838 |
[32m[20221213 21:05:23 @agent_ppo2.py:185][0m |           0.0035 |          91.2138 |          18.9630 |
[32m[20221213 21:05:23 @agent_ppo2.py:185][0m |          -0.0051 |          88.5390 |          18.9601 |
[32m[20221213 21:05:23 @agent_ppo2.py:185][0m |          -0.0035 |          88.4901 |          18.9518 |
[32m[20221213 21:05:23 @agent_ppo2.py:185][0m |          -0.0076 |          88.1459 |          18.9599 |
[32m[20221213 21:05:23 @agent_ppo2.py:185][0m |          -0.0070 |          88.2210 |          18.9474 |
[32m[20221213 21:05:23 @agent_ppo2.py:185][0m |          -0.0087 |          87.8876 |          18.9372 |
[32m[20221213 21:05:24 @agent_ppo2.py:185][0m |          -0.0052 |          88.1607 |          18.9417 |
[32m[20221213 21:05:24 @agent_ppo2.py:185][0m |          -0.0077 |          87.5194 |          18.9326 |
[32m[20221213 21:05:24 @agent_ppo2.py:185][0m |          -0.0080 |          87.9337 |          18.9332 |
[32m[20221213 21:05:24 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:05:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 566.00
[32m[20221213 21:05:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 630.00
[32m[20221213 21:05:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.00
[32m[20221213 21:05:24 @agent_ppo2.py:143][0m Total time:       9.82 min
[32m[20221213 21:05:24 @agent_ppo2.py:145][0m 956416 total steps have happened
[32m[20221213 21:05:24 @agent_ppo2.py:121][0m #------------------------ Iteration 467 --------------------------#
[32m[20221213 21:05:24 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:24 @agent_ppo2.py:185][0m |           0.0028 |          90.0095 |          18.9542 |
[32m[20221213 21:05:24 @agent_ppo2.py:185][0m |           0.0048 |          89.6728 |          18.9335 |
[32m[20221213 21:05:24 @agent_ppo2.py:185][0m |          -0.0031 |          86.9561 |          18.9449 |
[32m[20221213 21:05:25 @agent_ppo2.py:185][0m |          -0.0058 |          86.5847 |          18.9384 |
[32m[20221213 21:05:25 @agent_ppo2.py:185][0m |          -0.0072 |          86.3566 |          18.9374 |
[32m[20221213 21:05:25 @agent_ppo2.py:185][0m |           0.0100 |          95.2861 |          18.9370 |
[32m[20221213 21:05:25 @agent_ppo2.py:185][0m |          -0.0064 |          86.1271 |          18.9436 |
[32m[20221213 21:05:25 @agent_ppo2.py:185][0m |          -0.0029 |          86.2892 |          18.9395 |
[32m[20221213 21:05:25 @agent_ppo2.py:185][0m |          -0.0068 |          85.5767 |          18.9461 |
[32m[20221213 21:05:25 @agent_ppo2.py:185][0m |          -0.0032 |          86.6694 |          18.9447 |
[32m[20221213 21:05:25 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:05:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.00
[32m[20221213 21:05:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.00
[32m[20221213 21:05:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 604.00
[32m[20221213 21:05:25 @agent_ppo2.py:143][0m Total time:       9.84 min
[32m[20221213 21:05:25 @agent_ppo2.py:145][0m 958464 total steps have happened
[32m[20221213 21:05:25 @agent_ppo2.py:121][0m #------------------------ Iteration 468 --------------------------#
[32m[20221213 21:05:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:05:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:26 @agent_ppo2.py:185][0m |           0.0031 |          87.6258 |          19.0359 |
[32m[20221213 21:05:26 @agent_ppo2.py:185][0m |          -0.0012 |          86.6986 |          19.0224 |
[32m[20221213 21:05:26 @agent_ppo2.py:185][0m |          -0.0031 |          86.2643 |          19.0085 |
[32m[20221213 21:05:26 @agent_ppo2.py:185][0m |          -0.0066 |          86.1708 |          19.0027 |
[32m[20221213 21:05:26 @agent_ppo2.py:185][0m |          -0.0041 |          86.0774 |          19.0031 |
[32m[20221213 21:05:26 @agent_ppo2.py:185][0m |          -0.0051 |          85.8235 |          18.9976 |
[32m[20221213 21:05:26 @agent_ppo2.py:185][0m |          -0.0069 |          85.7241 |          18.9983 |
[32m[20221213 21:05:26 @agent_ppo2.py:185][0m |          -0.0061 |          85.5816 |          18.9966 |
[32m[20221213 21:05:27 @agent_ppo2.py:185][0m |          -0.0081 |          85.5361 |          18.9929 |
[32m[20221213 21:05:27 @agent_ppo2.py:185][0m |           0.0058 |          93.9768 |          18.9936 |
[32m[20221213 21:05:27 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:05:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 525.00
[32m[20221213 21:05:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 583.00
[32m[20221213 21:05:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 628.00
[32m[20221213 21:05:27 @agent_ppo2.py:143][0m Total time:       9.87 min
[32m[20221213 21:05:27 @agent_ppo2.py:145][0m 960512 total steps have happened
[32m[20221213 21:05:27 @agent_ppo2.py:121][0m #------------------------ Iteration 469 --------------------------#
[32m[20221213 21:05:27 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:27 @agent_ppo2.py:185][0m |          -0.0029 |          85.1009 |          18.8527 |
[32m[20221213 21:05:27 @agent_ppo2.py:185][0m |          -0.0057 |          84.0612 |          18.8330 |
[32m[20221213 21:05:27 @agent_ppo2.py:185][0m |          -0.0037 |          83.3968 |          18.8378 |
[32m[20221213 21:05:27 @agent_ppo2.py:185][0m |          -0.0040 |          83.0574 |          18.8247 |
[32m[20221213 21:05:28 @agent_ppo2.py:185][0m |          -0.0046 |          82.7169 |          18.8266 |
[32m[20221213 21:05:28 @agent_ppo2.py:185][0m |          -0.0036 |          82.4785 |          18.8251 |
[32m[20221213 21:05:28 @agent_ppo2.py:185][0m |           0.0061 |          88.8493 |          18.8232 |
[32m[20221213 21:05:28 @agent_ppo2.py:185][0m |          -0.0041 |          82.1029 |          18.8193 |
[32m[20221213 21:05:28 @agent_ppo2.py:185][0m |          -0.0025 |          82.0640 |          18.8218 |
[32m[20221213 21:05:28 @agent_ppo2.py:185][0m |          -0.0096 |          81.8435 |          18.8168 |
[32m[20221213 21:05:28 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:05:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 544.40
[32m[20221213 21:05:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 585.00
[32m[20221213 21:05:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:05:28 @agent_ppo2.py:143][0m Total time:       9.89 min
[32m[20221213 21:05:28 @agent_ppo2.py:145][0m 962560 total steps have happened
[32m[20221213 21:05:28 @agent_ppo2.py:121][0m #------------------------ Iteration 470 --------------------------#
[32m[20221213 21:05:28 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:29 @agent_ppo2.py:185][0m |          -0.0034 |          87.9929 |          19.1361 |
[32m[20221213 21:05:29 @agent_ppo2.py:185][0m |          -0.0047 |          86.9816 |          19.1182 |
[32m[20221213 21:05:29 @agent_ppo2.py:185][0m |          -0.0049 |          86.3061 |          19.1205 |
[32m[20221213 21:05:29 @agent_ppo2.py:185][0m |          -0.0073 |          85.8490 |          19.1099 |
[32m[20221213 21:05:29 @agent_ppo2.py:185][0m |          -0.0073 |          85.6568 |          19.1234 |
[32m[20221213 21:05:29 @agent_ppo2.py:185][0m |          -0.0062 |          85.2812 |          19.1058 |
[32m[20221213 21:05:29 @agent_ppo2.py:185][0m |          -0.0063 |          84.8841 |          19.1146 |
[32m[20221213 21:05:29 @agent_ppo2.py:185][0m |           0.0010 |          89.2817 |          19.1170 |
[32m[20221213 21:05:29 @agent_ppo2.py:185][0m |          -0.0075 |          84.5090 |          19.1143 |
[32m[20221213 21:05:30 @agent_ppo2.py:185][0m |          -0.0043 |          85.5114 |          19.1138 |
[32m[20221213 21:05:30 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:05:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 561.20
[32m[20221213 21:05:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 579.00
[32m[20221213 21:05:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 617.00
[32m[20221213 21:05:30 @agent_ppo2.py:143][0m Total time:       9.92 min
[32m[20221213 21:05:30 @agent_ppo2.py:145][0m 964608 total steps have happened
[32m[20221213 21:05:30 @agent_ppo2.py:121][0m #------------------------ Iteration 471 --------------------------#
[32m[20221213 21:05:30 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:30 @agent_ppo2.py:185][0m |           0.0028 |          86.5995 |          18.9277 |
[32m[20221213 21:05:30 @agent_ppo2.py:185][0m |          -0.0083 |          85.6477 |          18.8950 |
[32m[20221213 21:05:30 @agent_ppo2.py:185][0m |           0.0007 |          89.8983 |          18.9001 |
[32m[20221213 21:05:30 @agent_ppo2.py:185][0m |          -0.0059 |          84.7540 |          18.8838 |
[32m[20221213 21:05:30 @agent_ppo2.py:185][0m |          -0.0074 |          84.5224 |          18.8870 |
[32m[20221213 21:05:31 @agent_ppo2.py:185][0m |          -0.0089 |          84.3591 |          18.8847 |
[32m[20221213 21:05:31 @agent_ppo2.py:185][0m |          -0.0108 |          84.1834 |          18.8877 |
[32m[20221213 21:05:31 @agent_ppo2.py:185][0m |          -0.0112 |          84.0227 |          18.8877 |
[32m[20221213 21:05:31 @agent_ppo2.py:185][0m |          -0.0098 |          83.7955 |          18.8862 |
[32m[20221213 21:05:31 @agent_ppo2.py:185][0m |          -0.0084 |          83.5875 |          18.8863 |
[32m[20221213 21:05:31 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:05:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.80
[32m[20221213 21:05:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 602.00
[32m[20221213 21:05:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.00
[32m[20221213 21:05:31 @agent_ppo2.py:143][0m Total time:       9.94 min
[32m[20221213 21:05:31 @agent_ppo2.py:145][0m 966656 total steps have happened
[32m[20221213 21:05:31 @agent_ppo2.py:121][0m #------------------------ Iteration 472 --------------------------#
[32m[20221213 21:05:31 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:31 @agent_ppo2.py:185][0m |           0.0020 |          87.4659 |          18.9768 |
[32m[20221213 21:05:32 @agent_ppo2.py:185][0m |          -0.0038 |          86.1568 |          18.9595 |
[32m[20221213 21:05:32 @agent_ppo2.py:185][0m |           0.0081 |          95.7739 |          18.9557 |
[32m[20221213 21:05:32 @agent_ppo2.py:185][0m |          -0.0028 |          85.1993 |          18.9353 |
[32m[20221213 21:05:32 @agent_ppo2.py:185][0m |          -0.0044 |          84.8175 |          18.9555 |
[32m[20221213 21:05:32 @agent_ppo2.py:185][0m |          -0.0076 |          84.6271 |          18.9582 |
[32m[20221213 21:05:32 @agent_ppo2.py:185][0m |          -0.0064 |          84.5129 |          18.9453 |
[32m[20221213 21:05:32 @agent_ppo2.py:185][0m |          -0.0075 |          84.2871 |          18.9498 |
[32m[20221213 21:05:32 @agent_ppo2.py:185][0m |          -0.0058 |          84.0017 |          18.9526 |
[32m[20221213 21:05:32 @agent_ppo2.py:185][0m |           0.0019 |          89.0916 |          18.9514 |
[32m[20221213 21:05:32 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:05:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 521.20
[32m[20221213 21:05:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.00
[32m[20221213 21:05:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 588.00
[32m[20221213 21:05:33 @agent_ppo2.py:143][0m Total time:       9.96 min
[32m[20221213 21:05:33 @agent_ppo2.py:145][0m 968704 total steps have happened
[32m[20221213 21:05:33 @agent_ppo2.py:121][0m #------------------------ Iteration 473 --------------------------#
[32m[20221213 21:05:33 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:33 @agent_ppo2.py:185][0m |          -0.0029 |          83.7243 |          18.9354 |
[32m[20221213 21:05:33 @agent_ppo2.py:185][0m |          -0.0044 |          82.7472 |          18.9231 |
[32m[20221213 21:05:33 @agent_ppo2.py:185][0m |          -0.0076 |          82.3181 |          18.9253 |
[32m[20221213 21:05:33 @agent_ppo2.py:185][0m |          -0.0061 |          82.0717 |          18.9168 |
[32m[20221213 21:05:33 @agent_ppo2.py:185][0m |          -0.0053 |          81.7264 |          18.9164 |
[32m[20221213 21:05:33 @agent_ppo2.py:185][0m |          -0.0056 |          81.3613 |          18.9191 |
[32m[20221213 21:05:34 @agent_ppo2.py:185][0m |          -0.0079 |          81.0710 |          18.9153 |
[32m[20221213 21:05:34 @agent_ppo2.py:185][0m |          -0.0048 |          80.8746 |          18.9118 |
[32m[20221213 21:05:34 @agent_ppo2.py:185][0m |           0.0008 |          84.7143 |          18.9187 |
[32m[20221213 21:05:34 @agent_ppo2.py:185][0m |          -0.0041 |          80.9510 |          18.9173 |
[32m[20221213 21:05:34 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:05:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 531.40
[32m[20221213 21:05:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.00
[32m[20221213 21:05:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 591.00
[32m[20221213 21:05:34 @agent_ppo2.py:143][0m Total time:       9.99 min
[32m[20221213 21:05:34 @agent_ppo2.py:145][0m 970752 total steps have happened
[32m[20221213 21:05:34 @agent_ppo2.py:121][0m #------------------------ Iteration 474 --------------------------#
[32m[20221213 21:05:34 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:34 @agent_ppo2.py:185][0m |           0.0003 |          84.4627 |          18.9310 |
[32m[20221213 21:05:34 @agent_ppo2.py:185][0m |           0.0041 |          85.8926 |          18.9140 |
[32m[20221213 21:05:35 @agent_ppo2.py:185][0m |          -0.0081 |          83.9446 |          18.9071 |
[32m[20221213 21:05:35 @agent_ppo2.py:185][0m |          -0.0089 |          83.7310 |          18.9080 |
[32m[20221213 21:05:35 @agent_ppo2.py:185][0m |          -0.0076 |          83.5519 |          18.9111 |
[32m[20221213 21:05:35 @agent_ppo2.py:185][0m |          -0.0068 |          83.7960 |          18.9084 |
[32m[20221213 21:05:35 @agent_ppo2.py:185][0m |          -0.0087 |          83.2584 |          18.9124 |
[32m[20221213 21:05:35 @agent_ppo2.py:185][0m |          -0.0099 |          83.1677 |          18.9161 |
[32m[20221213 21:05:35 @agent_ppo2.py:185][0m |          -0.0094 |          83.0393 |          18.9140 |
[32m[20221213 21:05:35 @agent_ppo2.py:185][0m |          -0.0086 |          82.9500 |          18.9189 |
[32m[20221213 21:05:35 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:05:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.60
[32m[20221213 21:05:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 572.00
[32m[20221213 21:05:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 646.00
[32m[20221213 21:05:35 @agent_ppo2.py:143][0m Total time:      10.01 min
[32m[20221213 21:05:35 @agent_ppo2.py:145][0m 972800 total steps have happened
[32m[20221213 21:05:35 @agent_ppo2.py:121][0m #------------------------ Iteration 475 --------------------------#
[32m[20221213 21:05:36 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:36 @agent_ppo2.py:185][0m |           0.0016 |          84.0227 |          18.8719 |
[32m[20221213 21:05:36 @agent_ppo2.py:185][0m |          -0.0035 |          83.5459 |          18.8758 |
[32m[20221213 21:05:36 @agent_ppo2.py:185][0m |          -0.0048 |          83.1100 |          18.8699 |
[32m[20221213 21:05:36 @agent_ppo2.py:185][0m |          -0.0074 |          82.9886 |          18.8681 |
[32m[20221213 21:05:36 @agent_ppo2.py:185][0m |          -0.0036 |          82.5973 |          18.8702 |
[32m[20221213 21:05:36 @agent_ppo2.py:185][0m |           0.0073 |          91.3490 |          18.8669 |
[32m[20221213 21:05:36 @agent_ppo2.py:185][0m |           0.0062 |          86.2839 |          18.8619 |
[32m[20221213 21:05:37 @agent_ppo2.py:185][0m |          -0.0059 |          82.1834 |          18.8608 |
[32m[20221213 21:05:37 @agent_ppo2.py:185][0m |          -0.0040 |          81.9886 |          18.8619 |
[32m[20221213 21:05:37 @agent_ppo2.py:185][0m |          -0.0073 |          81.7871 |          18.8644 |
[32m[20221213 21:05:37 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:05:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.80
[32m[20221213 21:05:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 541.00
[32m[20221213 21:05:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 602.00
[32m[20221213 21:05:37 @agent_ppo2.py:143][0m Total time:      10.04 min
[32m[20221213 21:05:37 @agent_ppo2.py:145][0m 974848 total steps have happened
[32m[20221213 21:05:37 @agent_ppo2.py:121][0m #------------------------ Iteration 476 --------------------------#
[32m[20221213 21:05:37 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:37 @agent_ppo2.py:185][0m |          -0.0025 |          82.7388 |          18.9749 |
[32m[20221213 21:05:37 @agent_ppo2.py:185][0m |          -0.0014 |          81.8085 |          18.9642 |
[32m[20221213 21:05:37 @agent_ppo2.py:185][0m |           0.0020 |          81.9621 |          18.9427 |
[32m[20221213 21:05:38 @agent_ppo2.py:185][0m |          -0.0081 |          80.1304 |          18.9307 |
[32m[20221213 21:05:38 @agent_ppo2.py:185][0m |          -0.0057 |          79.8141 |          18.9434 |
[32m[20221213 21:05:38 @agent_ppo2.py:185][0m |          -0.0072 |          79.5715 |          18.9312 |
[32m[20221213 21:05:38 @agent_ppo2.py:185][0m |           0.0009 |          84.2140 |          18.9330 |
[32m[20221213 21:05:38 @agent_ppo2.py:185][0m |          -0.0072 |          79.1351 |          18.9234 |
[32m[20221213 21:05:38 @agent_ppo2.py:185][0m |          -0.0094 |          79.1687 |          18.9356 |
[32m[20221213 21:05:38 @agent_ppo2.py:185][0m |          -0.0055 |          78.7954 |          18.9262 |
[32m[20221213 21:05:38 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:05:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.80
[32m[20221213 21:05:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.00
[32m[20221213 21:05:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.00
[32m[20221213 21:05:38 @agent_ppo2.py:143][0m Total time:      10.06 min
[32m[20221213 21:05:38 @agent_ppo2.py:145][0m 976896 total steps have happened
[32m[20221213 21:05:38 @agent_ppo2.py:121][0m #------------------------ Iteration 477 --------------------------#
[32m[20221213 21:05:38 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:39 @agent_ppo2.py:185][0m |           0.0085 |          85.6643 |          19.0992 |
[32m[20221213 21:05:39 @agent_ppo2.py:185][0m |          -0.0034 |          81.0257 |          19.0923 |
[32m[20221213 21:05:39 @agent_ppo2.py:185][0m |          -0.0082 |          80.4052 |          19.0819 |
[32m[20221213 21:05:39 @agent_ppo2.py:185][0m |          -0.0048 |          79.8967 |          19.0779 |
[32m[20221213 21:05:39 @agent_ppo2.py:185][0m |          -0.0005 |          80.8441 |          19.0639 |
[32m[20221213 21:05:39 @agent_ppo2.py:185][0m |          -0.0096 |          79.2863 |          19.0542 |
[32m[20221213 21:05:39 @agent_ppo2.py:185][0m |          -0.0062 |          78.7819 |          19.0558 |
[32m[20221213 21:05:39 @agent_ppo2.py:185][0m |           0.0027 |          85.6098 |          19.0530 |
[32m[20221213 21:05:39 @agent_ppo2.py:185][0m |          -0.0084 |          78.2476 |          19.0438 |
[32m[20221213 21:05:39 @agent_ppo2.py:185][0m |          -0.0083 |          78.1045 |          19.0317 |
[32m[20221213 21:05:39 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:05:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 559.60
[32m[20221213 21:05:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 619.00
[32m[20221213 21:05:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 634.00
[32m[20221213 21:05:40 @agent_ppo2.py:143][0m Total time:      10.08 min
[32m[20221213 21:05:40 @agent_ppo2.py:145][0m 978944 total steps have happened
[32m[20221213 21:05:40 @agent_ppo2.py:121][0m #------------------------ Iteration 478 --------------------------#
[32m[20221213 21:05:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:05:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:40 @agent_ppo2.py:185][0m |          -0.0017 |          87.4024 |          19.0823 |
[32m[20221213 21:05:40 @agent_ppo2.py:185][0m |           0.0035 |          86.7996 |          19.0679 |
[32m[20221213 21:05:40 @agent_ppo2.py:185][0m |          -0.0049 |          85.7355 |          19.0584 |
[32m[20221213 21:05:40 @agent_ppo2.py:185][0m |          -0.0055 |          85.4403 |          19.0329 |
[32m[20221213 21:05:40 @agent_ppo2.py:185][0m |          -0.0036 |          85.2698 |          19.0430 |
[32m[20221213 21:05:40 @agent_ppo2.py:185][0m |           0.0034 |          88.8774 |          19.0465 |
[32m[20221213 21:05:40 @agent_ppo2.py:185][0m |          -0.0058 |          84.8006 |          19.0368 |
[32m[20221213 21:05:41 @agent_ppo2.py:185][0m |          -0.0080 |          84.8107 |          19.0371 |
[32m[20221213 21:05:41 @agent_ppo2.py:185][0m |          -0.0061 |          84.5872 |          19.0307 |
[32m[20221213 21:05:41 @agent_ppo2.py:185][0m |          -0.0074 |          84.4968 |          19.0378 |
[32m[20221213 21:05:41 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:05:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 552.20
[32m[20221213 21:05:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 580.00
[32m[20221213 21:05:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 586.00
[32m[20221213 21:05:41 @agent_ppo2.py:143][0m Total time:      10.10 min
[32m[20221213 21:05:41 @agent_ppo2.py:145][0m 980992 total steps have happened
[32m[20221213 21:05:41 @agent_ppo2.py:121][0m #------------------------ Iteration 479 --------------------------#
[32m[20221213 21:05:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:05:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:41 @agent_ppo2.py:185][0m |          -0.0010 |          85.8836 |          18.9150 |
[32m[20221213 21:05:41 @agent_ppo2.py:185][0m |          -0.0048 |          85.3746 |          18.8992 |
[32m[20221213 21:05:41 @agent_ppo2.py:185][0m |          -0.0062 |          84.9952 |          18.8998 |
[32m[20221213 21:05:42 @agent_ppo2.py:185][0m |          -0.0026 |          84.8491 |          18.8957 |
[32m[20221213 21:05:42 @agent_ppo2.py:185][0m |           0.0013 |          87.6479 |          18.8928 |
[32m[20221213 21:05:42 @agent_ppo2.py:185][0m |          -0.0055 |          84.4390 |          18.8833 |
[32m[20221213 21:05:42 @agent_ppo2.py:185][0m |          -0.0024 |          84.1294 |          18.8995 |
[32m[20221213 21:05:42 @agent_ppo2.py:185][0m |          -0.0007 |          84.6052 |          18.8944 |
[32m[20221213 21:05:42 @agent_ppo2.py:185][0m |           0.0009 |          86.0664 |          18.8889 |
[32m[20221213 21:05:42 @agent_ppo2.py:185][0m |          -0.0074 |          83.8880 |          18.8824 |
[32m[20221213 21:05:42 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:05:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 558.80
[32m[20221213 21:05:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.00
[32m[20221213 21:05:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 611.00
[32m[20221213 21:05:42 @agent_ppo2.py:143][0m Total time:      10.13 min
[32m[20221213 21:05:42 @agent_ppo2.py:145][0m 983040 total steps have happened
[32m[20221213 21:05:42 @agent_ppo2.py:121][0m #------------------------ Iteration 480 --------------------------#
[32m[20221213 21:05:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:05:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:43 @agent_ppo2.py:185][0m |          -0.0010 |          88.3305 |          19.1380 |
[32m[20221213 21:05:43 @agent_ppo2.py:185][0m |           0.0104 |          98.9705 |          19.1314 |
[32m[20221213 21:05:43 @agent_ppo2.py:185][0m |          -0.0063 |          87.6751 |          19.1150 |
[32m[20221213 21:05:43 @agent_ppo2.py:185][0m |          -0.0063 |          87.0421 |          19.1186 |
[32m[20221213 21:05:43 @agent_ppo2.py:185][0m |           0.0051 |          94.8367 |          19.1224 |
[32m[20221213 21:05:43 @agent_ppo2.py:185][0m |          -0.0048 |          86.8391 |          19.1251 |
[32m[20221213 21:05:43 @agent_ppo2.py:185][0m |          -0.0067 |          86.3820 |          19.1180 |
[32m[20221213 21:05:43 @agent_ppo2.py:185][0m |          -0.0038 |          86.7590 |          19.1183 |
[32m[20221213 21:05:43 @agent_ppo2.py:185][0m |          -0.0048 |          87.5327 |          19.1178 |
[32m[20221213 21:05:43 @agent_ppo2.py:185][0m |          -0.0073 |          85.9734 |          19.1187 |
[32m[20221213 21:05:43 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:05:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.00
[32m[20221213 21:05:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.00
[32m[20221213 21:05:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.00
[32m[20221213 21:05:43 @agent_ppo2.py:143][0m Total time:      10.15 min
[32m[20221213 21:05:43 @agent_ppo2.py:145][0m 985088 total steps have happened
[32m[20221213 21:05:43 @agent_ppo2.py:121][0m #------------------------ Iteration 481 --------------------------#
[32m[20221213 21:05:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:05:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:44 @agent_ppo2.py:185][0m |          -0.0017 |          88.5218 |          18.9694 |
[32m[20221213 21:05:44 @agent_ppo2.py:185][0m |          -0.0025 |          87.8153 |          18.9633 |
[32m[20221213 21:05:44 @agent_ppo2.py:185][0m |          -0.0056 |          87.2292 |          18.9523 |
[32m[20221213 21:05:44 @agent_ppo2.py:185][0m |          -0.0062 |          86.9731 |          18.9506 |
[32m[20221213 21:05:44 @agent_ppo2.py:185][0m |          -0.0068 |          86.6090 |          18.9463 |
[32m[20221213 21:05:44 @agent_ppo2.py:185][0m |          -0.0046 |          86.3701 |          18.9453 |
[32m[20221213 21:05:44 @agent_ppo2.py:185][0m |          -0.0018 |          86.3699 |          18.9452 |
[32m[20221213 21:05:44 @agent_ppo2.py:185][0m |          -0.0068 |          86.0268 |          18.9448 |
[32m[20221213 21:05:44 @agent_ppo2.py:185][0m |          -0.0081 |          85.9638 |          18.9361 |
[32m[20221213 21:05:45 @agent_ppo2.py:185][0m |          -0.0087 |          85.8900 |          18.9402 |
[32m[20221213 21:05:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:05:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.20
[32m[20221213 21:05:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 557.00
[32m[20221213 21:05:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 602.00
[32m[20221213 21:05:45 @agent_ppo2.py:143][0m Total time:      10.17 min
[32m[20221213 21:05:45 @agent_ppo2.py:145][0m 987136 total steps have happened
[32m[20221213 21:05:45 @agent_ppo2.py:121][0m #------------------------ Iteration 482 --------------------------#
[32m[20221213 21:05:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:05:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:45 @agent_ppo2.py:185][0m |          -0.0023 |          89.7607 |          18.9724 |
[32m[20221213 21:05:45 @agent_ppo2.py:185][0m |           0.0039 |          93.1309 |          18.9590 |
[32m[20221213 21:05:45 @agent_ppo2.py:185][0m |          -0.0063 |          88.4969 |          18.9439 |
[32m[20221213 21:05:45 @agent_ppo2.py:185][0m |          -0.0053 |          89.1670 |          18.9418 |
[32m[20221213 21:05:45 @agent_ppo2.py:185][0m |           0.0071 |         100.2825 |          18.9448 |
[32m[20221213 21:05:45 @agent_ppo2.py:185][0m |          -0.0079 |          88.0287 |          18.9389 |
[32m[20221213 21:05:46 @agent_ppo2.py:185][0m |          -0.0084 |          87.8756 |          18.9367 |
[32m[20221213 21:05:46 @agent_ppo2.py:185][0m |          -0.0100 |          87.7675 |          18.9347 |
[32m[20221213 21:05:46 @agent_ppo2.py:185][0m |          -0.0085 |          87.9468 |          18.9422 |
[32m[20221213 21:05:46 @agent_ppo2.py:185][0m |          -0.0054 |          88.1278 |          18.9387 |
[32m[20221213 21:05:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:05:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.20
[32m[20221213 21:05:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 569.00
[32m[20221213 21:05:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.00
[32m[20221213 21:05:46 @agent_ppo2.py:143][0m Total time:      10.19 min
[32m[20221213 21:05:46 @agent_ppo2.py:145][0m 989184 total steps have happened
[32m[20221213 21:05:46 @agent_ppo2.py:121][0m #------------------------ Iteration 483 --------------------------#
[32m[20221213 21:05:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:05:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:46 @agent_ppo2.py:185][0m |           0.0005 |          88.1487 |          19.1395 |
[32m[20221213 21:05:46 @agent_ppo2.py:185][0m |          -0.0035 |          86.0320 |          19.1228 |
[32m[20221213 21:05:46 @agent_ppo2.py:185][0m |          -0.0086 |          85.6902 |          19.1208 |
[32m[20221213 21:05:46 @agent_ppo2.py:185][0m |          -0.0067 |          85.2942 |          19.1192 |
[32m[20221213 21:05:47 @agent_ppo2.py:185][0m |          -0.0026 |          86.6119 |          19.1208 |
[32m[20221213 21:05:47 @agent_ppo2.py:185][0m |          -0.0081 |          84.6362 |          19.1240 |
[32m[20221213 21:05:47 @agent_ppo2.py:185][0m |          -0.0080 |          84.5633 |          19.1089 |
[32m[20221213 21:05:47 @agent_ppo2.py:185][0m |          -0.0091 |          84.3088 |          19.1252 |
[32m[20221213 21:05:47 @agent_ppo2.py:185][0m |          -0.0124 |          84.3427 |          19.1136 |
[32m[20221213 21:05:47 @agent_ppo2.py:185][0m |          -0.0117 |          84.4109 |          19.1275 |
[32m[20221213 21:05:47 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:05:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 521.40
[32m[20221213 21:05:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.00
[32m[20221213 21:05:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 601.00
[32m[20221213 21:05:47 @agent_ppo2.py:143][0m Total time:      10.21 min
[32m[20221213 21:05:47 @agent_ppo2.py:145][0m 991232 total steps have happened
[32m[20221213 21:05:47 @agent_ppo2.py:121][0m #------------------------ Iteration 484 --------------------------#
[32m[20221213 21:05:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:05:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:47 @agent_ppo2.py:185][0m |           0.0179 |         103.1442 |          19.1214 |
[32m[20221213 21:05:47 @agent_ppo2.py:185][0m |          -0.0033 |          88.1708 |          19.1028 |
[32m[20221213 21:05:48 @agent_ppo2.py:185][0m |          -0.0041 |          87.6206 |          19.0896 |
[32m[20221213 21:05:48 @agent_ppo2.py:185][0m |          -0.0026 |          87.6105 |          19.0958 |
[32m[20221213 21:05:48 @agent_ppo2.py:185][0m |          -0.0046 |          87.2929 |          19.0887 |
[32m[20221213 21:05:48 @agent_ppo2.py:185][0m |          -0.0072 |          87.0704 |          19.0920 |
[32m[20221213 21:05:48 @agent_ppo2.py:185][0m |           0.0048 |          97.9096 |          19.0821 |
[32m[20221213 21:05:48 @agent_ppo2.py:185][0m |          -0.0056 |          86.9517 |          19.0507 |
[32m[20221213 21:05:48 @agent_ppo2.py:185][0m |          -0.0062 |          86.7019 |          19.0639 |
[32m[20221213 21:05:48 @agent_ppo2.py:185][0m |          -0.0093 |          86.7135 |          19.0694 |
[32m[20221213 21:05:48 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:05:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 557.60
[32m[20221213 21:05:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 579.00
[32m[20221213 21:05:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 568.00
[32m[20221213 21:05:48 @agent_ppo2.py:143][0m Total time:      10.23 min
[32m[20221213 21:05:48 @agent_ppo2.py:145][0m 993280 total steps have happened
[32m[20221213 21:05:48 @agent_ppo2.py:121][0m #------------------------ Iteration 485 --------------------------#
[32m[20221213 21:05:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:05:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:49 @agent_ppo2.py:185][0m |           0.0003 |          85.9549 |          18.9899 |
[32m[20221213 21:05:49 @agent_ppo2.py:185][0m |           0.0016 |          86.6364 |          18.9718 |
[32m[20221213 21:05:49 @agent_ppo2.py:185][0m |          -0.0037 |          85.0148 |          18.9642 |
[32m[20221213 21:05:49 @agent_ppo2.py:185][0m |          -0.0024 |          84.9999 |          18.9600 |
[32m[20221213 21:05:49 @agent_ppo2.py:185][0m |          -0.0043 |          84.2918 |          18.9580 |
[32m[20221213 21:05:49 @agent_ppo2.py:185][0m |           0.0036 |          87.2082 |          18.9517 |
[32m[20221213 21:05:49 @agent_ppo2.py:185][0m |          -0.0052 |          83.9001 |          18.9576 |
[32m[20221213 21:05:50 @agent_ppo2.py:185][0m |          -0.0060 |          83.6842 |          18.9511 |
[32m[20221213 21:05:50 @agent_ppo2.py:185][0m |          -0.0059 |          83.4972 |          18.9530 |
[32m[20221213 21:05:50 @agent_ppo2.py:185][0m |          -0.0069 |          83.3474 |          18.9472 |
[32m[20221213 21:05:50 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:05:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.00
[32m[20221213 21:05:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 615.00
[32m[20221213 21:05:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 578.00
[32m[20221213 21:05:50 @agent_ppo2.py:143][0m Total time:      10.25 min
[32m[20221213 21:05:50 @agent_ppo2.py:145][0m 995328 total steps have happened
[32m[20221213 21:05:50 @agent_ppo2.py:121][0m #------------------------ Iteration 486 --------------------------#
[32m[20221213 21:05:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:05:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:50 @agent_ppo2.py:185][0m |          -0.0047 |          90.7102 |          19.0744 |
[32m[20221213 21:05:50 @agent_ppo2.py:185][0m |          -0.0068 |          89.4397 |          19.0609 |
[32m[20221213 21:05:50 @agent_ppo2.py:185][0m |          -0.0067 |          89.0147 |          19.0557 |
[32m[20221213 21:05:50 @agent_ppo2.py:185][0m |          -0.0090 |          88.8364 |          19.0365 |
[32m[20221213 21:05:51 @agent_ppo2.py:185][0m |          -0.0088 |          88.5180 |          19.0408 |
[32m[20221213 21:05:51 @agent_ppo2.py:185][0m |          -0.0062 |          88.3162 |          19.0482 |
[32m[20221213 21:05:51 @agent_ppo2.py:185][0m |           0.0039 |          91.8519 |          19.0427 |
[32m[20221213 21:05:51 @agent_ppo2.py:185][0m |          -0.0098 |          87.9616 |          19.0510 |
[32m[20221213 21:05:51 @agent_ppo2.py:185][0m |          -0.0085 |          87.7819 |          19.0349 |
[32m[20221213 21:05:51 @agent_ppo2.py:185][0m |          -0.0041 |          91.2091 |          19.0429 |
[32m[20221213 21:05:51 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:05:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 559.80
[32m[20221213 21:05:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.00
[32m[20221213 21:05:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 572.00
[32m[20221213 21:05:51 @agent_ppo2.py:143][0m Total time:      10.27 min
[32m[20221213 21:05:51 @agent_ppo2.py:145][0m 997376 total steps have happened
[32m[20221213 21:05:51 @agent_ppo2.py:121][0m #------------------------ Iteration 487 --------------------------#
[32m[20221213 21:05:51 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:05:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:51 @agent_ppo2.py:185][0m |          -0.0026 |          91.5558 |          19.0083 |
[32m[20221213 21:05:52 @agent_ppo2.py:185][0m |          -0.0041 |          90.4656 |          19.0070 |
[32m[20221213 21:05:52 @agent_ppo2.py:185][0m |           0.0008 |          92.8883 |          19.0004 |
[32m[20221213 21:05:52 @agent_ppo2.py:185][0m |          -0.0055 |          89.7742 |          18.9946 |
[32m[20221213 21:05:52 @agent_ppo2.py:185][0m |          -0.0053 |          89.6659 |          19.0009 |
[32m[20221213 21:05:52 @agent_ppo2.py:185][0m |          -0.0061 |          89.4589 |          18.9983 |
[32m[20221213 21:05:52 @agent_ppo2.py:185][0m |          -0.0024 |          91.9084 |          19.0036 |
[32m[20221213 21:05:52 @agent_ppo2.py:185][0m |          -0.0067 |          89.1542 |          19.0060 |
[32m[20221213 21:05:52 @agent_ppo2.py:185][0m |           0.0038 |          99.3119 |          18.9992 |
[32m[20221213 21:05:52 @agent_ppo2.py:185][0m |          -0.0012 |          90.9451 |          19.0017 |
[32m[20221213 21:05:52 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:05:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 566.00
[32m[20221213 21:05:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.00
[32m[20221213 21:05:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.00
[32m[20221213 21:05:52 @agent_ppo2.py:143][0m Total time:      10.30 min
[32m[20221213 21:05:52 @agent_ppo2.py:145][0m 999424 total steps have happened
[32m[20221213 21:05:52 @agent_ppo2.py:121][0m #------------------------ Iteration 488 --------------------------#
[32m[20221213 21:05:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:05:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:53 @agent_ppo2.py:185][0m |          -0.0011 |          90.7591 |          19.0133 |
[32m[20221213 21:05:53 @agent_ppo2.py:185][0m |           0.0041 |          91.2335 |          18.9990 |
[32m[20221213 21:05:53 @agent_ppo2.py:185][0m |          -0.0035 |          89.5100 |          19.0024 |
[32m[20221213 21:05:53 @agent_ppo2.py:185][0m |          -0.0053 |          89.2326 |          18.9907 |
[32m[20221213 21:05:53 @agent_ppo2.py:185][0m |          -0.0030 |          89.0472 |          18.9950 |
[32m[20221213 21:05:53 @agent_ppo2.py:185][0m |          -0.0039 |          88.9822 |          18.9938 |
[32m[20221213 21:05:53 @agent_ppo2.py:185][0m |           0.0038 |          92.8078 |          18.9802 |
[32m[20221213 21:05:53 @agent_ppo2.py:185][0m |          -0.0062 |          88.6874 |          18.9772 |
[32m[20221213 21:05:54 @agent_ppo2.py:185][0m |          -0.0026 |          89.4920 |          18.9771 |
[32m[20221213 21:05:54 @agent_ppo2.py:185][0m |          -0.0013 |          89.9618 |          18.9689 |
[32m[20221213 21:05:54 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:05:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 550.40
[32m[20221213 21:05:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 593.00
[32m[20221213 21:05:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.00
[32m[20221213 21:05:54 @agent_ppo2.py:143][0m Total time:      10.32 min
[32m[20221213 21:05:54 @agent_ppo2.py:145][0m 1001472 total steps have happened
[32m[20221213 21:05:54 @agent_ppo2.py:121][0m #------------------------ Iteration 489 --------------------------#
[32m[20221213 21:05:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:05:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:54 @agent_ppo2.py:185][0m |          -0.0009 |          86.5214 |          19.1033 |
[32m[20221213 21:05:54 @agent_ppo2.py:185][0m |           0.0105 |          96.0358 |          19.0667 |
[32m[20221213 21:05:54 @agent_ppo2.py:185][0m |          -0.0051 |          85.4949 |          19.0726 |
[32m[20221213 21:05:54 @agent_ppo2.py:185][0m |          -0.0055 |          85.1880 |          19.0583 |
[32m[20221213 21:05:55 @agent_ppo2.py:185][0m |          -0.0067 |          85.0134 |          19.0624 |
[32m[20221213 21:05:55 @agent_ppo2.py:185][0m |          -0.0055 |          84.9063 |          19.0582 |
[32m[20221213 21:05:55 @agent_ppo2.py:185][0m |          -0.0061 |          84.6227 |          19.0544 |
[32m[20221213 21:05:55 @agent_ppo2.py:185][0m |          -0.0084 |          84.5114 |          19.0415 |
[32m[20221213 21:05:55 @agent_ppo2.py:185][0m |          -0.0073 |          84.4104 |          19.0428 |
[32m[20221213 21:05:55 @agent_ppo2.py:185][0m |          -0.0057 |          84.2892 |          19.0350 |
[32m[20221213 21:05:55 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 21:05:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.40
[32m[20221213 21:05:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 620.00
[32m[20221213 21:05:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 574.00
[32m[20221213 21:05:55 @agent_ppo2.py:143][0m Total time:      10.34 min
[32m[20221213 21:05:55 @agent_ppo2.py:145][0m 1003520 total steps have happened
[32m[20221213 21:05:55 @agent_ppo2.py:121][0m #------------------------ Iteration 490 --------------------------#
[32m[20221213 21:05:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:05:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:56 @agent_ppo2.py:185][0m |          -0.0026 |          84.5917 |          19.0327 |
[32m[20221213 21:05:56 @agent_ppo2.py:185][0m |          -0.0069 |          83.1059 |          19.0033 |
[32m[20221213 21:05:56 @agent_ppo2.py:185][0m |          -0.0070 |          82.3102 |          18.9963 |
[32m[20221213 21:05:56 @agent_ppo2.py:185][0m |          -0.0047 |          82.4828 |          18.9866 |
[32m[20221213 21:05:56 @agent_ppo2.py:185][0m |          -0.0081 |          81.8349 |          18.9816 |
[32m[20221213 21:05:56 @agent_ppo2.py:185][0m |          -0.0071 |          81.3580 |          18.9837 |
[32m[20221213 21:05:56 @agent_ppo2.py:185][0m |          -0.0095 |          81.1389 |          18.9707 |
[32m[20221213 21:05:56 @agent_ppo2.py:185][0m |          -0.0095 |          81.1056 |          18.9654 |
[32m[20221213 21:05:56 @agent_ppo2.py:185][0m |          -0.0104 |          80.9804 |          18.9682 |
[32m[20221213 21:05:56 @agent_ppo2.py:185][0m |          -0.0092 |          80.7803 |          18.9600 |
[32m[20221213 21:05:56 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:05:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 534.80
[32m[20221213 21:05:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.00
[32m[20221213 21:05:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 605.00
[32m[20221213 21:05:57 @agent_ppo2.py:143][0m Total time:      10.36 min
[32m[20221213 21:05:57 @agent_ppo2.py:145][0m 1005568 total steps have happened
[32m[20221213 21:05:57 @agent_ppo2.py:121][0m #------------------------ Iteration 491 --------------------------#
[32m[20221213 21:05:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:05:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:57 @agent_ppo2.py:185][0m |          -0.0044 |          90.8837 |          19.1181 |
[32m[20221213 21:05:57 @agent_ppo2.py:185][0m |          -0.0042 |          90.7248 |          19.0725 |
[32m[20221213 21:05:57 @agent_ppo2.py:185][0m |           0.0013 |          93.5092 |          19.0686 |
[32m[20221213 21:05:57 @agent_ppo2.py:185][0m |          -0.0086 |          88.9168 |          19.0562 |
[32m[20221213 21:05:57 @agent_ppo2.py:185][0m |           0.0049 |          97.9718 |          19.0532 |
[32m[20221213 21:05:57 @agent_ppo2.py:185][0m |          -0.0082 |          88.3341 |          19.0405 |
[32m[20221213 21:05:57 @agent_ppo2.py:185][0m |          -0.0080 |          88.0918 |          19.0539 |
[32m[20221213 21:05:58 @agent_ppo2.py:185][0m |          -0.0084 |          87.9584 |          19.0590 |
[32m[20221213 21:05:58 @agent_ppo2.py:185][0m |          -0.0107 |          87.8505 |          19.0610 |
[32m[20221213 21:05:58 @agent_ppo2.py:185][0m |          -0.0067 |          88.5282 |          19.0638 |
[32m[20221213 21:05:58 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:05:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 563.80
[32m[20221213 21:05:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 590.00
[32m[20221213 21:05:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 608.00
[32m[20221213 21:05:58 @agent_ppo2.py:143][0m Total time:      10.39 min
[32m[20221213 21:05:58 @agent_ppo2.py:145][0m 1007616 total steps have happened
[32m[20221213 21:05:58 @agent_ppo2.py:121][0m #------------------------ Iteration 492 --------------------------#
[32m[20221213 21:05:58 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:05:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:58 @agent_ppo2.py:185][0m |           0.0000 |          87.9227 |          19.0111 |
[32m[20221213 21:05:58 @agent_ppo2.py:185][0m |          -0.0056 |          87.1554 |          19.0027 |
[32m[20221213 21:05:58 @agent_ppo2.py:185][0m |          -0.0066 |          86.9054 |          18.9970 |
[32m[20221213 21:05:58 @agent_ppo2.py:185][0m |          -0.0035 |          87.4117 |          18.9948 |
[32m[20221213 21:05:59 @agent_ppo2.py:185][0m |          -0.0097 |          86.5461 |          18.9964 |
[32m[20221213 21:05:59 @agent_ppo2.py:185][0m |          -0.0051 |          86.2494 |          18.9972 |
[32m[20221213 21:05:59 @agent_ppo2.py:185][0m |          -0.0087 |          86.2443 |          18.9993 |
[32m[20221213 21:05:59 @agent_ppo2.py:185][0m |          -0.0080 |          86.1521 |          18.9881 |
[32m[20221213 21:05:59 @agent_ppo2.py:185][0m |          -0.0086 |          86.1545 |          18.9939 |
[32m[20221213 21:05:59 @agent_ppo2.py:185][0m |          -0.0101 |          86.0036 |          18.9976 |
[32m[20221213 21:05:59 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 21:05:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.40
[32m[20221213 21:05:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 576.00
[32m[20221213 21:05:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.00
[32m[20221213 21:05:59 @agent_ppo2.py:143][0m Total time:      10.41 min
[32m[20221213 21:05:59 @agent_ppo2.py:145][0m 1009664 total steps have happened
[32m[20221213 21:05:59 @agent_ppo2.py:121][0m #------------------------ Iteration 493 --------------------------#
[32m[20221213 21:05:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:05:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:05:59 @agent_ppo2.py:185][0m |          -0.0031 |          87.6752 |          19.1238 |
[32m[20221213 21:06:00 @agent_ppo2.py:185][0m |           0.0003 |          88.1939 |          19.1013 |
[32m[20221213 21:06:00 @agent_ppo2.py:185][0m |          -0.0075 |          86.1047 |          19.1007 |
[32m[20221213 21:06:00 @agent_ppo2.py:185][0m |          -0.0075 |          85.4989 |          19.0988 |
[32m[20221213 21:06:00 @agent_ppo2.py:185][0m |          -0.0040 |          87.2285 |          19.1030 |
[32m[20221213 21:06:00 @agent_ppo2.py:185][0m |          -0.0083 |          84.9732 |          19.0990 |
[32m[20221213 21:06:00 @agent_ppo2.py:185][0m |          -0.0068 |          84.8333 |          19.1047 |
[32m[20221213 21:06:00 @agent_ppo2.py:185][0m |          -0.0110 |          84.5223 |          19.1012 |
[32m[20221213 21:06:00 @agent_ppo2.py:185][0m |          -0.0068 |          84.2629 |          19.0987 |
[32m[20221213 21:06:00 @agent_ppo2.py:185][0m |          -0.0095 |          84.3193 |          19.0968 |
[32m[20221213 21:06:00 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:06:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 530.20
[32m[20221213 21:06:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.00
[32m[20221213 21:06:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 547.00
[32m[20221213 21:06:00 @agent_ppo2.py:143][0m Total time:      10.43 min
[32m[20221213 21:06:00 @agent_ppo2.py:145][0m 1011712 total steps have happened
[32m[20221213 21:06:00 @agent_ppo2.py:121][0m #------------------------ Iteration 494 --------------------------#
[32m[20221213 21:06:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:06:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:01 @agent_ppo2.py:185][0m |          -0.0008 |          87.5442 |          19.2508 |
[32m[20221213 21:06:01 @agent_ppo2.py:185][0m |          -0.0022 |          86.5861 |          19.2317 |
[32m[20221213 21:06:01 @agent_ppo2.py:185][0m |          -0.0042 |          86.1313 |          19.2220 |
[32m[20221213 21:06:01 @agent_ppo2.py:185][0m |          -0.0071 |          85.8125 |          19.2215 |
[32m[20221213 21:06:01 @agent_ppo2.py:185][0m |          -0.0059 |          85.7022 |          19.2069 |
[32m[20221213 21:06:01 @agent_ppo2.py:185][0m |          -0.0092 |          85.5000 |          19.2112 |
[32m[20221213 21:06:01 @agent_ppo2.py:185][0m |          -0.0010 |          88.8493 |          19.2072 |
[32m[20221213 21:06:01 @agent_ppo2.py:185][0m |          -0.0091 |          85.2845 |          19.2017 |
[32m[20221213 21:06:01 @agent_ppo2.py:185][0m |           0.0009 |          86.6869 |          19.2045 |
[32m[20221213 21:06:02 @agent_ppo2.py:185][0m |          -0.0073 |          84.8879 |          19.2032 |
[32m[20221213 21:06:02 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:06:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.40
[32m[20221213 21:06:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.00
[32m[20221213 21:06:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 579.00
[32m[20221213 21:06:02 @agent_ppo2.py:143][0m Total time:      10.45 min
[32m[20221213 21:06:02 @agent_ppo2.py:145][0m 1013760 total steps have happened
[32m[20221213 21:06:02 @agent_ppo2.py:121][0m #------------------------ Iteration 495 --------------------------#
[32m[20221213 21:06:02 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:06:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:02 @agent_ppo2.py:185][0m |          -0.0048 |          88.8068 |          19.1401 |
[32m[20221213 21:06:02 @agent_ppo2.py:185][0m |          -0.0029 |          87.8451 |          19.1225 |
[32m[20221213 21:06:02 @agent_ppo2.py:185][0m |          -0.0071 |          87.3213 |          19.1243 |
[32m[20221213 21:06:02 @agent_ppo2.py:185][0m |          -0.0050 |          86.9170 |          19.1143 |
[32m[20221213 21:06:02 @agent_ppo2.py:185][0m |          -0.0060 |          86.5988 |          19.1179 |
[32m[20221213 21:06:02 @agent_ppo2.py:185][0m |          -0.0082 |          86.4216 |          19.1266 |
[32m[20221213 21:06:03 @agent_ppo2.py:185][0m |          -0.0044 |          86.6876 |          19.1131 |
[32m[20221213 21:06:03 @agent_ppo2.py:185][0m |          -0.0060 |          86.5749 |          19.1185 |
[32m[20221213 21:06:03 @agent_ppo2.py:185][0m |          -0.0086 |          86.0171 |          19.1055 |
[32m[20221213 21:06:03 @agent_ppo2.py:185][0m |          -0.0073 |          86.0648 |          19.1132 |
[32m[20221213 21:06:03 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:06:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 544.80
[32m[20221213 21:06:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 610.00
[32m[20221213 21:06:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 601.00
[32m[20221213 21:06:03 @agent_ppo2.py:143][0m Total time:      10.47 min
[32m[20221213 21:06:03 @agent_ppo2.py:145][0m 1015808 total steps have happened
[32m[20221213 21:06:03 @agent_ppo2.py:121][0m #------------------------ Iteration 496 --------------------------#
[32m[20221213 21:06:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:06:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:03 @agent_ppo2.py:185][0m |          -0.0008 |          89.5199 |          19.0981 |
[32m[20221213 21:06:03 @agent_ppo2.py:185][0m |          -0.0043 |          88.8305 |          19.0886 |
[32m[20221213 21:06:03 @agent_ppo2.py:185][0m |          -0.0040 |          88.4605 |          19.0823 |
[32m[20221213 21:06:04 @agent_ppo2.py:185][0m |          -0.0059 |          88.1797 |          19.0786 |
[32m[20221213 21:06:04 @agent_ppo2.py:185][0m |          -0.0052 |          87.7733 |          19.0663 |
[32m[20221213 21:06:04 @agent_ppo2.py:185][0m |          -0.0067 |          87.4694 |          19.0741 |
[32m[20221213 21:06:04 @agent_ppo2.py:185][0m |          -0.0072 |          87.3135 |          19.0741 |
[32m[20221213 21:06:04 @agent_ppo2.py:185][0m |          -0.0073 |          87.1780 |          19.0710 |
[32m[20221213 21:06:04 @agent_ppo2.py:185][0m |          -0.0076 |          87.0100 |          19.0747 |
[32m[20221213 21:06:04 @agent_ppo2.py:185][0m |          -0.0042 |          87.5172 |          19.0663 |
[32m[20221213 21:06:04 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:06:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.60
[32m[20221213 21:06:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 585.00
[32m[20221213 21:06:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 594.00
[32m[20221213 21:06:04 @agent_ppo2.py:143][0m Total time:      10.49 min
[32m[20221213 21:06:04 @agent_ppo2.py:145][0m 1017856 total steps have happened
[32m[20221213 21:06:04 @agent_ppo2.py:121][0m #------------------------ Iteration 497 --------------------------#
[32m[20221213 21:06:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:06:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:04 @agent_ppo2.py:185][0m |          -0.0016 |          88.2781 |          19.0732 |
[32m[20221213 21:06:05 @agent_ppo2.py:185][0m |          -0.0030 |          87.3269 |          19.0492 |
[32m[20221213 21:06:05 @agent_ppo2.py:185][0m |           0.0131 |          98.4082 |          19.0306 |
[32m[20221213 21:06:05 @agent_ppo2.py:185][0m |          -0.0059 |          86.4490 |          19.0327 |
[32m[20221213 21:06:05 @agent_ppo2.py:185][0m |          -0.0030 |          86.9232 |          19.0340 |
[32m[20221213 21:06:05 @agent_ppo2.py:185][0m |           0.0023 |          93.2094 |          19.0406 |
[32m[20221213 21:06:05 @agent_ppo2.py:185][0m |          -0.0091 |          85.8189 |          19.0318 |
[32m[20221213 21:06:05 @agent_ppo2.py:185][0m |          -0.0081 |          85.4869 |          19.0227 |
[32m[20221213 21:06:05 @agent_ppo2.py:185][0m |          -0.0097 |          85.4480 |          19.0338 |
[32m[20221213 21:06:05 @agent_ppo2.py:185][0m |          -0.0000 |          91.2896 |          19.0259 |
[32m[20221213 21:06:05 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:06:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.20
[32m[20221213 21:06:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.00
[32m[20221213 21:06:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.00
[32m[20221213 21:06:05 @agent_ppo2.py:143][0m Total time:      10.51 min
[32m[20221213 21:06:05 @agent_ppo2.py:145][0m 1019904 total steps have happened
[32m[20221213 21:06:05 @agent_ppo2.py:121][0m #------------------------ Iteration 498 --------------------------#
[32m[20221213 21:06:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:06:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:06 @agent_ppo2.py:185][0m |          -0.0020 |          90.8854 |          19.1916 |
[32m[20221213 21:06:06 @agent_ppo2.py:185][0m |           0.0012 |          91.1952 |          19.1622 |
[32m[20221213 21:06:06 @agent_ppo2.py:185][0m |          -0.0083 |          89.1961 |          19.1534 |
[32m[20221213 21:06:06 @agent_ppo2.py:185][0m |          -0.0097 |          88.7895 |          19.1508 |
[32m[20221213 21:06:06 @agent_ppo2.py:185][0m |          -0.0098 |          88.4899 |          19.1392 |
[32m[20221213 21:06:06 @agent_ppo2.py:185][0m |           0.0005 |          94.7562 |          19.1329 |
[32m[20221213 21:06:06 @agent_ppo2.py:185][0m |          -0.0079 |          88.0231 |          19.1254 |
[32m[20221213 21:06:06 @agent_ppo2.py:185][0m |          -0.0040 |          90.1624 |          19.1293 |
[32m[20221213 21:06:06 @agent_ppo2.py:185][0m |          -0.0032 |          90.0627 |          19.1283 |
[32m[20221213 21:06:07 @agent_ppo2.py:185][0m |          -0.0112 |          87.5078 |          19.1217 |
[32m[20221213 21:06:07 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:06:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.80
[32m[20221213 21:06:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.00
[32m[20221213 21:06:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.00
[32m[20221213 21:06:07 @agent_ppo2.py:143][0m Total time:      10.53 min
[32m[20221213 21:06:07 @agent_ppo2.py:145][0m 1021952 total steps have happened
[32m[20221213 21:06:07 @agent_ppo2.py:121][0m #------------------------ Iteration 499 --------------------------#
[32m[20221213 21:06:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:06:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:07 @agent_ppo2.py:185][0m |          -0.0016 |          87.2792 |          19.1215 |
[32m[20221213 21:06:07 @agent_ppo2.py:185][0m |          -0.0023 |          86.6502 |          19.0904 |
[32m[20221213 21:06:07 @agent_ppo2.py:185][0m |          -0.0059 |          86.4671 |          19.0716 |
[32m[20221213 21:06:07 @agent_ppo2.py:185][0m |          -0.0048 |          86.2442 |          19.0569 |
[32m[20221213 21:06:07 @agent_ppo2.py:185][0m |          -0.0085 |          85.9772 |          19.0546 |
[32m[20221213 21:06:07 @agent_ppo2.py:185][0m |          -0.0064 |          85.6543 |          19.0482 |
[32m[20221213 21:06:08 @agent_ppo2.py:185][0m |          -0.0004 |          88.6332 |          19.0404 |
[32m[20221213 21:06:08 @agent_ppo2.py:185][0m |          -0.0085 |          85.5837 |          19.0358 |
[32m[20221213 21:06:08 @agent_ppo2.py:185][0m |          -0.0027 |          87.3813 |          19.0243 |
[32m[20221213 21:06:08 @agent_ppo2.py:185][0m |           0.0005 |          91.6125 |          19.0144 |
[32m[20221213 21:06:08 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:06:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 528.20
[32m[20221213 21:06:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 569.00
[32m[20221213 21:06:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.00
[32m[20221213 21:06:08 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 663.00
[32m[20221213 21:06:08 @agent_ppo2.py:143][0m Total time:      10.55 min
[32m[20221213 21:06:08 @agent_ppo2.py:145][0m 1024000 total steps have happened
[32m[20221213 21:06:08 @agent_ppo2.py:121][0m #------------------------ Iteration 500 --------------------------#
[32m[20221213 21:06:08 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:06:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:08 @agent_ppo2.py:185][0m |           0.0016 |          85.1083 |          19.0389 |
[32m[20221213 21:06:08 @agent_ppo2.py:185][0m |          -0.0026 |          84.3631 |          19.0262 |
[32m[20221213 21:06:08 @agent_ppo2.py:185][0m |          -0.0070 |          83.4073 |          19.0234 |
[32m[20221213 21:06:09 @agent_ppo2.py:185][0m |          -0.0062 |          82.4361 |          19.0206 |
[32m[20221213 21:06:09 @agent_ppo2.py:185][0m |          -0.0083 |          81.8494 |          19.0026 |
[32m[20221213 21:06:09 @agent_ppo2.py:185][0m |          -0.0012 |          82.3376 |          19.0152 |
[32m[20221213 21:06:09 @agent_ppo2.py:185][0m |          -0.0016 |          81.8554 |          19.0124 |
[32m[20221213 21:06:09 @agent_ppo2.py:185][0m |          -0.0099 |          80.6893 |          19.0191 |
[32m[20221213 21:06:09 @agent_ppo2.py:185][0m |          -0.0071 |          80.3480 |          19.0183 |
[32m[20221213 21:06:09 @agent_ppo2.py:185][0m |          -0.0028 |          81.9040 |          19.0187 |
[32m[20221213 21:06:09 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:06:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.60
[32m[20221213 21:06:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.00
[32m[20221213 21:06:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 536.00
[32m[20221213 21:06:09 @agent_ppo2.py:143][0m Total time:      10.58 min
[32m[20221213 21:06:09 @agent_ppo2.py:145][0m 1026048 total steps have happened
[32m[20221213 21:06:09 @agent_ppo2.py:121][0m #------------------------ Iteration 501 --------------------------#
[32m[20221213 21:06:09 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:06:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:10 @agent_ppo2.py:185][0m |          -0.0011 |          84.6622 |          19.2395 |
[32m[20221213 21:06:10 @agent_ppo2.py:185][0m |          -0.0036 |          83.6280 |          19.2099 |
[32m[20221213 21:06:10 @agent_ppo2.py:185][0m |           0.0005 |          83.4169 |          19.1949 |
[32m[20221213 21:06:10 @agent_ppo2.py:185][0m |          -0.0059 |          83.1053 |          19.1765 |
[32m[20221213 21:06:10 @agent_ppo2.py:185][0m |          -0.0068 |          82.7476 |          19.1693 |
[32m[20221213 21:06:10 @agent_ppo2.py:185][0m |          -0.0043 |          82.4685 |          19.1627 |
[32m[20221213 21:06:10 @agent_ppo2.py:185][0m |          -0.0036 |          82.4030 |          19.1726 |
[32m[20221213 21:06:10 @agent_ppo2.py:185][0m |          -0.0043 |          82.1332 |          19.1502 |
[32m[20221213 21:06:10 @agent_ppo2.py:185][0m |          -0.0057 |          81.9932 |          19.1619 |
[32m[20221213 21:06:10 @agent_ppo2.py:185][0m |          -0.0076 |          81.8961 |          19.1608 |
[32m[20221213 21:06:10 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:06:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.80
[32m[20221213 21:06:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.00
[32m[20221213 21:06:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.00
[32m[20221213 21:06:10 @agent_ppo2.py:143][0m Total time:      10.60 min
[32m[20221213 21:06:10 @agent_ppo2.py:145][0m 1028096 total steps have happened
[32m[20221213 21:06:10 @agent_ppo2.py:121][0m #------------------------ Iteration 502 --------------------------#
[32m[20221213 21:06:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:06:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:11 @agent_ppo2.py:185][0m |          -0.0029 |          85.7718 |          19.2026 |
[32m[20221213 21:06:11 @agent_ppo2.py:185][0m |           0.0028 |          89.6898 |          19.1930 |
[32m[20221213 21:06:11 @agent_ppo2.py:185][0m |          -0.0057 |          84.2718 |          19.1902 |
[32m[20221213 21:06:11 @agent_ppo2.py:185][0m |          -0.0058 |          84.1994 |          19.1912 |
[32m[20221213 21:06:11 @agent_ppo2.py:185][0m |          -0.0063 |          83.5794 |          19.1890 |
[32m[20221213 21:06:11 @agent_ppo2.py:185][0m |          -0.0077 |          83.1702 |          19.1945 |
[32m[20221213 21:06:11 @agent_ppo2.py:185][0m |          -0.0078 |          82.9684 |          19.2019 |
[32m[20221213 21:06:11 @agent_ppo2.py:185][0m |          -0.0092 |          82.8545 |          19.2020 |
[32m[20221213 21:06:11 @agent_ppo2.py:185][0m |          -0.0055 |          82.6075 |          19.2040 |
[32m[20221213 21:06:12 @agent_ppo2.py:185][0m |          -0.0063 |          82.5586 |          19.2078 |
[32m[20221213 21:06:12 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:06:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.60
[32m[20221213 21:06:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 570.00
[32m[20221213 21:06:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 591.00
[32m[20221213 21:06:12 @agent_ppo2.py:143][0m Total time:      10.62 min
[32m[20221213 21:06:12 @agent_ppo2.py:145][0m 1030144 total steps have happened
[32m[20221213 21:06:12 @agent_ppo2.py:121][0m #------------------------ Iteration 503 --------------------------#
[32m[20221213 21:06:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:06:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:12 @agent_ppo2.py:185][0m |           0.0101 |          92.5477 |          19.2470 |
[32m[20221213 21:06:12 @agent_ppo2.py:185][0m |          -0.0026 |          85.7325 |          19.2372 |
[32m[20221213 21:06:12 @agent_ppo2.py:185][0m |          -0.0002 |          85.7665 |          19.2228 |
[32m[20221213 21:06:12 @agent_ppo2.py:185][0m |          -0.0019 |          84.9741 |          19.2114 |
[32m[20221213 21:06:12 @agent_ppo2.py:185][0m |          -0.0062 |          84.7127 |          19.2190 |
[32m[20221213 21:06:12 @agent_ppo2.py:185][0m |          -0.0035 |          85.1243 |          19.2232 |
[32m[20221213 21:06:13 @agent_ppo2.py:185][0m |          -0.0090 |          84.4495 |          19.2288 |
[32m[20221213 21:06:13 @agent_ppo2.py:185][0m |          -0.0053 |          84.2686 |          19.2256 |
[32m[20221213 21:06:13 @agent_ppo2.py:185][0m |          -0.0090 |          84.1505 |          19.2261 |
[32m[20221213 21:06:13 @agent_ppo2.py:185][0m |          -0.0084 |          83.9372 |          19.2223 |
[32m[20221213 21:06:13 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:06:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 559.60
[32m[20221213 21:06:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.00
[32m[20221213 21:06:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 596.00
[32m[20221213 21:06:13 @agent_ppo2.py:143][0m Total time:      10.64 min
[32m[20221213 21:06:13 @agent_ppo2.py:145][0m 1032192 total steps have happened
[32m[20221213 21:06:13 @agent_ppo2.py:121][0m #------------------------ Iteration 504 --------------------------#
[32m[20221213 21:06:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:06:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:13 @agent_ppo2.py:185][0m |          -0.0036 |          87.1503 |          19.2182 |
[32m[20221213 21:06:13 @agent_ppo2.py:185][0m |          -0.0045 |          86.2700 |          19.2141 |
[32m[20221213 21:06:13 @agent_ppo2.py:185][0m |          -0.0068 |          85.5630 |          19.2137 |
[32m[20221213 21:06:14 @agent_ppo2.py:185][0m |          -0.0049 |          85.1418 |          19.2089 |
[32m[20221213 21:06:14 @agent_ppo2.py:185][0m |          -0.0004 |          84.9498 |          19.2019 |
[32m[20221213 21:06:14 @agent_ppo2.py:185][0m |          -0.0032 |          84.6404 |          19.2109 |
[32m[20221213 21:06:14 @agent_ppo2.py:185][0m |          -0.0024 |          85.4594 |          19.2080 |
[32m[20221213 21:06:14 @agent_ppo2.py:185][0m |           0.0000 |          85.0988 |          19.2115 |
[32m[20221213 21:06:14 @agent_ppo2.py:185][0m |          -0.0075 |          84.2681 |          19.2031 |
[32m[20221213 21:06:14 @agent_ppo2.py:185][0m |          -0.0056 |          84.1477 |          19.2104 |
[32m[20221213 21:06:14 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:06:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.00
[32m[20221213 21:06:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.00
[32m[20221213 21:06:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 551.00
[32m[20221213 21:06:14 @agent_ppo2.py:143][0m Total time:      10.66 min
[32m[20221213 21:06:14 @agent_ppo2.py:145][0m 1034240 total steps have happened
[32m[20221213 21:06:14 @agent_ppo2.py:121][0m #------------------------ Iteration 505 --------------------------#
[32m[20221213 21:06:14 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:06:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:15 @agent_ppo2.py:185][0m |          -0.0012 |          87.7738 |          19.1498 |
[32m[20221213 21:06:15 @agent_ppo2.py:185][0m |          -0.0013 |          86.8574 |          19.1314 |
[32m[20221213 21:06:15 @agent_ppo2.py:185][0m |          -0.0047 |          86.4697 |          19.1145 |
[32m[20221213 21:06:15 @agent_ppo2.py:185][0m |          -0.0034 |          86.0774 |          19.1206 |
[32m[20221213 21:06:15 @agent_ppo2.py:185][0m |           0.0037 |          89.0894 |          19.1207 |
[32m[20221213 21:06:15 @agent_ppo2.py:185][0m |          -0.0040 |          85.7126 |          19.0968 |
[32m[20221213 21:06:15 @agent_ppo2.py:185][0m |          -0.0073 |          85.5830 |          19.1141 |
[32m[20221213 21:06:16 @agent_ppo2.py:185][0m |          -0.0053 |          85.4887 |          19.1120 |
[32m[20221213 21:06:16 @agent_ppo2.py:185][0m |          -0.0049 |          85.5486 |          19.1053 |
[32m[20221213 21:06:16 @agent_ppo2.py:185][0m |          -0.0094 |          85.2623 |          19.1005 |
[32m[20221213 21:06:16 @agent_ppo2.py:130][0m Policy update time: 1.45 s
[32m[20221213 21:06:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.00
[32m[20221213 21:06:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 21:06:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 556.00
[32m[20221213 21:06:16 @agent_ppo2.py:143][0m Total time:      10.69 min
[32m[20221213 21:06:16 @agent_ppo2.py:145][0m 1036288 total steps have happened
[32m[20221213 21:06:16 @agent_ppo2.py:121][0m #------------------------ Iteration 506 --------------------------#
[32m[20221213 21:06:16 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:06:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:16 @agent_ppo2.py:185][0m |           0.0015 |          86.0672 |          19.0799 |
[32m[20221213 21:06:16 @agent_ppo2.py:185][0m |          -0.0043 |          85.6775 |          19.0757 |
[32m[20221213 21:06:16 @agent_ppo2.py:185][0m |          -0.0034 |          85.2023 |          19.0843 |
[32m[20221213 21:06:17 @agent_ppo2.py:185][0m |          -0.0019 |          85.2586 |          19.0970 |
[32m[20221213 21:06:17 @agent_ppo2.py:185][0m |          -0.0057 |          84.9484 |          19.0914 |
[32m[20221213 21:06:17 @agent_ppo2.py:185][0m |           0.0014 |          86.0481 |          19.1000 |
[32m[20221213 21:06:17 @agent_ppo2.py:185][0m |          -0.0022 |          85.0803 |          19.1046 |
[32m[20221213 21:06:17 @agent_ppo2.py:185][0m |          -0.0050 |          84.5697 |          19.0972 |
[32m[20221213 21:06:17 @agent_ppo2.py:185][0m |          -0.0057 |          84.5219 |          19.1072 |
[32m[20221213 21:06:17 @agent_ppo2.py:185][0m |           0.0025 |          90.0915 |          19.1126 |
[32m[20221213 21:06:17 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:06:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 538.40
[32m[20221213 21:06:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 590.00
[32m[20221213 21:06:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.00
[32m[20221213 21:06:17 @agent_ppo2.py:143][0m Total time:      10.71 min
[32m[20221213 21:06:17 @agent_ppo2.py:145][0m 1038336 total steps have happened
[32m[20221213 21:06:17 @agent_ppo2.py:121][0m #------------------------ Iteration 507 --------------------------#
[32m[20221213 21:06:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:06:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:18 @agent_ppo2.py:185][0m |          -0.0010 |          89.7454 |          19.1910 |
[32m[20221213 21:06:18 @agent_ppo2.py:185][0m |          -0.0014 |          89.8075 |          19.1886 |
[32m[20221213 21:06:18 @agent_ppo2.py:185][0m |          -0.0056 |          88.8266 |          19.1903 |
[32m[20221213 21:06:18 @agent_ppo2.py:185][0m |           0.0022 |          93.0971 |          19.1892 |
[32m[20221213 21:06:18 @agent_ppo2.py:185][0m |          -0.0063 |          88.4430 |          19.1643 |
[32m[20221213 21:06:18 @agent_ppo2.py:185][0m |          -0.0043 |          88.2546 |          19.1868 |
[32m[20221213 21:06:18 @agent_ppo2.py:185][0m |          -0.0074 |          88.1984 |          19.1829 |
[32m[20221213 21:06:18 @agent_ppo2.py:185][0m |          -0.0099 |          88.1817 |          19.1746 |
[32m[20221213 21:06:18 @agent_ppo2.py:185][0m |          -0.0094 |          88.0512 |          19.1837 |
[32m[20221213 21:06:18 @agent_ppo2.py:185][0m |          -0.0100 |          87.9541 |          19.1821 |
[32m[20221213 21:06:18 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:06:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 559.60
[32m[20221213 21:06:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 602.00
[32m[20221213 21:06:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 577.00
[32m[20221213 21:06:19 @agent_ppo2.py:143][0m Total time:      10.73 min
[32m[20221213 21:06:19 @agent_ppo2.py:145][0m 1040384 total steps have happened
[32m[20221213 21:06:19 @agent_ppo2.py:121][0m #------------------------ Iteration 508 --------------------------#
[32m[20221213 21:06:19 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:06:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:19 @agent_ppo2.py:185][0m |          -0.0018 |          85.5170 |          19.2386 |
[32m[20221213 21:06:19 @agent_ppo2.py:185][0m |           0.0088 |          90.3970 |          19.2195 |
[32m[20221213 21:06:19 @agent_ppo2.py:185][0m |          -0.0034 |          83.6317 |          19.2131 |
[32m[20221213 21:06:19 @agent_ppo2.py:185][0m |          -0.0056 |          83.1373 |          19.2154 |
[32m[20221213 21:06:19 @agent_ppo2.py:185][0m |          -0.0067 |          82.8748 |          19.2043 |
[32m[20221213 21:06:19 @agent_ppo2.py:185][0m |          -0.0091 |          82.6781 |          19.2016 |
[32m[20221213 21:06:20 @agent_ppo2.py:185][0m |          -0.0053 |          83.4760 |          19.2107 |
[32m[20221213 21:06:20 @agent_ppo2.py:185][0m |          -0.0091 |          82.1505 |          19.2024 |
[32m[20221213 21:06:20 @agent_ppo2.py:185][0m |          -0.0099 |          82.0640 |          19.2033 |
[32m[20221213 21:06:20 @agent_ppo2.py:185][0m |          -0.0076 |          81.9111 |          19.2033 |
[32m[20221213 21:06:20 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:06:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.20
[32m[20221213 21:06:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.00
[32m[20221213 21:06:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 596.00
[32m[20221213 21:06:20 @agent_ppo2.py:143][0m Total time:      10.75 min
[32m[20221213 21:06:20 @agent_ppo2.py:145][0m 1042432 total steps have happened
[32m[20221213 21:06:20 @agent_ppo2.py:121][0m #------------------------ Iteration 509 --------------------------#
[32m[20221213 21:06:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:06:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:20 @agent_ppo2.py:185][0m |           0.0015 |          85.2682 |          19.2488 |
[32m[20221213 21:06:20 @agent_ppo2.py:185][0m |          -0.0046 |          84.6775 |          19.2435 |
[32m[20221213 21:06:20 @agent_ppo2.py:185][0m |          -0.0057 |          84.3582 |          19.2325 |
[32m[20221213 21:06:21 @agent_ppo2.py:185][0m |          -0.0071 |          84.1362 |          19.2299 |
[32m[20221213 21:06:21 @agent_ppo2.py:185][0m |          -0.0062 |          83.9670 |          19.2228 |
[32m[20221213 21:06:21 @agent_ppo2.py:185][0m |           0.0029 |          91.8036 |          19.2242 |
[32m[20221213 21:06:21 @agent_ppo2.py:185][0m |          -0.0072 |          83.8267 |          19.2195 |
[32m[20221213 21:06:21 @agent_ppo2.py:185][0m |          -0.0071 |          83.7037 |          19.2262 |
[32m[20221213 21:06:21 @agent_ppo2.py:185][0m |          -0.0066 |          83.5386 |          19.2280 |
[32m[20221213 21:06:21 @agent_ppo2.py:185][0m |          -0.0098 |          83.5224 |          19.2218 |
[32m[20221213 21:06:21 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:06:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.00
[32m[20221213 21:06:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.00
[32m[20221213 21:06:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 601.00
[32m[20221213 21:06:21 @agent_ppo2.py:143][0m Total time:      10.78 min
[32m[20221213 21:06:21 @agent_ppo2.py:145][0m 1044480 total steps have happened
[32m[20221213 21:06:21 @agent_ppo2.py:121][0m #------------------------ Iteration 510 --------------------------#
[32m[20221213 21:06:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:06:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:22 @agent_ppo2.py:185][0m |          -0.0047 |          86.3994 |          19.0684 |
[32m[20221213 21:06:22 @agent_ppo2.py:185][0m |          -0.0051 |          85.8973 |          19.0525 |
[32m[20221213 21:06:22 @agent_ppo2.py:185][0m |          -0.0055 |          86.0379 |          19.0480 |
[32m[20221213 21:06:22 @agent_ppo2.py:185][0m |          -0.0071 |          85.7581 |          19.0345 |
[32m[20221213 21:06:22 @agent_ppo2.py:185][0m |           0.0048 |          93.5829 |          19.0368 |
[32m[20221213 21:06:22 @agent_ppo2.py:185][0m |           0.0004 |          89.8460 |          19.0371 |
[32m[20221213 21:06:22 @agent_ppo2.py:185][0m |          -0.0083 |          84.9390 |          19.0218 |
[32m[20221213 21:06:22 @agent_ppo2.py:185][0m |          -0.0105 |          84.8094 |          19.0232 |
[32m[20221213 21:06:22 @agent_ppo2.py:185][0m |          -0.0077 |          84.6089 |          19.0315 |
[32m[20221213 21:06:22 @agent_ppo2.py:185][0m |          -0.0107 |          84.7420 |          19.0343 |
[32m[20221213 21:06:22 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 21:06:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.40
[32m[20221213 21:06:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 584.00
[32m[20221213 21:06:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.00
[32m[20221213 21:06:23 @agent_ppo2.py:143][0m Total time:      10.80 min
[32m[20221213 21:06:23 @agent_ppo2.py:145][0m 1046528 total steps have happened
[32m[20221213 21:06:23 @agent_ppo2.py:121][0m #------------------------ Iteration 511 --------------------------#
[32m[20221213 21:06:23 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:06:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:23 @agent_ppo2.py:185][0m |          -0.0016 |          87.2491 |          19.2775 |
[32m[20221213 21:06:23 @agent_ppo2.py:185][0m |          -0.0028 |          81.1974 |          19.2604 |
[32m[20221213 21:06:23 @agent_ppo2.py:185][0m |          -0.0044 |          80.0945 |          19.2467 |
[32m[20221213 21:06:23 @agent_ppo2.py:185][0m |           0.0077 |          83.2542 |          19.2467 |
[32m[20221213 21:06:23 @agent_ppo2.py:185][0m |          -0.0027 |          79.1326 |          19.2170 |
[32m[20221213 21:06:23 @agent_ppo2.py:185][0m |          -0.0024 |          79.1140 |          19.2232 |
[32m[20221213 21:06:23 @agent_ppo2.py:185][0m |          -0.0056 |          78.8805 |          19.2289 |
[32m[20221213 21:06:24 @agent_ppo2.py:185][0m |          -0.0012 |          79.8822 |          19.2266 |
[32m[20221213 21:06:24 @agent_ppo2.py:185][0m |          -0.0081 |          78.5771 |          19.2256 |
[32m[20221213 21:06:24 @agent_ppo2.py:185][0m |          -0.0046 |          78.4758 |          19.2288 |
[32m[20221213 21:06:24 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:06:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 550.40
[32m[20221213 21:06:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 592.00
[32m[20221213 21:06:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 544.00
[32m[20221213 21:06:24 @agent_ppo2.py:143][0m Total time:      10.82 min
[32m[20221213 21:06:24 @agent_ppo2.py:145][0m 1048576 total steps have happened
[32m[20221213 21:06:24 @agent_ppo2.py:121][0m #------------------------ Iteration 512 --------------------------#
[32m[20221213 21:06:24 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:06:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:24 @agent_ppo2.py:185][0m |           0.0004 |          87.8776 |          19.0915 |
[32m[20221213 21:06:24 @agent_ppo2.py:185][0m |          -0.0049 |          86.3125 |          19.0818 |
[32m[20221213 21:06:24 @agent_ppo2.py:185][0m |          -0.0054 |          85.7665 |          19.0733 |
[32m[20221213 21:06:24 @agent_ppo2.py:185][0m |          -0.0004 |          85.0777 |          19.0740 |
[32m[20221213 21:06:25 @agent_ppo2.py:185][0m |          -0.0019 |          84.8132 |          19.0859 |
[32m[20221213 21:06:25 @agent_ppo2.py:185][0m |          -0.0023 |          84.6924 |          19.0744 |
[32m[20221213 21:06:25 @agent_ppo2.py:185][0m |          -0.0071 |          84.2884 |          19.0772 |
[32m[20221213 21:06:25 @agent_ppo2.py:185][0m |          -0.0058 |          84.1887 |          19.0809 |
[32m[20221213 21:06:25 @agent_ppo2.py:185][0m |          -0.0076 |          84.1412 |          19.0722 |
[32m[20221213 21:06:25 @agent_ppo2.py:185][0m |          -0.0083 |          83.8652 |          19.0705 |
[32m[20221213 21:06:25 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:06:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.40
[32m[20221213 21:06:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 570.00
[32m[20221213 21:06:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.00
[32m[20221213 21:06:25 @agent_ppo2.py:143][0m Total time:      10.84 min
[32m[20221213 21:06:25 @agent_ppo2.py:145][0m 1050624 total steps have happened
[32m[20221213 21:06:25 @agent_ppo2.py:121][0m #------------------------ Iteration 513 --------------------------#
[32m[20221213 21:06:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:06:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:26 @agent_ppo2.py:185][0m |          -0.0035 |          87.3835 |          19.1381 |
[32m[20221213 21:06:26 @agent_ppo2.py:185][0m |          -0.0022 |          86.0315 |          19.1444 |
[32m[20221213 21:06:26 @agent_ppo2.py:185][0m |          -0.0060 |          85.5151 |          19.1398 |
[32m[20221213 21:06:26 @agent_ppo2.py:185][0m |          -0.0036 |          85.0941 |          19.1349 |
[32m[20221213 21:06:26 @agent_ppo2.py:185][0m |          -0.0032 |          85.0250 |          19.1408 |
[32m[20221213 21:06:26 @agent_ppo2.py:185][0m |          -0.0034 |          84.7567 |          19.1427 |
[32m[20221213 21:06:26 @agent_ppo2.py:185][0m |          -0.0052 |          84.0685 |          19.1415 |
[32m[20221213 21:06:26 @agent_ppo2.py:185][0m |           0.0028 |          92.1421 |          19.1431 |
[32m[20221213 21:06:26 @agent_ppo2.py:185][0m |           0.0000 |          86.1040 |          19.1455 |
[32m[20221213 21:06:26 @agent_ppo2.py:185][0m |          -0.0080 |          83.5591 |          19.1243 |
[32m[20221213 21:06:26 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:06:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 534.40
[32m[20221213 21:06:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.00
[32m[20221213 21:06:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.00
[32m[20221213 21:06:26 @agent_ppo2.py:143][0m Total time:      10.86 min
[32m[20221213 21:06:26 @agent_ppo2.py:145][0m 1052672 total steps have happened
[32m[20221213 21:06:26 @agent_ppo2.py:121][0m #------------------------ Iteration 514 --------------------------#
[32m[20221213 21:06:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:06:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:27 @agent_ppo2.py:185][0m |          -0.0012 |          86.2731 |          19.1729 |
[32m[20221213 21:06:27 @agent_ppo2.py:185][0m |          -0.0073 |          84.8441 |          19.1507 |
[32m[20221213 21:06:27 @agent_ppo2.py:185][0m |          -0.0054 |          83.8748 |          19.1440 |
[32m[20221213 21:06:27 @agent_ppo2.py:185][0m |          -0.0083 |          83.1792 |          19.1411 |
[32m[20221213 21:06:27 @agent_ppo2.py:185][0m |          -0.0085 |          82.6602 |          19.1438 |
[32m[20221213 21:06:27 @agent_ppo2.py:185][0m |          -0.0070 |          82.2156 |          19.1483 |
[32m[20221213 21:06:27 @agent_ppo2.py:185][0m |          -0.0072 |          81.4119 |          19.1430 |
[32m[20221213 21:06:28 @agent_ppo2.py:185][0m |          -0.0059 |          81.7341 |          19.1471 |
[32m[20221213 21:06:28 @agent_ppo2.py:185][0m |          -0.0101 |          80.9231 |          19.1410 |
[32m[20221213 21:06:28 @agent_ppo2.py:185][0m |          -0.0112 |          80.8016 |          19.1375 |
[32m[20221213 21:06:28 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:06:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.40
[32m[20221213 21:06:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.00
[32m[20221213 21:06:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 566.00
[32m[20221213 21:06:28 @agent_ppo2.py:143][0m Total time:      10.89 min
[32m[20221213 21:06:28 @agent_ppo2.py:145][0m 1054720 total steps have happened
[32m[20221213 21:06:28 @agent_ppo2.py:121][0m #------------------------ Iteration 515 --------------------------#
[32m[20221213 21:06:28 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 21:06:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:28 @agent_ppo2.py:185][0m |          -0.0001 |          91.4410 |          19.1720 |
[32m[20221213 21:06:29 @agent_ppo2.py:185][0m |          -0.0063 |          90.1494 |          19.1628 |
[32m[20221213 21:06:29 @agent_ppo2.py:185][0m |          -0.0009 |          91.4027 |          19.1684 |
[32m[20221213 21:06:29 @agent_ppo2.py:185][0m |          -0.0062 |          89.4748 |          19.1767 |
[32m[20221213 21:06:29 @agent_ppo2.py:185][0m |          -0.0058 |          89.2565 |          19.1706 |
[32m[20221213 21:06:29 @agent_ppo2.py:185][0m |          -0.0080 |          89.2087 |          19.1685 |
[32m[20221213 21:06:30 @agent_ppo2.py:185][0m |          -0.0072 |          89.0207 |          19.1679 |
[32m[20221213 21:06:30 @agent_ppo2.py:185][0m |          -0.0043 |          89.5408 |          19.1632 |
[32m[20221213 21:06:30 @agent_ppo2.py:185][0m |          -0.0091 |          88.6924 |          19.1755 |
[32m[20221213 21:06:30 @agent_ppo2.py:185][0m |          -0.0068 |          88.9358 |          19.1681 |
[32m[20221213 21:06:30 @agent_ppo2.py:130][0m Policy update time: 1.66 s
[32m[20221213 21:06:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.00
[32m[20221213 21:06:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.00
[32m[20221213 21:06:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 576.00
[32m[20221213 21:06:30 @agent_ppo2.py:143][0m Total time:      10.92 min
[32m[20221213 21:06:30 @agent_ppo2.py:145][0m 1056768 total steps have happened
[32m[20221213 21:06:30 @agent_ppo2.py:121][0m #------------------------ Iteration 516 --------------------------#
[32m[20221213 21:06:30 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:06:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:30 @agent_ppo2.py:185][0m |           0.0044 |          88.3533 |          19.1408 |
[32m[20221213 21:06:30 @agent_ppo2.py:185][0m |          -0.0035 |          86.2675 |          19.1410 |
[32m[20221213 21:06:31 @agent_ppo2.py:185][0m |          -0.0036 |          85.6329 |          19.1317 |
[32m[20221213 21:06:31 @agent_ppo2.py:185][0m |          -0.0041 |          85.3534 |          19.1177 |
[32m[20221213 21:06:31 @agent_ppo2.py:185][0m |          -0.0046 |          85.2652 |          19.1148 |
[32m[20221213 21:06:31 @agent_ppo2.py:185][0m |          -0.0050 |          85.1522 |          19.1095 |
[32m[20221213 21:06:31 @agent_ppo2.py:185][0m |          -0.0089 |          84.8722 |          19.1006 |
[32m[20221213 21:06:31 @agent_ppo2.py:185][0m |          -0.0070 |          84.6318 |          19.1032 |
[32m[20221213 21:06:31 @agent_ppo2.py:185][0m |          -0.0062 |          84.5366 |          19.0982 |
[32m[20221213 21:06:31 @agent_ppo2.py:185][0m |          -0.0070 |          84.5418 |          19.0909 |
[32m[20221213 21:06:31 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 21:06:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 566.00
[32m[20221213 21:06:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 602.00
[32m[20221213 21:06:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 589.00
[32m[20221213 21:06:31 @agent_ppo2.py:143][0m Total time:      10.95 min
[32m[20221213 21:06:31 @agent_ppo2.py:145][0m 1058816 total steps have happened
[32m[20221213 21:06:31 @agent_ppo2.py:121][0m #------------------------ Iteration 517 --------------------------#
[32m[20221213 21:06:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 21:06:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:32 @agent_ppo2.py:185][0m |          -0.0031 |          87.2102 |          19.1975 |
[32m[20221213 21:06:32 @agent_ppo2.py:185][0m |          -0.0070 |          86.5852 |          19.1800 |
[32m[20221213 21:06:32 @agent_ppo2.py:185][0m |          -0.0052 |          85.9912 |          19.1649 |
[32m[20221213 21:06:32 @agent_ppo2.py:185][0m |          -0.0089 |          85.6169 |          19.1702 |
[32m[20221213 21:06:32 @agent_ppo2.py:185][0m |          -0.0085 |          85.4122 |          19.1618 |
[32m[20221213 21:06:32 @agent_ppo2.py:185][0m |          -0.0096 |          85.0554 |          19.1500 |
[32m[20221213 21:06:33 @agent_ppo2.py:185][0m |          -0.0051 |          84.8108 |          19.1548 |
[32m[20221213 21:06:33 @agent_ppo2.py:185][0m |          -0.0109 |          84.7283 |          19.1417 |
[32m[20221213 21:06:33 @agent_ppo2.py:185][0m |          -0.0076 |          84.4886 |          19.1436 |
[32m[20221213 21:06:33 @agent_ppo2.py:185][0m |          -0.0034 |          86.6945 |          19.1483 |
[32m[20221213 21:06:33 @agent_ppo2.py:130][0m Policy update time: 1.30 s
[32m[20221213 21:06:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 567.00
[32m[20221213 21:06:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 608.00
[32m[20221213 21:06:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.00
[32m[20221213 21:06:33 @agent_ppo2.py:143][0m Total time:      10.97 min
[32m[20221213 21:06:33 @agent_ppo2.py:145][0m 1060864 total steps have happened
[32m[20221213 21:06:33 @agent_ppo2.py:121][0m #------------------------ Iteration 518 --------------------------#
[32m[20221213 21:06:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:06:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:33 @agent_ppo2.py:185][0m |          -0.0017 |          91.7013 |          19.0614 |
[32m[20221213 21:06:34 @agent_ppo2.py:185][0m |          -0.0051 |          90.7893 |          19.0455 |
[32m[20221213 21:06:34 @agent_ppo2.py:185][0m |          -0.0040 |          90.3446 |          19.0423 |
[32m[20221213 21:06:34 @agent_ppo2.py:185][0m |          -0.0069 |          90.0501 |          19.0415 |
[32m[20221213 21:06:34 @agent_ppo2.py:185][0m |          -0.0105 |          89.9504 |          19.0448 |
[32m[20221213 21:06:34 @agent_ppo2.py:185][0m |           0.0045 |          98.7101 |          19.0454 |
[32m[20221213 21:06:34 @agent_ppo2.py:185][0m |          -0.0072 |          89.5220 |          19.0369 |
[32m[20221213 21:06:34 @agent_ppo2.py:185][0m |          -0.0076 |          89.3502 |          19.0465 |
[32m[20221213 21:06:34 @agent_ppo2.py:185][0m |          -0.0079 |          89.1817 |          19.0457 |
[32m[20221213 21:06:35 @agent_ppo2.py:185][0m |          -0.0060 |          89.0930 |          19.0419 |
[32m[20221213 21:06:35 @agent_ppo2.py:130][0m Policy update time: 1.30 s
[32m[20221213 21:06:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.00
[32m[20221213 21:06:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.00
[32m[20221213 21:06:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.00
[32m[20221213 21:06:35 @agent_ppo2.py:143][0m Total time:      11.00 min
[32m[20221213 21:06:35 @agent_ppo2.py:145][0m 1062912 total steps have happened
[32m[20221213 21:06:35 @agent_ppo2.py:121][0m #------------------------ Iteration 519 --------------------------#
[32m[20221213 21:06:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:06:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:35 @agent_ppo2.py:185][0m |          -0.0007 |          91.1367 |          19.3046 |
[32m[20221213 21:06:35 @agent_ppo2.py:185][0m |          -0.0046 |          89.0943 |          19.2982 |
[32m[20221213 21:06:35 @agent_ppo2.py:185][0m |          -0.0069 |          88.3991 |          19.2878 |
[32m[20221213 21:06:35 @agent_ppo2.py:185][0m |          -0.0055 |          87.9473 |          19.2991 |
[32m[20221213 21:06:36 @agent_ppo2.py:185][0m |          -0.0050 |          87.5571 |          19.2961 |
[32m[20221213 21:06:36 @agent_ppo2.py:185][0m |          -0.0081 |          87.3272 |          19.2952 |
[32m[20221213 21:06:36 @agent_ppo2.py:185][0m |          -0.0082 |          87.1241 |          19.3091 |
[32m[20221213 21:06:36 @agent_ppo2.py:185][0m |          -0.0078 |          86.6267 |          19.3048 |
[32m[20221213 21:06:36 @agent_ppo2.py:185][0m |          -0.0080 |          86.2652 |          19.2976 |
[32m[20221213 21:06:36 @agent_ppo2.py:185][0m |          -0.0074 |          86.2006 |          19.2967 |
[32m[20221213 21:06:36 @agent_ppo2.py:130][0m Policy update time: 1.29 s
[32m[20221213 21:06:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.20
[32m[20221213 21:06:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.00
[32m[20221213 21:06:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 640.00
[32m[20221213 21:06:36 @agent_ppo2.py:143][0m Total time:      11.03 min
[32m[20221213 21:06:36 @agent_ppo2.py:145][0m 1064960 total steps have happened
[32m[20221213 21:06:36 @agent_ppo2.py:121][0m #------------------------ Iteration 520 --------------------------#
[32m[20221213 21:06:36 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 21:06:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:37 @agent_ppo2.py:185][0m |          -0.0036 |          94.2972 |          19.2502 |
[32m[20221213 21:06:37 @agent_ppo2.py:185][0m |          -0.0042 |          93.1830 |          19.2399 |
[32m[20221213 21:06:37 @agent_ppo2.py:185][0m |           0.0010 |          98.2370 |          19.2493 |
[32m[20221213 21:06:37 @agent_ppo2.py:185][0m |          -0.0071 |          92.4737 |          19.2450 |
[32m[20221213 21:06:37 @agent_ppo2.py:185][0m |          -0.0023 |          94.3756 |          19.2501 |
[32m[20221213 21:06:37 @agent_ppo2.py:185][0m |          -0.0066 |          92.0561 |          19.2431 |
[32m[20221213 21:06:37 @agent_ppo2.py:185][0m |          -0.0078 |          91.7952 |          19.2525 |
[32m[20221213 21:06:37 @agent_ppo2.py:185][0m |          -0.0005 |          94.1258 |          19.2453 |
[32m[20221213 21:06:38 @agent_ppo2.py:185][0m |          -0.0071 |          91.4453 |          19.2461 |
[32m[20221213 21:06:38 @agent_ppo2.py:185][0m |          -0.0117 |          91.5379 |          19.2525 |
[32m[20221213 21:06:38 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:06:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 543.40
[32m[20221213 21:06:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 562.00
[32m[20221213 21:06:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.00
[32m[20221213 21:06:38 @agent_ppo2.py:143][0m Total time:      11.05 min
[32m[20221213 21:06:38 @agent_ppo2.py:145][0m 1067008 total steps have happened
[32m[20221213 21:06:38 @agent_ppo2.py:121][0m #------------------------ Iteration 521 --------------------------#
[32m[20221213 21:06:38 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:06:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:38 @agent_ppo2.py:185][0m |          -0.0002 |          92.6765 |          19.2181 |
[32m[20221213 21:06:38 @agent_ppo2.py:185][0m |          -0.0033 |          91.8900 |          19.2048 |
[32m[20221213 21:06:38 @agent_ppo2.py:185][0m |           0.0026 |          97.6040 |          19.2014 |
[32m[20221213 21:06:38 @agent_ppo2.py:185][0m |          -0.0046 |          91.0421 |          19.1765 |
[32m[20221213 21:06:39 @agent_ppo2.py:185][0m |          -0.0058 |          90.6866 |          19.1836 |
[32m[20221213 21:06:39 @agent_ppo2.py:185][0m |          -0.0023 |          91.3722 |          19.1795 |
[32m[20221213 21:06:39 @agent_ppo2.py:185][0m |          -0.0057 |          90.2874 |          19.1799 |
[32m[20221213 21:06:39 @agent_ppo2.py:185][0m |          -0.0086 |          90.1035 |          19.1858 |
[32m[20221213 21:06:39 @agent_ppo2.py:185][0m |          -0.0067 |          90.9662 |          19.1787 |
[32m[20221213 21:06:39 @agent_ppo2.py:185][0m |          -0.0088 |          89.8566 |          19.1670 |
[32m[20221213 21:06:39 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221213 21:06:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 557.60
[32m[20221213 21:06:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 576.00
[32m[20221213 21:06:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 579.00
[32m[20221213 21:06:39 @agent_ppo2.py:143][0m Total time:      11.08 min
[32m[20221213 21:06:39 @agent_ppo2.py:145][0m 1069056 total steps have happened
[32m[20221213 21:06:39 @agent_ppo2.py:121][0m #------------------------ Iteration 522 --------------------------#
[32m[20221213 21:06:39 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:06:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:40 @agent_ppo2.py:185][0m |          -0.0016 |          95.4140 |          19.1656 |
[32m[20221213 21:06:40 @agent_ppo2.py:185][0m |          -0.0021 |          94.7899 |          19.1463 |
[32m[20221213 21:06:40 @agent_ppo2.py:185][0m |          -0.0045 |          93.5363 |          19.1533 |
[32m[20221213 21:06:40 @agent_ppo2.py:185][0m |          -0.0072 |          93.1157 |          19.1451 |
[32m[20221213 21:06:40 @agent_ppo2.py:185][0m |           0.0125 |         107.5433 |          19.1412 |
[32m[20221213 21:06:40 @agent_ppo2.py:185][0m |           0.0042 |         100.0131 |          19.1480 |
[32m[20221213 21:06:40 @agent_ppo2.py:185][0m |          -0.0003 |          92.4879 |          19.1345 |
[32m[20221213 21:06:40 @agent_ppo2.py:185][0m |           0.0010 |          94.5836 |          19.1484 |
[32m[20221213 21:06:41 @agent_ppo2.py:185][0m |          -0.0005 |          92.1628 |          19.1468 |
[32m[20221213 21:06:41 @agent_ppo2.py:185][0m |          -0.0049 |          91.6265 |          19.1591 |
[32m[20221213 21:06:41 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 21:06:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 549.60
[32m[20221213 21:06:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 583.00
[32m[20221213 21:06:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 603.00
[32m[20221213 21:06:41 @agent_ppo2.py:143][0m Total time:      11.10 min
[32m[20221213 21:06:41 @agent_ppo2.py:145][0m 1071104 total steps have happened
[32m[20221213 21:06:41 @agent_ppo2.py:121][0m #------------------------ Iteration 523 --------------------------#
[32m[20221213 21:06:41 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:06:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:41 @agent_ppo2.py:185][0m |           0.0005 |          89.8804 |          19.0896 |
[32m[20221213 21:06:41 @agent_ppo2.py:185][0m |          -0.0058 |          89.2398 |          19.0666 |
[32m[20221213 21:06:41 @agent_ppo2.py:185][0m |          -0.0083 |          88.9079 |          19.0656 |
[32m[20221213 21:06:41 @agent_ppo2.py:185][0m |          -0.0058 |          88.5891 |          19.0611 |
[32m[20221213 21:06:42 @agent_ppo2.py:185][0m |          -0.0075 |          88.4722 |          19.0607 |
[32m[20221213 21:06:42 @agent_ppo2.py:185][0m |          -0.0096 |          88.2292 |          19.0628 |
[32m[20221213 21:06:42 @agent_ppo2.py:185][0m |          -0.0120 |          88.2425 |          19.0673 |
[32m[20221213 21:06:42 @agent_ppo2.py:185][0m |           0.0049 |          98.9308 |          19.0613 |
[32m[20221213 21:06:42 @agent_ppo2.py:185][0m |           0.0058 |          99.6394 |          19.0496 |
[32m[20221213 21:06:42 @agent_ppo2.py:185][0m |          -0.0100 |          87.8439 |          19.0697 |
[32m[20221213 21:06:42 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 21:06:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.80
[32m[20221213 21:06:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 616.00
[32m[20221213 21:06:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.00
[32m[20221213 21:06:42 @agent_ppo2.py:143][0m Total time:      11.13 min
[32m[20221213 21:06:42 @agent_ppo2.py:145][0m 1073152 total steps have happened
[32m[20221213 21:06:42 @agent_ppo2.py:121][0m #------------------------ Iteration 524 --------------------------#
[32m[20221213 21:06:42 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:06:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:43 @agent_ppo2.py:185][0m |           0.0006 |          88.6506 |          19.1435 |
[32m[20221213 21:06:43 @agent_ppo2.py:185][0m |          -0.0014 |          87.9296 |          19.1318 |
[32m[20221213 21:06:43 @agent_ppo2.py:185][0m |          -0.0054 |          87.6787 |          19.1324 |
[32m[20221213 21:06:43 @agent_ppo2.py:185][0m |          -0.0014 |          87.6645 |          19.1296 |
[32m[20221213 21:06:43 @agent_ppo2.py:185][0m |          -0.0045 |          87.2235 |          19.1297 |
[32m[20221213 21:06:43 @agent_ppo2.py:185][0m |          -0.0070 |          87.0173 |          19.1298 |
[32m[20221213 21:06:43 @agent_ppo2.py:185][0m |          -0.0059 |          86.7334 |          19.1274 |
[32m[20221213 21:06:43 @agent_ppo2.py:185][0m |          -0.0073 |          86.6695 |          19.1286 |
[32m[20221213 21:06:43 @agent_ppo2.py:185][0m |          -0.0055 |          86.8374 |          19.1312 |
[32m[20221213 21:06:44 @agent_ppo2.py:185][0m |          -0.0086 |          86.5522 |          19.1368 |
[32m[20221213 21:06:44 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 21:06:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.60
[32m[20221213 21:06:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 577.00
[32m[20221213 21:06:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 576.00
[32m[20221213 21:06:44 @agent_ppo2.py:143][0m Total time:      11.15 min
[32m[20221213 21:06:44 @agent_ppo2.py:145][0m 1075200 total steps have happened
[32m[20221213 21:06:44 @agent_ppo2.py:121][0m #------------------------ Iteration 525 --------------------------#
[32m[20221213 21:06:44 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:06:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:44 @agent_ppo2.py:185][0m |           0.0041 |          87.3277 |          19.0915 |
[32m[20221213 21:06:44 @agent_ppo2.py:185][0m |          -0.0055 |          86.4264 |          19.0866 |
[32m[20221213 21:06:44 @agent_ppo2.py:185][0m |          -0.0016 |          85.8393 |          19.0817 |
[32m[20221213 21:06:44 @agent_ppo2.py:185][0m |          -0.0084 |          85.6679 |          19.0805 |
[32m[20221213 21:06:45 @agent_ppo2.py:185][0m |          -0.0068 |          85.2164 |          19.0757 |
[32m[20221213 21:06:45 @agent_ppo2.py:185][0m |          -0.0061 |          85.0375 |          19.0751 |
[32m[20221213 21:06:45 @agent_ppo2.py:185][0m |          -0.0070 |          84.8407 |          19.0714 |
[32m[20221213 21:06:45 @agent_ppo2.py:185][0m |          -0.0040 |          84.8452 |          19.0776 |
[32m[20221213 21:06:45 @agent_ppo2.py:185][0m |          -0.0068 |          84.5318 |          19.0773 |
[32m[20221213 21:06:45 @agent_ppo2.py:185][0m |          -0.0029 |          84.5899 |          19.0779 |
[32m[20221213 21:06:45 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 21:06:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.80
[32m[20221213 21:06:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 571.00
[32m[20221213 21:06:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 594.00
[32m[20221213 21:06:45 @agent_ppo2.py:143][0m Total time:      11.18 min
[32m[20221213 21:06:45 @agent_ppo2.py:145][0m 1077248 total steps have happened
[32m[20221213 21:06:45 @agent_ppo2.py:121][0m #------------------------ Iteration 526 --------------------------#
[32m[20221213 21:06:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:06:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:46 @agent_ppo2.py:185][0m |          -0.0029 |          91.2922 |          19.1668 |
[32m[20221213 21:06:46 @agent_ppo2.py:185][0m |          -0.0100 |          90.2430 |          19.1426 |
[32m[20221213 21:06:46 @agent_ppo2.py:185][0m |          -0.0098 |          89.8871 |          19.1382 |
[32m[20221213 21:06:46 @agent_ppo2.py:185][0m |          -0.0059 |          89.4696 |          19.1380 |
[32m[20221213 21:06:46 @agent_ppo2.py:185][0m |          -0.0049 |          89.1484 |          19.1440 |
[32m[20221213 21:06:46 @agent_ppo2.py:185][0m |          -0.0068 |          88.9176 |          19.1470 |
[32m[20221213 21:06:46 @agent_ppo2.py:185][0m |          -0.0073 |          88.9802 |          19.1429 |
[32m[20221213 21:06:46 @agent_ppo2.py:185][0m |          -0.0093 |          88.7666 |          19.1429 |
[32m[20221213 21:06:46 @agent_ppo2.py:185][0m |          -0.0057 |          89.8027 |          19.1429 |
[32m[20221213 21:06:47 @agent_ppo2.py:185][0m |          -0.0111 |          88.5018 |          19.1336 |
[32m[20221213 21:06:47 @agent_ppo2.py:130][0m Policy update time: 1.24 s
[32m[20221213 21:06:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 559.80
[32m[20221213 21:06:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 585.00
[32m[20221213 21:06:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 576.00
[32m[20221213 21:06:47 @agent_ppo2.py:143][0m Total time:      11.20 min
[32m[20221213 21:06:47 @agent_ppo2.py:145][0m 1079296 total steps have happened
[32m[20221213 21:06:47 @agent_ppo2.py:121][0m #------------------------ Iteration 527 --------------------------#
[32m[20221213 21:06:47 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:06:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:47 @agent_ppo2.py:185][0m |           0.0039 |          95.4789 |          19.1993 |
[32m[20221213 21:06:47 @agent_ppo2.py:185][0m |          -0.0023 |          90.4696 |          19.1966 |
[32m[20221213 21:06:47 @agent_ppo2.py:185][0m |          -0.0004 |          92.9993 |          19.1964 |
[32m[20221213 21:06:47 @agent_ppo2.py:185][0m |          -0.0019 |          90.4263 |          19.1825 |
[32m[20221213 21:06:48 @agent_ppo2.py:185][0m |          -0.0071 |          89.8389 |          19.2005 |
[32m[20221213 21:06:48 @agent_ppo2.py:185][0m |          -0.0063 |          89.4956 |          19.2015 |
[32m[20221213 21:06:48 @agent_ppo2.py:185][0m |          -0.0046 |          89.4493 |          19.1908 |
[32m[20221213 21:06:48 @agent_ppo2.py:185][0m |          -0.0075 |          89.4208 |          19.1974 |
[32m[20221213 21:06:48 @agent_ppo2.py:185][0m |          -0.0062 |          89.2261 |          19.2033 |
[32m[20221213 21:06:48 @agent_ppo2.py:185][0m |          -0.0070 |          89.1092 |          19.1966 |
[32m[20221213 21:06:48 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:06:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 568.00
[32m[20221213 21:06:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 583.00
[32m[20221213 21:06:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.00
[32m[20221213 21:06:48 @agent_ppo2.py:143][0m Total time:      11.23 min
[32m[20221213 21:06:48 @agent_ppo2.py:145][0m 1081344 total steps have happened
[32m[20221213 21:06:48 @agent_ppo2.py:121][0m #------------------------ Iteration 528 --------------------------#
[32m[20221213 21:06:48 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:06:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:49 @agent_ppo2.py:185][0m |          -0.0043 |          91.7310 |          19.1630 |
[32m[20221213 21:06:49 @agent_ppo2.py:185][0m |          -0.0062 |          90.8780 |          19.1490 |
[32m[20221213 21:06:49 @agent_ppo2.py:185][0m |           0.0041 |          92.8092 |          19.1439 |
[32m[20221213 21:06:49 @agent_ppo2.py:185][0m |          -0.0069 |          90.1475 |          19.1491 |
[32m[20221213 21:06:49 @agent_ppo2.py:185][0m |          -0.0064 |          89.8570 |          19.1461 |
[32m[20221213 21:06:49 @agent_ppo2.py:185][0m |          -0.0077 |          89.6187 |          19.1370 |
[32m[20221213 21:06:49 @agent_ppo2.py:185][0m |          -0.0089 |          89.3595 |          19.1406 |
[32m[20221213 21:06:49 @agent_ppo2.py:185][0m |          -0.0106 |          89.3907 |          19.1455 |
[32m[20221213 21:06:49 @agent_ppo2.py:185][0m |          -0.0090 |          89.3218 |          19.1438 |
[32m[20221213 21:06:50 @agent_ppo2.py:185][0m |          -0.0042 |          89.2644 |          19.1521 |
[32m[20221213 21:06:50 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221213 21:06:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.40
[32m[20221213 21:06:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 575.00
[32m[20221213 21:06:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 601.00
[32m[20221213 21:06:50 @agent_ppo2.py:143][0m Total time:      11.25 min
[32m[20221213 21:06:50 @agent_ppo2.py:145][0m 1083392 total steps have happened
[32m[20221213 21:06:50 @agent_ppo2.py:121][0m #------------------------ Iteration 529 --------------------------#
[32m[20221213 21:06:50 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:06:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:50 @agent_ppo2.py:185][0m |           0.0015 |          91.9565 |          19.1983 |
[32m[20221213 21:06:50 @agent_ppo2.py:185][0m |          -0.0075 |          90.8734 |          19.1829 |
[32m[20221213 21:06:50 @agent_ppo2.py:185][0m |          -0.0011 |          93.2061 |          19.1883 |
[32m[20221213 21:06:50 @agent_ppo2.py:185][0m |          -0.0089 |          90.3047 |          19.1645 |
[32m[20221213 21:06:50 @agent_ppo2.py:185][0m |          -0.0066 |          90.2079 |          19.1799 |
[32m[20221213 21:06:51 @agent_ppo2.py:185][0m |          -0.0082 |          90.0474 |          19.1806 |
[32m[20221213 21:06:51 @agent_ppo2.py:185][0m |          -0.0078 |          89.8789 |          19.1789 |
[32m[20221213 21:06:51 @agent_ppo2.py:185][0m |          -0.0078 |          89.7562 |          19.1793 |
[32m[20221213 21:06:51 @agent_ppo2.py:185][0m |          -0.0091 |          89.6810 |          19.1795 |
[32m[20221213 21:06:51 @agent_ppo2.py:185][0m |          -0.0091 |          89.5175 |          19.1861 |
[32m[20221213 21:06:51 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 21:06:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 561.40
[32m[20221213 21:06:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 622.00
[32m[20221213 21:06:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 596.00
[32m[20221213 21:06:51 @agent_ppo2.py:143][0m Total time:      11.27 min
[32m[20221213 21:06:51 @agent_ppo2.py:145][0m 1085440 total steps have happened
[32m[20221213 21:06:51 @agent_ppo2.py:121][0m #------------------------ Iteration 530 --------------------------#
[32m[20221213 21:06:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:06:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:52 @agent_ppo2.py:185][0m |          -0.0030 |          91.0385 |          19.2242 |
[32m[20221213 21:06:52 @agent_ppo2.py:185][0m |          -0.0073 |          89.6917 |          19.2088 |
[32m[20221213 21:06:52 @agent_ppo2.py:185][0m |          -0.0086 |          89.2383 |          19.1989 |
[32m[20221213 21:06:52 @agent_ppo2.py:185][0m |          -0.0065 |          88.5895 |          19.2093 |
[32m[20221213 21:06:52 @agent_ppo2.py:185][0m |          -0.0101 |          88.3434 |          19.1939 |
[32m[20221213 21:06:52 @agent_ppo2.py:185][0m |          -0.0061 |          88.2856 |          19.1914 |
[32m[20221213 21:06:52 @agent_ppo2.py:185][0m |          -0.0090 |          87.6910 |          19.2018 |
[32m[20221213 21:06:52 @agent_ppo2.py:185][0m |           0.0017 |          96.1181 |          19.1903 |
[32m[20221213 21:06:52 @agent_ppo2.py:185][0m |          -0.0087 |          87.2889 |          19.1870 |
[32m[20221213 21:06:53 @agent_ppo2.py:185][0m |          -0.0086 |          87.2145 |          19.1941 |
[32m[20221213 21:06:53 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 21:06:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.40
[32m[20221213 21:06:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 569.00
[32m[20221213 21:06:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 648.00
[32m[20221213 21:06:53 @agent_ppo2.py:143][0m Total time:      11.30 min
[32m[20221213 21:06:53 @agent_ppo2.py:145][0m 1087488 total steps have happened
[32m[20221213 21:06:53 @agent_ppo2.py:121][0m #------------------------ Iteration 531 --------------------------#
[32m[20221213 21:06:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:06:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:53 @agent_ppo2.py:185][0m |           0.0016 |          95.2093 |          19.1910 |
[32m[20221213 21:06:53 @agent_ppo2.py:185][0m |           0.0058 |          96.2884 |          19.1723 |
[32m[20221213 21:06:53 @agent_ppo2.py:185][0m |          -0.0054 |          92.3645 |          19.1924 |
[32m[20221213 21:06:53 @agent_ppo2.py:185][0m |          -0.0050 |          91.9018 |          19.1980 |
[32m[20221213 21:06:53 @agent_ppo2.py:185][0m |          -0.0059 |          91.1868 |          19.1976 |
[32m[20221213 21:06:53 @agent_ppo2.py:185][0m |          -0.0044 |          90.6363 |          19.1994 |
[32m[20221213 21:06:54 @agent_ppo2.py:185][0m |          -0.0043 |          90.1725 |          19.1952 |
[32m[20221213 21:06:54 @agent_ppo2.py:185][0m |           0.0003 |          91.8760 |          19.2097 |
[32m[20221213 21:06:54 @agent_ppo2.py:185][0m |          -0.0071 |          89.6142 |          19.2053 |
[32m[20221213 21:06:54 @agent_ppo2.py:185][0m |          -0.0029 |          91.2904 |          19.2111 |
[32m[20221213 21:06:54 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:06:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 537.60
[32m[20221213 21:06:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 584.00
[32m[20221213 21:06:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.00
[32m[20221213 21:06:54 @agent_ppo2.py:143][0m Total time:      11.32 min
[32m[20221213 21:06:54 @agent_ppo2.py:145][0m 1089536 total steps have happened
[32m[20221213 21:06:54 @agent_ppo2.py:121][0m #------------------------ Iteration 532 --------------------------#
[32m[20221213 21:06:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:06:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:54 @agent_ppo2.py:185][0m |           0.0115 |          95.9435 |          19.1292 |
[32m[20221213 21:06:54 @agent_ppo2.py:185][0m |          -0.0043 |          88.1199 |          19.1242 |
[32m[20221213 21:06:54 @agent_ppo2.py:185][0m |          -0.0016 |          87.6657 |          19.1115 |
[32m[20221213 21:06:55 @agent_ppo2.py:185][0m |          -0.0042 |          86.9531 |          19.1205 |
[32m[20221213 21:06:55 @agent_ppo2.py:185][0m |           0.0023 |          89.9687 |          19.1101 |
[32m[20221213 21:06:55 @agent_ppo2.py:185][0m |          -0.0052 |          86.4053 |          19.1055 |
[32m[20221213 21:06:55 @agent_ppo2.py:185][0m |          -0.0073 |          86.3756 |          19.1156 |
[32m[20221213 21:06:55 @agent_ppo2.py:185][0m |          -0.0031 |          86.5016 |          19.0948 |
[32m[20221213 21:06:55 @agent_ppo2.py:185][0m |          -0.0038 |          86.6829 |          19.0926 |
[32m[20221213 21:06:55 @agent_ppo2.py:185][0m |          -0.0079 |          86.1130 |          19.0987 |
[32m[20221213 21:06:55 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:06:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.80
[32m[20221213 21:06:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 566.00
[32m[20221213 21:06:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 637.00
[32m[20221213 21:06:55 @agent_ppo2.py:143][0m Total time:      11.34 min
[32m[20221213 21:06:55 @agent_ppo2.py:145][0m 1091584 total steps have happened
[32m[20221213 21:06:55 @agent_ppo2.py:121][0m #------------------------ Iteration 533 --------------------------#
[32m[20221213 21:06:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:06:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:56 @agent_ppo2.py:185][0m |          -0.0054 |          93.9150 |          19.2038 |
[32m[20221213 21:06:56 @agent_ppo2.py:185][0m |          -0.0082 |          92.8795 |          19.1881 |
[32m[20221213 21:06:56 @agent_ppo2.py:185][0m |          -0.0096 |          92.4129 |          19.1794 |
[32m[20221213 21:06:56 @agent_ppo2.py:185][0m |           0.0035 |          95.1005 |          19.1753 |
[32m[20221213 21:06:56 @agent_ppo2.py:185][0m |          -0.0091 |          91.5594 |          19.1726 |
[32m[20221213 21:06:56 @agent_ppo2.py:185][0m |          -0.0012 |          95.7421 |          19.1839 |
[32m[20221213 21:06:56 @agent_ppo2.py:185][0m |          -0.0092 |          91.5220 |          19.1764 |
[32m[20221213 21:06:56 @agent_ppo2.py:185][0m |          -0.0107 |          90.8173 |          19.1805 |
[32m[20221213 21:06:56 @agent_ppo2.py:185][0m |          -0.0152 |          90.7611 |          19.1773 |
[32m[20221213 21:06:56 @agent_ppo2.py:185][0m |          -0.0092 |          90.3674 |          19.1727 |
[32m[20221213 21:06:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:06:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.40
[32m[20221213 21:06:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.00
[32m[20221213 21:06:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 610.00
[32m[20221213 21:06:57 @agent_ppo2.py:143][0m Total time:      11.36 min
[32m[20221213 21:06:57 @agent_ppo2.py:145][0m 1093632 total steps have happened
[32m[20221213 21:06:57 @agent_ppo2.py:121][0m #------------------------ Iteration 534 --------------------------#
[32m[20221213 21:06:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:06:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:57 @agent_ppo2.py:185][0m |          -0.0026 |          91.8654 |          19.2020 |
[32m[20221213 21:06:57 @agent_ppo2.py:185][0m |          -0.0027 |          90.8668 |          19.1857 |
[32m[20221213 21:06:57 @agent_ppo2.py:185][0m |          -0.0015 |          90.6039 |          19.1714 |
[32m[20221213 21:06:57 @agent_ppo2.py:185][0m |           0.0004 |          91.3891 |          19.1706 |
[32m[20221213 21:06:57 @agent_ppo2.py:185][0m |          -0.0061 |          89.8959 |          19.1593 |
[32m[20221213 21:06:57 @agent_ppo2.py:185][0m |          -0.0064 |          89.7854 |          19.1589 |
[32m[20221213 21:06:57 @agent_ppo2.py:185][0m |          -0.0046 |          89.7947 |          19.1540 |
[32m[20221213 21:06:57 @agent_ppo2.py:185][0m |          -0.0061 |          89.5315 |          19.1567 |
[32m[20221213 21:06:58 @agent_ppo2.py:185][0m |          -0.0080 |          89.6317 |          19.1450 |
[32m[20221213 21:06:58 @agent_ppo2.py:185][0m |          -0.0094 |          89.3605 |          19.1433 |
[32m[20221213 21:06:58 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:06:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 545.40
[32m[20221213 21:06:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 574.00
[32m[20221213 21:06:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.00
[32m[20221213 21:06:58 @agent_ppo2.py:143][0m Total time:      11.38 min
[32m[20221213 21:06:58 @agent_ppo2.py:145][0m 1095680 total steps have happened
[32m[20221213 21:06:58 @agent_ppo2.py:121][0m #------------------------ Iteration 535 --------------------------#
[32m[20221213 21:06:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:06:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:58 @agent_ppo2.py:185][0m |           0.0064 |          92.4292 |          19.1227 |
[32m[20221213 21:06:58 @agent_ppo2.py:185][0m |          -0.0043 |          88.9641 |          19.1068 |
[32m[20221213 21:06:58 @agent_ppo2.py:185][0m |           0.0061 |          94.5795 |          19.1088 |
[32m[20221213 21:06:58 @agent_ppo2.py:185][0m |          -0.0057 |          87.2320 |          19.1046 |
[32m[20221213 21:06:58 @agent_ppo2.py:185][0m |          -0.0063 |          86.7984 |          19.1067 |
[32m[20221213 21:06:59 @agent_ppo2.py:185][0m |          -0.0064 |          86.5267 |          19.1018 |
[32m[20221213 21:06:59 @agent_ppo2.py:185][0m |          -0.0019 |          87.3598 |          19.1112 |
[32m[20221213 21:06:59 @agent_ppo2.py:185][0m |          -0.0085 |          86.6134 |          19.1016 |
[32m[20221213 21:06:59 @agent_ppo2.py:185][0m |          -0.0029 |          85.9273 |          19.1040 |
[32m[20221213 21:06:59 @agent_ppo2.py:185][0m |          -0.0026 |          86.4371 |          19.1043 |
[32m[20221213 21:06:59 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:06:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.80
[32m[20221213 21:06:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 592.00
[32m[20221213 21:06:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 602.00
[32m[20221213 21:06:59 @agent_ppo2.py:143][0m Total time:      11.41 min
[32m[20221213 21:06:59 @agent_ppo2.py:145][0m 1097728 total steps have happened
[32m[20221213 21:06:59 @agent_ppo2.py:121][0m #------------------------ Iteration 536 --------------------------#
[32m[20221213 21:06:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:06:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:06:59 @agent_ppo2.py:185][0m |          -0.0008 |          94.9309 |          19.1907 |
[32m[20221213 21:06:59 @agent_ppo2.py:185][0m |          -0.0037 |          94.6221 |          19.1798 |
[32m[20221213 21:06:59 @agent_ppo2.py:185][0m |          -0.0079 |          92.9448 |          19.1668 |
[32m[20221213 21:07:00 @agent_ppo2.py:185][0m |          -0.0056 |          93.0516 |          19.1728 |
[32m[20221213 21:07:00 @agent_ppo2.py:185][0m |          -0.0060 |          92.1306 |          19.1574 |
[32m[20221213 21:07:00 @agent_ppo2.py:185][0m |          -0.0064 |          93.4563 |          19.1659 |
[32m[20221213 21:07:00 @agent_ppo2.py:185][0m |          -0.0125 |          91.5913 |          19.1529 |
[32m[20221213 21:07:00 @agent_ppo2.py:185][0m |          -0.0088 |          91.2530 |          19.1573 |
[32m[20221213 21:07:00 @agent_ppo2.py:185][0m |          -0.0101 |          91.2199 |          19.1593 |
[32m[20221213 21:07:00 @agent_ppo2.py:185][0m |          -0.0127 |          90.8293 |          19.1609 |
[32m[20221213 21:07:00 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:07:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.80
[32m[20221213 21:07:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.00
[32m[20221213 21:07:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.00
[32m[20221213 21:07:00 @agent_ppo2.py:143][0m Total time:      11.43 min
[32m[20221213 21:07:00 @agent_ppo2.py:145][0m 1099776 total steps have happened
[32m[20221213 21:07:00 @agent_ppo2.py:121][0m #------------------------ Iteration 537 --------------------------#
[32m[20221213 21:07:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:01 @agent_ppo2.py:185][0m |          -0.0034 |          89.0772 |          19.2094 |
[32m[20221213 21:07:01 @agent_ppo2.py:185][0m |          -0.0090 |          87.9725 |          19.1899 |
[32m[20221213 21:07:01 @agent_ppo2.py:185][0m |          -0.0082 |          87.2990 |          19.1918 |
[32m[20221213 21:07:01 @agent_ppo2.py:185][0m |          -0.0118 |          87.3464 |          19.1833 |
[32m[20221213 21:07:01 @agent_ppo2.py:185][0m |          -0.0121 |          86.8714 |          19.1665 |
[32m[20221213 21:07:01 @agent_ppo2.py:185][0m |          -0.0125 |          86.7298 |          19.1667 |
[32m[20221213 21:07:01 @agent_ppo2.py:185][0m |          -0.0130 |          86.7387 |          19.1684 |
[32m[20221213 21:07:01 @agent_ppo2.py:185][0m |          -0.0038 |          89.0263 |          19.1632 |
[32m[20221213 21:07:01 @agent_ppo2.py:185][0m |          -0.0102 |          86.2655 |          19.1680 |
[32m[20221213 21:07:01 @agent_ppo2.py:185][0m |          -0.0128 |          86.2075 |          19.1686 |
[32m[20221213 21:07:01 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:07:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 526.60
[32m[20221213 21:07:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 584.00
[32m[20221213 21:07:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 611.00
[32m[20221213 21:07:01 @agent_ppo2.py:143][0m Total time:      11.45 min
[32m[20221213 21:07:01 @agent_ppo2.py:145][0m 1101824 total steps have happened
[32m[20221213 21:07:01 @agent_ppo2.py:121][0m #------------------------ Iteration 538 --------------------------#
[32m[20221213 21:07:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:02 @agent_ppo2.py:185][0m |          -0.0007 |          93.3542 |          19.0743 |
[32m[20221213 21:07:02 @agent_ppo2.py:185][0m |          -0.0038 |          92.4664 |          19.0473 |
[32m[20221213 21:07:02 @agent_ppo2.py:185][0m |          -0.0055 |          91.9440 |          19.0365 |
[32m[20221213 21:07:02 @agent_ppo2.py:185][0m |          -0.0061 |          91.6609 |          19.0381 |
[32m[20221213 21:07:02 @agent_ppo2.py:185][0m |          -0.0042 |          91.6092 |          19.0420 |
[32m[20221213 21:07:02 @agent_ppo2.py:185][0m |          -0.0086 |          91.2168 |          19.0439 |
[32m[20221213 21:07:02 @agent_ppo2.py:185][0m |           0.0037 |         100.1621 |          19.0404 |
[32m[20221213 21:07:02 @agent_ppo2.py:185][0m |          -0.0063 |          90.9130 |          19.0397 |
[32m[20221213 21:07:03 @agent_ppo2.py:185][0m |          -0.0079 |          90.6601 |          19.0442 |
[32m[20221213 21:07:03 @agent_ppo2.py:185][0m |          -0.0112 |          90.6866 |          19.0409 |
[32m[20221213 21:07:03 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:07:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 558.00
[32m[20221213 21:07:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 600.00
[32m[20221213 21:07:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 686.00
[32m[20221213 21:07:03 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 686.00
[32m[20221213 21:07:03 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 686.00
[32m[20221213 21:07:03 @agent_ppo2.py:143][0m Total time:      11.47 min
[32m[20221213 21:07:03 @agent_ppo2.py:145][0m 1103872 total steps have happened
[32m[20221213 21:07:03 @agent_ppo2.py:121][0m #------------------------ Iteration 539 --------------------------#
[32m[20221213 21:07:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:03 @agent_ppo2.py:185][0m |          -0.0011 |          91.2028 |          19.1428 |
[32m[20221213 21:07:03 @agent_ppo2.py:185][0m |          -0.0048 |          89.8719 |          19.1218 |
[32m[20221213 21:07:03 @agent_ppo2.py:185][0m |          -0.0059 |          88.9190 |          19.1020 |
[32m[20221213 21:07:03 @agent_ppo2.py:185][0m |           0.0018 |          91.6669 |          19.1079 |
[32m[20221213 21:07:03 @agent_ppo2.py:185][0m |          -0.0001 |          89.1857 |          19.0985 |
[32m[20221213 21:07:03 @agent_ppo2.py:185][0m |          -0.0059 |          88.1621 |          19.0948 |
[32m[20221213 21:07:04 @agent_ppo2.py:185][0m |          -0.0103 |          88.2338 |          19.0861 |
[32m[20221213 21:07:04 @agent_ppo2.py:185][0m |          -0.0085 |          87.7826 |          19.0870 |
[32m[20221213 21:07:04 @agent_ppo2.py:185][0m |          -0.0062 |          87.6710 |          19.0893 |
[32m[20221213 21:07:04 @agent_ppo2.py:185][0m |          -0.0086 |          87.7838 |          19.0790 |
[32m[20221213 21:07:04 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:07:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 543.80
[32m[20221213 21:07:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 601.00
[32m[20221213 21:07:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 660.00
[32m[20221213 21:07:04 @agent_ppo2.py:143][0m Total time:      11.49 min
[32m[20221213 21:07:04 @agent_ppo2.py:145][0m 1105920 total steps have happened
[32m[20221213 21:07:04 @agent_ppo2.py:121][0m #------------------------ Iteration 540 --------------------------#
[32m[20221213 21:07:04 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:07:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:04 @agent_ppo2.py:185][0m |           0.0014 |          90.3544 |          19.1938 |
[32m[20221213 21:07:04 @agent_ppo2.py:185][0m |          -0.0025 |          89.2506 |          19.1983 |
[32m[20221213 21:07:04 @agent_ppo2.py:185][0m |          -0.0042 |          88.8337 |          19.1680 |
[32m[20221213 21:07:05 @agent_ppo2.py:185][0m |          -0.0050 |          88.6830 |          19.1689 |
[32m[20221213 21:07:05 @agent_ppo2.py:185][0m |          -0.0054 |          88.6367 |          19.1587 |
[32m[20221213 21:07:05 @agent_ppo2.py:185][0m |          -0.0059 |          88.5128 |          19.1477 |
[32m[20221213 21:07:05 @agent_ppo2.py:185][0m |          -0.0075 |          88.4413 |          19.1437 |
[32m[20221213 21:07:05 @agent_ppo2.py:185][0m |           0.0125 |          99.5072 |          19.1329 |
[32m[20221213 21:07:05 @agent_ppo2.py:185][0m |          -0.0025 |          89.3319 |          19.1220 |
[32m[20221213 21:07:05 @agent_ppo2.py:185][0m |          -0.0087 |          88.0302 |          19.1192 |
[32m[20221213 21:07:05 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:07:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 543.00
[32m[20221213 21:07:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 606.00
[32m[20221213 21:07:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 603.00
[32m[20221213 21:07:05 @agent_ppo2.py:143][0m Total time:      11.51 min
[32m[20221213 21:07:05 @agent_ppo2.py:145][0m 1107968 total steps have happened
[32m[20221213 21:07:05 @agent_ppo2.py:121][0m #------------------------ Iteration 541 --------------------------#
[32m[20221213 21:07:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:06 @agent_ppo2.py:185][0m |          -0.0055 |          87.4847 |          19.1635 |
[32m[20221213 21:07:06 @agent_ppo2.py:185][0m |          -0.0060 |          85.9016 |          19.1406 |
[32m[20221213 21:07:06 @agent_ppo2.py:185][0m |          -0.0076 |          85.5678 |          19.1390 |
[32m[20221213 21:07:06 @agent_ppo2.py:185][0m |           0.0030 |          88.0399 |          19.1376 |
[32m[20221213 21:07:06 @agent_ppo2.py:185][0m |          -0.0026 |          85.0732 |          19.1313 |
[32m[20221213 21:07:06 @agent_ppo2.py:185][0m |          -0.0085 |          85.0147 |          19.1299 |
[32m[20221213 21:07:06 @agent_ppo2.py:185][0m |          -0.0086 |          84.7739 |          19.1305 |
[32m[20221213 21:07:06 @agent_ppo2.py:185][0m |          -0.0078 |          84.7214 |          19.1276 |
[32m[20221213 21:07:06 @agent_ppo2.py:185][0m |          -0.0062 |          85.0916 |          19.1252 |
[32m[20221213 21:07:06 @agent_ppo2.py:185][0m |          -0.0063 |          84.6233 |          19.1245 |
[32m[20221213 21:07:06 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:07:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.20
[32m[20221213 21:07:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.00
[32m[20221213 21:07:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 524.00
[32m[20221213 21:07:06 @agent_ppo2.py:143][0m Total time:      11.53 min
[32m[20221213 21:07:06 @agent_ppo2.py:145][0m 1110016 total steps have happened
[32m[20221213 21:07:06 @agent_ppo2.py:121][0m #------------------------ Iteration 542 --------------------------#
[32m[20221213 21:07:07 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:07:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:07 @agent_ppo2.py:185][0m |           0.0017 |          91.6840 |          19.1560 |
[32m[20221213 21:07:07 @agent_ppo2.py:185][0m |          -0.0062 |          89.5609 |          19.1429 |
[32m[20221213 21:07:07 @agent_ppo2.py:185][0m |          -0.0080 |          88.8303 |          19.1339 |
[32m[20221213 21:07:07 @agent_ppo2.py:185][0m |          -0.0072 |          88.3973 |          19.1269 |
[32m[20221213 21:07:07 @agent_ppo2.py:185][0m |          -0.0067 |          88.2774 |          19.1266 |
[32m[20221213 21:07:07 @agent_ppo2.py:185][0m |          -0.0008 |          91.7443 |          19.1271 |
[32m[20221213 21:07:07 @agent_ppo2.py:185][0m |          -0.0089 |          87.8057 |          19.1223 |
[32m[20221213 21:07:07 @agent_ppo2.py:185][0m |          -0.0076 |          87.6139 |          19.1229 |
[32m[20221213 21:07:07 @agent_ppo2.py:185][0m |          -0.0094 |          87.4367 |          19.1238 |
[32m[20221213 21:07:08 @agent_ppo2.py:185][0m |          -0.0059 |          87.7026 |          19.1205 |
[32m[20221213 21:07:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:07:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 556.00
[32m[20221213 21:07:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.00
[32m[20221213 21:07:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 621.00
[32m[20221213 21:07:08 @agent_ppo2.py:143][0m Total time:      11.55 min
[32m[20221213 21:07:08 @agent_ppo2.py:145][0m 1112064 total steps have happened
[32m[20221213 21:07:08 @agent_ppo2.py:121][0m #------------------------ Iteration 543 --------------------------#
[32m[20221213 21:07:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:07:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:08 @agent_ppo2.py:185][0m |          -0.0020 |          92.0833 |          19.1592 |
[32m[20221213 21:07:08 @agent_ppo2.py:185][0m |          -0.0062 |          90.3946 |          19.1324 |
[32m[20221213 21:07:08 @agent_ppo2.py:185][0m |          -0.0037 |          89.7152 |          19.1239 |
[32m[20221213 21:07:08 @agent_ppo2.py:185][0m |          -0.0085 |          89.1543 |          19.1155 |
[32m[20221213 21:07:08 @agent_ppo2.py:185][0m |          -0.0068 |          88.7026 |          19.1128 |
[32m[20221213 21:07:08 @agent_ppo2.py:185][0m |          -0.0012 |          91.9943 |          19.1065 |
[32m[20221213 21:07:09 @agent_ppo2.py:185][0m |           0.0090 |          95.1470 |          19.1006 |
[32m[20221213 21:07:09 @agent_ppo2.py:185][0m |          -0.0080 |          88.0363 |          19.0972 |
[32m[20221213 21:07:09 @agent_ppo2.py:185][0m |          -0.0108 |          87.7659 |          19.0971 |
[32m[20221213 21:07:09 @agent_ppo2.py:185][0m |          -0.0113 |          87.5257 |          19.0827 |
[32m[20221213 21:07:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:07:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.00
[32m[20221213 21:07:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.00
[32m[20221213 21:07:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 577.00
[32m[20221213 21:07:09 @agent_ppo2.py:143][0m Total time:      11.57 min
[32m[20221213 21:07:09 @agent_ppo2.py:145][0m 1114112 total steps have happened
[32m[20221213 21:07:09 @agent_ppo2.py:121][0m #------------------------ Iteration 544 --------------------------#
[32m[20221213 21:07:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:09 @agent_ppo2.py:185][0m |           0.0055 |          90.1804 |          19.2326 |
[32m[20221213 21:07:09 @agent_ppo2.py:185][0m |          -0.0031 |          85.6064 |          19.2198 |
[32m[20221213 21:07:09 @agent_ppo2.py:185][0m |          -0.0053 |          84.9388 |          19.2062 |
[32m[20221213 21:07:10 @agent_ppo2.py:185][0m |          -0.0059 |          84.4258 |          19.2018 |
[32m[20221213 21:07:10 @agent_ppo2.py:185][0m |          -0.0014 |          86.7641 |          19.1888 |
[32m[20221213 21:07:10 @agent_ppo2.py:185][0m |          -0.0089 |          83.9721 |          19.1814 |
[32m[20221213 21:07:10 @agent_ppo2.py:185][0m |          -0.0090 |          83.6649 |          19.1757 |
[32m[20221213 21:07:10 @agent_ppo2.py:185][0m |           0.0015 |          91.9213 |          19.1681 |
[32m[20221213 21:07:10 @agent_ppo2.py:185][0m |          -0.0048 |          83.5286 |          19.1708 |
[32m[20221213 21:07:10 @agent_ppo2.py:185][0m |          -0.0035 |          85.5141 |          19.1543 |
[32m[20221213 21:07:10 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:07:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.20
[32m[20221213 21:07:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.00
[32m[20221213 21:07:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 697.00
[32m[20221213 21:07:10 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 697.00
[32m[20221213 21:07:10 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 697.00
[32m[20221213 21:07:10 @agent_ppo2.py:143][0m Total time:      11.59 min
[32m[20221213 21:07:10 @agent_ppo2.py:145][0m 1116160 total steps have happened
[32m[20221213 21:07:10 @agent_ppo2.py:121][0m #------------------------ Iteration 545 --------------------------#
[32m[20221213 21:07:10 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:07:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:11 @agent_ppo2.py:185][0m |          -0.0025 |          90.2964 |          19.2787 |
[32m[20221213 21:07:11 @agent_ppo2.py:185][0m |          -0.0033 |          89.7516 |          19.2606 |
[32m[20221213 21:07:11 @agent_ppo2.py:185][0m |          -0.0029 |          89.2076 |          19.2471 |
[32m[20221213 21:07:11 @agent_ppo2.py:185][0m |          -0.0065 |          88.7063 |          19.2411 |
[32m[20221213 21:07:11 @agent_ppo2.py:185][0m |          -0.0031 |          89.2350 |          19.2323 |
[32m[20221213 21:07:11 @agent_ppo2.py:185][0m |          -0.0046 |          88.3268 |          19.2457 |
[32m[20221213 21:07:11 @agent_ppo2.py:185][0m |          -0.0060 |          88.3941 |          19.2357 |
[32m[20221213 21:07:11 @agent_ppo2.py:185][0m |           0.0046 |          95.4236 |          19.2407 |
[32m[20221213 21:07:11 @agent_ppo2.py:185][0m |          -0.0084 |          88.0203 |          19.2359 |
[32m[20221213 21:07:11 @agent_ppo2.py:185][0m |          -0.0098 |          87.9662 |          19.2323 |
[32m[20221213 21:07:11 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:07:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 570.20
[32m[20221213 21:07:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 628.00
[32m[20221213 21:07:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 637.00
[32m[20221213 21:07:11 @agent_ppo2.py:143][0m Total time:      11.61 min
[32m[20221213 21:07:11 @agent_ppo2.py:145][0m 1118208 total steps have happened
[32m[20221213 21:07:11 @agent_ppo2.py:121][0m #------------------------ Iteration 546 --------------------------#
[32m[20221213 21:07:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:07:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:12 @agent_ppo2.py:185][0m |          -0.0030 |          88.0719 |          19.0770 |
[32m[20221213 21:07:12 @agent_ppo2.py:185][0m |          -0.0059 |          86.8903 |          19.0550 |
[32m[20221213 21:07:12 @agent_ppo2.py:185][0m |          -0.0047 |          86.4688 |          19.0451 |
[32m[20221213 21:07:12 @agent_ppo2.py:185][0m |          -0.0113 |          86.3096 |          19.0414 |
[32m[20221213 21:07:12 @agent_ppo2.py:185][0m |          -0.0095 |          85.7486 |          19.0392 |
[32m[20221213 21:07:12 @agent_ppo2.py:185][0m |          -0.0081 |          85.5191 |          19.0244 |
[32m[20221213 21:07:12 @agent_ppo2.py:185][0m |          -0.0094 |          85.3037 |          19.0343 |
[32m[20221213 21:07:12 @agent_ppo2.py:185][0m |          -0.0082 |          85.1504 |          19.0335 |
[32m[20221213 21:07:13 @agent_ppo2.py:185][0m |          -0.0061 |          85.1011 |          19.0253 |
[32m[20221213 21:07:13 @agent_ppo2.py:185][0m |          -0.0069 |          84.9764 |          19.0301 |
[32m[20221213 21:07:13 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:07:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 574.40
[32m[20221213 21:07:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 592.00
[32m[20221213 21:07:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.00
[32m[20221213 21:07:13 @agent_ppo2.py:143][0m Total time:      11.63 min
[32m[20221213 21:07:13 @agent_ppo2.py:145][0m 1120256 total steps have happened
[32m[20221213 21:07:13 @agent_ppo2.py:121][0m #------------------------ Iteration 547 --------------------------#
[32m[20221213 21:07:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:13 @agent_ppo2.py:185][0m |           0.0001 |          86.6445 |          19.1449 |
[32m[20221213 21:07:13 @agent_ppo2.py:185][0m |          -0.0052 |          85.7903 |          19.1387 |
[32m[20221213 21:07:13 @agent_ppo2.py:185][0m |           0.0070 |          90.9464 |          19.1314 |
[32m[20221213 21:07:13 @agent_ppo2.py:185][0m |          -0.0055 |          85.1366 |          19.1206 |
[32m[20221213 21:07:13 @agent_ppo2.py:185][0m |          -0.0045 |          84.7274 |          19.1256 |
[32m[20221213 21:07:13 @agent_ppo2.py:185][0m |          -0.0074 |          84.6867 |          19.1218 |
[32m[20221213 21:07:14 @agent_ppo2.py:185][0m |          -0.0069 |          84.2859 |          19.1239 |
[32m[20221213 21:07:14 @agent_ppo2.py:185][0m |          -0.0031 |          85.2466 |          19.1200 |
[32m[20221213 21:07:14 @agent_ppo2.py:185][0m |          -0.0077 |          84.0703 |          19.1112 |
[32m[20221213 21:07:14 @agent_ppo2.py:185][0m |          -0.0083 |          83.9989 |          19.1126 |
[32m[20221213 21:07:14 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:07:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 567.00
[32m[20221213 21:07:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.00
[32m[20221213 21:07:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 619.00
[32m[20221213 21:07:14 @agent_ppo2.py:143][0m Total time:      11.65 min
[32m[20221213 21:07:14 @agent_ppo2.py:145][0m 1122304 total steps have happened
[32m[20221213 21:07:14 @agent_ppo2.py:121][0m #------------------------ Iteration 548 --------------------------#
[32m[20221213 21:07:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:14 @agent_ppo2.py:185][0m |          -0.0006 |          91.4664 |          19.1569 |
[32m[20221213 21:07:14 @agent_ppo2.py:185][0m |          -0.0030 |          90.0343 |          19.1402 |
[32m[20221213 21:07:14 @agent_ppo2.py:185][0m |          -0.0038 |          89.6487 |          19.1372 |
[32m[20221213 21:07:15 @agent_ppo2.py:185][0m |          -0.0028 |          89.2976 |          19.1401 |
[32m[20221213 21:07:15 @agent_ppo2.py:185][0m |          -0.0035 |          89.3929 |          19.1391 |
[32m[20221213 21:07:15 @agent_ppo2.py:185][0m |          -0.0051 |          89.0974 |          19.1264 |
[32m[20221213 21:07:15 @agent_ppo2.py:185][0m |          -0.0021 |          89.2395 |          19.1439 |
[32m[20221213 21:07:15 @agent_ppo2.py:185][0m |          -0.0068 |          88.7718 |          19.1418 |
[32m[20221213 21:07:15 @agent_ppo2.py:185][0m |          -0.0004 |          89.1159 |          19.1364 |
[32m[20221213 21:07:15 @agent_ppo2.py:185][0m |          -0.0079 |          88.6632 |          19.1522 |
[32m[20221213 21:07:15 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:07:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 559.80
[32m[20221213 21:07:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.00
[32m[20221213 21:07:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 660.00
[32m[20221213 21:07:15 @agent_ppo2.py:143][0m Total time:      11.68 min
[32m[20221213 21:07:15 @agent_ppo2.py:145][0m 1124352 total steps have happened
[32m[20221213 21:07:15 @agent_ppo2.py:121][0m #------------------------ Iteration 549 --------------------------#
[32m[20221213 21:07:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:16 @agent_ppo2.py:185][0m |          -0.0019 |          89.8280 |          19.1544 |
[32m[20221213 21:07:16 @agent_ppo2.py:185][0m |           0.0040 |          92.8796 |          19.1442 |
[32m[20221213 21:07:16 @agent_ppo2.py:185][0m |          -0.0052 |          89.0605 |          19.1326 |
[32m[20221213 21:07:16 @agent_ppo2.py:185][0m |          -0.0022 |          88.8876 |          19.1315 |
[32m[20221213 21:07:16 @agent_ppo2.py:185][0m |          -0.0070 |          88.6240 |          19.1223 |
[32m[20221213 21:07:16 @agent_ppo2.py:185][0m |           0.0039 |          94.9267 |          19.1236 |
[32m[20221213 21:07:16 @agent_ppo2.py:185][0m |           0.0022 |          92.8911 |          19.1183 |
[32m[20221213 21:07:16 @agent_ppo2.py:185][0m |          -0.0058 |          88.1474 |          19.1203 |
[32m[20221213 21:07:16 @agent_ppo2.py:185][0m |          -0.0070 |          88.0500 |          19.1137 |
[32m[20221213 21:07:16 @agent_ppo2.py:185][0m |          -0.0004 |          92.7902 |          19.1189 |
[32m[20221213 21:07:16 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:07:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 572.20
[32m[20221213 21:07:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.00
[32m[20221213 21:07:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 637.00
[32m[20221213 21:07:16 @agent_ppo2.py:143][0m Total time:      11.70 min
[32m[20221213 21:07:16 @agent_ppo2.py:145][0m 1126400 total steps have happened
[32m[20221213 21:07:16 @agent_ppo2.py:121][0m #------------------------ Iteration 550 --------------------------#
[32m[20221213 21:07:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:17 @agent_ppo2.py:185][0m |           0.0082 |          97.6723 |          19.1849 |
[32m[20221213 21:07:17 @agent_ppo2.py:185][0m |          -0.0033 |          91.6830 |          19.2013 |
[32m[20221213 21:07:17 @agent_ppo2.py:185][0m |          -0.0063 |          91.0672 |          19.1892 |
[32m[20221213 21:07:17 @agent_ppo2.py:185][0m |          -0.0046 |          90.9837 |          19.1829 |
[32m[20221213 21:07:17 @agent_ppo2.py:185][0m |          -0.0068 |          90.3750 |          19.1940 |
[32m[20221213 21:07:17 @agent_ppo2.py:185][0m |          -0.0086 |          90.1766 |          19.1857 |
[32m[20221213 21:07:17 @agent_ppo2.py:185][0m |          -0.0058 |          89.9237 |          19.1852 |
[32m[20221213 21:07:17 @agent_ppo2.py:185][0m |          -0.0085 |          89.7872 |          19.1874 |
[32m[20221213 21:07:18 @agent_ppo2.py:185][0m |          -0.0102 |          89.6173 |          19.1830 |
[32m[20221213 21:07:18 @agent_ppo2.py:185][0m |          -0.0082 |          89.4662 |          19.1822 |
[32m[20221213 21:07:18 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:07:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 562.20
[32m[20221213 21:07:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.00
[32m[20221213 21:07:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.00
[32m[20221213 21:07:18 @agent_ppo2.py:143][0m Total time:      11.72 min
[32m[20221213 21:07:18 @agent_ppo2.py:145][0m 1128448 total steps have happened
[32m[20221213 21:07:18 @agent_ppo2.py:121][0m #------------------------ Iteration 551 --------------------------#
[32m[20221213 21:07:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:18 @agent_ppo2.py:185][0m |          -0.0000 |          89.9533 |          19.0637 |
[32m[20221213 21:07:18 @agent_ppo2.py:185][0m |          -0.0005 |          90.1067 |          19.0546 |
[32m[20221213 21:07:18 @agent_ppo2.py:185][0m |          -0.0038 |          88.9778 |          19.0506 |
[32m[20221213 21:07:18 @agent_ppo2.py:185][0m |           0.0026 |          92.6964 |          19.0619 |
[32m[20221213 21:07:18 @agent_ppo2.py:185][0m |          -0.0063 |          88.6226 |          19.0696 |
[32m[20221213 21:07:18 @agent_ppo2.py:185][0m |          -0.0070 |          88.3540 |          19.0648 |
[32m[20221213 21:07:19 @agent_ppo2.py:185][0m |          -0.0059 |          88.2207 |          19.0674 |
[32m[20221213 21:07:19 @agent_ppo2.py:185][0m |          -0.0078 |          88.1011 |          19.0704 |
[32m[20221213 21:07:19 @agent_ppo2.py:185][0m |          -0.0078 |          88.0128 |          19.0680 |
[32m[20221213 21:07:19 @agent_ppo2.py:185][0m |          -0.0077 |          87.8746 |          19.0753 |
[32m[20221213 21:07:19 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:07:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 550.60
[32m[20221213 21:07:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 579.00
[32m[20221213 21:07:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 615.00
[32m[20221213 21:07:19 @agent_ppo2.py:143][0m Total time:      11.74 min
[32m[20221213 21:07:19 @agent_ppo2.py:145][0m 1130496 total steps have happened
[32m[20221213 21:07:19 @agent_ppo2.py:121][0m #------------------------ Iteration 552 --------------------------#
[32m[20221213 21:07:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:19 @agent_ppo2.py:185][0m |          -0.0010 |          95.7087 |          19.2371 |
[32m[20221213 21:07:19 @agent_ppo2.py:185][0m |          -0.0065 |          94.7634 |          19.2257 |
[32m[20221213 21:07:19 @agent_ppo2.py:185][0m |          -0.0030 |          95.7566 |          19.2191 |
[32m[20221213 21:07:20 @agent_ppo2.py:185][0m |          -0.0049 |          94.6842 |          19.2306 |
[32m[20221213 21:07:20 @agent_ppo2.py:185][0m |          -0.0092 |          93.8083 |          19.2287 |
[32m[20221213 21:07:20 @agent_ppo2.py:185][0m |          -0.0096 |          93.7532 |          19.2341 |
[32m[20221213 21:07:20 @agent_ppo2.py:185][0m |          -0.0093 |          93.4799 |          19.2328 |
[32m[20221213 21:07:20 @agent_ppo2.py:185][0m |          -0.0113 |          93.4232 |          19.2349 |
[32m[20221213 21:07:20 @agent_ppo2.py:185][0m |          -0.0112 |          93.2738 |          19.2445 |
[32m[20221213 21:07:20 @agent_ppo2.py:185][0m |          -0.0033 |          96.4645 |          19.2445 |
[32m[20221213 21:07:20 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:07:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 586.60
[32m[20221213 21:07:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 616.00
[32m[20221213 21:07:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 598.00
[32m[20221213 21:07:20 @agent_ppo2.py:143][0m Total time:      11.76 min
[32m[20221213 21:07:20 @agent_ppo2.py:145][0m 1132544 total steps have happened
[32m[20221213 21:07:20 @agent_ppo2.py:121][0m #------------------------ Iteration 553 --------------------------#
[32m[20221213 21:07:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:21 @agent_ppo2.py:185][0m |           0.0026 |          91.9411 |          19.1465 |
[32m[20221213 21:07:21 @agent_ppo2.py:185][0m |          -0.0059 |          90.2666 |          19.1299 |
[32m[20221213 21:07:21 @agent_ppo2.py:185][0m |          -0.0056 |          89.8705 |          19.1304 |
[32m[20221213 21:07:21 @agent_ppo2.py:185][0m |          -0.0066 |          90.1042 |          19.1282 |
[32m[20221213 21:07:21 @agent_ppo2.py:185][0m |          -0.0074 |          89.5888 |          19.1100 |
[32m[20221213 21:07:21 @agent_ppo2.py:185][0m |          -0.0071 |          89.5842 |          19.1160 |
[32m[20221213 21:07:21 @agent_ppo2.py:185][0m |          -0.0095 |          89.2973 |          19.1220 |
[32m[20221213 21:07:21 @agent_ppo2.py:185][0m |          -0.0079 |          89.3834 |          19.1152 |
[32m[20221213 21:07:21 @agent_ppo2.py:185][0m |          -0.0085 |          89.2334 |          19.1138 |
[32m[20221213 21:07:21 @agent_ppo2.py:185][0m |          -0.0101 |          89.3152 |          19.1063 |
[32m[20221213 21:07:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:07:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.00
[32m[20221213 21:07:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 616.00
[32m[20221213 21:07:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 559.00
[32m[20221213 21:07:21 @agent_ppo2.py:143][0m Total time:      11.78 min
[32m[20221213 21:07:21 @agent_ppo2.py:145][0m 1134592 total steps have happened
[32m[20221213 21:07:21 @agent_ppo2.py:121][0m #------------------------ Iteration 554 --------------------------#
[32m[20221213 21:07:22 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:07:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:22 @agent_ppo2.py:185][0m |          -0.0030 |          92.0074 |          19.1648 |
[32m[20221213 21:07:22 @agent_ppo2.py:185][0m |          -0.0057 |          91.5512 |          19.1574 |
[32m[20221213 21:07:22 @agent_ppo2.py:185][0m |          -0.0059 |          91.1565 |          19.1546 |
[32m[20221213 21:07:22 @agent_ppo2.py:185][0m |          -0.0048 |          91.0023 |          19.1540 |
[32m[20221213 21:07:22 @agent_ppo2.py:185][0m |          -0.0032 |          91.3179 |          19.1593 |
[32m[20221213 21:07:22 @agent_ppo2.py:185][0m |          -0.0017 |          91.7187 |          19.1620 |
[32m[20221213 21:07:22 @agent_ppo2.py:185][0m |           0.0037 |          98.0770 |          19.1587 |
[32m[20221213 21:07:22 @agent_ppo2.py:185][0m |          -0.0063 |          90.2725 |          19.1558 |
[32m[20221213 21:07:23 @agent_ppo2.py:185][0m |          -0.0094 |          90.1390 |          19.1558 |
[32m[20221213 21:07:23 @agent_ppo2.py:185][0m |          -0.0036 |          91.2001 |          19.1596 |
[32m[20221213 21:07:23 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:07:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.60
[32m[20221213 21:07:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 570.00
[32m[20221213 21:07:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.00
[32m[20221213 21:07:23 @agent_ppo2.py:143][0m Total time:      11.80 min
[32m[20221213 21:07:23 @agent_ppo2.py:145][0m 1136640 total steps have happened
[32m[20221213 21:07:23 @agent_ppo2.py:121][0m #------------------------ Iteration 555 --------------------------#
[32m[20221213 21:07:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:23 @agent_ppo2.py:185][0m |          -0.0012 |          90.5977 |          19.1868 |
[32m[20221213 21:07:23 @agent_ppo2.py:185][0m |          -0.0023 |          89.8397 |          19.1821 |
[32m[20221213 21:07:23 @agent_ppo2.py:185][0m |          -0.0061 |          89.6549 |          19.1710 |
[32m[20221213 21:07:23 @agent_ppo2.py:185][0m |          -0.0020 |          89.7328 |          19.1656 |
[32m[20221213 21:07:23 @agent_ppo2.py:185][0m |          -0.0004 |          90.0012 |          19.1635 |
[32m[20221213 21:07:23 @agent_ppo2.py:185][0m |          -0.0059 |          88.7395 |          19.1605 |
[32m[20221213 21:07:24 @agent_ppo2.py:185][0m |          -0.0060 |          88.5678 |          19.1625 |
[32m[20221213 21:07:24 @agent_ppo2.py:185][0m |          -0.0044 |          88.5261 |          19.1546 |
[32m[20221213 21:07:24 @agent_ppo2.py:185][0m |          -0.0077 |          88.2831 |          19.1550 |
[32m[20221213 21:07:24 @agent_ppo2.py:185][0m |          -0.0080 |          88.1166 |          19.1460 |
[32m[20221213 21:07:24 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:07:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 544.20
[32m[20221213 21:07:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 622.00
[32m[20221213 21:07:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 608.00
[32m[20221213 21:07:24 @agent_ppo2.py:143][0m Total time:      11.82 min
[32m[20221213 21:07:24 @agent_ppo2.py:145][0m 1138688 total steps have happened
[32m[20221213 21:07:24 @agent_ppo2.py:121][0m #------------------------ Iteration 556 --------------------------#
[32m[20221213 21:07:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:07:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:24 @agent_ppo2.py:185][0m |           0.0099 |          99.3289 |          19.2039 |
[32m[20221213 21:07:24 @agent_ppo2.py:185][0m |          -0.0033 |          89.6836 |          19.1749 |
[32m[20221213 21:07:24 @agent_ppo2.py:185][0m |          -0.0058 |          89.1799 |          19.1851 |
[32m[20221213 21:07:25 @agent_ppo2.py:185][0m |          -0.0047 |          88.7908 |          19.1760 |
[32m[20221213 21:07:25 @agent_ppo2.py:185][0m |          -0.0074 |          88.6519 |          19.1771 |
[32m[20221213 21:07:25 @agent_ppo2.py:185][0m |          -0.0078 |          88.5910 |          19.1767 |
[32m[20221213 21:07:25 @agent_ppo2.py:185][0m |          -0.0076 |          88.2452 |          19.1724 |
[32m[20221213 21:07:25 @agent_ppo2.py:185][0m |          -0.0073 |          88.0266 |          19.1606 |
[32m[20221213 21:07:25 @agent_ppo2.py:185][0m |          -0.0071 |          87.9340 |          19.1671 |
[32m[20221213 21:07:25 @agent_ppo2.py:185][0m |          -0.0082 |          87.7880 |          19.1612 |
[32m[20221213 21:07:25 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:07:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 582.00
[32m[20221213 21:07:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 631.00
[32m[20221213 21:07:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 643.00
[32m[20221213 21:07:25 @agent_ppo2.py:143][0m Total time:      11.84 min
[32m[20221213 21:07:25 @agent_ppo2.py:145][0m 1140736 total steps have happened
[32m[20221213 21:07:25 @agent_ppo2.py:121][0m #------------------------ Iteration 557 --------------------------#
[32m[20221213 21:07:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:26 @agent_ppo2.py:185][0m |           0.0009 |          92.6895 |          19.2588 |
[32m[20221213 21:07:26 @agent_ppo2.py:185][0m |          -0.0021 |          92.1913 |          19.2600 |
[32m[20221213 21:07:26 @agent_ppo2.py:185][0m |           0.0018 |          94.5826 |          19.2611 |
[32m[20221213 21:07:26 @agent_ppo2.py:185][0m |           0.0000 |          92.9064 |          19.2550 |
[32m[20221213 21:07:26 @agent_ppo2.py:185][0m |          -0.0049 |          90.9463 |          19.2535 |
[32m[20221213 21:07:26 @agent_ppo2.py:185][0m |          -0.0066 |          90.6910 |          19.2574 |
[32m[20221213 21:07:26 @agent_ppo2.py:185][0m |          -0.0072 |          90.3884 |          19.2575 |
[32m[20221213 21:07:26 @agent_ppo2.py:185][0m |          -0.0031 |          91.8799 |          19.2530 |
[32m[20221213 21:07:26 @agent_ppo2.py:185][0m |          -0.0057 |          90.0586 |          19.2530 |
[32m[20221213 21:07:26 @agent_ppo2.py:185][0m |          -0.0083 |          89.9361 |          19.2536 |
[32m[20221213 21:07:26 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:07:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 563.20
[32m[20221213 21:07:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 621.00
[32m[20221213 21:07:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 650.00
[32m[20221213 21:07:26 @agent_ppo2.py:143][0m Total time:      11.86 min
[32m[20221213 21:07:26 @agent_ppo2.py:145][0m 1142784 total steps have happened
[32m[20221213 21:07:26 @agent_ppo2.py:121][0m #------------------------ Iteration 558 --------------------------#
[32m[20221213 21:07:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:27 @agent_ppo2.py:185][0m |          -0.0009 |          89.8833 |          19.2125 |
[32m[20221213 21:07:27 @agent_ppo2.py:185][0m |           0.0056 |          92.8543 |          19.2023 |
[32m[20221213 21:07:27 @agent_ppo2.py:185][0m |           0.0074 |          94.5150 |          19.2011 |
[32m[20221213 21:07:27 @agent_ppo2.py:185][0m |           0.0140 |          98.1850 |          19.2001 |
[32m[20221213 21:07:27 @agent_ppo2.py:185][0m |          -0.0046 |          86.7264 |          19.2030 |
[32m[20221213 21:07:27 @agent_ppo2.py:185][0m |          -0.0049 |          86.5114 |          19.2009 |
[32m[20221213 21:07:27 @agent_ppo2.py:185][0m |          -0.0039 |          86.1281 |          19.1995 |
[32m[20221213 21:07:27 @agent_ppo2.py:185][0m |          -0.0073 |          85.9661 |          19.2043 |
[32m[20221213 21:07:28 @agent_ppo2.py:185][0m |          -0.0060 |          85.7703 |          19.1973 |
[32m[20221213 21:07:28 @agent_ppo2.py:185][0m |          -0.0073 |          85.4904 |          19.2044 |
[32m[20221213 21:07:28 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:07:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.00
[32m[20221213 21:07:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 21:07:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 657.00
[32m[20221213 21:07:28 @agent_ppo2.py:143][0m Total time:      11.88 min
[32m[20221213 21:07:28 @agent_ppo2.py:145][0m 1144832 total steps have happened
[32m[20221213 21:07:28 @agent_ppo2.py:121][0m #------------------------ Iteration 559 --------------------------#
[32m[20221213 21:07:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:07:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:28 @agent_ppo2.py:185][0m |          -0.0013 |          91.0850 |          19.0298 |
[32m[20221213 21:07:28 @agent_ppo2.py:185][0m |          -0.0091 |          90.3397 |          19.0232 |
[32m[20221213 21:07:28 @agent_ppo2.py:185][0m |          -0.0065 |          89.9083 |          19.0162 |
[32m[20221213 21:07:28 @agent_ppo2.py:185][0m |          -0.0102 |          89.6277 |          19.0021 |
[32m[20221213 21:07:28 @agent_ppo2.py:185][0m |          -0.0096 |          89.4049 |          19.0022 |
[32m[20221213 21:07:29 @agent_ppo2.py:185][0m |          -0.0026 |          92.1586 |          18.9968 |
[32m[20221213 21:07:29 @agent_ppo2.py:185][0m |          -0.0080 |          89.1754 |          18.9879 |
[32m[20221213 21:07:29 @agent_ppo2.py:185][0m |          -0.0110 |          89.0249 |          18.9934 |
[32m[20221213 21:07:29 @agent_ppo2.py:185][0m |          -0.0121 |          89.0314 |          18.9887 |
[32m[20221213 21:07:29 @agent_ppo2.py:185][0m |          -0.0103 |          88.8860 |          18.9854 |
[32m[20221213 21:07:29 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:07:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 557.40
[32m[20221213 21:07:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 593.00
[32m[20221213 21:07:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 636.00
[32m[20221213 21:07:29 @agent_ppo2.py:143][0m Total time:      11.91 min
[32m[20221213 21:07:29 @agent_ppo2.py:145][0m 1146880 total steps have happened
[32m[20221213 21:07:29 @agent_ppo2.py:121][0m #------------------------ Iteration 560 --------------------------#
[32m[20221213 21:07:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:29 @agent_ppo2.py:185][0m |           0.0007 |          93.0912 |          19.2474 |
[32m[20221213 21:07:29 @agent_ppo2.py:185][0m |           0.0052 |          97.2882 |          19.2152 |
[32m[20221213 21:07:30 @agent_ppo2.py:185][0m |          -0.0049 |          91.4734 |          19.1918 |
[32m[20221213 21:07:30 @agent_ppo2.py:185][0m |          -0.0072 |          91.0945 |          19.2015 |
[32m[20221213 21:07:30 @agent_ppo2.py:185][0m |          -0.0014 |          91.3998 |          19.1974 |
[32m[20221213 21:07:30 @agent_ppo2.py:185][0m |          -0.0073 |          90.5324 |          19.2023 |
[32m[20221213 21:07:30 @agent_ppo2.py:185][0m |           0.0072 |         101.3541 |          19.1845 |
[32m[20221213 21:07:30 @agent_ppo2.py:185][0m |          -0.0086 |          90.2467 |          19.1825 |
[32m[20221213 21:07:30 @agent_ppo2.py:185][0m |          -0.0087 |          89.8954 |          19.1804 |
[32m[20221213 21:07:30 @agent_ppo2.py:185][0m |          -0.0102 |          89.9309 |          19.1854 |
[32m[20221213 21:07:30 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:07:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 563.40
[32m[20221213 21:07:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 21:07:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 634.00
[32m[20221213 21:07:30 @agent_ppo2.py:143][0m Total time:      11.93 min
[32m[20221213 21:07:30 @agent_ppo2.py:145][0m 1148928 total steps have happened
[32m[20221213 21:07:30 @agent_ppo2.py:121][0m #------------------------ Iteration 561 --------------------------#
[32m[20221213 21:07:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:31 @agent_ppo2.py:185][0m |          -0.0009 |          92.9941 |          19.2296 |
[32m[20221213 21:07:31 @agent_ppo2.py:185][0m |           0.0029 |          95.6840 |          19.2150 |
[32m[20221213 21:07:31 @agent_ppo2.py:185][0m |          -0.0042 |          91.7141 |          19.2087 |
[32m[20221213 21:07:31 @agent_ppo2.py:185][0m |          -0.0064 |          91.1767 |          19.2144 |
[32m[20221213 21:07:31 @agent_ppo2.py:185][0m |          -0.0071 |          90.9190 |          19.2100 |
[32m[20221213 21:07:31 @agent_ppo2.py:185][0m |          -0.0072 |          90.7095 |          19.2071 |
[32m[20221213 21:07:31 @agent_ppo2.py:185][0m |          -0.0075 |          90.5098 |          19.2146 |
[32m[20221213 21:07:31 @agent_ppo2.py:185][0m |          -0.0082 |          90.4218 |          19.2114 |
[32m[20221213 21:07:31 @agent_ppo2.py:185][0m |          -0.0081 |          90.2355 |          19.2212 |
[32m[20221213 21:07:31 @agent_ppo2.py:185][0m |          -0.0104 |          90.4674 |          19.2149 |
[32m[20221213 21:07:31 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:07:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 561.80
[32m[20221213 21:07:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 21:07:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.00
[32m[20221213 21:07:32 @agent_ppo2.py:143][0m Total time:      11.95 min
[32m[20221213 21:07:32 @agent_ppo2.py:145][0m 1150976 total steps have happened
[32m[20221213 21:07:32 @agent_ppo2.py:121][0m #------------------------ Iteration 562 --------------------------#
[32m[20221213 21:07:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:32 @agent_ppo2.py:185][0m |          -0.0007 |          94.0158 |          19.2609 |
[32m[20221213 21:07:32 @agent_ppo2.py:185][0m |          -0.0043 |          93.3270 |          19.2634 |
[32m[20221213 21:07:32 @agent_ppo2.py:185][0m |          -0.0043 |          92.9370 |          19.2554 |
[32m[20221213 21:07:32 @agent_ppo2.py:185][0m |          -0.0052 |          92.3967 |          19.2672 |
[32m[20221213 21:07:32 @agent_ppo2.py:185][0m |          -0.0037 |          92.2737 |          19.2656 |
[32m[20221213 21:07:32 @agent_ppo2.py:185][0m |          -0.0062 |          91.9597 |          19.2698 |
[32m[20221213 21:07:32 @agent_ppo2.py:185][0m |          -0.0038 |          91.8945 |          19.2700 |
[32m[20221213 21:07:32 @agent_ppo2.py:185][0m |          -0.0026 |          91.7937 |          19.2687 |
[32m[20221213 21:07:33 @agent_ppo2.py:185][0m |          -0.0043 |          91.4475 |          19.2733 |
[32m[20221213 21:07:33 @agent_ppo2.py:185][0m |          -0.0089 |          91.4833 |          19.2684 |
[32m[20221213 21:07:33 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:07:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 567.00
[32m[20221213 21:07:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.00
[32m[20221213 21:07:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 579.00
[32m[20221213 21:07:33 @agent_ppo2.py:143][0m Total time:      11.97 min
[32m[20221213 21:07:33 @agent_ppo2.py:145][0m 1153024 total steps have happened
[32m[20221213 21:07:33 @agent_ppo2.py:121][0m #------------------------ Iteration 563 --------------------------#
[32m[20221213 21:07:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:33 @agent_ppo2.py:185][0m |          -0.0020 |          95.2624 |          19.2276 |
[32m[20221213 21:07:33 @agent_ppo2.py:185][0m |          -0.0035 |          94.2578 |          19.2251 |
[32m[20221213 21:07:33 @agent_ppo2.py:185][0m |          -0.0065 |          93.8905 |          19.2116 |
[32m[20221213 21:07:33 @agent_ppo2.py:185][0m |          -0.0064 |          93.4964 |          19.2130 |
[32m[20221213 21:07:33 @agent_ppo2.py:185][0m |          -0.0089 |          93.4218 |          19.2176 |
[32m[20221213 21:07:34 @agent_ppo2.py:185][0m |          -0.0083 |          93.1816 |          19.2218 |
[32m[20221213 21:07:34 @agent_ppo2.py:185][0m |          -0.0027 |          95.6382 |          19.2195 |
[32m[20221213 21:07:34 @agent_ppo2.py:185][0m |          -0.0088 |          92.8212 |          19.2302 |
[32m[20221213 21:07:34 @agent_ppo2.py:185][0m |          -0.0071 |          92.6134 |          19.2225 |
[32m[20221213 21:07:34 @agent_ppo2.py:185][0m |          -0.0078 |          92.4169 |          19.2357 |
[32m[20221213 21:07:34 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:07:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 577.40
[32m[20221213 21:07:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 601.00
[32m[20221213 21:07:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 707.00
[32m[20221213 21:07:34 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 707.00
[32m[20221213 21:07:34 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 707.00
[32m[20221213 21:07:34 @agent_ppo2.py:143][0m Total time:      11.99 min
[32m[20221213 21:07:34 @agent_ppo2.py:145][0m 1155072 total steps have happened
[32m[20221213 21:07:34 @agent_ppo2.py:121][0m #------------------------ Iteration 564 --------------------------#
[32m[20221213 21:07:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:34 @agent_ppo2.py:185][0m |          -0.0010 |          92.3668 |          19.1055 |
[32m[20221213 21:07:35 @agent_ppo2.py:185][0m |          -0.0023 |          91.3712 |          19.0994 |
[32m[20221213 21:07:35 @agent_ppo2.py:185][0m |          -0.0016 |          91.2606 |          19.0935 |
[32m[20221213 21:07:35 @agent_ppo2.py:185][0m |          -0.0051 |          90.5558 |          19.0994 |
[32m[20221213 21:07:35 @agent_ppo2.py:185][0m |          -0.0030 |          90.0872 |          19.0926 |
[32m[20221213 21:07:35 @agent_ppo2.py:185][0m |          -0.0059 |          90.0808 |          19.1043 |
[32m[20221213 21:07:35 @agent_ppo2.py:185][0m |          -0.0065 |          89.7998 |          19.1019 |
[32m[20221213 21:07:35 @agent_ppo2.py:185][0m |          -0.0012 |          91.7066 |          19.1117 |
[32m[20221213 21:07:35 @agent_ppo2.py:185][0m |          -0.0074 |          89.4306 |          19.1153 |
[32m[20221213 21:07:35 @agent_ppo2.py:185][0m |          -0.0079 |          89.2767 |          19.1124 |
[32m[20221213 21:07:35 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 21:07:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 577.20
[32m[20221213 21:07:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 625.00
[32m[20221213 21:07:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 618.00
[32m[20221213 21:07:35 @agent_ppo2.py:143][0m Total time:      12.01 min
[32m[20221213 21:07:35 @agent_ppo2.py:145][0m 1157120 total steps have happened
[32m[20221213 21:07:36 @agent_ppo2.py:121][0m #------------------------ Iteration 565 --------------------------#
[32m[20221213 21:07:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:36 @agent_ppo2.py:185][0m |          -0.0033 |          93.8312 |          19.0785 |
[32m[20221213 21:07:36 @agent_ppo2.py:185][0m |          -0.0069 |          91.3489 |          19.0707 |
[32m[20221213 21:07:36 @agent_ppo2.py:185][0m |          -0.0082 |          90.1018 |          19.0730 |
[32m[20221213 21:07:36 @agent_ppo2.py:185][0m |          -0.0082 |          89.4313 |          19.0671 |
[32m[20221213 21:07:36 @agent_ppo2.py:185][0m |           0.0033 |          99.3702 |          19.0731 |
[32m[20221213 21:07:36 @agent_ppo2.py:185][0m |          -0.0013 |          92.1722 |          19.0700 |
[32m[20221213 21:07:36 @agent_ppo2.py:185][0m |          -0.0093 |          86.4735 |          19.0672 |
[32m[20221213 21:07:36 @agent_ppo2.py:185][0m |          -0.0006 |          88.2932 |          19.0891 |
[32m[20221213 21:07:37 @agent_ppo2.py:185][0m |          -0.0094 |          85.4579 |          19.0819 |
[32m[20221213 21:07:37 @agent_ppo2.py:185][0m |          -0.0133 |          85.0868 |          19.0819 |
[32m[20221213 21:07:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:07:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 558.00
[32m[20221213 21:07:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 574.00
[32m[20221213 21:07:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 627.00
[32m[20221213 21:07:37 @agent_ppo2.py:143][0m Total time:      12.04 min
[32m[20221213 21:07:37 @agent_ppo2.py:145][0m 1159168 total steps have happened
[32m[20221213 21:07:37 @agent_ppo2.py:121][0m #------------------------ Iteration 566 --------------------------#
[32m[20221213 21:07:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:37 @agent_ppo2.py:185][0m |           0.0084 |         107.7162 |          19.2328 |
[32m[20221213 21:07:37 @agent_ppo2.py:185][0m |           0.0132 |         108.4561 |          19.2125 |
[32m[20221213 21:07:37 @agent_ppo2.py:185][0m |          -0.0057 |          96.8100 |          19.2027 |
[32m[20221213 21:07:37 @agent_ppo2.py:185][0m |          -0.0083 |          96.2429 |          19.2041 |
[32m[20221213 21:07:37 @agent_ppo2.py:185][0m |          -0.0079 |          95.9324 |          19.1962 |
[32m[20221213 21:07:38 @agent_ppo2.py:185][0m |          -0.0089 |          95.6765 |          19.1965 |
[32m[20221213 21:07:38 @agent_ppo2.py:185][0m |          -0.0054 |          95.6488 |          19.1982 |
[32m[20221213 21:07:38 @agent_ppo2.py:185][0m |          -0.0077 |          95.4121 |          19.1905 |
[32m[20221213 21:07:38 @agent_ppo2.py:185][0m |          -0.0071 |          95.0992 |          19.1931 |
[32m[20221213 21:07:38 @agent_ppo2.py:185][0m |          -0.0076 |          95.6938 |          19.1928 |
[32m[20221213 21:07:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:07:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 559.20
[32m[20221213 21:07:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 609.00
[32m[20221213 21:07:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 659.00
[32m[20221213 21:07:38 @agent_ppo2.py:143][0m Total time:      12.06 min
[32m[20221213 21:07:38 @agent_ppo2.py:145][0m 1161216 total steps have happened
[32m[20221213 21:07:38 @agent_ppo2.py:121][0m #------------------------ Iteration 567 --------------------------#
[32m[20221213 21:07:38 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:07:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:38 @agent_ppo2.py:185][0m |           0.0019 |          92.6647 |          19.1460 |
[32m[20221213 21:07:38 @agent_ppo2.py:185][0m |           0.0011 |          91.7425 |          19.1387 |
[32m[20221213 21:07:38 @agent_ppo2.py:185][0m |          -0.0039 |          90.4437 |          19.1344 |
[32m[20221213 21:07:39 @agent_ppo2.py:185][0m |          -0.0071 |          90.1880 |          19.1218 |
[32m[20221213 21:07:39 @agent_ppo2.py:185][0m |          -0.0070 |          89.9092 |          19.1243 |
[32m[20221213 21:07:39 @agent_ppo2.py:185][0m |          -0.0081 |          89.7142 |          19.1112 |
[32m[20221213 21:07:39 @agent_ppo2.py:185][0m |          -0.0011 |          91.0927 |          19.1234 |
[32m[20221213 21:07:39 @agent_ppo2.py:185][0m |           0.0034 |          96.5185 |          19.1143 |
[32m[20221213 21:07:39 @agent_ppo2.py:185][0m |          -0.0034 |          90.4205 |          19.1128 |
[32m[20221213 21:07:39 @agent_ppo2.py:185][0m |          -0.0066 |          89.1977 |          19.1081 |
[32m[20221213 21:07:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:07:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.40
[32m[20221213 21:07:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 636.00
[32m[20221213 21:07:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 643.00
[32m[20221213 21:07:39 @agent_ppo2.py:143][0m Total time:      12.08 min
[32m[20221213 21:07:39 @agent_ppo2.py:145][0m 1163264 total steps have happened
[32m[20221213 21:07:39 @agent_ppo2.py:121][0m #------------------------ Iteration 568 --------------------------#
[32m[20221213 21:07:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:40 @agent_ppo2.py:185][0m |          -0.0031 |          93.6057 |          19.1974 |
[32m[20221213 21:07:40 @agent_ppo2.py:185][0m |          -0.0025 |          92.8977 |          19.1804 |
[32m[20221213 21:07:40 @agent_ppo2.py:185][0m |          -0.0040 |          92.6638 |          19.1753 |
[32m[20221213 21:07:40 @agent_ppo2.py:185][0m |          -0.0076 |          92.5754 |          19.1782 |
[32m[20221213 21:07:40 @agent_ppo2.py:185][0m |           0.0149 |         106.0398 |          19.1773 |
[32m[20221213 21:07:40 @agent_ppo2.py:185][0m |           0.0043 |         101.7496 |          19.1580 |
[32m[20221213 21:07:40 @agent_ppo2.py:185][0m |          -0.0045 |          92.0607 |          19.1684 |
[32m[20221213 21:07:40 @agent_ppo2.py:185][0m |          -0.0070 |          91.9929 |          19.1786 |
[32m[20221213 21:07:40 @agent_ppo2.py:185][0m |          -0.0061 |          91.8176 |          19.1775 |
[32m[20221213 21:07:40 @agent_ppo2.py:185][0m |          -0.0076 |          91.7501 |          19.1744 |
[32m[20221213 21:07:40 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:07:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 607.40
[32m[20221213 21:07:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 663.00
[32m[20221213 21:07:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 640.00
[32m[20221213 21:07:41 @agent_ppo2.py:143][0m Total time:      12.10 min
[32m[20221213 21:07:41 @agent_ppo2.py:145][0m 1165312 total steps have happened
[32m[20221213 21:07:41 @agent_ppo2.py:121][0m #------------------------ Iteration 569 --------------------------#
[32m[20221213 21:07:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:07:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:41 @agent_ppo2.py:185][0m |           0.0117 |         104.9360 |          19.0400 |
[32m[20221213 21:07:41 @agent_ppo2.py:185][0m |           0.0116 |          98.2521 |          19.0095 |
[32m[20221213 21:07:41 @agent_ppo2.py:185][0m |          -0.0043 |          91.2988 |          19.0078 |
[32m[20221213 21:07:41 @agent_ppo2.py:185][0m |          -0.0038 |          91.3382 |          19.0052 |
[32m[20221213 21:07:41 @agent_ppo2.py:185][0m |           0.0021 |          96.1651 |          18.9952 |
[32m[20221213 21:07:41 @agent_ppo2.py:185][0m |          -0.0088 |          90.5635 |          18.9870 |
[32m[20221213 21:07:41 @agent_ppo2.py:185][0m |          -0.0090 |          90.3106 |          18.9822 |
[32m[20221213 21:07:41 @agent_ppo2.py:185][0m |          -0.0077 |          90.2366 |          18.9814 |
[32m[20221213 21:07:42 @agent_ppo2.py:185][0m |          -0.0072 |          89.9765 |          18.9837 |
[32m[20221213 21:07:42 @agent_ppo2.py:185][0m |          -0.0100 |          89.9357 |          18.9724 |
[32m[20221213 21:07:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:07:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 562.80
[32m[20221213 21:07:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.00
[32m[20221213 21:07:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 640.00
[32m[20221213 21:07:42 @agent_ppo2.py:143][0m Total time:      12.12 min
[32m[20221213 21:07:42 @agent_ppo2.py:145][0m 1167360 total steps have happened
[32m[20221213 21:07:42 @agent_ppo2.py:121][0m #------------------------ Iteration 570 --------------------------#
[32m[20221213 21:07:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:42 @agent_ppo2.py:185][0m |          -0.0019 |          93.3555 |          19.3369 |
[32m[20221213 21:07:42 @agent_ppo2.py:185][0m |          -0.0000 |          90.6475 |          19.3254 |
[32m[20221213 21:07:42 @agent_ppo2.py:185][0m |           0.0015 |          93.9059 |          19.3354 |
[32m[20221213 21:07:42 @agent_ppo2.py:185][0m |           0.0023 |          88.4057 |          19.3166 |
[32m[20221213 21:07:42 @agent_ppo2.py:185][0m |          -0.0070 |          84.5877 |          19.3351 |
[32m[20221213 21:07:42 @agent_ppo2.py:185][0m |          -0.0068 |          83.4704 |          19.3421 |
[32m[20221213 21:07:43 @agent_ppo2.py:185][0m |          -0.0083 |          83.1229 |          19.3482 |
[32m[20221213 21:07:43 @agent_ppo2.py:185][0m |          -0.0073 |          82.1092 |          19.3521 |
[32m[20221213 21:07:43 @agent_ppo2.py:185][0m |           0.0008 |          82.1300 |          19.3590 |
[32m[20221213 21:07:43 @agent_ppo2.py:185][0m |          -0.0092 |          81.3008 |          19.3647 |
[32m[20221213 21:07:43 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:07:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.40
[32m[20221213 21:07:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.00
[32m[20221213 21:07:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.00
[32m[20221213 21:07:43 @agent_ppo2.py:143][0m Total time:      12.14 min
[32m[20221213 21:07:43 @agent_ppo2.py:145][0m 1169408 total steps have happened
[32m[20221213 21:07:43 @agent_ppo2.py:121][0m #------------------------ Iteration 571 --------------------------#
[32m[20221213 21:07:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:43 @agent_ppo2.py:185][0m |           0.0003 |          92.5708 |          19.3264 |
[32m[20221213 21:07:43 @agent_ppo2.py:185][0m |           0.0003 |          92.2615 |          19.3217 |
[32m[20221213 21:07:43 @agent_ppo2.py:185][0m |          -0.0051 |          91.6014 |          19.3103 |
[32m[20221213 21:07:43 @agent_ppo2.py:185][0m |          -0.0025 |          91.2403 |          19.3161 |
[32m[20221213 21:07:44 @agent_ppo2.py:185][0m |          -0.0059 |          90.9972 |          19.3146 |
[32m[20221213 21:07:44 @agent_ppo2.py:185][0m |          -0.0062 |          90.8200 |          19.3208 |
[32m[20221213 21:07:44 @agent_ppo2.py:185][0m |          -0.0077 |          90.7966 |          19.3197 |
[32m[20221213 21:07:44 @agent_ppo2.py:185][0m |          -0.0037 |          90.5256 |          19.3156 |
[32m[20221213 21:07:44 @agent_ppo2.py:185][0m |          -0.0054 |          90.3898 |          19.3247 |
[32m[20221213 21:07:44 @agent_ppo2.py:185][0m |          -0.0071 |          90.5895 |          19.3191 |
[32m[20221213 21:07:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:07:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 555.20
[32m[20221213 21:07:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.00
[32m[20221213 21:07:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 584.00
[32m[20221213 21:07:44 @agent_ppo2.py:143][0m Total time:      12.16 min
[32m[20221213 21:07:44 @agent_ppo2.py:145][0m 1171456 total steps have happened
[32m[20221213 21:07:44 @agent_ppo2.py:121][0m #------------------------ Iteration 572 --------------------------#
[32m[20221213 21:07:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:07:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:44 @agent_ppo2.py:185][0m |           0.0031 |          98.5143 |          19.1480 |
[32m[20221213 21:07:45 @agent_ppo2.py:185][0m |          -0.0037 |          97.0904 |          19.1259 |
[32m[20221213 21:07:45 @agent_ppo2.py:185][0m |          -0.0072 |          96.6074 |          19.1222 |
[32m[20221213 21:07:45 @agent_ppo2.py:185][0m |           0.0000 |          97.6367 |          19.1164 |
[32m[20221213 21:07:45 @agent_ppo2.py:185][0m |          -0.0064 |          95.8847 |          19.1152 |
[32m[20221213 21:07:45 @agent_ppo2.py:185][0m |          -0.0080 |          95.7581 |          19.1039 |
[32m[20221213 21:07:45 @agent_ppo2.py:185][0m |          -0.0054 |          95.5055 |          19.1164 |
[32m[20221213 21:07:45 @agent_ppo2.py:185][0m |          -0.0059 |          95.3594 |          19.0978 |
[32m[20221213 21:07:45 @agent_ppo2.py:185][0m |          -0.0022 |          97.0652 |          19.0952 |
[32m[20221213 21:07:45 @agent_ppo2.py:185][0m |          -0.0060 |          94.9955 |          19.0973 |
[32m[20221213 21:07:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:07:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.40
[32m[20221213 21:07:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 621.00
[32m[20221213 21:07:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.00
[32m[20221213 21:07:45 @agent_ppo2.py:143][0m Total time:      12.18 min
[32m[20221213 21:07:45 @agent_ppo2.py:145][0m 1173504 total steps have happened
[32m[20221213 21:07:45 @agent_ppo2.py:121][0m #------------------------ Iteration 573 --------------------------#
[32m[20221213 21:07:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:46 @agent_ppo2.py:185][0m |           0.0013 |          94.6772 |          19.1013 |
[32m[20221213 21:07:46 @agent_ppo2.py:185][0m |          -0.0109 |          91.8933 |          19.0648 |
[32m[20221213 21:07:46 @agent_ppo2.py:185][0m |          -0.0073 |          91.2146 |          19.0462 |
[32m[20221213 21:07:46 @agent_ppo2.py:185][0m |          -0.0081 |          90.9492 |          19.0556 |
[32m[20221213 21:07:46 @agent_ppo2.py:185][0m |          -0.0059 |          91.4093 |          19.0498 |
[32m[20221213 21:07:46 @agent_ppo2.py:185][0m |          -0.0089 |          90.4287 |          19.0544 |
[32m[20221213 21:07:46 @agent_ppo2.py:185][0m |          -0.0075 |          90.2701 |          19.0571 |
[32m[20221213 21:07:46 @agent_ppo2.py:185][0m |          -0.0089 |          90.0339 |          19.0665 |
[32m[20221213 21:07:46 @agent_ppo2.py:185][0m |          -0.0097 |          90.2378 |          19.0602 |
[32m[20221213 21:07:46 @agent_ppo2.py:185][0m |          -0.0048 |          91.7914 |          19.0671 |
[32m[20221213 21:07:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:07:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.80
[32m[20221213 21:07:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.00
[32m[20221213 21:07:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.00
[32m[20221213 21:07:47 @agent_ppo2.py:143][0m Total time:      12.20 min
[32m[20221213 21:07:47 @agent_ppo2.py:145][0m 1175552 total steps have happened
[32m[20221213 21:07:47 @agent_ppo2.py:121][0m #------------------------ Iteration 574 --------------------------#
[32m[20221213 21:07:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:47 @agent_ppo2.py:185][0m |          -0.0033 |          97.5354 |          19.2458 |
[32m[20221213 21:07:47 @agent_ppo2.py:185][0m |          -0.0060 |          96.4661 |          19.2239 |
[32m[20221213 21:07:47 @agent_ppo2.py:185][0m |          -0.0046 |          95.9607 |          19.2105 |
[32m[20221213 21:07:47 @agent_ppo2.py:185][0m |          -0.0069 |          95.5395 |          19.1996 |
[32m[20221213 21:07:47 @agent_ppo2.py:185][0m |          -0.0072 |          95.1230 |          19.1949 |
[32m[20221213 21:07:47 @agent_ppo2.py:185][0m |          -0.0085 |          94.8742 |          19.1881 |
[32m[20221213 21:07:47 @agent_ppo2.py:185][0m |          -0.0116 |          94.6864 |          19.1911 |
[32m[20221213 21:07:47 @agent_ppo2.py:185][0m |          -0.0087 |          94.4576 |          19.1814 |
[32m[20221213 21:07:48 @agent_ppo2.py:185][0m |          -0.0089 |          94.4294 |          19.1794 |
[32m[20221213 21:07:48 @agent_ppo2.py:185][0m |          -0.0092 |          94.0965 |          19.1780 |
[32m[20221213 21:07:48 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:07:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 559.00
[32m[20221213 21:07:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.00
[32m[20221213 21:07:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 533.00
[32m[20221213 21:07:48 @agent_ppo2.py:143][0m Total time:      12.22 min
[32m[20221213 21:07:48 @agent_ppo2.py:145][0m 1177600 total steps have happened
[32m[20221213 21:07:48 @agent_ppo2.py:121][0m #------------------------ Iteration 575 --------------------------#
[32m[20221213 21:07:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:48 @agent_ppo2.py:185][0m |          -0.0022 |          91.8544 |          19.0931 |
[32m[20221213 21:07:48 @agent_ppo2.py:185][0m |          -0.0042 |          91.0583 |          19.0871 |
[32m[20221213 21:07:48 @agent_ppo2.py:185][0m |           0.0022 |          98.4576 |          19.0726 |
[32m[20221213 21:07:48 @agent_ppo2.py:185][0m |          -0.0068 |          90.5408 |          19.0644 |
[32m[20221213 21:07:48 @agent_ppo2.py:185][0m |          -0.0053 |          90.5908 |          19.0661 |
[32m[20221213 21:07:48 @agent_ppo2.py:185][0m |          -0.0081 |          90.2617 |          19.0518 |
[32m[20221213 21:07:49 @agent_ppo2.py:185][0m |          -0.0092 |          90.0869 |          19.0488 |
[32m[20221213 21:07:49 @agent_ppo2.py:185][0m |          -0.0079 |          90.1044 |          19.0468 |
[32m[20221213 21:07:49 @agent_ppo2.py:185][0m |          -0.0072 |          89.7697 |          19.0436 |
[32m[20221213 21:07:49 @agent_ppo2.py:185][0m |          -0.0101 |          89.7476 |          19.0349 |
[32m[20221213 21:07:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:07:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.40
[32m[20221213 21:07:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 21:07:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 615.00
[32m[20221213 21:07:49 @agent_ppo2.py:143][0m Total time:      12.24 min
[32m[20221213 21:07:49 @agent_ppo2.py:145][0m 1179648 total steps have happened
[32m[20221213 21:07:49 @agent_ppo2.py:121][0m #------------------------ Iteration 576 --------------------------#
[32m[20221213 21:07:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:49 @agent_ppo2.py:185][0m |          -0.0016 |          89.4626 |          19.1565 |
[32m[20221213 21:07:49 @agent_ppo2.py:185][0m |           0.0073 |          99.0059 |          19.1438 |
[32m[20221213 21:07:49 @agent_ppo2.py:185][0m |          -0.0059 |          88.1322 |          19.1330 |
[32m[20221213 21:07:49 @agent_ppo2.py:185][0m |          -0.0064 |          88.0616 |          19.1361 |
[32m[20221213 21:07:50 @agent_ppo2.py:185][0m |          -0.0076 |          87.5640 |          19.1323 |
[32m[20221213 21:07:50 @agent_ppo2.py:185][0m |          -0.0073 |          87.3590 |          19.1274 |
[32m[20221213 21:07:50 @agent_ppo2.py:185][0m |          -0.0079 |          87.1059 |          19.1321 |
[32m[20221213 21:07:50 @agent_ppo2.py:185][0m |          -0.0004 |          90.6665 |          19.1412 |
[32m[20221213 21:07:50 @agent_ppo2.py:185][0m |          -0.0023 |          88.7033 |          19.1320 |
[32m[20221213 21:07:50 @agent_ppo2.py:185][0m |          -0.0087 |          86.9433 |          19.1410 |
[32m[20221213 21:07:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:07:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 541.60
[32m[20221213 21:07:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 595.00
[32m[20221213 21:07:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 590.00
[32m[20221213 21:07:50 @agent_ppo2.py:143][0m Total time:      12.26 min
[32m[20221213 21:07:50 @agent_ppo2.py:145][0m 1181696 total steps have happened
[32m[20221213 21:07:50 @agent_ppo2.py:121][0m #------------------------ Iteration 577 --------------------------#
[32m[20221213 21:07:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:50 @agent_ppo2.py:185][0m |          -0.0006 |          94.2643 |          19.2083 |
[32m[20221213 21:07:51 @agent_ppo2.py:185][0m |          -0.0046 |          93.0877 |          19.2154 |
[32m[20221213 21:07:51 @agent_ppo2.py:185][0m |          -0.0069 |          92.5126 |          19.2021 |
[32m[20221213 21:07:51 @agent_ppo2.py:185][0m |          -0.0058 |          92.1083 |          19.2036 |
[32m[20221213 21:07:51 @agent_ppo2.py:185][0m |          -0.0060 |          91.5956 |          19.2056 |
[32m[20221213 21:07:51 @agent_ppo2.py:185][0m |           0.0049 |          96.2656 |          19.2043 |
[32m[20221213 21:07:51 @agent_ppo2.py:185][0m |           0.0003 |          93.8370 |          19.1962 |
[32m[20221213 21:07:51 @agent_ppo2.py:185][0m |          -0.0070 |          91.1886 |          19.2005 |
[32m[20221213 21:07:51 @agent_ppo2.py:185][0m |           0.0015 |          95.5702 |          19.2032 |
[32m[20221213 21:07:51 @agent_ppo2.py:185][0m |          -0.0061 |          90.7076 |          19.2031 |
[32m[20221213 21:07:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:07:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.20
[32m[20221213 21:07:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 609.00
[32m[20221213 21:07:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 572.00
[32m[20221213 21:07:51 @agent_ppo2.py:143][0m Total time:      12.28 min
[32m[20221213 21:07:51 @agent_ppo2.py:145][0m 1183744 total steps have happened
[32m[20221213 21:07:51 @agent_ppo2.py:121][0m #------------------------ Iteration 578 --------------------------#
[32m[20221213 21:07:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:52 @agent_ppo2.py:185][0m |           0.0154 |          96.6707 |          19.2467 |
[32m[20221213 21:07:52 @agent_ppo2.py:185][0m |          -0.0027 |          86.4280 |          19.2335 |
[32m[20221213 21:07:52 @agent_ppo2.py:185][0m |          -0.0031 |          86.3253 |          19.2220 |
[32m[20221213 21:07:52 @agent_ppo2.py:185][0m |          -0.0043 |          85.8696 |          19.2136 |
[32m[20221213 21:07:52 @agent_ppo2.py:185][0m |          -0.0064 |          85.7486 |          19.2108 |
[32m[20221213 21:07:52 @agent_ppo2.py:185][0m |          -0.0055 |          85.6330 |          19.2023 |
[32m[20221213 21:07:52 @agent_ppo2.py:185][0m |           0.0027 |          87.1490 |          19.1957 |
[32m[20221213 21:07:52 @agent_ppo2.py:185][0m |          -0.0047 |          85.8164 |          19.1902 |
[32m[20221213 21:07:52 @agent_ppo2.py:185][0m |          -0.0068 |          85.2657 |          19.1871 |
[32m[20221213 21:07:52 @agent_ppo2.py:185][0m |          -0.0005 |          89.3152 |          19.1727 |
[32m[20221213 21:07:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:07:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 556.40
[32m[20221213 21:07:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.00
[32m[20221213 21:07:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 559.00
[32m[20221213 21:07:53 @agent_ppo2.py:143][0m Total time:      12.30 min
[32m[20221213 21:07:53 @agent_ppo2.py:145][0m 1185792 total steps have happened
[32m[20221213 21:07:53 @agent_ppo2.py:121][0m #------------------------ Iteration 579 --------------------------#
[32m[20221213 21:07:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:53 @agent_ppo2.py:185][0m |          -0.0025 |          95.9786 |          19.0656 |
[32m[20221213 21:07:53 @agent_ppo2.py:185][0m |          -0.0010 |          95.1771 |          19.0660 |
[32m[20221213 21:07:53 @agent_ppo2.py:185][0m |          -0.0002 |          95.9369 |          19.0577 |
[32m[20221213 21:07:53 @agent_ppo2.py:185][0m |          -0.0051 |          92.9553 |          19.0517 |
[32m[20221213 21:07:53 @agent_ppo2.py:185][0m |          -0.0066 |          92.5247 |          19.0597 |
[32m[20221213 21:07:53 @agent_ppo2.py:185][0m |          -0.0069 |          92.3332 |          19.0553 |
[32m[20221213 21:07:53 @agent_ppo2.py:185][0m |          -0.0067 |          92.0822 |          19.0507 |
[32m[20221213 21:07:53 @agent_ppo2.py:185][0m |          -0.0101 |          91.9462 |          19.0528 |
[32m[20221213 21:07:54 @agent_ppo2.py:185][0m |          -0.0094 |          91.8056 |          19.0493 |
[32m[20221213 21:07:54 @agent_ppo2.py:185][0m |          -0.0062 |          91.6225 |          19.0496 |
[32m[20221213 21:07:54 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:07:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 512.60
[32m[20221213 21:07:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.00
[32m[20221213 21:07:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 662.00
[32m[20221213 21:07:54 @agent_ppo2.py:143][0m Total time:      12.32 min
[32m[20221213 21:07:54 @agent_ppo2.py:145][0m 1187840 total steps have happened
[32m[20221213 21:07:54 @agent_ppo2.py:121][0m #------------------------ Iteration 580 --------------------------#
[32m[20221213 21:07:54 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:07:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:54 @agent_ppo2.py:185][0m |           0.0057 |          92.0032 |          19.1537 |
[32m[20221213 21:07:54 @agent_ppo2.py:185][0m |           0.0012 |          89.0384 |          19.1406 |
[32m[20221213 21:07:54 @agent_ppo2.py:185][0m |          -0.0081 |          86.5717 |          19.1385 |
[32m[20221213 21:07:54 @agent_ppo2.py:185][0m |          -0.0032 |          86.1115 |          19.1374 |
[32m[20221213 21:07:54 @agent_ppo2.py:185][0m |          -0.0084 |          85.7921 |          19.1298 |
[32m[20221213 21:07:54 @agent_ppo2.py:185][0m |           0.0009 |          86.7290 |          19.1271 |
[32m[20221213 21:07:55 @agent_ppo2.py:185][0m |          -0.0085 |          85.2134 |          19.1224 |
[32m[20221213 21:07:55 @agent_ppo2.py:185][0m |          -0.0052 |          85.9967 |          19.1246 |
[32m[20221213 21:07:55 @agent_ppo2.py:185][0m |          -0.0086 |          84.7969 |          19.1265 |
[32m[20221213 21:07:55 @agent_ppo2.py:185][0m |          -0.0102 |          84.6361 |          19.1235 |
[32m[20221213 21:07:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:07:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 557.00
[32m[20221213 21:07:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 579.00
[32m[20221213 21:07:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 589.00
[32m[20221213 21:07:55 @agent_ppo2.py:143][0m Total time:      12.34 min
[32m[20221213 21:07:55 @agent_ppo2.py:145][0m 1189888 total steps have happened
[32m[20221213 21:07:55 @agent_ppo2.py:121][0m #------------------------ Iteration 581 --------------------------#
[32m[20221213 21:07:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:55 @agent_ppo2.py:185][0m |          -0.0019 |          93.3291 |          19.0762 |
[32m[20221213 21:07:55 @agent_ppo2.py:185][0m |          -0.0043 |          92.4216 |          19.0561 |
[32m[20221213 21:07:55 @agent_ppo2.py:185][0m |          -0.0046 |          91.7428 |          19.0558 |
[32m[20221213 21:07:56 @agent_ppo2.py:185][0m |          -0.0073 |          91.4147 |          19.0494 |
[32m[20221213 21:07:56 @agent_ppo2.py:185][0m |          -0.0064 |          91.0020 |          19.0488 |
[32m[20221213 21:07:56 @agent_ppo2.py:185][0m |          -0.0029 |          91.5060 |          19.0474 |
[32m[20221213 21:07:56 @agent_ppo2.py:185][0m |          -0.0079 |          90.6260 |          19.0483 |
[32m[20221213 21:07:56 @agent_ppo2.py:185][0m |           0.0011 |          96.8440 |          19.0419 |
[32m[20221213 21:07:56 @agent_ppo2.py:185][0m |          -0.0051 |          90.5109 |          19.0456 |
[32m[20221213 21:07:56 @agent_ppo2.py:185][0m |          -0.0095 |          90.1593 |          19.0408 |
[32m[20221213 21:07:56 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:07:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 556.00
[32m[20221213 21:07:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 606.00
[32m[20221213 21:07:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 605.00
[32m[20221213 21:07:56 @agent_ppo2.py:143][0m Total time:      12.36 min
[32m[20221213 21:07:56 @agent_ppo2.py:145][0m 1191936 total steps have happened
[32m[20221213 21:07:56 @agent_ppo2.py:121][0m #------------------------ Iteration 582 --------------------------#
[32m[20221213 21:07:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:07:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:56 @agent_ppo2.py:185][0m |           0.0016 |          90.5305 |          19.2175 |
[32m[20221213 21:07:57 @agent_ppo2.py:185][0m |          -0.0055 |          89.1895 |          19.1805 |
[32m[20221213 21:07:57 @agent_ppo2.py:185][0m |          -0.0082 |          89.0155 |          19.1742 |
[32m[20221213 21:07:57 @agent_ppo2.py:185][0m |          -0.0042 |          88.6271 |          19.1643 |
[32m[20221213 21:07:57 @agent_ppo2.py:185][0m |          -0.0093 |          88.5634 |          19.1609 |
[32m[20221213 21:07:57 @agent_ppo2.py:185][0m |          -0.0068 |          88.3801 |          19.1494 |
[32m[20221213 21:07:57 @agent_ppo2.py:185][0m |           0.0024 |          94.6670 |          19.1586 |
[32m[20221213 21:07:57 @agent_ppo2.py:185][0m |          -0.0072 |          88.1509 |          19.1547 |
[32m[20221213 21:07:57 @agent_ppo2.py:185][0m |          -0.0084 |          88.0685 |          19.1542 |
[32m[20221213 21:07:57 @agent_ppo2.py:185][0m |          -0.0077 |          87.9317 |          19.1481 |
[32m[20221213 21:07:57 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:07:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 558.20
[32m[20221213 21:07:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 613.00
[32m[20221213 21:07:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 557.00
[32m[20221213 21:07:57 @agent_ppo2.py:143][0m Total time:      12.38 min
[32m[20221213 21:07:57 @agent_ppo2.py:145][0m 1193984 total steps have happened
[32m[20221213 21:07:57 @agent_ppo2.py:121][0m #------------------------ Iteration 583 --------------------------#
[32m[20221213 21:07:57 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:07:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:58 @agent_ppo2.py:185][0m |           0.0004 |          90.4895 |          19.1303 |
[32m[20221213 21:07:58 @agent_ppo2.py:185][0m |          -0.0009 |          89.9539 |          19.1208 |
[32m[20221213 21:07:58 @agent_ppo2.py:185][0m |          -0.0065 |          89.5333 |          19.1134 |
[32m[20221213 21:07:58 @agent_ppo2.py:185][0m |          -0.0063 |          89.2662 |          19.1103 |
[32m[20221213 21:07:58 @agent_ppo2.py:185][0m |          -0.0075 |          89.0758 |          19.1039 |
[32m[20221213 21:07:58 @agent_ppo2.py:185][0m |          -0.0083 |          89.0043 |          19.1119 |
[32m[20221213 21:07:58 @agent_ppo2.py:185][0m |          -0.0070 |          88.8253 |          19.1087 |
[32m[20221213 21:07:58 @agent_ppo2.py:185][0m |          -0.0095 |          88.8080 |          19.1217 |
[32m[20221213 21:07:58 @agent_ppo2.py:185][0m |          -0.0079 |          88.7686 |          19.1092 |
[32m[20221213 21:07:58 @agent_ppo2.py:185][0m |          -0.0090 |          88.4352 |          19.1069 |
[32m[20221213 21:07:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:07:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.20
[32m[20221213 21:07:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 578.00
[32m[20221213 21:07:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 597.00
[32m[20221213 21:07:59 @agent_ppo2.py:143][0m Total time:      12.40 min
[32m[20221213 21:07:59 @agent_ppo2.py:145][0m 1196032 total steps have happened
[32m[20221213 21:07:59 @agent_ppo2.py:121][0m #------------------------ Iteration 584 --------------------------#
[32m[20221213 21:07:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:07:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:07:59 @agent_ppo2.py:185][0m |           0.0019 |          92.7685 |          19.2817 |
[32m[20221213 21:07:59 @agent_ppo2.py:185][0m |          -0.0023 |          90.1094 |          19.2628 |
[32m[20221213 21:07:59 @agent_ppo2.py:185][0m |          -0.0046 |          89.6383 |          19.2701 |
[32m[20221213 21:07:59 @agent_ppo2.py:185][0m |          -0.0069 |          89.2921 |          19.2632 |
[32m[20221213 21:07:59 @agent_ppo2.py:185][0m |          -0.0071 |          89.2131 |          19.2675 |
[32m[20221213 21:07:59 @agent_ppo2.py:185][0m |           0.0059 |          97.7921 |          19.2658 |
[32m[20221213 21:07:59 @agent_ppo2.py:185][0m |          -0.0056 |          88.7748 |          19.2699 |
[32m[20221213 21:07:59 @agent_ppo2.py:185][0m |          -0.0067 |          88.6633 |          19.2703 |
[32m[20221213 21:08:00 @agent_ppo2.py:185][0m |          -0.0102 |          88.7020 |          19.2803 |
[32m[20221213 21:08:00 @agent_ppo2.py:185][0m |          -0.0022 |          89.6976 |          19.2857 |
[32m[20221213 21:08:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:08:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 579.00
[32m[20221213 21:08:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 615.00
[32m[20221213 21:08:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.00
[32m[20221213 21:08:00 @agent_ppo2.py:143][0m Total time:      12.42 min
[32m[20221213 21:08:00 @agent_ppo2.py:145][0m 1198080 total steps have happened
[32m[20221213 21:08:00 @agent_ppo2.py:121][0m #------------------------ Iteration 585 --------------------------#
[32m[20221213 21:08:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:08:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:00 @agent_ppo2.py:185][0m |           0.0045 |          93.4765 |          19.1230 |
[32m[20221213 21:08:00 @agent_ppo2.py:185][0m |          -0.0029 |          91.3840 |          19.1145 |
[32m[20221213 21:08:00 @agent_ppo2.py:185][0m |          -0.0041 |          91.1082 |          19.1039 |
[32m[20221213 21:08:00 @agent_ppo2.py:185][0m |          -0.0079 |          90.6813 |          19.1010 |
[32m[20221213 21:08:00 @agent_ppo2.py:185][0m |          -0.0068 |          90.5560 |          19.0978 |
[32m[20221213 21:08:00 @agent_ppo2.py:185][0m |          -0.0092 |          90.3505 |          19.0908 |
[32m[20221213 21:08:01 @agent_ppo2.py:185][0m |          -0.0069 |          90.1163 |          19.0892 |
[32m[20221213 21:08:01 @agent_ppo2.py:185][0m |          -0.0054 |          90.1435 |          19.0851 |
[32m[20221213 21:08:01 @agent_ppo2.py:185][0m |          -0.0056 |          90.2733 |          19.0744 |
[32m[20221213 21:08:01 @agent_ppo2.py:185][0m |          -0.0105 |          89.7432 |          19.0769 |
[32m[20221213 21:08:01 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:08:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 552.80
[32m[20221213 21:08:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.00
[32m[20221213 21:08:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 648.00
[32m[20221213 21:08:01 @agent_ppo2.py:143][0m Total time:      12.44 min
[32m[20221213 21:08:01 @agent_ppo2.py:145][0m 1200128 total steps have happened
[32m[20221213 21:08:01 @agent_ppo2.py:121][0m #------------------------ Iteration 586 --------------------------#
[32m[20221213 21:08:01 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:08:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:01 @agent_ppo2.py:185][0m |          -0.0004 |          87.4436 |          19.1018 |
[32m[20221213 21:08:01 @agent_ppo2.py:185][0m |          -0.0059 |          86.1628 |          19.0949 |
[32m[20221213 21:08:01 @agent_ppo2.py:185][0m |          -0.0085 |          85.6550 |          19.0899 |
[32m[20221213 21:08:02 @agent_ppo2.py:185][0m |           0.0005 |          86.4529 |          19.0715 |
[32m[20221213 21:08:02 @agent_ppo2.py:185][0m |          -0.0047 |          85.1082 |          19.0670 |
[32m[20221213 21:08:02 @agent_ppo2.py:185][0m |          -0.0016 |          85.5599 |          19.0677 |
[32m[20221213 21:08:02 @agent_ppo2.py:185][0m |          -0.0086 |          84.6712 |          19.0664 |
[32m[20221213 21:08:02 @agent_ppo2.py:185][0m |          -0.0092 |          84.4398 |          19.0591 |
[32m[20221213 21:08:02 @agent_ppo2.py:185][0m |          -0.0075 |          84.3265 |          19.0588 |
[32m[20221213 21:08:02 @agent_ppo2.py:185][0m |          -0.0100 |          84.2133 |          19.0605 |
[32m[20221213 21:08:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.80
[32m[20221213 21:08:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.00
[32m[20221213 21:08:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.00
[32m[20221213 21:08:02 @agent_ppo2.py:143][0m Total time:      12.46 min
[32m[20221213 21:08:02 @agent_ppo2.py:145][0m 1202176 total steps have happened
[32m[20221213 21:08:02 @agent_ppo2.py:121][0m #------------------------ Iteration 587 --------------------------#
[32m[20221213 21:08:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:02 @agent_ppo2.py:185][0m |          -0.0017 |          91.0586 |          19.2022 |
[32m[20221213 21:08:03 @agent_ppo2.py:185][0m |          -0.0037 |          90.0493 |          19.1913 |
[32m[20221213 21:08:03 @agent_ppo2.py:185][0m |          -0.0050 |          89.6762 |          19.1917 |
[32m[20221213 21:08:03 @agent_ppo2.py:185][0m |          -0.0031 |          90.0040 |          19.1949 |
[32m[20221213 21:08:03 @agent_ppo2.py:185][0m |          -0.0072 |          89.0647 |          19.1911 |
[32m[20221213 21:08:03 @agent_ppo2.py:185][0m |          -0.0058 |          88.9461 |          19.1994 |
[32m[20221213 21:08:03 @agent_ppo2.py:185][0m |          -0.0068 |          88.6635 |          19.2047 |
[32m[20221213 21:08:03 @agent_ppo2.py:185][0m |          -0.0069 |          88.5319 |          19.2203 |
[32m[20221213 21:08:03 @agent_ppo2.py:185][0m |          -0.0095 |          88.3168 |          19.2157 |
[32m[20221213 21:08:03 @agent_ppo2.py:185][0m |           0.0023 |          96.9071 |          19.2220 |
[32m[20221213 21:08:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 539.20
[32m[20221213 21:08:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 583.00
[32m[20221213 21:08:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 571.00
[32m[20221213 21:08:03 @agent_ppo2.py:143][0m Total time:      12.48 min
[32m[20221213 21:08:03 @agent_ppo2.py:145][0m 1204224 total steps have happened
[32m[20221213 21:08:03 @agent_ppo2.py:121][0m #------------------------ Iteration 588 --------------------------#
[32m[20221213 21:08:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:04 @agent_ppo2.py:185][0m |           0.0077 |          93.9939 |          19.0336 |
[32m[20221213 21:08:04 @agent_ppo2.py:185][0m |           0.0013 |          91.9579 |          19.0209 |
[32m[20221213 21:08:04 @agent_ppo2.py:185][0m |          -0.0051 |          86.9071 |          19.0119 |
[32m[20221213 21:08:04 @agent_ppo2.py:185][0m |          -0.0089 |          86.0690 |          19.0044 |
[32m[20221213 21:08:04 @agent_ppo2.py:185][0m |          -0.0097 |          85.6413 |          18.9997 |
[32m[20221213 21:08:04 @agent_ppo2.py:185][0m |          -0.0068 |          84.9442 |          18.9935 |
[32m[20221213 21:08:04 @agent_ppo2.py:185][0m |          -0.0081 |          84.3393 |          18.9847 |
[32m[20221213 21:08:04 @agent_ppo2.py:185][0m |          -0.0049 |          84.9316 |          18.9871 |
[32m[20221213 21:08:04 @agent_ppo2.py:185][0m |          -0.0098 |          83.9286 |          18.9856 |
[32m[20221213 21:08:04 @agent_ppo2.py:185][0m |           0.0015 |          93.0964 |          18.9781 |
[32m[20221213 21:08:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:08:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 576.00
[32m[20221213 21:08:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 590.00
[32m[20221213 21:08:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 648.00
[32m[20221213 21:08:05 @agent_ppo2.py:143][0m Total time:      12.50 min
[32m[20221213 21:08:05 @agent_ppo2.py:145][0m 1206272 total steps have happened
[32m[20221213 21:08:05 @agent_ppo2.py:121][0m #------------------------ Iteration 589 --------------------------#
[32m[20221213 21:08:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:05 @agent_ppo2.py:185][0m |           0.0003 |          90.2333 |          19.1083 |
[32m[20221213 21:08:05 @agent_ppo2.py:185][0m |          -0.0044 |          89.2535 |          19.0980 |
[32m[20221213 21:08:05 @agent_ppo2.py:185][0m |          -0.0040 |          88.7400 |          19.0978 |
[32m[20221213 21:08:05 @agent_ppo2.py:185][0m |           0.0132 |         100.3109 |          19.0830 |
[32m[20221213 21:08:05 @agent_ppo2.py:185][0m |          -0.0070 |          88.0273 |          19.0675 |
[32m[20221213 21:08:05 @agent_ppo2.py:185][0m |          -0.0076 |          87.6419 |          19.0604 |
[32m[20221213 21:08:05 @agent_ppo2.py:185][0m |          -0.0075 |          87.6679 |          19.0555 |
[32m[20221213 21:08:05 @agent_ppo2.py:185][0m |          -0.0107 |          87.4116 |          19.0519 |
[32m[20221213 21:08:06 @agent_ppo2.py:185][0m |          -0.0071 |          87.3895 |          19.0457 |
[32m[20221213 21:08:06 @agent_ppo2.py:185][0m |          -0.0103 |          86.8440 |          19.0407 |
[32m[20221213 21:08:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 556.20
[32m[20221213 21:08:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.00
[32m[20221213 21:08:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 632.00
[32m[20221213 21:08:06 @agent_ppo2.py:143][0m Total time:      12.52 min
[32m[20221213 21:08:06 @agent_ppo2.py:145][0m 1208320 total steps have happened
[32m[20221213 21:08:06 @agent_ppo2.py:121][0m #------------------------ Iteration 590 --------------------------#
[32m[20221213 21:08:06 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:08:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:06 @agent_ppo2.py:185][0m |          -0.0013 |          89.5872 |          19.1855 |
[32m[20221213 21:08:06 @agent_ppo2.py:185][0m |          -0.0058 |          85.3014 |          19.1710 |
[32m[20221213 21:08:06 @agent_ppo2.py:185][0m |           0.0054 |          91.5322 |          19.1646 |
[32m[20221213 21:08:06 @agent_ppo2.py:185][0m |          -0.0002 |          84.9395 |          19.1709 |
[32m[20221213 21:08:06 @agent_ppo2.py:185][0m |          -0.0054 |          83.0213 |          19.1686 |
[32m[20221213 21:08:06 @agent_ppo2.py:185][0m |          -0.0083 |          82.6856 |          19.1804 |
[32m[20221213 21:08:07 @agent_ppo2.py:185][0m |          -0.0088 |          82.4596 |          19.1712 |
[32m[20221213 21:08:07 @agent_ppo2.py:185][0m |          -0.0114 |          81.9541 |          19.1825 |
[32m[20221213 21:08:07 @agent_ppo2.py:185][0m |          -0.0090 |          81.5498 |          19.1848 |
[32m[20221213 21:08:07 @agent_ppo2.py:185][0m |          -0.0116 |          81.4250 |          19.1929 |
[32m[20221213 21:08:07 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:08:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 569.60
[32m[20221213 21:08:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 642.00
[32m[20221213 21:08:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.00
[32m[20221213 21:08:07 @agent_ppo2.py:143][0m Total time:      12.54 min
[32m[20221213 21:08:07 @agent_ppo2.py:145][0m 1210368 total steps have happened
[32m[20221213 21:08:07 @agent_ppo2.py:121][0m #------------------------ Iteration 591 --------------------------#
[32m[20221213 21:08:07 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:08:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:07 @agent_ppo2.py:185][0m |          -0.0004 |          95.5185 |          19.2188 |
[32m[20221213 21:08:07 @agent_ppo2.py:185][0m |          -0.0052 |          93.9743 |          19.2074 |
[32m[20221213 21:08:07 @agent_ppo2.py:185][0m |          -0.0027 |          93.2803 |          19.1898 |
[32m[20221213 21:08:07 @agent_ppo2.py:185][0m |          -0.0050 |          93.0010 |          19.1815 |
[32m[20221213 21:08:08 @agent_ppo2.py:185][0m |          -0.0067 |          92.6843 |          19.1825 |
[32m[20221213 21:08:08 @agent_ppo2.py:185][0m |           0.0008 |          97.5249 |          19.1797 |
[32m[20221213 21:08:08 @agent_ppo2.py:185][0m |           0.0041 |          97.3141 |          19.1782 |
[32m[20221213 21:08:08 @agent_ppo2.py:185][0m |          -0.0052 |          92.0071 |          19.1612 |
[32m[20221213 21:08:08 @agent_ppo2.py:185][0m |          -0.0070 |          91.8246 |          19.1697 |
[32m[20221213 21:08:08 @agent_ppo2.py:185][0m |           0.0015 |          97.5371 |          19.1653 |
[32m[20221213 21:08:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.40
[32m[20221213 21:08:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.00
[32m[20221213 21:08:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.00
[32m[20221213 21:08:08 @agent_ppo2.py:143][0m Total time:      12.56 min
[32m[20221213 21:08:08 @agent_ppo2.py:145][0m 1212416 total steps have happened
[32m[20221213 21:08:08 @agent_ppo2.py:121][0m #------------------------ Iteration 592 --------------------------#
[32m[20221213 21:08:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:08 @agent_ppo2.py:185][0m |           0.0002 |          95.8909 |          19.1297 |
[32m[20221213 21:08:09 @agent_ppo2.py:185][0m |          -0.0050 |          95.1521 |          19.1241 |
[32m[20221213 21:08:09 @agent_ppo2.py:185][0m |           0.0078 |         103.4301 |          19.1314 |
[32m[20221213 21:08:09 @agent_ppo2.py:185][0m |          -0.0035 |          94.2486 |          19.1262 |
[32m[20221213 21:08:09 @agent_ppo2.py:185][0m |          -0.0039 |          94.5279 |          19.1298 |
[32m[20221213 21:08:09 @agent_ppo2.py:185][0m |          -0.0073 |          93.7125 |          19.1413 |
[32m[20221213 21:08:09 @agent_ppo2.py:185][0m |          -0.0060 |          93.5025 |          19.1444 |
[32m[20221213 21:08:09 @agent_ppo2.py:185][0m |          -0.0068 |          93.3842 |          19.1486 |
[32m[20221213 21:08:09 @agent_ppo2.py:185][0m |          -0.0048 |          93.2663 |          19.1550 |
[32m[20221213 21:08:09 @agent_ppo2.py:185][0m |          -0.0076 |          93.1983 |          19.1588 |
[32m[20221213 21:08:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.80
[32m[20221213 21:08:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 585.00
[32m[20221213 21:08:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 574.00
[32m[20221213 21:08:09 @agent_ppo2.py:143][0m Total time:      12.58 min
[32m[20221213 21:08:09 @agent_ppo2.py:145][0m 1214464 total steps have happened
[32m[20221213 21:08:09 @agent_ppo2.py:121][0m #------------------------ Iteration 593 --------------------------#
[32m[20221213 21:08:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:10 @agent_ppo2.py:185][0m |          -0.0048 |          92.3787 |          19.0640 |
[32m[20221213 21:08:10 @agent_ppo2.py:185][0m |           0.0045 |          95.2329 |          19.0429 |
[32m[20221213 21:08:10 @agent_ppo2.py:185][0m |          -0.0073 |          90.5475 |          19.0355 |
[32m[20221213 21:08:10 @agent_ppo2.py:185][0m |          -0.0078 |          90.2633 |          19.0356 |
[32m[20221213 21:08:10 @agent_ppo2.py:185][0m |          -0.0081 |          90.0709 |          19.0288 |
[32m[20221213 21:08:10 @agent_ppo2.py:185][0m |          -0.0103 |          89.9443 |          19.0336 |
[32m[20221213 21:08:10 @agent_ppo2.py:185][0m |          -0.0087 |          89.6821 |          19.0296 |
[32m[20221213 21:08:10 @agent_ppo2.py:185][0m |          -0.0119 |          89.6073 |          19.0301 |
[32m[20221213 21:08:10 @agent_ppo2.py:185][0m |          -0.0028 |          92.9686 |          19.0251 |
[32m[20221213 21:08:10 @agent_ppo2.py:185][0m |          -0.0119 |          89.3864 |          19.0236 |
[32m[20221213 21:08:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:08:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 578.80
[32m[20221213 21:08:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 617.00
[32m[20221213 21:08:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 594.00
[32m[20221213 21:08:11 @agent_ppo2.py:143][0m Total time:      12.60 min
[32m[20221213 21:08:11 @agent_ppo2.py:145][0m 1216512 total steps have happened
[32m[20221213 21:08:11 @agent_ppo2.py:121][0m #------------------------ Iteration 594 --------------------------#
[32m[20221213 21:08:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:11 @agent_ppo2.py:185][0m |           0.0015 |          93.0862 |          19.1661 |
[32m[20221213 21:08:11 @agent_ppo2.py:185][0m |          -0.0004 |          93.7961 |          19.1594 |
[32m[20221213 21:08:11 @agent_ppo2.py:185][0m |          -0.0066 |          90.9447 |          19.1431 |
[32m[20221213 21:08:11 @agent_ppo2.py:185][0m |          -0.0003 |          95.1309 |          19.1433 |
[32m[20221213 21:08:11 @agent_ppo2.py:185][0m |          -0.0094 |          89.7535 |          19.1299 |
[32m[20221213 21:08:11 @agent_ppo2.py:185][0m |          -0.0072 |          89.3375 |          19.1404 |
[32m[20221213 21:08:11 @agent_ppo2.py:185][0m |          -0.0017 |          91.8565 |          19.1236 |
[32m[20221213 21:08:11 @agent_ppo2.py:185][0m |          -0.0091 |          88.5849 |          19.1280 |
[32m[20221213 21:08:11 @agent_ppo2.py:185][0m |          -0.0106 |          88.2592 |          19.1255 |
[32m[20221213 21:08:12 @agent_ppo2.py:185][0m |          -0.0086 |          88.3279 |          19.1157 |
[32m[20221213 21:08:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:08:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.60
[32m[20221213 21:08:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 578.00
[32m[20221213 21:08:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 583.00
[32m[20221213 21:08:12 @agent_ppo2.py:143][0m Total time:      12.62 min
[32m[20221213 21:08:12 @agent_ppo2.py:145][0m 1218560 total steps have happened
[32m[20221213 21:08:12 @agent_ppo2.py:121][0m #------------------------ Iteration 595 --------------------------#
[32m[20221213 21:08:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:08:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:12 @agent_ppo2.py:185][0m |          -0.0019 |          96.5543 |          19.0768 |
[32m[20221213 21:08:12 @agent_ppo2.py:185][0m |          -0.0052 |          92.7757 |          19.0669 |
[32m[20221213 21:08:12 @agent_ppo2.py:185][0m |          -0.0048 |          91.3684 |          19.0570 |
[32m[20221213 21:08:12 @agent_ppo2.py:185][0m |          -0.0035 |          90.9529 |          19.0573 |
[32m[20221213 21:08:12 @agent_ppo2.py:185][0m |          -0.0091 |          90.0159 |          19.0539 |
[32m[20221213 21:08:12 @agent_ppo2.py:185][0m |          -0.0026 |          92.6636 |          19.0520 |
[32m[20221213 21:08:13 @agent_ppo2.py:185][0m |          -0.0071 |          89.1828 |          19.0478 |
[32m[20221213 21:08:13 @agent_ppo2.py:185][0m |          -0.0088 |          88.9004 |          19.0525 |
[32m[20221213 21:08:13 @agent_ppo2.py:185][0m |          -0.0112 |          88.5730 |          19.0466 |
[32m[20221213 21:08:13 @agent_ppo2.py:185][0m |          -0.0086 |          88.2688 |          19.0463 |
[32m[20221213 21:08:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.40
[32m[20221213 21:08:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 559.00
[32m[20221213 21:08:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.00
[32m[20221213 21:08:13 @agent_ppo2.py:143][0m Total time:      12.64 min
[32m[20221213 21:08:13 @agent_ppo2.py:145][0m 1220608 total steps have happened
[32m[20221213 21:08:13 @agent_ppo2.py:121][0m #------------------------ Iteration 596 --------------------------#
[32m[20221213 21:08:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:13 @agent_ppo2.py:185][0m |          -0.0021 |          98.4166 |          19.0180 |
[32m[20221213 21:08:13 @agent_ppo2.py:185][0m |          -0.0055 |          96.5676 |          19.0211 |
[32m[20221213 21:08:13 @agent_ppo2.py:185][0m |          -0.0029 |          95.8895 |          19.0191 |
[32m[20221213 21:08:13 @agent_ppo2.py:185][0m |          -0.0041 |          95.7957 |          19.0131 |
[32m[20221213 21:08:14 @agent_ppo2.py:185][0m |          -0.0049 |          95.0269 |          19.0185 |
[32m[20221213 21:08:14 @agent_ppo2.py:185][0m |          -0.0045 |          94.7374 |          19.0225 |
[32m[20221213 21:08:14 @agent_ppo2.py:185][0m |          -0.0062 |          94.4386 |          19.0183 |
[32m[20221213 21:08:14 @agent_ppo2.py:185][0m |           0.0013 |          96.5201 |          19.0127 |
[32m[20221213 21:08:14 @agent_ppo2.py:185][0m |          -0.0066 |          94.1498 |          19.0098 |
[32m[20221213 21:08:14 @agent_ppo2.py:185][0m |          -0.0071 |          93.9757 |          19.0123 |
[32m[20221213 21:08:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 550.00
[32m[20221213 21:08:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.00
[32m[20221213 21:08:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 571.00
[32m[20221213 21:08:14 @agent_ppo2.py:143][0m Total time:      12.66 min
[32m[20221213 21:08:14 @agent_ppo2.py:145][0m 1222656 total steps have happened
[32m[20221213 21:08:14 @agent_ppo2.py:121][0m #------------------------ Iteration 597 --------------------------#
[32m[20221213 21:08:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:14 @agent_ppo2.py:185][0m |          -0.0013 |          93.0460 |          19.2445 |
[32m[20221213 21:08:14 @agent_ppo2.py:185][0m |          -0.0090 |          92.4131 |          19.2499 |
[32m[20221213 21:08:15 @agent_ppo2.py:185][0m |          -0.0096 |          91.2590 |          19.2405 |
[32m[20221213 21:08:15 @agent_ppo2.py:185][0m |          -0.0091 |          90.7915 |          19.2415 |
[32m[20221213 21:08:15 @agent_ppo2.py:185][0m |          -0.0084 |          90.1827 |          19.2441 |
[32m[20221213 21:08:15 @agent_ppo2.py:185][0m |           0.0048 |         101.2240 |          19.2492 |
[32m[20221213 21:08:15 @agent_ppo2.py:185][0m |          -0.0091 |          89.3557 |          19.2398 |
[32m[20221213 21:08:15 @agent_ppo2.py:185][0m |          -0.0099 |          89.0121 |          19.2470 |
[32m[20221213 21:08:15 @agent_ppo2.py:185][0m |          -0.0101 |          88.7671 |          19.2408 |
[32m[20221213 21:08:15 @agent_ppo2.py:185][0m |          -0.0098 |          88.4646 |          19.2467 |
[32m[20221213 21:08:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:08:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 566.20
[32m[20221213 21:08:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 619.00
[32m[20221213 21:08:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 599.00
[32m[20221213 21:08:15 @agent_ppo2.py:143][0m Total time:      12.68 min
[32m[20221213 21:08:15 @agent_ppo2.py:145][0m 1224704 total steps have happened
[32m[20221213 21:08:15 @agent_ppo2.py:121][0m #------------------------ Iteration 598 --------------------------#
[32m[20221213 21:08:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:08:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:16 @agent_ppo2.py:185][0m |          -0.0011 |          93.5969 |          19.1128 |
[32m[20221213 21:08:16 @agent_ppo2.py:185][0m |          -0.0052 |          92.4870 |          19.0919 |
[32m[20221213 21:08:16 @agent_ppo2.py:185][0m |          -0.0039 |          91.7003 |          19.0888 |
[32m[20221213 21:08:16 @agent_ppo2.py:185][0m |           0.0066 |         100.0696 |          19.0918 |
[32m[20221213 21:08:16 @agent_ppo2.py:185][0m |           0.0098 |         103.5097 |          19.0894 |
[32m[20221213 21:08:16 @agent_ppo2.py:185][0m |          -0.0077 |          90.8457 |          19.1074 |
[32m[20221213 21:08:16 @agent_ppo2.py:185][0m |          -0.0059 |          90.4214 |          19.1058 |
[32m[20221213 21:08:16 @agent_ppo2.py:185][0m |          -0.0072 |          90.2196 |          19.1005 |
[32m[20221213 21:08:16 @agent_ppo2.py:185][0m |          -0.0082 |          89.9867 |          19.1028 |
[32m[20221213 21:08:16 @agent_ppo2.py:185][0m |          -0.0085 |          89.9119 |          19.1064 |
[32m[20221213 21:08:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:08:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.80
[32m[20221213 21:08:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 602.00
[32m[20221213 21:08:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 653.00
[32m[20221213 21:08:17 @agent_ppo2.py:143][0m Total time:      12.70 min
[32m[20221213 21:08:17 @agent_ppo2.py:145][0m 1226752 total steps have happened
[32m[20221213 21:08:17 @agent_ppo2.py:121][0m #------------------------ Iteration 599 --------------------------#
[32m[20221213 21:08:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:17 @agent_ppo2.py:185][0m |          -0.0014 |          92.4186 |          19.1969 |
[32m[20221213 21:08:17 @agent_ppo2.py:185][0m |          -0.0050 |          91.5402 |          19.1783 |
[32m[20221213 21:08:17 @agent_ppo2.py:185][0m |          -0.0096 |          91.2196 |          19.1808 |
[32m[20221213 21:08:17 @agent_ppo2.py:185][0m |          -0.0094 |          90.9854 |          19.1678 |
[32m[20221213 21:08:17 @agent_ppo2.py:185][0m |          -0.0062 |          90.6981 |          19.1695 |
[32m[20221213 21:08:17 @agent_ppo2.py:185][0m |          -0.0051 |          91.3082 |          19.1709 |
[32m[20221213 21:08:17 @agent_ppo2.py:185][0m |          -0.0067 |          90.5817 |          19.1672 |
[32m[20221213 21:08:17 @agent_ppo2.py:185][0m |          -0.0074 |          90.4760 |          19.1663 |
[32m[20221213 21:08:17 @agent_ppo2.py:185][0m |          -0.0082 |          90.3261 |          19.1653 |
[32m[20221213 21:08:18 @agent_ppo2.py:185][0m |          -0.0107 |          90.3986 |          19.1644 |
[32m[20221213 21:08:18 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:08:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 500.60
[32m[20221213 21:08:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.00
[32m[20221213 21:08:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 615.00
[32m[20221213 21:08:18 @agent_ppo2.py:143][0m Total time:      12.72 min
[32m[20221213 21:08:18 @agent_ppo2.py:145][0m 1228800 total steps have happened
[32m[20221213 21:08:18 @agent_ppo2.py:121][0m #------------------------ Iteration 600 --------------------------#
[32m[20221213 21:08:18 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:08:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:18 @agent_ppo2.py:185][0m |          -0.0017 |          88.8100 |          19.2147 |
[32m[20221213 21:08:18 @agent_ppo2.py:185][0m |           0.0011 |          90.6889 |          19.2004 |
[32m[20221213 21:08:18 @agent_ppo2.py:185][0m |          -0.0025 |          88.6886 |          19.1868 |
[32m[20221213 21:08:18 @agent_ppo2.py:185][0m |          -0.0069 |          87.7251 |          19.1941 |
[32m[20221213 21:08:18 @agent_ppo2.py:185][0m |          -0.0085 |          87.4730 |          19.1882 |
[32m[20221213 21:08:18 @agent_ppo2.py:185][0m |          -0.0101 |          87.2671 |          19.1925 |
[32m[20221213 21:08:19 @agent_ppo2.py:185][0m |          -0.0120 |          87.0632 |          19.1863 |
[32m[20221213 21:08:19 @agent_ppo2.py:185][0m |          -0.0109 |          86.8489 |          19.1920 |
[32m[20221213 21:08:19 @agent_ppo2.py:185][0m |          -0.0063 |          86.7632 |          19.1944 |
[32m[20221213 21:08:19 @agent_ppo2.py:185][0m |          -0.0099 |          86.4704 |          19.1924 |
[32m[20221213 21:08:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 559.20
[32m[20221213 21:08:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 608.00
[32m[20221213 21:08:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 604.00
[32m[20221213 21:08:19 @agent_ppo2.py:143][0m Total time:      12.74 min
[32m[20221213 21:08:19 @agent_ppo2.py:145][0m 1230848 total steps have happened
[32m[20221213 21:08:19 @agent_ppo2.py:121][0m #------------------------ Iteration 601 --------------------------#
[32m[20221213 21:08:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:19 @agent_ppo2.py:185][0m |           0.0009 |          89.9002 |          19.1438 |
[32m[20221213 21:08:19 @agent_ppo2.py:185][0m |          -0.0059 |          89.4953 |          19.1449 |
[32m[20221213 21:08:19 @agent_ppo2.py:185][0m |          -0.0071 |          89.2461 |          19.1354 |
[32m[20221213 21:08:19 @agent_ppo2.py:185][0m |          -0.0062 |          89.0164 |          19.1345 |
[32m[20221213 21:08:20 @agent_ppo2.py:185][0m |          -0.0085 |          88.9762 |          19.1362 |
[32m[20221213 21:08:20 @agent_ppo2.py:185][0m |          -0.0069 |          88.6826 |          19.1295 |
[32m[20221213 21:08:20 @agent_ppo2.py:185][0m |          -0.0058 |          88.5421 |          19.1291 |
[32m[20221213 21:08:20 @agent_ppo2.py:185][0m |           0.0016 |          94.6434 |          19.1292 |
[32m[20221213 21:08:20 @agent_ppo2.py:185][0m |          -0.0061 |          88.4204 |          19.1321 |
[32m[20221213 21:08:20 @agent_ppo2.py:185][0m |           0.0105 |         102.7366 |          19.1332 |
[32m[20221213 21:08:20 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:08:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.00
[32m[20221213 21:08:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 21:08:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.00
[32m[20221213 21:08:20 @agent_ppo2.py:143][0m Total time:      12.76 min
[32m[20221213 21:08:20 @agent_ppo2.py:145][0m 1232896 total steps have happened
[32m[20221213 21:08:20 @agent_ppo2.py:121][0m #------------------------ Iteration 602 --------------------------#
[32m[20221213 21:08:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:20 @agent_ppo2.py:185][0m |          -0.0002 |          92.8652 |          19.0866 |
[32m[20221213 21:08:20 @agent_ppo2.py:185][0m |          -0.0057 |          91.7172 |          19.0624 |
[32m[20221213 21:08:21 @agent_ppo2.py:185][0m |          -0.0050 |          91.0592 |          19.0486 |
[32m[20221213 21:08:21 @agent_ppo2.py:185][0m |           0.0017 |          95.2403 |          19.0451 |
[32m[20221213 21:08:21 @agent_ppo2.py:185][0m |          -0.0081 |          90.3381 |          19.0344 |
[32m[20221213 21:08:21 @agent_ppo2.py:185][0m |           0.0064 |         100.8904 |          19.0345 |
[32m[20221213 21:08:21 @agent_ppo2.py:185][0m |          -0.0053 |          90.2119 |          19.0241 |
[32m[20221213 21:08:21 @agent_ppo2.py:185][0m |          -0.0099 |          89.8518 |          19.0353 |
[32m[20221213 21:08:21 @agent_ppo2.py:185][0m |          -0.0094 |          89.6246 |          19.0166 |
[32m[20221213 21:08:21 @agent_ppo2.py:185][0m |          -0.0078 |          89.5427 |          19.0153 |
[32m[20221213 21:08:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.80
[32m[20221213 21:08:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 600.00
[32m[20221213 21:08:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.00
[32m[20221213 21:08:21 @agent_ppo2.py:143][0m Total time:      12.78 min
[32m[20221213 21:08:21 @agent_ppo2.py:145][0m 1234944 total steps have happened
[32m[20221213 21:08:21 @agent_ppo2.py:121][0m #------------------------ Iteration 603 --------------------------#
[32m[20221213 21:08:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:22 @agent_ppo2.py:185][0m |          -0.0037 |          91.1918 |          19.1770 |
[32m[20221213 21:08:22 @agent_ppo2.py:185][0m |          -0.0031 |          91.2794 |          19.1630 |
[32m[20221213 21:08:22 @agent_ppo2.py:185][0m |          -0.0077 |          89.8190 |          19.1559 |
[32m[20221213 21:08:22 @agent_ppo2.py:185][0m |          -0.0108 |          89.2690 |          19.1485 |
[32m[20221213 21:08:22 @agent_ppo2.py:185][0m |          -0.0010 |          95.2352 |          19.1549 |
[32m[20221213 21:08:22 @agent_ppo2.py:185][0m |          -0.0123 |          88.7532 |          19.1522 |
[32m[20221213 21:08:22 @agent_ppo2.py:185][0m |          -0.0112 |          88.4327 |          19.1605 |
[32m[20221213 21:08:22 @agent_ppo2.py:185][0m |          -0.0113 |          88.2387 |          19.1645 |
[32m[20221213 21:08:22 @agent_ppo2.py:185][0m |           0.0030 |          97.6043 |          19.1584 |
[32m[20221213 21:08:22 @agent_ppo2.py:185][0m |          -0.0121 |          88.3410 |          19.1589 |
[32m[20221213 21:08:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:08:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.80
[32m[20221213 21:08:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.00
[32m[20221213 21:08:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 565.00
[32m[20221213 21:08:22 @agent_ppo2.py:143][0m Total time:      12.80 min
[32m[20221213 21:08:22 @agent_ppo2.py:145][0m 1236992 total steps have happened
[32m[20221213 21:08:22 @agent_ppo2.py:121][0m #------------------------ Iteration 604 --------------------------#
[32m[20221213 21:08:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:23 @agent_ppo2.py:185][0m |           0.0037 |          92.6407 |          19.2287 |
[32m[20221213 21:08:23 @agent_ppo2.py:185][0m |          -0.0017 |          91.1647 |          19.1949 |
[32m[20221213 21:08:23 @agent_ppo2.py:185][0m |          -0.0052 |          90.7982 |          19.1952 |
[32m[20221213 21:08:23 @agent_ppo2.py:185][0m |          -0.0031 |          90.1886 |          19.1735 |
[32m[20221213 21:08:23 @agent_ppo2.py:185][0m |          -0.0054 |          89.9765 |          19.1797 |
[32m[20221213 21:08:23 @agent_ppo2.py:185][0m |          -0.0068 |          89.7497 |          19.1675 |
[32m[20221213 21:08:23 @agent_ppo2.py:185][0m |          -0.0044 |          89.4777 |          19.1606 |
[32m[20221213 21:08:23 @agent_ppo2.py:185][0m |          -0.0087 |          89.3991 |          19.1650 |
[32m[20221213 21:08:23 @agent_ppo2.py:185][0m |           0.0003 |          94.4895 |          19.1683 |
[32m[20221213 21:08:24 @agent_ppo2.py:185][0m |          -0.0051 |          89.2118 |          19.1596 |
[32m[20221213 21:08:24 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:08:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 562.20
[32m[20221213 21:08:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 613.00
[32m[20221213 21:08:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 574.00
[32m[20221213 21:08:24 @agent_ppo2.py:143][0m Total time:      12.82 min
[32m[20221213 21:08:24 @agent_ppo2.py:145][0m 1239040 total steps have happened
[32m[20221213 21:08:24 @agent_ppo2.py:121][0m #------------------------ Iteration 605 --------------------------#
[32m[20221213 21:08:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:24 @agent_ppo2.py:185][0m |          -0.0038 |          92.0853 |          19.0933 |
[32m[20221213 21:08:24 @agent_ppo2.py:185][0m |          -0.0051 |          91.1935 |          19.0763 |
[32m[20221213 21:08:24 @agent_ppo2.py:185][0m |           0.0046 |         100.1086 |          19.0770 |
[32m[20221213 21:08:24 @agent_ppo2.py:185][0m |          -0.0054 |          90.4963 |          19.0329 |
[32m[20221213 21:08:24 @agent_ppo2.py:185][0m |          -0.0084 |          90.2771 |          19.0511 |
[32m[20221213 21:08:24 @agent_ppo2.py:185][0m |          -0.0094 |          90.0578 |          19.0584 |
[32m[20221213 21:08:24 @agent_ppo2.py:185][0m |          -0.0082 |          89.7307 |          19.0522 |
[32m[20221213 21:08:25 @agent_ppo2.py:185][0m |          -0.0096 |          89.6906 |          19.0589 |
[32m[20221213 21:08:25 @agent_ppo2.py:185][0m |          -0.0074 |          89.4872 |          19.0556 |
[32m[20221213 21:08:25 @agent_ppo2.py:185][0m |          -0.0104 |          89.3199 |          19.0390 |
[32m[20221213 21:08:25 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:08:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 590.40
[32m[20221213 21:08:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 639.00
[32m[20221213 21:08:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 624.00
[32m[20221213 21:08:25 @agent_ppo2.py:143][0m Total time:      12.84 min
[32m[20221213 21:08:25 @agent_ppo2.py:145][0m 1241088 total steps have happened
[32m[20221213 21:08:25 @agent_ppo2.py:121][0m #------------------------ Iteration 606 --------------------------#
[32m[20221213 21:08:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:08:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:25 @agent_ppo2.py:185][0m |          -0.0006 |          95.1006 |          19.2126 |
[32m[20221213 21:08:25 @agent_ppo2.py:185][0m |           0.0079 |         103.1531 |          19.1806 |
[32m[20221213 21:08:25 @agent_ppo2.py:185][0m |          -0.0034 |          93.3194 |          19.1698 |
[32m[20221213 21:08:25 @agent_ppo2.py:185][0m |          -0.0052 |          92.8092 |          19.1621 |
[32m[20221213 21:08:26 @agent_ppo2.py:185][0m |          -0.0094 |          92.6875 |          19.1664 |
[32m[20221213 21:08:26 @agent_ppo2.py:185][0m |          -0.0071 |          92.3405 |          19.1643 |
[32m[20221213 21:08:26 @agent_ppo2.py:185][0m |          -0.0085 |          92.1878 |          19.1523 |
[32m[20221213 21:08:26 @agent_ppo2.py:185][0m |          -0.0097 |          92.0936 |          19.1585 |
[32m[20221213 21:08:26 @agent_ppo2.py:185][0m |          -0.0095 |          91.9172 |          19.1516 |
[32m[20221213 21:08:26 @agent_ppo2.py:185][0m |          -0.0092 |          91.8992 |          19.1531 |
[32m[20221213 21:08:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 531.20
[32m[20221213 21:08:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 554.00
[32m[20221213 21:08:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 602.00
[32m[20221213 21:08:26 @agent_ppo2.py:143][0m Total time:      12.86 min
[32m[20221213 21:08:26 @agent_ppo2.py:145][0m 1243136 total steps have happened
[32m[20221213 21:08:26 @agent_ppo2.py:121][0m #------------------------ Iteration 607 --------------------------#
[32m[20221213 21:08:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:26 @agent_ppo2.py:185][0m |          -0.0030 |          95.8043 |          19.1824 |
[32m[20221213 21:08:26 @agent_ppo2.py:185][0m |          -0.0048 |          93.4944 |          19.1614 |
[32m[20221213 21:08:27 @agent_ppo2.py:185][0m |          -0.0034 |          92.1375 |          19.1576 |
[32m[20221213 21:08:27 @agent_ppo2.py:185][0m |          -0.0071 |          91.3055 |          19.1490 |
[32m[20221213 21:08:27 @agent_ppo2.py:185][0m |           0.0066 |         100.1515 |          19.1447 |
[32m[20221213 21:08:27 @agent_ppo2.py:185][0m |           0.0020 |          93.4797 |          19.1372 |
[32m[20221213 21:08:27 @agent_ppo2.py:185][0m |          -0.0087 |          89.3486 |          19.1353 |
[32m[20221213 21:08:27 @agent_ppo2.py:185][0m |          -0.0053 |          89.2896 |          19.1403 |
[32m[20221213 21:08:27 @agent_ppo2.py:185][0m |          -0.0078 |          88.6924 |          19.1361 |
[32m[20221213 21:08:27 @agent_ppo2.py:185][0m |          -0.0056 |          88.6677 |          19.1291 |
[32m[20221213 21:08:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.40
[32m[20221213 21:08:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 610.00
[32m[20221213 21:08:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.00
[32m[20221213 21:08:27 @agent_ppo2.py:143][0m Total time:      12.88 min
[32m[20221213 21:08:27 @agent_ppo2.py:145][0m 1245184 total steps have happened
[32m[20221213 21:08:27 @agent_ppo2.py:121][0m #------------------------ Iteration 608 --------------------------#
[32m[20221213 21:08:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:08:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:28 @agent_ppo2.py:185][0m |          -0.0009 |          94.7988 |          19.0929 |
[32m[20221213 21:08:28 @agent_ppo2.py:185][0m |          -0.0046 |          94.0396 |          19.0933 |
[32m[20221213 21:08:28 @agent_ppo2.py:185][0m |          -0.0039 |          93.4817 |          19.0901 |
[32m[20221213 21:08:28 @agent_ppo2.py:185][0m |          -0.0058 |          93.1219 |          19.0885 |
[32m[20221213 21:08:28 @agent_ppo2.py:185][0m |          -0.0057 |          92.8399 |          19.0809 |
[32m[20221213 21:08:28 @agent_ppo2.py:185][0m |           0.0010 |          95.0970 |          19.0851 |
[32m[20221213 21:08:28 @agent_ppo2.py:185][0m |          -0.0059 |          92.3997 |          19.0825 |
[32m[20221213 21:08:28 @agent_ppo2.py:185][0m |          -0.0064 |          92.2792 |          19.0842 |
[32m[20221213 21:08:28 @agent_ppo2.py:185][0m |          -0.0054 |          92.2418 |          19.0915 |
[32m[20221213 21:08:28 @agent_ppo2.py:185][0m |          -0.0085 |          91.9804 |          19.0929 |
[32m[20221213 21:08:28 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:08:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 556.80
[32m[20221213 21:08:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 565.00
[32m[20221213 21:08:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 577.00
[32m[20221213 21:08:29 @agent_ppo2.py:143][0m Total time:      12.90 min
[32m[20221213 21:08:29 @agent_ppo2.py:145][0m 1247232 total steps have happened
[32m[20221213 21:08:29 @agent_ppo2.py:121][0m #------------------------ Iteration 609 --------------------------#
[32m[20221213 21:08:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:29 @agent_ppo2.py:185][0m |          -0.0036 |          96.7437 |          19.1431 |
[32m[20221213 21:08:29 @agent_ppo2.py:185][0m |          -0.0015 |          95.7518 |          19.1375 |
[32m[20221213 21:08:29 @agent_ppo2.py:185][0m |          -0.0032 |          94.9066 |          19.1298 |
[32m[20221213 21:08:29 @agent_ppo2.py:185][0m |          -0.0067 |          94.4488 |          19.1302 |
[32m[20221213 21:08:29 @agent_ppo2.py:185][0m |          -0.0062 |          94.1145 |          19.1319 |
[32m[20221213 21:08:29 @agent_ppo2.py:185][0m |          -0.0090 |          94.1584 |          19.1220 |
[32m[20221213 21:08:29 @agent_ppo2.py:185][0m |          -0.0065 |          93.6714 |          19.1273 |
[32m[20221213 21:08:29 @agent_ppo2.py:185][0m |          -0.0062 |          93.6878 |          19.1196 |
[32m[20221213 21:08:29 @agent_ppo2.py:185][0m |          -0.0059 |          93.2778 |          19.1256 |
[32m[20221213 21:08:30 @agent_ppo2.py:185][0m |          -0.0040 |          94.1380 |          19.1221 |
[32m[20221213 21:08:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:08:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.40
[32m[20221213 21:08:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.00
[32m[20221213 21:08:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.00
[32m[20221213 21:08:30 @agent_ppo2.py:143][0m Total time:      12.92 min
[32m[20221213 21:08:30 @agent_ppo2.py:145][0m 1249280 total steps have happened
[32m[20221213 21:08:30 @agent_ppo2.py:121][0m #------------------------ Iteration 610 --------------------------#
[32m[20221213 21:08:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:30 @agent_ppo2.py:185][0m |           0.0056 |          96.8050 |          19.0428 |
[32m[20221213 21:08:30 @agent_ppo2.py:185][0m |          -0.0041 |          92.3841 |          19.0345 |
[32m[20221213 21:08:30 @agent_ppo2.py:185][0m |          -0.0035 |          93.2050 |          19.0299 |
[32m[20221213 21:08:30 @agent_ppo2.py:185][0m |          -0.0079 |          91.1511 |          19.0198 |
[32m[20221213 21:08:30 @agent_ppo2.py:185][0m |          -0.0020 |          93.1483 |          19.0119 |
[32m[20221213 21:08:30 @agent_ppo2.py:185][0m |          -0.0090 |          90.6673 |          19.0215 |
[32m[20221213 21:08:31 @agent_ppo2.py:185][0m |          -0.0101 |          90.5645 |          19.0070 |
[32m[20221213 21:08:31 @agent_ppo2.py:185][0m |          -0.0103 |          90.1773 |          19.0083 |
[32m[20221213 21:08:31 @agent_ppo2.py:185][0m |          -0.0060 |          90.5324 |          19.0051 |
[32m[20221213 21:08:31 @agent_ppo2.py:185][0m |          -0.0101 |          89.9789 |          19.0041 |
[32m[20221213 21:08:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.40
[32m[20221213 21:08:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 599.00
[32m[20221213 21:08:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 616.00
[32m[20221213 21:08:31 @agent_ppo2.py:143][0m Total time:      12.94 min
[32m[20221213 21:08:31 @agent_ppo2.py:145][0m 1251328 total steps have happened
[32m[20221213 21:08:31 @agent_ppo2.py:121][0m #------------------------ Iteration 611 --------------------------#
[32m[20221213 21:08:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:08:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:31 @agent_ppo2.py:185][0m |          -0.0008 |          89.7864 |          19.1176 |
[32m[20221213 21:08:31 @agent_ppo2.py:185][0m |          -0.0028 |          88.7585 |          19.0996 |
[32m[20221213 21:08:31 @agent_ppo2.py:185][0m |          -0.0075 |          88.4610 |          19.0908 |
[32m[20221213 21:08:31 @agent_ppo2.py:185][0m |          -0.0065 |          88.1496 |          19.0980 |
[32m[20221213 21:08:32 @agent_ppo2.py:185][0m |          -0.0048 |          87.9351 |          19.0971 |
[32m[20221213 21:08:32 @agent_ppo2.py:185][0m |          -0.0066 |          87.7432 |          19.0906 |
[32m[20221213 21:08:32 @agent_ppo2.py:185][0m |          -0.0071 |          87.6530 |          19.1001 |
[32m[20221213 21:08:32 @agent_ppo2.py:185][0m |          -0.0079 |          87.4072 |          19.0859 |
[32m[20221213 21:08:32 @agent_ppo2.py:185][0m |          -0.0060 |          87.3681 |          19.0910 |
[32m[20221213 21:08:32 @agent_ppo2.py:185][0m |          -0.0064 |          87.4093 |          19.0908 |
[32m[20221213 21:08:32 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:08:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.00
[32m[20221213 21:08:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 614.00
[32m[20221213 21:08:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.00
[32m[20221213 21:08:32 @agent_ppo2.py:143][0m Total time:      12.96 min
[32m[20221213 21:08:32 @agent_ppo2.py:145][0m 1253376 total steps have happened
[32m[20221213 21:08:32 @agent_ppo2.py:121][0m #------------------------ Iteration 612 --------------------------#
[32m[20221213 21:08:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:32 @agent_ppo2.py:185][0m |           0.0055 |          95.3949 |          19.1089 |
[32m[20221213 21:08:32 @agent_ppo2.py:185][0m |          -0.0005 |          90.6370 |          19.0944 |
[32m[20221213 21:08:33 @agent_ppo2.py:185][0m |          -0.0023 |          87.4598 |          19.0991 |
[32m[20221213 21:08:33 @agent_ppo2.py:185][0m |          -0.0055 |          86.0821 |          19.0922 |
[32m[20221213 21:08:33 @agent_ppo2.py:185][0m |          -0.0065 |          84.3023 |          19.0949 |
[32m[20221213 21:08:33 @agent_ppo2.py:185][0m |          -0.0046 |          83.1137 |          19.0962 |
[32m[20221213 21:08:33 @agent_ppo2.py:185][0m |          -0.0027 |          81.2397 |          19.1007 |
[32m[20221213 21:08:33 @agent_ppo2.py:185][0m |          -0.0053 |          80.2053 |          19.1049 |
[32m[20221213 21:08:33 @agent_ppo2.py:185][0m |           0.0072 |          86.8152 |          19.1053 |
[32m[20221213 21:08:33 @agent_ppo2.py:185][0m |          -0.0050 |          78.8173 |          19.1073 |
[32m[20221213 21:08:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 582.60
[32m[20221213 21:08:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 625.00
[32m[20221213 21:08:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 584.00
[32m[20221213 21:08:33 @agent_ppo2.py:143][0m Total time:      12.98 min
[32m[20221213 21:08:33 @agent_ppo2.py:145][0m 1255424 total steps have happened
[32m[20221213 21:08:33 @agent_ppo2.py:121][0m #------------------------ Iteration 613 --------------------------#
[32m[20221213 21:08:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:34 @agent_ppo2.py:185][0m |           0.0020 |          90.5162 |          19.1839 |
[32m[20221213 21:08:34 @agent_ppo2.py:185][0m |          -0.0048 |          88.5799 |          19.1532 |
[32m[20221213 21:08:34 @agent_ppo2.py:185][0m |          -0.0050 |          88.3096 |          19.1525 |
[32m[20221213 21:08:34 @agent_ppo2.py:185][0m |          -0.0024 |          89.5186 |          19.1560 |
[32m[20221213 21:08:34 @agent_ppo2.py:185][0m |          -0.0063 |          87.7975 |          19.1591 |
[32m[20221213 21:08:34 @agent_ppo2.py:185][0m |          -0.0025 |          91.3251 |          19.1500 |
[32m[20221213 21:08:34 @agent_ppo2.py:185][0m |          -0.0062 |          87.3883 |          19.1385 |
[32m[20221213 21:08:34 @agent_ppo2.py:185][0m |          -0.0052 |          87.2694 |          19.1415 |
[32m[20221213 21:08:34 @agent_ppo2.py:185][0m |          -0.0063 |          87.1807 |          19.1404 |
[32m[20221213 21:08:34 @agent_ppo2.py:185][0m |          -0.0052 |          87.0224 |          19.1335 |
[32m[20221213 21:08:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:08:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.80
[32m[20221213 21:08:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 585.00
[32m[20221213 21:08:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 588.00
[32m[20221213 21:08:34 @agent_ppo2.py:143][0m Total time:      13.00 min
[32m[20221213 21:08:34 @agent_ppo2.py:145][0m 1257472 total steps have happened
[32m[20221213 21:08:34 @agent_ppo2.py:121][0m #------------------------ Iteration 614 --------------------------#
[32m[20221213 21:08:35 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:08:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:35 @agent_ppo2.py:185][0m |          -0.0011 |          90.7580 |          19.2256 |
[32m[20221213 21:08:35 @agent_ppo2.py:185][0m |          -0.0033 |          89.9373 |          19.2124 |
[32m[20221213 21:08:35 @agent_ppo2.py:185][0m |          -0.0009 |          90.6512 |          19.2028 |
[32m[20221213 21:08:35 @agent_ppo2.py:185][0m |           0.0086 |         101.4448 |          19.1989 |
[32m[20221213 21:08:35 @agent_ppo2.py:185][0m |          -0.0074 |          89.2658 |          19.1961 |
[32m[20221213 21:08:35 @agent_ppo2.py:185][0m |          -0.0095 |          89.0976 |          19.1865 |
[32m[20221213 21:08:35 @agent_ppo2.py:185][0m |          -0.0061 |          88.8542 |          19.1903 |
[32m[20221213 21:08:35 @agent_ppo2.py:185][0m |          -0.0075 |          88.8337 |          19.1793 |
[32m[20221213 21:08:35 @agent_ppo2.py:185][0m |          -0.0097 |          88.6938 |          19.1802 |
[32m[20221213 21:08:36 @agent_ppo2.py:185][0m |          -0.0081 |          88.5314 |          19.1774 |
[32m[20221213 21:08:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 560.80
[32m[20221213 21:08:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 621.00
[32m[20221213 21:08:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 607.00
[32m[20221213 21:08:36 @agent_ppo2.py:143][0m Total time:      13.02 min
[32m[20221213 21:08:36 @agent_ppo2.py:145][0m 1259520 total steps have happened
[32m[20221213 21:08:36 @agent_ppo2.py:121][0m #------------------------ Iteration 615 --------------------------#
[32m[20221213 21:08:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:36 @agent_ppo2.py:185][0m |          -0.0012 |          94.0740 |          19.1274 |
[32m[20221213 21:08:36 @agent_ppo2.py:185][0m |          -0.0011 |          93.8085 |          19.1147 |
[32m[20221213 21:08:36 @agent_ppo2.py:185][0m |          -0.0030 |          92.9727 |          19.1099 |
[32m[20221213 21:08:36 @agent_ppo2.py:185][0m |          -0.0064 |          92.5569 |          19.1116 |
[32m[20221213 21:08:36 @agent_ppo2.py:185][0m |          -0.0073 |          92.3813 |          19.1135 |
[32m[20221213 21:08:36 @agent_ppo2.py:185][0m |          -0.0073 |          92.1356 |          19.1010 |
[32m[20221213 21:08:36 @agent_ppo2.py:185][0m |          -0.0061 |          92.1046 |          19.1139 |
[32m[20221213 21:08:37 @agent_ppo2.py:185][0m |          -0.0047 |          91.9695 |          19.1154 |
[32m[20221213 21:08:37 @agent_ppo2.py:185][0m |          -0.0096 |          91.7785 |          19.1199 |
[32m[20221213 21:08:37 @agent_ppo2.py:185][0m |           0.0041 |          94.9086 |          19.1190 |
[32m[20221213 21:08:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:08:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 567.40
[32m[20221213 21:08:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.00
[32m[20221213 21:08:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.00
[32m[20221213 21:08:37 @agent_ppo2.py:143][0m Total time:      13.04 min
[32m[20221213 21:08:37 @agent_ppo2.py:145][0m 1261568 total steps have happened
[32m[20221213 21:08:37 @agent_ppo2.py:121][0m #------------------------ Iteration 616 --------------------------#
[32m[20221213 21:08:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:37 @agent_ppo2.py:185][0m |           0.0092 |          99.1716 |          19.0663 |
[32m[20221213 21:08:37 @agent_ppo2.py:185][0m |          -0.0034 |          92.8495 |          19.0695 |
[32m[20221213 21:08:37 @agent_ppo2.py:185][0m |           0.0002 |          92.9097 |          19.0569 |
[32m[20221213 21:08:37 @agent_ppo2.py:185][0m |          -0.0075 |          91.5187 |          19.0616 |
[32m[20221213 21:08:38 @agent_ppo2.py:185][0m |          -0.0067 |          91.0167 |          19.0593 |
[32m[20221213 21:08:38 @agent_ppo2.py:185][0m |          -0.0088 |          90.2675 |          19.0608 |
[32m[20221213 21:08:38 @agent_ppo2.py:185][0m |          -0.0057 |          89.8323 |          19.0512 |
[32m[20221213 21:08:38 @agent_ppo2.py:185][0m |          -0.0002 |          93.3501 |          19.0605 |
[32m[20221213 21:08:38 @agent_ppo2.py:185][0m |          -0.0043 |          89.0954 |          19.0534 |
[32m[20221213 21:08:38 @agent_ppo2.py:185][0m |          -0.0088 |          88.1871 |          19.0487 |
[32m[20221213 21:08:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.80
[32m[20221213 21:08:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 600.00
[32m[20221213 21:08:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 603.00
[32m[20221213 21:08:38 @agent_ppo2.py:143][0m Total time:      13.06 min
[32m[20221213 21:08:38 @agent_ppo2.py:145][0m 1263616 total steps have happened
[32m[20221213 21:08:38 @agent_ppo2.py:121][0m #------------------------ Iteration 617 --------------------------#
[32m[20221213 21:08:38 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:08:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:38 @agent_ppo2.py:185][0m |          -0.0002 |         100.1684 |          19.1742 |
[32m[20221213 21:08:38 @agent_ppo2.py:185][0m |          -0.0033 |          97.9378 |          19.1628 |
[32m[20221213 21:08:39 @agent_ppo2.py:185][0m |          -0.0028 |          96.9646 |          19.1636 |
[32m[20221213 21:08:39 @agent_ppo2.py:185][0m |          -0.0071 |          96.2890 |          19.1537 |
[32m[20221213 21:08:39 @agent_ppo2.py:185][0m |          -0.0069 |          96.0169 |          19.1521 |
[32m[20221213 21:08:39 @agent_ppo2.py:185][0m |           0.0093 |         105.5278 |          19.1574 |
[32m[20221213 21:08:39 @agent_ppo2.py:185][0m |          -0.0055 |          95.4661 |          19.1202 |
[32m[20221213 21:08:39 @agent_ppo2.py:185][0m |          -0.0063 |          94.9197 |          19.1446 |
[32m[20221213 21:08:39 @agent_ppo2.py:185][0m |          -0.0058 |          95.0501 |          19.1498 |
[32m[20221213 21:08:39 @agent_ppo2.py:185][0m |          -0.0070 |          94.5658 |          19.1524 |
[32m[20221213 21:08:39 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:08:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.00
[32m[20221213 21:08:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 569.00
[32m[20221213 21:08:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 618.00
[32m[20221213 21:08:39 @agent_ppo2.py:143][0m Total time:      13.08 min
[32m[20221213 21:08:39 @agent_ppo2.py:145][0m 1265664 total steps have happened
[32m[20221213 21:08:39 @agent_ppo2.py:121][0m #------------------------ Iteration 618 --------------------------#
[32m[20221213 21:08:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:40 @agent_ppo2.py:185][0m |          -0.0044 |          97.8759 |          19.1437 |
[32m[20221213 21:08:40 @agent_ppo2.py:185][0m |           0.0082 |         104.2458 |          19.1261 |
[32m[20221213 21:08:40 @agent_ppo2.py:185][0m |          -0.0061 |          96.4118 |          19.1097 |
[32m[20221213 21:08:40 @agent_ppo2.py:185][0m |          -0.0087 |          95.7395 |          19.1013 |
[32m[20221213 21:08:40 @agent_ppo2.py:185][0m |          -0.0092 |          95.4518 |          19.1016 |
[32m[20221213 21:08:40 @agent_ppo2.py:185][0m |          -0.0060 |          95.0682 |          19.0969 |
[32m[20221213 21:08:40 @agent_ppo2.py:185][0m |          -0.0052 |          95.0326 |          19.0853 |
[32m[20221213 21:08:40 @agent_ppo2.py:185][0m |          -0.0092 |          94.5073 |          19.0855 |
[32m[20221213 21:08:40 @agent_ppo2.py:185][0m |          -0.0048 |          95.3793 |          19.0800 |
[32m[20221213 21:08:40 @agent_ppo2.py:185][0m |          -0.0093 |          94.0425 |          19.0791 |
[32m[20221213 21:08:40 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:08:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.40
[32m[20221213 21:08:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 575.00
[32m[20221213 21:08:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 617.00
[32m[20221213 21:08:40 @agent_ppo2.py:143][0m Total time:      13.10 min
[32m[20221213 21:08:40 @agent_ppo2.py:145][0m 1267712 total steps have happened
[32m[20221213 21:08:40 @agent_ppo2.py:121][0m #------------------------ Iteration 619 --------------------------#
[32m[20221213 21:08:41 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:08:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:41 @agent_ppo2.py:185][0m |          -0.0012 |          94.9485 |          19.1589 |
[32m[20221213 21:08:41 @agent_ppo2.py:185][0m |          -0.0021 |          93.7208 |          19.1495 |
[32m[20221213 21:08:41 @agent_ppo2.py:185][0m |          -0.0077 |          93.1889 |          19.1510 |
[32m[20221213 21:08:41 @agent_ppo2.py:185][0m |          -0.0066 |          92.6463 |          19.1469 |
[32m[20221213 21:08:41 @agent_ppo2.py:185][0m |          -0.0098 |          92.4272 |          19.1433 |
[32m[20221213 21:08:41 @agent_ppo2.py:185][0m |          -0.0094 |          92.2031 |          19.1391 |
[32m[20221213 21:08:41 @agent_ppo2.py:185][0m |          -0.0089 |          91.9696 |          19.1394 |
[32m[20221213 21:08:41 @agent_ppo2.py:185][0m |          -0.0076 |          91.7769 |          19.1391 |
[32m[20221213 21:08:41 @agent_ppo2.py:185][0m |          -0.0046 |          91.7688 |          19.1358 |
[32m[20221213 21:08:42 @agent_ppo2.py:185][0m |          -0.0040 |          94.0262 |          19.1393 |
[32m[20221213 21:08:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:08:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 555.40
[32m[20221213 21:08:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.00
[32m[20221213 21:08:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.00
[32m[20221213 21:08:42 @agent_ppo2.py:143][0m Total time:      13.12 min
[32m[20221213 21:08:42 @agent_ppo2.py:145][0m 1269760 total steps have happened
[32m[20221213 21:08:42 @agent_ppo2.py:121][0m #------------------------ Iteration 620 --------------------------#
[32m[20221213 21:08:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:42 @agent_ppo2.py:185][0m |          -0.0019 |          96.4515 |          18.9771 |
[32m[20221213 21:08:42 @agent_ppo2.py:185][0m |          -0.0015 |          95.8420 |          18.9650 |
[32m[20221213 21:08:42 @agent_ppo2.py:185][0m |          -0.0041 |          94.8984 |          18.9756 |
[32m[20221213 21:08:42 @agent_ppo2.py:185][0m |          -0.0064 |          94.3538 |          18.9742 |
[32m[20221213 21:08:42 @agent_ppo2.py:185][0m |          -0.0069 |          94.2451 |          18.9713 |
[32m[20221213 21:08:42 @agent_ppo2.py:185][0m |          -0.0073 |          93.9158 |          18.9862 |
[32m[20221213 21:08:42 @agent_ppo2.py:185][0m |          -0.0028 |          96.1160 |          18.9854 |
[32m[20221213 21:08:43 @agent_ppo2.py:185][0m |          -0.0038 |          93.6141 |          18.9878 |
[32m[20221213 21:08:43 @agent_ppo2.py:185][0m |          -0.0097 |          93.4797 |          18.9913 |
[32m[20221213 21:08:43 @agent_ppo2.py:185][0m |          -0.0106 |          93.5452 |          18.9789 |
[32m[20221213 21:08:43 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:08:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 560.20
[32m[20221213 21:08:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 602.00
[32m[20221213 21:08:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 622.00
[32m[20221213 21:08:43 @agent_ppo2.py:143][0m Total time:      13.14 min
[32m[20221213 21:08:43 @agent_ppo2.py:145][0m 1271808 total steps have happened
[32m[20221213 21:08:43 @agent_ppo2.py:121][0m #------------------------ Iteration 621 --------------------------#
[32m[20221213 21:08:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:08:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:43 @agent_ppo2.py:185][0m |          -0.0031 |          93.3473 |          19.0806 |
[32m[20221213 21:08:43 @agent_ppo2.py:185][0m |          -0.0022 |          92.4194 |          19.0749 |
[32m[20221213 21:08:43 @agent_ppo2.py:185][0m |          -0.0006 |          92.3512 |          19.0632 |
[32m[20221213 21:08:43 @agent_ppo2.py:185][0m |          -0.0040 |          91.4714 |          19.0502 |
[32m[20221213 21:08:43 @agent_ppo2.py:185][0m |          -0.0039 |          91.4765 |          19.0588 |
[32m[20221213 21:08:44 @agent_ppo2.py:185][0m |          -0.0065 |          91.0618 |          19.0448 |
[32m[20221213 21:08:44 @agent_ppo2.py:185][0m |          -0.0042 |          90.8614 |          19.0431 |
[32m[20221213 21:08:44 @agent_ppo2.py:185][0m |          -0.0072 |          90.8881 |          19.0407 |
[32m[20221213 21:08:44 @agent_ppo2.py:185][0m |          -0.0074 |          90.5173 |          19.0341 |
[32m[20221213 21:08:44 @agent_ppo2.py:185][0m |          -0.0035 |          92.2745 |          19.0339 |
[32m[20221213 21:08:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:08:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 549.80
[32m[20221213 21:08:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 595.00
[32m[20221213 21:08:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 641.00
[32m[20221213 21:08:44 @agent_ppo2.py:143][0m Total time:      13.16 min
[32m[20221213 21:08:44 @agent_ppo2.py:145][0m 1273856 total steps have happened
[32m[20221213 21:08:44 @agent_ppo2.py:121][0m #------------------------ Iteration 622 --------------------------#
[32m[20221213 21:08:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:44 @agent_ppo2.py:185][0m |          -0.0005 |          91.6503 |          19.1156 |
[32m[20221213 21:08:44 @agent_ppo2.py:185][0m |          -0.0057 |          90.6862 |          19.1104 |
[32m[20221213 21:08:45 @agent_ppo2.py:185][0m |          -0.0068 |          90.1686 |          19.1009 |
[32m[20221213 21:08:45 @agent_ppo2.py:185][0m |          -0.0074 |          89.6483 |          19.1103 |
[32m[20221213 21:08:45 @agent_ppo2.py:185][0m |          -0.0055 |          89.2335 |          19.1017 |
[32m[20221213 21:08:45 @agent_ppo2.py:185][0m |          -0.0086 |          88.7917 |          19.1017 |
[32m[20221213 21:08:45 @agent_ppo2.py:185][0m |          -0.0082 |          88.3744 |          19.0961 |
[32m[20221213 21:08:45 @agent_ppo2.py:185][0m |          -0.0094 |          88.0756 |          19.1032 |
[32m[20221213 21:08:45 @agent_ppo2.py:185][0m |          -0.0080 |          88.0720 |          19.0942 |
[32m[20221213 21:08:45 @agent_ppo2.py:185][0m |          -0.0051 |          88.0541 |          19.0909 |
[32m[20221213 21:08:45 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:08:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 578.60
[32m[20221213 21:08:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 602.00
[32m[20221213 21:08:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 606.00
[32m[20221213 21:08:45 @agent_ppo2.py:143][0m Total time:      13.18 min
[32m[20221213 21:08:45 @agent_ppo2.py:145][0m 1275904 total steps have happened
[32m[20221213 21:08:45 @agent_ppo2.py:121][0m #------------------------ Iteration 623 --------------------------#
[32m[20221213 21:08:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:46 @agent_ppo2.py:185][0m |           0.0013 |          97.3705 |          19.1750 |
[32m[20221213 21:08:46 @agent_ppo2.py:185][0m |          -0.0029 |          96.2207 |          19.1743 |
[32m[20221213 21:08:46 @agent_ppo2.py:185][0m |          -0.0051 |          95.4451 |          19.1779 |
[32m[20221213 21:08:46 @agent_ppo2.py:185][0m |          -0.0054 |          95.0977 |          19.1854 |
[32m[20221213 21:08:46 @agent_ppo2.py:185][0m |          -0.0063 |          94.6847 |          19.1906 |
[32m[20221213 21:08:46 @agent_ppo2.py:185][0m |          -0.0055 |          94.5250 |          19.1953 |
[32m[20221213 21:08:46 @agent_ppo2.py:185][0m |          -0.0071 |          94.3181 |          19.1920 |
[32m[20221213 21:08:46 @agent_ppo2.py:185][0m |          -0.0075 |          94.1244 |          19.2002 |
[32m[20221213 21:08:46 @agent_ppo2.py:185][0m |          -0.0060 |          94.1020 |          19.2105 |
[32m[20221213 21:08:46 @agent_ppo2.py:185][0m |          -0.0079 |          94.3052 |          19.2104 |
[32m[20221213 21:08:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:08:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 562.20
[32m[20221213 21:08:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 614.00
[32m[20221213 21:08:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 642.00
[32m[20221213 21:08:46 @agent_ppo2.py:143][0m Total time:      13.20 min
[32m[20221213 21:08:46 @agent_ppo2.py:145][0m 1277952 total steps have happened
[32m[20221213 21:08:46 @agent_ppo2.py:121][0m #------------------------ Iteration 624 --------------------------#
[32m[20221213 21:08:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:08:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:47 @agent_ppo2.py:185][0m |          -0.0014 |          94.0185 |          19.3220 |
[32m[20221213 21:08:47 @agent_ppo2.py:185][0m |          -0.0057 |          92.8751 |          19.3157 |
[32m[20221213 21:08:47 @agent_ppo2.py:185][0m |          -0.0074 |          92.1959 |          19.3112 |
[32m[20221213 21:08:47 @agent_ppo2.py:185][0m |          -0.0054 |          91.6107 |          19.3019 |
[32m[20221213 21:08:47 @agent_ppo2.py:185][0m |          -0.0059 |          91.3835 |          19.3011 |
[32m[20221213 21:08:47 @agent_ppo2.py:185][0m |          -0.0074 |          91.1485 |          19.2930 |
[32m[20221213 21:08:47 @agent_ppo2.py:185][0m |          -0.0044 |          91.0003 |          19.2887 |
[32m[20221213 21:08:47 @agent_ppo2.py:185][0m |          -0.0086 |          90.8920 |          19.2758 |
[32m[20221213 21:08:47 @agent_ppo2.py:185][0m |          -0.0082 |          90.6166 |          19.2818 |
[32m[20221213 21:08:48 @agent_ppo2.py:185][0m |          -0.0082 |          90.5259 |          19.2765 |
[32m[20221213 21:08:48 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:08:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.60
[32m[20221213 21:08:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 613.00
[32m[20221213 21:08:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 677.00
[32m[20221213 21:08:48 @agent_ppo2.py:143][0m Total time:      13.22 min
[32m[20221213 21:08:48 @agent_ppo2.py:145][0m 1280000 total steps have happened
[32m[20221213 21:08:48 @agent_ppo2.py:121][0m #------------------------ Iteration 625 --------------------------#
[32m[20221213 21:08:48 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:08:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:48 @agent_ppo2.py:185][0m |          -0.0044 |          91.2494 |          19.0024 |
[32m[20221213 21:08:48 @agent_ppo2.py:185][0m |          -0.0027 |          89.3270 |          18.9939 |
[32m[20221213 21:08:48 @agent_ppo2.py:185][0m |          -0.0025 |          88.2989 |          18.9915 |
[32m[20221213 21:08:48 @agent_ppo2.py:185][0m |           0.0005 |          88.3781 |          18.9936 |
[32m[20221213 21:08:48 @agent_ppo2.py:185][0m |           0.0057 |          91.3193 |          18.9934 |
[32m[20221213 21:08:48 @agent_ppo2.py:185][0m |          -0.0004 |          89.1886 |          18.9890 |
[32m[20221213 21:08:49 @agent_ppo2.py:185][0m |          -0.0039 |          86.6258 |          18.9858 |
[32m[20221213 21:08:49 @agent_ppo2.py:185][0m |          -0.0087 |          86.4621 |          18.9931 |
[32m[20221213 21:08:49 @agent_ppo2.py:185][0m |          -0.0083 |          85.9404 |          18.9985 |
[32m[20221213 21:08:49 @agent_ppo2.py:185][0m |          -0.0069 |          85.6626 |          18.9956 |
[32m[20221213 21:08:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:08:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 566.00
[32m[20221213 21:08:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 636.00
[32m[20221213 21:08:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 641.00
[32m[20221213 21:08:49 @agent_ppo2.py:143][0m Total time:      13.24 min
[32m[20221213 21:08:49 @agent_ppo2.py:145][0m 1282048 total steps have happened
[32m[20221213 21:08:49 @agent_ppo2.py:121][0m #------------------------ Iteration 626 --------------------------#
[32m[20221213 21:08:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:49 @agent_ppo2.py:185][0m |          -0.0002 |          96.3241 |          19.0485 |
[32m[20221213 21:08:49 @agent_ppo2.py:185][0m |          -0.0020 |          94.8180 |          19.0456 |
[32m[20221213 21:08:49 @agent_ppo2.py:185][0m |          -0.0032 |          94.2470 |          19.0328 |
[32m[20221213 21:08:49 @agent_ppo2.py:185][0m |          -0.0076 |          93.7202 |          19.0238 |
[32m[20221213 21:08:50 @agent_ppo2.py:185][0m |          -0.0077 |          93.5221 |          19.0397 |
[32m[20221213 21:08:50 @agent_ppo2.py:185][0m |          -0.0046 |          93.2154 |          19.0442 |
[32m[20221213 21:08:50 @agent_ppo2.py:185][0m |          -0.0074 |          93.0218 |          19.0353 |
[32m[20221213 21:08:50 @agent_ppo2.py:185][0m |          -0.0042 |          92.9253 |          19.0415 |
[32m[20221213 21:08:50 @agent_ppo2.py:185][0m |          -0.0068 |          92.7446 |          19.0446 |
[32m[20221213 21:08:50 @agent_ppo2.py:185][0m |          -0.0075 |          92.8252 |          19.0391 |
[32m[20221213 21:08:50 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:08:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.00
[32m[20221213 21:08:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.00
[32m[20221213 21:08:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 577.00
[32m[20221213 21:08:50 @agent_ppo2.py:143][0m Total time:      13.26 min
[32m[20221213 21:08:50 @agent_ppo2.py:145][0m 1284096 total steps have happened
[32m[20221213 21:08:50 @agent_ppo2.py:121][0m #------------------------ Iteration 627 --------------------------#
[32m[20221213 21:08:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:50 @agent_ppo2.py:185][0m |           0.0077 |          97.5218 |          19.1355 |
[32m[20221213 21:08:51 @agent_ppo2.py:185][0m |          -0.0055 |          93.4426 |          19.1287 |
[32m[20221213 21:08:51 @agent_ppo2.py:185][0m |          -0.0047 |          92.6062 |          19.1236 |
[32m[20221213 21:08:51 @agent_ppo2.py:185][0m |          -0.0066 |          92.2447 |          19.1294 |
[32m[20221213 21:08:51 @agent_ppo2.py:185][0m |          -0.0040 |          92.0685 |          19.1202 |
[32m[20221213 21:08:51 @agent_ppo2.py:185][0m |          -0.0056 |          91.7225 |          19.1368 |
[32m[20221213 21:08:51 @agent_ppo2.py:185][0m |          -0.0068 |          91.5108 |          19.1299 |
[32m[20221213 21:08:51 @agent_ppo2.py:185][0m |          -0.0047 |          91.4395 |          19.1370 |
[32m[20221213 21:08:51 @agent_ppo2.py:185][0m |          -0.0074 |          91.3526 |          19.1408 |
[32m[20221213 21:08:52 @agent_ppo2.py:185][0m |           0.0045 |          99.2626 |          19.1339 |
[32m[20221213 21:08:52 @agent_ppo2.py:130][0m Policy update time: 1.30 s
[32m[20221213 21:08:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 575.60
[32m[20221213 21:08:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 608.00
[32m[20221213 21:08:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 622.00
[32m[20221213 21:08:52 @agent_ppo2.py:143][0m Total time:      13.28 min
[32m[20221213 21:08:52 @agent_ppo2.py:145][0m 1286144 total steps have happened
[32m[20221213 21:08:52 @agent_ppo2.py:121][0m #------------------------ Iteration 628 --------------------------#
[32m[20221213 21:08:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:52 @agent_ppo2.py:185][0m |          -0.0017 |          93.1864 |          19.0611 |
[32m[20221213 21:08:52 @agent_ppo2.py:185][0m |          -0.0050 |          91.8652 |          19.0654 |
[32m[20221213 21:08:52 @agent_ppo2.py:185][0m |          -0.0016 |          91.1560 |          19.0649 |
[32m[20221213 21:08:52 @agent_ppo2.py:185][0m |          -0.0058 |          90.7247 |          19.0520 |
[32m[20221213 21:08:52 @agent_ppo2.py:185][0m |          -0.0029 |          90.4432 |          19.0746 |
[32m[20221213 21:08:52 @agent_ppo2.py:185][0m |          -0.0060 |          89.8577 |          19.0644 |
[32m[20221213 21:08:53 @agent_ppo2.py:185][0m |          -0.0047 |          89.5162 |          19.0763 |
[32m[20221213 21:08:53 @agent_ppo2.py:185][0m |           0.0029 |         100.7572 |          19.0741 |
[32m[20221213 21:08:53 @agent_ppo2.py:185][0m |          -0.0067 |          89.1650 |          19.0426 |
[32m[20221213 21:08:53 @agent_ppo2.py:185][0m |          -0.0073 |          88.6883 |          19.0738 |
[32m[20221213 21:08:53 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:08:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 567.40
[32m[20221213 21:08:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 21:08:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 627.00
[32m[20221213 21:08:53 @agent_ppo2.py:143][0m Total time:      13.30 min
[32m[20221213 21:08:53 @agent_ppo2.py:145][0m 1288192 total steps have happened
[32m[20221213 21:08:53 @agent_ppo2.py:121][0m #------------------------ Iteration 629 --------------------------#
[32m[20221213 21:08:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:53 @agent_ppo2.py:185][0m |          -0.0036 |          88.9691 |          19.1171 |
[32m[20221213 21:08:53 @agent_ppo2.py:185][0m |           0.0044 |          93.7647 |          19.0938 |
[32m[20221213 21:08:53 @agent_ppo2.py:185][0m |          -0.0073 |          87.0665 |          19.0877 |
[32m[20221213 21:08:54 @agent_ppo2.py:185][0m |          -0.0082 |          86.7288 |          19.0912 |
[32m[20221213 21:08:54 @agent_ppo2.py:185][0m |           0.0140 |          98.9890 |          19.0885 |
[32m[20221213 21:08:54 @agent_ppo2.py:185][0m |          -0.0103 |          86.4047 |          19.0673 |
[32m[20221213 21:08:54 @agent_ppo2.py:185][0m |          -0.0072 |          85.9758 |          19.0733 |
[32m[20221213 21:08:54 @agent_ppo2.py:185][0m |          -0.0057 |          85.7174 |          19.0832 |
[32m[20221213 21:08:54 @agent_ppo2.py:185][0m |          -0.0097 |          85.7804 |          19.0657 |
[32m[20221213 21:08:54 @agent_ppo2.py:185][0m |          -0.0062 |          85.4957 |          19.0805 |
[32m[20221213 21:08:54 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:08:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 562.40
[32m[20221213 21:08:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.00
[32m[20221213 21:08:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 595.00
[32m[20221213 21:08:54 @agent_ppo2.py:143][0m Total time:      13.33 min
[32m[20221213 21:08:54 @agent_ppo2.py:145][0m 1290240 total steps have happened
[32m[20221213 21:08:54 @agent_ppo2.py:121][0m #------------------------ Iteration 630 --------------------------#
[32m[20221213 21:08:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:55 @agent_ppo2.py:185][0m |           0.0030 |          94.2413 |          19.1332 |
[32m[20221213 21:08:55 @agent_ppo2.py:185][0m |          -0.0058 |          88.5197 |          19.0732 |
[32m[20221213 21:08:55 @agent_ppo2.py:185][0m |          -0.0054 |          87.7300 |          19.0752 |
[32m[20221213 21:08:55 @agent_ppo2.py:185][0m |          -0.0098 |          87.4783 |          19.0692 |
[32m[20221213 21:08:55 @agent_ppo2.py:185][0m |          -0.0065 |          86.9568 |          19.0665 |
[32m[20221213 21:08:55 @agent_ppo2.py:185][0m |          -0.0071 |          86.7026 |          19.0603 |
[32m[20221213 21:08:55 @agent_ppo2.py:185][0m |          -0.0107 |          86.2383 |          19.0597 |
[32m[20221213 21:08:55 @agent_ppo2.py:185][0m |          -0.0102 |          85.9174 |          19.0371 |
[32m[20221213 21:08:55 @agent_ppo2.py:185][0m |          -0.0110 |          85.7737 |          19.0530 |
[32m[20221213 21:08:55 @agent_ppo2.py:185][0m |          -0.0082 |          85.6233 |          19.0383 |
[32m[20221213 21:08:55 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:08:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.60
[32m[20221213 21:08:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.00
[32m[20221213 21:08:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 602.00
[32m[20221213 21:08:55 @agent_ppo2.py:143][0m Total time:      13.35 min
[32m[20221213 21:08:55 @agent_ppo2.py:145][0m 1292288 total steps have happened
[32m[20221213 21:08:55 @agent_ppo2.py:121][0m #------------------------ Iteration 631 --------------------------#
[32m[20221213 21:08:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:56 @agent_ppo2.py:185][0m |           0.0107 |         100.2781 |          19.1342 |
[32m[20221213 21:08:56 @agent_ppo2.py:185][0m |          -0.0053 |          92.6688 |          19.1245 |
[32m[20221213 21:08:56 @agent_ppo2.py:185][0m |           0.0019 |          96.7677 |          19.1136 |
[32m[20221213 21:08:56 @agent_ppo2.py:185][0m |          -0.0089 |          91.7228 |          19.1046 |
[32m[20221213 21:08:56 @agent_ppo2.py:185][0m |          -0.0062 |          91.3290 |          19.1048 |
[32m[20221213 21:08:56 @agent_ppo2.py:185][0m |          -0.0084 |          91.2381 |          19.1016 |
[32m[20221213 21:08:56 @agent_ppo2.py:185][0m |          -0.0078 |          90.8588 |          19.1039 |
[32m[20221213 21:08:56 @agent_ppo2.py:185][0m |          -0.0007 |          93.3994 |          19.0917 |
[32m[20221213 21:08:56 @agent_ppo2.py:185][0m |          -0.0112 |          90.8862 |          19.0943 |
[32m[20221213 21:08:57 @agent_ppo2.py:185][0m |          -0.0081 |          90.5327 |          19.0815 |
[32m[20221213 21:08:57 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:08:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 567.00
[32m[20221213 21:08:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 590.00
[32m[20221213 21:08:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 621.00
[32m[20221213 21:08:57 @agent_ppo2.py:143][0m Total time:      13.37 min
[32m[20221213 21:08:57 @agent_ppo2.py:145][0m 1294336 total steps have happened
[32m[20221213 21:08:57 @agent_ppo2.py:121][0m #------------------------ Iteration 632 --------------------------#
[32m[20221213 21:08:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:57 @agent_ppo2.py:185][0m |           0.0003 |          94.5044 |          19.2212 |
[32m[20221213 21:08:57 @agent_ppo2.py:185][0m |          -0.0014 |          92.6316 |          19.2112 |
[32m[20221213 21:08:57 @agent_ppo2.py:185][0m |          -0.0057 |          92.1516 |          19.2039 |
[32m[20221213 21:08:57 @agent_ppo2.py:185][0m |          -0.0038 |          91.4874 |          19.1964 |
[32m[20221213 21:08:57 @agent_ppo2.py:185][0m |          -0.0038 |          91.1370 |          19.1938 |
[32m[20221213 21:08:57 @agent_ppo2.py:185][0m |          -0.0086 |          91.0495 |          19.1962 |
[32m[20221213 21:08:57 @agent_ppo2.py:185][0m |          -0.0074 |          90.7584 |          19.1884 |
[32m[20221213 21:08:58 @agent_ppo2.py:185][0m |          -0.0046 |          90.4966 |          19.1843 |
[32m[20221213 21:08:58 @agent_ppo2.py:185][0m |          -0.0087 |          90.2875 |          19.1866 |
[32m[20221213 21:08:58 @agent_ppo2.py:185][0m |           0.0010 |          94.8792 |          19.1871 |
[32m[20221213 21:08:58 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:08:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.20
[32m[20221213 21:08:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 589.00
[32m[20221213 21:08:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.00
[32m[20221213 21:08:58 @agent_ppo2.py:143][0m Total time:      13.39 min
[32m[20221213 21:08:58 @agent_ppo2.py:145][0m 1296384 total steps have happened
[32m[20221213 21:08:58 @agent_ppo2.py:121][0m #------------------------ Iteration 633 --------------------------#
[32m[20221213 21:08:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:08:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:08:58 @agent_ppo2.py:185][0m |          -0.0006 |          90.6795 |          19.0623 |
[32m[20221213 21:08:58 @agent_ppo2.py:185][0m |          -0.0000 |          90.2832 |          19.0412 |
[32m[20221213 21:08:58 @agent_ppo2.py:185][0m |          -0.0043 |          89.3344 |          19.0268 |
[32m[20221213 21:08:58 @agent_ppo2.py:185][0m |          -0.0056 |          88.9450 |          19.0362 |
[32m[20221213 21:08:59 @agent_ppo2.py:185][0m |          -0.0061 |          88.7636 |          19.0204 |
[32m[20221213 21:08:59 @agent_ppo2.py:185][0m |          -0.0055 |          88.5665 |          19.0087 |
[32m[20221213 21:08:59 @agent_ppo2.py:185][0m |          -0.0043 |          88.3293 |          19.0172 |
[32m[20221213 21:08:59 @agent_ppo2.py:185][0m |          -0.0067 |          88.2132 |          19.0065 |
[32m[20221213 21:08:59 @agent_ppo2.py:185][0m |          -0.0089 |          87.9129 |          19.0086 |
[32m[20221213 21:08:59 @agent_ppo2.py:185][0m |          -0.0054 |          87.7549 |          19.0009 |
[32m[20221213 21:08:59 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:08:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 571.00
[32m[20221213 21:08:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 624.00
[32m[20221213 21:08:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 666.00
[32m[20221213 21:08:59 @agent_ppo2.py:143][0m Total time:      13.41 min
[32m[20221213 21:08:59 @agent_ppo2.py:145][0m 1298432 total steps have happened
[32m[20221213 21:08:59 @agent_ppo2.py:121][0m #------------------------ Iteration 634 --------------------------#
[32m[20221213 21:08:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:08:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:00 @agent_ppo2.py:185][0m |           0.0029 |          95.4542 |          19.1284 |
[32m[20221213 21:09:00 @agent_ppo2.py:185][0m |          -0.0000 |          96.3837 |          19.1046 |
[32m[20221213 21:09:00 @agent_ppo2.py:185][0m |          -0.0040 |          93.8254 |          19.1104 |
[32m[20221213 21:09:00 @agent_ppo2.py:185][0m |          -0.0052 |          93.3078 |          19.0937 |
[32m[20221213 21:09:00 @agent_ppo2.py:185][0m |          -0.0087 |          93.1039 |          19.0989 |
[32m[20221213 21:09:00 @agent_ppo2.py:185][0m |          -0.0069 |          92.6609 |          19.0928 |
[32m[20221213 21:09:00 @agent_ppo2.py:185][0m |          -0.0079 |          92.5310 |          19.0952 |
[32m[20221213 21:09:00 @agent_ppo2.py:185][0m |          -0.0095 |          92.2558 |          19.0910 |
[32m[20221213 21:09:00 @agent_ppo2.py:185][0m |          -0.0097 |          91.9467 |          19.0955 |
[32m[20221213 21:09:00 @agent_ppo2.py:185][0m |          -0.0090 |          91.9144 |          19.0859 |
[32m[20221213 21:09:00 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:09:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.20
[32m[20221213 21:09:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 595.00
[32m[20221213 21:09:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.00
[32m[20221213 21:09:00 @agent_ppo2.py:143][0m Total time:      13.43 min
[32m[20221213 21:09:00 @agent_ppo2.py:145][0m 1300480 total steps have happened
[32m[20221213 21:09:00 @agent_ppo2.py:121][0m #------------------------ Iteration 635 --------------------------#
[32m[20221213 21:09:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:01 @agent_ppo2.py:185][0m |          -0.0016 |          94.3150 |          19.0985 |
[32m[20221213 21:09:01 @agent_ppo2.py:185][0m |           0.0038 |          98.2315 |          19.0925 |
[32m[20221213 21:09:01 @agent_ppo2.py:185][0m |           0.0028 |          97.0009 |          19.0800 |
[32m[20221213 21:09:01 @agent_ppo2.py:185][0m |          -0.0066 |          92.9286 |          19.0827 |
[32m[20221213 21:09:01 @agent_ppo2.py:185][0m |          -0.0107 |          92.7982 |          19.0944 |
[32m[20221213 21:09:01 @agent_ppo2.py:185][0m |          -0.0071 |          92.5771 |          19.0846 |
[32m[20221213 21:09:01 @agent_ppo2.py:185][0m |          -0.0077 |          92.0768 |          19.0970 |
[32m[20221213 21:09:01 @agent_ppo2.py:185][0m |          -0.0064 |          92.0084 |          19.0859 |
[32m[20221213 21:09:01 @agent_ppo2.py:185][0m |          -0.0074 |          91.7498 |          19.1005 |
[32m[20221213 21:09:02 @agent_ppo2.py:185][0m |          -0.0058 |          91.9784 |          19.0944 |
[32m[20221213 21:09:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:09:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 550.40
[32m[20221213 21:09:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 592.00
[32m[20221213 21:09:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 565.00
[32m[20221213 21:09:02 @agent_ppo2.py:143][0m Total time:      13.45 min
[32m[20221213 21:09:02 @agent_ppo2.py:145][0m 1302528 total steps have happened
[32m[20221213 21:09:02 @agent_ppo2.py:121][0m #------------------------ Iteration 636 --------------------------#
[32m[20221213 21:09:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:02 @agent_ppo2.py:185][0m |           0.0180 |         103.6004 |          19.0653 |
[32m[20221213 21:09:02 @agent_ppo2.py:185][0m |          -0.0034 |          90.0006 |          19.0548 |
[32m[20221213 21:09:02 @agent_ppo2.py:185][0m |          -0.0051 |          89.3108 |          19.0416 |
[32m[20221213 21:09:02 @agent_ppo2.py:185][0m |          -0.0037 |          88.7568 |          19.0543 |
[32m[20221213 21:09:02 @agent_ppo2.py:185][0m |          -0.0038 |          88.5997 |          19.0520 |
[32m[20221213 21:09:02 @agent_ppo2.py:185][0m |          -0.0075 |          88.1010 |          19.0525 |
[32m[20221213 21:09:02 @agent_ppo2.py:185][0m |          -0.0068 |          87.8654 |          19.0522 |
[32m[20221213 21:09:03 @agent_ppo2.py:185][0m |          -0.0030 |          87.5312 |          19.0470 |
[32m[20221213 21:09:03 @agent_ppo2.py:185][0m |          -0.0076 |          87.4996 |          19.0537 |
[32m[20221213 21:09:03 @agent_ppo2.py:185][0m |          -0.0047 |          87.1427 |          19.0542 |
[32m[20221213 21:09:03 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:09:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 557.80
[32m[20221213 21:09:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 610.00
[32m[20221213 21:09:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.00
[32m[20221213 21:09:03 @agent_ppo2.py:143][0m Total time:      13.47 min
[32m[20221213 21:09:03 @agent_ppo2.py:145][0m 1304576 total steps have happened
[32m[20221213 21:09:03 @agent_ppo2.py:121][0m #------------------------ Iteration 637 --------------------------#
[32m[20221213 21:09:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:09:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:03 @agent_ppo2.py:185][0m |           0.0095 |          96.3110 |          19.0606 |
[32m[20221213 21:09:03 @agent_ppo2.py:185][0m |          -0.0007 |          89.6653 |          19.0422 |
[32m[20221213 21:09:03 @agent_ppo2.py:185][0m |          -0.0059 |          89.2766 |          19.0540 |
[32m[20221213 21:09:03 @agent_ppo2.py:185][0m |          -0.0048 |          88.9007 |          19.0652 |
[32m[20221213 21:09:03 @agent_ppo2.py:185][0m |          -0.0055 |          88.6751 |          19.0726 |
[32m[20221213 21:09:04 @agent_ppo2.py:185][0m |          -0.0023 |          88.5089 |          19.0681 |
[32m[20221213 21:09:04 @agent_ppo2.py:185][0m |          -0.0048 |          88.3060 |          19.0753 |
[32m[20221213 21:09:04 @agent_ppo2.py:185][0m |           0.0072 |          92.4517 |          19.0820 |
[32m[20221213 21:09:04 @agent_ppo2.py:185][0m |          -0.0068 |          87.8916 |          19.0752 |
[32m[20221213 21:09:04 @agent_ppo2.py:185][0m |           0.0042 |          95.5051 |          19.0811 |
[32m[20221213 21:09:04 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:09:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 562.60
[32m[20221213 21:09:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.00
[32m[20221213 21:09:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 603.00
[32m[20221213 21:09:04 @agent_ppo2.py:143][0m Total time:      13.49 min
[32m[20221213 21:09:04 @agent_ppo2.py:145][0m 1306624 total steps have happened
[32m[20221213 21:09:04 @agent_ppo2.py:121][0m #------------------------ Iteration 638 --------------------------#
[32m[20221213 21:09:04 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:09:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:04 @agent_ppo2.py:185][0m |           0.0035 |          91.5554 |          19.2279 |
[32m[20221213 21:09:04 @agent_ppo2.py:185][0m |           0.0035 |          91.4850 |          19.2164 |
[32m[20221213 21:09:05 @agent_ppo2.py:185][0m |           0.0101 |          98.9824 |          19.2182 |
[32m[20221213 21:09:05 @agent_ppo2.py:185][0m |          -0.0029 |          89.3532 |          19.2060 |
[32m[20221213 21:09:05 @agent_ppo2.py:185][0m |          -0.0070 |          89.3041 |          19.2176 |
[32m[20221213 21:09:05 @agent_ppo2.py:185][0m |          -0.0043 |          89.1389 |          19.2115 |
[32m[20221213 21:09:05 @agent_ppo2.py:185][0m |          -0.0069 |          88.9527 |          19.2258 |
[32m[20221213 21:09:05 @agent_ppo2.py:185][0m |          -0.0074 |          88.8010 |          19.2245 |
[32m[20221213 21:09:05 @agent_ppo2.py:185][0m |           0.0022 |          95.0928 |          19.2259 |
[32m[20221213 21:09:05 @agent_ppo2.py:185][0m |          -0.0078 |          88.6341 |          19.2272 |
[32m[20221213 21:09:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:09:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.20
[32m[20221213 21:09:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.00
[32m[20221213 21:09:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.00
[32m[20221213 21:09:05 @agent_ppo2.py:143][0m Total time:      13.51 min
[32m[20221213 21:09:05 @agent_ppo2.py:145][0m 1308672 total steps have happened
[32m[20221213 21:09:05 @agent_ppo2.py:121][0m #------------------------ Iteration 639 --------------------------#
[32m[20221213 21:09:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:06 @agent_ppo2.py:185][0m |          -0.0005 |          90.0474 |          19.1317 |
[32m[20221213 21:09:06 @agent_ppo2.py:185][0m |          -0.0037 |          89.3051 |          19.1274 |
[32m[20221213 21:09:06 @agent_ppo2.py:185][0m |          -0.0050 |          88.8817 |          19.1303 |
[32m[20221213 21:09:06 @agent_ppo2.py:185][0m |           0.0022 |          92.4435 |          19.1284 |
[32m[20221213 21:09:06 @agent_ppo2.py:185][0m |          -0.0059 |          88.4921 |          19.1224 |
[32m[20221213 21:09:06 @agent_ppo2.py:185][0m |          -0.0065 |          88.2762 |          19.1279 |
[32m[20221213 21:09:06 @agent_ppo2.py:185][0m |          -0.0072 |          88.1160 |          19.1346 |
[32m[20221213 21:09:06 @agent_ppo2.py:185][0m |          -0.0054 |          88.0382 |          19.1284 |
[32m[20221213 21:09:06 @agent_ppo2.py:185][0m |          -0.0061 |          87.9676 |          19.1441 |
[32m[20221213 21:09:06 @agent_ppo2.py:185][0m |          -0.0037 |          87.8089 |          19.1397 |
[32m[20221213 21:09:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:09:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 559.60
[32m[20221213 21:09:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 576.00
[32m[20221213 21:09:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 658.00
[32m[20221213 21:09:06 @agent_ppo2.py:143][0m Total time:      13.53 min
[32m[20221213 21:09:06 @agent_ppo2.py:145][0m 1310720 total steps have happened
[32m[20221213 21:09:06 @agent_ppo2.py:121][0m #------------------------ Iteration 640 --------------------------#
[32m[20221213 21:09:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:07 @agent_ppo2.py:185][0m |          -0.0003 |          91.3827 |          19.2562 |
[32m[20221213 21:09:07 @agent_ppo2.py:185][0m |          -0.0060 |          90.0571 |          19.2472 |
[32m[20221213 21:09:07 @agent_ppo2.py:185][0m |          -0.0053 |          89.4876 |          19.2336 |
[32m[20221213 21:09:07 @agent_ppo2.py:185][0m |          -0.0066 |          88.9829 |          19.2349 |
[32m[20221213 21:09:07 @agent_ppo2.py:185][0m |          -0.0074 |          88.5854 |          19.2433 |
[32m[20221213 21:09:07 @agent_ppo2.py:185][0m |          -0.0056 |          88.5000 |          19.2429 |
[32m[20221213 21:09:07 @agent_ppo2.py:185][0m |          -0.0055 |          88.1733 |          19.2412 |
[32m[20221213 21:09:07 @agent_ppo2.py:185][0m |          -0.0084 |          87.9306 |          19.2345 |
[32m[20221213 21:09:07 @agent_ppo2.py:185][0m |          -0.0076 |          88.8672 |          19.2499 |
[32m[20221213 21:09:07 @agent_ppo2.py:185][0m |          -0.0083 |          88.1853 |          19.2478 |
[32m[20221213 21:09:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:09:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.60
[32m[20221213 21:09:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 599.00
[32m[20221213 21:09:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.00
[32m[20221213 21:09:08 @agent_ppo2.py:143][0m Total time:      13.55 min
[32m[20221213 21:09:08 @agent_ppo2.py:145][0m 1312768 total steps have happened
[32m[20221213 21:09:08 @agent_ppo2.py:121][0m #------------------------ Iteration 641 --------------------------#
[32m[20221213 21:09:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:08 @agent_ppo2.py:185][0m |           0.0004 |          93.2792 |          19.0737 |
[32m[20221213 21:09:08 @agent_ppo2.py:185][0m |           0.0006 |          93.0967 |          19.0466 |
[32m[20221213 21:09:08 @agent_ppo2.py:185][0m |          -0.0081 |          91.9517 |          19.0410 |
[32m[20221213 21:09:08 @agent_ppo2.py:185][0m |          -0.0070 |          91.5684 |          19.0352 |
[32m[20221213 21:09:08 @agent_ppo2.py:185][0m |          -0.0069 |          91.2481 |          19.0345 |
[32m[20221213 21:09:08 @agent_ppo2.py:185][0m |          -0.0038 |          93.4598 |          19.0284 |
[32m[20221213 21:09:08 @agent_ppo2.py:185][0m |          -0.0004 |          97.9514 |          19.0333 |
[32m[20221213 21:09:09 @agent_ppo2.py:185][0m |          -0.0066 |          90.9655 |          19.0235 |
[32m[20221213 21:09:09 @agent_ppo2.py:185][0m |          -0.0091 |          90.7900 |          19.0229 |
[32m[20221213 21:09:09 @agent_ppo2.py:185][0m |          -0.0104 |          90.6401 |          19.0218 |
[32m[20221213 21:09:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:09:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.40
[32m[20221213 21:09:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 590.00
[32m[20221213 21:09:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 597.00
[32m[20221213 21:09:09 @agent_ppo2.py:143][0m Total time:      13.57 min
[32m[20221213 21:09:09 @agent_ppo2.py:145][0m 1314816 total steps have happened
[32m[20221213 21:09:09 @agent_ppo2.py:121][0m #------------------------ Iteration 642 --------------------------#
[32m[20221213 21:09:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:09 @agent_ppo2.py:185][0m |           0.0016 |          87.0547 |          19.0187 |
[32m[20221213 21:09:09 @agent_ppo2.py:185][0m |          -0.0027 |          86.3961 |          19.0118 |
[32m[20221213 21:09:09 @agent_ppo2.py:185][0m |          -0.0025 |          86.0369 |          19.0077 |
[32m[20221213 21:09:09 @agent_ppo2.py:185][0m |          -0.0022 |          86.0337 |          19.0157 |
[32m[20221213 21:09:09 @agent_ppo2.py:185][0m |          -0.0046 |          85.9177 |          19.0047 |
[32m[20221213 21:09:10 @agent_ppo2.py:185][0m |          -0.0050 |          85.6080 |          19.0080 |
[32m[20221213 21:09:10 @agent_ppo2.py:185][0m |          -0.0073 |          85.5891 |          19.0004 |
[32m[20221213 21:09:10 @agent_ppo2.py:185][0m |          -0.0034 |          85.4705 |          19.0075 |
[32m[20221213 21:09:10 @agent_ppo2.py:185][0m |           0.0068 |          97.4517 |          19.0080 |
[32m[20221213 21:09:10 @agent_ppo2.py:185][0m |          -0.0045 |          85.4590 |          18.9898 |
[32m[20221213 21:09:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:09:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 572.80
[32m[20221213 21:09:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 612.00
[32m[20221213 21:09:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 585.00
[32m[20221213 21:09:10 @agent_ppo2.py:143][0m Total time:      13.59 min
[32m[20221213 21:09:10 @agent_ppo2.py:145][0m 1316864 total steps have happened
[32m[20221213 21:09:10 @agent_ppo2.py:121][0m #------------------------ Iteration 643 --------------------------#
[32m[20221213 21:09:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:10 @agent_ppo2.py:185][0m |           0.0003 |          91.7470 |          19.2018 |
[32m[20221213 21:09:10 @agent_ppo2.py:185][0m |          -0.0022 |          89.8393 |          19.2009 |
[32m[20221213 21:09:10 @agent_ppo2.py:185][0m |          -0.0060 |          89.3325 |          19.2010 |
[32m[20221213 21:09:11 @agent_ppo2.py:185][0m |          -0.0053 |          88.9915 |          19.1949 |
[32m[20221213 21:09:11 @agent_ppo2.py:185][0m |          -0.0081 |          88.8258 |          19.2047 |
[32m[20221213 21:09:11 @agent_ppo2.py:185][0m |          -0.0063 |          88.5187 |          19.1996 |
[32m[20221213 21:09:11 @agent_ppo2.py:185][0m |          -0.0059 |          89.0419 |          19.2183 |
[32m[20221213 21:09:11 @agent_ppo2.py:185][0m |          -0.0073 |          88.3445 |          19.2191 |
[32m[20221213 21:09:11 @agent_ppo2.py:185][0m |          -0.0063 |          88.0728 |          19.2266 |
[32m[20221213 21:09:11 @agent_ppo2.py:185][0m |          -0.0059 |          87.9079 |          19.2266 |
[32m[20221213 21:09:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:09:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 538.00
[32m[20221213 21:09:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 578.00
[32m[20221213 21:09:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.00
[32m[20221213 21:09:11 @agent_ppo2.py:143][0m Total time:      13.61 min
[32m[20221213 21:09:11 @agent_ppo2.py:145][0m 1318912 total steps have happened
[32m[20221213 21:09:11 @agent_ppo2.py:121][0m #------------------------ Iteration 644 --------------------------#
[32m[20221213 21:09:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:12 @agent_ppo2.py:185][0m |           0.0033 |          91.7352 |          19.1065 |
[32m[20221213 21:09:12 @agent_ppo2.py:185][0m |          -0.0018 |          88.7419 |          19.1016 |
[32m[20221213 21:09:12 @agent_ppo2.py:185][0m |           0.0059 |          94.2458 |          19.0807 |
[32m[20221213 21:09:12 @agent_ppo2.py:185][0m |          -0.0023 |          87.9438 |          19.0849 |
[32m[20221213 21:09:12 @agent_ppo2.py:185][0m |          -0.0031 |          87.6526 |          19.0805 |
[32m[20221213 21:09:12 @agent_ppo2.py:185][0m |          -0.0057 |          87.3058 |          19.0829 |
[32m[20221213 21:09:12 @agent_ppo2.py:185][0m |          -0.0089 |          87.2030 |          19.0787 |
[32m[20221213 21:09:12 @agent_ppo2.py:185][0m |          -0.0064 |          86.9971 |          19.0797 |
[32m[20221213 21:09:12 @agent_ppo2.py:185][0m |          -0.0077 |          86.9223 |          19.0736 |
[32m[20221213 21:09:12 @agent_ppo2.py:185][0m |          -0.0088 |          86.8532 |          19.0660 |
[32m[20221213 21:09:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:09:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 562.40
[32m[20221213 21:09:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 622.00
[32m[20221213 21:09:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 627.00
[32m[20221213 21:09:12 @agent_ppo2.py:143][0m Total time:      13.63 min
[32m[20221213 21:09:12 @agent_ppo2.py:145][0m 1320960 total steps have happened
[32m[20221213 21:09:12 @agent_ppo2.py:121][0m #------------------------ Iteration 645 --------------------------#
[32m[20221213 21:09:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:13 @agent_ppo2.py:185][0m |          -0.0003 |          90.8541 |          19.0532 |
[32m[20221213 21:09:13 @agent_ppo2.py:185][0m |          -0.0057 |          90.1772 |          19.0474 |
[32m[20221213 21:09:13 @agent_ppo2.py:185][0m |          -0.0048 |          89.7646 |          19.0457 |
[32m[20221213 21:09:13 @agent_ppo2.py:185][0m |          -0.0075 |          89.6094 |          19.0496 |
[32m[20221213 21:09:13 @agent_ppo2.py:185][0m |          -0.0082 |          89.5319 |          19.0316 |
[32m[20221213 21:09:13 @agent_ppo2.py:185][0m |          -0.0076 |          89.2646 |          19.0551 |
[32m[20221213 21:09:13 @agent_ppo2.py:185][0m |          -0.0099 |          89.0909 |          19.0534 |
[32m[20221213 21:09:13 @agent_ppo2.py:185][0m |          -0.0092 |          88.9439 |          19.0543 |
[32m[20221213 21:09:13 @agent_ppo2.py:185][0m |          -0.0098 |          88.9110 |          19.0492 |
[32m[20221213 21:09:13 @agent_ppo2.py:185][0m |          -0.0020 |          95.9291 |          19.0602 |
[32m[20221213 21:09:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:09:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 572.80
[32m[20221213 21:09:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.00
[32m[20221213 21:09:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 606.00
[32m[20221213 21:09:14 @agent_ppo2.py:143][0m Total time:      13.65 min
[32m[20221213 21:09:14 @agent_ppo2.py:145][0m 1323008 total steps have happened
[32m[20221213 21:09:14 @agent_ppo2.py:121][0m #------------------------ Iteration 646 --------------------------#
[32m[20221213 21:09:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:14 @agent_ppo2.py:185][0m |          -0.0021 |          88.6807 |          19.2236 |
[32m[20221213 21:09:14 @agent_ppo2.py:185][0m |           0.0018 |          88.8718 |          19.2139 |
[32m[20221213 21:09:14 @agent_ppo2.py:185][0m |          -0.0043 |          87.7431 |          19.2131 |
[32m[20221213 21:09:14 @agent_ppo2.py:185][0m |          -0.0049 |          87.4257 |          19.2179 |
[32m[20221213 21:09:14 @agent_ppo2.py:185][0m |           0.0018 |          90.5183 |          19.2089 |
[32m[20221213 21:09:14 @agent_ppo2.py:185][0m |          -0.0043 |          87.0446 |          19.2200 |
[32m[20221213 21:09:14 @agent_ppo2.py:185][0m |          -0.0047 |          87.1015 |          19.2070 |
[32m[20221213 21:09:14 @agent_ppo2.py:185][0m |          -0.0005 |          89.7791 |          19.2177 |
[32m[20221213 21:09:15 @agent_ppo2.py:185][0m |          -0.0079 |          86.7081 |          19.2114 |
[32m[20221213 21:09:15 @agent_ppo2.py:185][0m |          -0.0067 |          86.6052 |          19.2170 |
[32m[20221213 21:09:15 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:09:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.00
[32m[20221213 21:09:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.00
[32m[20221213 21:09:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 617.00
[32m[20221213 21:09:15 @agent_ppo2.py:143][0m Total time:      13.67 min
[32m[20221213 21:09:15 @agent_ppo2.py:145][0m 1325056 total steps have happened
[32m[20221213 21:09:15 @agent_ppo2.py:121][0m #------------------------ Iteration 647 --------------------------#
[32m[20221213 21:09:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:09:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:15 @agent_ppo2.py:185][0m |          -0.0011 |          94.1433 |          19.1430 |
[32m[20221213 21:09:15 @agent_ppo2.py:185][0m |           0.0046 |          98.0692 |          19.1387 |
[32m[20221213 21:09:15 @agent_ppo2.py:185][0m |          -0.0042 |          92.2343 |          19.1137 |
[32m[20221213 21:09:15 @agent_ppo2.py:185][0m |          -0.0082 |          91.6362 |          19.1049 |
[32m[20221213 21:09:15 @agent_ppo2.py:185][0m |          -0.0063 |          91.2719 |          19.1006 |
[32m[20221213 21:09:16 @agent_ppo2.py:185][0m |           0.0057 |         100.6343 |          19.0985 |
[32m[20221213 21:09:16 @agent_ppo2.py:185][0m |           0.0024 |          97.4239 |          19.0866 |
[32m[20221213 21:09:16 @agent_ppo2.py:185][0m |          -0.0085 |          90.4448 |          19.0837 |
[32m[20221213 21:09:16 @agent_ppo2.py:185][0m |          -0.0087 |          90.0053 |          19.0845 |
[32m[20221213 21:09:16 @agent_ppo2.py:185][0m |          -0.0077 |          89.6979 |          19.0652 |
[32m[20221213 21:09:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:09:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 531.80
[32m[20221213 21:09:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 560.00
[32m[20221213 21:09:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 614.00
[32m[20221213 21:09:16 @agent_ppo2.py:143][0m Total time:      13.69 min
[32m[20221213 21:09:16 @agent_ppo2.py:145][0m 1327104 total steps have happened
[32m[20221213 21:09:16 @agent_ppo2.py:121][0m #------------------------ Iteration 648 --------------------------#
[32m[20221213 21:09:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:16 @agent_ppo2.py:185][0m |           0.0081 |         101.3226 |          19.1510 |
[32m[20221213 21:09:16 @agent_ppo2.py:185][0m |          -0.0010 |          95.8359 |          19.1107 |
[32m[20221213 21:09:16 @agent_ppo2.py:185][0m |          -0.0070 |          95.2817 |          19.1077 |
[32m[20221213 21:09:17 @agent_ppo2.py:185][0m |          -0.0055 |          94.7692 |          19.1161 |
[32m[20221213 21:09:17 @agent_ppo2.py:185][0m |          -0.0062 |          94.5316 |          19.1078 |
[32m[20221213 21:09:17 @agent_ppo2.py:185][0m |          -0.0060 |          94.3168 |          19.1081 |
[32m[20221213 21:09:17 @agent_ppo2.py:185][0m |          -0.0065 |          94.0132 |          19.0983 |
[32m[20221213 21:09:17 @agent_ppo2.py:185][0m |          -0.0060 |          93.9339 |          19.1019 |
[32m[20221213 21:09:17 @agent_ppo2.py:185][0m |           0.0007 |          95.8600 |          19.0993 |
[32m[20221213 21:09:17 @agent_ppo2.py:185][0m |          -0.0025 |          94.3355 |          19.1040 |
[32m[20221213 21:09:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:09:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.40
[32m[20221213 21:09:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 602.00
[32m[20221213 21:09:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 617.00
[32m[20221213 21:09:17 @agent_ppo2.py:143][0m Total time:      13.71 min
[32m[20221213 21:09:17 @agent_ppo2.py:145][0m 1329152 total steps have happened
[32m[20221213 21:09:17 @agent_ppo2.py:121][0m #------------------------ Iteration 649 --------------------------#
[32m[20221213 21:09:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:17 @agent_ppo2.py:185][0m |           0.0052 |          96.6240 |          19.1885 |
[32m[20221213 21:09:18 @agent_ppo2.py:185][0m |          -0.0050 |          91.2977 |          19.1822 |
[32m[20221213 21:09:18 @agent_ppo2.py:185][0m |          -0.0039 |          90.9109 |          19.1756 |
[32m[20221213 21:09:18 @agent_ppo2.py:185][0m |          -0.0054 |          90.2068 |          19.1715 |
[32m[20221213 21:09:18 @agent_ppo2.py:185][0m |          -0.0049 |          90.3682 |          19.1731 |
[32m[20221213 21:09:18 @agent_ppo2.py:185][0m |          -0.0060 |          89.7617 |          19.1707 |
[32m[20221213 21:09:18 @agent_ppo2.py:185][0m |           0.0073 |          99.1106 |          19.1702 |
[32m[20221213 21:09:18 @agent_ppo2.py:185][0m |          -0.0076 |          89.6362 |          19.1793 |
[32m[20221213 21:09:18 @agent_ppo2.py:185][0m |          -0.0084 |          89.4530 |          19.1702 |
[32m[20221213 21:09:18 @agent_ppo2.py:185][0m |          -0.0094 |          89.2188 |          19.1695 |
[32m[20221213 21:09:18 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:09:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.20
[32m[20221213 21:09:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.00
[32m[20221213 21:09:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 603.00
[32m[20221213 21:09:18 @agent_ppo2.py:143][0m Total time:      13.73 min
[32m[20221213 21:09:18 @agent_ppo2.py:145][0m 1331200 total steps have happened
[32m[20221213 21:09:18 @agent_ppo2.py:121][0m #------------------------ Iteration 650 --------------------------#
[32m[20221213 21:09:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:09:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:19 @agent_ppo2.py:185][0m |           0.0097 |          99.0234 |          19.1335 |
[32m[20221213 21:09:19 @agent_ppo2.py:185][0m |          -0.0032 |          92.0038 |          19.1365 |
[32m[20221213 21:09:19 @agent_ppo2.py:185][0m |           0.0004 |          92.5771 |          19.1182 |
[32m[20221213 21:09:19 @agent_ppo2.py:185][0m |          -0.0094 |          90.7231 |          19.1266 |
[32m[20221213 21:09:19 @agent_ppo2.py:185][0m |          -0.0086 |          90.5011 |          19.1280 |
[32m[20221213 21:09:19 @agent_ppo2.py:185][0m |          -0.0047 |          92.9995 |          19.1369 |
[32m[20221213 21:09:19 @agent_ppo2.py:185][0m |          -0.0093 |          90.0792 |          19.1291 |
[32m[20221213 21:09:19 @agent_ppo2.py:185][0m |          -0.0098 |          90.0194 |          19.1317 |
[32m[20221213 21:09:19 @agent_ppo2.py:185][0m |          -0.0084 |          89.7360 |          19.1379 |
[32m[20221213 21:09:19 @agent_ppo2.py:185][0m |          -0.0120 |          89.6243 |          19.1296 |
[32m[20221213 21:09:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:09:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 588.00
[32m[20221213 21:09:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 614.00
[32m[20221213 21:09:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 553.00
[32m[20221213 21:09:20 @agent_ppo2.py:143][0m Total time:      13.75 min
[32m[20221213 21:09:20 @agent_ppo2.py:145][0m 1333248 total steps have happened
[32m[20221213 21:09:20 @agent_ppo2.py:121][0m #------------------------ Iteration 651 --------------------------#
[32m[20221213 21:09:20 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:09:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:20 @agent_ppo2.py:185][0m |          -0.0009 |          91.1797 |          19.1376 |
[32m[20221213 21:09:20 @agent_ppo2.py:185][0m |          -0.0063 |          89.8986 |          19.1090 |
[32m[20221213 21:09:20 @agent_ppo2.py:185][0m |           0.0027 |          94.2376 |          19.1087 |
[32m[20221213 21:09:20 @agent_ppo2.py:185][0m |          -0.0024 |          90.7886 |          19.1028 |
[32m[20221213 21:09:20 @agent_ppo2.py:185][0m |          -0.0085 |          88.4443 |          19.1003 |
[32m[20221213 21:09:20 @agent_ppo2.py:185][0m |          -0.0096 |          88.1923 |          19.1058 |
[32m[20221213 21:09:20 @agent_ppo2.py:185][0m |          -0.0071 |          87.9528 |          19.1138 |
[32m[20221213 21:09:20 @agent_ppo2.py:185][0m |          -0.0092 |          87.8217 |          19.1086 |
[32m[20221213 21:09:21 @agent_ppo2.py:185][0m |          -0.0076 |          87.7281 |          19.1022 |
[32m[20221213 21:09:21 @agent_ppo2.py:185][0m |           0.0056 |          97.3104 |          19.1072 |
[32m[20221213 21:09:21 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:09:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 532.40
[32m[20221213 21:09:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.00
[32m[20221213 21:09:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.00
[32m[20221213 21:09:21 @agent_ppo2.py:143][0m Total time:      13.77 min
[32m[20221213 21:09:21 @agent_ppo2.py:145][0m 1335296 total steps have happened
[32m[20221213 21:09:21 @agent_ppo2.py:121][0m #------------------------ Iteration 652 --------------------------#
[32m[20221213 21:09:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:21 @agent_ppo2.py:185][0m |           0.0005 |          91.1277 |          19.1568 |
[32m[20221213 21:09:21 @agent_ppo2.py:185][0m |          -0.0069 |          89.3055 |          19.1581 |
[32m[20221213 21:09:21 @agent_ppo2.py:185][0m |          -0.0079 |          88.8194 |          19.1604 |
[32m[20221213 21:09:21 @agent_ppo2.py:185][0m |           0.0067 |         100.5663 |          19.1531 |
[32m[20221213 21:09:21 @agent_ppo2.py:185][0m |          -0.0038 |          89.1942 |          19.1419 |
[32m[20221213 21:09:21 @agent_ppo2.py:185][0m |          -0.0080 |          88.3362 |          19.1446 |
[32m[20221213 21:09:22 @agent_ppo2.py:185][0m |          -0.0001 |          91.0652 |          19.1418 |
[32m[20221213 21:09:22 @agent_ppo2.py:185][0m |          -0.0007 |          91.7998 |          19.1411 |
[32m[20221213 21:09:22 @agent_ppo2.py:185][0m |          -0.0092 |          87.9730 |          19.1424 |
[32m[20221213 21:09:22 @agent_ppo2.py:185][0m |          -0.0092 |          87.8328 |          19.1410 |
[32m[20221213 21:09:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:09:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 562.60
[32m[20221213 21:09:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 582.00
[32m[20221213 21:09:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 630.00
[32m[20221213 21:09:22 @agent_ppo2.py:143][0m Total time:      13.79 min
[32m[20221213 21:09:22 @agent_ppo2.py:145][0m 1337344 total steps have happened
[32m[20221213 21:09:22 @agent_ppo2.py:121][0m #------------------------ Iteration 653 --------------------------#
[32m[20221213 21:09:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:22 @agent_ppo2.py:185][0m |           0.0070 |          96.9194 |          19.1842 |
[32m[20221213 21:09:22 @agent_ppo2.py:185][0m |          -0.0023 |          91.1322 |          19.1593 |
[32m[20221213 21:09:22 @agent_ppo2.py:185][0m |           0.0008 |          94.0722 |          19.1519 |
[32m[20221213 21:09:23 @agent_ppo2.py:185][0m |          -0.0063 |          90.3728 |          19.1523 |
[32m[20221213 21:09:23 @agent_ppo2.py:185][0m |          -0.0069 |          90.1655 |          19.1494 |
[32m[20221213 21:09:23 @agent_ppo2.py:185][0m |          -0.0063 |          90.0611 |          19.1470 |
[32m[20221213 21:09:23 @agent_ppo2.py:185][0m |          -0.0045 |          89.8502 |          19.1417 |
[32m[20221213 21:09:23 @agent_ppo2.py:185][0m |          -0.0056 |          89.6622 |          19.1412 |
[32m[20221213 21:09:23 @agent_ppo2.py:185][0m |          -0.0060 |          89.6453 |          19.1425 |
[32m[20221213 21:09:23 @agent_ppo2.py:185][0m |          -0.0097 |          89.5808 |          19.1419 |
[32m[20221213 21:09:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:09:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 561.60
[32m[20221213 21:09:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 612.00
[32m[20221213 21:09:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 621.00
[32m[20221213 21:09:23 @agent_ppo2.py:143][0m Total time:      13.81 min
[32m[20221213 21:09:23 @agent_ppo2.py:145][0m 1339392 total steps have happened
[32m[20221213 21:09:23 @agent_ppo2.py:121][0m #------------------------ Iteration 654 --------------------------#
[32m[20221213 21:09:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:23 @agent_ppo2.py:185][0m |          -0.0020 |          91.5745 |          19.1456 |
[32m[20221213 21:09:24 @agent_ppo2.py:185][0m |          -0.0075 |          90.9326 |          19.1141 |
[32m[20221213 21:09:24 @agent_ppo2.py:185][0m |          -0.0065 |          90.4388 |          19.1119 |
[32m[20221213 21:09:24 @agent_ppo2.py:185][0m |          -0.0085 |          90.1629 |          19.0978 |
[32m[20221213 21:09:24 @agent_ppo2.py:185][0m |          -0.0089 |          89.8502 |          19.0956 |
[32m[20221213 21:09:24 @agent_ppo2.py:185][0m |          -0.0054 |          90.9251 |          19.0859 |
[32m[20221213 21:09:24 @agent_ppo2.py:185][0m |          -0.0053 |          89.2911 |          19.0824 |
[32m[20221213 21:09:24 @agent_ppo2.py:185][0m |          -0.0078 |          89.2195 |          19.0823 |
[32m[20221213 21:09:24 @agent_ppo2.py:185][0m |          -0.0013 |          94.3786 |          19.0742 |
[32m[20221213 21:09:24 @agent_ppo2.py:185][0m |          -0.0075 |          88.6707 |          19.0652 |
[32m[20221213 21:09:24 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:09:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.60
[32m[20221213 21:09:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 610.00
[32m[20221213 21:09:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.00
[32m[20221213 21:09:24 @agent_ppo2.py:143][0m Total time:      13.83 min
[32m[20221213 21:09:24 @agent_ppo2.py:145][0m 1341440 total steps have happened
[32m[20221213 21:09:24 @agent_ppo2.py:121][0m #------------------------ Iteration 655 --------------------------#
[32m[20221213 21:09:24 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:09:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:25 @agent_ppo2.py:185][0m |           0.0128 |         103.8611 |          19.0832 |
[32m[20221213 21:09:25 @agent_ppo2.py:185][0m |          -0.0044 |          94.0683 |          19.0814 |
[32m[20221213 21:09:25 @agent_ppo2.py:185][0m |          -0.0064 |          93.4702 |          19.0836 |
[32m[20221213 21:09:25 @agent_ppo2.py:185][0m |          -0.0064 |          93.0774 |          19.0823 |
[32m[20221213 21:09:25 @agent_ppo2.py:185][0m |          -0.0065 |          92.5903 |          19.0838 |
[32m[20221213 21:09:25 @agent_ppo2.py:185][0m |          -0.0062 |          92.3128 |          19.0904 |
[32m[20221213 21:09:25 @agent_ppo2.py:185][0m |          -0.0069 |          92.2220 |          19.0933 |
[32m[20221213 21:09:25 @agent_ppo2.py:185][0m |          -0.0085 |          91.9523 |          19.0889 |
[32m[20221213 21:09:25 @agent_ppo2.py:185][0m |          -0.0063 |          91.7337 |          19.0965 |
[32m[20221213 21:09:25 @agent_ppo2.py:185][0m |          -0.0016 |          93.9519 |          19.0978 |
[32m[20221213 21:09:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:09:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 572.00
[32m[20221213 21:09:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 623.00
[32m[20221213 21:09:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.00
[32m[20221213 21:09:26 @agent_ppo2.py:143][0m Total time:      13.85 min
[32m[20221213 21:09:26 @agent_ppo2.py:145][0m 1343488 total steps have happened
[32m[20221213 21:09:26 @agent_ppo2.py:121][0m #------------------------ Iteration 656 --------------------------#
[32m[20221213 21:09:26 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:09:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:26 @agent_ppo2.py:185][0m |          -0.0012 |          95.3006 |          19.1958 |
[32m[20221213 21:09:26 @agent_ppo2.py:185][0m |          -0.0038 |          90.6316 |          19.1689 |
[32m[20221213 21:09:26 @agent_ppo2.py:185][0m |          -0.0034 |          89.2844 |          19.1731 |
[32m[20221213 21:09:26 @agent_ppo2.py:185][0m |          -0.0016 |          89.1996 |          19.1765 |
[32m[20221213 21:09:26 @agent_ppo2.py:185][0m |           0.0032 |          92.0807 |          19.1801 |
[32m[20221213 21:09:26 @agent_ppo2.py:185][0m |           0.0064 |          92.8181 |          19.1818 |
[32m[20221213 21:09:26 @agent_ppo2.py:185][0m |          -0.0021 |          87.6814 |          19.1546 |
[32m[20221213 21:09:26 @agent_ppo2.py:185][0m |          -0.0049 |          87.3698 |          19.1774 |
[32m[20221213 21:09:27 @agent_ppo2.py:185][0m |          -0.0030 |          87.3576 |          19.1780 |
[32m[20221213 21:09:27 @agent_ppo2.py:185][0m |          -0.0067 |          86.7833 |          19.1780 |
[32m[20221213 21:09:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:09:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 569.40
[32m[20221213 21:09:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.00
[32m[20221213 21:09:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:09:27 @agent_ppo2.py:143][0m Total time:      13.87 min
[32m[20221213 21:09:27 @agent_ppo2.py:145][0m 1345536 total steps have happened
[32m[20221213 21:09:27 @agent_ppo2.py:121][0m #------------------------ Iteration 657 --------------------------#
[32m[20221213 21:09:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:27 @agent_ppo2.py:185][0m |           0.0001 |          98.6543 |          19.1594 |
[32m[20221213 21:09:27 @agent_ppo2.py:185][0m |          -0.0017 |          97.5449 |          19.1454 |
[32m[20221213 21:09:27 @agent_ppo2.py:185][0m |          -0.0045 |          96.3241 |          19.1384 |
[32m[20221213 21:09:27 @agent_ppo2.py:185][0m |          -0.0031 |          95.7181 |          19.1305 |
[32m[20221213 21:09:27 @agent_ppo2.py:185][0m |          -0.0054 |          95.4730 |          19.1366 |
[32m[20221213 21:09:27 @agent_ppo2.py:185][0m |          -0.0064 |          95.2904 |          19.1324 |
[32m[20221213 21:09:28 @agent_ppo2.py:185][0m |          -0.0088 |          94.9805 |          19.1267 |
[32m[20221213 21:09:28 @agent_ppo2.py:185][0m |          -0.0085 |          94.6926 |          19.1219 |
[32m[20221213 21:09:28 @agent_ppo2.py:185][0m |          -0.0084 |          94.3486 |          19.1200 |
[32m[20221213 21:09:28 @agent_ppo2.py:185][0m |          -0.0100 |          94.2238 |          19.1168 |
[32m[20221213 21:09:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:09:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 578.00
[32m[20221213 21:09:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 602.00
[32m[20221213 21:09:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 603.00
[32m[20221213 21:09:28 @agent_ppo2.py:143][0m Total time:      13.89 min
[32m[20221213 21:09:28 @agent_ppo2.py:145][0m 1347584 total steps have happened
[32m[20221213 21:09:28 @agent_ppo2.py:121][0m #------------------------ Iteration 658 --------------------------#
[32m[20221213 21:09:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:28 @agent_ppo2.py:185][0m |          -0.0031 |          91.5225 |          19.1562 |
[32m[20221213 21:09:28 @agent_ppo2.py:185][0m |          -0.0063 |          90.8200 |          19.1410 |
[32m[20221213 21:09:28 @agent_ppo2.py:185][0m |          -0.0081 |          90.6001 |          19.1425 |
[32m[20221213 21:09:29 @agent_ppo2.py:185][0m |           0.0044 |          94.7721 |          19.1316 |
[32m[20221213 21:09:29 @agent_ppo2.py:185][0m |           0.0018 |          94.6993 |          19.1239 |
[32m[20221213 21:09:29 @agent_ppo2.py:185][0m |          -0.0025 |          90.4857 |          19.1243 |
[32m[20221213 21:09:29 @agent_ppo2.py:185][0m |           0.0042 |          95.8936 |          19.1137 |
[32m[20221213 21:09:29 @agent_ppo2.py:185][0m |           0.0048 |          99.9076 |          19.1261 |
[32m[20221213 21:09:29 @agent_ppo2.py:185][0m |           0.0013 |          93.0870 |          19.1341 |
[32m[20221213 21:09:29 @agent_ppo2.py:185][0m |          -0.0095 |          89.2484 |          19.1168 |
[32m[20221213 21:09:29 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:09:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 575.20
[32m[20221213 21:09:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 598.00
[32m[20221213 21:09:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 574.00
[32m[20221213 21:09:29 @agent_ppo2.py:143][0m Total time:      13.91 min
[32m[20221213 21:09:29 @agent_ppo2.py:145][0m 1349632 total steps have happened
[32m[20221213 21:09:29 @agent_ppo2.py:121][0m #------------------------ Iteration 659 --------------------------#
[32m[20221213 21:09:29 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:09:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:29 @agent_ppo2.py:185][0m |          -0.0018 |          96.2350 |          19.0931 |
[32m[20221213 21:09:30 @agent_ppo2.py:185][0m |          -0.0047 |          95.3884 |          19.0748 |
[32m[20221213 21:09:30 @agent_ppo2.py:185][0m |          -0.0057 |          94.9419 |          19.0738 |
[32m[20221213 21:09:30 @agent_ppo2.py:185][0m |          -0.0059 |          94.6288 |          19.0630 |
[32m[20221213 21:09:30 @agent_ppo2.py:185][0m |           0.0010 |          96.8930 |          19.0606 |
[32m[20221213 21:09:30 @agent_ppo2.py:185][0m |          -0.0093 |          94.1460 |          19.0578 |
[32m[20221213 21:09:30 @agent_ppo2.py:185][0m |          -0.0077 |          93.9895 |          19.0533 |
[32m[20221213 21:09:30 @agent_ppo2.py:185][0m |           0.0001 |         100.0585 |          19.0528 |
[32m[20221213 21:09:30 @agent_ppo2.py:185][0m |          -0.0082 |          93.5577 |          19.0391 |
[32m[20221213 21:09:30 @agent_ppo2.py:185][0m |           0.0011 |          97.3621 |          19.0384 |
[32m[20221213 21:09:30 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:09:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 563.00
[32m[20221213 21:09:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 584.00
[32m[20221213 21:09:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.00
[32m[20221213 21:09:30 @agent_ppo2.py:143][0m Total time:      13.93 min
[32m[20221213 21:09:30 @agent_ppo2.py:145][0m 1351680 total steps have happened
[32m[20221213 21:09:30 @agent_ppo2.py:121][0m #------------------------ Iteration 660 --------------------------#
[32m[20221213 21:09:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:09:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:31 @agent_ppo2.py:185][0m |          -0.0005 |          98.3457 |          19.1777 |
[32m[20221213 21:09:31 @agent_ppo2.py:185][0m |          -0.0055 |          97.2834 |          19.1744 |
[32m[20221213 21:09:31 @agent_ppo2.py:185][0m |          -0.0066 |          96.7999 |          19.1574 |
[32m[20221213 21:09:31 @agent_ppo2.py:185][0m |          -0.0021 |          99.0441 |          19.1511 |
[32m[20221213 21:09:31 @agent_ppo2.py:185][0m |          -0.0061 |          96.1686 |          19.1510 |
[32m[20221213 21:09:31 @agent_ppo2.py:185][0m |          -0.0061 |          96.0495 |          19.1399 |
[32m[20221213 21:09:31 @agent_ppo2.py:185][0m |          -0.0064 |          95.8088 |          19.1361 |
[32m[20221213 21:09:31 @agent_ppo2.py:185][0m |          -0.0061 |          95.6970 |          19.1339 |
[32m[20221213 21:09:31 @agent_ppo2.py:185][0m |          -0.0036 |          96.6093 |          19.1230 |
[32m[20221213 21:09:32 @agent_ppo2.py:185][0m |          -0.0077 |          95.5844 |          19.1244 |
[32m[20221213 21:09:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:09:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.00
[32m[20221213 21:09:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.00
[32m[20221213 21:09:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 615.00
[32m[20221213 21:09:32 @agent_ppo2.py:143][0m Total time:      13.95 min
[32m[20221213 21:09:32 @agent_ppo2.py:145][0m 1353728 total steps have happened
[32m[20221213 21:09:32 @agent_ppo2.py:121][0m #------------------------ Iteration 661 --------------------------#
[32m[20221213 21:09:32 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:09:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:32 @agent_ppo2.py:185][0m |           0.0061 |          97.9604 |          19.0631 |
[32m[20221213 21:09:32 @agent_ppo2.py:185][0m |          -0.0069 |          94.7217 |          19.0484 |
[32m[20221213 21:09:32 @agent_ppo2.py:185][0m |          -0.0049 |          93.9718 |          19.0474 |
[32m[20221213 21:09:32 @agent_ppo2.py:185][0m |          -0.0014 |          94.3260 |          19.0539 |
[32m[20221213 21:09:32 @agent_ppo2.py:185][0m |          -0.0070 |          93.2995 |          19.0667 |
[32m[20221213 21:09:32 @agent_ppo2.py:185][0m |          -0.0078 |          93.1939 |          19.0703 |
[32m[20221213 21:09:33 @agent_ppo2.py:185][0m |          -0.0084 |          92.9782 |          19.0718 |
[32m[20221213 21:09:33 @agent_ppo2.py:185][0m |          -0.0085 |          92.7913 |          19.0675 |
[32m[20221213 21:09:33 @agent_ppo2.py:185][0m |          -0.0096 |          92.6526 |          19.0866 |
[32m[20221213 21:09:33 @agent_ppo2.py:185][0m |          -0.0090 |          92.5363 |          19.0826 |
[32m[20221213 21:09:33 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:09:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 567.40
[32m[20221213 21:09:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 21:09:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 553.00
[32m[20221213 21:09:33 @agent_ppo2.py:143][0m Total time:      13.97 min
[32m[20221213 21:09:33 @agent_ppo2.py:145][0m 1355776 total steps have happened
[32m[20221213 21:09:33 @agent_ppo2.py:121][0m #------------------------ Iteration 662 --------------------------#
[32m[20221213 21:09:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:33 @agent_ppo2.py:185][0m |           0.0017 |          96.7291 |          19.0771 |
[32m[20221213 21:09:33 @agent_ppo2.py:185][0m |          -0.0040 |          94.2113 |          19.0574 |
[32m[20221213 21:09:33 @agent_ppo2.py:185][0m |          -0.0028 |          92.9528 |          19.0518 |
[32m[20221213 21:09:34 @agent_ppo2.py:185][0m |          -0.0044 |          92.0844 |          19.0374 |
[32m[20221213 21:09:34 @agent_ppo2.py:185][0m |          -0.0067 |          91.2030 |          19.0353 |
[32m[20221213 21:09:34 @agent_ppo2.py:185][0m |          -0.0026 |          91.3286 |          19.0243 |
[32m[20221213 21:09:34 @agent_ppo2.py:185][0m |          -0.0066 |          90.0294 |          19.0240 |
[32m[20221213 21:09:34 @agent_ppo2.py:185][0m |          -0.0059 |          89.1751 |          19.0143 |
[32m[20221213 21:09:34 @agent_ppo2.py:185][0m |          -0.0076 |          88.6607 |          19.0064 |
[32m[20221213 21:09:34 @agent_ppo2.py:185][0m |          -0.0058 |          88.0006 |          18.9988 |
[32m[20221213 21:09:34 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:09:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 574.60
[32m[20221213 21:09:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 592.00
[32m[20221213 21:09:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.00
[32m[20221213 21:09:34 @agent_ppo2.py:143][0m Total time:      13.99 min
[32m[20221213 21:09:34 @agent_ppo2.py:145][0m 1357824 total steps have happened
[32m[20221213 21:09:34 @agent_ppo2.py:121][0m #------------------------ Iteration 663 --------------------------#
[32m[20221213 21:09:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:09:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:35 @agent_ppo2.py:185][0m |           0.0099 |         107.0988 |          19.1739 |
[32m[20221213 21:09:35 @agent_ppo2.py:185][0m |          -0.0052 |          98.6087 |          19.1471 |
[32m[20221213 21:09:35 @agent_ppo2.py:185][0m |          -0.0050 |          97.8516 |          19.1390 |
[32m[20221213 21:09:35 @agent_ppo2.py:185][0m |          -0.0094 |          97.4494 |          19.1350 |
[32m[20221213 21:09:35 @agent_ppo2.py:185][0m |          -0.0053 |          96.9484 |          19.1332 |
[32m[20221213 21:09:35 @agent_ppo2.py:185][0m |          -0.0058 |          96.8335 |          19.1231 |
[32m[20221213 21:09:35 @agent_ppo2.py:185][0m |           0.0059 |         103.9856 |          19.1266 |
[32m[20221213 21:09:35 @agent_ppo2.py:185][0m |          -0.0087 |          96.0912 |          19.1232 |
[32m[20221213 21:09:35 @agent_ppo2.py:185][0m |          -0.0081 |          96.0558 |          19.1157 |
[32m[20221213 21:09:35 @agent_ppo2.py:185][0m |          -0.0064 |          95.9272 |          19.1156 |
[32m[20221213 21:09:35 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:09:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 560.40
[32m[20221213 21:09:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 576.00
[32m[20221213 21:09:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.00
[32m[20221213 21:09:35 @agent_ppo2.py:143][0m Total time:      14.01 min
[32m[20221213 21:09:35 @agent_ppo2.py:145][0m 1359872 total steps have happened
[32m[20221213 21:09:35 @agent_ppo2.py:121][0m #------------------------ Iteration 664 --------------------------#
[32m[20221213 21:09:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:36 @agent_ppo2.py:185][0m |           0.0052 |         102.0110 |          19.1531 |
[32m[20221213 21:09:36 @agent_ppo2.py:185][0m |          -0.0054 |          96.8531 |          19.1455 |
[32m[20221213 21:09:36 @agent_ppo2.py:185][0m |          -0.0027 |          96.0513 |          19.1394 |
[32m[20221213 21:09:36 @agent_ppo2.py:185][0m |          -0.0071 |          95.5628 |          19.1362 |
[32m[20221213 21:09:36 @agent_ppo2.py:185][0m |          -0.0056 |          94.7524 |          19.1436 |
[32m[20221213 21:09:36 @agent_ppo2.py:185][0m |           0.0050 |          99.1458 |          19.1409 |
[32m[20221213 21:09:36 @agent_ppo2.py:185][0m |          -0.0065 |          94.0229 |          19.1264 |
[32m[20221213 21:09:36 @agent_ppo2.py:185][0m |           0.0027 |          99.7806 |          19.1445 |
[32m[20221213 21:09:36 @agent_ppo2.py:185][0m |           0.0076 |          98.5660 |          19.1414 |
[32m[20221213 21:09:37 @agent_ppo2.py:185][0m |          -0.0041 |          93.4769 |          19.1455 |
[32m[20221213 21:09:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:09:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 589.40
[32m[20221213 21:09:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.00
[32m[20221213 21:09:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:09:37 @agent_ppo2.py:143][0m Total time:      14.03 min
[32m[20221213 21:09:37 @agent_ppo2.py:145][0m 1361920 total steps have happened
[32m[20221213 21:09:37 @agent_ppo2.py:121][0m #------------------------ Iteration 665 --------------------------#
[32m[20221213 21:09:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:37 @agent_ppo2.py:185][0m |          -0.0010 |          97.0219 |          19.0310 |
[32m[20221213 21:09:37 @agent_ppo2.py:185][0m |          -0.0057 |          95.1824 |          19.0059 |
[32m[20221213 21:09:37 @agent_ppo2.py:185][0m |           0.0020 |          98.5183 |          19.0041 |
[32m[20221213 21:09:37 @agent_ppo2.py:185][0m |           0.0034 |         101.8695 |          18.9700 |
[32m[20221213 21:09:37 @agent_ppo2.py:185][0m |          -0.0061 |          93.2817 |          18.9734 |
[32m[20221213 21:09:37 @agent_ppo2.py:185][0m |           0.0005 |          98.2620 |          18.9778 |
[32m[20221213 21:09:37 @agent_ppo2.py:185][0m |          -0.0107 |          92.7648 |          18.9589 |
[32m[20221213 21:09:38 @agent_ppo2.py:185][0m |          -0.0109 |          92.5056 |          18.9651 |
[32m[20221213 21:09:38 @agent_ppo2.py:185][0m |          -0.0092 |          92.2925 |          18.9559 |
[32m[20221213 21:09:38 @agent_ppo2.py:185][0m |          -0.0053 |          93.3202 |          18.9552 |
[32m[20221213 21:09:38 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:09:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 549.20
[32m[20221213 21:09:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 574.00
[32m[20221213 21:09:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 570.00
[32m[20221213 21:09:38 @agent_ppo2.py:143][0m Total time:      14.05 min
[32m[20221213 21:09:38 @agent_ppo2.py:145][0m 1363968 total steps have happened
[32m[20221213 21:09:38 @agent_ppo2.py:121][0m #------------------------ Iteration 666 --------------------------#
[32m[20221213 21:09:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:38 @agent_ppo2.py:185][0m |           0.0096 |         100.0365 |          19.1960 |
[32m[20221213 21:09:38 @agent_ppo2.py:185][0m |           0.0093 |         101.2507 |          19.1701 |
[32m[20221213 21:09:38 @agent_ppo2.py:185][0m |           0.0039 |          95.7423 |          19.1606 |
[32m[20221213 21:09:38 @agent_ppo2.py:185][0m |          -0.0045 |          92.4391 |          19.1599 |
[32m[20221213 21:09:38 @agent_ppo2.py:185][0m |          -0.0056 |          92.2470 |          19.1637 |
[32m[20221213 21:09:39 @agent_ppo2.py:185][0m |          -0.0065 |          92.0452 |          19.1540 |
[32m[20221213 21:09:39 @agent_ppo2.py:185][0m |          -0.0079 |          91.8062 |          19.1762 |
[32m[20221213 21:09:39 @agent_ppo2.py:185][0m |          -0.0059 |          91.5088 |          19.1676 |
[32m[20221213 21:09:39 @agent_ppo2.py:185][0m |          -0.0046 |          91.4275 |          19.1659 |
[32m[20221213 21:09:39 @agent_ppo2.py:185][0m |          -0.0011 |          92.3951 |          19.1627 |
[32m[20221213 21:09:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:09:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.00
[32m[20221213 21:09:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 603.00
[32m[20221213 21:09:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 579.00
[32m[20221213 21:09:39 @agent_ppo2.py:143][0m Total time:      14.07 min
[32m[20221213 21:09:39 @agent_ppo2.py:145][0m 1366016 total steps have happened
[32m[20221213 21:09:39 @agent_ppo2.py:121][0m #------------------------ Iteration 667 --------------------------#
[32m[20221213 21:09:39 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:09:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:39 @agent_ppo2.py:185][0m |          -0.0002 |          97.0249 |          19.1569 |
[32m[20221213 21:09:39 @agent_ppo2.py:185][0m |          -0.0037 |          95.7855 |          19.1668 |
[32m[20221213 21:09:39 @agent_ppo2.py:185][0m |          -0.0023 |          95.3892 |          19.1633 |
[32m[20221213 21:09:40 @agent_ppo2.py:185][0m |          -0.0041 |          95.3686 |          19.1725 |
[32m[20221213 21:09:40 @agent_ppo2.py:185][0m |           0.0072 |         103.3853 |          19.1702 |
[32m[20221213 21:09:40 @agent_ppo2.py:185][0m |          -0.0047 |          94.0733 |          19.1761 |
[32m[20221213 21:09:40 @agent_ppo2.py:185][0m |          -0.0048 |          93.8398 |          19.1852 |
[32m[20221213 21:09:40 @agent_ppo2.py:185][0m |           0.0140 |         108.3031 |          19.1846 |
[32m[20221213 21:09:40 @agent_ppo2.py:185][0m |          -0.0081 |          93.7539 |          19.1939 |
[32m[20221213 21:09:40 @agent_ppo2.py:185][0m |          -0.0035 |          93.4752 |          19.2021 |
[32m[20221213 21:09:40 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:09:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 582.20
[32m[20221213 21:09:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 633.00
[32m[20221213 21:09:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 547.00
[32m[20221213 21:09:40 @agent_ppo2.py:143][0m Total time:      14.09 min
[32m[20221213 21:09:40 @agent_ppo2.py:145][0m 1368064 total steps have happened
[32m[20221213 21:09:40 @agent_ppo2.py:121][0m #------------------------ Iteration 668 --------------------------#
[32m[20221213 21:09:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:41 @agent_ppo2.py:185][0m |           0.0001 |          98.1157 |          19.1467 |
[32m[20221213 21:09:41 @agent_ppo2.py:185][0m |          -0.0044 |          97.4911 |          19.1211 |
[32m[20221213 21:09:41 @agent_ppo2.py:185][0m |          -0.0069 |          96.8918 |          19.1144 |
[32m[20221213 21:09:41 @agent_ppo2.py:185][0m |          -0.0054 |          96.8525 |          19.1161 |
[32m[20221213 21:09:41 @agent_ppo2.py:185][0m |          -0.0031 |          96.9502 |          19.1219 |
[32m[20221213 21:09:41 @agent_ppo2.py:185][0m |          -0.0080 |          96.1500 |          19.1138 |
[32m[20221213 21:09:41 @agent_ppo2.py:185][0m |          -0.0086 |          95.9071 |          19.1133 |
[32m[20221213 21:09:41 @agent_ppo2.py:185][0m |          -0.0046 |          96.6240 |          19.1040 |
[32m[20221213 21:09:41 @agent_ppo2.py:185][0m |          -0.0063 |          95.6296 |          19.1029 |
[32m[20221213 21:09:41 @agent_ppo2.py:185][0m |          -0.0073 |          95.3639 |          19.1052 |
[32m[20221213 21:09:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:09:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 562.60
[32m[20221213 21:09:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 590.00
[32m[20221213 21:09:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.00
[32m[20221213 21:09:41 @agent_ppo2.py:143][0m Total time:      14.11 min
[32m[20221213 21:09:41 @agent_ppo2.py:145][0m 1370112 total steps have happened
[32m[20221213 21:09:41 @agent_ppo2.py:121][0m #------------------------ Iteration 669 --------------------------#
[32m[20221213 21:09:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:42 @agent_ppo2.py:185][0m |          -0.0032 |         102.1830 |          19.1397 |
[32m[20221213 21:09:42 @agent_ppo2.py:185][0m |          -0.0006 |         101.0615 |          19.1385 |
[32m[20221213 21:09:42 @agent_ppo2.py:185][0m |          -0.0045 |         100.8900 |          19.1292 |
[32m[20221213 21:09:42 @agent_ppo2.py:185][0m |           0.0070 |         106.5822 |          19.1459 |
[32m[20221213 21:09:42 @agent_ppo2.py:185][0m |           0.0072 |         110.4375 |          19.1486 |
[32m[20221213 21:09:42 @agent_ppo2.py:185][0m |          -0.0081 |          99.6129 |          19.1356 |
[32m[20221213 21:09:42 @agent_ppo2.py:185][0m |          -0.0084 |          99.2435 |          19.1430 |
[32m[20221213 21:09:42 @agent_ppo2.py:185][0m |          -0.0070 |          99.0125 |          19.1476 |
[32m[20221213 21:09:42 @agent_ppo2.py:185][0m |          -0.0028 |          99.5954 |          19.1446 |
[32m[20221213 21:09:42 @agent_ppo2.py:185][0m |          -0.0057 |          99.0426 |          19.1517 |
[32m[20221213 21:09:42 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:09:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 569.00
[32m[20221213 21:09:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 603.00
[32m[20221213 21:09:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.00
[32m[20221213 21:09:43 @agent_ppo2.py:143][0m Total time:      14.13 min
[32m[20221213 21:09:43 @agent_ppo2.py:145][0m 1372160 total steps have happened
[32m[20221213 21:09:43 @agent_ppo2.py:121][0m #------------------------ Iteration 670 --------------------------#
[32m[20221213 21:09:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:43 @agent_ppo2.py:185][0m |          -0.0017 |          95.4377 |          19.1640 |
[32m[20221213 21:09:43 @agent_ppo2.py:185][0m |           0.0002 |          96.0377 |          19.1491 |
[32m[20221213 21:09:43 @agent_ppo2.py:185][0m |          -0.0073 |          93.5437 |          19.1495 |
[32m[20221213 21:09:43 @agent_ppo2.py:185][0m |          -0.0053 |          93.1410 |          19.1418 |
[32m[20221213 21:09:43 @agent_ppo2.py:185][0m |          -0.0067 |          93.0739 |          19.1506 |
[32m[20221213 21:09:43 @agent_ppo2.py:185][0m |          -0.0070 |          92.5015 |          19.1437 |
[32m[20221213 21:09:43 @agent_ppo2.py:185][0m |          -0.0057 |          93.0921 |          19.1470 |
[32m[20221213 21:09:44 @agent_ppo2.py:185][0m |          -0.0076 |          92.0589 |          19.1436 |
[32m[20221213 21:09:44 @agent_ppo2.py:185][0m |          -0.0072 |          91.9782 |          19.1435 |
[32m[20221213 21:09:44 @agent_ppo2.py:185][0m |          -0.0021 |          93.6050 |          19.1529 |
[32m[20221213 21:09:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:09:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 589.40
[32m[20221213 21:09:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 617.00
[32m[20221213 21:09:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 607.00
[32m[20221213 21:09:44 @agent_ppo2.py:143][0m Total time:      14.15 min
[32m[20221213 21:09:44 @agent_ppo2.py:145][0m 1374208 total steps have happened
[32m[20221213 21:09:44 @agent_ppo2.py:121][0m #------------------------ Iteration 671 --------------------------#
[32m[20221213 21:09:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:44 @agent_ppo2.py:185][0m |          -0.0012 |          93.3889 |          19.1261 |
[32m[20221213 21:09:44 @agent_ppo2.py:185][0m |          -0.0025 |          93.2038 |          19.1014 |
[32m[20221213 21:09:44 @agent_ppo2.py:185][0m |          -0.0051 |          92.5150 |          19.1002 |
[32m[20221213 21:09:44 @agent_ppo2.py:185][0m |          -0.0076 |          92.3641 |          19.0845 |
[32m[20221213 21:09:44 @agent_ppo2.py:185][0m |           0.0033 |          96.3043 |          19.0821 |
[32m[20221213 21:09:45 @agent_ppo2.py:185][0m |          -0.0014 |          93.6818 |          19.0703 |
[32m[20221213 21:09:45 @agent_ppo2.py:185][0m |          -0.0076 |          91.9453 |          19.0623 |
[32m[20221213 21:09:45 @agent_ppo2.py:185][0m |          -0.0056 |          91.7557 |          19.0534 |
[32m[20221213 21:09:45 @agent_ppo2.py:185][0m |           0.0037 |          96.4812 |          19.0619 |
[32m[20221213 21:09:45 @agent_ppo2.py:185][0m |          -0.0105 |          91.8990 |          19.0533 |
[32m[20221213 21:09:45 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:09:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 612.60
[32m[20221213 21:09:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 640.00
[32m[20221213 21:09:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 565.00
[32m[20221213 21:09:45 @agent_ppo2.py:143][0m Total time:      14.17 min
[32m[20221213 21:09:45 @agent_ppo2.py:145][0m 1376256 total steps have happened
[32m[20221213 21:09:45 @agent_ppo2.py:121][0m #------------------------ Iteration 672 --------------------------#
[32m[20221213 21:09:45 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:09:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:45 @agent_ppo2.py:185][0m |          -0.0001 |          90.1631 |          19.1166 |
[32m[20221213 21:09:45 @agent_ppo2.py:185][0m |          -0.0031 |          89.1888 |          19.1041 |
[32m[20221213 21:09:45 @agent_ppo2.py:185][0m |          -0.0057 |          88.7281 |          19.0963 |
[32m[20221213 21:09:46 @agent_ppo2.py:185][0m |          -0.0062 |          88.4276 |          19.0903 |
[32m[20221213 21:09:46 @agent_ppo2.py:185][0m |          -0.0082 |          88.1939 |          19.0877 |
[32m[20221213 21:09:46 @agent_ppo2.py:185][0m |           0.0016 |          92.9921 |          19.0831 |
[32m[20221213 21:09:46 @agent_ppo2.py:185][0m |          -0.0074 |          87.7967 |          19.0829 |
[32m[20221213 21:09:46 @agent_ppo2.py:185][0m |          -0.0024 |          90.1688 |          19.0784 |
[32m[20221213 21:09:46 @agent_ppo2.py:185][0m |           0.0020 |          89.5365 |          19.0720 |
[32m[20221213 21:09:46 @agent_ppo2.py:185][0m |          -0.0091 |          87.4184 |          19.0678 |
[32m[20221213 21:09:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:09:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 518.60
[32m[20221213 21:09:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 577.00
[32m[20221213 21:09:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 582.00
[32m[20221213 21:09:46 @agent_ppo2.py:143][0m Total time:      14.19 min
[32m[20221213 21:09:46 @agent_ppo2.py:145][0m 1378304 total steps have happened
[32m[20221213 21:09:46 @agent_ppo2.py:121][0m #------------------------ Iteration 673 --------------------------#
[32m[20221213 21:09:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:09:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:47 @agent_ppo2.py:185][0m |          -0.0017 |          92.6116 |          19.1785 |
[32m[20221213 21:09:47 @agent_ppo2.py:185][0m |          -0.0035 |          91.8557 |          19.1729 |
[32m[20221213 21:09:47 @agent_ppo2.py:185][0m |          -0.0029 |          91.5192 |          19.1618 |
[32m[20221213 21:09:47 @agent_ppo2.py:185][0m |           0.0006 |          93.1502 |          19.1561 |
[32m[20221213 21:09:47 @agent_ppo2.py:185][0m |           0.0049 |          94.5842 |          19.1434 |
[32m[20221213 21:09:47 @agent_ppo2.py:185][0m |           0.0048 |          98.4596 |          19.1457 |
[32m[20221213 21:09:47 @agent_ppo2.py:185][0m |          -0.0023 |          91.7478 |          19.1498 |
[32m[20221213 21:09:47 @agent_ppo2.py:185][0m |          -0.0085 |          90.5957 |          19.1395 |
[32m[20221213 21:09:47 @agent_ppo2.py:185][0m |          -0.0019 |          92.5652 |          19.1462 |
[32m[20221213 21:09:47 @agent_ppo2.py:185][0m |          -0.0101 |          90.4129 |          19.1363 |
[32m[20221213 21:09:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:09:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.80
[32m[20221213 21:09:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 600.00
[32m[20221213 21:09:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 571.00
[32m[20221213 21:09:47 @agent_ppo2.py:143][0m Total time:      14.21 min
[32m[20221213 21:09:47 @agent_ppo2.py:145][0m 1380352 total steps have happened
[32m[20221213 21:09:47 @agent_ppo2.py:121][0m #------------------------ Iteration 674 --------------------------#
[32m[20221213 21:09:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:48 @agent_ppo2.py:185][0m |           0.0215 |         118.8710 |          19.0407 |
[32m[20221213 21:09:48 @agent_ppo2.py:185][0m |          -0.0035 |          93.2072 |          19.0126 |
[32m[20221213 21:09:48 @agent_ppo2.py:185][0m |           0.0052 |          99.4164 |          19.0247 |
[32m[20221213 21:09:48 @agent_ppo2.py:185][0m |          -0.0064 |          92.3797 |          19.0196 |
[32m[20221213 21:09:48 @agent_ppo2.py:185][0m |          -0.0067 |          91.9524 |          19.0173 |
[32m[20221213 21:09:48 @agent_ppo2.py:185][0m |          -0.0069 |          91.9743 |          19.0142 |
[32m[20221213 21:09:48 @agent_ppo2.py:185][0m |          -0.0060 |          91.8916 |          19.0089 |
[32m[20221213 21:09:48 @agent_ppo2.py:185][0m |          -0.0035 |          91.5652 |          19.0058 |
[32m[20221213 21:09:48 @agent_ppo2.py:185][0m |           0.0033 |          98.3485 |          19.0054 |
[32m[20221213 21:09:48 @agent_ppo2.py:185][0m |          -0.0057 |          91.5013 |          18.9886 |
[32m[20221213 21:09:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:09:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 572.20
[32m[20221213 21:09:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 592.00
[32m[20221213 21:09:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 536.00
[32m[20221213 21:09:49 @agent_ppo2.py:143][0m Total time:      14.23 min
[32m[20221213 21:09:49 @agent_ppo2.py:145][0m 1382400 total steps have happened
[32m[20221213 21:09:49 @agent_ppo2.py:121][0m #------------------------ Iteration 675 --------------------------#
[32m[20221213 21:09:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:49 @agent_ppo2.py:185][0m |           0.0036 |          96.9878 |          19.0762 |
[32m[20221213 21:09:49 @agent_ppo2.py:185][0m |           0.0065 |         100.0848 |          19.0693 |
[32m[20221213 21:09:49 @agent_ppo2.py:185][0m |          -0.0042 |          94.8265 |          19.0511 |
[32m[20221213 21:09:49 @agent_ppo2.py:185][0m |          -0.0017 |          94.3127 |          19.0545 |
[32m[20221213 21:09:49 @agent_ppo2.py:185][0m |          -0.0016 |          94.6606 |          19.0557 |
[32m[20221213 21:09:49 @agent_ppo2.py:185][0m |           0.0079 |         101.5026 |          19.0582 |
[32m[20221213 21:09:49 @agent_ppo2.py:185][0m |          -0.0038 |          93.5061 |          19.0458 |
[32m[20221213 21:09:49 @agent_ppo2.py:185][0m |          -0.0076 |          93.2423 |          19.0500 |
[32m[20221213 21:09:50 @agent_ppo2.py:185][0m |          -0.0082 |          93.0698 |          19.0451 |
[32m[20221213 21:09:50 @agent_ppo2.py:185][0m |          -0.0063 |          92.9409 |          19.0378 |
[32m[20221213 21:09:50 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:09:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 609.60
[32m[20221213 21:09:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 639.00
[32m[20221213 21:09:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:09:50 @agent_ppo2.py:143][0m Total time:      14.25 min
[32m[20221213 21:09:50 @agent_ppo2.py:145][0m 1384448 total steps have happened
[32m[20221213 21:09:50 @agent_ppo2.py:121][0m #------------------------ Iteration 676 --------------------------#
[32m[20221213 21:09:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:09:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:50 @agent_ppo2.py:185][0m |          -0.0005 |          96.1928 |          19.0699 |
[32m[20221213 21:09:50 @agent_ppo2.py:185][0m |          -0.0070 |          95.5489 |          19.0607 |
[32m[20221213 21:09:50 @agent_ppo2.py:185][0m |          -0.0058 |          95.3516 |          19.0427 |
[32m[20221213 21:09:50 @agent_ppo2.py:185][0m |          -0.0041 |          95.7685 |          19.0538 |
[32m[20221213 21:09:51 @agent_ppo2.py:185][0m |          -0.0063 |          95.1126 |          19.0446 |
[32m[20221213 21:09:51 @agent_ppo2.py:185][0m |          -0.0084 |          94.9040 |          19.0439 |
[32m[20221213 21:09:51 @agent_ppo2.py:185][0m |          -0.0091 |          94.8079 |          19.0509 |
[32m[20221213 21:09:51 @agent_ppo2.py:185][0m |          -0.0091 |          94.6294 |          19.0425 |
[32m[20221213 21:09:51 @agent_ppo2.py:185][0m |          -0.0069 |          94.4779 |          19.0415 |
[32m[20221213 21:09:51 @agent_ppo2.py:185][0m |          -0.0104 |          94.5250 |          19.0417 |
[32m[20221213 21:09:51 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:09:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 579.20
[32m[20221213 21:09:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 610.00
[32m[20221213 21:09:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.00
[32m[20221213 21:09:51 @agent_ppo2.py:143][0m Total time:      14.27 min
[32m[20221213 21:09:51 @agent_ppo2.py:145][0m 1386496 total steps have happened
[32m[20221213 21:09:51 @agent_ppo2.py:121][0m #------------------------ Iteration 677 --------------------------#
[32m[20221213 21:09:52 @agent_ppo2.py:127][0m Sampling time: 0.34 s by 5 slaves
[32m[20221213 21:09:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:52 @agent_ppo2.py:185][0m |          -0.0027 |          96.3971 |          19.0264 |
[32m[20221213 21:09:52 @agent_ppo2.py:185][0m |          -0.0005 |          95.6680 |          19.0321 |
[32m[20221213 21:09:52 @agent_ppo2.py:185][0m |          -0.0063 |          94.7603 |          19.0179 |
[32m[20221213 21:09:52 @agent_ppo2.py:185][0m |          -0.0066 |          94.0431 |          19.0179 |
[32m[20221213 21:09:52 @agent_ppo2.py:185][0m |          -0.0075 |          93.6572 |          19.0156 |
[32m[20221213 21:09:52 @agent_ppo2.py:185][0m |          -0.0080 |          93.5147 |          19.0275 |
[32m[20221213 21:09:52 @agent_ppo2.py:185][0m |          -0.0059 |          93.6094 |          19.0203 |
[32m[20221213 21:09:52 @agent_ppo2.py:185][0m |          -0.0055 |          93.5122 |          19.0208 |
[32m[20221213 21:09:53 @agent_ppo2.py:185][0m |          -0.0083 |          93.0757 |          19.0199 |
[32m[20221213 21:09:53 @agent_ppo2.py:185][0m |          -0.0043 |          94.9099 |          19.0247 |
[32m[20221213 21:09:53 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 21:09:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 544.60
[32m[20221213 21:09:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.00
[32m[20221213 21:09:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 579.00
[32m[20221213 21:09:53 @agent_ppo2.py:143][0m Total time:      14.30 min
[32m[20221213 21:09:53 @agent_ppo2.py:145][0m 1388544 total steps have happened
[32m[20221213 21:09:53 @agent_ppo2.py:121][0m #------------------------ Iteration 678 --------------------------#
[32m[20221213 21:09:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:53 @agent_ppo2.py:185][0m |          -0.0023 |          92.3554 |          18.9175 |
[32m[20221213 21:09:53 @agent_ppo2.py:185][0m |          -0.0033 |          91.3853 |          18.9045 |
[32m[20221213 21:09:53 @agent_ppo2.py:185][0m |          -0.0055 |          90.8313 |          18.9075 |
[32m[20221213 21:09:53 @agent_ppo2.py:185][0m |          -0.0074 |          90.7291 |          18.9059 |
[32m[20221213 21:09:54 @agent_ppo2.py:185][0m |          -0.0051 |          90.2750 |          18.8998 |
[32m[20221213 21:09:54 @agent_ppo2.py:185][0m |          -0.0061 |          90.0380 |          18.9054 |
[32m[20221213 21:09:54 @agent_ppo2.py:185][0m |          -0.0036 |          89.9933 |          18.9031 |
[32m[20221213 21:09:54 @agent_ppo2.py:185][0m |          -0.0032 |          89.7286 |          18.9031 |
[32m[20221213 21:09:54 @agent_ppo2.py:185][0m |          -0.0013 |          90.8895 |          18.9089 |
[32m[20221213 21:09:54 @agent_ppo2.py:185][0m |          -0.0061 |          89.5932 |          18.9064 |
[32m[20221213 21:09:54 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 21:09:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 578.00
[32m[20221213 21:09:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 621.00
[32m[20221213 21:09:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.00
[32m[20221213 21:09:54 @agent_ppo2.py:143][0m Total time:      14.32 min
[32m[20221213 21:09:54 @agent_ppo2.py:145][0m 1390592 total steps have happened
[32m[20221213 21:09:54 @agent_ppo2.py:121][0m #------------------------ Iteration 679 --------------------------#
[32m[20221213 21:09:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:54 @agent_ppo2.py:185][0m |          -0.0000 |          93.5448 |          19.0322 |
[32m[20221213 21:09:54 @agent_ppo2.py:185][0m |           0.0008 |          93.3809 |          19.0080 |
[32m[20221213 21:09:55 @agent_ppo2.py:185][0m |           0.0037 |          95.1062 |          19.0002 |
[32m[20221213 21:09:55 @agent_ppo2.py:185][0m |          -0.0067 |          91.6831 |          18.9944 |
[32m[20221213 21:09:55 @agent_ppo2.py:185][0m |          -0.0111 |          91.4975 |          18.9935 |
[32m[20221213 21:09:55 @agent_ppo2.py:185][0m |          -0.0072 |          91.3849 |          18.9927 |
[32m[20221213 21:09:55 @agent_ppo2.py:185][0m |          -0.0090 |          91.1018 |          18.9926 |
[32m[20221213 21:09:55 @agent_ppo2.py:185][0m |           0.0029 |         101.9235 |          18.9854 |
[32m[20221213 21:09:55 @agent_ppo2.py:185][0m |          -0.0004 |          92.9522 |          18.9887 |
[32m[20221213 21:09:55 @agent_ppo2.py:185][0m |           0.0010 |          97.2532 |          18.9926 |
[32m[20221213 21:09:55 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:09:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 585.20
[32m[20221213 21:09:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 613.00
[32m[20221213 21:09:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 589.00
[32m[20221213 21:09:55 @agent_ppo2.py:143][0m Total time:      14.34 min
[32m[20221213 21:09:55 @agent_ppo2.py:145][0m 1392640 total steps have happened
[32m[20221213 21:09:55 @agent_ppo2.py:121][0m #------------------------ Iteration 680 --------------------------#
[32m[20221213 21:09:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:56 @agent_ppo2.py:185][0m |          -0.0009 |          94.4882 |          19.0364 |
[32m[20221213 21:09:56 @agent_ppo2.py:185][0m |          -0.0019 |          93.7718 |          19.0324 |
[32m[20221213 21:09:56 @agent_ppo2.py:185][0m |          -0.0071 |          93.2901 |          19.0232 |
[32m[20221213 21:09:56 @agent_ppo2.py:185][0m |          -0.0059 |          92.8967 |          19.0285 |
[32m[20221213 21:09:56 @agent_ppo2.py:185][0m |          -0.0091 |          92.6749 |          19.0195 |
[32m[20221213 21:09:56 @agent_ppo2.py:185][0m |          -0.0075 |          92.4701 |          19.0255 |
[32m[20221213 21:09:56 @agent_ppo2.py:185][0m |          -0.0058 |          92.2288 |          19.0237 |
[32m[20221213 21:09:56 @agent_ppo2.py:185][0m |          -0.0049 |          92.1660 |          19.0235 |
[32m[20221213 21:09:56 @agent_ppo2.py:185][0m |          -0.0082 |          91.8020 |          19.0220 |
[32m[20221213 21:09:56 @agent_ppo2.py:185][0m |          -0.0082 |          91.7622 |          19.0266 |
[32m[20221213 21:09:56 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:09:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 545.20
[32m[20221213 21:09:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 565.00
[32m[20221213 21:09:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 557.00
[32m[20221213 21:09:57 @agent_ppo2.py:143][0m Total time:      14.37 min
[32m[20221213 21:09:57 @agent_ppo2.py:145][0m 1394688 total steps have happened
[32m[20221213 21:09:57 @agent_ppo2.py:121][0m #------------------------ Iteration 681 --------------------------#
[32m[20221213 21:09:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:57 @agent_ppo2.py:185][0m |          -0.0017 |          91.4910 |          19.0978 |
[32m[20221213 21:09:57 @agent_ppo2.py:185][0m |          -0.0061 |          90.5513 |          19.0807 |
[32m[20221213 21:09:57 @agent_ppo2.py:185][0m |          -0.0057 |          90.1504 |          19.0863 |
[32m[20221213 21:09:57 @agent_ppo2.py:185][0m |          -0.0060 |          89.8764 |          19.0926 |
[32m[20221213 21:09:57 @agent_ppo2.py:185][0m |          -0.0086 |          89.6338 |          19.0821 |
[32m[20221213 21:09:57 @agent_ppo2.py:185][0m |          -0.0063 |          89.5087 |          19.0887 |
[32m[20221213 21:09:57 @agent_ppo2.py:185][0m |           0.0037 |         102.1574 |          19.0867 |
[32m[20221213 21:09:58 @agent_ppo2.py:185][0m |          -0.0078 |          89.1167 |          19.0908 |
[32m[20221213 21:09:58 @agent_ppo2.py:185][0m |          -0.0102 |          88.8973 |          19.0919 |
[32m[20221213 21:09:58 @agent_ppo2.py:185][0m |          -0.0059 |          89.2587 |          19.0929 |
[32m[20221213 21:09:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:09:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 558.20
[32m[20221213 21:09:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 602.00
[32m[20221213 21:09:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 644.00
[32m[20221213 21:09:58 @agent_ppo2.py:143][0m Total time:      14.39 min
[32m[20221213 21:09:58 @agent_ppo2.py:145][0m 1396736 total steps have happened
[32m[20221213 21:09:58 @agent_ppo2.py:121][0m #------------------------ Iteration 682 --------------------------#
[32m[20221213 21:09:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:58 @agent_ppo2.py:185][0m |          -0.0005 |          91.6717 |          19.1672 |
[32m[20221213 21:09:58 @agent_ppo2.py:185][0m |          -0.0058 |          90.8921 |          19.1574 |
[32m[20221213 21:09:58 @agent_ppo2.py:185][0m |          -0.0046 |          90.3907 |          19.1501 |
[32m[20221213 21:09:58 @agent_ppo2.py:185][0m |          -0.0027 |          90.3163 |          19.1731 |
[32m[20221213 21:09:58 @agent_ppo2.py:185][0m |           0.0007 |          90.8011 |          19.1610 |
[32m[20221213 21:09:59 @agent_ppo2.py:185][0m |           0.0122 |         101.1888 |          19.1661 |
[32m[20221213 21:09:59 @agent_ppo2.py:185][0m |           0.0028 |          94.3786 |          19.1762 |
[32m[20221213 21:09:59 @agent_ppo2.py:185][0m |          -0.0094 |          89.6331 |          19.1688 |
[32m[20221213 21:09:59 @agent_ppo2.py:185][0m |          -0.0065 |          89.4281 |          19.1742 |
[32m[20221213 21:09:59 @agent_ppo2.py:185][0m |          -0.0050 |          90.2635 |          19.1756 |
[32m[20221213 21:09:59 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:09:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.80
[32m[20221213 21:09:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 598.00
[32m[20221213 21:09:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.00
[32m[20221213 21:09:59 @agent_ppo2.py:143][0m Total time:      14.41 min
[32m[20221213 21:09:59 @agent_ppo2.py:145][0m 1398784 total steps have happened
[32m[20221213 21:09:59 @agent_ppo2.py:121][0m #------------------------ Iteration 683 --------------------------#
[32m[20221213 21:09:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:09:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:09:59 @agent_ppo2.py:185][0m |          -0.0028 |          97.1293 |          19.0371 |
[32m[20221213 21:09:59 @agent_ppo2.py:185][0m |          -0.0013 |          97.8731 |          19.0277 |
[32m[20221213 21:10:00 @agent_ppo2.py:185][0m |          -0.0079 |          95.6431 |          19.0180 |
[32m[20221213 21:10:00 @agent_ppo2.py:185][0m |          -0.0062 |          95.2254 |          19.0216 |
[32m[20221213 21:10:00 @agent_ppo2.py:185][0m |          -0.0107 |          94.8907 |          19.0193 |
[32m[20221213 21:10:00 @agent_ppo2.py:185][0m |          -0.0032 |          98.0092 |          19.0092 |
[32m[20221213 21:10:00 @agent_ppo2.py:185][0m |          -0.0103 |          94.1873 |          19.0273 |
[32m[20221213 21:10:00 @agent_ppo2.py:185][0m |           0.0016 |          98.5419 |          19.0208 |
[32m[20221213 21:10:00 @agent_ppo2.py:185][0m |          -0.0101 |          93.7780 |          19.0189 |
[32m[20221213 21:10:00 @agent_ppo2.py:185][0m |          -0.0095 |          93.4356 |          19.0282 |
[32m[20221213 21:10:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:10:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 561.60
[32m[20221213 21:10:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 603.00
[32m[20221213 21:10:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 563.00
[32m[20221213 21:10:00 @agent_ppo2.py:143][0m Total time:      14.43 min
[32m[20221213 21:10:00 @agent_ppo2.py:145][0m 1400832 total steps have happened
[32m[20221213 21:10:00 @agent_ppo2.py:121][0m #------------------------ Iteration 684 --------------------------#
[32m[20221213 21:10:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:01 @agent_ppo2.py:185][0m |          -0.0007 |          92.4425 |          19.0320 |
[32m[20221213 21:10:01 @agent_ppo2.py:185][0m |          -0.0066 |          91.3269 |          19.0288 |
[32m[20221213 21:10:01 @agent_ppo2.py:185][0m |          -0.0082 |          90.8413 |          19.0311 |
[32m[20221213 21:10:01 @agent_ppo2.py:185][0m |          -0.0010 |          94.1728 |          19.0268 |
[32m[20221213 21:10:01 @agent_ppo2.py:185][0m |          -0.0074 |          90.0346 |          19.0155 |
[32m[20221213 21:10:01 @agent_ppo2.py:185][0m |          -0.0101 |          89.8494 |          19.0170 |
[32m[20221213 21:10:01 @agent_ppo2.py:185][0m |          -0.0081 |          89.5256 |          19.0212 |
[32m[20221213 21:10:01 @agent_ppo2.py:185][0m |           0.0002 |          94.0877 |          19.0105 |
[32m[20221213 21:10:01 @agent_ppo2.py:185][0m |          -0.0116 |          89.3362 |          19.0220 |
[32m[20221213 21:10:01 @agent_ppo2.py:185][0m |          -0.0109 |          89.0360 |          19.0172 |
[32m[20221213 21:10:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:10:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.60
[32m[20221213 21:10:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 623.00
[32m[20221213 21:10:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.00
[32m[20221213 21:10:01 @agent_ppo2.py:143][0m Total time:      14.45 min
[32m[20221213 21:10:01 @agent_ppo2.py:145][0m 1402880 total steps have happened
[32m[20221213 21:10:01 @agent_ppo2.py:121][0m #------------------------ Iteration 685 --------------------------#
[32m[20221213 21:10:02 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:10:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:02 @agent_ppo2.py:185][0m |          -0.0021 |          97.0377 |          19.0903 |
[32m[20221213 21:10:02 @agent_ppo2.py:185][0m |          -0.0005 |          97.3838 |          19.0676 |
[32m[20221213 21:10:02 @agent_ppo2.py:185][0m |          -0.0069 |          95.8704 |          19.0571 |
[32m[20221213 21:10:02 @agent_ppo2.py:185][0m |          -0.0057 |          95.5091 |          19.0559 |
[32m[20221213 21:10:02 @agent_ppo2.py:185][0m |          -0.0068 |          95.4317 |          19.0563 |
[32m[20221213 21:10:02 @agent_ppo2.py:185][0m |          -0.0004 |          95.9705 |          19.0613 |
[32m[20221213 21:10:02 @agent_ppo2.py:185][0m |           0.0029 |          99.2739 |          19.0443 |
[32m[20221213 21:10:02 @agent_ppo2.py:185][0m |          -0.0092 |          94.7965 |          19.0470 |
[32m[20221213 21:10:02 @agent_ppo2.py:185][0m |          -0.0059 |          94.5272 |          19.0638 |
[32m[20221213 21:10:03 @agent_ppo2.py:185][0m |          -0.0075 |          94.4200 |          19.0521 |
[32m[20221213 21:10:03 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:10:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.40
[32m[20221213 21:10:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 640.00
[32m[20221213 21:10:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 571.00
[32m[20221213 21:10:03 @agent_ppo2.py:143][0m Total time:      14.47 min
[32m[20221213 21:10:03 @agent_ppo2.py:145][0m 1404928 total steps have happened
[32m[20221213 21:10:03 @agent_ppo2.py:121][0m #------------------------ Iteration 686 --------------------------#
[32m[20221213 21:10:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:10:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:03 @agent_ppo2.py:185][0m |           0.0015 |          99.2321 |          19.0838 |
[32m[20221213 21:10:03 @agent_ppo2.py:185][0m |          -0.0022 |          95.8189 |          19.0657 |
[32m[20221213 21:10:03 @agent_ppo2.py:185][0m |           0.0003 |          97.6352 |          19.0706 |
[32m[20221213 21:10:03 @agent_ppo2.py:185][0m |          -0.0039 |          94.9301 |          19.0745 |
[32m[20221213 21:10:03 @agent_ppo2.py:185][0m |           0.0071 |         101.2631 |          19.0709 |
[32m[20221213 21:10:03 @agent_ppo2.py:185][0m |          -0.0042 |          94.5257 |          19.0679 |
[32m[20221213 21:10:04 @agent_ppo2.py:185][0m |           0.0016 |          96.2062 |          19.0786 |
[32m[20221213 21:10:04 @agent_ppo2.py:185][0m |          -0.0065 |          93.8850 |          19.0824 |
[32m[20221213 21:10:04 @agent_ppo2.py:185][0m |          -0.0071 |          93.6006 |          19.0928 |
[32m[20221213 21:10:04 @agent_ppo2.py:185][0m |           0.0012 |          98.7544 |          19.0952 |
[32m[20221213 21:10:04 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:10:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 585.40
[32m[20221213 21:10:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 649.00
[32m[20221213 21:10:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.00
[32m[20221213 21:10:04 @agent_ppo2.py:143][0m Total time:      14.49 min
[32m[20221213 21:10:04 @agent_ppo2.py:145][0m 1406976 total steps have happened
[32m[20221213 21:10:04 @agent_ppo2.py:121][0m #------------------------ Iteration 687 --------------------------#
[32m[20221213 21:10:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:04 @agent_ppo2.py:185][0m |           0.0004 |          93.9534 |          19.1593 |
[32m[20221213 21:10:04 @agent_ppo2.py:185][0m |          -0.0072 |          90.5721 |          19.1389 |
[32m[20221213 21:10:04 @agent_ppo2.py:185][0m |          -0.0051 |          89.8624 |          19.1206 |
[32m[20221213 21:10:04 @agent_ppo2.py:185][0m |          -0.0047 |          89.1849 |          19.1088 |
[32m[20221213 21:10:05 @agent_ppo2.py:185][0m |          -0.0075 |          88.8022 |          19.1045 |
[32m[20221213 21:10:05 @agent_ppo2.py:185][0m |          -0.0029 |          90.1325 |          19.1016 |
[32m[20221213 21:10:05 @agent_ppo2.py:185][0m |          -0.0112 |          88.5503 |          19.0903 |
[32m[20221213 21:10:05 @agent_ppo2.py:185][0m |          -0.0088 |          88.3291 |          19.0883 |
[32m[20221213 21:10:05 @agent_ppo2.py:185][0m |          -0.0070 |          88.0481 |          19.0847 |
[32m[20221213 21:10:05 @agent_ppo2.py:185][0m |          -0.0072 |          87.7817 |          19.0775 |
[32m[20221213 21:10:05 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:10:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 569.20
[32m[20221213 21:10:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 599.00
[32m[20221213 21:10:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 627.00
[32m[20221213 21:10:05 @agent_ppo2.py:143][0m Total time:      14.51 min
[32m[20221213 21:10:05 @agent_ppo2.py:145][0m 1409024 total steps have happened
[32m[20221213 21:10:05 @agent_ppo2.py:121][0m #------------------------ Iteration 688 --------------------------#
[32m[20221213 21:10:05 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:10:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:05 @agent_ppo2.py:185][0m |          -0.0009 |         103.3813 |          19.0323 |
[32m[20221213 21:10:06 @agent_ppo2.py:185][0m |          -0.0058 |         102.3478 |          19.0401 |
[32m[20221213 21:10:06 @agent_ppo2.py:185][0m |          -0.0098 |         101.7550 |          19.0452 |
[32m[20221213 21:10:06 @agent_ppo2.py:185][0m |          -0.0073 |         101.2750 |          19.0501 |
[32m[20221213 21:10:06 @agent_ppo2.py:185][0m |          -0.0087 |         101.0711 |          19.0473 |
[32m[20221213 21:10:06 @agent_ppo2.py:185][0m |          -0.0075 |         101.1122 |          19.0558 |
[32m[20221213 21:10:06 @agent_ppo2.py:185][0m |          -0.0070 |         101.0634 |          19.0520 |
[32m[20221213 21:10:06 @agent_ppo2.py:185][0m |          -0.0098 |         100.7216 |          19.0615 |
[32m[20221213 21:10:06 @agent_ppo2.py:185][0m |          -0.0079 |         100.5364 |          19.0656 |
[32m[20221213 21:10:06 @agent_ppo2.py:185][0m |          -0.0099 |         100.4628 |          19.0703 |
[32m[20221213 21:10:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:10:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 563.40
[32m[20221213 21:10:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 619.00
[32m[20221213 21:10:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 608.00
[32m[20221213 21:10:06 @agent_ppo2.py:143][0m Total time:      14.53 min
[32m[20221213 21:10:06 @agent_ppo2.py:145][0m 1411072 total steps have happened
[32m[20221213 21:10:06 @agent_ppo2.py:121][0m #------------------------ Iteration 689 --------------------------#
[32m[20221213 21:10:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:10:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:07 @agent_ppo2.py:185][0m |          -0.0011 |          95.6905 |          19.1399 |
[32m[20221213 21:10:07 @agent_ppo2.py:185][0m |          -0.0062 |          94.0096 |          19.1124 |
[32m[20221213 21:10:07 @agent_ppo2.py:185][0m |          -0.0062 |          93.0877 |          19.1050 |
[32m[20221213 21:10:07 @agent_ppo2.py:185][0m |          -0.0074 |          92.5416 |          19.1041 |
[32m[20221213 21:10:07 @agent_ppo2.py:185][0m |          -0.0066 |          92.1583 |          19.0963 |
[32m[20221213 21:10:07 @agent_ppo2.py:185][0m |          -0.0069 |          91.8328 |          19.0883 |
[32m[20221213 21:10:07 @agent_ppo2.py:185][0m |          -0.0088 |          91.5925 |          19.0800 |
[32m[20221213 21:10:07 @agent_ppo2.py:185][0m |          -0.0081 |          91.4301 |          19.0832 |
[32m[20221213 21:10:07 @agent_ppo2.py:185][0m |          -0.0079 |          91.3259 |          19.0773 |
[32m[20221213 21:10:07 @agent_ppo2.py:185][0m |          -0.0095 |          91.2437 |          19.0733 |
[32m[20221213 21:10:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:10:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 592.40
[32m[20221213 21:10:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 638.00
[32m[20221213 21:10:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 599.00
[32m[20221213 21:10:08 @agent_ppo2.py:143][0m Total time:      14.55 min
[32m[20221213 21:10:08 @agent_ppo2.py:145][0m 1413120 total steps have happened
[32m[20221213 21:10:08 @agent_ppo2.py:121][0m #------------------------ Iteration 690 --------------------------#
[32m[20221213 21:10:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:08 @agent_ppo2.py:185][0m |          -0.0010 |          97.4187 |          19.0756 |
[32m[20221213 21:10:08 @agent_ppo2.py:185][0m |           0.0001 |          96.9434 |          19.0517 |
[32m[20221213 21:10:08 @agent_ppo2.py:185][0m |          -0.0036 |          96.1997 |          19.0576 |
[32m[20221213 21:10:08 @agent_ppo2.py:185][0m |           0.0061 |         102.4749 |          19.0463 |
[32m[20221213 21:10:08 @agent_ppo2.py:185][0m |          -0.0056 |          95.8024 |          19.0718 |
[32m[20221213 21:10:08 @agent_ppo2.py:185][0m |          -0.0077 |          95.5513 |          19.0641 |
[32m[20221213 21:10:08 @agent_ppo2.py:185][0m |          -0.0060 |          95.3884 |          19.0715 |
[32m[20221213 21:10:08 @agent_ppo2.py:185][0m |           0.0027 |         102.3666 |          19.0650 |
[32m[20221213 21:10:09 @agent_ppo2.py:185][0m |          -0.0075 |          95.3748 |          19.0522 |
[32m[20221213 21:10:09 @agent_ppo2.py:185][0m |           0.0042 |          98.9433 |          19.0678 |
[32m[20221213 21:10:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:10:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 573.60
[32m[20221213 21:10:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.00
[32m[20221213 21:10:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.00
[32m[20221213 21:10:09 @agent_ppo2.py:143][0m Total time:      14.57 min
[32m[20221213 21:10:09 @agent_ppo2.py:145][0m 1415168 total steps have happened
[32m[20221213 21:10:09 @agent_ppo2.py:121][0m #------------------------ Iteration 691 --------------------------#
[32m[20221213 21:10:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:09 @agent_ppo2.py:185][0m |          -0.0012 |          96.0028 |          19.0108 |
[32m[20221213 21:10:09 @agent_ppo2.py:185][0m |          -0.0025 |          94.7563 |          18.9955 |
[32m[20221213 21:10:09 @agent_ppo2.py:185][0m |          -0.0076 |          94.4160 |          18.9793 |
[32m[20221213 21:10:09 @agent_ppo2.py:185][0m |          -0.0052 |          93.8895 |          18.9697 |
[32m[20221213 21:10:09 @agent_ppo2.py:185][0m |          -0.0040 |          93.6184 |          18.9514 |
[32m[20221213 21:10:09 @agent_ppo2.py:185][0m |          -0.0083 |          93.0996 |          18.9333 |
[32m[20221213 21:10:10 @agent_ppo2.py:185][0m |          -0.0070 |          93.0611 |          18.9307 |
[32m[20221213 21:10:10 @agent_ppo2.py:185][0m |          -0.0036 |          93.7249 |          18.9149 |
[32m[20221213 21:10:10 @agent_ppo2.py:185][0m |          -0.0083 |          92.3472 |          18.8977 |
[32m[20221213 21:10:10 @agent_ppo2.py:185][0m |          -0.0072 |          92.5267 |          18.9064 |
[32m[20221213 21:10:10 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:10:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 593.20
[32m[20221213 21:10:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 637.00
[32m[20221213 21:10:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.00
[32m[20221213 21:10:10 @agent_ppo2.py:143][0m Total time:      14.59 min
[32m[20221213 21:10:10 @agent_ppo2.py:145][0m 1417216 total steps have happened
[32m[20221213 21:10:10 @agent_ppo2.py:121][0m #------------------------ Iteration 692 --------------------------#
[32m[20221213 21:10:10 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:10:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:10 @agent_ppo2.py:185][0m |          -0.0007 |         100.4385 |          18.9447 |
[32m[20221213 21:10:10 @agent_ppo2.py:185][0m |          -0.0027 |          99.3905 |          18.9336 |
[32m[20221213 21:10:10 @agent_ppo2.py:185][0m |           0.0065 |         103.1687 |          18.9357 |
[32m[20221213 21:10:11 @agent_ppo2.py:185][0m |          -0.0044 |          98.6501 |          18.9357 |
[32m[20221213 21:10:11 @agent_ppo2.py:185][0m |          -0.0059 |          98.2937 |          18.9425 |
[32m[20221213 21:10:11 @agent_ppo2.py:185][0m |          -0.0064 |          98.0469 |          18.9404 |
[32m[20221213 21:10:11 @agent_ppo2.py:185][0m |          -0.0035 |          98.0038 |          18.9371 |
[32m[20221213 21:10:11 @agent_ppo2.py:185][0m |          -0.0095 |          98.0685 |          18.9386 |
[32m[20221213 21:10:11 @agent_ppo2.py:185][0m |          -0.0038 |          99.5482 |          18.9373 |
[32m[20221213 21:10:11 @agent_ppo2.py:185][0m |          -0.0060 |          97.6648 |          18.9442 |
[32m[20221213 21:10:11 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:10:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 585.00
[32m[20221213 21:10:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 606.00
[32m[20221213 21:10:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 632.00
[32m[20221213 21:10:11 @agent_ppo2.py:143][0m Total time:      14.61 min
[32m[20221213 21:10:11 @agent_ppo2.py:145][0m 1419264 total steps have happened
[32m[20221213 21:10:11 @agent_ppo2.py:121][0m #------------------------ Iteration 693 --------------------------#
[32m[20221213 21:10:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:12 @agent_ppo2.py:185][0m |           0.0113 |         107.5583 |          19.0658 |
[32m[20221213 21:10:12 @agent_ppo2.py:185][0m |          -0.0043 |          96.3656 |          19.0416 |
[32m[20221213 21:10:12 @agent_ppo2.py:185][0m |          -0.0013 |          96.6155 |          19.0448 |
[32m[20221213 21:10:12 @agent_ppo2.py:185][0m |           0.0164 |         105.4883 |          19.0596 |
[32m[20221213 21:10:12 @agent_ppo2.py:185][0m |          -0.0063 |          95.3356 |          19.0455 |
[32m[20221213 21:10:12 @agent_ppo2.py:185][0m |          -0.0092 |          95.1662 |          19.0452 |
[32m[20221213 21:10:12 @agent_ppo2.py:185][0m |          -0.0043 |          94.6777 |          19.0575 |
[32m[20221213 21:10:12 @agent_ppo2.py:185][0m |          -0.0066 |          94.5691 |          19.0568 |
[32m[20221213 21:10:12 @agent_ppo2.py:185][0m |          -0.0068 |          94.4192 |          19.0575 |
[32m[20221213 21:10:12 @agent_ppo2.py:185][0m |          -0.0084 |          94.3283 |          19.0543 |
[32m[20221213 21:10:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:10:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 561.40
[32m[20221213 21:10:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.00
[32m[20221213 21:10:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 596.00
[32m[20221213 21:10:12 @agent_ppo2.py:143][0m Total time:      14.63 min
[32m[20221213 21:10:12 @agent_ppo2.py:145][0m 1421312 total steps have happened
[32m[20221213 21:10:12 @agent_ppo2.py:121][0m #------------------------ Iteration 694 --------------------------#
[32m[20221213 21:10:13 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:10:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:13 @agent_ppo2.py:185][0m |          -0.0016 |          94.8165 |          19.0821 |
[32m[20221213 21:10:13 @agent_ppo2.py:185][0m |          -0.0053 |          94.0272 |          19.0697 |
[32m[20221213 21:10:13 @agent_ppo2.py:185][0m |          -0.0099 |          93.8171 |          19.0635 |
[32m[20221213 21:10:13 @agent_ppo2.py:185][0m |          -0.0085 |          93.3719 |          19.0568 |
[32m[20221213 21:10:13 @agent_ppo2.py:185][0m |          -0.0071 |          93.5065 |          19.0562 |
[32m[20221213 21:10:13 @agent_ppo2.py:185][0m |          -0.0090 |          93.0139 |          19.0539 |
[32m[20221213 21:10:13 @agent_ppo2.py:185][0m |          -0.0112 |          92.9121 |          19.0452 |
[32m[20221213 21:10:13 @agent_ppo2.py:185][0m |          -0.0089 |          92.6494 |          19.0505 |
[32m[20221213 21:10:13 @agent_ppo2.py:185][0m |          -0.0095 |          92.5880 |          19.0362 |
[32m[20221213 21:10:13 @agent_ppo2.py:185][0m |          -0.0086 |          92.4054 |          19.0362 |
[32m[20221213 21:10:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:10:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 556.80
[32m[20221213 21:10:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.00
[32m[20221213 21:10:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 570.00
[32m[20221213 21:10:14 @agent_ppo2.py:143][0m Total time:      14.65 min
[32m[20221213 21:10:14 @agent_ppo2.py:145][0m 1423360 total steps have happened
[32m[20221213 21:10:14 @agent_ppo2.py:121][0m #------------------------ Iteration 695 --------------------------#
[32m[20221213 21:10:14 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:10:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:14 @agent_ppo2.py:185][0m |           0.0003 |          95.7465 |          19.0794 |
[32m[20221213 21:10:14 @agent_ppo2.py:185][0m |          -0.0042 |          94.8251 |          19.0661 |
[32m[20221213 21:10:14 @agent_ppo2.py:185][0m |          -0.0058 |          94.4542 |          19.0565 |
[32m[20221213 21:10:14 @agent_ppo2.py:185][0m |          -0.0047 |          94.0064 |          19.0455 |
[32m[20221213 21:10:14 @agent_ppo2.py:185][0m |           0.0034 |         105.0991 |          19.0571 |
[32m[20221213 21:10:14 @agent_ppo2.py:185][0m |          -0.0045 |          93.6350 |          19.0326 |
[32m[20221213 21:10:14 @agent_ppo2.py:185][0m |          -0.0060 |          93.3357 |          19.0488 |
[32m[20221213 21:10:14 @agent_ppo2.py:185][0m |          -0.0076 |          93.1963 |          19.0543 |
[32m[20221213 21:10:15 @agent_ppo2.py:185][0m |          -0.0062 |          93.0130 |          19.0478 |
[32m[20221213 21:10:15 @agent_ppo2.py:185][0m |          -0.0066 |          92.9325 |          19.0459 |
[32m[20221213 21:10:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:10:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 597.60
[32m[20221213 21:10:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 613.00
[32m[20221213 21:10:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 608.00
[32m[20221213 21:10:15 @agent_ppo2.py:143][0m Total time:      14.67 min
[32m[20221213 21:10:15 @agent_ppo2.py:145][0m 1425408 total steps have happened
[32m[20221213 21:10:15 @agent_ppo2.py:121][0m #------------------------ Iteration 696 --------------------------#
[32m[20221213 21:10:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:15 @agent_ppo2.py:185][0m |          -0.0017 |          94.0928 |          19.0379 |
[32m[20221213 21:10:15 @agent_ppo2.py:185][0m |          -0.0017 |          91.6925 |          19.0253 |
[32m[20221213 21:10:15 @agent_ppo2.py:185][0m |          -0.0060 |          90.5073 |          19.0210 |
[32m[20221213 21:10:15 @agent_ppo2.py:185][0m |          -0.0080 |          89.5992 |          19.0125 |
[32m[20221213 21:10:15 @agent_ppo2.py:185][0m |          -0.0070 |          89.7437 |          19.0227 |
[32m[20221213 21:10:15 @agent_ppo2.py:185][0m |          -0.0077 |          88.9346 |          19.0176 |
[32m[20221213 21:10:16 @agent_ppo2.py:185][0m |          -0.0069 |          88.3173 |          19.0209 |
[32m[20221213 21:10:16 @agent_ppo2.py:185][0m |          -0.0072 |          88.2543 |          19.0194 |
[32m[20221213 21:10:16 @agent_ppo2.py:185][0m |          -0.0068 |          87.9402 |          19.0135 |
[32m[20221213 21:10:16 @agent_ppo2.py:185][0m |          -0.0088 |          87.9704 |          19.0208 |
[32m[20221213 21:10:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:10:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.60
[32m[20221213 21:10:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 581.00
[32m[20221213 21:10:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 593.00
[32m[20221213 21:10:16 @agent_ppo2.py:143][0m Total time:      14.69 min
[32m[20221213 21:10:16 @agent_ppo2.py:145][0m 1427456 total steps have happened
[32m[20221213 21:10:16 @agent_ppo2.py:121][0m #------------------------ Iteration 697 --------------------------#
[32m[20221213 21:10:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:16 @agent_ppo2.py:185][0m |           0.0001 |         102.1262 |          18.9954 |
[32m[20221213 21:10:16 @agent_ppo2.py:185][0m |           0.0055 |         102.9978 |          18.9935 |
[32m[20221213 21:10:16 @agent_ppo2.py:185][0m |          -0.0033 |          99.9406 |          19.0026 |
[32m[20221213 21:10:17 @agent_ppo2.py:185][0m |          -0.0052 |          99.4520 |          19.0201 |
[32m[20221213 21:10:17 @agent_ppo2.py:185][0m |          -0.0035 |          99.2156 |          19.0101 |
[32m[20221213 21:10:17 @agent_ppo2.py:185][0m |          -0.0062 |          99.1332 |          19.0325 |
[32m[20221213 21:10:17 @agent_ppo2.py:185][0m |          -0.0070 |          98.8537 |          19.0268 |
[32m[20221213 21:10:17 @agent_ppo2.py:185][0m |          -0.0069 |          98.6296 |          19.0453 |
[32m[20221213 21:10:17 @agent_ppo2.py:185][0m |          -0.0051 |          98.6525 |          19.0471 |
[32m[20221213 21:10:17 @agent_ppo2.py:185][0m |          -0.0082 |          98.4870 |          19.0559 |
[32m[20221213 21:10:17 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:10:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.80
[32m[20221213 21:10:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 651.00
[32m[20221213 21:10:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.00
[32m[20221213 21:10:17 @agent_ppo2.py:143][0m Total time:      14.71 min
[32m[20221213 21:10:17 @agent_ppo2.py:145][0m 1429504 total steps have happened
[32m[20221213 21:10:17 @agent_ppo2.py:121][0m #------------------------ Iteration 698 --------------------------#
[32m[20221213 21:10:17 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:10:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:18 @agent_ppo2.py:185][0m |           0.0037 |         100.7041 |          18.8897 |
[32m[20221213 21:10:18 @agent_ppo2.py:185][0m |          -0.0034 |          98.8847 |          18.8754 |
[32m[20221213 21:10:18 @agent_ppo2.py:185][0m |          -0.0040 |          98.6318 |          18.8627 |
[32m[20221213 21:10:18 @agent_ppo2.py:185][0m |           0.0006 |         100.1733 |          18.8604 |
[32m[20221213 21:10:18 @agent_ppo2.py:185][0m |          -0.0067 |          96.6947 |          18.8498 |
[32m[20221213 21:10:18 @agent_ppo2.py:185][0m |          -0.0071 |          96.1048 |          18.8351 |
[32m[20221213 21:10:18 @agent_ppo2.py:185][0m |          -0.0096 |          95.7793 |          18.8338 |
[32m[20221213 21:10:18 @agent_ppo2.py:185][0m |          -0.0078 |          96.0297 |          18.8333 |
[32m[20221213 21:10:18 @agent_ppo2.py:185][0m |          -0.0097 |          95.0954 |          18.8307 |
[32m[20221213 21:10:18 @agent_ppo2.py:185][0m |          -0.0049 |          95.4763 |          18.8327 |
[32m[20221213 21:10:18 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:10:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 578.40
[32m[20221213 21:10:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 617.00
[32m[20221213 21:10:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.00
[32m[20221213 21:10:19 @agent_ppo2.py:143][0m Total time:      14.73 min
[32m[20221213 21:10:19 @agent_ppo2.py:145][0m 1431552 total steps have happened
[32m[20221213 21:10:19 @agent_ppo2.py:121][0m #------------------------ Iteration 699 --------------------------#
[32m[20221213 21:10:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:10:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:19 @agent_ppo2.py:185][0m |          -0.0044 |          96.8714 |          19.0799 |
[32m[20221213 21:10:19 @agent_ppo2.py:185][0m |          -0.0057 |          95.9188 |          19.0590 |
[32m[20221213 21:10:19 @agent_ppo2.py:185][0m |          -0.0081 |          95.5697 |          19.0578 |
[32m[20221213 21:10:19 @agent_ppo2.py:185][0m |          -0.0077 |          95.1641 |          19.0625 |
[32m[20221213 21:10:19 @agent_ppo2.py:185][0m |          -0.0068 |          94.9005 |          19.0663 |
[32m[20221213 21:10:19 @agent_ppo2.py:185][0m |          -0.0067 |          94.7721 |          19.0693 |
[32m[20221213 21:10:19 @agent_ppo2.py:185][0m |          -0.0097 |          94.5742 |          19.0677 |
[32m[20221213 21:10:20 @agent_ppo2.py:185][0m |          -0.0081 |          94.4129 |          19.0644 |
[32m[20221213 21:10:20 @agent_ppo2.py:185][0m |          -0.0103 |          94.3058 |          19.0610 |
[32m[20221213 21:10:20 @agent_ppo2.py:185][0m |          -0.0093 |          94.1601 |          19.0695 |
[32m[20221213 21:10:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:10:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 589.20
[32m[20221213 21:10:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 645.00
[32m[20221213 21:10:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 553.00
[32m[20221213 21:10:20 @agent_ppo2.py:143][0m Total time:      14.75 min
[32m[20221213 21:10:20 @agent_ppo2.py:145][0m 1433600 total steps have happened
[32m[20221213 21:10:20 @agent_ppo2.py:121][0m #------------------------ Iteration 700 --------------------------#
[32m[20221213 21:10:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:20 @agent_ppo2.py:185][0m |           0.0026 |          94.4423 |          19.0393 |
[32m[20221213 21:10:20 @agent_ppo2.py:185][0m |          -0.0007 |          93.6778 |          19.0323 |
[32m[20221213 21:10:20 @agent_ppo2.py:185][0m |          -0.0031 |          93.2938 |          19.0166 |
[32m[20221213 21:10:20 @agent_ppo2.py:185][0m |          -0.0056 |          92.8710 |          19.0111 |
[32m[20221213 21:10:21 @agent_ppo2.py:185][0m |          -0.0045 |          92.7023 |          19.0105 |
[32m[20221213 21:10:21 @agent_ppo2.py:185][0m |           0.0059 |         102.0514 |          19.0150 |
[32m[20221213 21:10:21 @agent_ppo2.py:185][0m |          -0.0062 |          92.2752 |          19.0124 |
[32m[20221213 21:10:21 @agent_ppo2.py:185][0m |          -0.0076 |          92.2131 |          18.9988 |
[32m[20221213 21:10:21 @agent_ppo2.py:185][0m |          -0.0024 |          93.4798 |          19.0002 |
[32m[20221213 21:10:21 @agent_ppo2.py:185][0m |          -0.0080 |          92.0794 |          18.9837 |
[32m[20221213 21:10:21 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:10:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 549.60
[32m[20221213 21:10:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 579.00
[32m[20221213 21:10:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 598.00
[32m[20221213 21:10:21 @agent_ppo2.py:143][0m Total time:      14.77 min
[32m[20221213 21:10:21 @agent_ppo2.py:145][0m 1435648 total steps have happened
[32m[20221213 21:10:21 @agent_ppo2.py:121][0m #------------------------ Iteration 701 --------------------------#
[32m[20221213 21:10:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:21 @agent_ppo2.py:185][0m |          -0.0005 |          94.5257 |          19.1592 |
[32m[20221213 21:10:21 @agent_ppo2.py:185][0m |          -0.0067 |          91.1724 |          19.1515 |
[32m[20221213 21:10:22 @agent_ppo2.py:185][0m |          -0.0080 |          90.5028 |          19.1486 |
[32m[20221213 21:10:22 @agent_ppo2.py:185][0m |          -0.0048 |          89.8516 |          19.1600 |
[32m[20221213 21:10:22 @agent_ppo2.py:185][0m |          -0.0100 |          89.5635 |          19.1585 |
[32m[20221213 21:10:22 @agent_ppo2.py:185][0m |          -0.0083 |          89.1680 |          19.1426 |
[32m[20221213 21:10:22 @agent_ppo2.py:185][0m |          -0.0071 |          88.8188 |          19.1522 |
[32m[20221213 21:10:22 @agent_ppo2.py:185][0m |          -0.0099 |          88.8029 |          19.1398 |
[32m[20221213 21:10:22 @agent_ppo2.py:185][0m |          -0.0058 |          88.2476 |          19.1368 |
[32m[20221213 21:10:22 @agent_ppo2.py:185][0m |          -0.0076 |          88.0941 |          19.1501 |
[32m[20221213 21:10:22 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:10:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.00
[32m[20221213 21:10:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 576.00
[32m[20221213 21:10:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.00
[32m[20221213 21:10:22 @agent_ppo2.py:143][0m Total time:      14.80 min
[32m[20221213 21:10:22 @agent_ppo2.py:145][0m 1437696 total steps have happened
[32m[20221213 21:10:22 @agent_ppo2.py:121][0m #------------------------ Iteration 702 --------------------------#
[32m[20221213 21:10:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:10:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:23 @agent_ppo2.py:185][0m |          -0.0023 |          98.2853 |          19.0790 |
[32m[20221213 21:10:23 @agent_ppo2.py:185][0m |          -0.0047 |          97.2366 |          19.0793 |
[32m[20221213 21:10:23 @agent_ppo2.py:185][0m |          -0.0056 |          96.7080 |          19.0818 |
[32m[20221213 21:10:23 @agent_ppo2.py:185][0m |          -0.0078 |          96.4954 |          19.0683 |
[32m[20221213 21:10:23 @agent_ppo2.py:185][0m |          -0.0069 |          96.0306 |          19.0732 |
[32m[20221213 21:10:23 @agent_ppo2.py:185][0m |          -0.0066 |          95.7668 |          19.0673 |
[32m[20221213 21:10:23 @agent_ppo2.py:185][0m |           0.0060 |         107.3681 |          19.0612 |
[32m[20221213 21:10:23 @agent_ppo2.py:185][0m |           0.0015 |         101.2970 |          19.0634 |
[32m[20221213 21:10:23 @agent_ppo2.py:185][0m |          -0.0065 |          95.4545 |          19.0583 |
[32m[20221213 21:10:23 @agent_ppo2.py:185][0m |          -0.0035 |          96.1897 |          19.0613 |
[32m[20221213 21:10:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:10:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 587.80
[32m[20221213 21:10:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 630.00
[32m[20221213 21:10:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 599.00
[32m[20221213 21:10:24 @agent_ppo2.py:143][0m Total time:      14.82 min
[32m[20221213 21:10:24 @agent_ppo2.py:145][0m 1439744 total steps have happened
[32m[20221213 21:10:24 @agent_ppo2.py:121][0m #------------------------ Iteration 703 --------------------------#
[32m[20221213 21:10:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:24 @agent_ppo2.py:185][0m |           0.0030 |          97.8794 |          19.1596 |
[32m[20221213 21:10:24 @agent_ppo2.py:185][0m |          -0.0055 |          95.4717 |          19.1462 |
[32m[20221213 21:10:24 @agent_ppo2.py:185][0m |          -0.0056 |          95.1140 |          19.1541 |
[32m[20221213 21:10:24 @agent_ppo2.py:185][0m |          -0.0056 |          94.1648 |          19.1560 |
[32m[20221213 21:10:24 @agent_ppo2.py:185][0m |          -0.0042 |          94.4802 |          19.1545 |
[32m[20221213 21:10:24 @agent_ppo2.py:185][0m |          -0.0059 |          93.5724 |          19.1522 |
[32m[20221213 21:10:24 @agent_ppo2.py:185][0m |          -0.0059 |          93.1438 |          19.1686 |
[32m[20221213 21:10:25 @agent_ppo2.py:185][0m |          -0.0082 |          92.9966 |          19.1620 |
[32m[20221213 21:10:25 @agent_ppo2.py:185][0m |          -0.0092 |          93.0007 |          19.1725 |
[32m[20221213 21:10:25 @agent_ppo2.py:185][0m |          -0.0077 |          92.9218 |          19.1658 |
[32m[20221213 21:10:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:10:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 567.60
[32m[20221213 21:10:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 630.00
[32m[20221213 21:10:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 635.00
[32m[20221213 21:10:25 @agent_ppo2.py:143][0m Total time:      14.84 min
[32m[20221213 21:10:25 @agent_ppo2.py:145][0m 1441792 total steps have happened
[32m[20221213 21:10:25 @agent_ppo2.py:121][0m #------------------------ Iteration 704 --------------------------#
[32m[20221213 21:10:25 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:10:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:25 @agent_ppo2.py:185][0m |           0.0051 |         100.4683 |          19.1502 |
[32m[20221213 21:10:25 @agent_ppo2.py:185][0m |           0.0069 |         101.9545 |          19.1425 |
[32m[20221213 21:10:25 @agent_ppo2.py:185][0m |           0.0026 |          98.6921 |          19.1291 |
[32m[20221213 21:10:25 @agent_ppo2.py:185][0m |           0.0046 |         100.0886 |          19.1320 |
[32m[20221213 21:10:25 @agent_ppo2.py:185][0m |           0.0035 |         101.9619 |          19.1369 |
[32m[20221213 21:10:26 @agent_ppo2.py:185][0m |          -0.0059 |          95.5794 |          19.1182 |
[32m[20221213 21:10:26 @agent_ppo2.py:185][0m |          -0.0075 |          95.2015 |          19.1229 |
[32m[20221213 21:10:26 @agent_ppo2.py:185][0m |          -0.0091 |          95.0283 |          19.1119 |
[32m[20221213 21:10:26 @agent_ppo2.py:185][0m |          -0.0045 |          96.8722 |          19.0985 |
[32m[20221213 21:10:26 @agent_ppo2.py:185][0m |           0.0047 |         103.8534 |          19.0939 |
[32m[20221213 21:10:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:10:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.40
[32m[20221213 21:10:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.00
[32m[20221213 21:10:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 604.00
[32m[20221213 21:10:26 @agent_ppo2.py:143][0m Total time:      14.86 min
[32m[20221213 21:10:26 @agent_ppo2.py:145][0m 1443840 total steps have happened
[32m[20221213 21:10:26 @agent_ppo2.py:121][0m #------------------------ Iteration 705 --------------------------#
[32m[20221213 21:10:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:26 @agent_ppo2.py:185][0m |           0.0002 |          96.3634 |          19.0189 |
[32m[20221213 21:10:26 @agent_ppo2.py:185][0m |          -0.0055 |          95.3601 |          19.0147 |
[32m[20221213 21:10:26 @agent_ppo2.py:185][0m |          -0.0064 |          94.7771 |          19.0065 |
[32m[20221213 21:10:27 @agent_ppo2.py:185][0m |          -0.0090 |          94.4235 |          19.0053 |
[32m[20221213 21:10:27 @agent_ppo2.py:185][0m |          -0.0087 |          94.1490 |          19.0128 |
[32m[20221213 21:10:27 @agent_ppo2.py:185][0m |          -0.0078 |          93.7735 |          19.0090 |
[32m[20221213 21:10:27 @agent_ppo2.py:185][0m |          -0.0064 |          94.6740 |          19.0058 |
[32m[20221213 21:10:27 @agent_ppo2.py:185][0m |          -0.0069 |          94.0136 |          19.0064 |
[32m[20221213 21:10:27 @agent_ppo2.py:185][0m |          -0.0068 |          93.1761 |          19.0071 |
[32m[20221213 21:10:27 @agent_ppo2.py:185][0m |          -0.0136 |          93.2129 |          19.0075 |
[32m[20221213 21:10:27 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:10:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 601.20
[32m[20221213 21:10:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 633.00
[32m[20221213 21:10:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.00
[32m[20221213 21:10:27 @agent_ppo2.py:143][0m Total time:      14.88 min
[32m[20221213 21:10:27 @agent_ppo2.py:145][0m 1445888 total steps have happened
[32m[20221213 21:10:27 @agent_ppo2.py:121][0m #------------------------ Iteration 706 --------------------------#
[32m[20221213 21:10:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:28 @agent_ppo2.py:185][0m |           0.0030 |         100.0760 |          19.0170 |
[32m[20221213 21:10:28 @agent_ppo2.py:185][0m |           0.0009 |         101.5124 |          19.0220 |
[32m[20221213 21:10:28 @agent_ppo2.py:185][0m |          -0.0062 |          97.7203 |          19.0142 |
[32m[20221213 21:10:28 @agent_ppo2.py:185][0m |          -0.0061 |          97.3129 |          19.0117 |
[32m[20221213 21:10:28 @agent_ppo2.py:185][0m |          -0.0043 |          97.0899 |          19.0111 |
[32m[20221213 21:10:28 @agent_ppo2.py:185][0m |          -0.0079 |          96.7446 |          19.0225 |
[32m[20221213 21:10:28 @agent_ppo2.py:185][0m |          -0.0083 |          96.5300 |          19.0154 |
[32m[20221213 21:10:28 @agent_ppo2.py:185][0m |          -0.0081 |          96.4526 |          19.0124 |
[32m[20221213 21:10:28 @agent_ppo2.py:185][0m |           0.0029 |         106.8667 |          19.0187 |
[32m[20221213 21:10:28 @agent_ppo2.py:185][0m |           0.0004 |         101.3098 |          19.0253 |
[32m[20221213 21:10:28 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:10:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.60
[32m[20221213 21:10:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 666.00
[32m[20221213 21:10:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.00
[32m[20221213 21:10:28 @agent_ppo2.py:143][0m Total time:      14.90 min
[32m[20221213 21:10:28 @agent_ppo2.py:145][0m 1447936 total steps have happened
[32m[20221213 21:10:28 @agent_ppo2.py:121][0m #------------------------ Iteration 707 --------------------------#
[32m[20221213 21:10:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:29 @agent_ppo2.py:185][0m |           0.0001 |          96.6128 |          18.9594 |
[32m[20221213 21:10:29 @agent_ppo2.py:185][0m |          -0.0056 |          94.9998 |          18.9364 |
[32m[20221213 21:10:29 @agent_ppo2.py:185][0m |          -0.0053 |          94.6190 |          18.9310 |
[32m[20221213 21:10:29 @agent_ppo2.py:185][0m |          -0.0075 |          94.4869 |          18.9108 |
[32m[20221213 21:10:29 @agent_ppo2.py:185][0m |          -0.0067 |          94.2402 |          18.9184 |
[32m[20221213 21:10:29 @agent_ppo2.py:185][0m |          -0.0063 |          93.9980 |          18.9099 |
[32m[20221213 21:10:29 @agent_ppo2.py:185][0m |          -0.0038 |          93.8219 |          18.9113 |
[32m[20221213 21:10:29 @agent_ppo2.py:185][0m |          -0.0090 |          93.8098 |          18.9159 |
[32m[20221213 21:10:29 @agent_ppo2.py:185][0m |          -0.0091 |          93.8056 |          18.9060 |
[32m[20221213 21:10:30 @agent_ppo2.py:185][0m |          -0.0103 |          93.6579 |          18.9053 |
[32m[20221213 21:10:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:10:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.80
[32m[20221213 21:10:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 615.00
[32m[20221213 21:10:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 635.00
[32m[20221213 21:10:30 @agent_ppo2.py:143][0m Total time:      14.92 min
[32m[20221213 21:10:30 @agent_ppo2.py:145][0m 1449984 total steps have happened
[32m[20221213 21:10:30 @agent_ppo2.py:121][0m #------------------------ Iteration 708 --------------------------#
[32m[20221213 21:10:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:30 @agent_ppo2.py:185][0m |           0.0065 |         101.5955 |          18.9772 |
[32m[20221213 21:10:30 @agent_ppo2.py:185][0m |           0.0025 |          97.8961 |          18.9568 |
[32m[20221213 21:10:30 @agent_ppo2.py:185][0m |          -0.0002 |          96.7541 |          18.9496 |
[32m[20221213 21:10:30 @agent_ppo2.py:185][0m |           0.0007 |          96.0905 |          18.9357 |
[32m[20221213 21:10:30 @agent_ppo2.py:185][0m |          -0.0041 |          95.6385 |          18.9150 |
[32m[20221213 21:10:30 @agent_ppo2.py:185][0m |          -0.0027 |          95.4310 |          18.9128 |
[32m[20221213 21:10:31 @agent_ppo2.py:185][0m |           0.0018 |         100.3982 |          18.9103 |
[32m[20221213 21:10:31 @agent_ppo2.py:185][0m |          -0.0069 |          95.2380 |          18.8723 |
[32m[20221213 21:10:31 @agent_ppo2.py:185][0m |           0.0048 |         101.9567 |          18.8817 |
[32m[20221213 21:10:31 @agent_ppo2.py:185][0m |          -0.0024 |          95.0259 |          18.8853 |
[32m[20221213 21:10:31 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:10:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 593.80
[32m[20221213 21:10:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 606.00
[32m[20221213 21:10:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 621.00
[32m[20221213 21:10:31 @agent_ppo2.py:143][0m Total time:      14.94 min
[32m[20221213 21:10:31 @agent_ppo2.py:145][0m 1452032 total steps have happened
[32m[20221213 21:10:31 @agent_ppo2.py:121][0m #------------------------ Iteration 709 --------------------------#
[32m[20221213 21:10:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:31 @agent_ppo2.py:185][0m |           0.0012 |          99.5978 |          18.9703 |
[32m[20221213 21:10:31 @agent_ppo2.py:185][0m |          -0.0042 |          98.3465 |          18.9539 |
[32m[20221213 21:10:31 @agent_ppo2.py:185][0m |           0.0004 |          99.4272 |          18.9420 |
[32m[20221213 21:10:32 @agent_ppo2.py:185][0m |          -0.0056 |          97.1781 |          18.9366 |
[32m[20221213 21:10:32 @agent_ppo2.py:185][0m |          -0.0081 |          96.7390 |          18.9381 |
[32m[20221213 21:10:32 @agent_ppo2.py:185][0m |          -0.0086 |          96.3596 |          18.9169 |
[32m[20221213 21:10:32 @agent_ppo2.py:185][0m |          -0.0088 |          95.8240 |          18.9188 |
[32m[20221213 21:10:32 @agent_ppo2.py:185][0m |           0.0041 |         105.6300 |          18.8985 |
[32m[20221213 21:10:32 @agent_ppo2.py:185][0m |          -0.0081 |          95.7284 |          18.9054 |
[32m[20221213 21:10:32 @agent_ppo2.py:185][0m |          -0.0093 |          95.2683 |          18.9051 |
[32m[20221213 21:10:32 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:10:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 617.00
[32m[20221213 21:10:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 665.00
[32m[20221213 21:10:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.00
[32m[20221213 21:10:32 @agent_ppo2.py:143][0m Total time:      14.96 min
[32m[20221213 21:10:32 @agent_ppo2.py:145][0m 1454080 total steps have happened
[32m[20221213 21:10:32 @agent_ppo2.py:121][0m #------------------------ Iteration 710 --------------------------#
[32m[20221213 21:10:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:32 @agent_ppo2.py:185][0m |          -0.0031 |          98.8019 |          19.0607 |
[32m[20221213 21:10:33 @agent_ppo2.py:185][0m |          -0.0087 |          97.5337 |          19.0563 |
[32m[20221213 21:10:33 @agent_ppo2.py:185][0m |          -0.0057 |          96.7279 |          19.0586 |
[32m[20221213 21:10:33 @agent_ppo2.py:185][0m |          -0.0082 |          96.3211 |          19.0693 |
[32m[20221213 21:10:33 @agent_ppo2.py:185][0m |          -0.0037 |          96.7382 |          19.0646 |
[32m[20221213 21:10:33 @agent_ppo2.py:185][0m |          -0.0090 |          95.5605 |          19.0712 |
[32m[20221213 21:10:33 @agent_ppo2.py:185][0m |          -0.0087 |          95.3293 |          19.0680 |
[32m[20221213 21:10:33 @agent_ppo2.py:185][0m |          -0.0101 |          95.0291 |          19.0762 |
[32m[20221213 21:10:33 @agent_ppo2.py:185][0m |          -0.0083 |          94.8984 |          19.0619 |
[32m[20221213 21:10:33 @agent_ppo2.py:185][0m |          -0.0107 |          94.8933 |          19.0752 |
[32m[20221213 21:10:33 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 21:10:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.80
[32m[20221213 21:10:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 608.00
[32m[20221213 21:10:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 520.00
[32m[20221213 21:10:34 @agent_ppo2.py:143][0m Total time:      14.98 min
[32m[20221213 21:10:34 @agent_ppo2.py:145][0m 1456128 total steps have happened
[32m[20221213 21:10:34 @agent_ppo2.py:121][0m #------------------------ Iteration 711 --------------------------#
[32m[20221213 21:10:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:10:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:34 @agent_ppo2.py:185][0m |          -0.0005 |          99.7559 |          18.9768 |
[32m[20221213 21:10:34 @agent_ppo2.py:185][0m |           0.0049 |         102.1767 |          18.9833 |
[32m[20221213 21:10:34 @agent_ppo2.py:185][0m |          -0.0058 |          97.3594 |          18.9664 |
[32m[20221213 21:10:34 @agent_ppo2.py:185][0m |          -0.0025 |          97.5461 |          18.9673 |
[32m[20221213 21:10:34 @agent_ppo2.py:185][0m |          -0.0063 |          96.2772 |          18.9672 |
[32m[20221213 21:10:34 @agent_ppo2.py:185][0m |           0.0236 |         116.9808 |          18.9624 |
[32m[20221213 21:10:34 @agent_ppo2.py:185][0m |          -0.0063 |          96.0756 |          18.9709 |
[32m[20221213 21:10:35 @agent_ppo2.py:185][0m |          -0.0054 |          95.4837 |          18.9677 |
[32m[20221213 21:10:35 @agent_ppo2.py:185][0m |          -0.0114 |          95.2981 |          18.9740 |
[32m[20221213 21:10:35 @agent_ppo2.py:185][0m |          -0.0110 |          95.2699 |          18.9803 |
[32m[20221213 21:10:35 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:10:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 560.60
[32m[20221213 21:10:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 575.00
[32m[20221213 21:10:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 606.00
[32m[20221213 21:10:35 @agent_ppo2.py:143][0m Total time:      15.00 min
[32m[20221213 21:10:35 @agent_ppo2.py:145][0m 1458176 total steps have happened
[32m[20221213 21:10:35 @agent_ppo2.py:121][0m #------------------------ Iteration 712 --------------------------#
[32m[20221213 21:10:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:10:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:35 @agent_ppo2.py:185][0m |          -0.0020 |          98.1700 |          18.9303 |
[32m[20221213 21:10:35 @agent_ppo2.py:185][0m |           0.0002 |          98.2089 |          18.9361 |
[32m[20221213 21:10:35 @agent_ppo2.py:185][0m |          -0.0057 |          96.7633 |          18.9216 |
[32m[20221213 21:10:35 @agent_ppo2.py:185][0m |          -0.0051 |          96.4077 |          18.9316 |
[32m[20221213 21:10:36 @agent_ppo2.py:185][0m |           0.0006 |         100.9813 |          18.9351 |
[32m[20221213 21:10:36 @agent_ppo2.py:185][0m |          -0.0044 |          96.0141 |          18.9303 |
[32m[20221213 21:10:36 @agent_ppo2.py:185][0m |          -0.0071 |          95.9164 |          18.9383 |
[32m[20221213 21:10:36 @agent_ppo2.py:185][0m |          -0.0080 |          95.7407 |          18.9376 |
[32m[20221213 21:10:36 @agent_ppo2.py:185][0m |          -0.0000 |          98.9428 |          18.9399 |
[32m[20221213 21:10:36 @agent_ppo2.py:185][0m |          -0.0065 |          95.4520 |          18.9445 |
[32m[20221213 21:10:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:10:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 612.20
[32m[20221213 21:10:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 636.00
[32m[20221213 21:10:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.00
[32m[20221213 21:10:36 @agent_ppo2.py:143][0m Total time:      15.02 min
[32m[20221213 21:10:36 @agent_ppo2.py:145][0m 1460224 total steps have happened
[32m[20221213 21:10:36 @agent_ppo2.py:121][0m #------------------------ Iteration 713 --------------------------#
[32m[20221213 21:10:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:36 @agent_ppo2.py:185][0m |           0.0009 |         100.1905 |          19.0429 |
[32m[20221213 21:10:36 @agent_ppo2.py:185][0m |           0.0061 |         105.1145 |          19.0262 |
[32m[20221213 21:10:37 @agent_ppo2.py:185][0m |          -0.0060 |          97.2610 |          19.0392 |
[32m[20221213 21:10:37 @agent_ppo2.py:185][0m |          -0.0056 |          96.4198 |          19.0263 |
[32m[20221213 21:10:37 @agent_ppo2.py:185][0m |          -0.0096 |          96.0910 |          19.0230 |
[32m[20221213 21:10:37 @agent_ppo2.py:185][0m |          -0.0084 |          95.5062 |          19.0219 |
[32m[20221213 21:10:37 @agent_ppo2.py:185][0m |          -0.0090 |          95.2193 |          19.0297 |
[32m[20221213 21:10:37 @agent_ppo2.py:185][0m |          -0.0079 |          94.9918 |          19.0286 |
[32m[20221213 21:10:37 @agent_ppo2.py:185][0m |          -0.0102 |          94.7027 |          19.0259 |
[32m[20221213 21:10:37 @agent_ppo2.py:185][0m |          -0.0084 |          94.7125 |          19.0221 |
[32m[20221213 21:10:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:10:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 577.20
[32m[20221213 21:10:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 617.00
[32m[20221213 21:10:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 575.00
[32m[20221213 21:10:37 @agent_ppo2.py:143][0m Total time:      15.04 min
[32m[20221213 21:10:37 @agent_ppo2.py:145][0m 1462272 total steps have happened
[32m[20221213 21:10:37 @agent_ppo2.py:121][0m #------------------------ Iteration 714 --------------------------#
[32m[20221213 21:10:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:38 @agent_ppo2.py:185][0m |          -0.0017 |         100.5008 |          18.8948 |
[32m[20221213 21:10:38 @agent_ppo2.py:185][0m |           0.0012 |         103.0157 |          18.8946 |
[32m[20221213 21:10:38 @agent_ppo2.py:185][0m |          -0.0084 |          99.3720 |          18.8903 |
[32m[20221213 21:10:38 @agent_ppo2.py:185][0m |          -0.0058 |          99.0167 |          18.9019 |
[32m[20221213 21:10:38 @agent_ppo2.py:185][0m |          -0.0091 |          98.5989 |          18.8974 |
[32m[20221213 21:10:38 @agent_ppo2.py:185][0m |          -0.0042 |          99.3236 |          18.9130 |
[32m[20221213 21:10:38 @agent_ppo2.py:185][0m |          -0.0089 |          98.5465 |          18.9148 |
[32m[20221213 21:10:38 @agent_ppo2.py:185][0m |          -0.0083 |          98.1078 |          18.9215 |
[32m[20221213 21:10:38 @agent_ppo2.py:185][0m |          -0.0103 |          98.1155 |          18.9199 |
[32m[20221213 21:10:38 @agent_ppo2.py:185][0m |          -0.0067 |          98.2881 |          18.9303 |
[32m[20221213 21:10:38 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:10:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 587.80
[32m[20221213 21:10:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 644.00
[32m[20221213 21:10:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.00
[32m[20221213 21:10:39 @agent_ppo2.py:143][0m Total time:      15.06 min
[32m[20221213 21:10:39 @agent_ppo2.py:145][0m 1464320 total steps have happened
[32m[20221213 21:10:39 @agent_ppo2.py:121][0m #------------------------ Iteration 715 --------------------------#
[32m[20221213 21:10:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:10:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:39 @agent_ppo2.py:185][0m |           0.0006 |          98.3057 |          18.9615 |
[32m[20221213 21:10:39 @agent_ppo2.py:185][0m |          -0.0029 |          95.4343 |          18.9538 |
[32m[20221213 21:10:39 @agent_ppo2.py:185][0m |          -0.0034 |          94.5474 |          18.9548 |
[32m[20221213 21:10:39 @agent_ppo2.py:185][0m |           0.0042 |          99.0974 |          18.9408 |
[32m[20221213 21:10:39 @agent_ppo2.py:185][0m |           0.0020 |          94.5609 |          18.9364 |
[32m[20221213 21:10:39 @agent_ppo2.py:185][0m |          -0.0052 |          93.4636 |          18.9298 |
[32m[20221213 21:10:39 @agent_ppo2.py:185][0m |          -0.0072 |          93.2773 |          18.9265 |
[32m[20221213 21:10:39 @agent_ppo2.py:185][0m |          -0.0037 |          93.4610 |          18.9259 |
[32m[20221213 21:10:40 @agent_ppo2.py:185][0m |          -0.0022 |          93.3485 |          18.9218 |
[32m[20221213 21:10:40 @agent_ppo2.py:185][0m |          -0.0059 |          92.3678 |          18.9126 |
[32m[20221213 21:10:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:10:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.40
[32m[20221213 21:10:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 554.00
[32m[20221213 21:10:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 623.00
[32m[20221213 21:10:40 @agent_ppo2.py:143][0m Total time:      15.08 min
[32m[20221213 21:10:40 @agent_ppo2.py:145][0m 1466368 total steps have happened
[32m[20221213 21:10:40 @agent_ppo2.py:121][0m #------------------------ Iteration 716 --------------------------#
[32m[20221213 21:10:40 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:10:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:40 @agent_ppo2.py:185][0m |          -0.0011 |         100.4180 |          19.0668 |
[32m[20221213 21:10:40 @agent_ppo2.py:185][0m |           0.0004 |         100.2048 |          19.0659 |
[32m[20221213 21:10:40 @agent_ppo2.py:185][0m |          -0.0045 |          99.3453 |          19.0544 |
[32m[20221213 21:10:40 @agent_ppo2.py:185][0m |          -0.0044 |          99.0368 |          19.0533 |
[32m[20221213 21:10:40 @agent_ppo2.py:185][0m |          -0.0061 |          98.8022 |          19.0447 |
[32m[20221213 21:10:40 @agent_ppo2.py:185][0m |           0.0069 |         111.0226 |          19.0480 |
[32m[20221213 21:10:41 @agent_ppo2.py:185][0m |          -0.0039 |          98.3653 |          19.0166 |
[32m[20221213 21:10:41 @agent_ppo2.py:185][0m |          -0.0053 |         100.4685 |          19.0291 |
[32m[20221213 21:10:41 @agent_ppo2.py:185][0m |          -0.0058 |          97.8491 |          19.0366 |
[32m[20221213 21:10:41 @agent_ppo2.py:185][0m |          -0.0053 |          97.9031 |          19.0454 |
[32m[20221213 21:10:41 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:10:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 563.60
[32m[20221213 21:10:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 608.00
[32m[20221213 21:10:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 202.00
[32m[20221213 21:10:41 @agent_ppo2.py:143][0m Total time:      15.10 min
[32m[20221213 21:10:41 @agent_ppo2.py:145][0m 1468416 total steps have happened
[32m[20221213 21:10:41 @agent_ppo2.py:121][0m #------------------------ Iteration 717 --------------------------#
[32m[20221213 21:10:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:41 @agent_ppo2.py:185][0m |          -0.0038 |         100.2597 |          19.0854 |
[32m[20221213 21:10:41 @agent_ppo2.py:185][0m |          -0.0053 |          98.8946 |          19.0696 |
[32m[20221213 21:10:41 @agent_ppo2.py:185][0m |          -0.0078 |          98.0517 |          19.0789 |
[32m[20221213 21:10:42 @agent_ppo2.py:185][0m |          -0.0026 |          98.2134 |          19.0686 |
[32m[20221213 21:10:42 @agent_ppo2.py:185][0m |          -0.0084 |          97.0968 |          19.0686 |
[32m[20221213 21:10:42 @agent_ppo2.py:185][0m |          -0.0087 |          96.5293 |          19.0615 |
[32m[20221213 21:10:42 @agent_ppo2.py:185][0m |          -0.0016 |         101.0726 |          19.0698 |
[32m[20221213 21:10:42 @agent_ppo2.py:185][0m |          -0.0058 |          96.6896 |          19.0665 |
[32m[20221213 21:10:42 @agent_ppo2.py:185][0m |          -0.0126 |          95.3506 |          19.0569 |
[32m[20221213 21:10:42 @agent_ppo2.py:185][0m |          -0.0107 |          95.0210 |          19.0557 |
[32m[20221213 21:10:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:10:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 592.60
[32m[20221213 21:10:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 638.00
[32m[20221213 21:10:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 667.00
[32m[20221213 21:10:42 @agent_ppo2.py:143][0m Total time:      15.12 min
[32m[20221213 21:10:42 @agent_ppo2.py:145][0m 1470464 total steps have happened
[32m[20221213 21:10:42 @agent_ppo2.py:121][0m #------------------------ Iteration 718 --------------------------#
[32m[20221213 21:10:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:42 @agent_ppo2.py:185][0m |          -0.0030 |          99.9588 |          19.0575 |
[32m[20221213 21:10:43 @agent_ppo2.py:185][0m |          -0.0036 |          98.8249 |          19.0375 |
[32m[20221213 21:10:43 @agent_ppo2.py:185][0m |           0.0016 |         102.2211 |          19.0287 |
[32m[20221213 21:10:43 @agent_ppo2.py:185][0m |          -0.0060 |          98.2410 |          19.0200 |
[32m[20221213 21:10:43 @agent_ppo2.py:185][0m |          -0.0071 |          97.7873 |          19.0177 |
[32m[20221213 21:10:43 @agent_ppo2.py:185][0m |          -0.0066 |          97.5763 |          19.0178 |
[32m[20221213 21:10:43 @agent_ppo2.py:185][0m |          -0.0054 |          97.3784 |          19.0171 |
[32m[20221213 21:10:43 @agent_ppo2.py:185][0m |          -0.0078 |          97.1509 |          19.0068 |
[32m[20221213 21:10:43 @agent_ppo2.py:185][0m |          -0.0073 |          97.1556 |          18.9988 |
[32m[20221213 21:10:43 @agent_ppo2.py:185][0m |          -0.0081 |          96.9908 |          18.9984 |
[32m[20221213 21:10:43 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:10:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.60
[32m[20221213 21:10:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 625.00
[32m[20221213 21:10:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 571.00
[32m[20221213 21:10:43 @agent_ppo2.py:143][0m Total time:      15.14 min
[32m[20221213 21:10:43 @agent_ppo2.py:145][0m 1472512 total steps have happened
[32m[20221213 21:10:43 @agent_ppo2.py:121][0m #------------------------ Iteration 719 --------------------------#
[32m[20221213 21:10:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:44 @agent_ppo2.py:185][0m |           0.0063 |         105.4738 |          19.0724 |
[32m[20221213 21:10:44 @agent_ppo2.py:185][0m |          -0.0042 |          97.2859 |          19.0618 |
[32m[20221213 21:10:44 @agent_ppo2.py:185][0m |          -0.0076 |          97.0390 |          19.0621 |
[32m[20221213 21:10:44 @agent_ppo2.py:185][0m |          -0.0057 |          96.6003 |          19.0533 |
[32m[20221213 21:10:44 @agent_ppo2.py:185][0m |          -0.0052 |          96.3633 |          19.0545 |
[32m[20221213 21:10:44 @agent_ppo2.py:185][0m |          -0.0082 |          96.1389 |          19.0515 |
[32m[20221213 21:10:44 @agent_ppo2.py:185][0m |          -0.0090 |          95.9799 |          19.0565 |
[32m[20221213 21:10:44 @agent_ppo2.py:185][0m |          -0.0066 |          96.0308 |          19.0558 |
[32m[20221213 21:10:44 @agent_ppo2.py:185][0m |          -0.0058 |          96.9510 |          19.0650 |
[32m[20221213 21:10:44 @agent_ppo2.py:185][0m |          -0.0107 |          95.5565 |          19.0603 |
[32m[20221213 21:10:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:10:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 571.00
[32m[20221213 21:10:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.00
[32m[20221213 21:10:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 559.00
[32m[20221213 21:10:45 @agent_ppo2.py:143][0m Total time:      15.16 min
[32m[20221213 21:10:45 @agent_ppo2.py:145][0m 1474560 total steps have happened
[32m[20221213 21:10:45 @agent_ppo2.py:121][0m #------------------------ Iteration 720 --------------------------#
[32m[20221213 21:10:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:45 @agent_ppo2.py:185][0m |          -0.0008 |         100.2193 |          18.9254 |
[32m[20221213 21:10:45 @agent_ppo2.py:185][0m |          -0.0030 |          98.9348 |          18.9245 |
[32m[20221213 21:10:45 @agent_ppo2.py:185][0m |          -0.0069 |          98.5401 |          18.9145 |
[32m[20221213 21:10:45 @agent_ppo2.py:185][0m |          -0.0061 |          97.8939 |          18.9319 |
[32m[20221213 21:10:45 @agent_ppo2.py:185][0m |          -0.0078 |          97.6440 |          18.9213 |
[32m[20221213 21:10:45 @agent_ppo2.py:185][0m |          -0.0066 |          97.3639 |          18.9404 |
[32m[20221213 21:10:45 @agent_ppo2.py:185][0m |          -0.0074 |          97.0630 |          18.9375 |
[32m[20221213 21:10:45 @agent_ppo2.py:185][0m |          -0.0056 |          96.8310 |          18.9441 |
[32m[20221213 21:10:46 @agent_ppo2.py:185][0m |          -0.0115 |          96.6311 |          18.9500 |
[32m[20221213 21:10:46 @agent_ppo2.py:185][0m |          -0.0067 |          96.5080 |          18.9590 |
[32m[20221213 21:10:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:10:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 568.60
[32m[20221213 21:10:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 617.00
[32m[20221213 21:10:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 633.00
[32m[20221213 21:10:46 @agent_ppo2.py:143][0m Total time:      15.18 min
[32m[20221213 21:10:46 @agent_ppo2.py:145][0m 1476608 total steps have happened
[32m[20221213 21:10:46 @agent_ppo2.py:121][0m #------------------------ Iteration 721 --------------------------#
[32m[20221213 21:10:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:46 @agent_ppo2.py:185][0m |          -0.0005 |          99.8357 |          18.9990 |
[32m[20221213 21:10:46 @agent_ppo2.py:185][0m |          -0.0046 |          99.0259 |          18.9831 |
[32m[20221213 21:10:46 @agent_ppo2.py:185][0m |          -0.0027 |          98.4627 |          18.9792 |
[32m[20221213 21:10:46 @agent_ppo2.py:185][0m |          -0.0052 |          97.7465 |          18.9828 |
[32m[20221213 21:10:46 @agent_ppo2.py:185][0m |          -0.0064 |          97.4921 |          18.9818 |
[32m[20221213 21:10:46 @agent_ppo2.py:185][0m |          -0.0083 |          96.9104 |          18.9811 |
[32m[20221213 21:10:47 @agent_ppo2.py:185][0m |          -0.0088 |          96.7295 |          18.9776 |
[32m[20221213 21:10:47 @agent_ppo2.py:185][0m |          -0.0073 |          96.4267 |          18.9810 |
[32m[20221213 21:10:47 @agent_ppo2.py:185][0m |          -0.0045 |          96.4948 |          18.9832 |
[32m[20221213 21:10:47 @agent_ppo2.py:185][0m |          -0.0077 |          96.0556 |          18.9895 |
[32m[20221213 21:10:47 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:10:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.00
[32m[20221213 21:10:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 670.00
[32m[20221213 21:10:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.00
[32m[20221213 21:10:47 @agent_ppo2.py:143][0m Total time:      15.20 min
[32m[20221213 21:10:47 @agent_ppo2.py:145][0m 1478656 total steps have happened
[32m[20221213 21:10:47 @agent_ppo2.py:121][0m #------------------------ Iteration 722 --------------------------#
[32m[20221213 21:10:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:47 @agent_ppo2.py:185][0m |           0.0035 |         101.4052 |          19.1272 |
[32m[20221213 21:10:47 @agent_ppo2.py:185][0m |          -0.0045 |          97.6368 |          19.1050 |
[32m[20221213 21:10:47 @agent_ppo2.py:185][0m |           0.0030 |         101.5893 |          19.1121 |
[32m[20221213 21:10:48 @agent_ppo2.py:185][0m |          -0.0024 |          96.4761 |          19.1002 |
[32m[20221213 21:10:48 @agent_ppo2.py:185][0m |          -0.0069 |          95.6294 |          19.0962 |
[32m[20221213 21:10:48 @agent_ppo2.py:185][0m |          -0.0085 |          95.1702 |          19.0949 |
[32m[20221213 21:10:48 @agent_ppo2.py:185][0m |          -0.0038 |          94.9527 |          19.0880 |
[32m[20221213 21:10:48 @agent_ppo2.py:185][0m |          -0.0060 |          94.1385 |          19.0820 |
[32m[20221213 21:10:48 @agent_ppo2.py:185][0m |          -0.0062 |          94.1340 |          19.0838 |
[32m[20221213 21:10:48 @agent_ppo2.py:185][0m |          -0.0071 |          93.8905 |          19.0777 |
[32m[20221213 21:10:48 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:10:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 561.00
[32m[20221213 21:10:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 606.00
[32m[20221213 21:10:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 676.00
[32m[20221213 21:10:48 @agent_ppo2.py:143][0m Total time:      15.22 min
[32m[20221213 21:10:48 @agent_ppo2.py:145][0m 1480704 total steps have happened
[32m[20221213 21:10:48 @agent_ppo2.py:121][0m #------------------------ Iteration 723 --------------------------#
[32m[20221213 21:10:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:49 @agent_ppo2.py:185][0m |          -0.0026 |          98.9582 |          19.0443 |
[32m[20221213 21:10:49 @agent_ppo2.py:185][0m |          -0.0040 |          96.1439 |          19.0232 |
[32m[20221213 21:10:49 @agent_ppo2.py:185][0m |          -0.0036 |          94.9724 |          19.0104 |
[32m[20221213 21:10:49 @agent_ppo2.py:185][0m |          -0.0073 |          94.6442 |          18.9994 |
[32m[20221213 21:10:49 @agent_ppo2.py:185][0m |          -0.0074 |          94.2353 |          18.9785 |
[32m[20221213 21:10:49 @agent_ppo2.py:185][0m |          -0.0110 |          93.8995 |          18.9707 |
[32m[20221213 21:10:49 @agent_ppo2.py:185][0m |          -0.0062 |          93.9367 |          18.9446 |
[32m[20221213 21:10:49 @agent_ppo2.py:185][0m |          -0.0101 |          93.4192 |          18.9452 |
[32m[20221213 21:10:49 @agent_ppo2.py:185][0m |          -0.0007 |         101.1441 |          18.9318 |
[32m[20221213 21:10:49 @agent_ppo2.py:185][0m |          -0.0131 |          92.9892 |          18.9185 |
[32m[20221213 21:10:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:10:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 558.40
[32m[20221213 21:10:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.00
[32m[20221213 21:10:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 609.00
[32m[20221213 21:10:49 @agent_ppo2.py:143][0m Total time:      15.25 min
[32m[20221213 21:10:49 @agent_ppo2.py:145][0m 1482752 total steps have happened
[32m[20221213 21:10:49 @agent_ppo2.py:121][0m #------------------------ Iteration 724 --------------------------#
[32m[20221213 21:10:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:50 @agent_ppo2.py:185][0m |          -0.0004 |          95.7404 |          18.9755 |
[32m[20221213 21:10:50 @agent_ppo2.py:185][0m |          -0.0030 |          95.0331 |          18.9749 |
[32m[20221213 21:10:50 @agent_ppo2.py:185][0m |          -0.0072 |          94.3582 |          18.9694 |
[32m[20221213 21:10:50 @agent_ppo2.py:185][0m |          -0.0050 |          94.1753 |          18.9729 |
[32m[20221213 21:10:50 @agent_ppo2.py:185][0m |          -0.0034 |          93.4935 |          18.9625 |
[32m[20221213 21:10:50 @agent_ppo2.py:185][0m |          -0.0062 |          93.3116 |          18.9582 |
[32m[20221213 21:10:50 @agent_ppo2.py:185][0m |          -0.0009 |          94.9238 |          18.9566 |
[32m[20221213 21:10:50 @agent_ppo2.py:185][0m |          -0.0057 |          92.7480 |          18.9557 |
[32m[20221213 21:10:50 @agent_ppo2.py:185][0m |          -0.0094 |          92.3747 |          18.9636 |
[32m[20221213 21:10:50 @agent_ppo2.py:185][0m |          -0.0048 |          92.2032 |          18.9508 |
[32m[20221213 21:10:50 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:10:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 577.80
[32m[20221213 21:10:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 631.00
[32m[20221213 21:10:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.00
[32m[20221213 21:10:51 @agent_ppo2.py:143][0m Total time:      15.27 min
[32m[20221213 21:10:51 @agent_ppo2.py:145][0m 1484800 total steps have happened
[32m[20221213 21:10:51 @agent_ppo2.py:121][0m #------------------------ Iteration 725 --------------------------#
[32m[20221213 21:10:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:10:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:51 @agent_ppo2.py:185][0m |           0.0075 |         111.5411 |          18.8673 |
[32m[20221213 21:10:51 @agent_ppo2.py:185][0m |          -0.0043 |          99.1715 |          18.8174 |
[32m[20221213 21:10:51 @agent_ppo2.py:185][0m |          -0.0042 |          98.4210 |          18.8493 |
[32m[20221213 21:10:51 @agent_ppo2.py:185][0m |          -0.0012 |          98.4062 |          18.8576 |
[32m[20221213 21:10:51 @agent_ppo2.py:185][0m |          -0.0068 |          97.3862 |          18.8445 |
[32m[20221213 21:10:51 @agent_ppo2.py:185][0m |          -0.0066 |          96.9038 |          18.8419 |
[32m[20221213 21:10:51 @agent_ppo2.py:185][0m |          -0.0039 |          96.9292 |          18.8382 |
[32m[20221213 21:10:52 @agent_ppo2.py:185][0m |          -0.0073 |          96.5120 |          18.8301 |
[32m[20221213 21:10:52 @agent_ppo2.py:185][0m |          -0.0113 |          96.4510 |          18.8345 |
[32m[20221213 21:10:52 @agent_ppo2.py:185][0m |          -0.0088 |          96.2178 |          18.8317 |
[32m[20221213 21:10:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:10:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.00
[32m[20221213 21:10:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 610.00
[32m[20221213 21:10:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 615.00
[32m[20221213 21:10:52 @agent_ppo2.py:143][0m Total time:      15.29 min
[32m[20221213 21:10:52 @agent_ppo2.py:145][0m 1486848 total steps have happened
[32m[20221213 21:10:52 @agent_ppo2.py:121][0m #------------------------ Iteration 726 --------------------------#
[32m[20221213 21:10:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:52 @agent_ppo2.py:185][0m |          -0.0008 |          98.4079 |          19.0401 |
[32m[20221213 21:10:52 @agent_ppo2.py:185][0m |          -0.0048 |          97.2202 |          19.0290 |
[32m[20221213 21:10:52 @agent_ppo2.py:185][0m |          -0.0027 |          96.6804 |          19.0267 |
[32m[20221213 21:10:52 @agent_ppo2.py:185][0m |          -0.0032 |          96.5974 |          19.0260 |
[32m[20221213 21:10:53 @agent_ppo2.py:185][0m |          -0.0042 |          96.2480 |          19.0215 |
[32m[20221213 21:10:53 @agent_ppo2.py:185][0m |          -0.0058 |          96.0169 |          19.0173 |
[32m[20221213 21:10:53 @agent_ppo2.py:185][0m |          -0.0082 |          95.9867 |          19.0154 |
[32m[20221213 21:10:53 @agent_ppo2.py:185][0m |          -0.0055 |          95.5178 |          19.0128 |
[32m[20221213 21:10:53 @agent_ppo2.py:185][0m |          -0.0054 |          97.2524 |          19.0140 |
[32m[20221213 21:10:53 @agent_ppo2.py:185][0m |          -0.0103 |          95.3405 |          19.0027 |
[32m[20221213 21:10:53 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:10:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.60
[32m[20221213 21:10:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 589.00
[32m[20221213 21:10:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.00
[32m[20221213 21:10:53 @agent_ppo2.py:143][0m Total time:      15.31 min
[32m[20221213 21:10:53 @agent_ppo2.py:145][0m 1488896 total steps have happened
[32m[20221213 21:10:53 @agent_ppo2.py:121][0m #------------------------ Iteration 727 --------------------------#
[32m[20221213 21:10:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:53 @agent_ppo2.py:185][0m |          -0.0000 |          97.0227 |          18.9332 |
[32m[20221213 21:10:53 @agent_ppo2.py:185][0m |          -0.0048 |          96.2585 |          18.9337 |
[32m[20221213 21:10:54 @agent_ppo2.py:185][0m |          -0.0054 |          95.7430 |          18.9234 |
[32m[20221213 21:10:54 @agent_ppo2.py:185][0m |          -0.0049 |          95.3819 |          18.9277 |
[32m[20221213 21:10:54 @agent_ppo2.py:185][0m |          -0.0021 |          95.5699 |          18.9311 |
[32m[20221213 21:10:54 @agent_ppo2.py:185][0m |          -0.0067 |          94.9506 |          18.9305 |
[32m[20221213 21:10:54 @agent_ppo2.py:185][0m |          -0.0051 |          94.7420 |          18.9305 |
[32m[20221213 21:10:54 @agent_ppo2.py:185][0m |          -0.0070 |          94.6947 |          18.9492 |
[32m[20221213 21:10:54 @agent_ppo2.py:185][0m |          -0.0057 |          94.5420 |          18.9521 |
[32m[20221213 21:10:54 @agent_ppo2.py:185][0m |          -0.0096 |          94.7720 |          18.9551 |
[32m[20221213 21:10:54 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:10:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.20
[32m[20221213 21:10:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 631.00
[32m[20221213 21:10:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 568.00
[32m[20221213 21:10:54 @agent_ppo2.py:143][0m Total time:      15.33 min
[32m[20221213 21:10:54 @agent_ppo2.py:145][0m 1490944 total steps have happened
[32m[20221213 21:10:54 @agent_ppo2.py:121][0m #------------------------ Iteration 728 --------------------------#
[32m[20221213 21:10:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:10:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:55 @agent_ppo2.py:185][0m |          -0.0034 |          97.3609 |          19.0380 |
[32m[20221213 21:10:55 @agent_ppo2.py:185][0m |          -0.0003 |          97.0955 |          19.0437 |
[32m[20221213 21:10:55 @agent_ppo2.py:185][0m |          -0.0039 |          96.3489 |          19.0382 |
[32m[20221213 21:10:55 @agent_ppo2.py:185][0m |          -0.0063 |          96.3217 |          19.0531 |
[32m[20221213 21:10:55 @agent_ppo2.py:185][0m |          -0.0056 |          96.0085 |          19.0639 |
[32m[20221213 21:10:55 @agent_ppo2.py:185][0m |          -0.0056 |          95.7356 |          19.0668 |
[32m[20221213 21:10:55 @agent_ppo2.py:185][0m |          -0.0026 |          96.7822 |          19.0693 |
[32m[20221213 21:10:55 @agent_ppo2.py:185][0m |          -0.0054 |          95.5244 |          19.0658 |
[32m[20221213 21:10:55 @agent_ppo2.py:185][0m |          -0.0072 |          95.3692 |          19.0784 |
[32m[20221213 21:10:55 @agent_ppo2.py:185][0m |          -0.0096 |          95.2621 |          19.0888 |
[32m[20221213 21:10:55 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:10:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.40
[32m[20221213 21:10:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 638.00
[32m[20221213 21:10:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 621.00
[32m[20221213 21:10:56 @agent_ppo2.py:143][0m Total time:      15.35 min
[32m[20221213 21:10:56 @agent_ppo2.py:145][0m 1492992 total steps have happened
[32m[20221213 21:10:56 @agent_ppo2.py:121][0m #------------------------ Iteration 729 --------------------------#
[32m[20221213 21:10:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:56 @agent_ppo2.py:185][0m |           0.0041 |         101.6455 |          18.9538 |
[32m[20221213 21:10:56 @agent_ppo2.py:185][0m |          -0.0010 |          97.9485 |          18.9381 |
[32m[20221213 21:10:56 @agent_ppo2.py:185][0m |          -0.0043 |          97.4551 |          18.9366 |
[32m[20221213 21:10:56 @agent_ppo2.py:185][0m |          -0.0026 |          97.1456 |          18.9408 |
[32m[20221213 21:10:56 @agent_ppo2.py:185][0m |          -0.0060 |          96.7879 |          18.9381 |
[32m[20221213 21:10:56 @agent_ppo2.py:185][0m |          -0.0065 |          96.4964 |          18.9293 |
[32m[20221213 21:10:56 @agent_ppo2.py:185][0m |          -0.0043 |          96.3444 |          18.9332 |
[32m[20221213 21:10:56 @agent_ppo2.py:185][0m |          -0.0072 |          96.0673 |          18.9277 |
[32m[20221213 21:10:57 @agent_ppo2.py:185][0m |          -0.0080 |          95.8127 |          18.9259 |
[32m[20221213 21:10:57 @agent_ppo2.py:185][0m |          -0.0103 |          95.9724 |          18.9353 |
[32m[20221213 21:10:57 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:10:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.60
[32m[20221213 21:10:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 581.00
[32m[20221213 21:10:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 596.00
[32m[20221213 21:10:57 @agent_ppo2.py:143][0m Total time:      15.37 min
[32m[20221213 21:10:57 @agent_ppo2.py:145][0m 1495040 total steps have happened
[32m[20221213 21:10:57 @agent_ppo2.py:121][0m #------------------------ Iteration 730 --------------------------#
[32m[20221213 21:10:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:57 @agent_ppo2.py:185][0m |           0.0064 |          98.0558 |          18.9052 |
[32m[20221213 21:10:57 @agent_ppo2.py:185][0m |          -0.0026 |          94.5798 |          18.9079 |
[32m[20221213 21:10:57 @agent_ppo2.py:185][0m |          -0.0060 |          94.4754 |          18.9068 |
[32m[20221213 21:10:57 @agent_ppo2.py:185][0m |          -0.0034 |          94.0730 |          18.9066 |
[32m[20221213 21:10:57 @agent_ppo2.py:185][0m |           0.0041 |          98.5555 |          18.9017 |
[32m[20221213 21:10:58 @agent_ppo2.py:185][0m |          -0.0065 |          93.6667 |          18.9051 |
[32m[20221213 21:10:58 @agent_ppo2.py:185][0m |           0.0060 |         102.9275 |          18.9091 |
[32m[20221213 21:10:58 @agent_ppo2.py:185][0m |          -0.0037 |          93.1591 |          18.9177 |
[32m[20221213 21:10:58 @agent_ppo2.py:185][0m |          -0.0052 |          93.1147 |          18.9168 |
[32m[20221213 21:10:58 @agent_ppo2.py:185][0m |          -0.0070 |          92.9287 |          18.9206 |
[32m[20221213 21:10:58 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:10:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 605.60
[32m[20221213 21:10:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 630.00
[32m[20221213 21:10:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 624.00
[32m[20221213 21:10:58 @agent_ppo2.py:143][0m Total time:      15.39 min
[32m[20221213 21:10:58 @agent_ppo2.py:145][0m 1497088 total steps have happened
[32m[20221213 21:10:58 @agent_ppo2.py:121][0m #------------------------ Iteration 731 --------------------------#
[32m[20221213 21:10:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:10:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:10:58 @agent_ppo2.py:185][0m |          -0.0032 |         100.6768 |          18.9171 |
[32m[20221213 21:10:58 @agent_ppo2.py:185][0m |          -0.0064 |          99.2172 |          18.9110 |
[32m[20221213 21:10:59 @agent_ppo2.py:185][0m |          -0.0058 |          98.6259 |          18.9048 |
[32m[20221213 21:10:59 @agent_ppo2.py:185][0m |          -0.0064 |          98.0032 |          18.8876 |
[32m[20221213 21:10:59 @agent_ppo2.py:185][0m |          -0.0034 |          98.9868 |          18.8914 |
[32m[20221213 21:10:59 @agent_ppo2.py:185][0m |          -0.0071 |          97.3008 |          18.8840 |
[32m[20221213 21:10:59 @agent_ppo2.py:185][0m |          -0.0070 |          96.9835 |          18.8858 |
[32m[20221213 21:10:59 @agent_ppo2.py:185][0m |          -0.0027 |          98.1709 |          18.8837 |
[32m[20221213 21:10:59 @agent_ppo2.py:185][0m |          -0.0082 |          96.6180 |          18.8711 |
[32m[20221213 21:10:59 @agent_ppo2.py:185][0m |          -0.0087 |          96.6036 |          18.8801 |
[32m[20221213 21:10:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:10:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 573.60
[32m[20221213 21:10:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 651.00
[32m[20221213 21:10:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 570.00
[32m[20221213 21:10:59 @agent_ppo2.py:143][0m Total time:      15.41 min
[32m[20221213 21:10:59 @agent_ppo2.py:145][0m 1499136 total steps have happened
[32m[20221213 21:10:59 @agent_ppo2.py:121][0m #------------------------ Iteration 732 --------------------------#
[32m[20221213 21:10:59 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:10:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:00 @agent_ppo2.py:185][0m |          -0.0023 |         100.2242 |          18.7683 |
[32m[20221213 21:11:00 @agent_ppo2.py:185][0m |          -0.0066 |          99.3746 |          18.7551 |
[32m[20221213 21:11:00 @agent_ppo2.py:185][0m |          -0.0049 |          98.8197 |          18.7424 |
[32m[20221213 21:11:00 @agent_ppo2.py:185][0m |          -0.0086 |          98.5891 |          18.7379 |
[32m[20221213 21:11:00 @agent_ppo2.py:185][0m |          -0.0058 |          98.2998 |          18.7553 |
[32m[20221213 21:11:00 @agent_ppo2.py:185][0m |          -0.0073 |          98.3604 |          18.7532 |
[32m[20221213 21:11:00 @agent_ppo2.py:185][0m |          -0.0070 |          98.1018 |          18.7573 |
[32m[20221213 21:11:00 @agent_ppo2.py:185][0m |          -0.0065 |          97.7351 |          18.7527 |
[32m[20221213 21:11:00 @agent_ppo2.py:185][0m |          -0.0089 |          97.5481 |          18.7462 |
[32m[20221213 21:11:00 @agent_ppo2.py:185][0m |          -0.0074 |          97.2665 |          18.7591 |
[32m[20221213 21:11:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:11:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 575.40
[32m[20221213 21:11:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 639.00
[32m[20221213 21:11:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:11:00 @agent_ppo2.py:143][0m Total time:      15.43 min
[32m[20221213 21:11:00 @agent_ppo2.py:145][0m 1501184 total steps have happened
[32m[20221213 21:11:00 @agent_ppo2.py:121][0m #------------------------ Iteration 733 --------------------------#
[32m[20221213 21:11:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:01 @agent_ppo2.py:185][0m |          -0.0046 |         100.3119 |          19.0258 |
[32m[20221213 21:11:01 @agent_ppo2.py:185][0m |          -0.0029 |          99.1334 |          19.0125 |
[32m[20221213 21:11:01 @agent_ppo2.py:185][0m |          -0.0086 |          98.7640 |          19.0152 |
[32m[20221213 21:11:01 @agent_ppo2.py:185][0m |          -0.0052 |         100.2462 |          19.0190 |
[32m[20221213 21:11:01 @agent_ppo2.py:185][0m |          -0.0112 |          98.0747 |          19.0145 |
[32m[20221213 21:11:01 @agent_ppo2.py:185][0m |          -0.0106 |          97.8213 |          19.0267 |
[32m[20221213 21:11:01 @agent_ppo2.py:185][0m |          -0.0023 |         101.2358 |          19.0250 |
[32m[20221213 21:11:01 @agent_ppo2.py:185][0m |          -0.0093 |          97.3190 |          19.0249 |
[32m[20221213 21:11:01 @agent_ppo2.py:185][0m |          -0.0115 |          97.1992 |          19.0376 |
[32m[20221213 21:11:02 @agent_ppo2.py:185][0m |          -0.0108 |          96.9959 |          19.0362 |
[32m[20221213 21:11:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:11:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.60
[32m[20221213 21:11:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 598.00
[32m[20221213 21:11:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 632.00
[32m[20221213 21:11:02 @agent_ppo2.py:143][0m Total time:      15.45 min
[32m[20221213 21:11:02 @agent_ppo2.py:145][0m 1503232 total steps have happened
[32m[20221213 21:11:02 @agent_ppo2.py:121][0m #------------------------ Iteration 734 --------------------------#
[32m[20221213 21:11:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:02 @agent_ppo2.py:185][0m |           0.0020 |          97.9743 |          19.0294 |
[32m[20221213 21:11:02 @agent_ppo2.py:185][0m |          -0.0033 |          97.5103 |          19.0326 |
[32m[20221213 21:11:02 @agent_ppo2.py:185][0m |          -0.0031 |          97.2378 |          19.0239 |
[32m[20221213 21:11:02 @agent_ppo2.py:185][0m |          -0.0052 |          97.0662 |          19.0223 |
[32m[20221213 21:11:02 @agent_ppo2.py:185][0m |           0.0097 |         104.1584 |          19.0254 |
[32m[20221213 21:11:02 @agent_ppo2.py:185][0m |          -0.0068 |          96.6305 |          19.0177 |
[32m[20221213 21:11:02 @agent_ppo2.py:185][0m |          -0.0053 |          97.5921 |          19.0226 |
[32m[20221213 21:11:03 @agent_ppo2.py:185][0m |          -0.0059 |          96.3215 |          19.0272 |
[32m[20221213 21:11:03 @agent_ppo2.py:185][0m |          -0.0070 |          96.3434 |          19.0261 |
[32m[20221213 21:11:03 @agent_ppo2.py:185][0m |          -0.0045 |          96.9717 |          19.0297 |
[32m[20221213 21:11:03 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:11:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 582.00
[32m[20221213 21:11:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.00
[32m[20221213 21:11:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.00
[32m[20221213 21:11:03 @agent_ppo2.py:143][0m Total time:      15.47 min
[32m[20221213 21:11:03 @agent_ppo2.py:145][0m 1505280 total steps have happened
[32m[20221213 21:11:03 @agent_ppo2.py:121][0m #------------------------ Iteration 735 --------------------------#
[32m[20221213 21:11:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:03 @agent_ppo2.py:185][0m |           0.0019 |          97.9333 |          19.0718 |
[32m[20221213 21:11:03 @agent_ppo2.py:185][0m |           0.0049 |         100.4685 |          19.0658 |
[32m[20221213 21:11:03 @agent_ppo2.py:185][0m |          -0.0055 |          96.0008 |          19.0936 |
[32m[20221213 21:11:03 @agent_ppo2.py:185][0m |          -0.0063 |          95.9337 |          19.0893 |
[32m[20221213 21:11:03 @agent_ppo2.py:185][0m |          -0.0051 |          95.4778 |          19.0946 |
[32m[20221213 21:11:04 @agent_ppo2.py:185][0m |          -0.0091 |          95.2306 |          19.1011 |
[32m[20221213 21:11:04 @agent_ppo2.py:185][0m |          -0.0098 |          95.1463 |          19.0994 |
[32m[20221213 21:11:04 @agent_ppo2.py:185][0m |          -0.0060 |          95.0547 |          19.0995 |
[32m[20221213 21:11:04 @agent_ppo2.py:185][0m |          -0.0011 |         101.4406 |          19.1088 |
[32m[20221213 21:11:04 @agent_ppo2.py:185][0m |          -0.0085 |          94.8364 |          19.1046 |
[32m[20221213 21:11:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:11:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 593.20
[32m[20221213 21:11:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 616.00
[32m[20221213 21:11:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 652.00
[32m[20221213 21:11:04 @agent_ppo2.py:143][0m Total time:      15.49 min
[32m[20221213 21:11:04 @agent_ppo2.py:145][0m 1507328 total steps have happened
[32m[20221213 21:11:04 @agent_ppo2.py:121][0m #------------------------ Iteration 736 --------------------------#
[32m[20221213 21:11:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:04 @agent_ppo2.py:185][0m |          -0.0018 |          98.2094 |          19.0779 |
[32m[20221213 21:11:04 @agent_ppo2.py:185][0m |           0.0017 |          98.9149 |          19.0595 |
[32m[20221213 21:11:05 @agent_ppo2.py:185][0m |           0.0041 |          99.1200 |          19.0543 |
[32m[20221213 21:11:05 @agent_ppo2.py:185][0m |          -0.0001 |          98.7200 |          19.0433 |
[32m[20221213 21:11:05 @agent_ppo2.py:185][0m |          -0.0057 |          96.2099 |          19.0471 |
[32m[20221213 21:11:05 @agent_ppo2.py:185][0m |          -0.0053 |          95.9666 |          19.0333 |
[32m[20221213 21:11:05 @agent_ppo2.py:185][0m |           0.0027 |          99.9299 |          19.0437 |
[32m[20221213 21:11:05 @agent_ppo2.py:185][0m |          -0.0052 |          95.6504 |          19.0376 |
[32m[20221213 21:11:05 @agent_ppo2.py:185][0m |          -0.0045 |          95.3583 |          19.0290 |
[32m[20221213 21:11:05 @agent_ppo2.py:185][0m |          -0.0069 |          95.0200 |          19.0241 |
[32m[20221213 21:11:05 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:11:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 588.00
[32m[20221213 21:11:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 639.00
[32m[20221213 21:11:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 608.00
[32m[20221213 21:11:05 @agent_ppo2.py:143][0m Total time:      15.51 min
[32m[20221213 21:11:05 @agent_ppo2.py:145][0m 1509376 total steps have happened
[32m[20221213 21:11:05 @agent_ppo2.py:121][0m #------------------------ Iteration 737 --------------------------#
[32m[20221213 21:11:05 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:11:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:06 @agent_ppo2.py:185][0m |          -0.0000 |         101.5424 |          18.9672 |
[32m[20221213 21:11:06 @agent_ppo2.py:185][0m |          -0.0069 |         100.2400 |          18.9614 |
[32m[20221213 21:11:06 @agent_ppo2.py:185][0m |          -0.0074 |          99.4505 |          18.9517 |
[32m[20221213 21:11:06 @agent_ppo2.py:185][0m |          -0.0073 |          98.7994 |          18.9488 |
[32m[20221213 21:11:06 @agent_ppo2.py:185][0m |          -0.0094 |          98.4243 |          18.9440 |
[32m[20221213 21:11:06 @agent_ppo2.py:185][0m |          -0.0081 |          98.0151 |          18.9472 |
[32m[20221213 21:11:06 @agent_ppo2.py:185][0m |          -0.0100 |          97.7470 |          18.9512 |
[32m[20221213 21:11:06 @agent_ppo2.py:185][0m |          -0.0106 |          97.5022 |          18.9414 |
[32m[20221213 21:11:06 @agent_ppo2.py:185][0m |          -0.0108 |          97.1641 |          18.9395 |
[32m[20221213 21:11:06 @agent_ppo2.py:185][0m |          -0.0069 |          97.9496 |          18.9486 |
[32m[20221213 21:11:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:11:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 561.20
[32m[20221213 21:11:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 611.00
[32m[20221213 21:11:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 556.00
[32m[20221213 21:11:06 @agent_ppo2.py:143][0m Total time:      15.53 min
[32m[20221213 21:11:06 @agent_ppo2.py:145][0m 1511424 total steps have happened
[32m[20221213 21:11:06 @agent_ppo2.py:121][0m #------------------------ Iteration 738 --------------------------#
[32m[20221213 21:11:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:11:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:07 @agent_ppo2.py:185][0m |          -0.0041 |         100.1865 |          18.9141 |
[32m[20221213 21:11:07 @agent_ppo2.py:185][0m |          -0.0083 |          99.1177 |          18.9068 |
[32m[20221213 21:11:07 @agent_ppo2.py:185][0m |          -0.0084 |          98.4240 |          18.9021 |
[32m[20221213 21:11:07 @agent_ppo2.py:185][0m |          -0.0102 |          97.8402 |          18.9087 |
[32m[20221213 21:11:07 @agent_ppo2.py:185][0m |          -0.0082 |          97.6226 |          18.9034 |
[32m[20221213 21:11:07 @agent_ppo2.py:185][0m |          -0.0128 |          97.2361 |          18.9057 |
[32m[20221213 21:11:07 @agent_ppo2.py:185][0m |          -0.0115 |          97.1433 |          18.8796 |
[32m[20221213 21:11:07 @agent_ppo2.py:185][0m |          -0.0095 |          97.3005 |          18.8913 |
[32m[20221213 21:11:07 @agent_ppo2.py:185][0m |           0.0040 |         108.3732 |          18.8783 |
[32m[20221213 21:11:08 @agent_ppo2.py:185][0m |           0.0107 |         115.0847 |          18.8535 |
[32m[20221213 21:11:08 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:11:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.00
[32m[20221213 21:11:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 617.00
[32m[20221213 21:11:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 651.00
[32m[20221213 21:11:08 @agent_ppo2.py:143][0m Total time:      15.55 min
[32m[20221213 21:11:08 @agent_ppo2.py:145][0m 1513472 total steps have happened
[32m[20221213 21:11:08 @agent_ppo2.py:121][0m #------------------------ Iteration 739 --------------------------#
[32m[20221213 21:11:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:08 @agent_ppo2.py:185][0m |           0.0034 |         101.9490 |          18.9999 |
[32m[20221213 21:11:08 @agent_ppo2.py:185][0m |          -0.0063 |          99.5289 |          18.9812 |
[32m[20221213 21:11:08 @agent_ppo2.py:185][0m |          -0.0049 |          98.4627 |          18.9746 |
[32m[20221213 21:11:08 @agent_ppo2.py:185][0m |          -0.0058 |          97.7150 |          18.9731 |
[32m[20221213 21:11:08 @agent_ppo2.py:185][0m |          -0.0035 |          97.7836 |          18.9597 |
[32m[20221213 21:11:08 @agent_ppo2.py:185][0m |          -0.0076 |          96.9531 |          18.9647 |
[32m[20221213 21:11:09 @agent_ppo2.py:185][0m |          -0.0070 |          96.7996 |          18.9612 |
[32m[20221213 21:11:09 @agent_ppo2.py:185][0m |          -0.0079 |          96.5334 |          18.9510 |
[32m[20221213 21:11:09 @agent_ppo2.py:185][0m |          -0.0083 |          96.2350 |          18.9461 |
[32m[20221213 21:11:09 @agent_ppo2.py:185][0m |          -0.0073 |          96.1320 |          18.9524 |
[32m[20221213 21:11:09 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:11:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.00
[32m[20221213 21:11:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 640.00
[32m[20221213 21:11:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 647.00
[32m[20221213 21:11:09 @agent_ppo2.py:143][0m Total time:      15.57 min
[32m[20221213 21:11:09 @agent_ppo2.py:145][0m 1515520 total steps have happened
[32m[20221213 21:11:09 @agent_ppo2.py:121][0m #------------------------ Iteration 740 --------------------------#
[32m[20221213 21:11:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:09 @agent_ppo2.py:185][0m |          -0.0021 |         100.8769 |          18.9506 |
[32m[20221213 21:11:09 @agent_ppo2.py:185][0m |          -0.0052 |          94.9843 |          18.9339 |
[32m[20221213 21:11:09 @agent_ppo2.py:185][0m |          -0.0069 |          93.1259 |          18.9279 |
[32m[20221213 21:11:09 @agent_ppo2.py:185][0m |          -0.0093 |          91.7010 |          18.9210 |
[32m[20221213 21:11:10 @agent_ppo2.py:185][0m |          -0.0058 |          91.9705 |          18.9142 |
[32m[20221213 21:11:10 @agent_ppo2.py:185][0m |          -0.0110 |          90.4320 |          18.9134 |
[32m[20221213 21:11:10 @agent_ppo2.py:185][0m |          -0.0063 |          90.0864 |          18.9058 |
[32m[20221213 21:11:10 @agent_ppo2.py:185][0m |          -0.0063 |          89.9813 |          18.8989 |
[32m[20221213 21:11:10 @agent_ppo2.py:185][0m |          -0.0082 |          89.3678 |          18.8996 |
[32m[20221213 21:11:10 @agent_ppo2.py:185][0m |          -0.0092 |          89.2907 |          18.9074 |
[32m[20221213 21:11:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:11:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 598.60
[32m[20221213 21:11:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 646.00
[32m[20221213 21:11:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 675.00
[32m[20221213 21:11:10 @agent_ppo2.py:143][0m Total time:      15.59 min
[32m[20221213 21:11:10 @agent_ppo2.py:145][0m 1517568 total steps have happened
[32m[20221213 21:11:10 @agent_ppo2.py:121][0m #------------------------ Iteration 741 --------------------------#
[32m[20221213 21:11:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:11:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:10 @agent_ppo2.py:185][0m |          -0.0016 |         104.3093 |          19.0618 |
[32m[20221213 21:11:11 @agent_ppo2.py:185][0m |          -0.0045 |         102.5958 |          19.0470 |
[32m[20221213 21:11:11 @agent_ppo2.py:185][0m |           0.0062 |         111.5511 |          19.0476 |
[32m[20221213 21:11:11 @agent_ppo2.py:185][0m |           0.0068 |         115.2886 |          19.0470 |
[32m[20221213 21:11:11 @agent_ppo2.py:185][0m |          -0.0036 |         101.1637 |          19.0552 |
[32m[20221213 21:11:11 @agent_ppo2.py:185][0m |          -0.0070 |         100.9409 |          19.0552 |
[32m[20221213 21:11:11 @agent_ppo2.py:185][0m |          -0.0092 |         100.7897 |          19.0625 |
[32m[20221213 21:11:11 @agent_ppo2.py:185][0m |          -0.0079 |         100.5095 |          19.0696 |
[32m[20221213 21:11:11 @agent_ppo2.py:185][0m |          -0.0056 |         100.8398 |          19.0548 |
[32m[20221213 21:11:11 @agent_ppo2.py:185][0m |          -0.0075 |         100.0440 |          19.0806 |
[32m[20221213 21:11:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:11:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.40
[32m[20221213 21:11:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 608.00
[32m[20221213 21:11:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 623.00
[32m[20221213 21:11:11 @agent_ppo2.py:143][0m Total time:      15.61 min
[32m[20221213 21:11:11 @agent_ppo2.py:145][0m 1519616 total steps have happened
[32m[20221213 21:11:11 @agent_ppo2.py:121][0m #------------------------ Iteration 742 --------------------------#
[32m[20221213 21:11:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:12 @agent_ppo2.py:185][0m |          -0.0021 |         101.9227 |          18.9820 |
[32m[20221213 21:11:12 @agent_ppo2.py:185][0m |          -0.0028 |         100.6779 |          18.9753 |
[32m[20221213 21:11:12 @agent_ppo2.py:185][0m |           0.0051 |         105.3103 |          18.9734 |
[32m[20221213 21:11:12 @agent_ppo2.py:185][0m |          -0.0030 |          99.5029 |          18.9774 |
[32m[20221213 21:11:12 @agent_ppo2.py:185][0m |          -0.0052 |          98.9168 |          18.9743 |
[32m[20221213 21:11:12 @agent_ppo2.py:185][0m |          -0.0061 |          98.8035 |          18.9836 |
[32m[20221213 21:11:12 @agent_ppo2.py:185][0m |          -0.0009 |         100.8756 |          18.9690 |
[32m[20221213 21:11:12 @agent_ppo2.py:185][0m |          -0.0042 |          98.1237 |          18.9743 |
[32m[20221213 21:11:12 @agent_ppo2.py:185][0m |          -0.0061 |          97.8272 |          18.9755 |
[32m[20221213 21:11:12 @agent_ppo2.py:185][0m |           0.0039 |         104.7205 |          18.9731 |
[32m[20221213 21:11:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:11:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 587.00
[32m[20221213 21:11:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 640.00
[32m[20221213 21:11:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.00
[32m[20221213 21:11:13 @agent_ppo2.py:143][0m Total time:      15.63 min
[32m[20221213 21:11:13 @agent_ppo2.py:145][0m 1521664 total steps have happened
[32m[20221213 21:11:13 @agent_ppo2.py:121][0m #------------------------ Iteration 743 --------------------------#
[32m[20221213 21:11:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:13 @agent_ppo2.py:185][0m |           0.0019 |         102.6330 |          18.8282 |
[32m[20221213 21:11:13 @agent_ppo2.py:185][0m |           0.0032 |         103.9248 |          18.8232 |
[32m[20221213 21:11:13 @agent_ppo2.py:185][0m |          -0.0026 |         100.6797 |          18.8219 |
[32m[20221213 21:11:13 @agent_ppo2.py:185][0m |          -0.0054 |         100.1748 |          18.8112 |
[32m[20221213 21:11:13 @agent_ppo2.py:185][0m |          -0.0039 |          99.7591 |          18.7979 |
[32m[20221213 21:11:13 @agent_ppo2.py:185][0m |          -0.0035 |          99.5680 |          18.7919 |
[32m[20221213 21:11:13 @agent_ppo2.py:185][0m |           0.0032 |         103.6032 |          18.7892 |
[32m[20221213 21:11:13 @agent_ppo2.py:185][0m |          -0.0069 |          98.7648 |          18.7944 |
[32m[20221213 21:11:14 @agent_ppo2.py:185][0m |          -0.0084 |          98.5802 |          18.7795 |
[32m[20221213 21:11:14 @agent_ppo2.py:185][0m |          -0.0068 |          98.4766 |          18.7768 |
[32m[20221213 21:11:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:11:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 578.40
[32m[20221213 21:11:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.00
[32m[20221213 21:11:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.00
[32m[20221213 21:11:14 @agent_ppo2.py:143][0m Total time:      15.65 min
[32m[20221213 21:11:14 @agent_ppo2.py:145][0m 1523712 total steps have happened
[32m[20221213 21:11:14 @agent_ppo2.py:121][0m #------------------------ Iteration 744 --------------------------#
[32m[20221213 21:11:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:14 @agent_ppo2.py:185][0m |          -0.0023 |         105.1338 |          18.8660 |
[32m[20221213 21:11:14 @agent_ppo2.py:185][0m |          -0.0059 |         103.5140 |          18.8460 |
[32m[20221213 21:11:14 @agent_ppo2.py:185][0m |          -0.0097 |         102.6766 |          18.8519 |
[32m[20221213 21:11:14 @agent_ppo2.py:185][0m |          -0.0039 |         101.9291 |          18.8524 |
[32m[20221213 21:11:14 @agent_ppo2.py:185][0m |          -0.0066 |         101.7615 |          18.8491 |
[32m[20221213 21:11:14 @agent_ppo2.py:185][0m |          -0.0086 |         101.1813 |          18.8378 |
[32m[20221213 21:11:15 @agent_ppo2.py:185][0m |          -0.0104 |         101.0068 |          18.8455 |
[32m[20221213 21:11:15 @agent_ppo2.py:185][0m |          -0.0101 |         100.8091 |          18.8412 |
[32m[20221213 21:11:15 @agent_ppo2.py:185][0m |          -0.0103 |         100.4785 |          18.8447 |
[32m[20221213 21:11:15 @agent_ppo2.py:185][0m |          -0.0025 |         102.8346 |          18.8400 |
[32m[20221213 21:11:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:11:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 577.20
[32m[20221213 21:11:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 655.00
[32m[20221213 21:11:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 637.00
[32m[20221213 21:11:15 @agent_ppo2.py:143][0m Total time:      15.67 min
[32m[20221213 21:11:15 @agent_ppo2.py:145][0m 1525760 total steps have happened
[32m[20221213 21:11:15 @agent_ppo2.py:121][0m #------------------------ Iteration 745 --------------------------#
[32m[20221213 21:11:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:11:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:15 @agent_ppo2.py:185][0m |           0.0014 |         101.5961 |          18.9991 |
[32m[20221213 21:11:15 @agent_ppo2.py:185][0m |          -0.0055 |         100.6148 |          18.9766 |
[32m[20221213 21:11:15 @agent_ppo2.py:185][0m |          -0.0038 |         100.1704 |          18.9640 |
[32m[20221213 21:11:16 @agent_ppo2.py:185][0m |          -0.0051 |          99.8824 |          18.9603 |
[32m[20221213 21:11:16 @agent_ppo2.py:185][0m |          -0.0064 |          99.5615 |          18.9509 |
[32m[20221213 21:11:16 @agent_ppo2.py:185][0m |          -0.0073 |          99.4915 |          18.9519 |
[32m[20221213 21:11:16 @agent_ppo2.py:185][0m |          -0.0059 |         100.2416 |          18.9485 |
[32m[20221213 21:11:16 @agent_ppo2.py:185][0m |          -0.0062 |          99.2997 |          18.9434 |
[32m[20221213 21:11:16 @agent_ppo2.py:185][0m |          -0.0067 |          98.8081 |          18.9388 |
[32m[20221213 21:11:16 @agent_ppo2.py:185][0m |          -0.0089 |          98.6990 |          18.9426 |
[32m[20221213 21:11:16 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221213 21:11:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 587.80
[32m[20221213 21:11:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.00
[32m[20221213 21:11:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 638.00
[32m[20221213 21:11:17 @agent_ppo2.py:143][0m Total time:      15.70 min
[32m[20221213 21:11:17 @agent_ppo2.py:145][0m 1527808 total steps have happened
[32m[20221213 21:11:17 @agent_ppo2.py:121][0m #------------------------ Iteration 746 --------------------------#
[32m[20221213 21:11:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 21:11:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:17 @agent_ppo2.py:185][0m |          -0.0034 |         100.7224 |          18.9520 |
[32m[20221213 21:11:17 @agent_ppo2.py:185][0m |          -0.0044 |          98.8163 |          18.9497 |
[32m[20221213 21:11:17 @agent_ppo2.py:185][0m |          -0.0013 |          98.7715 |          18.9559 |
[32m[20221213 21:11:17 @agent_ppo2.py:185][0m |          -0.0048 |          97.9488 |          18.9448 |
[32m[20221213 21:11:17 @agent_ppo2.py:185][0m |          -0.0043 |          97.7067 |          18.9494 |
[32m[20221213 21:11:17 @agent_ppo2.py:185][0m |          -0.0039 |          97.4614 |          18.9655 |
[32m[20221213 21:11:17 @agent_ppo2.py:185][0m |          -0.0060 |          97.1862 |          18.9559 |
[32m[20221213 21:11:17 @agent_ppo2.py:185][0m |          -0.0026 |          98.3039 |          18.9676 |
[32m[20221213 21:11:18 @agent_ppo2.py:185][0m |           0.0021 |         104.3307 |          18.9674 |
[32m[20221213 21:11:18 @agent_ppo2.py:185][0m |          -0.0065 |          96.6975 |          18.9708 |
[32m[20221213 21:11:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:11:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.20
[32m[20221213 21:11:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.00
[32m[20221213 21:11:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 585.00
[32m[20221213 21:11:18 @agent_ppo2.py:143][0m Total time:      15.72 min
[32m[20221213 21:11:18 @agent_ppo2.py:145][0m 1529856 total steps have happened
[32m[20221213 21:11:18 @agent_ppo2.py:121][0m #------------------------ Iteration 747 --------------------------#
[32m[20221213 21:11:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:18 @agent_ppo2.py:185][0m |          -0.0031 |         100.6157 |          19.0422 |
[32m[20221213 21:11:18 @agent_ppo2.py:185][0m |          -0.0065 |          99.8217 |          19.0353 |
[32m[20221213 21:11:18 @agent_ppo2.py:185][0m |          -0.0068 |          99.4830 |          19.0401 |
[32m[20221213 21:11:18 @agent_ppo2.py:185][0m |          -0.0036 |          99.6013 |          19.0394 |
[32m[20221213 21:11:18 @agent_ppo2.py:185][0m |          -0.0081 |          98.8265 |          19.0450 |
[32m[20221213 21:11:18 @agent_ppo2.py:185][0m |          -0.0096 |          98.5176 |          19.0424 |
[32m[20221213 21:11:19 @agent_ppo2.py:185][0m |           0.0025 |         108.2835 |          19.0479 |
[32m[20221213 21:11:19 @agent_ppo2.py:185][0m |          -0.0080 |          98.0336 |          19.0424 |
[32m[20221213 21:11:19 @agent_ppo2.py:185][0m |          -0.0110 |          97.6959 |          19.0535 |
[32m[20221213 21:11:19 @agent_ppo2.py:185][0m |          -0.0116 |          97.6159 |          19.0490 |
[32m[20221213 21:11:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:11:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 605.80
[32m[20221213 21:11:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 637.00
[32m[20221213 21:11:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 633.00
[32m[20221213 21:11:19 @agent_ppo2.py:143][0m Total time:      15.74 min
[32m[20221213 21:11:19 @agent_ppo2.py:145][0m 1531904 total steps have happened
[32m[20221213 21:11:19 @agent_ppo2.py:121][0m #------------------------ Iteration 748 --------------------------#
[32m[20221213 21:11:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:19 @agent_ppo2.py:185][0m |          -0.0030 |         103.9362 |          18.9309 |
[32m[20221213 21:11:19 @agent_ppo2.py:185][0m |          -0.0063 |         103.0621 |          18.9121 |
[32m[20221213 21:11:19 @agent_ppo2.py:185][0m |          -0.0080 |         102.0829 |          18.9177 |
[32m[20221213 21:11:20 @agent_ppo2.py:185][0m |          -0.0034 |         101.7991 |          18.9049 |
[32m[20221213 21:11:20 @agent_ppo2.py:185][0m |           0.0063 |         115.6420 |          18.9093 |
[32m[20221213 21:11:20 @agent_ppo2.py:185][0m |          -0.0081 |         100.7981 |          18.9122 |
[32m[20221213 21:11:20 @agent_ppo2.py:185][0m |          -0.0115 |         100.4331 |          18.9185 |
[32m[20221213 21:11:20 @agent_ppo2.py:185][0m |          -0.0089 |         100.0324 |          18.9235 |
[32m[20221213 21:11:20 @agent_ppo2.py:185][0m |          -0.0081 |          99.6847 |          18.9361 |
[32m[20221213 21:11:20 @agent_ppo2.py:185][0m |          -0.0113 |          99.4676 |          18.9282 |
[32m[20221213 21:11:20 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:11:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 579.80
[32m[20221213 21:11:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 624.00
[32m[20221213 21:11:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 595.00
[32m[20221213 21:11:20 @agent_ppo2.py:143][0m Total time:      15.76 min
[32m[20221213 21:11:20 @agent_ppo2.py:145][0m 1533952 total steps have happened
[32m[20221213 21:11:20 @agent_ppo2.py:121][0m #------------------------ Iteration 749 --------------------------#
[32m[20221213 21:11:20 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:11:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:20 @agent_ppo2.py:185][0m |           0.0116 |         115.0340 |          18.8060 |
[32m[20221213 21:11:21 @agent_ppo2.py:185][0m |          -0.0055 |         101.4904 |          18.7472 |
[32m[20221213 21:11:21 @agent_ppo2.py:185][0m |          -0.0064 |         100.8507 |          18.7559 |
[32m[20221213 21:11:21 @agent_ppo2.py:185][0m |          -0.0050 |         101.3954 |          18.7654 |
[32m[20221213 21:11:21 @agent_ppo2.py:185][0m |          -0.0103 |         100.2432 |          18.7603 |
[32m[20221213 21:11:21 @agent_ppo2.py:185][0m |          -0.0073 |          99.9178 |          18.7598 |
[32m[20221213 21:11:21 @agent_ppo2.py:185][0m |          -0.0080 |          99.6577 |          18.7571 |
[32m[20221213 21:11:21 @agent_ppo2.py:185][0m |          -0.0102 |          99.4778 |          18.7542 |
[32m[20221213 21:11:21 @agent_ppo2.py:185][0m |          -0.0055 |         101.0966 |          18.7571 |
[32m[20221213 21:11:21 @agent_ppo2.py:185][0m |          -0.0104 |          99.0553 |          18.7578 |
[32m[20221213 21:11:21 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:11:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.20
[32m[20221213 21:11:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 618.00
[32m[20221213 21:11:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 583.00
[32m[20221213 21:11:21 @agent_ppo2.py:143][0m Total time:      15.78 min
[32m[20221213 21:11:21 @agent_ppo2.py:145][0m 1536000 total steps have happened
[32m[20221213 21:11:21 @agent_ppo2.py:121][0m #------------------------ Iteration 750 --------------------------#
[32m[20221213 21:11:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:22 @agent_ppo2.py:185][0m |           0.0027 |         105.0093 |          18.9386 |
[32m[20221213 21:11:22 @agent_ppo2.py:185][0m |          -0.0055 |         103.8737 |          18.9286 |
[32m[20221213 21:11:22 @agent_ppo2.py:185][0m |          -0.0028 |         103.4289 |          18.9325 |
[32m[20221213 21:11:22 @agent_ppo2.py:185][0m |          -0.0014 |         103.4234 |          18.9248 |
[32m[20221213 21:11:22 @agent_ppo2.py:185][0m |          -0.0064 |         102.4883 |          18.9345 |
[32m[20221213 21:11:22 @agent_ppo2.py:185][0m |          -0.0052 |         102.2779 |          18.9287 |
[32m[20221213 21:11:22 @agent_ppo2.py:185][0m |          -0.0064 |         101.9933 |          18.9356 |
[32m[20221213 21:11:22 @agent_ppo2.py:185][0m |          -0.0055 |         101.6429 |          18.9293 |
[32m[20221213 21:11:22 @agent_ppo2.py:185][0m |          -0.0070 |         101.6490 |          18.9286 |
[32m[20221213 21:11:23 @agent_ppo2.py:185][0m |          -0.0087 |         101.4879 |          18.9289 |
[32m[20221213 21:11:23 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 21:11:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 617.60
[32m[20221213 21:11:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 660.00
[32m[20221213 21:11:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 578.00
[32m[20221213 21:11:23 @agent_ppo2.py:143][0m Total time:      15.80 min
[32m[20221213 21:11:23 @agent_ppo2.py:145][0m 1538048 total steps have happened
[32m[20221213 21:11:23 @agent_ppo2.py:121][0m #------------------------ Iteration 751 --------------------------#
[32m[20221213 21:11:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:11:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:23 @agent_ppo2.py:185][0m |           0.0083 |         111.7893 |          19.0471 |
[32m[20221213 21:11:23 @agent_ppo2.py:185][0m |          -0.0056 |         104.9072 |          19.0156 |
[32m[20221213 21:11:23 @agent_ppo2.py:185][0m |          -0.0042 |         104.0279 |          19.0035 |
[32m[20221213 21:11:23 @agent_ppo2.py:185][0m |          -0.0050 |         103.0534 |          18.9959 |
[32m[20221213 21:11:23 @agent_ppo2.py:185][0m |          -0.0061 |         102.4534 |          18.9870 |
[32m[20221213 21:11:23 @agent_ppo2.py:185][0m |          -0.0093 |         101.9108 |          18.9720 |
[32m[20221213 21:11:24 @agent_ppo2.py:185][0m |          -0.0076 |         101.5409 |          18.9607 |
[32m[20221213 21:11:24 @agent_ppo2.py:185][0m |          -0.0097 |         101.1124 |          18.9548 |
[32m[20221213 21:11:24 @agent_ppo2.py:185][0m |          -0.0070 |         101.0361 |          18.9589 |
[32m[20221213 21:11:24 @agent_ppo2.py:185][0m |          -0.0115 |         100.6861 |          18.9408 |
[32m[20221213 21:11:24 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:11:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 609.40
[32m[20221213 21:11:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 646.00
[32m[20221213 21:11:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 657.00
[32m[20221213 21:11:24 @agent_ppo2.py:143][0m Total time:      15.82 min
[32m[20221213 21:11:24 @agent_ppo2.py:145][0m 1540096 total steps have happened
[32m[20221213 21:11:24 @agent_ppo2.py:121][0m #------------------------ Iteration 752 --------------------------#
[32m[20221213 21:11:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:24 @agent_ppo2.py:185][0m |          -0.0008 |         107.2534 |          18.8251 |
[32m[20221213 21:11:24 @agent_ppo2.py:185][0m |          -0.0034 |         103.7756 |          18.8038 |
[32m[20221213 21:11:24 @agent_ppo2.py:185][0m |          -0.0063 |         102.2126 |          18.7903 |
[32m[20221213 21:11:25 @agent_ppo2.py:185][0m |          -0.0027 |         102.9797 |          18.7873 |
[32m[20221213 21:11:25 @agent_ppo2.py:185][0m |          -0.0060 |         100.7126 |          18.7794 |
[32m[20221213 21:11:25 @agent_ppo2.py:185][0m |          -0.0023 |         101.3346 |          18.7674 |
[32m[20221213 21:11:25 @agent_ppo2.py:185][0m |          -0.0094 |          99.7853 |          18.7515 |
[32m[20221213 21:11:25 @agent_ppo2.py:185][0m |          -0.0089 |          99.4649 |          18.7413 |
[32m[20221213 21:11:25 @agent_ppo2.py:185][0m |          -0.0055 |          99.1736 |          18.7504 |
[32m[20221213 21:11:25 @agent_ppo2.py:185][0m |          -0.0011 |          99.9576 |          18.7268 |
[32m[20221213 21:11:25 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:11:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 594.60
[32m[20221213 21:11:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 617.00
[32m[20221213 21:11:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.00
[32m[20221213 21:11:25 @agent_ppo2.py:143][0m Total time:      15.84 min
[32m[20221213 21:11:25 @agent_ppo2.py:145][0m 1542144 total steps have happened
[32m[20221213 21:11:25 @agent_ppo2.py:121][0m #------------------------ Iteration 753 --------------------------#
[32m[20221213 21:11:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:11:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:26 @agent_ppo2.py:185][0m |           0.0058 |         107.8483 |          18.9765 |
[32m[20221213 21:11:26 @agent_ppo2.py:185][0m |          -0.0057 |         103.8980 |          18.9750 |
[32m[20221213 21:11:26 @agent_ppo2.py:185][0m |          -0.0054 |         102.7098 |          18.9788 |
[32m[20221213 21:11:26 @agent_ppo2.py:185][0m |          -0.0080 |         101.7718 |          18.9791 |
[32m[20221213 21:11:26 @agent_ppo2.py:185][0m |          -0.0105 |         101.2510 |          18.9894 |
[32m[20221213 21:11:26 @agent_ppo2.py:185][0m |          -0.0067 |         100.2291 |          18.9889 |
[32m[20221213 21:11:26 @agent_ppo2.py:185][0m |          -0.0020 |         101.2307 |          18.9951 |
[32m[20221213 21:11:26 @agent_ppo2.py:185][0m |          -0.0128 |          99.1305 |          19.0060 |
[32m[20221213 21:11:26 @agent_ppo2.py:185][0m |          -0.0086 |          98.5459 |          19.0009 |
[32m[20221213 21:11:26 @agent_ppo2.py:185][0m |          -0.0093 |          97.7427 |          19.0074 |
[32m[20221213 21:11:26 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:11:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 589.40
[32m[20221213 21:11:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 689.00
[32m[20221213 21:11:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 630.00
[32m[20221213 21:11:26 @agent_ppo2.py:143][0m Total time:      15.86 min
[32m[20221213 21:11:26 @agent_ppo2.py:145][0m 1544192 total steps have happened
[32m[20221213 21:11:26 @agent_ppo2.py:121][0m #------------------------ Iteration 754 --------------------------#
[32m[20221213 21:11:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:11:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:27 @agent_ppo2.py:185][0m |           0.0037 |         108.7539 |          19.0013 |
[32m[20221213 21:11:27 @agent_ppo2.py:185][0m |           0.0008 |         105.7093 |          18.9944 |
[32m[20221213 21:11:27 @agent_ppo2.py:185][0m |          -0.0030 |         103.5096 |          18.9949 |
[32m[20221213 21:11:27 @agent_ppo2.py:185][0m |          -0.0086 |         102.9050 |          18.9904 |
[32m[20221213 21:11:27 @agent_ppo2.py:185][0m |          -0.0043 |         102.6563 |          18.9958 |
[32m[20221213 21:11:27 @agent_ppo2.py:185][0m |          -0.0062 |         102.3196 |          18.9984 |
[32m[20221213 21:11:27 @agent_ppo2.py:185][0m |          -0.0078 |         102.2667 |          18.9996 |
[32m[20221213 21:11:27 @agent_ppo2.py:185][0m |          -0.0105 |         102.0821 |          19.0007 |
[32m[20221213 21:11:28 @agent_ppo2.py:185][0m |          -0.0064 |         101.8359 |          19.0051 |
[32m[20221213 21:11:28 @agent_ppo2.py:185][0m |          -0.0076 |         101.7253 |          19.0112 |
[32m[20221213 21:11:28 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:11:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.40
[32m[20221213 21:11:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 695.00
[32m[20221213 21:11:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.00
[32m[20221213 21:11:28 @agent_ppo2.py:143][0m Total time:      15.88 min
[32m[20221213 21:11:28 @agent_ppo2.py:145][0m 1546240 total steps have happened
[32m[20221213 21:11:28 @agent_ppo2.py:121][0m #------------------------ Iteration 755 --------------------------#
[32m[20221213 21:11:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:28 @agent_ppo2.py:185][0m |          -0.0041 |         107.1388 |          18.8849 |
[32m[20221213 21:11:28 @agent_ppo2.py:185][0m |          -0.0063 |         104.3485 |          18.8754 |
[32m[20221213 21:11:28 @agent_ppo2.py:185][0m |          -0.0044 |         103.5677 |          18.8710 |
[32m[20221213 21:11:28 @agent_ppo2.py:185][0m |          -0.0081 |         103.1343 |          18.8673 |
[32m[20221213 21:11:28 @agent_ppo2.py:185][0m |          -0.0073 |         102.7297 |          18.8644 |
[32m[20221213 21:11:29 @agent_ppo2.py:185][0m |          -0.0081 |         102.2983 |          18.8648 |
[32m[20221213 21:11:29 @agent_ppo2.py:185][0m |          -0.0125 |         102.0305 |          18.8683 |
[32m[20221213 21:11:29 @agent_ppo2.py:185][0m |          -0.0130 |         101.7159 |          18.8673 |
[32m[20221213 21:11:29 @agent_ppo2.py:185][0m |          -0.0114 |         101.4288 |          18.8644 |
[32m[20221213 21:11:29 @agent_ppo2.py:185][0m |          -0.0117 |         101.4271 |          18.8600 |
[32m[20221213 21:11:29 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:11:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 561.80
[32m[20221213 21:11:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 677.00
[32m[20221213 21:11:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 564.00
[32m[20221213 21:11:29 @agent_ppo2.py:143][0m Total time:      15.91 min
[32m[20221213 21:11:29 @agent_ppo2.py:145][0m 1548288 total steps have happened
[32m[20221213 21:11:29 @agent_ppo2.py:121][0m #------------------------ Iteration 756 --------------------------#
[32m[20221213 21:11:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:29 @agent_ppo2.py:185][0m |          -0.0012 |         108.0084 |          19.0279 |
[32m[20221213 21:11:29 @agent_ppo2.py:185][0m |          -0.0053 |         106.6600 |          19.0145 |
[32m[20221213 21:11:30 @agent_ppo2.py:185][0m |          -0.0056 |         105.9043 |          18.9962 |
[32m[20221213 21:11:30 @agent_ppo2.py:185][0m |          -0.0053 |         105.4073 |          18.9898 |
[32m[20221213 21:11:30 @agent_ppo2.py:185][0m |          -0.0075 |         104.9715 |          18.9870 |
[32m[20221213 21:11:30 @agent_ppo2.py:185][0m |          -0.0064 |         104.4226 |          18.9887 |
[32m[20221213 21:11:30 @agent_ppo2.py:185][0m |          -0.0089 |         104.2265 |          18.9892 |
[32m[20221213 21:11:30 @agent_ppo2.py:185][0m |          -0.0105 |         104.0378 |          18.9814 |
[32m[20221213 21:11:30 @agent_ppo2.py:185][0m |          -0.0056 |         103.6299 |          18.9739 |
[32m[20221213 21:11:30 @agent_ppo2.py:185][0m |          -0.0002 |         109.2886 |          18.9775 |
[32m[20221213 21:11:30 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:11:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.40
[32m[20221213 21:11:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 606.00
[32m[20221213 21:11:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 626.00
[32m[20221213 21:11:30 @agent_ppo2.py:143][0m Total time:      15.93 min
[32m[20221213 21:11:30 @agent_ppo2.py:145][0m 1550336 total steps have happened
[32m[20221213 21:11:30 @agent_ppo2.py:121][0m #------------------------ Iteration 757 --------------------------#
[32m[20221213 21:11:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:31 @agent_ppo2.py:185][0m |           0.0011 |         108.5901 |          18.9917 |
[32m[20221213 21:11:31 @agent_ppo2.py:185][0m |          -0.0062 |         104.6252 |          18.9698 |
[32m[20221213 21:11:31 @agent_ppo2.py:185][0m |          -0.0054 |         103.2791 |          18.9789 |
[32m[20221213 21:11:31 @agent_ppo2.py:185][0m |          -0.0012 |         103.9671 |          18.9735 |
[32m[20221213 21:11:31 @agent_ppo2.py:185][0m |          -0.0068 |         102.5983 |          18.9773 |
[32m[20221213 21:11:31 @agent_ppo2.py:185][0m |          -0.0100 |         102.2461 |          18.9727 |
[32m[20221213 21:11:31 @agent_ppo2.py:185][0m |          -0.0085 |         101.8546 |          18.9745 |
[32m[20221213 21:11:31 @agent_ppo2.py:185][0m |          -0.0067 |         101.4829 |          18.9757 |
[32m[20221213 21:11:31 @agent_ppo2.py:185][0m |          -0.0095 |         101.5811 |          18.9585 |
[32m[20221213 21:11:31 @agent_ppo2.py:185][0m |          -0.0068 |         101.1083 |          18.9587 |
[32m[20221213 21:11:31 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:11:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 598.20
[32m[20221213 21:11:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 657.00
[32m[20221213 21:11:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.00
[32m[20221213 21:11:32 @agent_ppo2.py:143][0m Total time:      15.95 min
[32m[20221213 21:11:32 @agent_ppo2.py:145][0m 1552384 total steps have happened
[32m[20221213 21:11:32 @agent_ppo2.py:121][0m #------------------------ Iteration 758 --------------------------#
[32m[20221213 21:11:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:32 @agent_ppo2.py:185][0m |          -0.0010 |         103.9978 |          18.9962 |
[32m[20221213 21:11:32 @agent_ppo2.py:185][0m |           0.0040 |         104.9350 |          18.9884 |
[32m[20221213 21:11:32 @agent_ppo2.py:185][0m |          -0.0053 |         102.7165 |          18.9809 |
[32m[20221213 21:11:32 @agent_ppo2.py:185][0m |          -0.0035 |         102.5373 |          18.9745 |
[32m[20221213 21:11:32 @agent_ppo2.py:185][0m |          -0.0032 |         101.9901 |          18.9631 |
[32m[20221213 21:11:32 @agent_ppo2.py:185][0m |           0.0067 |         116.3766 |          18.9704 |
[32m[20221213 21:11:32 @agent_ppo2.py:185][0m |          -0.0060 |         101.5223 |          18.9504 |
[32m[20221213 21:11:33 @agent_ppo2.py:185][0m |          -0.0045 |         101.7455 |          18.9694 |
[32m[20221213 21:11:33 @agent_ppo2.py:185][0m |          -0.0046 |         101.0969 |          18.9516 |
[32m[20221213 21:11:33 @agent_ppo2.py:185][0m |          -0.0047 |         101.0975 |          18.9513 |
[32m[20221213 21:11:33 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:11:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.20
[32m[20221213 21:11:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 641.00
[32m[20221213 21:11:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.00
[32m[20221213 21:11:33 @agent_ppo2.py:143][0m Total time:      15.97 min
[32m[20221213 21:11:33 @agent_ppo2.py:145][0m 1554432 total steps have happened
[32m[20221213 21:11:33 @agent_ppo2.py:121][0m #------------------------ Iteration 759 --------------------------#
[32m[20221213 21:11:33 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:11:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:33 @agent_ppo2.py:185][0m |          -0.0015 |         102.3367 |          18.9021 |
[32m[20221213 21:11:33 @agent_ppo2.py:185][0m |          -0.0050 |         101.9729 |          18.8890 |
[32m[20221213 21:11:33 @agent_ppo2.py:185][0m |          -0.0069 |         101.6055 |          18.8856 |
[32m[20221213 21:11:33 @agent_ppo2.py:185][0m |          -0.0073 |         101.4685 |          18.8729 |
[32m[20221213 21:11:34 @agent_ppo2.py:185][0m |          -0.0082 |         101.1967 |          18.8956 |
[32m[20221213 21:11:34 @agent_ppo2.py:185][0m |          -0.0089 |         101.1529 |          18.8865 |
[32m[20221213 21:11:34 @agent_ppo2.py:185][0m |           0.0021 |         108.5405 |          18.8981 |
[32m[20221213 21:11:34 @agent_ppo2.py:185][0m |          -0.0089 |         100.8750 |          18.8792 |
[32m[20221213 21:11:34 @agent_ppo2.py:185][0m |          -0.0081 |         100.4961 |          18.8923 |
[32m[20221213 21:11:34 @agent_ppo2.py:185][0m |          -0.0102 |         100.4399 |          18.8978 |
[32m[20221213 21:11:34 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:11:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 590.60
[32m[20221213 21:11:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 659.00
[32m[20221213 21:11:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 650.00
[32m[20221213 21:11:34 @agent_ppo2.py:143][0m Total time:      15.99 min
[32m[20221213 21:11:34 @agent_ppo2.py:145][0m 1556480 total steps have happened
[32m[20221213 21:11:34 @agent_ppo2.py:121][0m #------------------------ Iteration 760 --------------------------#
[32m[20221213 21:11:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:34 @agent_ppo2.py:185][0m |          -0.0043 |         103.6811 |          18.9159 |
[32m[20221213 21:11:35 @agent_ppo2.py:185][0m |          -0.0039 |         101.3772 |          18.9253 |
[32m[20221213 21:11:35 @agent_ppo2.py:185][0m |          -0.0057 |         100.3351 |          18.9306 |
[32m[20221213 21:11:35 @agent_ppo2.py:185][0m |           0.0037 |         111.4393 |          18.9185 |
[32m[20221213 21:11:35 @agent_ppo2.py:185][0m |          -0.0065 |          99.5204 |          18.9120 |
[32m[20221213 21:11:35 @agent_ppo2.py:185][0m |          -0.0060 |          99.4211 |          18.9301 |
[32m[20221213 21:11:35 @agent_ppo2.py:185][0m |          -0.0048 |          99.0548 |          18.9322 |
[32m[20221213 21:11:35 @agent_ppo2.py:185][0m |          -0.0056 |          98.6636 |          18.9307 |
[32m[20221213 21:11:35 @agent_ppo2.py:185][0m |          -0.0043 |          99.3117 |          18.9341 |
[32m[20221213 21:11:35 @agent_ppo2.py:185][0m |          -0.0044 |          98.4293 |          18.9343 |
[32m[20221213 21:11:35 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:11:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 610.80
[32m[20221213 21:11:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 671.00
[32m[20221213 21:11:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 698.00
[32m[20221213 21:11:35 @agent_ppo2.py:143][0m Total time:      16.01 min
[32m[20221213 21:11:35 @agent_ppo2.py:145][0m 1558528 total steps have happened
[32m[20221213 21:11:35 @agent_ppo2.py:121][0m #------------------------ Iteration 761 --------------------------#
[32m[20221213 21:11:36 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:11:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:36 @agent_ppo2.py:185][0m |           0.0074 |         115.9129 |          18.8970 |
[32m[20221213 21:11:36 @agent_ppo2.py:185][0m |           0.0013 |         117.4797 |          18.8985 |
[32m[20221213 21:11:36 @agent_ppo2.py:185][0m |           0.0013 |         113.1949 |          18.9035 |
[32m[20221213 21:11:36 @agent_ppo2.py:185][0m |          -0.0098 |         108.9398 |          18.9001 |
[32m[20221213 21:11:36 @agent_ppo2.py:185][0m |          -0.0082 |         108.4773 |          18.8982 |
[32m[20221213 21:11:36 @agent_ppo2.py:185][0m |          -0.0069 |         108.1384 |          18.9007 |
[32m[20221213 21:11:36 @agent_ppo2.py:185][0m |          -0.0078 |         107.7866 |          18.8988 |
[32m[20221213 21:11:36 @agent_ppo2.py:185][0m |          -0.0050 |         107.7417 |          18.9038 |
[32m[20221213 21:11:36 @agent_ppo2.py:185][0m |          -0.0034 |         109.3562 |          18.9005 |
[32m[20221213 21:11:36 @agent_ppo2.py:185][0m |          -0.0076 |         107.1383 |          18.9014 |
[32m[20221213 21:11:36 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:11:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.00
[32m[20221213 21:11:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 619.00
[32m[20221213 21:11:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.00
[32m[20221213 21:11:37 @agent_ppo2.py:143][0m Total time:      16.03 min
[32m[20221213 21:11:37 @agent_ppo2.py:145][0m 1560576 total steps have happened
[32m[20221213 21:11:37 @agent_ppo2.py:121][0m #------------------------ Iteration 762 --------------------------#
[32m[20221213 21:11:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:37 @agent_ppo2.py:185][0m |          -0.0031 |         105.2314 |          18.8918 |
[32m[20221213 21:11:37 @agent_ppo2.py:185][0m |          -0.0055 |         103.9352 |          18.8818 |
[32m[20221213 21:11:37 @agent_ppo2.py:185][0m |          -0.0050 |         103.1455 |          18.8740 |
[32m[20221213 21:11:37 @agent_ppo2.py:185][0m |           0.0049 |         114.8004 |          18.8771 |
[32m[20221213 21:11:37 @agent_ppo2.py:185][0m |          -0.0048 |         102.5100 |          18.8856 |
[32m[20221213 21:11:37 @agent_ppo2.py:185][0m |          -0.0072 |         102.3544 |          18.8959 |
[32m[20221213 21:11:37 @agent_ppo2.py:185][0m |          -0.0083 |         101.9397 |          18.8954 |
[32m[20221213 21:11:38 @agent_ppo2.py:185][0m |          -0.0087 |         101.8105 |          18.9014 |
[32m[20221213 21:11:38 @agent_ppo2.py:185][0m |          -0.0070 |         101.6256 |          18.8986 |
[32m[20221213 21:11:38 @agent_ppo2.py:185][0m |          -0.0096 |         101.4661 |          18.9071 |
[32m[20221213 21:11:38 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:11:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 590.60
[32m[20221213 21:11:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 630.00
[32m[20221213 21:11:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 662.00
[32m[20221213 21:11:38 @agent_ppo2.py:143][0m Total time:      16.05 min
[32m[20221213 21:11:38 @agent_ppo2.py:145][0m 1562624 total steps have happened
[32m[20221213 21:11:38 @agent_ppo2.py:121][0m #------------------------ Iteration 763 --------------------------#
[32m[20221213 21:11:38 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:11:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:38 @agent_ppo2.py:185][0m |           0.0074 |         112.0872 |          19.0330 |
[32m[20221213 21:11:38 @agent_ppo2.py:185][0m |          -0.0067 |         104.4232 |          19.0240 |
[32m[20221213 21:11:38 @agent_ppo2.py:185][0m |           0.0005 |         105.1979 |          19.0161 |
[32m[20221213 21:11:38 @agent_ppo2.py:185][0m |          -0.0075 |         103.3921 |          19.0057 |
[32m[20221213 21:11:38 @agent_ppo2.py:185][0m |          -0.0086 |         103.0820 |          19.0156 |
[32m[20221213 21:11:39 @agent_ppo2.py:185][0m |          -0.0083 |         102.8073 |          19.0053 |
[32m[20221213 21:11:39 @agent_ppo2.py:185][0m |           0.0054 |         112.3272 |          18.9955 |
[32m[20221213 21:11:39 @agent_ppo2.py:185][0m |          -0.0080 |         102.5757 |          18.9992 |
[32m[20221213 21:11:39 @agent_ppo2.py:185][0m |          -0.0118 |         102.2442 |          18.9959 |
[32m[20221213 21:11:39 @agent_ppo2.py:185][0m |           0.0007 |         106.6694 |          18.9897 |
[32m[20221213 21:11:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:11:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 608.40
[32m[20221213 21:11:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 637.00
[32m[20221213 21:11:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.00
[32m[20221213 21:11:39 @agent_ppo2.py:143][0m Total time:      16.07 min
[32m[20221213 21:11:39 @agent_ppo2.py:145][0m 1564672 total steps have happened
[32m[20221213 21:11:39 @agent_ppo2.py:121][0m #------------------------ Iteration 764 --------------------------#
[32m[20221213 21:11:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:11:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:39 @agent_ppo2.py:185][0m |          -0.0018 |         104.9317 |          18.9182 |
[32m[20221213 21:11:39 @agent_ppo2.py:185][0m |          -0.0042 |         103.6569 |          18.9218 |
[32m[20221213 21:11:40 @agent_ppo2.py:185][0m |           0.0070 |         112.6915 |          18.9254 |
[32m[20221213 21:11:40 @agent_ppo2.py:185][0m |          -0.0053 |         102.4807 |          18.9235 |
[32m[20221213 21:11:40 @agent_ppo2.py:185][0m |          -0.0051 |         102.0773 |          18.9333 |
[32m[20221213 21:11:40 @agent_ppo2.py:185][0m |          -0.0020 |         101.8470 |          18.9454 |
[32m[20221213 21:11:40 @agent_ppo2.py:185][0m |          -0.0059 |         101.5234 |          18.9490 |
[32m[20221213 21:11:40 @agent_ppo2.py:185][0m |          -0.0075 |         101.4683 |          18.9419 |
[32m[20221213 21:11:40 @agent_ppo2.py:185][0m |          -0.0061 |         101.3804 |          18.9598 |
[32m[20221213 21:11:40 @agent_ppo2.py:185][0m |          -0.0058 |         100.9538 |          18.9467 |
[32m[20221213 21:11:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:11:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.00
[32m[20221213 21:11:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.00
[32m[20221213 21:11:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 688.00
[32m[20221213 21:11:40 @agent_ppo2.py:143][0m Total time:      16.09 min
[32m[20221213 21:11:40 @agent_ppo2.py:145][0m 1566720 total steps have happened
[32m[20221213 21:11:40 @agent_ppo2.py:121][0m #------------------------ Iteration 765 --------------------------#
[32m[20221213 21:11:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:41 @agent_ppo2.py:185][0m |          -0.0012 |         102.7872 |          18.9220 |
[32m[20221213 21:11:41 @agent_ppo2.py:185][0m |          -0.0047 |         101.7798 |          18.9130 |
[32m[20221213 21:11:41 @agent_ppo2.py:185][0m |          -0.0055 |         101.1269 |          18.9174 |
[32m[20221213 21:11:41 @agent_ppo2.py:185][0m |          -0.0057 |         100.8769 |          18.9177 |
[32m[20221213 21:11:41 @agent_ppo2.py:185][0m |          -0.0074 |         100.3629 |          18.9081 |
[32m[20221213 21:11:41 @agent_ppo2.py:185][0m |          -0.0086 |         100.2916 |          18.9090 |
[32m[20221213 21:11:41 @agent_ppo2.py:185][0m |          -0.0054 |         100.5072 |          18.9122 |
[32m[20221213 21:11:41 @agent_ppo2.py:185][0m |          -0.0096 |          99.6766 |          18.9109 |
[32m[20221213 21:11:41 @agent_ppo2.py:185][0m |          -0.0018 |         103.6143 |          18.9106 |
[32m[20221213 21:11:41 @agent_ppo2.py:185][0m |          -0.0054 |          99.4668 |          18.9124 |
[32m[20221213 21:11:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:11:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 605.00
[32m[20221213 21:11:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 629.00
[32m[20221213 21:11:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 679.00
[32m[20221213 21:11:41 @agent_ppo2.py:143][0m Total time:      16.11 min
[32m[20221213 21:11:41 @agent_ppo2.py:145][0m 1568768 total steps have happened
[32m[20221213 21:11:41 @agent_ppo2.py:121][0m #------------------------ Iteration 766 --------------------------#
[32m[20221213 21:11:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:42 @agent_ppo2.py:185][0m |          -0.0012 |         105.3733 |          19.0188 |
[32m[20221213 21:11:42 @agent_ppo2.py:185][0m |          -0.0060 |         103.9446 |          18.9965 |
[32m[20221213 21:11:42 @agent_ppo2.py:185][0m |          -0.0069 |         103.2116 |          18.9955 |
[32m[20221213 21:11:42 @agent_ppo2.py:185][0m |          -0.0078 |         102.9476 |          19.0098 |
[32m[20221213 21:11:42 @agent_ppo2.py:185][0m |          -0.0081 |         102.5910 |          19.0032 |
[32m[20221213 21:11:42 @agent_ppo2.py:185][0m |          -0.0069 |         102.2873 |          19.0103 |
[32m[20221213 21:11:42 @agent_ppo2.py:185][0m |          -0.0106 |         102.1621 |          19.0198 |
[32m[20221213 21:11:42 @agent_ppo2.py:185][0m |          -0.0103 |         102.0054 |          19.0140 |
[32m[20221213 21:11:42 @agent_ppo2.py:185][0m |          -0.0106 |         101.8728 |          19.0275 |
[32m[20221213 21:11:43 @agent_ppo2.py:185][0m |          -0.0081 |         101.5563 |          19.0242 |
[32m[20221213 21:11:43 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:11:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 588.80
[32m[20221213 21:11:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 649.00
[32m[20221213 21:11:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.00
[32m[20221213 21:11:43 @agent_ppo2.py:143][0m Total time:      16.13 min
[32m[20221213 21:11:43 @agent_ppo2.py:145][0m 1570816 total steps have happened
[32m[20221213 21:11:43 @agent_ppo2.py:121][0m #------------------------ Iteration 767 --------------------------#
[32m[20221213 21:11:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:11:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:43 @agent_ppo2.py:185][0m |          -0.0017 |         105.1828 |          19.0153 |
[32m[20221213 21:11:43 @agent_ppo2.py:185][0m |          -0.0036 |         102.7473 |          18.9896 |
[32m[20221213 21:11:43 @agent_ppo2.py:185][0m |          -0.0041 |         101.5195 |          18.9973 |
[32m[20221213 21:11:43 @agent_ppo2.py:185][0m |          -0.0049 |         100.6695 |          18.9913 |
[32m[20221213 21:11:43 @agent_ppo2.py:185][0m |           0.0065 |         109.0245 |          18.9987 |
[32m[20221213 21:11:43 @agent_ppo2.py:185][0m |          -0.0073 |          98.4004 |          18.9952 |
[32m[20221213 21:11:43 @agent_ppo2.py:185][0m |          -0.0084 |          97.5319 |          19.0013 |
[32m[20221213 21:11:44 @agent_ppo2.py:185][0m |          -0.0048 |          97.7058 |          18.9966 |
[32m[20221213 21:11:44 @agent_ppo2.py:185][0m |          -0.0001 |         100.1754 |          19.0077 |
[32m[20221213 21:11:44 @agent_ppo2.py:185][0m |          -0.0065 |          96.7295 |          19.0094 |
[32m[20221213 21:11:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:11:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.20
[32m[20221213 21:11:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 664.00
[32m[20221213 21:11:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.00
[32m[20221213 21:11:44 @agent_ppo2.py:143][0m Total time:      16.15 min
[32m[20221213 21:11:44 @agent_ppo2.py:145][0m 1572864 total steps have happened
[32m[20221213 21:11:44 @agent_ppo2.py:121][0m #------------------------ Iteration 768 --------------------------#
[32m[20221213 21:11:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:44 @agent_ppo2.py:185][0m |          -0.0009 |         110.2098 |          19.0477 |
[32m[20221213 21:11:44 @agent_ppo2.py:185][0m |          -0.0021 |         108.7813 |          19.0380 |
[32m[20221213 21:11:44 @agent_ppo2.py:185][0m |          -0.0051 |         107.8821 |          19.0293 |
[32m[20221213 21:11:44 @agent_ppo2.py:185][0m |          -0.0054 |         107.4029 |          19.0260 |
[32m[20221213 21:11:44 @agent_ppo2.py:185][0m |          -0.0015 |         109.3803 |          19.0307 |
[32m[20221213 21:11:45 @agent_ppo2.py:185][0m |          -0.0048 |         106.8741 |          19.0196 |
[32m[20221213 21:11:45 @agent_ppo2.py:185][0m |          -0.0023 |         107.6571 |          19.0195 |
[32m[20221213 21:11:45 @agent_ppo2.py:185][0m |          -0.0086 |         106.4831 |          19.0018 |
[32m[20221213 21:11:45 @agent_ppo2.py:185][0m |          -0.0092 |         106.3187 |          19.0178 |
[32m[20221213 21:11:45 @agent_ppo2.py:185][0m |          -0.0084 |         106.1684 |          19.0089 |
[32m[20221213 21:11:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:11:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.20
[32m[20221213 21:11:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 614.00
[32m[20221213 21:11:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.00
[32m[20221213 21:11:45 @agent_ppo2.py:143][0m Total time:      16.17 min
[32m[20221213 21:11:45 @agent_ppo2.py:145][0m 1574912 total steps have happened
[32m[20221213 21:11:45 @agent_ppo2.py:121][0m #------------------------ Iteration 769 --------------------------#
[32m[20221213 21:11:45 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:11:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:45 @agent_ppo2.py:185][0m |          -0.0015 |         104.5365 |          18.9285 |
[32m[20221213 21:11:45 @agent_ppo2.py:185][0m |          -0.0050 |         101.7585 |          18.9197 |
[32m[20221213 21:11:46 @agent_ppo2.py:185][0m |           0.0049 |         102.4041 |          18.9167 |
[32m[20221213 21:11:46 @agent_ppo2.py:185][0m |           0.0012 |         101.2868 |          18.9158 |
[32m[20221213 21:11:46 @agent_ppo2.py:185][0m |          -0.0016 |          99.1339 |          18.9117 |
[32m[20221213 21:11:46 @agent_ppo2.py:185][0m |           0.0069 |         110.0706 |          18.9138 |
[32m[20221213 21:11:46 @agent_ppo2.py:185][0m |          -0.0050 |          98.6613 |          18.9185 |
[32m[20221213 21:11:46 @agent_ppo2.py:185][0m |          -0.0067 |          98.7969 |          18.9162 |
[32m[20221213 21:11:46 @agent_ppo2.py:185][0m |          -0.0055 |          98.0804 |          18.9184 |
[32m[20221213 21:11:46 @agent_ppo2.py:185][0m |           0.0060 |         111.3416 |          18.9198 |
[32m[20221213 21:11:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:11:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 594.40
[32m[20221213 21:11:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 613.00
[32m[20221213 21:11:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 630.00
[32m[20221213 21:11:46 @agent_ppo2.py:143][0m Total time:      16.19 min
[32m[20221213 21:11:46 @agent_ppo2.py:145][0m 1576960 total steps have happened
[32m[20221213 21:11:46 @agent_ppo2.py:121][0m #------------------------ Iteration 770 --------------------------#
[32m[20221213 21:11:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:47 @agent_ppo2.py:185][0m |          -0.0029 |         102.8717 |          19.1501 |
[32m[20221213 21:11:47 @agent_ppo2.py:185][0m |          -0.0067 |          98.5465 |          19.1306 |
[32m[20221213 21:11:47 @agent_ppo2.py:185][0m |          -0.0061 |          96.4963 |          19.1260 |
[32m[20221213 21:11:47 @agent_ppo2.py:185][0m |          -0.0092 |          95.7109 |          19.1195 |
[32m[20221213 21:11:47 @agent_ppo2.py:185][0m |          -0.0086 |          94.6787 |          19.1244 |
[32m[20221213 21:11:47 @agent_ppo2.py:185][0m |          -0.0099 |          95.5035 |          19.1146 |
[32m[20221213 21:11:47 @agent_ppo2.py:185][0m |           0.0026 |          97.0606 |          19.1244 |
[32m[20221213 21:11:47 @agent_ppo2.py:185][0m |          -0.0077 |          93.0496 |          19.1149 |
[32m[20221213 21:11:47 @agent_ppo2.py:185][0m |          -0.0088 |          92.7275 |          19.1181 |
[32m[20221213 21:11:47 @agent_ppo2.py:185][0m |          -0.0100 |          92.2812 |          19.1294 |
[32m[20221213 21:11:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:11:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 592.40
[32m[20221213 21:11:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 653.00
[32m[20221213 21:11:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.00
[32m[20221213 21:11:47 @agent_ppo2.py:143][0m Total time:      16.21 min
[32m[20221213 21:11:47 @agent_ppo2.py:145][0m 1579008 total steps have happened
[32m[20221213 21:11:47 @agent_ppo2.py:121][0m #------------------------ Iteration 771 --------------------------#
[32m[20221213 21:11:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:48 @agent_ppo2.py:185][0m |          -0.0028 |         109.7530 |          19.0097 |
[32m[20221213 21:11:48 @agent_ppo2.py:185][0m |          -0.0035 |         108.3270 |          18.9947 |
[32m[20221213 21:11:48 @agent_ppo2.py:185][0m |          -0.0070 |         106.7553 |          18.9893 |
[32m[20221213 21:11:48 @agent_ppo2.py:185][0m |          -0.0087 |         106.0481 |          18.9926 |
[32m[20221213 21:11:48 @agent_ppo2.py:185][0m |          -0.0100 |         105.4904 |          18.9812 |
[32m[20221213 21:11:48 @agent_ppo2.py:185][0m |          -0.0076 |         105.0405 |          18.9747 |
[32m[20221213 21:11:48 @agent_ppo2.py:185][0m |          -0.0106 |         104.8724 |          18.9671 |
[32m[20221213 21:11:48 @agent_ppo2.py:185][0m |          -0.0086 |         104.4798 |          18.9663 |
[32m[20221213 21:11:48 @agent_ppo2.py:185][0m |          -0.0108 |         104.3734 |          18.9440 |
[32m[20221213 21:11:49 @agent_ppo2.py:185][0m |          -0.0095 |         104.3041 |          18.9490 |
[32m[20221213 21:11:49 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:11:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 590.20
[32m[20221213 21:11:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 674.00
[32m[20221213 21:11:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 657.00
[32m[20221213 21:11:49 @agent_ppo2.py:143][0m Total time:      16.23 min
[32m[20221213 21:11:49 @agent_ppo2.py:145][0m 1581056 total steps have happened
[32m[20221213 21:11:49 @agent_ppo2.py:121][0m #------------------------ Iteration 772 --------------------------#
[32m[20221213 21:11:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:49 @agent_ppo2.py:185][0m |           0.0115 |         113.1130 |          18.9786 |
[32m[20221213 21:11:49 @agent_ppo2.py:185][0m |          -0.0063 |         104.2278 |          18.9465 |
[32m[20221213 21:11:49 @agent_ppo2.py:185][0m |          -0.0054 |         103.8629 |          18.9481 |
[32m[20221213 21:11:49 @agent_ppo2.py:185][0m |          -0.0087 |         103.4789 |          18.9402 |
[32m[20221213 21:11:49 @agent_ppo2.py:185][0m |          -0.0059 |         103.0848 |          18.9254 |
[32m[20221213 21:11:50 @agent_ppo2.py:185][0m |          -0.0051 |         103.0014 |          18.9113 |
[32m[20221213 21:11:50 @agent_ppo2.py:185][0m |          -0.0099 |         102.7708 |          18.9010 |
[32m[20221213 21:11:50 @agent_ppo2.py:185][0m |          -0.0075 |         102.5349 |          18.8975 |
[32m[20221213 21:11:50 @agent_ppo2.py:185][0m |          -0.0114 |         102.3665 |          18.8881 |
[32m[20221213 21:11:50 @agent_ppo2.py:185][0m |          -0.0078 |         102.1932 |          18.8872 |
[32m[20221213 21:11:50 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 21:11:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.60
[32m[20221213 21:11:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 654.00
[32m[20221213 21:11:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 655.00
[32m[20221213 21:11:50 @agent_ppo2.py:143][0m Total time:      16.26 min
[32m[20221213 21:11:50 @agent_ppo2.py:145][0m 1583104 total steps have happened
[32m[20221213 21:11:50 @agent_ppo2.py:121][0m #------------------------ Iteration 773 --------------------------#
[32m[20221213 21:11:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:50 @agent_ppo2.py:185][0m |           0.0121 |         111.8764 |          18.9687 |
[32m[20221213 21:11:50 @agent_ppo2.py:185][0m |          -0.0038 |         104.6667 |          18.9511 |
[32m[20221213 21:11:51 @agent_ppo2.py:185][0m |          -0.0052 |         103.9917 |          18.9471 |
[32m[20221213 21:11:51 @agent_ppo2.py:185][0m |          -0.0059 |         103.6712 |          18.9568 |
[32m[20221213 21:11:51 @agent_ppo2.py:185][0m |           0.0020 |         113.3146 |          18.9583 |
[32m[20221213 21:11:51 @agent_ppo2.py:185][0m |           0.0051 |         109.0443 |          18.9590 |
[32m[20221213 21:11:51 @agent_ppo2.py:185][0m |          -0.0102 |         103.4023 |          18.9670 |
[32m[20221213 21:11:51 @agent_ppo2.py:185][0m |          -0.0086 |         103.1261 |          18.9573 |
[32m[20221213 21:11:51 @agent_ppo2.py:185][0m |          -0.0086 |         103.0693 |          18.9556 |
[32m[20221213 21:11:51 @agent_ppo2.py:185][0m |           0.0019 |         116.0240 |          18.9594 |
[32m[20221213 21:11:51 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:11:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.00
[32m[20221213 21:11:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 629.00
[32m[20221213 21:11:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.00
[32m[20221213 21:11:51 @agent_ppo2.py:143][0m Total time:      16.28 min
[32m[20221213 21:11:51 @agent_ppo2.py:145][0m 1585152 total steps have happened
[32m[20221213 21:11:51 @agent_ppo2.py:121][0m #------------------------ Iteration 774 --------------------------#
[32m[20221213 21:11:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:52 @agent_ppo2.py:185][0m |          -0.0004 |         107.7548 |          18.9826 |
[32m[20221213 21:11:52 @agent_ppo2.py:185][0m |          -0.0045 |         106.9032 |          18.9898 |
[32m[20221213 21:11:52 @agent_ppo2.py:185][0m |          -0.0041 |         106.2770 |          18.9972 |
[32m[20221213 21:11:52 @agent_ppo2.py:185][0m |          -0.0019 |         106.0214 |          18.9855 |
[32m[20221213 21:11:52 @agent_ppo2.py:185][0m |           0.0042 |         110.3648 |          18.9856 |
[32m[20221213 21:11:52 @agent_ppo2.py:185][0m |          -0.0080 |         105.6507 |          18.9701 |
[32m[20221213 21:11:52 @agent_ppo2.py:185][0m |           0.0013 |         110.0033 |          18.9740 |
[32m[20221213 21:11:52 @agent_ppo2.py:185][0m |          -0.0049 |         106.1194 |          18.9643 |
[32m[20221213 21:11:52 @agent_ppo2.py:185][0m |          -0.0084 |         105.1733 |          18.9625 |
[32m[20221213 21:11:52 @agent_ppo2.py:185][0m |          -0.0067 |         105.0669 |          18.9700 |
[32m[20221213 21:11:52 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:11:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 578.40
[32m[20221213 21:11:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 635.00
[32m[20221213 21:11:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 649.00
[32m[20221213 21:11:53 @agent_ppo2.py:143][0m Total time:      16.30 min
[32m[20221213 21:11:53 @agent_ppo2.py:145][0m 1587200 total steps have happened
[32m[20221213 21:11:53 @agent_ppo2.py:121][0m #------------------------ Iteration 775 --------------------------#
[32m[20221213 21:11:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:53 @agent_ppo2.py:185][0m |          -0.0019 |         105.0841 |          18.9545 |
[32m[20221213 21:11:53 @agent_ppo2.py:185][0m |           0.0045 |         109.1198 |          18.9240 |
[32m[20221213 21:11:53 @agent_ppo2.py:185][0m |           0.0043 |         110.2696 |          18.9163 |
[32m[20221213 21:11:53 @agent_ppo2.py:185][0m |           0.0014 |         109.8760 |          18.9142 |
[32m[20221213 21:11:53 @agent_ppo2.py:185][0m |          -0.0003 |         107.8822 |          18.9090 |
[32m[20221213 21:11:53 @agent_ppo2.py:185][0m |          -0.0068 |         101.7870 |          18.9042 |
[32m[20221213 21:11:53 @agent_ppo2.py:185][0m |          -0.0094 |         101.6167 |          18.9198 |
[32m[20221213 21:11:53 @agent_ppo2.py:185][0m |          -0.0124 |         101.4696 |          18.9123 |
[32m[20221213 21:11:54 @agent_ppo2.py:185][0m |          -0.0086 |         101.0385 |          18.9081 |
[32m[20221213 21:11:54 @agent_ppo2.py:185][0m |          -0.0135 |         101.1221 |          18.9121 |
[32m[20221213 21:11:54 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:11:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 595.20
[32m[20221213 21:11:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 645.00
[32m[20221213 21:11:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 722.00
[32m[20221213 21:11:54 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 722.00
[32m[20221213 21:11:54 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 722.00
[32m[20221213 21:11:54 @agent_ppo2.py:143][0m Total time:      16.32 min
[32m[20221213 21:11:54 @agent_ppo2.py:145][0m 1589248 total steps have happened
[32m[20221213 21:11:54 @agent_ppo2.py:121][0m #------------------------ Iteration 776 --------------------------#
[32m[20221213 21:11:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:54 @agent_ppo2.py:185][0m |           0.0157 |         120.0402 |          18.9465 |
[32m[20221213 21:11:54 @agent_ppo2.py:185][0m |          -0.0026 |         108.1847 |          18.9182 |
[32m[20221213 21:11:54 @agent_ppo2.py:185][0m |          -0.0049 |         107.4221 |          18.9119 |
[32m[20221213 21:11:54 @agent_ppo2.py:185][0m |          -0.0089 |         107.0803 |          18.9061 |
[32m[20221213 21:11:55 @agent_ppo2.py:185][0m |          -0.0085 |         106.6815 |          18.8978 |
[32m[20221213 21:11:55 @agent_ppo2.py:185][0m |           0.0027 |         116.0325 |          18.8903 |
[32m[20221213 21:11:55 @agent_ppo2.py:185][0m |           0.0092 |         116.2108 |          18.8799 |
[32m[20221213 21:11:55 @agent_ppo2.py:185][0m |          -0.0080 |         106.1922 |          18.8615 |
[32m[20221213 21:11:55 @agent_ppo2.py:185][0m |          -0.0084 |         105.9031 |          18.8672 |
[32m[20221213 21:11:55 @agent_ppo2.py:185][0m |          -0.0078 |         105.7019 |          18.8654 |
[32m[20221213 21:11:55 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 21:11:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 611.40
[32m[20221213 21:11:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 647.00
[32m[20221213 21:11:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 627.00
[32m[20221213 21:11:55 @agent_ppo2.py:143][0m Total time:      16.34 min
[32m[20221213 21:11:55 @agent_ppo2.py:145][0m 1591296 total steps have happened
[32m[20221213 21:11:55 @agent_ppo2.py:121][0m #------------------------ Iteration 777 --------------------------#
[32m[20221213 21:11:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:11:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:55 @agent_ppo2.py:185][0m |          -0.0007 |         110.9083 |          18.8936 |
[32m[20221213 21:11:56 @agent_ppo2.py:185][0m |          -0.0065 |         110.1112 |          18.8677 |
[32m[20221213 21:11:56 @agent_ppo2.py:185][0m |          -0.0074 |         109.6209 |          18.8577 |
[32m[20221213 21:11:56 @agent_ppo2.py:185][0m |          -0.0069 |         109.2807 |          18.8439 |
[32m[20221213 21:11:56 @agent_ppo2.py:185][0m |          -0.0089 |         109.0063 |          18.8355 |
[32m[20221213 21:11:56 @agent_ppo2.py:185][0m |          -0.0102 |         108.8834 |          18.8356 |
[32m[20221213 21:11:56 @agent_ppo2.py:185][0m |           0.0019 |         115.8499 |          18.8423 |
[32m[20221213 21:11:56 @agent_ppo2.py:185][0m |          -0.0068 |         108.3638 |          18.8235 |
[32m[20221213 21:11:56 @agent_ppo2.py:185][0m |          -0.0097 |         108.2629 |          18.8264 |
[32m[20221213 21:11:56 @agent_ppo2.py:185][0m |          -0.0064 |         109.5164 |          18.8201 |
[32m[20221213 21:11:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:11:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 609.00
[32m[20221213 21:11:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 672.00
[32m[20221213 21:11:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 659.00
[32m[20221213 21:11:56 @agent_ppo2.py:143][0m Total time:      16.36 min
[32m[20221213 21:11:56 @agent_ppo2.py:145][0m 1593344 total steps have happened
[32m[20221213 21:11:56 @agent_ppo2.py:121][0m #------------------------ Iteration 778 --------------------------#
[32m[20221213 21:11:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:57 @agent_ppo2.py:185][0m |          -0.0017 |         112.1706 |          18.9748 |
[32m[20221213 21:11:57 @agent_ppo2.py:185][0m |          -0.0044 |         110.7405 |          18.9783 |
[32m[20221213 21:11:57 @agent_ppo2.py:185][0m |          -0.0052 |         110.1238 |          18.9629 |
[32m[20221213 21:11:57 @agent_ppo2.py:185][0m |          -0.0062 |         110.3167 |          18.9582 |
[32m[20221213 21:11:57 @agent_ppo2.py:185][0m |          -0.0016 |         112.1841 |          18.9530 |
[32m[20221213 21:11:57 @agent_ppo2.py:185][0m |          -0.0078 |         109.4373 |          18.9442 |
[32m[20221213 21:11:57 @agent_ppo2.py:185][0m |          -0.0082 |         108.9038 |          18.9366 |
[32m[20221213 21:11:57 @agent_ppo2.py:185][0m |          -0.0080 |         108.6192 |          18.9329 |
[32m[20221213 21:11:57 @agent_ppo2.py:185][0m |          -0.0101 |         108.6385 |          18.9228 |
[32m[20221213 21:11:58 @agent_ppo2.py:185][0m |          -0.0116 |         108.3951 |          18.9231 |
[32m[20221213 21:11:58 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:11:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.00
[32m[20221213 21:11:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 629.00
[32m[20221213 21:11:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 628.00
[32m[20221213 21:11:58 @agent_ppo2.py:143][0m Total time:      16.38 min
[32m[20221213 21:11:58 @agent_ppo2.py:145][0m 1595392 total steps have happened
[32m[20221213 21:11:58 @agent_ppo2.py:121][0m #------------------------ Iteration 779 --------------------------#
[32m[20221213 21:11:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:11:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:58 @agent_ppo2.py:185][0m |          -0.0039 |         108.0963 |          18.6656 |
[32m[20221213 21:11:58 @agent_ppo2.py:185][0m |          -0.0014 |         106.9628 |          18.6578 |
[32m[20221213 21:11:58 @agent_ppo2.py:185][0m |          -0.0047 |         106.4640 |          18.6493 |
[32m[20221213 21:11:58 @agent_ppo2.py:185][0m |           0.0039 |         118.4365 |          18.6517 |
[32m[20221213 21:11:58 @agent_ppo2.py:185][0m |          -0.0088 |         105.9894 |          18.6502 |
[32m[20221213 21:11:58 @agent_ppo2.py:185][0m |          -0.0060 |         105.4471 |          18.6425 |
[32m[20221213 21:11:59 @agent_ppo2.py:185][0m |           0.0036 |         117.8787 |          18.6497 |
[32m[20221213 21:11:59 @agent_ppo2.py:185][0m |          -0.0062 |         105.2948 |          18.6385 |
[32m[20221213 21:11:59 @agent_ppo2.py:185][0m |          -0.0076 |         104.9927 |          18.6458 |
[32m[20221213 21:11:59 @agent_ppo2.py:185][0m |          -0.0083 |         105.0665 |          18.6463 |
[32m[20221213 21:11:59 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:11:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 627.80
[32m[20221213 21:11:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 686.00
[32m[20221213 21:11:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 676.00
[32m[20221213 21:11:59 @agent_ppo2.py:143][0m Total time:      16.40 min
[32m[20221213 21:11:59 @agent_ppo2.py:145][0m 1597440 total steps have happened
[32m[20221213 21:11:59 @agent_ppo2.py:121][0m #------------------------ Iteration 780 --------------------------#
[32m[20221213 21:11:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:11:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:11:59 @agent_ppo2.py:185][0m |           0.0153 |         117.0323 |          18.8224 |
[32m[20221213 21:11:59 @agent_ppo2.py:185][0m |          -0.0027 |         105.5777 |          18.8123 |
[32m[20221213 21:11:59 @agent_ppo2.py:185][0m |          -0.0048 |         104.8590 |          18.8148 |
[32m[20221213 21:11:59 @agent_ppo2.py:185][0m |          -0.0074 |         104.3565 |          18.8194 |
[32m[20221213 21:12:00 @agent_ppo2.py:185][0m |          -0.0036 |         103.6705 |          18.8110 |
[32m[20221213 21:12:00 @agent_ppo2.py:185][0m |          -0.0027 |         105.3519 |          18.8119 |
[32m[20221213 21:12:00 @agent_ppo2.py:185][0m |          -0.0062 |         103.2395 |          18.8081 |
[32m[20221213 21:12:00 @agent_ppo2.py:185][0m |          -0.0084 |         102.5937 |          18.8145 |
[32m[20221213 21:12:00 @agent_ppo2.py:185][0m |          -0.0068 |         102.4430 |          18.8123 |
[32m[20221213 21:12:00 @agent_ppo2.py:185][0m |          -0.0058 |         102.0689 |          18.8132 |
[32m[20221213 21:12:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.60
[32m[20221213 21:12:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 644.00
[32m[20221213 21:12:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.00
[32m[20221213 21:12:00 @agent_ppo2.py:143][0m Total time:      16.42 min
[32m[20221213 21:12:00 @agent_ppo2.py:145][0m 1599488 total steps have happened
[32m[20221213 21:12:00 @agent_ppo2.py:121][0m #------------------------ Iteration 781 --------------------------#
[32m[20221213 21:12:00 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:12:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:00 @agent_ppo2.py:185][0m |          -0.0044 |         108.9604 |          18.8781 |
[32m[20221213 21:12:01 @agent_ppo2.py:185][0m |          -0.0057 |         107.0028 |          18.8682 |
[32m[20221213 21:12:01 @agent_ppo2.py:185][0m |          -0.0056 |         105.9287 |          18.8685 |
[32m[20221213 21:12:01 @agent_ppo2.py:185][0m |          -0.0078 |         105.1712 |          18.8603 |
[32m[20221213 21:12:01 @agent_ppo2.py:185][0m |          -0.0078 |         104.6570 |          18.8655 |
[32m[20221213 21:12:01 @agent_ppo2.py:185][0m |          -0.0077 |         104.3969 |          18.8621 |
[32m[20221213 21:12:01 @agent_ppo2.py:185][0m |          -0.0085 |         104.0027 |          18.8608 |
[32m[20221213 21:12:01 @agent_ppo2.py:185][0m |          -0.0088 |         103.7279 |          18.8672 |
[32m[20221213 21:12:01 @agent_ppo2.py:185][0m |           0.0020 |         116.3108 |          18.8701 |
[32m[20221213 21:12:01 @agent_ppo2.py:185][0m |           0.0037 |         112.3864 |          18.8703 |
[32m[20221213 21:12:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.80
[32m[20221213 21:12:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 612.00
[32m[20221213 21:12:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 685.00
[32m[20221213 21:12:01 @agent_ppo2.py:143][0m Total time:      16.44 min
[32m[20221213 21:12:01 @agent_ppo2.py:145][0m 1601536 total steps have happened
[32m[20221213 21:12:01 @agent_ppo2.py:121][0m #------------------------ Iteration 782 --------------------------#
[32m[20221213 21:12:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:02 @agent_ppo2.py:185][0m |          -0.0044 |         106.5448 |          18.8027 |
[32m[20221213 21:12:02 @agent_ppo2.py:185][0m |          -0.0062 |         105.4005 |          18.7852 |
[32m[20221213 21:12:02 @agent_ppo2.py:185][0m |          -0.0059 |         104.7694 |          18.7780 |
[32m[20221213 21:12:02 @agent_ppo2.py:185][0m |          -0.0071 |         104.3092 |          18.7714 |
[32m[20221213 21:12:02 @agent_ppo2.py:185][0m |          -0.0087 |         103.8263 |          18.7836 |
[32m[20221213 21:12:02 @agent_ppo2.py:185][0m |          -0.0085 |         103.4509 |          18.7888 |
[32m[20221213 21:12:02 @agent_ppo2.py:185][0m |          -0.0046 |         108.7380 |          18.7827 |
[32m[20221213 21:12:02 @agent_ppo2.py:185][0m |          -0.0073 |         102.9189 |          18.7764 |
[32m[20221213 21:12:02 @agent_ppo2.py:185][0m |          -0.0104 |         102.7741 |          18.7765 |
[32m[20221213 21:12:02 @agent_ppo2.py:185][0m |          -0.0098 |         102.5565 |          18.7788 |
[32m[20221213 21:12:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 599.40
[32m[20221213 21:12:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 643.00
[32m[20221213 21:12:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 635.00
[32m[20221213 21:12:02 @agent_ppo2.py:143][0m Total time:      16.46 min
[32m[20221213 21:12:02 @agent_ppo2.py:145][0m 1603584 total steps have happened
[32m[20221213 21:12:02 @agent_ppo2.py:121][0m #------------------------ Iteration 783 --------------------------#
[32m[20221213 21:12:03 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:12:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:03 @agent_ppo2.py:185][0m |           0.0003 |         103.6271 |          18.6456 |
[32m[20221213 21:12:03 @agent_ppo2.py:185][0m |          -0.0033 |          98.8614 |          18.6398 |
[32m[20221213 21:12:03 @agent_ppo2.py:185][0m |           0.0148 |         114.0985 |          18.6313 |
[32m[20221213 21:12:03 @agent_ppo2.py:185][0m |          -0.0011 |          97.6809 |          18.6325 |
[32m[20221213 21:12:03 @agent_ppo2.py:185][0m |          -0.0076 |          96.4808 |          18.6436 |
[32m[20221213 21:12:03 @agent_ppo2.py:185][0m |          -0.0079 |          96.2488 |          18.6464 |
[32m[20221213 21:12:03 @agent_ppo2.py:185][0m |          -0.0082 |          95.7021 |          18.6615 |
[32m[20221213 21:12:03 @agent_ppo2.py:185][0m |          -0.0106 |          95.6882 |          18.6661 |
[32m[20221213 21:12:03 @agent_ppo2.py:185][0m |          -0.0058 |          95.4893 |          18.6804 |
[32m[20221213 21:12:04 @agent_ppo2.py:185][0m |          -0.0083 |          95.0807 |          18.6706 |
[32m[20221213 21:12:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:12:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 568.00
[32m[20221213 21:12:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 602.00
[32m[20221213 21:12:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 683.00
[32m[20221213 21:12:04 @agent_ppo2.py:143][0m Total time:      16.48 min
[32m[20221213 21:12:04 @agent_ppo2.py:145][0m 1605632 total steps have happened
[32m[20221213 21:12:04 @agent_ppo2.py:121][0m #------------------------ Iteration 784 --------------------------#
[32m[20221213 21:12:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:04 @agent_ppo2.py:185][0m |          -0.0015 |         114.6170 |          18.9295 |
[32m[20221213 21:12:04 @agent_ppo2.py:185][0m |           0.0044 |         119.4546 |          18.9378 |
[32m[20221213 21:12:04 @agent_ppo2.py:185][0m |           0.0011 |         112.4694 |          18.9275 |
[32m[20221213 21:12:04 @agent_ppo2.py:185][0m |          -0.0049 |         107.0190 |          18.9203 |
[32m[20221213 21:12:04 @agent_ppo2.py:185][0m |           0.0048 |         111.0921 |          18.9274 |
[32m[20221213 21:12:04 @agent_ppo2.py:185][0m |          -0.0062 |         105.0113 |          18.9273 |
[32m[20221213 21:12:05 @agent_ppo2.py:185][0m |          -0.0073 |         104.1555 |          18.9366 |
[32m[20221213 21:12:05 @agent_ppo2.py:185][0m |           0.0092 |         112.6261 |          18.9346 |
[32m[20221213 21:12:05 @agent_ppo2.py:185][0m |          -0.0024 |         104.9709 |          18.9274 |
[32m[20221213 21:12:05 @agent_ppo2.py:185][0m |          -0.0073 |         103.0185 |          18.9321 |
[32m[20221213 21:12:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:12:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 596.80
[32m[20221213 21:12:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 639.00
[32m[20221213 21:12:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 524.00
[32m[20221213 21:12:05 @agent_ppo2.py:143][0m Total time:      16.50 min
[32m[20221213 21:12:05 @agent_ppo2.py:145][0m 1607680 total steps have happened
[32m[20221213 21:12:05 @agent_ppo2.py:121][0m #------------------------ Iteration 785 --------------------------#
[32m[20221213 21:12:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:05 @agent_ppo2.py:185][0m |          -0.0024 |         107.5595 |          18.8108 |
[32m[20221213 21:12:05 @agent_ppo2.py:185][0m |          -0.0045 |         105.0642 |          18.7849 |
[32m[20221213 21:12:05 @agent_ppo2.py:185][0m |          -0.0048 |         104.0980 |          18.7973 |
[32m[20221213 21:12:06 @agent_ppo2.py:185][0m |          -0.0043 |         103.6084 |          18.7910 |
[32m[20221213 21:12:06 @agent_ppo2.py:185][0m |           0.0008 |         106.4693 |          18.7844 |
[32m[20221213 21:12:06 @agent_ppo2.py:185][0m |          -0.0064 |         102.8960 |          18.7847 |
[32m[20221213 21:12:06 @agent_ppo2.py:185][0m |          -0.0054 |         102.4414 |          18.7950 |
[32m[20221213 21:12:06 @agent_ppo2.py:185][0m |          -0.0058 |         102.3664 |          18.7799 |
[32m[20221213 21:12:06 @agent_ppo2.py:185][0m |           0.0005 |         104.9608 |          18.7774 |
[32m[20221213 21:12:06 @agent_ppo2.py:185][0m |          -0.0021 |         102.1366 |          18.7784 |
[32m[20221213 21:12:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 632.00
[32m[20221213 21:12:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 667.00
[32m[20221213 21:12:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.00
[32m[20221213 21:12:06 @agent_ppo2.py:143][0m Total time:      16.52 min
[32m[20221213 21:12:06 @agent_ppo2.py:145][0m 1609728 total steps have happened
[32m[20221213 21:12:06 @agent_ppo2.py:121][0m #------------------------ Iteration 786 --------------------------#
[32m[20221213 21:12:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:06 @agent_ppo2.py:185][0m |          -0.0012 |         114.7234 |          18.9456 |
[32m[20221213 21:12:07 @agent_ppo2.py:185][0m |          -0.0023 |         112.8927 |          18.9282 |
[32m[20221213 21:12:07 @agent_ppo2.py:185][0m |           0.0032 |         114.8632 |          18.9123 |
[32m[20221213 21:12:07 @agent_ppo2.py:185][0m |          -0.0017 |         111.6902 |          18.9153 |
[32m[20221213 21:12:07 @agent_ppo2.py:185][0m |          -0.0089 |         110.5574 |          18.9120 |
[32m[20221213 21:12:07 @agent_ppo2.py:185][0m |           0.0033 |         113.7428 |          18.9169 |
[32m[20221213 21:12:07 @agent_ppo2.py:185][0m |          -0.0042 |         109.9905 |          18.9042 |
[32m[20221213 21:12:07 @agent_ppo2.py:185][0m |          -0.0081 |         109.8454 |          18.9068 |
[32m[20221213 21:12:07 @agent_ppo2.py:185][0m |          -0.0007 |         117.9396 |          18.8949 |
[32m[20221213 21:12:07 @agent_ppo2.py:185][0m |          -0.0026 |         110.4043 |          18.8948 |
[32m[20221213 21:12:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 587.20
[32m[20221213 21:12:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 630.00
[32m[20221213 21:12:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.00
[32m[20221213 21:12:07 @agent_ppo2.py:143][0m Total time:      16.54 min
[32m[20221213 21:12:07 @agent_ppo2.py:145][0m 1611776 total steps have happened
[32m[20221213 21:12:07 @agent_ppo2.py:121][0m #------------------------ Iteration 787 --------------------------#
[32m[20221213 21:12:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:08 @agent_ppo2.py:185][0m |          -0.0001 |         108.4222 |          18.9507 |
[32m[20221213 21:12:08 @agent_ppo2.py:185][0m |           0.0007 |         107.6826 |          18.9475 |
[32m[20221213 21:12:08 @agent_ppo2.py:185][0m |          -0.0041 |         105.4992 |          18.9435 |
[32m[20221213 21:12:08 @agent_ppo2.py:185][0m |          -0.0097 |         105.1174 |          18.9475 |
[32m[20221213 21:12:08 @agent_ppo2.py:185][0m |          -0.0020 |         106.0449 |          18.9333 |
[32m[20221213 21:12:08 @agent_ppo2.py:185][0m |          -0.0075 |         103.8367 |          18.9401 |
[32m[20221213 21:12:08 @agent_ppo2.py:185][0m |          -0.0070 |         103.9662 |          18.9403 |
[32m[20221213 21:12:08 @agent_ppo2.py:185][0m |          -0.0058 |         103.0516 |          18.9381 |
[32m[20221213 21:12:08 @agent_ppo2.py:185][0m |          -0.0034 |         103.9910 |          18.9390 |
[32m[20221213 21:12:08 @agent_ppo2.py:185][0m |          -0.0087 |         102.7449 |          18.9338 |
[32m[20221213 21:12:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:12:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 566.00
[32m[20221213 21:12:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 590.00
[32m[20221213 21:12:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.00
[32m[20221213 21:12:09 @agent_ppo2.py:143][0m Total time:      16.56 min
[32m[20221213 21:12:09 @agent_ppo2.py:145][0m 1613824 total steps have happened
[32m[20221213 21:12:09 @agent_ppo2.py:121][0m #------------------------ Iteration 788 --------------------------#
[32m[20221213 21:12:09 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:12:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:09 @agent_ppo2.py:185][0m |           0.0055 |         109.5551 |          18.8964 |
[32m[20221213 21:12:09 @agent_ppo2.py:185][0m |          -0.0012 |         106.7816 |          18.8843 |
[32m[20221213 21:12:09 @agent_ppo2.py:185][0m |          -0.0041 |         105.9874 |          18.8669 |
[32m[20221213 21:12:09 @agent_ppo2.py:185][0m |          -0.0066 |         105.4271 |          18.8755 |
[32m[20221213 21:12:09 @agent_ppo2.py:185][0m |          -0.0072 |         105.2519 |          18.8663 |
[32m[20221213 21:12:09 @agent_ppo2.py:185][0m |          -0.0031 |         104.7762 |          18.8641 |
[32m[20221213 21:12:09 @agent_ppo2.py:185][0m |          -0.0043 |         104.5910 |          18.8692 |
[32m[20221213 21:12:09 @agent_ppo2.py:185][0m |           0.0051 |         109.5356 |          18.8612 |
[32m[20221213 21:12:10 @agent_ppo2.py:185][0m |          -0.0072 |         104.1709 |          18.8327 |
[32m[20221213 21:12:10 @agent_ppo2.py:185][0m |          -0.0054 |         103.9498 |          18.8497 |
[32m[20221213 21:12:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.40
[32m[20221213 21:12:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 654.00
[32m[20221213 21:12:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 713.00
[32m[20221213 21:12:10 @agent_ppo2.py:143][0m Total time:      16.58 min
[32m[20221213 21:12:10 @agent_ppo2.py:145][0m 1615872 total steps have happened
[32m[20221213 21:12:10 @agent_ppo2.py:121][0m #------------------------ Iteration 789 --------------------------#
[32m[20221213 21:12:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:10 @agent_ppo2.py:185][0m |          -0.0022 |         109.6517 |          18.7367 |
[32m[20221213 21:12:10 @agent_ppo2.py:185][0m |          -0.0033 |         108.6361 |          18.7413 |
[32m[20221213 21:12:10 @agent_ppo2.py:185][0m |          -0.0036 |         107.8408 |          18.7407 |
[32m[20221213 21:12:10 @agent_ppo2.py:185][0m |          -0.0051 |         107.2539 |          18.7438 |
[32m[20221213 21:12:10 @agent_ppo2.py:185][0m |          -0.0054 |         106.9510 |          18.7307 |
[32m[20221213 21:12:10 @agent_ppo2.py:185][0m |          -0.0067 |         107.1450 |          18.7303 |
[32m[20221213 21:12:11 @agent_ppo2.py:185][0m |          -0.0069 |         106.4118 |          18.7426 |
[32m[20221213 21:12:11 @agent_ppo2.py:185][0m |          -0.0079 |         106.0927 |          18.7362 |
[32m[20221213 21:12:11 @agent_ppo2.py:185][0m |          -0.0068 |         106.1105 |          18.7304 |
[32m[20221213 21:12:11 @agent_ppo2.py:185][0m |          -0.0068 |         105.9017 |          18.7374 |
[32m[20221213 21:12:11 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:12:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.60
[32m[20221213 21:12:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.00
[32m[20221213 21:12:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 689.00
[32m[20221213 21:12:11 @agent_ppo2.py:143][0m Total time:      16.60 min
[32m[20221213 21:12:11 @agent_ppo2.py:145][0m 1617920 total steps have happened
[32m[20221213 21:12:11 @agent_ppo2.py:121][0m #------------------------ Iteration 790 --------------------------#
[32m[20221213 21:12:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:12:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:11 @agent_ppo2.py:185][0m |           0.0026 |         111.0848 |          18.7845 |
[32m[20221213 21:12:11 @agent_ppo2.py:185][0m |           0.0043 |         111.9668 |          18.7682 |
[32m[20221213 21:12:11 @agent_ppo2.py:185][0m |          -0.0063 |         106.3414 |          18.7514 |
[32m[20221213 21:12:12 @agent_ppo2.py:185][0m |          -0.0068 |         105.4191 |          18.7383 |
[32m[20221213 21:12:12 @agent_ppo2.py:185][0m |          -0.0066 |         104.5906 |          18.7274 |
[32m[20221213 21:12:12 @agent_ppo2.py:185][0m |          -0.0003 |         106.5320 |          18.7135 |
[32m[20221213 21:12:12 @agent_ppo2.py:185][0m |          -0.0108 |         103.6328 |          18.6970 |
[32m[20221213 21:12:12 @agent_ppo2.py:185][0m |          -0.0067 |         103.3947 |          18.6827 |
[32m[20221213 21:12:12 @agent_ppo2.py:185][0m |          -0.0106 |         102.9854 |          18.6626 |
[32m[20221213 21:12:12 @agent_ppo2.py:185][0m |          -0.0129 |         102.5769 |          18.6533 |
[32m[20221213 21:12:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:12:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 585.80
[32m[20221213 21:12:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 669.00
[32m[20221213 21:12:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.00
[32m[20221213 21:12:12 @agent_ppo2.py:143][0m Total time:      16.62 min
[32m[20221213 21:12:12 @agent_ppo2.py:145][0m 1619968 total steps have happened
[32m[20221213 21:12:12 @agent_ppo2.py:121][0m #------------------------ Iteration 791 --------------------------#
[32m[20221213 21:12:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:12 @agent_ppo2.py:185][0m |           0.0004 |         111.8451 |          18.8051 |
[32m[20221213 21:12:13 @agent_ppo2.py:185][0m |          -0.0040 |         109.0520 |          18.7929 |
[32m[20221213 21:12:13 @agent_ppo2.py:185][0m |          -0.0049 |         107.9422 |          18.7873 |
[32m[20221213 21:12:13 @agent_ppo2.py:185][0m |          -0.0053 |         107.6679 |          18.7750 |
[32m[20221213 21:12:13 @agent_ppo2.py:185][0m |          -0.0053 |         106.9860 |          18.7742 |
[32m[20221213 21:12:13 @agent_ppo2.py:185][0m |           0.0040 |         114.0123 |          18.7718 |
[32m[20221213 21:12:13 @agent_ppo2.py:185][0m |           0.0032 |         115.9305 |          18.7785 |
[32m[20221213 21:12:13 @agent_ppo2.py:185][0m |          -0.0073 |         105.8413 |          18.7784 |
[32m[20221213 21:12:13 @agent_ppo2.py:185][0m |          -0.0095 |         105.6937 |          18.7813 |
[32m[20221213 21:12:13 @agent_ppo2.py:185][0m |          -0.0107 |         105.3870 |          18.7689 |
[32m[20221213 21:12:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 577.00
[32m[20221213 21:12:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 626.00
[32m[20221213 21:12:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.00
[32m[20221213 21:12:13 @agent_ppo2.py:143][0m Total time:      16.64 min
[32m[20221213 21:12:13 @agent_ppo2.py:145][0m 1622016 total steps have happened
[32m[20221213 21:12:13 @agent_ppo2.py:121][0m #------------------------ Iteration 792 --------------------------#
[32m[20221213 21:12:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:14 @agent_ppo2.py:185][0m |          -0.0042 |         113.7849 |          18.7712 |
[32m[20221213 21:12:14 @agent_ppo2.py:185][0m |          -0.0015 |         113.3777 |          18.7690 |
[32m[20221213 21:12:14 @agent_ppo2.py:185][0m |           0.0052 |         122.2541 |          18.7579 |
[32m[20221213 21:12:14 @agent_ppo2.py:185][0m |          -0.0046 |         111.5264 |          18.7410 |
[32m[20221213 21:12:14 @agent_ppo2.py:185][0m |          -0.0064 |         111.2221 |          18.7393 |
[32m[20221213 21:12:14 @agent_ppo2.py:185][0m |           0.0004 |         113.7634 |          18.7434 |
[32m[20221213 21:12:14 @agent_ppo2.py:185][0m |          -0.0011 |         117.6803 |          18.7373 |
[32m[20221213 21:12:14 @agent_ppo2.py:185][0m |          -0.0097 |         110.3475 |          18.7311 |
[32m[20221213 21:12:14 @agent_ppo2.py:185][0m |          -0.0069 |         110.1496 |          18.7329 |
[32m[20221213 21:12:14 @agent_ppo2.py:185][0m |          -0.0085 |         110.1414 |          18.7375 |
[32m[20221213 21:12:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:12:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 576.20
[32m[20221213 21:12:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 632.00
[32m[20221213 21:12:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.00
[32m[20221213 21:12:15 @agent_ppo2.py:143][0m Total time:      16.66 min
[32m[20221213 21:12:15 @agent_ppo2.py:145][0m 1624064 total steps have happened
[32m[20221213 21:12:15 @agent_ppo2.py:121][0m #------------------------ Iteration 793 --------------------------#
[32m[20221213 21:12:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:12:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:15 @agent_ppo2.py:185][0m |          -0.0019 |         103.4072 |          18.8117 |
[32m[20221213 21:12:15 @agent_ppo2.py:185][0m |          -0.0058 |         102.1265 |          18.7971 |
[32m[20221213 21:12:15 @agent_ppo2.py:185][0m |           0.0054 |         104.9647 |          18.7885 |
[32m[20221213 21:12:15 @agent_ppo2.py:185][0m |          -0.0062 |         100.5181 |          18.7804 |
[32m[20221213 21:12:15 @agent_ppo2.py:185][0m |          -0.0066 |          99.8491 |          18.7710 |
[32m[20221213 21:12:15 @agent_ppo2.py:185][0m |          -0.0051 |          99.4958 |          18.7813 |
[32m[20221213 21:12:15 @agent_ppo2.py:185][0m |          -0.0081 |          99.2359 |          18.7679 |
[32m[20221213 21:12:15 @agent_ppo2.py:185][0m |          -0.0035 |          99.5454 |          18.7639 |
[32m[20221213 21:12:16 @agent_ppo2.py:185][0m |          -0.0058 |          98.7140 |          18.7534 |
[32m[20221213 21:12:16 @agent_ppo2.py:185][0m |          -0.0086 |          98.5541 |          18.7496 |
[32m[20221213 21:12:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 604.60
[32m[20221213 21:12:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 644.00
[32m[20221213 21:12:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 662.00
[32m[20221213 21:12:16 @agent_ppo2.py:143][0m Total time:      16.68 min
[32m[20221213 21:12:16 @agent_ppo2.py:145][0m 1626112 total steps have happened
[32m[20221213 21:12:16 @agent_ppo2.py:121][0m #------------------------ Iteration 794 --------------------------#
[32m[20221213 21:12:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:16 @agent_ppo2.py:185][0m |          -0.0021 |         112.1965 |          18.6231 |
[32m[20221213 21:12:16 @agent_ppo2.py:185][0m |           0.0048 |         118.5692 |          18.5907 |
[32m[20221213 21:12:16 @agent_ppo2.py:185][0m |          -0.0033 |         108.3075 |          18.5652 |
[32m[20221213 21:12:16 @agent_ppo2.py:185][0m |          -0.0076 |         107.3928 |          18.5636 |
[32m[20221213 21:12:16 @agent_ppo2.py:185][0m |          -0.0080 |         107.0068 |          18.5802 |
[32m[20221213 21:12:16 @agent_ppo2.py:185][0m |          -0.0039 |         106.0567 |          18.5647 |
[32m[20221213 21:12:17 @agent_ppo2.py:185][0m |          -0.0064 |         106.6852 |          18.5568 |
[32m[20221213 21:12:17 @agent_ppo2.py:185][0m |          -0.0071 |         105.3627 |          18.5617 |
[32m[20221213 21:12:17 @agent_ppo2.py:185][0m |          -0.0032 |         106.5109 |          18.5624 |
[32m[20221213 21:12:17 @agent_ppo2.py:185][0m |          -0.0013 |         106.5733 |          18.5582 |
[32m[20221213 21:12:17 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:12:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 578.00
[32m[20221213 21:12:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 611.00
[32m[20221213 21:12:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 633.00
[32m[20221213 21:12:17 @agent_ppo2.py:143][0m Total time:      16.70 min
[32m[20221213 21:12:17 @agent_ppo2.py:145][0m 1628160 total steps have happened
[32m[20221213 21:12:17 @agent_ppo2.py:121][0m #------------------------ Iteration 795 --------------------------#
[32m[20221213 21:12:17 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:12:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:17 @agent_ppo2.py:185][0m |          -0.0045 |         107.8862 |          18.7233 |
[32m[20221213 21:12:17 @agent_ppo2.py:185][0m |          -0.0053 |         106.6660 |          18.7087 |
[32m[20221213 21:12:17 @agent_ppo2.py:185][0m |          -0.0037 |         106.8062 |          18.7020 |
[32m[20221213 21:12:17 @agent_ppo2.py:185][0m |          -0.0086 |         105.8726 |          18.7075 |
[32m[20221213 21:12:18 @agent_ppo2.py:185][0m |          -0.0093 |         105.7500 |          18.7016 |
[32m[20221213 21:12:18 @agent_ppo2.py:185][0m |          -0.0059 |         106.1056 |          18.7089 |
[32m[20221213 21:12:18 @agent_ppo2.py:185][0m |          -0.0103 |         105.0861 |          18.6976 |
[32m[20221213 21:12:18 @agent_ppo2.py:185][0m |          -0.0094 |         104.9340 |          18.7218 |
[32m[20221213 21:12:18 @agent_ppo2.py:185][0m |          -0.0120 |         104.8664 |          18.7123 |
[32m[20221213 21:12:18 @agent_ppo2.py:185][0m |          -0.0063 |         104.6406 |          18.7078 |
[32m[20221213 21:12:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 622.20
[32m[20221213 21:12:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 646.00
[32m[20221213 21:12:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 589.00
[32m[20221213 21:12:18 @agent_ppo2.py:143][0m Total time:      16.72 min
[32m[20221213 21:12:18 @agent_ppo2.py:145][0m 1630208 total steps have happened
[32m[20221213 21:12:18 @agent_ppo2.py:121][0m #------------------------ Iteration 796 --------------------------#
[32m[20221213 21:12:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:18 @agent_ppo2.py:185][0m |          -0.0010 |         106.0053 |          18.8151 |
[32m[20221213 21:12:19 @agent_ppo2.py:185][0m |          -0.0067 |         105.4495 |          18.8114 |
[32m[20221213 21:12:19 @agent_ppo2.py:185][0m |          -0.0052 |         104.7955 |          18.8014 |
[32m[20221213 21:12:19 @agent_ppo2.py:185][0m |          -0.0078 |         104.4163 |          18.8022 |
[32m[20221213 21:12:19 @agent_ppo2.py:185][0m |          -0.0086 |         104.2628 |          18.7905 |
[32m[20221213 21:12:19 @agent_ppo2.py:185][0m |          -0.0074 |         103.9286 |          18.7922 |
[32m[20221213 21:12:19 @agent_ppo2.py:185][0m |          -0.0102 |         103.6590 |          18.7895 |
[32m[20221213 21:12:19 @agent_ppo2.py:185][0m |          -0.0112 |         103.7688 |          18.7843 |
[32m[20221213 21:12:19 @agent_ppo2.py:185][0m |          -0.0072 |         103.4718 |          18.7772 |
[32m[20221213 21:12:19 @agent_ppo2.py:185][0m |          -0.0094 |         103.0329 |          18.7831 |
[32m[20221213 21:12:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.20
[32m[20221213 21:12:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 649.00
[32m[20221213 21:12:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.00
[32m[20221213 21:12:19 @agent_ppo2.py:143][0m Total time:      16.74 min
[32m[20221213 21:12:19 @agent_ppo2.py:145][0m 1632256 total steps have happened
[32m[20221213 21:12:19 @agent_ppo2.py:121][0m #------------------------ Iteration 797 --------------------------#
[32m[20221213 21:12:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:20 @agent_ppo2.py:185][0m |           0.0000 |         107.2518 |          18.5312 |
[32m[20221213 21:12:20 @agent_ppo2.py:185][0m |          -0.0006 |         106.0574 |          18.5104 |
[32m[20221213 21:12:20 @agent_ppo2.py:185][0m |           0.0028 |         108.8071 |          18.5047 |
[32m[20221213 21:12:20 @agent_ppo2.py:185][0m |          -0.0037 |         105.0925 |          18.4752 |
[32m[20221213 21:12:20 @agent_ppo2.py:185][0m |           0.0025 |         114.6975 |          18.4894 |
[32m[20221213 21:12:20 @agent_ppo2.py:185][0m |          -0.0075 |         104.6423 |          18.4914 |
[32m[20221213 21:12:20 @agent_ppo2.py:185][0m |           0.0042 |         108.5527 |          18.4736 |
[32m[20221213 21:12:20 @agent_ppo2.py:185][0m |          -0.0091 |         104.3012 |          18.4777 |
[32m[20221213 21:12:20 @agent_ppo2.py:185][0m |           0.0004 |         108.2757 |          18.4749 |
[32m[20221213 21:12:20 @agent_ppo2.py:185][0m |          -0.0080 |         103.8919 |          18.4717 |
[32m[20221213 21:12:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:12:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 578.80
[32m[20221213 21:12:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 635.00
[32m[20221213 21:12:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 611.00
[32m[20221213 21:12:21 @agent_ppo2.py:143][0m Total time:      16.76 min
[32m[20221213 21:12:21 @agent_ppo2.py:145][0m 1634304 total steps have happened
[32m[20221213 21:12:21 @agent_ppo2.py:121][0m #------------------------ Iteration 798 --------------------------#
[32m[20221213 21:12:21 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:12:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:21 @agent_ppo2.py:185][0m |          -0.0009 |         108.5074 |          18.7752 |
[32m[20221213 21:12:21 @agent_ppo2.py:185][0m |          -0.0026 |         108.2993 |          18.7786 |
[32m[20221213 21:12:21 @agent_ppo2.py:185][0m |          -0.0077 |         107.0010 |          18.7646 |
[32m[20221213 21:12:21 @agent_ppo2.py:185][0m |          -0.0074 |         106.6980 |          18.7683 |
[32m[20221213 21:12:21 @agent_ppo2.py:185][0m |          -0.0083 |         106.1784 |          18.7526 |
[32m[20221213 21:12:21 @agent_ppo2.py:185][0m |           0.0054 |         118.1847 |          18.7551 |
[32m[20221213 21:12:21 @agent_ppo2.py:185][0m |          -0.0077 |         105.7921 |          18.7381 |
[32m[20221213 21:12:21 @agent_ppo2.py:185][0m |          -0.0099 |         105.5541 |          18.7451 |
[32m[20221213 21:12:22 @agent_ppo2.py:185][0m |          -0.0074 |         105.2263 |          18.7447 |
[32m[20221213 21:12:22 @agent_ppo2.py:185][0m |          -0.0079 |         105.0410 |          18.7412 |
[32m[20221213 21:12:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 571.20
[32m[20221213 21:12:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 628.00
[32m[20221213 21:12:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 642.00
[32m[20221213 21:12:22 @agent_ppo2.py:143][0m Total time:      16.78 min
[32m[20221213 21:12:22 @agent_ppo2.py:145][0m 1636352 total steps have happened
[32m[20221213 21:12:22 @agent_ppo2.py:121][0m #------------------------ Iteration 799 --------------------------#
[32m[20221213 21:12:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:22 @agent_ppo2.py:185][0m |           0.0015 |         107.1419 |          18.6704 |
[32m[20221213 21:12:22 @agent_ppo2.py:185][0m |           0.0062 |         109.6313 |          18.6554 |
[32m[20221213 21:12:22 @agent_ppo2.py:185][0m |          -0.0033 |         103.4110 |          18.6555 |
[32m[20221213 21:12:22 @agent_ppo2.py:185][0m |           0.0026 |         106.8696 |          18.6642 |
[32m[20221213 21:12:22 @agent_ppo2.py:185][0m |          -0.0053 |         102.3073 |          18.6458 |
[32m[20221213 21:12:22 @agent_ppo2.py:185][0m |          -0.0044 |         102.1170 |          18.6533 |
[32m[20221213 21:12:23 @agent_ppo2.py:185][0m |           0.0013 |         106.9831 |          18.6571 |
[32m[20221213 21:12:23 @agent_ppo2.py:185][0m |          -0.0079 |         101.4321 |          18.6557 |
[32m[20221213 21:12:23 @agent_ppo2.py:185][0m |          -0.0058 |         101.3536 |          18.6717 |
[32m[20221213 21:12:23 @agent_ppo2.py:185][0m |          -0.0080 |         101.1967 |          18.6759 |
[32m[20221213 21:12:23 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:12:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 603.20
[32m[20221213 21:12:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 639.00
[32m[20221213 21:12:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 644.00
[32m[20221213 21:12:23 @agent_ppo2.py:143][0m Total time:      16.80 min
[32m[20221213 21:12:23 @agent_ppo2.py:145][0m 1638400 total steps have happened
[32m[20221213 21:12:23 @agent_ppo2.py:121][0m #------------------------ Iteration 800 --------------------------#
[32m[20221213 21:12:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:23 @agent_ppo2.py:185][0m |           0.0024 |         105.2891 |          18.7142 |
[32m[20221213 21:12:23 @agent_ppo2.py:185][0m |          -0.0029 |         103.3269 |          18.7044 |
[32m[20221213 21:12:23 @agent_ppo2.py:185][0m |          -0.0068 |         102.4673 |          18.6906 |
[32m[20221213 21:12:24 @agent_ppo2.py:185][0m |          -0.0060 |         102.0671 |          18.6930 |
[32m[20221213 21:12:24 @agent_ppo2.py:185][0m |          -0.0074 |         101.4331 |          18.6886 |
[32m[20221213 21:12:24 @agent_ppo2.py:185][0m |          -0.0035 |         101.5299 |          18.6889 |
[32m[20221213 21:12:24 @agent_ppo2.py:185][0m |          -0.0051 |         100.8623 |          18.6818 |
[32m[20221213 21:12:24 @agent_ppo2.py:185][0m |          -0.0071 |         100.5634 |          18.6922 |
[32m[20221213 21:12:24 @agent_ppo2.py:185][0m |          -0.0071 |         100.4883 |          18.6900 |
[32m[20221213 21:12:24 @agent_ppo2.py:185][0m |          -0.0077 |         100.3271 |          18.6936 |
[32m[20221213 21:12:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:12:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 604.40
[32m[20221213 21:12:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 689.00
[32m[20221213 21:12:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 594.00
[32m[20221213 21:12:24 @agent_ppo2.py:143][0m Total time:      16.83 min
[32m[20221213 21:12:24 @agent_ppo2.py:145][0m 1640448 total steps have happened
[32m[20221213 21:12:24 @agent_ppo2.py:121][0m #------------------------ Iteration 801 --------------------------#
[32m[20221213 21:12:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:25 @agent_ppo2.py:185][0m |          -0.0015 |         105.6608 |          18.7718 |
[32m[20221213 21:12:25 @agent_ppo2.py:185][0m |          -0.0020 |         104.6863 |          18.7337 |
[32m[20221213 21:12:25 @agent_ppo2.py:185][0m |          -0.0079 |         103.4275 |          18.7281 |
[32m[20221213 21:12:25 @agent_ppo2.py:185][0m |          -0.0104 |         102.9366 |          18.7138 |
[32m[20221213 21:12:25 @agent_ppo2.py:185][0m |           0.0002 |         104.4571 |          18.7136 |
[32m[20221213 21:12:25 @agent_ppo2.py:185][0m |          -0.0063 |         103.7452 |          18.7175 |
[32m[20221213 21:12:25 @agent_ppo2.py:185][0m |          -0.0113 |         101.9597 |          18.7152 |
[32m[20221213 21:12:25 @agent_ppo2.py:185][0m |          -0.0100 |         101.7754 |          18.7075 |
[32m[20221213 21:12:25 @agent_ppo2.py:185][0m |          -0.0122 |         101.4597 |          18.7055 |
[32m[20221213 21:12:25 @agent_ppo2.py:185][0m |          -0.0065 |         101.8012 |          18.7011 |
[32m[20221213 21:12:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 624.40
[32m[20221213 21:12:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 665.00
[32m[20221213 21:12:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.00
[32m[20221213 21:12:25 @agent_ppo2.py:143][0m Total time:      16.84 min
[32m[20221213 21:12:25 @agent_ppo2.py:145][0m 1642496 total steps have happened
[32m[20221213 21:12:25 @agent_ppo2.py:121][0m #------------------------ Iteration 802 --------------------------#
[32m[20221213 21:12:26 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:12:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:26 @agent_ppo2.py:185][0m |          -0.0002 |         106.6220 |          18.6808 |
[32m[20221213 21:12:26 @agent_ppo2.py:185][0m |          -0.0051 |         104.8454 |          18.6745 |
[32m[20221213 21:12:26 @agent_ppo2.py:185][0m |          -0.0044 |         103.4105 |          18.6556 |
[32m[20221213 21:12:26 @agent_ppo2.py:185][0m |          -0.0071 |         102.4233 |          18.6510 |
[32m[20221213 21:12:26 @agent_ppo2.py:185][0m |          -0.0053 |         101.8161 |          18.6383 |
[32m[20221213 21:12:26 @agent_ppo2.py:185][0m |          -0.0046 |         101.2344 |          18.6380 |
[32m[20221213 21:12:26 @agent_ppo2.py:185][0m |          -0.0065 |         100.5764 |          18.6220 |
[32m[20221213 21:12:26 @agent_ppo2.py:185][0m |          -0.0042 |         100.7080 |          18.6189 |
[32m[20221213 21:12:26 @agent_ppo2.py:185][0m |          -0.0078 |          99.7323 |          18.6083 |
[32m[20221213 21:12:26 @agent_ppo2.py:185][0m |          -0.0083 |          99.4362 |          18.5998 |
[32m[20221213 21:12:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 623.00
[32m[20221213 21:12:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 671.00
[32m[20221213 21:12:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 574.00
[32m[20221213 21:12:27 @agent_ppo2.py:143][0m Total time:      16.86 min
[32m[20221213 21:12:27 @agent_ppo2.py:145][0m 1644544 total steps have happened
[32m[20221213 21:12:27 @agent_ppo2.py:121][0m #------------------------ Iteration 803 --------------------------#
[32m[20221213 21:12:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:12:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:27 @agent_ppo2.py:185][0m |           0.0086 |         112.3697 |          18.6402 |
[32m[20221213 21:12:27 @agent_ppo2.py:185][0m |          -0.0061 |         104.4007 |          18.6385 |
[32m[20221213 21:12:27 @agent_ppo2.py:185][0m |          -0.0066 |         103.7733 |          18.6231 |
[32m[20221213 21:12:27 @agent_ppo2.py:185][0m |          -0.0100 |         103.5937 |          18.6197 |
[32m[20221213 21:12:27 @agent_ppo2.py:185][0m |          -0.0083 |         103.2581 |          18.6241 |
[32m[20221213 21:12:27 @agent_ppo2.py:185][0m |          -0.0117 |         103.0469 |          18.6286 |
[32m[20221213 21:12:27 @agent_ppo2.py:185][0m |          -0.0108 |         102.8504 |          18.6283 |
[32m[20221213 21:12:27 @agent_ppo2.py:185][0m |          -0.0118 |         102.6985 |          18.6280 |
[32m[20221213 21:12:28 @agent_ppo2.py:185][0m |           0.0018 |         117.2108 |          18.6327 |
[32m[20221213 21:12:28 @agent_ppo2.py:185][0m |          -0.0074 |         102.2800 |          18.6211 |
[32m[20221213 21:12:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 563.40
[32m[20221213 21:12:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.00
[32m[20221213 21:12:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 559.00
[32m[20221213 21:12:28 @agent_ppo2.py:143][0m Total time:      16.88 min
[32m[20221213 21:12:28 @agent_ppo2.py:145][0m 1646592 total steps have happened
[32m[20221213 21:12:28 @agent_ppo2.py:121][0m #------------------------ Iteration 804 --------------------------#
[32m[20221213 21:12:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:28 @agent_ppo2.py:185][0m |          -0.0002 |         103.7123 |          18.5792 |
[32m[20221213 21:12:28 @agent_ppo2.py:185][0m |          -0.0015 |         102.9994 |          18.5666 |
[32m[20221213 21:12:28 @agent_ppo2.py:185][0m |          -0.0033 |         102.5743 |          18.5518 |
[32m[20221213 21:12:28 @agent_ppo2.py:185][0m |          -0.0026 |         102.3375 |          18.5479 |
[32m[20221213 21:12:28 @agent_ppo2.py:185][0m |          -0.0078 |         102.1717 |          18.5526 |
[32m[20221213 21:12:29 @agent_ppo2.py:185][0m |           0.0103 |         111.2652 |          18.5512 |
[32m[20221213 21:12:29 @agent_ppo2.py:185][0m |          -0.0029 |         101.8245 |          18.5511 |
[32m[20221213 21:12:29 @agent_ppo2.py:185][0m |          -0.0068 |         101.4582 |          18.5510 |
[32m[20221213 21:12:29 @agent_ppo2.py:185][0m |          -0.0058 |         101.3120 |          18.5455 |
[32m[20221213 21:12:29 @agent_ppo2.py:185][0m |          -0.0088 |         101.1807 |          18.5438 |
[32m[20221213 21:12:29 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:12:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.00
[32m[20221213 21:12:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 599.00
[32m[20221213 21:12:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 688.00
[32m[20221213 21:12:29 @agent_ppo2.py:143][0m Total time:      16.91 min
[32m[20221213 21:12:29 @agent_ppo2.py:145][0m 1648640 total steps have happened
[32m[20221213 21:12:29 @agent_ppo2.py:121][0m #------------------------ Iteration 805 --------------------------#
[32m[20221213 21:12:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:29 @agent_ppo2.py:185][0m |          -0.0002 |         103.4959 |          18.6905 |
[32m[20221213 21:12:29 @agent_ppo2.py:185][0m |          -0.0041 |         101.6881 |          18.6937 |
[32m[20221213 21:12:29 @agent_ppo2.py:185][0m |          -0.0067 |         101.2155 |          18.6862 |
[32m[20221213 21:12:30 @agent_ppo2.py:185][0m |           0.0069 |         112.1062 |          18.6928 |
[32m[20221213 21:12:30 @agent_ppo2.py:185][0m |          -0.0041 |         100.7123 |          18.7029 |
[32m[20221213 21:12:30 @agent_ppo2.py:185][0m |          -0.0085 |         100.4999 |          18.6955 |
[32m[20221213 21:12:30 @agent_ppo2.py:185][0m |           0.0031 |         108.1965 |          18.7153 |
[32m[20221213 21:12:30 @agent_ppo2.py:185][0m |          -0.0086 |         100.0683 |          18.7173 |
[32m[20221213 21:12:30 @agent_ppo2.py:185][0m |          -0.0071 |          99.8264 |          18.7310 |
[32m[20221213 21:12:30 @agent_ppo2.py:185][0m |           0.0019 |         111.5931 |          18.7310 |
[32m[20221213 21:12:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:12:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 583.40
[32m[20221213 21:12:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 636.00
[32m[20221213 21:12:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.00
[32m[20221213 21:12:30 @agent_ppo2.py:143][0m Total time:      16.92 min
[32m[20221213 21:12:30 @agent_ppo2.py:145][0m 1650688 total steps have happened
[32m[20221213 21:12:30 @agent_ppo2.py:121][0m #------------------------ Iteration 806 --------------------------#
[32m[20221213 21:12:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:12:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:30 @agent_ppo2.py:185][0m |          -0.0019 |         101.4459 |          18.7002 |
[32m[20221213 21:12:31 @agent_ppo2.py:185][0m |          -0.0070 |         100.9045 |          18.6896 |
[32m[20221213 21:12:31 @agent_ppo2.py:185][0m |          -0.0064 |         100.6365 |          18.6785 |
[32m[20221213 21:12:31 @agent_ppo2.py:185][0m |          -0.0081 |         100.2984 |          18.6835 |
[32m[20221213 21:12:31 @agent_ppo2.py:185][0m |          -0.0050 |         101.2481 |          18.6878 |
[32m[20221213 21:12:31 @agent_ppo2.py:185][0m |          -0.0054 |          99.9488 |          18.6808 |
[32m[20221213 21:12:31 @agent_ppo2.py:185][0m |          -0.0089 |          99.8587 |          18.6844 |
[32m[20221213 21:12:31 @agent_ppo2.py:185][0m |          -0.0098 |          99.8031 |          18.6895 |
[32m[20221213 21:12:31 @agent_ppo2.py:185][0m |          -0.0075 |          99.4326 |          18.6773 |
[32m[20221213 21:12:31 @agent_ppo2.py:185][0m |          -0.0080 |          99.3664 |          18.6874 |
[32m[20221213 21:12:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:12:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 589.00
[32m[20221213 21:12:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 650.00
[32m[20221213 21:12:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 618.00
[32m[20221213 21:12:31 @agent_ppo2.py:143][0m Total time:      16.94 min
[32m[20221213 21:12:31 @agent_ppo2.py:145][0m 1652736 total steps have happened
[32m[20221213 21:12:31 @agent_ppo2.py:121][0m #------------------------ Iteration 807 --------------------------#
[32m[20221213 21:12:32 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:12:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:32 @agent_ppo2.py:185][0m |          -0.0007 |         104.3073 |          18.5873 |
[32m[20221213 21:12:32 @agent_ppo2.py:185][0m |          -0.0022 |         102.9129 |          18.5693 |
[32m[20221213 21:12:32 @agent_ppo2.py:185][0m |          -0.0037 |         102.2308 |          18.5646 |
[32m[20221213 21:12:32 @agent_ppo2.py:185][0m |          -0.0042 |         101.8293 |          18.5550 |
[32m[20221213 21:12:32 @agent_ppo2.py:185][0m |          -0.0074 |         101.4897 |          18.5449 |
[32m[20221213 21:12:32 @agent_ppo2.py:185][0m |          -0.0030 |         101.3867 |          18.5491 |
[32m[20221213 21:12:32 @agent_ppo2.py:185][0m |          -0.0067 |         101.1497 |          18.5431 |
[32m[20221213 21:12:32 @agent_ppo2.py:185][0m |          -0.0068 |         100.5837 |          18.5315 |
[32m[20221213 21:12:32 @agent_ppo2.py:185][0m |          -0.0072 |         100.4922 |          18.5367 |
[32m[20221213 21:12:32 @agent_ppo2.py:185][0m |          -0.0091 |         100.2355 |          18.5323 |
[32m[20221213 21:12:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 637.00
[32m[20221213 21:12:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 689.00
[32m[20221213 21:12:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 689.00
[32m[20221213 21:12:33 @agent_ppo2.py:143][0m Total time:      16.96 min
[32m[20221213 21:12:33 @agent_ppo2.py:145][0m 1654784 total steps have happened
[32m[20221213 21:12:33 @agent_ppo2.py:121][0m #------------------------ Iteration 808 --------------------------#
[32m[20221213 21:12:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:33 @agent_ppo2.py:185][0m |          -0.0007 |         106.3781 |          18.6103 |
[32m[20221213 21:12:33 @agent_ppo2.py:185][0m |           0.0080 |         115.5968 |          18.6040 |
[32m[20221213 21:12:33 @agent_ppo2.py:185][0m |          -0.0029 |         104.5865 |          18.5639 |
[32m[20221213 21:12:33 @agent_ppo2.py:185][0m |          -0.0037 |         104.1125 |          18.5796 |
[32m[20221213 21:12:33 @agent_ppo2.py:185][0m |          -0.0077 |         103.6173 |          18.5858 |
[32m[20221213 21:12:33 @agent_ppo2.py:185][0m |          -0.0057 |         103.1946 |          18.5855 |
[32m[20221213 21:12:33 @agent_ppo2.py:185][0m |          -0.0056 |         102.8297 |          18.5794 |
[32m[20221213 21:12:33 @agent_ppo2.py:185][0m |          -0.0019 |         103.4033 |          18.5846 |
[32m[20221213 21:12:34 @agent_ppo2.py:185][0m |          -0.0101 |         102.3220 |          18.5878 |
[32m[20221213 21:12:34 @agent_ppo2.py:185][0m |          -0.0031 |         102.7481 |          18.5985 |
[32m[20221213 21:12:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:12:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 587.00
[32m[20221213 21:12:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 636.00
[32m[20221213 21:12:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.00
[32m[20221213 21:12:34 @agent_ppo2.py:143][0m Total time:      16.98 min
[32m[20221213 21:12:34 @agent_ppo2.py:145][0m 1656832 total steps have happened
[32m[20221213 21:12:34 @agent_ppo2.py:121][0m #------------------------ Iteration 809 --------------------------#
[32m[20221213 21:12:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:34 @agent_ppo2.py:185][0m |          -0.0013 |         108.8828 |          18.6218 |
[32m[20221213 21:12:34 @agent_ppo2.py:185][0m |          -0.0017 |         107.5999 |          18.6049 |
[32m[20221213 21:12:34 @agent_ppo2.py:185][0m |          -0.0070 |         107.0285 |          18.5909 |
[32m[20221213 21:12:34 @agent_ppo2.py:185][0m |          -0.0075 |         106.4961 |          18.5869 |
[32m[20221213 21:12:34 @agent_ppo2.py:185][0m |          -0.0052 |         106.2687 |          18.5817 |
[32m[20221213 21:12:35 @agent_ppo2.py:185][0m |          -0.0097 |         105.9256 |          18.5746 |
[32m[20221213 21:12:35 @agent_ppo2.py:185][0m |          -0.0072 |         105.5721 |          18.5649 |
[32m[20221213 21:12:35 @agent_ppo2.py:185][0m |          -0.0052 |         106.4160 |          18.5594 |
[32m[20221213 21:12:35 @agent_ppo2.py:185][0m |          -0.0091 |         105.1932 |          18.5500 |
[32m[20221213 21:12:35 @agent_ppo2.py:185][0m |          -0.0090 |         105.0863 |          18.5348 |
[32m[20221213 21:12:35 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:12:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 563.80
[32m[20221213 21:12:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 631.00
[32m[20221213 21:12:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 585.00
[32m[20221213 21:12:35 @agent_ppo2.py:143][0m Total time:      17.01 min
[32m[20221213 21:12:35 @agent_ppo2.py:145][0m 1658880 total steps have happened
[32m[20221213 21:12:35 @agent_ppo2.py:121][0m #------------------------ Iteration 810 --------------------------#
[32m[20221213 21:12:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:35 @agent_ppo2.py:185][0m |          -0.0028 |         105.3412 |          18.6341 |
[32m[20221213 21:12:35 @agent_ppo2.py:185][0m |          -0.0048 |         104.5199 |          18.6334 |
[32m[20221213 21:12:35 @agent_ppo2.py:185][0m |          -0.0023 |         104.4967 |          18.6278 |
[32m[20221213 21:12:36 @agent_ppo2.py:185][0m |           0.0015 |         108.6926 |          18.6227 |
[32m[20221213 21:12:36 @agent_ppo2.py:185][0m |          -0.0058 |         103.6872 |          18.6321 |
[32m[20221213 21:12:36 @agent_ppo2.py:185][0m |          -0.0097 |         102.9365 |          18.6367 |
[32m[20221213 21:12:36 @agent_ppo2.py:185][0m |          -0.0095 |         102.5743 |          18.6255 |
[32m[20221213 21:12:36 @agent_ppo2.py:185][0m |          -0.0090 |         102.5013 |          18.6234 |
[32m[20221213 21:12:36 @agent_ppo2.py:185][0m |          -0.0108 |         102.2536 |          18.6276 |
[32m[20221213 21:12:36 @agent_ppo2.py:185][0m |          -0.0083 |         102.0914 |          18.6321 |
[32m[20221213 21:12:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 606.40
[32m[20221213 21:12:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 660.00
[32m[20221213 21:12:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 693.00
[32m[20221213 21:12:36 @agent_ppo2.py:143][0m Total time:      17.02 min
[32m[20221213 21:12:36 @agent_ppo2.py:145][0m 1660928 total steps have happened
[32m[20221213 21:12:36 @agent_ppo2.py:121][0m #------------------------ Iteration 811 --------------------------#
[32m[20221213 21:12:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:36 @agent_ppo2.py:185][0m |          -0.0042 |         104.6824 |          18.6184 |
[32m[20221213 21:12:37 @agent_ppo2.py:185][0m |          -0.0089 |         101.5963 |          18.6095 |
[32m[20221213 21:12:37 @agent_ppo2.py:185][0m |          -0.0083 |         100.0745 |          18.6082 |
[32m[20221213 21:12:37 @agent_ppo2.py:185][0m |          -0.0021 |         104.7545 |          18.5990 |
[32m[20221213 21:12:37 @agent_ppo2.py:185][0m |          -0.0113 |          98.6594 |          18.5862 |
[32m[20221213 21:12:37 @agent_ppo2.py:185][0m |          -0.0098 |          98.1572 |          18.6011 |
[32m[20221213 21:12:37 @agent_ppo2.py:185][0m |          -0.0008 |         110.7431 |          18.6077 |
[32m[20221213 21:12:37 @agent_ppo2.py:185][0m |          -0.0097 |          97.7706 |          18.6019 |
[32m[20221213 21:12:37 @agent_ppo2.py:185][0m |          -0.0123 |          97.1272 |          18.6105 |
[32m[20221213 21:12:37 @agent_ppo2.py:185][0m |          -0.0112 |          96.8088 |          18.6019 |
[32m[20221213 21:12:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:12:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 609.20
[32m[20221213 21:12:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 662.00
[32m[20221213 21:12:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 602.00
[32m[20221213 21:12:37 @agent_ppo2.py:143][0m Total time:      17.04 min
[32m[20221213 21:12:37 @agent_ppo2.py:145][0m 1662976 total steps have happened
[32m[20221213 21:12:37 @agent_ppo2.py:121][0m #------------------------ Iteration 812 --------------------------#
[32m[20221213 21:12:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:38 @agent_ppo2.py:185][0m |          -0.0017 |         113.3327 |          18.6399 |
[32m[20221213 21:12:38 @agent_ppo2.py:185][0m |          -0.0042 |         111.0414 |          18.6284 |
[32m[20221213 21:12:38 @agent_ppo2.py:185][0m |          -0.0068 |         110.2404 |          18.6246 |
[32m[20221213 21:12:38 @agent_ppo2.py:185][0m |          -0.0056 |         109.7898 |          18.6228 |
[32m[20221213 21:12:38 @agent_ppo2.py:185][0m |          -0.0093 |         109.4632 |          18.6206 |
[32m[20221213 21:12:38 @agent_ppo2.py:185][0m |          -0.0091 |         109.3367 |          18.6116 |
[32m[20221213 21:12:38 @agent_ppo2.py:185][0m |          -0.0015 |         113.0912 |          18.6180 |
[32m[20221213 21:12:38 @agent_ppo2.py:185][0m |           0.0017 |         119.1664 |          18.6176 |
[32m[20221213 21:12:38 @agent_ppo2.py:185][0m |          -0.0064 |         110.4036 |          18.6101 |
[32m[20221213 21:12:38 @agent_ppo2.py:185][0m |          -0.0102 |         108.8062 |          18.6144 |
[32m[20221213 21:12:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:12:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 574.60
[32m[20221213 21:12:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 661.00
[32m[20221213 21:12:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.00
[32m[20221213 21:12:39 @agent_ppo2.py:143][0m Total time:      17.07 min
[32m[20221213 21:12:39 @agent_ppo2.py:145][0m 1665024 total steps have happened
[32m[20221213 21:12:39 @agent_ppo2.py:121][0m #------------------------ Iteration 813 --------------------------#
[32m[20221213 21:12:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:39 @agent_ppo2.py:185][0m |          -0.0028 |         106.9908 |          18.4755 |
[32m[20221213 21:12:39 @agent_ppo2.py:185][0m |          -0.0023 |         106.3979 |          18.4568 |
[32m[20221213 21:12:39 @agent_ppo2.py:185][0m |          -0.0001 |         110.1046 |          18.4441 |
[32m[20221213 21:12:39 @agent_ppo2.py:185][0m |          -0.0032 |         107.4431 |          18.4486 |
[32m[20221213 21:12:39 @agent_ppo2.py:185][0m |          -0.0059 |         104.2243 |          18.4463 |
[32m[20221213 21:12:39 @agent_ppo2.py:185][0m |          -0.0084 |         104.0346 |          18.4527 |
[32m[20221213 21:12:39 @agent_ppo2.py:185][0m |          -0.0081 |         103.8675 |          18.4528 |
[32m[20221213 21:12:39 @agent_ppo2.py:185][0m |          -0.0067 |         103.9677 |          18.4428 |
[32m[20221213 21:12:40 @agent_ppo2.py:185][0m |          -0.0090 |         103.8193 |          18.4465 |
[32m[20221213 21:12:40 @agent_ppo2.py:185][0m |          -0.0125 |         103.5070 |          18.4454 |
[32m[20221213 21:12:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 597.00
[32m[20221213 21:12:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 643.00
[32m[20221213 21:12:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 637.00
[32m[20221213 21:12:40 @agent_ppo2.py:143][0m Total time:      17.08 min
[32m[20221213 21:12:40 @agent_ppo2.py:145][0m 1667072 total steps have happened
[32m[20221213 21:12:40 @agent_ppo2.py:121][0m #------------------------ Iteration 814 --------------------------#
[32m[20221213 21:12:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:40 @agent_ppo2.py:185][0m |          -0.0039 |         106.2729 |          18.6658 |
[32m[20221213 21:12:40 @agent_ppo2.py:185][0m |           0.0005 |         105.8155 |          18.6571 |
[32m[20221213 21:12:40 @agent_ppo2.py:185][0m |           0.0024 |         106.2945 |          18.6719 |
[32m[20221213 21:12:40 @agent_ppo2.py:185][0m |          -0.0035 |         103.6586 |          18.6597 |
[32m[20221213 21:12:40 @agent_ppo2.py:185][0m |          -0.0049 |         103.4018 |          18.6733 |
[32m[20221213 21:12:41 @agent_ppo2.py:185][0m |          -0.0053 |         103.1099 |          18.6707 |
[32m[20221213 21:12:41 @agent_ppo2.py:185][0m |          -0.0054 |         102.8872 |          18.6712 |
[32m[20221213 21:12:41 @agent_ppo2.py:185][0m |          -0.0077 |         102.7368 |          18.6875 |
[32m[20221213 21:12:41 @agent_ppo2.py:185][0m |          -0.0081 |         102.4669 |          18.6846 |
[32m[20221213 21:12:41 @agent_ppo2.py:185][0m |          -0.0081 |         102.4801 |          18.6746 |
[32m[20221213 21:12:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:12:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 555.20
[32m[20221213 21:12:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 602.00
[32m[20221213 21:12:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 660.00
[32m[20221213 21:12:41 @agent_ppo2.py:143][0m Total time:      17.11 min
[32m[20221213 21:12:41 @agent_ppo2.py:145][0m 1669120 total steps have happened
[32m[20221213 21:12:41 @agent_ppo2.py:121][0m #------------------------ Iteration 815 --------------------------#
[32m[20221213 21:12:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:41 @agent_ppo2.py:185][0m |          -0.0030 |         104.1989 |          18.7260 |
[32m[20221213 21:12:41 @agent_ppo2.py:185][0m |          -0.0065 |         103.3226 |          18.7146 |
[32m[20221213 21:12:41 @agent_ppo2.py:185][0m |          -0.0057 |         102.8532 |          18.7170 |
[32m[20221213 21:12:42 @agent_ppo2.py:185][0m |          -0.0070 |         102.4441 |          18.7264 |
[32m[20221213 21:12:42 @agent_ppo2.py:185][0m |          -0.0024 |         105.3316 |          18.7092 |
[32m[20221213 21:12:42 @agent_ppo2.py:185][0m |           0.0010 |         109.4182 |          18.7076 |
[32m[20221213 21:12:42 @agent_ppo2.py:185][0m |          -0.0068 |         102.2699 |          18.7035 |
[32m[20221213 21:12:42 @agent_ppo2.py:185][0m |          -0.0092 |         101.9335 |          18.7046 |
[32m[20221213 21:12:42 @agent_ppo2.py:185][0m |          -0.0097 |         101.6241 |          18.7067 |
[32m[20221213 21:12:42 @agent_ppo2.py:185][0m |          -0.0095 |         101.4073 |          18.7212 |
[32m[20221213 21:12:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 603.60
[32m[20221213 21:12:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 635.00
[32m[20221213 21:12:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.00
[32m[20221213 21:12:42 @agent_ppo2.py:143][0m Total time:      17.12 min
[32m[20221213 21:12:42 @agent_ppo2.py:145][0m 1671168 total steps have happened
[32m[20221213 21:12:42 @agent_ppo2.py:121][0m #------------------------ Iteration 816 --------------------------#
[32m[20221213 21:12:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:12:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:43 @agent_ppo2.py:185][0m |          -0.0035 |         107.5421 |          18.5412 |
[32m[20221213 21:12:43 @agent_ppo2.py:185][0m |          -0.0052 |         106.0611 |          18.5279 |
[32m[20221213 21:12:43 @agent_ppo2.py:185][0m |          -0.0077 |         105.6638 |          18.5410 |
[32m[20221213 21:12:43 @agent_ppo2.py:185][0m |          -0.0088 |         105.1891 |          18.5331 |
[32m[20221213 21:12:43 @agent_ppo2.py:185][0m |          -0.0055 |         104.9409 |          18.5389 |
[32m[20221213 21:12:43 @agent_ppo2.py:185][0m |          -0.0055 |         104.5478 |          18.5461 |
[32m[20221213 21:12:43 @agent_ppo2.py:185][0m |          -0.0070 |         104.4366 |          18.5546 |
[32m[20221213 21:12:43 @agent_ppo2.py:185][0m |          -0.0079 |         104.0346 |          18.5560 |
[32m[20221213 21:12:43 @agent_ppo2.py:185][0m |           0.0016 |         105.8753 |          18.5410 |
[32m[20221213 21:12:43 @agent_ppo2.py:185][0m |          -0.0091 |         103.7607 |          18.5480 |
[32m[20221213 21:12:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 585.80
[32m[20221213 21:12:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 606.00
[32m[20221213 21:12:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 654.00
[32m[20221213 21:12:43 @agent_ppo2.py:143][0m Total time:      17.15 min
[32m[20221213 21:12:43 @agent_ppo2.py:145][0m 1673216 total steps have happened
[32m[20221213 21:12:43 @agent_ppo2.py:121][0m #------------------------ Iteration 817 --------------------------#
[32m[20221213 21:12:44 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:12:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:44 @agent_ppo2.py:185][0m |          -0.0010 |         105.6827 |          18.5888 |
[32m[20221213 21:12:44 @agent_ppo2.py:185][0m |           0.0142 |         111.9223 |          18.5855 |
[32m[20221213 21:12:44 @agent_ppo2.py:185][0m |          -0.0033 |         104.0180 |          18.5806 |
[32m[20221213 21:12:44 @agent_ppo2.py:185][0m |          -0.0029 |         103.7739 |          18.5920 |
[32m[20221213 21:12:44 @agent_ppo2.py:185][0m |          -0.0020 |         103.4166 |          18.6095 |
[32m[20221213 21:12:44 @agent_ppo2.py:185][0m |          -0.0082 |         103.1561 |          18.6004 |
[32m[20221213 21:12:44 @agent_ppo2.py:185][0m |          -0.0038 |         104.5826 |          18.5947 |
[32m[20221213 21:12:44 @agent_ppo2.py:185][0m |          -0.0004 |         104.9099 |          18.6032 |
[32m[20221213 21:12:44 @agent_ppo2.py:185][0m |          -0.0055 |         102.7109 |          18.6225 |
[32m[20221213 21:12:44 @agent_ppo2.py:185][0m |          -0.0089 |         102.5148 |          18.6182 |
[32m[20221213 21:12:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:12:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 640.20
[32m[20221213 21:12:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.00
[32m[20221213 21:12:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.00
[32m[20221213 21:12:45 @agent_ppo2.py:143][0m Total time:      17.17 min
[32m[20221213 21:12:45 @agent_ppo2.py:145][0m 1675264 total steps have happened
[32m[20221213 21:12:45 @agent_ppo2.py:121][0m #------------------------ Iteration 818 --------------------------#
[32m[20221213 21:12:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:45 @agent_ppo2.py:185][0m |          -0.0033 |         107.1669 |          18.6147 |
[32m[20221213 21:12:45 @agent_ppo2.py:185][0m |          -0.0076 |         106.6613 |          18.6135 |
[32m[20221213 21:12:45 @agent_ppo2.py:185][0m |          -0.0069 |         106.0228 |          18.6059 |
[32m[20221213 21:12:45 @agent_ppo2.py:185][0m |          -0.0056 |         106.1311 |          18.6013 |
[32m[20221213 21:12:45 @agent_ppo2.py:185][0m |          -0.0091 |         105.6993 |          18.5993 |
[32m[20221213 21:12:45 @agent_ppo2.py:185][0m |          -0.0074 |         105.0748 |          18.6085 |
[32m[20221213 21:12:46 @agent_ppo2.py:185][0m |          -0.0090 |         105.0164 |          18.6119 |
[32m[20221213 21:12:46 @agent_ppo2.py:185][0m |          -0.0049 |         106.0257 |          18.6070 |
[32m[20221213 21:12:46 @agent_ppo2.py:185][0m |           0.0008 |         111.5318 |          18.5846 |
[32m[20221213 21:12:46 @agent_ppo2.py:185][0m |          -0.0099 |         104.6672 |          18.6010 |
[32m[20221213 21:12:46 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 21:12:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 603.00
[32m[20221213 21:12:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 638.00
[32m[20221213 21:12:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 640.00
[32m[20221213 21:12:46 @agent_ppo2.py:143][0m Total time:      17.19 min
[32m[20221213 21:12:46 @agent_ppo2.py:145][0m 1677312 total steps have happened
[32m[20221213 21:12:46 @agent_ppo2.py:121][0m #------------------------ Iteration 819 --------------------------#
[32m[20221213 21:12:46 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 21:12:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:46 @agent_ppo2.py:185][0m |          -0.0031 |         103.8994 |          18.6711 |
[32m[20221213 21:12:46 @agent_ppo2.py:185][0m |          -0.0058 |         103.1758 |          18.6642 |
[32m[20221213 21:12:47 @agent_ppo2.py:185][0m |          -0.0075 |         102.7139 |          18.6601 |
[32m[20221213 21:12:47 @agent_ppo2.py:185][0m |          -0.0046 |         102.5108 |          18.6569 |
[32m[20221213 21:12:47 @agent_ppo2.py:185][0m |          -0.0073 |         102.0433 |          18.6489 |
[32m[20221213 21:12:47 @agent_ppo2.py:185][0m |          -0.0054 |         102.3996 |          18.6613 |
[32m[20221213 21:12:47 @agent_ppo2.py:185][0m |           0.0017 |         105.8370 |          18.6585 |
[32m[20221213 21:12:47 @agent_ppo2.py:185][0m |          -0.0097 |         101.6401 |          18.6633 |
[32m[20221213 21:12:47 @agent_ppo2.py:185][0m |          -0.0097 |         101.3535 |          18.6611 |
[32m[20221213 21:12:47 @agent_ppo2.py:185][0m |          -0.0124 |         101.2228 |          18.6641 |
[32m[20221213 21:12:47 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:12:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 588.80
[32m[20221213 21:12:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 668.00
[32m[20221213 21:12:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 695.00
[32m[20221213 21:12:47 @agent_ppo2.py:143][0m Total time:      17.21 min
[32m[20221213 21:12:47 @agent_ppo2.py:145][0m 1679360 total steps have happened
[32m[20221213 21:12:47 @agent_ppo2.py:121][0m #------------------------ Iteration 820 --------------------------#
[32m[20221213 21:12:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:48 @agent_ppo2.py:185][0m |          -0.0030 |         104.1390 |          18.7813 |
[32m[20221213 21:12:48 @agent_ppo2.py:185][0m |           0.0029 |         106.0354 |          18.7720 |
[32m[20221213 21:12:48 @agent_ppo2.py:185][0m |          -0.0024 |         101.9949 |          18.7692 |
[32m[20221213 21:12:48 @agent_ppo2.py:185][0m |          -0.0029 |         102.3567 |          18.7633 |
[32m[20221213 21:12:48 @agent_ppo2.py:185][0m |          -0.0046 |         100.8948 |          18.7627 |
[32m[20221213 21:12:48 @agent_ppo2.py:185][0m |          -0.0060 |          99.8895 |          18.7545 |
[32m[20221213 21:12:48 @agent_ppo2.py:185][0m |          -0.0072 |          99.3294 |          18.7634 |
[32m[20221213 21:12:48 @agent_ppo2.py:185][0m |          -0.0090 |          99.1368 |          18.7543 |
[32m[20221213 21:12:48 @agent_ppo2.py:185][0m |          -0.0083 |          98.7533 |          18.7563 |
[32m[20221213 21:12:48 @agent_ppo2.py:185][0m |           0.0011 |         103.9027 |          18.7504 |
[32m[20221213 21:12:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 614.40
[32m[20221213 21:12:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 700.00
[32m[20221213 21:12:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 643.00
[32m[20221213 21:12:48 @agent_ppo2.py:143][0m Total time:      17.23 min
[32m[20221213 21:12:48 @agent_ppo2.py:145][0m 1681408 total steps have happened
[32m[20221213 21:12:48 @agent_ppo2.py:121][0m #------------------------ Iteration 821 --------------------------#
[32m[20221213 21:12:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:49 @agent_ppo2.py:185][0m |          -0.0005 |         107.3743 |          18.7131 |
[32m[20221213 21:12:49 @agent_ppo2.py:185][0m |           0.0017 |         106.8831 |          18.7103 |
[32m[20221213 21:12:49 @agent_ppo2.py:185][0m |          -0.0041 |         105.1482 |          18.7130 |
[32m[20221213 21:12:49 @agent_ppo2.py:185][0m |           0.0022 |         110.8556 |          18.6983 |
[32m[20221213 21:12:49 @agent_ppo2.py:185][0m |          -0.0036 |         105.1496 |          18.6975 |
[32m[20221213 21:12:49 @agent_ppo2.py:185][0m |          -0.0088 |         104.5368 |          18.7029 |
[32m[20221213 21:12:49 @agent_ppo2.py:185][0m |          -0.0090 |         104.5053 |          18.7045 |
[32m[20221213 21:12:49 @agent_ppo2.py:185][0m |          -0.0075 |         104.2475 |          18.6994 |
[32m[20221213 21:12:49 @agent_ppo2.py:185][0m |          -0.0077 |         103.9781 |          18.6951 |
[32m[20221213 21:12:50 @agent_ppo2.py:185][0m |          -0.0098 |         104.0665 |          18.6927 |
[32m[20221213 21:12:50 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:12:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 606.20
[32m[20221213 21:12:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 653.00
[32m[20221213 21:12:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 659.00
[32m[20221213 21:12:50 @agent_ppo2.py:143][0m Total time:      17.25 min
[32m[20221213 21:12:50 @agent_ppo2.py:145][0m 1683456 total steps have happened
[32m[20221213 21:12:50 @agent_ppo2.py:121][0m #------------------------ Iteration 822 --------------------------#
[32m[20221213 21:12:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:50 @agent_ppo2.py:185][0m |           0.0068 |         110.2031 |          18.6666 |
[32m[20221213 21:12:50 @agent_ppo2.py:185][0m |          -0.0028 |         104.0560 |          18.6456 |
[32m[20221213 21:12:50 @agent_ppo2.py:185][0m |          -0.0020 |         102.0943 |          18.6511 |
[32m[20221213 21:12:50 @agent_ppo2.py:185][0m |           0.0181 |         117.1046 |          18.6482 |
[32m[20221213 21:12:50 @agent_ppo2.py:185][0m |          -0.0049 |         100.7163 |          18.6203 |
[32m[20221213 21:12:50 @agent_ppo2.py:185][0m |          -0.0078 |          99.9607 |          18.6277 |
[32m[20221213 21:12:50 @agent_ppo2.py:185][0m |           0.0075 |         113.9272 |          18.6321 |
[32m[20221213 21:12:51 @agent_ppo2.py:185][0m |          -0.0038 |          98.9606 |          18.6101 |
[32m[20221213 21:12:51 @agent_ppo2.py:185][0m |          -0.0033 |         100.2770 |          18.6296 |
[32m[20221213 21:12:51 @agent_ppo2.py:185][0m |          -0.0069 |          98.1816 |          18.6203 |
[32m[20221213 21:12:51 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:12:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 590.20
[32m[20221213 21:12:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 610.00
[32m[20221213 21:12:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 671.00
[32m[20221213 21:12:51 @agent_ppo2.py:143][0m Total time:      17.27 min
[32m[20221213 21:12:51 @agent_ppo2.py:145][0m 1685504 total steps have happened
[32m[20221213 21:12:51 @agent_ppo2.py:121][0m #------------------------ Iteration 823 --------------------------#
[32m[20221213 21:12:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:51 @agent_ppo2.py:185][0m |          -0.0006 |         107.0293 |          18.4779 |
[32m[20221213 21:12:51 @agent_ppo2.py:185][0m |          -0.0060 |         104.9483 |          18.4659 |
[32m[20221213 21:12:51 @agent_ppo2.py:185][0m |           0.0079 |         118.0265 |          18.4550 |
[32m[20221213 21:12:51 @agent_ppo2.py:185][0m |          -0.0068 |         103.7097 |          18.4546 |
[32m[20221213 21:12:52 @agent_ppo2.py:185][0m |          -0.0067 |         103.2411 |          18.4604 |
[32m[20221213 21:12:52 @agent_ppo2.py:185][0m |           0.0026 |         117.7027 |          18.4460 |
[32m[20221213 21:12:52 @agent_ppo2.py:185][0m |          -0.0071 |         102.7287 |          18.4464 |
[32m[20221213 21:12:52 @agent_ppo2.py:185][0m |          -0.0088 |         102.6388 |          18.4472 |
[32m[20221213 21:12:52 @agent_ppo2.py:185][0m |          -0.0090 |         102.4847 |          18.4426 |
[32m[20221213 21:12:52 @agent_ppo2.py:185][0m |          -0.0072 |         102.1152 |          18.4381 |
[32m[20221213 21:12:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 595.60
[32m[20221213 21:12:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 640.00
[32m[20221213 21:12:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 698.00
[32m[20221213 21:12:52 @agent_ppo2.py:143][0m Total time:      17.29 min
[32m[20221213 21:12:52 @agent_ppo2.py:145][0m 1687552 total steps have happened
[32m[20221213 21:12:52 @agent_ppo2.py:121][0m #------------------------ Iteration 824 --------------------------#
[32m[20221213 21:12:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:52 @agent_ppo2.py:185][0m |           0.0080 |         109.9997 |          18.6667 |
[32m[20221213 21:12:52 @agent_ppo2.py:185][0m |          -0.0004 |         100.1553 |          18.6529 |
[32m[20221213 21:12:53 @agent_ppo2.py:185][0m |          -0.0045 |          98.2943 |          18.6527 |
[32m[20221213 21:12:53 @agent_ppo2.py:185][0m |          -0.0049 |          97.6463 |          18.6497 |
[32m[20221213 21:12:53 @agent_ppo2.py:185][0m |          -0.0051 |          97.0623 |          18.6583 |
[32m[20221213 21:12:53 @agent_ppo2.py:185][0m |           0.0048 |         107.6215 |          18.6716 |
[32m[20221213 21:12:53 @agent_ppo2.py:185][0m |          -0.0063 |          95.6186 |          18.6699 |
[32m[20221213 21:12:53 @agent_ppo2.py:185][0m |          -0.0080 |          95.4245 |          18.6869 |
[32m[20221213 21:12:53 @agent_ppo2.py:185][0m |          -0.0105 |          95.1252 |          18.6832 |
[32m[20221213 21:12:53 @agent_ppo2.py:185][0m |          -0.0062 |          94.6674 |          18.6916 |
[32m[20221213 21:12:53 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:12:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 585.40
[32m[20221213 21:12:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 638.00
[32m[20221213 21:12:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 564.00
[32m[20221213 21:12:53 @agent_ppo2.py:143][0m Total time:      17.31 min
[32m[20221213 21:12:53 @agent_ppo2.py:145][0m 1689600 total steps have happened
[32m[20221213 21:12:53 @agent_ppo2.py:121][0m #------------------------ Iteration 825 --------------------------#
[32m[20221213 21:12:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:54 @agent_ppo2.py:185][0m |          -0.0014 |         108.2975 |          18.7401 |
[32m[20221213 21:12:54 @agent_ppo2.py:185][0m |          -0.0042 |         107.3282 |          18.7151 |
[32m[20221213 21:12:54 @agent_ppo2.py:185][0m |           0.0026 |         109.4201 |          18.7234 |
[32m[20221213 21:12:54 @agent_ppo2.py:185][0m |          -0.0073 |         106.2476 |          18.6953 |
[32m[20221213 21:12:54 @agent_ppo2.py:185][0m |          -0.0092 |         105.8647 |          18.6873 |
[32m[20221213 21:12:54 @agent_ppo2.py:185][0m |          -0.0048 |         105.4064 |          18.6801 |
[32m[20221213 21:12:54 @agent_ppo2.py:185][0m |          -0.0110 |         105.3281 |          18.6820 |
[32m[20221213 21:12:54 @agent_ppo2.py:185][0m |          -0.0083 |         104.9335 |          18.6772 |
[32m[20221213 21:12:54 @agent_ppo2.py:185][0m |          -0.0089 |         104.6836 |          18.6758 |
[32m[20221213 21:12:54 @agent_ppo2.py:185][0m |          -0.0092 |         104.4056 |          18.6689 |
[32m[20221213 21:12:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:12:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 612.20
[32m[20221213 21:12:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 652.00
[32m[20221213 21:12:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 617.00
[32m[20221213 21:12:54 @agent_ppo2.py:143][0m Total time:      17.33 min
[32m[20221213 21:12:54 @agent_ppo2.py:145][0m 1691648 total steps have happened
[32m[20221213 21:12:54 @agent_ppo2.py:121][0m #------------------------ Iteration 826 --------------------------#
[32m[20221213 21:12:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:55 @agent_ppo2.py:185][0m |           0.0001 |         108.6219 |          18.6981 |
[32m[20221213 21:12:55 @agent_ppo2.py:185][0m |          -0.0042 |         105.1902 |          18.6888 |
[32m[20221213 21:12:55 @agent_ppo2.py:185][0m |          -0.0045 |         103.9289 |          18.6969 |
[32m[20221213 21:12:55 @agent_ppo2.py:185][0m |          -0.0017 |         103.4374 |          18.7022 |
[32m[20221213 21:12:55 @agent_ppo2.py:185][0m |          -0.0067 |         103.1941 |          18.7066 |
[32m[20221213 21:12:55 @agent_ppo2.py:185][0m |          -0.0084 |         102.9513 |          18.7145 |
[32m[20221213 21:12:55 @agent_ppo2.py:185][0m |          -0.0018 |         103.3466 |          18.7154 |
[32m[20221213 21:12:55 @agent_ppo2.py:185][0m |          -0.0063 |         102.3110 |          18.7108 |
[32m[20221213 21:12:55 @agent_ppo2.py:185][0m |          -0.0078 |         102.2008 |          18.7141 |
[32m[20221213 21:12:56 @agent_ppo2.py:185][0m |          -0.0016 |         108.8572 |          18.7106 |
[32m[20221213 21:12:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 608.00
[32m[20221213 21:12:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 636.00
[32m[20221213 21:12:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 634.00
[32m[20221213 21:12:56 @agent_ppo2.py:143][0m Total time:      17.35 min
[32m[20221213 21:12:56 @agent_ppo2.py:145][0m 1693696 total steps have happened
[32m[20221213 21:12:56 @agent_ppo2.py:121][0m #------------------------ Iteration 827 --------------------------#
[32m[20221213 21:12:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:12:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:56 @agent_ppo2.py:185][0m |           0.0006 |         108.8890 |          18.6155 |
[32m[20221213 21:12:56 @agent_ppo2.py:185][0m |          -0.0029 |         107.4870 |          18.5838 |
[32m[20221213 21:12:56 @agent_ppo2.py:185][0m |          -0.0061 |         106.6978 |          18.5730 |
[32m[20221213 21:12:56 @agent_ppo2.py:185][0m |           0.0008 |         109.4692 |          18.5717 |
[32m[20221213 21:12:56 @agent_ppo2.py:185][0m |          -0.0067 |         105.9175 |          18.5694 |
[32m[20221213 21:12:56 @agent_ppo2.py:185][0m |          -0.0075 |         105.8644 |          18.5554 |
[32m[20221213 21:12:56 @agent_ppo2.py:185][0m |           0.0046 |         111.2274 |          18.5653 |
[32m[20221213 21:12:57 @agent_ppo2.py:185][0m |          -0.0066 |         105.2141 |          18.5666 |
[32m[20221213 21:12:57 @agent_ppo2.py:185][0m |          -0.0092 |         104.8950 |          18.5536 |
[32m[20221213 21:12:57 @agent_ppo2.py:185][0m |          -0.0083 |         104.9593 |          18.5504 |
[32m[20221213 21:12:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 576.00
[32m[20221213 21:12:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 603.00
[32m[20221213 21:12:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.00
[32m[20221213 21:12:57 @agent_ppo2.py:143][0m Total time:      17.37 min
[32m[20221213 21:12:57 @agent_ppo2.py:145][0m 1695744 total steps have happened
[32m[20221213 21:12:57 @agent_ppo2.py:121][0m #------------------------ Iteration 828 --------------------------#
[32m[20221213 21:12:57 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:12:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:57 @agent_ppo2.py:185][0m |          -0.0023 |         105.5239 |          18.6766 |
[32m[20221213 21:12:57 @agent_ppo2.py:185][0m |          -0.0041 |         104.4509 |          18.6686 |
[32m[20221213 21:12:57 @agent_ppo2.py:185][0m |          -0.0029 |         103.8534 |          18.6710 |
[32m[20221213 21:12:57 @agent_ppo2.py:185][0m |           0.0028 |         105.8707 |          18.6814 |
[32m[20221213 21:12:58 @agent_ppo2.py:185][0m |          -0.0064 |         103.2052 |          18.6915 |
[32m[20221213 21:12:58 @agent_ppo2.py:185][0m |          -0.0060 |         102.9875 |          18.6935 |
[32m[20221213 21:12:58 @agent_ppo2.py:185][0m |          -0.0076 |         102.8428 |          18.6992 |
[32m[20221213 21:12:58 @agent_ppo2.py:185][0m |          -0.0031 |         103.8999 |          18.7085 |
[32m[20221213 21:12:58 @agent_ppo2.py:185][0m |          -0.0087 |         102.5164 |          18.7131 |
[32m[20221213 21:12:58 @agent_ppo2.py:185][0m |          -0.0017 |         104.1491 |          18.7136 |
[32m[20221213 21:12:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:12:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.60
[32m[20221213 21:12:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 609.00
[32m[20221213 21:12:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 606.00
[32m[20221213 21:12:58 @agent_ppo2.py:143][0m Total time:      17.39 min
[32m[20221213 21:12:58 @agent_ppo2.py:145][0m 1697792 total steps have happened
[32m[20221213 21:12:58 @agent_ppo2.py:121][0m #------------------------ Iteration 829 --------------------------#
[32m[20221213 21:12:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:12:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:12:58 @agent_ppo2.py:185][0m |           0.0171 |         120.5111 |          18.6262 |
[32m[20221213 21:12:58 @agent_ppo2.py:185][0m |          -0.0018 |         105.9991 |          18.6204 |
[32m[20221213 21:12:59 @agent_ppo2.py:185][0m |           0.0005 |         105.5558 |          18.6293 |
[32m[20221213 21:12:59 @agent_ppo2.py:185][0m |          -0.0067 |         104.7749 |          18.6163 |
[32m[20221213 21:12:59 @agent_ppo2.py:185][0m |           0.0026 |         108.2981 |          18.6163 |
[32m[20221213 21:12:59 @agent_ppo2.py:185][0m |          -0.0055 |         104.2288 |          18.6147 |
[32m[20221213 21:12:59 @agent_ppo2.py:185][0m |          -0.0084 |         103.9450 |          18.6158 |
[32m[20221213 21:12:59 @agent_ppo2.py:185][0m |          -0.0015 |         105.2089 |          18.6119 |
[32m[20221213 21:12:59 @agent_ppo2.py:185][0m |          -0.0048 |         103.8799 |          18.5945 |
[32m[20221213 21:12:59 @agent_ppo2.py:185][0m |          -0.0038 |         104.4506 |          18.5874 |
[32m[20221213 21:12:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:12:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 585.20
[32m[20221213 21:12:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 628.00
[32m[20221213 21:12:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 605.00
[32m[20221213 21:12:59 @agent_ppo2.py:143][0m Total time:      17.41 min
[32m[20221213 21:12:59 @agent_ppo2.py:145][0m 1699840 total steps have happened
[32m[20221213 21:12:59 @agent_ppo2.py:121][0m #------------------------ Iteration 830 --------------------------#
[32m[20221213 21:12:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:00 @agent_ppo2.py:185][0m |           0.0104 |         116.0642 |          18.5854 |
[32m[20221213 21:13:00 @agent_ppo2.py:185][0m |          -0.0018 |         109.0803 |          18.5843 |
[32m[20221213 21:13:00 @agent_ppo2.py:185][0m |          -0.0023 |         107.9671 |          18.5765 |
[32m[20221213 21:13:00 @agent_ppo2.py:185][0m |          -0.0053 |         107.3391 |          18.5700 |
[32m[20221213 21:13:00 @agent_ppo2.py:185][0m |          -0.0031 |         106.9264 |          18.5711 |
[32m[20221213 21:13:00 @agent_ppo2.py:185][0m |          -0.0024 |         106.6385 |          18.5706 |
[32m[20221213 21:13:00 @agent_ppo2.py:185][0m |          -0.0054 |         106.2896 |          18.5656 |
[32m[20221213 21:13:00 @agent_ppo2.py:185][0m |          -0.0074 |         106.0369 |          18.5637 |
[32m[20221213 21:13:00 @agent_ppo2.py:185][0m |          -0.0069 |         105.7951 |          18.5584 |
[32m[20221213 21:13:00 @agent_ppo2.py:185][0m |          -0.0046 |         105.7694 |          18.5710 |
[32m[20221213 21:13:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:13:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 604.40
[32m[20221213 21:13:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 644.00
[32m[20221213 21:13:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 648.00
[32m[20221213 21:13:00 @agent_ppo2.py:143][0m Total time:      17.43 min
[32m[20221213 21:13:00 @agent_ppo2.py:145][0m 1701888 total steps have happened
[32m[20221213 21:13:00 @agent_ppo2.py:121][0m #------------------------ Iteration 831 --------------------------#
[32m[20221213 21:13:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:01 @agent_ppo2.py:185][0m |           0.0180 |         120.6671 |          18.5689 |
[32m[20221213 21:13:01 @agent_ppo2.py:185][0m |          -0.0041 |         105.9625 |          18.5636 |
[32m[20221213 21:13:01 @agent_ppo2.py:185][0m |           0.0007 |         105.9694 |          18.5664 |
[32m[20221213 21:13:01 @agent_ppo2.py:185][0m |          -0.0068 |         104.0439 |          18.5626 |
[32m[20221213 21:13:01 @agent_ppo2.py:185][0m |          -0.0069 |         103.6592 |          18.5736 |
[32m[20221213 21:13:01 @agent_ppo2.py:185][0m |          -0.0045 |         103.6869 |          18.5665 |
[32m[20221213 21:13:01 @agent_ppo2.py:185][0m |          -0.0052 |         103.3811 |          18.5662 |
[32m[20221213 21:13:01 @agent_ppo2.py:185][0m |          -0.0059 |         102.9528 |          18.5651 |
[32m[20221213 21:13:01 @agent_ppo2.py:185][0m |          -0.0078 |         103.1674 |          18.5612 |
[32m[20221213 21:13:02 @agent_ppo2.py:185][0m |          -0.0040 |         103.0304 |          18.5621 |
[32m[20221213 21:13:02 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:13:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 619.80
[32m[20221213 21:13:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 648.00
[32m[20221213 21:13:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 658.00
[32m[20221213 21:13:02 @agent_ppo2.py:143][0m Total time:      17.45 min
[32m[20221213 21:13:02 @agent_ppo2.py:145][0m 1703936 total steps have happened
[32m[20221213 21:13:02 @agent_ppo2.py:121][0m #------------------------ Iteration 832 --------------------------#
[32m[20221213 21:13:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:13:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:02 @agent_ppo2.py:185][0m |          -0.0035 |         108.5347 |          18.5689 |
[32m[20221213 21:13:02 @agent_ppo2.py:185][0m |          -0.0069 |         106.7677 |          18.5420 |
[32m[20221213 21:13:02 @agent_ppo2.py:185][0m |           0.0056 |         109.9711 |          18.5476 |
[32m[20221213 21:13:02 @agent_ppo2.py:185][0m |          -0.0008 |         105.5635 |          18.5282 |
[32m[20221213 21:13:02 @agent_ppo2.py:185][0m |          -0.0058 |         105.2443 |          18.5205 |
[32m[20221213 21:13:02 @agent_ppo2.py:185][0m |          -0.0067 |         104.9663 |          18.5208 |
[32m[20221213 21:13:02 @agent_ppo2.py:185][0m |          -0.0050 |         104.5672 |          18.5220 |
[32m[20221213 21:13:03 @agent_ppo2.py:185][0m |          -0.0055 |         104.6358 |          18.5221 |
[32m[20221213 21:13:03 @agent_ppo2.py:185][0m |          -0.0049 |         105.0592 |          18.5239 |
[32m[20221213 21:13:03 @agent_ppo2.py:185][0m |          -0.0105 |         104.1636 |          18.5154 |
[32m[20221213 21:13:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:13:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 569.80
[32m[20221213 21:13:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 674.00
[32m[20221213 21:13:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 637.00
[32m[20221213 21:13:03 @agent_ppo2.py:143][0m Total time:      17.47 min
[32m[20221213 21:13:03 @agent_ppo2.py:145][0m 1705984 total steps have happened
[32m[20221213 21:13:03 @agent_ppo2.py:121][0m #------------------------ Iteration 833 --------------------------#
[32m[20221213 21:13:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:03 @agent_ppo2.py:185][0m |           0.0176 |         118.2153 |          18.5969 |
[32m[20221213 21:13:03 @agent_ppo2.py:185][0m |          -0.0038 |         104.3087 |          18.5929 |
[32m[20221213 21:13:03 @agent_ppo2.py:185][0m |           0.0018 |         104.6967 |          18.5852 |
[32m[20221213 21:13:03 @agent_ppo2.py:185][0m |          -0.0036 |         102.9523 |          18.5901 |
[32m[20221213 21:13:04 @agent_ppo2.py:185][0m |          -0.0007 |         103.2998 |          18.5827 |
[32m[20221213 21:13:04 @agent_ppo2.py:185][0m |          -0.0065 |         102.3280 |          18.5853 |
[32m[20221213 21:13:04 @agent_ppo2.py:185][0m |          -0.0078 |         102.2206 |          18.5891 |
[32m[20221213 21:13:04 @agent_ppo2.py:185][0m |          -0.0073 |         101.9398 |          18.5857 |
[32m[20221213 21:13:04 @agent_ppo2.py:185][0m |          -0.0080 |         101.7866 |          18.5818 |
[32m[20221213 21:13:04 @agent_ppo2.py:185][0m |          -0.0068 |         101.7009 |          18.5810 |
[32m[20221213 21:13:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:13:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 645.00
[32m[20221213 21:13:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 692.00
[32m[20221213 21:13:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 713.00
[32m[20221213 21:13:04 @agent_ppo2.py:143][0m Total time:      17.49 min
[32m[20221213 21:13:04 @agent_ppo2.py:145][0m 1708032 total steps have happened
[32m[20221213 21:13:04 @agent_ppo2.py:121][0m #------------------------ Iteration 834 --------------------------#
[32m[20221213 21:13:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:04 @agent_ppo2.py:185][0m |           0.0001 |         107.8308 |          18.6986 |
[32m[20221213 21:13:04 @agent_ppo2.py:185][0m |          -0.0010 |         107.7698 |          18.6955 |
[32m[20221213 21:13:05 @agent_ppo2.py:185][0m |          -0.0045 |         105.8114 |          18.6737 |
[32m[20221213 21:13:05 @agent_ppo2.py:185][0m |          -0.0083 |         105.2129 |          18.6791 |
[32m[20221213 21:13:05 @agent_ppo2.py:185][0m |          -0.0086 |         105.0166 |          18.6846 |
[32m[20221213 21:13:05 @agent_ppo2.py:185][0m |          -0.0093 |         104.9690 |          18.6809 |
[32m[20221213 21:13:05 @agent_ppo2.py:185][0m |          -0.0045 |         106.1446 |          18.6735 |
[32m[20221213 21:13:05 @agent_ppo2.py:185][0m |          -0.0075 |         104.4022 |          18.6808 |
[32m[20221213 21:13:05 @agent_ppo2.py:185][0m |           0.0022 |         113.2204 |          18.6802 |
[32m[20221213 21:13:05 @agent_ppo2.py:185][0m |          -0.0024 |         107.2595 |          18.6616 |
[32m[20221213 21:13:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:13:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 635.80
[32m[20221213 21:13:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 675.00
[32m[20221213 21:13:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 665.00
[32m[20221213 21:13:05 @agent_ppo2.py:143][0m Total time:      17.51 min
[32m[20221213 21:13:05 @agent_ppo2.py:145][0m 1710080 total steps have happened
[32m[20221213 21:13:05 @agent_ppo2.py:121][0m #------------------------ Iteration 835 --------------------------#
[32m[20221213 21:13:05 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:13:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:06 @agent_ppo2.py:185][0m |          -0.0021 |         110.0679 |          18.4198 |
[32m[20221213 21:13:06 @agent_ppo2.py:185][0m |          -0.0066 |         108.1021 |          18.4171 |
[32m[20221213 21:13:06 @agent_ppo2.py:185][0m |          -0.0066 |         107.0290 |          18.4147 |
[32m[20221213 21:13:06 @agent_ppo2.py:185][0m |          -0.0051 |         106.2493 |          18.4186 |
[32m[20221213 21:13:06 @agent_ppo2.py:185][0m |          -0.0057 |         106.8250 |          18.4243 |
[32m[20221213 21:13:06 @agent_ppo2.py:185][0m |          -0.0102 |         105.3318 |          18.4231 |
[32m[20221213 21:13:06 @agent_ppo2.py:185][0m |          -0.0121 |         105.0515 |          18.4262 |
[32m[20221213 21:13:06 @agent_ppo2.py:185][0m |          -0.0101 |         104.5964 |          18.4192 |
[32m[20221213 21:13:06 @agent_ppo2.py:185][0m |          -0.0006 |         112.9356 |          18.4235 |
[32m[20221213 21:13:06 @agent_ppo2.py:185][0m |          -0.0105 |         104.2198 |          18.4160 |
[32m[20221213 21:13:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:13:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.60
[32m[20221213 21:13:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 672.00
[32m[20221213 21:13:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 556.00
[32m[20221213 21:13:06 @agent_ppo2.py:143][0m Total time:      17.53 min
[32m[20221213 21:13:06 @agent_ppo2.py:145][0m 1712128 total steps have happened
[32m[20221213 21:13:06 @agent_ppo2.py:121][0m #------------------------ Iteration 836 --------------------------#
[32m[20221213 21:13:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:07 @agent_ppo2.py:185][0m |           0.0042 |         112.5779 |          18.6653 |
[32m[20221213 21:13:07 @agent_ppo2.py:185][0m |          -0.0030 |         110.7665 |          18.6524 |
[32m[20221213 21:13:07 @agent_ppo2.py:185][0m |          -0.0028 |         110.5093 |          18.6507 |
[32m[20221213 21:13:07 @agent_ppo2.py:185][0m |          -0.0074 |         109.8532 |          18.6571 |
[32m[20221213 21:13:07 @agent_ppo2.py:185][0m |          -0.0069 |         109.7513 |          18.6529 |
[32m[20221213 21:13:07 @agent_ppo2.py:185][0m |          -0.0059 |         109.3994 |          18.6487 |
[32m[20221213 21:13:07 @agent_ppo2.py:185][0m |          -0.0068 |         109.3251 |          18.6524 |
[32m[20221213 21:13:07 @agent_ppo2.py:185][0m |          -0.0071 |         109.1305 |          18.6481 |
[32m[20221213 21:13:07 @agent_ppo2.py:185][0m |          -0.0090 |         109.1275 |          18.6404 |
[32m[20221213 21:13:08 @agent_ppo2.py:185][0m |          -0.0014 |         114.9928 |          18.6438 |
[32m[20221213 21:13:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:13:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.80
[32m[20221213 21:13:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 646.00
[32m[20221213 21:13:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 710.00
[32m[20221213 21:13:08 @agent_ppo2.py:143][0m Total time:      17.55 min
[32m[20221213 21:13:08 @agent_ppo2.py:145][0m 1714176 total steps have happened
[32m[20221213 21:13:08 @agent_ppo2.py:121][0m #------------------------ Iteration 837 --------------------------#
[32m[20221213 21:13:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:08 @agent_ppo2.py:185][0m |           0.0021 |         116.4032 |          18.7954 |
[32m[20221213 21:13:08 @agent_ppo2.py:185][0m |          -0.0061 |         113.9360 |          18.7930 |
[32m[20221213 21:13:08 @agent_ppo2.py:185][0m |          -0.0022 |         114.0922 |          18.7901 |
[32m[20221213 21:13:08 @agent_ppo2.py:185][0m |          -0.0047 |         113.1583 |          18.7871 |
[32m[20221213 21:13:08 @agent_ppo2.py:185][0m |           0.0062 |         123.2696 |          18.7845 |
[32m[20221213 21:13:08 @agent_ppo2.py:185][0m |          -0.0068 |         112.6415 |          18.7800 |
[32m[20221213 21:13:08 @agent_ppo2.py:185][0m |          -0.0063 |         112.2623 |          18.7783 |
[32m[20221213 21:13:09 @agent_ppo2.py:185][0m |          -0.0050 |         112.1757 |          18.7795 |
[32m[20221213 21:13:09 @agent_ppo2.py:185][0m |          -0.0102 |         112.2265 |          18.7664 |
[32m[20221213 21:13:09 @agent_ppo2.py:185][0m |           0.0018 |         120.5310 |          18.7555 |
[32m[20221213 21:13:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:13:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 659.40
[32m[20221213 21:13:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.00
[32m[20221213 21:13:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 642.00
[32m[20221213 21:13:09 @agent_ppo2.py:143][0m Total time:      17.57 min
[32m[20221213 21:13:09 @agent_ppo2.py:145][0m 1716224 total steps have happened
[32m[20221213 21:13:09 @agent_ppo2.py:121][0m #------------------------ Iteration 838 --------------------------#
[32m[20221213 21:13:09 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:13:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:09 @agent_ppo2.py:185][0m |           0.0008 |         112.2584 |          18.4471 |
[32m[20221213 21:13:09 @agent_ppo2.py:185][0m |          -0.0068 |         111.1264 |          18.4556 |
[32m[20221213 21:13:09 @agent_ppo2.py:185][0m |          -0.0077 |         110.6182 |          18.4490 |
[32m[20221213 21:13:09 @agent_ppo2.py:185][0m |           0.0031 |         115.9883 |          18.4564 |
[32m[20221213 21:13:09 @agent_ppo2.py:185][0m |          -0.0064 |         109.7491 |          18.4421 |
[32m[20221213 21:13:10 @agent_ppo2.py:185][0m |          -0.0090 |         109.5357 |          18.4531 |
[32m[20221213 21:13:10 @agent_ppo2.py:185][0m |           0.0021 |         115.8926 |          18.4589 |
[32m[20221213 21:13:10 @agent_ppo2.py:185][0m |          -0.0072 |         109.1401 |          18.4612 |
[32m[20221213 21:13:10 @agent_ppo2.py:185][0m |          -0.0092 |         108.8045 |          18.4602 |
[32m[20221213 21:13:10 @agent_ppo2.py:185][0m |          -0.0093 |         108.5686 |          18.4738 |
[32m[20221213 21:13:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:13:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 622.40
[32m[20221213 21:13:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 668.00
[32m[20221213 21:13:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 653.00
[32m[20221213 21:13:10 @agent_ppo2.py:143][0m Total time:      17.59 min
[32m[20221213 21:13:10 @agent_ppo2.py:145][0m 1718272 total steps have happened
[32m[20221213 21:13:10 @agent_ppo2.py:121][0m #------------------------ Iteration 839 --------------------------#
[32m[20221213 21:13:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:10 @agent_ppo2.py:185][0m |           0.0037 |         113.3819 |          18.6614 |
[32m[20221213 21:13:10 @agent_ppo2.py:185][0m |           0.0050 |         118.4489 |          18.6538 |
[32m[20221213 21:13:11 @agent_ppo2.py:185][0m |          -0.0047 |         111.5482 |          18.6492 |
[32m[20221213 21:13:11 @agent_ppo2.py:185][0m |          -0.0070 |         111.2696 |          18.6660 |
[32m[20221213 21:13:11 @agent_ppo2.py:185][0m |          -0.0079 |         110.9282 |          18.6726 |
[32m[20221213 21:13:11 @agent_ppo2.py:185][0m |          -0.0068 |         110.7262 |          18.6839 |
[32m[20221213 21:13:11 @agent_ppo2.py:185][0m |           0.0116 |         124.6051 |          18.6941 |
[32m[20221213 21:13:11 @agent_ppo2.py:185][0m |          -0.0076 |         110.6456 |          18.6697 |
[32m[20221213 21:13:11 @agent_ppo2.py:185][0m |          -0.0083 |         110.1176 |          18.6942 |
[32m[20221213 21:13:11 @agent_ppo2.py:185][0m |          -0.0089 |         109.9297 |          18.6995 |
[32m[20221213 21:13:11 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:13:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 629.60
[32m[20221213 21:13:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 685.00
[32m[20221213 21:13:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 616.00
[32m[20221213 21:13:11 @agent_ppo2.py:143][0m Total time:      17.61 min
[32m[20221213 21:13:11 @agent_ppo2.py:145][0m 1720320 total steps have happened
[32m[20221213 21:13:11 @agent_ppo2.py:121][0m #------------------------ Iteration 840 --------------------------#
[32m[20221213 21:13:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:12 @agent_ppo2.py:185][0m |          -0.0009 |         113.5640 |          18.7089 |
[32m[20221213 21:13:12 @agent_ppo2.py:185][0m |          -0.0038 |         112.8425 |          18.7174 |
[32m[20221213 21:13:12 @agent_ppo2.py:185][0m |          -0.0030 |         112.2002 |          18.7172 |
[32m[20221213 21:13:12 @agent_ppo2.py:185][0m |          -0.0075 |         111.8376 |          18.7312 |
[32m[20221213 21:13:12 @agent_ppo2.py:185][0m |          -0.0083 |         111.6855 |          18.7362 |
[32m[20221213 21:13:12 @agent_ppo2.py:185][0m |           0.0047 |         115.5458 |          18.7373 |
[32m[20221213 21:13:12 @agent_ppo2.py:185][0m |          -0.0075 |         111.2804 |          18.7498 |
[32m[20221213 21:13:12 @agent_ppo2.py:185][0m |          -0.0072 |         111.2488 |          18.7554 |
[32m[20221213 21:13:12 @agent_ppo2.py:185][0m |          -0.0083 |         111.1809 |          18.7617 |
[32m[20221213 21:13:12 @agent_ppo2.py:185][0m |           0.0034 |         122.2975 |          18.7671 |
[32m[20221213 21:13:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:13:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 594.20
[32m[20221213 21:13:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 661.00
[32m[20221213 21:13:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 715.00
[32m[20221213 21:13:12 @agent_ppo2.py:143][0m Total time:      17.63 min
[32m[20221213 21:13:12 @agent_ppo2.py:145][0m 1722368 total steps have happened
[32m[20221213 21:13:12 @agent_ppo2.py:121][0m #------------------------ Iteration 841 --------------------------#
[32m[20221213 21:13:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:13 @agent_ppo2.py:185][0m |          -0.0009 |         111.0357 |          18.6419 |
[32m[20221213 21:13:13 @agent_ppo2.py:185][0m |          -0.0028 |         110.3062 |          18.6184 |
[32m[20221213 21:13:13 @agent_ppo2.py:185][0m |          -0.0072 |         109.5825 |          18.6184 |
[32m[20221213 21:13:13 @agent_ppo2.py:185][0m |          -0.0062 |         109.0694 |          18.6187 |
[32m[20221213 21:13:13 @agent_ppo2.py:185][0m |          -0.0080 |         108.7546 |          18.6207 |
[32m[20221213 21:13:13 @agent_ppo2.py:185][0m |          -0.0075 |         108.4814 |          18.6117 |
[32m[20221213 21:13:13 @agent_ppo2.py:185][0m |          -0.0091 |         108.4862 |          18.6210 |
[32m[20221213 21:13:13 @agent_ppo2.py:185][0m |          -0.0075 |         108.1519 |          18.6011 |
[32m[20221213 21:13:13 @agent_ppo2.py:185][0m |          -0.0078 |         107.9805 |          18.6041 |
[32m[20221213 21:13:14 @agent_ppo2.py:185][0m |          -0.0070 |         107.7905 |          18.5953 |
[32m[20221213 21:13:14 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:13:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 626.80
[32m[20221213 21:13:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 636.00
[32m[20221213 21:13:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 679.00
[32m[20221213 21:13:14 @agent_ppo2.py:143][0m Total time:      17.65 min
[32m[20221213 21:13:14 @agent_ppo2.py:145][0m 1724416 total steps have happened
[32m[20221213 21:13:14 @agent_ppo2.py:121][0m #------------------------ Iteration 842 --------------------------#
[32m[20221213 21:13:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:13:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:14 @agent_ppo2.py:185][0m |          -0.0036 |         115.1725 |          18.6115 |
[32m[20221213 21:13:14 @agent_ppo2.py:185][0m |          -0.0049 |         112.3220 |          18.6154 |
[32m[20221213 21:13:14 @agent_ppo2.py:185][0m |          -0.0070 |         110.8735 |          18.6115 |
[32m[20221213 21:13:14 @agent_ppo2.py:185][0m |          -0.0085 |         110.4154 |          18.6167 |
[32m[20221213 21:13:14 @agent_ppo2.py:185][0m |          -0.0079 |         109.8727 |          18.6178 |
[32m[20221213 21:13:15 @agent_ppo2.py:185][0m |          -0.0079 |         109.1355 |          18.6170 |
[32m[20221213 21:13:15 @agent_ppo2.py:185][0m |          -0.0076 |         108.8464 |          18.6150 |
[32m[20221213 21:13:15 @agent_ppo2.py:185][0m |          -0.0082 |         108.5255 |          18.6189 |
[32m[20221213 21:13:15 @agent_ppo2.py:185][0m |          -0.0011 |         109.9335 |          18.6229 |
[32m[20221213 21:13:15 @agent_ppo2.py:185][0m |          -0.0101 |         108.0204 |          18.6236 |
[32m[20221213 21:13:15 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 21:13:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 575.40
[32m[20221213 21:13:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 606.00
[32m[20221213 21:13:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 661.00
[32m[20221213 21:13:15 @agent_ppo2.py:143][0m Total time:      17.67 min
[32m[20221213 21:13:15 @agent_ppo2.py:145][0m 1726464 total steps have happened
[32m[20221213 21:13:15 @agent_ppo2.py:121][0m #------------------------ Iteration 843 --------------------------#
[32m[20221213 21:13:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:15 @agent_ppo2.py:185][0m |           0.0004 |         115.6936 |          18.7753 |
[32m[20221213 21:13:15 @agent_ppo2.py:185][0m |          -0.0014 |         113.7675 |          18.7735 |
[32m[20221213 21:13:16 @agent_ppo2.py:185][0m |          -0.0016 |         112.4956 |          18.7718 |
[32m[20221213 21:13:16 @agent_ppo2.py:185][0m |          -0.0041 |         111.3889 |          18.7790 |
[32m[20221213 21:13:16 @agent_ppo2.py:185][0m |          -0.0038 |         110.3855 |          18.7734 |
[32m[20221213 21:13:16 @agent_ppo2.py:185][0m |          -0.0058 |         110.2218 |          18.7618 |
[32m[20221213 21:13:16 @agent_ppo2.py:185][0m |          -0.0082 |         109.2212 |          18.7638 |
[32m[20221213 21:13:16 @agent_ppo2.py:185][0m |          -0.0065 |         108.9256 |          18.7641 |
[32m[20221213 21:13:16 @agent_ppo2.py:185][0m |          -0.0068 |         108.2883 |          18.7674 |
[32m[20221213 21:13:16 @agent_ppo2.py:185][0m |          -0.0074 |         107.9848 |          18.7642 |
[32m[20221213 21:13:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:13:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 620.60
[32m[20221213 21:13:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.00
[32m[20221213 21:13:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 646.00
[32m[20221213 21:13:16 @agent_ppo2.py:143][0m Total time:      17.69 min
[32m[20221213 21:13:16 @agent_ppo2.py:145][0m 1728512 total steps have happened
[32m[20221213 21:13:16 @agent_ppo2.py:121][0m #------------------------ Iteration 844 --------------------------#
[32m[20221213 21:13:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:17 @agent_ppo2.py:185][0m |           0.0027 |         114.8435 |          18.6491 |
[32m[20221213 21:13:17 @agent_ppo2.py:185][0m |           0.0002 |         112.1910 |          18.6298 |
[32m[20221213 21:13:17 @agent_ppo2.py:185][0m |          -0.0084 |         108.9876 |          18.6201 |
[32m[20221213 21:13:17 @agent_ppo2.py:185][0m |          -0.0066 |         108.3850 |          18.6130 |
[32m[20221213 21:13:17 @agent_ppo2.py:185][0m |          -0.0096 |         108.2033 |          18.6031 |
[32m[20221213 21:13:17 @agent_ppo2.py:185][0m |          -0.0115 |         107.9234 |          18.6099 |
[32m[20221213 21:13:17 @agent_ppo2.py:185][0m |          -0.0110 |         107.8094 |          18.5963 |
[32m[20221213 21:13:17 @agent_ppo2.py:185][0m |          -0.0107 |         107.5517 |          18.5872 |
[32m[20221213 21:13:17 @agent_ppo2.py:185][0m |          -0.0108 |         107.4924 |          18.5894 |
[32m[20221213 21:13:17 @agent_ppo2.py:185][0m |          -0.0110 |         107.5406 |          18.5887 |
[32m[20221213 21:13:17 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:13:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 582.80
[32m[20221213 21:13:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.00
[32m[20221213 21:13:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 670.00
[32m[20221213 21:13:18 @agent_ppo2.py:143][0m Total time:      17.71 min
[32m[20221213 21:13:18 @agent_ppo2.py:145][0m 1730560 total steps have happened
[32m[20221213 21:13:18 @agent_ppo2.py:121][0m #------------------------ Iteration 845 --------------------------#
[32m[20221213 21:13:18 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 21:13:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:18 @agent_ppo2.py:185][0m |           0.0134 |         124.0882 |          18.7342 |
[32m[20221213 21:13:18 @agent_ppo2.py:185][0m |          -0.0009 |         109.4121 |          18.7104 |
[32m[20221213 21:13:18 @agent_ppo2.py:185][0m |          -0.0041 |         108.6030 |          18.7208 |
[32m[20221213 21:13:18 @agent_ppo2.py:185][0m |          -0.0052 |         108.4108 |          18.7191 |
[32m[20221213 21:13:18 @agent_ppo2.py:185][0m |          -0.0056 |         107.9186 |          18.7245 |
[32m[20221213 21:13:18 @agent_ppo2.py:185][0m |          -0.0086 |         107.8662 |          18.7119 |
[32m[20221213 21:13:18 @agent_ppo2.py:185][0m |          -0.0065 |         107.4709 |          18.7138 |
[32m[20221213 21:13:19 @agent_ppo2.py:185][0m |          -0.0065 |         107.5824 |          18.7148 |
[32m[20221213 21:13:19 @agent_ppo2.py:185][0m |          -0.0061 |         107.1612 |          18.7284 |
[32m[20221213 21:13:19 @agent_ppo2.py:185][0m |          -0.0102 |         107.1984 |          18.7288 |
[32m[20221213 21:13:19 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:13:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 611.20
[32m[20221213 21:13:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 656.00
[32m[20221213 21:13:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 640.00
[32m[20221213 21:13:19 @agent_ppo2.py:143][0m Total time:      17.74 min
[32m[20221213 21:13:19 @agent_ppo2.py:145][0m 1732608 total steps have happened
[32m[20221213 21:13:19 @agent_ppo2.py:121][0m #------------------------ Iteration 846 --------------------------#
[32m[20221213 21:13:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:19 @agent_ppo2.py:185][0m |          -0.0001 |         108.9201 |          18.6777 |
[32m[20221213 21:13:19 @agent_ppo2.py:185][0m |          -0.0046 |         107.8732 |          18.6783 |
[32m[20221213 21:13:19 @agent_ppo2.py:185][0m |          -0.0011 |         109.0929 |          18.6784 |
[32m[20221213 21:13:19 @agent_ppo2.py:185][0m |           0.0005 |         108.1603 |          18.6751 |
[32m[20221213 21:13:19 @agent_ppo2.py:185][0m |          -0.0069 |         106.6205 |          18.6750 |
[32m[20221213 21:13:20 @agent_ppo2.py:185][0m |          -0.0054 |         106.2796 |          18.6724 |
[32m[20221213 21:13:20 @agent_ppo2.py:185][0m |          -0.0052 |         106.1273 |          18.6588 |
[32m[20221213 21:13:20 @agent_ppo2.py:185][0m |          -0.0084 |         105.9137 |          18.6544 |
[32m[20221213 21:13:20 @agent_ppo2.py:185][0m |          -0.0092 |         105.7376 |          18.6338 |
[32m[20221213 21:13:20 @agent_ppo2.py:185][0m |          -0.0044 |         106.6013 |          18.6657 |
[32m[20221213 21:13:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:13:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 617.20
[32m[20221213 21:13:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 623.00
[32m[20221213 21:13:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.00
[32m[20221213 21:13:20 @agent_ppo2.py:143][0m Total time:      17.75 min
[32m[20221213 21:13:20 @agent_ppo2.py:145][0m 1734656 total steps have happened
[32m[20221213 21:13:20 @agent_ppo2.py:121][0m #------------------------ Iteration 847 --------------------------#
[32m[20221213 21:13:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:20 @agent_ppo2.py:185][0m |           0.0037 |         118.4486 |          18.5895 |
[32m[20221213 21:13:20 @agent_ppo2.py:185][0m |          -0.0050 |         112.9056 |          18.5859 |
[32m[20221213 21:13:20 @agent_ppo2.py:185][0m |          -0.0064 |         111.6171 |          18.5735 |
[32m[20221213 21:13:21 @agent_ppo2.py:185][0m |           0.0021 |         114.2143 |          18.5625 |
[32m[20221213 21:13:21 @agent_ppo2.py:185][0m |          -0.0042 |         110.0069 |          18.5656 |
[32m[20221213 21:13:21 @agent_ppo2.py:185][0m |          -0.0082 |         109.2222 |          18.5407 |
[32m[20221213 21:13:21 @agent_ppo2.py:185][0m |           0.0009 |         112.7575 |          18.5444 |
[32m[20221213 21:13:21 @agent_ppo2.py:185][0m |          -0.0078 |         108.2795 |          18.5411 |
[32m[20221213 21:13:21 @agent_ppo2.py:185][0m |          -0.0116 |         107.9924 |          18.5392 |
[32m[20221213 21:13:21 @agent_ppo2.py:185][0m |          -0.0044 |         108.8543 |          18.5253 |
[32m[20221213 21:13:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:13:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 646.60
[32m[20221213 21:13:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 683.00
[32m[20221213 21:13:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 695.00
[32m[20221213 21:13:21 @agent_ppo2.py:143][0m Total time:      17.77 min
[32m[20221213 21:13:21 @agent_ppo2.py:145][0m 1736704 total steps have happened
[32m[20221213 21:13:21 @agent_ppo2.py:121][0m #------------------------ Iteration 848 --------------------------#
[32m[20221213 21:13:21 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:13:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:21 @agent_ppo2.py:185][0m |          -0.0018 |         115.6522 |          18.5601 |
[32m[20221213 21:13:22 @agent_ppo2.py:185][0m |           0.0088 |         129.4614 |          18.5586 |
[32m[20221213 21:13:22 @agent_ppo2.py:185][0m |          -0.0014 |         114.1308 |          18.5391 |
[32m[20221213 21:13:22 @agent_ppo2.py:185][0m |          -0.0073 |         113.0743 |          18.5384 |
[32m[20221213 21:13:22 @agent_ppo2.py:185][0m |          -0.0062 |         112.4346 |          18.5364 |
[32m[20221213 21:13:22 @agent_ppo2.py:185][0m |          -0.0086 |         112.1624 |          18.5357 |
[32m[20221213 21:13:22 @agent_ppo2.py:185][0m |          -0.0065 |         111.9983 |          18.5426 |
[32m[20221213 21:13:22 @agent_ppo2.py:185][0m |          -0.0096 |         111.8804 |          18.5519 |
[32m[20221213 21:13:22 @agent_ppo2.py:185][0m |          -0.0095 |         111.5396 |          18.5327 |
[32m[20221213 21:13:22 @agent_ppo2.py:185][0m |          -0.0081 |         111.4612 |          18.5455 |
[32m[20221213 21:13:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:13:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 619.20
[32m[20221213 21:13:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 646.00
[32m[20221213 21:13:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 724.00
[32m[20221213 21:13:22 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 724.00
[32m[20221213 21:13:22 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 724.00
[32m[20221213 21:13:22 @agent_ppo2.py:143][0m Total time:      17.80 min
[32m[20221213 21:13:22 @agent_ppo2.py:145][0m 1738752 total steps have happened
[32m[20221213 21:13:22 @agent_ppo2.py:121][0m #------------------------ Iteration 849 --------------------------#
[32m[20221213 21:13:23 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:13:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:23 @agent_ppo2.py:185][0m |          -0.0012 |         111.8902 |          18.6609 |
[32m[20221213 21:13:23 @agent_ppo2.py:185][0m |          -0.0029 |         110.5885 |          18.6584 |
[32m[20221213 21:13:23 @agent_ppo2.py:185][0m |          -0.0049 |         109.9474 |          18.6411 |
[32m[20221213 21:13:23 @agent_ppo2.py:185][0m |          -0.0025 |         109.2921 |          18.6361 |
[32m[20221213 21:13:23 @agent_ppo2.py:185][0m |          -0.0091 |         108.8168 |          18.6319 |
[32m[20221213 21:13:23 @agent_ppo2.py:185][0m |          -0.0075 |         108.3222 |          18.6318 |
[32m[20221213 21:13:23 @agent_ppo2.py:185][0m |          -0.0075 |         107.9555 |          18.6361 |
[32m[20221213 21:13:23 @agent_ppo2.py:185][0m |          -0.0011 |         110.7471 |          18.6290 |
[32m[20221213 21:13:23 @agent_ppo2.py:185][0m |          -0.0054 |         107.3461 |          18.5932 |
[32m[20221213 21:13:24 @agent_ppo2.py:185][0m |          -0.0083 |         107.0963 |          18.6149 |
[32m[20221213 21:13:24 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:13:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 656.00
[32m[20221213 21:13:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 674.00
[32m[20221213 21:13:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.00
[32m[20221213 21:13:24 @agent_ppo2.py:143][0m Total time:      17.82 min
[32m[20221213 21:13:24 @agent_ppo2.py:145][0m 1740800 total steps have happened
[32m[20221213 21:13:24 @agent_ppo2.py:121][0m #------------------------ Iteration 850 --------------------------#
[32m[20221213 21:13:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:24 @agent_ppo2.py:185][0m |          -0.0017 |         111.8425 |          18.5844 |
[32m[20221213 21:13:24 @agent_ppo2.py:185][0m |          -0.0046 |         110.7735 |          18.5718 |
[32m[20221213 21:13:24 @agent_ppo2.py:185][0m |          -0.0073 |         110.2633 |          18.5708 |
[32m[20221213 21:13:24 @agent_ppo2.py:185][0m |          -0.0063 |         109.9322 |          18.5638 |
[32m[20221213 21:13:24 @agent_ppo2.py:185][0m |          -0.0066 |         109.5000 |          18.5564 |
[32m[20221213 21:13:24 @agent_ppo2.py:185][0m |          -0.0089 |         109.3964 |          18.5737 |
[32m[20221213 21:13:24 @agent_ppo2.py:185][0m |          -0.0088 |         109.0485 |          18.5669 |
[32m[20221213 21:13:25 @agent_ppo2.py:185][0m |          -0.0076 |         109.1433 |          18.5662 |
[32m[20221213 21:13:25 @agent_ppo2.py:185][0m |          -0.0084 |         108.6641 |          18.5668 |
[32m[20221213 21:13:25 @agent_ppo2.py:185][0m |          -0.0044 |         109.0625 |          18.5569 |
[32m[20221213 21:13:25 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:13:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 629.80
[32m[20221213 21:13:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 659.00
[32m[20221213 21:13:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 606.00
[32m[20221213 21:13:25 @agent_ppo2.py:143][0m Total time:      17.84 min
[32m[20221213 21:13:25 @agent_ppo2.py:145][0m 1742848 total steps have happened
[32m[20221213 21:13:25 @agent_ppo2.py:121][0m #------------------------ Iteration 851 --------------------------#
[32m[20221213 21:13:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:25 @agent_ppo2.py:185][0m |          -0.0021 |         112.0169 |          18.3922 |
[32m[20221213 21:13:25 @agent_ppo2.py:185][0m |          -0.0017 |         111.1335 |          18.3923 |
[32m[20221213 21:13:25 @agent_ppo2.py:185][0m |          -0.0036 |         110.6496 |          18.3946 |
[32m[20221213 21:13:25 @agent_ppo2.py:185][0m |           0.0043 |         116.3456 |          18.4008 |
[32m[20221213 21:13:25 @agent_ppo2.py:185][0m |          -0.0021 |         111.5558 |          18.3981 |
[32m[20221213 21:13:26 @agent_ppo2.py:185][0m |           0.0002 |         113.9047 |          18.4034 |
[32m[20221213 21:13:26 @agent_ppo2.py:185][0m |          -0.0063 |         109.7302 |          18.3918 |
[32m[20221213 21:13:26 @agent_ppo2.py:185][0m |          -0.0070 |         109.3794 |          18.4046 |
[32m[20221213 21:13:26 @agent_ppo2.py:185][0m |           0.0056 |         123.7221 |          18.4106 |
[32m[20221213 21:13:26 @agent_ppo2.py:185][0m |           0.0001 |         113.2024 |          18.4096 |
[32m[20221213 21:13:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:13:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 626.40
[32m[20221213 21:13:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 680.00
[32m[20221213 21:13:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 620.00
[32m[20221213 21:13:26 @agent_ppo2.py:143][0m Total time:      17.86 min
[32m[20221213 21:13:26 @agent_ppo2.py:145][0m 1744896 total steps have happened
[32m[20221213 21:13:26 @agent_ppo2.py:121][0m #------------------------ Iteration 852 --------------------------#
[32m[20221213 21:13:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:26 @agent_ppo2.py:185][0m |          -0.0004 |         110.8161 |          18.6051 |
[32m[20221213 21:13:26 @agent_ppo2.py:185][0m |           0.0100 |         120.7231 |          18.6032 |
[32m[20221213 21:13:27 @agent_ppo2.py:185][0m |          -0.0044 |         109.8621 |          18.5547 |
[32m[20221213 21:13:27 @agent_ppo2.py:185][0m |          -0.0058 |         109.4408 |          18.5698 |
[32m[20221213 21:13:27 @agent_ppo2.py:185][0m |           0.0019 |         118.3410 |          18.5594 |
[32m[20221213 21:13:27 @agent_ppo2.py:185][0m |          -0.0026 |         110.2922 |          18.5461 |
[32m[20221213 21:13:27 @agent_ppo2.py:185][0m |          -0.0049 |         108.9685 |          18.5446 |
[32m[20221213 21:13:27 @agent_ppo2.py:185][0m |          -0.0057 |         108.8668 |          18.5474 |
[32m[20221213 21:13:27 @agent_ppo2.py:185][0m |          -0.0049 |         109.3066 |          18.5520 |
[32m[20221213 21:13:27 @agent_ppo2.py:185][0m |          -0.0033 |         111.0576 |          18.5388 |
[32m[20221213 21:13:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:13:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 599.20
[32m[20221213 21:13:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 623.00
[32m[20221213 21:13:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 668.00
[32m[20221213 21:13:27 @agent_ppo2.py:143][0m Total time:      17.88 min
[32m[20221213 21:13:27 @agent_ppo2.py:145][0m 1746944 total steps have happened
[32m[20221213 21:13:27 @agent_ppo2.py:121][0m #------------------------ Iteration 853 --------------------------#
[32m[20221213 21:13:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:28 @agent_ppo2.py:185][0m |           0.0013 |         111.1822 |          18.5345 |
[32m[20221213 21:13:28 @agent_ppo2.py:185][0m |          -0.0037 |         109.7907 |          18.5158 |
[32m[20221213 21:13:28 @agent_ppo2.py:185][0m |          -0.0021 |         109.5768 |          18.5224 |
[32m[20221213 21:13:28 @agent_ppo2.py:185][0m |          -0.0046 |         109.0541 |          18.5205 |
[32m[20221213 21:13:28 @agent_ppo2.py:185][0m |          -0.0036 |         109.5741 |          18.5137 |
[32m[20221213 21:13:28 @agent_ppo2.py:185][0m |          -0.0041 |         109.3032 |          18.5199 |
[32m[20221213 21:13:28 @agent_ppo2.py:185][0m |          -0.0083 |         108.3793 |          18.5140 |
[32m[20221213 21:13:28 @agent_ppo2.py:185][0m |          -0.0056 |         108.8283 |          18.5150 |
[32m[20221213 21:13:28 @agent_ppo2.py:185][0m |          -0.0088 |         108.0514 |          18.5068 |
[32m[20221213 21:13:28 @agent_ppo2.py:185][0m |          -0.0086 |         108.0039 |          18.5088 |
[32m[20221213 21:13:28 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:13:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.80
[32m[20221213 21:13:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 646.00
[32m[20221213 21:13:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 678.00
[32m[20221213 21:13:28 @agent_ppo2.py:143][0m Total time:      17.90 min
[32m[20221213 21:13:28 @agent_ppo2.py:145][0m 1748992 total steps have happened
[32m[20221213 21:13:28 @agent_ppo2.py:121][0m #------------------------ Iteration 854 --------------------------#
[32m[20221213 21:13:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:29 @agent_ppo2.py:185][0m |          -0.0020 |         112.9949 |          18.5997 |
[32m[20221213 21:13:29 @agent_ppo2.py:185][0m |          -0.0041 |         112.5131 |          18.6003 |
[32m[20221213 21:13:29 @agent_ppo2.py:185][0m |           0.0107 |         127.4387 |          18.5871 |
[32m[20221213 21:13:29 @agent_ppo2.py:185][0m |          -0.0022 |         112.7153 |          18.5731 |
[32m[20221213 21:13:29 @agent_ppo2.py:185][0m |           0.0013 |         113.4629 |          18.5648 |
[32m[20221213 21:13:29 @agent_ppo2.py:185][0m |          -0.0083 |         111.5540 |          18.5707 |
[32m[20221213 21:13:29 @agent_ppo2.py:185][0m |          -0.0074 |         111.5189 |          18.5688 |
[32m[20221213 21:13:29 @agent_ppo2.py:185][0m |          -0.0069 |         111.3371 |          18.5654 |
[32m[20221213 21:13:29 @agent_ppo2.py:185][0m |          -0.0099 |         111.2947 |          18.5693 |
[32m[20221213 21:13:30 @agent_ppo2.py:185][0m |          -0.0106 |         111.2065 |          18.5633 |
[32m[20221213 21:13:30 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:13:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 639.60
[32m[20221213 21:13:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 680.00
[32m[20221213 21:13:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.00
[32m[20221213 21:13:30 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 760.00
[32m[20221213 21:13:30 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 760.00
[32m[20221213 21:13:30 @agent_ppo2.py:143][0m Total time:      17.92 min
[32m[20221213 21:13:30 @agent_ppo2.py:145][0m 1751040 total steps have happened
[32m[20221213 21:13:30 @agent_ppo2.py:121][0m #------------------------ Iteration 855 --------------------------#
[32m[20221213 21:13:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:13:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:30 @agent_ppo2.py:185][0m |           0.0007 |         110.0899 |          18.5462 |
[32m[20221213 21:13:30 @agent_ppo2.py:185][0m |          -0.0064 |         108.0495 |          18.5499 |
[32m[20221213 21:13:30 @agent_ppo2.py:185][0m |          -0.0092 |         107.3124 |          18.5482 |
[32m[20221213 21:13:30 @agent_ppo2.py:185][0m |          -0.0067 |         106.6949 |          18.5534 |
[32m[20221213 21:13:30 @agent_ppo2.py:185][0m |          -0.0086 |         106.1005 |          18.5558 |
[32m[20221213 21:13:30 @agent_ppo2.py:185][0m |          -0.0088 |         105.6657 |          18.5538 |
[32m[20221213 21:13:31 @agent_ppo2.py:185][0m |          -0.0118 |         105.3161 |          18.5625 |
[32m[20221213 21:13:31 @agent_ppo2.py:185][0m |          -0.0103 |         105.1116 |          18.5636 |
[32m[20221213 21:13:31 @agent_ppo2.py:185][0m |          -0.0103 |         105.1308 |          18.5704 |
[32m[20221213 21:13:31 @agent_ppo2.py:185][0m |          -0.0101 |         104.5196 |          18.5743 |
[32m[20221213 21:13:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:13:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 538.40
[32m[20221213 21:13:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 609.00
[32m[20221213 21:13:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 674.00
[32m[20221213 21:13:31 @agent_ppo2.py:143][0m Total time:      17.94 min
[32m[20221213 21:13:31 @agent_ppo2.py:145][0m 1753088 total steps have happened
[32m[20221213 21:13:31 @agent_ppo2.py:121][0m #------------------------ Iteration 856 --------------------------#
[32m[20221213 21:13:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:31 @agent_ppo2.py:185][0m |          -0.0026 |         110.7031 |          18.5779 |
[32m[20221213 21:13:31 @agent_ppo2.py:185][0m |          -0.0048 |         109.4269 |          18.5789 |
[32m[20221213 21:13:31 @agent_ppo2.py:185][0m |          -0.0029 |         109.8225 |          18.5742 |
[32m[20221213 21:13:31 @agent_ppo2.py:185][0m |          -0.0049 |         109.7518 |          18.5659 |
[32m[20221213 21:13:32 @agent_ppo2.py:185][0m |          -0.0080 |         108.4953 |          18.5579 |
[32m[20221213 21:13:32 @agent_ppo2.py:185][0m |          -0.0077 |         108.1576 |          18.5534 |
[32m[20221213 21:13:32 @agent_ppo2.py:185][0m |          -0.0103 |         107.9217 |          18.5520 |
[32m[20221213 21:13:32 @agent_ppo2.py:185][0m |          -0.0080 |         107.7496 |          18.5485 |
[32m[20221213 21:13:32 @agent_ppo2.py:185][0m |          -0.0095 |         107.6394 |          18.5342 |
[32m[20221213 21:13:32 @agent_ppo2.py:185][0m |          -0.0111 |         107.4681 |          18.5312 |
[32m[20221213 21:13:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:13:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 628.20
[32m[20221213 21:13:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 666.00
[32m[20221213 21:13:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.00
[32m[20221213 21:13:32 @agent_ppo2.py:143][0m Total time:      17.96 min
[32m[20221213 21:13:32 @agent_ppo2.py:145][0m 1755136 total steps have happened
[32m[20221213 21:13:32 @agent_ppo2.py:121][0m #------------------------ Iteration 857 --------------------------#
[32m[20221213 21:13:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:32 @agent_ppo2.py:185][0m |          -0.0026 |         112.5911 |          18.4639 |
[32m[20221213 21:13:32 @agent_ppo2.py:185][0m |           0.0085 |         123.4684 |          18.4523 |
[32m[20221213 21:13:33 @agent_ppo2.py:185][0m |          -0.0028 |         111.1194 |          18.4696 |
[32m[20221213 21:13:33 @agent_ppo2.py:185][0m |          -0.0070 |         110.3523 |          18.4725 |
[32m[20221213 21:13:33 @agent_ppo2.py:185][0m |          -0.0059 |         109.9365 |          18.4824 |
[32m[20221213 21:13:33 @agent_ppo2.py:185][0m |           0.0053 |         118.0201 |          18.4928 |
[32m[20221213 21:13:33 @agent_ppo2.py:185][0m |          -0.0084 |         109.6109 |          18.4933 |
[32m[20221213 21:13:33 @agent_ppo2.py:185][0m |          -0.0077 |         109.3596 |          18.5083 |
[32m[20221213 21:13:33 @agent_ppo2.py:185][0m |          -0.0092 |         109.2922 |          18.5039 |
[32m[20221213 21:13:33 @agent_ppo2.py:185][0m |          -0.0091 |         108.8871 |          18.5068 |
[32m[20221213 21:13:33 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:13:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 566.20
[32m[20221213 21:13:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 593.00
[32m[20221213 21:13:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 697.00
[32m[20221213 21:13:33 @agent_ppo2.py:143][0m Total time:      17.98 min
[32m[20221213 21:13:33 @agent_ppo2.py:145][0m 1757184 total steps have happened
[32m[20221213 21:13:33 @agent_ppo2.py:121][0m #------------------------ Iteration 858 --------------------------#
[32m[20221213 21:13:34 @agent_ppo2.py:127][0m Sampling time: 0.29 s by 5 slaves
[32m[20221213 21:13:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:34 @agent_ppo2.py:185][0m |           0.0125 |         123.4191 |          18.6046 |
[32m[20221213 21:13:34 @agent_ppo2.py:185][0m |          -0.0011 |         112.9527 |          18.5947 |
[32m[20221213 21:13:34 @agent_ppo2.py:185][0m |          -0.0062 |         112.2182 |          18.5875 |
[32m[20221213 21:13:34 @agent_ppo2.py:185][0m |          -0.0087 |         111.8445 |          18.5901 |
[32m[20221213 21:13:34 @agent_ppo2.py:185][0m |          -0.0054 |         111.4527 |          18.5984 |
[32m[20221213 21:13:34 @agent_ppo2.py:185][0m |          -0.0105 |         111.3662 |          18.5731 |
[32m[20221213 21:13:35 @agent_ppo2.py:185][0m |          -0.0056 |         111.0885 |          18.5892 |
[32m[20221213 21:13:35 @agent_ppo2.py:185][0m |          -0.0085 |         111.0604 |          18.5796 |
[32m[20221213 21:13:35 @agent_ppo2.py:185][0m |          -0.0107 |         111.0576 |          18.5766 |
[32m[20221213 21:13:35 @agent_ppo2.py:185][0m |          -0.0107 |         110.7026 |          18.5759 |
[32m[20221213 21:13:35 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:13:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.00
[32m[20221213 21:13:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 695.00
[32m[20221213 21:13:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 653.00
[32m[20221213 21:13:35 @agent_ppo2.py:143][0m Total time:      18.00 min
[32m[20221213 21:13:35 @agent_ppo2.py:145][0m 1759232 total steps have happened
[32m[20221213 21:13:35 @agent_ppo2.py:121][0m #------------------------ Iteration 859 --------------------------#
[32m[20221213 21:13:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:35 @agent_ppo2.py:185][0m |          -0.0038 |         111.3214 |          18.6628 |
[32m[20221213 21:13:35 @agent_ppo2.py:185][0m |          -0.0024 |         110.4213 |          18.6531 |
[32m[20221213 21:13:35 @agent_ppo2.py:185][0m |          -0.0076 |         109.2218 |          18.6489 |
[32m[20221213 21:13:36 @agent_ppo2.py:185][0m |          -0.0079 |         108.6503 |          18.6329 |
[32m[20221213 21:13:36 @agent_ppo2.py:185][0m |          -0.0055 |         108.8155 |          18.6342 |
[32m[20221213 21:13:36 @agent_ppo2.py:185][0m |          -0.0066 |         107.9572 |          18.6373 |
[32m[20221213 21:13:36 @agent_ppo2.py:185][0m |           0.0003 |         112.0358 |          18.6319 |
[32m[20221213 21:13:36 @agent_ppo2.py:185][0m |          -0.0079 |         107.4299 |          18.6304 |
[32m[20221213 21:13:36 @agent_ppo2.py:185][0m |          -0.0074 |         107.3302 |          18.6330 |
[32m[20221213 21:13:36 @agent_ppo2.py:185][0m |          -0.0088 |         107.3642 |          18.6176 |
[32m[20221213 21:13:36 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:13:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 635.80
[32m[20221213 21:13:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 692.00
[32m[20221213 21:13:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 628.00
[32m[20221213 21:13:36 @agent_ppo2.py:143][0m Total time:      18.03 min
[32m[20221213 21:13:36 @agent_ppo2.py:145][0m 1761280 total steps have happened
[32m[20221213 21:13:36 @agent_ppo2.py:121][0m #------------------------ Iteration 860 --------------------------#
[32m[20221213 21:13:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:36 @agent_ppo2.py:185][0m |          -0.0025 |         113.9588 |          18.6077 |
[32m[20221213 21:13:37 @agent_ppo2.py:185][0m |          -0.0054 |         112.2677 |          18.5957 |
[32m[20221213 21:13:37 @agent_ppo2.py:185][0m |          -0.0045 |         111.5720 |          18.5975 |
[32m[20221213 21:13:37 @agent_ppo2.py:185][0m |          -0.0101 |         111.0378 |          18.5949 |
[32m[20221213 21:13:37 @agent_ppo2.py:185][0m |          -0.0083 |         110.4878 |          18.5922 |
[32m[20221213 21:13:37 @agent_ppo2.py:185][0m |          -0.0075 |         110.0382 |          18.5958 |
[32m[20221213 21:13:37 @agent_ppo2.py:185][0m |          -0.0082 |         109.8613 |          18.5931 |
[32m[20221213 21:13:37 @agent_ppo2.py:185][0m |          -0.0085 |         109.7623 |          18.5835 |
[32m[20221213 21:13:37 @agent_ppo2.py:185][0m |          -0.0087 |         109.2609 |          18.5826 |
[32m[20221213 21:13:37 @agent_ppo2.py:185][0m |          -0.0108 |         109.1851 |          18.5876 |
[32m[20221213 21:13:37 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:13:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 634.80
[32m[20221213 21:13:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 693.00
[32m[20221213 21:13:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 604.00
[32m[20221213 21:13:37 @agent_ppo2.py:143][0m Total time:      18.05 min
[32m[20221213 21:13:37 @agent_ppo2.py:145][0m 1763328 total steps have happened
[32m[20221213 21:13:37 @agent_ppo2.py:121][0m #------------------------ Iteration 861 --------------------------#
[32m[20221213 21:13:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:38 @agent_ppo2.py:185][0m |          -0.0020 |         112.2628 |          18.4767 |
[32m[20221213 21:13:38 @agent_ppo2.py:185][0m |           0.0041 |         112.4876 |          18.4578 |
[32m[20221213 21:13:38 @agent_ppo2.py:185][0m |          -0.0047 |         110.1915 |          18.4486 |
[32m[20221213 21:13:38 @agent_ppo2.py:185][0m |          -0.0024 |         109.6820 |          18.4497 |
[32m[20221213 21:13:38 @agent_ppo2.py:185][0m |          -0.0068 |         109.4689 |          18.4581 |
[32m[20221213 21:13:38 @agent_ppo2.py:185][0m |           0.0058 |         115.4343 |          18.4405 |
[32m[20221213 21:13:38 @agent_ppo2.py:185][0m |          -0.0049 |         109.1918 |          18.4481 |
[32m[20221213 21:13:38 @agent_ppo2.py:185][0m |          -0.0053 |         108.8209 |          18.4381 |
[32m[20221213 21:13:39 @agent_ppo2.py:185][0m |          -0.0063 |         108.6060 |          18.4316 |
[32m[20221213 21:13:39 @agent_ppo2.py:185][0m |           0.0012 |         112.0291 |          18.4419 |
[32m[20221213 21:13:39 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:13:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 614.00
[32m[20221213 21:13:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 656.00
[32m[20221213 21:13:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 605.00
[32m[20221213 21:13:39 @agent_ppo2.py:143][0m Total time:      18.07 min
[32m[20221213 21:13:39 @agent_ppo2.py:145][0m 1765376 total steps have happened
[32m[20221213 21:13:39 @agent_ppo2.py:121][0m #------------------------ Iteration 862 --------------------------#
[32m[20221213 21:13:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:39 @agent_ppo2.py:185][0m |           0.0104 |         125.2547 |          18.4991 |
[32m[20221213 21:13:39 @agent_ppo2.py:185][0m |           0.0079 |         125.5117 |          18.4690 |
[32m[20221213 21:13:39 @agent_ppo2.py:185][0m |          -0.0025 |         113.4957 |          18.4661 |
[32m[20221213 21:13:39 @agent_ppo2.py:185][0m |          -0.0053 |         112.5393 |          18.4773 |
[32m[20221213 21:13:40 @agent_ppo2.py:185][0m |          -0.0061 |         112.1615 |          18.4839 |
[32m[20221213 21:13:40 @agent_ppo2.py:185][0m |          -0.0041 |         111.6065 |          18.4855 |
[32m[20221213 21:13:40 @agent_ppo2.py:185][0m |          -0.0052 |         111.5234 |          18.4964 |
[32m[20221213 21:13:40 @agent_ppo2.py:185][0m |          -0.0034 |         112.0458 |          18.4856 |
[32m[20221213 21:13:40 @agent_ppo2.py:185][0m |          -0.0100 |         110.9531 |          18.4827 |
[32m[20221213 21:13:40 @agent_ppo2.py:185][0m |          -0.0097 |         110.7585 |          18.4931 |
[32m[20221213 21:13:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:13:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 572.80
[32m[20221213 21:13:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 647.00
[32m[20221213 21:13:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 643.00
[32m[20221213 21:13:40 @agent_ppo2.py:143][0m Total time:      18.09 min
[32m[20221213 21:13:40 @agent_ppo2.py:145][0m 1767424 total steps have happened
[32m[20221213 21:13:40 @agent_ppo2.py:121][0m #------------------------ Iteration 863 --------------------------#
[32m[20221213 21:13:40 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:13:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:40 @agent_ppo2.py:185][0m |          -0.0008 |         114.1879 |          18.5432 |
[32m[20221213 21:13:41 @agent_ppo2.py:185][0m |          -0.0040 |         112.4620 |          18.5220 |
[32m[20221213 21:13:41 @agent_ppo2.py:185][0m |           0.0091 |         122.3015 |          18.4926 |
[32m[20221213 21:13:41 @agent_ppo2.py:185][0m |          -0.0070 |         111.7489 |          18.4917 |
[32m[20221213 21:13:41 @agent_ppo2.py:185][0m |          -0.0048 |         111.5046 |          18.4869 |
[32m[20221213 21:13:41 @agent_ppo2.py:185][0m |          -0.0060 |         111.0432 |          18.4876 |
[32m[20221213 21:13:41 @agent_ppo2.py:185][0m |          -0.0103 |         110.8725 |          18.4821 |
[32m[20221213 21:13:41 @agent_ppo2.py:185][0m |          -0.0097 |         110.7313 |          18.4758 |
[32m[20221213 21:13:41 @agent_ppo2.py:185][0m |          -0.0049 |         110.6678 |          18.4666 |
[32m[20221213 21:13:41 @agent_ppo2.py:185][0m |          -0.0105 |         110.6278 |          18.4704 |
[32m[20221213 21:13:41 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:13:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 609.80
[32m[20221213 21:13:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 672.00
[32m[20221213 21:13:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 601.00
[32m[20221213 21:13:41 @agent_ppo2.py:143][0m Total time:      18.11 min
[32m[20221213 21:13:41 @agent_ppo2.py:145][0m 1769472 total steps have happened
[32m[20221213 21:13:41 @agent_ppo2.py:121][0m #------------------------ Iteration 864 --------------------------#
[32m[20221213 21:13:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:13:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:42 @agent_ppo2.py:185][0m |          -0.0002 |         110.8258 |          18.5770 |
[32m[20221213 21:13:42 @agent_ppo2.py:185][0m |          -0.0034 |         109.8677 |          18.5837 |
[32m[20221213 21:13:42 @agent_ppo2.py:185][0m |          -0.0059 |         109.2998 |          18.5929 |
[32m[20221213 21:13:42 @agent_ppo2.py:185][0m |          -0.0071 |         108.8608 |          18.5985 |
[32m[20221213 21:13:42 @agent_ppo2.py:185][0m |          -0.0061 |         108.8484 |          18.5993 |
[32m[20221213 21:13:42 @agent_ppo2.py:185][0m |          -0.0033 |         108.8097 |          18.6027 |
[32m[20221213 21:13:42 @agent_ppo2.py:185][0m |          -0.0043 |         107.9881 |          18.6097 |
[32m[20221213 21:13:42 @agent_ppo2.py:185][0m |          -0.0086 |         107.7030 |          18.6167 |
[32m[20221213 21:13:43 @agent_ppo2.py:185][0m |          -0.0058 |         107.4308 |          18.6135 |
[32m[20221213 21:13:43 @agent_ppo2.py:185][0m |          -0.0084 |         107.3253 |          18.6303 |
[32m[20221213 21:13:43 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 21:13:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 596.60
[32m[20221213 21:13:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 628.00
[32m[20221213 21:13:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 609.00
[32m[20221213 21:13:43 @agent_ppo2.py:143][0m Total time:      18.14 min
[32m[20221213 21:13:43 @agent_ppo2.py:145][0m 1771520 total steps have happened
[32m[20221213 21:13:43 @agent_ppo2.py:121][0m #------------------------ Iteration 865 --------------------------#
[32m[20221213 21:13:43 @agent_ppo2.py:127][0m Sampling time: 0.26 s by 5 slaves
[32m[20221213 21:13:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:43 @agent_ppo2.py:185][0m |          -0.0054 |         111.0878 |          18.4134 |
[32m[20221213 21:13:44 @agent_ppo2.py:185][0m |          -0.0050 |         109.8420 |          18.4006 |
[32m[20221213 21:13:44 @agent_ppo2.py:185][0m |          -0.0034 |         113.0013 |          18.4032 |
[32m[20221213 21:13:44 @agent_ppo2.py:185][0m |          -0.0073 |         109.2772 |          18.3858 |
[32m[20221213 21:13:44 @agent_ppo2.py:185][0m |          -0.0059 |         108.7901 |          18.3980 |
[32m[20221213 21:13:44 @agent_ppo2.py:185][0m |          -0.0081 |         108.7474 |          18.3990 |
[32m[20221213 21:13:44 @agent_ppo2.py:185][0m |          -0.0093 |         108.4108 |          18.3977 |
[32m[20221213 21:13:44 @agent_ppo2.py:185][0m |           0.0060 |         117.0546 |          18.4026 |
[32m[20221213 21:13:44 @agent_ppo2.py:185][0m |          -0.0098 |         108.1539 |          18.4072 |
[32m[20221213 21:13:45 @agent_ppo2.py:185][0m |          -0.0097 |         108.0839 |          18.4046 |
[32m[20221213 21:13:45 @agent_ppo2.py:130][0m Policy update time: 1.31 s
[32m[20221213 21:13:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.20
[32m[20221213 21:13:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 634.00
[32m[20221213 21:13:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 696.00
[32m[20221213 21:13:45 @agent_ppo2.py:143][0m Total time:      18.17 min
[32m[20221213 21:13:45 @agent_ppo2.py:145][0m 1773568 total steps have happened
[32m[20221213 21:13:45 @agent_ppo2.py:121][0m #------------------------ Iteration 866 --------------------------#
[32m[20221213 21:13:45 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:13:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:45 @agent_ppo2.py:185][0m |           0.0006 |         109.4054 |          18.4526 |
[32m[20221213 21:13:45 @agent_ppo2.py:185][0m |          -0.0056 |         108.2604 |          18.4405 |
[32m[20221213 21:13:45 @agent_ppo2.py:185][0m |          -0.0056 |         107.6842 |          18.4281 |
[32m[20221213 21:13:45 @agent_ppo2.py:185][0m |          -0.0072 |         107.2896 |          18.4224 |
[32m[20221213 21:13:45 @agent_ppo2.py:185][0m |          -0.0060 |         107.1683 |          18.4228 |
[32m[20221213 21:13:46 @agent_ppo2.py:185][0m |          -0.0079 |         106.9024 |          18.4142 |
[32m[20221213 21:13:46 @agent_ppo2.py:185][0m |          -0.0093 |         106.8230 |          18.4264 |
[32m[20221213 21:13:46 @agent_ppo2.py:185][0m |          -0.0085 |         106.7487 |          18.4207 |
[32m[20221213 21:13:46 @agent_ppo2.py:185][0m |          -0.0088 |         106.3661 |          18.4195 |
[32m[20221213 21:13:46 @agent_ppo2.py:185][0m |          -0.0054 |         108.6351 |          18.4146 |
[32m[20221213 21:13:46 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:13:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 572.60
[32m[20221213 21:13:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.00
[32m[20221213 21:13:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.00
[32m[20221213 21:13:46 @agent_ppo2.py:143][0m Total time:      18.19 min
[32m[20221213 21:13:46 @agent_ppo2.py:145][0m 1775616 total steps have happened
[32m[20221213 21:13:46 @agent_ppo2.py:121][0m #------------------------ Iteration 867 --------------------------#
[32m[20221213 21:13:46 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:13:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:46 @agent_ppo2.py:185][0m |          -0.0004 |         106.7451 |          18.5424 |
[32m[20221213 21:13:47 @agent_ppo2.py:185][0m |          -0.0034 |         105.5886 |          18.5182 |
[32m[20221213 21:13:47 @agent_ppo2.py:185][0m |          -0.0077 |         105.0158 |          18.5031 |
[32m[20221213 21:13:47 @agent_ppo2.py:185][0m |          -0.0089 |         104.6250 |          18.4851 |
[32m[20221213 21:13:47 @agent_ppo2.py:185][0m |          -0.0088 |         104.2614 |          18.4908 |
[32m[20221213 21:13:47 @agent_ppo2.py:185][0m |          -0.0088 |         104.0933 |          18.5000 |
[32m[20221213 21:13:47 @agent_ppo2.py:185][0m |          -0.0120 |         103.9851 |          18.4826 |
[32m[20221213 21:13:47 @agent_ppo2.py:185][0m |          -0.0092 |         103.8166 |          18.4692 |
[32m[20221213 21:13:47 @agent_ppo2.py:185][0m |          -0.0029 |         106.5943 |          18.4873 |
[32m[20221213 21:13:47 @agent_ppo2.py:185][0m |          -0.0107 |         103.5556 |          18.4821 |
[32m[20221213 21:13:47 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 21:13:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 592.20
[32m[20221213 21:13:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 635.00
[32m[20221213 21:13:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.00
[32m[20221213 21:13:48 @agent_ppo2.py:143][0m Total time:      18.21 min
[32m[20221213 21:13:48 @agent_ppo2.py:145][0m 1777664 total steps have happened
[32m[20221213 21:13:48 @agent_ppo2.py:121][0m #------------------------ Iteration 868 --------------------------#
[32m[20221213 21:13:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:13:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:48 @agent_ppo2.py:185][0m |           0.0013 |         108.3147 |          18.4008 |
[32m[20221213 21:13:48 @agent_ppo2.py:185][0m |          -0.0061 |         107.6665 |          18.4003 |
[32m[20221213 21:13:48 @agent_ppo2.py:185][0m |          -0.0053 |         107.3089 |          18.3768 |
[32m[20221213 21:13:48 @agent_ppo2.py:185][0m |           0.0015 |         115.1111 |          18.3865 |
[32m[20221213 21:13:48 @agent_ppo2.py:185][0m |          -0.0049 |         107.0215 |          18.3789 |
[32m[20221213 21:13:48 @agent_ppo2.py:185][0m |          -0.0081 |         106.7372 |          18.3785 |
[32m[20221213 21:13:49 @agent_ppo2.py:185][0m |           0.0001 |         114.1757 |          18.3829 |
[32m[20221213 21:13:49 @agent_ppo2.py:185][0m |          -0.0057 |         106.7076 |          18.3745 |
[32m[20221213 21:13:49 @agent_ppo2.py:185][0m |          -0.0037 |         106.8310 |          18.3858 |
[32m[20221213 21:13:49 @agent_ppo2.py:185][0m |          -0.0118 |         106.3013 |          18.3833 |
[32m[20221213 21:13:49 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 21:13:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 587.40
[32m[20221213 21:13:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 662.00
[32m[20221213 21:13:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 639.00
[32m[20221213 21:13:49 @agent_ppo2.py:143][0m Total time:      18.24 min
[32m[20221213 21:13:49 @agent_ppo2.py:145][0m 1779712 total steps have happened
[32m[20221213 21:13:49 @agent_ppo2.py:121][0m #------------------------ Iteration 869 --------------------------#
[32m[20221213 21:13:49 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:13:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:49 @agent_ppo2.py:185][0m |          -0.0008 |         106.1186 |          18.4685 |
[32m[20221213 21:13:49 @agent_ppo2.py:185][0m |          -0.0044 |         104.8584 |          18.4838 |
[32m[20221213 21:13:50 @agent_ppo2.py:185][0m |          -0.0046 |         104.1010 |          18.4937 |
[32m[20221213 21:13:50 @agent_ppo2.py:185][0m |          -0.0085 |         103.5654 |          18.5002 |
[32m[20221213 21:13:50 @agent_ppo2.py:185][0m |          -0.0099 |         103.1154 |          18.5140 |
[32m[20221213 21:13:50 @agent_ppo2.py:185][0m |          -0.0078 |         103.0140 |          18.5173 |
[32m[20221213 21:13:50 @agent_ppo2.py:185][0m |           0.0007 |         106.4513 |          18.5223 |
[32m[20221213 21:13:50 @agent_ppo2.py:185][0m |          -0.0063 |         102.3641 |          18.5350 |
[32m[20221213 21:13:50 @agent_ppo2.py:185][0m |          -0.0076 |         101.8802 |          18.5290 |
[32m[20221213 21:13:50 @agent_ppo2.py:185][0m |          -0.0120 |         101.8520 |          18.5495 |
[32m[20221213 21:13:50 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:13:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 586.20
[32m[20221213 21:13:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 614.00
[32m[20221213 21:13:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.00
[32m[20221213 21:13:50 @agent_ppo2.py:143][0m Total time:      18.26 min
[32m[20221213 21:13:50 @agent_ppo2.py:145][0m 1781760 total steps have happened
[32m[20221213 21:13:50 @agent_ppo2.py:121][0m #------------------------ Iteration 870 --------------------------#
[32m[20221213 21:13:51 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:13:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:51 @agent_ppo2.py:185][0m |          -0.0028 |         108.5110 |          18.5282 |
[32m[20221213 21:13:51 @agent_ppo2.py:185][0m |          -0.0037 |         107.1572 |          18.5041 |
[32m[20221213 21:13:51 @agent_ppo2.py:185][0m |          -0.0062 |         106.2717 |          18.5157 |
[32m[20221213 21:13:51 @agent_ppo2.py:185][0m |          -0.0096 |         105.5708 |          18.5180 |
[32m[20221213 21:13:51 @agent_ppo2.py:185][0m |          -0.0057 |         105.6932 |          18.5193 |
[32m[20221213 21:13:51 @agent_ppo2.py:185][0m |          -0.0041 |         108.0443 |          18.5183 |
[32m[20221213 21:13:51 @agent_ppo2.py:185][0m |          -0.0117 |         104.9101 |          18.5186 |
[32m[20221213 21:13:52 @agent_ppo2.py:185][0m |          -0.0121 |         104.5478 |          18.5207 |
[32m[20221213 21:13:52 @agent_ppo2.py:185][0m |          -0.0132 |         104.5529 |          18.5299 |
[32m[20221213 21:13:52 @agent_ppo2.py:185][0m |          -0.0135 |         104.1579 |          18.5302 |
[32m[20221213 21:13:52 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:13:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 602.40
[32m[20221213 21:13:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 671.00
[32m[20221213 21:13:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 563.00
[32m[20221213 21:13:52 @agent_ppo2.py:143][0m Total time:      18.29 min
[32m[20221213 21:13:52 @agent_ppo2.py:145][0m 1783808 total steps have happened
[32m[20221213 21:13:52 @agent_ppo2.py:121][0m #------------------------ Iteration 871 --------------------------#
[32m[20221213 21:13:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:13:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:52 @agent_ppo2.py:185][0m |          -0.0039 |         104.3088 |          18.5528 |
[32m[20221213 21:13:52 @agent_ppo2.py:185][0m |          -0.0058 |         102.4057 |          18.5460 |
[32m[20221213 21:13:52 @agent_ppo2.py:185][0m |          -0.0062 |         101.7991 |          18.5316 |
[32m[20221213 21:13:53 @agent_ppo2.py:185][0m |          -0.0122 |         101.2692 |          18.5277 |
[32m[20221213 21:13:53 @agent_ppo2.py:185][0m |          -0.0110 |         100.7042 |          18.5348 |
[32m[20221213 21:13:53 @agent_ppo2.py:185][0m |          -0.0091 |         100.3326 |          18.5304 |
[32m[20221213 21:13:53 @agent_ppo2.py:185][0m |          -0.0079 |          99.9890 |          18.5294 |
[32m[20221213 21:13:53 @agent_ppo2.py:185][0m |          -0.0093 |          99.7890 |          18.5287 |
[32m[20221213 21:13:53 @agent_ppo2.py:185][0m |          -0.0110 |          99.4470 |          18.5272 |
[32m[20221213 21:13:53 @agent_ppo2.py:185][0m |          -0.0049 |         101.8074 |          18.5330 |
[32m[20221213 21:13:53 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:13:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 587.00
[32m[20221213 21:13:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 611.00
[32m[20221213 21:13:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 682.00
[32m[20221213 21:13:53 @agent_ppo2.py:143][0m Total time:      18.31 min
[32m[20221213 21:13:53 @agent_ppo2.py:145][0m 1785856 total steps have happened
[32m[20221213 21:13:53 @agent_ppo2.py:121][0m #------------------------ Iteration 872 --------------------------#
[32m[20221213 21:13:53 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:13:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:54 @agent_ppo2.py:185][0m |           0.0095 |         115.8240 |          18.4765 |
[32m[20221213 21:13:54 @agent_ppo2.py:185][0m |          -0.0022 |         107.7550 |          18.4779 |
[32m[20221213 21:13:54 @agent_ppo2.py:185][0m |          -0.0040 |         106.8813 |          18.4731 |
[32m[20221213 21:13:54 @agent_ppo2.py:185][0m |          -0.0030 |         107.3147 |          18.4741 |
[32m[20221213 21:13:54 @agent_ppo2.py:185][0m |          -0.0067 |         106.1355 |          18.4744 |
[32m[20221213 21:13:54 @agent_ppo2.py:185][0m |          -0.0078 |         105.7895 |          18.4667 |
[32m[20221213 21:13:54 @agent_ppo2.py:185][0m |          -0.0095 |         105.3973 |          18.4624 |
[32m[20221213 21:13:54 @agent_ppo2.py:185][0m |          -0.0074 |         105.2892 |          18.4592 |
[32m[20221213 21:13:55 @agent_ppo2.py:185][0m |          -0.0064 |         105.3176 |          18.4590 |
[32m[20221213 21:13:55 @agent_ppo2.py:185][0m |           0.0004 |         110.6963 |          18.4608 |
[32m[20221213 21:13:55 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:13:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 555.40
[32m[20221213 21:13:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 654.00
[32m[20221213 21:13:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 605.00
[32m[20221213 21:13:55 @agent_ppo2.py:143][0m Total time:      18.33 min
[32m[20221213 21:13:55 @agent_ppo2.py:145][0m 1787904 total steps have happened
[32m[20221213 21:13:55 @agent_ppo2.py:121][0m #------------------------ Iteration 873 --------------------------#
[32m[20221213 21:13:55 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:13:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:55 @agent_ppo2.py:185][0m |          -0.0020 |         104.5363 |          18.5186 |
[32m[20221213 21:13:55 @agent_ppo2.py:185][0m |           0.0061 |         106.1585 |          18.5046 |
[32m[20221213 21:13:55 @agent_ppo2.py:185][0m |          -0.0069 |         102.2577 |          18.5028 |
[32m[20221213 21:13:55 @agent_ppo2.py:185][0m |          -0.0092 |         101.6556 |          18.4878 |
[32m[20221213 21:13:55 @agent_ppo2.py:185][0m |          -0.0063 |         101.1651 |          18.4953 |
[32m[20221213 21:13:56 @agent_ppo2.py:185][0m |          -0.0073 |         100.7412 |          18.4967 |
[32m[20221213 21:13:56 @agent_ppo2.py:185][0m |          -0.0073 |         100.2759 |          18.4871 |
[32m[20221213 21:13:56 @agent_ppo2.py:185][0m |          -0.0094 |         100.0330 |          18.4885 |
[32m[20221213 21:13:56 @agent_ppo2.py:185][0m |          -0.0094 |          99.8346 |          18.4901 |
[32m[20221213 21:13:56 @agent_ppo2.py:185][0m |          -0.0096 |          99.6688 |          18.4873 |
[32m[20221213 21:13:56 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:13:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.60
[32m[20221213 21:13:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 648.00
[32m[20221213 21:13:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 670.00
[32m[20221213 21:13:56 @agent_ppo2.py:143][0m Total time:      18.36 min
[32m[20221213 21:13:56 @agent_ppo2.py:145][0m 1789952 total steps have happened
[32m[20221213 21:13:56 @agent_ppo2.py:121][0m #------------------------ Iteration 874 --------------------------#
[32m[20221213 21:13:56 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:13:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:56 @agent_ppo2.py:185][0m |          -0.0006 |         111.6684 |          18.5687 |
[32m[20221213 21:13:57 @agent_ppo2.py:185][0m |          -0.0049 |         109.8117 |          18.5450 |
[32m[20221213 21:13:57 @agent_ppo2.py:185][0m |          -0.0069 |         108.7222 |          18.5425 |
[32m[20221213 21:13:57 @agent_ppo2.py:185][0m |          -0.0073 |         107.9523 |          18.5276 |
[32m[20221213 21:13:57 @agent_ppo2.py:185][0m |          -0.0039 |         107.4943 |          18.5213 |
[32m[20221213 21:13:57 @agent_ppo2.py:185][0m |          -0.0044 |         109.9944 |          18.5105 |
[32m[20221213 21:13:57 @agent_ppo2.py:185][0m |          -0.0008 |         109.5496 |          18.5031 |
[32m[20221213 21:13:57 @agent_ppo2.py:185][0m |          -0.0092 |         106.3972 |          18.5067 |
[32m[20221213 21:13:57 @agent_ppo2.py:185][0m |          -0.0074 |         106.2850 |          18.5047 |
[32m[20221213 21:13:57 @agent_ppo2.py:185][0m |          -0.0072 |         106.0048 |          18.5014 |
[32m[20221213 21:13:57 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:13:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.60
[32m[20221213 21:13:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 660.00
[32m[20221213 21:13:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 646.00
[32m[20221213 21:13:58 @agent_ppo2.py:143][0m Total time:      18.38 min
[32m[20221213 21:13:58 @agent_ppo2.py:145][0m 1792000 total steps have happened
[32m[20221213 21:13:58 @agent_ppo2.py:121][0m #------------------------ Iteration 875 --------------------------#
[32m[20221213 21:13:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:58 @agent_ppo2.py:185][0m |           0.0009 |         109.2047 |          18.4727 |
[32m[20221213 21:13:58 @agent_ppo2.py:185][0m |           0.0016 |         108.8314 |          18.4570 |
[32m[20221213 21:13:58 @agent_ppo2.py:185][0m |          -0.0053 |         107.4911 |          18.4442 |
[32m[20221213 21:13:58 @agent_ppo2.py:185][0m |          -0.0069 |         106.4916 |          18.4105 |
[32m[20221213 21:13:58 @agent_ppo2.py:185][0m |           0.0051 |         116.2520 |          18.4165 |
[32m[20221213 21:13:58 @agent_ppo2.py:185][0m |          -0.0055 |         105.8724 |          18.4076 |
[32m[20221213 21:13:59 @agent_ppo2.py:185][0m |          -0.0096 |         105.7921 |          18.3923 |
[32m[20221213 21:13:59 @agent_ppo2.py:185][0m |          -0.0076 |         105.4785 |          18.3844 |
[32m[20221213 21:13:59 @agent_ppo2.py:185][0m |          -0.0097 |         105.1215 |          18.3855 |
[32m[20221213 21:13:59 @agent_ppo2.py:185][0m |          -0.0076 |         104.9164 |          18.3628 |
[32m[20221213 21:13:59 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:13:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 636.40
[32m[20221213 21:13:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 696.00
[32m[20221213 21:13:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 585.00
[32m[20221213 21:13:59 @agent_ppo2.py:143][0m Total time:      18.41 min
[32m[20221213 21:13:59 @agent_ppo2.py:145][0m 1794048 total steps have happened
[32m[20221213 21:13:59 @agent_ppo2.py:121][0m #------------------------ Iteration 876 --------------------------#
[32m[20221213 21:13:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:13:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:13:59 @agent_ppo2.py:185][0m |          -0.0017 |         106.7035 |          18.3239 |
[32m[20221213 21:13:59 @agent_ppo2.py:185][0m |          -0.0032 |         105.5692 |          18.3194 |
[32m[20221213 21:14:00 @agent_ppo2.py:185][0m |           0.0074 |         114.1095 |          18.3169 |
[32m[20221213 21:14:00 @agent_ppo2.py:185][0m |          -0.0054 |         104.5240 |          18.3266 |
[32m[20221213 21:14:00 @agent_ppo2.py:185][0m |          -0.0062 |         104.2905 |          18.3301 |
[32m[20221213 21:14:00 @agent_ppo2.py:185][0m |          -0.0034 |         104.8045 |          18.3325 |
[32m[20221213 21:14:00 @agent_ppo2.py:185][0m |          -0.0065 |         103.4041 |          18.3359 |
[32m[20221213 21:14:00 @agent_ppo2.py:185][0m |           0.0044 |         108.5518 |          18.3311 |
[32m[20221213 21:14:00 @agent_ppo2.py:185][0m |          -0.0047 |         103.0264 |          18.3358 |
[32m[20221213 21:14:00 @agent_ppo2.py:185][0m |          -0.0087 |         102.8606 |          18.3450 |
[32m[20221213 21:14:00 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:14:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 596.60
[32m[20221213 21:14:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 618.00
[32m[20221213 21:14:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 602.00
[32m[20221213 21:14:00 @agent_ppo2.py:143][0m Total time:      18.43 min
[32m[20221213 21:14:00 @agent_ppo2.py:145][0m 1796096 total steps have happened
[32m[20221213 21:14:00 @agent_ppo2.py:121][0m #------------------------ Iteration 877 --------------------------#
[32m[20221213 21:14:01 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:14:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:01 @agent_ppo2.py:185][0m |           0.0008 |         106.9923 |          18.3216 |
[32m[20221213 21:14:01 @agent_ppo2.py:185][0m |          -0.0025 |         106.1486 |          18.3092 |
[32m[20221213 21:14:01 @agent_ppo2.py:185][0m |          -0.0058 |         105.7060 |          18.3141 |
[32m[20221213 21:14:01 @agent_ppo2.py:185][0m |          -0.0045 |         105.2911 |          18.3018 |
[32m[20221213 21:14:01 @agent_ppo2.py:185][0m |          -0.0058 |         104.8875 |          18.3021 |
[32m[20221213 21:14:01 @agent_ppo2.py:185][0m |          -0.0060 |         104.9555 |          18.3065 |
[32m[20221213 21:14:01 @agent_ppo2.py:185][0m |          -0.0087 |         104.3883 |          18.3066 |
[32m[20221213 21:14:02 @agent_ppo2.py:185][0m |          -0.0060 |         104.6922 |          18.2904 |
[32m[20221213 21:14:02 @agent_ppo2.py:185][0m |          -0.0064 |         104.0812 |          18.3014 |
[32m[20221213 21:14:02 @agent_ppo2.py:185][0m |          -0.0068 |         104.2220 |          18.2954 |
[32m[20221213 21:14:02 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:14:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 611.60
[32m[20221213 21:14:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 651.00
[32m[20221213 21:14:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 697.00
[32m[20221213 21:14:02 @agent_ppo2.py:143][0m Total time:      18.45 min
[32m[20221213 21:14:02 @agent_ppo2.py:145][0m 1798144 total steps have happened
[32m[20221213 21:14:02 @agent_ppo2.py:121][0m #------------------------ Iteration 878 --------------------------#
[32m[20221213 21:14:02 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:14:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:02 @agent_ppo2.py:185][0m |          -0.0038 |         105.3757 |          18.5024 |
[32m[20221213 21:14:02 @agent_ppo2.py:185][0m |          -0.0045 |         104.4215 |          18.5003 |
[32m[20221213 21:14:02 @agent_ppo2.py:185][0m |          -0.0055 |         103.9083 |          18.5084 |
[32m[20221213 21:14:03 @agent_ppo2.py:185][0m |          -0.0015 |         107.5604 |          18.5167 |
[32m[20221213 21:14:03 @agent_ppo2.py:185][0m |           0.0057 |         111.6838 |          18.5194 |
[32m[20221213 21:14:03 @agent_ppo2.py:185][0m |          -0.0085 |         103.5759 |          18.5300 |
[32m[20221213 21:14:03 @agent_ppo2.py:185][0m |          -0.0055 |         103.2406 |          18.5379 |
[32m[20221213 21:14:03 @agent_ppo2.py:185][0m |          -0.0074 |         103.0386 |          18.5436 |
[32m[20221213 21:14:03 @agent_ppo2.py:185][0m |          -0.0048 |         103.8978 |          18.5383 |
[32m[20221213 21:14:03 @agent_ppo2.py:185][0m |          -0.0048 |         103.1825 |          18.5412 |
[32m[20221213 21:14:03 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:14:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 570.20
[32m[20221213 21:14:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.00
[32m[20221213 21:14:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 708.00
[32m[20221213 21:14:03 @agent_ppo2.py:143][0m Total time:      18.48 min
[32m[20221213 21:14:03 @agent_ppo2.py:145][0m 1800192 total steps have happened
[32m[20221213 21:14:03 @agent_ppo2.py:121][0m #------------------------ Iteration 879 --------------------------#
[32m[20221213 21:14:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:04 @agent_ppo2.py:185][0m |          -0.0048 |         104.7936 |          18.5412 |
[32m[20221213 21:14:04 @agent_ppo2.py:185][0m |          -0.0057 |         101.8610 |          18.5409 |
[32m[20221213 21:14:04 @agent_ppo2.py:185][0m |          -0.0053 |         100.8439 |          18.5763 |
[32m[20221213 21:14:04 @agent_ppo2.py:185][0m |           0.0019 |         104.7198 |          18.5670 |
[32m[20221213 21:14:04 @agent_ppo2.py:185][0m |          -0.0060 |          99.7553 |          18.5913 |
[32m[20221213 21:14:04 @agent_ppo2.py:185][0m |           0.0057 |         106.2227 |          18.5966 |
[32m[20221213 21:14:04 @agent_ppo2.py:185][0m |          -0.0078 |          99.3249 |          18.6035 |
[32m[20221213 21:14:04 @agent_ppo2.py:185][0m |          -0.0040 |         102.1397 |          18.6159 |
[32m[20221213 21:14:05 @agent_ppo2.py:185][0m |          -0.0089 |          98.6005 |          18.6210 |
[32m[20221213 21:14:05 @agent_ppo2.py:185][0m |          -0.0130 |          98.1288 |          18.6343 |
[32m[20221213 21:14:05 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 21:14:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 552.20
[32m[20221213 21:14:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 616.00
[32m[20221213 21:14:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 701.00
[32m[20221213 21:14:05 @agent_ppo2.py:143][0m Total time:      18.50 min
[32m[20221213 21:14:05 @agent_ppo2.py:145][0m 1802240 total steps have happened
[32m[20221213 21:14:05 @agent_ppo2.py:121][0m #------------------------ Iteration 880 --------------------------#
[32m[20221213 21:14:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:14:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:05 @agent_ppo2.py:185][0m |          -0.0007 |         106.9227 |          18.5649 |
[32m[20221213 21:14:05 @agent_ppo2.py:185][0m |          -0.0029 |         106.1671 |          18.5475 |
[32m[20221213 21:14:05 @agent_ppo2.py:185][0m |          -0.0079 |         105.0873 |          18.5298 |
[32m[20221213 21:14:05 @agent_ppo2.py:185][0m |          -0.0071 |         104.6690 |          18.5148 |
[32m[20221213 21:14:06 @agent_ppo2.py:185][0m |          -0.0075 |         104.3758 |          18.5055 |
[32m[20221213 21:14:06 @agent_ppo2.py:185][0m |          -0.0000 |         111.8009 |          18.4965 |
[32m[20221213 21:14:06 @agent_ppo2.py:185][0m |          -0.0005 |         106.1870 |          18.4856 |
[32m[20221213 21:14:06 @agent_ppo2.py:185][0m |          -0.0076 |         103.8218 |          18.4887 |
[32m[20221213 21:14:06 @agent_ppo2.py:185][0m |          -0.0098 |         103.6320 |          18.4787 |
[32m[20221213 21:14:06 @agent_ppo2.py:185][0m |          -0.0108 |         103.4069 |          18.4649 |
[32m[20221213 21:14:06 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:14:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 604.60
[32m[20221213 21:14:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 630.00
[32m[20221213 21:14:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 723.00
[32m[20221213 21:14:06 @agent_ppo2.py:143][0m Total time:      18.52 min
[32m[20221213 21:14:06 @agent_ppo2.py:145][0m 1804288 total steps have happened
[32m[20221213 21:14:06 @agent_ppo2.py:121][0m #------------------------ Iteration 881 --------------------------#
[32m[20221213 21:14:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:14:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:07 @agent_ppo2.py:185][0m |          -0.0003 |         107.3828 |          18.5324 |
[32m[20221213 21:14:07 @agent_ppo2.py:185][0m |          -0.0030 |         106.3819 |          18.5121 |
[32m[20221213 21:14:07 @agent_ppo2.py:185][0m |          -0.0043 |         105.5656 |          18.5194 |
[32m[20221213 21:14:07 @agent_ppo2.py:185][0m |          -0.0043 |         105.1625 |          18.5175 |
[32m[20221213 21:14:07 @agent_ppo2.py:185][0m |          -0.0084 |         104.7706 |          18.5288 |
[32m[20221213 21:14:07 @agent_ppo2.py:185][0m |          -0.0066 |         104.3096 |          18.5103 |
[32m[20221213 21:14:07 @agent_ppo2.py:185][0m |          -0.0053 |         104.0710 |          18.5197 |
[32m[20221213 21:14:07 @agent_ppo2.py:185][0m |          -0.0056 |         104.0659 |          18.5230 |
[32m[20221213 21:14:07 @agent_ppo2.py:185][0m |           0.0056 |         113.5912 |          18.5364 |
[32m[20221213 21:14:07 @agent_ppo2.py:185][0m |          -0.0051 |         103.6335 |          18.4859 |
[32m[20221213 21:14:07 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:14:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 627.00
[32m[20221213 21:14:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 661.00
[32m[20221213 21:14:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 678.00
[32m[20221213 21:14:07 @agent_ppo2.py:143][0m Total time:      18.55 min
[32m[20221213 21:14:07 @agent_ppo2.py:145][0m 1806336 total steps have happened
[32m[20221213 21:14:07 @agent_ppo2.py:121][0m #------------------------ Iteration 882 --------------------------#
[32m[20221213 21:14:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:08 @agent_ppo2.py:185][0m |           0.0018 |         112.0222 |          18.5252 |
[32m[20221213 21:14:08 @agent_ppo2.py:185][0m |          -0.0048 |         110.5292 |          18.5310 |
[32m[20221213 21:14:08 @agent_ppo2.py:185][0m |          -0.0060 |         109.8046 |          18.5303 |
[32m[20221213 21:14:08 @agent_ppo2.py:185][0m |          -0.0023 |         110.6818 |          18.5412 |
[32m[20221213 21:14:08 @agent_ppo2.py:185][0m |          -0.0037 |         109.0598 |          18.5439 |
[32m[20221213 21:14:08 @agent_ppo2.py:185][0m |          -0.0083 |         108.8294 |          18.5461 |
[32m[20221213 21:14:08 @agent_ppo2.py:185][0m |          -0.0052 |         108.4458 |          18.5509 |
[32m[20221213 21:14:08 @agent_ppo2.py:185][0m |           0.0030 |         122.7671 |          18.5505 |
[32m[20221213 21:14:09 @agent_ppo2.py:185][0m |          -0.0045 |         108.1295 |          18.5539 |
[32m[20221213 21:14:09 @agent_ppo2.py:185][0m |          -0.0080 |         108.1614 |          18.5564 |
[32m[20221213 21:14:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:14:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 610.20
[32m[20221213 21:14:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 689.00
[32m[20221213 21:14:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 657.00
[32m[20221213 21:14:09 @agent_ppo2.py:143][0m Total time:      18.57 min
[32m[20221213 21:14:09 @agent_ppo2.py:145][0m 1808384 total steps have happened
[32m[20221213 21:14:09 @agent_ppo2.py:121][0m #------------------------ Iteration 883 --------------------------#
[32m[20221213 21:14:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:09 @agent_ppo2.py:185][0m |          -0.0012 |         105.8861 |          18.5564 |
[32m[20221213 21:14:09 @agent_ppo2.py:185][0m |           0.0072 |         112.5511 |          18.5239 |
[32m[20221213 21:14:09 @agent_ppo2.py:185][0m |          -0.0011 |         104.8128 |          18.4963 |
[32m[20221213 21:14:09 @agent_ppo2.py:185][0m |          -0.0089 |         103.9859 |          18.5135 |
[32m[20221213 21:14:09 @agent_ppo2.py:185][0m |          -0.0038 |         103.7967 |          18.4872 |
[32m[20221213 21:14:09 @agent_ppo2.py:185][0m |          -0.0111 |         103.6283 |          18.4982 |
[32m[20221213 21:14:10 @agent_ppo2.py:185][0m |          -0.0045 |         104.4070 |          18.4843 |
[32m[20221213 21:14:10 @agent_ppo2.py:185][0m |           0.0035 |         108.9264 |          18.4859 |
[32m[20221213 21:14:10 @agent_ppo2.py:185][0m |          -0.0088 |         102.9829 |          18.4985 |
[32m[20221213 21:14:10 @agent_ppo2.py:185][0m |          -0.0078 |         102.7358 |          18.4803 |
[32m[20221213 21:14:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:14:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 606.40
[32m[20221213 21:14:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 658.00
[32m[20221213 21:14:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.00
[32m[20221213 21:14:10 @agent_ppo2.py:143][0m Total time:      18.59 min
[32m[20221213 21:14:10 @agent_ppo2.py:145][0m 1810432 total steps have happened
[32m[20221213 21:14:10 @agent_ppo2.py:121][0m #------------------------ Iteration 884 --------------------------#
[32m[20221213 21:14:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:14:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:10 @agent_ppo2.py:185][0m |          -0.0005 |         108.1108 |          18.4798 |
[32m[20221213 21:14:10 @agent_ppo2.py:185][0m |           0.0044 |         117.9140 |          18.4777 |
[32m[20221213 21:14:10 @agent_ppo2.py:185][0m |           0.0038 |         114.6020 |          18.4720 |
[32m[20221213 21:14:11 @agent_ppo2.py:185][0m |          -0.0067 |         105.2107 |          18.4473 |
[32m[20221213 21:14:11 @agent_ppo2.py:185][0m |          -0.0097 |         104.6707 |          18.4627 |
[32m[20221213 21:14:11 @agent_ppo2.py:185][0m |          -0.0074 |         104.3383 |          18.4529 |
[32m[20221213 21:14:11 @agent_ppo2.py:185][0m |          -0.0078 |         104.2635 |          18.4403 |
[32m[20221213 21:14:11 @agent_ppo2.py:185][0m |          -0.0095 |         104.0460 |          18.4357 |
[32m[20221213 21:14:11 @agent_ppo2.py:185][0m |          -0.0077 |         103.7248 |          18.4298 |
[32m[20221213 21:14:11 @agent_ppo2.py:185][0m |           0.0017 |         111.6743 |          18.4257 |
[32m[20221213 21:14:11 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:14:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 645.00
[32m[20221213 21:14:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 703.00
[32m[20221213 21:14:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 661.00
[32m[20221213 21:14:11 @agent_ppo2.py:143][0m Total time:      18.61 min
[32m[20221213 21:14:11 @agent_ppo2.py:145][0m 1812480 total steps have happened
[32m[20221213 21:14:11 @agent_ppo2.py:121][0m #------------------------ Iteration 885 --------------------------#
[32m[20221213 21:14:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:11 @agent_ppo2.py:185][0m |          -0.0001 |         108.8002 |          18.3980 |
[32m[20221213 21:14:12 @agent_ppo2.py:185][0m |          -0.0051 |         107.3697 |          18.3911 |
[32m[20221213 21:14:12 @agent_ppo2.py:185][0m |          -0.0028 |         106.7500 |          18.3964 |
[32m[20221213 21:14:12 @agent_ppo2.py:185][0m |          -0.0029 |         106.3687 |          18.3709 |
[32m[20221213 21:14:12 @agent_ppo2.py:185][0m |          -0.0063 |         105.8738 |          18.3601 |
[32m[20221213 21:14:12 @agent_ppo2.py:185][0m |          -0.0066 |         105.7555 |          18.3559 |
[32m[20221213 21:14:12 @agent_ppo2.py:185][0m |          -0.0083 |         105.4680 |          18.3507 |
[32m[20221213 21:14:12 @agent_ppo2.py:185][0m |          -0.0065 |         105.2041 |          18.3441 |
[32m[20221213 21:14:12 @agent_ppo2.py:185][0m |          -0.0093 |         105.0943 |          18.3423 |
[32m[20221213 21:14:12 @agent_ppo2.py:185][0m |          -0.0082 |         104.8875 |          18.3283 |
[32m[20221213 21:14:12 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:14:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 613.20
[32m[20221213 21:14:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 674.00
[32m[20221213 21:14:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 608.00
[32m[20221213 21:14:12 @agent_ppo2.py:143][0m Total time:      18.63 min
[32m[20221213 21:14:12 @agent_ppo2.py:145][0m 1814528 total steps have happened
[32m[20221213 21:14:12 @agent_ppo2.py:121][0m #------------------------ Iteration 886 --------------------------#
[32m[20221213 21:14:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:13 @agent_ppo2.py:185][0m |           0.0000 |         109.2543 |          18.3937 |
[32m[20221213 21:14:13 @agent_ppo2.py:185][0m |          -0.0047 |         108.1559 |          18.3543 |
[32m[20221213 21:14:13 @agent_ppo2.py:185][0m |          -0.0043 |         107.4950 |          18.3464 |
[32m[20221213 21:14:13 @agent_ppo2.py:185][0m |          -0.0025 |         108.5736 |          18.3539 |
[32m[20221213 21:14:13 @agent_ppo2.py:185][0m |          -0.0073 |         106.6800 |          18.3273 |
[32m[20221213 21:14:13 @agent_ppo2.py:185][0m |          -0.0069 |         106.2845 |          18.3181 |
[32m[20221213 21:14:13 @agent_ppo2.py:185][0m |          -0.0080 |         106.0186 |          18.3033 |
[32m[20221213 21:14:13 @agent_ppo2.py:185][0m |          -0.0082 |         105.6552 |          18.3053 |
[32m[20221213 21:14:13 @agent_ppo2.py:185][0m |          -0.0086 |         105.4647 |          18.3000 |
[32m[20221213 21:14:14 @agent_ppo2.py:185][0m |          -0.0067 |         105.5302 |          18.2950 |
[32m[20221213 21:14:14 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:14:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 599.00
[32m[20221213 21:14:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 619.00
[32m[20221213 21:14:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:14:14 @agent_ppo2.py:143][0m Total time:      18.65 min
[32m[20221213 21:14:14 @agent_ppo2.py:145][0m 1816576 total steps have happened
[32m[20221213 21:14:14 @agent_ppo2.py:121][0m #------------------------ Iteration 887 --------------------------#
[32m[20221213 21:14:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:14 @agent_ppo2.py:185][0m |           0.0078 |         111.9758 |          18.1804 |
[32m[20221213 21:14:14 @agent_ppo2.py:185][0m |          -0.0038 |         106.8989 |          18.1626 |
[32m[20221213 21:14:14 @agent_ppo2.py:185][0m |          -0.0060 |         106.2710 |          18.1542 |
[32m[20221213 21:14:14 @agent_ppo2.py:185][0m |          -0.0061 |         105.8389 |          18.1542 |
[32m[20221213 21:14:14 @agent_ppo2.py:185][0m |          -0.0044 |         105.6066 |          18.1535 |
[32m[20221213 21:14:14 @agent_ppo2.py:185][0m |          -0.0061 |         105.3853 |          18.1604 |
[32m[20221213 21:14:15 @agent_ppo2.py:185][0m |          -0.0086 |         105.0834 |          18.1546 |
[32m[20221213 21:14:15 @agent_ppo2.py:185][0m |          -0.0054 |         105.3629 |          18.1498 |
[32m[20221213 21:14:15 @agent_ppo2.py:185][0m |          -0.0084 |         104.6491 |          18.1480 |
[32m[20221213 21:14:15 @agent_ppo2.py:185][0m |          -0.0107 |         104.5000 |          18.1486 |
[32m[20221213 21:14:15 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:14:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 610.40
[32m[20221213 21:14:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 666.00
[32m[20221213 21:14:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 655.00
[32m[20221213 21:14:15 @agent_ppo2.py:143][0m Total time:      18.67 min
[32m[20221213 21:14:15 @agent_ppo2.py:145][0m 1818624 total steps have happened
[32m[20221213 21:14:15 @agent_ppo2.py:121][0m #------------------------ Iteration 888 --------------------------#
[32m[20221213 21:14:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:15 @agent_ppo2.py:185][0m |           0.0027 |         112.1290 |          18.3867 |
[32m[20221213 21:14:15 @agent_ppo2.py:185][0m |          -0.0047 |         110.1240 |          18.3767 |
[32m[20221213 21:14:15 @agent_ppo2.py:185][0m |          -0.0013 |         110.4438 |          18.3707 |
[32m[20221213 21:14:16 @agent_ppo2.py:185][0m |          -0.0067 |         109.2480 |          18.3746 |
[32m[20221213 21:14:16 @agent_ppo2.py:185][0m |          -0.0049 |         110.8524 |          18.3661 |
[32m[20221213 21:14:16 @agent_ppo2.py:185][0m |          -0.0061 |         108.7636 |          18.3782 |
[32m[20221213 21:14:16 @agent_ppo2.py:185][0m |          -0.0087 |         108.6057 |          18.3784 |
[32m[20221213 21:14:16 @agent_ppo2.py:185][0m |          -0.0086 |         108.4075 |          18.3634 |
[32m[20221213 21:14:16 @agent_ppo2.py:185][0m |          -0.0109 |         108.2264 |          18.3645 |
[32m[20221213 21:14:16 @agent_ppo2.py:185][0m |          -0.0126 |         108.1404 |          18.3592 |
[32m[20221213 21:14:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:14:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 608.40
[32m[20221213 21:14:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 644.00
[32m[20221213 21:14:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 738.00
[32m[20221213 21:14:16 @agent_ppo2.py:143][0m Total time:      18.69 min
[32m[20221213 21:14:16 @agent_ppo2.py:145][0m 1820672 total steps have happened
[32m[20221213 21:14:16 @agent_ppo2.py:121][0m #------------------------ Iteration 889 --------------------------#
[32m[20221213 21:14:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:17 @agent_ppo2.py:185][0m |           0.0010 |         109.9943 |          18.3253 |
[32m[20221213 21:14:17 @agent_ppo2.py:185][0m |          -0.0051 |         109.1251 |          18.3025 |
[32m[20221213 21:14:17 @agent_ppo2.py:185][0m |          -0.0065 |         108.5775 |          18.2840 |
[32m[20221213 21:14:17 @agent_ppo2.py:185][0m |          -0.0048 |         108.2666 |          18.2680 |
[32m[20221213 21:14:17 @agent_ppo2.py:185][0m |          -0.0053 |         108.1669 |          18.2737 |
[32m[20221213 21:14:17 @agent_ppo2.py:185][0m |          -0.0051 |         107.9957 |          18.2665 |
[32m[20221213 21:14:17 @agent_ppo2.py:185][0m |          -0.0101 |         107.6366 |          18.2631 |
[32m[20221213 21:14:17 @agent_ppo2.py:185][0m |          -0.0031 |         108.9979 |          18.2595 |
[32m[20221213 21:14:17 @agent_ppo2.py:185][0m |          -0.0044 |         107.6584 |          18.2552 |
[32m[20221213 21:14:17 @agent_ppo2.py:185][0m |          -0.0073 |         106.7998 |          18.2461 |
[32m[20221213 21:14:17 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:14:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.40
[32m[20221213 21:14:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 651.00
[32m[20221213 21:14:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 662.00
[32m[20221213 21:14:17 @agent_ppo2.py:143][0m Total time:      18.71 min
[32m[20221213 21:14:17 @agent_ppo2.py:145][0m 1822720 total steps have happened
[32m[20221213 21:14:17 @agent_ppo2.py:121][0m #------------------------ Iteration 890 --------------------------#
[32m[20221213 21:14:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:18 @agent_ppo2.py:185][0m |          -0.0018 |         113.4513 |          18.1974 |
[32m[20221213 21:14:18 @agent_ppo2.py:185][0m |           0.0085 |         123.0619 |          18.1808 |
[32m[20221213 21:14:18 @agent_ppo2.py:185][0m |          -0.0019 |         111.7138 |          18.1573 |
[32m[20221213 21:14:18 @agent_ppo2.py:185][0m |          -0.0075 |         111.3106 |          18.1796 |
[32m[20221213 21:14:18 @agent_ppo2.py:185][0m |          -0.0062 |         111.1385 |          18.1680 |
[32m[20221213 21:14:18 @agent_ppo2.py:185][0m |          -0.0017 |         111.6776 |          18.1663 |
[32m[20221213 21:14:18 @agent_ppo2.py:185][0m |          -0.0018 |         111.1933 |          18.1603 |
[32m[20221213 21:14:18 @agent_ppo2.py:185][0m |          -0.0061 |         111.8605 |          18.1542 |
[32m[20221213 21:14:19 @agent_ppo2.py:185][0m |          -0.0078 |         110.4076 |          18.1611 |
[32m[20221213 21:14:19 @agent_ppo2.py:185][0m |          -0.0066 |         110.6257 |          18.1594 |
[32m[20221213 21:14:19 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:14:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 601.60
[32m[20221213 21:14:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 641.00
[32m[20221213 21:14:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 647.00
[32m[20221213 21:14:19 @agent_ppo2.py:143][0m Total time:      18.73 min
[32m[20221213 21:14:19 @agent_ppo2.py:145][0m 1824768 total steps have happened
[32m[20221213 21:14:19 @agent_ppo2.py:121][0m #------------------------ Iteration 891 --------------------------#
[32m[20221213 21:14:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:19 @agent_ppo2.py:185][0m |          -0.0037 |         112.9472 |          18.3055 |
[32m[20221213 21:14:19 @agent_ppo2.py:185][0m |          -0.0042 |         111.0879 |          18.2813 |
[32m[20221213 21:14:19 @agent_ppo2.py:185][0m |          -0.0080 |         110.1751 |          18.2913 |
[32m[20221213 21:14:19 @agent_ppo2.py:185][0m |          -0.0086 |         109.7707 |          18.2708 |
[32m[20221213 21:14:19 @agent_ppo2.py:185][0m |          -0.0093 |         109.4576 |          18.2697 |
[32m[20221213 21:14:19 @agent_ppo2.py:185][0m |          -0.0081 |         109.4494 |          18.2553 |
[32m[20221213 21:14:20 @agent_ppo2.py:185][0m |          -0.0042 |         113.6364 |          18.2535 |
[32m[20221213 21:14:20 @agent_ppo2.py:185][0m |          -0.0108 |         108.5284 |          18.2471 |
[32m[20221213 21:14:20 @agent_ppo2.py:185][0m |          -0.0113 |         108.3202 |          18.2462 |
[32m[20221213 21:14:20 @agent_ppo2.py:185][0m |          -0.0143 |         108.5623 |          18.2258 |
[32m[20221213 21:14:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:14:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 603.80
[32m[20221213 21:14:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 678.00
[32m[20221213 21:14:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 658.00
[32m[20221213 21:14:20 @agent_ppo2.py:143][0m Total time:      18.75 min
[32m[20221213 21:14:20 @agent_ppo2.py:145][0m 1826816 total steps have happened
[32m[20221213 21:14:20 @agent_ppo2.py:121][0m #------------------------ Iteration 892 --------------------------#
[32m[20221213 21:14:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:20 @agent_ppo2.py:185][0m |          -0.0033 |         116.8860 |          18.0852 |
[32m[20221213 21:14:20 @agent_ppo2.py:185][0m |          -0.0047 |         113.9122 |          18.0743 |
[32m[20221213 21:14:20 @agent_ppo2.py:185][0m |          -0.0061 |         113.6450 |          18.0580 |
[32m[20221213 21:14:20 @agent_ppo2.py:185][0m |          -0.0057 |         113.0842 |          18.0489 |
[32m[20221213 21:14:21 @agent_ppo2.py:185][0m |          -0.0049 |         112.5763 |          18.0238 |
[32m[20221213 21:14:21 @agent_ppo2.py:185][0m |          -0.0059 |         112.3451 |          18.0229 |
[32m[20221213 21:14:21 @agent_ppo2.py:185][0m |          -0.0070 |         112.1750 |          18.0101 |
[32m[20221213 21:14:21 @agent_ppo2.py:185][0m |          -0.0097 |         112.1468 |          18.0028 |
[32m[20221213 21:14:21 @agent_ppo2.py:185][0m |          -0.0087 |         111.7064 |          17.9917 |
[32m[20221213 21:14:21 @agent_ppo2.py:185][0m |          -0.0091 |         111.7867 |          17.9747 |
[32m[20221213 21:14:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:14:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 617.80
[32m[20221213 21:14:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 702.00
[32m[20221213 21:14:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.00
[32m[20221213 21:14:21 @agent_ppo2.py:143][0m Total time:      18.77 min
[32m[20221213 21:14:21 @agent_ppo2.py:145][0m 1828864 total steps have happened
[32m[20221213 21:14:21 @agent_ppo2.py:121][0m #------------------------ Iteration 893 --------------------------#
[32m[20221213 21:14:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:21 @agent_ppo2.py:185][0m |           0.0010 |         116.1855 |          18.1210 |
[32m[20221213 21:14:22 @agent_ppo2.py:185][0m |          -0.0060 |         111.3126 |          18.1285 |
[32m[20221213 21:14:22 @agent_ppo2.py:185][0m |          -0.0056 |         108.4781 |          18.1340 |
[32m[20221213 21:14:22 @agent_ppo2.py:185][0m |          -0.0081 |         106.2910 |          18.1406 |
[32m[20221213 21:14:22 @agent_ppo2.py:185][0m |          -0.0083 |         104.8754 |          18.1296 |
[32m[20221213 21:14:22 @agent_ppo2.py:185][0m |          -0.0085 |         103.4631 |          18.1408 |
[32m[20221213 21:14:22 @agent_ppo2.py:185][0m |          -0.0062 |         102.9933 |          18.1410 |
[32m[20221213 21:14:22 @agent_ppo2.py:185][0m |          -0.0014 |         103.5649 |          18.1468 |
[32m[20221213 21:14:22 @agent_ppo2.py:185][0m |          -0.0069 |          99.8143 |          18.1403 |
[32m[20221213 21:14:22 @agent_ppo2.py:185][0m |          -0.0089 |          99.0113 |          18.1543 |
[32m[20221213 21:14:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:14:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 614.20
[32m[20221213 21:14:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 649.00
[32m[20221213 21:14:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 710.00
[32m[20221213 21:14:22 @agent_ppo2.py:143][0m Total time:      18.79 min
[32m[20221213 21:14:22 @agent_ppo2.py:145][0m 1830912 total steps have happened
[32m[20221213 21:14:22 @agent_ppo2.py:121][0m #------------------------ Iteration 894 --------------------------#
[32m[20221213 21:14:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:14:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:23 @agent_ppo2.py:185][0m |          -0.0044 |         126.1843 |          18.1204 |
[32m[20221213 21:14:23 @agent_ppo2.py:185][0m |          -0.0066 |         121.2907 |          18.1142 |
[32m[20221213 21:14:23 @agent_ppo2.py:185][0m |          -0.0085 |         119.3048 |          18.1016 |
[32m[20221213 21:14:23 @agent_ppo2.py:185][0m |          -0.0073 |         118.9128 |          18.1146 |
[32m[20221213 21:14:23 @agent_ppo2.py:185][0m |          -0.0078 |         118.0467 |          18.1214 |
[32m[20221213 21:14:23 @agent_ppo2.py:185][0m |          -0.0065 |         117.5723 |          18.1224 |
[32m[20221213 21:14:23 @agent_ppo2.py:185][0m |          -0.0081 |         117.2871 |          18.1193 |
[32m[20221213 21:14:23 @agent_ppo2.py:185][0m |          -0.0030 |         119.6937 |          18.1235 |
[32m[20221213 21:14:23 @agent_ppo2.py:185][0m |          -0.0061 |         117.3292 |          18.1243 |
[32m[20221213 21:14:23 @agent_ppo2.py:185][0m |          -0.0081 |         116.4380 |          18.1246 |
[32m[20221213 21:14:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:14:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 599.80
[32m[20221213 21:14:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 642.00
[32m[20221213 21:14:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 675.00
[32m[20221213 21:14:24 @agent_ppo2.py:143][0m Total time:      18.81 min
[32m[20221213 21:14:24 @agent_ppo2.py:145][0m 1832960 total steps have happened
[32m[20221213 21:14:24 @agent_ppo2.py:121][0m #------------------------ Iteration 895 --------------------------#
[32m[20221213 21:14:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:24 @agent_ppo2.py:185][0m |          -0.0046 |         114.8591 |          17.9806 |
[32m[20221213 21:14:24 @agent_ppo2.py:185][0m |          -0.0053 |         113.2587 |          17.9371 |
[32m[20221213 21:14:24 @agent_ppo2.py:185][0m |          -0.0040 |         113.8111 |          17.9411 |
[32m[20221213 21:14:24 @agent_ppo2.py:185][0m |          -0.0069 |         112.1533 |          17.9178 |
[32m[20221213 21:14:24 @agent_ppo2.py:185][0m |          -0.0106 |         111.9280 |          17.9175 |
[32m[20221213 21:14:24 @agent_ppo2.py:185][0m |          -0.0076 |         111.4824 |          17.8947 |
[32m[20221213 21:14:24 @agent_ppo2.py:185][0m |          -0.0105 |         111.1743 |          17.8768 |
[32m[20221213 21:14:24 @agent_ppo2.py:185][0m |          -0.0091 |         111.0098 |          17.8853 |
[32m[20221213 21:14:25 @agent_ppo2.py:185][0m |          -0.0000 |         122.7282 |          17.8673 |
[32m[20221213 21:14:25 @agent_ppo2.py:185][0m |          -0.0090 |         110.8718 |          17.8631 |
[32m[20221213 21:14:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:14:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 623.60
[32m[20221213 21:14:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 670.00
[32m[20221213 21:14:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 733.00
[32m[20221213 21:14:25 @agent_ppo2.py:143][0m Total time:      18.83 min
[32m[20221213 21:14:25 @agent_ppo2.py:145][0m 1835008 total steps have happened
[32m[20221213 21:14:25 @agent_ppo2.py:121][0m #------------------------ Iteration 896 --------------------------#
[32m[20221213 21:14:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:25 @agent_ppo2.py:185][0m |          -0.0015 |         115.1156 |          18.0780 |
[32m[20221213 21:14:25 @agent_ppo2.py:185][0m |           0.0129 |         127.9772 |          18.0738 |
[32m[20221213 21:14:25 @agent_ppo2.py:185][0m |          -0.0043 |         111.5475 |          18.0763 |
[32m[20221213 21:14:25 @agent_ppo2.py:185][0m |           0.0003 |         111.8642 |          18.0722 |
[32m[20221213 21:14:25 @agent_ppo2.py:185][0m |           0.0090 |         124.3753 |          18.0871 |
[32m[20221213 21:14:25 @agent_ppo2.py:185][0m |          -0.0040 |         109.9940 |          18.0680 |
[32m[20221213 21:14:26 @agent_ppo2.py:185][0m |          -0.0066 |         109.6025 |          18.0745 |
[32m[20221213 21:14:26 @agent_ppo2.py:185][0m |          -0.0082 |         109.3894 |          18.0748 |
[32m[20221213 21:14:26 @agent_ppo2.py:185][0m |          -0.0086 |         109.2179 |          18.0770 |
[32m[20221213 21:14:26 @agent_ppo2.py:185][0m |          -0.0072 |         108.9106 |          18.0758 |
[32m[20221213 21:14:26 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:14:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 621.80
[32m[20221213 21:14:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 672.00
[32m[20221213 21:14:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 624.00
[32m[20221213 21:14:26 @agent_ppo2.py:143][0m Total time:      18.85 min
[32m[20221213 21:14:26 @agent_ppo2.py:145][0m 1837056 total steps have happened
[32m[20221213 21:14:26 @agent_ppo2.py:121][0m #------------------------ Iteration 897 --------------------------#
[32m[20221213 21:14:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:14:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:26 @agent_ppo2.py:185][0m |           0.0003 |         116.1223 |          17.9199 |
[32m[20221213 21:14:26 @agent_ppo2.py:185][0m |          -0.0030 |         113.4059 |          17.9155 |
[32m[20221213 21:14:26 @agent_ppo2.py:185][0m |          -0.0065 |         112.3359 |          17.9128 |
[32m[20221213 21:14:27 @agent_ppo2.py:185][0m |           0.0064 |         122.7582 |          17.8983 |
[32m[20221213 21:14:27 @agent_ppo2.py:185][0m |          -0.0055 |         110.6253 |          17.8952 |
[32m[20221213 21:14:27 @agent_ppo2.py:185][0m |          -0.0021 |         112.3765 |          17.8862 |
[32m[20221213 21:14:27 @agent_ppo2.py:185][0m |          -0.0083 |         109.7499 |          17.8804 |
[32m[20221213 21:14:27 @agent_ppo2.py:185][0m |          -0.0062 |         109.4349 |          17.8809 |
[32m[20221213 21:14:27 @agent_ppo2.py:185][0m |          -0.0022 |         110.7258 |          17.8815 |
[32m[20221213 21:14:27 @agent_ppo2.py:185][0m |          -0.0051 |         109.3577 |          17.8775 |
[32m[20221213 21:14:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:14:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 627.80
[32m[20221213 21:14:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 656.00
[32m[20221213 21:14:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.00
[32m[20221213 21:14:27 @agent_ppo2.py:143][0m Total time:      18.87 min
[32m[20221213 21:14:27 @agent_ppo2.py:145][0m 1839104 total steps have happened
[32m[20221213 21:14:27 @agent_ppo2.py:121][0m #------------------------ Iteration 898 --------------------------#
[32m[20221213 21:14:27 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:14:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:27 @agent_ppo2.py:185][0m |           0.0008 |         113.6179 |          18.0960 |
[32m[20221213 21:14:28 @agent_ppo2.py:185][0m |          -0.0001 |         110.4679 |          18.0748 |
[32m[20221213 21:14:28 @agent_ppo2.py:185][0m |          -0.0066 |         109.5819 |          18.0754 |
[32m[20221213 21:14:28 @agent_ppo2.py:185][0m |          -0.0059 |         108.7513 |          18.0684 |
[32m[20221213 21:14:28 @agent_ppo2.py:185][0m |          -0.0054 |         108.3950 |          18.0644 |
[32m[20221213 21:14:28 @agent_ppo2.py:185][0m |          -0.0059 |         108.3880 |          18.0568 |
[32m[20221213 21:14:28 @agent_ppo2.py:185][0m |          -0.0066 |         107.8597 |          18.0539 |
[32m[20221213 21:14:28 @agent_ppo2.py:185][0m |           0.0001 |         109.9299 |          18.0499 |
[32m[20221213 21:14:28 @agent_ppo2.py:185][0m |          -0.0051 |         107.5854 |          18.0401 |
[32m[20221213 21:14:28 @agent_ppo2.py:185][0m |          -0.0076 |         107.4610 |          18.0334 |
[32m[20221213 21:14:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:14:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.20
[32m[20221213 21:14:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.00
[32m[20221213 21:14:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 654.00
[32m[20221213 21:14:28 @agent_ppo2.py:143][0m Total time:      18.90 min
[32m[20221213 21:14:28 @agent_ppo2.py:145][0m 1841152 total steps have happened
[32m[20221213 21:14:28 @agent_ppo2.py:121][0m #------------------------ Iteration 899 --------------------------#
[32m[20221213 21:14:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:29 @agent_ppo2.py:185][0m |           0.0082 |         126.8711 |          17.9294 |
[32m[20221213 21:14:29 @agent_ppo2.py:185][0m |           0.0029 |         114.4492 |          17.9401 |
[32m[20221213 21:14:29 @agent_ppo2.py:185][0m |          -0.0036 |         111.6123 |          17.9382 |
[32m[20221213 21:14:29 @agent_ppo2.py:185][0m |          -0.0005 |         113.6769 |          17.9477 |
[32m[20221213 21:14:29 @agent_ppo2.py:185][0m |          -0.0080 |         110.8895 |          17.9519 |
[32m[20221213 21:14:29 @agent_ppo2.py:185][0m |          -0.0059 |         110.6491 |          17.9468 |
[32m[20221213 21:14:29 @agent_ppo2.py:185][0m |          -0.0102 |         110.5550 |          17.9460 |
[32m[20221213 21:14:29 @agent_ppo2.py:185][0m |          -0.0056 |         110.2376 |          17.9368 |
[32m[20221213 21:14:29 @agent_ppo2.py:185][0m |          -0.0071 |         110.1902 |          17.9430 |
[32m[20221213 21:14:30 @agent_ppo2.py:185][0m |          -0.0084 |         109.9044 |          17.9516 |
[32m[20221213 21:14:30 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:14:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 614.20
[32m[20221213 21:14:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 641.00
[32m[20221213 21:14:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 626.00
[32m[20221213 21:14:30 @agent_ppo2.py:143][0m Total time:      18.92 min
[32m[20221213 21:14:30 @agent_ppo2.py:145][0m 1843200 total steps have happened
[32m[20221213 21:14:30 @agent_ppo2.py:121][0m #------------------------ Iteration 900 --------------------------#
[32m[20221213 21:14:30 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:14:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:30 @agent_ppo2.py:185][0m |          -0.0004 |         108.6553 |          18.0238 |
[32m[20221213 21:14:30 @agent_ppo2.py:185][0m |          -0.0015 |         108.1429 |          18.0072 |
[32m[20221213 21:14:30 @agent_ppo2.py:185][0m |          -0.0042 |         107.2521 |          18.0124 |
[32m[20221213 21:14:30 @agent_ppo2.py:185][0m |          -0.0040 |         106.8064 |          18.0037 |
[32m[20221213 21:14:30 @agent_ppo2.py:185][0m |          -0.0061 |         106.4272 |          18.0041 |
[32m[20221213 21:14:30 @agent_ppo2.py:185][0m |          -0.0051 |         106.3831 |          17.9915 |
[32m[20221213 21:14:30 @agent_ppo2.py:185][0m |          -0.0098 |         106.0678 |          17.9909 |
[32m[20221213 21:14:31 @agent_ppo2.py:185][0m |          -0.0074 |         105.7437 |          17.9754 |
[32m[20221213 21:14:31 @agent_ppo2.py:185][0m |          -0.0074 |         105.6947 |          17.9810 |
[32m[20221213 21:14:31 @agent_ppo2.py:185][0m |          -0.0083 |         105.7231 |          17.9632 |
[32m[20221213 21:14:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:14:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 579.00
[32m[20221213 21:14:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 639.00
[32m[20221213 21:14:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 599.00
[32m[20221213 21:14:31 @agent_ppo2.py:143][0m Total time:      18.94 min
[32m[20221213 21:14:31 @agent_ppo2.py:145][0m 1845248 total steps have happened
[32m[20221213 21:14:31 @agent_ppo2.py:121][0m #------------------------ Iteration 901 --------------------------#
[32m[20221213 21:14:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:31 @agent_ppo2.py:185][0m |          -0.0015 |         110.1035 |          18.0046 |
[32m[20221213 21:14:31 @agent_ppo2.py:185][0m |          -0.0009 |         108.7227 |          17.9989 |
[32m[20221213 21:14:31 @agent_ppo2.py:185][0m |          -0.0039 |         107.9459 |          17.9999 |
[32m[20221213 21:14:31 @agent_ppo2.py:185][0m |          -0.0045 |         107.3098 |          17.9997 |
[32m[20221213 21:14:31 @agent_ppo2.py:185][0m |          -0.0033 |         107.1331 |          17.9877 |
[32m[20221213 21:14:32 @agent_ppo2.py:185][0m |          -0.0066 |         106.6185 |          17.9790 |
[32m[20221213 21:14:32 @agent_ppo2.py:185][0m |          -0.0075 |         106.1898 |          17.9740 |
[32m[20221213 21:14:32 @agent_ppo2.py:185][0m |          -0.0074 |         106.2116 |          17.9794 |
[32m[20221213 21:14:32 @agent_ppo2.py:185][0m |          -0.0087 |         105.9665 |          17.9647 |
[32m[20221213 21:14:32 @agent_ppo2.py:185][0m |          -0.0055 |         105.6802 |          17.9662 |
[32m[20221213 21:14:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:14:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 646.00
[32m[20221213 21:14:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 668.00
[32m[20221213 21:14:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 676.00
[32m[20221213 21:14:32 @agent_ppo2.py:143][0m Total time:      18.96 min
[32m[20221213 21:14:32 @agent_ppo2.py:145][0m 1847296 total steps have happened
[32m[20221213 21:14:32 @agent_ppo2.py:121][0m #------------------------ Iteration 902 --------------------------#
[32m[20221213 21:14:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:32 @agent_ppo2.py:185][0m |          -0.0002 |         114.8992 |          17.9817 |
[32m[20221213 21:14:32 @agent_ppo2.py:185][0m |          -0.0034 |         113.0571 |          17.9882 |
[32m[20221213 21:14:33 @agent_ppo2.py:185][0m |          -0.0047 |         112.1725 |          17.9880 |
[32m[20221213 21:14:33 @agent_ppo2.py:185][0m |          -0.0050 |         111.7944 |          17.9796 |
[32m[20221213 21:14:33 @agent_ppo2.py:185][0m |           0.0049 |         118.9151 |          17.9806 |
[32m[20221213 21:14:33 @agent_ppo2.py:185][0m |          -0.0015 |         111.2552 |          17.9601 |
[32m[20221213 21:14:33 @agent_ppo2.py:185][0m |           0.0028 |         114.3953 |          17.9794 |
[32m[20221213 21:14:33 @agent_ppo2.py:185][0m |          -0.0060 |         110.5715 |          17.9657 |
[32m[20221213 21:14:33 @agent_ppo2.py:185][0m |          -0.0079 |         110.4155 |          17.9753 |
[32m[20221213 21:14:33 @agent_ppo2.py:185][0m |          -0.0060 |         110.3188 |          17.9736 |
[32m[20221213 21:14:33 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:14:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 623.60
[32m[20221213 21:14:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 658.00
[32m[20221213 21:14:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 668.00
[32m[20221213 21:14:33 @agent_ppo2.py:143][0m Total time:      18.98 min
[32m[20221213 21:14:33 @agent_ppo2.py:145][0m 1849344 total steps have happened
[32m[20221213 21:14:33 @agent_ppo2.py:121][0m #------------------------ Iteration 903 --------------------------#
[32m[20221213 21:14:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:34 @agent_ppo2.py:185][0m |          -0.0014 |         114.7433 |          17.8366 |
[32m[20221213 21:14:34 @agent_ppo2.py:185][0m |           0.0015 |         118.2792 |          17.8358 |
[32m[20221213 21:14:34 @agent_ppo2.py:185][0m |          -0.0055 |         112.6934 |          17.8329 |
[32m[20221213 21:14:34 @agent_ppo2.py:185][0m |           0.0095 |         124.0967 |          17.8311 |
[32m[20221213 21:14:34 @agent_ppo2.py:185][0m |          -0.0035 |         111.9281 |          17.8285 |
[32m[20221213 21:14:34 @agent_ppo2.py:185][0m |          -0.0028 |         117.4042 |          17.8156 |
[32m[20221213 21:14:34 @agent_ppo2.py:185][0m |          -0.0060 |         111.4864 |          17.8125 |
[32m[20221213 21:14:34 @agent_ppo2.py:185][0m |          -0.0070 |         111.1452 |          17.8139 |
[32m[20221213 21:14:34 @agent_ppo2.py:185][0m |          -0.0040 |         111.5569 |          17.8066 |
[32m[20221213 21:14:34 @agent_ppo2.py:185][0m |          -0.0092 |         110.5947 |          17.8242 |
[32m[20221213 21:14:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:14:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 636.20
[32m[20221213 21:14:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 662.00
[32m[20221213 21:14:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 609.00
[32m[20221213 21:14:34 @agent_ppo2.py:143][0m Total time:      19.00 min
[32m[20221213 21:14:34 @agent_ppo2.py:145][0m 1851392 total steps have happened
[32m[20221213 21:14:34 @agent_ppo2.py:121][0m #------------------------ Iteration 904 --------------------------#
[32m[20221213 21:14:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:35 @agent_ppo2.py:185][0m |          -0.0057 |         110.7063 |          17.8297 |
[32m[20221213 21:14:35 @agent_ppo2.py:185][0m |          -0.0074 |         106.2202 |          17.7831 |
[32m[20221213 21:14:35 @agent_ppo2.py:185][0m |          -0.0096 |         104.4463 |          17.7633 |
[32m[20221213 21:14:35 @agent_ppo2.py:185][0m |          -0.0063 |         103.4072 |          17.7512 |
[32m[20221213 21:14:35 @agent_ppo2.py:185][0m |          -0.0024 |         103.2933 |          17.7367 |
[32m[20221213 21:14:35 @agent_ppo2.py:185][0m |          -0.0041 |         106.7316 |          17.7280 |
[32m[20221213 21:14:35 @agent_ppo2.py:185][0m |          -0.0117 |         101.5169 |          17.7085 |
[32m[20221213 21:14:35 @agent_ppo2.py:185][0m |          -0.0107 |         100.8871 |          17.7062 |
[32m[20221213 21:14:35 @agent_ppo2.py:185][0m |          -0.0099 |         100.5476 |          17.6967 |
[32m[20221213 21:14:36 @agent_ppo2.py:185][0m |          -0.0079 |         101.0781 |          17.6706 |
[32m[20221213 21:14:36 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:14:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 627.80
[32m[20221213 21:14:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 663.00
[32m[20221213 21:14:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.00
[32m[20221213 21:14:36 @agent_ppo2.py:143][0m Total time:      19.02 min
[32m[20221213 21:14:36 @agent_ppo2.py:145][0m 1853440 total steps have happened
[32m[20221213 21:14:36 @agent_ppo2.py:121][0m #------------------------ Iteration 905 --------------------------#
[32m[20221213 21:14:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:36 @agent_ppo2.py:185][0m |          -0.0014 |         115.7789 |          17.8460 |
[32m[20221213 21:14:36 @agent_ppo2.py:185][0m |          -0.0042 |         114.6472 |          17.8536 |
[32m[20221213 21:14:36 @agent_ppo2.py:185][0m |          -0.0035 |         114.5375 |          17.8585 |
[32m[20221213 21:14:36 @agent_ppo2.py:185][0m |          -0.0073 |         113.8050 |          17.8677 |
[32m[20221213 21:14:36 @agent_ppo2.py:185][0m |          -0.0070 |         113.5272 |          17.8624 |
[32m[20221213 21:14:36 @agent_ppo2.py:185][0m |          -0.0123 |         113.4793 |          17.8625 |
[32m[20221213 21:14:37 @agent_ppo2.py:185][0m |          -0.0047 |         113.1177 |          17.8566 |
[32m[20221213 21:14:37 @agent_ppo2.py:185][0m |          -0.0105 |         113.1487 |          17.8508 |
[32m[20221213 21:14:37 @agent_ppo2.py:185][0m |          -0.0030 |         114.6872 |          17.8627 |
[32m[20221213 21:14:37 @agent_ppo2.py:185][0m |          -0.0074 |         112.9140 |          17.8487 |
[32m[20221213 21:14:37 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:14:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 638.00
[32m[20221213 21:14:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.00
[32m[20221213 21:14:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 679.00
[32m[20221213 21:14:37 @agent_ppo2.py:143][0m Total time:      19.04 min
[32m[20221213 21:14:37 @agent_ppo2.py:145][0m 1855488 total steps have happened
[32m[20221213 21:14:37 @agent_ppo2.py:121][0m #------------------------ Iteration 906 --------------------------#
[32m[20221213 21:14:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:37 @agent_ppo2.py:185][0m |           0.0002 |         115.5201 |          17.8072 |
[32m[20221213 21:14:37 @agent_ppo2.py:185][0m |          -0.0006 |         115.0624 |          17.8046 |
[32m[20221213 21:14:37 @agent_ppo2.py:185][0m |          -0.0054 |         113.7609 |          17.7939 |
[32m[20221213 21:14:37 @agent_ppo2.py:185][0m |          -0.0018 |         114.1834 |          17.7973 |
[32m[20221213 21:14:38 @agent_ppo2.py:185][0m |           0.0049 |         119.9228 |          17.7928 |
[32m[20221213 21:14:38 @agent_ppo2.py:185][0m |          -0.0036 |         113.0996 |          17.7977 |
[32m[20221213 21:14:38 @agent_ppo2.py:185][0m |           0.0052 |         122.8606 |          17.7827 |
[32m[20221213 21:14:38 @agent_ppo2.py:185][0m |          -0.0066 |         112.8286 |          17.7868 |
[32m[20221213 21:14:38 @agent_ppo2.py:185][0m |          -0.0072 |         112.6822 |          17.7928 |
[32m[20221213 21:14:38 @agent_ppo2.py:185][0m |          -0.0058 |         112.5512 |          17.7913 |
[32m[20221213 21:14:38 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:14:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 678.40
[32m[20221213 21:14:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 696.00
[32m[20221213 21:14:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.00
[32m[20221213 21:14:38 @agent_ppo2.py:143][0m Total time:      19.06 min
[32m[20221213 21:14:38 @agent_ppo2.py:145][0m 1857536 total steps have happened
[32m[20221213 21:14:38 @agent_ppo2.py:121][0m #------------------------ Iteration 907 --------------------------#
[32m[20221213 21:14:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:14:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:38 @agent_ppo2.py:185][0m |          -0.0047 |         117.1196 |          17.7593 |
[32m[20221213 21:14:39 @agent_ppo2.py:185][0m |          -0.0008 |         116.1859 |          17.7698 |
[32m[20221213 21:14:39 @agent_ppo2.py:185][0m |          -0.0053 |         114.8845 |          17.7505 |
[32m[20221213 21:14:39 @agent_ppo2.py:185][0m |          -0.0084 |         114.6713 |          17.7623 |
[32m[20221213 21:14:39 @agent_ppo2.py:185][0m |          -0.0056 |         114.3625 |          17.7693 |
[32m[20221213 21:14:39 @agent_ppo2.py:185][0m |          -0.0088 |         114.0013 |          17.7629 |
[32m[20221213 21:14:39 @agent_ppo2.py:185][0m |          -0.0088 |         113.8808 |          17.7635 |
[32m[20221213 21:14:39 @agent_ppo2.py:185][0m |          -0.0077 |         113.8143 |          17.7610 |
[32m[20221213 21:14:39 @agent_ppo2.py:185][0m |          -0.0084 |         113.6334 |          17.7582 |
[32m[20221213 21:14:39 @agent_ppo2.py:185][0m |          -0.0087 |         113.6029 |          17.7561 |
[32m[20221213 21:14:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:14:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 561.20
[32m[20221213 21:14:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 624.00
[32m[20221213 21:14:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 614.00
[32m[20221213 21:14:39 @agent_ppo2.py:143][0m Total time:      19.08 min
[32m[20221213 21:14:39 @agent_ppo2.py:145][0m 1859584 total steps have happened
[32m[20221213 21:14:39 @agent_ppo2.py:121][0m #------------------------ Iteration 908 --------------------------#
[32m[20221213 21:14:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:40 @agent_ppo2.py:185][0m |          -0.0027 |         117.0101 |          17.8979 |
[32m[20221213 21:14:40 @agent_ppo2.py:185][0m |          -0.0047 |         115.9019 |          17.8800 |
[32m[20221213 21:14:40 @agent_ppo2.py:185][0m |          -0.0031 |         115.5084 |          17.8790 |
[32m[20221213 21:14:40 @agent_ppo2.py:185][0m |          -0.0058 |         114.8650 |          17.8784 |
[32m[20221213 21:14:40 @agent_ppo2.py:185][0m |          -0.0049 |         114.5578 |          17.8698 |
[32m[20221213 21:14:40 @agent_ppo2.py:185][0m |          -0.0068 |         114.1781 |          17.8735 |
[32m[20221213 21:14:40 @agent_ppo2.py:185][0m |          -0.0034 |         115.3063 |          17.8679 |
[32m[20221213 21:14:40 @agent_ppo2.py:185][0m |          -0.0047 |         114.2912 |          17.8451 |
[32m[20221213 21:14:40 @agent_ppo2.py:185][0m |          -0.0081 |         113.5669 |          17.8606 |
[32m[20221213 21:14:40 @agent_ppo2.py:185][0m |           0.0184 |         139.7385 |          17.8503 |
[32m[20221213 21:14:40 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:14:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 614.40
[32m[20221213 21:14:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 649.00
[32m[20221213 21:14:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 610.00
[32m[20221213 21:14:41 @agent_ppo2.py:143][0m Total time:      19.10 min
[32m[20221213 21:14:41 @agent_ppo2.py:145][0m 1861632 total steps have happened
[32m[20221213 21:14:41 @agent_ppo2.py:121][0m #------------------------ Iteration 909 --------------------------#
[32m[20221213 21:14:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:41 @agent_ppo2.py:185][0m |           0.0071 |         118.2391 |          17.8352 |
[32m[20221213 21:14:41 @agent_ppo2.py:185][0m |          -0.0038 |         115.6088 |          17.8656 |
[32m[20221213 21:14:41 @agent_ppo2.py:185][0m |          -0.0035 |         114.8674 |          17.8491 |
[32m[20221213 21:14:41 @agent_ppo2.py:185][0m |           0.0003 |         114.6615 |          17.8665 |
[32m[20221213 21:14:41 @agent_ppo2.py:185][0m |          -0.0046 |         114.5138 |          17.8637 |
[32m[20221213 21:14:41 @agent_ppo2.py:185][0m |          -0.0076 |         113.7723 |          17.8584 |
[32m[20221213 21:14:41 @agent_ppo2.py:185][0m |          -0.0004 |         114.9980 |          17.8724 |
[32m[20221213 21:14:42 @agent_ppo2.py:185][0m |          -0.0057 |         113.4512 |          17.8703 |
[32m[20221213 21:14:42 @agent_ppo2.py:185][0m |           0.0004 |         118.5406 |          17.8841 |
[32m[20221213 21:14:42 @agent_ppo2.py:185][0m |          -0.0053 |         113.4162 |          17.8764 |
[32m[20221213 21:14:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:14:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 624.20
[32m[20221213 21:14:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 656.00
[32m[20221213 21:14:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.00
[32m[20221213 21:14:42 @agent_ppo2.py:143][0m Total time:      19.12 min
[32m[20221213 21:14:42 @agent_ppo2.py:145][0m 1863680 total steps have happened
[32m[20221213 21:14:42 @agent_ppo2.py:121][0m #------------------------ Iteration 910 --------------------------#
[32m[20221213 21:14:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:14:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:42 @agent_ppo2.py:185][0m |          -0.0004 |         113.5971 |          17.7743 |
[32m[20221213 21:14:42 @agent_ppo2.py:185][0m |          -0.0016 |         112.5775 |          17.7684 |
[32m[20221213 21:14:42 @agent_ppo2.py:185][0m |          -0.0039 |         111.8367 |          17.7433 |
[32m[20221213 21:14:42 @agent_ppo2.py:185][0m |          -0.0058 |         111.4664 |          17.7491 |
[32m[20221213 21:14:42 @agent_ppo2.py:185][0m |          -0.0059 |         111.3221 |          17.7560 |
[32m[20221213 21:14:43 @agent_ppo2.py:185][0m |          -0.0054 |         111.0861 |          17.7352 |
[32m[20221213 21:14:43 @agent_ppo2.py:185][0m |          -0.0069 |         110.8969 |          17.7483 |
[32m[20221213 21:14:43 @agent_ppo2.py:185][0m |          -0.0065 |         110.6860 |          17.7340 |
[32m[20221213 21:14:43 @agent_ppo2.py:185][0m |          -0.0030 |         113.1639 |          17.7446 |
[32m[20221213 21:14:43 @agent_ppo2.py:185][0m |          -0.0057 |         110.2112 |          17.7257 |
[32m[20221213 21:14:43 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:14:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.60
[32m[20221213 21:14:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 655.00
[32m[20221213 21:14:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 649.00
[32m[20221213 21:14:43 @agent_ppo2.py:143][0m Total time:      19.14 min
[32m[20221213 21:14:43 @agent_ppo2.py:145][0m 1865728 total steps have happened
[32m[20221213 21:14:43 @agent_ppo2.py:121][0m #------------------------ Iteration 911 --------------------------#
[32m[20221213 21:14:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:43 @agent_ppo2.py:185][0m |           0.0066 |         120.7050 |          17.7611 |
[32m[20221213 21:14:43 @agent_ppo2.py:185][0m |           0.0026 |         116.4663 |          17.7334 |
[32m[20221213 21:14:44 @agent_ppo2.py:185][0m |           0.0022 |         118.2833 |          17.7574 |
[32m[20221213 21:14:44 @agent_ppo2.py:185][0m |          -0.0024 |         113.8857 |          17.7528 |
[32m[20221213 21:14:44 @agent_ppo2.py:185][0m |          -0.0070 |         113.9246 |          17.7235 |
[32m[20221213 21:14:44 @agent_ppo2.py:185][0m |          -0.0016 |         113.6902 |          17.7294 |
[32m[20221213 21:14:44 @agent_ppo2.py:185][0m |          -0.0039 |         113.3646 |          17.7365 |
[32m[20221213 21:14:44 @agent_ppo2.py:185][0m |          -0.0044 |         113.0329 |          17.7342 |
[32m[20221213 21:14:44 @agent_ppo2.py:185][0m |          -0.0060 |         112.9992 |          17.7272 |
[32m[20221213 21:14:44 @agent_ppo2.py:185][0m |          -0.0053 |         112.9240 |          17.7275 |
[32m[20221213 21:14:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:14:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 651.40
[32m[20221213 21:14:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.00
[32m[20221213 21:14:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 621.00
[32m[20221213 21:14:44 @agent_ppo2.py:143][0m Total time:      19.16 min
[32m[20221213 21:14:44 @agent_ppo2.py:145][0m 1867776 total steps have happened
[32m[20221213 21:14:44 @agent_ppo2.py:121][0m #------------------------ Iteration 912 --------------------------#
[32m[20221213 21:14:44 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:14:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:45 @agent_ppo2.py:185][0m |           0.0027 |         117.1295 |          17.7528 |
[32m[20221213 21:14:45 @agent_ppo2.py:185][0m |          -0.0044 |         115.0044 |          17.7443 |
[32m[20221213 21:14:45 @agent_ppo2.py:185][0m |           0.0068 |         124.7891 |          17.7523 |
[32m[20221213 21:14:45 @agent_ppo2.py:185][0m |          -0.0076 |         114.1011 |          17.7482 |
[32m[20221213 21:14:45 @agent_ppo2.py:185][0m |          -0.0096 |         113.8119 |          17.7546 |
[32m[20221213 21:14:45 @agent_ppo2.py:185][0m |          -0.0046 |         115.2186 |          17.7454 |
[32m[20221213 21:14:45 @agent_ppo2.py:185][0m |          -0.0096 |         113.4867 |          17.7450 |
[32m[20221213 21:14:45 @agent_ppo2.py:185][0m |          -0.0096 |         113.1368 |          17.7375 |
[32m[20221213 21:14:45 @agent_ppo2.py:185][0m |          -0.0126 |         113.0554 |          17.7446 |
[32m[20221213 21:14:45 @agent_ppo2.py:185][0m |          -0.0104 |         112.9039 |          17.7378 |
[32m[20221213 21:14:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:14:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 669.40
[32m[20221213 21:14:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 702.00
[32m[20221213 21:14:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 652.00
[32m[20221213 21:14:45 @agent_ppo2.py:143][0m Total time:      19.18 min
[32m[20221213 21:14:45 @agent_ppo2.py:145][0m 1869824 total steps have happened
[32m[20221213 21:14:45 @agent_ppo2.py:121][0m #------------------------ Iteration 913 --------------------------#
[32m[20221213 21:14:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:46 @agent_ppo2.py:185][0m |          -0.0026 |         114.9865 |          17.8574 |
[32m[20221213 21:14:46 @agent_ppo2.py:185][0m |           0.0055 |         126.6298 |          17.8536 |
[32m[20221213 21:14:46 @agent_ppo2.py:185][0m |          -0.0057 |         113.5676 |          17.8300 |
[32m[20221213 21:14:46 @agent_ppo2.py:185][0m |          -0.0088 |         112.3652 |          17.8149 |
[32m[20221213 21:14:46 @agent_ppo2.py:185][0m |          -0.0059 |         111.7499 |          17.8195 |
[32m[20221213 21:14:46 @agent_ppo2.py:185][0m |          -0.0046 |         112.4369 |          17.8069 |
[32m[20221213 21:14:46 @agent_ppo2.py:185][0m |          -0.0127 |         111.2540 |          17.7977 |
[32m[20221213 21:14:46 @agent_ppo2.py:185][0m |          -0.0094 |         110.7237 |          17.7937 |
[32m[20221213 21:14:46 @agent_ppo2.py:185][0m |          -0.0056 |         112.2070 |          17.7775 |
[32m[20221213 21:14:47 @agent_ppo2.py:185][0m |          -0.0126 |         110.2462 |          17.7712 |
[32m[20221213 21:14:47 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:14:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 605.60
[32m[20221213 21:14:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 657.00
[32m[20221213 21:14:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 652.00
[32m[20221213 21:14:47 @agent_ppo2.py:143][0m Total time:      19.20 min
[32m[20221213 21:14:47 @agent_ppo2.py:145][0m 1871872 total steps have happened
[32m[20221213 21:14:47 @agent_ppo2.py:121][0m #------------------------ Iteration 914 --------------------------#
[32m[20221213 21:14:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:47 @agent_ppo2.py:185][0m |          -0.0007 |         119.5087 |          17.5762 |
[32m[20221213 21:14:47 @agent_ppo2.py:185][0m |          -0.0042 |         118.0545 |          17.5673 |
[32m[20221213 21:14:47 @agent_ppo2.py:185][0m |          -0.0055 |         117.2080 |          17.5526 |
[32m[20221213 21:14:47 @agent_ppo2.py:185][0m |          -0.0046 |         116.6758 |          17.5580 |
[32m[20221213 21:14:47 @agent_ppo2.py:185][0m |          -0.0084 |         116.1642 |          17.5491 |
[32m[20221213 21:14:47 @agent_ppo2.py:185][0m |          -0.0061 |         115.8188 |          17.5605 |
[32m[20221213 21:14:48 @agent_ppo2.py:185][0m |           0.0015 |         120.5333 |          17.5506 |
[32m[20221213 21:14:48 @agent_ppo2.py:185][0m |           0.0049 |         127.9591 |          17.5335 |
[32m[20221213 21:14:48 @agent_ppo2.py:185][0m |          -0.0114 |         114.7099 |          17.5512 |
[32m[20221213 21:14:48 @agent_ppo2.py:185][0m |          -0.0050 |         115.6624 |          17.5488 |
[32m[20221213 21:14:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:14:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 636.20
[32m[20221213 21:14:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 675.00
[32m[20221213 21:14:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 615.00
[32m[20221213 21:14:48 @agent_ppo2.py:143][0m Total time:      19.22 min
[32m[20221213 21:14:48 @agent_ppo2.py:145][0m 1873920 total steps have happened
[32m[20221213 21:14:48 @agent_ppo2.py:121][0m #------------------------ Iteration 915 --------------------------#
[32m[20221213 21:14:48 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:14:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:48 @agent_ppo2.py:185][0m |           0.0108 |         137.4607 |          17.7669 |
[32m[20221213 21:14:48 @agent_ppo2.py:185][0m |          -0.0034 |         121.2802 |          17.7747 |
[32m[20221213 21:14:48 @agent_ppo2.py:185][0m |          -0.0048 |         120.3934 |          17.7563 |
[32m[20221213 21:14:48 @agent_ppo2.py:185][0m |          -0.0057 |         119.8632 |          17.7593 |
[32m[20221213 21:14:49 @agent_ppo2.py:185][0m |          -0.0073 |         119.4139 |          17.7627 |
[32m[20221213 21:14:49 @agent_ppo2.py:185][0m |          -0.0066 |         119.1078 |          17.7629 |
[32m[20221213 21:14:49 @agent_ppo2.py:185][0m |          -0.0057 |         118.9488 |          17.7509 |
[32m[20221213 21:14:49 @agent_ppo2.py:185][0m |          -0.0091 |         118.7228 |          17.7730 |
[32m[20221213 21:14:49 @agent_ppo2.py:185][0m |          -0.0073 |         118.4266 |          17.7779 |
[32m[20221213 21:14:49 @agent_ppo2.py:185][0m |          -0.0076 |         118.2616 |          17.7623 |
[32m[20221213 21:14:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:14:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.60
[32m[20221213 21:14:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 643.00
[32m[20221213 21:14:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 678.00
[32m[20221213 21:14:49 @agent_ppo2.py:143][0m Total time:      19.24 min
[32m[20221213 21:14:49 @agent_ppo2.py:145][0m 1875968 total steps have happened
[32m[20221213 21:14:49 @agent_ppo2.py:121][0m #------------------------ Iteration 916 --------------------------#
[32m[20221213 21:14:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:49 @agent_ppo2.py:185][0m |          -0.0044 |         119.1239 |          17.6615 |
[32m[20221213 21:14:49 @agent_ppo2.py:185][0m |          -0.0056 |         117.0818 |          17.6470 |
[32m[20221213 21:14:50 @agent_ppo2.py:185][0m |          -0.0054 |         116.2923 |          17.6303 |
[32m[20221213 21:14:50 @agent_ppo2.py:185][0m |          -0.0055 |         115.7347 |          17.6298 |
[32m[20221213 21:14:50 @agent_ppo2.py:185][0m |          -0.0085 |         115.3470 |          17.6191 |
[32m[20221213 21:14:50 @agent_ppo2.py:185][0m |          -0.0045 |         115.4481 |          17.6190 |
[32m[20221213 21:14:50 @agent_ppo2.py:185][0m |          -0.0065 |         117.1311 |          17.5995 |
[32m[20221213 21:14:50 @agent_ppo2.py:185][0m |          -0.0063 |         114.2968 |          17.5919 |
[32m[20221213 21:14:50 @agent_ppo2.py:185][0m |          -0.0085 |         114.0382 |          17.5798 |
[32m[20221213 21:14:50 @agent_ppo2.py:185][0m |          -0.0118 |         114.1959 |          17.5858 |
[32m[20221213 21:14:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:14:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 629.00
[32m[20221213 21:14:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 670.00
[32m[20221213 21:14:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 672.00
[32m[20221213 21:14:50 @agent_ppo2.py:143][0m Total time:      19.26 min
[32m[20221213 21:14:50 @agent_ppo2.py:145][0m 1878016 total steps have happened
[32m[20221213 21:14:50 @agent_ppo2.py:121][0m #------------------------ Iteration 917 --------------------------#
[32m[20221213 21:14:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:51 @agent_ppo2.py:185][0m |          -0.0016 |         118.6039 |          17.7214 |
[32m[20221213 21:14:51 @agent_ppo2.py:185][0m |          -0.0051 |         116.6483 |          17.7283 |
[32m[20221213 21:14:51 @agent_ppo2.py:185][0m |          -0.0060 |         115.7464 |          17.7195 |
[32m[20221213 21:14:51 @agent_ppo2.py:185][0m |          -0.0011 |         118.9109 |          17.7100 |
[32m[20221213 21:14:51 @agent_ppo2.py:185][0m |          -0.0071 |         114.7739 |          17.7173 |
[32m[20221213 21:14:51 @agent_ppo2.py:185][0m |          -0.0063 |         114.4104 |          17.7030 |
[32m[20221213 21:14:51 @agent_ppo2.py:185][0m |          -0.0067 |         114.2421 |          17.7256 |
[32m[20221213 21:14:51 @agent_ppo2.py:185][0m |           0.0057 |         122.3359 |          17.7155 |
[32m[20221213 21:14:51 @agent_ppo2.py:185][0m |          -0.0101 |         114.0087 |          17.7184 |
[32m[20221213 21:14:51 @agent_ppo2.py:185][0m |           0.0048 |         129.1258 |          17.7240 |
[32m[20221213 21:14:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:14:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 598.40
[32m[20221213 21:14:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 687.00
[32m[20221213 21:14:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 771.00
[32m[20221213 21:14:52 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 771.00
[32m[20221213 21:14:52 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 771.00
[32m[20221213 21:14:52 @agent_ppo2.py:143][0m Total time:      19.28 min
[32m[20221213 21:14:52 @agent_ppo2.py:145][0m 1880064 total steps have happened
[32m[20221213 21:14:52 @agent_ppo2.py:121][0m #------------------------ Iteration 918 --------------------------#
[32m[20221213 21:14:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:52 @agent_ppo2.py:185][0m |          -0.0021 |         118.2624 |          17.5652 |
[32m[20221213 21:14:52 @agent_ppo2.py:185][0m |          -0.0075 |         116.6599 |          17.5755 |
[32m[20221213 21:14:52 @agent_ppo2.py:185][0m |          -0.0064 |         115.7113 |          17.5749 |
[32m[20221213 21:14:52 @agent_ppo2.py:185][0m |          -0.0084 |         115.0545 |          17.5738 |
[32m[20221213 21:14:52 @agent_ppo2.py:185][0m |          -0.0100 |         114.5468 |          17.5774 |
[32m[20221213 21:14:52 @agent_ppo2.py:185][0m |          -0.0076 |         114.3678 |          17.5727 |
[32m[20221213 21:14:52 @agent_ppo2.py:185][0m |          -0.0077 |         113.9117 |          17.5735 |
[32m[20221213 21:14:52 @agent_ppo2.py:185][0m |          -0.0097 |         113.6351 |          17.5840 |
[32m[20221213 21:14:53 @agent_ppo2.py:185][0m |          -0.0089 |         113.4955 |          17.5753 |
[32m[20221213 21:14:53 @agent_ppo2.py:185][0m |          -0.0084 |         113.3608 |          17.5784 |
[32m[20221213 21:14:53 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:14:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 654.00
[32m[20221213 21:14:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 725.00
[32m[20221213 21:14:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 669.00
[32m[20221213 21:14:53 @agent_ppo2.py:143][0m Total time:      19.30 min
[32m[20221213 21:14:53 @agent_ppo2.py:145][0m 1882112 total steps have happened
[32m[20221213 21:14:53 @agent_ppo2.py:121][0m #------------------------ Iteration 919 --------------------------#
[32m[20221213 21:14:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:53 @agent_ppo2.py:185][0m |          -0.0027 |         119.0549 |          17.5533 |
[32m[20221213 21:14:53 @agent_ppo2.py:185][0m |           0.0011 |         120.1752 |          17.5684 |
[32m[20221213 21:14:53 @agent_ppo2.py:185][0m |          -0.0042 |         115.5665 |          17.5869 |
[32m[20221213 21:14:53 @agent_ppo2.py:185][0m |          -0.0085 |         114.8103 |          17.5788 |
[32m[20221213 21:14:53 @agent_ppo2.py:185][0m |          -0.0080 |         114.2481 |          17.5836 |
[32m[20221213 21:14:53 @agent_ppo2.py:185][0m |          -0.0061 |         113.9555 |          17.6135 |
[32m[20221213 21:14:54 @agent_ppo2.py:185][0m |          -0.0036 |         114.6131 |          17.6068 |
[32m[20221213 21:14:54 @agent_ppo2.py:185][0m |          -0.0022 |         115.6457 |          17.6215 |
[32m[20221213 21:14:54 @agent_ppo2.py:185][0m |          -0.0092 |         113.4975 |          17.6373 |
[32m[20221213 21:14:54 @agent_ppo2.py:185][0m |          -0.0095 |         113.2329 |          17.6412 |
[32m[20221213 21:14:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:14:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 628.80
[32m[20221213 21:14:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 683.00
[32m[20221213 21:14:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 731.00
[32m[20221213 21:14:54 @agent_ppo2.py:143][0m Total time:      19.32 min
[32m[20221213 21:14:54 @agent_ppo2.py:145][0m 1884160 total steps have happened
[32m[20221213 21:14:54 @agent_ppo2.py:121][0m #------------------------ Iteration 920 --------------------------#
[32m[20221213 21:14:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:14:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:54 @agent_ppo2.py:185][0m |          -0.0008 |         119.7601 |          17.7635 |
[32m[20221213 21:14:54 @agent_ppo2.py:185][0m |          -0.0082 |         118.0881 |          17.7581 |
[32m[20221213 21:14:54 @agent_ppo2.py:185][0m |          -0.0082 |         116.7344 |          17.7501 |
[32m[20221213 21:14:55 @agent_ppo2.py:185][0m |          -0.0108 |         116.1173 |          17.7221 |
[32m[20221213 21:14:55 @agent_ppo2.py:185][0m |           0.0007 |         126.4018 |          17.7146 |
[32m[20221213 21:14:55 @agent_ppo2.py:185][0m |          -0.0091 |         115.5888 |          17.7218 |
[32m[20221213 21:14:55 @agent_ppo2.py:185][0m |          -0.0100 |         114.8124 |          17.7160 |
[32m[20221213 21:14:55 @agent_ppo2.py:185][0m |          -0.0115 |         114.5448 |          17.6977 |
[32m[20221213 21:14:55 @agent_ppo2.py:185][0m |          -0.0072 |         119.4222 |          17.7032 |
[32m[20221213 21:14:55 @agent_ppo2.py:185][0m |          -0.0018 |         119.0221 |          17.6799 |
[32m[20221213 21:14:55 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:14:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 615.20
[32m[20221213 21:14:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 664.00
[32m[20221213 21:14:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 642.00
[32m[20221213 21:14:55 @agent_ppo2.py:143][0m Total time:      19.34 min
[32m[20221213 21:14:55 @agent_ppo2.py:145][0m 1886208 total steps have happened
[32m[20221213 21:14:55 @agent_ppo2.py:121][0m #------------------------ Iteration 921 --------------------------#
[32m[20221213 21:14:55 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:14:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:55 @agent_ppo2.py:185][0m |           0.0181 |         130.8516 |          17.6073 |
[32m[20221213 21:14:56 @agent_ppo2.py:185][0m |          -0.0046 |         116.2571 |          17.5595 |
[32m[20221213 21:14:56 @agent_ppo2.py:185][0m |           0.0058 |         121.2249 |          17.5464 |
[32m[20221213 21:14:56 @agent_ppo2.py:185][0m |          -0.0099 |         115.6478 |          17.5390 |
[32m[20221213 21:14:56 @agent_ppo2.py:185][0m |          -0.0049 |         115.7685 |          17.5238 |
[32m[20221213 21:14:56 @agent_ppo2.py:185][0m |          -0.0092 |         114.9024 |          17.5058 |
[32m[20221213 21:14:56 @agent_ppo2.py:185][0m |          -0.0116 |         114.4742 |          17.4993 |
[32m[20221213 21:14:56 @agent_ppo2.py:185][0m |          -0.0118 |         114.2156 |          17.4975 |
[32m[20221213 21:14:56 @agent_ppo2.py:185][0m |          -0.0112 |         114.1037 |          17.4900 |
[32m[20221213 21:14:56 @agent_ppo2.py:185][0m |          -0.0112 |         113.8991 |          17.4784 |
[32m[20221213 21:14:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:14:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 621.20
[32m[20221213 21:14:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 696.00
[32m[20221213 21:14:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 653.00
[32m[20221213 21:14:56 @agent_ppo2.py:143][0m Total time:      19.36 min
[32m[20221213 21:14:56 @agent_ppo2.py:145][0m 1888256 total steps have happened
[32m[20221213 21:14:56 @agent_ppo2.py:121][0m #------------------------ Iteration 922 --------------------------#
[32m[20221213 21:14:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:57 @agent_ppo2.py:185][0m |           0.0015 |         117.0279 |          17.6090 |
[32m[20221213 21:14:57 @agent_ppo2.py:185][0m |          -0.0041 |         115.5966 |          17.6024 |
[32m[20221213 21:14:57 @agent_ppo2.py:185][0m |          -0.0046 |         114.8966 |          17.6074 |
[32m[20221213 21:14:57 @agent_ppo2.py:185][0m |           0.0025 |         124.8803 |          17.5906 |
[32m[20221213 21:14:57 @agent_ppo2.py:185][0m |          -0.0020 |         114.8484 |          17.5833 |
[32m[20221213 21:14:57 @agent_ppo2.py:185][0m |          -0.0078 |         113.8364 |          17.5779 |
[32m[20221213 21:14:57 @agent_ppo2.py:185][0m |           0.0052 |         127.5284 |          17.5752 |
[32m[20221213 21:14:57 @agent_ppo2.py:185][0m |          -0.0061 |         113.6952 |          17.5713 |
[32m[20221213 21:14:57 @agent_ppo2.py:185][0m |          -0.0069 |         113.4841 |          17.5514 |
[32m[20221213 21:14:57 @agent_ppo2.py:185][0m |          -0.0077 |         113.2245 |          17.5621 |
[32m[20221213 21:14:57 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:14:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 613.60
[32m[20221213 21:14:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 677.00
[32m[20221213 21:14:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 654.00
[32m[20221213 21:14:58 @agent_ppo2.py:143][0m Total time:      19.38 min
[32m[20221213 21:14:58 @agent_ppo2.py:145][0m 1890304 total steps have happened
[32m[20221213 21:14:58 @agent_ppo2.py:121][0m #------------------------ Iteration 923 --------------------------#
[32m[20221213 21:14:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:14:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:58 @agent_ppo2.py:185][0m |           0.0114 |         128.4124 |          17.6712 |
[32m[20221213 21:14:58 @agent_ppo2.py:185][0m |          -0.0062 |         112.9314 |          17.6279 |
[32m[20221213 21:14:58 @agent_ppo2.py:185][0m |          -0.0064 |         112.4117 |          17.6269 |
[32m[20221213 21:14:58 @agent_ppo2.py:185][0m |          -0.0078 |         112.3239 |          17.6145 |
[32m[20221213 21:14:58 @agent_ppo2.py:185][0m |          -0.0020 |         113.8887 |          17.6027 |
[32m[20221213 21:14:58 @agent_ppo2.py:185][0m |          -0.0078 |         111.7252 |          17.6002 |
[32m[20221213 21:14:58 @agent_ppo2.py:185][0m |          -0.0087 |         111.4554 |          17.5890 |
[32m[20221213 21:14:59 @agent_ppo2.py:185][0m |          -0.0113 |         111.1586 |          17.5743 |
[32m[20221213 21:14:59 @agent_ppo2.py:185][0m |          -0.0095 |         111.2520 |          17.5690 |
[32m[20221213 21:14:59 @agent_ppo2.py:185][0m |          -0.0072 |         110.7979 |          17.5666 |
[32m[20221213 21:14:59 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:14:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 587.00
[32m[20221213 21:14:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 677.00
[32m[20221213 21:14:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 643.00
[32m[20221213 21:14:59 @agent_ppo2.py:143][0m Total time:      19.40 min
[32m[20221213 21:14:59 @agent_ppo2.py:145][0m 1892352 total steps have happened
[32m[20221213 21:14:59 @agent_ppo2.py:121][0m #------------------------ Iteration 924 --------------------------#
[32m[20221213 21:14:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:14:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:14:59 @agent_ppo2.py:185][0m |          -0.0002 |         116.7812 |          17.4932 |
[32m[20221213 21:14:59 @agent_ppo2.py:185][0m |          -0.0017 |         114.8629 |          17.4606 |
[32m[20221213 21:14:59 @agent_ppo2.py:185][0m |           0.0028 |         120.3763 |          17.4612 |
[32m[20221213 21:14:59 @agent_ppo2.py:185][0m |          -0.0078 |         113.6406 |          17.4554 |
[32m[20221213 21:14:59 @agent_ppo2.py:185][0m |          -0.0075 |         112.9502 |          17.4552 |
[32m[20221213 21:15:00 @agent_ppo2.py:185][0m |          -0.0073 |         112.6470 |          17.4580 |
[32m[20221213 21:15:00 @agent_ppo2.py:185][0m |          -0.0074 |         112.3177 |          17.4670 |
[32m[20221213 21:15:00 @agent_ppo2.py:185][0m |          -0.0084 |         112.4540 |          17.4483 |
[32m[20221213 21:15:00 @agent_ppo2.py:185][0m |          -0.0081 |         112.1356 |          17.4561 |
[32m[20221213 21:15:00 @agent_ppo2.py:185][0m |          -0.0078 |         111.6027 |          17.4602 |
[32m[20221213 21:15:00 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:15:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 619.60
[32m[20221213 21:15:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 695.00
[32m[20221213 21:15:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 555.00
[32m[20221213 21:15:00 @agent_ppo2.py:143][0m Total time:      19.42 min
[32m[20221213 21:15:00 @agent_ppo2.py:145][0m 1894400 total steps have happened
[32m[20221213 21:15:00 @agent_ppo2.py:121][0m #------------------------ Iteration 925 --------------------------#
[32m[20221213 21:15:00 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:15:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:00 @agent_ppo2.py:185][0m |          -0.0009 |         120.8666 |          17.4911 |
[32m[20221213 21:15:00 @agent_ppo2.py:185][0m |          -0.0066 |         117.3193 |          17.4789 |
[32m[20221213 21:15:01 @agent_ppo2.py:185][0m |          -0.0029 |         117.2086 |          17.4916 |
[32m[20221213 21:15:01 @agent_ppo2.py:185][0m |          -0.0094 |         115.8459 |          17.4847 |
[32m[20221213 21:15:01 @agent_ppo2.py:185][0m |          -0.0048 |         115.2883 |          17.4744 |
[32m[20221213 21:15:01 @agent_ppo2.py:185][0m |          -0.0080 |         115.1656 |          17.4905 |
[32m[20221213 21:15:01 @agent_ppo2.py:185][0m |          -0.0059 |         114.7209 |          17.4925 |
[32m[20221213 21:15:01 @agent_ppo2.py:185][0m |          -0.0003 |         119.1049 |          17.4963 |
[32m[20221213 21:15:01 @agent_ppo2.py:185][0m |          -0.0090 |         114.4480 |          17.5069 |
[32m[20221213 21:15:01 @agent_ppo2.py:185][0m |          -0.0095 |         114.9621 |          17.5215 |
[32m[20221213 21:15:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:15:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.20
[32m[20221213 21:15:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 714.00
[32m[20221213 21:15:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.00
[32m[20221213 21:15:01 @agent_ppo2.py:143][0m Total time:      19.44 min
[32m[20221213 21:15:01 @agent_ppo2.py:145][0m 1896448 total steps have happened
[32m[20221213 21:15:01 @agent_ppo2.py:121][0m #------------------------ Iteration 926 --------------------------#
[32m[20221213 21:15:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:02 @agent_ppo2.py:185][0m |           0.0001 |         113.7906 |          17.3731 |
[32m[20221213 21:15:02 @agent_ppo2.py:185][0m |          -0.0022 |         111.3013 |          17.3470 |
[32m[20221213 21:15:02 @agent_ppo2.py:185][0m |           0.0054 |         113.4554 |          17.3443 |
[32m[20221213 21:15:02 @agent_ppo2.py:185][0m |          -0.0023 |         109.6504 |          17.3462 |
[32m[20221213 21:15:02 @agent_ppo2.py:185][0m |          -0.0066 |         109.1368 |          17.3520 |
[32m[20221213 21:15:02 @agent_ppo2.py:185][0m |          -0.0073 |         108.7302 |          17.3434 |
[32m[20221213 21:15:02 @agent_ppo2.py:185][0m |          -0.0104 |         108.5837 |          17.3338 |
[32m[20221213 21:15:02 @agent_ppo2.py:185][0m |          -0.0065 |         108.1962 |          17.3341 |
[32m[20221213 21:15:02 @agent_ppo2.py:185][0m |          -0.0088 |         107.8714 |          17.3396 |
[32m[20221213 21:15:02 @agent_ppo2.py:185][0m |          -0.0070 |         107.6578 |          17.3382 |
[32m[20221213 21:15:02 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:15:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 653.80
[32m[20221213 21:15:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 703.00
[32m[20221213 21:15:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 696.00
[32m[20221213 21:15:03 @agent_ppo2.py:143][0m Total time:      19.46 min
[32m[20221213 21:15:03 @agent_ppo2.py:145][0m 1898496 total steps have happened
[32m[20221213 21:15:03 @agent_ppo2.py:121][0m #------------------------ Iteration 927 --------------------------#
[32m[20221213 21:15:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:03 @agent_ppo2.py:185][0m |           0.0004 |         113.6461 |          17.6358 |
[32m[20221213 21:15:03 @agent_ppo2.py:185][0m |          -0.0000 |         113.2403 |          17.6174 |
[32m[20221213 21:15:03 @agent_ppo2.py:185][0m |           0.0025 |         114.0827 |          17.6167 |
[32m[20221213 21:15:03 @agent_ppo2.py:185][0m |          -0.0066 |         111.8849 |          17.6195 |
[32m[20221213 21:15:03 @agent_ppo2.py:185][0m |          -0.0038 |         111.5070 |          17.6042 |
[32m[20221213 21:15:03 @agent_ppo2.py:185][0m |          -0.0039 |         111.4214 |          17.6281 |
[32m[20221213 21:15:03 @agent_ppo2.py:185][0m |          -0.0062 |         110.9152 |          17.6004 |
[32m[20221213 21:15:03 @agent_ppo2.py:185][0m |           0.0016 |         112.5545 |          17.6117 |
[32m[20221213 21:15:04 @agent_ppo2.py:185][0m |          -0.0056 |         110.5821 |          17.5913 |
[32m[20221213 21:15:04 @agent_ppo2.py:185][0m |           0.0016 |         115.3126 |          17.5962 |
[32m[20221213 21:15:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:15:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 651.20
[32m[20221213 21:15:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 703.00
[32m[20221213 21:15:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 711.00
[32m[20221213 21:15:04 @agent_ppo2.py:143][0m Total time:      19.48 min
[32m[20221213 21:15:04 @agent_ppo2.py:145][0m 1900544 total steps have happened
[32m[20221213 21:15:04 @agent_ppo2.py:121][0m #------------------------ Iteration 928 --------------------------#
[32m[20221213 21:15:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:04 @agent_ppo2.py:185][0m |           0.0006 |         121.7971 |          17.4437 |
[32m[20221213 21:15:04 @agent_ppo2.py:185][0m |          -0.0011 |         118.9896 |          17.4472 |
[32m[20221213 21:15:04 @agent_ppo2.py:185][0m |          -0.0001 |         119.1462 |          17.4562 |
[32m[20221213 21:15:04 @agent_ppo2.py:185][0m |          -0.0050 |         117.0017 |          17.4501 |
[32m[20221213 21:15:04 @agent_ppo2.py:185][0m |          -0.0045 |         116.3651 |          17.4641 |
[32m[20221213 21:15:05 @agent_ppo2.py:185][0m |          -0.0052 |         116.0585 |          17.4699 |
[32m[20221213 21:15:05 @agent_ppo2.py:185][0m |          -0.0031 |         116.5276 |          17.4769 |
[32m[20221213 21:15:05 @agent_ppo2.py:185][0m |          -0.0099 |         115.2143 |          17.4972 |
[32m[20221213 21:15:05 @agent_ppo2.py:185][0m |           0.0003 |         115.6105 |          17.5126 |
[32m[20221213 21:15:05 @agent_ppo2.py:185][0m |          -0.0115 |         114.7983 |          17.5203 |
[32m[20221213 21:15:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:15:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 608.00
[32m[20221213 21:15:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 662.00
[32m[20221213 21:15:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.00
[32m[20221213 21:15:05 @agent_ppo2.py:143][0m Total time:      19.51 min
[32m[20221213 21:15:05 @agent_ppo2.py:145][0m 1902592 total steps have happened
[32m[20221213 21:15:05 @agent_ppo2.py:121][0m #------------------------ Iteration 929 --------------------------#
[32m[20221213 21:15:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:05 @agent_ppo2.py:185][0m |           0.0026 |         124.9483 |          17.5560 |
[32m[20221213 21:15:05 @agent_ppo2.py:185][0m |          -0.0053 |         121.1140 |          17.5427 |
[32m[20221213 21:15:05 @agent_ppo2.py:185][0m |          -0.0049 |         120.1923 |          17.5531 |
[32m[20221213 21:15:06 @agent_ppo2.py:185][0m |          -0.0063 |         120.2083 |          17.5465 |
[32m[20221213 21:15:06 @agent_ppo2.py:185][0m |          -0.0041 |         119.9648 |          17.5775 |
[32m[20221213 21:15:06 @agent_ppo2.py:185][0m |          -0.0074 |         119.9238 |          17.5693 |
[32m[20221213 21:15:06 @agent_ppo2.py:185][0m |          -0.0069 |         119.4446 |          17.5722 |
[32m[20221213 21:15:06 @agent_ppo2.py:185][0m |          -0.0087 |         119.1868 |          17.5785 |
[32m[20221213 21:15:06 @agent_ppo2.py:185][0m |          -0.0048 |         119.4420 |          17.5860 |
[32m[20221213 21:15:06 @agent_ppo2.py:185][0m |          -0.0090 |         118.9312 |          17.5750 |
[32m[20221213 21:15:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:15:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 621.20
[32m[20221213 21:15:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 666.00
[32m[20221213 21:15:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 607.00
[32m[20221213 21:15:06 @agent_ppo2.py:143][0m Total time:      19.53 min
[32m[20221213 21:15:06 @agent_ppo2.py:145][0m 1904640 total steps have happened
[32m[20221213 21:15:06 @agent_ppo2.py:121][0m #------------------------ Iteration 930 --------------------------#
[32m[20221213 21:15:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:07 @agent_ppo2.py:185][0m |           0.0056 |         126.6311 |          17.6163 |
[32m[20221213 21:15:07 @agent_ppo2.py:185][0m |          -0.0036 |         116.7118 |          17.6360 |
[32m[20221213 21:15:07 @agent_ppo2.py:185][0m |          -0.0064 |         116.0324 |          17.6382 |
[32m[20221213 21:15:07 @agent_ppo2.py:185][0m |          -0.0114 |         115.7323 |          17.6275 |
[32m[20221213 21:15:07 @agent_ppo2.py:185][0m |          -0.0101 |         115.2064 |          17.6197 |
[32m[20221213 21:15:07 @agent_ppo2.py:185][0m |          -0.0094 |         114.9825 |          17.6335 |
[32m[20221213 21:15:07 @agent_ppo2.py:185][0m |          -0.0103 |         114.6016 |          17.6406 |
[32m[20221213 21:15:07 @agent_ppo2.py:185][0m |          -0.0097 |         114.3684 |          17.6269 |
[32m[20221213 21:15:07 @agent_ppo2.py:185][0m |          -0.0124 |         114.4368 |          17.6217 |
[32m[20221213 21:15:07 @agent_ppo2.py:185][0m |          -0.0126 |         113.9768 |          17.6276 |
[32m[20221213 21:15:07 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:15:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 644.00
[32m[20221213 21:15:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 687.00
[32m[20221213 21:15:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 677.00
[32m[20221213 21:15:07 @agent_ppo2.py:143][0m Total time:      19.55 min
[32m[20221213 21:15:07 @agent_ppo2.py:145][0m 1906688 total steps have happened
[32m[20221213 21:15:07 @agent_ppo2.py:121][0m #------------------------ Iteration 931 --------------------------#
[32m[20221213 21:15:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:08 @agent_ppo2.py:185][0m |           0.0042 |         115.4748 |          17.3597 |
[32m[20221213 21:15:08 @agent_ppo2.py:185][0m |          -0.0067 |         113.4938 |          17.3691 |
[32m[20221213 21:15:08 @agent_ppo2.py:185][0m |          -0.0043 |         112.9307 |          17.3714 |
[32m[20221213 21:15:08 @agent_ppo2.py:185][0m |          -0.0055 |         112.5648 |          17.3784 |
[32m[20221213 21:15:08 @agent_ppo2.py:185][0m |          -0.0086 |         112.4150 |          17.3906 |
[32m[20221213 21:15:08 @agent_ppo2.py:185][0m |          -0.0079 |         112.3729 |          17.3968 |
[32m[20221213 21:15:08 @agent_ppo2.py:185][0m |          -0.0076 |         112.1971 |          17.3959 |
[32m[20221213 21:15:08 @agent_ppo2.py:185][0m |          -0.0063 |         111.8149 |          17.4074 |
[32m[20221213 21:15:08 @agent_ppo2.py:185][0m |          -0.0066 |         111.7973 |          17.4153 |
[32m[20221213 21:15:09 @agent_ppo2.py:185][0m |          -0.0005 |         114.0536 |          17.4178 |
[32m[20221213 21:15:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:15:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 624.80
[32m[20221213 21:15:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 653.00
[32m[20221213 21:15:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 684.00
[32m[20221213 21:15:09 @agent_ppo2.py:143][0m Total time:      19.57 min
[32m[20221213 21:15:09 @agent_ppo2.py:145][0m 1908736 total steps have happened
[32m[20221213 21:15:09 @agent_ppo2.py:121][0m #------------------------ Iteration 932 --------------------------#
[32m[20221213 21:15:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:09 @agent_ppo2.py:185][0m |          -0.0028 |         117.1315 |          17.4595 |
[32m[20221213 21:15:09 @agent_ppo2.py:185][0m |           0.0028 |         115.1132 |          17.4545 |
[32m[20221213 21:15:09 @agent_ppo2.py:185][0m |          -0.0011 |         113.2000 |          17.4387 |
[32m[20221213 21:15:09 @agent_ppo2.py:185][0m |          -0.0070 |         111.4477 |          17.4295 |
[32m[20221213 21:15:09 @agent_ppo2.py:185][0m |          -0.0053 |         110.7063 |          17.4249 |
[32m[20221213 21:15:09 @agent_ppo2.py:185][0m |          -0.0082 |         110.0033 |          17.4305 |
[32m[20221213 21:15:09 @agent_ppo2.py:185][0m |          -0.0106 |         109.3334 |          17.4143 |
[32m[20221213 21:15:10 @agent_ppo2.py:185][0m |          -0.0006 |         110.4431 |          17.4099 |
[32m[20221213 21:15:10 @agent_ppo2.py:185][0m |           0.0000 |         116.0448 |          17.4219 |
[32m[20221213 21:15:10 @agent_ppo2.py:185][0m |           0.0010 |         118.6225 |          17.4092 |
[32m[20221213 21:15:10 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:15:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.80
[32m[20221213 21:15:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 659.00
[32m[20221213 21:15:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 578.00
[32m[20221213 21:15:10 @agent_ppo2.py:143][0m Total time:      19.59 min
[32m[20221213 21:15:10 @agent_ppo2.py:145][0m 1910784 total steps have happened
[32m[20221213 21:15:10 @agent_ppo2.py:121][0m #------------------------ Iteration 933 --------------------------#
[32m[20221213 21:15:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:15:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:10 @agent_ppo2.py:185][0m |          -0.0002 |         122.2000 |          17.5627 |
[32m[20221213 21:15:10 @agent_ppo2.py:185][0m |          -0.0077 |         119.7643 |          17.5559 |
[32m[20221213 21:15:10 @agent_ppo2.py:185][0m |           0.0018 |         131.9473 |          17.5497 |
[32m[20221213 21:15:11 @agent_ppo2.py:185][0m |          -0.0065 |         118.3661 |          17.5464 |
[32m[20221213 21:15:11 @agent_ppo2.py:185][0m |          -0.0032 |         119.0874 |          17.5371 |
[32m[20221213 21:15:11 @agent_ppo2.py:185][0m |          -0.0119 |         116.6688 |          17.5342 |
[32m[20221213 21:15:11 @agent_ppo2.py:185][0m |          -0.0036 |         117.8423 |          17.5167 |
[32m[20221213 21:15:11 @agent_ppo2.py:185][0m |          -0.0103 |         115.8155 |          17.5376 |
[32m[20221213 21:15:11 @agent_ppo2.py:185][0m |          -0.0057 |         115.2295 |          17.5258 |
[32m[20221213 21:15:11 @agent_ppo2.py:185][0m |          -0.0115 |         114.9665 |          17.5254 |
[32m[20221213 21:15:11 @agent_ppo2.py:130][0m Policy update time: 1.35 s
[32m[20221213 21:15:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 610.20
[32m[20221213 21:15:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 651.00
[32m[20221213 21:15:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 663.00
[32m[20221213 21:15:12 @agent_ppo2.py:143][0m Total time:      19.61 min
[32m[20221213 21:15:12 @agent_ppo2.py:145][0m 1912832 total steps have happened
[32m[20221213 21:15:12 @agent_ppo2.py:121][0m #------------------------ Iteration 934 --------------------------#
[32m[20221213 21:15:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:12 @agent_ppo2.py:185][0m |          -0.0033 |         121.7936 |          17.4448 |
[32m[20221213 21:15:12 @agent_ppo2.py:185][0m |          -0.0074 |         119.9690 |          17.4650 |
[32m[20221213 21:15:12 @agent_ppo2.py:185][0m |          -0.0070 |         118.8996 |          17.4478 |
[32m[20221213 21:15:12 @agent_ppo2.py:185][0m |          -0.0045 |         118.5932 |          17.4616 |
[32m[20221213 21:15:12 @agent_ppo2.py:185][0m |          -0.0062 |         119.1966 |          17.4663 |
[32m[20221213 21:15:12 @agent_ppo2.py:185][0m |          -0.0071 |         117.8676 |          17.4521 |
[32m[20221213 21:15:12 @agent_ppo2.py:185][0m |          -0.0084 |         118.1135 |          17.4676 |
[32m[20221213 21:15:12 @agent_ppo2.py:185][0m |          -0.0079 |         117.4277 |          17.4742 |
[32m[20221213 21:15:13 @agent_ppo2.py:185][0m |          -0.0114 |         117.4068 |          17.4693 |
[32m[20221213 21:15:13 @agent_ppo2.py:185][0m |          -0.0060 |         118.3730 |          17.4785 |
[32m[20221213 21:15:13 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:15:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 661.40
[32m[20221213 21:15:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 690.00
[32m[20221213 21:15:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 612.00
[32m[20221213 21:15:13 @agent_ppo2.py:143][0m Total time:      19.63 min
[32m[20221213 21:15:13 @agent_ppo2.py:145][0m 1914880 total steps have happened
[32m[20221213 21:15:13 @agent_ppo2.py:121][0m #------------------------ Iteration 935 --------------------------#
[32m[20221213 21:15:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:13 @agent_ppo2.py:185][0m |          -0.0002 |         122.3486 |          17.5952 |
[32m[20221213 21:15:13 @agent_ppo2.py:185][0m |           0.0023 |         120.3580 |          17.5902 |
[32m[20221213 21:15:13 @agent_ppo2.py:185][0m |          -0.0011 |         119.6655 |          17.5855 |
[32m[20221213 21:15:13 @agent_ppo2.py:185][0m |           0.0074 |         127.5956 |          17.6008 |
[32m[20221213 21:15:13 @agent_ppo2.py:185][0m |          -0.0066 |         118.5792 |          17.5883 |
[32m[20221213 21:15:14 @agent_ppo2.py:185][0m |          -0.0067 |         118.4050 |          17.6036 |
[32m[20221213 21:15:14 @agent_ppo2.py:185][0m |          -0.0083 |         118.0693 |          17.5915 |
[32m[20221213 21:15:14 @agent_ppo2.py:185][0m |          -0.0036 |         120.3443 |          17.5938 |
[32m[20221213 21:15:14 @agent_ppo2.py:185][0m |          -0.0091 |         117.9411 |          17.5782 |
[32m[20221213 21:15:14 @agent_ppo2.py:185][0m |           0.0024 |         125.8684 |          17.5804 |
[32m[20221213 21:15:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:15:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 607.60
[32m[20221213 21:15:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 662.00
[32m[20221213 21:15:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 667.00
[32m[20221213 21:15:14 @agent_ppo2.py:143][0m Total time:      19.66 min
[32m[20221213 21:15:14 @agent_ppo2.py:145][0m 1916928 total steps have happened
[32m[20221213 21:15:14 @agent_ppo2.py:121][0m #------------------------ Iteration 936 --------------------------#
[32m[20221213 21:15:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:15:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:14 @agent_ppo2.py:185][0m |          -0.0031 |         120.9498 |          17.7006 |
[32m[20221213 21:15:14 @agent_ppo2.py:185][0m |          -0.0006 |         118.0527 |          17.7280 |
[32m[20221213 21:15:14 @agent_ppo2.py:185][0m |          -0.0057 |         116.8923 |          17.7389 |
[32m[20221213 21:15:15 @agent_ppo2.py:185][0m |           0.0080 |         129.2971 |          17.7412 |
[32m[20221213 21:15:15 @agent_ppo2.py:185][0m |          -0.0045 |         115.9599 |          17.7689 |
[32m[20221213 21:15:15 @agent_ppo2.py:185][0m |          -0.0036 |         115.5631 |          17.7668 |
[32m[20221213 21:15:15 @agent_ppo2.py:185][0m |          -0.0092 |         115.4519 |          17.7728 |
[32m[20221213 21:15:15 @agent_ppo2.py:185][0m |          -0.0082 |         115.0606 |          17.7921 |
[32m[20221213 21:15:15 @agent_ppo2.py:185][0m |          -0.0112 |         114.9492 |          17.7978 |
[32m[20221213 21:15:15 @agent_ppo2.py:185][0m |          -0.0079 |         114.7828 |          17.8051 |
[32m[20221213 21:15:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:15:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 641.00
[32m[20221213 21:15:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 687.00
[32m[20221213 21:15:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 763.00
[32m[20221213 21:15:15 @agent_ppo2.py:143][0m Total time:      19.68 min
[32m[20221213 21:15:15 @agent_ppo2.py:145][0m 1918976 total steps have happened
[32m[20221213 21:15:15 @agent_ppo2.py:121][0m #------------------------ Iteration 937 --------------------------#
[32m[20221213 21:15:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:16 @agent_ppo2.py:185][0m |          -0.0049 |         120.5829 |          17.7125 |
[32m[20221213 21:15:16 @agent_ppo2.py:185][0m |          -0.0033 |         119.4143 |          17.7022 |
[32m[20221213 21:15:16 @agent_ppo2.py:185][0m |          -0.0071 |         119.0402 |          17.6889 |
[32m[20221213 21:15:16 @agent_ppo2.py:185][0m |          -0.0087 |         118.7810 |          17.6903 |
[32m[20221213 21:15:16 @agent_ppo2.py:185][0m |           0.0024 |         126.1344 |          17.6809 |
[32m[20221213 21:15:16 @agent_ppo2.py:185][0m |          -0.0018 |         119.4491 |          17.6628 |
[32m[20221213 21:15:16 @agent_ppo2.py:185][0m |          -0.0054 |         117.9844 |          17.6743 |
[32m[20221213 21:15:16 @agent_ppo2.py:185][0m |          -0.0092 |         117.6505 |          17.6703 |
[32m[20221213 21:15:16 @agent_ppo2.py:185][0m |          -0.0098 |         117.8652 |          17.6678 |
[32m[20221213 21:15:16 @agent_ppo2.py:185][0m |          -0.0069 |         117.5601 |          17.6637 |
[32m[20221213 21:15:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:15:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 652.60
[32m[20221213 21:15:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 687.00
[32m[20221213 21:15:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 658.00
[32m[20221213 21:15:16 @agent_ppo2.py:143][0m Total time:      19.70 min
[32m[20221213 21:15:16 @agent_ppo2.py:145][0m 1921024 total steps have happened
[32m[20221213 21:15:16 @agent_ppo2.py:121][0m #------------------------ Iteration 938 --------------------------#
[32m[20221213 21:15:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:17 @agent_ppo2.py:185][0m |          -0.0040 |         120.9696 |          17.6851 |
[32m[20221213 21:15:17 @agent_ppo2.py:185][0m |          -0.0060 |         119.5484 |          17.6659 |
[32m[20221213 21:15:17 @agent_ppo2.py:185][0m |          -0.0061 |         118.8022 |          17.6761 |
[32m[20221213 21:15:17 @agent_ppo2.py:185][0m |          -0.0037 |         118.8284 |          17.6749 |
[32m[20221213 21:15:17 @agent_ppo2.py:185][0m |          -0.0067 |         118.1932 |          17.6609 |
[32m[20221213 21:15:17 @agent_ppo2.py:185][0m |          -0.0083 |         118.1092 |          17.6678 |
[32m[20221213 21:15:17 @agent_ppo2.py:185][0m |          -0.0080 |         117.7018 |          17.6697 |
[32m[20221213 21:15:17 @agent_ppo2.py:185][0m |          -0.0061 |         117.3861 |          17.6609 |
[32m[20221213 21:15:17 @agent_ppo2.py:185][0m |          -0.0082 |         117.2408 |          17.6665 |
[32m[20221213 21:15:18 @agent_ppo2.py:185][0m |           0.0032 |         131.1175 |          17.6673 |
[32m[20221213 21:15:18 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:15:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 614.00
[32m[20221213 21:15:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 716.00
[32m[20221213 21:15:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.00
[32m[20221213 21:15:18 @agent_ppo2.py:143][0m Total time:      19.72 min
[32m[20221213 21:15:18 @agent_ppo2.py:145][0m 1923072 total steps have happened
[32m[20221213 21:15:18 @agent_ppo2.py:121][0m #------------------------ Iteration 939 --------------------------#
[32m[20221213 21:15:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:18 @agent_ppo2.py:185][0m |          -0.0009 |         119.6659 |          17.6372 |
[32m[20221213 21:15:18 @agent_ppo2.py:185][0m |          -0.0043 |         118.7796 |          17.6155 |
[32m[20221213 21:15:18 @agent_ppo2.py:185][0m |          -0.0002 |         119.2608 |          17.6078 |
[32m[20221213 21:15:18 @agent_ppo2.py:185][0m |           0.0049 |         123.1752 |          17.6104 |
[32m[20221213 21:15:18 @agent_ppo2.py:185][0m |          -0.0050 |         117.7249 |          17.5946 |
[32m[20221213 21:15:18 @agent_ppo2.py:185][0m |          -0.0061 |         117.4232 |          17.5846 |
[32m[20221213 21:15:19 @agent_ppo2.py:185][0m |          -0.0054 |         117.1122 |          17.5798 |
[32m[20221213 21:15:19 @agent_ppo2.py:185][0m |          -0.0081 |         117.0030 |          17.5709 |
[32m[20221213 21:15:19 @agent_ppo2.py:185][0m |          -0.0084 |         116.9314 |          17.5679 |
[32m[20221213 21:15:19 @agent_ppo2.py:185][0m |          -0.0063 |         116.6461 |          17.5514 |
[32m[20221213 21:15:19 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:15:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 652.80
[32m[20221213 21:15:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.00
[32m[20221213 21:15:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:15:19 @agent_ppo2.py:143][0m Total time:      19.74 min
[32m[20221213 21:15:19 @agent_ppo2.py:145][0m 1925120 total steps have happened
[32m[20221213 21:15:19 @agent_ppo2.py:121][0m #------------------------ Iteration 940 --------------------------#
[32m[20221213 21:15:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:19 @agent_ppo2.py:185][0m |          -0.0041 |         119.3954 |          17.6440 |
[32m[20221213 21:15:19 @agent_ppo2.py:185][0m |           0.0083 |         134.7260 |          17.6336 |
[32m[20221213 21:15:19 @agent_ppo2.py:185][0m |          -0.0051 |         117.8969 |          17.6567 |
[32m[20221213 21:15:20 @agent_ppo2.py:185][0m |          -0.0052 |         117.5272 |          17.6613 |
[32m[20221213 21:15:20 @agent_ppo2.py:185][0m |          -0.0052 |         116.9800 |          17.6714 |
[32m[20221213 21:15:20 @agent_ppo2.py:185][0m |          -0.0082 |         116.7896 |          17.6815 |
[32m[20221213 21:15:20 @agent_ppo2.py:185][0m |          -0.0110 |         116.6804 |          17.6880 |
[32m[20221213 21:15:20 @agent_ppo2.py:185][0m |          -0.0063 |         116.2943 |          17.6961 |
[32m[20221213 21:15:20 @agent_ppo2.py:185][0m |          -0.0103 |         116.2132 |          17.7017 |
[32m[20221213 21:15:20 @agent_ppo2.py:185][0m |          -0.0074 |         115.9236 |          17.7046 |
[32m[20221213 21:15:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:15:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 634.80
[32m[20221213 21:15:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 649.00
[32m[20221213 21:15:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 657.00
[32m[20221213 21:15:20 @agent_ppo2.py:143][0m Total time:      19.76 min
[32m[20221213 21:15:20 @agent_ppo2.py:145][0m 1927168 total steps have happened
[32m[20221213 21:15:20 @agent_ppo2.py:121][0m #------------------------ Iteration 941 --------------------------#
[32m[20221213 21:15:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:20 @agent_ppo2.py:185][0m |           0.0085 |         123.3183 |          17.6400 |
[32m[20221213 21:15:21 @agent_ppo2.py:185][0m |          -0.0044 |         115.5835 |          17.6218 |
[32m[20221213 21:15:21 @agent_ppo2.py:185][0m |           0.0167 |         132.4404 |          17.6276 |
[32m[20221213 21:15:21 @agent_ppo2.py:185][0m |          -0.0061 |         114.0491 |          17.6058 |
[32m[20221213 21:15:21 @agent_ppo2.py:185][0m |          -0.0068 |         113.3061 |          17.6061 |
[32m[20221213 21:15:21 @agent_ppo2.py:185][0m |          -0.0078 |         113.2749 |          17.5903 |
[32m[20221213 21:15:21 @agent_ppo2.py:185][0m |          -0.0071 |         112.6249 |          17.5991 |
[32m[20221213 21:15:21 @agent_ppo2.py:185][0m |          -0.0051 |         112.3370 |          17.5716 |
[32m[20221213 21:15:21 @agent_ppo2.py:185][0m |          -0.0087 |         111.7670 |          17.5659 |
[32m[20221213 21:15:21 @agent_ppo2.py:185][0m |          -0.0100 |         111.4175 |          17.5722 |
[32m[20221213 21:15:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:15:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 619.80
[32m[20221213 21:15:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 687.00
[32m[20221213 21:15:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:15:21 @agent_ppo2.py:143][0m Total time:      19.78 min
[32m[20221213 21:15:21 @agent_ppo2.py:145][0m 1929216 total steps have happened
[32m[20221213 21:15:21 @agent_ppo2.py:121][0m #------------------------ Iteration 942 --------------------------#
[32m[20221213 21:15:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:22 @agent_ppo2.py:185][0m |          -0.0037 |         120.8547 |          17.6129 |
[32m[20221213 21:15:22 @agent_ppo2.py:185][0m |          -0.0075 |         119.4729 |          17.6122 |
[32m[20221213 21:15:22 @agent_ppo2.py:185][0m |          -0.0059 |         119.2548 |          17.6213 |
[32m[20221213 21:15:22 @agent_ppo2.py:185][0m |          -0.0041 |         120.1473 |          17.5863 |
[32m[20221213 21:15:22 @agent_ppo2.py:185][0m |          -0.0094 |         118.0073 |          17.6035 |
[32m[20221213 21:15:22 @agent_ppo2.py:185][0m |           0.0032 |         129.8914 |          17.6079 |
[32m[20221213 21:15:22 @agent_ppo2.py:185][0m |          -0.0092 |         117.4777 |          17.5990 |
[32m[20221213 21:15:22 @agent_ppo2.py:185][0m |          -0.0079 |         117.2599 |          17.5915 |
[32m[20221213 21:15:22 @agent_ppo2.py:185][0m |          -0.0103 |         117.2268 |          17.6001 |
[32m[20221213 21:15:22 @agent_ppo2.py:185][0m |          -0.0057 |         117.3342 |          17.5953 |
[32m[20221213 21:15:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:15:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 645.80
[32m[20221213 21:15:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 665.00
[32m[20221213 21:15:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 708.00
[32m[20221213 21:15:23 @agent_ppo2.py:143][0m Total time:      19.80 min
[32m[20221213 21:15:23 @agent_ppo2.py:145][0m 1931264 total steps have happened
[32m[20221213 21:15:23 @agent_ppo2.py:121][0m #------------------------ Iteration 943 --------------------------#
[32m[20221213 21:15:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:23 @agent_ppo2.py:185][0m |          -0.0007 |         120.4209 |          17.5211 |
[32m[20221213 21:15:23 @agent_ppo2.py:185][0m |          -0.0053 |         118.7154 |          17.5303 |
[32m[20221213 21:15:23 @agent_ppo2.py:185][0m |          -0.0050 |         117.8858 |          17.5440 |
[32m[20221213 21:15:23 @agent_ppo2.py:185][0m |          -0.0079 |         117.2772 |          17.5384 |
[32m[20221213 21:15:23 @agent_ppo2.py:185][0m |          -0.0067 |         117.0361 |          17.5472 |
[32m[20221213 21:15:23 @agent_ppo2.py:185][0m |          -0.0083 |         116.8067 |          17.5521 |
[32m[20221213 21:15:23 @agent_ppo2.py:185][0m |          -0.0085 |         116.5643 |          17.5618 |
[32m[20221213 21:15:23 @agent_ppo2.py:185][0m |          -0.0094 |         116.4483 |          17.5589 |
[32m[20221213 21:15:24 @agent_ppo2.py:185][0m |          -0.0022 |         125.2272 |          17.5660 |
[32m[20221213 21:15:24 @agent_ppo2.py:185][0m |          -0.0088 |         116.2812 |          17.5721 |
[32m[20221213 21:15:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:15:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 632.00
[32m[20221213 21:15:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.00
[32m[20221213 21:15:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 644.00
[32m[20221213 21:15:24 @agent_ppo2.py:143][0m Total time:      19.82 min
[32m[20221213 21:15:24 @agent_ppo2.py:145][0m 1933312 total steps have happened
[32m[20221213 21:15:24 @agent_ppo2.py:121][0m #------------------------ Iteration 944 --------------------------#
[32m[20221213 21:15:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:24 @agent_ppo2.py:185][0m |          -0.0008 |         116.5690 |          17.5282 |
[32m[20221213 21:15:24 @agent_ppo2.py:185][0m |          -0.0057 |         114.7774 |          17.4883 |
[32m[20221213 21:15:24 @agent_ppo2.py:185][0m |          -0.0019 |         115.1400 |          17.4805 |
[32m[20221213 21:15:24 @agent_ppo2.py:185][0m |          -0.0088 |         113.7354 |          17.4594 |
[32m[20221213 21:15:24 @agent_ppo2.py:185][0m |          -0.0072 |         113.2012 |          17.4678 |
[32m[20221213 21:15:24 @agent_ppo2.py:185][0m |          -0.0060 |         112.8816 |          17.4481 |
[32m[20221213 21:15:25 @agent_ppo2.py:185][0m |          -0.0067 |         112.4775 |          17.4404 |
[32m[20221213 21:15:25 @agent_ppo2.py:185][0m |          -0.0068 |         112.6026 |          17.4408 |
[32m[20221213 21:15:25 @agent_ppo2.py:185][0m |          -0.0085 |         112.0291 |          17.4241 |
[32m[20221213 21:15:25 @agent_ppo2.py:185][0m |          -0.0043 |         113.2720 |          17.4112 |
[32m[20221213 21:15:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:15:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 654.00
[32m[20221213 21:15:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 687.00
[32m[20221213 21:15:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 709.00
[32m[20221213 21:15:25 @agent_ppo2.py:143][0m Total time:      19.84 min
[32m[20221213 21:15:25 @agent_ppo2.py:145][0m 1935360 total steps have happened
[32m[20221213 21:15:25 @agent_ppo2.py:121][0m #------------------------ Iteration 945 --------------------------#
[32m[20221213 21:15:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:25 @agent_ppo2.py:185][0m |          -0.0019 |         120.0807 |          17.4986 |
[32m[20221213 21:15:25 @agent_ppo2.py:185][0m |          -0.0049 |         117.0689 |          17.4925 |
[32m[20221213 21:15:25 @agent_ppo2.py:185][0m |          -0.0080 |         115.7151 |          17.4971 |
[32m[20221213 21:15:26 @agent_ppo2.py:185][0m |          -0.0056 |         113.8504 |          17.4887 |
[32m[20221213 21:15:26 @agent_ppo2.py:185][0m |          -0.0052 |         112.5472 |          17.4958 |
[32m[20221213 21:15:26 @agent_ppo2.py:185][0m |          -0.0046 |         111.4988 |          17.4807 |
[32m[20221213 21:15:26 @agent_ppo2.py:185][0m |          -0.0032 |         111.7588 |          17.4856 |
[32m[20221213 21:15:26 @agent_ppo2.py:185][0m |          -0.0098 |         110.5516 |          17.4699 |
[32m[20221213 21:15:26 @agent_ppo2.py:185][0m |          -0.0077 |         110.2196 |          17.4701 |
[32m[20221213 21:15:26 @agent_ppo2.py:185][0m |          -0.0097 |         109.7915 |          17.4781 |
[32m[20221213 21:15:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:15:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 663.60
[32m[20221213 21:15:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 708.00
[32m[20221213 21:15:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 646.00
[32m[20221213 21:15:26 @agent_ppo2.py:143][0m Total time:      19.86 min
[32m[20221213 21:15:26 @agent_ppo2.py:145][0m 1937408 total steps have happened
[32m[20221213 21:15:26 @agent_ppo2.py:121][0m #------------------------ Iteration 946 --------------------------#
[32m[20221213 21:15:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:15:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:26 @agent_ppo2.py:185][0m |          -0.0006 |         122.1478 |          17.3018 |
[32m[20221213 21:15:27 @agent_ppo2.py:185][0m |           0.0053 |         129.2049 |          17.2849 |
[32m[20221213 21:15:27 @agent_ppo2.py:185][0m |          -0.0071 |         119.9561 |          17.2760 |
[32m[20221213 21:15:27 @agent_ppo2.py:185][0m |          -0.0067 |         119.4183 |          17.2664 |
[32m[20221213 21:15:27 @agent_ppo2.py:185][0m |          -0.0067 |         118.9254 |          17.2631 |
[32m[20221213 21:15:27 @agent_ppo2.py:185][0m |          -0.0059 |         118.8764 |          17.2490 |
[32m[20221213 21:15:27 @agent_ppo2.py:185][0m |          -0.0060 |         118.6021 |          17.2411 |
[32m[20221213 21:15:27 @agent_ppo2.py:185][0m |          -0.0055 |         118.2196 |          17.2330 |
[32m[20221213 21:15:27 @agent_ppo2.py:185][0m |          -0.0103 |         118.4169 |          17.2202 |
[32m[20221213 21:15:27 @agent_ppo2.py:185][0m |          -0.0093 |         117.9955 |          17.2162 |
[32m[20221213 21:15:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:15:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 629.20
[32m[20221213 21:15:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 659.00
[32m[20221213 21:15:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 682.00
[32m[20221213 21:15:27 @agent_ppo2.py:143][0m Total time:      19.88 min
[32m[20221213 21:15:27 @agent_ppo2.py:145][0m 1939456 total steps have happened
[32m[20221213 21:15:27 @agent_ppo2.py:121][0m #------------------------ Iteration 947 --------------------------#
[32m[20221213 21:15:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:28 @agent_ppo2.py:185][0m |           0.0088 |         127.8449 |          17.3934 |
[32m[20221213 21:15:28 @agent_ppo2.py:185][0m |          -0.0049 |         121.6339 |          17.3828 |
[32m[20221213 21:15:28 @agent_ppo2.py:185][0m |          -0.0039 |         120.8890 |          17.4124 |
[32m[20221213 21:15:28 @agent_ppo2.py:185][0m |          -0.0058 |         120.4654 |          17.4110 |
[32m[20221213 21:15:28 @agent_ppo2.py:185][0m |          -0.0027 |         120.6201 |          17.4029 |
[32m[20221213 21:15:28 @agent_ppo2.py:185][0m |          -0.0063 |         119.7833 |          17.4164 |
[32m[20221213 21:15:28 @agent_ppo2.py:185][0m |           0.0028 |         126.9909 |          17.4175 |
[32m[20221213 21:15:28 @agent_ppo2.py:185][0m |          -0.0050 |         119.4436 |          17.4057 |
[32m[20221213 21:15:28 @agent_ppo2.py:185][0m |          -0.0061 |         119.1539 |          17.4304 |
[32m[20221213 21:15:28 @agent_ppo2.py:185][0m |          -0.0048 |         120.8115 |          17.4390 |
[32m[20221213 21:15:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:15:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 639.00
[32m[20221213 21:15:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 685.00
[32m[20221213 21:15:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 699.00
[32m[20221213 21:15:29 @agent_ppo2.py:143][0m Total time:      19.90 min
[32m[20221213 21:15:29 @agent_ppo2.py:145][0m 1941504 total steps have happened
[32m[20221213 21:15:29 @agent_ppo2.py:121][0m #------------------------ Iteration 948 --------------------------#
[32m[20221213 21:15:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:29 @agent_ppo2.py:185][0m |           0.0003 |         118.7177 |          17.5037 |
[32m[20221213 21:15:29 @agent_ppo2.py:185][0m |          -0.0057 |         116.5925 |          17.4898 |
[32m[20221213 21:15:29 @agent_ppo2.py:185][0m |          -0.0020 |         115.8530 |          17.4889 |
[32m[20221213 21:15:29 @agent_ppo2.py:185][0m |           0.0020 |         118.7630 |          17.4598 |
[32m[20221213 21:15:29 @agent_ppo2.py:185][0m |          -0.0052 |         115.5358 |          17.4420 |
[32m[20221213 21:15:29 @agent_ppo2.py:185][0m |          -0.0058 |         115.4849 |          17.4349 |
[32m[20221213 21:15:29 @agent_ppo2.py:185][0m |          -0.0059 |         115.0509 |          17.4174 |
[32m[20221213 21:15:29 @agent_ppo2.py:185][0m |          -0.0031 |         116.2192 |          17.4143 |
[32m[20221213 21:15:30 @agent_ppo2.py:185][0m |          -0.0064 |         114.6523 |          17.4204 |
[32m[20221213 21:15:30 @agent_ppo2.py:185][0m |           0.0002 |         115.2668 |          17.3937 |
[32m[20221213 21:15:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:15:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 641.40
[32m[20221213 21:15:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 686.00
[32m[20221213 21:15:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.00
[32m[20221213 21:15:30 @agent_ppo2.py:143][0m Total time:      19.92 min
[32m[20221213 21:15:30 @agent_ppo2.py:145][0m 1943552 total steps have happened
[32m[20221213 21:15:30 @agent_ppo2.py:121][0m #------------------------ Iteration 949 --------------------------#
[32m[20221213 21:15:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:15:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:30 @agent_ppo2.py:185][0m |          -0.0006 |         118.4870 |          17.4523 |
[32m[20221213 21:15:30 @agent_ppo2.py:185][0m |          -0.0029 |         117.3218 |          17.4604 |
[32m[20221213 21:15:30 @agent_ppo2.py:185][0m |          -0.0068 |         117.0426 |          17.4582 |
[32m[20221213 21:15:30 @agent_ppo2.py:185][0m |          -0.0061 |         116.8861 |          17.4607 |
[32m[20221213 21:15:30 @agent_ppo2.py:185][0m |          -0.0036 |         116.2664 |          17.4402 |
[32m[20221213 21:15:30 @agent_ppo2.py:185][0m |          -0.0040 |         116.1169 |          17.4546 |
[32m[20221213 21:15:31 @agent_ppo2.py:185][0m |          -0.0047 |         116.6299 |          17.4513 |
[32m[20221213 21:15:31 @agent_ppo2.py:185][0m |           0.0074 |         128.7770 |          17.4548 |
[32m[20221213 21:15:31 @agent_ppo2.py:185][0m |          -0.0041 |         116.2079 |          17.4450 |
[32m[20221213 21:15:31 @agent_ppo2.py:185][0m |           0.0017 |         122.2659 |          17.4407 |
[32m[20221213 21:15:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:15:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 662.40
[32m[20221213 21:15:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 723.00
[32m[20221213 21:15:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 594.00
[32m[20221213 21:15:31 @agent_ppo2.py:143][0m Total time:      19.94 min
[32m[20221213 21:15:31 @agent_ppo2.py:145][0m 1945600 total steps have happened
[32m[20221213 21:15:31 @agent_ppo2.py:121][0m #------------------------ Iteration 950 --------------------------#
[32m[20221213 21:15:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:31 @agent_ppo2.py:185][0m |          -0.0034 |         121.0842 |          17.4490 |
[32m[20221213 21:15:31 @agent_ppo2.py:185][0m |          -0.0056 |         120.2174 |          17.4383 |
[32m[20221213 21:15:31 @agent_ppo2.py:185][0m |          -0.0016 |         122.1714 |          17.4324 |
[32m[20221213 21:15:32 @agent_ppo2.py:185][0m |          -0.0052 |         119.5698 |          17.4265 |
[32m[20221213 21:15:32 @agent_ppo2.py:185][0m |          -0.0059 |         119.4952 |          17.4311 |
[32m[20221213 21:15:32 @agent_ppo2.py:185][0m |          -0.0016 |         121.8180 |          17.4315 |
[32m[20221213 21:15:32 @agent_ppo2.py:185][0m |          -0.0076 |         118.6092 |          17.4235 |
[32m[20221213 21:15:32 @agent_ppo2.py:185][0m |          -0.0065 |         118.4847 |          17.4261 |
[32m[20221213 21:15:32 @agent_ppo2.py:185][0m |          -0.0026 |         122.8029 |          17.4376 |
[32m[20221213 21:15:32 @agent_ppo2.py:185][0m |          -0.0085 |         118.3826 |          17.4332 |
[32m[20221213 21:15:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:15:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 656.60
[32m[20221213 21:15:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.00
[32m[20221213 21:15:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 653.00
[32m[20221213 21:15:32 @agent_ppo2.py:143][0m Total time:      19.96 min
[32m[20221213 21:15:32 @agent_ppo2.py:145][0m 1947648 total steps have happened
[32m[20221213 21:15:32 @agent_ppo2.py:121][0m #------------------------ Iteration 951 --------------------------#
[32m[20221213 21:15:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:32 @agent_ppo2.py:185][0m |           0.0148 |         136.7639 |          17.4761 |
[32m[20221213 21:15:33 @agent_ppo2.py:185][0m |          -0.0026 |         123.2591 |          17.4677 |
[32m[20221213 21:15:33 @agent_ppo2.py:185][0m |          -0.0036 |         122.6628 |          17.4642 |
[32m[20221213 21:15:33 @agent_ppo2.py:185][0m |          -0.0079 |         121.8746 |          17.4689 |
[32m[20221213 21:15:33 @agent_ppo2.py:185][0m |          -0.0049 |         121.4186 |          17.4558 |
[32m[20221213 21:15:33 @agent_ppo2.py:185][0m |          -0.0056 |         121.8017 |          17.4728 |
[32m[20221213 21:15:33 @agent_ppo2.py:185][0m |          -0.0072 |         120.6813 |          17.4776 |
[32m[20221213 21:15:33 @agent_ppo2.py:185][0m |          -0.0105 |         120.3874 |          17.4817 |
[32m[20221213 21:15:33 @agent_ppo2.py:185][0m |          -0.0126 |         120.3338 |          17.4634 |
[32m[20221213 21:15:33 @agent_ppo2.py:185][0m |          -0.0090 |         119.9361 |          17.4826 |
[32m[20221213 21:15:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:15:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 639.20
[32m[20221213 21:15:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 714.00
[32m[20221213 21:15:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 705.00
[32m[20221213 21:15:33 @agent_ppo2.py:143][0m Total time:      19.98 min
[32m[20221213 21:15:33 @agent_ppo2.py:145][0m 1949696 total steps have happened
[32m[20221213 21:15:33 @agent_ppo2.py:121][0m #------------------------ Iteration 952 --------------------------#
[32m[20221213 21:15:33 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:15:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:34 @agent_ppo2.py:185][0m |          -0.0016 |         120.9028 |          17.4187 |
[32m[20221213 21:15:34 @agent_ppo2.py:185][0m |          -0.0028 |         119.5110 |          17.4128 |
[32m[20221213 21:15:34 @agent_ppo2.py:185][0m |          -0.0052 |         119.0097 |          17.4184 |
[32m[20221213 21:15:34 @agent_ppo2.py:185][0m |          -0.0080 |         119.0992 |          17.4458 |
[32m[20221213 21:15:34 @agent_ppo2.py:185][0m |          -0.0074 |         118.6740 |          17.4383 |
[32m[20221213 21:15:34 @agent_ppo2.py:185][0m |          -0.0066 |         118.4075 |          17.4258 |
[32m[20221213 21:15:34 @agent_ppo2.py:185][0m |          -0.0076 |         118.2008 |          17.4404 |
[32m[20221213 21:15:34 @agent_ppo2.py:185][0m |          -0.0076 |         118.1738 |          17.4307 |
[32m[20221213 21:15:34 @agent_ppo2.py:185][0m |           0.0027 |         125.5244 |          17.4356 |
[32m[20221213 21:15:34 @agent_ppo2.py:185][0m |          -0.0097 |         117.8534 |          17.4475 |
[32m[20221213 21:15:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:15:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 663.20
[32m[20221213 21:15:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 690.00
[32m[20221213 21:15:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 661.00
[32m[20221213 21:15:35 @agent_ppo2.py:143][0m Total time:      20.00 min
[32m[20221213 21:15:35 @agent_ppo2.py:145][0m 1951744 total steps have happened
[32m[20221213 21:15:35 @agent_ppo2.py:121][0m #------------------------ Iteration 953 --------------------------#
[32m[20221213 21:15:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:35 @agent_ppo2.py:185][0m |          -0.0018 |         122.4879 |          17.4848 |
[32m[20221213 21:15:35 @agent_ppo2.py:185][0m |          -0.0028 |         121.8886 |          17.4828 |
[32m[20221213 21:15:35 @agent_ppo2.py:185][0m |           0.0062 |         127.4332 |          17.5062 |
[32m[20221213 21:15:35 @agent_ppo2.py:185][0m |           0.0039 |         124.2827 |          17.4986 |
[32m[20221213 21:15:35 @agent_ppo2.py:185][0m |          -0.0066 |         121.4914 |          17.5103 |
[32m[20221213 21:15:35 @agent_ppo2.py:185][0m |          -0.0022 |         122.4268 |          17.5057 |
[32m[20221213 21:15:35 @agent_ppo2.py:185][0m |          -0.0062 |         120.8556 |          17.5165 |
[32m[20221213 21:15:35 @agent_ppo2.py:185][0m |          -0.0046 |         120.7250 |          17.5130 |
[32m[20221213 21:15:36 @agent_ppo2.py:185][0m |          -0.0088 |         120.7051 |          17.5111 |
[32m[20221213 21:15:36 @agent_ppo2.py:185][0m |          -0.0076 |         120.6703 |          17.5039 |
[32m[20221213 21:15:36 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:15:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 670.40
[32m[20221213 21:15:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 727.00
[32m[20221213 21:15:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 635.00
[32m[20221213 21:15:36 @agent_ppo2.py:143][0m Total time:      20.02 min
[32m[20221213 21:15:36 @agent_ppo2.py:145][0m 1953792 total steps have happened
[32m[20221213 21:15:36 @agent_ppo2.py:121][0m #------------------------ Iteration 954 --------------------------#
[32m[20221213 21:15:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:36 @agent_ppo2.py:185][0m |          -0.0020 |         125.6322 |          17.4199 |
[32m[20221213 21:15:36 @agent_ppo2.py:185][0m |          -0.0074 |         123.9468 |          17.4164 |
[32m[20221213 21:15:36 @agent_ppo2.py:185][0m |          -0.0060 |         123.0819 |          17.4155 |
[32m[20221213 21:15:36 @agent_ppo2.py:185][0m |          -0.0090 |         122.5500 |          17.4258 |
[32m[20221213 21:15:36 @agent_ppo2.py:185][0m |          -0.0081 |         122.0858 |          17.4385 |
[32m[20221213 21:15:37 @agent_ppo2.py:185][0m |          -0.0083 |         121.6479 |          17.4449 |
[32m[20221213 21:15:37 @agent_ppo2.py:185][0m |          -0.0001 |         128.5219 |          17.4444 |
[32m[20221213 21:15:37 @agent_ppo2.py:185][0m |          -0.0112 |         121.1074 |          17.4568 |
[32m[20221213 21:15:37 @agent_ppo2.py:185][0m |          -0.0100 |         120.8784 |          17.4647 |
[32m[20221213 21:15:37 @agent_ppo2.py:185][0m |          -0.0124 |         120.5976 |          17.4830 |
[32m[20221213 21:15:37 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:15:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 628.40
[32m[20221213 21:15:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 697.00
[32m[20221213 21:15:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 572.00
[32m[20221213 21:15:37 @agent_ppo2.py:143][0m Total time:      20.04 min
[32m[20221213 21:15:37 @agent_ppo2.py:145][0m 1955840 total steps have happened
[32m[20221213 21:15:37 @agent_ppo2.py:121][0m #------------------------ Iteration 955 --------------------------#
[32m[20221213 21:15:37 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:15:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:37 @agent_ppo2.py:185][0m |          -0.0013 |         125.8269 |          17.6373 |
[32m[20221213 21:15:37 @agent_ppo2.py:185][0m |           0.0008 |         123.9061 |          17.6333 |
[32m[20221213 21:15:37 @agent_ppo2.py:185][0m |          -0.0046 |         122.0799 |          17.6380 |
[32m[20221213 21:15:38 @agent_ppo2.py:185][0m |          -0.0033 |         121.4018 |          17.6480 |
[32m[20221213 21:15:38 @agent_ppo2.py:185][0m |          -0.0040 |         121.1054 |          17.6529 |
[32m[20221213 21:15:38 @agent_ppo2.py:185][0m |          -0.0058 |         120.7948 |          17.6548 |
[32m[20221213 21:15:38 @agent_ppo2.py:185][0m |          -0.0048 |         120.5086 |          17.6582 |
[32m[20221213 21:15:38 @agent_ppo2.py:185][0m |          -0.0067 |         120.4223 |          17.6685 |
[32m[20221213 21:15:38 @agent_ppo2.py:185][0m |          -0.0073 |         120.3267 |          17.6854 |
[32m[20221213 21:15:38 @agent_ppo2.py:185][0m |          -0.0041 |         120.4846 |          17.6874 |
[32m[20221213 21:15:38 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:15:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 596.80
[32m[20221213 21:15:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 651.00
[32m[20221213 21:15:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.00
[32m[20221213 21:15:38 @agent_ppo2.py:143][0m Total time:      20.06 min
[32m[20221213 21:15:38 @agent_ppo2.py:145][0m 1957888 total steps have happened
[32m[20221213 21:15:38 @agent_ppo2.py:121][0m #------------------------ Iteration 956 --------------------------#
[32m[20221213 21:15:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:39 @agent_ppo2.py:185][0m |          -0.0006 |         124.8461 |          17.6525 |
[32m[20221213 21:15:39 @agent_ppo2.py:185][0m |          -0.0080 |         123.5692 |          17.6438 |
[32m[20221213 21:15:39 @agent_ppo2.py:185][0m |          -0.0053 |         122.7759 |          17.6442 |
[32m[20221213 21:15:39 @agent_ppo2.py:185][0m |          -0.0098 |         121.9173 |          17.6354 |
[32m[20221213 21:15:39 @agent_ppo2.py:185][0m |          -0.0094 |         121.5224 |          17.6437 |
[32m[20221213 21:15:39 @agent_ppo2.py:185][0m |          -0.0103 |         121.0384 |          17.6438 |
[32m[20221213 21:15:39 @agent_ppo2.py:185][0m |          -0.0071 |         120.6201 |          17.6393 |
[32m[20221213 21:15:39 @agent_ppo2.py:185][0m |          -0.0070 |         120.2865 |          17.6353 |
[32m[20221213 21:15:39 @agent_ppo2.py:185][0m |          -0.0059 |         119.9321 |          17.6428 |
[32m[20221213 21:15:39 @agent_ppo2.py:185][0m |          -0.0087 |         119.7503 |          17.6418 |
[32m[20221213 21:15:39 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:15:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 625.20
[32m[20221213 21:15:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 688.00
[32m[20221213 21:15:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 640.00
[32m[20221213 21:15:40 @agent_ppo2.py:143][0m Total time:      20.08 min
[32m[20221213 21:15:40 @agent_ppo2.py:145][0m 1959936 total steps have happened
[32m[20221213 21:15:40 @agent_ppo2.py:121][0m #------------------------ Iteration 957 --------------------------#
[32m[20221213 21:15:40 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:15:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:40 @agent_ppo2.py:185][0m |          -0.0013 |         124.3086 |          17.5823 |
[32m[20221213 21:15:40 @agent_ppo2.py:185][0m |          -0.0039 |         122.9902 |          17.5497 |
[32m[20221213 21:15:40 @agent_ppo2.py:185][0m |           0.0048 |         129.8868 |          17.5615 |
[32m[20221213 21:15:40 @agent_ppo2.py:185][0m |           0.0036 |         127.5148 |          17.5496 |
[32m[20221213 21:15:40 @agent_ppo2.py:185][0m |           0.0040 |         124.4105 |          17.5469 |
[32m[20221213 21:15:40 @agent_ppo2.py:185][0m |          -0.0035 |         121.1255 |          17.5546 |
[32m[20221213 21:15:40 @agent_ppo2.py:185][0m |          -0.0055 |         121.1038 |          17.5479 |
[32m[20221213 21:15:40 @agent_ppo2.py:185][0m |          -0.0043 |         120.6508 |          17.5584 |
[32m[20221213 21:15:41 @agent_ppo2.py:185][0m |          -0.0056 |         120.8564 |          17.5485 |
[32m[20221213 21:15:41 @agent_ppo2.py:185][0m |          -0.0069 |         120.5342 |          17.5476 |
[32m[20221213 21:15:41 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:15:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 649.40
[32m[20221213 21:15:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 677.00
[32m[20221213 21:15:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 716.00
[32m[20221213 21:15:41 @agent_ppo2.py:143][0m Total time:      20.10 min
[32m[20221213 21:15:41 @agent_ppo2.py:145][0m 1961984 total steps have happened
[32m[20221213 21:15:41 @agent_ppo2.py:121][0m #------------------------ Iteration 958 --------------------------#
[32m[20221213 21:15:41 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 21:15:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:41 @agent_ppo2.py:185][0m |           0.0107 |         132.8186 |          17.4822 |
[32m[20221213 21:15:41 @agent_ppo2.py:185][0m |          -0.0027 |         121.8917 |          17.4827 |
[32m[20221213 21:15:41 @agent_ppo2.py:185][0m |          -0.0025 |         121.1838 |          17.4830 |
[32m[20221213 21:15:42 @agent_ppo2.py:185][0m |           0.0006 |         122.0405 |          17.5049 |
[32m[20221213 21:15:42 @agent_ppo2.py:185][0m |          -0.0038 |         120.7339 |          17.4941 |
[32m[20221213 21:15:42 @agent_ppo2.py:185][0m |           0.0063 |         130.9195 |          17.5100 |
[32m[20221213 21:15:42 @agent_ppo2.py:185][0m |          -0.0065 |         120.7140 |          17.4976 |
[32m[20221213 21:15:42 @agent_ppo2.py:185][0m |          -0.0052 |         120.3055 |          17.5051 |
[32m[20221213 21:15:42 @agent_ppo2.py:185][0m |          -0.0081 |         120.1338 |          17.4965 |
[32m[20221213 21:15:42 @agent_ppo2.py:185][0m |          -0.0044 |         119.9607 |          17.5008 |
[32m[20221213 21:15:42 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:15:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.80
[32m[20221213 21:15:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 659.00
[32m[20221213 21:15:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 532.00
[32m[20221213 21:15:42 @agent_ppo2.py:143][0m Total time:      20.13 min
[32m[20221213 21:15:42 @agent_ppo2.py:145][0m 1964032 total steps have happened
[32m[20221213 21:15:42 @agent_ppo2.py:121][0m #------------------------ Iteration 959 --------------------------#
[32m[20221213 21:15:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:15:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:43 @agent_ppo2.py:185][0m |          -0.0053 |         124.2570 |          17.5601 |
[32m[20221213 21:15:43 @agent_ppo2.py:185][0m |           0.0071 |         135.3976 |          17.5543 |
[32m[20221213 21:15:43 @agent_ppo2.py:185][0m |          -0.0040 |         121.3008 |          17.5492 |
[32m[20221213 21:15:43 @agent_ppo2.py:185][0m |          -0.0047 |         120.6684 |          17.5401 |
[32m[20221213 21:15:43 @agent_ppo2.py:185][0m |          -0.0071 |         119.9385 |          17.5501 |
[32m[20221213 21:15:43 @agent_ppo2.py:185][0m |           0.0008 |         125.5309 |          17.5547 |
[32m[20221213 21:15:43 @agent_ppo2.py:185][0m |          -0.0081 |         119.1257 |          17.5601 |
[32m[20221213 21:15:43 @agent_ppo2.py:185][0m |          -0.0126 |         118.5120 |          17.5711 |
[32m[20221213 21:15:43 @agent_ppo2.py:185][0m |          -0.0121 |         118.2822 |          17.5633 |
[32m[20221213 21:15:44 @agent_ppo2.py:185][0m |          -0.0134 |         117.9671 |          17.5819 |
[32m[20221213 21:15:44 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 21:15:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 653.00
[32m[20221213 21:15:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 703.00
[32m[20221213 21:15:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 646.00
[32m[20221213 21:15:44 @agent_ppo2.py:143][0m Total time:      20.15 min
[32m[20221213 21:15:44 @agent_ppo2.py:145][0m 1966080 total steps have happened
[32m[20221213 21:15:44 @agent_ppo2.py:121][0m #------------------------ Iteration 960 --------------------------#
[32m[20221213 21:15:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:44 @agent_ppo2.py:185][0m |          -0.0024 |         125.6458 |          17.5592 |
[32m[20221213 21:15:44 @agent_ppo2.py:185][0m |          -0.0021 |         123.8397 |          17.5926 |
[32m[20221213 21:15:44 @agent_ppo2.py:185][0m |          -0.0066 |         123.4165 |          17.6074 |
[32m[20221213 21:15:44 @agent_ppo2.py:185][0m |          -0.0005 |         124.1602 |          17.6233 |
[32m[20221213 21:15:44 @agent_ppo2.py:185][0m |          -0.0068 |         122.7165 |          17.6381 |
[32m[20221213 21:15:44 @agent_ppo2.py:185][0m |          -0.0095 |         122.5920 |          17.6335 |
[32m[20221213 21:15:45 @agent_ppo2.py:185][0m |          -0.0093 |         122.1445 |          17.6404 |
[32m[20221213 21:15:45 @agent_ppo2.py:185][0m |          -0.0078 |         122.2351 |          17.6577 |
[32m[20221213 21:15:45 @agent_ppo2.py:185][0m |          -0.0038 |         122.0943 |          17.6622 |
[32m[20221213 21:15:45 @agent_ppo2.py:185][0m |          -0.0108 |         121.7972 |          17.6686 |
[32m[20221213 21:15:45 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:15:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 623.00
[32m[20221213 21:15:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 643.00
[32m[20221213 21:15:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 715.00
[32m[20221213 21:15:45 @agent_ppo2.py:143][0m Total time:      20.17 min
[32m[20221213 21:15:45 @agent_ppo2.py:145][0m 1968128 total steps have happened
[32m[20221213 21:15:45 @agent_ppo2.py:121][0m #------------------------ Iteration 961 --------------------------#
[32m[20221213 21:15:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:45 @agent_ppo2.py:185][0m |          -0.0014 |         123.5323 |          17.7276 |
[32m[20221213 21:15:45 @agent_ppo2.py:185][0m |          -0.0016 |         122.1778 |          17.7014 |
[32m[20221213 21:15:45 @agent_ppo2.py:185][0m |          -0.0058 |         122.0037 |          17.7091 |
[32m[20221213 21:15:45 @agent_ppo2.py:185][0m |          -0.0053 |         121.3325 |          17.7032 |
[32m[20221213 21:15:46 @agent_ppo2.py:185][0m |          -0.0031 |         121.0496 |          17.7035 |
[32m[20221213 21:15:46 @agent_ppo2.py:185][0m |           0.0038 |         127.9194 |          17.7243 |
[32m[20221213 21:15:46 @agent_ppo2.py:185][0m |          -0.0024 |         120.6147 |          17.7082 |
[32m[20221213 21:15:46 @agent_ppo2.py:185][0m |          -0.0071 |         120.4124 |          17.7178 |
[32m[20221213 21:15:46 @agent_ppo2.py:185][0m |          -0.0057 |         120.2106 |          17.7241 |
[32m[20221213 21:15:46 @agent_ppo2.py:185][0m |          -0.0069 |         120.0801 |          17.7344 |
[32m[20221213 21:15:46 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:15:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 642.20
[32m[20221213 21:15:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.00
[32m[20221213 21:15:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 716.00
[32m[20221213 21:15:46 @agent_ppo2.py:143][0m Total time:      20.19 min
[32m[20221213 21:15:46 @agent_ppo2.py:145][0m 1970176 total steps have happened
[32m[20221213 21:15:46 @agent_ppo2.py:121][0m #------------------------ Iteration 962 --------------------------#
[32m[20221213 21:15:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:15:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:46 @agent_ppo2.py:185][0m |          -0.0008 |         119.6244 |          17.6905 |
[32m[20221213 21:15:47 @agent_ppo2.py:185][0m |          -0.0042 |         118.5273 |          17.6857 |
[32m[20221213 21:15:47 @agent_ppo2.py:185][0m |          -0.0022 |         117.3769 |          17.6826 |
[32m[20221213 21:15:47 @agent_ppo2.py:185][0m |          -0.0056 |         117.0403 |          17.6830 |
[32m[20221213 21:15:47 @agent_ppo2.py:185][0m |          -0.0050 |         116.7110 |          17.6688 |
[32m[20221213 21:15:47 @agent_ppo2.py:185][0m |          -0.0072 |         116.4077 |          17.6726 |
[32m[20221213 21:15:47 @agent_ppo2.py:185][0m |          -0.0085 |         116.3196 |          17.6814 |
[32m[20221213 21:15:47 @agent_ppo2.py:185][0m |          -0.0076 |         116.0391 |          17.6793 |
[32m[20221213 21:15:47 @agent_ppo2.py:185][0m |          -0.0016 |         117.3838 |          17.6753 |
[32m[20221213 21:15:47 @agent_ppo2.py:185][0m |          -0.0081 |         115.8986 |          17.6869 |
[32m[20221213 21:15:47 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:15:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 651.60
[32m[20221213 21:15:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 695.00
[32m[20221213 21:15:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 647.00
[32m[20221213 21:15:48 @agent_ppo2.py:143][0m Total time:      20.21 min
[32m[20221213 21:15:48 @agent_ppo2.py:145][0m 1972224 total steps have happened
[32m[20221213 21:15:48 @agent_ppo2.py:121][0m #------------------------ Iteration 963 --------------------------#
[32m[20221213 21:15:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:48 @agent_ppo2.py:185][0m |           0.0006 |         120.3277 |          17.7740 |
[32m[20221213 21:15:48 @agent_ppo2.py:185][0m |          -0.0052 |         119.0566 |          17.7531 |
[32m[20221213 21:15:48 @agent_ppo2.py:185][0m |          -0.0057 |         118.3460 |          17.7466 |
[32m[20221213 21:15:48 @agent_ppo2.py:185][0m |          -0.0078 |         117.9542 |          17.7560 |
[32m[20221213 21:15:48 @agent_ppo2.py:185][0m |          -0.0080 |         117.7291 |          17.7534 |
[32m[20221213 21:15:48 @agent_ppo2.py:185][0m |          -0.0064 |         117.5030 |          17.7524 |
[32m[20221213 21:15:48 @agent_ppo2.py:185][0m |          -0.0073 |         117.5735 |          17.7530 |
[32m[20221213 21:15:48 @agent_ppo2.py:185][0m |          -0.0086 |         117.3469 |          17.7483 |
[32m[20221213 21:15:49 @agent_ppo2.py:185][0m |          -0.0078 |         117.2491 |          17.7493 |
[32m[20221213 21:15:49 @agent_ppo2.py:185][0m |          -0.0108 |         117.1123 |          17.7402 |
[32m[20221213 21:15:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:15:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 686.60
[32m[20221213 21:15:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 754.00
[32m[20221213 21:15:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 683.00
[32m[20221213 21:15:49 @agent_ppo2.py:143][0m Total time:      20.23 min
[32m[20221213 21:15:49 @agent_ppo2.py:145][0m 1974272 total steps have happened
[32m[20221213 21:15:49 @agent_ppo2.py:121][0m #------------------------ Iteration 964 --------------------------#
[32m[20221213 21:15:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:49 @agent_ppo2.py:185][0m |          -0.0036 |         125.7352 |          17.7640 |
[32m[20221213 21:15:49 @agent_ppo2.py:185][0m |          -0.0050 |         123.4786 |          17.7697 |
[32m[20221213 21:15:49 @agent_ppo2.py:185][0m |           0.0134 |         141.0506 |          17.7726 |
[32m[20221213 21:15:49 @agent_ppo2.py:185][0m |          -0.0052 |         121.0154 |          17.7562 |
[32m[20221213 21:15:49 @agent_ppo2.py:185][0m |           0.0017 |         131.0998 |          17.7851 |
[32m[20221213 21:15:50 @agent_ppo2.py:185][0m |           0.0036 |         133.2076 |          17.7956 |
[32m[20221213 21:15:50 @agent_ppo2.py:185][0m |          -0.0055 |         119.5684 |          17.7836 |
[32m[20221213 21:15:50 @agent_ppo2.py:185][0m |          -0.0063 |         119.1950 |          17.7987 |
[32m[20221213 21:15:50 @agent_ppo2.py:185][0m |          -0.0076 |         118.6205 |          17.8040 |
[32m[20221213 21:15:50 @agent_ppo2.py:185][0m |           0.0071 |         129.8577 |          17.8151 |
[32m[20221213 21:15:50 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:15:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 666.60
[32m[20221213 21:15:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 685.00
[32m[20221213 21:15:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 637.00
[32m[20221213 21:15:50 @agent_ppo2.py:143][0m Total time:      20.26 min
[32m[20221213 21:15:50 @agent_ppo2.py:145][0m 1976320 total steps have happened
[32m[20221213 21:15:50 @agent_ppo2.py:121][0m #------------------------ Iteration 965 --------------------------#
[32m[20221213 21:15:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:50 @agent_ppo2.py:185][0m |           0.0004 |         128.4415 |          17.7690 |
[32m[20221213 21:15:50 @agent_ppo2.py:185][0m |           0.0102 |         141.5649 |          17.7595 |
[32m[20221213 21:15:51 @agent_ppo2.py:185][0m |          -0.0071 |         126.4701 |          17.7655 |
[32m[20221213 21:15:51 @agent_ppo2.py:185][0m |          -0.0002 |         126.9768 |          17.7687 |
[32m[20221213 21:15:51 @agent_ppo2.py:185][0m |          -0.0074 |         125.7093 |          17.7741 |
[32m[20221213 21:15:51 @agent_ppo2.py:185][0m |          -0.0053 |         125.5321 |          17.7758 |
[32m[20221213 21:15:51 @agent_ppo2.py:185][0m |          -0.0059 |         125.0380 |          17.7882 |
[32m[20221213 21:15:51 @agent_ppo2.py:185][0m |          -0.0061 |         124.8975 |          17.7882 |
[32m[20221213 21:15:51 @agent_ppo2.py:185][0m |          -0.0076 |         124.9703 |          17.7963 |
[32m[20221213 21:15:51 @agent_ppo2.py:185][0m |          -0.0078 |         124.5085 |          17.8048 |
[32m[20221213 21:15:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:15:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 622.80
[32m[20221213 21:15:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 657.00
[32m[20221213 21:15:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 614.00
[32m[20221213 21:15:51 @agent_ppo2.py:143][0m Total time:      20.28 min
[32m[20221213 21:15:51 @agent_ppo2.py:145][0m 1978368 total steps have happened
[32m[20221213 21:15:51 @agent_ppo2.py:121][0m #------------------------ Iteration 966 --------------------------#
[32m[20221213 21:15:51 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:15:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:52 @agent_ppo2.py:185][0m |          -0.0015 |         122.8965 |          17.8222 |
[32m[20221213 21:15:52 @agent_ppo2.py:185][0m |          -0.0044 |         121.7402 |          17.8334 |
[32m[20221213 21:15:52 @agent_ppo2.py:185][0m |          -0.0009 |         121.4498 |          17.8452 |
[32m[20221213 21:15:52 @agent_ppo2.py:185][0m |          -0.0071 |         120.6322 |          17.8378 |
[32m[20221213 21:15:52 @agent_ppo2.py:185][0m |          -0.0060 |         120.3941 |          17.8510 |
[32m[20221213 21:15:52 @agent_ppo2.py:185][0m |          -0.0062 |         120.0817 |          17.8686 |
[32m[20221213 21:15:52 @agent_ppo2.py:185][0m |          -0.0032 |         124.5534 |          17.8738 |
[32m[20221213 21:15:52 @agent_ppo2.py:185][0m |          -0.0103 |         119.5503 |          17.8889 |
[32m[20221213 21:15:52 @agent_ppo2.py:185][0m |          -0.0047 |         120.2338 |          17.8935 |
[32m[20221213 21:15:52 @agent_ppo2.py:185][0m |          -0.0069 |         119.0469 |          17.8992 |
[32m[20221213 21:15:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:15:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 648.20
[32m[20221213 21:15:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.00
[32m[20221213 21:15:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 660.00
[32m[20221213 21:15:52 @agent_ppo2.py:143][0m Total time:      20.30 min
[32m[20221213 21:15:52 @agent_ppo2.py:145][0m 1980416 total steps have happened
[32m[20221213 21:15:52 @agent_ppo2.py:121][0m #------------------------ Iteration 967 --------------------------#
[32m[20221213 21:15:53 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:15:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:53 @agent_ppo2.py:185][0m |          -0.0012 |         126.3126 |          17.9078 |
[32m[20221213 21:15:53 @agent_ppo2.py:185][0m |          -0.0028 |         125.7154 |          17.9081 |
[32m[20221213 21:15:53 @agent_ppo2.py:185][0m |          -0.0031 |         124.9069 |          17.8969 |
[32m[20221213 21:15:53 @agent_ppo2.py:185][0m |          -0.0078 |         124.4827 |          17.8914 |
[32m[20221213 21:15:53 @agent_ppo2.py:185][0m |          -0.0036 |         125.2260 |          17.8954 |
[32m[20221213 21:15:53 @agent_ppo2.py:185][0m |           0.0023 |         132.4121 |          17.8911 |
[32m[20221213 21:15:53 @agent_ppo2.py:185][0m |          -0.0048 |         123.7816 |          17.8791 |
[32m[20221213 21:15:53 @agent_ppo2.py:185][0m |          -0.0055 |         123.6078 |          17.8757 |
[32m[20221213 21:15:53 @agent_ppo2.py:185][0m |          -0.0067 |         123.5142 |          17.8796 |
[32m[20221213 21:15:54 @agent_ppo2.py:185][0m |          -0.0088 |         123.4274 |          17.8756 |
[32m[20221213 21:15:54 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:15:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 626.20
[32m[20221213 21:15:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 654.00
[32m[20221213 21:15:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 635.00
[32m[20221213 21:15:54 @agent_ppo2.py:143][0m Total time:      20.32 min
[32m[20221213 21:15:54 @agent_ppo2.py:145][0m 1982464 total steps have happened
[32m[20221213 21:15:54 @agent_ppo2.py:121][0m #------------------------ Iteration 968 --------------------------#
[32m[20221213 21:15:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:54 @agent_ppo2.py:185][0m |          -0.0005 |         126.2844 |          17.8915 |
[32m[20221213 21:15:54 @agent_ppo2.py:185][0m |          -0.0040 |         123.6794 |          17.8847 |
[32m[20221213 21:15:54 @agent_ppo2.py:185][0m |           0.0007 |         124.6094 |          17.8811 |
[32m[20221213 21:15:54 @agent_ppo2.py:185][0m |          -0.0073 |         121.9159 |          17.8924 |
[32m[20221213 21:15:54 @agent_ppo2.py:185][0m |          -0.0078 |         121.8233 |          17.8758 |
[32m[20221213 21:15:54 @agent_ppo2.py:185][0m |          -0.0066 |         121.2310 |          17.8923 |
[32m[20221213 21:15:55 @agent_ppo2.py:185][0m |          -0.0061 |         121.0913 |          17.8748 |
[32m[20221213 21:15:55 @agent_ppo2.py:185][0m |          -0.0056 |         120.4195 |          17.8908 |
[32m[20221213 21:15:55 @agent_ppo2.py:185][0m |          -0.0078 |         120.1328 |          17.8824 |
[32m[20221213 21:15:55 @agent_ppo2.py:185][0m |          -0.0101 |         120.0077 |          17.8866 |
[32m[20221213 21:15:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:15:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 615.00
[32m[20221213 21:15:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 669.00
[32m[20221213 21:15:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 643.00
[32m[20221213 21:15:55 @agent_ppo2.py:143][0m Total time:      20.34 min
[32m[20221213 21:15:55 @agent_ppo2.py:145][0m 1984512 total steps have happened
[32m[20221213 21:15:55 @agent_ppo2.py:121][0m #------------------------ Iteration 969 --------------------------#
[32m[20221213 21:15:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:55 @agent_ppo2.py:185][0m |           0.0033 |         133.7065 |          17.8018 |
[32m[20221213 21:15:55 @agent_ppo2.py:185][0m |          -0.0022 |         129.9686 |          17.8204 |
[32m[20221213 21:15:55 @agent_ppo2.py:185][0m |          -0.0049 |         127.3707 |          17.8274 |
[32m[20221213 21:15:55 @agent_ppo2.py:185][0m |          -0.0048 |         126.8331 |          17.8077 |
[32m[20221213 21:15:56 @agent_ppo2.py:185][0m |          -0.0084 |         126.4348 |          17.8188 |
[32m[20221213 21:15:56 @agent_ppo2.py:185][0m |          -0.0093 |         126.0899 |          17.8241 |
[32m[20221213 21:15:56 @agent_ppo2.py:185][0m |          -0.0094 |         126.7728 |          17.8290 |
[32m[20221213 21:15:56 @agent_ppo2.py:185][0m |          -0.0090 |         125.8222 |          17.8380 |
[32m[20221213 21:15:56 @agent_ppo2.py:185][0m |          -0.0093 |         125.4641 |          17.8335 |
[32m[20221213 21:15:56 @agent_ppo2.py:185][0m |          -0.0086 |         125.1802 |          17.8275 |
[32m[20221213 21:15:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:15:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 674.40
[32m[20221213 21:15:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.00
[32m[20221213 21:15:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 624.00
[32m[20221213 21:15:56 @agent_ppo2.py:143][0m Total time:      20.36 min
[32m[20221213 21:15:56 @agent_ppo2.py:145][0m 1986560 total steps have happened
[32m[20221213 21:15:56 @agent_ppo2.py:121][0m #------------------------ Iteration 970 --------------------------#
[32m[20221213 21:15:56 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:15:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:56 @agent_ppo2.py:185][0m |           0.0001 |         128.5128 |          18.0094 |
[32m[20221213 21:15:57 @agent_ppo2.py:185][0m |          -0.0055 |         126.7178 |          17.9946 |
[32m[20221213 21:15:57 @agent_ppo2.py:185][0m |           0.0114 |         142.0077 |          17.9822 |
[32m[20221213 21:15:57 @agent_ppo2.py:185][0m |          -0.0038 |         125.2647 |          17.9702 |
[32m[20221213 21:15:57 @agent_ppo2.py:185][0m |          -0.0089 |         124.8082 |          17.9801 |
[32m[20221213 21:15:57 @agent_ppo2.py:185][0m |          -0.0078 |         124.3237 |          17.9737 |
[32m[20221213 21:15:57 @agent_ppo2.py:185][0m |          -0.0069 |         123.7834 |          17.9814 |
[32m[20221213 21:15:57 @agent_ppo2.py:185][0m |          -0.0077 |         123.6200 |          17.9913 |
[32m[20221213 21:15:57 @agent_ppo2.py:185][0m |          -0.0037 |         124.3019 |          17.9708 |
[32m[20221213 21:15:57 @agent_ppo2.py:185][0m |          -0.0083 |         123.1556 |          17.9726 |
[32m[20221213 21:15:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:15:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.20
[32m[20221213 21:15:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 684.00
[32m[20221213 21:15:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 626.00
[32m[20221213 21:15:57 @agent_ppo2.py:143][0m Total time:      20.38 min
[32m[20221213 21:15:57 @agent_ppo2.py:145][0m 1988608 total steps have happened
[32m[20221213 21:15:57 @agent_ppo2.py:121][0m #------------------------ Iteration 971 --------------------------#
[32m[20221213 21:15:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:15:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:58 @agent_ppo2.py:185][0m |           0.0072 |         129.9888 |          17.9155 |
[32m[20221213 21:15:58 @agent_ppo2.py:185][0m |          -0.0052 |         123.8003 |          17.8964 |
[32m[20221213 21:15:58 @agent_ppo2.py:185][0m |          -0.0089 |         122.6903 |          17.8878 |
[32m[20221213 21:15:58 @agent_ppo2.py:185][0m |          -0.0082 |         122.4137 |          17.8653 |
[32m[20221213 21:15:58 @agent_ppo2.py:185][0m |          -0.0024 |         124.5365 |          17.8614 |
[32m[20221213 21:15:58 @agent_ppo2.py:185][0m |          -0.0088 |         121.0649 |          17.8693 |
[32m[20221213 21:15:58 @agent_ppo2.py:185][0m |          -0.0103 |         120.5833 |          17.8640 |
[32m[20221213 21:15:58 @agent_ppo2.py:185][0m |          -0.0099 |         120.2098 |          17.8624 |
[32m[20221213 21:15:58 @agent_ppo2.py:185][0m |          -0.0093 |         119.9707 |          17.8522 |
[32m[20221213 21:15:58 @agent_ppo2.py:185][0m |          -0.0114 |         119.9039 |          17.8577 |
[32m[20221213 21:15:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:15:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 649.60
[32m[20221213 21:15:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 686.00
[32m[20221213 21:15:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 642.00
[32m[20221213 21:15:58 @agent_ppo2.py:143][0m Total time:      20.40 min
[32m[20221213 21:15:58 @agent_ppo2.py:145][0m 1990656 total steps have happened
[32m[20221213 21:15:58 @agent_ppo2.py:121][0m #------------------------ Iteration 972 --------------------------#
[32m[20221213 21:15:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:15:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:15:59 @agent_ppo2.py:185][0m |           0.0038 |         127.2768 |          17.8817 |
[32m[20221213 21:15:59 @agent_ppo2.py:185][0m |          -0.0054 |         123.5145 |          17.8694 |
[32m[20221213 21:15:59 @agent_ppo2.py:185][0m |          -0.0038 |         122.9608 |          17.8619 |
[32m[20221213 21:15:59 @agent_ppo2.py:185][0m |          -0.0037 |         121.9906 |          17.8469 |
[32m[20221213 21:15:59 @agent_ppo2.py:185][0m |          -0.0078 |         121.5405 |          17.8577 |
[32m[20221213 21:15:59 @agent_ppo2.py:185][0m |          -0.0067 |         121.2090 |          17.8322 |
[32m[20221213 21:15:59 @agent_ppo2.py:185][0m |          -0.0055 |         120.7267 |          17.8344 |
[32m[20221213 21:15:59 @agent_ppo2.py:185][0m |          -0.0074 |         120.4476 |          17.8305 |
[32m[20221213 21:16:00 @agent_ppo2.py:185][0m |          -0.0073 |         120.0373 |          17.8300 |
[32m[20221213 21:16:00 @agent_ppo2.py:185][0m |          -0.0091 |         119.9897 |          17.8294 |
[32m[20221213 21:16:00 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:16:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 652.80
[32m[20221213 21:16:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.00
[32m[20221213 21:16:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 744.00
[32m[20221213 21:16:00 @agent_ppo2.py:143][0m Total time:      20.42 min
[32m[20221213 21:16:00 @agent_ppo2.py:145][0m 1992704 total steps have happened
[32m[20221213 21:16:00 @agent_ppo2.py:121][0m #------------------------ Iteration 973 --------------------------#
[32m[20221213 21:16:00 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:16:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:00 @agent_ppo2.py:185][0m |           0.0021 |         125.2614 |          17.8153 |
[32m[20221213 21:16:00 @agent_ppo2.py:185][0m |          -0.0023 |         123.6094 |          17.8059 |
[32m[20221213 21:16:00 @agent_ppo2.py:185][0m |          -0.0050 |         122.8445 |          17.8005 |
[32m[20221213 21:16:00 @agent_ppo2.py:185][0m |          -0.0017 |         123.1561 |          17.7997 |
[32m[20221213 21:16:00 @agent_ppo2.py:185][0m |           0.0035 |         123.5547 |          17.7933 |
[32m[20221213 21:16:00 @agent_ppo2.py:185][0m |          -0.0023 |         122.3463 |          17.7751 |
[32m[20221213 21:16:01 @agent_ppo2.py:185][0m |          -0.0022 |         122.1432 |          17.7667 |
[32m[20221213 21:16:01 @agent_ppo2.py:185][0m |           0.0039 |         130.5777 |          17.7695 |
[32m[20221213 21:16:01 @agent_ppo2.py:185][0m |           0.0051 |         134.5536 |          17.7671 |
[32m[20221213 21:16:01 @agent_ppo2.py:185][0m |          -0.0043 |         120.8917 |          17.7541 |
[32m[20221213 21:16:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 644.20
[32m[20221213 21:16:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.00
[32m[20221213 21:16:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 690.00
[32m[20221213 21:16:01 @agent_ppo2.py:143][0m Total time:      20.44 min
[32m[20221213 21:16:01 @agent_ppo2.py:145][0m 1994752 total steps have happened
[32m[20221213 21:16:01 @agent_ppo2.py:121][0m #------------------------ Iteration 974 --------------------------#
[32m[20221213 21:16:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:01 @agent_ppo2.py:185][0m |          -0.0020 |         122.4927 |          17.7361 |
[32m[20221213 21:16:01 @agent_ppo2.py:185][0m |          -0.0037 |         121.6942 |          17.7113 |
[32m[20221213 21:16:01 @agent_ppo2.py:185][0m |          -0.0038 |         121.2222 |          17.7140 |
[32m[20221213 21:16:01 @agent_ppo2.py:185][0m |          -0.0046 |         120.6379 |          17.7149 |
[32m[20221213 21:16:02 @agent_ppo2.py:185][0m |          -0.0048 |         120.3543 |          17.7051 |
[32m[20221213 21:16:02 @agent_ppo2.py:185][0m |          -0.0048 |         120.1209 |          17.7138 |
[32m[20221213 21:16:02 @agent_ppo2.py:185][0m |          -0.0003 |         122.5859 |          17.7047 |
[32m[20221213 21:16:02 @agent_ppo2.py:185][0m |          -0.0023 |         120.3160 |          17.7036 |
[32m[20221213 21:16:02 @agent_ppo2.py:185][0m |          -0.0066 |         119.4904 |          17.6960 |
[32m[20221213 21:16:02 @agent_ppo2.py:185][0m |          -0.0085 |         119.3828 |          17.6927 |
[32m[20221213 21:16:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 665.60
[32m[20221213 21:16:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 713.00
[32m[20221213 21:16:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 633.00
[32m[20221213 21:16:02 @agent_ppo2.py:143][0m Total time:      20.46 min
[32m[20221213 21:16:02 @agent_ppo2.py:145][0m 1996800 total steps have happened
[32m[20221213 21:16:02 @agent_ppo2.py:121][0m #------------------------ Iteration 975 --------------------------#
[32m[20221213 21:16:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:16:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:02 @agent_ppo2.py:185][0m |           0.0123 |         134.1738 |          17.8533 |
[32m[20221213 21:16:03 @agent_ppo2.py:185][0m |          -0.0034 |         124.8392 |          17.8488 |
[32m[20221213 21:16:03 @agent_ppo2.py:185][0m |          -0.0045 |         123.3812 |          17.8561 |
[32m[20221213 21:16:03 @agent_ppo2.py:185][0m |          -0.0074 |         122.6594 |          17.8466 |
[32m[20221213 21:16:03 @agent_ppo2.py:185][0m |          -0.0023 |         122.2728 |          17.8501 |
[32m[20221213 21:16:03 @agent_ppo2.py:185][0m |          -0.0074 |         121.5314 |          17.8426 |
[32m[20221213 21:16:03 @agent_ppo2.py:185][0m |          -0.0088 |         120.8653 |          17.8385 |
[32m[20221213 21:16:03 @agent_ppo2.py:185][0m |          -0.0117 |         120.6592 |          17.8413 |
[32m[20221213 21:16:03 @agent_ppo2.py:185][0m |          -0.0088 |         120.0256 |          17.8382 |
[32m[20221213 21:16:03 @agent_ppo2.py:185][0m |          -0.0099 |         119.8903 |          17.8357 |
[32m[20221213 21:16:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 628.40
[32m[20221213 21:16:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 709.00
[32m[20221213 21:16:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 694.00
[32m[20221213 21:16:03 @agent_ppo2.py:143][0m Total time:      20.48 min
[32m[20221213 21:16:03 @agent_ppo2.py:145][0m 1998848 total steps have happened
[32m[20221213 21:16:03 @agent_ppo2.py:121][0m #------------------------ Iteration 976 --------------------------#
[32m[20221213 21:16:03 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:16:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:04 @agent_ppo2.py:185][0m |          -0.0000 |         123.4842 |          17.7976 |
[32m[20221213 21:16:04 @agent_ppo2.py:185][0m |          -0.0009 |         121.8936 |          17.7730 |
[32m[20221213 21:16:04 @agent_ppo2.py:185][0m |          -0.0044 |         121.2896 |          17.7707 |
[32m[20221213 21:16:04 @agent_ppo2.py:185][0m |          -0.0025 |         120.7901 |          17.7385 |
[32m[20221213 21:16:04 @agent_ppo2.py:185][0m |          -0.0045 |         120.4176 |          17.7465 |
[32m[20221213 21:16:04 @agent_ppo2.py:185][0m |          -0.0029 |         120.1269 |          17.7505 |
[32m[20221213 21:16:04 @agent_ppo2.py:185][0m |          -0.0035 |         119.8448 |          17.7412 |
[32m[20221213 21:16:04 @agent_ppo2.py:185][0m |          -0.0038 |         119.7190 |          17.7227 |
[32m[20221213 21:16:04 @agent_ppo2.py:185][0m |          -0.0032 |         119.4398 |          17.7191 |
[32m[20221213 21:16:04 @agent_ppo2.py:185][0m |           0.0058 |         127.5591 |          17.7107 |
[32m[20221213 21:16:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 655.20
[32m[20221213 21:16:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.00
[32m[20221213 21:16:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 714.00
[32m[20221213 21:16:04 @agent_ppo2.py:143][0m Total time:      20.50 min
[32m[20221213 21:16:04 @agent_ppo2.py:145][0m 2000896 total steps have happened
[32m[20221213 21:16:04 @agent_ppo2.py:121][0m #------------------------ Iteration 977 --------------------------#
[32m[20221213 21:16:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:05 @agent_ppo2.py:185][0m |           0.0008 |         124.0690 |          17.7808 |
[32m[20221213 21:16:05 @agent_ppo2.py:185][0m |          -0.0053 |         122.2482 |          17.7949 |
[32m[20221213 21:16:05 @agent_ppo2.py:185][0m |          -0.0061 |         121.7660 |          17.7833 |
[32m[20221213 21:16:05 @agent_ppo2.py:185][0m |          -0.0059 |         121.1482 |          17.7838 |
[32m[20221213 21:16:05 @agent_ppo2.py:185][0m |          -0.0067 |         120.8119 |          17.7893 |
[32m[20221213 21:16:05 @agent_ppo2.py:185][0m |          -0.0055 |         120.6578 |          17.8029 |
[32m[20221213 21:16:05 @agent_ppo2.py:185][0m |          -0.0059 |         121.6216 |          17.8112 |
[32m[20221213 21:16:05 @agent_ppo2.py:185][0m |          -0.0093 |         119.8769 |          17.8073 |
[32m[20221213 21:16:05 @agent_ppo2.py:185][0m |          -0.0066 |         119.9628 |          17.8208 |
[32m[20221213 21:16:06 @agent_ppo2.py:185][0m |          -0.0014 |         121.9622 |          17.8326 |
[32m[20221213 21:16:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:16:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 667.00
[32m[20221213 21:16:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 711.00
[32m[20221213 21:16:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 686.00
[32m[20221213 21:16:06 @agent_ppo2.py:143][0m Total time:      20.52 min
[32m[20221213 21:16:06 @agent_ppo2.py:145][0m 2002944 total steps have happened
[32m[20221213 21:16:06 @agent_ppo2.py:121][0m #------------------------ Iteration 978 --------------------------#
[32m[20221213 21:16:06 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:16:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:06 @agent_ppo2.py:185][0m |           0.0007 |         114.2873 |          17.8334 |
[32m[20221213 21:16:06 @agent_ppo2.py:185][0m |          -0.0069 |         108.4918 |          17.8380 |
[32m[20221213 21:16:06 @agent_ppo2.py:185][0m |          -0.0074 |         105.8379 |          17.8242 |
[32m[20221213 21:16:06 @agent_ppo2.py:185][0m |          -0.0024 |         105.1879 |          17.8224 |
[32m[20221213 21:16:06 @agent_ppo2.py:185][0m |          -0.0038 |         103.3823 |          17.8171 |
[32m[20221213 21:16:06 @agent_ppo2.py:185][0m |           0.0015 |         106.8409 |          17.8077 |
[32m[20221213 21:16:07 @agent_ppo2.py:185][0m |          -0.0073 |         101.8368 |          17.8008 |
[32m[20221213 21:16:07 @agent_ppo2.py:185][0m |          -0.0043 |         101.3393 |          17.7998 |
[32m[20221213 21:16:07 @agent_ppo2.py:185][0m |          -0.0061 |         101.0657 |          17.7818 |
[32m[20221213 21:16:07 @agent_ppo2.py:185][0m |          -0.0077 |         100.4415 |          17.7964 |
[32m[20221213 21:16:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 621.60
[32m[20221213 21:16:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 671.00
[32m[20221213 21:16:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 669.00
[32m[20221213 21:16:07 @agent_ppo2.py:143][0m Total time:      20.54 min
[32m[20221213 21:16:07 @agent_ppo2.py:145][0m 2004992 total steps have happened
[32m[20221213 21:16:07 @agent_ppo2.py:121][0m #------------------------ Iteration 979 --------------------------#
[32m[20221213 21:16:07 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:16:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:07 @agent_ppo2.py:185][0m |          -0.0021 |         126.1503 |          17.5921 |
[32m[20221213 21:16:07 @agent_ppo2.py:185][0m |          -0.0031 |         123.8897 |          17.5868 |
[32m[20221213 21:16:07 @agent_ppo2.py:185][0m |          -0.0043 |         123.1299 |          17.5787 |
[32m[20221213 21:16:07 @agent_ppo2.py:185][0m |          -0.0048 |         122.5993 |          17.5504 |
[32m[20221213 21:16:08 @agent_ppo2.py:185][0m |          -0.0041 |         122.1808 |          17.5570 |
[32m[20221213 21:16:08 @agent_ppo2.py:185][0m |          -0.0056 |         121.8869 |          17.5312 |
[32m[20221213 21:16:08 @agent_ppo2.py:185][0m |          -0.0068 |         121.9307 |          17.5277 |
[32m[20221213 21:16:08 @agent_ppo2.py:185][0m |          -0.0006 |         127.8177 |          17.5286 |
[32m[20221213 21:16:08 @agent_ppo2.py:185][0m |          -0.0093 |         121.1349 |          17.5085 |
[32m[20221213 21:16:08 @agent_ppo2.py:185][0m |          -0.0100 |         120.9878 |          17.5101 |
[32m[20221213 21:16:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.00
[32m[20221213 21:16:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 689.00
[32m[20221213 21:16:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 699.00
[32m[20221213 21:16:08 @agent_ppo2.py:143][0m Total time:      20.56 min
[32m[20221213 21:16:08 @agent_ppo2.py:145][0m 2007040 total steps have happened
[32m[20221213 21:16:08 @agent_ppo2.py:121][0m #------------------------ Iteration 980 --------------------------#
[32m[20221213 21:16:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:08 @agent_ppo2.py:185][0m |          -0.0025 |         123.7700 |          17.5855 |
[32m[20221213 21:16:08 @agent_ppo2.py:185][0m |          -0.0024 |         122.7930 |          17.5888 |
[32m[20221213 21:16:09 @agent_ppo2.py:185][0m |          -0.0079 |         121.9442 |          17.5861 |
[32m[20221213 21:16:09 @agent_ppo2.py:185][0m |          -0.0054 |         121.5576 |          17.5844 |
[32m[20221213 21:16:09 @agent_ppo2.py:185][0m |          -0.0085 |         121.1587 |          17.5773 |
[32m[20221213 21:16:09 @agent_ppo2.py:185][0m |          -0.0092 |         121.0002 |          17.5869 |
[32m[20221213 21:16:09 @agent_ppo2.py:185][0m |          -0.0064 |         120.6940 |          17.5748 |
[32m[20221213 21:16:09 @agent_ppo2.py:185][0m |          -0.0060 |         120.9860 |          17.5800 |
[32m[20221213 21:16:09 @agent_ppo2.py:185][0m |          -0.0063 |         120.4674 |          17.5647 |
[32m[20221213 21:16:09 @agent_ppo2.py:185][0m |          -0.0077 |         120.2190 |          17.5706 |
[32m[20221213 21:16:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:16:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 637.20
[32m[20221213 21:16:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 654.00
[32m[20221213 21:16:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 648.00
[32m[20221213 21:16:09 @agent_ppo2.py:143][0m Total time:      20.58 min
[32m[20221213 21:16:09 @agent_ppo2.py:145][0m 2009088 total steps have happened
[32m[20221213 21:16:09 @agent_ppo2.py:121][0m #------------------------ Iteration 981 --------------------------#
[32m[20221213 21:16:09 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:16:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:10 @agent_ppo2.py:185][0m |           0.0039 |         121.2093 |          17.7556 |
[32m[20221213 21:16:10 @agent_ppo2.py:185][0m |          -0.0046 |         119.6642 |          17.7646 |
[32m[20221213 21:16:10 @agent_ppo2.py:185][0m |          -0.0051 |         119.1676 |          17.7526 |
[32m[20221213 21:16:10 @agent_ppo2.py:185][0m |          -0.0044 |         118.7912 |          17.7595 |
[32m[20221213 21:16:10 @agent_ppo2.py:185][0m |          -0.0046 |         118.7166 |          17.7673 |
[32m[20221213 21:16:10 @agent_ppo2.py:185][0m |          -0.0043 |         118.5704 |          17.7733 |
[32m[20221213 21:16:10 @agent_ppo2.py:185][0m |          -0.0094 |         118.3895 |          17.7718 |
[32m[20221213 21:16:10 @agent_ppo2.py:185][0m |          -0.0064 |         118.1014 |          17.7820 |
[32m[20221213 21:16:10 @agent_ppo2.py:185][0m |          -0.0103 |         117.8712 |          17.7801 |
[32m[20221213 21:16:10 @agent_ppo2.py:185][0m |          -0.0008 |         119.6801 |          17.7899 |
[32m[20221213 21:16:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 641.60
[32m[20221213 21:16:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 695.00
[32m[20221213 21:16:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 618.00
[32m[20221213 21:16:10 @agent_ppo2.py:143][0m Total time:      20.60 min
[32m[20221213 21:16:10 @agent_ppo2.py:145][0m 2011136 total steps have happened
[32m[20221213 21:16:10 @agent_ppo2.py:121][0m #------------------------ Iteration 982 --------------------------#
[32m[20221213 21:16:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:11 @agent_ppo2.py:185][0m |           0.0003 |         122.2267 |          17.6288 |
[32m[20221213 21:16:11 @agent_ppo2.py:185][0m |           0.0005 |         123.2925 |          17.6150 |
[32m[20221213 21:16:11 @agent_ppo2.py:185][0m |          -0.0058 |         119.5585 |          17.5928 |
[32m[20221213 21:16:11 @agent_ppo2.py:185][0m |          -0.0079 |         119.0151 |          17.5886 |
[32m[20221213 21:16:11 @agent_ppo2.py:185][0m |          -0.0049 |         118.6939 |          17.5905 |
[32m[20221213 21:16:11 @agent_ppo2.py:185][0m |          -0.0102 |         118.2668 |          17.5753 |
[32m[20221213 21:16:11 @agent_ppo2.py:185][0m |          -0.0021 |         119.9138 |          17.5680 |
[32m[20221213 21:16:11 @agent_ppo2.py:185][0m |          -0.0098 |         117.9397 |          17.5761 |
[32m[20221213 21:16:12 @agent_ppo2.py:185][0m |          -0.0082 |         117.5905 |          17.5588 |
[32m[20221213 21:16:12 @agent_ppo2.py:185][0m |          -0.0046 |         119.4908 |          17.5490 |
[32m[20221213 21:16:12 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:16:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 626.00
[32m[20221213 21:16:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 680.00
[32m[20221213 21:16:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 710.00
[32m[20221213 21:16:12 @agent_ppo2.py:143][0m Total time:      20.62 min
[32m[20221213 21:16:12 @agent_ppo2.py:145][0m 2013184 total steps have happened
[32m[20221213 21:16:12 @agent_ppo2.py:121][0m #------------------------ Iteration 983 --------------------------#
[32m[20221213 21:16:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:12 @agent_ppo2.py:185][0m |           0.0013 |         121.5971 |          17.5751 |
[32m[20221213 21:16:12 @agent_ppo2.py:185][0m |          -0.0037 |         120.3555 |          17.5697 |
[32m[20221213 21:16:12 @agent_ppo2.py:185][0m |          -0.0025 |         119.9660 |          17.5746 |
[32m[20221213 21:16:12 @agent_ppo2.py:185][0m |          -0.0035 |         119.9605 |          17.5804 |
[32m[20221213 21:16:12 @agent_ppo2.py:185][0m |          -0.0000 |         121.0142 |          17.5781 |
[32m[20221213 21:16:12 @agent_ppo2.py:185][0m |          -0.0068 |         119.3103 |          17.5884 |
[32m[20221213 21:16:13 @agent_ppo2.py:185][0m |          -0.0044 |         119.1423 |          17.5986 |
[32m[20221213 21:16:13 @agent_ppo2.py:185][0m |          -0.0086 |         118.8392 |          17.6073 |
[32m[20221213 21:16:13 @agent_ppo2.py:185][0m |          -0.0065 |         118.7139 |          17.6214 |
[32m[20221213 21:16:13 @agent_ppo2.py:185][0m |          -0.0073 |         118.6384 |          17.6225 |
[32m[20221213 21:16:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 651.60
[32m[20221213 21:16:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 702.00
[32m[20221213 21:16:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 609.00
[32m[20221213 21:16:13 @agent_ppo2.py:143][0m Total time:      20.64 min
[32m[20221213 21:16:13 @agent_ppo2.py:145][0m 2015232 total steps have happened
[32m[20221213 21:16:13 @agent_ppo2.py:121][0m #------------------------ Iteration 984 --------------------------#
[32m[20221213 21:16:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:13 @agent_ppo2.py:185][0m |           0.0023 |         121.9068 |          17.6394 |
[32m[20221213 21:16:13 @agent_ppo2.py:185][0m |          -0.0012 |         120.2903 |          17.6489 |
[32m[20221213 21:16:13 @agent_ppo2.py:185][0m |          -0.0063 |         119.4232 |          17.6628 |
[32m[20221213 21:16:13 @agent_ppo2.py:185][0m |          -0.0033 |         119.4395 |          17.6695 |
[32m[20221213 21:16:14 @agent_ppo2.py:185][0m |          -0.0057 |         118.5100 |          17.6666 |
[32m[20221213 21:16:14 @agent_ppo2.py:185][0m |          -0.0011 |         120.1956 |          17.6670 |
[32m[20221213 21:16:14 @agent_ppo2.py:185][0m |          -0.0045 |         118.5312 |          17.6709 |
[32m[20221213 21:16:14 @agent_ppo2.py:185][0m |          -0.0001 |         119.7446 |          17.6668 |
[32m[20221213 21:16:14 @agent_ppo2.py:185][0m |          -0.0082 |         118.0086 |          17.6664 |
[32m[20221213 21:16:14 @agent_ppo2.py:185][0m |          -0.0067 |         117.5102 |          17.6704 |
[32m[20221213 21:16:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 642.00
[32m[20221213 21:16:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 710.00
[32m[20221213 21:16:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 702.00
[32m[20221213 21:16:14 @agent_ppo2.py:143][0m Total time:      20.66 min
[32m[20221213 21:16:14 @agent_ppo2.py:145][0m 2017280 total steps have happened
[32m[20221213 21:16:14 @agent_ppo2.py:121][0m #------------------------ Iteration 985 --------------------------#
[32m[20221213 21:16:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:16:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:14 @agent_ppo2.py:185][0m |          -0.0001 |         125.5797 |          17.8052 |
[32m[20221213 21:16:14 @agent_ppo2.py:185][0m |          -0.0044 |         124.2809 |          17.8031 |
[32m[20221213 21:16:15 @agent_ppo2.py:185][0m |          -0.0074 |         123.8038 |          17.8017 |
[32m[20221213 21:16:15 @agent_ppo2.py:185][0m |          -0.0091 |         123.3199 |          17.8083 |
[32m[20221213 21:16:15 @agent_ppo2.py:185][0m |          -0.0057 |         122.8632 |          17.7967 |
[32m[20221213 21:16:15 @agent_ppo2.py:185][0m |          -0.0069 |         122.6719 |          17.7923 |
[32m[20221213 21:16:15 @agent_ppo2.py:185][0m |          -0.0078 |         122.9678 |          17.7891 |
[32m[20221213 21:16:15 @agent_ppo2.py:185][0m |          -0.0133 |         122.2955 |          17.7893 |
[32m[20221213 21:16:15 @agent_ppo2.py:185][0m |          -0.0092 |         121.9769 |          17.7894 |
[32m[20221213 21:16:15 @agent_ppo2.py:185][0m |          -0.0005 |         125.4205 |          17.7847 |
[32m[20221213 21:16:15 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:16:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 612.80
[32m[20221213 21:16:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 672.00
[32m[20221213 21:16:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 566.00
[32m[20221213 21:16:15 @agent_ppo2.py:143][0m Total time:      20.68 min
[32m[20221213 21:16:15 @agent_ppo2.py:145][0m 2019328 total steps have happened
[32m[20221213 21:16:15 @agent_ppo2.py:121][0m #------------------------ Iteration 986 --------------------------#
[32m[20221213 21:16:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:16 @agent_ppo2.py:185][0m |           0.0092 |         139.0441 |          17.6787 |
[32m[20221213 21:16:16 @agent_ppo2.py:185][0m |          -0.0053 |         120.6559 |          17.6595 |
[32m[20221213 21:16:16 @agent_ppo2.py:185][0m |          -0.0034 |         120.0872 |          17.6488 |
[32m[20221213 21:16:16 @agent_ppo2.py:185][0m |          -0.0074 |         119.6696 |          17.6488 |
[32m[20221213 21:16:16 @agent_ppo2.py:185][0m |           0.0011 |         126.2698 |          17.6360 |
[32m[20221213 21:16:16 @agent_ppo2.py:185][0m |          -0.0089 |         119.1318 |          17.6348 |
[32m[20221213 21:16:16 @agent_ppo2.py:185][0m |          -0.0083 |         118.9196 |          17.6367 |
[32m[20221213 21:16:16 @agent_ppo2.py:185][0m |          -0.0064 |         118.8557 |          17.6424 |
[32m[20221213 21:16:16 @agent_ppo2.py:185][0m |          -0.0098 |         118.5934 |          17.6316 |
[32m[20221213 21:16:16 @agent_ppo2.py:185][0m |          -0.0073 |         118.4837 |          17.6285 |
[32m[20221213 21:16:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 610.00
[32m[20221213 21:16:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 645.00
[32m[20221213 21:16:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 673.00
[32m[20221213 21:16:16 @agent_ppo2.py:143][0m Total time:      20.70 min
[32m[20221213 21:16:16 @agent_ppo2.py:145][0m 2021376 total steps have happened
[32m[20221213 21:16:16 @agent_ppo2.py:121][0m #------------------------ Iteration 987 --------------------------#
[32m[20221213 21:16:17 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:16:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:17 @agent_ppo2.py:185][0m |          -0.0017 |         124.6398 |          17.5496 |
[32m[20221213 21:16:17 @agent_ppo2.py:185][0m |          -0.0040 |         123.3478 |          17.5345 |
[32m[20221213 21:16:17 @agent_ppo2.py:185][0m |          -0.0048 |         122.5341 |          17.5408 |
[32m[20221213 21:16:17 @agent_ppo2.py:185][0m |           0.0015 |         124.4213 |          17.5260 |
[32m[20221213 21:16:17 @agent_ppo2.py:185][0m |          -0.0042 |         122.0200 |          17.5435 |
[32m[20221213 21:16:17 @agent_ppo2.py:185][0m |           0.0010 |         123.1327 |          17.5312 |
[32m[20221213 21:16:17 @agent_ppo2.py:185][0m |          -0.0049 |         121.1419 |          17.5508 |
[32m[20221213 21:16:17 @agent_ppo2.py:185][0m |          -0.0031 |         120.8779 |          17.5477 |
[32m[20221213 21:16:17 @agent_ppo2.py:185][0m |          -0.0042 |         120.9526 |          17.5684 |
[32m[20221213 21:16:18 @agent_ppo2.py:185][0m |          -0.0058 |         120.6390 |          17.5580 |
[32m[20221213 21:16:18 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:16:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 648.40
[32m[20221213 21:16:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 676.00
[32m[20221213 21:16:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 685.00
[32m[20221213 21:16:18 @agent_ppo2.py:143][0m Total time:      20.72 min
[32m[20221213 21:16:18 @agent_ppo2.py:145][0m 2023424 total steps have happened
[32m[20221213 21:16:18 @agent_ppo2.py:121][0m #------------------------ Iteration 988 --------------------------#
[32m[20221213 21:16:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:16:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:18 @agent_ppo2.py:185][0m |          -0.0021 |         120.3503 |          17.6147 |
[32m[20221213 21:16:18 @agent_ppo2.py:185][0m |           0.0014 |         121.2959 |          17.6216 |
[32m[20221213 21:16:18 @agent_ppo2.py:185][0m |          -0.0009 |         121.7039 |          17.6218 |
[32m[20221213 21:16:18 @agent_ppo2.py:185][0m |          -0.0030 |         117.6715 |          17.6151 |
[32m[20221213 21:16:18 @agent_ppo2.py:185][0m |          -0.0066 |         116.7757 |          17.6324 |
[32m[20221213 21:16:18 @agent_ppo2.py:185][0m |          -0.0053 |         116.1603 |          17.6113 |
[32m[20221213 21:16:19 @agent_ppo2.py:185][0m |          -0.0085 |         115.3867 |          17.6106 |
[32m[20221213 21:16:19 @agent_ppo2.py:185][0m |          -0.0084 |         114.7393 |          17.6228 |
[32m[20221213 21:16:19 @agent_ppo2.py:185][0m |          -0.0081 |         114.0677 |          17.6256 |
[32m[20221213 21:16:19 @agent_ppo2.py:185][0m |          -0.0092 |         113.5496 |          17.6114 |
[32m[20221213 21:16:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:16:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 681.00
[32m[20221213 21:16:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 724.00
[32m[20221213 21:16:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 693.00
[32m[20221213 21:16:19 @agent_ppo2.py:143][0m Total time:      20.74 min
[32m[20221213 21:16:19 @agent_ppo2.py:145][0m 2025472 total steps have happened
[32m[20221213 21:16:19 @agent_ppo2.py:121][0m #------------------------ Iteration 989 --------------------------#
[32m[20221213 21:16:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:19 @agent_ppo2.py:185][0m |           0.0016 |         126.2348 |          17.7020 |
[32m[20221213 21:16:19 @agent_ppo2.py:185][0m |          -0.0064 |         120.3785 |          17.6783 |
[32m[20221213 21:16:19 @agent_ppo2.py:185][0m |          -0.0080 |         119.0831 |          17.6707 |
[32m[20221213 21:16:19 @agent_ppo2.py:185][0m |          -0.0052 |         117.5001 |          17.6561 |
[32m[20221213 21:16:20 @agent_ppo2.py:185][0m |          -0.0038 |         117.1729 |          17.6540 |
[32m[20221213 21:16:20 @agent_ppo2.py:185][0m |          -0.0004 |         119.6878 |          17.6443 |
[32m[20221213 21:16:20 @agent_ppo2.py:185][0m |          -0.0117 |         115.7818 |          17.6265 |
[32m[20221213 21:16:20 @agent_ppo2.py:185][0m |          -0.0114 |         115.0629 |          17.6270 |
[32m[20221213 21:16:20 @agent_ppo2.py:185][0m |          -0.0063 |         114.7412 |          17.6109 |
[32m[20221213 21:16:20 @agent_ppo2.py:185][0m |          -0.0099 |         114.1837 |          17.6109 |
[32m[20221213 21:16:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 651.20
[32m[20221213 21:16:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 736.00
[32m[20221213 21:16:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.00
[32m[20221213 21:16:20 @agent_ppo2.py:143][0m Total time:      20.76 min
[32m[20221213 21:16:20 @agent_ppo2.py:145][0m 2027520 total steps have happened
[32m[20221213 21:16:20 @agent_ppo2.py:121][0m #------------------------ Iteration 990 --------------------------#
[32m[20221213 21:16:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:20 @agent_ppo2.py:185][0m |          -0.0024 |         131.7193 |          17.5387 |
[32m[20221213 21:16:20 @agent_ppo2.py:185][0m |          -0.0047 |         130.1498 |          17.5315 |
[32m[20221213 21:16:21 @agent_ppo2.py:185][0m |          -0.0075 |         129.2702 |          17.5461 |
[32m[20221213 21:16:21 @agent_ppo2.py:185][0m |          -0.0058 |         128.6727 |          17.5407 |
[32m[20221213 21:16:21 @agent_ppo2.py:185][0m |          -0.0074 |         128.4024 |          17.5419 |
[32m[20221213 21:16:21 @agent_ppo2.py:185][0m |          -0.0059 |         127.8656 |          17.5426 |
[32m[20221213 21:16:21 @agent_ppo2.py:185][0m |           0.0070 |         139.0026 |          17.5477 |
[32m[20221213 21:16:21 @agent_ppo2.py:185][0m |          -0.0041 |         127.5202 |          17.5478 |
[32m[20221213 21:16:21 @agent_ppo2.py:185][0m |          -0.0066 |         127.0977 |          17.5575 |
[32m[20221213 21:16:21 @agent_ppo2.py:185][0m |          -0.0110 |         127.0108 |          17.5555 |
[32m[20221213 21:16:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 628.00
[32m[20221213 21:16:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 695.00
[32m[20221213 21:16:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 637.00
[32m[20221213 21:16:21 @agent_ppo2.py:143][0m Total time:      20.78 min
[32m[20221213 21:16:21 @agent_ppo2.py:145][0m 2029568 total steps have happened
[32m[20221213 21:16:21 @agent_ppo2.py:121][0m #------------------------ Iteration 991 --------------------------#
[32m[20221213 21:16:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:22 @agent_ppo2.py:185][0m |           0.0119 |         133.7594 |          17.7110 |
[32m[20221213 21:16:22 @agent_ppo2.py:185][0m |          -0.0032 |         121.4314 |          17.6915 |
[32m[20221213 21:16:22 @agent_ppo2.py:185][0m |           0.0035 |         133.4983 |          17.6585 |
[32m[20221213 21:16:22 @agent_ppo2.py:185][0m |          -0.0038 |         119.7413 |          17.6258 |
[32m[20221213 21:16:22 @agent_ppo2.py:185][0m |          -0.0084 |         119.2434 |          17.6249 |
[32m[20221213 21:16:22 @agent_ppo2.py:185][0m |          -0.0094 |         118.9971 |          17.5987 |
[32m[20221213 21:16:22 @agent_ppo2.py:185][0m |          -0.0108 |         118.7388 |          17.6108 |
[32m[20221213 21:16:22 @agent_ppo2.py:185][0m |          -0.0088 |         118.6083 |          17.5841 |
[32m[20221213 21:16:22 @agent_ppo2.py:185][0m |          -0.0034 |         121.0202 |          17.5879 |
[32m[20221213 21:16:22 @agent_ppo2.py:185][0m |          -0.0080 |         118.3308 |          17.5797 |
[32m[20221213 21:16:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 625.40
[32m[20221213 21:16:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 683.00
[32m[20221213 21:16:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 669.00
[32m[20221213 21:16:22 @agent_ppo2.py:143][0m Total time:      20.80 min
[32m[20221213 21:16:22 @agent_ppo2.py:145][0m 2031616 total steps have happened
[32m[20221213 21:16:22 @agent_ppo2.py:121][0m #------------------------ Iteration 992 --------------------------#
[32m[20221213 21:16:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:23 @agent_ppo2.py:185][0m |           0.0031 |         124.4339 |          17.5575 |
[32m[20221213 21:16:23 @agent_ppo2.py:185][0m |           0.0090 |         134.2859 |          17.5579 |
[32m[20221213 21:16:23 @agent_ppo2.py:185][0m |          -0.0046 |         121.0636 |          17.5734 |
[32m[20221213 21:16:23 @agent_ppo2.py:185][0m |          -0.0084 |         120.5168 |          17.5766 |
[32m[20221213 21:16:23 @agent_ppo2.py:185][0m |          -0.0066 |         120.3580 |          17.5771 |
[32m[20221213 21:16:23 @agent_ppo2.py:185][0m |          -0.0085 |         120.3296 |          17.5639 |
[32m[20221213 21:16:23 @agent_ppo2.py:185][0m |          -0.0056 |         121.1939 |          17.5720 |
[32m[20221213 21:16:23 @agent_ppo2.py:185][0m |          -0.0032 |         122.5685 |          17.5735 |
[32m[20221213 21:16:23 @agent_ppo2.py:185][0m |           0.0006 |         125.9022 |          17.5863 |
[32m[20221213 21:16:24 @agent_ppo2.py:185][0m |          -0.0073 |         120.3211 |          17.5907 |
[32m[20221213 21:16:24 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:16:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 649.60
[32m[20221213 21:16:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 702.00
[32m[20221213 21:16:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 618.00
[32m[20221213 21:16:24 @agent_ppo2.py:143][0m Total time:      20.82 min
[32m[20221213 21:16:24 @agent_ppo2.py:145][0m 2033664 total steps have happened
[32m[20221213 21:16:24 @agent_ppo2.py:121][0m #------------------------ Iteration 993 --------------------------#
[32m[20221213 21:16:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:24 @agent_ppo2.py:185][0m |           0.0028 |         122.5834 |          17.5550 |
[32m[20221213 21:16:24 @agent_ppo2.py:185][0m |           0.0063 |         122.0309 |          17.5415 |
[32m[20221213 21:16:24 @agent_ppo2.py:185][0m |          -0.0001 |         119.7199 |          17.5035 |
[32m[20221213 21:16:24 @agent_ppo2.py:185][0m |           0.0075 |         134.4722 |          17.5251 |
[32m[20221213 21:16:24 @agent_ppo2.py:185][0m |          -0.0013 |         118.7792 |          17.5214 |
[32m[20221213 21:16:24 @agent_ppo2.py:185][0m |          -0.0055 |         118.5711 |          17.5130 |
[32m[20221213 21:16:25 @agent_ppo2.py:185][0m |          -0.0069 |         118.3188 |          17.5160 |
[32m[20221213 21:16:25 @agent_ppo2.py:185][0m |          -0.0048 |         118.3929 |          17.5137 |
[32m[20221213 21:16:25 @agent_ppo2.py:185][0m |          -0.0049 |         117.9238 |          17.5091 |
[32m[20221213 21:16:25 @agent_ppo2.py:185][0m |          -0.0068 |         117.8095 |          17.5114 |
[32m[20221213 21:16:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 629.00
[32m[20221213 21:16:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.00
[32m[20221213 21:16:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 668.00
[32m[20221213 21:16:25 @agent_ppo2.py:143][0m Total time:      20.84 min
[32m[20221213 21:16:25 @agent_ppo2.py:145][0m 2035712 total steps have happened
[32m[20221213 21:16:25 @agent_ppo2.py:121][0m #------------------------ Iteration 994 --------------------------#
[32m[20221213 21:16:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:25 @agent_ppo2.py:185][0m |           0.0010 |         120.2179 |          17.4369 |
[32m[20221213 21:16:25 @agent_ppo2.py:185][0m |          -0.0012 |         119.9986 |          17.4139 |
[32m[20221213 21:16:25 @agent_ppo2.py:185][0m |          -0.0032 |         118.4113 |          17.3846 |
[32m[20221213 21:16:25 @agent_ppo2.py:185][0m |           0.0031 |         129.5366 |          17.3695 |
[32m[20221213 21:16:26 @agent_ppo2.py:185][0m |          -0.0072 |         117.5114 |          17.3640 |
[32m[20221213 21:16:26 @agent_ppo2.py:185][0m |           0.0023 |         122.2067 |          17.3629 |
[32m[20221213 21:16:26 @agent_ppo2.py:185][0m |          -0.0074 |         117.0072 |          17.3419 |
[32m[20221213 21:16:26 @agent_ppo2.py:185][0m |          -0.0095 |         116.6278 |          17.3568 |
[32m[20221213 21:16:26 @agent_ppo2.py:185][0m |          -0.0112 |         116.4364 |          17.3502 |
[32m[20221213 21:16:26 @agent_ppo2.py:185][0m |          -0.0125 |         116.3151 |          17.3430 |
[32m[20221213 21:16:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 661.40
[32m[20221213 21:16:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.00
[32m[20221213 21:16:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 662.00
[32m[20221213 21:16:26 @agent_ppo2.py:143][0m Total time:      20.86 min
[32m[20221213 21:16:26 @agent_ppo2.py:145][0m 2037760 total steps have happened
[32m[20221213 21:16:26 @agent_ppo2.py:121][0m #------------------------ Iteration 995 --------------------------#
[32m[20221213 21:16:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:26 @agent_ppo2.py:185][0m |          -0.0039 |         124.8325 |          17.5861 |
[32m[20221213 21:16:26 @agent_ppo2.py:185][0m |           0.0042 |         134.4531 |          17.5665 |
[32m[20221213 21:16:27 @agent_ppo2.py:185][0m |          -0.0051 |         122.9935 |          17.5699 |
[32m[20221213 21:16:27 @agent_ppo2.py:185][0m |          -0.0078 |         122.4057 |          17.5726 |
[32m[20221213 21:16:27 @agent_ppo2.py:185][0m |          -0.0080 |         122.2023 |          17.5649 |
[32m[20221213 21:16:27 @agent_ppo2.py:185][0m |          -0.0084 |         122.1148 |          17.5637 |
[32m[20221213 21:16:27 @agent_ppo2.py:185][0m |          -0.0055 |         121.9366 |          17.5607 |
[32m[20221213 21:16:27 @agent_ppo2.py:185][0m |          -0.0105 |         121.3681 |          17.5637 |
[32m[20221213 21:16:27 @agent_ppo2.py:185][0m |          -0.0112 |         120.9845 |          17.5607 |
[32m[20221213 21:16:27 @agent_ppo2.py:185][0m |          -0.0113 |         120.7892 |          17.5565 |
[32m[20221213 21:16:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 614.40
[32m[20221213 21:16:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 669.00
[32m[20221213 21:16:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.00
[32m[20221213 21:16:27 @agent_ppo2.py:143][0m Total time:      20.88 min
[32m[20221213 21:16:27 @agent_ppo2.py:145][0m 2039808 total steps have happened
[32m[20221213 21:16:27 @agent_ppo2.py:121][0m #------------------------ Iteration 996 --------------------------#
[32m[20221213 21:16:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:28 @agent_ppo2.py:185][0m |           0.0025 |         118.9861 |          17.2203 |
[32m[20221213 21:16:28 @agent_ppo2.py:185][0m |          -0.0042 |         118.0689 |          17.2107 |
[32m[20221213 21:16:28 @agent_ppo2.py:185][0m |          -0.0009 |         117.9816 |          17.2018 |
[32m[20221213 21:16:28 @agent_ppo2.py:185][0m |           0.0053 |         126.1666 |          17.1887 |
[32m[20221213 21:16:28 @agent_ppo2.py:185][0m |          -0.0032 |         116.9736 |          17.1911 |
[32m[20221213 21:16:28 @agent_ppo2.py:185][0m |          -0.0087 |         116.7042 |          17.1790 |
[32m[20221213 21:16:28 @agent_ppo2.py:185][0m |          -0.0042 |         116.4419 |          17.1716 |
[32m[20221213 21:16:28 @agent_ppo2.py:185][0m |          -0.0049 |         116.7449 |          17.1644 |
[32m[20221213 21:16:28 @agent_ppo2.py:185][0m |          -0.0060 |         116.0398 |          17.1459 |
[32m[20221213 21:16:28 @agent_ppo2.py:185][0m |          -0.0076 |         116.1542 |          17.1528 |
[32m[20221213 21:16:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 656.60
[32m[20221213 21:16:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.00
[32m[20221213 21:16:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 729.00
[32m[20221213 21:16:28 @agent_ppo2.py:143][0m Total time:      20.90 min
[32m[20221213 21:16:28 @agent_ppo2.py:145][0m 2041856 total steps have happened
[32m[20221213 21:16:28 @agent_ppo2.py:121][0m #------------------------ Iteration 997 --------------------------#
[32m[20221213 21:16:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:29 @agent_ppo2.py:185][0m |           0.0101 |         128.6042 |          17.3468 |
[32m[20221213 21:16:29 @agent_ppo2.py:185][0m |          -0.0011 |         123.1653 |          17.3453 |
[32m[20221213 21:16:29 @agent_ppo2.py:185][0m |          -0.0035 |         122.3941 |          17.3363 |
[32m[20221213 21:16:29 @agent_ppo2.py:185][0m |          -0.0035 |         121.8610 |          17.3211 |
[32m[20221213 21:16:29 @agent_ppo2.py:185][0m |          -0.0058 |         121.2980 |          17.3074 |
[32m[20221213 21:16:29 @agent_ppo2.py:185][0m |          -0.0071 |         120.9869 |          17.2928 |
[32m[20221213 21:16:29 @agent_ppo2.py:185][0m |          -0.0065 |         120.4996 |          17.2846 |
[32m[20221213 21:16:29 @agent_ppo2.py:185][0m |          -0.0082 |         120.4413 |          17.2800 |
[32m[20221213 21:16:29 @agent_ppo2.py:185][0m |          -0.0075 |         120.1920 |          17.2637 |
[32m[20221213 21:16:30 @agent_ppo2.py:185][0m |          -0.0067 |         119.7740 |          17.2652 |
[32m[20221213 21:16:30 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:16:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 605.80
[32m[20221213 21:16:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.00
[32m[20221213 21:16:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 697.00
[32m[20221213 21:16:30 @agent_ppo2.py:143][0m Total time:      20.92 min
[32m[20221213 21:16:30 @agent_ppo2.py:145][0m 2043904 total steps have happened
[32m[20221213 21:16:30 @agent_ppo2.py:121][0m #------------------------ Iteration 998 --------------------------#
[32m[20221213 21:16:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:16:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:30 @agent_ppo2.py:185][0m |          -0.0009 |         122.7829 |          17.1691 |
[32m[20221213 21:16:30 @agent_ppo2.py:185][0m |          -0.0010 |         121.9123 |          17.1717 |
[32m[20221213 21:16:30 @agent_ppo2.py:185][0m |           0.0006 |         120.6088 |          17.1732 |
[32m[20221213 21:16:30 @agent_ppo2.py:185][0m |          -0.0070 |         119.8505 |          17.1687 |
[32m[20221213 21:16:30 @agent_ppo2.py:185][0m |           0.0008 |         121.9853 |          17.1574 |
[32m[20221213 21:16:30 @agent_ppo2.py:185][0m |          -0.0074 |         119.3710 |          17.1609 |
[32m[20221213 21:16:31 @agent_ppo2.py:185][0m |          -0.0078 |         118.9249 |          17.1508 |
[32m[20221213 21:16:31 @agent_ppo2.py:185][0m |          -0.0101 |         118.7451 |          17.1451 |
[32m[20221213 21:16:31 @agent_ppo2.py:185][0m |          -0.0047 |         119.5583 |          17.1486 |
[32m[20221213 21:16:31 @agent_ppo2.py:185][0m |          -0.0097 |         118.2711 |          17.1439 |
[32m[20221213 21:16:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 662.40
[32m[20221213 21:16:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 704.00
[32m[20221213 21:16:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 652.00
[32m[20221213 21:16:31 @agent_ppo2.py:143][0m Total time:      20.94 min
[32m[20221213 21:16:31 @agent_ppo2.py:145][0m 2045952 total steps have happened
[32m[20221213 21:16:31 @agent_ppo2.py:121][0m #------------------------ Iteration 999 --------------------------#
[32m[20221213 21:16:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:31 @agent_ppo2.py:185][0m |          -0.0014 |         122.1220 |          17.0719 |
[32m[20221213 21:16:31 @agent_ppo2.py:185][0m |          -0.0043 |         121.2748 |          17.0833 |
[32m[20221213 21:16:31 @agent_ppo2.py:185][0m |          -0.0053 |         120.5300 |          17.0958 |
[32m[20221213 21:16:31 @agent_ppo2.py:185][0m |          -0.0081 |         120.4336 |          17.0915 |
[32m[20221213 21:16:32 @agent_ppo2.py:185][0m |          -0.0077 |         119.9985 |          17.0785 |
[32m[20221213 21:16:32 @agent_ppo2.py:185][0m |          -0.0069 |         119.7418 |          17.0835 |
[32m[20221213 21:16:32 @agent_ppo2.py:185][0m |          -0.0031 |         120.2517 |          17.0963 |
[32m[20221213 21:16:32 @agent_ppo2.py:185][0m |          -0.0095 |         119.2638 |          17.0872 |
[32m[20221213 21:16:32 @agent_ppo2.py:185][0m |          -0.0106 |         118.9014 |          17.0883 |
[32m[20221213 21:16:32 @agent_ppo2.py:185][0m |          -0.0093 |         118.6766 |          17.0989 |
[32m[20221213 21:16:32 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:16:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 654.00
[32m[20221213 21:16:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 673.00
[32m[20221213 21:16:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 653.00
[32m[20221213 21:16:32 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 771.00
[32m[20221213 21:16:32 @agent_ppo2.py:143][0m Total time:      20.96 min
[32m[20221213 21:16:32 @agent_ppo2.py:145][0m 2048000 total steps have happened
[32m[20221213 21:16:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1000 --------------------------#
[32m[20221213 21:16:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:32 @agent_ppo2.py:185][0m |           0.0005 |         125.1577 |          17.1140 |
[32m[20221213 21:16:33 @agent_ppo2.py:185][0m |           0.0043 |         127.2237 |          17.1166 |
[32m[20221213 21:16:33 @agent_ppo2.py:185][0m |          -0.0057 |         122.6931 |          17.1302 |
[32m[20221213 21:16:33 @agent_ppo2.py:185][0m |          -0.0068 |         122.2238 |          17.1411 |
[32m[20221213 21:16:33 @agent_ppo2.py:185][0m |          -0.0054 |         121.7383 |          17.1417 |
[32m[20221213 21:16:33 @agent_ppo2.py:185][0m |          -0.0034 |         121.8165 |          17.1522 |
[32m[20221213 21:16:33 @agent_ppo2.py:185][0m |          -0.0080 |         120.9686 |          17.1511 |
[32m[20221213 21:16:33 @agent_ppo2.py:185][0m |          -0.0093 |         120.7833 |          17.1623 |
[32m[20221213 21:16:33 @agent_ppo2.py:185][0m |          -0.0108 |         120.5745 |          17.1528 |
[32m[20221213 21:16:33 @agent_ppo2.py:185][0m |          -0.0064 |         120.2447 |          17.1763 |
[32m[20221213 21:16:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 622.40
[32m[20221213 21:16:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 678.00
[32m[20221213 21:16:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 721.00
[32m[20221213 21:16:33 @agent_ppo2.py:143][0m Total time:      20.98 min
[32m[20221213 21:16:33 @agent_ppo2.py:145][0m 2050048 total steps have happened
[32m[20221213 21:16:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1001 --------------------------#
[32m[20221213 21:16:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:16:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:34 @agent_ppo2.py:185][0m |           0.0025 |         129.1123 |          17.2389 |
[32m[20221213 21:16:34 @agent_ppo2.py:185][0m |          -0.0048 |         125.6326 |          17.2128 |
[32m[20221213 21:16:34 @agent_ppo2.py:185][0m |          -0.0044 |         124.7882 |          17.1966 |
[32m[20221213 21:16:34 @agent_ppo2.py:185][0m |          -0.0006 |         128.0638 |          17.1724 |
[32m[20221213 21:16:34 @agent_ppo2.py:185][0m |          -0.0048 |         122.3903 |          17.1693 |
[32m[20221213 21:16:34 @agent_ppo2.py:185][0m |          -0.0109 |         122.1233 |          17.1543 |
[32m[20221213 21:16:34 @agent_ppo2.py:185][0m |          -0.0012 |         126.6121 |          17.1285 |
[32m[20221213 21:16:34 @agent_ppo2.py:185][0m |          -0.0102 |         120.8693 |          17.1091 |
[32m[20221213 21:16:34 @agent_ppo2.py:185][0m |          -0.0090 |         120.5561 |          17.0980 |
[32m[20221213 21:16:34 @agent_ppo2.py:185][0m |          -0.0069 |         120.1053 |          17.0942 |
[32m[20221213 21:16:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 605.80
[32m[20221213 21:16:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.00
[32m[20221213 21:16:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 634.00
[32m[20221213 21:16:35 @agent_ppo2.py:143][0m Total time:      21.00 min
[32m[20221213 21:16:35 @agent_ppo2.py:145][0m 2052096 total steps have happened
[32m[20221213 21:16:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1002 --------------------------#
[32m[20221213 21:16:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:35 @agent_ppo2.py:185][0m |          -0.0005 |         125.1477 |          17.0845 |
[32m[20221213 21:16:35 @agent_ppo2.py:185][0m |          -0.0057 |         123.8637 |          17.0642 |
[32m[20221213 21:16:35 @agent_ppo2.py:185][0m |          -0.0050 |         123.8593 |          17.0514 |
[32m[20221213 21:16:35 @agent_ppo2.py:185][0m |          -0.0067 |         123.0354 |          17.0593 |
[32m[20221213 21:16:35 @agent_ppo2.py:185][0m |          -0.0023 |         124.6693 |          17.0417 |
[32m[20221213 21:16:35 @agent_ppo2.py:185][0m |          -0.0066 |         122.7259 |          17.0452 |
[32m[20221213 21:16:35 @agent_ppo2.py:185][0m |           0.0009 |         130.3137 |          17.0308 |
[32m[20221213 21:16:35 @agent_ppo2.py:185][0m |          -0.0090 |         121.6897 |          17.0339 |
[32m[20221213 21:16:36 @agent_ppo2.py:185][0m |          -0.0087 |         121.7145 |          17.0261 |
[32m[20221213 21:16:36 @agent_ppo2.py:185][0m |          -0.0123 |         121.1402 |          17.0287 |
[32m[20221213 21:16:36 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:16:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 644.80
[32m[20221213 21:16:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 718.00
[32m[20221213 21:16:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 705.00
[32m[20221213 21:16:36 @agent_ppo2.py:143][0m Total time:      21.02 min
[32m[20221213 21:16:36 @agent_ppo2.py:145][0m 2054144 total steps have happened
[32m[20221213 21:16:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1003 --------------------------#
[32m[20221213 21:16:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:36 @agent_ppo2.py:185][0m |          -0.0011 |         125.3055 |          17.0068 |
[32m[20221213 21:16:36 @agent_ppo2.py:185][0m |          -0.0077 |         123.6145 |          16.9980 |
[32m[20221213 21:16:36 @agent_ppo2.py:185][0m |          -0.0075 |         122.2867 |          17.0069 |
[32m[20221213 21:16:36 @agent_ppo2.py:185][0m |          -0.0080 |         121.3727 |          17.0124 |
[32m[20221213 21:16:36 @agent_ppo2.py:185][0m |          -0.0100 |         120.9591 |          17.0287 |
[32m[20221213 21:16:36 @agent_ppo2.py:185][0m |          -0.0082 |         120.3208 |          17.0271 |
[32m[20221213 21:16:37 @agent_ppo2.py:185][0m |          -0.0089 |         120.1352 |          17.0267 |
[32m[20221213 21:16:37 @agent_ppo2.py:185][0m |          -0.0070 |         119.6826 |          17.0329 |
[32m[20221213 21:16:37 @agent_ppo2.py:185][0m |          -0.0096 |         119.2293 |          17.0511 |
[32m[20221213 21:16:37 @agent_ppo2.py:185][0m |          -0.0106 |         118.8749 |          17.0540 |
[32m[20221213 21:16:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 643.80
[32m[20221213 21:16:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.00
[32m[20221213 21:16:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 591.00
[32m[20221213 21:16:37 @agent_ppo2.py:143][0m Total time:      21.04 min
[32m[20221213 21:16:37 @agent_ppo2.py:145][0m 2056192 total steps have happened
[32m[20221213 21:16:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1004 --------------------------#
[32m[20221213 21:16:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:37 @agent_ppo2.py:185][0m |          -0.0008 |         126.8516 |          17.1030 |
[32m[20221213 21:16:37 @agent_ppo2.py:185][0m |          -0.0061 |         125.5108 |          17.0780 |
[32m[20221213 21:16:37 @agent_ppo2.py:185][0m |          -0.0034 |         124.4683 |          17.0694 |
[32m[20221213 21:16:38 @agent_ppo2.py:185][0m |          -0.0081 |         123.7987 |          17.0705 |
[32m[20221213 21:16:38 @agent_ppo2.py:185][0m |          -0.0054 |         123.3341 |          17.0739 |
[32m[20221213 21:16:38 @agent_ppo2.py:185][0m |          -0.0017 |         124.4647 |          17.0686 |
[32m[20221213 21:16:38 @agent_ppo2.py:185][0m |          -0.0048 |         122.5004 |          17.0659 |
[32m[20221213 21:16:38 @agent_ppo2.py:185][0m |          -0.0080 |         122.6457 |          17.0686 |
[32m[20221213 21:16:38 @agent_ppo2.py:185][0m |          -0.0088 |         122.0235 |          17.0712 |
[32m[20221213 21:16:38 @agent_ppo2.py:185][0m |          -0.0062 |         124.8069 |          17.0670 |
[32m[20221213 21:16:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 613.80
[32m[20221213 21:16:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 663.00
[32m[20221213 21:16:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 664.00
[32m[20221213 21:16:38 @agent_ppo2.py:143][0m Total time:      21.06 min
[32m[20221213 21:16:38 @agent_ppo2.py:145][0m 2058240 total steps have happened
[32m[20221213 21:16:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1005 --------------------------#
[32m[20221213 21:16:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:38 @agent_ppo2.py:185][0m |           0.0008 |         119.4489 |          17.0534 |
[32m[20221213 21:16:39 @agent_ppo2.py:185][0m |          -0.0028 |         117.4846 |          17.0407 |
[32m[20221213 21:16:39 @agent_ppo2.py:185][0m |          -0.0014 |         116.9104 |          17.0398 |
[32m[20221213 21:16:39 @agent_ppo2.py:185][0m |          -0.0058 |         116.3493 |          17.0439 |
[32m[20221213 21:16:39 @agent_ppo2.py:185][0m |          -0.0050 |         116.2233 |          17.0190 |
[32m[20221213 21:16:39 @agent_ppo2.py:185][0m |          -0.0023 |         116.4646 |          17.0057 |
[32m[20221213 21:16:39 @agent_ppo2.py:185][0m |          -0.0066 |         115.6175 |          17.0198 |
[32m[20221213 21:16:39 @agent_ppo2.py:185][0m |          -0.0084 |         115.4659 |          17.0000 |
[32m[20221213 21:16:39 @agent_ppo2.py:185][0m |          -0.0065 |         115.1701 |          17.0063 |
[32m[20221213 21:16:39 @agent_ppo2.py:185][0m |          -0.0071 |         115.0149 |          16.9974 |
[32m[20221213 21:16:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 636.20
[32m[20221213 21:16:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 706.00
[32m[20221213 21:16:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 647.00
[32m[20221213 21:16:39 @agent_ppo2.py:143][0m Total time:      21.08 min
[32m[20221213 21:16:39 @agent_ppo2.py:145][0m 2060288 total steps have happened
[32m[20221213 21:16:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1006 --------------------------#
[32m[20221213 21:16:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:40 @agent_ppo2.py:185][0m |           0.0073 |         127.7194 |          16.9966 |
[32m[20221213 21:16:40 @agent_ppo2.py:185][0m |          -0.0036 |         121.2662 |          16.9935 |
[32m[20221213 21:16:40 @agent_ppo2.py:185][0m |          -0.0051 |         120.1144 |          16.9733 |
[32m[20221213 21:16:40 @agent_ppo2.py:185][0m |          -0.0055 |         119.5854 |          16.9686 |
[32m[20221213 21:16:40 @agent_ppo2.py:185][0m |          -0.0077 |         119.2717 |          16.9755 |
[32m[20221213 21:16:40 @agent_ppo2.py:185][0m |          -0.0085 |         118.9742 |          16.9584 |
[32m[20221213 21:16:40 @agent_ppo2.py:185][0m |          -0.0078 |         118.9342 |          16.9564 |
[32m[20221213 21:16:40 @agent_ppo2.py:185][0m |          -0.0083 |         118.5314 |          16.9466 |
[32m[20221213 21:16:40 @agent_ppo2.py:185][0m |          -0.0103 |         118.3801 |          16.9450 |
[32m[20221213 21:16:40 @agent_ppo2.py:185][0m |          -0.0071 |         118.3716 |          16.9328 |
[32m[20221213 21:16:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:16:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 674.20
[32m[20221213 21:16:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.00
[32m[20221213 21:16:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.00
[32m[20221213 21:16:41 @agent_ppo2.py:143][0m Total time:      21.10 min
[32m[20221213 21:16:41 @agent_ppo2.py:145][0m 2062336 total steps have happened
[32m[20221213 21:16:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1007 --------------------------#
[32m[20221213 21:16:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:41 @agent_ppo2.py:185][0m |          -0.0008 |         125.0816 |          17.0107 |
[32m[20221213 21:16:41 @agent_ppo2.py:185][0m |           0.0015 |         127.8215 |          16.9865 |
[32m[20221213 21:16:41 @agent_ppo2.py:185][0m |          -0.0036 |         123.2568 |          17.0010 |
[32m[20221213 21:16:41 @agent_ppo2.py:185][0m |          -0.0079 |         122.8013 |          17.0283 |
[32m[20221213 21:16:41 @agent_ppo2.py:185][0m |          -0.0027 |         123.6731 |          17.0366 |
[32m[20221213 21:16:41 @agent_ppo2.py:185][0m |          -0.0085 |         122.2116 |          17.0297 |
[32m[20221213 21:16:41 @agent_ppo2.py:185][0m |          -0.0013 |         125.0876 |          17.0355 |
[32m[20221213 21:16:41 @agent_ppo2.py:185][0m |          -0.0009 |         127.0375 |          17.0335 |
[32m[20221213 21:16:42 @agent_ppo2.py:185][0m |          -0.0060 |         121.6626 |          17.0438 |
[32m[20221213 21:16:42 @agent_ppo2.py:185][0m |          -0.0095 |         121.3143 |          17.0467 |
[32m[20221213 21:16:42 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:16:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 639.00
[32m[20221213 21:16:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 672.00
[32m[20221213 21:16:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 735.00
[32m[20221213 21:16:42 @agent_ppo2.py:143][0m Total time:      21.12 min
[32m[20221213 21:16:42 @agent_ppo2.py:145][0m 2064384 total steps have happened
[32m[20221213 21:16:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1008 --------------------------#
[32m[20221213 21:16:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:42 @agent_ppo2.py:185][0m |          -0.0031 |         121.9524 |          16.9628 |
[32m[20221213 21:16:42 @agent_ppo2.py:185][0m |          -0.0006 |         120.8216 |          16.9335 |
[32m[20221213 21:16:42 @agent_ppo2.py:185][0m |          -0.0018 |         121.9843 |          16.9344 |
[32m[20221213 21:16:42 @agent_ppo2.py:185][0m |          -0.0074 |         119.0829 |          16.9033 |
[32m[20221213 21:16:42 @agent_ppo2.py:185][0m |          -0.0083 |         118.5998 |          16.8785 |
[32m[20221213 21:16:43 @agent_ppo2.py:185][0m |          -0.0065 |         119.0146 |          16.9018 |
[32m[20221213 21:16:43 @agent_ppo2.py:185][0m |          -0.0088 |         118.0957 |          16.8599 |
[32m[20221213 21:16:43 @agent_ppo2.py:185][0m |          -0.0041 |         118.4868 |          16.8360 |
[32m[20221213 21:16:43 @agent_ppo2.py:185][0m |          -0.0096 |         117.6862 |          16.8480 |
[32m[20221213 21:16:43 @agent_ppo2.py:185][0m |          -0.0072 |         119.7415 |          16.8440 |
[32m[20221213 21:16:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 622.60
[32m[20221213 21:16:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 638.00
[32m[20221213 21:16:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 699.00
[32m[20221213 21:16:43 @agent_ppo2.py:143][0m Total time:      21.14 min
[32m[20221213 21:16:43 @agent_ppo2.py:145][0m 2066432 total steps have happened
[32m[20221213 21:16:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1009 --------------------------#
[32m[20221213 21:16:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:43 @agent_ppo2.py:185][0m |           0.0046 |         124.2952 |          16.9825 |
[32m[20221213 21:16:43 @agent_ppo2.py:185][0m |          -0.0039 |         118.4014 |          17.0055 |
[32m[20221213 21:16:43 @agent_ppo2.py:185][0m |          -0.0048 |         117.9021 |          17.0054 |
[32m[20221213 21:16:44 @agent_ppo2.py:185][0m |          -0.0057 |         117.3192 |          17.0286 |
[32m[20221213 21:16:44 @agent_ppo2.py:185][0m |          -0.0076 |         117.1870 |          17.0292 |
[32m[20221213 21:16:44 @agent_ppo2.py:185][0m |          -0.0064 |         116.6597 |          17.0410 |
[32m[20221213 21:16:44 @agent_ppo2.py:185][0m |          -0.0078 |         116.5761 |          17.0550 |
[32m[20221213 21:16:44 @agent_ppo2.py:185][0m |          -0.0053 |         116.4251 |          17.0547 |
[32m[20221213 21:16:44 @agent_ppo2.py:185][0m |          -0.0072 |         116.1996 |          17.0552 |
[32m[20221213 21:16:44 @agent_ppo2.py:185][0m |          -0.0107 |         116.1802 |          17.0667 |
[32m[20221213 21:16:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:16:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.60
[32m[20221213 21:16:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 645.00
[32m[20221213 21:16:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 626.00
[32m[20221213 21:16:44 @agent_ppo2.py:143][0m Total time:      21.16 min
[32m[20221213 21:16:44 @agent_ppo2.py:145][0m 2068480 total steps have happened
[32m[20221213 21:16:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1010 --------------------------#
[32m[20221213 21:16:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:44 @agent_ppo2.py:185][0m |           0.0020 |         120.9057 |          17.0206 |
[32m[20221213 21:16:45 @agent_ppo2.py:185][0m |          -0.0041 |         119.3841 |          17.0433 |
[32m[20221213 21:16:45 @agent_ppo2.py:185][0m |          -0.0045 |         118.8098 |          17.0174 |
[32m[20221213 21:16:45 @agent_ppo2.py:185][0m |          -0.0077 |         118.1377 |          17.0272 |
[32m[20221213 21:16:45 @agent_ppo2.py:185][0m |          -0.0090 |         118.1052 |          17.0207 |
[32m[20221213 21:16:45 @agent_ppo2.py:185][0m |          -0.0078 |         117.5943 |          17.0142 |
[32m[20221213 21:16:45 @agent_ppo2.py:185][0m |           0.0004 |         121.1684 |          17.0168 |
[32m[20221213 21:16:45 @agent_ppo2.py:185][0m |          -0.0074 |         117.2952 |          17.0101 |
[32m[20221213 21:16:45 @agent_ppo2.py:185][0m |          -0.0082 |         117.0321 |          17.0047 |
[32m[20221213 21:16:45 @agent_ppo2.py:185][0m |          -0.0114 |         117.0683 |          17.0118 |
[32m[20221213 21:16:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 639.40
[32m[20221213 21:16:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 656.00
[32m[20221213 21:16:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 639.00
[32m[20221213 21:16:45 @agent_ppo2.py:143][0m Total time:      21.18 min
[32m[20221213 21:16:45 @agent_ppo2.py:145][0m 2070528 total steps have happened
[32m[20221213 21:16:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1011 --------------------------#
[32m[20221213 21:16:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:16:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:46 @agent_ppo2.py:185][0m |          -0.0034 |         120.6236 |          17.0605 |
[32m[20221213 21:16:46 @agent_ppo2.py:185][0m |          -0.0032 |         118.5775 |          17.0458 |
[32m[20221213 21:16:46 @agent_ppo2.py:185][0m |          -0.0054 |         117.4831 |          17.0252 |
[32m[20221213 21:16:46 @agent_ppo2.py:185][0m |          -0.0057 |         117.1668 |          17.0145 |
[32m[20221213 21:16:46 @agent_ppo2.py:185][0m |          -0.0079 |         116.7436 |          17.0286 |
[32m[20221213 21:16:46 @agent_ppo2.py:185][0m |          -0.0055 |         116.3664 |          17.0078 |
[32m[20221213 21:16:46 @agent_ppo2.py:185][0m |          -0.0069 |         116.1248 |          17.0085 |
[32m[20221213 21:16:46 @agent_ppo2.py:185][0m |          -0.0073 |         116.0538 |          17.0125 |
[32m[20221213 21:16:46 @agent_ppo2.py:185][0m |          -0.0028 |         117.1223 |          16.9972 |
[32m[20221213 21:16:46 @agent_ppo2.py:185][0m |          -0.0082 |         115.7435 |          17.0066 |
[32m[20221213 21:16:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:16:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 647.20
[32m[20221213 21:16:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 727.00
[32m[20221213 21:16:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 710.00
[32m[20221213 21:16:47 @agent_ppo2.py:143][0m Total time:      21.20 min
[32m[20221213 21:16:47 @agent_ppo2.py:145][0m 2072576 total steps have happened
[32m[20221213 21:16:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1012 --------------------------#
[32m[20221213 21:16:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:47 @agent_ppo2.py:185][0m |           0.0017 |         127.5556 |          17.0214 |
[32m[20221213 21:16:47 @agent_ppo2.py:185][0m |          -0.0014 |         125.3699 |          17.0368 |
[32m[20221213 21:16:47 @agent_ppo2.py:185][0m |          -0.0035 |         124.4168 |          17.0271 |
[32m[20221213 21:16:47 @agent_ppo2.py:185][0m |          -0.0008 |         125.7873 |          17.0118 |
[32m[20221213 21:16:47 @agent_ppo2.py:185][0m |           0.0069 |         136.9793 |          17.0223 |
[32m[20221213 21:16:47 @agent_ppo2.py:185][0m |          -0.0048 |         122.9943 |          16.9927 |
[32m[20221213 21:16:47 @agent_ppo2.py:185][0m |          -0.0066 |         122.4567 |          17.0159 |
[32m[20221213 21:16:48 @agent_ppo2.py:185][0m |          -0.0065 |         122.3941 |          17.0118 |
[32m[20221213 21:16:48 @agent_ppo2.py:185][0m |          -0.0078 |         121.9809 |          16.9969 |
[32m[20221213 21:16:48 @agent_ppo2.py:185][0m |          -0.0084 |         122.0100 |          16.9827 |
[32m[20221213 21:16:48 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:16:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 628.60
[32m[20221213 21:16:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 706.00
[32m[20221213 21:16:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 682.00
[32m[20221213 21:16:48 @agent_ppo2.py:143][0m Total time:      21.22 min
[32m[20221213 21:16:48 @agent_ppo2.py:145][0m 2074624 total steps have happened
[32m[20221213 21:16:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1013 --------------------------#
[32m[20221213 21:16:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:48 @agent_ppo2.py:185][0m |          -0.0027 |         122.9603 |          16.8664 |
[32m[20221213 21:16:48 @agent_ppo2.py:185][0m |          -0.0015 |         120.7620 |          16.8579 |
[32m[20221213 21:16:48 @agent_ppo2.py:185][0m |          -0.0044 |         119.8512 |          16.8635 |
[32m[20221213 21:16:48 @agent_ppo2.py:185][0m |           0.0026 |         123.2436 |          16.8245 |
[32m[20221213 21:16:48 @agent_ppo2.py:185][0m |          -0.0044 |         119.0215 |          16.8327 |
[32m[20221213 21:16:49 @agent_ppo2.py:185][0m |          -0.0080 |         118.4426 |          16.8045 |
[32m[20221213 21:16:49 @agent_ppo2.py:185][0m |          -0.0023 |         120.1237 |          16.8020 |
[32m[20221213 21:16:49 @agent_ppo2.py:185][0m |          -0.0083 |         118.1310 |          16.7895 |
[32m[20221213 21:16:49 @agent_ppo2.py:185][0m |          -0.0091 |         117.7257 |          16.7728 |
[32m[20221213 21:16:49 @agent_ppo2.py:185][0m |          -0.0096 |         117.2369 |          16.7947 |
[32m[20221213 21:16:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:16:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 608.00
[32m[20221213 21:16:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 657.00
[32m[20221213 21:16:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 653.00
[32m[20221213 21:16:49 @agent_ppo2.py:143][0m Total time:      21.24 min
[32m[20221213 21:16:49 @agent_ppo2.py:145][0m 2076672 total steps have happened
[32m[20221213 21:16:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1014 --------------------------#
[32m[20221213 21:16:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:16:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:49 @agent_ppo2.py:185][0m |          -0.0014 |         128.7986 |          16.9996 |
[32m[20221213 21:16:49 @agent_ppo2.py:185][0m |          -0.0044 |         125.6960 |          17.0158 |
[32m[20221213 21:16:50 @agent_ppo2.py:185][0m |          -0.0035 |         124.0024 |          17.0307 |
[32m[20221213 21:16:50 @agent_ppo2.py:185][0m |          -0.0070 |         122.8684 |          17.0505 |
[32m[20221213 21:16:50 @agent_ppo2.py:185][0m |          -0.0069 |         122.6462 |          17.0586 |
[32m[20221213 21:16:50 @agent_ppo2.py:185][0m |          -0.0078 |         121.6925 |          17.0674 |
[32m[20221213 21:16:50 @agent_ppo2.py:185][0m |          -0.0022 |         122.7351 |          17.0869 |
[32m[20221213 21:16:50 @agent_ppo2.py:185][0m |          -0.0077 |         120.5857 |          17.0853 |
[32m[20221213 21:16:50 @agent_ppo2.py:185][0m |          -0.0061 |         119.9836 |          17.0952 |
[32m[20221213 21:16:50 @agent_ppo2.py:185][0m |          -0.0065 |         120.0551 |          17.1010 |
[32m[20221213 21:16:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.40
[32m[20221213 21:16:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 679.00
[32m[20221213 21:16:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 588.00
[32m[20221213 21:16:50 @agent_ppo2.py:143][0m Total time:      21.26 min
[32m[20221213 21:16:50 @agent_ppo2.py:145][0m 2078720 total steps have happened
[32m[20221213 21:16:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1015 --------------------------#
[32m[20221213 21:16:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:51 @agent_ppo2.py:185][0m |          -0.0008 |         124.5136 |          16.9847 |
[32m[20221213 21:16:51 @agent_ppo2.py:185][0m |          -0.0060 |         121.8156 |          16.9660 |
[32m[20221213 21:16:51 @agent_ppo2.py:185][0m |          -0.0073 |         121.3132 |          16.9570 |
[32m[20221213 21:16:51 @agent_ppo2.py:185][0m |          -0.0018 |         124.3461 |          16.9499 |
[32m[20221213 21:16:51 @agent_ppo2.py:185][0m |          -0.0066 |         120.6143 |          16.9612 |
[32m[20221213 21:16:51 @agent_ppo2.py:185][0m |          -0.0079 |         120.2443 |          16.9502 |
[32m[20221213 21:16:51 @agent_ppo2.py:185][0m |          -0.0091 |         119.9573 |          16.9340 |
[32m[20221213 21:16:51 @agent_ppo2.py:185][0m |          -0.0064 |         119.7486 |          16.9558 |
[32m[20221213 21:16:51 @agent_ppo2.py:185][0m |          -0.0106 |         119.4263 |          16.9396 |
[32m[20221213 21:16:51 @agent_ppo2.py:185][0m |          -0.0131 |         119.5326 |          16.9341 |
[32m[20221213 21:16:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 656.20
[32m[20221213 21:16:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 729.00
[32m[20221213 21:16:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.00
[32m[20221213 21:16:51 @agent_ppo2.py:143][0m Total time:      21.28 min
[32m[20221213 21:16:51 @agent_ppo2.py:145][0m 2080768 total steps have happened
[32m[20221213 21:16:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1016 --------------------------#
[32m[20221213 21:16:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:52 @agent_ppo2.py:185][0m |           0.0002 |         126.4494 |          16.7922 |
[32m[20221213 21:16:52 @agent_ppo2.py:185][0m |          -0.0041 |         124.2906 |          16.7826 |
[32m[20221213 21:16:52 @agent_ppo2.py:185][0m |          -0.0019 |         123.6993 |          16.8037 |
[32m[20221213 21:16:52 @agent_ppo2.py:185][0m |          -0.0021 |         122.7086 |          16.7997 |
[32m[20221213 21:16:52 @agent_ppo2.py:185][0m |          -0.0070 |         121.9705 |          16.7976 |
[32m[20221213 21:16:52 @agent_ppo2.py:185][0m |          -0.0052 |         121.8059 |          16.7913 |
[32m[20221213 21:16:52 @agent_ppo2.py:185][0m |           0.0023 |         131.7987 |          16.8074 |
[32m[20221213 21:16:52 @agent_ppo2.py:185][0m |          -0.0065 |         121.1716 |          16.7976 |
[32m[20221213 21:16:52 @agent_ppo2.py:185][0m |          -0.0053 |         121.0061 |          16.8085 |
[32m[20221213 21:16:52 @agent_ppo2.py:185][0m |          -0.0090 |         120.4148 |          16.8038 |
[32m[20221213 21:16:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 657.60
[32m[20221213 21:16:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 687.00
[32m[20221213 21:16:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 669.00
[32m[20221213 21:16:53 @agent_ppo2.py:143][0m Total time:      21.30 min
[32m[20221213 21:16:53 @agent_ppo2.py:145][0m 2082816 total steps have happened
[32m[20221213 21:16:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1017 --------------------------#
[32m[20221213 21:16:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:53 @agent_ppo2.py:185][0m |          -0.0015 |         122.2111 |          16.8736 |
[32m[20221213 21:16:53 @agent_ppo2.py:185][0m |          -0.0043 |         119.6795 |          16.8351 |
[32m[20221213 21:16:53 @agent_ppo2.py:185][0m |          -0.0063 |         118.4944 |          16.8331 |
[32m[20221213 21:16:53 @agent_ppo2.py:185][0m |          -0.0090 |         117.9868 |          16.8120 |
[32m[20221213 21:16:53 @agent_ppo2.py:185][0m |          -0.0065 |         117.5908 |          16.8039 |
[32m[20221213 21:16:53 @agent_ppo2.py:185][0m |          -0.0072 |         117.1415 |          16.8041 |
[32m[20221213 21:16:53 @agent_ppo2.py:185][0m |          -0.0097 |         116.9711 |          16.7845 |
[32m[20221213 21:16:54 @agent_ppo2.py:185][0m |          -0.0106 |         116.8525 |          16.7811 |
[32m[20221213 21:16:54 @agent_ppo2.py:185][0m |          -0.0077 |         116.6360 |          16.7745 |
[32m[20221213 21:16:54 @agent_ppo2.py:185][0m |          -0.0031 |         118.7891 |          16.7661 |
[32m[20221213 21:16:54 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:16:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 659.00
[32m[20221213 21:16:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 729.00
[32m[20221213 21:16:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 715.00
[32m[20221213 21:16:54 @agent_ppo2.py:143][0m Total time:      21.32 min
[32m[20221213 21:16:54 @agent_ppo2.py:145][0m 2084864 total steps have happened
[32m[20221213 21:16:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1018 --------------------------#
[32m[20221213 21:16:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:54 @agent_ppo2.py:185][0m |          -0.0001 |         121.9766 |          16.9500 |
[32m[20221213 21:16:54 @agent_ppo2.py:185][0m |          -0.0045 |         121.0054 |          16.9503 |
[32m[20221213 21:16:54 @agent_ppo2.py:185][0m |          -0.0090 |         120.5465 |          16.9571 |
[32m[20221213 21:16:54 @agent_ppo2.py:185][0m |          -0.0076 |         120.0605 |          16.9485 |
[32m[20221213 21:16:54 @agent_ppo2.py:185][0m |          -0.0091 |         120.0128 |          16.9543 |
[32m[20221213 21:16:55 @agent_ppo2.py:185][0m |          -0.0074 |         119.6991 |          16.9642 |
[32m[20221213 21:16:55 @agent_ppo2.py:185][0m |          -0.0053 |         119.5016 |          16.9723 |
[32m[20221213 21:16:55 @agent_ppo2.py:185][0m |          -0.0079 |         119.4183 |          17.0037 |
[32m[20221213 21:16:55 @agent_ppo2.py:185][0m |          -0.0095 |         119.1768 |          16.9979 |
[32m[20221213 21:16:55 @agent_ppo2.py:185][0m |          -0.0082 |         119.0605 |          17.0097 |
[32m[20221213 21:16:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 666.20
[32m[20221213 21:16:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 702.00
[32m[20221213 21:16:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 643.00
[32m[20221213 21:16:55 @agent_ppo2.py:143][0m Total time:      21.34 min
[32m[20221213 21:16:55 @agent_ppo2.py:145][0m 2086912 total steps have happened
[32m[20221213 21:16:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1019 --------------------------#
[32m[20221213 21:16:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:55 @agent_ppo2.py:185][0m |           0.0003 |         123.9138 |          16.9268 |
[32m[20221213 21:16:55 @agent_ppo2.py:185][0m |          -0.0031 |         121.4544 |          16.9088 |
[32m[20221213 21:16:56 @agent_ppo2.py:185][0m |           0.0018 |         125.7593 |          16.9002 |
[32m[20221213 21:16:56 @agent_ppo2.py:185][0m |          -0.0056 |         119.4347 |          16.8941 |
[32m[20221213 21:16:56 @agent_ppo2.py:185][0m |          -0.0086 |         118.8723 |          16.8753 |
[32m[20221213 21:16:56 @agent_ppo2.py:185][0m |          -0.0089 |         118.3607 |          16.8921 |
[32m[20221213 21:16:56 @agent_ppo2.py:185][0m |          -0.0091 |         117.9426 |          16.8726 |
[32m[20221213 21:16:56 @agent_ppo2.py:185][0m |           0.0000 |         121.7630 |          16.8546 |
[32m[20221213 21:16:56 @agent_ppo2.py:185][0m |          -0.0079 |         117.3252 |          16.8585 |
[32m[20221213 21:16:56 @agent_ppo2.py:185][0m |          -0.0092 |         117.8447 |          16.8434 |
[32m[20221213 21:16:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 645.80
[32m[20221213 21:16:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 720.00
[32m[20221213 21:16:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 733.00
[32m[20221213 21:16:56 @agent_ppo2.py:143][0m Total time:      21.36 min
[32m[20221213 21:16:56 @agent_ppo2.py:145][0m 2088960 total steps have happened
[32m[20221213 21:16:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1020 --------------------------#
[32m[20221213 21:16:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:57 @agent_ppo2.py:185][0m |           0.0115 |         141.8580 |          16.9051 |
[32m[20221213 21:16:57 @agent_ppo2.py:185][0m |           0.0087 |         140.6921 |          16.9224 |
[32m[20221213 21:16:57 @agent_ppo2.py:185][0m |           0.0006 |         127.2432 |          16.9374 |
[32m[20221213 21:16:57 @agent_ppo2.py:185][0m |          -0.0081 |         123.7402 |          16.9486 |
[32m[20221213 21:16:57 @agent_ppo2.py:185][0m |          -0.0068 |         123.1958 |          16.9718 |
[32m[20221213 21:16:57 @agent_ppo2.py:185][0m |          -0.0059 |         122.9441 |          16.9653 |
[32m[20221213 21:16:57 @agent_ppo2.py:185][0m |          -0.0100 |         122.6840 |          16.9823 |
[32m[20221213 21:16:57 @agent_ppo2.py:185][0m |          -0.0100 |         122.3926 |          16.9949 |
[32m[20221213 21:16:57 @agent_ppo2.py:185][0m |          -0.0075 |         121.9442 |          17.0038 |
[32m[20221213 21:16:57 @agent_ppo2.py:185][0m |          -0.0099 |         121.9085 |          17.0299 |
[32m[20221213 21:16:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 619.40
[32m[20221213 21:16:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 691.00
[32m[20221213 21:16:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 724.00
[32m[20221213 21:16:57 @agent_ppo2.py:143][0m Total time:      21.38 min
[32m[20221213 21:16:57 @agent_ppo2.py:145][0m 2091008 total steps have happened
[32m[20221213 21:16:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1021 --------------------------#
[32m[20221213 21:16:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:16:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:58 @agent_ppo2.py:185][0m |           0.0132 |         134.0486 |          16.9525 |
[32m[20221213 21:16:58 @agent_ppo2.py:185][0m |           0.0002 |         119.3706 |          16.9021 |
[32m[20221213 21:16:58 @agent_ppo2.py:185][0m |          -0.0047 |         118.5004 |          16.9481 |
[32m[20221213 21:16:58 @agent_ppo2.py:185][0m |          -0.0055 |         117.4941 |          16.9701 |
[32m[20221213 21:16:58 @agent_ppo2.py:185][0m |          -0.0059 |         117.0429 |          16.9542 |
[32m[20221213 21:16:58 @agent_ppo2.py:185][0m |          -0.0052 |         116.6122 |          16.9590 |
[32m[20221213 21:16:58 @agent_ppo2.py:185][0m |          -0.0061 |         116.2064 |          16.9614 |
[32m[20221213 21:16:58 @agent_ppo2.py:185][0m |          -0.0049 |         116.0142 |          16.9808 |
[32m[20221213 21:16:58 @agent_ppo2.py:185][0m |          -0.0052 |         115.6207 |          16.9766 |
[32m[20221213 21:16:58 @agent_ppo2.py:185][0m |          -0.0074 |         115.4987 |          16.9627 |
[32m[20221213 21:16:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:16:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.40
[32m[20221213 21:16:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.00
[32m[20221213 21:16:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 682.00
[32m[20221213 21:16:59 @agent_ppo2.py:143][0m Total time:      21.40 min
[32m[20221213 21:16:59 @agent_ppo2.py:145][0m 2093056 total steps have happened
[32m[20221213 21:16:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1022 --------------------------#
[32m[20221213 21:16:59 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:16:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:16:59 @agent_ppo2.py:185][0m |           0.0008 |         122.0712 |          17.0349 |
[32m[20221213 21:16:59 @agent_ppo2.py:185][0m |          -0.0015 |         120.8409 |          17.0300 |
[32m[20221213 21:16:59 @agent_ppo2.py:185][0m |          -0.0045 |         119.8930 |          17.0414 |
[32m[20221213 21:16:59 @agent_ppo2.py:185][0m |           0.0077 |         127.8041 |          17.0361 |
[32m[20221213 21:16:59 @agent_ppo2.py:185][0m |          -0.0060 |         119.2698 |          17.0179 |
[32m[20221213 21:16:59 @agent_ppo2.py:185][0m |          -0.0036 |         118.9885 |          17.0214 |
[32m[20221213 21:16:59 @agent_ppo2.py:185][0m |          -0.0068 |         118.6960 |          17.0028 |
[32m[20221213 21:17:00 @agent_ppo2.py:185][0m |          -0.0066 |         118.3860 |          16.9897 |
[32m[20221213 21:17:00 @agent_ppo2.py:185][0m |          -0.0071 |         118.2688 |          17.0012 |
[32m[20221213 21:17:00 @agent_ppo2.py:185][0m |          -0.0113 |         117.9426 |          16.9890 |
[32m[20221213 21:17:00 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:17:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 649.40
[32m[20221213 21:17:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 675.00
[32m[20221213 21:17:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 711.00
[32m[20221213 21:17:00 @agent_ppo2.py:143][0m Total time:      21.42 min
[32m[20221213 21:17:00 @agent_ppo2.py:145][0m 2095104 total steps have happened
[32m[20221213 21:17:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1023 --------------------------#
[32m[20221213 21:17:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:00 @agent_ppo2.py:185][0m |           0.0124 |         134.3464 |          17.1736 |
[32m[20221213 21:17:00 @agent_ppo2.py:185][0m |          -0.0030 |         120.9926 |          17.1388 |
[32m[20221213 21:17:00 @agent_ppo2.py:185][0m |           0.0012 |         121.7085 |          17.1148 |
[32m[20221213 21:17:00 @agent_ppo2.py:185][0m |          -0.0010 |         120.4720 |          17.0722 |
[32m[20221213 21:17:00 @agent_ppo2.py:185][0m |          -0.0092 |         120.1666 |          17.0355 |
[32m[20221213 21:17:01 @agent_ppo2.py:185][0m |          -0.0089 |         119.8994 |          17.0599 |
[32m[20221213 21:17:01 @agent_ppo2.py:185][0m |          -0.0096 |         119.6951 |          17.0378 |
[32m[20221213 21:17:01 @agent_ppo2.py:185][0m |          -0.0023 |         123.2975 |          16.9966 |
[32m[20221213 21:17:01 @agent_ppo2.py:185][0m |          -0.0042 |         120.8739 |          16.9840 |
[32m[20221213 21:17:01 @agent_ppo2.py:185][0m |          -0.0072 |         119.1458 |          17.0035 |
[32m[20221213 21:17:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 690.00
[32m[20221213 21:17:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 732.00
[32m[20221213 21:17:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 728.00
[32m[20221213 21:17:01 @agent_ppo2.py:143][0m Total time:      21.44 min
[32m[20221213 21:17:01 @agent_ppo2.py:145][0m 2097152 total steps have happened
[32m[20221213 21:17:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1024 --------------------------#
[32m[20221213 21:17:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:17:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:01 @agent_ppo2.py:185][0m |          -0.0021 |         123.0849 |          16.9059 |
[32m[20221213 21:17:01 @agent_ppo2.py:185][0m |          -0.0021 |         119.1821 |          16.8919 |
[32m[20221213 21:17:02 @agent_ppo2.py:185][0m |           0.0029 |         121.9552 |          16.8669 |
[32m[20221213 21:17:02 @agent_ppo2.py:185][0m |          -0.0062 |         115.4860 |          16.8598 |
[32m[20221213 21:17:02 @agent_ppo2.py:185][0m |          -0.0068 |         115.0489 |          16.8568 |
[32m[20221213 21:17:02 @agent_ppo2.py:185][0m |          -0.0081 |         114.0675 |          16.8545 |
[32m[20221213 21:17:02 @agent_ppo2.py:185][0m |          -0.0094 |         113.5314 |          16.8536 |
[32m[20221213 21:17:02 @agent_ppo2.py:185][0m |          -0.0075 |         113.1107 |          16.8375 |
[32m[20221213 21:17:02 @agent_ppo2.py:185][0m |          -0.0103 |         112.5812 |          16.8454 |
[32m[20221213 21:17:02 @agent_ppo2.py:185][0m |          -0.0008 |         117.3083 |          16.8334 |
[32m[20221213 21:17:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 652.20
[32m[20221213 21:17:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.00
[32m[20221213 21:17:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 745.00
[32m[20221213 21:17:02 @agent_ppo2.py:143][0m Total time:      21.46 min
[32m[20221213 21:17:02 @agent_ppo2.py:145][0m 2099200 total steps have happened
[32m[20221213 21:17:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1025 --------------------------#
[32m[20221213 21:17:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:03 @agent_ppo2.py:185][0m |          -0.0032 |         130.6300 |          16.6237 |
[32m[20221213 21:17:03 @agent_ppo2.py:185][0m |          -0.0072 |         126.7571 |          16.6408 |
[32m[20221213 21:17:03 @agent_ppo2.py:185][0m |          -0.0085 |         124.4685 |          16.6318 |
[32m[20221213 21:17:03 @agent_ppo2.py:185][0m |          -0.0043 |         124.0590 |          16.6431 |
[32m[20221213 21:17:03 @agent_ppo2.py:185][0m |          -0.0061 |         122.3380 |          16.6357 |
[32m[20221213 21:17:03 @agent_ppo2.py:185][0m |          -0.0115 |         121.7702 |          16.6297 |
[32m[20221213 21:17:03 @agent_ppo2.py:185][0m |          -0.0095 |         121.3609 |          16.6436 |
[32m[20221213 21:17:03 @agent_ppo2.py:185][0m |          -0.0103 |         120.9225 |          16.6441 |
[32m[20221213 21:17:03 @agent_ppo2.py:185][0m |          -0.0063 |         123.6998 |          16.6356 |
[32m[20221213 21:17:03 @agent_ppo2.py:185][0m |          -0.0059 |         123.4381 |          16.6375 |
[32m[20221213 21:17:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 643.20
[32m[20221213 21:17:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 696.00
[32m[20221213 21:17:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 659.00
[32m[20221213 21:17:03 @agent_ppo2.py:143][0m Total time:      21.48 min
[32m[20221213 21:17:03 @agent_ppo2.py:145][0m 2101248 total steps have happened
[32m[20221213 21:17:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1026 --------------------------#
[32m[20221213 21:17:04 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:17:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:04 @agent_ppo2.py:185][0m |           0.0034 |         132.8929 |          16.8381 |
[32m[20221213 21:17:04 @agent_ppo2.py:185][0m |           0.0041 |         139.7880 |          16.8460 |
[32m[20221213 21:17:04 @agent_ppo2.py:185][0m |           0.0007 |         132.2891 |          16.8442 |
[32m[20221213 21:17:04 @agent_ppo2.py:185][0m |          -0.0057 |         129.8479 |          16.8259 |
[32m[20221213 21:17:04 @agent_ppo2.py:185][0m |          -0.0051 |         129.2490 |          16.8138 |
[32m[20221213 21:17:04 @agent_ppo2.py:185][0m |          -0.0052 |         129.2939 |          16.8239 |
[32m[20221213 21:17:04 @agent_ppo2.py:185][0m |          -0.0071 |         128.7713 |          16.8040 |
[32m[20221213 21:17:04 @agent_ppo2.py:185][0m |          -0.0036 |         130.9826 |          16.8046 |
[32m[20221213 21:17:04 @agent_ppo2.py:185][0m |          -0.0061 |         128.3339 |          16.8034 |
[32m[20221213 21:17:04 @agent_ppo2.py:185][0m |          -0.0116 |         128.2155 |          16.7890 |
[32m[20221213 21:17:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 660.80
[32m[20221213 21:17:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.00
[32m[20221213 21:17:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 717.00
[32m[20221213 21:17:05 @agent_ppo2.py:143][0m Total time:      21.50 min
[32m[20221213 21:17:05 @agent_ppo2.py:145][0m 2103296 total steps have happened
[32m[20221213 21:17:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1027 --------------------------#
[32m[20221213 21:17:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:17:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:05 @agent_ppo2.py:185][0m |          -0.0029 |         121.7209 |          16.7700 |
[32m[20221213 21:17:05 @agent_ppo2.py:185][0m |          -0.0024 |         118.4405 |          16.7605 |
[32m[20221213 21:17:05 @agent_ppo2.py:185][0m |          -0.0043 |         117.3564 |          16.7665 |
[32m[20221213 21:17:05 @agent_ppo2.py:185][0m |          -0.0057 |         116.7520 |          16.7830 |
[32m[20221213 21:17:05 @agent_ppo2.py:185][0m |          -0.0064 |         116.5749 |          16.7741 |
[32m[20221213 21:17:05 @agent_ppo2.py:185][0m |          -0.0080 |         115.6487 |          16.7707 |
[32m[20221213 21:17:05 @agent_ppo2.py:185][0m |          -0.0068 |         115.1823 |          16.7843 |
[32m[20221213 21:17:06 @agent_ppo2.py:185][0m |          -0.0092 |         114.6677 |          16.7927 |
[32m[20221213 21:17:06 @agent_ppo2.py:185][0m |          -0.0082 |         114.4526 |          16.7860 |
[32m[20221213 21:17:06 @agent_ppo2.py:185][0m |          -0.0080 |         114.2011 |          16.8080 |
[32m[20221213 21:17:06 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:17:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 648.20
[32m[20221213 21:17:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 716.00
[32m[20221213 21:17:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 585.00
[32m[20221213 21:17:06 @agent_ppo2.py:143][0m Total time:      21.52 min
[32m[20221213 21:17:06 @agent_ppo2.py:145][0m 2105344 total steps have happened
[32m[20221213 21:17:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1028 --------------------------#
[32m[20221213 21:17:06 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:17:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:06 @agent_ppo2.py:185][0m |          -0.0001 |         130.4045 |          16.8327 |
[32m[20221213 21:17:06 @agent_ppo2.py:185][0m |          -0.0051 |         129.0259 |          16.8166 |
[32m[20221213 21:17:06 @agent_ppo2.py:185][0m |          -0.0056 |         128.4806 |          16.8089 |
[32m[20221213 21:17:06 @agent_ppo2.py:185][0m |          -0.0027 |         128.1198 |          16.8261 |
[32m[20221213 21:17:06 @agent_ppo2.py:185][0m |          -0.0061 |         127.7322 |          16.8096 |
[32m[20221213 21:17:07 @agent_ppo2.py:185][0m |          -0.0069 |         127.5210 |          16.8034 |
[32m[20221213 21:17:07 @agent_ppo2.py:185][0m |          -0.0033 |         127.9925 |          16.7920 |
[32m[20221213 21:17:07 @agent_ppo2.py:185][0m |          -0.0074 |         127.0311 |          16.8001 |
[32m[20221213 21:17:07 @agent_ppo2.py:185][0m |          -0.0068 |         126.8193 |          16.7969 |
[32m[20221213 21:17:07 @agent_ppo2.py:185][0m |          -0.0103 |         126.7900 |          16.7806 |
[32m[20221213 21:17:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 640.00
[32m[20221213 21:17:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 666.00
[32m[20221213 21:17:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 658.00
[32m[20221213 21:17:07 @agent_ppo2.py:143][0m Total time:      21.54 min
[32m[20221213 21:17:07 @agent_ppo2.py:145][0m 2107392 total steps have happened
[32m[20221213 21:17:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1029 --------------------------#
[32m[20221213 21:17:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:07 @agent_ppo2.py:185][0m |           0.0036 |         129.8045 |          16.6453 |
[32m[20221213 21:17:07 @agent_ppo2.py:185][0m |          -0.0020 |         127.4537 |          16.6052 |
[32m[20221213 21:17:07 @agent_ppo2.py:185][0m |          -0.0008 |         126.8433 |          16.5705 |
[32m[20221213 21:17:08 @agent_ppo2.py:185][0m |          -0.0033 |         125.7236 |          16.5670 |
[32m[20221213 21:17:08 @agent_ppo2.py:185][0m |          -0.0082 |         125.3425 |          16.5458 |
[32m[20221213 21:17:08 @agent_ppo2.py:185][0m |          -0.0036 |         125.4497 |          16.5062 |
[32m[20221213 21:17:08 @agent_ppo2.py:185][0m |          -0.0074 |         124.6547 |          16.4889 |
[32m[20221213 21:17:08 @agent_ppo2.py:185][0m |          -0.0084 |         124.4326 |          16.4547 |
[32m[20221213 21:17:08 @agent_ppo2.py:185][0m |          -0.0032 |         125.8778 |          16.4691 |
[32m[20221213 21:17:08 @agent_ppo2.py:185][0m |          -0.0102 |         124.0096 |          16.4119 |
[32m[20221213 21:17:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 637.20
[32m[20221213 21:17:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 674.00
[32m[20221213 21:17:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 707.00
[32m[20221213 21:17:08 @agent_ppo2.py:143][0m Total time:      21.56 min
[32m[20221213 21:17:08 @agent_ppo2.py:145][0m 2109440 total steps have happened
[32m[20221213 21:17:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1030 --------------------------#
[32m[20221213 21:17:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:09 @agent_ppo2.py:185][0m |          -0.0007 |         130.1214 |          16.5628 |
[32m[20221213 21:17:09 @agent_ppo2.py:185][0m |          -0.0011 |         126.9688 |          16.5818 |
[32m[20221213 21:17:09 @agent_ppo2.py:185][0m |          -0.0033 |         125.3090 |          16.5738 |
[32m[20221213 21:17:09 @agent_ppo2.py:185][0m |          -0.0066 |         124.2134 |          16.5802 |
[32m[20221213 21:17:09 @agent_ppo2.py:185][0m |          -0.0082 |         123.6678 |          16.5753 |
[32m[20221213 21:17:09 @agent_ppo2.py:185][0m |          -0.0059 |         124.0180 |          16.5657 |
[32m[20221213 21:17:09 @agent_ppo2.py:185][0m |          -0.0099 |         122.5787 |          16.5675 |
[32m[20221213 21:17:09 @agent_ppo2.py:185][0m |          -0.0102 |         122.1679 |          16.5817 |
[32m[20221213 21:17:09 @agent_ppo2.py:185][0m |          -0.0035 |         125.3163 |          16.5833 |
[32m[20221213 21:17:09 @agent_ppo2.py:185][0m |          -0.0122 |         121.7110 |          16.5792 |
[32m[20221213 21:17:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:17:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 638.80
[32m[20221213 21:17:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 677.00
[32m[20221213 21:17:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 714.00
[32m[20221213 21:17:09 @agent_ppo2.py:143][0m Total time:      21.58 min
[32m[20221213 21:17:09 @agent_ppo2.py:145][0m 2111488 total steps have happened
[32m[20221213 21:17:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1031 --------------------------#
[32m[20221213 21:17:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:10 @agent_ppo2.py:185][0m |          -0.0005 |         128.5346 |          16.4852 |
[32m[20221213 21:17:10 @agent_ppo2.py:185][0m |          -0.0096 |         125.0426 |          16.4666 |
[32m[20221213 21:17:10 @agent_ppo2.py:185][0m |          -0.0069 |         123.6174 |          16.4745 |
[32m[20221213 21:17:10 @agent_ppo2.py:185][0m |          -0.0101 |         122.8958 |          16.4833 |
[32m[20221213 21:17:10 @agent_ppo2.py:185][0m |          -0.0079 |         122.3025 |          16.4714 |
[32m[20221213 21:17:10 @agent_ppo2.py:185][0m |          -0.0068 |         121.8209 |          16.4845 |
[32m[20221213 21:17:10 @agent_ppo2.py:185][0m |          -0.0078 |         122.5680 |          16.4869 |
[32m[20221213 21:17:10 @agent_ppo2.py:185][0m |          -0.0088 |         121.6852 |          16.4925 |
[32m[20221213 21:17:10 @agent_ppo2.py:185][0m |          -0.0080 |         121.1157 |          16.5031 |
[32m[20221213 21:17:11 @agent_ppo2.py:185][0m |          -0.0057 |         121.9538 |          16.5042 |
[32m[20221213 21:17:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:17:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 653.20
[32m[20221213 21:17:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 686.00
[32m[20221213 21:17:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 673.00
[32m[20221213 21:17:11 @agent_ppo2.py:143][0m Total time:      21.60 min
[32m[20221213 21:17:11 @agent_ppo2.py:145][0m 2113536 total steps have happened
[32m[20221213 21:17:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1032 --------------------------#
[32m[20221213 21:17:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:11 @agent_ppo2.py:185][0m |           0.0070 |         134.5766 |          16.4380 |
[32m[20221213 21:17:11 @agent_ppo2.py:185][0m |          -0.0031 |         125.7943 |          16.4335 |
[32m[20221213 21:17:11 @agent_ppo2.py:185][0m |          -0.0041 |         124.3796 |          16.4616 |
[32m[20221213 21:17:11 @agent_ppo2.py:185][0m |           0.0005 |         124.8157 |          16.4765 |
[32m[20221213 21:17:11 @agent_ppo2.py:185][0m |          -0.0075 |         123.4329 |          16.4884 |
[32m[20221213 21:17:11 @agent_ppo2.py:185][0m |          -0.0044 |         122.8150 |          16.4866 |
[32m[20221213 21:17:11 @agent_ppo2.py:185][0m |          -0.0016 |         124.6199 |          16.5047 |
[32m[20221213 21:17:12 @agent_ppo2.py:185][0m |          -0.0089 |         122.3301 |          16.4885 |
[32m[20221213 21:17:12 @agent_ppo2.py:185][0m |          -0.0095 |         122.0194 |          16.4993 |
[32m[20221213 21:17:12 @agent_ppo2.py:185][0m |          -0.0072 |         121.7813 |          16.5131 |
[32m[20221213 21:17:12 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:17:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 606.40
[32m[20221213 21:17:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 651.00
[32m[20221213 21:17:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 627.00
[32m[20221213 21:17:12 @agent_ppo2.py:143][0m Total time:      21.62 min
[32m[20221213 21:17:12 @agent_ppo2.py:145][0m 2115584 total steps have happened
[32m[20221213 21:17:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1033 --------------------------#
[32m[20221213 21:17:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:12 @agent_ppo2.py:185][0m |          -0.0005 |         123.2367 |          16.5781 |
[32m[20221213 21:17:12 @agent_ppo2.py:185][0m |           0.0006 |         123.3220 |          16.5592 |
[32m[20221213 21:17:12 @agent_ppo2.py:185][0m |          -0.0010 |         121.0779 |          16.5732 |
[32m[20221213 21:17:12 @agent_ppo2.py:185][0m |          -0.0065 |         120.1784 |          16.5582 |
[32m[20221213 21:17:12 @agent_ppo2.py:185][0m |          -0.0060 |         119.6259 |          16.5410 |
[32m[20221213 21:17:13 @agent_ppo2.py:185][0m |          -0.0062 |         119.4059 |          16.5360 |
[32m[20221213 21:17:13 @agent_ppo2.py:185][0m |          -0.0071 |         118.8299 |          16.5407 |
[32m[20221213 21:17:13 @agent_ppo2.py:185][0m |          -0.0052 |         119.5721 |          16.5355 |
[32m[20221213 21:17:13 @agent_ppo2.py:185][0m |          -0.0058 |         118.7537 |          16.5229 |
[32m[20221213 21:17:13 @agent_ppo2.py:185][0m |          -0.0074 |         118.2737 |          16.5271 |
[32m[20221213 21:17:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:17:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 660.80
[32m[20221213 21:17:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 707.00
[32m[20221213 21:17:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 646.00
[32m[20221213 21:17:13 @agent_ppo2.py:143][0m Total time:      21.64 min
[32m[20221213 21:17:13 @agent_ppo2.py:145][0m 2117632 total steps have happened
[32m[20221213 21:17:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1034 --------------------------#
[32m[20221213 21:17:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:13 @agent_ppo2.py:185][0m |          -0.0047 |         128.6067 |          16.4147 |
[32m[20221213 21:17:13 @agent_ppo2.py:185][0m |          -0.0060 |         126.0281 |          16.4168 |
[32m[20221213 21:17:14 @agent_ppo2.py:185][0m |          -0.0032 |         125.1633 |          16.4121 |
[32m[20221213 21:17:14 @agent_ppo2.py:185][0m |          -0.0018 |         124.8334 |          16.3920 |
[32m[20221213 21:17:14 @agent_ppo2.py:185][0m |          -0.0059 |         124.7018 |          16.3728 |
[32m[20221213 21:17:14 @agent_ppo2.py:185][0m |          -0.0062 |         124.1128 |          16.3710 |
[32m[20221213 21:17:14 @agent_ppo2.py:185][0m |          -0.0073 |         123.9181 |          16.3668 |
[32m[20221213 21:17:14 @agent_ppo2.py:185][0m |          -0.0035 |         123.5956 |          16.3768 |
[32m[20221213 21:17:14 @agent_ppo2.py:185][0m |          -0.0063 |         123.5459 |          16.3636 |
[32m[20221213 21:17:14 @agent_ppo2.py:185][0m |          -0.0073 |         123.3901 |          16.3656 |
[32m[20221213 21:17:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 654.20
[32m[20221213 21:17:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.00
[32m[20221213 21:17:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 681.00
[32m[20221213 21:17:14 @agent_ppo2.py:143][0m Total time:      21.66 min
[32m[20221213 21:17:14 @agent_ppo2.py:145][0m 2119680 total steps have happened
[32m[20221213 21:17:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1035 --------------------------#
[32m[20221213 21:17:14 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:17:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:15 @agent_ppo2.py:185][0m |           0.0019 |         125.3360 |          16.5400 |
[32m[20221213 21:17:15 @agent_ppo2.py:185][0m |          -0.0069 |         123.2404 |          16.4936 |
[32m[20221213 21:17:15 @agent_ppo2.py:185][0m |          -0.0093 |         122.2957 |          16.5004 |
[32m[20221213 21:17:15 @agent_ppo2.py:185][0m |          -0.0102 |         121.6384 |          16.4733 |
[32m[20221213 21:17:15 @agent_ppo2.py:185][0m |          -0.0083 |         120.8517 |          16.4685 |
[32m[20221213 21:17:15 @agent_ppo2.py:185][0m |          -0.0118 |         120.6531 |          16.4545 |
[32m[20221213 21:17:15 @agent_ppo2.py:185][0m |          -0.0122 |         120.2898 |          16.4694 |
[32m[20221213 21:17:15 @agent_ppo2.py:185][0m |          -0.0131 |         119.8000 |          16.4454 |
[32m[20221213 21:17:15 @agent_ppo2.py:185][0m |          -0.0065 |         120.6756 |          16.4348 |
[32m[20221213 21:17:15 @agent_ppo2.py:185][0m |          -0.0125 |         119.2158 |          16.4222 |
[32m[20221213 21:17:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:17:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 643.80
[32m[20221213 21:17:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 727.00
[32m[20221213 21:17:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 716.00
[32m[20221213 21:17:15 @agent_ppo2.py:143][0m Total time:      21.68 min
[32m[20221213 21:17:15 @agent_ppo2.py:145][0m 2121728 total steps have happened
[32m[20221213 21:17:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1036 --------------------------#
[32m[20221213 21:17:16 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:17:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:16 @agent_ppo2.py:185][0m |           0.0034 |         128.3904 |          16.3431 |
[32m[20221213 21:17:16 @agent_ppo2.py:185][0m |          -0.0075 |         125.8667 |          16.3404 |
[32m[20221213 21:17:16 @agent_ppo2.py:185][0m |          -0.0080 |         124.6040 |          16.3488 |
[32m[20221213 21:17:16 @agent_ppo2.py:185][0m |          -0.0072 |         123.6734 |          16.3579 |
[32m[20221213 21:17:16 @agent_ppo2.py:185][0m |          -0.0093 |         123.0789 |          16.3706 |
[32m[20221213 21:17:16 @agent_ppo2.py:185][0m |          -0.0112 |         122.5376 |          16.3821 |
[32m[20221213 21:17:16 @agent_ppo2.py:185][0m |          -0.0087 |         122.0368 |          16.3964 |
[32m[20221213 21:17:16 @agent_ppo2.py:185][0m |          -0.0073 |         122.0594 |          16.3935 |
[32m[20221213 21:17:16 @agent_ppo2.py:185][0m |          -0.0110 |         121.2882 |          16.3880 |
[32m[20221213 21:17:17 @agent_ppo2.py:185][0m |          -0.0128 |         121.2070 |          16.4036 |
[32m[20221213 21:17:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:17:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 668.00
[32m[20221213 21:17:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 726.00
[32m[20221213 21:17:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.00
[32m[20221213 21:17:17 @agent_ppo2.py:143][0m Total time:      21.70 min
[32m[20221213 21:17:17 @agent_ppo2.py:145][0m 2123776 total steps have happened
[32m[20221213 21:17:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1037 --------------------------#
[32m[20221213 21:17:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:17:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:17 @agent_ppo2.py:185][0m |          -0.0014 |         128.6377 |          16.3476 |
[32m[20221213 21:17:17 @agent_ppo2.py:185][0m |          -0.0047 |         127.3586 |          16.3599 |
[32m[20221213 21:17:17 @agent_ppo2.py:185][0m |          -0.0069 |         126.8952 |          16.3643 |
[32m[20221213 21:17:17 @agent_ppo2.py:185][0m |           0.0035 |         133.3509 |          16.3781 |
[32m[20221213 21:17:17 @agent_ppo2.py:185][0m |          -0.0067 |         126.1279 |          16.3826 |
[32m[20221213 21:17:17 @agent_ppo2.py:185][0m |          -0.0069 |         125.7002 |          16.4013 |
[32m[20221213 21:17:18 @agent_ppo2.py:185][0m |          -0.0090 |         125.4787 |          16.3946 |
[32m[20221213 21:17:18 @agent_ppo2.py:185][0m |          -0.0103 |         125.2968 |          16.4111 |
[32m[20221213 21:17:18 @agent_ppo2.py:185][0m |          -0.0076 |         125.1252 |          16.3984 |
[32m[20221213 21:17:18 @agent_ppo2.py:185][0m |          -0.0114 |         124.9847 |          16.4129 |
[32m[20221213 21:17:18 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:17:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 684.60
[32m[20221213 21:17:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 725.00
[32m[20221213 21:17:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.00
[32m[20221213 21:17:18 @agent_ppo2.py:143][0m Total time:      21.72 min
[32m[20221213 21:17:18 @agent_ppo2.py:145][0m 2125824 total steps have happened
[32m[20221213 21:17:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1038 --------------------------#
[32m[20221213 21:17:18 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:17:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:18 @agent_ppo2.py:185][0m |           0.0005 |         130.3188 |          16.4018 |
[32m[20221213 21:17:18 @agent_ppo2.py:185][0m |           0.0024 |         130.3806 |          16.4010 |
[32m[20221213 21:17:18 @agent_ppo2.py:185][0m |          -0.0074 |         126.7452 |          16.3821 |
[32m[20221213 21:17:18 @agent_ppo2.py:185][0m |          -0.0031 |         126.4136 |          16.3590 |
[32m[20221213 21:17:19 @agent_ppo2.py:185][0m |          -0.0074 |         125.8695 |          16.3609 |
[32m[20221213 21:17:19 @agent_ppo2.py:185][0m |          -0.0066 |         125.3226 |          16.3515 |
[32m[20221213 21:17:19 @agent_ppo2.py:185][0m |           0.0010 |         134.5213 |          16.3511 |
[32m[20221213 21:17:19 @agent_ppo2.py:185][0m |          -0.0017 |         133.3943 |          16.3453 |
[32m[20221213 21:17:19 @agent_ppo2.py:185][0m |          -0.0089 |         124.7009 |          16.3202 |
[32m[20221213 21:17:19 @agent_ppo2.py:185][0m |          -0.0100 |         124.4092 |          16.3190 |
[32m[20221213 21:17:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:17:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 633.80
[32m[20221213 21:17:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 684.00
[32m[20221213 21:17:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 658.00
[32m[20221213 21:17:19 @agent_ppo2.py:143][0m Total time:      21.74 min
[32m[20221213 21:17:19 @agent_ppo2.py:145][0m 2127872 total steps have happened
[32m[20221213 21:17:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1039 --------------------------#
[32m[20221213 21:17:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:19 @agent_ppo2.py:185][0m |           0.0148 |         139.5718 |          16.3435 |
[32m[20221213 21:17:19 @agent_ppo2.py:185][0m |          -0.0002 |         124.0028 |          16.3038 |
[32m[20221213 21:17:20 @agent_ppo2.py:185][0m |          -0.0073 |         123.2454 |          16.3097 |
[32m[20221213 21:17:20 @agent_ppo2.py:185][0m |          -0.0019 |         123.1909 |          16.2990 |
[32m[20221213 21:17:20 @agent_ppo2.py:185][0m |          -0.0067 |         122.1324 |          16.2877 |
[32m[20221213 21:17:20 @agent_ppo2.py:185][0m |          -0.0065 |         121.6006 |          16.2848 |
[32m[20221213 21:17:20 @agent_ppo2.py:185][0m |          -0.0062 |         121.1938 |          16.2736 |
[32m[20221213 21:17:20 @agent_ppo2.py:185][0m |          -0.0083 |         120.9895 |          16.2866 |
[32m[20221213 21:17:20 @agent_ppo2.py:185][0m |          -0.0098 |         120.5292 |          16.2814 |
[32m[20221213 21:17:20 @agent_ppo2.py:185][0m |          -0.0087 |         120.2541 |          16.2629 |
[32m[20221213 21:17:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:17:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 624.00
[32m[20221213 21:17:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.00
[32m[20221213 21:17:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 699.00
[32m[20221213 21:17:20 @agent_ppo2.py:143][0m Total time:      21.76 min
[32m[20221213 21:17:20 @agent_ppo2.py:145][0m 2129920 total steps have happened
[32m[20221213 21:17:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1040 --------------------------#
[32m[20221213 21:17:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:17:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:21 @agent_ppo2.py:185][0m |           0.0012 |         126.5360 |          16.3322 |
[32m[20221213 21:17:21 @agent_ppo2.py:185][0m |          -0.0031 |         125.1123 |          16.3229 |
[32m[20221213 21:17:21 @agent_ppo2.py:185][0m |          -0.0047 |         124.4928 |          16.3108 |
[32m[20221213 21:17:21 @agent_ppo2.py:185][0m |          -0.0068 |         123.8425 |          16.2950 |
[32m[20221213 21:17:21 @agent_ppo2.py:185][0m |          -0.0032 |         124.7883 |          16.2944 |
[32m[20221213 21:17:21 @agent_ppo2.py:185][0m |           0.0061 |         137.1478 |          16.3025 |
[32m[20221213 21:17:21 @agent_ppo2.py:185][0m |          -0.0070 |         123.2108 |          16.2841 |
[32m[20221213 21:17:21 @agent_ppo2.py:185][0m |          -0.0046 |         122.8306 |          16.2914 |
[32m[20221213 21:17:21 @agent_ppo2.py:185][0m |          -0.0064 |         122.5350 |          16.2971 |
[32m[20221213 21:17:21 @agent_ppo2.py:185][0m |          -0.0087 |         122.3278 |          16.2931 |
[32m[20221213 21:17:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 652.60
[32m[20221213 21:17:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 720.00
[32m[20221213 21:17:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 714.00
[32m[20221213 21:17:22 @agent_ppo2.py:143][0m Total time:      21.78 min
[32m[20221213 21:17:22 @agent_ppo2.py:145][0m 2131968 total steps have happened
[32m[20221213 21:17:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1041 --------------------------#
[32m[20221213 21:17:22 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:17:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:22 @agent_ppo2.py:185][0m |          -0.0008 |         127.3640 |          16.4183 |
[32m[20221213 21:17:22 @agent_ppo2.py:185][0m |          -0.0039 |         123.8769 |          16.4164 |
[32m[20221213 21:17:22 @agent_ppo2.py:185][0m |          -0.0069 |         122.9667 |          16.4091 |
[32m[20221213 21:17:22 @agent_ppo2.py:185][0m |          -0.0069 |         122.1475 |          16.4078 |
[32m[20221213 21:17:22 @agent_ppo2.py:185][0m |          -0.0049 |         122.2472 |          16.4091 |
[32m[20221213 21:17:22 @agent_ppo2.py:185][0m |          -0.0083 |         121.6645 |          16.4099 |
[32m[20221213 21:17:22 @agent_ppo2.py:185][0m |          -0.0075 |         121.0151 |          16.4177 |
[32m[20221213 21:17:22 @agent_ppo2.py:185][0m |          -0.0085 |         120.8720 |          16.4243 |
[32m[20221213 21:17:22 @agent_ppo2.py:185][0m |          -0.0082 |         120.6054 |          16.4163 |
[32m[20221213 21:17:23 @agent_ppo2.py:185][0m |           0.0028 |         132.1743 |          16.4175 |
[32m[20221213 21:17:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 611.00
[32m[20221213 21:17:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 670.00
[32m[20221213 21:17:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 727.00
[32m[20221213 21:17:23 @agent_ppo2.py:143][0m Total time:      21.80 min
[32m[20221213 21:17:23 @agent_ppo2.py:145][0m 2134016 total steps have happened
[32m[20221213 21:17:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1042 --------------------------#
[32m[20221213 21:17:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:23 @agent_ppo2.py:185][0m |           0.0003 |         127.8561 |          16.3706 |
[32m[20221213 21:17:23 @agent_ppo2.py:185][0m |          -0.0037 |         125.0967 |          16.4001 |
[32m[20221213 21:17:23 @agent_ppo2.py:185][0m |          -0.0056 |         123.9803 |          16.4054 |
[32m[20221213 21:17:23 @agent_ppo2.py:185][0m |          -0.0051 |         123.4063 |          16.4160 |
[32m[20221213 21:17:23 @agent_ppo2.py:185][0m |          -0.0056 |         122.7815 |          16.4165 |
[32m[20221213 21:17:23 @agent_ppo2.py:185][0m |          -0.0080 |         122.5046 |          16.4289 |
[32m[20221213 21:17:24 @agent_ppo2.py:185][0m |          -0.0082 |         121.9251 |          16.4359 |
[32m[20221213 21:17:24 @agent_ppo2.py:185][0m |          -0.0075 |         121.5883 |          16.4485 |
[32m[20221213 21:17:24 @agent_ppo2.py:185][0m |          -0.0041 |         122.0211 |          16.4605 |
[32m[20221213 21:17:24 @agent_ppo2.py:185][0m |          -0.0072 |         121.0984 |          16.4705 |
[32m[20221213 21:17:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:17:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 653.80
[32m[20221213 21:17:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 754.00
[32m[20221213 21:17:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 721.00
[32m[20221213 21:17:24 @agent_ppo2.py:143][0m Total time:      21.82 min
[32m[20221213 21:17:24 @agent_ppo2.py:145][0m 2136064 total steps have happened
[32m[20221213 21:17:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1043 --------------------------#
[32m[20221213 21:17:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:24 @agent_ppo2.py:185][0m |          -0.0012 |         128.4355 |          16.4695 |
[32m[20221213 21:17:24 @agent_ppo2.py:185][0m |          -0.0017 |         126.9026 |          16.4229 |
[32m[20221213 21:17:24 @agent_ppo2.py:185][0m |          -0.0052 |         125.9317 |          16.4045 |
[32m[20221213 21:17:24 @agent_ppo2.py:185][0m |          -0.0012 |         127.0896 |          16.3747 |
[32m[20221213 21:17:25 @agent_ppo2.py:185][0m |          -0.0087 |         124.5373 |          16.3849 |
[32m[20221213 21:17:25 @agent_ppo2.py:185][0m |          -0.0044 |         124.2206 |          16.2987 |
[32m[20221213 21:17:25 @agent_ppo2.py:185][0m |          -0.0091 |         123.9106 |          16.3085 |
[32m[20221213 21:17:25 @agent_ppo2.py:185][0m |          -0.0088 |         123.3778 |          16.2922 |
[32m[20221213 21:17:25 @agent_ppo2.py:185][0m |          -0.0047 |         123.3496 |          16.2753 |
[32m[20221213 21:17:25 @agent_ppo2.py:185][0m |          -0.0091 |         122.9983 |          16.2707 |
[32m[20221213 21:17:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 652.20
[32m[20221213 21:17:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.00
[32m[20221213 21:17:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 689.00
[32m[20221213 21:17:25 @agent_ppo2.py:143][0m Total time:      21.84 min
[32m[20221213 21:17:25 @agent_ppo2.py:145][0m 2138112 total steps have happened
[32m[20221213 21:17:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1044 --------------------------#
[32m[20221213 21:17:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:25 @agent_ppo2.py:185][0m |          -0.0016 |         127.0484 |          16.3930 |
[32m[20221213 21:17:26 @agent_ppo2.py:185][0m |           0.0031 |         133.0092 |          16.3940 |
[32m[20221213 21:17:26 @agent_ppo2.py:185][0m |          -0.0076 |         124.2814 |          16.4009 |
[32m[20221213 21:17:26 @agent_ppo2.py:185][0m |          -0.0082 |         123.6824 |          16.4041 |
[32m[20221213 21:17:26 @agent_ppo2.py:185][0m |          -0.0058 |         123.1463 |          16.4081 |
[32m[20221213 21:17:26 @agent_ppo2.py:185][0m |          -0.0095 |         122.7797 |          16.4266 |
[32m[20221213 21:17:26 @agent_ppo2.py:185][0m |          -0.0122 |         122.7836 |          16.4312 |
[32m[20221213 21:17:26 @agent_ppo2.py:185][0m |          -0.0035 |         125.6372 |          16.4276 |
[32m[20221213 21:17:26 @agent_ppo2.py:185][0m |          -0.0114 |         121.9804 |          16.4375 |
[32m[20221213 21:17:26 @agent_ppo2.py:185][0m |          -0.0111 |         121.8256 |          16.4362 |
[32m[20221213 21:17:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 647.00
[32m[20221213 21:17:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 746.00
[32m[20221213 21:17:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 692.00
[32m[20221213 21:17:26 @agent_ppo2.py:143][0m Total time:      21.86 min
[32m[20221213 21:17:26 @agent_ppo2.py:145][0m 2140160 total steps have happened
[32m[20221213 21:17:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1045 --------------------------#
[32m[20221213 21:17:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:27 @agent_ppo2.py:185][0m |           0.0131 |         136.0353 |          16.6029 |
[32m[20221213 21:17:27 @agent_ppo2.py:185][0m |          -0.0014 |         124.6381 |          16.6102 |
[32m[20221213 21:17:27 @agent_ppo2.py:185][0m |          -0.0052 |         122.9100 |          16.6150 |
[32m[20221213 21:17:27 @agent_ppo2.py:185][0m |          -0.0069 |         122.3896 |          16.6087 |
[32m[20221213 21:17:27 @agent_ppo2.py:185][0m |          -0.0030 |         122.0928 |          16.6033 |
[32m[20221213 21:17:27 @agent_ppo2.py:185][0m |          -0.0086 |         121.8240 |          16.6254 |
[32m[20221213 21:17:27 @agent_ppo2.py:185][0m |          -0.0073 |         120.8949 |          16.6003 |
[32m[20221213 21:17:27 @agent_ppo2.py:185][0m |          -0.0062 |         120.7237 |          16.6100 |
[32m[20221213 21:17:27 @agent_ppo2.py:185][0m |          -0.0059 |         120.5993 |          16.5982 |
[32m[20221213 21:17:27 @agent_ppo2.py:185][0m |          -0.0050 |         122.5473 |          16.5936 |
[32m[20221213 21:17:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 642.80
[32m[20221213 21:17:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 736.00
[32m[20221213 21:17:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 718.00
[32m[20221213 21:17:27 @agent_ppo2.py:143][0m Total time:      21.88 min
[32m[20221213 21:17:27 @agent_ppo2.py:145][0m 2142208 total steps have happened
[32m[20221213 21:17:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1046 --------------------------#
[32m[20221213 21:17:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:28 @agent_ppo2.py:185][0m |           0.0005 |         128.9052 |          16.3561 |
[32m[20221213 21:17:28 @agent_ppo2.py:185][0m |          -0.0040 |         127.2860 |          16.3672 |
[32m[20221213 21:17:28 @agent_ppo2.py:185][0m |          -0.0018 |         127.4141 |          16.3806 |
[32m[20221213 21:17:28 @agent_ppo2.py:185][0m |          -0.0082 |         126.2904 |          16.3624 |
[32m[20221213 21:17:28 @agent_ppo2.py:185][0m |          -0.0024 |         126.6145 |          16.3939 |
[32m[20221213 21:17:28 @agent_ppo2.py:185][0m |          -0.0076 |         125.4676 |          16.3928 |
[32m[20221213 21:17:28 @agent_ppo2.py:185][0m |          -0.0116 |         125.5512 |          16.4181 |
[32m[20221213 21:17:28 @agent_ppo2.py:185][0m |           0.0117 |         144.0388 |          16.4129 |
[32m[20221213 21:17:28 @agent_ppo2.py:185][0m |          -0.0067 |         125.3512 |          16.4430 |
[32m[20221213 21:17:29 @agent_ppo2.py:185][0m |          -0.0073 |         124.5019 |          16.4354 |
[32m[20221213 21:17:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 650.00
[32m[20221213 21:17:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 726.00
[32m[20221213 21:17:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 698.00
[32m[20221213 21:17:29 @agent_ppo2.py:143][0m Total time:      21.90 min
[32m[20221213 21:17:29 @agent_ppo2.py:145][0m 2144256 total steps have happened
[32m[20221213 21:17:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1047 --------------------------#
[32m[20221213 21:17:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:29 @agent_ppo2.py:185][0m |          -0.0009 |         129.3957 |          16.4261 |
[32m[20221213 21:17:29 @agent_ppo2.py:185][0m |          -0.0034 |         127.2965 |          16.4412 |
[32m[20221213 21:17:29 @agent_ppo2.py:185][0m |          -0.0059 |         126.0774 |          16.4573 |
[32m[20221213 21:17:29 @agent_ppo2.py:185][0m |          -0.0074 |         125.0672 |          16.4619 |
[32m[20221213 21:17:29 @agent_ppo2.py:185][0m |          -0.0085 |         124.5764 |          16.4658 |
[32m[20221213 21:17:29 @agent_ppo2.py:185][0m |          -0.0055 |         124.0089 |          16.4865 |
[32m[20221213 21:17:30 @agent_ppo2.py:185][0m |          -0.0069 |         123.5640 |          16.4946 |
[32m[20221213 21:17:30 @agent_ppo2.py:185][0m |          -0.0091 |         123.1546 |          16.5001 |
[32m[20221213 21:17:30 @agent_ppo2.py:185][0m |          -0.0087 |         123.0093 |          16.5202 |
[32m[20221213 21:17:30 @agent_ppo2.py:185][0m |          -0.0094 |         122.7077 |          16.5428 |
[32m[20221213 21:17:30 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:17:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 657.20
[32m[20221213 21:17:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 749.00
[32m[20221213 21:17:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 700.00
[32m[20221213 21:17:30 @agent_ppo2.py:143][0m Total time:      21.92 min
[32m[20221213 21:17:30 @agent_ppo2.py:145][0m 2146304 total steps have happened
[32m[20221213 21:17:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1048 --------------------------#
[32m[20221213 21:17:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:30 @agent_ppo2.py:185][0m |          -0.0018 |         129.3219 |          16.6621 |
[32m[20221213 21:17:30 @agent_ppo2.py:185][0m |          -0.0030 |         128.2181 |          16.6433 |
[32m[20221213 21:17:30 @agent_ppo2.py:185][0m |          -0.0047 |         127.5275 |          16.6182 |
[32m[20221213 21:17:30 @agent_ppo2.py:185][0m |          -0.0014 |         129.2550 |          16.6000 |
[32m[20221213 21:17:31 @agent_ppo2.py:185][0m |          -0.0054 |         126.9000 |          16.5916 |
[32m[20221213 21:17:31 @agent_ppo2.py:185][0m |          -0.0061 |         126.5498 |          16.5719 |
[32m[20221213 21:17:31 @agent_ppo2.py:185][0m |          -0.0085 |         126.3710 |          16.5628 |
[32m[20221213 21:17:31 @agent_ppo2.py:185][0m |          -0.0113 |         126.1932 |          16.5620 |
[32m[20221213 21:17:31 @agent_ppo2.py:185][0m |          -0.0091 |         126.2898 |          16.5434 |
[32m[20221213 21:17:31 @agent_ppo2.py:185][0m |          -0.0096 |         125.5270 |          16.5164 |
[32m[20221213 21:17:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 654.60
[32m[20221213 21:17:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.00
[32m[20221213 21:17:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 724.00
[32m[20221213 21:17:31 @agent_ppo2.py:143][0m Total time:      21.94 min
[32m[20221213 21:17:31 @agent_ppo2.py:145][0m 2148352 total steps have happened
[32m[20221213 21:17:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1049 --------------------------#
[32m[20221213 21:17:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:31 @agent_ppo2.py:185][0m |           0.0044 |         127.2190 |          16.5736 |
[32m[20221213 21:17:31 @agent_ppo2.py:185][0m |          -0.0019 |         124.1495 |          16.6112 |
[32m[20221213 21:17:32 @agent_ppo2.py:185][0m |           0.0025 |         125.1851 |          16.6213 |
[32m[20221213 21:17:32 @agent_ppo2.py:185][0m |          -0.0044 |         122.2420 |          16.6594 |
[32m[20221213 21:17:32 @agent_ppo2.py:185][0m |          -0.0069 |         121.7425 |          16.6756 |
[32m[20221213 21:17:32 @agent_ppo2.py:185][0m |          -0.0037 |         121.3078 |          16.6943 |
[32m[20221213 21:17:32 @agent_ppo2.py:185][0m |           0.0075 |         130.5939 |          16.6951 |
[32m[20221213 21:17:32 @agent_ppo2.py:185][0m |          -0.0077 |         121.1322 |          16.7197 |
[32m[20221213 21:17:32 @agent_ppo2.py:185][0m |           0.0039 |         133.7220 |          16.7347 |
[32m[20221213 21:17:32 @agent_ppo2.py:185][0m |           0.0020 |         127.2572 |          16.7565 |
[32m[20221213 21:17:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 639.00
[32m[20221213 21:17:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 686.00
[32m[20221213 21:17:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 658.00
[32m[20221213 21:17:32 @agent_ppo2.py:143][0m Total time:      21.96 min
[32m[20221213 21:17:32 @agent_ppo2.py:145][0m 2150400 total steps have happened
[32m[20221213 21:17:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1050 --------------------------#
[32m[20221213 21:17:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:17:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:33 @agent_ppo2.py:185][0m |           0.0006 |         123.6727 |          16.6490 |
[32m[20221213 21:17:33 @agent_ppo2.py:185][0m |          -0.0045 |         122.6529 |          16.6446 |
[32m[20221213 21:17:33 @agent_ppo2.py:185][0m |          -0.0047 |         122.2315 |          16.6155 |
[32m[20221213 21:17:33 @agent_ppo2.py:185][0m |          -0.0000 |         123.2353 |          16.6154 |
[32m[20221213 21:17:33 @agent_ppo2.py:185][0m |           0.0008 |         124.4675 |          16.5955 |
[32m[20221213 21:17:33 @agent_ppo2.py:185][0m |          -0.0103 |         121.2756 |          16.5808 |
[32m[20221213 21:17:33 @agent_ppo2.py:185][0m |          -0.0064 |         120.8806 |          16.5760 |
[32m[20221213 21:17:33 @agent_ppo2.py:185][0m |           0.0025 |         129.6407 |          16.5671 |
[32m[20221213 21:17:33 @agent_ppo2.py:185][0m |          -0.0050 |         120.5774 |          16.5556 |
[32m[20221213 21:17:33 @agent_ppo2.py:185][0m |          -0.0096 |         120.3885 |          16.5465 |
[32m[20221213 21:17:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 640.20
[32m[20221213 21:17:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 687.00
[32m[20221213 21:17:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 659.00
[32m[20221213 21:17:34 @agent_ppo2.py:143][0m Total time:      21.98 min
[32m[20221213 21:17:34 @agent_ppo2.py:145][0m 2152448 total steps have happened
[32m[20221213 21:17:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1051 --------------------------#
[32m[20221213 21:17:34 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:17:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:34 @agent_ppo2.py:185][0m |          -0.0031 |         124.5468 |          16.4462 |
[32m[20221213 21:17:34 @agent_ppo2.py:185][0m |          -0.0015 |         123.2117 |          16.4412 |
[32m[20221213 21:17:34 @agent_ppo2.py:185][0m |          -0.0024 |         120.8042 |          16.4261 |
[32m[20221213 21:17:34 @agent_ppo2.py:185][0m |           0.0070 |         126.7131 |          16.4097 |
[32m[20221213 21:17:34 @agent_ppo2.py:185][0m |           0.0019 |         120.5479 |          16.4109 |
[32m[20221213 21:17:34 @agent_ppo2.py:185][0m |          -0.0016 |         117.2255 |          16.4008 |
[32m[20221213 21:17:34 @agent_ppo2.py:185][0m |          -0.0084 |         115.0470 |          16.4111 |
[32m[20221213 21:17:34 @agent_ppo2.py:185][0m |          -0.0061 |         114.1003 |          16.4005 |
[32m[20221213 21:17:35 @agent_ppo2.py:185][0m |          -0.0063 |         113.2549 |          16.3916 |
[32m[20221213 21:17:35 @agent_ppo2.py:185][0m |           0.0012 |         115.5287 |          16.3933 |
[32m[20221213 21:17:35 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:17:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 651.00
[32m[20221213 21:17:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 720.00
[32m[20221213 21:17:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 763.00
[32m[20221213 21:17:35 @agent_ppo2.py:143][0m Total time:      22.00 min
[32m[20221213 21:17:35 @agent_ppo2.py:145][0m 2154496 total steps have happened
[32m[20221213 21:17:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1052 --------------------------#
[32m[20221213 21:17:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:35 @agent_ppo2.py:185][0m |          -0.0002 |         126.8653 |          16.4995 |
[32m[20221213 21:17:35 @agent_ppo2.py:185][0m |           0.0063 |         136.8913 |          16.4944 |
[32m[20221213 21:17:35 @agent_ppo2.py:185][0m |          -0.0036 |         124.6187 |          16.5039 |
[32m[20221213 21:17:35 @agent_ppo2.py:185][0m |          -0.0049 |         123.9164 |          16.5155 |
[32m[20221213 21:17:35 @agent_ppo2.py:185][0m |          -0.0055 |         123.2241 |          16.5207 |
[32m[20221213 21:17:35 @agent_ppo2.py:185][0m |          -0.0071 |         122.7585 |          16.5269 |
[32m[20221213 21:17:36 @agent_ppo2.py:185][0m |          -0.0077 |         122.7071 |          16.5316 |
[32m[20221213 21:17:36 @agent_ppo2.py:185][0m |          -0.0018 |         123.1712 |          16.5438 |
[32m[20221213 21:17:36 @agent_ppo2.py:185][0m |           0.0016 |         127.5337 |          16.5501 |
[32m[20221213 21:17:36 @agent_ppo2.py:185][0m |          -0.0071 |         121.8469 |          16.5590 |
[32m[20221213 21:17:36 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:17:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 628.80
[32m[20221213 21:17:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 677.00
[32m[20221213 21:17:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 598.00
[32m[20221213 21:17:36 @agent_ppo2.py:143][0m Total time:      22.02 min
[32m[20221213 21:17:36 @agent_ppo2.py:145][0m 2156544 total steps have happened
[32m[20221213 21:17:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1053 --------------------------#
[32m[20221213 21:17:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:17:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:36 @agent_ppo2.py:185][0m |          -0.0007 |         125.3123 |          16.5507 |
[32m[20221213 21:17:36 @agent_ppo2.py:185][0m |          -0.0020 |         126.0838 |          16.5071 |
[32m[20221213 21:17:36 @agent_ppo2.py:185][0m |           0.0152 |         137.4175 |          16.4806 |
[32m[20221213 21:17:37 @agent_ppo2.py:185][0m |          -0.0056 |         123.1011 |          16.4884 |
[32m[20221213 21:17:37 @agent_ppo2.py:185][0m |          -0.0062 |         122.5341 |          16.4661 |
[32m[20221213 21:17:37 @agent_ppo2.py:185][0m |          -0.0049 |         122.8405 |          16.4812 |
[32m[20221213 21:17:37 @agent_ppo2.py:185][0m |          -0.0048 |         121.5257 |          16.4624 |
[32m[20221213 21:17:37 @agent_ppo2.py:185][0m |          -0.0079 |         121.2171 |          16.4607 |
[32m[20221213 21:17:37 @agent_ppo2.py:185][0m |          -0.0092 |         120.8680 |          16.4469 |
[32m[20221213 21:17:37 @agent_ppo2.py:185][0m |          -0.0081 |         120.6181 |          16.4640 |
[32m[20221213 21:17:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 638.80
[32m[20221213 21:17:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 654.00
[32m[20221213 21:17:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 676.00
[32m[20221213 21:17:37 @agent_ppo2.py:143][0m Total time:      22.04 min
[32m[20221213 21:17:37 @agent_ppo2.py:145][0m 2158592 total steps have happened
[32m[20221213 21:17:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1054 --------------------------#
[32m[20221213 21:17:37 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:17:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:37 @agent_ppo2.py:185][0m |          -0.0016 |         129.0782 |          16.3318 |
[32m[20221213 21:17:38 @agent_ppo2.py:185][0m |          -0.0077 |         127.5632 |          16.3159 |
[32m[20221213 21:17:38 @agent_ppo2.py:185][0m |          -0.0029 |         126.6822 |          16.3472 |
[32m[20221213 21:17:38 @agent_ppo2.py:185][0m |          -0.0036 |         126.1907 |          16.3297 |
[32m[20221213 21:17:38 @agent_ppo2.py:185][0m |          -0.0084 |         125.8856 |          16.3208 |
[32m[20221213 21:17:38 @agent_ppo2.py:185][0m |           0.0017 |         131.4451 |          16.3364 |
[32m[20221213 21:17:38 @agent_ppo2.py:185][0m |          -0.0068 |         125.1887 |          16.3255 |
[32m[20221213 21:17:38 @agent_ppo2.py:185][0m |          -0.0102 |         124.7146 |          16.3296 |
[32m[20221213 21:17:38 @agent_ppo2.py:185][0m |          -0.0091 |         124.4036 |          16.3275 |
[32m[20221213 21:17:38 @agent_ppo2.py:185][0m |          -0.0097 |         124.3466 |          16.3292 |
[32m[20221213 21:17:38 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:17:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 631.00
[32m[20221213 21:17:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 702.00
[32m[20221213 21:17:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 714.00
[32m[20221213 21:17:38 @agent_ppo2.py:143][0m Total time:      22.06 min
[32m[20221213 21:17:38 @agent_ppo2.py:145][0m 2160640 total steps have happened
[32m[20221213 21:17:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1055 --------------------------#
[32m[20221213 21:17:39 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:17:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:39 @agent_ppo2.py:185][0m |           0.0005 |         125.6844 |          16.4237 |
[32m[20221213 21:17:39 @agent_ppo2.py:185][0m |           0.0105 |         133.2112 |          16.4089 |
[32m[20221213 21:17:39 @agent_ppo2.py:185][0m |          -0.0012 |         123.3896 |          16.4297 |
[32m[20221213 21:17:39 @agent_ppo2.py:185][0m |          -0.0069 |         123.2192 |          16.4068 |
[32m[20221213 21:17:39 @agent_ppo2.py:185][0m |          -0.0034 |         123.0563 |          16.3970 |
[32m[20221213 21:17:39 @agent_ppo2.py:185][0m |          -0.0052 |         122.4514 |          16.4223 |
[32m[20221213 21:17:39 @agent_ppo2.py:185][0m |          -0.0075 |         122.2977 |          16.4016 |
[32m[20221213 21:17:39 @agent_ppo2.py:185][0m |          -0.0090 |         122.3457 |          16.3923 |
[32m[20221213 21:17:39 @agent_ppo2.py:185][0m |           0.0102 |         136.4523 |          16.4034 |
[32m[20221213 21:17:39 @agent_ppo2.py:185][0m |          -0.0059 |         121.9753 |          16.4123 |
[32m[20221213 21:17:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:17:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 676.20
[32m[20221213 21:17:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.00
[32m[20221213 21:17:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.00
[32m[20221213 21:17:40 @agent_ppo2.py:143][0m Total time:      22.08 min
[32m[20221213 21:17:40 @agent_ppo2.py:145][0m 2162688 total steps have happened
[32m[20221213 21:17:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1056 --------------------------#
[32m[20221213 21:17:40 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:17:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:40 @agent_ppo2.py:185][0m |           0.0102 |         134.0397 |          16.4856 |
[32m[20221213 21:17:40 @agent_ppo2.py:185][0m |           0.0001 |         125.3490 |          16.4846 |
[32m[20221213 21:17:40 @agent_ppo2.py:185][0m |          -0.0037 |         124.1692 |          16.4982 |
[32m[20221213 21:17:40 @agent_ppo2.py:185][0m |          -0.0040 |         123.7661 |          16.4796 |
[32m[20221213 21:17:40 @agent_ppo2.py:185][0m |          -0.0048 |         123.3723 |          16.5079 |
[32m[20221213 21:17:40 @agent_ppo2.py:185][0m |          -0.0013 |         125.1509 |          16.5085 |
[32m[20221213 21:17:40 @agent_ppo2.py:185][0m |           0.0105 |         132.9061 |          16.4929 |
[32m[20221213 21:17:41 @agent_ppo2.py:185][0m |          -0.0079 |         122.8073 |          16.5085 |
[32m[20221213 21:17:41 @agent_ppo2.py:185][0m |          -0.0093 |         122.5790 |          16.5061 |
[32m[20221213 21:17:41 @agent_ppo2.py:185][0m |          -0.0051 |         122.4335 |          16.5040 |
[32m[20221213 21:17:41 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:17:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 646.20
[32m[20221213 21:17:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 690.00
[32m[20221213 21:17:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 715.00
[32m[20221213 21:17:41 @agent_ppo2.py:143][0m Total time:      22.10 min
[32m[20221213 21:17:41 @agent_ppo2.py:145][0m 2164736 total steps have happened
[32m[20221213 21:17:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1057 --------------------------#
[32m[20221213 21:17:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:41 @agent_ppo2.py:185][0m |          -0.0033 |         120.3387 |          16.7106 |
[32m[20221213 21:17:41 @agent_ppo2.py:185][0m |          -0.0020 |         120.3455 |          16.6842 |
[32m[20221213 21:17:41 @agent_ppo2.py:185][0m |          -0.0028 |         119.2665 |          16.6784 |
[32m[20221213 21:17:41 @agent_ppo2.py:185][0m |          -0.0055 |         118.4002 |          16.6897 |
[32m[20221213 21:17:42 @agent_ppo2.py:185][0m |          -0.0072 |         118.1438 |          16.6873 |
[32m[20221213 21:17:42 @agent_ppo2.py:185][0m |          -0.0079 |         117.9816 |          16.6932 |
[32m[20221213 21:17:42 @agent_ppo2.py:185][0m |          -0.0064 |         117.7529 |          16.6948 |
[32m[20221213 21:17:42 @agent_ppo2.py:185][0m |          -0.0067 |         117.6274 |          16.6903 |
[32m[20221213 21:17:42 @agent_ppo2.py:185][0m |          -0.0059 |         117.7266 |          16.6900 |
[32m[20221213 21:17:42 @agent_ppo2.py:185][0m |          -0.0076 |         117.2572 |          16.7010 |
[32m[20221213 21:17:42 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:17:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 663.20
[32m[20221213 21:17:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 688.00
[32m[20221213 21:17:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 655.00
[32m[20221213 21:17:42 @agent_ppo2.py:143][0m Total time:      22.12 min
[32m[20221213 21:17:42 @agent_ppo2.py:145][0m 2166784 total steps have happened
[32m[20221213 21:17:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1058 --------------------------#
[32m[20221213 21:17:42 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:17:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:42 @agent_ppo2.py:185][0m |          -0.0021 |         124.4917 |          16.5199 |
[32m[20221213 21:17:43 @agent_ppo2.py:185][0m |          -0.0038 |         122.2298 |          16.5185 |
[32m[20221213 21:17:43 @agent_ppo2.py:185][0m |          -0.0070 |         120.9931 |          16.5225 |
[32m[20221213 21:17:43 @agent_ppo2.py:185][0m |          -0.0061 |         120.2188 |          16.5240 |
[32m[20221213 21:17:43 @agent_ppo2.py:185][0m |          -0.0062 |         119.5196 |          16.5363 |
[32m[20221213 21:17:43 @agent_ppo2.py:185][0m |          -0.0066 |         119.0193 |          16.5243 |
[32m[20221213 21:17:43 @agent_ppo2.py:185][0m |          -0.0048 |         118.4024 |          16.5422 |
[32m[20221213 21:17:43 @agent_ppo2.py:185][0m |          -0.0097 |         118.0060 |          16.5323 |
[32m[20221213 21:17:43 @agent_ppo2.py:185][0m |          -0.0081 |         117.6037 |          16.5593 |
[32m[20221213 21:17:43 @agent_ppo2.py:185][0m |           0.0023 |         125.1517 |          16.5488 |
[32m[20221213 21:17:43 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:17:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 693.60
[32m[20221213 21:17:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.00
[32m[20221213 21:17:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 745.00
[32m[20221213 21:17:43 @agent_ppo2.py:143][0m Total time:      22.14 min
[32m[20221213 21:17:43 @agent_ppo2.py:145][0m 2168832 total steps have happened
[32m[20221213 21:17:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1059 --------------------------#
[32m[20221213 21:17:44 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:17:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:44 @agent_ppo2.py:185][0m |           0.0005 |         129.2049 |          16.6611 |
[32m[20221213 21:17:44 @agent_ppo2.py:185][0m |          -0.0020 |         126.7665 |          16.6235 |
[32m[20221213 21:17:44 @agent_ppo2.py:185][0m |           0.0012 |         127.0118 |          16.6224 |
[32m[20221213 21:17:44 @agent_ppo2.py:185][0m |          -0.0030 |         124.5669 |          16.6039 |
[32m[20221213 21:17:44 @agent_ppo2.py:185][0m |          -0.0078 |         124.3476 |          16.5959 |
[32m[20221213 21:17:44 @agent_ppo2.py:185][0m |          -0.0051 |         123.9943 |          16.5923 |
[32m[20221213 21:17:44 @agent_ppo2.py:185][0m |           0.0032 |         129.2675 |          16.5836 |
[32m[20221213 21:17:44 @agent_ppo2.py:185][0m |          -0.0085 |         123.4712 |          16.6033 |
[32m[20221213 21:17:44 @agent_ppo2.py:185][0m |          -0.0084 |         123.0951 |          16.5757 |
[32m[20221213 21:17:44 @agent_ppo2.py:185][0m |          -0.0111 |         122.8379 |          16.5499 |
[32m[20221213 21:17:44 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:17:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 661.00
[32m[20221213 21:17:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 683.00
[32m[20221213 21:17:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 711.00
[32m[20221213 21:17:45 @agent_ppo2.py:143][0m Total time:      22.17 min
[32m[20221213 21:17:45 @agent_ppo2.py:145][0m 2170880 total steps have happened
[32m[20221213 21:17:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1060 --------------------------#
[32m[20221213 21:17:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:45 @agent_ppo2.py:185][0m |          -0.0024 |         129.1142 |          16.4961 |
[32m[20221213 21:17:45 @agent_ppo2.py:185][0m |          -0.0053 |         127.4851 |          16.4783 |
[32m[20221213 21:17:45 @agent_ppo2.py:185][0m |          -0.0090 |         126.6890 |          16.4620 |
[32m[20221213 21:17:45 @agent_ppo2.py:185][0m |          -0.0060 |         126.3992 |          16.4534 |
[32m[20221213 21:17:45 @agent_ppo2.py:185][0m |          -0.0066 |         126.1764 |          16.4282 |
[32m[20221213 21:17:45 @agent_ppo2.py:185][0m |           0.0055 |         143.3687 |          16.4195 |
[32m[20221213 21:17:45 @agent_ppo2.py:185][0m |          -0.0097 |         125.6964 |          16.4229 |
[32m[20221213 21:17:46 @agent_ppo2.py:185][0m |          -0.0098 |         125.3352 |          16.3952 |
[32m[20221213 21:17:46 @agent_ppo2.py:185][0m |          -0.0120 |         125.2629 |          16.4082 |
[32m[20221213 21:17:46 @agent_ppo2.py:185][0m |          -0.0102 |         125.0262 |          16.3889 |
[32m[20221213 21:17:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 633.80
[32m[20221213 21:17:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 662.00
[32m[20221213 21:17:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 671.00
[32m[20221213 21:17:46 @agent_ppo2.py:143][0m Total time:      22.19 min
[32m[20221213 21:17:46 @agent_ppo2.py:145][0m 2172928 total steps have happened
[32m[20221213 21:17:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1061 --------------------------#
[32m[20221213 21:17:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:46 @agent_ppo2.py:185][0m |          -0.0009 |         128.6447 |          16.4047 |
[32m[20221213 21:17:46 @agent_ppo2.py:185][0m |          -0.0057 |         126.2694 |          16.4103 |
[32m[20221213 21:17:46 @agent_ppo2.py:185][0m |          -0.0055 |         125.3799 |          16.4137 |
[32m[20221213 21:17:46 @agent_ppo2.py:185][0m |          -0.0065 |         124.8536 |          16.4204 |
[32m[20221213 21:17:46 @agent_ppo2.py:185][0m |          -0.0052 |         124.2553 |          16.4433 |
[32m[20221213 21:17:47 @agent_ppo2.py:185][0m |          -0.0057 |         124.0847 |          16.4592 |
[32m[20221213 21:17:47 @agent_ppo2.py:185][0m |          -0.0083 |         123.6453 |          16.4672 |
[32m[20221213 21:17:47 @agent_ppo2.py:185][0m |          -0.0012 |         125.2099 |          16.4589 |
[32m[20221213 21:17:47 @agent_ppo2.py:185][0m |          -0.0063 |         123.4069 |          16.5053 |
[32m[20221213 21:17:47 @agent_ppo2.py:185][0m |          -0.0065 |         123.0474 |          16.5115 |
[32m[20221213 21:17:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 612.20
[32m[20221213 21:17:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 714.00
[32m[20221213 21:17:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 730.00
[32m[20221213 21:17:47 @agent_ppo2.py:143][0m Total time:      22.21 min
[32m[20221213 21:17:47 @agent_ppo2.py:145][0m 2174976 total steps have happened
[32m[20221213 21:17:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1062 --------------------------#
[32m[20221213 21:17:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:47 @agent_ppo2.py:185][0m |          -0.0040 |         127.6015 |          16.2967 |
[32m[20221213 21:17:47 @agent_ppo2.py:185][0m |          -0.0045 |         125.9073 |          16.3223 |
[32m[20221213 21:17:47 @agent_ppo2.py:185][0m |          -0.0064 |         124.9995 |          16.3387 |
[32m[20221213 21:17:48 @agent_ppo2.py:185][0m |           0.0073 |         137.0401 |          16.3528 |
[32m[20221213 21:17:48 @agent_ppo2.py:185][0m |          -0.0050 |         123.8533 |          16.3573 |
[32m[20221213 21:17:48 @agent_ppo2.py:185][0m |           0.0035 |         130.4754 |          16.3676 |
[32m[20221213 21:17:48 @agent_ppo2.py:185][0m |          -0.0060 |         123.1462 |          16.3852 |
[32m[20221213 21:17:48 @agent_ppo2.py:185][0m |          -0.0035 |         122.8938 |          16.4020 |
[32m[20221213 21:17:48 @agent_ppo2.py:185][0m |          -0.0064 |         122.5608 |          16.3961 |
[32m[20221213 21:17:48 @agent_ppo2.py:185][0m |          -0.0108 |         122.0794 |          16.4069 |
[32m[20221213 21:17:48 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:17:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.00
[32m[20221213 21:17:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 702.00
[32m[20221213 21:17:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 747.00
[32m[20221213 21:17:48 @agent_ppo2.py:143][0m Total time:      22.23 min
[32m[20221213 21:17:48 @agent_ppo2.py:145][0m 2177024 total steps have happened
[32m[20221213 21:17:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1063 --------------------------#
[32m[20221213 21:17:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:17:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:49 @agent_ppo2.py:185][0m |           0.0054 |         130.7455 |          16.5625 |
[32m[20221213 21:17:49 @agent_ppo2.py:185][0m |          -0.0003 |         123.2870 |          16.5746 |
[32m[20221213 21:17:49 @agent_ppo2.py:185][0m |          -0.0024 |         122.1875 |          16.5844 |
[32m[20221213 21:17:49 @agent_ppo2.py:185][0m |           0.0073 |         135.3891 |          16.6055 |
[32m[20221213 21:17:49 @agent_ppo2.py:185][0m |          -0.0038 |         120.3338 |          16.5885 |
[32m[20221213 21:17:49 @agent_ppo2.py:185][0m |          -0.0072 |         119.8165 |          16.6146 |
[32m[20221213 21:17:49 @agent_ppo2.py:185][0m |          -0.0051 |         119.7553 |          16.6436 |
[32m[20221213 21:17:49 @agent_ppo2.py:185][0m |          -0.0079 |         119.1731 |          16.6483 |
[32m[20221213 21:17:49 @agent_ppo2.py:185][0m |          -0.0039 |         119.1206 |          16.6370 |
[32m[20221213 21:17:49 @agent_ppo2.py:185][0m |          -0.0099 |         118.6035 |          16.6630 |
[32m[20221213 21:17:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 615.40
[32m[20221213 21:17:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 653.00
[32m[20221213 21:17:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 605.00
[32m[20221213 21:17:49 @agent_ppo2.py:143][0m Total time:      22.25 min
[32m[20221213 21:17:49 @agent_ppo2.py:145][0m 2179072 total steps have happened
[32m[20221213 21:17:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1064 --------------------------#
[32m[20221213 21:17:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:50 @agent_ppo2.py:185][0m |          -0.0002 |         128.8382 |          16.6265 |
[32m[20221213 21:17:50 @agent_ppo2.py:185][0m |          -0.0041 |         127.7183 |          16.6273 |
[32m[20221213 21:17:50 @agent_ppo2.py:185][0m |          -0.0064 |         126.6516 |          16.6332 |
[32m[20221213 21:17:50 @agent_ppo2.py:185][0m |          -0.0064 |         125.8877 |          16.6285 |
[32m[20221213 21:17:50 @agent_ppo2.py:185][0m |          -0.0074 |         125.1630 |          16.6322 |
[32m[20221213 21:17:50 @agent_ppo2.py:185][0m |          -0.0113 |         125.0108 |          16.6221 |
[32m[20221213 21:17:50 @agent_ppo2.py:185][0m |          -0.0076 |         124.3949 |          16.6026 |
[32m[20221213 21:17:50 @agent_ppo2.py:185][0m |          -0.0104 |         124.2607 |          16.6170 |
[32m[20221213 21:17:50 @agent_ppo2.py:185][0m |          -0.0098 |         123.9199 |          16.6249 |
[32m[20221213 21:17:51 @agent_ppo2.py:185][0m |          -0.0059 |         123.4271 |          16.6107 |
[32m[20221213 21:17:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 658.00
[32m[20221213 21:17:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.00
[32m[20221213 21:17:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 742.00
[32m[20221213 21:17:51 @agent_ppo2.py:143][0m Total time:      22.27 min
[32m[20221213 21:17:51 @agent_ppo2.py:145][0m 2181120 total steps have happened
[32m[20221213 21:17:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1065 --------------------------#
[32m[20221213 21:17:51 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:17:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:51 @agent_ppo2.py:185][0m |          -0.0000 |         130.1140 |          16.6080 |
[32m[20221213 21:17:51 @agent_ppo2.py:185][0m |          -0.0040 |         126.8534 |          16.6163 |
[32m[20221213 21:17:51 @agent_ppo2.py:185][0m |          -0.0069 |         125.3642 |          16.6160 |
[32m[20221213 21:17:51 @agent_ppo2.py:185][0m |          -0.0055 |         124.6201 |          16.6254 |
[32m[20221213 21:17:51 @agent_ppo2.py:185][0m |          -0.0075 |         123.8003 |          16.6332 |
[32m[20221213 21:17:51 @agent_ppo2.py:185][0m |          -0.0029 |         123.6609 |          16.6281 |
[32m[20221213 21:17:51 @agent_ppo2.py:185][0m |          -0.0080 |         123.1891 |          16.6459 |
[32m[20221213 21:17:52 @agent_ppo2.py:185][0m |          -0.0074 |         123.1288 |          16.6493 |
[32m[20221213 21:17:52 @agent_ppo2.py:185][0m |          -0.0076 |         122.1628 |          16.6507 |
[32m[20221213 21:17:52 @agent_ppo2.py:185][0m |          -0.0094 |         121.8552 |          16.6427 |
[32m[20221213 21:17:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 639.60
[32m[20221213 21:17:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 680.00
[32m[20221213 21:17:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 670.00
[32m[20221213 21:17:52 @agent_ppo2.py:143][0m Total time:      22.29 min
[32m[20221213 21:17:52 @agent_ppo2.py:145][0m 2183168 total steps have happened
[32m[20221213 21:17:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1066 --------------------------#
[32m[20221213 21:17:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:17:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:52 @agent_ppo2.py:185][0m |           0.0022 |         129.1756 |          16.7090 |
[32m[20221213 21:17:52 @agent_ppo2.py:185][0m |          -0.0056 |         126.4377 |          16.7109 |
[32m[20221213 21:17:52 @agent_ppo2.py:185][0m |           0.0008 |         127.9608 |          16.7121 |
[32m[20221213 21:17:52 @agent_ppo2.py:185][0m |          -0.0010 |         126.4509 |          16.7148 |
[32m[20221213 21:17:52 @agent_ppo2.py:185][0m |          -0.0073 |         124.9423 |          16.7221 |
[32m[20221213 21:17:53 @agent_ppo2.py:185][0m |          -0.0023 |         124.7508 |          16.7234 |
[32m[20221213 21:17:53 @agent_ppo2.py:185][0m |          -0.0090 |         124.5634 |          16.7240 |
[32m[20221213 21:17:53 @agent_ppo2.py:185][0m |          -0.0093 |         124.4418 |          16.7238 |
[32m[20221213 21:17:53 @agent_ppo2.py:185][0m |          -0.0011 |         126.6948 |          16.7240 |
[32m[20221213 21:17:53 @agent_ppo2.py:185][0m |          -0.0053 |         123.9339 |          16.7272 |
[32m[20221213 21:17:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:17:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 673.40
[32m[20221213 21:17:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 707.00
[32m[20221213 21:17:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 774.00
[32m[20221213 21:17:53 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 774.00
[32m[20221213 21:17:53 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 774.00
[32m[20221213 21:17:53 @agent_ppo2.py:143][0m Total time:      22.31 min
[32m[20221213 21:17:53 @agent_ppo2.py:145][0m 2185216 total steps have happened
[32m[20221213 21:17:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1067 --------------------------#
[32m[20221213 21:17:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:53 @agent_ppo2.py:185][0m |          -0.0030 |         127.6364 |          16.7104 |
[32m[20221213 21:17:53 @agent_ppo2.py:185][0m |          -0.0035 |         125.9901 |          16.6979 |
[32m[20221213 21:17:54 @agent_ppo2.py:185][0m |          -0.0063 |         125.4325 |          16.6896 |
[32m[20221213 21:17:54 @agent_ppo2.py:185][0m |          -0.0076 |         124.9416 |          16.7041 |
[32m[20221213 21:17:54 @agent_ppo2.py:185][0m |          -0.0084 |         124.7767 |          16.6806 |
[32m[20221213 21:17:54 @agent_ppo2.py:185][0m |          -0.0058 |         124.8015 |          16.6873 |
[32m[20221213 21:17:54 @agent_ppo2.py:185][0m |          -0.0107 |         124.3144 |          16.6779 |
[32m[20221213 21:17:54 @agent_ppo2.py:185][0m |          -0.0048 |         125.5151 |          16.6819 |
[32m[20221213 21:17:54 @agent_ppo2.py:185][0m |          -0.0103 |         124.1402 |          16.6870 |
[32m[20221213 21:17:54 @agent_ppo2.py:185][0m |          -0.0083 |         123.8820 |          16.6769 |
[32m[20221213 21:17:54 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:17:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 655.20
[32m[20221213 21:17:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.00
[32m[20221213 21:17:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 773.00
[32m[20221213 21:17:54 @agent_ppo2.py:143][0m Total time:      22.33 min
[32m[20221213 21:17:54 @agent_ppo2.py:145][0m 2187264 total steps have happened
[32m[20221213 21:17:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1068 --------------------------#
[32m[20221213 21:17:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:55 @agent_ppo2.py:185][0m |           0.0007 |         121.9418 |          16.6938 |
[32m[20221213 21:17:55 @agent_ppo2.py:185][0m |          -0.0031 |         121.4917 |          16.6607 |
[32m[20221213 21:17:55 @agent_ppo2.py:185][0m |          -0.0052 |         120.6968 |          16.6792 |
[32m[20221213 21:17:55 @agent_ppo2.py:185][0m |          -0.0039 |         120.7869 |          16.6687 |
[32m[20221213 21:17:55 @agent_ppo2.py:185][0m |           0.0001 |         125.2146 |          16.6605 |
[32m[20221213 21:17:55 @agent_ppo2.py:185][0m |          -0.0036 |         121.8339 |          16.6455 |
[32m[20221213 21:17:55 @agent_ppo2.py:185][0m |          -0.0070 |         119.9876 |          16.6740 |
[32m[20221213 21:17:55 @agent_ppo2.py:185][0m |           0.0062 |         136.1545 |          16.6669 |
[32m[20221213 21:17:55 @agent_ppo2.py:185][0m |          -0.0079 |         119.8769 |          16.6728 |
[32m[20221213 21:17:55 @agent_ppo2.py:185][0m |          -0.0026 |         121.8007 |          16.6826 |
[32m[20221213 21:17:55 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:17:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 684.20
[32m[20221213 21:17:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 732.00
[32m[20221213 21:17:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 607.00
[32m[20221213 21:17:55 @agent_ppo2.py:143][0m Total time:      22.35 min
[32m[20221213 21:17:55 @agent_ppo2.py:145][0m 2189312 total steps have happened
[32m[20221213 21:17:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1069 --------------------------#
[32m[20221213 21:17:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:56 @agent_ppo2.py:185][0m |           0.0003 |         127.0704 |          16.8874 |
[32m[20221213 21:17:56 @agent_ppo2.py:185][0m |           0.0104 |         138.8488 |          16.8770 |
[32m[20221213 21:17:56 @agent_ppo2.py:185][0m |           0.0020 |         125.4075 |          16.8808 |
[32m[20221213 21:17:56 @agent_ppo2.py:185][0m |          -0.0017 |         123.9559 |          16.8691 |
[32m[20221213 21:17:56 @agent_ppo2.py:185][0m |           0.0063 |         131.1113 |          16.8500 |
[32m[20221213 21:17:56 @agent_ppo2.py:185][0m |          -0.0055 |         122.9882 |          16.8495 |
[32m[20221213 21:17:56 @agent_ppo2.py:185][0m |          -0.0007 |         123.1424 |          16.8473 |
[32m[20221213 21:17:56 @agent_ppo2.py:185][0m |          -0.0027 |         122.4608 |          16.8380 |
[32m[20221213 21:17:56 @agent_ppo2.py:185][0m |          -0.0060 |         122.2607 |          16.8269 |
[32m[20221213 21:17:57 @agent_ppo2.py:185][0m |          -0.0068 |         121.6899 |          16.8284 |
[32m[20221213 21:17:57 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:17:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 678.60
[32m[20221213 21:17:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 736.00
[32m[20221213 21:17:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 714.00
[32m[20221213 21:17:57 @agent_ppo2.py:143][0m Total time:      22.37 min
[32m[20221213 21:17:57 @agent_ppo2.py:145][0m 2191360 total steps have happened
[32m[20221213 21:17:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1070 --------------------------#
[32m[20221213 21:17:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:57 @agent_ppo2.py:185][0m |           0.0004 |         128.0274 |          16.6324 |
[32m[20221213 21:17:57 @agent_ppo2.py:185][0m |           0.0110 |         138.3211 |          16.6175 |
[32m[20221213 21:17:57 @agent_ppo2.py:185][0m |          -0.0060 |         124.3600 |          16.6068 |
[32m[20221213 21:17:57 @agent_ppo2.py:185][0m |          -0.0078 |         123.5168 |          16.5873 |
[32m[20221213 21:17:57 @agent_ppo2.py:185][0m |          -0.0050 |         123.3208 |          16.5937 |
[32m[20221213 21:17:57 @agent_ppo2.py:185][0m |          -0.0065 |         122.6218 |          16.5626 |
[32m[20221213 21:17:58 @agent_ppo2.py:185][0m |          -0.0094 |         122.1625 |          16.5633 |
[32m[20221213 21:17:58 @agent_ppo2.py:185][0m |          -0.0086 |         121.7402 |          16.5433 |
[32m[20221213 21:17:58 @agent_ppo2.py:185][0m |          -0.0003 |         127.4453 |          16.5371 |
[32m[20221213 21:17:58 @agent_ppo2.py:185][0m |          -0.0076 |         120.7800 |          16.5267 |
[32m[20221213 21:17:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:17:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 625.40
[32m[20221213 21:17:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 681.00
[32m[20221213 21:17:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.00
[32m[20221213 21:17:58 @agent_ppo2.py:143][0m Total time:      22.39 min
[32m[20221213 21:17:58 @agent_ppo2.py:145][0m 2193408 total steps have happened
[32m[20221213 21:17:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1071 --------------------------#
[32m[20221213 21:17:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:17:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:58 @agent_ppo2.py:185][0m |           0.0054 |         130.7773 |          16.5199 |
[32m[20221213 21:17:58 @agent_ppo2.py:185][0m |          -0.0036 |         126.1778 |          16.5513 |
[32m[20221213 21:17:58 @agent_ppo2.py:185][0m |           0.0049 |         135.2767 |          16.5516 |
[32m[20221213 21:17:58 @agent_ppo2.py:185][0m |           0.0014 |         134.2499 |          16.5686 |
[32m[20221213 21:17:59 @agent_ppo2.py:185][0m |          -0.0037 |         125.4127 |          16.5606 |
[32m[20221213 21:17:59 @agent_ppo2.py:185][0m |          -0.0047 |         124.9820 |          16.5882 |
[32m[20221213 21:17:59 @agent_ppo2.py:185][0m |          -0.0072 |         124.4617 |          16.5991 |
[32m[20221213 21:17:59 @agent_ppo2.py:185][0m |          -0.0078 |         124.1211 |          16.5969 |
[32m[20221213 21:17:59 @agent_ppo2.py:185][0m |          -0.0016 |         127.4658 |          16.5868 |
[32m[20221213 21:17:59 @agent_ppo2.py:185][0m |          -0.0046 |         123.9721 |          16.6099 |
[32m[20221213 21:17:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:17:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 674.80
[32m[20221213 21:17:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 736.00
[32m[20221213 21:17:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 691.00
[32m[20221213 21:17:59 @agent_ppo2.py:143][0m Total time:      22.41 min
[32m[20221213 21:17:59 @agent_ppo2.py:145][0m 2195456 total steps have happened
[32m[20221213 21:17:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1072 --------------------------#
[32m[20221213 21:17:59 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:17:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:17:59 @agent_ppo2.py:185][0m |           0.0074 |         122.4563 |          16.5853 |
[32m[20221213 21:17:59 @agent_ppo2.py:185][0m |          -0.0054 |         116.3138 |          16.5787 |
[32m[20221213 21:18:00 @agent_ppo2.py:185][0m |          -0.0057 |         114.9964 |          16.5717 |
[32m[20221213 21:18:00 @agent_ppo2.py:185][0m |          -0.0055 |         113.5908 |          16.5756 |
[32m[20221213 21:18:00 @agent_ppo2.py:185][0m |          -0.0043 |         112.1516 |          16.5778 |
[32m[20221213 21:18:00 @agent_ppo2.py:185][0m |          -0.0043 |         111.8965 |          16.5891 |
[32m[20221213 21:18:00 @agent_ppo2.py:185][0m |          -0.0056 |         111.1573 |          16.5747 |
[32m[20221213 21:18:00 @agent_ppo2.py:185][0m |          -0.0072 |         110.8692 |          16.5624 |
[32m[20221213 21:18:00 @agent_ppo2.py:185][0m |          -0.0058 |         110.3595 |          16.5951 |
[32m[20221213 21:18:00 @agent_ppo2.py:185][0m |          -0.0050 |         109.9884 |          16.5794 |
[32m[20221213 21:18:00 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:18:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 612.60
[32m[20221213 21:18:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.00
[32m[20221213 21:18:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 771.00
[32m[20221213 21:18:00 @agent_ppo2.py:143][0m Total time:      22.43 min
[32m[20221213 21:18:00 @agent_ppo2.py:145][0m 2197504 total steps have happened
[32m[20221213 21:18:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1073 --------------------------#
[32m[20221213 21:18:00 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:18:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:01 @agent_ppo2.py:185][0m |          -0.0033 |         136.3765 |          16.5231 |
[32m[20221213 21:18:01 @agent_ppo2.py:185][0m |          -0.0061 |         132.5855 |          16.5017 |
[32m[20221213 21:18:01 @agent_ppo2.py:185][0m |          -0.0067 |         131.4300 |          16.4850 |
[32m[20221213 21:18:01 @agent_ppo2.py:185][0m |          -0.0082 |         130.9651 |          16.4722 |
[32m[20221213 21:18:01 @agent_ppo2.py:185][0m |          -0.0071 |         130.7912 |          16.4528 |
[32m[20221213 21:18:01 @agent_ppo2.py:185][0m |          -0.0061 |         130.1926 |          16.4503 |
[32m[20221213 21:18:01 @agent_ppo2.py:185][0m |          -0.0078 |         130.0540 |          16.4512 |
[32m[20221213 21:18:01 @agent_ppo2.py:185][0m |          -0.0079 |         129.7192 |          16.4426 |
[32m[20221213 21:18:01 @agent_ppo2.py:185][0m |          -0.0068 |         129.8666 |          16.4242 |
[32m[20221213 21:18:01 @agent_ppo2.py:185][0m |          -0.0109 |         129.4951 |          16.4449 |
[32m[20221213 21:18:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 658.20
[32m[20221213 21:18:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 692.00
[32m[20221213 21:18:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.00
[32m[20221213 21:18:02 @agent_ppo2.py:143][0m Total time:      22.45 min
[32m[20221213 21:18:02 @agent_ppo2.py:145][0m 2199552 total steps have happened
[32m[20221213 21:18:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1074 --------------------------#
[32m[20221213 21:18:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:02 @agent_ppo2.py:185][0m |           0.0042 |         130.3043 |          16.4707 |
[32m[20221213 21:18:02 @agent_ppo2.py:185][0m |          -0.0070 |         126.5583 |          16.4589 |
[32m[20221213 21:18:02 @agent_ppo2.py:185][0m |          -0.0060 |         125.6911 |          16.4381 |
[32m[20221213 21:18:02 @agent_ppo2.py:185][0m |          -0.0054 |         125.3186 |          16.4332 |
[32m[20221213 21:18:02 @agent_ppo2.py:185][0m |          -0.0080 |         124.7407 |          16.4136 |
[32m[20221213 21:18:02 @agent_ppo2.py:185][0m |          -0.0088 |         124.3044 |          16.4261 |
[32m[20221213 21:18:02 @agent_ppo2.py:185][0m |          -0.0050 |         124.0709 |          16.4052 |
[32m[20221213 21:18:02 @agent_ppo2.py:185][0m |          -0.0079 |         123.8060 |          16.4038 |
[32m[20221213 21:18:03 @agent_ppo2.py:185][0m |          -0.0092 |         123.3092 |          16.4055 |
[32m[20221213 21:18:03 @agent_ppo2.py:185][0m |          -0.0098 |         122.9976 |          16.3992 |
[32m[20221213 21:18:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 659.40
[32m[20221213 21:18:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 741.00
[32m[20221213 21:18:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 767.00
[32m[20221213 21:18:03 @agent_ppo2.py:143][0m Total time:      22.47 min
[32m[20221213 21:18:03 @agent_ppo2.py:145][0m 2201600 total steps have happened
[32m[20221213 21:18:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1075 --------------------------#
[32m[20221213 21:18:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:03 @agent_ppo2.py:185][0m |          -0.0006 |         128.0552 |          16.4224 |
[32m[20221213 21:18:03 @agent_ppo2.py:185][0m |          -0.0048 |         126.6545 |          16.3998 |
[32m[20221213 21:18:03 @agent_ppo2.py:185][0m |           0.0050 |         141.7367 |          16.3902 |
[32m[20221213 21:18:03 @agent_ppo2.py:185][0m |          -0.0045 |         125.7688 |          16.3460 |
[32m[20221213 21:18:03 @agent_ppo2.py:185][0m |          -0.0056 |         125.3176 |          16.3527 |
[32m[20221213 21:18:03 @agent_ppo2.py:185][0m |          -0.0069 |         124.8347 |          16.3462 |
[32m[20221213 21:18:04 @agent_ppo2.py:185][0m |          -0.0082 |         124.6768 |          16.3652 |
[32m[20221213 21:18:04 @agent_ppo2.py:185][0m |           0.0014 |         135.0550 |          16.3545 |
[32m[20221213 21:18:04 @agent_ppo2.py:185][0m |          -0.0092 |         124.5683 |          16.3245 |
[32m[20221213 21:18:04 @agent_ppo2.py:185][0m |          -0.0076 |         124.1854 |          16.3136 |
[32m[20221213 21:18:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 696.40
[32m[20221213 21:18:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.00
[32m[20221213 21:18:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 721.00
[32m[20221213 21:18:04 @agent_ppo2.py:143][0m Total time:      22.49 min
[32m[20221213 21:18:04 @agent_ppo2.py:145][0m 2203648 total steps have happened
[32m[20221213 21:18:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1076 --------------------------#
[32m[20221213 21:18:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:18:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:04 @agent_ppo2.py:185][0m |           0.0023 |         131.2049 |          16.3494 |
[32m[20221213 21:18:04 @agent_ppo2.py:185][0m |           0.0019 |         131.6089 |          16.3593 |
[32m[20221213 21:18:04 @agent_ppo2.py:185][0m |          -0.0067 |         127.5287 |          16.3655 |
[32m[20221213 21:18:04 @agent_ppo2.py:185][0m |          -0.0041 |         126.6742 |          16.3452 |
[32m[20221213 21:18:05 @agent_ppo2.py:185][0m |          -0.0013 |         129.5443 |          16.3585 |
[32m[20221213 21:18:05 @agent_ppo2.py:185][0m |          -0.0072 |         126.2117 |          16.3671 |
[32m[20221213 21:18:05 @agent_ppo2.py:185][0m |          -0.0068 |         125.8591 |          16.3696 |
[32m[20221213 21:18:05 @agent_ppo2.py:185][0m |          -0.0094 |         125.4104 |          16.3536 |
[32m[20221213 21:18:05 @agent_ppo2.py:185][0m |          -0.0095 |         125.1268 |          16.3633 |
[32m[20221213 21:18:05 @agent_ppo2.py:185][0m |          -0.0046 |         125.0617 |          16.3586 |
[32m[20221213 21:18:05 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:18:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 663.20
[32m[20221213 21:18:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 713.00
[32m[20221213 21:18:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 666.00
[32m[20221213 21:18:05 @agent_ppo2.py:143][0m Total time:      22.51 min
[32m[20221213 21:18:05 @agent_ppo2.py:145][0m 2205696 total steps have happened
[32m[20221213 21:18:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1077 --------------------------#
[32m[20221213 21:18:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:05 @agent_ppo2.py:185][0m |          -0.0005 |         131.8131 |          16.3778 |
[32m[20221213 21:18:06 @agent_ppo2.py:185][0m |          -0.0024 |         130.3615 |          16.3755 |
[32m[20221213 21:18:06 @agent_ppo2.py:185][0m |          -0.0048 |         129.6472 |          16.3753 |
[32m[20221213 21:18:06 @agent_ppo2.py:185][0m |          -0.0046 |         129.0916 |          16.3653 |
[32m[20221213 21:18:06 @agent_ppo2.py:185][0m |          -0.0010 |         128.8800 |          16.3664 |
[32m[20221213 21:18:06 @agent_ppo2.py:185][0m |          -0.0093 |         128.5210 |          16.3560 |
[32m[20221213 21:18:06 @agent_ppo2.py:185][0m |          -0.0059 |         128.3690 |          16.3321 |
[32m[20221213 21:18:06 @agent_ppo2.py:185][0m |           0.0009 |         131.6612 |          16.3248 |
[32m[20221213 21:18:06 @agent_ppo2.py:185][0m |          -0.0081 |         127.7711 |          16.3059 |
[32m[20221213 21:18:06 @agent_ppo2.py:185][0m |          -0.0074 |         127.8169 |          16.2968 |
[32m[20221213 21:18:06 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:18:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 652.40
[32m[20221213 21:18:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 713.00
[32m[20221213 21:18:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 697.00
[32m[20221213 21:18:06 @agent_ppo2.py:143][0m Total time:      22.53 min
[32m[20221213 21:18:06 @agent_ppo2.py:145][0m 2207744 total steps have happened
[32m[20221213 21:18:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1078 --------------------------#
[32m[20221213 21:18:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:07 @agent_ppo2.py:185][0m |          -0.0051 |         131.4345 |          16.3366 |
[32m[20221213 21:18:07 @agent_ppo2.py:185][0m |          -0.0051 |         129.9981 |          16.3659 |
[32m[20221213 21:18:07 @agent_ppo2.py:185][0m |          -0.0035 |         129.5888 |          16.3697 |
[32m[20221213 21:18:07 @agent_ppo2.py:185][0m |          -0.0077 |         128.9289 |          16.3907 |
[32m[20221213 21:18:07 @agent_ppo2.py:185][0m |          -0.0066 |         128.5177 |          16.4067 |
[32m[20221213 21:18:07 @agent_ppo2.py:185][0m |          -0.0086 |         128.2040 |          16.4164 |
[32m[20221213 21:18:07 @agent_ppo2.py:185][0m |          -0.0063 |         127.8287 |          16.4140 |
[32m[20221213 21:18:07 @agent_ppo2.py:185][0m |          -0.0070 |         127.6814 |          16.4291 |
[32m[20221213 21:18:07 @agent_ppo2.py:185][0m |          -0.0064 |         127.4178 |          16.4509 |
[32m[20221213 21:18:08 @agent_ppo2.py:185][0m |          -0.0089 |         127.3086 |          16.4353 |
[32m[20221213 21:18:08 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:18:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 676.00
[32m[20221213 21:18:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.00
[32m[20221213 21:18:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.00
[32m[20221213 21:18:08 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 796.00
[32m[20221213 21:18:08 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 796.00
[32m[20221213 21:18:08 @agent_ppo2.py:143][0m Total time:      22.55 min
[32m[20221213 21:18:08 @agent_ppo2.py:145][0m 2209792 total steps have happened
[32m[20221213 21:18:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1079 --------------------------#
[32m[20221213 21:18:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:18:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:08 @agent_ppo2.py:185][0m |           0.0011 |         131.4325 |          16.3434 |
[32m[20221213 21:18:08 @agent_ppo2.py:185][0m |          -0.0053 |         129.3473 |          16.3364 |
[32m[20221213 21:18:08 @agent_ppo2.py:185][0m |          -0.0043 |         128.4206 |          16.3398 |
[32m[20221213 21:18:08 @agent_ppo2.py:185][0m |           0.0047 |         140.1002 |          16.3430 |
[32m[20221213 21:18:08 @agent_ppo2.py:185][0m |           0.0005 |         133.6080 |          16.3264 |
[32m[20221213 21:18:08 @agent_ppo2.py:185][0m |          -0.0081 |         126.6051 |          16.3384 |
[32m[20221213 21:18:08 @agent_ppo2.py:185][0m |          -0.0066 |         126.3914 |          16.3387 |
[32m[20221213 21:18:09 @agent_ppo2.py:185][0m |          -0.0086 |         125.7245 |          16.3485 |
[32m[20221213 21:18:09 @agent_ppo2.py:185][0m |          -0.0095 |         125.5004 |          16.3365 |
[32m[20221213 21:18:09 @agent_ppo2.py:185][0m |          -0.0066 |         125.9797 |          16.3383 |
[32m[20221213 21:18:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:18:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 639.00
[32m[20221213 21:18:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.00
[32m[20221213 21:18:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 764.00
[32m[20221213 21:18:09 @agent_ppo2.py:143][0m Total time:      22.57 min
[32m[20221213 21:18:09 @agent_ppo2.py:145][0m 2211840 total steps have happened
[32m[20221213 21:18:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1080 --------------------------#
[32m[20221213 21:18:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:09 @agent_ppo2.py:185][0m |          -0.0032 |         132.2440 |          16.4438 |
[32m[20221213 21:18:09 @agent_ppo2.py:185][0m |          -0.0000 |         131.8718 |          16.4643 |
[32m[20221213 21:18:09 @agent_ppo2.py:185][0m |          -0.0028 |         130.9388 |          16.4832 |
[32m[20221213 21:18:09 @agent_ppo2.py:185][0m |          -0.0044 |         129.6641 |          16.4864 |
[32m[20221213 21:18:10 @agent_ppo2.py:185][0m |           0.0016 |         131.4951 |          16.5155 |
[32m[20221213 21:18:10 @agent_ppo2.py:185][0m |          -0.0084 |         128.9774 |          16.5038 |
[32m[20221213 21:18:10 @agent_ppo2.py:185][0m |          -0.0094 |         128.5547 |          16.5267 |
[32m[20221213 21:18:10 @agent_ppo2.py:185][0m |          -0.0065 |         128.1995 |          16.5086 |
[32m[20221213 21:18:10 @agent_ppo2.py:185][0m |          -0.0104 |         128.0287 |          16.5246 |
[32m[20221213 21:18:10 @agent_ppo2.py:185][0m |           0.0035 |         138.0429 |          16.5309 |
[32m[20221213 21:18:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:18:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 662.80
[32m[20221213 21:18:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.00
[32m[20221213 21:18:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.00
[32m[20221213 21:18:10 @agent_ppo2.py:143][0m Total time:      22.59 min
[32m[20221213 21:18:10 @agent_ppo2.py:145][0m 2213888 total steps have happened
[32m[20221213 21:18:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1081 --------------------------#
[32m[20221213 21:18:10 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:18:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:10 @agent_ppo2.py:185][0m |           0.0039 |         131.8997 |          16.4495 |
[32m[20221213 21:18:10 @agent_ppo2.py:185][0m |          -0.0042 |         129.4950 |          16.4521 |
[32m[20221213 21:18:11 @agent_ppo2.py:185][0m |          -0.0011 |         129.2691 |          16.4393 |
[32m[20221213 21:18:11 @agent_ppo2.py:185][0m |          -0.0040 |         128.8016 |          16.4391 |
[32m[20221213 21:18:11 @agent_ppo2.py:185][0m |          -0.0069 |         128.5757 |          16.4332 |
[32m[20221213 21:18:11 @agent_ppo2.py:185][0m |          -0.0004 |         128.5302 |          16.4455 |
[32m[20221213 21:18:11 @agent_ppo2.py:185][0m |          -0.0059 |         128.1668 |          16.4402 |
[32m[20221213 21:18:11 @agent_ppo2.py:185][0m |          -0.0034 |         128.2455 |          16.4362 |
[32m[20221213 21:18:11 @agent_ppo2.py:185][0m |          -0.0076 |         127.7947 |          16.4351 |
[32m[20221213 21:18:12 @agent_ppo2.py:185][0m |          -0.0073 |         127.8697 |          16.4307 |
[32m[20221213 21:18:12 @agent_ppo2.py:130][0m Policy update time: 1.42 s
[32m[20221213 21:18:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 694.40
[32m[20221213 21:18:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.00
[32m[20221213 21:18:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 749.00
[32m[20221213 21:18:12 @agent_ppo2.py:143][0m Total time:      22.62 min
[32m[20221213 21:18:12 @agent_ppo2.py:145][0m 2215936 total steps have happened
[32m[20221213 21:18:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1082 --------------------------#
[32m[20221213 21:18:12 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 21:18:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:12 @agent_ppo2.py:185][0m |          -0.0011 |         131.2304 |          16.4083 |
[32m[20221213 21:18:12 @agent_ppo2.py:185][0m |          -0.0018 |         130.4022 |          16.4032 |
[32m[20221213 21:18:12 @agent_ppo2.py:185][0m |          -0.0041 |         129.8909 |          16.3956 |
[32m[20221213 21:18:12 @agent_ppo2.py:185][0m |          -0.0057 |         129.3130 |          16.3972 |
[32m[20221213 21:18:13 @agent_ppo2.py:185][0m |           0.0006 |         133.1340 |          16.4038 |
[32m[20221213 21:18:13 @agent_ppo2.py:185][0m |          -0.0072 |         128.6815 |          16.4042 |
[32m[20221213 21:18:13 @agent_ppo2.py:185][0m |          -0.0058 |         128.3118 |          16.4056 |
[32m[20221213 21:18:13 @agent_ppo2.py:185][0m |          -0.0054 |         128.9973 |          16.4066 |
[32m[20221213 21:18:13 @agent_ppo2.py:185][0m |          -0.0017 |         130.9794 |          16.4328 |
[32m[20221213 21:18:13 @agent_ppo2.py:185][0m |          -0.0090 |         127.5878 |          16.4286 |
[32m[20221213 21:18:13 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:18:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 676.60
[32m[20221213 21:18:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 710.00
[32m[20221213 21:18:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.00
[32m[20221213 21:18:13 @agent_ppo2.py:143][0m Total time:      22.64 min
[32m[20221213 21:18:13 @agent_ppo2.py:145][0m 2217984 total steps have happened
[32m[20221213 21:18:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1083 --------------------------#
[32m[20221213 21:18:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:14 @agent_ppo2.py:185][0m |           0.0101 |         139.5114 |          16.5417 |
[32m[20221213 21:18:14 @agent_ppo2.py:185][0m |          -0.0030 |         127.9446 |          16.5333 |
[32m[20221213 21:18:14 @agent_ppo2.py:185][0m |          -0.0067 |         127.1458 |          16.5390 |
[32m[20221213 21:18:14 @agent_ppo2.py:185][0m |          -0.0084 |         126.7020 |          16.5420 |
[32m[20221213 21:18:14 @agent_ppo2.py:185][0m |          -0.0057 |         126.9106 |          16.5495 |
[32m[20221213 21:18:14 @agent_ppo2.py:185][0m |          -0.0078 |         126.1639 |          16.5427 |
[32m[20221213 21:18:14 @agent_ppo2.py:185][0m |          -0.0088 |         125.9721 |          16.5573 |
[32m[20221213 21:18:14 @agent_ppo2.py:185][0m |          -0.0095 |         125.8449 |          16.5293 |
[32m[20221213 21:18:14 @agent_ppo2.py:185][0m |          -0.0087 |         125.7847 |          16.5381 |
[32m[20221213 21:18:14 @agent_ppo2.py:185][0m |          -0.0085 |         125.2827 |          16.5445 |
[32m[20221213 21:18:14 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:18:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 654.60
[32m[20221213 21:18:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 707.00
[32m[20221213 21:18:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 708.00
[32m[20221213 21:18:14 @agent_ppo2.py:143][0m Total time:      22.66 min
[32m[20221213 21:18:14 @agent_ppo2.py:145][0m 2220032 total steps have happened
[32m[20221213 21:18:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1084 --------------------------#
[32m[20221213 21:18:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:15 @agent_ppo2.py:185][0m |           0.0021 |         127.9110 |          16.5183 |
[32m[20221213 21:18:15 @agent_ppo2.py:185][0m |          -0.0010 |         125.9764 |          16.5105 |
[32m[20221213 21:18:15 @agent_ppo2.py:185][0m |           0.0003 |         126.9361 |          16.5307 |
[32m[20221213 21:18:15 @agent_ppo2.py:185][0m |           0.0081 |         131.2730 |          16.5407 |
[32m[20221213 21:18:15 @agent_ppo2.py:185][0m |           0.0032 |         127.0709 |          16.5277 |
[32m[20221213 21:18:15 @agent_ppo2.py:185][0m |          -0.0076 |         122.9857 |          16.5491 |
[32m[20221213 21:18:15 @agent_ppo2.py:185][0m |          -0.0030 |         122.9291 |          16.5649 |
[32m[20221213 21:18:15 @agent_ppo2.py:185][0m |          -0.0071 |         121.5404 |          16.5459 |
[32m[20221213 21:18:16 @agent_ppo2.py:185][0m |          -0.0065 |         121.0305 |          16.5624 |
[32m[20221213 21:18:16 @agent_ppo2.py:185][0m |          -0.0080 |         120.4370 |          16.5733 |
[32m[20221213 21:18:16 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 21:18:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 638.60
[32m[20221213 21:18:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 681.00
[32m[20221213 21:18:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 734.00
[32m[20221213 21:18:16 @agent_ppo2.py:143][0m Total time:      22.68 min
[32m[20221213 21:18:16 @agent_ppo2.py:145][0m 2222080 total steps have happened
[32m[20221213 21:18:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1085 --------------------------#
[32m[20221213 21:18:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:16 @agent_ppo2.py:185][0m |           0.0009 |         132.1405 |          16.6701 |
[32m[20221213 21:18:16 @agent_ppo2.py:185][0m |          -0.0051 |         129.9555 |          16.6574 |
[32m[20221213 21:18:16 @agent_ppo2.py:185][0m |          -0.0084 |         129.3276 |          16.6501 |
[32m[20221213 21:18:16 @agent_ppo2.py:185][0m |          -0.0094 |         128.4197 |          16.6381 |
[32m[20221213 21:18:17 @agent_ppo2.py:185][0m |          -0.0082 |         127.9782 |          16.6237 |
[32m[20221213 21:18:17 @agent_ppo2.py:185][0m |          -0.0060 |         128.4280 |          16.6238 |
[32m[20221213 21:18:17 @agent_ppo2.py:185][0m |          -0.0069 |         127.9674 |          16.6072 |
[32m[20221213 21:18:17 @agent_ppo2.py:185][0m |          -0.0093 |         127.1360 |          16.6188 |
[32m[20221213 21:18:17 @agent_ppo2.py:185][0m |          -0.0088 |         126.7870 |          16.6139 |
[32m[20221213 21:18:17 @agent_ppo2.py:185][0m |          -0.0100 |         126.5423 |          16.6084 |
[32m[20221213 21:18:17 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 21:18:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 625.80
[32m[20221213 21:18:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.00
[32m[20221213 21:18:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 709.00
[32m[20221213 21:18:17 @agent_ppo2.py:143][0m Total time:      22.71 min
[32m[20221213 21:18:17 @agent_ppo2.py:145][0m 2224128 total steps have happened
[32m[20221213 21:18:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1086 --------------------------#
[32m[20221213 21:18:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:17 @agent_ppo2.py:185][0m |           0.0000 |         129.7003 |          16.6081 |
[32m[20221213 21:18:18 @agent_ppo2.py:185][0m |          -0.0019 |         128.6884 |          16.5967 |
[32m[20221213 21:18:18 @agent_ppo2.py:185][0m |          -0.0050 |         128.0891 |          16.6143 |
[32m[20221213 21:18:18 @agent_ppo2.py:185][0m |           0.0001 |         130.4044 |          16.6049 |
[32m[20221213 21:18:18 @agent_ppo2.py:185][0m |          -0.0078 |         127.3489 |          16.6175 |
[32m[20221213 21:18:18 @agent_ppo2.py:185][0m |          -0.0070 |         127.0826 |          16.6007 |
[32m[20221213 21:18:18 @agent_ppo2.py:185][0m |          -0.0069 |         127.0957 |          16.6269 |
[32m[20221213 21:18:18 @agent_ppo2.py:185][0m |          -0.0025 |         127.9637 |          16.6186 |
[32m[20221213 21:18:18 @agent_ppo2.py:185][0m |          -0.0059 |         126.5224 |          16.6259 |
[32m[20221213 21:18:18 @agent_ppo2.py:185][0m |          -0.0074 |         126.2369 |          16.6423 |
[32m[20221213 21:18:18 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:18:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 674.40
[32m[20221213 21:18:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.00
[32m[20221213 21:18:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 708.00
[32m[20221213 21:18:18 @agent_ppo2.py:143][0m Total time:      22.73 min
[32m[20221213 21:18:18 @agent_ppo2.py:145][0m 2226176 total steps have happened
[32m[20221213 21:18:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1087 --------------------------#
[32m[20221213 21:18:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:19 @agent_ppo2.py:185][0m |          -0.0042 |         126.4637 |          16.4472 |
[32m[20221213 21:18:19 @agent_ppo2.py:185][0m |          -0.0028 |         125.9869 |          16.4519 |
[32m[20221213 21:18:19 @agent_ppo2.py:185][0m |          -0.0047 |         125.5030 |          16.4412 |
[32m[20221213 21:18:19 @agent_ppo2.py:185][0m |          -0.0067 |         125.1108 |          16.4299 |
[32m[20221213 21:18:19 @agent_ppo2.py:185][0m |          -0.0070 |         124.8305 |          16.4239 |
[32m[20221213 21:18:19 @agent_ppo2.py:185][0m |          -0.0028 |         127.5548 |          16.4291 |
[32m[20221213 21:18:19 @agent_ppo2.py:185][0m |          -0.0082 |         124.5713 |          16.4443 |
[32m[20221213 21:18:19 @agent_ppo2.py:185][0m |          -0.0057 |         124.2626 |          16.4360 |
[32m[20221213 21:18:19 @agent_ppo2.py:185][0m |          -0.0067 |         124.7462 |          16.4426 |
[32m[20221213 21:18:20 @agent_ppo2.py:185][0m |          -0.0067 |         124.1436 |          16.4337 |
[32m[20221213 21:18:20 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:18:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 705.80
[32m[20221213 21:18:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 784.00
[32m[20221213 21:18:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 735.00
[32m[20221213 21:18:20 @agent_ppo2.py:143][0m Total time:      22.75 min
[32m[20221213 21:18:20 @agent_ppo2.py:145][0m 2228224 total steps have happened
[32m[20221213 21:18:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1088 --------------------------#
[32m[20221213 21:18:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:20 @agent_ppo2.py:185][0m |          -0.0001 |         129.2611 |          16.5186 |
[32m[20221213 21:18:20 @agent_ppo2.py:185][0m |          -0.0059 |         128.0030 |          16.4756 |
[32m[20221213 21:18:20 @agent_ppo2.py:185][0m |          -0.0066 |         127.3867 |          16.4574 |
[32m[20221213 21:18:20 @agent_ppo2.py:185][0m |          -0.0081 |         126.7608 |          16.4483 |
[32m[20221213 21:18:20 @agent_ppo2.py:185][0m |          -0.0069 |         126.4317 |          16.4388 |
[32m[20221213 21:18:20 @agent_ppo2.py:185][0m |          -0.0060 |         126.4186 |          16.4153 |
[32m[20221213 21:18:21 @agent_ppo2.py:185][0m |          -0.0096 |         126.0309 |          16.4146 |
[32m[20221213 21:18:21 @agent_ppo2.py:185][0m |          -0.0087 |         125.8351 |          16.4151 |
[32m[20221213 21:18:21 @agent_ppo2.py:185][0m |          -0.0081 |         125.6237 |          16.3807 |
[32m[20221213 21:18:21 @agent_ppo2.py:185][0m |          -0.0084 |         125.2022 |          16.3755 |
[32m[20221213 21:18:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:18:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 655.80
[32m[20221213 21:18:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.00
[32m[20221213 21:18:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 733.00
[32m[20221213 21:18:21 @agent_ppo2.py:143][0m Total time:      22.77 min
[32m[20221213 21:18:21 @agent_ppo2.py:145][0m 2230272 total steps have happened
[32m[20221213 21:18:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1089 --------------------------#
[32m[20221213 21:18:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:18:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:21 @agent_ppo2.py:185][0m |           0.0002 |         127.6691 |          16.5277 |
[32m[20221213 21:18:21 @agent_ppo2.py:185][0m |          -0.0034 |         125.9653 |          16.4919 |
[32m[20221213 21:18:21 @agent_ppo2.py:185][0m |          -0.0051 |         125.2498 |          16.4878 |
[32m[20221213 21:18:22 @agent_ppo2.py:185][0m |          -0.0010 |         127.7150 |          16.4769 |
[32m[20221213 21:18:22 @agent_ppo2.py:185][0m |           0.0028 |         125.3585 |          16.5015 |
[32m[20221213 21:18:22 @agent_ppo2.py:185][0m |          -0.0054 |         123.8776 |          16.4566 |
[32m[20221213 21:18:22 @agent_ppo2.py:185][0m |          -0.0006 |         123.9947 |          16.4634 |
[32m[20221213 21:18:22 @agent_ppo2.py:185][0m |          -0.0072 |         123.5340 |          16.4716 |
[32m[20221213 21:18:22 @agent_ppo2.py:185][0m |          -0.0064 |         123.3155 |          16.4662 |
[32m[20221213 21:18:22 @agent_ppo2.py:185][0m |          -0.0081 |         123.4366 |          16.4497 |
[32m[20221213 21:18:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 661.20
[32m[20221213 21:18:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.00
[32m[20221213 21:18:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.00
[32m[20221213 21:18:22 @agent_ppo2.py:143][0m Total time:      22.79 min
[32m[20221213 21:18:22 @agent_ppo2.py:145][0m 2232320 total steps have happened
[32m[20221213 21:18:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1090 --------------------------#
[32m[20221213 21:18:22 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:18:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:22 @agent_ppo2.py:185][0m |          -0.0001 |         125.3387 |          16.4342 |
[32m[20221213 21:18:23 @agent_ppo2.py:185][0m |          -0.0036 |         123.9950 |          16.4179 |
[32m[20221213 21:18:23 @agent_ppo2.py:185][0m |          -0.0066 |         123.1675 |          16.4348 |
[32m[20221213 21:18:23 @agent_ppo2.py:185][0m |          -0.0046 |         122.8186 |          16.4350 |
[32m[20221213 21:18:23 @agent_ppo2.py:185][0m |          -0.0040 |         122.6203 |          16.4281 |
[32m[20221213 21:18:23 @agent_ppo2.py:185][0m |          -0.0056 |         121.9565 |          16.4483 |
[32m[20221213 21:18:23 @agent_ppo2.py:185][0m |          -0.0066 |         121.7875 |          16.4403 |
[32m[20221213 21:18:23 @agent_ppo2.py:185][0m |          -0.0069 |         121.7074 |          16.4532 |
[32m[20221213 21:18:23 @agent_ppo2.py:185][0m |          -0.0094 |         121.5548 |          16.4668 |
[32m[20221213 21:18:23 @agent_ppo2.py:185][0m |          -0.0090 |         121.4309 |          16.4667 |
[32m[20221213 21:18:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 666.20
[32m[20221213 21:18:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.00
[32m[20221213 21:18:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 784.00
[32m[20221213 21:18:23 @agent_ppo2.py:143][0m Total time:      22.81 min
[32m[20221213 21:18:23 @agent_ppo2.py:145][0m 2234368 total steps have happened
[32m[20221213 21:18:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1091 --------------------------#
[32m[20221213 21:18:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:24 @agent_ppo2.py:185][0m |          -0.0014 |         125.8752 |          16.1511 |
[32m[20221213 21:18:24 @agent_ppo2.py:185][0m |           0.0058 |         128.7864 |          16.1396 |
[32m[20221213 21:18:24 @agent_ppo2.py:185][0m |          -0.0025 |         125.0809 |          16.1628 |
[32m[20221213 21:18:24 @agent_ppo2.py:185][0m |          -0.0047 |         124.2458 |          16.1696 |
[32m[20221213 21:18:24 @agent_ppo2.py:185][0m |          -0.0049 |         123.9721 |          16.1940 |
[32m[20221213 21:18:24 @agent_ppo2.py:185][0m |          -0.0063 |         123.7277 |          16.2175 |
[32m[20221213 21:18:24 @agent_ppo2.py:185][0m |          -0.0003 |         125.7692 |          16.1975 |
[32m[20221213 21:18:24 @agent_ppo2.py:185][0m |          -0.0050 |         123.4305 |          16.2267 |
[32m[20221213 21:18:24 @agent_ppo2.py:185][0m |          -0.0044 |         123.2049 |          16.2195 |
[32m[20221213 21:18:24 @agent_ppo2.py:185][0m |          -0.0072 |         123.3269 |          16.2209 |
[32m[20221213 21:18:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:18:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 674.20
[32m[20221213 21:18:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 711.00
[32m[20221213 21:18:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 704.00
[32m[20221213 21:18:25 @agent_ppo2.py:143][0m Total time:      22.83 min
[32m[20221213 21:18:25 @agent_ppo2.py:145][0m 2236416 total steps have happened
[32m[20221213 21:18:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1092 --------------------------#
[32m[20221213 21:18:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:18:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:25 @agent_ppo2.py:185][0m |           0.0020 |         132.3293 |          16.5702 |
[32m[20221213 21:18:25 @agent_ppo2.py:185][0m |          -0.0060 |         129.8035 |          16.5416 |
[32m[20221213 21:18:25 @agent_ppo2.py:185][0m |           0.0018 |         133.4973 |          16.5201 |
[32m[20221213 21:18:25 @agent_ppo2.py:185][0m |          -0.0110 |         129.1624 |          16.5499 |
[32m[20221213 21:18:25 @agent_ppo2.py:185][0m |          -0.0093 |         128.6886 |          16.5221 |
[32m[20221213 21:18:25 @agent_ppo2.py:185][0m |          -0.0101 |         128.6028 |          16.5379 |
[32m[20221213 21:18:25 @agent_ppo2.py:185][0m |          -0.0014 |         133.1476 |          16.5247 |
[32m[20221213 21:18:25 @agent_ppo2.py:185][0m |          -0.0093 |         128.2265 |          16.5361 |
[32m[20221213 21:18:26 @agent_ppo2.py:185][0m |          -0.0077 |         127.9332 |          16.5369 |
[32m[20221213 21:18:26 @agent_ppo2.py:185][0m |          -0.0121 |         127.8538 |          16.5298 |
[32m[20221213 21:18:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 645.80
[32m[20221213 21:18:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 680.00
[32m[20221213 21:18:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 721.00
[32m[20221213 21:18:26 @agent_ppo2.py:143][0m Total time:      22.85 min
[32m[20221213 21:18:26 @agent_ppo2.py:145][0m 2238464 total steps have happened
[32m[20221213 21:18:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1093 --------------------------#
[32m[20221213 21:18:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:26 @agent_ppo2.py:185][0m |           0.0043 |         135.2312 |          16.6387 |
[32m[20221213 21:18:26 @agent_ppo2.py:185][0m |          -0.0074 |         128.1380 |          16.6391 |
[32m[20221213 21:18:26 @agent_ppo2.py:185][0m |          -0.0023 |         126.5389 |          16.6505 |
[32m[20221213 21:18:26 @agent_ppo2.py:185][0m |          -0.0062 |         125.4559 |          16.6477 |
[32m[20221213 21:18:26 @agent_ppo2.py:185][0m |          -0.0069 |         125.0186 |          16.6343 |
[32m[20221213 21:18:27 @agent_ppo2.py:185][0m |          -0.0087 |         124.3316 |          16.6489 |
[32m[20221213 21:18:27 @agent_ppo2.py:185][0m |          -0.0063 |         123.9238 |          16.6524 |
[32m[20221213 21:18:27 @agent_ppo2.py:185][0m |          -0.0037 |         124.1156 |          16.6560 |
[32m[20221213 21:18:27 @agent_ppo2.py:185][0m |          -0.0087 |         123.0278 |          16.6562 |
[32m[20221213 21:18:27 @agent_ppo2.py:185][0m |          -0.0098 |         122.5329 |          16.6605 |
[32m[20221213 21:18:27 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:18:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.40
[32m[20221213 21:18:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 628.00
[32m[20221213 21:18:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 772.00
[32m[20221213 21:18:27 @agent_ppo2.py:143][0m Total time:      22.87 min
[32m[20221213 21:18:27 @agent_ppo2.py:145][0m 2240512 total steps have happened
[32m[20221213 21:18:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1094 --------------------------#
[32m[20221213 21:18:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:27 @agent_ppo2.py:185][0m |          -0.0022 |         131.2727 |          16.5577 |
[32m[20221213 21:18:27 @agent_ppo2.py:185][0m |          -0.0015 |         131.5115 |          16.5485 |
[32m[20221213 21:18:27 @agent_ppo2.py:185][0m |          -0.0059 |         129.1334 |          16.5420 |
[32m[20221213 21:18:28 @agent_ppo2.py:185][0m |          -0.0097 |         128.6885 |          16.5361 |
[32m[20221213 21:18:28 @agent_ppo2.py:185][0m |           0.0056 |         137.3030 |          16.5427 |
[32m[20221213 21:18:28 @agent_ppo2.py:185][0m |          -0.0068 |         127.6333 |          16.5473 |
[32m[20221213 21:18:28 @agent_ppo2.py:185][0m |          -0.0023 |         130.8196 |          16.5358 |
[32m[20221213 21:18:28 @agent_ppo2.py:185][0m |          -0.0028 |         132.6849 |          16.5419 |
[32m[20221213 21:18:28 @agent_ppo2.py:185][0m |          -0.0067 |         126.8425 |          16.5599 |
[32m[20221213 21:18:28 @agent_ppo2.py:185][0m |          -0.0066 |         128.3524 |          16.5440 |
[32m[20221213 21:18:28 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:18:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 696.00
[32m[20221213 21:18:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 761.00
[32m[20221213 21:18:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 699.00
[32m[20221213 21:18:28 @agent_ppo2.py:143][0m Total time:      22.89 min
[32m[20221213 21:18:28 @agent_ppo2.py:145][0m 2242560 total steps have happened
[32m[20221213 21:18:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1095 --------------------------#
[32m[20221213 21:18:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:29 @agent_ppo2.py:185][0m |          -0.0015 |         133.5348 |          16.4008 |
[32m[20221213 21:18:29 @agent_ppo2.py:185][0m |          -0.0032 |         130.3411 |          16.4034 |
[32m[20221213 21:18:29 @agent_ppo2.py:185][0m |          -0.0019 |         127.6145 |          16.4224 |
[32m[20221213 21:18:29 @agent_ppo2.py:185][0m |          -0.0047 |         126.4393 |          16.4145 |
[32m[20221213 21:18:29 @agent_ppo2.py:185][0m |          -0.0068 |         125.7118 |          16.4166 |
[32m[20221213 21:18:29 @agent_ppo2.py:185][0m |          -0.0068 |         125.0011 |          16.4246 |
[32m[20221213 21:18:29 @agent_ppo2.py:185][0m |          -0.0072 |         124.6130 |          16.4301 |
[32m[20221213 21:18:29 @agent_ppo2.py:185][0m |          -0.0084 |         123.9213 |          16.4312 |
[32m[20221213 21:18:29 @agent_ppo2.py:185][0m |          -0.0072 |         123.7636 |          16.4409 |
[32m[20221213 21:18:29 @agent_ppo2.py:185][0m |          -0.0089 |         123.2467 |          16.4333 |
[32m[20221213 21:18:29 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:18:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 696.20
[32m[20221213 21:18:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 732.00
[32m[20221213 21:18:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 771.00
[32m[20221213 21:18:29 @agent_ppo2.py:143][0m Total time:      22.91 min
[32m[20221213 21:18:29 @agent_ppo2.py:145][0m 2244608 total steps have happened
[32m[20221213 21:18:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1096 --------------------------#
[32m[20221213 21:18:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:30 @agent_ppo2.py:185][0m |          -0.0051 |         138.0906 |          16.4774 |
[32m[20221213 21:18:30 @agent_ppo2.py:185][0m |          -0.0044 |         136.1256 |          16.4762 |
[32m[20221213 21:18:30 @agent_ppo2.py:185][0m |           0.0154 |         150.0134 |          16.4697 |
[32m[20221213 21:18:30 @agent_ppo2.py:185][0m |          -0.0003 |         136.5635 |          16.4792 |
[32m[20221213 21:18:30 @agent_ppo2.py:185][0m |          -0.0039 |         134.6144 |          16.4552 |
[32m[20221213 21:18:30 @agent_ppo2.py:185][0m |          -0.0027 |         135.4054 |          16.4515 |
[32m[20221213 21:18:30 @agent_ppo2.py:185][0m |          -0.0042 |         134.0502 |          16.4583 |
[32m[20221213 21:18:30 @agent_ppo2.py:185][0m |          -0.0071 |         133.6906 |          16.4565 |
[32m[20221213 21:18:31 @agent_ppo2.py:185][0m |          -0.0084 |         133.6133 |          16.4440 |
[32m[20221213 21:18:31 @agent_ppo2.py:185][0m |          -0.0061 |         133.9122 |          16.4430 |
[32m[20221213 21:18:31 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:18:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 648.40
[32m[20221213 21:18:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 697.00
[32m[20221213 21:18:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 784.00
[32m[20221213 21:18:31 @agent_ppo2.py:143][0m Total time:      22.93 min
[32m[20221213 21:18:31 @agent_ppo2.py:145][0m 2246656 total steps have happened
[32m[20221213 21:18:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1097 --------------------------#
[32m[20221213 21:18:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:31 @agent_ppo2.py:185][0m |          -0.0019 |         131.3927 |          16.5076 |
[32m[20221213 21:18:31 @agent_ppo2.py:185][0m |          -0.0049 |         129.8550 |          16.5204 |
[32m[20221213 21:18:31 @agent_ppo2.py:185][0m |          -0.0050 |         129.5181 |          16.5289 |
[32m[20221213 21:18:31 @agent_ppo2.py:185][0m |          -0.0064 |         128.3621 |          16.5509 |
[32m[20221213 21:18:31 @agent_ppo2.py:185][0m |          -0.0073 |         128.0038 |          16.5424 |
[32m[20221213 21:18:31 @agent_ppo2.py:185][0m |          -0.0064 |         127.5040 |          16.5446 |
[32m[20221213 21:18:32 @agent_ppo2.py:185][0m |          -0.0097 |         127.3571 |          16.5511 |
[32m[20221213 21:18:32 @agent_ppo2.py:185][0m |          -0.0061 |         127.3274 |          16.5398 |
[32m[20221213 21:18:32 @agent_ppo2.py:185][0m |          -0.0064 |         126.6277 |          16.5415 |
[32m[20221213 21:18:32 @agent_ppo2.py:185][0m |          -0.0095 |         126.3499 |          16.5477 |
[32m[20221213 21:18:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 672.60
[32m[20221213 21:18:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 738.00
[32m[20221213 21:18:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 781.00
[32m[20221213 21:18:32 @agent_ppo2.py:143][0m Total time:      22.95 min
[32m[20221213 21:18:32 @agent_ppo2.py:145][0m 2248704 total steps have happened
[32m[20221213 21:18:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1098 --------------------------#
[32m[20221213 21:18:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:32 @agent_ppo2.py:185][0m |          -0.0008 |         132.9136 |          16.5729 |
[32m[20221213 21:18:32 @agent_ppo2.py:185][0m |          -0.0063 |         130.7285 |          16.5946 |
[32m[20221213 21:18:32 @agent_ppo2.py:185][0m |          -0.0073 |         129.8967 |          16.5834 |
[32m[20221213 21:18:32 @agent_ppo2.py:185][0m |          -0.0057 |         129.6560 |          16.5633 |
[32m[20221213 21:18:33 @agent_ppo2.py:185][0m |          -0.0083 |         129.4026 |          16.5724 |
[32m[20221213 21:18:33 @agent_ppo2.py:185][0m |          -0.0075 |         128.8880 |          16.5644 |
[32m[20221213 21:18:33 @agent_ppo2.py:185][0m |          -0.0102 |         128.8800 |          16.5517 |
[32m[20221213 21:18:33 @agent_ppo2.py:185][0m |          -0.0070 |         129.0981 |          16.5490 |
[32m[20221213 21:18:33 @agent_ppo2.py:185][0m |          -0.0050 |         130.8813 |          16.5393 |
[32m[20221213 21:18:33 @agent_ppo2.py:185][0m |          -0.0053 |         130.8735 |          16.5424 |
[32m[20221213 21:18:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 683.80
[32m[20221213 21:18:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.00
[32m[20221213 21:18:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.00
[32m[20221213 21:18:33 @agent_ppo2.py:143][0m Total time:      22.97 min
[32m[20221213 21:18:33 @agent_ppo2.py:145][0m 2250752 total steps have happened
[32m[20221213 21:18:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1099 --------------------------#
[32m[20221213 21:18:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:33 @agent_ppo2.py:185][0m |           0.0001 |         133.6430 |          16.8362 |
[32m[20221213 21:18:33 @agent_ppo2.py:185][0m |          -0.0051 |         131.9410 |          16.8059 |
[32m[20221213 21:18:34 @agent_ppo2.py:185][0m |          -0.0046 |         131.1066 |          16.7890 |
[32m[20221213 21:18:34 @agent_ppo2.py:185][0m |          -0.0042 |         130.6178 |          16.7995 |
[32m[20221213 21:18:34 @agent_ppo2.py:185][0m |          -0.0033 |         130.1827 |          16.8005 |
[32m[20221213 21:18:34 @agent_ppo2.py:185][0m |          -0.0080 |         129.8331 |          16.7715 |
[32m[20221213 21:18:34 @agent_ppo2.py:185][0m |          -0.0073 |         129.4827 |          16.7711 |
[32m[20221213 21:18:34 @agent_ppo2.py:185][0m |          -0.0090 |         129.4243 |          16.7858 |
[32m[20221213 21:18:34 @agent_ppo2.py:185][0m |          -0.0081 |         129.1336 |          16.7852 |
[32m[20221213 21:18:34 @agent_ppo2.py:185][0m |          -0.0053 |         129.2446 |          16.7695 |
[32m[20221213 21:18:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 649.00
[32m[20221213 21:18:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 692.00
[32m[20221213 21:18:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 736.00
[32m[20221213 21:18:34 @agent_ppo2.py:143][0m Total time:      22.99 min
[32m[20221213 21:18:34 @agent_ppo2.py:145][0m 2252800 total steps have happened
[32m[20221213 21:18:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1100 --------------------------#
[32m[20221213 21:18:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:35 @agent_ppo2.py:185][0m |           0.0061 |         142.0160 |          16.5127 |
[32m[20221213 21:18:35 @agent_ppo2.py:185][0m |          -0.0021 |         132.8723 |          16.4802 |
[32m[20221213 21:18:35 @agent_ppo2.py:185][0m |          -0.0068 |         132.4683 |          16.4822 |
[32m[20221213 21:18:35 @agent_ppo2.py:185][0m |          -0.0078 |         131.8428 |          16.4680 |
[32m[20221213 21:18:35 @agent_ppo2.py:185][0m |          -0.0097 |         131.4960 |          16.4612 |
[32m[20221213 21:18:35 @agent_ppo2.py:185][0m |          -0.0084 |         131.2603 |          16.4407 |
[32m[20221213 21:18:35 @agent_ppo2.py:185][0m |          -0.0099 |         131.2323 |          16.4430 |
[32m[20221213 21:18:35 @agent_ppo2.py:185][0m |          -0.0119 |         130.9569 |          16.4378 |
[32m[20221213 21:18:35 @agent_ppo2.py:185][0m |          -0.0055 |         132.2658 |          16.4179 |
[32m[20221213 21:18:35 @agent_ppo2.py:185][0m |          -0.0106 |         130.5596 |          16.4086 |
[32m[20221213 21:18:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 686.80
[32m[20221213 21:18:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 722.00
[32m[20221213 21:18:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.00
[32m[20221213 21:18:35 @agent_ppo2.py:143][0m Total time:      23.01 min
[32m[20221213 21:18:35 @agent_ppo2.py:145][0m 2254848 total steps have happened
[32m[20221213 21:18:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1101 --------------------------#
[32m[20221213 21:18:36 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:18:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:36 @agent_ppo2.py:185][0m |          -0.0020 |         131.8406 |          16.3806 |
[32m[20221213 21:18:36 @agent_ppo2.py:185][0m |           0.0028 |         133.1145 |          16.3716 |
[32m[20221213 21:18:36 @agent_ppo2.py:185][0m |          -0.0007 |         130.6917 |          16.3400 |
[32m[20221213 21:18:36 @agent_ppo2.py:185][0m |          -0.0064 |         128.3289 |          16.3364 |
[32m[20221213 21:18:36 @agent_ppo2.py:185][0m |          -0.0047 |         127.7105 |          16.3347 |
[32m[20221213 21:18:36 @agent_ppo2.py:185][0m |          -0.0056 |         127.3769 |          16.3204 |
[32m[20221213 21:18:36 @agent_ppo2.py:185][0m |          -0.0062 |         127.0886 |          16.2924 |
[32m[20221213 21:18:36 @agent_ppo2.py:185][0m |          -0.0048 |         127.6536 |          16.2847 |
[32m[20221213 21:18:36 @agent_ppo2.py:185][0m |          -0.0068 |         126.5503 |          16.2834 |
[32m[20221213 21:18:37 @agent_ppo2.py:185][0m |          -0.0097 |         126.3226 |          16.2644 |
[32m[20221213 21:18:37 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:18:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 604.80
[32m[20221213 21:18:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 704.00
[32m[20221213 21:18:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.00
[32m[20221213 21:18:37 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 811.00
[32m[20221213 21:18:37 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 811.00
[32m[20221213 21:18:37 @agent_ppo2.py:143][0m Total time:      23.03 min
[32m[20221213 21:18:37 @agent_ppo2.py:145][0m 2256896 total steps have happened
[32m[20221213 21:18:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1102 --------------------------#
[32m[20221213 21:18:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:18:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:37 @agent_ppo2.py:185][0m |           0.0049 |         129.5816 |          16.0873 |
[32m[20221213 21:18:37 @agent_ppo2.py:185][0m |          -0.0037 |         125.5207 |          16.0502 |
[32m[20221213 21:18:37 @agent_ppo2.py:185][0m |          -0.0054 |         124.6019 |          16.0396 |
[32m[20221213 21:18:37 @agent_ppo2.py:185][0m |          -0.0072 |         123.7393 |          16.0190 |
[32m[20221213 21:18:37 @agent_ppo2.py:185][0m |          -0.0054 |         124.3761 |          15.9885 |
[32m[20221213 21:18:37 @agent_ppo2.py:185][0m |          -0.0076 |         122.5431 |          15.9750 |
[32m[20221213 21:18:38 @agent_ppo2.py:185][0m |           0.0041 |         132.1833 |          15.9547 |
[32m[20221213 21:18:38 @agent_ppo2.py:185][0m |          -0.0017 |         128.8284 |          15.9356 |
[32m[20221213 21:18:38 @agent_ppo2.py:185][0m |          -0.0094 |         121.3525 |          15.9282 |
[32m[20221213 21:18:38 @agent_ppo2.py:185][0m |          -0.0083 |         120.6232 |          15.8939 |
[32m[20221213 21:18:38 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:18:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 683.00
[32m[20221213 21:18:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.00
[32m[20221213 21:18:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.00
[32m[20221213 21:18:38 @agent_ppo2.py:143][0m Total time:      23.05 min
[32m[20221213 21:18:38 @agent_ppo2.py:145][0m 2258944 total steps have happened
[32m[20221213 21:18:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1103 --------------------------#
[32m[20221213 21:18:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:38 @agent_ppo2.py:185][0m |          -0.0048 |         134.6023 |          16.1320 |
[32m[20221213 21:18:38 @agent_ppo2.py:185][0m |          -0.0039 |         131.4307 |          16.1312 |
[32m[20221213 21:18:38 @agent_ppo2.py:185][0m |          -0.0044 |         130.4271 |          16.1444 |
[32m[20221213 21:18:38 @agent_ppo2.py:185][0m |          -0.0066 |         129.4161 |          16.1356 |
[32m[20221213 21:18:39 @agent_ppo2.py:185][0m |          -0.0089 |         129.1606 |          16.1308 |
[32m[20221213 21:18:39 @agent_ppo2.py:185][0m |          -0.0075 |         128.6203 |          16.1141 |
[32m[20221213 21:18:39 @agent_ppo2.py:185][0m |           0.0056 |         143.0521 |          16.1384 |
[32m[20221213 21:18:39 @agent_ppo2.py:185][0m |          -0.0109 |         128.2747 |          16.1380 |
[32m[20221213 21:18:39 @agent_ppo2.py:185][0m |          -0.0078 |         127.9196 |          16.1479 |
[32m[20221213 21:18:39 @agent_ppo2.py:185][0m |          -0.0115 |         127.7202 |          16.1390 |
[32m[20221213 21:18:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:18:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 626.60
[32m[20221213 21:18:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 681.00
[32m[20221213 21:18:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 21:18:39 @agent_ppo2.py:143][0m Total time:      23.07 min
[32m[20221213 21:18:39 @agent_ppo2.py:145][0m 2260992 total steps have happened
[32m[20221213 21:18:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1104 --------------------------#
[32m[20221213 21:18:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:39 @agent_ppo2.py:185][0m |           0.0076 |         137.7324 |          16.1979 |
[32m[20221213 21:18:39 @agent_ppo2.py:185][0m |          -0.0016 |         133.8675 |          16.1806 |
[32m[20221213 21:18:40 @agent_ppo2.py:185][0m |          -0.0055 |         133.0577 |          16.1768 |
[32m[20221213 21:18:40 @agent_ppo2.py:185][0m |          -0.0009 |         133.5578 |          16.1579 |
[32m[20221213 21:18:40 @agent_ppo2.py:185][0m |          -0.0054 |         132.5977 |          16.1401 |
[32m[20221213 21:18:40 @agent_ppo2.py:185][0m |          -0.0064 |         132.3507 |          16.1326 |
[32m[20221213 21:18:40 @agent_ppo2.py:185][0m |          -0.0069 |         132.1307 |          16.1220 |
[32m[20221213 21:18:40 @agent_ppo2.py:185][0m |          -0.0079 |         132.0706 |          16.1186 |
[32m[20221213 21:18:40 @agent_ppo2.py:185][0m |          -0.0082 |         131.8062 |          16.0900 |
[32m[20221213 21:18:40 @agent_ppo2.py:185][0m |          -0.0080 |         131.5834 |          16.0851 |
[32m[20221213 21:18:40 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:18:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 701.00
[32m[20221213 21:18:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.00
[32m[20221213 21:18:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 784.00
[32m[20221213 21:18:40 @agent_ppo2.py:143][0m Total time:      23.09 min
[32m[20221213 21:18:40 @agent_ppo2.py:145][0m 2263040 total steps have happened
[32m[20221213 21:18:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1105 --------------------------#
[32m[20221213 21:18:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:18:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:41 @agent_ppo2.py:185][0m |           0.0004 |         132.2164 |          16.0226 |
[32m[20221213 21:18:41 @agent_ppo2.py:185][0m |          -0.0042 |         131.2750 |          16.0140 |
[32m[20221213 21:18:41 @agent_ppo2.py:185][0m |           0.0056 |         140.6078 |          15.9948 |
[32m[20221213 21:18:41 @agent_ppo2.py:185][0m |          -0.0072 |         130.4116 |          16.0136 |
[32m[20221213 21:18:41 @agent_ppo2.py:185][0m |           0.0018 |         144.3488 |          15.9953 |
[32m[20221213 21:18:41 @agent_ppo2.py:185][0m |          -0.0082 |         129.8756 |          16.0065 |
[32m[20221213 21:18:41 @agent_ppo2.py:185][0m |          -0.0082 |         129.6386 |          15.9824 |
[32m[20221213 21:18:42 @agent_ppo2.py:185][0m |          -0.0072 |         129.4612 |          15.9932 |
[32m[20221213 21:18:42 @agent_ppo2.py:185][0m |          -0.0066 |         129.4955 |          15.9826 |
[32m[20221213 21:18:42 @agent_ppo2.py:185][0m |           0.0095 |         144.9689 |          15.9894 |
[32m[20221213 21:18:42 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:18:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 701.40
[32m[20221213 21:18:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.00
[32m[20221213 21:18:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 785.00
[32m[20221213 21:18:42 @agent_ppo2.py:143][0m Total time:      23.12 min
[32m[20221213 21:18:42 @agent_ppo2.py:145][0m 2265088 total steps have happened
[32m[20221213 21:18:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1106 --------------------------#
[32m[20221213 21:18:42 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:18:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:42 @agent_ppo2.py:185][0m |          -0.0009 |         137.5372 |          15.8594 |
[32m[20221213 21:18:42 @agent_ppo2.py:185][0m |          -0.0068 |         135.9773 |          15.8804 |
[32m[20221213 21:18:42 @agent_ppo2.py:185][0m |           0.0066 |         152.0638 |          15.8806 |
[32m[20221213 21:18:42 @agent_ppo2.py:185][0m |          -0.0038 |         134.9093 |          15.8645 |
[32m[20221213 21:18:43 @agent_ppo2.py:185][0m |          -0.0026 |         135.2327 |          15.8714 |
[32m[20221213 21:18:43 @agent_ppo2.py:185][0m |          -0.0086 |         134.3353 |          15.8834 |
[32m[20221213 21:18:43 @agent_ppo2.py:185][0m |          -0.0059 |         134.1233 |          15.8660 |
[32m[20221213 21:18:43 @agent_ppo2.py:185][0m |          -0.0011 |         144.0625 |          15.8743 |
[32m[20221213 21:18:43 @agent_ppo2.py:185][0m |           0.0012 |         149.3125 |          15.8684 |
[32m[20221213 21:18:43 @agent_ppo2.py:185][0m |          -0.0096 |         133.8570 |          15.8684 |
[32m[20221213 21:18:43 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:18:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 650.20
[32m[20221213 21:18:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 686.00
[32m[20221213 21:18:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 724.00
[32m[20221213 21:18:43 @agent_ppo2.py:143][0m Total time:      23.14 min
[32m[20221213 21:18:43 @agent_ppo2.py:145][0m 2267136 total steps have happened
[32m[20221213 21:18:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1107 --------------------------#
[32m[20221213 21:18:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:43 @agent_ppo2.py:185][0m |          -0.0006 |         138.7204 |          15.8802 |
[32m[20221213 21:18:43 @agent_ppo2.py:185][0m |          -0.0069 |         136.4124 |          15.8537 |
[32m[20221213 21:18:44 @agent_ppo2.py:185][0m |          -0.0074 |         135.2766 |          15.8451 |
[32m[20221213 21:18:44 @agent_ppo2.py:185][0m |          -0.0084 |         134.5411 |          15.8520 |
[32m[20221213 21:18:44 @agent_ppo2.py:185][0m |          -0.0007 |         135.7321 |          15.8346 |
[32m[20221213 21:18:44 @agent_ppo2.py:185][0m |          -0.0056 |         133.7694 |          15.8266 |
[32m[20221213 21:18:44 @agent_ppo2.py:185][0m |          -0.0086 |         133.5063 |          15.8278 |
[32m[20221213 21:18:44 @agent_ppo2.py:185][0m |          -0.0062 |         133.0842 |          15.8232 |
[32m[20221213 21:18:44 @agent_ppo2.py:185][0m |          -0.0091 |         132.6504 |          15.7926 |
[32m[20221213 21:18:44 @agent_ppo2.py:185][0m |          -0.0095 |         132.4798 |          15.8076 |
[32m[20221213 21:18:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:18:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 670.20
[32m[20221213 21:18:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 716.00
[32m[20221213 21:18:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 754.00
[32m[20221213 21:18:44 @agent_ppo2.py:143][0m Total time:      23.16 min
[32m[20221213 21:18:44 @agent_ppo2.py:145][0m 2269184 total steps have happened
[32m[20221213 21:18:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1108 --------------------------#
[32m[20221213 21:18:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:45 @agent_ppo2.py:185][0m |          -0.0015 |         136.1054 |          15.9160 |
[32m[20221213 21:18:45 @agent_ppo2.py:185][0m |          -0.0043 |         133.1856 |          15.9483 |
[32m[20221213 21:18:45 @agent_ppo2.py:185][0m |           0.0020 |         135.7293 |          15.9408 |
[32m[20221213 21:18:45 @agent_ppo2.py:185][0m |           0.0029 |         134.3163 |          15.9311 |
[32m[20221213 21:18:45 @agent_ppo2.py:185][0m |          -0.0018 |         133.7966 |          15.9234 |
[32m[20221213 21:18:45 @agent_ppo2.py:185][0m |           0.0033 |         138.9447 |          15.9314 |
[32m[20221213 21:18:45 @agent_ppo2.py:185][0m |          -0.0069 |         129.5555 |          15.9277 |
[32m[20221213 21:18:45 @agent_ppo2.py:185][0m |          -0.0072 |         129.3374 |          15.9435 |
[32m[20221213 21:18:45 @agent_ppo2.py:185][0m |          -0.0069 |         129.2747 |          15.9289 |
[32m[20221213 21:18:45 @agent_ppo2.py:185][0m |          -0.0102 |         128.5871 |          15.9263 |
[32m[20221213 21:18:45 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:18:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 682.00
[32m[20221213 21:18:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 709.00
[32m[20221213 21:18:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.00
[32m[20221213 21:18:46 @agent_ppo2.py:143][0m Total time:      23.18 min
[32m[20221213 21:18:46 @agent_ppo2.py:145][0m 2271232 total steps have happened
[32m[20221213 21:18:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1109 --------------------------#
[32m[20221213 21:18:46 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:18:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:46 @agent_ppo2.py:185][0m |          -0.0007 |         137.8772 |          15.8161 |
[32m[20221213 21:18:46 @agent_ppo2.py:185][0m |          -0.0051 |         135.8202 |          15.8023 |
[32m[20221213 21:18:46 @agent_ppo2.py:185][0m |           0.0011 |         138.4958 |          15.7987 |
[32m[20221213 21:18:46 @agent_ppo2.py:185][0m |          -0.0010 |         135.8670 |          15.7854 |
[32m[20221213 21:18:46 @agent_ppo2.py:185][0m |          -0.0051 |         133.9547 |          15.7754 |
[32m[20221213 21:18:46 @agent_ppo2.py:185][0m |          -0.0077 |         133.6188 |          15.7295 |
[32m[20221213 21:18:46 @agent_ppo2.py:185][0m |          -0.0061 |         133.3540 |          15.7370 |
[32m[20221213 21:18:47 @agent_ppo2.py:185][0m |          -0.0100 |         133.1331 |          15.7249 |
[32m[20221213 21:18:47 @agent_ppo2.py:185][0m |           0.0025 |         144.8100 |          15.7120 |
[32m[20221213 21:18:47 @agent_ppo2.py:185][0m |          -0.0035 |         133.7955 |          15.6738 |
[32m[20221213 21:18:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:18:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 739.80
[32m[20221213 21:18:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.00
[32m[20221213 21:18:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.00
[32m[20221213 21:18:47 @agent_ppo2.py:143][0m Total time:      23.20 min
[32m[20221213 21:18:47 @agent_ppo2.py:145][0m 2273280 total steps have happened
[32m[20221213 21:18:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1110 --------------------------#
[32m[20221213 21:18:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:47 @agent_ppo2.py:185][0m |           0.0001 |         136.4635 |          15.7308 |
[32m[20221213 21:18:47 @agent_ppo2.py:185][0m |           0.0018 |         142.9058 |          15.7317 |
[32m[20221213 21:18:47 @agent_ppo2.py:185][0m |          -0.0084 |         135.2155 |          15.7310 |
[32m[20221213 21:18:47 @agent_ppo2.py:185][0m |          -0.0090 |         134.7454 |          15.7512 |
[32m[20221213 21:18:47 @agent_ppo2.py:185][0m |          -0.0097 |         134.4890 |          15.7350 |
[32m[20221213 21:18:48 @agent_ppo2.py:185][0m |          -0.0092 |         134.1837 |          15.7512 |
[32m[20221213 21:18:48 @agent_ppo2.py:185][0m |          -0.0116 |         134.1543 |          15.7436 |
[32m[20221213 21:18:48 @agent_ppo2.py:185][0m |          -0.0075 |         133.8781 |          15.7536 |
[32m[20221213 21:18:48 @agent_ppo2.py:185][0m |          -0.0087 |         133.7827 |          15.7666 |
[32m[20221213 21:18:48 @agent_ppo2.py:185][0m |          -0.0113 |         133.6460 |          15.7862 |
[32m[20221213 21:18:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:18:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 716.00
[32m[20221213 21:18:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 730.00
[32m[20221213 21:18:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 778.00
[32m[20221213 21:18:48 @agent_ppo2.py:143][0m Total time:      23.22 min
[32m[20221213 21:18:48 @agent_ppo2.py:145][0m 2275328 total steps have happened
[32m[20221213 21:18:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1111 --------------------------#
[32m[20221213 21:18:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:48 @agent_ppo2.py:185][0m |           0.0090 |         150.7786 |          15.6947 |
[32m[20221213 21:18:48 @agent_ppo2.py:185][0m |          -0.0061 |         136.3455 |          15.6844 |
[32m[20221213 21:18:49 @agent_ppo2.py:185][0m |          -0.0067 |         135.4296 |          15.7099 |
[32m[20221213 21:18:49 @agent_ppo2.py:185][0m |          -0.0035 |         137.9974 |          15.7089 |
[32m[20221213 21:18:49 @agent_ppo2.py:185][0m |          -0.0108 |         134.3665 |          15.6964 |
[32m[20221213 21:18:49 @agent_ppo2.py:185][0m |          -0.0050 |         137.4800 |          15.6957 |
[32m[20221213 21:18:49 @agent_ppo2.py:185][0m |          -0.0087 |         133.9019 |          15.6893 |
[32m[20221213 21:18:49 @agent_ppo2.py:185][0m |          -0.0122 |         133.4228 |          15.6853 |
[32m[20221213 21:18:49 @agent_ppo2.py:185][0m |          -0.0079 |         133.0474 |          15.6957 |
[32m[20221213 21:18:49 @agent_ppo2.py:185][0m |          -0.0119 |         132.7324 |          15.6906 |
[32m[20221213 21:18:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 694.60
[32m[20221213 21:18:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 725.00
[32m[20221213 21:18:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.00
[32m[20221213 21:18:49 @agent_ppo2.py:143][0m Total time:      23.24 min
[32m[20221213 21:18:49 @agent_ppo2.py:145][0m 2277376 total steps have happened
[32m[20221213 21:18:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1112 --------------------------#
[32m[20221213 21:18:49 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:18:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:50 @agent_ppo2.py:185][0m |           0.0102 |         154.7411 |          15.8191 |
[32m[20221213 21:18:50 @agent_ppo2.py:185][0m |          -0.0026 |         138.6566 |          15.8104 |
[32m[20221213 21:18:50 @agent_ppo2.py:185][0m |          -0.0047 |         137.1369 |          15.8356 |
[32m[20221213 21:18:50 @agent_ppo2.py:185][0m |           0.0022 |         139.3566 |          15.8169 |
[32m[20221213 21:18:50 @agent_ppo2.py:185][0m |          -0.0071 |         135.6313 |          15.7988 |
[32m[20221213 21:18:50 @agent_ppo2.py:185][0m |          -0.0077 |         134.9436 |          15.8109 |
[32m[20221213 21:18:50 @agent_ppo2.py:185][0m |          -0.0082 |         134.1341 |          15.8272 |
[32m[20221213 21:18:50 @agent_ppo2.py:185][0m |          -0.0044 |         135.2435 |          15.7930 |
[32m[20221213 21:18:50 @agent_ppo2.py:185][0m |          -0.0110 |         133.8136 |          15.8120 |
[32m[20221213 21:18:50 @agent_ppo2.py:185][0m |          -0.0101 |         133.5520 |          15.7854 |
[32m[20221213 21:18:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:18:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 704.00
[32m[20221213 21:18:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 743.00
[32m[20221213 21:18:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 726.00
[32m[20221213 21:18:50 @agent_ppo2.py:143][0m Total time:      23.26 min
[32m[20221213 21:18:50 @agent_ppo2.py:145][0m 2279424 total steps have happened
[32m[20221213 21:18:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1113 --------------------------#
[32m[20221213 21:18:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:51 @agent_ppo2.py:185][0m |          -0.0015 |         145.4040 |          15.6093 |
[32m[20221213 21:18:51 @agent_ppo2.py:185][0m |          -0.0036 |         142.6297 |          15.6210 |
[32m[20221213 21:18:51 @agent_ppo2.py:185][0m |          -0.0042 |         141.6340 |          15.5945 |
[32m[20221213 21:18:51 @agent_ppo2.py:185][0m |          -0.0052 |         140.7228 |          15.5995 |
[32m[20221213 21:18:51 @agent_ppo2.py:185][0m |          -0.0071 |         140.1778 |          15.6045 |
[32m[20221213 21:18:51 @agent_ppo2.py:185][0m |          -0.0017 |         141.3367 |          15.5850 |
[32m[20221213 21:18:51 @agent_ppo2.py:185][0m |          -0.0040 |         140.6994 |          15.5850 |
[32m[20221213 21:18:51 @agent_ppo2.py:185][0m |          -0.0068 |         138.9771 |          15.5690 |
[32m[20221213 21:18:51 @agent_ppo2.py:185][0m |          -0.0103 |         138.9993 |          15.5685 |
[32m[20221213 21:18:52 @agent_ppo2.py:185][0m |          -0.0119 |         138.9282 |          15.5624 |
[32m[20221213 21:18:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 676.40
[32m[20221213 21:18:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 706.00
[32m[20221213 21:18:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.00
[32m[20221213 21:18:52 @agent_ppo2.py:143][0m Total time:      23.28 min
[32m[20221213 21:18:52 @agent_ppo2.py:145][0m 2281472 total steps have happened
[32m[20221213 21:18:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1114 --------------------------#
[32m[20221213 21:18:52 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:18:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:52 @agent_ppo2.py:185][0m |           0.0087 |         156.4393 |          15.7593 |
[32m[20221213 21:18:52 @agent_ppo2.py:185][0m |           0.0076 |         151.6464 |          15.7900 |
[32m[20221213 21:18:52 @agent_ppo2.py:185][0m |          -0.0038 |         141.6639 |          15.7797 |
[32m[20221213 21:18:52 @agent_ppo2.py:185][0m |          -0.0050 |         141.1091 |          15.7719 |
[32m[20221213 21:18:52 @agent_ppo2.py:185][0m |          -0.0063 |         140.6364 |          15.7805 |
[32m[20221213 21:18:52 @agent_ppo2.py:185][0m |          -0.0057 |         140.1176 |          15.7837 |
[32m[20221213 21:18:52 @agent_ppo2.py:185][0m |          -0.0032 |         142.4529 |          15.7830 |
[32m[20221213 21:18:53 @agent_ppo2.py:185][0m |           0.0103 |         156.9077 |          15.7938 |
[32m[20221213 21:18:53 @agent_ppo2.py:185][0m |          -0.0069 |         139.4400 |          15.7854 |
[32m[20221213 21:18:53 @agent_ppo2.py:185][0m |          -0.0067 |         138.9884 |          15.7965 |
[32m[20221213 21:18:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 694.60
[32m[20221213 21:18:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 742.00
[32m[20221213 21:18:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 697.00
[32m[20221213 21:18:53 @agent_ppo2.py:143][0m Total time:      23.30 min
[32m[20221213 21:18:53 @agent_ppo2.py:145][0m 2283520 total steps have happened
[32m[20221213 21:18:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1115 --------------------------#
[32m[20221213 21:18:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:18:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:53 @agent_ppo2.py:185][0m |           0.0020 |         138.2047 |          15.6503 |
[32m[20221213 21:18:53 @agent_ppo2.py:185][0m |          -0.0052 |         135.9810 |          15.6698 |
[32m[20221213 21:18:53 @agent_ppo2.py:185][0m |          -0.0042 |         134.6781 |          15.6762 |
[32m[20221213 21:18:53 @agent_ppo2.py:185][0m |          -0.0058 |         134.0239 |          15.6614 |
[32m[20221213 21:18:53 @agent_ppo2.py:185][0m |          -0.0066 |         133.5780 |          15.6593 |
[32m[20221213 21:18:54 @agent_ppo2.py:185][0m |          -0.0061 |         133.0148 |          15.6599 |
[32m[20221213 21:18:54 @agent_ppo2.py:185][0m |          -0.0068 |         132.6539 |          15.6618 |
[32m[20221213 21:18:54 @agent_ppo2.py:185][0m |          -0.0050 |         132.5445 |          15.6578 |
[32m[20221213 21:18:54 @agent_ppo2.py:185][0m |          -0.0094 |         132.2146 |          15.6532 |
[32m[20221213 21:18:54 @agent_ppo2.py:185][0m |          -0.0083 |         131.7036 |          15.6811 |
[32m[20221213 21:18:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:18:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 681.20
[32m[20221213 21:18:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.00
[32m[20221213 21:18:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 707.00
[32m[20221213 21:18:54 @agent_ppo2.py:143][0m Total time:      23.32 min
[32m[20221213 21:18:54 @agent_ppo2.py:145][0m 2285568 total steps have happened
[32m[20221213 21:18:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1116 --------------------------#
[32m[20221213 21:18:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:54 @agent_ppo2.py:185][0m |          -0.0032 |         140.7942 |          15.6830 |
[32m[20221213 21:18:54 @agent_ppo2.py:185][0m |          -0.0041 |         138.5875 |          15.6838 |
[32m[20221213 21:18:55 @agent_ppo2.py:185][0m |          -0.0072 |         137.2562 |          15.6708 |
[32m[20221213 21:18:55 @agent_ppo2.py:185][0m |          -0.0052 |         136.1746 |          15.6759 |
[32m[20221213 21:18:55 @agent_ppo2.py:185][0m |           0.0093 |         143.8935 |          15.6765 |
[32m[20221213 21:18:55 @agent_ppo2.py:185][0m |          -0.0056 |         134.9080 |          15.6744 |
[32m[20221213 21:18:55 @agent_ppo2.py:185][0m |           0.0028 |         141.9058 |          15.6864 |
[32m[20221213 21:18:55 @agent_ppo2.py:185][0m |          -0.0034 |         133.9291 |          15.6789 |
[32m[20221213 21:18:55 @agent_ppo2.py:185][0m |          -0.0006 |         141.7287 |          15.7001 |
[32m[20221213 21:18:55 @agent_ppo2.py:185][0m |           0.0015 |         138.9324 |          15.6992 |
[32m[20221213 21:18:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 686.60
[32m[20221213 21:18:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.00
[32m[20221213 21:18:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 784.00
[32m[20221213 21:18:55 @agent_ppo2.py:143][0m Total time:      23.34 min
[32m[20221213 21:18:55 @agent_ppo2.py:145][0m 2287616 total steps have happened
[32m[20221213 21:18:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1117 --------------------------#
[32m[20221213 21:18:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:56 @agent_ppo2.py:185][0m |          -0.0005 |         145.1941 |          15.5916 |
[32m[20221213 21:18:56 @agent_ppo2.py:185][0m |           0.0006 |         144.1938 |          15.5938 |
[32m[20221213 21:18:56 @agent_ppo2.py:185][0m |          -0.0051 |         142.3466 |          15.5991 |
[32m[20221213 21:18:56 @agent_ppo2.py:185][0m |          -0.0057 |         141.4089 |          15.5788 |
[32m[20221213 21:18:56 @agent_ppo2.py:185][0m |          -0.0056 |         141.2185 |          15.5695 |
[32m[20221213 21:18:56 @agent_ppo2.py:185][0m |          -0.0040 |         140.5175 |          15.5576 |
[32m[20221213 21:18:56 @agent_ppo2.py:185][0m |           0.0042 |         149.6471 |          15.5334 |
[32m[20221213 21:18:56 @agent_ppo2.py:185][0m |          -0.0052 |         141.3501 |          15.5115 |
[32m[20221213 21:18:56 @agent_ppo2.py:185][0m |          -0.0063 |         139.9664 |          15.5169 |
[32m[20221213 21:18:56 @agent_ppo2.py:185][0m |          -0.0081 |         139.3630 |          15.5026 |
[32m[20221213 21:18:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:18:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 671.40
[32m[20221213 21:18:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 727.00
[32m[20221213 21:18:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 738.00
[32m[20221213 21:18:56 @agent_ppo2.py:143][0m Total time:      23.36 min
[32m[20221213 21:18:56 @agent_ppo2.py:145][0m 2289664 total steps have happened
[32m[20221213 21:18:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1118 --------------------------#
[32m[20221213 21:18:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:18:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:57 @agent_ppo2.py:185][0m |          -0.0019 |         140.6443 |          15.5604 |
[32m[20221213 21:18:57 @agent_ppo2.py:185][0m |          -0.0010 |         138.3213 |          15.5603 |
[32m[20221213 21:18:57 @agent_ppo2.py:185][0m |          -0.0036 |         137.2220 |          15.5441 |
[32m[20221213 21:18:57 @agent_ppo2.py:185][0m |          -0.0044 |         136.6773 |          15.5313 |
[32m[20221213 21:18:57 @agent_ppo2.py:185][0m |          -0.0067 |         136.4069 |          15.5378 |
[32m[20221213 21:18:57 @agent_ppo2.py:185][0m |          -0.0033 |         137.3418 |          15.5441 |
[32m[20221213 21:18:57 @agent_ppo2.py:185][0m |          -0.0055 |         135.9755 |          15.5270 |
[32m[20221213 21:18:57 @agent_ppo2.py:185][0m |          -0.0085 |         135.7653 |          15.5350 |
[32m[20221213 21:18:57 @agent_ppo2.py:185][0m |          -0.0004 |         137.3984 |          15.5115 |
[32m[20221213 21:18:57 @agent_ppo2.py:185][0m |           0.0012 |         141.5732 |          15.5164 |
[32m[20221213 21:18:57 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:18:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 655.00
[32m[20221213 21:18:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 732.00
[32m[20221213 21:18:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.00
[32m[20221213 21:18:58 @agent_ppo2.py:143][0m Total time:      23.38 min
[32m[20221213 21:18:58 @agent_ppo2.py:145][0m 2291712 total steps have happened
[32m[20221213 21:18:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1119 --------------------------#
[32m[20221213 21:18:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:58 @agent_ppo2.py:185][0m |           0.0006 |         137.2748 |          15.5565 |
[32m[20221213 21:18:58 @agent_ppo2.py:185][0m |          -0.0024 |         136.0283 |          15.5680 |
[32m[20221213 21:18:58 @agent_ppo2.py:185][0m |          -0.0053 |         135.2696 |          15.6103 |
[32m[20221213 21:18:58 @agent_ppo2.py:185][0m |          -0.0069 |         134.6721 |          15.5856 |
[32m[20221213 21:18:58 @agent_ppo2.py:185][0m |          -0.0038 |         134.5598 |          15.6338 |
[32m[20221213 21:18:58 @agent_ppo2.py:185][0m |          -0.0068 |         133.9255 |          15.6420 |
[32m[20221213 21:18:58 @agent_ppo2.py:185][0m |          -0.0070 |         133.5809 |          15.6367 |
[32m[20221213 21:18:59 @agent_ppo2.py:185][0m |          -0.0047 |         133.1902 |          15.6739 |
[32m[20221213 21:18:59 @agent_ppo2.py:185][0m |          -0.0068 |         132.9331 |          15.6889 |
[32m[20221213 21:18:59 @agent_ppo2.py:185][0m |          -0.0076 |         132.8429 |          15.7098 |
[32m[20221213 21:18:59 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:18:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 697.00
[32m[20221213 21:18:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 720.00
[32m[20221213 21:18:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 652.00
[32m[20221213 21:18:59 @agent_ppo2.py:143][0m Total time:      23.40 min
[32m[20221213 21:18:59 @agent_ppo2.py:145][0m 2293760 total steps have happened
[32m[20221213 21:18:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1120 --------------------------#
[32m[20221213 21:18:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:18:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:18:59 @agent_ppo2.py:185][0m |          -0.0021 |         137.8989 |          15.8655 |
[32m[20221213 21:18:59 @agent_ppo2.py:185][0m |           0.0083 |         146.0295 |          15.8770 |
[32m[20221213 21:18:59 @agent_ppo2.py:185][0m |          -0.0031 |         135.0614 |          15.8580 |
[32m[20221213 21:18:59 @agent_ppo2.py:185][0m |          -0.0061 |         134.4243 |          15.8761 |
[32m[20221213 21:18:59 @agent_ppo2.py:185][0m |          -0.0066 |         134.0734 |          15.8757 |
[32m[20221213 21:19:00 @agent_ppo2.py:185][0m |          -0.0036 |         133.7784 |          15.8459 |
[32m[20221213 21:19:00 @agent_ppo2.py:185][0m |          -0.0062 |         133.5849 |          15.8560 |
[32m[20221213 21:19:00 @agent_ppo2.py:185][0m |          -0.0094 |         133.4733 |          15.8763 |
[32m[20221213 21:19:00 @agent_ppo2.py:185][0m |          -0.0083 |         133.1368 |          15.8694 |
[32m[20221213 21:19:00 @agent_ppo2.py:185][0m |          -0.0085 |         133.0116 |          15.8752 |
[32m[20221213 21:19:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:19:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 683.00
[32m[20221213 21:19:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.00
[32m[20221213 21:19:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 747.00
[32m[20221213 21:19:00 @agent_ppo2.py:143][0m Total time:      23.42 min
[32m[20221213 21:19:00 @agent_ppo2.py:145][0m 2295808 total steps have happened
[32m[20221213 21:19:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1121 --------------------------#
[32m[20221213 21:19:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:00 @agent_ppo2.py:185][0m |          -0.0039 |         134.7890 |          15.8161 |
[32m[20221213 21:19:00 @agent_ppo2.py:185][0m |          -0.0019 |         134.0992 |          15.8140 |
[32m[20221213 21:19:00 @agent_ppo2.py:185][0m |          -0.0060 |         133.8945 |          15.8151 |
[32m[20221213 21:19:01 @agent_ppo2.py:185][0m |          -0.0057 |         133.5193 |          15.8282 |
[32m[20221213 21:19:01 @agent_ppo2.py:185][0m |          -0.0057 |         134.0634 |          15.8219 |
[32m[20221213 21:19:01 @agent_ppo2.py:185][0m |          -0.0099 |         133.3145 |          15.8121 |
[32m[20221213 21:19:01 @agent_ppo2.py:185][0m |          -0.0070 |         133.0506 |          15.8299 |
[32m[20221213 21:19:01 @agent_ppo2.py:185][0m |          -0.0070 |         132.9217 |          15.8181 |
[32m[20221213 21:19:01 @agent_ppo2.py:185][0m |          -0.0057 |         132.7533 |          15.8294 |
[32m[20221213 21:19:01 @agent_ppo2.py:185][0m |          -0.0049 |         133.0230 |          15.8286 |
[32m[20221213 21:19:01 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:19:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 684.20
[32m[20221213 21:19:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.00
[32m[20221213 21:19:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.00
[32m[20221213 21:19:01 @agent_ppo2.py:143][0m Total time:      23.44 min
[32m[20221213 21:19:01 @agent_ppo2.py:145][0m 2297856 total steps have happened
[32m[20221213 21:19:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1122 --------------------------#
[32m[20221213 21:19:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:02 @agent_ppo2.py:185][0m |           0.0049 |         139.3312 |          15.8465 |
[32m[20221213 21:19:02 @agent_ppo2.py:185][0m |          -0.0030 |         134.6787 |          15.8203 |
[32m[20221213 21:19:02 @agent_ppo2.py:185][0m |          -0.0065 |         133.8407 |          15.8072 |
[32m[20221213 21:19:02 @agent_ppo2.py:185][0m |          -0.0063 |         133.4158 |          15.8157 |
[32m[20221213 21:19:02 @agent_ppo2.py:185][0m |          -0.0072 |         133.3042 |          15.8192 |
[32m[20221213 21:19:02 @agent_ppo2.py:185][0m |           0.0009 |         142.8327 |          15.7921 |
[32m[20221213 21:19:02 @agent_ppo2.py:185][0m |          -0.0042 |         132.7496 |          15.7722 |
[32m[20221213 21:19:02 @agent_ppo2.py:185][0m |          -0.0076 |         132.5947 |          15.7727 |
[32m[20221213 21:19:02 @agent_ppo2.py:185][0m |          -0.0052 |         132.5322 |          15.7513 |
[32m[20221213 21:19:02 @agent_ppo2.py:185][0m |          -0.0070 |         132.3020 |          15.7606 |
[32m[20221213 21:19:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:19:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 693.00
[32m[20221213 21:19:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 730.00
[32m[20221213 21:19:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 729.00
[32m[20221213 21:19:02 @agent_ppo2.py:143][0m Total time:      23.46 min
[32m[20221213 21:19:02 @agent_ppo2.py:145][0m 2299904 total steps have happened
[32m[20221213 21:19:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1123 --------------------------#
[32m[20221213 21:19:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:03 @agent_ppo2.py:185][0m |           0.0011 |         132.5212 |          15.7464 |
[32m[20221213 21:19:03 @agent_ppo2.py:185][0m |           0.0106 |         143.2722 |          15.7352 |
[32m[20221213 21:19:03 @agent_ppo2.py:185][0m |          -0.0026 |         131.3282 |          15.6925 |
[32m[20221213 21:19:03 @agent_ppo2.py:185][0m |          -0.0036 |         130.9224 |          15.7089 |
[32m[20221213 21:19:03 @agent_ppo2.py:185][0m |           0.0010 |         135.1365 |          15.7309 |
[32m[20221213 21:19:03 @agent_ppo2.py:185][0m |          -0.0034 |         130.4744 |          15.7061 |
[32m[20221213 21:19:03 @agent_ppo2.py:185][0m |           0.0030 |         132.0529 |          15.7040 |
[32m[20221213 21:19:03 @agent_ppo2.py:185][0m |           0.0024 |         134.2698 |          15.6845 |
[32m[20221213 21:19:03 @agent_ppo2.py:185][0m |          -0.0018 |         130.2685 |          15.6976 |
[32m[20221213 21:19:04 @agent_ppo2.py:185][0m |          -0.0063 |         130.1524 |          15.7092 |
[32m[20221213 21:19:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:19:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 698.80
[32m[20221213 21:19:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 743.00
[32m[20221213 21:19:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 702.00
[32m[20221213 21:19:04 @agent_ppo2.py:143][0m Total time:      23.48 min
[32m[20221213 21:19:04 @agent_ppo2.py:145][0m 2301952 total steps have happened
[32m[20221213 21:19:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1124 --------------------------#
[32m[20221213 21:19:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:04 @agent_ppo2.py:185][0m |          -0.0025 |         135.7212 |          15.5389 |
[32m[20221213 21:19:04 @agent_ppo2.py:185][0m |          -0.0065 |         133.6565 |          15.5497 |
[32m[20221213 21:19:04 @agent_ppo2.py:185][0m |          -0.0063 |         132.4529 |          15.5703 |
[32m[20221213 21:19:04 @agent_ppo2.py:185][0m |           0.0022 |         137.0700 |          15.5727 |
[32m[20221213 21:19:04 @agent_ppo2.py:185][0m |          -0.0070 |         131.0441 |          15.5447 |
[32m[20221213 21:19:04 @agent_ppo2.py:185][0m |          -0.0094 |         130.7061 |          15.5916 |
[32m[20221213 21:19:04 @agent_ppo2.py:185][0m |          -0.0088 |         130.4015 |          15.6124 |
[32m[20221213 21:19:05 @agent_ppo2.py:185][0m |          -0.0103 |         130.2509 |          15.6160 |
[32m[20221213 21:19:05 @agent_ppo2.py:185][0m |          -0.0111 |         129.5641 |          15.6203 |
[32m[20221213 21:19:05 @agent_ppo2.py:185][0m |          -0.0002 |         144.4218 |          15.6210 |
[32m[20221213 21:19:05 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:19:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 686.80
[32m[20221213 21:19:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 712.00
[32m[20221213 21:19:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 769.00
[32m[20221213 21:19:05 @agent_ppo2.py:143][0m Total time:      23.50 min
[32m[20221213 21:19:05 @agent_ppo2.py:145][0m 2304000 total steps have happened
[32m[20221213 21:19:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1125 --------------------------#
[32m[20221213 21:19:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:05 @agent_ppo2.py:185][0m |          -0.0027 |         138.8062 |          15.9302 |
[32m[20221213 21:19:05 @agent_ppo2.py:185][0m |          -0.0050 |         137.2320 |          15.9441 |
[32m[20221213 21:19:05 @agent_ppo2.py:185][0m |          -0.0060 |         136.4257 |          15.9123 |
[32m[20221213 21:19:06 @agent_ppo2.py:185][0m |          -0.0102 |         136.0463 |          15.9058 |
[32m[20221213 21:19:06 @agent_ppo2.py:185][0m |          -0.0063 |         135.5693 |          15.8820 |
[32m[20221213 21:19:06 @agent_ppo2.py:185][0m |           0.0045 |         141.5320 |          15.8882 |
[32m[20221213 21:19:06 @agent_ppo2.py:185][0m |          -0.0082 |         134.5224 |          15.8766 |
[32m[20221213 21:19:06 @agent_ppo2.py:185][0m |          -0.0052 |         135.9030 |          15.8893 |
[32m[20221213 21:19:06 @agent_ppo2.py:185][0m |          -0.0096 |         134.0081 |          15.8751 |
[32m[20221213 21:19:06 @agent_ppo2.py:185][0m |          -0.0084 |         133.8709 |          15.8793 |
[32m[20221213 21:19:06 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 21:19:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 661.60
[32m[20221213 21:19:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 689.00
[32m[20221213 21:19:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.00
[32m[20221213 21:19:06 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 824.00
[32m[20221213 21:19:06 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 824.00
[32m[20221213 21:19:06 @agent_ppo2.py:143][0m Total time:      23.53 min
[32m[20221213 21:19:06 @agent_ppo2.py:145][0m 2306048 total steps have happened
[32m[20221213 21:19:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1126 --------------------------#
[32m[20221213 21:19:07 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 21:19:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:07 @agent_ppo2.py:185][0m |          -0.0021 |         136.9249 |          15.6800 |
[32m[20221213 21:19:07 @agent_ppo2.py:185][0m |          -0.0043 |         135.6095 |          15.6663 |
[32m[20221213 21:19:07 @agent_ppo2.py:185][0m |          -0.0062 |         135.0129 |          15.6843 |
[32m[20221213 21:19:07 @agent_ppo2.py:185][0m |          -0.0071 |         134.7551 |          15.6654 |
[32m[20221213 21:19:07 @agent_ppo2.py:185][0m |          -0.0057 |         134.6737 |          15.6762 |
[32m[20221213 21:19:07 @agent_ppo2.py:185][0m |          -0.0090 |         134.1419 |          15.6752 |
[32m[20221213 21:19:07 @agent_ppo2.py:185][0m |          -0.0006 |         143.4513 |          15.6909 |
[32m[20221213 21:19:07 @agent_ppo2.py:185][0m |          -0.0029 |         134.9761 |          15.6915 |
[32m[20221213 21:19:07 @agent_ppo2.py:185][0m |          -0.0068 |         133.7981 |          15.7043 |
[32m[20221213 21:19:08 @agent_ppo2.py:185][0m |          -0.0065 |         133.9443 |          15.7042 |
[32m[20221213 21:19:08 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:19:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 680.00
[32m[20221213 21:19:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.00
[32m[20221213 21:19:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.00
[32m[20221213 21:19:08 @agent_ppo2.py:143][0m Total time:      23.55 min
[32m[20221213 21:19:08 @agent_ppo2.py:145][0m 2308096 total steps have happened
[32m[20221213 21:19:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1127 --------------------------#
[32m[20221213 21:19:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:08 @agent_ppo2.py:185][0m |          -0.0015 |         132.2922 |          15.8382 |
[32m[20221213 21:19:08 @agent_ppo2.py:185][0m |          -0.0042 |         129.7382 |          15.7999 |
[32m[20221213 21:19:08 @agent_ppo2.py:185][0m |          -0.0023 |         130.2459 |          15.8257 |
[32m[20221213 21:19:08 @agent_ppo2.py:185][0m |          -0.0065 |         128.8876 |          15.8150 |
[32m[20221213 21:19:08 @agent_ppo2.py:185][0m |          -0.0100 |         128.1617 |          15.8154 |
[32m[20221213 21:19:08 @agent_ppo2.py:185][0m |          -0.0066 |         127.7674 |          15.8142 |
[32m[20221213 21:19:09 @agent_ppo2.py:185][0m |          -0.0116 |         127.5208 |          15.8372 |
[32m[20221213 21:19:09 @agent_ppo2.py:185][0m |          -0.0086 |         127.1656 |          15.8366 |
[32m[20221213 21:19:09 @agent_ppo2.py:185][0m |          -0.0112 |         127.1031 |          15.8462 |
[32m[20221213 21:19:09 @agent_ppo2.py:185][0m |          -0.0106 |         126.7505 |          15.8507 |
[32m[20221213 21:19:09 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:19:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 658.60
[32m[20221213 21:19:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 777.00
[32m[20221213 21:19:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 741.00
[32m[20221213 21:19:09 @agent_ppo2.py:143][0m Total time:      23.57 min
[32m[20221213 21:19:09 @agent_ppo2.py:145][0m 2310144 total steps have happened
[32m[20221213 21:19:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1128 --------------------------#
[32m[20221213 21:19:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:19:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:09 @agent_ppo2.py:185][0m |           0.0033 |         139.1512 |          15.8622 |
[32m[20221213 21:19:09 @agent_ppo2.py:185][0m |          -0.0027 |         137.8211 |          15.8023 |
[32m[20221213 21:19:09 @agent_ppo2.py:185][0m |          -0.0041 |         136.8984 |          15.7766 |
[32m[20221213 21:19:10 @agent_ppo2.py:185][0m |          -0.0036 |         136.6161 |          15.7736 |
[32m[20221213 21:19:10 @agent_ppo2.py:185][0m |          -0.0048 |         135.9889 |          15.7751 |
[32m[20221213 21:19:10 @agent_ppo2.py:185][0m |          -0.0056 |         135.7021 |          15.7650 |
[32m[20221213 21:19:10 @agent_ppo2.py:185][0m |          -0.0051 |         135.5561 |          15.7625 |
[32m[20221213 21:19:10 @agent_ppo2.py:185][0m |          -0.0075 |         135.1599 |          15.7751 |
[32m[20221213 21:19:10 @agent_ppo2.py:185][0m |          -0.0059 |         135.0318 |          15.7451 |
[32m[20221213 21:19:10 @agent_ppo2.py:185][0m |           0.0076 |         153.6482 |          15.7750 |
[32m[20221213 21:19:10 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:19:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 665.60
[32m[20221213 21:19:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 728.00
[32m[20221213 21:19:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 754.00
[32m[20221213 21:19:10 @agent_ppo2.py:143][0m Total time:      23.59 min
[32m[20221213 21:19:10 @agent_ppo2.py:145][0m 2312192 total steps have happened
[32m[20221213 21:19:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1129 --------------------------#
[32m[20221213 21:19:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:11 @agent_ppo2.py:185][0m |           0.0045 |         142.7841 |          15.6886 |
[32m[20221213 21:19:11 @agent_ppo2.py:185][0m |           0.0032 |         139.8817 |          15.6713 |
[32m[20221213 21:19:11 @agent_ppo2.py:185][0m |          -0.0009 |         137.5164 |          15.6968 |
[32m[20221213 21:19:11 @agent_ppo2.py:185][0m |          -0.0064 |         136.9534 |          15.6904 |
[32m[20221213 21:19:11 @agent_ppo2.py:185][0m |          -0.0087 |         136.6732 |          15.6850 |
[32m[20221213 21:19:11 @agent_ppo2.py:185][0m |          -0.0030 |         138.5667 |          15.6762 |
[32m[20221213 21:19:11 @agent_ppo2.py:185][0m |           0.0004 |         139.1720 |          15.6970 |
[32m[20221213 21:19:11 @agent_ppo2.py:185][0m |          -0.0077 |         135.8006 |          15.6944 |
[32m[20221213 21:19:11 @agent_ppo2.py:185][0m |          -0.0034 |         137.3768 |          15.7012 |
[32m[20221213 21:19:11 @agent_ppo2.py:185][0m |          -0.0077 |         135.4787 |          15.7082 |
[32m[20221213 21:19:11 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:19:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.40
[32m[20221213 21:19:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 655.00
[32m[20221213 21:19:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 735.00
[32m[20221213 21:19:11 @agent_ppo2.py:143][0m Total time:      23.61 min
[32m[20221213 21:19:11 @agent_ppo2.py:145][0m 2314240 total steps have happened
[32m[20221213 21:19:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1130 --------------------------#
[32m[20221213 21:19:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:19:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:12 @agent_ppo2.py:185][0m |          -0.0038 |         133.4774 |          15.8328 |
[32m[20221213 21:19:12 @agent_ppo2.py:185][0m |          -0.0055 |         132.9279 |          15.8033 |
[32m[20221213 21:19:12 @agent_ppo2.py:185][0m |          -0.0067 |         131.9797 |          15.8084 |
[32m[20221213 21:19:12 @agent_ppo2.py:185][0m |          -0.0054 |         132.6820 |          15.8292 |
[32m[20221213 21:19:12 @agent_ppo2.py:185][0m |          -0.0079 |         131.2473 |          15.8315 |
[32m[20221213 21:19:12 @agent_ppo2.py:185][0m |          -0.0066 |         131.1413 |          15.8025 |
[32m[20221213 21:19:12 @agent_ppo2.py:185][0m |          -0.0053 |         131.4789 |          15.8081 |
[32m[20221213 21:19:12 @agent_ppo2.py:185][0m |          -0.0015 |         134.3769 |          15.8184 |
[32m[20221213 21:19:13 @agent_ppo2.py:185][0m |          -0.0065 |         130.5603 |          15.8125 |
[32m[20221213 21:19:13 @agent_ppo2.py:185][0m |          -0.0067 |         130.8162 |          15.8330 |
[32m[20221213 21:19:13 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:19:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 667.00
[32m[20221213 21:19:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.00
[32m[20221213 21:19:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 742.00
[32m[20221213 21:19:13 @agent_ppo2.py:143][0m Total time:      23.63 min
[32m[20221213 21:19:13 @agent_ppo2.py:145][0m 2316288 total steps have happened
[32m[20221213 21:19:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1131 --------------------------#
[32m[20221213 21:19:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:19:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:13 @agent_ppo2.py:185][0m |           0.0094 |         148.1938 |          15.7138 |
[32m[20221213 21:19:13 @agent_ppo2.py:185][0m |          -0.0046 |         134.6905 |          15.7131 |
[32m[20221213 21:19:13 @agent_ppo2.py:185][0m |          -0.0054 |         134.4343 |          15.7628 |
[32m[20221213 21:19:13 @agent_ppo2.py:185][0m |          -0.0076 |         134.2015 |          15.7380 |
[32m[20221213 21:19:13 @agent_ppo2.py:185][0m |          -0.0066 |         133.9238 |          15.7538 |
[32m[20221213 21:19:14 @agent_ppo2.py:185][0m |          -0.0079 |         133.7963 |          15.7417 |
[32m[20221213 21:19:14 @agent_ppo2.py:185][0m |          -0.0079 |         133.7519 |          15.7603 |
[32m[20221213 21:19:14 @agent_ppo2.py:185][0m |           0.0032 |         144.7688 |          15.7598 |
[32m[20221213 21:19:14 @agent_ppo2.py:185][0m |          -0.0043 |         135.7327 |          15.7705 |
[32m[20221213 21:19:14 @agent_ppo2.py:185][0m |          -0.0090 |         133.4573 |          15.7839 |
[32m[20221213 21:19:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 660.40
[32m[20221213 21:19:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 748.00
[32m[20221213 21:19:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 739.00
[32m[20221213 21:19:14 @agent_ppo2.py:143][0m Total time:      23.65 min
[32m[20221213 21:19:14 @agent_ppo2.py:145][0m 2318336 total steps have happened
[32m[20221213 21:19:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1132 --------------------------#
[32m[20221213 21:19:14 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:19:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:14 @agent_ppo2.py:185][0m |           0.0102 |         143.3630 |          15.7737 |
[32m[20221213 21:19:14 @agent_ppo2.py:185][0m |          -0.0015 |         128.2929 |          15.8088 |
[32m[20221213 21:19:14 @agent_ppo2.py:185][0m |          -0.0033 |         127.7858 |          15.7798 |
[32m[20221213 21:19:15 @agent_ppo2.py:185][0m |          -0.0050 |         127.4332 |          15.7924 |
[32m[20221213 21:19:15 @agent_ppo2.py:185][0m |          -0.0051 |         127.5759 |          15.7652 |
[32m[20221213 21:19:15 @agent_ppo2.py:185][0m |           0.0009 |         130.2521 |          15.7528 |
[32m[20221213 21:19:15 @agent_ppo2.py:185][0m |          -0.0073 |         127.1548 |          15.7691 |
[32m[20221213 21:19:15 @agent_ppo2.py:185][0m |          -0.0083 |         126.8065 |          15.7300 |
[32m[20221213 21:19:15 @agent_ppo2.py:185][0m |          -0.0084 |         126.6239 |          15.7393 |
[32m[20221213 21:19:15 @agent_ppo2.py:185][0m |          -0.0089 |         126.4782 |          15.7241 |
[32m[20221213 21:19:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:19:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 669.00
[32m[20221213 21:19:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 724.00
[32m[20221213 21:19:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 740.00
[32m[20221213 21:19:15 @agent_ppo2.py:143][0m Total time:      23.67 min
[32m[20221213 21:19:15 @agent_ppo2.py:145][0m 2320384 total steps have happened
[32m[20221213 21:19:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1133 --------------------------#
[32m[20221213 21:19:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:15 @agent_ppo2.py:185][0m |          -0.0003 |         131.0970 |          15.7232 |
[32m[20221213 21:19:16 @agent_ppo2.py:185][0m |          -0.0027 |         129.7151 |          15.7436 |
[32m[20221213 21:19:16 @agent_ppo2.py:185][0m |          -0.0079 |         129.1793 |          15.7320 |
[32m[20221213 21:19:16 @agent_ppo2.py:185][0m |          -0.0051 |         128.4861 |          15.7262 |
[32m[20221213 21:19:16 @agent_ppo2.py:185][0m |          -0.0042 |         128.0862 |          15.7492 |
[32m[20221213 21:19:16 @agent_ppo2.py:185][0m |          -0.0071 |         127.7619 |          15.7445 |
[32m[20221213 21:19:16 @agent_ppo2.py:185][0m |          -0.0041 |         127.7391 |          15.7321 |
[32m[20221213 21:19:16 @agent_ppo2.py:185][0m |          -0.0083 |         127.2765 |          15.7265 |
[32m[20221213 21:19:16 @agent_ppo2.py:185][0m |          -0.0073 |         127.0081 |          15.7449 |
[32m[20221213 21:19:16 @agent_ppo2.py:185][0m |           0.0128 |         143.7217 |          15.7276 |
[32m[20221213 21:19:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:19:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 681.80
[32m[20221213 21:19:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 715.00
[32m[20221213 21:19:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.00
[32m[20221213 21:19:16 @agent_ppo2.py:143][0m Total time:      23.69 min
[32m[20221213 21:19:16 @agent_ppo2.py:145][0m 2322432 total steps have happened
[32m[20221213 21:19:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1134 --------------------------#
[32m[20221213 21:19:16 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:19:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:17 @agent_ppo2.py:185][0m |           0.0111 |         139.9067 |          15.5990 |
[32m[20221213 21:19:17 @agent_ppo2.py:185][0m |          -0.0029 |         129.7970 |          15.6247 |
[32m[20221213 21:19:17 @agent_ppo2.py:185][0m |          -0.0046 |         128.5160 |          15.6200 |
[32m[20221213 21:19:17 @agent_ppo2.py:185][0m |          -0.0042 |         127.8893 |          15.6153 |
[32m[20221213 21:19:17 @agent_ppo2.py:185][0m |          -0.0058 |         127.2587 |          15.6275 |
[32m[20221213 21:19:17 @agent_ppo2.py:185][0m |          -0.0096 |         126.9542 |          15.6207 |
[32m[20221213 21:19:17 @agent_ppo2.py:185][0m |          -0.0040 |         127.3030 |          15.6407 |
[32m[20221213 21:19:17 @agent_ppo2.py:185][0m |          -0.0098 |         126.5077 |          15.6269 |
[32m[20221213 21:19:17 @agent_ppo2.py:185][0m |           0.0050 |         142.2081 |          15.6454 |
[32m[20221213 21:19:17 @agent_ppo2.py:185][0m |          -0.0059 |         126.1714 |          15.6395 |
[32m[20221213 21:19:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:19:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 673.60
[32m[20221213 21:19:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.00
[32m[20221213 21:19:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.00
[32m[20221213 21:19:18 @agent_ppo2.py:143][0m Total time:      23.71 min
[32m[20221213 21:19:18 @agent_ppo2.py:145][0m 2324480 total steps have happened
[32m[20221213 21:19:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1135 --------------------------#
[32m[20221213 21:19:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:18 @agent_ppo2.py:185][0m |          -0.0028 |         137.0662 |          15.7158 |
[32m[20221213 21:19:18 @agent_ppo2.py:185][0m |           0.0002 |         135.4337 |          15.7315 |
[32m[20221213 21:19:18 @agent_ppo2.py:185][0m |          -0.0048 |         133.9488 |          15.7140 |
[32m[20221213 21:19:18 @agent_ppo2.py:185][0m |          -0.0053 |         133.0939 |          15.7239 |
[32m[20221213 21:19:18 @agent_ppo2.py:185][0m |          -0.0004 |         133.9515 |          15.7149 |
[32m[20221213 21:19:18 @agent_ppo2.py:185][0m |          -0.0075 |         132.1604 |          15.7159 |
[32m[20221213 21:19:18 @agent_ppo2.py:185][0m |          -0.0057 |         131.8719 |          15.7062 |
[32m[20221213 21:19:18 @agent_ppo2.py:185][0m |          -0.0079 |         131.4524 |          15.6977 |
[32m[20221213 21:19:19 @agent_ppo2.py:185][0m |          -0.0070 |         131.0814 |          15.6892 |
[32m[20221213 21:19:19 @agent_ppo2.py:185][0m |           0.0044 |         148.7860 |          15.6910 |
[32m[20221213 21:19:19 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:19:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 681.40
[32m[20221213 21:19:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.00
[32m[20221213 21:19:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 721.00
[32m[20221213 21:19:19 @agent_ppo2.py:143][0m Total time:      23.73 min
[32m[20221213 21:19:19 @agent_ppo2.py:145][0m 2326528 total steps have happened
[32m[20221213 21:19:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1136 --------------------------#
[32m[20221213 21:19:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:19 @agent_ppo2.py:185][0m |           0.0075 |         141.8695 |          15.7156 |
[32m[20221213 21:19:19 @agent_ppo2.py:185][0m |          -0.0046 |         132.4025 |          15.7492 |
[32m[20221213 21:19:19 @agent_ppo2.py:185][0m |          -0.0055 |         130.3786 |          15.7670 |
[32m[20221213 21:19:19 @agent_ppo2.py:185][0m |          -0.0068 |         128.8837 |          15.7991 |
[32m[20221213 21:19:19 @agent_ppo2.py:185][0m |           0.0009 |         134.5490 |          15.8160 |
[32m[20221213 21:19:20 @agent_ppo2.py:185][0m |          -0.0081 |         127.5365 |          15.8219 |
[32m[20221213 21:19:20 @agent_ppo2.py:185][0m |          -0.0101 |         127.4690 |          15.8422 |
[32m[20221213 21:19:20 @agent_ppo2.py:185][0m |          -0.0091 |         126.6228 |          15.8710 |
[32m[20221213 21:19:20 @agent_ppo2.py:185][0m |          -0.0119 |         126.6157 |          15.8844 |
[32m[20221213 21:19:20 @agent_ppo2.py:185][0m |          -0.0085 |         125.6868 |          15.9120 |
[32m[20221213 21:19:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 696.60
[32m[20221213 21:19:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 725.00
[32m[20221213 21:19:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.00
[32m[20221213 21:19:20 @agent_ppo2.py:143][0m Total time:      23.75 min
[32m[20221213 21:19:20 @agent_ppo2.py:145][0m 2328576 total steps have happened
[32m[20221213 21:19:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1137 --------------------------#
[32m[20221213 21:19:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:20 @agent_ppo2.py:185][0m |           0.0025 |         141.5930 |          15.7723 |
[32m[20221213 21:19:20 @agent_ppo2.py:185][0m |          -0.0037 |         139.8885 |          15.8198 |
[32m[20221213 21:19:20 @agent_ppo2.py:185][0m |          -0.0049 |         138.7660 |          15.8262 |
[32m[20221213 21:19:21 @agent_ppo2.py:185][0m |          -0.0016 |         138.6519 |          15.8131 |
[32m[20221213 21:19:21 @agent_ppo2.py:185][0m |          -0.0065 |         137.9226 |          15.8171 |
[32m[20221213 21:19:21 @agent_ppo2.py:185][0m |          -0.0071 |         137.5455 |          15.8316 |
[32m[20221213 21:19:21 @agent_ppo2.py:185][0m |          -0.0079 |         137.0999 |          15.8308 |
[32m[20221213 21:19:21 @agent_ppo2.py:185][0m |           0.0066 |         149.7776 |          15.8260 |
[32m[20221213 21:19:21 @agent_ppo2.py:185][0m |          -0.0082 |         136.6278 |          15.8390 |
[32m[20221213 21:19:21 @agent_ppo2.py:185][0m |          -0.0032 |         136.8769 |          15.8501 |
[32m[20221213 21:19:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 663.80
[32m[20221213 21:19:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.00
[32m[20221213 21:19:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 706.00
[32m[20221213 21:19:21 @agent_ppo2.py:143][0m Total time:      23.77 min
[32m[20221213 21:19:21 @agent_ppo2.py:145][0m 2330624 total steps have happened
[32m[20221213 21:19:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1138 --------------------------#
[32m[20221213 21:19:21 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:19:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:21 @agent_ppo2.py:185][0m |          -0.0019 |         133.2142 |          15.9367 |
[32m[20221213 21:19:22 @agent_ppo2.py:185][0m |          -0.0055 |         130.3051 |          15.9718 |
[32m[20221213 21:19:22 @agent_ppo2.py:185][0m |           0.0040 |         137.0908 |          15.9679 |
[32m[20221213 21:19:22 @agent_ppo2.py:185][0m |          -0.0043 |         130.7873 |          15.9780 |
[32m[20221213 21:19:22 @agent_ppo2.py:185][0m |          -0.0077 |         128.5352 |          15.9829 |
[32m[20221213 21:19:22 @agent_ppo2.py:185][0m |          -0.0010 |         131.1964 |          15.9977 |
[32m[20221213 21:19:22 @agent_ppo2.py:185][0m |          -0.0035 |         128.9476 |          15.9463 |
[32m[20221213 21:19:22 @agent_ppo2.py:185][0m |          -0.0031 |         129.7915 |          15.9750 |
[32m[20221213 21:19:22 @agent_ppo2.py:185][0m |          -0.0000 |         135.5485 |          15.9853 |
[32m[20221213 21:19:22 @agent_ppo2.py:185][0m |          -0.0105 |         127.3382 |          15.9689 |
[32m[20221213 21:19:22 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:19:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 684.20
[32m[20221213 21:19:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 749.00
[32m[20221213 21:19:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 728.00
[32m[20221213 21:19:22 @agent_ppo2.py:143][0m Total time:      23.79 min
[32m[20221213 21:19:22 @agent_ppo2.py:145][0m 2332672 total steps have happened
[32m[20221213 21:19:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1139 --------------------------#
[32m[20221213 21:19:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:23 @agent_ppo2.py:185][0m |           0.0004 |         131.6479 |          15.9812 |
[32m[20221213 21:19:23 @agent_ppo2.py:185][0m |          -0.0038 |         127.5868 |          15.9547 |
[32m[20221213 21:19:23 @agent_ppo2.py:185][0m |          -0.0078 |         125.7365 |          15.9452 |
[32m[20221213 21:19:23 @agent_ppo2.py:185][0m |          -0.0097 |         124.5740 |          15.9470 |
[32m[20221213 21:19:23 @agent_ppo2.py:185][0m |          -0.0072 |         123.8177 |          15.9376 |
[32m[20221213 21:19:23 @agent_ppo2.py:185][0m |           0.0026 |         131.2734 |          15.9273 |
[32m[20221213 21:19:23 @agent_ppo2.py:185][0m |          -0.0058 |         122.8252 |          15.8994 |
[32m[20221213 21:19:23 @agent_ppo2.py:185][0m |          -0.0118 |         122.0678 |          15.9117 |
[32m[20221213 21:19:23 @agent_ppo2.py:185][0m |          -0.0072 |         121.9758 |          15.8845 |
[32m[20221213 21:19:23 @agent_ppo2.py:185][0m |          -0.0024 |         128.7806 |          15.9001 |
[32m[20221213 21:19:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 674.80
[32m[20221213 21:19:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.00
[32m[20221213 21:19:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.00
[32m[20221213 21:19:24 @agent_ppo2.py:143][0m Total time:      23.81 min
[32m[20221213 21:19:24 @agent_ppo2.py:145][0m 2334720 total steps have happened
[32m[20221213 21:19:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1140 --------------------------#
[32m[20221213 21:19:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:24 @agent_ppo2.py:185][0m |          -0.0002 |         142.3932 |          16.0098 |
[32m[20221213 21:19:24 @agent_ppo2.py:185][0m |          -0.0043 |         140.2519 |          15.9861 |
[32m[20221213 21:19:24 @agent_ppo2.py:185][0m |          -0.0062 |         139.3711 |          15.9734 |
[32m[20221213 21:19:24 @agent_ppo2.py:185][0m |          -0.0073 |         138.9762 |          15.9712 |
[32m[20221213 21:19:24 @agent_ppo2.py:185][0m |          -0.0079 |         138.4602 |          15.9640 |
[32m[20221213 21:19:24 @agent_ppo2.py:185][0m |          -0.0055 |         138.0957 |          15.9457 |
[32m[20221213 21:19:24 @agent_ppo2.py:185][0m |          -0.0088 |         138.1112 |          15.9513 |
[32m[20221213 21:19:24 @agent_ppo2.py:185][0m |          -0.0079 |         137.7179 |          15.9422 |
[32m[20221213 21:19:25 @agent_ppo2.py:185][0m |          -0.0101 |         137.5265 |          15.9146 |
[32m[20221213 21:19:25 @agent_ppo2.py:185][0m |          -0.0046 |         138.0275 |          15.9253 |
[32m[20221213 21:19:25 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:19:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 680.00
[32m[20221213 21:19:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 720.00
[32m[20221213 21:19:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.00
[32m[20221213 21:19:25 @agent_ppo2.py:143][0m Total time:      23.83 min
[32m[20221213 21:19:25 @agent_ppo2.py:145][0m 2336768 total steps have happened
[32m[20221213 21:19:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1141 --------------------------#
[32m[20221213 21:19:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:19:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:25 @agent_ppo2.py:185][0m |           0.0099 |         137.8040 |          15.8240 |
[32m[20221213 21:19:25 @agent_ppo2.py:185][0m |          -0.0021 |         129.8347 |          15.8016 |
[32m[20221213 21:19:25 @agent_ppo2.py:185][0m |          -0.0043 |         128.5320 |          15.7834 |
[32m[20221213 21:19:25 @agent_ppo2.py:185][0m |           0.0055 |         133.3949 |          15.7620 |
[32m[20221213 21:19:25 @agent_ppo2.py:185][0m |          -0.0062 |         127.3528 |          15.7676 |
[32m[20221213 21:19:26 @agent_ppo2.py:185][0m |          -0.0044 |         126.9360 |          15.7675 |
[32m[20221213 21:19:26 @agent_ppo2.py:185][0m |          -0.0075 |         125.6381 |          15.7493 |
[32m[20221213 21:19:26 @agent_ppo2.py:185][0m |          -0.0031 |         126.2948 |          15.7426 |
[32m[20221213 21:19:26 @agent_ppo2.py:185][0m |          -0.0070 |         124.6218 |          15.7341 |
[32m[20221213 21:19:26 @agent_ppo2.py:185][0m |          -0.0084 |         124.1217 |          15.7343 |
[32m[20221213 21:19:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 692.00
[32m[20221213 21:19:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 710.00
[32m[20221213 21:19:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 778.00
[32m[20221213 21:19:26 @agent_ppo2.py:143][0m Total time:      23.85 min
[32m[20221213 21:19:26 @agent_ppo2.py:145][0m 2338816 total steps have happened
[32m[20221213 21:19:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1142 --------------------------#
[32m[20221213 21:19:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:26 @agent_ppo2.py:185][0m |          -0.0005 |         138.8864 |          15.7339 |
[32m[20221213 21:19:26 @agent_ppo2.py:185][0m |          -0.0012 |         137.0823 |          15.7474 |
[32m[20221213 21:19:26 @agent_ppo2.py:185][0m |          -0.0048 |         135.6747 |          15.7391 |
[32m[20221213 21:19:27 @agent_ppo2.py:185][0m |          -0.0095 |         134.7199 |          15.7595 |
[32m[20221213 21:19:27 @agent_ppo2.py:185][0m |          -0.0066 |         134.2182 |          15.7472 |
[32m[20221213 21:19:27 @agent_ppo2.py:185][0m |          -0.0066 |         134.0335 |          15.7449 |
[32m[20221213 21:19:27 @agent_ppo2.py:185][0m |          -0.0099 |         133.7492 |          15.7147 |
[32m[20221213 21:19:27 @agent_ppo2.py:185][0m |          -0.0047 |         133.6218 |          15.7496 |
[32m[20221213 21:19:27 @agent_ppo2.py:185][0m |          -0.0087 |         133.1711 |          15.7357 |
[32m[20221213 21:19:27 @agent_ppo2.py:185][0m |          -0.0110 |         133.2200 |          15.7427 |
[32m[20221213 21:19:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 626.00
[32m[20221213 21:19:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 699.00
[32m[20221213 21:19:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 712.00
[32m[20221213 21:19:27 @agent_ppo2.py:143][0m Total time:      23.87 min
[32m[20221213 21:19:27 @agent_ppo2.py:145][0m 2340864 total steps have happened
[32m[20221213 21:19:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1143 --------------------------#
[32m[20221213 21:19:27 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:19:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:27 @agent_ppo2.py:185][0m |          -0.0018 |         133.8887 |          15.7555 |
[32m[20221213 21:19:28 @agent_ppo2.py:185][0m |           0.0001 |         133.2128 |          15.7440 |
[32m[20221213 21:19:28 @agent_ppo2.py:185][0m |           0.0021 |         133.2160 |          15.7003 |
[32m[20221213 21:19:28 @agent_ppo2.py:185][0m |          -0.0046 |         131.3300 |          15.6982 |
[32m[20221213 21:19:28 @agent_ppo2.py:185][0m |           0.0001 |         132.7164 |          15.6952 |
[32m[20221213 21:19:28 @agent_ppo2.py:185][0m |          -0.0057 |         130.6482 |          15.6730 |
[32m[20221213 21:19:28 @agent_ppo2.py:185][0m |          -0.0004 |         134.6460 |          15.6667 |
[32m[20221213 21:19:28 @agent_ppo2.py:185][0m |          -0.0084 |         130.3014 |          15.6515 |
[32m[20221213 21:19:28 @agent_ppo2.py:185][0m |          -0.0086 |         129.9818 |          15.6592 |
[32m[20221213 21:19:28 @agent_ppo2.py:185][0m |          -0.0077 |         129.7098 |          15.6460 |
[32m[20221213 21:19:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 701.80
[32m[20221213 21:19:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 731.00
[32m[20221213 21:19:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.00
[32m[20221213 21:19:28 @agent_ppo2.py:143][0m Total time:      23.89 min
[32m[20221213 21:19:28 @agent_ppo2.py:145][0m 2342912 total steps have happened
[32m[20221213 21:19:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1144 --------------------------#
[32m[20221213 21:19:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:19:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:29 @agent_ppo2.py:185][0m |           0.0044 |         137.1582 |          15.6613 |
[32m[20221213 21:19:29 @agent_ppo2.py:185][0m |          -0.0048 |         132.7369 |          15.6441 |
[32m[20221213 21:19:29 @agent_ppo2.py:185][0m |          -0.0053 |         131.4353 |          15.6419 |
[32m[20221213 21:19:29 @agent_ppo2.py:185][0m |          -0.0063 |         130.3709 |          15.6334 |
[32m[20221213 21:19:29 @agent_ppo2.py:185][0m |          -0.0081 |         129.7287 |          15.6320 |
[32m[20221213 21:19:29 @agent_ppo2.py:185][0m |          -0.0069 |         129.3401 |          15.6259 |
[32m[20221213 21:19:29 @agent_ppo2.py:185][0m |          -0.0084 |         128.8350 |          15.6237 |
[32m[20221213 21:19:29 @agent_ppo2.py:185][0m |          -0.0113 |         128.0832 |          15.6156 |
[32m[20221213 21:19:29 @agent_ppo2.py:185][0m |          -0.0100 |         127.7182 |          15.6318 |
[32m[20221213 21:19:29 @agent_ppo2.py:185][0m |          -0.0076 |         128.0469 |          15.6243 |
[32m[20221213 21:19:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 668.00
[32m[20221213 21:19:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 702.00
[32m[20221213 21:19:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 764.00
[32m[20221213 21:19:30 @agent_ppo2.py:143][0m Total time:      23.91 min
[32m[20221213 21:19:30 @agent_ppo2.py:145][0m 2344960 total steps have happened
[32m[20221213 21:19:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1145 --------------------------#
[32m[20221213 21:19:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:30 @agent_ppo2.py:185][0m |           0.0035 |         137.7086 |          15.6681 |
[32m[20221213 21:19:30 @agent_ppo2.py:185][0m |          -0.0022 |         135.0503 |          15.6785 |
[32m[20221213 21:19:30 @agent_ppo2.py:185][0m |          -0.0054 |         134.1059 |          15.6870 |
[32m[20221213 21:19:30 @agent_ppo2.py:185][0m |          -0.0051 |         133.6508 |          15.6676 |
[32m[20221213 21:19:30 @agent_ppo2.py:185][0m |          -0.0069 |         133.0113 |          15.6613 |
[32m[20221213 21:19:30 @agent_ppo2.py:185][0m |          -0.0098 |         132.7410 |          15.6715 |
[32m[20221213 21:19:30 @agent_ppo2.py:185][0m |          -0.0075 |         132.4559 |          15.6389 |
[32m[20221213 21:19:30 @agent_ppo2.py:185][0m |           0.0068 |         147.9482 |          15.6321 |
[32m[20221213 21:19:31 @agent_ppo2.py:185][0m |          -0.0057 |         132.0017 |          15.6365 |
[32m[20221213 21:19:31 @agent_ppo2.py:185][0m |          -0.0066 |         132.3919 |          15.6289 |
[32m[20221213 21:19:31 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:19:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 689.80
[32m[20221213 21:19:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 710.00
[32m[20221213 21:19:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 694.00
[32m[20221213 21:19:31 @agent_ppo2.py:143][0m Total time:      23.93 min
[32m[20221213 21:19:31 @agent_ppo2.py:145][0m 2347008 total steps have happened
[32m[20221213 21:19:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1146 --------------------------#
[32m[20221213 21:19:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:31 @agent_ppo2.py:185][0m |           0.0062 |         141.9897 |          15.6112 |
[32m[20221213 21:19:31 @agent_ppo2.py:185][0m |          -0.0031 |         135.5628 |          15.6215 |
[32m[20221213 21:19:31 @agent_ppo2.py:185][0m |          -0.0043 |         134.9150 |          15.6282 |
[32m[20221213 21:19:31 @agent_ppo2.py:185][0m |           0.0015 |         137.8626 |          15.6255 |
[32m[20221213 21:19:31 @agent_ppo2.py:185][0m |          -0.0031 |         134.2322 |          15.6197 |
[32m[20221213 21:19:32 @agent_ppo2.py:185][0m |          -0.0070 |         133.6518 |          15.6364 |
[32m[20221213 21:19:32 @agent_ppo2.py:185][0m |          -0.0099 |         133.3578 |          15.6527 |
[32m[20221213 21:19:32 @agent_ppo2.py:185][0m |          -0.0078 |         133.0655 |          15.6524 |
[32m[20221213 21:19:32 @agent_ppo2.py:185][0m |          -0.0072 |         132.7407 |          15.6737 |
[32m[20221213 21:19:32 @agent_ppo2.py:185][0m |          -0.0069 |         132.4917 |          15.6803 |
[32m[20221213 21:19:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 698.00
[32m[20221213 21:19:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 729.00
[32m[20221213 21:19:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 759.00
[32m[20221213 21:19:32 @agent_ppo2.py:143][0m Total time:      23.95 min
[32m[20221213 21:19:32 @agent_ppo2.py:145][0m 2349056 total steps have happened
[32m[20221213 21:19:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1147 --------------------------#
[32m[20221213 21:19:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:32 @agent_ppo2.py:185][0m |          -0.0003 |         137.7007 |          15.5878 |
[32m[20221213 21:19:32 @agent_ppo2.py:185][0m |          -0.0031 |         136.5132 |          15.5480 |
[32m[20221213 21:19:32 @agent_ppo2.py:185][0m |          -0.0009 |         136.2542 |          15.5180 |
[32m[20221213 21:19:33 @agent_ppo2.py:185][0m |          -0.0049 |         135.7508 |          15.5055 |
[32m[20221213 21:19:33 @agent_ppo2.py:185][0m |          -0.0051 |         135.6640 |          15.4807 |
[32m[20221213 21:19:33 @agent_ppo2.py:185][0m |          -0.0058 |         135.3067 |          15.4578 |
[32m[20221213 21:19:33 @agent_ppo2.py:185][0m |          -0.0062 |         135.3034 |          15.4496 |
[32m[20221213 21:19:33 @agent_ppo2.py:185][0m |          -0.0067 |         135.1669 |          15.4416 |
[32m[20221213 21:19:33 @agent_ppo2.py:185][0m |          -0.0090 |         134.9895 |          15.4547 |
[32m[20221213 21:19:33 @agent_ppo2.py:185][0m |          -0.0083 |         134.6807 |          15.4171 |
[32m[20221213 21:19:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 669.80
[32m[20221213 21:19:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 704.00
[32m[20221213 21:19:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 763.00
[32m[20221213 21:19:33 @agent_ppo2.py:143][0m Total time:      23.97 min
[32m[20221213 21:19:33 @agent_ppo2.py:145][0m 2351104 total steps have happened
[32m[20221213 21:19:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1148 --------------------------#
[32m[20221213 21:19:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:33 @agent_ppo2.py:185][0m |          -0.0026 |         135.9416 |          15.5594 |
[32m[20221213 21:19:34 @agent_ppo2.py:185][0m |          -0.0007 |         134.7697 |          15.5746 |
[32m[20221213 21:19:34 @agent_ppo2.py:185][0m |          -0.0046 |         133.7191 |          15.5877 |
[32m[20221213 21:19:34 @agent_ppo2.py:185][0m |          -0.0063 |         132.8703 |          15.5889 |
[32m[20221213 21:19:34 @agent_ppo2.py:185][0m |          -0.0057 |         132.6061 |          15.5699 |
[32m[20221213 21:19:34 @agent_ppo2.py:185][0m |          -0.0084 |         132.2155 |          15.5763 |
[32m[20221213 21:19:34 @agent_ppo2.py:185][0m |          -0.0084 |         131.9037 |          15.5704 |
[32m[20221213 21:19:34 @agent_ppo2.py:185][0m |          -0.0020 |         140.9939 |          15.5626 |
[32m[20221213 21:19:34 @agent_ppo2.py:185][0m |          -0.0100 |         131.8313 |          15.5784 |
[32m[20221213 21:19:34 @agent_ppo2.py:185][0m |          -0.0074 |         131.2756 |          15.5663 |
[32m[20221213 21:19:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 645.60
[32m[20221213 21:19:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 710.00
[32m[20221213 21:19:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 713.00
[32m[20221213 21:19:34 @agent_ppo2.py:143][0m Total time:      23.99 min
[32m[20221213 21:19:34 @agent_ppo2.py:145][0m 2353152 total steps have happened
[32m[20221213 21:19:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1149 --------------------------#
[32m[20221213 21:19:34 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:19:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:35 @agent_ppo2.py:185][0m |           0.0013 |         134.3054 |          15.4594 |
[32m[20221213 21:19:35 @agent_ppo2.py:185][0m |          -0.0007 |         131.1831 |          15.4523 |
[32m[20221213 21:19:35 @agent_ppo2.py:185][0m |          -0.0039 |         129.0613 |          15.4290 |
[32m[20221213 21:19:35 @agent_ppo2.py:185][0m |          -0.0045 |         128.2849 |          15.4210 |
[32m[20221213 21:19:35 @agent_ppo2.py:185][0m |          -0.0025 |         126.5186 |          15.4206 |
[32m[20221213 21:19:35 @agent_ppo2.py:185][0m |          -0.0053 |         124.9405 |          15.4023 |
[32m[20221213 21:19:35 @agent_ppo2.py:185][0m |          -0.0057 |         123.6115 |          15.3888 |
[32m[20221213 21:19:35 @agent_ppo2.py:185][0m |          -0.0063 |         122.9605 |          15.3873 |
[32m[20221213 21:19:35 @agent_ppo2.py:185][0m |          -0.0086 |         122.5088 |          15.3861 |
[32m[20221213 21:19:35 @agent_ppo2.py:185][0m |          -0.0109 |         122.7818 |          15.3675 |
[32m[20221213 21:19:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 675.60
[32m[20221213 21:19:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 720.00
[32m[20221213 21:19:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.00
[32m[20221213 21:19:36 @agent_ppo2.py:143][0m Total time:      24.01 min
[32m[20221213 21:19:36 @agent_ppo2.py:145][0m 2355200 total steps have happened
[32m[20221213 21:19:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1150 --------------------------#
[32m[20221213 21:19:36 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:19:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:36 @agent_ppo2.py:185][0m |          -0.0041 |         143.7754 |          15.2811 |
[32m[20221213 21:19:36 @agent_ppo2.py:185][0m |          -0.0064 |         140.3292 |          15.2696 |
[32m[20221213 21:19:36 @agent_ppo2.py:185][0m |          -0.0036 |         139.1346 |          15.2616 |
[32m[20221213 21:19:36 @agent_ppo2.py:185][0m |          -0.0064 |         138.2150 |          15.2778 |
[32m[20221213 21:19:36 @agent_ppo2.py:185][0m |          -0.0050 |         137.5782 |          15.2608 |
[32m[20221213 21:19:36 @agent_ppo2.py:185][0m |          -0.0043 |         137.9135 |          15.2655 |
[32m[20221213 21:19:36 @agent_ppo2.py:185][0m |          -0.0054 |         136.7506 |          15.2629 |
[32m[20221213 21:19:36 @agent_ppo2.py:185][0m |          -0.0054 |         136.4275 |          15.2655 |
[32m[20221213 21:19:37 @agent_ppo2.py:185][0m |           0.0029 |         147.5369 |          15.2817 |
[32m[20221213 21:19:37 @agent_ppo2.py:185][0m |          -0.0068 |         136.0356 |          15.2801 |
[32m[20221213 21:19:37 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:19:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 705.60
[32m[20221213 21:19:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.00
[32m[20221213 21:19:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 719.00
[32m[20221213 21:19:37 @agent_ppo2.py:143][0m Total time:      24.04 min
[32m[20221213 21:19:37 @agent_ppo2.py:145][0m 2357248 total steps have happened
[32m[20221213 21:19:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1151 --------------------------#
[32m[20221213 21:19:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:37 @agent_ppo2.py:185][0m |          -0.0030 |         135.7213 |          15.4929 |
[32m[20221213 21:19:37 @agent_ppo2.py:185][0m |          -0.0027 |         135.9817 |          15.5010 |
[32m[20221213 21:19:37 @agent_ppo2.py:185][0m |          -0.0056 |         134.0594 |          15.5000 |
[32m[20221213 21:19:37 @agent_ppo2.py:185][0m |          -0.0063 |         133.5044 |          15.5156 |
[32m[20221213 21:19:37 @agent_ppo2.py:185][0m |           0.0015 |         141.0021 |          15.5101 |
[32m[20221213 21:19:38 @agent_ppo2.py:185][0m |          -0.0048 |         133.4087 |          15.5252 |
[32m[20221213 21:19:38 @agent_ppo2.py:185][0m |          -0.0077 |         132.4928 |          15.5209 |
[32m[20221213 21:19:38 @agent_ppo2.py:185][0m |          -0.0100 |         132.2587 |          15.5215 |
[32m[20221213 21:19:38 @agent_ppo2.py:185][0m |          -0.0091 |         132.0380 |          15.5188 |
[32m[20221213 21:19:38 @agent_ppo2.py:185][0m |          -0.0107 |         131.8042 |          15.5303 |
[32m[20221213 21:19:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 698.20
[32m[20221213 21:19:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 708.00
[32m[20221213 21:19:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.00
[32m[20221213 21:19:38 @agent_ppo2.py:143][0m Total time:      24.05 min
[32m[20221213 21:19:38 @agent_ppo2.py:145][0m 2359296 total steps have happened
[32m[20221213 21:19:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1152 --------------------------#
[32m[20221213 21:19:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:38 @agent_ppo2.py:185][0m |          -0.0041 |         134.4619 |          15.4443 |
[32m[20221213 21:19:38 @agent_ppo2.py:185][0m |          -0.0082 |         133.7883 |          15.4566 |
[32m[20221213 21:19:38 @agent_ppo2.py:185][0m |          -0.0080 |         133.2358 |          15.4480 |
[32m[20221213 21:19:39 @agent_ppo2.py:185][0m |          -0.0107 |         132.8240 |          15.4532 |
[32m[20221213 21:19:39 @agent_ppo2.py:185][0m |          -0.0112 |         132.6445 |          15.4475 |
[32m[20221213 21:19:39 @agent_ppo2.py:185][0m |          -0.0111 |         132.5383 |          15.4561 |
[32m[20221213 21:19:39 @agent_ppo2.py:185][0m |          -0.0093 |         132.0435 |          15.4689 |
[32m[20221213 21:19:39 @agent_ppo2.py:185][0m |          -0.0115 |         131.9997 |          15.4697 |
[32m[20221213 21:19:39 @agent_ppo2.py:185][0m |          -0.0107 |         131.9213 |          15.4856 |
[32m[20221213 21:19:39 @agent_ppo2.py:185][0m |          -0.0106 |         131.7796 |          15.4777 |
[32m[20221213 21:19:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:19:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 693.40
[32m[20221213 21:19:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 714.00
[32m[20221213 21:19:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 773.00
[32m[20221213 21:19:39 @agent_ppo2.py:143][0m Total time:      24.07 min
[32m[20221213 21:19:39 @agent_ppo2.py:145][0m 2361344 total steps have happened
[32m[20221213 21:19:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1153 --------------------------#
[32m[20221213 21:19:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:39 @agent_ppo2.py:185][0m |           0.0024 |         133.0164 |          15.6690 |
[32m[20221213 21:19:40 @agent_ppo2.py:185][0m |          -0.0001 |         131.5216 |          15.6394 |
[32m[20221213 21:19:40 @agent_ppo2.py:185][0m |          -0.0047 |         130.7650 |          15.6502 |
[32m[20221213 21:19:40 @agent_ppo2.py:185][0m |          -0.0052 |         130.3559 |          15.6578 |
[32m[20221213 21:19:40 @agent_ppo2.py:185][0m |          -0.0069 |         130.0711 |          15.6467 |
[32m[20221213 21:19:40 @agent_ppo2.py:185][0m |          -0.0074 |         129.7101 |          15.6463 |
[32m[20221213 21:19:40 @agent_ppo2.py:185][0m |          -0.0056 |         129.4850 |          15.6483 |
[32m[20221213 21:19:40 @agent_ppo2.py:185][0m |          -0.0074 |         129.4878 |          15.6576 |
[32m[20221213 21:19:40 @agent_ppo2.py:185][0m |           0.0033 |         139.2844 |          15.6520 |
[32m[20221213 21:19:40 @agent_ppo2.py:185][0m |          -0.0079 |         128.9155 |          15.6656 |
[32m[20221213 21:19:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:19:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 688.40
[32m[20221213 21:19:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 716.00
[32m[20221213 21:19:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 801.00
[32m[20221213 21:19:40 @agent_ppo2.py:143][0m Total time:      24.10 min
[32m[20221213 21:19:40 @agent_ppo2.py:145][0m 2363392 total steps have happened
[32m[20221213 21:19:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1154 --------------------------#
[32m[20221213 21:19:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:19:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:41 @agent_ppo2.py:185][0m |          -0.0041 |         136.6640 |          15.5763 |
[32m[20221213 21:19:41 @agent_ppo2.py:185][0m |          -0.0066 |         135.2785 |          15.5805 |
[32m[20221213 21:19:41 @agent_ppo2.py:185][0m |          -0.0086 |         134.7405 |          15.6198 |
[32m[20221213 21:19:41 @agent_ppo2.py:185][0m |          -0.0039 |         133.9853 |          15.6485 |
[32m[20221213 21:19:41 @agent_ppo2.py:185][0m |           0.0007 |         135.1143 |          15.6680 |
[32m[20221213 21:19:41 @agent_ppo2.py:185][0m |           0.0044 |         143.8942 |          15.6738 |
[32m[20221213 21:19:41 @agent_ppo2.py:185][0m |          -0.0072 |         133.1775 |          15.7093 |
[32m[20221213 21:19:41 @agent_ppo2.py:185][0m |          -0.0064 |         132.8164 |          15.7155 |
[32m[20221213 21:19:41 @agent_ppo2.py:185][0m |           0.0014 |         139.4958 |          15.7411 |
[32m[20221213 21:19:42 @agent_ppo2.py:185][0m |          -0.0087 |         132.3836 |          15.7510 |
[32m[20221213 21:19:42 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:19:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 665.60
[32m[20221213 21:19:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.00
[32m[20221213 21:19:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 788.00
[32m[20221213 21:19:42 @agent_ppo2.py:143][0m Total time:      24.12 min
[32m[20221213 21:19:42 @agent_ppo2.py:145][0m 2365440 total steps have happened
[32m[20221213 21:19:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1155 --------------------------#
[32m[20221213 21:19:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:42 @agent_ppo2.py:185][0m |           0.0002 |         134.8376 |          15.5735 |
[32m[20221213 21:19:42 @agent_ppo2.py:185][0m |          -0.0037 |         133.3742 |          15.5529 |
[32m[20221213 21:19:42 @agent_ppo2.py:185][0m |           0.0008 |         135.5402 |          15.5629 |
[32m[20221213 21:19:42 @agent_ppo2.py:185][0m |          -0.0050 |         131.6841 |          15.5598 |
[32m[20221213 21:19:42 @agent_ppo2.py:185][0m |          -0.0070 |         131.1348 |          15.5445 |
[32m[20221213 21:19:42 @agent_ppo2.py:185][0m |           0.0019 |         142.4894 |          15.5325 |
[32m[20221213 21:19:43 @agent_ppo2.py:185][0m |          -0.0056 |         130.2464 |          15.5358 |
[32m[20221213 21:19:43 @agent_ppo2.py:185][0m |          -0.0066 |         129.8946 |          15.5283 |
[32m[20221213 21:19:43 @agent_ppo2.py:185][0m |          -0.0059 |         129.9113 |          15.5289 |
[32m[20221213 21:19:43 @agent_ppo2.py:185][0m |          -0.0065 |         130.0789 |          15.5103 |
[32m[20221213 21:19:43 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:19:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 715.60
[32m[20221213 21:19:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 759.00
[32m[20221213 21:19:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 713.00
[32m[20221213 21:19:43 @agent_ppo2.py:143][0m Total time:      24.14 min
[32m[20221213 21:19:43 @agent_ppo2.py:145][0m 2367488 total steps have happened
[32m[20221213 21:19:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1156 --------------------------#
[32m[20221213 21:19:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:43 @agent_ppo2.py:185][0m |          -0.0021 |         138.9531 |          15.7073 |
[32m[20221213 21:19:43 @agent_ppo2.py:185][0m |          -0.0057 |         136.0364 |          15.6936 |
[32m[20221213 21:19:43 @agent_ppo2.py:185][0m |          -0.0067 |         133.9957 |          15.6974 |
[32m[20221213 21:19:44 @agent_ppo2.py:185][0m |          -0.0048 |         132.1957 |          15.6794 |
[32m[20221213 21:19:44 @agent_ppo2.py:185][0m |          -0.0089 |         130.7951 |          15.6802 |
[32m[20221213 21:19:44 @agent_ppo2.py:185][0m |          -0.0095 |         129.6154 |          15.6757 |
[32m[20221213 21:19:44 @agent_ppo2.py:185][0m |          -0.0079 |         128.7591 |          15.6770 |
[32m[20221213 21:19:44 @agent_ppo2.py:185][0m |          -0.0114 |         127.5525 |          15.6599 |
[32m[20221213 21:19:44 @agent_ppo2.py:185][0m |          -0.0126 |         127.1365 |          15.6627 |
[32m[20221213 21:19:44 @agent_ppo2.py:185][0m |          -0.0135 |         126.9337 |          15.6653 |
[32m[20221213 21:19:44 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:19:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 683.40
[32m[20221213 21:19:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 710.00
[32m[20221213 21:19:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 709.00
[32m[20221213 21:19:44 @agent_ppo2.py:143][0m Total time:      24.16 min
[32m[20221213 21:19:44 @agent_ppo2.py:145][0m 2369536 total steps have happened
[32m[20221213 21:19:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1157 --------------------------#
[32m[20221213 21:19:45 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 21:19:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:45 @agent_ppo2.py:185][0m |           0.0013 |         146.6482 |          15.5503 |
[32m[20221213 21:19:45 @agent_ppo2.py:185][0m |          -0.0086 |         143.3898 |          15.5375 |
[32m[20221213 21:19:45 @agent_ppo2.py:185][0m |          -0.0019 |         144.7484 |          15.5429 |
[32m[20221213 21:19:45 @agent_ppo2.py:185][0m |          -0.0061 |         140.8893 |          15.5162 |
[32m[20221213 21:19:45 @agent_ppo2.py:185][0m |          -0.0077 |         140.2306 |          15.5286 |
[32m[20221213 21:19:45 @agent_ppo2.py:185][0m |          -0.0068 |         139.7736 |          15.5179 |
[32m[20221213 21:19:45 @agent_ppo2.py:185][0m |          -0.0065 |         139.6115 |          15.5076 |
[32m[20221213 21:19:45 @agent_ppo2.py:185][0m |          -0.0077 |         139.1252 |          15.5088 |
[32m[20221213 21:19:46 @agent_ppo2.py:185][0m |          -0.0074 |         138.4068 |          15.4884 |
[32m[20221213 21:19:46 @agent_ppo2.py:185][0m |          -0.0115 |         138.3265 |          15.4912 |
[32m[20221213 21:19:46 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:19:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 655.20
[32m[20221213 21:19:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 689.00
[32m[20221213 21:19:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.00
[32m[20221213 21:19:46 @agent_ppo2.py:143][0m Total time:      24.18 min
[32m[20221213 21:19:46 @agent_ppo2.py:145][0m 2371584 total steps have happened
[32m[20221213 21:19:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1158 --------------------------#
[32m[20221213 21:19:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:46 @agent_ppo2.py:185][0m |           0.0003 |         137.4342 |          15.5824 |
[32m[20221213 21:19:46 @agent_ppo2.py:185][0m |          -0.0016 |         135.4451 |          15.6027 |
[32m[20221213 21:19:46 @agent_ppo2.py:185][0m |          -0.0028 |         136.8790 |          15.5901 |
[32m[20221213 21:19:46 @agent_ppo2.py:185][0m |          -0.0079 |         133.6863 |          15.5651 |
[32m[20221213 21:19:46 @agent_ppo2.py:185][0m |          -0.0061 |         133.2723 |          15.5564 |
[32m[20221213 21:19:47 @agent_ppo2.py:185][0m |           0.0007 |         142.8492 |          15.5259 |
[32m[20221213 21:19:47 @agent_ppo2.py:185][0m |          -0.0065 |         132.5961 |          15.5318 |
[32m[20221213 21:19:47 @agent_ppo2.py:185][0m |          -0.0100 |         132.0408 |          15.5305 |
[32m[20221213 21:19:47 @agent_ppo2.py:185][0m |          -0.0088 |         131.6977 |          15.5227 |
[32m[20221213 21:19:47 @agent_ppo2.py:185][0m |          -0.0061 |         131.9831 |          15.5150 |
[32m[20221213 21:19:47 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:19:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 672.40
[32m[20221213 21:19:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 718.00
[32m[20221213 21:19:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 778.00
[32m[20221213 21:19:47 @agent_ppo2.py:143][0m Total time:      24.21 min
[32m[20221213 21:19:47 @agent_ppo2.py:145][0m 2373632 total steps have happened
[32m[20221213 21:19:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1159 --------------------------#
[32m[20221213 21:19:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:47 @agent_ppo2.py:185][0m |          -0.0001 |         138.9182 |          15.3056 |
[32m[20221213 21:19:47 @agent_ppo2.py:185][0m |          -0.0033 |         137.3940 |          15.3015 |
[32m[20221213 21:19:48 @agent_ppo2.py:185][0m |          -0.0039 |         136.6154 |          15.3017 |
[32m[20221213 21:19:48 @agent_ppo2.py:185][0m |          -0.0061 |         135.6597 |          15.2628 |
[32m[20221213 21:19:48 @agent_ppo2.py:185][0m |          -0.0055 |         135.2562 |          15.2669 |
[32m[20221213 21:19:48 @agent_ppo2.py:185][0m |          -0.0076 |         134.9462 |          15.2414 |
[32m[20221213 21:19:48 @agent_ppo2.py:185][0m |          -0.0097 |         134.2875 |          15.2486 |
[32m[20221213 21:19:48 @agent_ppo2.py:185][0m |          -0.0097 |         133.8270 |          15.2212 |
[32m[20221213 21:19:48 @agent_ppo2.py:185][0m |          -0.0107 |         133.5915 |          15.2236 |
[32m[20221213 21:19:48 @agent_ppo2.py:185][0m |          -0.0102 |         133.3387 |          15.2270 |
[32m[20221213 21:19:48 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 21:19:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 677.00
[32m[20221213 21:19:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.00
[32m[20221213 21:19:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.00
[32m[20221213 21:19:48 @agent_ppo2.py:143][0m Total time:      24.23 min
[32m[20221213 21:19:48 @agent_ppo2.py:145][0m 2375680 total steps have happened
[32m[20221213 21:19:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1160 --------------------------#
[32m[20221213 21:19:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:49 @agent_ppo2.py:185][0m |           0.0077 |         142.6564 |          15.2090 |
[32m[20221213 21:19:49 @agent_ppo2.py:185][0m |          -0.0010 |         135.8506 |          15.1487 |
[32m[20221213 21:19:49 @agent_ppo2.py:185][0m |          -0.0017 |         135.4560 |          15.1535 |
[32m[20221213 21:19:49 @agent_ppo2.py:185][0m |          -0.0070 |         133.9842 |          15.1574 |
[32m[20221213 21:19:49 @agent_ppo2.py:185][0m |          -0.0086 |         133.7876 |          15.1036 |
[32m[20221213 21:19:49 @agent_ppo2.py:185][0m |          -0.0090 |         133.2707 |          15.0824 |
[32m[20221213 21:19:49 @agent_ppo2.py:185][0m |          -0.0054 |         133.7821 |          15.1022 |
[32m[20221213 21:19:49 @agent_ppo2.py:185][0m |          -0.0081 |         132.4901 |          15.0732 |
[32m[20221213 21:19:49 @agent_ppo2.py:185][0m |          -0.0088 |         132.6936 |          15.0663 |
[32m[20221213 21:19:50 @agent_ppo2.py:185][0m |          -0.0099 |         131.8143 |          15.0279 |
[32m[20221213 21:19:50 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:19:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.60
[32m[20221213 21:19:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 761.00
[32m[20221213 21:19:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 786.00
[32m[20221213 21:19:50 @agent_ppo2.py:143][0m Total time:      24.25 min
[32m[20221213 21:19:50 @agent_ppo2.py:145][0m 2377728 total steps have happened
[32m[20221213 21:19:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1161 --------------------------#
[32m[20221213 21:19:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:50 @agent_ppo2.py:185][0m |           0.0017 |         140.5165 |          15.1059 |
[32m[20221213 21:19:50 @agent_ppo2.py:185][0m |          -0.0034 |         137.9142 |          15.1109 |
[32m[20221213 21:19:50 @agent_ppo2.py:185][0m |          -0.0072 |         135.7488 |          15.0676 |
[32m[20221213 21:19:50 @agent_ppo2.py:185][0m |          -0.0066 |         134.8237 |          15.0787 |
[32m[20221213 21:19:50 @agent_ppo2.py:185][0m |          -0.0058 |         133.7137 |          15.0744 |
[32m[20221213 21:19:50 @agent_ppo2.py:185][0m |          -0.0083 |         133.0773 |          15.0714 |
[32m[20221213 21:19:51 @agent_ppo2.py:185][0m |          -0.0085 |         132.5696 |          15.0795 |
[32m[20221213 21:19:51 @agent_ppo2.py:185][0m |           0.0053 |         139.8351 |          15.0570 |
[32m[20221213 21:19:51 @agent_ppo2.py:185][0m |          -0.0058 |         131.8551 |          15.0461 |
[32m[20221213 21:19:51 @agent_ppo2.py:185][0m |          -0.0108 |         131.1015 |          15.0388 |
[32m[20221213 21:19:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 699.40
[32m[20221213 21:19:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 730.00
[32m[20221213 21:19:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 777.00
[32m[20221213 21:19:51 @agent_ppo2.py:143][0m Total time:      24.27 min
[32m[20221213 21:19:51 @agent_ppo2.py:145][0m 2379776 total steps have happened
[32m[20221213 21:19:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1162 --------------------------#
[32m[20221213 21:19:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:51 @agent_ppo2.py:185][0m |          -0.0027 |         140.5412 |          15.0402 |
[32m[20221213 21:19:51 @agent_ppo2.py:185][0m |           0.0056 |         151.7899 |          15.0703 |
[32m[20221213 21:19:51 @agent_ppo2.py:185][0m |          -0.0048 |         138.1697 |          15.0663 |
[32m[20221213 21:19:51 @agent_ppo2.py:185][0m |           0.0085 |         154.0432 |          15.0951 |
[32m[20221213 21:19:52 @agent_ppo2.py:185][0m |          -0.0105 |         137.1510 |          15.0934 |
[32m[20221213 21:19:52 @agent_ppo2.py:185][0m |          -0.0047 |         136.6922 |          15.1057 |
[32m[20221213 21:19:52 @agent_ppo2.py:185][0m |          -0.0024 |         144.0308 |          15.1177 |
[32m[20221213 21:19:52 @agent_ppo2.py:185][0m |          -0.0042 |         137.4008 |          15.1195 |
[32m[20221213 21:19:52 @agent_ppo2.py:185][0m |          -0.0099 |         135.4211 |          15.1453 |
[32m[20221213 21:19:52 @agent_ppo2.py:185][0m |          -0.0129 |         135.2855 |          15.1616 |
[32m[20221213 21:19:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:19:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 644.00
[32m[20221213 21:19:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.00
[32m[20221213 21:19:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.00
[32m[20221213 21:19:52 @agent_ppo2.py:143][0m Total time:      24.29 min
[32m[20221213 21:19:52 @agent_ppo2.py:145][0m 2381824 total steps have happened
[32m[20221213 21:19:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1163 --------------------------#
[32m[20221213 21:19:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:52 @agent_ppo2.py:185][0m |          -0.0008 |         140.1230 |          14.8815 |
[32m[20221213 21:19:52 @agent_ppo2.py:185][0m |           0.0032 |         143.0211 |          14.8657 |
[32m[20221213 21:19:53 @agent_ppo2.py:185][0m |          -0.0041 |         137.2397 |          14.8724 |
[32m[20221213 21:19:53 @agent_ppo2.py:185][0m |          -0.0044 |         136.5122 |          14.8489 |
[32m[20221213 21:19:53 @agent_ppo2.py:185][0m |          -0.0065 |         136.0045 |          14.8378 |
[32m[20221213 21:19:53 @agent_ppo2.py:185][0m |           0.0030 |         146.0847 |          14.8475 |
[32m[20221213 21:19:53 @agent_ppo2.py:185][0m |          -0.0071 |         135.4092 |          14.8211 |
[32m[20221213 21:19:53 @agent_ppo2.py:185][0m |          -0.0068 |         135.5026 |          14.8145 |
[32m[20221213 21:19:53 @agent_ppo2.py:185][0m |          -0.0010 |         140.1672 |          14.8198 |
[32m[20221213 21:19:53 @agent_ppo2.py:185][0m |           0.0051 |         144.4327 |          14.8195 |
[32m[20221213 21:19:53 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:19:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 659.20
[32m[20221213 21:19:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.00
[32m[20221213 21:19:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 772.00
[32m[20221213 21:19:53 @agent_ppo2.py:143][0m Total time:      24.31 min
[32m[20221213 21:19:53 @agent_ppo2.py:145][0m 2383872 total steps have happened
[32m[20221213 21:19:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1164 --------------------------#
[32m[20221213 21:19:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:54 @agent_ppo2.py:185][0m |          -0.0009 |         133.7198 |          15.0852 |
[32m[20221213 21:19:54 @agent_ppo2.py:185][0m |          -0.0069 |         132.6898 |          15.1138 |
[32m[20221213 21:19:54 @agent_ppo2.py:185][0m |          -0.0037 |         132.0741 |          15.0926 |
[32m[20221213 21:19:54 @agent_ppo2.py:185][0m |           0.0025 |         135.9910 |          15.0832 |
[32m[20221213 21:19:54 @agent_ppo2.py:185][0m |          -0.0064 |         131.3960 |          15.1108 |
[32m[20221213 21:19:54 @agent_ppo2.py:185][0m |           0.0053 |         139.2802 |          15.0918 |
[32m[20221213 21:19:54 @agent_ppo2.py:185][0m |          -0.0076 |         130.7757 |          15.1093 |
[32m[20221213 21:19:54 @agent_ppo2.py:185][0m |          -0.0069 |         130.5958 |          15.0912 |
[32m[20221213 21:19:54 @agent_ppo2.py:185][0m |          -0.0098 |         130.3912 |          15.0700 |
[32m[20221213 21:19:55 @agent_ppo2.py:185][0m |          -0.0040 |         131.3918 |          15.0798 |
[32m[20221213 21:19:55 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:19:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 716.40
[32m[20221213 21:19:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 746.00
[32m[20221213 21:19:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 774.00
[32m[20221213 21:19:55 @agent_ppo2.py:143][0m Total time:      24.33 min
[32m[20221213 21:19:55 @agent_ppo2.py:145][0m 2385920 total steps have happened
[32m[20221213 21:19:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1165 --------------------------#
[32m[20221213 21:19:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:55 @agent_ppo2.py:185][0m |          -0.0030 |         138.7302 |          15.0336 |
[32m[20221213 21:19:55 @agent_ppo2.py:185][0m |          -0.0016 |         138.3419 |          15.0076 |
[32m[20221213 21:19:55 @agent_ppo2.py:185][0m |          -0.0065 |         136.8206 |          14.9832 |
[32m[20221213 21:19:55 @agent_ppo2.py:185][0m |          -0.0093 |         136.4703 |          14.9803 |
[32m[20221213 21:19:55 @agent_ppo2.py:185][0m |          -0.0078 |         135.8618 |          14.9728 |
[32m[20221213 21:19:55 @agent_ppo2.py:185][0m |          -0.0105 |         135.4241 |          14.9766 |
[32m[20221213 21:19:56 @agent_ppo2.py:185][0m |          -0.0091 |         134.9527 |          14.9629 |
[32m[20221213 21:19:56 @agent_ppo2.py:185][0m |          -0.0109 |         134.4807 |          14.9419 |
[32m[20221213 21:19:56 @agent_ppo2.py:185][0m |          -0.0077 |         134.3770 |          14.9339 |
[32m[20221213 21:19:56 @agent_ppo2.py:185][0m |          -0.0106 |         133.9532 |          14.9189 |
[32m[20221213 21:19:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:19:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 672.80
[32m[20221213 21:19:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.00
[32m[20221213 21:19:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.00
[32m[20221213 21:19:56 @agent_ppo2.py:143][0m Total time:      24.35 min
[32m[20221213 21:19:56 @agent_ppo2.py:145][0m 2387968 total steps have happened
[32m[20221213 21:19:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1166 --------------------------#
[32m[20221213 21:19:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:19:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:56 @agent_ppo2.py:185][0m |           0.0030 |         136.9094 |          14.8664 |
[32m[20221213 21:19:56 @agent_ppo2.py:185][0m |          -0.0025 |         135.5279 |          14.8720 |
[32m[20221213 21:19:56 @agent_ppo2.py:185][0m |           0.0003 |         137.3070 |          14.9082 |
[32m[20221213 21:19:57 @agent_ppo2.py:185][0m |          -0.0054 |         134.2714 |          14.9070 |
[32m[20221213 21:19:57 @agent_ppo2.py:185][0m |          -0.0087 |         133.5512 |          14.9069 |
[32m[20221213 21:19:57 @agent_ppo2.py:185][0m |          -0.0074 |         133.1279 |          14.9335 |
[32m[20221213 21:19:57 @agent_ppo2.py:185][0m |           0.0041 |         148.7934 |          14.9289 |
[32m[20221213 21:19:57 @agent_ppo2.py:185][0m |          -0.0084 |         132.5178 |          14.9461 |
[32m[20221213 21:19:57 @agent_ppo2.py:185][0m |          -0.0080 |         132.3514 |          14.9518 |
[32m[20221213 21:19:57 @agent_ppo2.py:185][0m |          -0.0030 |         134.6546 |          14.9641 |
[32m[20221213 21:19:57 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:19:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 671.80
[32m[20221213 21:19:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 726.00
[32m[20221213 21:19:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 741.00
[32m[20221213 21:19:57 @agent_ppo2.py:143][0m Total time:      24.38 min
[32m[20221213 21:19:57 @agent_ppo2.py:145][0m 2390016 total steps have happened
[32m[20221213 21:19:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1167 --------------------------#
[32m[20221213 21:19:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:19:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:58 @agent_ppo2.py:185][0m |          -0.0010 |         134.3650 |          14.7933 |
[32m[20221213 21:19:58 @agent_ppo2.py:185][0m |          -0.0014 |         133.6377 |          14.7723 |
[32m[20221213 21:19:58 @agent_ppo2.py:185][0m |          -0.0047 |         133.1647 |          14.7588 |
[32m[20221213 21:19:58 @agent_ppo2.py:185][0m |          -0.0045 |         132.7254 |          14.7682 |
[32m[20221213 21:19:58 @agent_ppo2.py:185][0m |          -0.0037 |         132.3574 |          14.7454 |
[32m[20221213 21:19:58 @agent_ppo2.py:185][0m |          -0.0084 |         131.9882 |          14.7347 |
[32m[20221213 21:19:58 @agent_ppo2.py:185][0m |          -0.0072 |         131.7699 |          14.7173 |
[32m[20221213 21:19:58 @agent_ppo2.py:185][0m |          -0.0099 |         131.4982 |          14.7029 |
[32m[20221213 21:19:58 @agent_ppo2.py:185][0m |          -0.0094 |         131.2447 |          14.6914 |
[32m[20221213 21:19:58 @agent_ppo2.py:185][0m |          -0.0111 |         131.2011 |          14.6832 |
[32m[20221213 21:19:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:19:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 724.80
[32m[20221213 21:19:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.00
[32m[20221213 21:19:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.00
[32m[20221213 21:19:58 @agent_ppo2.py:143][0m Total time:      24.40 min
[32m[20221213 21:19:58 @agent_ppo2.py:145][0m 2392064 total steps have happened
[32m[20221213 21:19:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1168 --------------------------#
[32m[20221213 21:19:59 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:19:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:19:59 @agent_ppo2.py:185][0m |           0.0014 |         140.9722 |          14.9202 |
[32m[20221213 21:19:59 @agent_ppo2.py:185][0m |          -0.0031 |         137.2787 |          14.9039 |
[32m[20221213 21:19:59 @agent_ppo2.py:185][0m |          -0.0069 |         136.4928 |          14.8990 |
[32m[20221213 21:19:59 @agent_ppo2.py:185][0m |          -0.0079 |         135.7161 |          14.9265 |
[32m[20221213 21:19:59 @agent_ppo2.py:185][0m |          -0.0033 |         137.3739 |          14.9063 |
[32m[20221213 21:19:59 @agent_ppo2.py:185][0m |          -0.0079 |         135.1578 |          14.9317 |
[32m[20221213 21:19:59 @agent_ppo2.py:185][0m |          -0.0066 |         134.9152 |          14.9326 |
[32m[20221213 21:19:59 @agent_ppo2.py:185][0m |          -0.0078 |         134.5887 |          14.9536 |
[32m[20221213 21:19:59 @agent_ppo2.py:185][0m |          -0.0119 |         134.7505 |          14.9402 |
[32m[20221213 21:20:00 @agent_ppo2.py:185][0m |           0.0006 |         146.6755 |          14.9525 |
[32m[20221213 21:20:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 696.40
[32m[20221213 21:20:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 777.00
[32m[20221213 21:20:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 781.00
[32m[20221213 21:20:00 @agent_ppo2.py:143][0m Total time:      24.42 min
[32m[20221213 21:20:00 @agent_ppo2.py:145][0m 2394112 total steps have happened
[32m[20221213 21:20:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1169 --------------------------#
[32m[20221213 21:20:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:00 @agent_ppo2.py:185][0m |           0.0033 |         139.0706 |          15.0687 |
[32m[20221213 21:20:00 @agent_ppo2.py:185][0m |           0.0005 |         138.3501 |          15.0318 |
[32m[20221213 21:20:00 @agent_ppo2.py:185][0m |          -0.0080 |         136.9963 |          15.0144 |
[32m[20221213 21:20:00 @agent_ppo2.py:185][0m |          -0.0066 |         136.5286 |          14.9941 |
[32m[20221213 21:20:00 @agent_ppo2.py:185][0m |          -0.0084 |         136.2311 |          14.9850 |
[32m[20221213 21:20:00 @agent_ppo2.py:185][0m |          -0.0071 |         135.7358 |          14.9912 |
[32m[20221213 21:20:00 @agent_ppo2.py:185][0m |          -0.0065 |         135.4614 |          14.9639 |
[32m[20221213 21:20:01 @agent_ppo2.py:185][0m |          -0.0069 |         135.5057 |          14.9598 |
[32m[20221213 21:20:01 @agent_ppo2.py:185][0m |          -0.0099 |         135.0054 |          14.9386 |
[32m[20221213 21:20:01 @agent_ppo2.py:185][0m |          -0.0107 |         134.9859 |          14.9521 |
[32m[20221213 21:20:01 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:20:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 726.20
[32m[20221213 21:20:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 21:20:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 638.00
[32m[20221213 21:20:01 @agent_ppo2.py:143][0m Total time:      24.44 min
[32m[20221213 21:20:01 @agent_ppo2.py:145][0m 2396160 total steps have happened
[32m[20221213 21:20:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1170 --------------------------#
[32m[20221213 21:20:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:20:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:01 @agent_ppo2.py:185][0m |          -0.0027 |         141.8446 |          14.8545 |
[32m[20221213 21:20:01 @agent_ppo2.py:185][0m |          -0.0062 |         140.9244 |          14.8513 |
[32m[20221213 21:20:01 @agent_ppo2.py:185][0m |          -0.0064 |         140.0924 |          14.8216 |
[32m[20221213 21:20:01 @agent_ppo2.py:185][0m |          -0.0090 |         139.5357 |          14.8001 |
[32m[20221213 21:20:02 @agent_ppo2.py:185][0m |          -0.0109 |         139.1917 |          14.8195 |
[32m[20221213 21:20:02 @agent_ppo2.py:185][0m |          -0.0107 |         138.9211 |          14.7744 |
[32m[20221213 21:20:02 @agent_ppo2.py:185][0m |          -0.0109 |         138.5702 |          14.7717 |
[32m[20221213 21:20:02 @agent_ppo2.py:185][0m |          -0.0103 |         138.3490 |          14.7549 |
[32m[20221213 21:20:02 @agent_ppo2.py:185][0m |          -0.0117 |         138.3738 |          14.7560 |
[32m[20221213 21:20:02 @agent_ppo2.py:185][0m |          -0.0069 |         139.8086 |          14.7476 |
[32m[20221213 21:20:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 688.40
[32m[20221213 21:20:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 718.00
[32m[20221213 21:20:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.00
[32m[20221213 21:20:02 @agent_ppo2.py:143][0m Total time:      24.46 min
[32m[20221213 21:20:02 @agent_ppo2.py:145][0m 2398208 total steps have happened
[32m[20221213 21:20:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1171 --------------------------#
[32m[20221213 21:20:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:02 @agent_ppo2.py:185][0m |          -0.0029 |         131.9418 |          14.7475 |
[32m[20221213 21:20:02 @agent_ppo2.py:185][0m |          -0.0004 |         128.9420 |          14.7353 |
[32m[20221213 21:20:03 @agent_ppo2.py:185][0m |          -0.0047 |         126.1419 |          14.7529 |
[32m[20221213 21:20:03 @agent_ppo2.py:185][0m |           0.0035 |         134.8846 |          14.7454 |
[32m[20221213 21:20:03 @agent_ppo2.py:185][0m |          -0.0041 |         123.7860 |          14.7193 |
[32m[20221213 21:20:03 @agent_ppo2.py:185][0m |          -0.0090 |         122.9379 |          14.7327 |
[32m[20221213 21:20:03 @agent_ppo2.py:185][0m |          -0.0013 |         122.7345 |          14.7163 |
[32m[20221213 21:20:03 @agent_ppo2.py:185][0m |          -0.0060 |         121.2526 |          14.7253 |
[32m[20221213 21:20:03 @agent_ppo2.py:185][0m |          -0.0087 |         120.4425 |          14.7146 |
[32m[20221213 21:20:03 @agent_ppo2.py:185][0m |          -0.0021 |         122.0679 |          14.6921 |
[32m[20221213 21:20:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 710.40
[32m[20221213 21:20:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.00
[32m[20221213 21:20:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 772.00
[32m[20221213 21:20:03 @agent_ppo2.py:143][0m Total time:      24.48 min
[32m[20221213 21:20:03 @agent_ppo2.py:145][0m 2400256 total steps have happened
[32m[20221213 21:20:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1172 --------------------------#
[32m[20221213 21:20:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:04 @agent_ppo2.py:185][0m |           0.0015 |         145.6205 |          14.5988 |
[32m[20221213 21:20:04 @agent_ppo2.py:185][0m |          -0.0026 |         143.0029 |          14.6078 |
[32m[20221213 21:20:04 @agent_ppo2.py:185][0m |           0.0017 |         144.3925 |          14.5826 |
[32m[20221213 21:20:04 @agent_ppo2.py:185][0m |          -0.0004 |         143.0277 |          14.5865 |
[32m[20221213 21:20:04 @agent_ppo2.py:185][0m |          -0.0051 |         140.3379 |          14.5879 |
[32m[20221213 21:20:04 @agent_ppo2.py:185][0m |           0.0004 |         145.9258 |          14.5902 |
[32m[20221213 21:20:04 @agent_ppo2.py:185][0m |          -0.0074 |         139.6622 |          14.5778 |
[32m[20221213 21:20:04 @agent_ppo2.py:185][0m |          -0.0093 |         139.1272 |          14.5783 |
[32m[20221213 21:20:04 @agent_ppo2.py:185][0m |          -0.0043 |         139.1294 |          14.5874 |
[32m[20221213 21:20:04 @agent_ppo2.py:185][0m |          -0.0082 |         138.4467 |          14.5525 |
[32m[20221213 21:20:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.60
[32m[20221213 21:20:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 748.00
[32m[20221213 21:20:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.00
[32m[20221213 21:20:04 @agent_ppo2.py:143][0m Total time:      24.50 min
[32m[20221213 21:20:04 @agent_ppo2.py:145][0m 2402304 total steps have happened
[32m[20221213 21:20:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1173 --------------------------#
[32m[20221213 21:20:05 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:20:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:05 @agent_ppo2.py:185][0m |           0.0025 |         147.6691 |          14.5976 |
[32m[20221213 21:20:05 @agent_ppo2.py:185][0m |          -0.0016 |         145.0904 |          14.5708 |
[32m[20221213 21:20:05 @agent_ppo2.py:185][0m |           0.0054 |         157.4423 |          14.5343 |
[32m[20221213 21:20:05 @agent_ppo2.py:185][0m |          -0.0045 |         143.5655 |          14.5245 |
[32m[20221213 21:20:05 @agent_ppo2.py:185][0m |          -0.0060 |         142.5841 |          14.5085 |
[32m[20221213 21:20:05 @agent_ppo2.py:185][0m |          -0.0038 |         142.5932 |          14.4939 |
[32m[20221213 21:20:05 @agent_ppo2.py:185][0m |          -0.0078 |         141.9098 |          14.4566 |
[32m[20221213 21:20:05 @agent_ppo2.py:185][0m |          -0.0094 |         141.4992 |          14.4444 |
[32m[20221213 21:20:05 @agent_ppo2.py:185][0m |          -0.0077 |         141.3966 |          14.4164 |
[32m[20221213 21:20:06 @agent_ppo2.py:185][0m |          -0.0057 |         140.9865 |          14.4158 |
[32m[20221213 21:20:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 687.80
[32m[20221213 21:20:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 727.00
[32m[20221213 21:20:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.00
[32m[20221213 21:20:06 @agent_ppo2.py:143][0m Total time:      24.52 min
[32m[20221213 21:20:06 @agent_ppo2.py:145][0m 2404352 total steps have happened
[32m[20221213 21:20:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1174 --------------------------#
[32m[20221213 21:20:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:06 @agent_ppo2.py:185][0m |          -0.0009 |         152.1259 |          14.4721 |
[32m[20221213 21:20:06 @agent_ppo2.py:185][0m |          -0.0042 |         149.3289 |          14.4975 |
[32m[20221213 21:20:06 @agent_ppo2.py:185][0m |          -0.0033 |         149.1952 |          14.5000 |
[32m[20221213 21:20:06 @agent_ppo2.py:185][0m |          -0.0067 |         146.7983 |          14.5031 |
[32m[20221213 21:20:06 @agent_ppo2.py:185][0m |          -0.0065 |         147.1988 |          14.5023 |
[32m[20221213 21:20:06 @agent_ppo2.py:185][0m |          -0.0088 |         145.9032 |          14.5094 |
[32m[20221213 21:20:06 @agent_ppo2.py:185][0m |          -0.0066 |         146.2623 |          14.5223 |
[32m[20221213 21:20:07 @agent_ppo2.py:185][0m |          -0.0084 |         144.9805 |          14.5278 |
[32m[20221213 21:20:07 @agent_ppo2.py:185][0m |          -0.0100 |         144.5645 |          14.5323 |
[32m[20221213 21:20:07 @agent_ppo2.py:185][0m |           0.0035 |         158.4430 |          14.5399 |
[32m[20221213 21:20:07 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:20:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 690.40
[32m[20221213 21:20:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 743.00
[32m[20221213 21:20:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.00
[32m[20221213 21:20:07 @agent_ppo2.py:143][0m Total time:      24.54 min
[32m[20221213 21:20:07 @agent_ppo2.py:145][0m 2406400 total steps have happened
[32m[20221213 21:20:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1175 --------------------------#
[32m[20221213 21:20:07 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:20:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:07 @agent_ppo2.py:185][0m |           0.0042 |         145.4900 |          14.2509 |
[32m[20221213 21:20:07 @agent_ppo2.py:185][0m |          -0.0031 |         141.0698 |          14.2808 |
[32m[20221213 21:20:07 @agent_ppo2.py:185][0m |          -0.0017 |         139.3278 |          14.2745 |
[32m[20221213 21:20:07 @agent_ppo2.py:185][0m |          -0.0033 |         138.7557 |          14.2622 |
[32m[20221213 21:20:08 @agent_ppo2.py:185][0m |          -0.0055 |         137.9541 |          14.2684 |
[32m[20221213 21:20:08 @agent_ppo2.py:185][0m |          -0.0071 |         137.6198 |          14.2644 |
[32m[20221213 21:20:08 @agent_ppo2.py:185][0m |          -0.0021 |         137.8375 |          14.2401 |
[32m[20221213 21:20:08 @agent_ppo2.py:185][0m |          -0.0042 |         136.9163 |          14.2490 |
[32m[20221213 21:20:08 @agent_ppo2.py:185][0m |          -0.0058 |         136.7044 |          14.2576 |
[32m[20221213 21:20:08 @agent_ppo2.py:185][0m |          -0.0012 |         137.4077 |          14.2584 |
[32m[20221213 21:20:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 661.80
[32m[20221213 21:20:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 708.00
[32m[20221213 21:20:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.00
[32m[20221213 21:20:08 @agent_ppo2.py:143][0m Total time:      24.56 min
[32m[20221213 21:20:08 @agent_ppo2.py:145][0m 2408448 total steps have happened
[32m[20221213 21:20:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1176 --------------------------#
[32m[20221213 21:20:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:08 @agent_ppo2.py:185][0m |          -0.0017 |         137.0410 |          14.3819 |
[32m[20221213 21:20:08 @agent_ppo2.py:185][0m |          -0.0071 |         131.9994 |          14.3981 |
[32m[20221213 21:20:09 @agent_ppo2.py:185][0m |          -0.0024 |         131.4206 |          14.4466 |
[32m[20221213 21:20:09 @agent_ppo2.py:185][0m |          -0.0064 |         129.0211 |          14.4352 |
[32m[20221213 21:20:09 @agent_ppo2.py:185][0m |          -0.0080 |         128.3376 |          14.4436 |
[32m[20221213 21:20:09 @agent_ppo2.py:185][0m |          -0.0081 |         127.8453 |          14.4588 |
[32m[20221213 21:20:09 @agent_ppo2.py:185][0m |          -0.0030 |         128.1161 |          14.4388 |
[32m[20221213 21:20:09 @agent_ppo2.py:185][0m |          -0.0094 |         126.5679 |          14.4578 |
[32m[20221213 21:20:09 @agent_ppo2.py:185][0m |           0.0035 |         138.4494 |          14.4406 |
[32m[20221213 21:20:09 @agent_ppo2.py:185][0m |          -0.0055 |         127.7126 |          14.4522 |
[32m[20221213 21:20:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:20:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 704.40
[32m[20221213 21:20:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 746.00
[32m[20221213 21:20:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.00
[32m[20221213 21:20:09 @agent_ppo2.py:143][0m Total time:      24.58 min
[32m[20221213 21:20:09 @agent_ppo2.py:145][0m 2410496 total steps have happened
[32m[20221213 21:20:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1177 --------------------------#
[32m[20221213 21:20:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:10 @agent_ppo2.py:185][0m |          -0.0001 |         146.9105 |          14.5119 |
[32m[20221213 21:20:10 @agent_ppo2.py:185][0m |           0.0027 |         148.3007 |          14.5167 |
[32m[20221213 21:20:10 @agent_ppo2.py:185][0m |          -0.0035 |         140.4615 |          14.4768 |
[32m[20221213 21:20:10 @agent_ppo2.py:185][0m |          -0.0002 |         139.6562 |          14.4958 |
[32m[20221213 21:20:10 @agent_ppo2.py:185][0m |          -0.0050 |         138.7857 |          14.4925 |
[32m[20221213 21:20:10 @agent_ppo2.py:185][0m |          -0.0065 |         138.2528 |          14.5357 |
[32m[20221213 21:20:10 @agent_ppo2.py:185][0m |          -0.0052 |         137.8089 |          14.4767 |
[32m[20221213 21:20:10 @agent_ppo2.py:185][0m |          -0.0090 |         137.4730 |          14.5345 |
[32m[20221213 21:20:10 @agent_ppo2.py:185][0m |          -0.0055 |         137.8372 |          14.4954 |
[32m[20221213 21:20:10 @agent_ppo2.py:185][0m |          -0.0059 |         136.8928 |          14.5071 |
[32m[20221213 21:20:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 734.20
[32m[20221213 21:20:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 775.00
[32m[20221213 21:20:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 779.00
[32m[20221213 21:20:10 @agent_ppo2.py:143][0m Total time:      24.60 min
[32m[20221213 21:20:10 @agent_ppo2.py:145][0m 2412544 total steps have happened
[32m[20221213 21:20:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1178 --------------------------#
[32m[20221213 21:20:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:11 @agent_ppo2.py:185][0m |           0.0017 |         144.2506 |          14.2913 |
[32m[20221213 21:20:11 @agent_ppo2.py:185][0m |          -0.0033 |         142.6424 |          14.2745 |
[32m[20221213 21:20:11 @agent_ppo2.py:185][0m |          -0.0035 |         141.8322 |          14.2800 |
[32m[20221213 21:20:11 @agent_ppo2.py:185][0m |           0.0063 |         153.9351 |          14.2839 |
[32m[20221213 21:20:11 @agent_ppo2.py:185][0m |          -0.0029 |         140.8362 |          14.3191 |
[32m[20221213 21:20:11 @agent_ppo2.py:185][0m |          -0.0009 |         144.0040 |          14.2934 |
[32m[20221213 21:20:11 @agent_ppo2.py:185][0m |           0.0053 |         155.2690 |          14.2880 |
[32m[20221213 21:20:11 @agent_ppo2.py:185][0m |          -0.0054 |         140.0403 |          14.2800 |
[32m[20221213 21:20:11 @agent_ppo2.py:185][0m |          -0.0076 |         139.9007 |          14.3012 |
[32m[20221213 21:20:12 @agent_ppo2.py:185][0m |          -0.0084 |         139.4596 |          14.2959 |
[32m[20221213 21:20:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:20:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 720.60
[32m[20221213 21:20:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.00
[32m[20221213 21:20:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 783.00
[32m[20221213 21:20:12 @agent_ppo2.py:143][0m Total time:      24.62 min
[32m[20221213 21:20:12 @agent_ppo2.py:145][0m 2414592 total steps have happened
[32m[20221213 21:20:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1179 --------------------------#
[32m[20221213 21:20:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:12 @agent_ppo2.py:185][0m |           0.0022 |         148.3642 |          14.6635 |
[32m[20221213 21:20:12 @agent_ppo2.py:185][0m |          -0.0077 |         144.6371 |          14.6451 |
[32m[20221213 21:20:12 @agent_ppo2.py:185][0m |           0.0118 |         166.6788 |          14.6459 |
[32m[20221213 21:20:12 @agent_ppo2.py:185][0m |           0.0024 |         150.2983 |          14.6552 |
[32m[20221213 21:20:12 @agent_ppo2.py:185][0m |          -0.0070 |         143.0468 |          14.6291 |
[32m[20221213 21:20:12 @agent_ppo2.py:185][0m |          -0.0106 |         142.6699 |          14.6224 |
[32m[20221213 21:20:12 @agent_ppo2.py:185][0m |          -0.0083 |         142.3647 |          14.5970 |
[32m[20221213 21:20:13 @agent_ppo2.py:185][0m |          -0.0127 |         142.0973 |          14.5870 |
[32m[20221213 21:20:13 @agent_ppo2.py:185][0m |          -0.0100 |         141.6797 |          14.5680 |
[32m[20221213 21:20:13 @agent_ppo2.py:185][0m |           0.0053 |         161.2807 |          14.5414 |
[32m[20221213 21:20:13 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:20:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 714.20
[32m[20221213 21:20:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.00
[32m[20221213 21:20:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.00
[32m[20221213 21:20:13 @agent_ppo2.py:143][0m Total time:      24.64 min
[32m[20221213 21:20:13 @agent_ppo2.py:145][0m 2416640 total steps have happened
[32m[20221213 21:20:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1180 --------------------------#
[32m[20221213 21:20:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:20:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:13 @agent_ppo2.py:185][0m |           0.0003 |         145.8025 |          14.3455 |
[32m[20221213 21:20:13 @agent_ppo2.py:185][0m |          -0.0044 |         143.5413 |          14.3341 |
[32m[20221213 21:20:13 @agent_ppo2.py:185][0m |          -0.0039 |         142.7273 |          14.3029 |
[32m[20221213 21:20:13 @agent_ppo2.py:185][0m |          -0.0046 |         141.6953 |          14.3162 |
[32m[20221213 21:20:14 @agent_ppo2.py:185][0m |          -0.0046 |         141.0242 |          14.2832 |
[32m[20221213 21:20:14 @agent_ppo2.py:185][0m |          -0.0062 |         140.4954 |          14.2759 |
[32m[20221213 21:20:14 @agent_ppo2.py:185][0m |          -0.0065 |         140.1615 |          14.2702 |
[32m[20221213 21:20:14 @agent_ppo2.py:185][0m |          -0.0028 |         141.0264 |          14.2398 |
[32m[20221213 21:20:14 @agent_ppo2.py:185][0m |          -0.0029 |         140.7774 |          14.2285 |
[32m[20221213 21:20:14 @agent_ppo2.py:185][0m |          -0.0066 |         139.2393 |          14.2160 |
[32m[20221213 21:20:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:20:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 730.40
[32m[20221213 21:20:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.00
[32m[20221213 21:20:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 696.00
[32m[20221213 21:20:14 @agent_ppo2.py:143][0m Total time:      24.66 min
[32m[20221213 21:20:14 @agent_ppo2.py:145][0m 2418688 total steps have happened
[32m[20221213 21:20:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1181 --------------------------#
[32m[20221213 21:20:14 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:20:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:14 @agent_ppo2.py:185][0m |          -0.0011 |         150.8679 |          13.9892 |
[32m[20221213 21:20:14 @agent_ppo2.py:185][0m |          -0.0032 |         149.2664 |          14.0234 |
[32m[20221213 21:20:15 @agent_ppo2.py:185][0m |          -0.0061 |         148.4349 |          13.9999 |
[32m[20221213 21:20:15 @agent_ppo2.py:185][0m |          -0.0079 |         147.8754 |          14.0132 |
[32m[20221213 21:20:15 @agent_ppo2.py:185][0m |          -0.0073 |         147.5024 |          14.0023 |
[32m[20221213 21:20:15 @agent_ppo2.py:185][0m |          -0.0074 |         147.4472 |          13.9986 |
[32m[20221213 21:20:15 @agent_ppo2.py:185][0m |          -0.0085 |         147.0544 |          14.0017 |
[32m[20221213 21:20:15 @agent_ppo2.py:185][0m |          -0.0080 |         146.8484 |          13.9867 |
[32m[20221213 21:20:15 @agent_ppo2.py:185][0m |          -0.0098 |         146.6519 |          13.9723 |
[32m[20221213 21:20:15 @agent_ppo2.py:185][0m |          -0.0093 |         146.5594 |          13.9571 |
[32m[20221213 21:20:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 673.40
[32m[20221213 21:20:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.00
[32m[20221213 21:20:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.00
[32m[20221213 21:20:15 @agent_ppo2.py:143][0m Total time:      24.68 min
[32m[20221213 21:20:15 @agent_ppo2.py:145][0m 2420736 total steps have happened
[32m[20221213 21:20:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1182 --------------------------#
[32m[20221213 21:20:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:16 @agent_ppo2.py:185][0m |          -0.0045 |         141.3532 |          14.1786 |
[32m[20221213 21:20:16 @agent_ppo2.py:185][0m |          -0.0053 |         140.0444 |          14.2052 |
[32m[20221213 21:20:16 @agent_ppo2.py:185][0m |          -0.0066 |         139.2721 |          14.1816 |
[32m[20221213 21:20:16 @agent_ppo2.py:185][0m |          -0.0048 |         139.1758 |          14.1893 |
[32m[20221213 21:20:16 @agent_ppo2.py:185][0m |          -0.0055 |         138.5892 |          14.1920 |
[32m[20221213 21:20:16 @agent_ppo2.py:185][0m |          -0.0063 |         138.4282 |          14.1904 |
[32m[20221213 21:20:16 @agent_ppo2.py:185][0m |          -0.0073 |         137.8085 |          14.1838 |
[32m[20221213 21:20:16 @agent_ppo2.py:185][0m |           0.0019 |         145.6850 |          14.1827 |
[32m[20221213 21:20:16 @agent_ppo2.py:185][0m |          -0.0060 |         138.2144 |          14.1949 |
[32m[20221213 21:20:16 @agent_ppo2.py:185][0m |          -0.0092 |         137.2170 |          14.2006 |
[32m[20221213 21:20:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:20:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 721.40
[32m[20221213 21:20:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.00
[32m[20221213 21:20:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.00
[32m[20221213 21:20:16 @agent_ppo2.py:143][0m Total time:      24.70 min
[32m[20221213 21:20:16 @agent_ppo2.py:145][0m 2422784 total steps have happened
[32m[20221213 21:20:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1183 --------------------------#
[32m[20221213 21:20:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:20:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:17 @agent_ppo2.py:185][0m |           0.0008 |         140.8851 |          13.9716 |
[32m[20221213 21:20:17 @agent_ppo2.py:185][0m |           0.0016 |         139.8098 |          13.9624 |
[32m[20221213 21:20:17 @agent_ppo2.py:185][0m |          -0.0054 |         138.8652 |          13.9749 |
[32m[20221213 21:20:17 @agent_ppo2.py:185][0m |          -0.0022 |         138.4295 |          13.9515 |
[32m[20221213 21:20:17 @agent_ppo2.py:185][0m |          -0.0047 |         138.0117 |          13.9816 |
[32m[20221213 21:20:17 @agent_ppo2.py:185][0m |          -0.0051 |         137.6241 |          13.9583 |
[32m[20221213 21:20:17 @agent_ppo2.py:185][0m |          -0.0066 |         137.4766 |          13.9667 |
[32m[20221213 21:20:17 @agent_ppo2.py:185][0m |          -0.0065 |         137.4752 |          13.9593 |
[32m[20221213 21:20:17 @agent_ppo2.py:185][0m |          -0.0066 |         137.0959 |          13.9712 |
[32m[20221213 21:20:18 @agent_ppo2.py:185][0m |          -0.0091 |         137.0330 |          13.9506 |
[32m[20221213 21:20:18 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:20:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 717.00
[32m[20221213 21:20:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 741.00
[32m[20221213 21:20:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.00
[32m[20221213 21:20:18 @agent_ppo2.py:143][0m Total time:      24.72 min
[32m[20221213 21:20:18 @agent_ppo2.py:145][0m 2424832 total steps have happened
[32m[20221213 21:20:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1184 --------------------------#
[32m[20221213 21:20:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:18 @agent_ppo2.py:185][0m |          -0.0004 |         139.7506 |          14.1120 |
[32m[20221213 21:20:18 @agent_ppo2.py:185][0m |          -0.0011 |         140.9258 |          14.1149 |
[32m[20221213 21:20:18 @agent_ppo2.py:185][0m |          -0.0046 |         138.6777 |          14.1111 |
[32m[20221213 21:20:18 @agent_ppo2.py:185][0m |          -0.0036 |         138.9380 |          14.0620 |
[32m[20221213 21:20:18 @agent_ppo2.py:185][0m |          -0.0071 |         137.9707 |          14.0715 |
[32m[20221213 21:20:18 @agent_ppo2.py:185][0m |          -0.0081 |         137.6193 |          14.0511 |
[32m[20221213 21:20:19 @agent_ppo2.py:185][0m |          -0.0075 |         137.5116 |          14.0459 |
[32m[20221213 21:20:19 @agent_ppo2.py:185][0m |          -0.0044 |         138.0139 |          14.0454 |
[32m[20221213 21:20:19 @agent_ppo2.py:185][0m |          -0.0092 |         137.1184 |          14.0350 |
[32m[20221213 21:20:19 @agent_ppo2.py:185][0m |          -0.0075 |         136.9563 |          14.0243 |
[32m[20221213 21:20:19 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:20:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 739.40
[32m[20221213 21:20:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.00
[32m[20221213 21:20:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 740.00
[32m[20221213 21:20:19 @agent_ppo2.py:143][0m Total time:      24.74 min
[32m[20221213 21:20:19 @agent_ppo2.py:145][0m 2426880 total steps have happened
[32m[20221213 21:20:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1185 --------------------------#
[32m[20221213 21:20:19 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:20:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:19 @agent_ppo2.py:185][0m |          -0.0029 |         143.5594 |          14.0281 |
[32m[20221213 21:20:19 @agent_ppo2.py:185][0m |          -0.0048 |         142.8299 |          14.0444 |
[32m[20221213 21:20:20 @agent_ppo2.py:185][0m |          -0.0041 |         141.3416 |          14.0160 |
[32m[20221213 21:20:20 @agent_ppo2.py:185][0m |          -0.0071 |         140.8398 |          14.0162 |
[32m[20221213 21:20:20 @agent_ppo2.py:185][0m |          -0.0057 |         140.5281 |          14.0229 |
[32m[20221213 21:20:20 @agent_ppo2.py:185][0m |          -0.0083 |         140.2411 |          13.9944 |
[32m[20221213 21:20:20 @agent_ppo2.py:185][0m |          -0.0079 |         139.9507 |          13.9864 |
[32m[20221213 21:20:20 @agent_ppo2.py:185][0m |          -0.0099 |         139.7825 |          13.9830 |
[32m[20221213 21:20:20 @agent_ppo2.py:185][0m |          -0.0069 |         139.7307 |          13.9846 |
[32m[20221213 21:20:20 @agent_ppo2.py:185][0m |          -0.0101 |         139.0960 |          13.9799 |
[32m[20221213 21:20:20 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 21:20:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 728.20
[32m[20221213 21:20:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 756.00
[32m[20221213 21:20:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.00
[32m[20221213 21:20:20 @agent_ppo2.py:143][0m Total time:      24.76 min
[32m[20221213 21:20:20 @agent_ppo2.py:145][0m 2428928 total steps have happened
[32m[20221213 21:20:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1186 --------------------------#
[32m[20221213 21:20:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:21 @agent_ppo2.py:185][0m |           0.0118 |         158.9358 |          14.1341 |
[32m[20221213 21:20:21 @agent_ppo2.py:185][0m |          -0.0043 |         142.9080 |          14.1535 |
[32m[20221213 21:20:21 @agent_ppo2.py:185][0m |           0.0061 |         154.2462 |          14.1062 |
[32m[20221213 21:20:21 @agent_ppo2.py:185][0m |           0.0086 |         157.6754 |          14.1000 |
[32m[20221213 21:20:21 @agent_ppo2.py:185][0m |          -0.0034 |         141.9393 |          14.0930 |
[32m[20221213 21:20:21 @agent_ppo2.py:185][0m |          -0.0070 |         141.6212 |          14.0820 |
[32m[20221213 21:20:21 @agent_ppo2.py:185][0m |          -0.0056 |         141.3964 |          14.0473 |
[32m[20221213 21:20:21 @agent_ppo2.py:185][0m |          -0.0061 |         141.2537 |          14.0437 |
[32m[20221213 21:20:21 @agent_ppo2.py:185][0m |          -0.0075 |         141.1958 |          14.0150 |
[32m[20221213 21:20:22 @agent_ppo2.py:185][0m |          -0.0060 |         141.1693 |          14.0291 |
[32m[20221213 21:20:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:20:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 741.00
[32m[20221213 21:20:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 759.00
[32m[20221213 21:20:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 791.00
[32m[20221213 21:20:22 @agent_ppo2.py:143][0m Total time:      24.78 min
[32m[20221213 21:20:22 @agent_ppo2.py:145][0m 2430976 total steps have happened
[32m[20221213 21:20:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1187 --------------------------#
[32m[20221213 21:20:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:22 @agent_ppo2.py:185][0m |          -0.0013 |         146.3451 |          13.9280 |
[32m[20221213 21:20:22 @agent_ppo2.py:185][0m |          -0.0064 |         145.6071 |          13.9039 |
[32m[20221213 21:20:22 @agent_ppo2.py:185][0m |          -0.0024 |         146.1719 |          13.9220 |
[32m[20221213 21:20:22 @agent_ppo2.py:185][0m |           0.0072 |         160.9409 |          13.8997 |
[32m[20221213 21:20:22 @agent_ppo2.py:185][0m |          -0.0089 |         144.8817 |          13.9100 |
[32m[20221213 21:20:22 @agent_ppo2.py:185][0m |          -0.0055 |         144.7620 |          13.9173 |
[32m[20221213 21:20:23 @agent_ppo2.py:185][0m |          -0.0096 |         144.5475 |          13.9181 |
[32m[20221213 21:20:23 @agent_ppo2.py:185][0m |          -0.0104 |         144.4637 |          13.8740 |
[32m[20221213 21:20:23 @agent_ppo2.py:185][0m |          -0.0074 |         144.2837 |          13.8894 |
[32m[20221213 21:20:23 @agent_ppo2.py:185][0m |          -0.0099 |         144.1635 |          13.8791 |
[32m[20221213 21:20:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 722.80
[32m[20221213 21:20:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 763.00
[32m[20221213 21:20:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.00
[32m[20221213 21:20:23 @agent_ppo2.py:143][0m Total time:      24.80 min
[32m[20221213 21:20:23 @agent_ppo2.py:145][0m 2433024 total steps have happened
[32m[20221213 21:20:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1188 --------------------------#
[32m[20221213 21:20:23 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:20:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:23 @agent_ppo2.py:185][0m |          -0.0015 |         143.9437 |          13.6643 |
[32m[20221213 21:20:23 @agent_ppo2.py:185][0m |           0.0019 |         145.4587 |          13.6599 |
[32m[20221213 21:20:23 @agent_ppo2.py:185][0m |          -0.0052 |         141.9829 |          13.6722 |
[32m[20221213 21:20:23 @agent_ppo2.py:185][0m |           0.0028 |         145.8724 |          13.6441 |
[32m[20221213 21:20:24 @agent_ppo2.py:185][0m |           0.0046 |         146.9476 |          13.5974 |
[32m[20221213 21:20:24 @agent_ppo2.py:185][0m |           0.0061 |         157.1086 |          13.5849 |
[32m[20221213 21:20:24 @agent_ppo2.py:185][0m |          -0.0088 |         140.1996 |          13.6063 |
[32m[20221213 21:20:24 @agent_ppo2.py:185][0m |          -0.0077 |         139.6802 |          13.5724 |
[32m[20221213 21:20:24 @agent_ppo2.py:185][0m |          -0.0103 |         139.3974 |          13.5727 |
[32m[20221213 21:20:24 @agent_ppo2.py:185][0m |          -0.0098 |         138.9283 |          13.5521 |
[32m[20221213 21:20:24 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:20:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.00
[32m[20221213 21:20:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.00
[32m[20221213 21:20:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.00
[32m[20221213 21:20:24 @agent_ppo2.py:143][0m Total time:      24.82 min
[32m[20221213 21:20:24 @agent_ppo2.py:145][0m 2435072 total steps have happened
[32m[20221213 21:20:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1189 --------------------------#
[32m[20221213 21:20:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:24 @agent_ppo2.py:185][0m |           0.0000 |         151.1794 |          13.6242 |
[32m[20221213 21:20:24 @agent_ppo2.py:185][0m |          -0.0062 |         148.7333 |          13.6141 |
[32m[20221213 21:20:25 @agent_ppo2.py:185][0m |          -0.0063 |         147.9688 |          13.5359 |
[32m[20221213 21:20:25 @agent_ppo2.py:185][0m |           0.0026 |         162.3823 |          13.5560 |
[32m[20221213 21:20:25 @agent_ppo2.py:185][0m |          -0.0074 |         146.6016 |          13.5657 |
[32m[20221213 21:20:25 @agent_ppo2.py:185][0m |          -0.0090 |         145.8703 |          13.5340 |
[32m[20221213 21:20:25 @agent_ppo2.py:185][0m |          -0.0058 |         146.2453 |          13.5083 |
[32m[20221213 21:20:25 @agent_ppo2.py:185][0m |          -0.0077 |         145.4204 |          13.5278 |
[32m[20221213 21:20:25 @agent_ppo2.py:185][0m |          -0.0121 |         144.7591 |          13.5274 |
[32m[20221213 21:20:25 @agent_ppo2.py:185][0m |          -0.0114 |         144.2986 |          13.5209 |
[32m[20221213 21:20:25 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:20:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 745.40
[32m[20221213 21:20:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.00
[32m[20221213 21:20:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.00
[32m[20221213 21:20:25 @agent_ppo2.py:143][0m Total time:      24.84 min
[32m[20221213 21:20:25 @agent_ppo2.py:145][0m 2437120 total steps have happened
[32m[20221213 21:20:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1190 --------------------------#
[32m[20221213 21:20:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:20:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:26 @agent_ppo2.py:185][0m |          -0.0010 |         153.1662 |          13.5747 |
[32m[20221213 21:20:26 @agent_ppo2.py:185][0m |           0.0114 |         165.8593 |          13.5354 |
[32m[20221213 21:20:26 @agent_ppo2.py:185][0m |          -0.0020 |         150.7816 |          13.5221 |
[32m[20221213 21:20:26 @agent_ppo2.py:185][0m |          -0.0056 |         150.2181 |          13.5241 |
[32m[20221213 21:20:26 @agent_ppo2.py:185][0m |          -0.0062 |         149.4391 |          13.5145 |
[32m[20221213 21:20:26 @agent_ppo2.py:185][0m |          -0.0101 |         148.9804 |          13.5219 |
[32m[20221213 21:20:26 @agent_ppo2.py:185][0m |          -0.0092 |         148.4653 |          13.5313 |
[32m[20221213 21:20:26 @agent_ppo2.py:185][0m |          -0.0094 |         148.2332 |          13.5068 |
[32m[20221213 21:20:26 @agent_ppo2.py:185][0m |          -0.0017 |         154.5836 |          13.4923 |
[32m[20221213 21:20:27 @agent_ppo2.py:185][0m |          -0.0077 |         148.0502 |          13.4939 |
[32m[20221213 21:20:27 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:20:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 760.80
[32m[20221213 21:20:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.00
[32m[20221213 21:20:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 779.00
[32m[20221213 21:20:27 @agent_ppo2.py:143][0m Total time:      24.87 min
[32m[20221213 21:20:27 @agent_ppo2.py:145][0m 2439168 total steps have happened
[32m[20221213 21:20:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1191 --------------------------#
[32m[20221213 21:20:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:27 @agent_ppo2.py:185][0m |           0.0001 |         154.8018 |          13.3507 |
[32m[20221213 21:20:27 @agent_ppo2.py:185][0m |          -0.0018 |         152.0441 |          13.3529 |
[32m[20221213 21:20:27 @agent_ppo2.py:185][0m |          -0.0052 |         149.7732 |          13.3606 |
[32m[20221213 21:20:27 @agent_ppo2.py:185][0m |          -0.0057 |         148.1785 |          13.3666 |
[32m[20221213 21:20:27 @agent_ppo2.py:185][0m |          -0.0058 |         147.5987 |          13.3687 |
[32m[20221213 21:20:27 @agent_ppo2.py:185][0m |          -0.0077 |         147.0069 |          13.3811 |
[32m[20221213 21:20:27 @agent_ppo2.py:185][0m |           0.0009 |         157.3272 |          13.3760 |
[32m[20221213 21:20:28 @agent_ppo2.py:185][0m |          -0.0104 |         146.1123 |          13.3873 |
[32m[20221213 21:20:28 @agent_ppo2.py:185][0m |          -0.0098 |         145.7039 |          13.3907 |
[32m[20221213 21:20:28 @agent_ppo2.py:185][0m |          -0.0091 |         145.4936 |          13.3875 |
[32m[20221213 21:20:28 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:20:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.00
[32m[20221213 21:20:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 754.00
[32m[20221213 21:20:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.00
[32m[20221213 21:20:28 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 837.00
[32m[20221213 21:20:28 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 837.00
[32m[20221213 21:20:28 @agent_ppo2.py:143][0m Total time:      24.89 min
[32m[20221213 21:20:28 @agent_ppo2.py:145][0m 2441216 total steps have happened
[32m[20221213 21:20:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1192 --------------------------#
[32m[20221213 21:20:28 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:20:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:28 @agent_ppo2.py:185][0m |           0.0004 |         158.0643 |          13.4155 |
[32m[20221213 21:20:28 @agent_ppo2.py:185][0m |          -0.0011 |         159.0357 |          13.3774 |
[32m[20221213 21:20:28 @agent_ppo2.py:185][0m |          -0.0041 |         154.9238 |          13.3422 |
[32m[20221213 21:20:28 @agent_ppo2.py:185][0m |          -0.0042 |         154.2698 |          13.3779 |
[32m[20221213 21:20:28 @agent_ppo2.py:185][0m |          -0.0062 |         153.9226 |          13.3824 |
[32m[20221213 21:20:29 @agent_ppo2.py:185][0m |          -0.0066 |         153.7022 |          13.3444 |
[32m[20221213 21:20:29 @agent_ppo2.py:185][0m |          -0.0066 |         153.5882 |          13.3391 |
[32m[20221213 21:20:29 @agent_ppo2.py:185][0m |          -0.0070 |         152.9158 |          13.3533 |
[32m[20221213 21:20:29 @agent_ppo2.py:185][0m |          -0.0022 |         154.2621 |          13.3470 |
[32m[20221213 21:20:29 @agent_ppo2.py:185][0m |          -0.0039 |         156.7700 |          13.3310 |
[32m[20221213 21:20:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:20:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.60
[32m[20221213 21:20:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 773.00
[32m[20221213 21:20:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 771.00
[32m[20221213 21:20:29 @agent_ppo2.py:143][0m Total time:      24.91 min
[32m[20221213 21:20:29 @agent_ppo2.py:145][0m 2443264 total steps have happened
[32m[20221213 21:20:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1193 --------------------------#
[32m[20221213 21:20:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:20:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:29 @agent_ppo2.py:185][0m |          -0.0020 |         153.3095 |          13.3622 |
[32m[20221213 21:20:29 @agent_ppo2.py:185][0m |           0.0057 |         159.9410 |          13.3408 |
[32m[20221213 21:20:30 @agent_ppo2.py:185][0m |          -0.0050 |         151.7884 |          13.3269 |
[32m[20221213 21:20:30 @agent_ppo2.py:185][0m |           0.0047 |         157.9875 |          13.3334 |
[32m[20221213 21:20:30 @agent_ppo2.py:185][0m |          -0.0049 |         151.6262 |          13.3320 |
[32m[20221213 21:20:30 @agent_ppo2.py:185][0m |          -0.0065 |         150.8978 |          13.3298 |
[32m[20221213 21:20:30 @agent_ppo2.py:185][0m |          -0.0061 |         150.9174 |          13.3192 |
[32m[20221213 21:20:30 @agent_ppo2.py:185][0m |          -0.0062 |         150.3229 |          13.3187 |
[32m[20221213 21:20:30 @agent_ppo2.py:185][0m |          -0.0078 |         150.0880 |          13.3223 |
[32m[20221213 21:20:30 @agent_ppo2.py:185][0m |          -0.0089 |         150.0339 |          13.3006 |
[32m[20221213 21:20:30 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:20:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.20
[32m[20221213 21:20:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.00
[32m[20221213 21:20:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:20:30 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 840.00
[32m[20221213 21:20:30 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 840.00
[32m[20221213 21:20:30 @agent_ppo2.py:143][0m Total time:      24.93 min
[32m[20221213 21:20:30 @agent_ppo2.py:145][0m 2445312 total steps have happened
[32m[20221213 21:20:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1194 --------------------------#
[32m[20221213 21:20:30 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:20:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:31 @agent_ppo2.py:185][0m |          -0.0037 |         154.8361 |          13.4362 |
[32m[20221213 21:20:31 @agent_ppo2.py:185][0m |           0.0005 |         162.2863 |          13.4585 |
[32m[20221213 21:20:31 @agent_ppo2.py:185][0m |          -0.0070 |         151.7425 |          13.4520 |
[32m[20221213 21:20:31 @agent_ppo2.py:185][0m |          -0.0068 |         150.8368 |          13.4491 |
[32m[20221213 21:20:31 @agent_ppo2.py:185][0m |          -0.0105 |         150.2653 |          13.4678 |
[32m[20221213 21:20:31 @agent_ppo2.py:185][0m |          -0.0033 |         150.9612 |          13.4860 |
[32m[20221213 21:20:31 @agent_ppo2.py:185][0m |          -0.0106 |         149.8124 |          13.5028 |
[32m[20221213 21:20:31 @agent_ppo2.py:185][0m |          -0.0091 |         149.4501 |          13.5083 |
[32m[20221213 21:20:31 @agent_ppo2.py:185][0m |          -0.0093 |         149.4161 |          13.5512 |
[32m[20221213 21:20:32 @agent_ppo2.py:185][0m |          -0.0129 |         148.8817 |          13.5601 |
[32m[20221213 21:20:32 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:20:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 701.00
[32m[20221213 21:20:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.00
[32m[20221213 21:20:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.00
[32m[20221213 21:20:32 @agent_ppo2.py:143][0m Total time:      24.95 min
[32m[20221213 21:20:32 @agent_ppo2.py:145][0m 2447360 total steps have happened
[32m[20221213 21:20:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1195 --------------------------#
[32m[20221213 21:20:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:32 @agent_ppo2.py:185][0m |          -0.0023 |         152.8698 |          13.4461 |
[32m[20221213 21:20:32 @agent_ppo2.py:185][0m |          -0.0009 |         151.2492 |          13.4654 |
[32m[20221213 21:20:32 @agent_ppo2.py:185][0m |          -0.0082 |         148.5131 |          13.4731 |
[32m[20221213 21:20:32 @agent_ppo2.py:185][0m |          -0.0114 |         147.5294 |          13.4799 |
[32m[20221213 21:20:32 @agent_ppo2.py:185][0m |          -0.0104 |         146.9621 |          13.4750 |
[32m[20221213 21:20:32 @agent_ppo2.py:185][0m |          -0.0064 |         146.7082 |          13.4842 |
[32m[20221213 21:20:32 @agent_ppo2.py:185][0m |          -0.0100 |         145.9730 |          13.4914 |
[32m[20221213 21:20:33 @agent_ppo2.py:185][0m |          -0.0106 |         145.7935 |          13.4838 |
[32m[20221213 21:20:33 @agent_ppo2.py:185][0m |           0.0003 |         154.1673 |          13.4863 |
[32m[20221213 21:20:33 @agent_ppo2.py:185][0m |          -0.0100 |         144.7929 |          13.4943 |
[32m[20221213 21:20:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:20:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 676.40
[32m[20221213 21:20:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 716.00
[32m[20221213 21:20:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.00
[32m[20221213 21:20:33 @agent_ppo2.py:143][0m Total time:      24.97 min
[32m[20221213 21:20:33 @agent_ppo2.py:145][0m 2449408 total steps have happened
[32m[20221213 21:20:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1196 --------------------------#
[32m[20221213 21:20:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:20:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:33 @agent_ppo2.py:185][0m |          -0.0021 |         155.9385 |          13.5274 |
[32m[20221213 21:20:33 @agent_ppo2.py:185][0m |          -0.0008 |         157.5489 |          13.5110 |
[32m[20221213 21:20:33 @agent_ppo2.py:185][0m |          -0.0068 |         153.2666 |          13.5234 |
[32m[20221213 21:20:33 @agent_ppo2.py:185][0m |          -0.0069 |         152.9026 |          13.5532 |
[32m[20221213 21:20:34 @agent_ppo2.py:185][0m |          -0.0069 |         152.3854 |          13.5731 |
[32m[20221213 21:20:34 @agent_ppo2.py:185][0m |          -0.0072 |         152.0885 |          13.5777 |
[32m[20221213 21:20:34 @agent_ppo2.py:185][0m |          -0.0100 |         151.6310 |          13.5869 |
[32m[20221213 21:20:34 @agent_ppo2.py:185][0m |           0.0054 |         163.7832 |          13.6302 |
[32m[20221213 21:20:34 @agent_ppo2.py:185][0m |          -0.0070 |         151.3180 |          13.6210 |
[32m[20221213 21:20:34 @agent_ppo2.py:185][0m |           0.0003 |         161.4201 |          13.6324 |
[32m[20221213 21:20:34 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:20:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 699.40
[32m[20221213 21:20:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 722.00
[32m[20221213 21:20:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.00
[32m[20221213 21:20:34 @agent_ppo2.py:143][0m Total time:      24.99 min
[32m[20221213 21:20:34 @agent_ppo2.py:145][0m 2451456 total steps have happened
[32m[20221213 21:20:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1197 --------------------------#
[32m[20221213 21:20:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:34 @agent_ppo2.py:185][0m |          -0.0002 |         152.2659 |          13.5129 |
[32m[20221213 21:20:35 @agent_ppo2.py:185][0m |          -0.0050 |         149.5499 |          13.5187 |
[32m[20221213 21:20:35 @agent_ppo2.py:185][0m |           0.0057 |         159.8645 |          13.5161 |
[32m[20221213 21:20:35 @agent_ppo2.py:185][0m |          -0.0074 |         147.8125 |          13.5269 |
[32m[20221213 21:20:35 @agent_ppo2.py:185][0m |          -0.0029 |         147.8500 |          13.5514 |
[32m[20221213 21:20:35 @agent_ppo2.py:185][0m |          -0.0085 |         147.1648 |          13.5486 |
[32m[20221213 21:20:35 @agent_ppo2.py:185][0m |          -0.0082 |         146.7173 |          13.5498 |
[32m[20221213 21:20:35 @agent_ppo2.py:185][0m |          -0.0043 |         148.0276 |          13.5816 |
[32m[20221213 21:20:35 @agent_ppo2.py:185][0m |           0.0088 |         167.0313 |          13.5933 |
[32m[20221213 21:20:35 @agent_ppo2.py:185][0m |          -0.0079 |         146.0233 |          13.6337 |
[32m[20221213 21:20:35 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:20:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 675.40
[32m[20221213 21:20:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 708.00
[32m[20221213 21:20:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.00
[32m[20221213 21:20:35 @agent_ppo2.py:143][0m Total time:      25.01 min
[32m[20221213 21:20:35 @agent_ppo2.py:145][0m 2453504 total steps have happened
[32m[20221213 21:20:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1198 --------------------------#
[32m[20221213 21:20:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:36 @agent_ppo2.py:185][0m |          -0.0006 |         146.3946 |          13.7215 |
[32m[20221213 21:20:36 @agent_ppo2.py:185][0m |          -0.0071 |         145.7037 |          13.7456 |
[32m[20221213 21:20:36 @agent_ppo2.py:185][0m |          -0.0067 |         145.1916 |          13.7514 |
[32m[20221213 21:20:36 @agent_ppo2.py:185][0m |          -0.0032 |         145.1401 |          13.7920 |
[32m[20221213 21:20:36 @agent_ppo2.py:185][0m |          -0.0066 |         144.5352 |          13.7566 |
[32m[20221213 21:20:36 @agent_ppo2.py:185][0m |          -0.0067 |         144.3428 |          13.7936 |
[32m[20221213 21:20:36 @agent_ppo2.py:185][0m |          -0.0020 |         145.0467 |          13.7910 |
[32m[20221213 21:20:36 @agent_ppo2.py:185][0m |          -0.0082 |         144.0492 |          13.7998 |
[32m[20221213 21:20:36 @agent_ppo2.py:185][0m |          -0.0028 |         144.8697 |          13.8179 |
[32m[20221213 21:20:36 @agent_ppo2.py:185][0m |          -0.0013 |         150.8556 |          13.8559 |
[32m[20221213 21:20:36 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:20:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 732.60
[32m[20221213 21:20:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 760.00
[32m[20221213 21:20:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 672.00
[32m[20221213 21:20:37 @agent_ppo2.py:143][0m Total time:      25.03 min
[32m[20221213 21:20:37 @agent_ppo2.py:145][0m 2455552 total steps have happened
[32m[20221213 21:20:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1199 --------------------------#
[32m[20221213 21:20:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:37 @agent_ppo2.py:185][0m |          -0.0006 |         146.7193 |          13.9528 |
[32m[20221213 21:20:37 @agent_ppo2.py:185][0m |          -0.0036 |         145.7847 |          13.9463 |
[32m[20221213 21:20:37 @agent_ppo2.py:185][0m |          -0.0025 |         144.9658 |          13.9180 |
[32m[20221213 21:20:37 @agent_ppo2.py:185][0m |          -0.0051 |         144.4986 |          13.8838 |
[32m[20221213 21:20:37 @agent_ppo2.py:185][0m |          -0.0071 |         144.2006 |          13.8544 |
[32m[20221213 21:20:37 @agent_ppo2.py:185][0m |          -0.0073 |         143.8433 |          13.8472 |
[32m[20221213 21:20:37 @agent_ppo2.py:185][0m |          -0.0003 |         152.3206 |          13.8245 |
[32m[20221213 21:20:38 @agent_ppo2.py:185][0m |          -0.0075 |         143.4444 |          13.8049 |
[32m[20221213 21:20:38 @agent_ppo2.py:185][0m |          -0.0008 |         146.7630 |          13.7913 |
[32m[20221213 21:20:38 @agent_ppo2.py:185][0m |          -0.0100 |         143.0683 |          13.7702 |
[32m[20221213 21:20:38 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:20:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.80
[32m[20221213 21:20:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.00
[32m[20221213 21:20:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.00
[32m[20221213 21:20:38 @agent_ppo2.py:143][0m Total time:      25.05 min
[32m[20221213 21:20:38 @agent_ppo2.py:145][0m 2457600 total steps have happened
[32m[20221213 21:20:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1200 --------------------------#
[32m[20221213 21:20:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:38 @agent_ppo2.py:185][0m |          -0.0009 |         148.9544 |          13.7086 |
[32m[20221213 21:20:38 @agent_ppo2.py:185][0m |          -0.0039 |         147.2790 |          13.7284 |
[32m[20221213 21:20:38 @agent_ppo2.py:185][0m |           0.0003 |         146.7770 |          13.7381 |
[32m[20221213 21:20:38 @agent_ppo2.py:185][0m |          -0.0068 |         145.9476 |          13.7060 |
[32m[20221213 21:20:39 @agent_ppo2.py:185][0m |          -0.0059 |         145.7350 |          13.7496 |
[32m[20221213 21:20:39 @agent_ppo2.py:185][0m |          -0.0060 |         145.4481 |          13.7221 |
[32m[20221213 21:20:39 @agent_ppo2.py:185][0m |          -0.0074 |         145.3919 |          13.7310 |
[32m[20221213 21:20:39 @agent_ppo2.py:185][0m |          -0.0074 |         145.3034 |          13.7292 |
[32m[20221213 21:20:39 @agent_ppo2.py:185][0m |          -0.0093 |         145.0377 |          13.7398 |
[32m[20221213 21:20:39 @agent_ppo2.py:185][0m |          -0.0090 |         145.0399 |          13.7310 |
[32m[20221213 21:20:39 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:20:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 714.00
[32m[20221213 21:20:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 740.00
[32m[20221213 21:20:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 787.00
[32m[20221213 21:20:39 @agent_ppo2.py:143][0m Total time:      25.07 min
[32m[20221213 21:20:39 @agent_ppo2.py:145][0m 2459648 total steps have happened
[32m[20221213 21:20:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1201 --------------------------#
[32m[20221213 21:20:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:39 @agent_ppo2.py:185][0m |           0.0046 |         150.7469 |          13.8163 |
[32m[20221213 21:20:40 @agent_ppo2.py:185][0m |           0.0055 |         157.9585 |          13.7999 |
[32m[20221213 21:20:40 @agent_ppo2.py:185][0m |          -0.0043 |         146.5072 |          13.8204 |
[32m[20221213 21:20:40 @agent_ppo2.py:185][0m |          -0.0056 |         145.7418 |          13.8295 |
[32m[20221213 21:20:40 @agent_ppo2.py:185][0m |           0.0053 |         159.8868 |          13.8483 |
[32m[20221213 21:20:40 @agent_ppo2.py:185][0m |          -0.0081 |         145.1061 |          13.8363 |
[32m[20221213 21:20:40 @agent_ppo2.py:185][0m |          -0.0078 |         144.6654 |          13.8616 |
[32m[20221213 21:20:40 @agent_ppo2.py:185][0m |          -0.0078 |         144.3177 |          13.8632 |
[32m[20221213 21:20:40 @agent_ppo2.py:185][0m |           0.0032 |         148.4989 |          13.8540 |
[32m[20221213 21:20:40 @agent_ppo2.py:185][0m |          -0.0066 |         144.1143 |          13.8886 |
[32m[20221213 21:20:40 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:20:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 723.20
[32m[20221213 21:20:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.00
[32m[20221213 21:20:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 731.00
[32m[20221213 21:20:40 @agent_ppo2.py:143][0m Total time:      25.09 min
[32m[20221213 21:20:40 @agent_ppo2.py:145][0m 2461696 total steps have happened
[32m[20221213 21:20:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1202 --------------------------#
[32m[20221213 21:20:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:41 @agent_ppo2.py:185][0m |           0.0137 |         166.6050 |          13.9016 |
[32m[20221213 21:20:41 @agent_ppo2.py:185][0m |          -0.0038 |         145.5468 |          13.9363 |
[32m[20221213 21:20:41 @agent_ppo2.py:185][0m |          -0.0053 |         144.9925 |          13.9333 |
[32m[20221213 21:20:41 @agent_ppo2.py:185][0m |           0.0014 |         150.1856 |          13.9348 |
[32m[20221213 21:20:41 @agent_ppo2.py:185][0m |           0.0015 |         150.5174 |          13.9498 |
[32m[20221213 21:20:41 @agent_ppo2.py:185][0m |          -0.0076 |         144.2223 |          13.9555 |
[32m[20221213 21:20:41 @agent_ppo2.py:185][0m |          -0.0005 |         145.8279 |          13.9638 |
[32m[20221213 21:20:41 @agent_ppo2.py:185][0m |          -0.0058 |         143.9975 |          13.9439 |
[32m[20221213 21:20:41 @agent_ppo2.py:185][0m |          -0.0078 |         143.7295 |          13.9724 |
[32m[20221213 21:20:42 @agent_ppo2.py:185][0m |          -0.0066 |         143.6086 |          13.9708 |
[32m[20221213 21:20:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:20:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 712.00
[32m[20221213 21:20:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 754.00
[32m[20221213 21:20:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 725.00
[32m[20221213 21:20:42 @agent_ppo2.py:143][0m Total time:      25.12 min
[32m[20221213 21:20:42 @agent_ppo2.py:145][0m 2463744 total steps have happened
[32m[20221213 21:20:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1203 --------------------------#
[32m[20221213 21:20:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:42 @agent_ppo2.py:185][0m |          -0.0008 |         149.2045 |          13.8817 |
[32m[20221213 21:20:42 @agent_ppo2.py:185][0m |          -0.0067 |         147.2716 |          13.9061 |
[32m[20221213 21:20:42 @agent_ppo2.py:185][0m |          -0.0071 |         146.5214 |          13.8860 |
[32m[20221213 21:20:42 @agent_ppo2.py:185][0m |          -0.0059 |         146.6584 |          13.9149 |
[32m[20221213 21:20:42 @agent_ppo2.py:185][0m |          -0.0117 |         144.9778 |          13.9385 |
[32m[20221213 21:20:42 @agent_ppo2.py:185][0m |          -0.0103 |         144.6687 |          13.9197 |
[32m[20221213 21:20:43 @agent_ppo2.py:185][0m |          -0.0119 |         144.1390 |          13.9387 |
[32m[20221213 21:20:43 @agent_ppo2.py:185][0m |          -0.0108 |         144.0861 |          13.9312 |
[32m[20221213 21:20:43 @agent_ppo2.py:185][0m |          -0.0118 |         143.6463 |          13.9347 |
[32m[20221213 21:20:43 @agent_ppo2.py:185][0m |          -0.0103 |         144.6950 |          13.9396 |
[32m[20221213 21:20:43 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:20:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 692.20
[32m[20221213 21:20:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 726.00
[32m[20221213 21:20:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.00
[32m[20221213 21:20:43 @agent_ppo2.py:143][0m Total time:      25.14 min
[32m[20221213 21:20:43 @agent_ppo2.py:145][0m 2465792 total steps have happened
[32m[20221213 21:20:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1204 --------------------------#
[32m[20221213 21:20:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:43 @agent_ppo2.py:185][0m |           0.0138 |         164.2985 |          14.0081 |
[32m[20221213 21:20:43 @agent_ppo2.py:185][0m |          -0.0036 |         146.5811 |          14.0063 |
[32m[20221213 21:20:43 @agent_ppo2.py:185][0m |          -0.0039 |         146.1935 |          14.0250 |
[32m[20221213 21:20:44 @agent_ppo2.py:185][0m |           0.0067 |         163.9635 |          14.0143 |
[32m[20221213 21:20:44 @agent_ppo2.py:185][0m |          -0.0055 |         145.8927 |          14.0435 |
[32m[20221213 21:20:44 @agent_ppo2.py:185][0m |          -0.0073 |         145.1700 |          14.0471 |
[32m[20221213 21:20:44 @agent_ppo2.py:185][0m |          -0.0090 |         144.9741 |          14.0694 |
[32m[20221213 21:20:44 @agent_ppo2.py:185][0m |          -0.0087 |         144.6738 |          14.0620 |
[32m[20221213 21:20:44 @agent_ppo2.py:185][0m |          -0.0044 |         144.5286 |          14.0814 |
[32m[20221213 21:20:44 @agent_ppo2.py:185][0m |          -0.0034 |         147.0268 |          14.1014 |
[32m[20221213 21:20:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:20:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 712.20
[32m[20221213 21:20:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 754.00
[32m[20221213 21:20:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 774.00
[32m[20221213 21:20:44 @agent_ppo2.py:143][0m Total time:      25.16 min
[32m[20221213 21:20:44 @agent_ppo2.py:145][0m 2467840 total steps have happened
[32m[20221213 21:20:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1205 --------------------------#
[32m[20221213 21:20:44 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:20:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:44 @agent_ppo2.py:185][0m |           0.0126 |         161.5208 |          13.9704 |
[32m[20221213 21:20:45 @agent_ppo2.py:185][0m |          -0.0025 |         145.9370 |          13.9288 |
[32m[20221213 21:20:45 @agent_ppo2.py:185][0m |          -0.0057 |         145.3763 |          13.9583 |
[32m[20221213 21:20:45 @agent_ppo2.py:185][0m |          -0.0042 |         145.3696 |          13.9324 |
[32m[20221213 21:20:45 @agent_ppo2.py:185][0m |          -0.0056 |         144.8581 |          13.9771 |
[32m[20221213 21:20:45 @agent_ppo2.py:185][0m |          -0.0082 |         144.4168 |          13.9316 |
[32m[20221213 21:20:45 @agent_ppo2.py:185][0m |          -0.0061 |         143.9915 |          13.9352 |
[32m[20221213 21:20:45 @agent_ppo2.py:185][0m |          -0.0057 |         143.7496 |          13.9196 |
[32m[20221213 21:20:45 @agent_ppo2.py:185][0m |          -0.0043 |         144.5582 |          13.9363 |
[32m[20221213 21:20:45 @agent_ppo2.py:185][0m |          -0.0080 |         143.4772 |          13.9367 |
[32m[20221213 21:20:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:20:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 683.80
[32m[20221213 21:20:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 728.00
[32m[20221213 21:20:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 774.00
[32m[20221213 21:20:45 @agent_ppo2.py:143][0m Total time:      25.18 min
[32m[20221213 21:20:45 @agent_ppo2.py:145][0m 2469888 total steps have happened
[32m[20221213 21:20:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1206 --------------------------#
[32m[20221213 21:20:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:20:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:46 @agent_ppo2.py:185][0m |          -0.0013 |         146.0670 |          13.5842 |
[32m[20221213 21:20:46 @agent_ppo2.py:185][0m |          -0.0052 |         145.1225 |          13.5654 |
[32m[20221213 21:20:46 @agent_ppo2.py:185][0m |          -0.0023 |         144.4816 |          13.5638 |
[32m[20221213 21:20:46 @agent_ppo2.py:185][0m |          -0.0061 |         144.1162 |          13.5460 |
[32m[20221213 21:20:46 @agent_ppo2.py:185][0m |          -0.0013 |         143.9055 |          13.5517 |
[32m[20221213 21:20:46 @agent_ppo2.py:185][0m |          -0.0055 |         143.6635 |          13.5349 |
[32m[20221213 21:20:46 @agent_ppo2.py:185][0m |          -0.0046 |         143.1788 |          13.5528 |
[32m[20221213 21:20:46 @agent_ppo2.py:185][0m |          -0.0073 |         143.0341 |          13.5336 |
[32m[20221213 21:20:46 @agent_ppo2.py:185][0m |          -0.0049 |         142.7476 |          13.4966 |
[32m[20221213 21:20:46 @agent_ppo2.py:185][0m |          -0.0053 |         142.5712 |          13.5278 |
[32m[20221213 21:20:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 709.40
[32m[20221213 21:20:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 757.00
[32m[20221213 21:20:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 786.00
[32m[20221213 21:20:47 @agent_ppo2.py:143][0m Total time:      25.20 min
[32m[20221213 21:20:47 @agent_ppo2.py:145][0m 2471936 total steps have happened
[32m[20221213 21:20:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1207 --------------------------#
[32m[20221213 21:20:47 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:20:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:47 @agent_ppo2.py:185][0m |          -0.0003 |         144.3096 |          13.7342 |
[32m[20221213 21:20:47 @agent_ppo2.py:185][0m |          -0.0054 |         143.3836 |          13.7241 |
[32m[20221213 21:20:47 @agent_ppo2.py:185][0m |          -0.0052 |         142.7803 |          13.7212 |
[32m[20221213 21:20:47 @agent_ppo2.py:185][0m |          -0.0080 |         142.3810 |          13.7141 |
[32m[20221213 21:20:47 @agent_ppo2.py:185][0m |          -0.0065 |         142.3951 |          13.6980 |
[32m[20221213 21:20:47 @agent_ppo2.py:185][0m |          -0.0024 |         143.6096 |          13.7322 |
[32m[20221213 21:20:47 @agent_ppo2.py:185][0m |          -0.0072 |         141.8224 |          13.6808 |
[32m[20221213 21:20:47 @agent_ppo2.py:185][0m |          -0.0056 |         143.0813 |          13.6691 |
[32m[20221213 21:20:48 @agent_ppo2.py:185][0m |          -0.0076 |         141.5689 |          13.6922 |
[32m[20221213 21:20:48 @agent_ppo2.py:185][0m |          -0.0089 |         141.4193 |          13.6602 |
[32m[20221213 21:20:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.60
[32m[20221213 21:20:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 760.00
[32m[20221213 21:20:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.00
[32m[20221213 21:20:48 @agent_ppo2.py:143][0m Total time:      25.22 min
[32m[20221213 21:20:48 @agent_ppo2.py:145][0m 2473984 total steps have happened
[32m[20221213 21:20:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1208 --------------------------#
[32m[20221213 21:20:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:48 @agent_ppo2.py:185][0m |          -0.0024 |         148.1625 |          13.7555 |
[32m[20221213 21:20:48 @agent_ppo2.py:185][0m |          -0.0034 |         147.1712 |          13.7364 |
[32m[20221213 21:20:48 @agent_ppo2.py:185][0m |          -0.0038 |         146.2395 |          13.7416 |
[32m[20221213 21:20:48 @agent_ppo2.py:185][0m |          -0.0048 |         145.3328 |          13.7078 |
[32m[20221213 21:20:48 @agent_ppo2.py:185][0m |          -0.0045 |         144.9204 |          13.7143 |
[32m[20221213 21:20:49 @agent_ppo2.py:185][0m |          -0.0074 |         144.2666 |          13.6922 |
[32m[20221213 21:20:49 @agent_ppo2.py:185][0m |          -0.0072 |         143.4911 |          13.6900 |
[32m[20221213 21:20:49 @agent_ppo2.py:185][0m |          -0.0089 |         143.2378 |          13.6853 |
[32m[20221213 21:20:49 @agent_ppo2.py:185][0m |          -0.0096 |         143.0851 |          13.6565 |
[32m[20221213 21:20:49 @agent_ppo2.py:185][0m |          -0.0076 |         142.4861 |          13.6708 |
[32m[20221213 21:20:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:20:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 698.00
[32m[20221213 21:20:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.00
[32m[20221213 21:20:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.00
[32m[20221213 21:20:49 @agent_ppo2.py:143][0m Total time:      25.24 min
[32m[20221213 21:20:49 @agent_ppo2.py:145][0m 2476032 total steps have happened
[32m[20221213 21:20:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1209 --------------------------#
[32m[20221213 21:20:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:20:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:49 @agent_ppo2.py:185][0m |           0.0042 |         149.8538 |          13.6079 |
[32m[20221213 21:20:49 @agent_ppo2.py:185][0m |          -0.0055 |         146.4551 |          13.6066 |
[32m[20221213 21:20:49 @agent_ppo2.py:185][0m |          -0.0041 |         146.5873 |          13.5919 |
[32m[20221213 21:20:50 @agent_ppo2.py:185][0m |          -0.0066 |         145.3048 |          13.5951 |
[32m[20221213 21:20:50 @agent_ppo2.py:185][0m |          -0.0048 |         144.9023 |          13.5804 |
[32m[20221213 21:20:50 @agent_ppo2.py:185][0m |          -0.0079 |         144.4518 |          13.5818 |
[32m[20221213 21:20:50 @agent_ppo2.py:185][0m |          -0.0072 |         144.1590 |          13.5504 |
[32m[20221213 21:20:50 @agent_ppo2.py:185][0m |          -0.0128 |         144.1768 |          13.5684 |
[32m[20221213 21:20:50 @agent_ppo2.py:185][0m |          -0.0044 |         143.7974 |          13.5802 |
[32m[20221213 21:20:50 @agent_ppo2.py:185][0m |          -0.0109 |         143.9228 |          13.5752 |
[32m[20221213 21:20:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.40
[32m[20221213 21:20:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.00
[32m[20221213 21:20:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.00
[32m[20221213 21:20:50 @agent_ppo2.py:143][0m Total time:      25.26 min
[32m[20221213 21:20:50 @agent_ppo2.py:145][0m 2478080 total steps have happened
[32m[20221213 21:20:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1210 --------------------------#
[32m[20221213 21:20:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:50 @agent_ppo2.py:185][0m |          -0.0018 |         149.5923 |          13.4496 |
[32m[20221213 21:20:51 @agent_ppo2.py:185][0m |          -0.0049 |         148.6311 |          13.4543 |
[32m[20221213 21:20:51 @agent_ppo2.py:185][0m |          -0.0016 |         148.2990 |          13.4449 |
[32m[20221213 21:20:51 @agent_ppo2.py:185][0m |          -0.0055 |         147.7487 |          13.4115 |
[32m[20221213 21:20:51 @agent_ppo2.py:185][0m |          -0.0065 |         147.3031 |          13.4075 |
[32m[20221213 21:20:51 @agent_ppo2.py:185][0m |          -0.0087 |         147.0319 |          13.3840 |
[32m[20221213 21:20:51 @agent_ppo2.py:185][0m |          -0.0068 |         146.8207 |          13.3834 |
[32m[20221213 21:20:51 @agent_ppo2.py:185][0m |          -0.0086 |         146.6086 |          13.3816 |
[32m[20221213 21:20:51 @agent_ppo2.py:185][0m |          -0.0098 |         146.4479 |          13.3877 |
[32m[20221213 21:20:51 @agent_ppo2.py:185][0m |          -0.0078 |         146.1831 |          13.3422 |
[32m[20221213 21:20:51 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:20:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 736.00
[32m[20221213 21:20:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 772.00
[32m[20221213 21:20:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.00
[32m[20221213 21:20:51 @agent_ppo2.py:143][0m Total time:      25.28 min
[32m[20221213 21:20:51 @agent_ppo2.py:145][0m 2480128 total steps have happened
[32m[20221213 21:20:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1211 --------------------------#
[32m[20221213 21:20:51 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:20:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:52 @agent_ppo2.py:185][0m |          -0.0014 |         147.8313 |          13.2862 |
[32m[20221213 21:20:52 @agent_ppo2.py:185][0m |          -0.0043 |         145.6741 |          13.2881 |
[32m[20221213 21:20:52 @agent_ppo2.py:185][0m |          -0.0072 |         144.8378 |          13.2816 |
[32m[20221213 21:20:52 @agent_ppo2.py:185][0m |          -0.0083 |         144.3403 |          13.3030 |
[32m[20221213 21:20:52 @agent_ppo2.py:185][0m |          -0.0065 |         143.9121 |          13.2979 |
[32m[20221213 21:20:52 @agent_ppo2.py:185][0m |          -0.0086 |         143.6710 |          13.2703 |
[32m[20221213 21:20:52 @agent_ppo2.py:185][0m |          -0.0004 |         151.2429 |          13.3029 |
[32m[20221213 21:20:52 @agent_ppo2.py:185][0m |          -0.0090 |         143.2363 |          13.2965 |
[32m[20221213 21:20:52 @agent_ppo2.py:185][0m |          -0.0073 |         143.1037 |          13.3054 |
[32m[20221213 21:20:52 @agent_ppo2.py:185][0m |          -0.0130 |         142.7560 |          13.2965 |
[32m[20221213 21:20:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 710.60
[32m[20221213 21:20:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.00
[32m[20221213 21:20:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 768.00
[32m[20221213 21:20:53 @agent_ppo2.py:143][0m Total time:      25.30 min
[32m[20221213 21:20:53 @agent_ppo2.py:145][0m 2482176 total steps have happened
[32m[20221213 21:20:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1212 --------------------------#
[32m[20221213 21:20:53 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:20:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:53 @agent_ppo2.py:185][0m |           0.0070 |         142.7491 |          13.3161 |
[32m[20221213 21:20:53 @agent_ppo2.py:185][0m |          -0.0025 |         130.4921 |          13.2856 |
[32m[20221213 21:20:53 @agent_ppo2.py:185][0m |          -0.0024 |         127.4586 |          13.2953 |
[32m[20221213 21:20:53 @agent_ppo2.py:185][0m |          -0.0058 |         125.6569 |          13.2769 |
[32m[20221213 21:20:53 @agent_ppo2.py:185][0m |          -0.0055 |         124.8018 |          13.3021 |
[32m[20221213 21:20:53 @agent_ppo2.py:185][0m |          -0.0057 |         124.1095 |          13.3274 |
[32m[20221213 21:20:53 @agent_ppo2.py:185][0m |          -0.0058 |         123.5867 |          13.3112 |
[32m[20221213 21:20:53 @agent_ppo2.py:185][0m |          -0.0025 |         123.4397 |          13.3248 |
[32m[20221213 21:20:54 @agent_ppo2.py:185][0m |          -0.0066 |         123.1574 |          13.3131 |
[32m[20221213 21:20:54 @agent_ppo2.py:185][0m |          -0.0072 |         122.7189 |          13.3339 |
[32m[20221213 21:20:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 758.20
[32m[20221213 21:20:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 777.00
[32m[20221213 21:20:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 755.00
[32m[20221213 21:20:54 @agent_ppo2.py:143][0m Total time:      25.32 min
[32m[20221213 21:20:54 @agent_ppo2.py:145][0m 2484224 total steps have happened
[32m[20221213 21:20:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1213 --------------------------#
[32m[20221213 21:20:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:54 @agent_ppo2.py:185][0m |           0.0024 |         158.9700 |          13.7748 |
[32m[20221213 21:20:54 @agent_ppo2.py:185][0m |           0.0070 |         173.8683 |          13.7383 |
[32m[20221213 21:20:54 @agent_ppo2.py:185][0m |          -0.0046 |         153.9036 |          13.7405 |
[32m[20221213 21:20:54 @agent_ppo2.py:185][0m |          -0.0071 |         153.3183 |          13.7466 |
[32m[20221213 21:20:54 @agent_ppo2.py:185][0m |          -0.0040 |         152.9485 |          13.7290 |
[32m[20221213 21:20:55 @agent_ppo2.py:185][0m |          -0.0059 |         152.6377 |          13.7520 |
[32m[20221213 21:20:55 @agent_ppo2.py:185][0m |          -0.0014 |         155.2409 |          13.7092 |
[32m[20221213 21:20:55 @agent_ppo2.py:185][0m |          -0.0091 |         152.0712 |          13.6831 |
[32m[20221213 21:20:55 @agent_ppo2.py:185][0m |          -0.0079 |         151.8080 |          13.7100 |
[32m[20221213 21:20:55 @agent_ppo2.py:185][0m |          -0.0098 |         151.7014 |          13.7205 |
[32m[20221213 21:20:55 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:20:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 704.20
[32m[20221213 21:20:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 742.00
[32m[20221213 21:20:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 781.00
[32m[20221213 21:20:55 @agent_ppo2.py:143][0m Total time:      25.34 min
[32m[20221213 21:20:55 @agent_ppo2.py:145][0m 2486272 total steps have happened
[32m[20221213 21:20:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1214 --------------------------#
[32m[20221213 21:20:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:55 @agent_ppo2.py:185][0m |           0.0070 |         154.9327 |          13.3247 |
[32m[20221213 21:20:55 @agent_ppo2.py:185][0m |          -0.0020 |         149.9742 |          13.3035 |
[32m[20221213 21:20:55 @agent_ppo2.py:185][0m |          -0.0049 |         148.5467 |          13.2972 |
[32m[20221213 21:20:56 @agent_ppo2.py:185][0m |          -0.0050 |         148.5857 |          13.2867 |
[32m[20221213 21:20:56 @agent_ppo2.py:185][0m |          -0.0078 |         147.5456 |          13.3021 |
[32m[20221213 21:20:56 @agent_ppo2.py:185][0m |           0.0168 |         181.2690 |          13.2410 |
[32m[20221213 21:20:56 @agent_ppo2.py:185][0m |           0.0032 |         153.6645 |          13.2737 |
[32m[20221213 21:20:56 @agent_ppo2.py:185][0m |          -0.0069 |         147.2841 |          13.2431 |
[32m[20221213 21:20:56 @agent_ppo2.py:185][0m |          -0.0107 |         146.7974 |          13.2098 |
[32m[20221213 21:20:56 @agent_ppo2.py:185][0m |          -0.0095 |         146.6904 |          13.2195 |
[32m[20221213 21:20:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 698.60
[32m[20221213 21:20:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 743.00
[32m[20221213 21:20:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.00
[32m[20221213 21:20:56 @agent_ppo2.py:143][0m Total time:      25.36 min
[32m[20221213 21:20:56 @agent_ppo2.py:145][0m 2488320 total steps have happened
[32m[20221213 21:20:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1215 --------------------------#
[32m[20221213 21:20:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:56 @agent_ppo2.py:185][0m |          -0.0001 |         149.6391 |          13.2013 |
[32m[20221213 21:20:57 @agent_ppo2.py:185][0m |          -0.0033 |         148.4214 |          13.2397 |
[32m[20221213 21:20:57 @agent_ppo2.py:185][0m |          -0.0057 |         147.5911 |          13.2693 |
[32m[20221213 21:20:57 @agent_ppo2.py:185][0m |           0.0005 |         150.2400 |          13.2664 |
[32m[20221213 21:20:57 @agent_ppo2.py:185][0m |          -0.0060 |         146.7935 |          13.2710 |
[32m[20221213 21:20:57 @agent_ppo2.py:185][0m |          -0.0067 |         146.4372 |          13.3183 |
[32m[20221213 21:20:57 @agent_ppo2.py:185][0m |          -0.0075 |         146.1894 |          13.2961 |
[32m[20221213 21:20:57 @agent_ppo2.py:185][0m |           0.0067 |         159.2707 |          13.3430 |
[32m[20221213 21:20:57 @agent_ppo2.py:185][0m |           0.0045 |         166.1947 |          13.3421 |
[32m[20221213 21:20:57 @agent_ppo2.py:185][0m |          -0.0002 |         153.9937 |          13.3323 |
[32m[20221213 21:20:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.40
[32m[20221213 21:20:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.00
[32m[20221213 21:20:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 801.00
[32m[20221213 21:20:57 @agent_ppo2.py:143][0m Total time:      25.38 min
[32m[20221213 21:20:57 @agent_ppo2.py:145][0m 2490368 total steps have happened
[32m[20221213 21:20:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1216 --------------------------#
[32m[20221213 21:20:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:58 @agent_ppo2.py:185][0m |          -0.0024 |         145.4232 |          13.4466 |
[32m[20221213 21:20:58 @agent_ppo2.py:185][0m |          -0.0052 |         144.4785 |          13.4256 |
[32m[20221213 21:20:58 @agent_ppo2.py:185][0m |          -0.0060 |         143.7990 |          13.4155 |
[32m[20221213 21:20:58 @agent_ppo2.py:185][0m |          -0.0052 |         143.3977 |          13.4233 |
[32m[20221213 21:20:58 @agent_ppo2.py:185][0m |          -0.0067 |         143.0845 |          13.3899 |
[32m[20221213 21:20:58 @agent_ppo2.py:185][0m |          -0.0081 |         142.8363 |          13.3971 |
[32m[20221213 21:20:58 @agent_ppo2.py:185][0m |          -0.0097 |         142.6045 |          13.4156 |
[32m[20221213 21:20:58 @agent_ppo2.py:185][0m |          -0.0030 |         144.3739 |          13.4098 |
[32m[20221213 21:20:58 @agent_ppo2.py:185][0m |          -0.0100 |         142.1680 |          13.3921 |
[32m[20221213 21:20:58 @agent_ppo2.py:185][0m |          -0.0084 |         142.0759 |          13.3795 |
[32m[20221213 21:20:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:20:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 728.20
[32m[20221213 21:20:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.00
[32m[20221213 21:20:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.00
[32m[20221213 21:20:59 @agent_ppo2.py:143][0m Total time:      25.40 min
[32m[20221213 21:20:59 @agent_ppo2.py:145][0m 2492416 total steps have happened
[32m[20221213 21:20:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1217 --------------------------#
[32m[20221213 21:20:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:20:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:20:59 @agent_ppo2.py:185][0m |          -0.0016 |         146.1126 |          13.1221 |
[32m[20221213 21:20:59 @agent_ppo2.py:185][0m |          -0.0038 |         145.5048 |          13.1274 |
[32m[20221213 21:20:59 @agent_ppo2.py:185][0m |          -0.0034 |         145.1023 |          13.1375 |
[32m[20221213 21:20:59 @agent_ppo2.py:185][0m |           0.0118 |         161.0081 |          13.1867 |
[32m[20221213 21:20:59 @agent_ppo2.py:185][0m |          -0.0005 |         144.3544 |          13.1720 |
[32m[20221213 21:20:59 @agent_ppo2.py:185][0m |          -0.0054 |         144.1119 |          13.1871 |
[32m[20221213 21:20:59 @agent_ppo2.py:185][0m |          -0.0042 |         143.9971 |          13.1932 |
[32m[20221213 21:20:59 @agent_ppo2.py:185][0m |           0.0019 |         149.4772 |          13.2214 |
[32m[20221213 21:21:00 @agent_ppo2.py:185][0m |          -0.0075 |         143.8564 |          13.2429 |
[32m[20221213 21:21:00 @agent_ppo2.py:185][0m |          -0.0066 |         143.4858 |          13.2523 |
[32m[20221213 21:21:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:21:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 741.00
[32m[20221213 21:21:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.00
[32m[20221213 21:21:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.00
[32m[20221213 21:21:00 @agent_ppo2.py:143][0m Total time:      25.42 min
[32m[20221213 21:21:00 @agent_ppo2.py:145][0m 2494464 total steps have happened
[32m[20221213 21:21:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1218 --------------------------#
[32m[20221213 21:21:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:00 @agent_ppo2.py:185][0m |          -0.0033 |         146.1800 |          13.4624 |
[32m[20221213 21:21:00 @agent_ppo2.py:185][0m |          -0.0034 |         145.6693 |          13.4330 |
[32m[20221213 21:21:00 @agent_ppo2.py:185][0m |          -0.0004 |         148.6445 |          13.4443 |
[32m[20221213 21:21:00 @agent_ppo2.py:185][0m |          -0.0052 |         145.1402 |          13.4425 |
[32m[20221213 21:21:00 @agent_ppo2.py:185][0m |          -0.0041 |         144.8809 |          13.4183 |
[32m[20221213 21:21:00 @agent_ppo2.py:185][0m |          -0.0061 |         144.8007 |          13.3854 |
[32m[20221213 21:21:01 @agent_ppo2.py:185][0m |          -0.0067 |         144.9054 |          13.3989 |
[32m[20221213 21:21:01 @agent_ppo2.py:185][0m |          -0.0072 |         144.4032 |          13.3815 |
[32m[20221213 21:21:01 @agent_ppo2.py:185][0m |          -0.0077 |         144.2348 |          13.3872 |
[32m[20221213 21:21:01 @agent_ppo2.py:185][0m |          -0.0067 |         144.3929 |          13.3989 |
[32m[20221213 21:21:01 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:21:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.60
[32m[20221213 21:21:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.00
[32m[20221213 21:21:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 785.00
[32m[20221213 21:21:01 @agent_ppo2.py:143][0m Total time:      25.44 min
[32m[20221213 21:21:01 @agent_ppo2.py:145][0m 2496512 total steps have happened
[32m[20221213 21:21:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1219 --------------------------#
[32m[20221213 21:21:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:21:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:01 @agent_ppo2.py:185][0m |          -0.0012 |         149.5828 |          13.4792 |
[32m[20221213 21:21:01 @agent_ppo2.py:185][0m |          -0.0046 |         148.4201 |          13.5251 |
[32m[20221213 21:21:01 @agent_ppo2.py:185][0m |          -0.0081 |         147.8769 |          13.5307 |
[32m[20221213 21:21:02 @agent_ppo2.py:185][0m |          -0.0070 |         147.3517 |          13.5285 |
[32m[20221213 21:21:02 @agent_ppo2.py:185][0m |          -0.0061 |         146.8906 |          13.5246 |
[32m[20221213 21:21:02 @agent_ppo2.py:185][0m |          -0.0073 |         146.6072 |          13.5132 |
[32m[20221213 21:21:02 @agent_ppo2.py:185][0m |          -0.0076 |         146.4016 |          13.4995 |
[32m[20221213 21:21:02 @agent_ppo2.py:185][0m |          -0.0054 |         146.4999 |          13.5187 |
[32m[20221213 21:21:02 @agent_ppo2.py:185][0m |          -0.0072 |         145.9261 |          13.5134 |
[32m[20221213 21:21:02 @agent_ppo2.py:185][0m |          -0.0088 |         146.0561 |          13.5346 |
[32m[20221213 21:21:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:21:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 706.00
[32m[20221213 21:21:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 753.00
[32m[20221213 21:21:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 733.00
[32m[20221213 21:21:02 @agent_ppo2.py:143][0m Total time:      25.46 min
[32m[20221213 21:21:02 @agent_ppo2.py:145][0m 2498560 total steps have happened
[32m[20221213 21:21:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1220 --------------------------#
[32m[20221213 21:21:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:02 @agent_ppo2.py:185][0m |          -0.0042 |         148.0585 |          13.2968 |
[32m[20221213 21:21:03 @agent_ppo2.py:185][0m |          -0.0065 |         146.3137 |          13.2595 |
[32m[20221213 21:21:03 @agent_ppo2.py:185][0m |          -0.0078 |         145.1576 |          13.2645 |
[32m[20221213 21:21:03 @agent_ppo2.py:185][0m |          -0.0091 |         144.4208 |          13.3085 |
[32m[20221213 21:21:03 @agent_ppo2.py:185][0m |          -0.0012 |         154.7608 |          13.3003 |
[32m[20221213 21:21:03 @agent_ppo2.py:185][0m |          -0.0099 |         143.5773 |          13.3082 |
[32m[20221213 21:21:03 @agent_ppo2.py:185][0m |          -0.0091 |         143.2393 |          13.3240 |
[32m[20221213 21:21:03 @agent_ppo2.py:185][0m |          -0.0096 |         142.9458 |          13.3067 |
[32m[20221213 21:21:03 @agent_ppo2.py:185][0m |          -0.0105 |         142.6912 |          13.2794 |
[32m[20221213 21:21:03 @agent_ppo2.py:185][0m |          -0.0120 |         142.4021 |          13.2699 |
[32m[20221213 21:21:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:21:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.20
[32m[20221213 21:21:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.00
[32m[20221213 21:21:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 748.00
[32m[20221213 21:21:03 @agent_ppo2.py:143][0m Total time:      25.48 min
[32m[20221213 21:21:03 @agent_ppo2.py:145][0m 2500608 total steps have happened
[32m[20221213 21:21:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1221 --------------------------#
[32m[20221213 21:21:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:04 @agent_ppo2.py:185][0m |          -0.0002 |         151.2942 |          13.6351 |
[32m[20221213 21:21:04 @agent_ppo2.py:185][0m |          -0.0038 |         148.7508 |          13.6376 |
[32m[20221213 21:21:04 @agent_ppo2.py:185][0m |          -0.0025 |         148.0235 |          13.6258 |
[32m[20221213 21:21:04 @agent_ppo2.py:185][0m |          -0.0060 |         146.7469 |          13.6066 |
[32m[20221213 21:21:04 @agent_ppo2.py:185][0m |          -0.0058 |         146.2751 |          13.5909 |
[32m[20221213 21:21:04 @agent_ppo2.py:185][0m |          -0.0070 |         145.3559 |          13.6138 |
[32m[20221213 21:21:04 @agent_ppo2.py:185][0m |          -0.0072 |         144.9298 |          13.6069 |
[32m[20221213 21:21:04 @agent_ppo2.py:185][0m |          -0.0087 |         144.2823 |          13.5896 |
[32m[20221213 21:21:04 @agent_ppo2.py:185][0m |          -0.0078 |         143.8656 |          13.6050 |
[32m[20221213 21:21:04 @agent_ppo2.py:185][0m |          -0.0098 |         143.6999 |          13.6110 |
[32m[20221213 21:21:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:21:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 717.60
[32m[20221213 21:21:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 743.00
[32m[20221213 21:21:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.00
[32m[20221213 21:21:05 @agent_ppo2.py:143][0m Total time:      25.50 min
[32m[20221213 21:21:05 @agent_ppo2.py:145][0m 2502656 total steps have happened
[32m[20221213 21:21:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1222 --------------------------#
[32m[20221213 21:21:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:21:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:05 @agent_ppo2.py:185][0m |          -0.0010 |         150.4409 |          13.1490 |
[32m[20221213 21:21:05 @agent_ppo2.py:185][0m |          -0.0008 |         151.8948 |          13.1801 |
[32m[20221213 21:21:05 @agent_ppo2.py:185][0m |          -0.0053 |         147.9875 |          13.2002 |
[32m[20221213 21:21:05 @agent_ppo2.py:185][0m |           0.0165 |         174.9306 |          13.2053 |
[32m[20221213 21:21:05 @agent_ppo2.py:185][0m |           0.0044 |         166.7525 |          13.2100 |
[32m[20221213 21:21:05 @agent_ppo2.py:185][0m |           0.0132 |         166.7822 |          13.2190 |
[32m[20221213 21:21:05 @agent_ppo2.py:185][0m |          -0.0075 |         146.2692 |          13.2642 |
[32m[20221213 21:21:05 @agent_ppo2.py:185][0m |          -0.0035 |         148.7122 |          13.2652 |
[32m[20221213 21:21:06 @agent_ppo2.py:185][0m |          -0.0073 |         145.5986 |          13.2917 |
[32m[20221213 21:21:06 @agent_ppo2.py:185][0m |          -0.0001 |         154.4666 |          13.3055 |
[32m[20221213 21:21:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:21:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 716.20
[32m[20221213 21:21:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.00
[32m[20221213 21:21:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 761.00
[32m[20221213 21:21:06 @agent_ppo2.py:143][0m Total time:      25.52 min
[32m[20221213 21:21:06 @agent_ppo2.py:145][0m 2504704 total steps have happened
[32m[20221213 21:21:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1223 --------------------------#
[32m[20221213 21:21:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:06 @agent_ppo2.py:185][0m |           0.0048 |         162.6176 |          13.5489 |
[32m[20221213 21:21:06 @agent_ppo2.py:185][0m |          -0.0052 |         150.1921 |          13.5616 |
[32m[20221213 21:21:06 @agent_ppo2.py:185][0m |          -0.0063 |         149.4622 |          13.5775 |
[32m[20221213 21:21:06 @agent_ppo2.py:185][0m |          -0.0080 |         148.3475 |          13.5952 |
[32m[20221213 21:21:06 @agent_ppo2.py:185][0m |          -0.0083 |         147.8458 |          13.6100 |
[32m[20221213 21:21:06 @agent_ppo2.py:185][0m |          -0.0082 |         147.4913 |          13.6337 |
[32m[20221213 21:21:07 @agent_ppo2.py:185][0m |          -0.0086 |         147.1672 |          13.6459 |
[32m[20221213 21:21:07 @agent_ppo2.py:185][0m |          -0.0046 |         150.7157 |          13.6574 |
[32m[20221213 21:21:07 @agent_ppo2.py:185][0m |          -0.0098 |         146.6428 |          13.6643 |
[32m[20221213 21:21:07 @agent_ppo2.py:185][0m |          -0.0091 |         146.5269 |          13.6701 |
[32m[20221213 21:21:07 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:21:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 714.60
[32m[20221213 21:21:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.00
[32m[20221213 21:21:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 735.00
[32m[20221213 21:21:07 @agent_ppo2.py:143][0m Total time:      25.54 min
[32m[20221213 21:21:07 @agent_ppo2.py:145][0m 2506752 total steps have happened
[32m[20221213 21:21:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1224 --------------------------#
[32m[20221213 21:21:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:07 @agent_ppo2.py:185][0m |          -0.0047 |         146.5482 |          13.5057 |
[32m[20221213 21:21:07 @agent_ppo2.py:185][0m |           0.0084 |         148.5508 |          13.5061 |
[32m[20221213 21:21:07 @agent_ppo2.py:185][0m |          -0.0029 |         144.6559 |          13.5149 |
[32m[20221213 21:21:08 @agent_ppo2.py:185][0m |          -0.0054 |         144.2194 |          13.5281 |
[32m[20221213 21:21:08 @agent_ppo2.py:185][0m |          -0.0050 |         144.0908 |          13.5483 |
[32m[20221213 21:21:08 @agent_ppo2.py:185][0m |          -0.0076 |         143.7594 |          13.5374 |
[32m[20221213 21:21:08 @agent_ppo2.py:185][0m |          -0.0071 |         143.6307 |          13.5979 |
[32m[20221213 21:21:08 @agent_ppo2.py:185][0m |          -0.0063 |         143.3565 |          13.5944 |
[32m[20221213 21:21:08 @agent_ppo2.py:185][0m |          -0.0090 |         143.2441 |          13.5699 |
[32m[20221213 21:21:08 @agent_ppo2.py:185][0m |          -0.0073 |         143.2049 |          13.5959 |
[32m[20221213 21:21:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:21:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 734.20
[32m[20221213 21:21:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.00
[32m[20221213 21:21:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.00
[32m[20221213 21:21:08 @agent_ppo2.py:143][0m Total time:      25.56 min
[32m[20221213 21:21:08 @agent_ppo2.py:145][0m 2508800 total steps have happened
[32m[20221213 21:21:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1225 --------------------------#
[32m[20221213 21:21:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:08 @agent_ppo2.py:185][0m |          -0.0027 |         149.2936 |          13.9091 |
[32m[20221213 21:21:09 @agent_ppo2.py:185][0m |          -0.0094 |         147.8676 |          13.8709 |
[32m[20221213 21:21:09 @agent_ppo2.py:185][0m |          -0.0060 |         146.8404 |          13.8858 |
[32m[20221213 21:21:09 @agent_ppo2.py:185][0m |          -0.0051 |         147.7400 |          13.8872 |
[32m[20221213 21:21:09 @agent_ppo2.py:185][0m |          -0.0093 |         145.4969 |          13.8837 |
[32m[20221213 21:21:09 @agent_ppo2.py:185][0m |          -0.0123 |         145.3155 |          13.8819 |
[32m[20221213 21:21:09 @agent_ppo2.py:185][0m |          -0.0087 |         144.9216 |          13.9159 |
[32m[20221213 21:21:09 @agent_ppo2.py:185][0m |          -0.0082 |         144.6877 |          13.8689 |
[32m[20221213 21:21:09 @agent_ppo2.py:185][0m |          -0.0102 |         144.5610 |          13.8436 |
[32m[20221213 21:21:09 @agent_ppo2.py:185][0m |          -0.0109 |         144.4299 |          13.8683 |
[32m[20221213 21:21:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:21:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 728.40
[32m[20221213 21:21:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.00
[32m[20221213 21:21:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.00
[32m[20221213 21:21:09 @agent_ppo2.py:143][0m Total time:      25.58 min
[32m[20221213 21:21:09 @agent_ppo2.py:145][0m 2510848 total steps have happened
[32m[20221213 21:21:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1226 --------------------------#
[32m[20221213 21:21:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:10 @agent_ppo2.py:185][0m |           0.0042 |         147.2563 |          13.5768 |
[32m[20221213 21:21:10 @agent_ppo2.py:185][0m |          -0.0036 |         145.9075 |          13.5338 |
[32m[20221213 21:21:10 @agent_ppo2.py:185][0m |          -0.0044 |         145.0335 |          13.5037 |
[32m[20221213 21:21:10 @agent_ppo2.py:185][0m |          -0.0046 |         144.7949 |          13.5138 |
[32m[20221213 21:21:10 @agent_ppo2.py:185][0m |          -0.0084 |         144.3490 |          13.4849 |
[32m[20221213 21:21:10 @agent_ppo2.py:185][0m |          -0.0061 |         143.9743 |          13.4439 |
[32m[20221213 21:21:10 @agent_ppo2.py:185][0m |          -0.0077 |         143.7992 |          13.4332 |
[32m[20221213 21:21:10 @agent_ppo2.py:185][0m |          -0.0071 |         143.4558 |          13.4282 |
[32m[20221213 21:21:10 @agent_ppo2.py:185][0m |          -0.0082 |         143.3478 |          13.4215 |
[32m[20221213 21:21:10 @agent_ppo2.py:185][0m |          -0.0092 |         142.9543 |          13.4295 |
[32m[20221213 21:21:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:21:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.00
[32m[20221213 21:21:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 772.00
[32m[20221213 21:21:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.00
[32m[20221213 21:21:11 @agent_ppo2.py:143][0m Total time:      25.60 min
[32m[20221213 21:21:11 @agent_ppo2.py:145][0m 2512896 total steps have happened
[32m[20221213 21:21:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1227 --------------------------#
[32m[20221213 21:21:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:11 @agent_ppo2.py:185][0m |          -0.0009 |         148.8391 |          13.8253 |
[32m[20221213 21:21:11 @agent_ppo2.py:185][0m |          -0.0045 |         147.6207 |          13.8080 |
[32m[20221213 21:21:11 @agent_ppo2.py:185][0m |          -0.0036 |         147.0093 |          13.8205 |
[32m[20221213 21:21:11 @agent_ppo2.py:185][0m |          -0.0057 |         146.3456 |          13.8244 |
[32m[20221213 21:21:11 @agent_ppo2.py:185][0m |          -0.0071 |         145.8944 |          13.8024 |
[32m[20221213 21:21:11 @agent_ppo2.py:185][0m |          -0.0050 |         146.7528 |          13.8061 |
[32m[20221213 21:21:11 @agent_ppo2.py:185][0m |          -0.0063 |         145.1328 |          13.8088 |
[32m[20221213 21:21:11 @agent_ppo2.py:185][0m |          -0.0078 |         145.1189 |          13.8066 |
[32m[20221213 21:21:12 @agent_ppo2.py:185][0m |           0.0169 |         178.2392 |          13.7818 |
[32m[20221213 21:21:12 @agent_ppo2.py:185][0m |          -0.0025 |         145.8665 |          13.7507 |
[32m[20221213 21:21:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:21:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 745.20
[32m[20221213 21:21:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 764.00
[32m[20221213 21:21:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.00
[32m[20221213 21:21:12 @agent_ppo2.py:143][0m Total time:      25.62 min
[32m[20221213 21:21:12 @agent_ppo2.py:145][0m 2514944 total steps have happened
[32m[20221213 21:21:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1228 --------------------------#
[32m[20221213 21:21:12 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:21:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:12 @agent_ppo2.py:185][0m |           0.0008 |         150.5172 |          13.4911 |
[32m[20221213 21:21:12 @agent_ppo2.py:185][0m |           0.0012 |         149.2495 |          13.4621 |
[32m[20221213 21:21:12 @agent_ppo2.py:185][0m |          -0.0039 |         147.2363 |          13.4126 |
[32m[20221213 21:21:12 @agent_ppo2.py:185][0m |           0.0093 |         154.7762 |          13.4320 |
[32m[20221213 21:21:12 @agent_ppo2.py:185][0m |          -0.0027 |         146.0933 |          13.4591 |
[32m[20221213 21:21:12 @agent_ppo2.py:185][0m |          -0.0029 |         146.3556 |          13.4268 |
[32m[20221213 21:21:13 @agent_ppo2.py:185][0m |          -0.0054 |         145.4842 |          13.4140 |
[32m[20221213 21:21:13 @agent_ppo2.py:185][0m |           0.0036 |         155.9787 |          13.3894 |
[32m[20221213 21:21:13 @agent_ppo2.py:185][0m |          -0.0064 |         144.8377 |          13.4210 |
[32m[20221213 21:21:13 @agent_ppo2.py:185][0m |          -0.0034 |         144.9718 |          13.4204 |
[32m[20221213 21:21:13 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:21:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.20
[32m[20221213 21:21:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 740.00
[32m[20221213 21:21:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 747.00
[32m[20221213 21:21:13 @agent_ppo2.py:143][0m Total time:      25.64 min
[32m[20221213 21:21:13 @agent_ppo2.py:145][0m 2516992 total steps have happened
[32m[20221213 21:21:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1229 --------------------------#
[32m[20221213 21:21:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:13 @agent_ppo2.py:185][0m |           0.0004 |         148.7910 |          13.3378 |
[32m[20221213 21:21:13 @agent_ppo2.py:185][0m |          -0.0032 |         146.8747 |          13.3588 |
[32m[20221213 21:21:13 @agent_ppo2.py:185][0m |          -0.0048 |         145.9936 |          13.3630 |
[32m[20221213 21:21:14 @agent_ppo2.py:185][0m |          -0.0083 |         145.5838 |          13.3890 |
[32m[20221213 21:21:14 @agent_ppo2.py:185][0m |          -0.0056 |         144.8899 |          13.3726 |
[32m[20221213 21:21:14 @agent_ppo2.py:185][0m |          -0.0036 |         146.9979 |          13.4040 |
[32m[20221213 21:21:14 @agent_ppo2.py:185][0m |          -0.0064 |         144.0659 |          13.4190 |
[32m[20221213 21:21:14 @agent_ppo2.py:185][0m |          -0.0042 |         147.8970 |          13.4505 |
[32m[20221213 21:21:14 @agent_ppo2.py:185][0m |          -0.0085 |         143.6375 |          13.4254 |
[32m[20221213 21:21:14 @agent_ppo2.py:185][0m |          -0.0098 |         143.3850 |          13.4510 |
[32m[20221213 21:21:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:21:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 729.60
[32m[20221213 21:21:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.00
[32m[20221213 21:21:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.00
[32m[20221213 21:21:14 @agent_ppo2.py:143][0m Total time:      25.66 min
[32m[20221213 21:21:14 @agent_ppo2.py:145][0m 2519040 total steps have happened
[32m[20221213 21:21:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1230 --------------------------#
[32m[20221213 21:21:14 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:21:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:14 @agent_ppo2.py:185][0m |           0.0056 |         152.7945 |          13.4525 |
[32m[20221213 21:21:15 @agent_ppo2.py:185][0m |          -0.0030 |         149.1127 |          13.4714 |
[32m[20221213 21:21:15 @agent_ppo2.py:185][0m |           0.0024 |         151.9606 |          13.4887 |
[32m[20221213 21:21:15 @agent_ppo2.py:185][0m |          -0.0049 |         147.6404 |          13.4755 |
[32m[20221213 21:21:15 @agent_ppo2.py:185][0m |          -0.0070 |         147.0956 |          13.4767 |
[32m[20221213 21:21:15 @agent_ppo2.py:185][0m |          -0.0089 |         146.5617 |          13.4885 |
[32m[20221213 21:21:15 @agent_ppo2.py:185][0m |           0.0031 |         159.7165 |          13.5097 |
[32m[20221213 21:21:15 @agent_ppo2.py:185][0m |          -0.0087 |         145.8852 |          13.5096 |
[32m[20221213 21:21:15 @agent_ppo2.py:185][0m |          -0.0086 |         145.2536 |          13.5205 |
[32m[20221213 21:21:15 @agent_ppo2.py:185][0m |          -0.0105 |         144.8630 |          13.5348 |
[32m[20221213 21:21:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:21:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 744.60
[32m[20221213 21:21:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.00
[32m[20221213 21:21:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 758.00
[32m[20221213 21:21:15 @agent_ppo2.py:143][0m Total time:      25.68 min
[32m[20221213 21:21:15 @agent_ppo2.py:145][0m 2521088 total steps have happened
[32m[20221213 21:21:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1231 --------------------------#
[32m[20221213 21:21:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:16 @agent_ppo2.py:185][0m |          -0.0026 |         152.8292 |          13.5082 |
[32m[20221213 21:21:16 @agent_ppo2.py:185][0m |          -0.0014 |         151.8079 |          13.5543 |
[32m[20221213 21:21:16 @agent_ppo2.py:185][0m |          -0.0055 |         151.3490 |          13.5309 |
[32m[20221213 21:21:16 @agent_ppo2.py:185][0m |          -0.0049 |         151.2743 |          13.5542 |
[32m[20221213 21:21:16 @agent_ppo2.py:185][0m |           0.0080 |         164.7819 |          13.5717 |
[32m[20221213 21:21:16 @agent_ppo2.py:185][0m |          -0.0073 |         150.7339 |          13.5680 |
[32m[20221213 21:21:16 @agent_ppo2.py:185][0m |          -0.0067 |         150.4886 |          13.5954 |
[32m[20221213 21:21:16 @agent_ppo2.py:185][0m |           0.0043 |         162.7450 |          13.5971 |
[32m[20221213 21:21:16 @agent_ppo2.py:185][0m |          -0.0057 |         150.5451 |          13.5768 |
[32m[20221213 21:21:16 @agent_ppo2.py:185][0m |          -0.0056 |         150.8477 |          13.6334 |
[32m[20221213 21:21:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:21:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 740.00
[32m[20221213 21:21:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:21:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.00
[32m[20221213 21:21:17 @agent_ppo2.py:143][0m Total time:      25.70 min
[32m[20221213 21:21:17 @agent_ppo2.py:145][0m 2523136 total steps have happened
[32m[20221213 21:21:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1232 --------------------------#
[32m[20221213 21:21:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:21:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:17 @agent_ppo2.py:185][0m |          -0.0025 |         148.5282 |          13.9381 |
[32m[20221213 21:21:17 @agent_ppo2.py:185][0m |          -0.0074 |         147.5125 |          13.9071 |
[32m[20221213 21:21:17 @agent_ppo2.py:185][0m |          -0.0073 |         147.1588 |          13.9251 |
[32m[20221213 21:21:17 @agent_ppo2.py:185][0m |          -0.0095 |         146.7083 |          13.9267 |
[32m[20221213 21:21:17 @agent_ppo2.py:185][0m |          -0.0088 |         146.4421 |          13.8936 |
[32m[20221213 21:21:17 @agent_ppo2.py:185][0m |          -0.0084 |         146.0205 |          13.9096 |
[32m[20221213 21:21:17 @agent_ppo2.py:185][0m |          -0.0089 |         145.8831 |          13.8905 |
[32m[20221213 21:21:17 @agent_ppo2.py:185][0m |          -0.0020 |         152.9445 |          13.9063 |
[32m[20221213 21:21:18 @agent_ppo2.py:185][0m |          -0.0090 |         145.4910 |          13.9059 |
[32m[20221213 21:21:18 @agent_ppo2.py:185][0m |          -0.0081 |         145.6544 |          13.9160 |
[32m[20221213 21:21:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:21:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 746.40
[32m[20221213 21:21:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.00
[32m[20221213 21:21:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 722.00
[32m[20221213 21:21:18 @agent_ppo2.py:143][0m Total time:      25.72 min
[32m[20221213 21:21:18 @agent_ppo2.py:145][0m 2525184 total steps have happened
[32m[20221213 21:21:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1233 --------------------------#
[32m[20221213 21:21:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:18 @agent_ppo2.py:185][0m |          -0.0001 |         151.1638 |          13.8211 |
[32m[20221213 21:21:18 @agent_ppo2.py:185][0m |          -0.0023 |         150.0680 |          13.7604 |
[32m[20221213 21:21:18 @agent_ppo2.py:185][0m |          -0.0058 |         149.4133 |          13.7249 |
[32m[20221213 21:21:18 @agent_ppo2.py:185][0m |          -0.0037 |         148.9556 |          13.7348 |
[32m[20221213 21:21:18 @agent_ppo2.py:185][0m |          -0.0069 |         148.6380 |          13.7414 |
[32m[20221213 21:21:19 @agent_ppo2.py:185][0m |          -0.0052 |         148.5370 |          13.7338 |
[32m[20221213 21:21:19 @agent_ppo2.py:185][0m |          -0.0087 |         148.1061 |          13.7067 |
[32m[20221213 21:21:19 @agent_ppo2.py:185][0m |          -0.0105 |         147.8898 |          13.6977 |
[32m[20221213 21:21:19 @agent_ppo2.py:185][0m |          -0.0042 |         150.4463 |          13.6786 |
[32m[20221213 21:21:19 @agent_ppo2.py:185][0m |          -0.0082 |         147.5222 |          13.6746 |
[32m[20221213 21:21:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:21:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 770.40
[32m[20221213 21:21:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.00
[32m[20221213 21:21:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.00
[32m[20221213 21:21:19 @agent_ppo2.py:143][0m Total time:      25.74 min
[32m[20221213 21:21:19 @agent_ppo2.py:145][0m 2527232 total steps have happened
[32m[20221213 21:21:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1234 --------------------------#
[32m[20221213 21:21:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:19 @agent_ppo2.py:185][0m |           0.0000 |         151.1878 |          13.5246 |
[32m[20221213 21:21:19 @agent_ppo2.py:185][0m |           0.0092 |         162.4998 |          13.5307 |
[32m[20221213 21:21:19 @agent_ppo2.py:185][0m |          -0.0040 |         149.9081 |          13.5395 |
[32m[20221213 21:21:20 @agent_ppo2.py:185][0m |          -0.0074 |         149.4192 |          13.5529 |
[32m[20221213 21:21:20 @agent_ppo2.py:185][0m |          -0.0057 |         149.0898 |          13.5514 |
[32m[20221213 21:21:20 @agent_ppo2.py:185][0m |          -0.0075 |         148.9379 |          13.5605 |
[32m[20221213 21:21:20 @agent_ppo2.py:185][0m |          -0.0067 |         148.8368 |          13.5461 |
[32m[20221213 21:21:20 @agent_ppo2.py:185][0m |           0.0058 |         167.9413 |          13.5431 |
[32m[20221213 21:21:20 @agent_ppo2.py:185][0m |          -0.0107 |         148.4243 |          13.5494 |
[32m[20221213 21:21:20 @agent_ppo2.py:185][0m |          -0.0090 |         148.2415 |          13.5584 |
[32m[20221213 21:21:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:21:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.60
[32m[20221213 21:21:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.00
[32m[20221213 21:21:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.00
[32m[20221213 21:21:20 @agent_ppo2.py:143][0m Total time:      25.76 min
[32m[20221213 21:21:20 @agent_ppo2.py:145][0m 2529280 total steps have happened
[32m[20221213 21:21:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1235 --------------------------#
[32m[20221213 21:21:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:21:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:20 @agent_ppo2.py:185][0m |          -0.0022 |         155.0609 |          13.4369 |
[32m[20221213 21:21:21 @agent_ppo2.py:185][0m |          -0.0033 |         153.8563 |          13.4129 |
[32m[20221213 21:21:21 @agent_ppo2.py:185][0m |          -0.0038 |         153.2636 |          13.3967 |
[32m[20221213 21:21:21 @agent_ppo2.py:185][0m |          -0.0058 |         152.4498 |          13.3769 |
[32m[20221213 21:21:21 @agent_ppo2.py:185][0m |          -0.0010 |         154.3495 |          13.3987 |
[32m[20221213 21:21:21 @agent_ppo2.py:185][0m |          -0.0027 |         153.5393 |          13.3478 |
[32m[20221213 21:21:21 @agent_ppo2.py:185][0m |          -0.0079 |         151.4582 |          13.3593 |
[32m[20221213 21:21:21 @agent_ppo2.py:185][0m |          -0.0023 |         157.1234 |          13.3207 |
[32m[20221213 21:21:21 @agent_ppo2.py:185][0m |          -0.0074 |         150.9082 |          13.3211 |
[32m[20221213 21:21:21 @agent_ppo2.py:185][0m |           0.0017 |         165.8501 |          13.2892 |
[32m[20221213 21:21:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:21:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 760.00
[32m[20221213 21:21:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.00
[32m[20221213 21:21:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.00
[32m[20221213 21:21:21 @agent_ppo2.py:143][0m Total time:      25.78 min
[32m[20221213 21:21:21 @agent_ppo2.py:145][0m 2531328 total steps have happened
[32m[20221213 21:21:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1236 --------------------------#
[32m[20221213 21:21:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:22 @agent_ppo2.py:185][0m |           0.0136 |         175.3527 |          13.3254 |
[32m[20221213 21:21:22 @agent_ppo2.py:185][0m |          -0.0044 |         152.8996 |          13.2614 |
[32m[20221213 21:21:22 @agent_ppo2.py:185][0m |          -0.0063 |         152.3661 |          13.2726 |
[32m[20221213 21:21:22 @agent_ppo2.py:185][0m |          -0.0072 |         151.4956 |          13.2535 |
[32m[20221213 21:21:22 @agent_ppo2.py:185][0m |          -0.0088 |         151.2487 |          13.2753 |
[32m[20221213 21:21:22 @agent_ppo2.py:185][0m |          -0.0116 |         151.0955 |          13.2344 |
[32m[20221213 21:21:22 @agent_ppo2.py:185][0m |          -0.0093 |         150.7486 |          13.2477 |
[32m[20221213 21:21:22 @agent_ppo2.py:185][0m |          -0.0053 |         151.8776 |          13.2418 |
[32m[20221213 21:21:22 @agent_ppo2.py:185][0m |          -0.0077 |         150.3381 |          13.2478 |
[32m[20221213 21:21:22 @agent_ppo2.py:185][0m |          -0.0018 |         156.9487 |          13.2238 |
[32m[20221213 21:21:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:21:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.80
[32m[20221213 21:21:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.00
[32m[20221213 21:21:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.00
[32m[20221213 21:21:23 @agent_ppo2.py:143][0m Total time:      25.80 min
[32m[20221213 21:21:23 @agent_ppo2.py:145][0m 2533376 total steps have happened
[32m[20221213 21:21:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1237 --------------------------#
[32m[20221213 21:21:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:23 @agent_ppo2.py:185][0m |          -0.0008 |         153.2771 |          13.3739 |
[32m[20221213 21:21:23 @agent_ppo2.py:185][0m |          -0.0041 |         152.0726 |          13.3344 |
[32m[20221213 21:21:23 @agent_ppo2.py:185][0m |           0.0015 |         154.3111 |          13.3332 |
[32m[20221213 21:21:23 @agent_ppo2.py:185][0m |          -0.0026 |         150.6732 |          13.2980 |
[32m[20221213 21:21:23 @agent_ppo2.py:185][0m |          -0.0052 |         150.2954 |          13.2844 |
[32m[20221213 21:21:23 @agent_ppo2.py:185][0m |          -0.0060 |         149.9394 |          13.2824 |
[32m[20221213 21:21:23 @agent_ppo2.py:185][0m |          -0.0062 |         149.4564 |          13.2859 |
[32m[20221213 21:21:23 @agent_ppo2.py:185][0m |          -0.0066 |         149.2175 |          13.2676 |
[32m[20221213 21:21:24 @agent_ppo2.py:185][0m |          -0.0084 |         148.7422 |          13.2670 |
[32m[20221213 21:21:24 @agent_ppo2.py:185][0m |          -0.0072 |         148.5302 |          13.2574 |
[32m[20221213 21:21:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:21:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 727.40
[32m[20221213 21:21:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.00
[32m[20221213 21:21:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.00
[32m[20221213 21:21:24 @agent_ppo2.py:143][0m Total time:      25.82 min
[32m[20221213 21:21:24 @agent_ppo2.py:145][0m 2535424 total steps have happened
[32m[20221213 21:21:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1238 --------------------------#
[32m[20221213 21:21:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:24 @agent_ppo2.py:185][0m |           0.0005 |         156.8239 |          13.0739 |
[32m[20221213 21:21:24 @agent_ppo2.py:185][0m |          -0.0044 |         154.3397 |          13.0954 |
[32m[20221213 21:21:24 @agent_ppo2.py:185][0m |          -0.0038 |         153.0488 |          13.0663 |
[32m[20221213 21:21:24 @agent_ppo2.py:185][0m |          -0.0075 |         152.5205 |          13.0474 |
[32m[20221213 21:21:24 @agent_ppo2.py:185][0m |          -0.0103 |         151.8277 |          13.0391 |
[32m[20221213 21:21:24 @agent_ppo2.py:185][0m |          -0.0055 |         152.8455 |          13.0127 |
[32m[20221213 21:21:25 @agent_ppo2.py:185][0m |          -0.0092 |         151.3014 |          13.0313 |
[32m[20221213 21:21:25 @agent_ppo2.py:185][0m |          -0.0090 |         150.9359 |          12.9827 |
[32m[20221213 21:21:25 @agent_ppo2.py:185][0m |          -0.0119 |         150.6375 |          12.9909 |
[32m[20221213 21:21:25 @agent_ppo2.py:185][0m |          -0.0124 |         150.3301 |          12.9733 |
[32m[20221213 21:21:25 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:21:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 730.00
[32m[20221213 21:21:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.00
[32m[20221213 21:21:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.00
[32m[20221213 21:21:25 @agent_ppo2.py:143][0m Total time:      25.84 min
[32m[20221213 21:21:25 @agent_ppo2.py:145][0m 2537472 total steps have happened
[32m[20221213 21:21:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1239 --------------------------#
[32m[20221213 21:21:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:25 @agent_ppo2.py:185][0m |           0.0033 |         158.0405 |          13.0694 |
[32m[20221213 21:21:25 @agent_ppo2.py:185][0m |          -0.0015 |         155.1817 |          13.1310 |
[32m[20221213 21:21:25 @agent_ppo2.py:185][0m |          -0.0042 |         154.3640 |          13.1107 |
[32m[20221213 21:21:26 @agent_ppo2.py:185][0m |          -0.0050 |         153.7644 |          13.1288 |
[32m[20221213 21:21:26 @agent_ppo2.py:185][0m |          -0.0054 |         153.5588 |          13.1252 |
[32m[20221213 21:21:26 @agent_ppo2.py:185][0m |           0.0025 |         161.5373 |          13.1498 |
[32m[20221213 21:21:26 @agent_ppo2.py:185][0m |          -0.0071 |         153.0012 |          13.1075 |
[32m[20221213 21:21:26 @agent_ppo2.py:185][0m |          -0.0083 |         152.5935 |          13.1754 |
[32m[20221213 21:21:26 @agent_ppo2.py:185][0m |          -0.0071 |         152.2863 |          13.1325 |
[32m[20221213 21:21:26 @agent_ppo2.py:185][0m |          -0.0087 |         152.1402 |          13.1477 |
[32m[20221213 21:21:26 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:21:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 721.20
[32m[20221213 21:21:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 746.00
[32m[20221213 21:21:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 21:21:26 @agent_ppo2.py:143][0m Total time:      25.86 min
[32m[20221213 21:21:26 @agent_ppo2.py:145][0m 2539520 total steps have happened
[32m[20221213 21:21:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1240 --------------------------#
[32m[20221213 21:21:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:27 @agent_ppo2.py:185][0m |          -0.0011 |         156.9221 |          12.9986 |
[32m[20221213 21:21:27 @agent_ppo2.py:185][0m |          -0.0045 |         156.0740 |          12.9662 |
[32m[20221213 21:21:27 @agent_ppo2.py:185][0m |          -0.0031 |         155.6164 |          12.9307 |
[32m[20221213 21:21:27 @agent_ppo2.py:185][0m |           0.0059 |         172.5256 |          12.9032 |
[32m[20221213 21:21:27 @agent_ppo2.py:185][0m |          -0.0054 |         154.8491 |          12.8834 |
[32m[20221213 21:21:27 @agent_ppo2.py:185][0m |          -0.0064 |         154.7882 |          12.8622 |
[32m[20221213 21:21:27 @agent_ppo2.py:185][0m |          -0.0083 |         154.5614 |          12.8830 |
[32m[20221213 21:21:27 @agent_ppo2.py:185][0m |          -0.0053 |         154.5359 |          12.8370 |
[32m[20221213 21:21:27 @agent_ppo2.py:185][0m |          -0.0084 |         154.2087 |          12.8583 |
[32m[20221213 21:21:27 @agent_ppo2.py:185][0m |          -0.0067 |         153.9967 |          12.8480 |
[32m[20221213 21:21:27 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:21:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.40
[32m[20221213 21:21:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:21:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.00
[32m[20221213 21:21:27 @agent_ppo2.py:143][0m Total time:      25.88 min
[32m[20221213 21:21:27 @agent_ppo2.py:145][0m 2541568 total steps have happened
[32m[20221213 21:21:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1241 --------------------------#
[32m[20221213 21:21:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:28 @agent_ppo2.py:185][0m |          -0.0007 |         157.4981 |          12.9079 |
[32m[20221213 21:21:28 @agent_ppo2.py:185][0m |          -0.0026 |         156.9435 |          12.8827 |
[32m[20221213 21:21:28 @agent_ppo2.py:185][0m |          -0.0034 |         155.3030 |          12.8776 |
[32m[20221213 21:21:28 @agent_ppo2.py:185][0m |          -0.0066 |         154.9034 |          12.8631 |
[32m[20221213 21:21:28 @agent_ppo2.py:185][0m |          -0.0081 |         154.4685 |          12.8708 |
[32m[20221213 21:21:28 @agent_ppo2.py:185][0m |          -0.0041 |         154.1390 |          12.8466 |
[32m[20221213 21:21:28 @agent_ppo2.py:185][0m |          -0.0049 |         153.9405 |          12.8563 |
[32m[20221213 21:21:28 @agent_ppo2.py:185][0m |          -0.0065 |         153.6480 |          12.8437 |
[32m[20221213 21:21:28 @agent_ppo2.py:185][0m |          -0.0068 |         153.4751 |          12.8475 |
[32m[20221213 21:21:28 @agent_ppo2.py:185][0m |          -0.0098 |         153.2171 |          12.8592 |
[32m[20221213 21:21:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:21:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 715.40
[32m[20221213 21:21:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 761.00
[32m[20221213 21:21:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 771.00
[32m[20221213 21:21:29 @agent_ppo2.py:143][0m Total time:      25.90 min
[32m[20221213 21:21:29 @agent_ppo2.py:145][0m 2543616 total steps have happened
[32m[20221213 21:21:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1242 --------------------------#
[32m[20221213 21:21:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:29 @agent_ppo2.py:185][0m |           0.0127 |         170.8609 |          12.7232 |
[32m[20221213 21:21:29 @agent_ppo2.py:185][0m |          -0.0005 |         156.2757 |          12.6882 |
[32m[20221213 21:21:29 @agent_ppo2.py:185][0m |          -0.0061 |         155.4238 |          12.6917 |
[32m[20221213 21:21:29 @agent_ppo2.py:185][0m |          -0.0065 |         155.0217 |          12.6894 |
[32m[20221213 21:21:29 @agent_ppo2.py:185][0m |          -0.0062 |         154.8681 |          12.7088 |
[32m[20221213 21:21:29 @agent_ppo2.py:185][0m |          -0.0077 |         154.6983 |          12.7077 |
[32m[20221213 21:21:30 @agent_ppo2.py:185][0m |          -0.0071 |         154.4249 |          12.7017 |
[32m[20221213 21:21:30 @agent_ppo2.py:185][0m |           0.0081 |         169.7062 |          12.7331 |
[32m[20221213 21:21:30 @agent_ppo2.py:185][0m |          -0.0070 |         154.0624 |          12.7231 |
[32m[20221213 21:21:30 @agent_ppo2.py:185][0m |          -0.0069 |         153.9709 |          12.6908 |
[32m[20221213 21:21:30 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:21:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 741.80
[32m[20221213 21:21:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 761.00
[32m[20221213 21:21:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.00
[32m[20221213 21:21:30 @agent_ppo2.py:143][0m Total time:      25.93 min
[32m[20221213 21:21:30 @agent_ppo2.py:145][0m 2545664 total steps have happened
[32m[20221213 21:21:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1243 --------------------------#
[32m[20221213 21:21:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:21:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:31 @agent_ppo2.py:185][0m |          -0.0000 |         156.0407 |          12.6269 |
[32m[20221213 21:21:31 @agent_ppo2.py:185][0m |          -0.0019 |         155.2401 |          12.6526 |
[32m[20221213 21:21:31 @agent_ppo2.py:185][0m |          -0.0061 |         154.3960 |          12.6470 |
[32m[20221213 21:21:31 @agent_ppo2.py:185][0m |          -0.0008 |         156.0477 |          12.6776 |
[32m[20221213 21:21:31 @agent_ppo2.py:185][0m |           0.0335 |         196.2374 |          12.6804 |
[32m[20221213 21:21:31 @agent_ppo2.py:185][0m |          -0.0084 |         152.8788 |          12.7308 |
[32m[20221213 21:21:31 @agent_ppo2.py:185][0m |          -0.0066 |         152.5180 |          12.7611 |
[32m[20221213 21:21:31 @agent_ppo2.py:185][0m |          -0.0078 |         151.8648 |          12.7863 |
[32m[20221213 21:21:31 @agent_ppo2.py:185][0m |          -0.0093 |         151.6517 |          12.7894 |
[32m[20221213 21:21:32 @agent_ppo2.py:185][0m |          -0.0093 |         151.4014 |          12.8039 |
[32m[20221213 21:21:32 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:21:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 752.20
[32m[20221213 21:21:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.00
[32m[20221213 21:21:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.00
[32m[20221213 21:21:32 @agent_ppo2.py:143][0m Total time:      25.95 min
[32m[20221213 21:21:32 @agent_ppo2.py:145][0m 2547712 total steps have happened
[32m[20221213 21:21:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1244 --------------------------#
[32m[20221213 21:21:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:32 @agent_ppo2.py:185][0m |          -0.0021 |         156.5877 |          12.9486 |
[32m[20221213 21:21:32 @agent_ppo2.py:185][0m |          -0.0027 |         156.0398 |          12.9756 |
[32m[20221213 21:21:32 @agent_ppo2.py:185][0m |          -0.0075 |         155.7743 |          13.0012 |
[32m[20221213 21:21:32 @agent_ppo2.py:185][0m |          -0.0064 |         155.2385 |          12.9902 |
[32m[20221213 21:21:32 @agent_ppo2.py:185][0m |          -0.0061 |         154.9304 |          12.9982 |
[32m[20221213 21:21:32 @agent_ppo2.py:185][0m |          -0.0063 |         154.8132 |          13.0112 |
[32m[20221213 21:21:33 @agent_ppo2.py:185][0m |           0.0007 |         163.4274 |          12.9826 |
[32m[20221213 21:21:33 @agent_ppo2.py:185][0m |          -0.0081 |         154.1945 |          13.0223 |
[32m[20221213 21:21:33 @agent_ppo2.py:185][0m |          -0.0031 |         156.8507 |          12.9914 |
[32m[20221213 21:21:33 @agent_ppo2.py:185][0m |          -0.0096 |         153.7997 |          13.0178 |
[32m[20221213 21:21:33 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:21:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 737.80
[32m[20221213 21:21:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 768.00
[32m[20221213 21:21:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 21:21:33 @agent_ppo2.py:143][0m Total time:      25.97 min
[32m[20221213 21:21:33 @agent_ppo2.py:145][0m 2549760 total steps have happened
[32m[20221213 21:21:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1245 --------------------------#
[32m[20221213 21:21:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:21:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:33 @agent_ppo2.py:185][0m |           0.0054 |         160.7490 |          12.9451 |
[32m[20221213 21:21:33 @agent_ppo2.py:185][0m |          -0.0033 |         152.3068 |          12.9782 |
[32m[20221213 21:21:33 @agent_ppo2.py:185][0m |          -0.0035 |         150.8628 |          12.8953 |
[32m[20221213 21:21:34 @agent_ppo2.py:185][0m |          -0.0058 |         149.0855 |          12.8976 |
[32m[20221213 21:21:34 @agent_ppo2.py:185][0m |          -0.0056 |         147.9566 |          12.9026 |
[32m[20221213 21:21:34 @agent_ppo2.py:185][0m |          -0.0038 |         150.1609 |          12.8723 |
[32m[20221213 21:21:34 @agent_ppo2.py:185][0m |          -0.0068 |         146.4057 |          12.8400 |
[32m[20221213 21:21:34 @agent_ppo2.py:185][0m |          -0.0096 |         144.8956 |          12.8598 |
[32m[20221213 21:21:34 @agent_ppo2.py:185][0m |          -0.0111 |         144.1433 |          12.8243 |
[32m[20221213 21:21:34 @agent_ppo2.py:185][0m |          -0.0115 |         144.2878 |          12.8137 |
[32m[20221213 21:21:34 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:21:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 761.40
[32m[20221213 21:21:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.00
[32m[20221213 21:21:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.00
[32m[20221213 21:21:34 @agent_ppo2.py:143][0m Total time:      25.99 min
[32m[20221213 21:21:34 @agent_ppo2.py:145][0m 2551808 total steps have happened
[32m[20221213 21:21:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1246 --------------------------#
[32m[20221213 21:21:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:35 @agent_ppo2.py:185][0m |          -0.0041 |         162.6599 |          12.8353 |
[32m[20221213 21:21:35 @agent_ppo2.py:185][0m |          -0.0019 |         160.0838 |          12.8705 |
[32m[20221213 21:21:35 @agent_ppo2.py:185][0m |          -0.0049 |         158.2537 |          12.8531 |
[32m[20221213 21:21:35 @agent_ppo2.py:185][0m |          -0.0082 |         157.8065 |          12.8523 |
[32m[20221213 21:21:35 @agent_ppo2.py:185][0m |          -0.0016 |         162.0139 |          12.8297 |
[32m[20221213 21:21:35 @agent_ppo2.py:185][0m |           0.0002 |         165.6890 |          12.8135 |
[32m[20221213 21:21:35 @agent_ppo2.py:185][0m |          -0.0079 |         156.8212 |          12.8289 |
[32m[20221213 21:21:35 @agent_ppo2.py:185][0m |          -0.0126 |         156.5553 |          12.7959 |
[32m[20221213 21:21:35 @agent_ppo2.py:185][0m |          -0.0109 |         156.4308 |          12.7944 |
[32m[20221213 21:21:36 @agent_ppo2.py:185][0m |          -0.0064 |         156.4281 |          12.7403 |
[32m[20221213 21:21:36 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:21:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 755.20
[32m[20221213 21:21:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 21:21:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.00
[32m[20221213 21:21:36 @agent_ppo2.py:143][0m Total time:      26.02 min
[32m[20221213 21:21:36 @agent_ppo2.py:145][0m 2553856 total steps have happened
[32m[20221213 21:21:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1247 --------------------------#
[32m[20221213 21:21:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:36 @agent_ppo2.py:185][0m |           0.0015 |         156.9522 |          12.6320 |
[32m[20221213 21:21:36 @agent_ppo2.py:185][0m |          -0.0041 |         156.0528 |          12.6520 |
[32m[20221213 21:21:36 @agent_ppo2.py:185][0m |          -0.0044 |         155.2963 |          12.6816 |
[32m[20221213 21:21:36 @agent_ppo2.py:185][0m |          -0.0054 |         154.8853 |          12.7159 |
[32m[20221213 21:21:36 @agent_ppo2.py:185][0m |          -0.0061 |         154.5029 |          12.7205 |
[32m[20221213 21:21:36 @agent_ppo2.py:185][0m |          -0.0065 |         154.0837 |          12.7465 |
[32m[20221213 21:21:37 @agent_ppo2.py:185][0m |          -0.0060 |         153.7273 |          12.7838 |
[32m[20221213 21:21:37 @agent_ppo2.py:185][0m |          -0.0076 |         153.4913 |          12.7910 |
[32m[20221213 21:21:37 @agent_ppo2.py:185][0m |          -0.0072 |         153.3721 |          12.7925 |
[32m[20221213 21:21:37 @agent_ppo2.py:185][0m |          -0.0036 |         156.2662 |          12.8117 |
[32m[20221213 21:21:37 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:21:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.20
[32m[20221213 21:21:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.00
[32m[20221213 21:21:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.00
[32m[20221213 21:21:37 @agent_ppo2.py:143][0m Total time:      26.04 min
[32m[20221213 21:21:37 @agent_ppo2.py:145][0m 2555904 total steps have happened
[32m[20221213 21:21:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1248 --------------------------#
[32m[20221213 21:21:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:21:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:37 @agent_ppo2.py:185][0m |           0.0021 |         159.4387 |          12.5911 |
[32m[20221213 21:21:37 @agent_ppo2.py:185][0m |          -0.0022 |         157.4143 |          12.5756 |
[32m[20221213 21:21:38 @agent_ppo2.py:185][0m |          -0.0049 |         156.8253 |          12.5574 |
[32m[20221213 21:21:38 @agent_ppo2.py:185][0m |          -0.0053 |         156.4301 |          12.5456 |
[32m[20221213 21:21:38 @agent_ppo2.py:185][0m |          -0.0073 |         156.0139 |          12.5403 |
[32m[20221213 21:21:38 @agent_ppo2.py:185][0m |          -0.0066 |         155.8518 |          12.5082 |
[32m[20221213 21:21:38 @agent_ppo2.py:185][0m |          -0.0083 |         155.5407 |          12.4891 |
[32m[20221213 21:21:38 @agent_ppo2.py:185][0m |          -0.0068 |         155.5090 |          12.4506 |
[32m[20221213 21:21:38 @agent_ppo2.py:185][0m |          -0.0091 |         155.3048 |          12.4387 |
[32m[20221213 21:21:38 @agent_ppo2.py:185][0m |          -0.0109 |         155.1757 |          12.4251 |
[32m[20221213 21:21:38 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:21:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 722.60
[32m[20221213 21:21:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 759.00
[32m[20221213 21:21:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.00
[32m[20221213 21:21:38 @agent_ppo2.py:143][0m Total time:      26.06 min
[32m[20221213 21:21:38 @agent_ppo2.py:145][0m 2557952 total steps have happened
[32m[20221213 21:21:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1249 --------------------------#
[32m[20221213 21:21:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:39 @agent_ppo2.py:185][0m |          -0.0006 |         158.3626 |          12.6477 |
[32m[20221213 21:21:39 @agent_ppo2.py:185][0m |          -0.0048 |         156.4805 |          12.6500 |
[32m[20221213 21:21:39 @agent_ppo2.py:185][0m |          -0.0096 |         155.7126 |          12.6658 |
[32m[20221213 21:21:39 @agent_ppo2.py:185][0m |          -0.0078 |         154.9854 |          12.6521 |
[32m[20221213 21:21:39 @agent_ppo2.py:185][0m |          -0.0076 |         154.6924 |          12.6340 |
[32m[20221213 21:21:39 @agent_ppo2.py:185][0m |          -0.0072 |         154.3459 |          12.6477 |
[32m[20221213 21:21:39 @agent_ppo2.py:185][0m |          -0.0095 |         154.0181 |          12.6439 |
[32m[20221213 21:21:39 @agent_ppo2.py:185][0m |          -0.0089 |         153.8060 |          12.6531 |
[32m[20221213 21:21:39 @agent_ppo2.py:185][0m |          -0.0108 |         153.6769 |          12.6547 |
[32m[20221213 21:21:40 @agent_ppo2.py:185][0m |          -0.0081 |         153.6731 |          12.6344 |
[32m[20221213 21:21:40 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:21:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 740.20
[32m[20221213 21:21:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 760.00
[32m[20221213 21:21:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 707.00
[32m[20221213 21:21:40 @agent_ppo2.py:143][0m Total time:      26.08 min
[32m[20221213 21:21:40 @agent_ppo2.py:145][0m 2560000 total steps have happened
[32m[20221213 21:21:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1250 --------------------------#
[32m[20221213 21:21:40 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:21:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:40 @agent_ppo2.py:185][0m |          -0.0018 |         159.9904 |          12.6705 |
[32m[20221213 21:21:40 @agent_ppo2.py:185][0m |          -0.0034 |         158.9535 |          12.7118 |
[32m[20221213 21:21:40 @agent_ppo2.py:185][0m |          -0.0049 |         158.5739 |          12.6719 |
[32m[20221213 21:21:40 @agent_ppo2.py:185][0m |          -0.0068 |         158.2791 |          12.6812 |
[32m[20221213 21:21:40 @agent_ppo2.py:185][0m |          -0.0050 |         158.0203 |          12.6899 |
[32m[20221213 21:21:40 @agent_ppo2.py:185][0m |          -0.0046 |         157.9764 |          12.6718 |
[32m[20221213 21:21:40 @agent_ppo2.py:185][0m |          -0.0036 |         158.8185 |          12.6899 |
[32m[20221213 21:21:41 @agent_ppo2.py:185][0m |          -0.0042 |         157.7411 |          12.6700 |
[32m[20221213 21:21:41 @agent_ppo2.py:185][0m |          -0.0072 |         157.4617 |          12.6903 |
[32m[20221213 21:21:41 @agent_ppo2.py:185][0m |          -0.0073 |         157.5059 |          12.6699 |
[32m[20221213 21:21:41 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:21:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.60
[32m[20221213 21:21:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 765.00
[32m[20221213 21:21:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.00
[32m[20221213 21:21:41 @agent_ppo2.py:143][0m Total time:      26.10 min
[32m[20221213 21:21:41 @agent_ppo2.py:145][0m 2562048 total steps have happened
[32m[20221213 21:21:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1251 --------------------------#
[32m[20221213 21:21:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:21:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:41 @agent_ppo2.py:185][0m |          -0.0001 |         158.2794 |          12.2953 |
[32m[20221213 21:21:41 @agent_ppo2.py:185][0m |          -0.0055 |         157.3059 |          12.2756 |
[32m[20221213 21:21:41 @agent_ppo2.py:185][0m |           0.0021 |         163.7470 |          12.2728 |
[32m[20221213 21:21:42 @agent_ppo2.py:185][0m |          -0.0049 |         156.1546 |          12.2595 |
[32m[20221213 21:21:42 @agent_ppo2.py:185][0m |          -0.0084 |         155.8751 |          12.2584 |
[32m[20221213 21:21:42 @agent_ppo2.py:185][0m |          -0.0084 |         155.4299 |          12.2344 |
[32m[20221213 21:21:42 @agent_ppo2.py:185][0m |          -0.0095 |         155.3546 |          12.2338 |
[32m[20221213 21:21:42 @agent_ppo2.py:185][0m |          -0.0108 |         155.0675 |          12.2444 |
[32m[20221213 21:21:42 @agent_ppo2.py:185][0m |           0.0019 |         162.5286 |          12.2153 |
[32m[20221213 21:21:42 @agent_ppo2.py:185][0m |          -0.0072 |         154.9547 |          12.2588 |
[32m[20221213 21:21:42 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:21:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 745.80
[32m[20221213 21:21:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.00
[32m[20221213 21:21:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 21:21:42 @agent_ppo2.py:143][0m Total time:      26.13 min
[32m[20221213 21:21:42 @agent_ppo2.py:145][0m 2564096 total steps have happened
[32m[20221213 21:21:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1252 --------------------------#
[32m[20221213 21:21:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:43 @agent_ppo2.py:185][0m |           0.0045 |         157.4048 |          12.6548 |
[32m[20221213 21:21:43 @agent_ppo2.py:185][0m |          -0.0014 |         155.1278 |          12.6315 |
[32m[20221213 21:21:43 @agent_ppo2.py:185][0m |          -0.0033 |         154.4467 |          12.6529 |
[32m[20221213 21:21:43 @agent_ppo2.py:185][0m |           0.0030 |         155.6950 |          12.6376 |
[32m[20221213 21:21:43 @agent_ppo2.py:185][0m |          -0.0061 |         153.3699 |          12.6268 |
[32m[20221213 21:21:43 @agent_ppo2.py:185][0m |           0.0018 |         161.6013 |          12.6350 |
[32m[20221213 21:21:43 @agent_ppo2.py:185][0m |          -0.0058 |         152.9447 |          12.5827 |
[32m[20221213 21:21:43 @agent_ppo2.py:185][0m |          -0.0064 |         152.6011 |          12.6188 |
[32m[20221213 21:21:43 @agent_ppo2.py:185][0m |           0.0023 |         158.8756 |          12.5807 |
[32m[20221213 21:21:43 @agent_ppo2.py:185][0m |          -0.0086 |         152.4454 |          12.5960 |
[32m[20221213 21:21:43 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:21:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.80
[32m[20221213 21:21:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.00
[32m[20221213 21:21:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.00
[32m[20221213 21:21:44 @agent_ppo2.py:143][0m Total time:      26.15 min
[32m[20221213 21:21:44 @agent_ppo2.py:145][0m 2566144 total steps have happened
[32m[20221213 21:21:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1253 --------------------------#
[32m[20221213 21:21:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:44 @agent_ppo2.py:185][0m |           0.0004 |         158.4952 |          12.1723 |
[32m[20221213 21:21:44 @agent_ppo2.py:185][0m |           0.0075 |         178.8963 |          12.1697 |
[32m[20221213 21:21:44 @agent_ppo2.py:185][0m |          -0.0019 |         157.2238 |          12.1227 |
[32m[20221213 21:21:44 @agent_ppo2.py:185][0m |          -0.0040 |         155.8498 |          12.1190 |
[32m[20221213 21:21:44 @agent_ppo2.py:185][0m |           0.0040 |         165.3202 |          12.1048 |
[32m[20221213 21:21:44 @agent_ppo2.py:185][0m |          -0.0069 |         153.9062 |          12.0582 |
[32m[20221213 21:21:44 @agent_ppo2.py:185][0m |          -0.0078 |         153.6775 |          12.0675 |
[32m[20221213 21:21:45 @agent_ppo2.py:185][0m |          -0.0081 |         153.2346 |          12.0256 |
[32m[20221213 21:21:45 @agent_ppo2.py:185][0m |           0.0001 |         165.6699 |          11.9968 |
[32m[20221213 21:21:45 @agent_ppo2.py:185][0m |           0.0053 |         160.0089 |          12.0266 |
[32m[20221213 21:21:45 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:21:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 725.80
[32m[20221213 21:21:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 763.00
[32m[20221213 21:21:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 775.00
[32m[20221213 21:21:45 @agent_ppo2.py:143][0m Total time:      26.17 min
[32m[20221213 21:21:45 @agent_ppo2.py:145][0m 2568192 total steps have happened
[32m[20221213 21:21:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1254 --------------------------#
[32m[20221213 21:21:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:45 @agent_ppo2.py:185][0m |          -0.0008 |         155.2031 |          12.0651 |
[32m[20221213 21:21:45 @agent_ppo2.py:185][0m |           0.0019 |         156.7240 |          12.0864 |
[32m[20221213 21:21:45 @agent_ppo2.py:185][0m |          -0.0080 |         153.6665 |          12.1119 |
[32m[20221213 21:21:45 @agent_ppo2.py:185][0m |          -0.0065 |         153.3607 |          12.1557 |
[32m[20221213 21:21:46 @agent_ppo2.py:185][0m |          -0.0081 |         152.7616 |          12.1842 |
[32m[20221213 21:21:46 @agent_ppo2.py:185][0m |          -0.0087 |         152.5436 |          12.2182 |
[32m[20221213 21:21:46 @agent_ppo2.py:185][0m |          -0.0075 |         152.2780 |          12.2188 |
[32m[20221213 21:21:46 @agent_ppo2.py:185][0m |           0.0011 |         164.0856 |          12.2354 |
[32m[20221213 21:21:46 @agent_ppo2.py:185][0m |          -0.0088 |         152.0859 |          12.2874 |
[32m[20221213 21:21:46 @agent_ppo2.py:185][0m |          -0.0081 |         151.6875 |          12.2802 |
[32m[20221213 21:21:46 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 21:21:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 754.60
[32m[20221213 21:21:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.00
[32m[20221213 21:21:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.00
[32m[20221213 21:21:46 @agent_ppo2.py:143][0m Total time:      26.19 min
[32m[20221213 21:21:46 @agent_ppo2.py:145][0m 2570240 total steps have happened
[32m[20221213 21:21:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1255 --------------------------#
[32m[20221213 21:21:46 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:21:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:47 @agent_ppo2.py:185][0m |           0.0023 |         158.6039 |          12.1483 |
[32m[20221213 21:21:47 @agent_ppo2.py:185][0m |          -0.0039 |         154.8769 |          12.1815 |
[32m[20221213 21:21:47 @agent_ppo2.py:185][0m |           0.0174 |         177.0824 |          12.1792 |
[32m[20221213 21:21:47 @agent_ppo2.py:185][0m |           0.0041 |         160.6156 |          12.1857 |
[32m[20221213 21:21:47 @agent_ppo2.py:185][0m |          -0.0067 |         152.9727 |          12.1743 |
[32m[20221213 21:21:47 @agent_ppo2.py:185][0m |          -0.0053 |         152.7698 |          12.1600 |
[32m[20221213 21:21:47 @agent_ppo2.py:185][0m |          -0.0077 |         152.4615 |          12.1580 |
[32m[20221213 21:21:47 @agent_ppo2.py:185][0m |           0.0036 |         157.3874 |          12.1719 |
[32m[20221213 21:21:47 @agent_ppo2.py:185][0m |          -0.0075 |         152.2531 |          12.1242 |
[32m[20221213 21:21:47 @agent_ppo2.py:185][0m |          -0.0068 |         151.8064 |          12.1282 |
[32m[20221213 21:21:47 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:21:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 737.20
[32m[20221213 21:21:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.00
[32m[20221213 21:21:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.00
[32m[20221213 21:21:47 @agent_ppo2.py:143][0m Total time:      26.21 min
[32m[20221213 21:21:47 @agent_ppo2.py:145][0m 2572288 total steps have happened
[32m[20221213 21:21:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1256 --------------------------#
[32m[20221213 21:21:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:48 @agent_ppo2.py:185][0m |           0.0112 |         170.4718 |          12.2868 |
[32m[20221213 21:21:48 @agent_ppo2.py:185][0m |          -0.0034 |         154.3698 |          12.2828 |
[32m[20221213 21:21:48 @agent_ppo2.py:185][0m |          -0.0066 |         153.4213 |          12.2720 |
[32m[20221213 21:21:48 @agent_ppo2.py:185][0m |          -0.0030 |         155.6388 |          12.2787 |
[32m[20221213 21:21:48 @agent_ppo2.py:185][0m |          -0.0084 |         152.3799 |          12.2656 |
[32m[20221213 21:21:48 @agent_ppo2.py:185][0m |          -0.0051 |         155.6071 |          12.2688 |
[32m[20221213 21:21:48 @agent_ppo2.py:185][0m |          -0.0073 |         151.6889 |          12.2543 |
[32m[20221213 21:21:48 @agent_ppo2.py:185][0m |          -0.0002 |         159.2418 |          12.2451 |
[32m[20221213 21:21:49 @agent_ppo2.py:185][0m |          -0.0078 |         151.2308 |          12.2517 |
[32m[20221213 21:21:49 @agent_ppo2.py:185][0m |          -0.0090 |         150.4293 |          12.2845 |
[32m[20221213 21:21:49 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:21:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.40
[32m[20221213 21:21:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.00
[32m[20221213 21:21:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.00
[32m[20221213 21:21:49 @agent_ppo2.py:143][0m Total time:      26.23 min
[32m[20221213 21:21:49 @agent_ppo2.py:145][0m 2574336 total steps have happened
[32m[20221213 21:21:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1257 --------------------------#
[32m[20221213 21:21:49 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:21:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:49 @agent_ppo2.py:185][0m |           0.0007 |         159.2325 |          12.1117 |
[32m[20221213 21:21:49 @agent_ppo2.py:185][0m |          -0.0030 |         157.8529 |          12.1428 |
[32m[20221213 21:21:49 @agent_ppo2.py:185][0m |          -0.0043 |         157.1494 |          12.1764 |
[32m[20221213 21:21:49 @agent_ppo2.py:185][0m |          -0.0065 |         156.6141 |          12.1992 |
[32m[20221213 21:21:49 @agent_ppo2.py:185][0m |           0.0035 |         166.6298 |          12.2000 |
[32m[20221213 21:21:49 @agent_ppo2.py:185][0m |          -0.0091 |         155.8737 |          12.1884 |
[32m[20221213 21:21:50 @agent_ppo2.py:185][0m |          -0.0076 |         155.7232 |          12.1893 |
[32m[20221213 21:21:50 @agent_ppo2.py:185][0m |          -0.0039 |         156.5181 |          12.1996 |
[32m[20221213 21:21:50 @agent_ppo2.py:185][0m |          -0.0090 |         155.1303 |          12.2104 |
[32m[20221213 21:21:50 @agent_ppo2.py:185][0m |           0.0113 |         176.5468 |          12.2134 |
[32m[20221213 21:21:50 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:21:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 752.20
[32m[20221213 21:21:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.00
[32m[20221213 21:21:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 772.00
[32m[20221213 21:21:50 @agent_ppo2.py:143][0m Total time:      26.25 min
[32m[20221213 21:21:50 @agent_ppo2.py:145][0m 2576384 total steps have happened
[32m[20221213 21:21:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1258 --------------------------#
[32m[20221213 21:21:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:21:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:50 @agent_ppo2.py:185][0m |           0.0024 |         161.6121 |          12.2831 |
[32m[20221213 21:21:50 @agent_ppo2.py:185][0m |          -0.0038 |         158.9453 |          12.3050 |
[32m[20221213 21:21:50 @agent_ppo2.py:185][0m |          -0.0051 |         158.0552 |          12.2934 |
[32m[20221213 21:21:51 @agent_ppo2.py:185][0m |          -0.0059 |         157.1341 |          12.2770 |
[32m[20221213 21:21:51 @agent_ppo2.py:185][0m |          -0.0084 |         155.8081 |          12.2735 |
[32m[20221213 21:21:51 @agent_ppo2.py:185][0m |          -0.0068 |         155.5672 |          12.2820 |
[32m[20221213 21:21:51 @agent_ppo2.py:185][0m |           0.0003 |         164.9978 |          12.2653 |
[32m[20221213 21:21:51 @agent_ppo2.py:185][0m |          -0.0056 |         155.1783 |          12.2911 |
[32m[20221213 21:21:51 @agent_ppo2.py:185][0m |          -0.0081 |         154.1977 |          12.2781 |
[32m[20221213 21:21:51 @agent_ppo2.py:185][0m |          -0.0110 |         153.7864 |          12.2683 |
[32m[20221213 21:21:51 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:21:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 717.00
[32m[20221213 21:21:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 759.00
[32m[20221213 21:21:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.00
[32m[20221213 21:21:51 @agent_ppo2.py:143][0m Total time:      26.28 min
[32m[20221213 21:21:51 @agent_ppo2.py:145][0m 2578432 total steps have happened
[32m[20221213 21:21:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1259 --------------------------#
[32m[20221213 21:21:51 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:21:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:52 @agent_ppo2.py:185][0m |           0.0052 |         159.4150 |          12.2747 |
[32m[20221213 21:21:52 @agent_ppo2.py:185][0m |          -0.0046 |         156.5734 |          12.2850 |
[32m[20221213 21:21:52 @agent_ppo2.py:185][0m |          -0.0008 |         156.3447 |          12.2946 |
[32m[20221213 21:21:52 @agent_ppo2.py:185][0m |          -0.0016 |         155.3931 |          12.3288 |
[32m[20221213 21:21:52 @agent_ppo2.py:185][0m |          -0.0057 |         154.7373 |          12.3441 |
[32m[20221213 21:21:52 @agent_ppo2.py:185][0m |          -0.0040 |         154.6483 |          12.3542 |
[32m[20221213 21:21:52 @agent_ppo2.py:185][0m |           0.0032 |         164.7590 |          12.3795 |
[32m[20221213 21:21:52 @agent_ppo2.py:185][0m |          -0.0038 |         155.4368 |          12.3948 |
[32m[20221213 21:21:52 @agent_ppo2.py:185][0m |          -0.0092 |         153.9050 |          12.4079 |
[32m[20221213 21:21:52 @agent_ppo2.py:185][0m |          -0.0091 |         153.4488 |          12.4101 |
[32m[20221213 21:21:52 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:21:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 709.60
[32m[20221213 21:21:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 761.00
[32m[20221213 21:21:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.00
[32m[20221213 21:21:52 @agent_ppo2.py:143][0m Total time:      26.30 min
[32m[20221213 21:21:52 @agent_ppo2.py:145][0m 2580480 total steps have happened
[32m[20221213 21:21:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1260 --------------------------#
[32m[20221213 21:21:53 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:21:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:53 @agent_ppo2.py:185][0m |           0.0036 |         157.8705 |          12.3452 |
[32m[20221213 21:21:53 @agent_ppo2.py:185][0m |          -0.0033 |         154.5609 |          12.3704 |
[32m[20221213 21:21:53 @agent_ppo2.py:185][0m |          -0.0009 |         155.0600 |          12.3523 |
[32m[20221213 21:21:53 @agent_ppo2.py:185][0m |           0.0100 |         175.4551 |          12.4104 |
[32m[20221213 21:21:53 @agent_ppo2.py:185][0m |          -0.0061 |         153.0117 |          12.4529 |
[32m[20221213 21:21:53 @agent_ppo2.py:185][0m |          -0.0062 |         152.5451 |          12.4715 |
[32m[20221213 21:21:53 @agent_ppo2.py:185][0m |          -0.0020 |         156.6197 |          12.4669 |
[32m[20221213 21:21:53 @agent_ppo2.py:185][0m |          -0.0075 |         152.0066 |          12.4871 |
[32m[20221213 21:21:53 @agent_ppo2.py:185][0m |           0.0017 |         160.3493 |          12.4803 |
[32m[20221213 21:21:54 @agent_ppo2.py:185][0m |          -0.0052 |         151.7099 |          12.5112 |
[32m[20221213 21:21:54 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:21:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.80
[32m[20221213 21:21:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.00
[32m[20221213 21:21:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.00
[32m[20221213 21:21:54 @agent_ppo2.py:143][0m Total time:      26.32 min
[32m[20221213 21:21:54 @agent_ppo2.py:145][0m 2582528 total steps have happened
[32m[20221213 21:21:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1261 --------------------------#
[32m[20221213 21:21:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:21:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:54 @agent_ppo2.py:185][0m |           0.0039 |         155.4786 |          12.5162 |
[32m[20221213 21:21:54 @agent_ppo2.py:185][0m |          -0.0004 |         153.2104 |          12.5605 |
[32m[20221213 21:21:54 @agent_ppo2.py:185][0m |          -0.0054 |         150.1594 |          12.5618 |
[32m[20221213 21:21:54 @agent_ppo2.py:185][0m |          -0.0023 |         150.5354 |          12.5772 |
[32m[20221213 21:21:54 @agent_ppo2.py:185][0m |          -0.0055 |         148.7494 |          12.5588 |
[32m[20221213 21:21:54 @agent_ppo2.py:185][0m |          -0.0062 |         148.1752 |          12.5921 |
[32m[20221213 21:21:55 @agent_ppo2.py:185][0m |          -0.0071 |         147.8960 |          12.5782 |
[32m[20221213 21:21:55 @agent_ppo2.py:185][0m |          -0.0082 |         147.5774 |          12.5961 |
[32m[20221213 21:21:55 @agent_ppo2.py:185][0m |          -0.0079 |         147.2866 |          12.5781 |
[32m[20221213 21:21:55 @agent_ppo2.py:185][0m |          -0.0053 |         147.0031 |          12.5890 |
[32m[20221213 21:21:55 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:21:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 725.80
[32m[20221213 21:21:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 768.00
[32m[20221213 21:21:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 21:21:55 @agent_ppo2.py:143][0m Total time:      26.34 min
[32m[20221213 21:21:55 @agent_ppo2.py:145][0m 2584576 total steps have happened
[32m[20221213 21:21:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1262 --------------------------#
[32m[20221213 21:21:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:55 @agent_ppo2.py:185][0m |           0.0015 |         157.9822 |          12.7318 |
[32m[20221213 21:21:55 @agent_ppo2.py:185][0m |          -0.0050 |         155.6978 |          12.7251 |
[32m[20221213 21:21:55 @agent_ppo2.py:185][0m |          -0.0050 |         154.5810 |          12.6959 |
[32m[20221213 21:21:56 @agent_ppo2.py:185][0m |          -0.0060 |         153.8758 |          12.6776 |
[32m[20221213 21:21:56 @agent_ppo2.py:185][0m |          -0.0065 |         153.2933 |          12.6720 |
[32m[20221213 21:21:56 @agent_ppo2.py:185][0m |          -0.0063 |         153.2979 |          12.6600 |
[32m[20221213 21:21:56 @agent_ppo2.py:185][0m |          -0.0063 |         152.4794 |          12.6531 |
[32m[20221213 21:21:56 @agent_ppo2.py:185][0m |          -0.0077 |         152.4548 |          12.6346 |
[32m[20221213 21:21:56 @agent_ppo2.py:185][0m |          -0.0082 |         152.0228 |          12.6352 |
[32m[20221213 21:21:56 @agent_ppo2.py:185][0m |          -0.0084 |         152.0233 |          12.6331 |
[32m[20221213 21:21:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:21:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 757.60
[32m[20221213 21:21:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.00
[32m[20221213 21:21:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.00
[32m[20221213 21:21:56 @agent_ppo2.py:143][0m Total time:      26.36 min
[32m[20221213 21:21:56 @agent_ppo2.py:145][0m 2586624 total steps have happened
[32m[20221213 21:21:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1263 --------------------------#
[32m[20221213 21:21:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:57 @agent_ppo2.py:185][0m |           0.0009 |         155.5325 |          12.5984 |
[32m[20221213 21:21:57 @agent_ppo2.py:185][0m |          -0.0028 |         153.2604 |          12.5544 |
[32m[20221213 21:21:57 @agent_ppo2.py:185][0m |          -0.0056 |         150.9959 |          12.5849 |
[32m[20221213 21:21:57 @agent_ppo2.py:185][0m |          -0.0030 |         150.7850 |          12.6169 |
[32m[20221213 21:21:57 @agent_ppo2.py:185][0m |          -0.0055 |         149.2127 |          12.6281 |
[32m[20221213 21:21:57 @agent_ppo2.py:185][0m |          -0.0052 |         149.0035 |          12.6198 |
[32m[20221213 21:21:57 @agent_ppo2.py:185][0m |          -0.0046 |         148.8981 |          12.6544 |
[32m[20221213 21:21:57 @agent_ppo2.py:185][0m |          -0.0083 |         148.2219 |          12.6496 |
[32m[20221213 21:21:57 @agent_ppo2.py:185][0m |          -0.0056 |         148.5900 |          12.6661 |
[32m[20221213 21:21:57 @agent_ppo2.py:185][0m |          -0.0032 |         148.3206 |          12.6655 |
[32m[20221213 21:21:57 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:21:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 730.00
[32m[20221213 21:21:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 760.00
[32m[20221213 21:21:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.00
[32m[20221213 21:21:57 @agent_ppo2.py:143][0m Total time:      26.38 min
[32m[20221213 21:21:57 @agent_ppo2.py:145][0m 2588672 total steps have happened
[32m[20221213 21:21:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1264 --------------------------#
[32m[20221213 21:21:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:58 @agent_ppo2.py:185][0m |          -0.0005 |         157.6457 |          12.4906 |
[32m[20221213 21:21:58 @agent_ppo2.py:185][0m |          -0.0048 |         156.5831 |          12.4842 |
[32m[20221213 21:21:58 @agent_ppo2.py:185][0m |          -0.0047 |         156.0622 |          12.5274 |
[32m[20221213 21:21:58 @agent_ppo2.py:185][0m |          -0.0066 |         155.4332 |          12.5235 |
[32m[20221213 21:21:58 @agent_ppo2.py:185][0m |           0.0071 |         169.4291 |          12.5422 |
[32m[20221213 21:21:58 @agent_ppo2.py:185][0m |          -0.0093 |         155.1284 |          12.5663 |
[32m[20221213 21:21:58 @agent_ppo2.py:185][0m |          -0.0068 |         154.8253 |          12.5982 |
[32m[20221213 21:21:58 @agent_ppo2.py:185][0m |          -0.0053 |         154.2562 |          12.5928 |
[32m[20221213 21:21:58 @agent_ppo2.py:185][0m |          -0.0069 |         154.1732 |          12.6301 |
[32m[20221213 21:21:59 @agent_ppo2.py:185][0m |          -0.0097 |         153.9883 |          12.6442 |
[32m[20221213 21:21:59 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:21:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 727.20
[32m[20221213 21:21:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.00
[32m[20221213 21:21:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 21:21:59 @agent_ppo2.py:143][0m Total time:      26.40 min
[32m[20221213 21:21:59 @agent_ppo2.py:145][0m 2590720 total steps have happened
[32m[20221213 21:21:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1265 --------------------------#
[32m[20221213 21:21:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:21:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:21:59 @agent_ppo2.py:185][0m |           0.0109 |         169.0645 |          12.6069 |
[32m[20221213 21:21:59 @agent_ppo2.py:185][0m |          -0.0032 |         154.4799 |          12.5811 |
[32m[20221213 21:21:59 @agent_ppo2.py:185][0m |           0.0099 |         175.9755 |          12.5854 |
[32m[20221213 21:21:59 @agent_ppo2.py:185][0m |          -0.0070 |         153.4212 |          12.6133 |
[32m[20221213 21:21:59 @agent_ppo2.py:185][0m |          -0.0053 |         153.0180 |          12.5800 |
[32m[20221213 21:21:59 @agent_ppo2.py:185][0m |           0.0163 |         186.6443 |          12.5940 |
[32m[20221213 21:22:00 @agent_ppo2.py:185][0m |          -0.0041 |         152.7925 |          12.5859 |
[32m[20221213 21:22:00 @agent_ppo2.py:185][0m |          -0.0067 |         152.3866 |          12.6005 |
[32m[20221213 21:22:00 @agent_ppo2.py:185][0m |          -0.0081 |         152.1165 |          12.6029 |
[32m[20221213 21:22:00 @agent_ppo2.py:185][0m |          -0.0086 |         151.9365 |          12.5996 |
[32m[20221213 21:22:00 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:22:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 754.00
[32m[20221213 21:22:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.00
[32m[20221213 21:22:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.00
[32m[20221213 21:22:00 @agent_ppo2.py:143][0m Total time:      26.42 min
[32m[20221213 21:22:00 @agent_ppo2.py:145][0m 2592768 total steps have happened
[32m[20221213 21:22:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1266 --------------------------#
[32m[20221213 21:22:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:00 @agent_ppo2.py:185][0m |           0.0028 |         155.9169 |          12.9289 |
[32m[20221213 21:22:00 @agent_ppo2.py:185][0m |          -0.0037 |         152.3155 |          12.9173 |
[32m[20221213 21:22:00 @agent_ppo2.py:185][0m |          -0.0030 |         149.2729 |          12.9588 |
[32m[20221213 21:22:01 @agent_ppo2.py:185][0m |           0.0003 |         151.1769 |          12.9248 |
[32m[20221213 21:22:01 @agent_ppo2.py:185][0m |          -0.0054 |         144.8066 |          12.9362 |
[32m[20221213 21:22:01 @agent_ppo2.py:185][0m |           0.0044 |         153.9146 |          12.9254 |
[32m[20221213 21:22:01 @agent_ppo2.py:185][0m |          -0.0073 |         141.3705 |          12.9425 |
[32m[20221213 21:22:01 @agent_ppo2.py:185][0m |          -0.0073 |         139.9792 |          12.9407 |
[32m[20221213 21:22:01 @agent_ppo2.py:185][0m |           0.0070 |         154.7132 |          12.9266 |
[32m[20221213 21:22:01 @agent_ppo2.py:185][0m |          -0.0077 |         137.9566 |          12.9481 |
[32m[20221213 21:22:01 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:22:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.80
[32m[20221213 21:22:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.00
[32m[20221213 21:22:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.00
[32m[20221213 21:22:01 @agent_ppo2.py:143][0m Total time:      26.44 min
[32m[20221213 21:22:01 @agent_ppo2.py:145][0m 2594816 total steps have happened
[32m[20221213 21:22:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1267 --------------------------#
[32m[20221213 21:22:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:02 @agent_ppo2.py:185][0m |           0.0004 |         157.1962 |          13.1595 |
[32m[20221213 21:22:02 @agent_ppo2.py:185][0m |          -0.0048 |         155.0757 |          13.1629 |
[32m[20221213 21:22:02 @agent_ppo2.py:185][0m |           0.0110 |         171.1846 |          13.1807 |
[32m[20221213 21:22:02 @agent_ppo2.py:185][0m |           0.0010 |         155.7091 |          13.1811 |
[32m[20221213 21:22:02 @agent_ppo2.py:185][0m |          -0.0050 |         153.2771 |          13.2019 |
[32m[20221213 21:22:02 @agent_ppo2.py:185][0m |          -0.0063 |         152.7729 |          13.1768 |
[32m[20221213 21:22:02 @agent_ppo2.py:185][0m |          -0.0081 |         152.3869 |          13.1478 |
[32m[20221213 21:22:02 @agent_ppo2.py:185][0m |          -0.0046 |         152.4528 |          13.1514 |
[32m[20221213 21:22:02 @agent_ppo2.py:185][0m |          -0.0087 |         151.9870 |          13.1428 |
[32m[20221213 21:22:02 @agent_ppo2.py:185][0m |          -0.0084 |         151.7608 |          13.1523 |
[32m[20221213 21:22:02 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:22:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.20
[32m[20221213 21:22:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.00
[32m[20221213 21:22:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.00
[32m[20221213 21:22:02 @agent_ppo2.py:143][0m Total time:      26.46 min
[32m[20221213 21:22:02 @agent_ppo2.py:145][0m 2596864 total steps have happened
[32m[20221213 21:22:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1268 --------------------------#
[32m[20221213 21:22:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:03 @agent_ppo2.py:185][0m |          -0.0005 |         156.8024 |          12.8676 |
[32m[20221213 21:22:03 @agent_ppo2.py:185][0m |          -0.0033 |         155.7654 |          12.8540 |
[32m[20221213 21:22:03 @agent_ppo2.py:185][0m |          -0.0050 |         155.2083 |          12.8507 |
[32m[20221213 21:22:03 @agent_ppo2.py:185][0m |          -0.0054 |         154.7555 |          12.8732 |
[32m[20221213 21:22:03 @agent_ppo2.py:185][0m |          -0.0060 |         154.5191 |          12.8867 |
[32m[20221213 21:22:03 @agent_ppo2.py:185][0m |          -0.0059 |         154.0881 |          12.8483 |
[32m[20221213 21:22:03 @agent_ppo2.py:185][0m |          -0.0080 |         153.9548 |          12.8589 |
[32m[20221213 21:22:03 @agent_ppo2.py:185][0m |          -0.0078 |         153.7892 |          12.8478 |
[32m[20221213 21:22:04 @agent_ppo2.py:185][0m |          -0.0073 |         153.5458 |          12.8607 |
[32m[20221213 21:22:04 @agent_ppo2.py:185][0m |          -0.0076 |         153.4748 |          12.8569 |
[32m[20221213 21:22:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:22:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 727.80
[32m[20221213 21:22:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.00
[32m[20221213 21:22:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.00
[32m[20221213 21:22:04 @agent_ppo2.py:143][0m Total time:      26.48 min
[32m[20221213 21:22:04 @agent_ppo2.py:145][0m 2598912 total steps have happened
[32m[20221213 21:22:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1269 --------------------------#
[32m[20221213 21:22:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:04 @agent_ppo2.py:185][0m |          -0.0003 |         155.0702 |          12.7024 |
[32m[20221213 21:22:04 @agent_ppo2.py:185][0m |          -0.0055 |         153.3550 |          12.7187 |
[32m[20221213 21:22:04 @agent_ppo2.py:185][0m |          -0.0074 |         151.9786 |          12.7106 |
[32m[20221213 21:22:04 @agent_ppo2.py:185][0m |          -0.0078 |         151.0061 |          12.7702 |
[32m[20221213 21:22:04 @agent_ppo2.py:185][0m |          -0.0069 |         150.2294 |          12.7810 |
[32m[20221213 21:22:04 @agent_ppo2.py:185][0m |          -0.0077 |         150.2218 |          12.7977 |
[32m[20221213 21:22:05 @agent_ppo2.py:185][0m |          -0.0011 |         153.2027 |          12.7984 |
[32m[20221213 21:22:05 @agent_ppo2.py:185][0m |          -0.0020 |         152.4112 |          12.8417 |
[32m[20221213 21:22:05 @agent_ppo2.py:185][0m |          -0.0085 |         148.0711 |          12.8286 |
[32m[20221213 21:22:05 @agent_ppo2.py:185][0m |          -0.0105 |         147.7330 |          12.8626 |
[32m[20221213 21:22:05 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:22:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.40
[32m[20221213 21:22:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.00
[32m[20221213 21:22:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.00
[32m[20221213 21:22:05 @agent_ppo2.py:143][0m Total time:      26.50 min
[32m[20221213 21:22:05 @agent_ppo2.py:145][0m 2600960 total steps have happened
[32m[20221213 21:22:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1270 --------------------------#
[32m[20221213 21:22:05 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:22:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:05 @agent_ppo2.py:185][0m |          -0.0043 |         158.9326 |          13.0676 |
[32m[20221213 21:22:05 @agent_ppo2.py:185][0m |          -0.0041 |         156.2941 |          13.0417 |
[32m[20221213 21:22:05 @agent_ppo2.py:185][0m |          -0.0071 |         155.1480 |          13.0180 |
[32m[20221213 21:22:06 @agent_ppo2.py:185][0m |          -0.0102 |         154.1933 |          13.0377 |
[32m[20221213 21:22:06 @agent_ppo2.py:185][0m |          -0.0108 |         153.5290 |          13.0294 |
[32m[20221213 21:22:06 @agent_ppo2.py:185][0m |          -0.0010 |         163.0706 |          13.0266 |
[32m[20221213 21:22:06 @agent_ppo2.py:185][0m |           0.0079 |         176.7712 |          13.0497 |
[32m[20221213 21:22:06 @agent_ppo2.py:185][0m |          -0.0072 |         154.7624 |          13.0696 |
[32m[20221213 21:22:06 @agent_ppo2.py:185][0m |          -0.0084 |         152.8169 |          13.0382 |
[32m[20221213 21:22:06 @agent_ppo2.py:185][0m |          -0.0094 |         152.1843 |          13.0495 |
[32m[20221213 21:22:06 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:22:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 709.80
[32m[20221213 21:22:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.00
[32m[20221213 21:22:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.00
[32m[20221213 21:22:06 @agent_ppo2.py:143][0m Total time:      26.53 min
[32m[20221213 21:22:06 @agent_ppo2.py:145][0m 2603008 total steps have happened
[32m[20221213 21:22:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1271 --------------------------#
[32m[20221213 21:22:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:22:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:07 @agent_ppo2.py:185][0m |          -0.0023 |         160.1139 |          12.9425 |
[32m[20221213 21:22:07 @agent_ppo2.py:185][0m |          -0.0022 |         157.0517 |          12.9943 |
[32m[20221213 21:22:07 @agent_ppo2.py:185][0m |          -0.0038 |         154.8932 |          12.9918 |
[32m[20221213 21:22:07 @agent_ppo2.py:185][0m |           0.0016 |         161.1029 |          12.9983 |
[32m[20221213 21:22:07 @agent_ppo2.py:185][0m |          -0.0095 |         152.6305 |          13.0326 |
[32m[20221213 21:22:07 @agent_ppo2.py:185][0m |          -0.0072 |         151.8568 |          13.0054 |
[32m[20221213 21:22:07 @agent_ppo2.py:185][0m |          -0.0111 |         150.9955 |          13.0264 |
[32m[20221213 21:22:07 @agent_ppo2.py:185][0m |          -0.0102 |         150.3359 |          13.0317 |
[32m[20221213 21:22:07 @agent_ppo2.py:185][0m |          -0.0094 |         149.6884 |          13.0497 |
[32m[20221213 21:22:07 @agent_ppo2.py:185][0m |          -0.0117 |         149.5457 |          13.0528 |
[32m[20221213 21:22:07 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:22:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 750.40
[32m[20221213 21:22:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.00
[32m[20221213 21:22:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.00
[32m[20221213 21:22:08 @agent_ppo2.py:143][0m Total time:      26.55 min
[32m[20221213 21:22:08 @agent_ppo2.py:145][0m 2605056 total steps have happened
[32m[20221213 21:22:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1272 --------------------------#
[32m[20221213 21:22:08 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:22:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:08 @agent_ppo2.py:185][0m |          -0.0019 |         156.5479 |          12.9307 |
[32m[20221213 21:22:08 @agent_ppo2.py:185][0m |          -0.0062 |         154.9432 |          12.9294 |
[32m[20221213 21:22:08 @agent_ppo2.py:185][0m |          -0.0075 |         154.0202 |          12.9490 |
[32m[20221213 21:22:08 @agent_ppo2.py:185][0m |          -0.0081 |         153.4798 |          12.9666 |
[32m[20221213 21:22:08 @agent_ppo2.py:185][0m |          -0.0099 |         152.9395 |          12.9605 |
[32m[20221213 21:22:08 @agent_ppo2.py:185][0m |          -0.0091 |         152.5226 |          12.9603 |
[32m[20221213 21:22:08 @agent_ppo2.py:185][0m |          -0.0112 |         152.0839 |          12.9891 |
[32m[20221213 21:22:08 @agent_ppo2.py:185][0m |          -0.0121 |         151.7720 |          12.9730 |
[32m[20221213 21:22:09 @agent_ppo2.py:185][0m |          -0.0091 |         151.2425 |          12.9894 |
[32m[20221213 21:22:09 @agent_ppo2.py:185][0m |          -0.0096 |         150.8303 |          13.0211 |
[32m[20221213 21:22:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:22:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 769.20
[32m[20221213 21:22:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:22:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.00
[32m[20221213 21:22:09 @agent_ppo2.py:143][0m Total time:      26.57 min
[32m[20221213 21:22:09 @agent_ppo2.py:145][0m 2607104 total steps have happened
[32m[20221213 21:22:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1273 --------------------------#
[32m[20221213 21:22:09 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:22:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:09 @agent_ppo2.py:185][0m |          -0.0014 |         159.2688 |          13.0785 |
[32m[20221213 21:22:09 @agent_ppo2.py:185][0m |          -0.0036 |         157.7241 |          13.1069 |
[32m[20221213 21:22:09 @agent_ppo2.py:185][0m |          -0.0078 |         157.0373 |          13.1202 |
[32m[20221213 21:22:09 @agent_ppo2.py:185][0m |          -0.0060 |         156.5070 |          13.1150 |
[32m[20221213 21:22:09 @agent_ppo2.py:185][0m |          -0.0054 |         156.0653 |          13.1511 |
[32m[20221213 21:22:10 @agent_ppo2.py:185][0m |          -0.0002 |         162.7513 |          13.1553 |
[32m[20221213 21:22:10 @agent_ppo2.py:185][0m |          -0.0058 |         155.5061 |          13.1677 |
[32m[20221213 21:22:10 @agent_ppo2.py:185][0m |          -0.0065 |         155.4987 |          13.1971 |
[32m[20221213 21:22:10 @agent_ppo2.py:185][0m |          -0.0104 |         154.9694 |          13.2014 |
[32m[20221213 21:22:10 @agent_ppo2.py:185][0m |          -0.0054 |         155.5989 |          13.2197 |
[32m[20221213 21:22:10 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:22:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 747.00
[32m[20221213 21:22:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.00
[32m[20221213 21:22:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 785.00
[32m[20221213 21:22:10 @agent_ppo2.py:143][0m Total time:      26.59 min
[32m[20221213 21:22:10 @agent_ppo2.py:145][0m 2609152 total steps have happened
[32m[20221213 21:22:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1274 --------------------------#
[32m[20221213 21:22:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:22:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:10 @agent_ppo2.py:185][0m |          -0.0012 |         157.0008 |          13.1742 |
[32m[20221213 21:22:10 @agent_ppo2.py:185][0m |          -0.0021 |         156.3369 |          13.1790 |
[32m[20221213 21:22:11 @agent_ppo2.py:185][0m |           0.0003 |         159.5955 |          13.1469 |
[32m[20221213 21:22:11 @agent_ppo2.py:185][0m |          -0.0053 |         155.7228 |          13.1072 |
[32m[20221213 21:22:11 @agent_ppo2.py:185][0m |          -0.0050 |         155.4635 |          13.1417 |
[32m[20221213 21:22:11 @agent_ppo2.py:185][0m |          -0.0041 |         155.6324 |          13.1391 |
[32m[20221213 21:22:11 @agent_ppo2.py:185][0m |          -0.0068 |         155.0393 |          13.1266 |
[32m[20221213 21:22:11 @agent_ppo2.py:185][0m |          -0.0079 |         154.9095 |          13.1254 |
[32m[20221213 21:22:11 @agent_ppo2.py:185][0m |          -0.0069 |         154.7043 |          13.1349 |
[32m[20221213 21:22:11 @agent_ppo2.py:185][0m |          -0.0032 |         155.1519 |          13.1260 |
[32m[20221213 21:22:11 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:22:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 725.40
[32m[20221213 21:22:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.00
[32m[20221213 21:22:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 781.00
[32m[20221213 21:22:11 @agent_ppo2.py:143][0m Total time:      26.61 min
[32m[20221213 21:22:11 @agent_ppo2.py:145][0m 2611200 total steps have happened
[32m[20221213 21:22:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1275 --------------------------#
[32m[20221213 21:22:11 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:22:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:12 @agent_ppo2.py:185][0m |           0.0018 |         156.2669 |          13.2284 |
[32m[20221213 21:22:12 @agent_ppo2.py:185][0m |          -0.0039 |         155.4013 |          13.1740 |
[32m[20221213 21:22:12 @agent_ppo2.py:185][0m |          -0.0028 |         154.6177 |          13.2186 |
[32m[20221213 21:22:12 @agent_ppo2.py:185][0m |          -0.0042 |         154.1770 |          13.2017 |
[32m[20221213 21:22:12 @agent_ppo2.py:185][0m |          -0.0021 |         154.0411 |          13.1888 |
[32m[20221213 21:22:12 @agent_ppo2.py:185][0m |          -0.0041 |         153.5136 |          13.1975 |
[32m[20221213 21:22:12 @agent_ppo2.py:185][0m |          -0.0046 |         153.2905 |          13.1692 |
[32m[20221213 21:22:12 @agent_ppo2.py:185][0m |          -0.0048 |         153.1087 |          13.1785 |
[32m[20221213 21:22:12 @agent_ppo2.py:185][0m |          -0.0054 |         152.7496 |          13.2148 |
[32m[20221213 21:22:12 @agent_ppo2.py:185][0m |          -0.0086 |         152.7181 |          13.2030 |
[32m[20221213 21:22:12 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:22:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 726.00
[32m[20221213 21:22:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.00
[32m[20221213 21:22:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.00
[32m[20221213 21:22:13 @agent_ppo2.py:143][0m Total time:      26.63 min
[32m[20221213 21:22:13 @agent_ppo2.py:145][0m 2613248 total steps have happened
[32m[20221213 21:22:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1276 --------------------------#
[32m[20221213 21:22:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:13 @agent_ppo2.py:185][0m |          -0.0009 |         155.2391 |          13.1898 |
[32m[20221213 21:22:13 @agent_ppo2.py:185][0m |          -0.0015 |         153.9895 |          13.2089 |
[32m[20221213 21:22:13 @agent_ppo2.py:185][0m |          -0.0010 |         153.2237 |          13.2173 |
[32m[20221213 21:22:13 @agent_ppo2.py:185][0m |          -0.0064 |         151.2772 |          13.2717 |
[32m[20221213 21:22:13 @agent_ppo2.py:185][0m |          -0.0035 |         151.7646 |          13.2534 |
[32m[20221213 21:22:13 @agent_ppo2.py:185][0m |          -0.0088 |         150.2384 |          13.2449 |
[32m[20221213 21:22:13 @agent_ppo2.py:185][0m |          -0.0036 |         151.8900 |          13.2734 |
[32m[20221213 21:22:13 @agent_ppo2.py:185][0m |          -0.0080 |         149.3636 |          13.2655 |
[32m[20221213 21:22:14 @agent_ppo2.py:185][0m |           0.0020 |         161.9701 |          13.2791 |
[32m[20221213 21:22:14 @agent_ppo2.py:185][0m |          -0.0078 |         149.0748 |          13.2801 |
[32m[20221213 21:22:14 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:22:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.80
[32m[20221213 21:22:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.00
[32m[20221213 21:22:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:22:14 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 848.00
[32m[20221213 21:22:14 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 848.00
[32m[20221213 21:22:14 @agent_ppo2.py:143][0m Total time:      26.65 min
[32m[20221213 21:22:14 @agent_ppo2.py:145][0m 2615296 total steps have happened
[32m[20221213 21:22:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1277 --------------------------#
[32m[20221213 21:22:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:14 @agent_ppo2.py:185][0m |           0.0217 |         170.6104 |          12.8961 |
[32m[20221213 21:22:14 @agent_ppo2.py:185][0m |          -0.0024 |         155.8491 |          12.8743 |
[32m[20221213 21:22:14 @agent_ppo2.py:185][0m |          -0.0056 |         155.2028 |          12.9058 |
[32m[20221213 21:22:14 @agent_ppo2.py:185][0m |           0.0210 |         184.6822 |          12.9165 |
[32m[20221213 21:22:14 @agent_ppo2.py:185][0m |          -0.0012 |         154.7081 |          12.9080 |
[32m[20221213 21:22:15 @agent_ppo2.py:185][0m |          -0.0072 |         154.0581 |          12.8982 |
[32m[20221213 21:22:15 @agent_ppo2.py:185][0m |          -0.0039 |         154.1443 |          12.8943 |
[32m[20221213 21:22:15 @agent_ppo2.py:185][0m |          -0.0068 |         154.0923 |          12.8915 |
[32m[20221213 21:22:15 @agent_ppo2.py:185][0m |          -0.0048 |         153.6459 |          12.9206 |
[32m[20221213 21:22:15 @agent_ppo2.py:185][0m |          -0.0060 |         153.4693 |          12.9098 |
[32m[20221213 21:22:15 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:22:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 734.00
[32m[20221213 21:22:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.00
[32m[20221213 21:22:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.00
[32m[20221213 21:22:15 @agent_ppo2.py:143][0m Total time:      26.67 min
[32m[20221213 21:22:15 @agent_ppo2.py:145][0m 2617344 total steps have happened
[32m[20221213 21:22:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1278 --------------------------#
[32m[20221213 21:22:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:15 @agent_ppo2.py:185][0m |           0.0014 |         158.6272 |          12.9912 |
[32m[20221213 21:22:15 @agent_ppo2.py:185][0m |           0.0023 |         162.4105 |          13.0347 |
[32m[20221213 21:22:16 @agent_ppo2.py:185][0m |           0.0062 |         175.4831 |          13.0769 |
[32m[20221213 21:22:16 @agent_ppo2.py:185][0m |          -0.0075 |         155.3536 |          13.0685 |
[32m[20221213 21:22:16 @agent_ppo2.py:185][0m |          -0.0092 |         154.8754 |          13.0848 |
[32m[20221213 21:22:16 @agent_ppo2.py:185][0m |          -0.0101 |         154.5916 |          13.0671 |
[32m[20221213 21:22:16 @agent_ppo2.py:185][0m |           0.0089 |         170.4644 |          13.0694 |
[32m[20221213 21:22:16 @agent_ppo2.py:185][0m |          -0.0108 |         154.2260 |          13.1217 |
[32m[20221213 21:22:16 @agent_ppo2.py:185][0m |          -0.0099 |         154.0271 |          13.0885 |
[32m[20221213 21:22:16 @agent_ppo2.py:185][0m |          -0.0121 |         154.0397 |          13.1046 |
[32m[20221213 21:22:16 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:22:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.60
[32m[20221213 21:22:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.00
[32m[20221213 21:22:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.00
[32m[20221213 21:22:17 @agent_ppo2.py:143][0m Total time:      26.70 min
[32m[20221213 21:22:17 @agent_ppo2.py:145][0m 2619392 total steps have happened
[32m[20221213 21:22:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1279 --------------------------#
[32m[20221213 21:22:17 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:22:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:17 @agent_ppo2.py:185][0m |          -0.0020 |         153.2112 |          13.0244 |
[32m[20221213 21:22:17 @agent_ppo2.py:185][0m |           0.0055 |         158.4188 |          13.0368 |
[32m[20221213 21:22:17 @agent_ppo2.py:185][0m |          -0.0047 |         151.2599 |          13.0181 |
[32m[20221213 21:22:17 @agent_ppo2.py:185][0m |           0.0014 |         154.1012 |          13.0279 |
[32m[20221213 21:22:17 @agent_ppo2.py:185][0m |           0.0102 |         165.7290 |          13.0105 |
[32m[20221213 21:22:17 @agent_ppo2.py:185][0m |          -0.0086 |         150.0740 |          13.0550 |
[32m[20221213 21:22:17 @agent_ppo2.py:185][0m |          -0.0064 |         149.6655 |          13.0788 |
[32m[20221213 21:22:17 @agent_ppo2.py:185][0m |           0.0071 |         171.1865 |          13.0751 |
[32m[20221213 21:22:18 @agent_ppo2.py:185][0m |          -0.0067 |         149.1612 |          13.0865 |
[32m[20221213 21:22:18 @agent_ppo2.py:185][0m |          -0.0084 |         149.0241 |          13.0995 |
[32m[20221213 21:22:18 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:22:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 732.80
[32m[20221213 21:22:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 754.00
[32m[20221213 21:22:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.00
[32m[20221213 21:22:18 @agent_ppo2.py:143][0m Total time:      26.72 min
[32m[20221213 21:22:18 @agent_ppo2.py:145][0m 2621440 total steps have happened
[32m[20221213 21:22:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1280 --------------------------#
[32m[20221213 21:22:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:18 @agent_ppo2.py:185][0m |          -0.0007 |         157.0526 |          13.1301 |
[32m[20221213 21:22:18 @agent_ppo2.py:185][0m |          -0.0041 |         156.2645 |          13.1650 |
[32m[20221213 21:22:18 @agent_ppo2.py:185][0m |          -0.0070 |         155.6912 |          13.1686 |
[32m[20221213 21:22:18 @agent_ppo2.py:185][0m |           0.0066 |         169.0161 |          13.1671 |
[32m[20221213 21:22:18 @agent_ppo2.py:185][0m |          -0.0032 |         156.0318 |          13.1889 |
[32m[20221213 21:22:19 @agent_ppo2.py:185][0m |          -0.0070 |         154.6896 |          13.2048 |
[32m[20221213 21:22:19 @agent_ppo2.py:185][0m |          -0.0089 |         154.7815 |          13.2148 |
[32m[20221213 21:22:19 @agent_ppo2.py:185][0m |           0.0029 |         160.3847 |          13.2161 |
[32m[20221213 21:22:19 @agent_ppo2.py:185][0m |          -0.0042 |         154.6273 |          13.2199 |
[32m[20221213 21:22:19 @agent_ppo2.py:185][0m |          -0.0062 |         154.1668 |          13.2227 |
[32m[20221213 21:22:19 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 21:22:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 714.20
[32m[20221213 21:22:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.00
[32m[20221213 21:22:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.00
[32m[20221213 21:22:19 @agent_ppo2.py:143][0m Total time:      26.74 min
[32m[20221213 21:22:19 @agent_ppo2.py:145][0m 2623488 total steps have happened
[32m[20221213 21:22:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1281 --------------------------#
[32m[20221213 21:22:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:19 @agent_ppo2.py:185][0m |          -0.0014 |         149.5529 |          12.9932 |
[32m[20221213 21:22:20 @agent_ppo2.py:185][0m |          -0.0050 |         147.6824 |          13.0644 |
[32m[20221213 21:22:20 @agent_ppo2.py:185][0m |          -0.0068 |         146.4004 |          13.0826 |
[32m[20221213 21:22:20 @agent_ppo2.py:185][0m |          -0.0085 |         145.6068 |          13.0910 |
[32m[20221213 21:22:20 @agent_ppo2.py:185][0m |          -0.0051 |         145.0299 |          13.0800 |
[32m[20221213 21:22:20 @agent_ppo2.py:185][0m |          -0.0037 |         146.6005 |          13.0802 |
[32m[20221213 21:22:20 @agent_ppo2.py:185][0m |          -0.0088 |         144.0107 |          13.1137 |
[32m[20221213 21:22:20 @agent_ppo2.py:185][0m |          -0.0112 |         143.5748 |          13.1035 |
[32m[20221213 21:22:20 @agent_ppo2.py:185][0m |          -0.0108 |         143.1659 |          13.0959 |
[32m[20221213 21:22:20 @agent_ppo2.py:185][0m |          -0.0094 |         143.1378 |          13.1017 |
[32m[20221213 21:22:20 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 21:22:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.00
[32m[20221213 21:22:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.00
[32m[20221213 21:22:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.00
[32m[20221213 21:22:21 @agent_ppo2.py:143][0m Total time:      26.76 min
[32m[20221213 21:22:21 @agent_ppo2.py:145][0m 2625536 total steps have happened
[32m[20221213 21:22:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1282 --------------------------#
[32m[20221213 21:22:21 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:22:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:21 @agent_ppo2.py:185][0m |          -0.0012 |         158.2604 |          13.3514 |
[32m[20221213 21:22:21 @agent_ppo2.py:185][0m |          -0.0032 |         155.3492 |          13.3284 |
[32m[20221213 21:22:21 @agent_ppo2.py:185][0m |          -0.0064 |         154.3597 |          13.3349 |
[32m[20221213 21:22:21 @agent_ppo2.py:185][0m |          -0.0057 |         153.9172 |          13.3270 |
[32m[20221213 21:22:21 @agent_ppo2.py:185][0m |          -0.0056 |         153.4167 |          13.3104 |
[32m[20221213 21:22:21 @agent_ppo2.py:185][0m |          -0.0063 |         153.1621 |          13.3267 |
[32m[20221213 21:22:21 @agent_ppo2.py:185][0m |          -0.0078 |         152.6643 |          13.3213 |
[32m[20221213 21:22:22 @agent_ppo2.py:185][0m |          -0.0044 |         153.1566 |          13.3176 |
[32m[20221213 21:22:22 @agent_ppo2.py:185][0m |          -0.0080 |         152.5113 |          13.3293 |
[32m[20221213 21:22:22 @agent_ppo2.py:185][0m |          -0.0094 |         152.0637 |          13.3091 |
[32m[20221213 21:22:22 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:22:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 720.60
[32m[20221213 21:22:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 754.00
[32m[20221213 21:22:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.00
[32m[20221213 21:22:22 @agent_ppo2.py:143][0m Total time:      26.79 min
[32m[20221213 21:22:22 @agent_ppo2.py:145][0m 2627584 total steps have happened
[32m[20221213 21:22:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1283 --------------------------#
[32m[20221213 21:22:22 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:22:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:22 @agent_ppo2.py:185][0m |          -0.0005 |         154.9420 |          13.2185 |
[32m[20221213 21:22:22 @agent_ppo2.py:185][0m |          -0.0051 |         153.4741 |          13.1720 |
[32m[20221213 21:22:23 @agent_ppo2.py:185][0m |          -0.0061 |         153.0078 |          13.1902 |
[32m[20221213 21:22:23 @agent_ppo2.py:185][0m |           0.0025 |         163.1897 |          13.1387 |
[32m[20221213 21:22:23 @agent_ppo2.py:185][0m |          -0.0070 |         152.5734 |          13.1417 |
[32m[20221213 21:22:23 @agent_ppo2.py:185][0m |          -0.0062 |         152.1823 |          13.1433 |
[32m[20221213 21:22:23 @agent_ppo2.py:185][0m |          -0.0065 |         152.1118 |          13.0957 |
[32m[20221213 21:22:23 @agent_ppo2.py:185][0m |          -0.0074 |         151.9435 |          13.0718 |
[32m[20221213 21:22:23 @agent_ppo2.py:185][0m |          -0.0090 |         151.6663 |          13.0302 |
[32m[20221213 21:22:23 @agent_ppo2.py:185][0m |          -0.0110 |         151.4732 |          13.0084 |
[32m[20221213 21:22:23 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:22:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.20
[32m[20221213 21:22:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 757.00
[32m[20221213 21:22:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 766.00
[32m[20221213 21:22:23 @agent_ppo2.py:143][0m Total time:      26.81 min
[32m[20221213 21:22:23 @agent_ppo2.py:145][0m 2629632 total steps have happened
[32m[20221213 21:22:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1284 --------------------------#
[32m[20221213 21:22:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:22:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:24 @agent_ppo2.py:185][0m |           0.0003 |         151.0077 |          13.1990 |
[32m[20221213 21:22:24 @agent_ppo2.py:185][0m |          -0.0039 |         150.0935 |          13.2350 |
[32m[20221213 21:22:24 @agent_ppo2.py:185][0m |          -0.0059 |         149.3446 |          13.2367 |
[32m[20221213 21:22:24 @agent_ppo2.py:185][0m |          -0.0005 |         152.9315 |          13.2500 |
[32m[20221213 21:22:24 @agent_ppo2.py:185][0m |          -0.0053 |         148.5124 |          13.2707 |
[32m[20221213 21:22:24 @agent_ppo2.py:185][0m |          -0.0069 |         148.1358 |          13.2253 |
[32m[20221213 21:22:24 @agent_ppo2.py:185][0m |          -0.0073 |         147.7932 |          13.2449 |
[32m[20221213 21:22:24 @agent_ppo2.py:185][0m |          -0.0084 |         147.7141 |          13.2594 |
[32m[20221213 21:22:24 @agent_ppo2.py:185][0m |          -0.0089 |         147.4462 |          13.2376 |
[32m[20221213 21:22:24 @agent_ppo2.py:185][0m |          -0.0085 |         147.1404 |          13.2490 |
[32m[20221213 21:22:24 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:22:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.00
[32m[20221213 21:22:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.00
[32m[20221213 21:22:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.00
[32m[20221213 21:22:25 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 850.00
[32m[20221213 21:22:25 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 850.00
[32m[20221213 21:22:25 @agent_ppo2.py:143][0m Total time:      26.83 min
[32m[20221213 21:22:25 @agent_ppo2.py:145][0m 2631680 total steps have happened
[32m[20221213 21:22:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1285 --------------------------#
[32m[20221213 21:22:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:25 @agent_ppo2.py:185][0m |           0.0093 |         159.8568 |          13.0802 |
[32m[20221213 21:22:25 @agent_ppo2.py:185][0m |           0.0029 |         152.4320 |          13.0613 |
[32m[20221213 21:22:25 @agent_ppo2.py:185][0m |          -0.0028 |         152.2007 |          13.0403 |
[32m[20221213 21:22:25 @agent_ppo2.py:185][0m |           0.0087 |         171.0788 |          13.0309 |
[32m[20221213 21:22:25 @agent_ppo2.py:185][0m |           0.0077 |         165.5270 |          12.9993 |
[32m[20221213 21:22:25 @agent_ppo2.py:185][0m |          -0.0054 |         151.3867 |          13.0273 |
[32m[20221213 21:22:25 @agent_ppo2.py:185][0m |          -0.0082 |         149.1495 |          13.0075 |
[32m[20221213 21:22:26 @agent_ppo2.py:185][0m |          -0.0096 |         148.8483 |          12.9951 |
[32m[20221213 21:22:26 @agent_ppo2.py:185][0m |          -0.0100 |         148.5875 |          12.9714 |
[32m[20221213 21:22:26 @agent_ppo2.py:185][0m |          -0.0107 |         148.4813 |          12.9481 |
[32m[20221213 21:22:26 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:22:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 711.40
[32m[20221213 21:22:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.00
[32m[20221213 21:22:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.00
[32m[20221213 21:22:26 @agent_ppo2.py:143][0m Total time:      26.85 min
[32m[20221213 21:22:26 @agent_ppo2.py:145][0m 2633728 total steps have happened
[32m[20221213 21:22:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1286 --------------------------#
[32m[20221213 21:22:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:26 @agent_ppo2.py:185][0m |           0.0020 |         154.9273 |          12.6527 |
[32m[20221213 21:22:26 @agent_ppo2.py:185][0m |          -0.0036 |         153.4054 |          12.6473 |
[32m[20221213 21:22:26 @agent_ppo2.py:185][0m |          -0.0048 |         152.5325 |          12.6575 |
[32m[20221213 21:22:26 @agent_ppo2.py:185][0m |          -0.0036 |         152.5342 |          12.6475 |
[32m[20221213 21:22:27 @agent_ppo2.py:185][0m |          -0.0054 |         151.5056 |          12.6609 |
[32m[20221213 21:22:27 @agent_ppo2.py:185][0m |          -0.0055 |         151.1339 |          12.6674 |
[32m[20221213 21:22:27 @agent_ppo2.py:185][0m |          -0.0068 |         150.7236 |          12.6534 |
[32m[20221213 21:22:27 @agent_ppo2.py:185][0m |          -0.0069 |         150.5128 |          12.6558 |
[32m[20221213 21:22:27 @agent_ppo2.py:185][0m |          -0.0065 |         150.3111 |          12.6346 |
[32m[20221213 21:22:27 @agent_ppo2.py:185][0m |          -0.0048 |         150.2599 |          12.6344 |
[32m[20221213 21:22:27 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 21:22:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.00
[32m[20221213 21:22:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 21:22:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.00
[32m[20221213 21:22:27 @agent_ppo2.py:143][0m Total time:      26.88 min
[32m[20221213 21:22:27 @agent_ppo2.py:145][0m 2635776 total steps have happened
[32m[20221213 21:22:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1287 --------------------------#
[32m[20221213 21:22:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:22:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:28 @agent_ppo2.py:185][0m |           0.0039 |         150.9274 |          12.7729 |
[32m[20221213 21:22:28 @agent_ppo2.py:185][0m |          -0.0024 |         149.3695 |          12.7173 |
[32m[20221213 21:22:28 @agent_ppo2.py:185][0m |          -0.0046 |         148.6348 |          12.7247 |
[32m[20221213 21:22:28 @agent_ppo2.py:185][0m |          -0.0068 |         148.0960 |          12.6718 |
[32m[20221213 21:22:28 @agent_ppo2.py:185][0m |          -0.0076 |         147.7089 |          12.6605 |
[32m[20221213 21:22:28 @agent_ppo2.py:185][0m |           0.0120 |         171.1869 |          12.6368 |
[32m[20221213 21:22:28 @agent_ppo2.py:185][0m |          -0.0069 |         147.2217 |          12.6322 |
[32m[20221213 21:22:28 @agent_ppo2.py:185][0m |          -0.0049 |         147.5740 |          12.6009 |
[32m[20221213 21:22:28 @agent_ppo2.py:185][0m |           0.0055 |         168.2625 |          12.5626 |
[32m[20221213 21:22:28 @agent_ppo2.py:185][0m |          -0.0074 |         146.4009 |          12.6514 |
[32m[20221213 21:22:28 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:22:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.60
[32m[20221213 21:22:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 779.00
[32m[20221213 21:22:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.00
[32m[20221213 21:22:29 @agent_ppo2.py:143][0m Total time:      26.90 min
[32m[20221213 21:22:29 @agent_ppo2.py:145][0m 2637824 total steps have happened
[32m[20221213 21:22:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1288 --------------------------#
[32m[20221213 21:22:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:29 @agent_ppo2.py:185][0m |          -0.0009 |         155.8194 |          12.7219 |
[32m[20221213 21:22:29 @agent_ppo2.py:185][0m |          -0.0046 |         154.1686 |          12.7415 |
[32m[20221213 21:22:29 @agent_ppo2.py:185][0m |           0.0045 |         155.0284 |          12.7477 |
[32m[20221213 21:22:29 @agent_ppo2.py:185][0m |          -0.0062 |         152.4571 |          12.7249 |
[32m[20221213 21:22:29 @agent_ppo2.py:185][0m |           0.0018 |         156.8733 |          12.7421 |
[32m[20221213 21:22:29 @agent_ppo2.py:185][0m |          -0.0002 |         158.2136 |          12.7587 |
[32m[20221213 21:22:29 @agent_ppo2.py:185][0m |          -0.0041 |         151.8251 |          12.7577 |
[32m[20221213 21:22:29 @agent_ppo2.py:185][0m |           0.0011 |         156.5124 |          12.7964 |
[32m[20221213 21:22:30 @agent_ppo2.py:185][0m |          -0.0092 |         150.8077 |          12.7980 |
[32m[20221213 21:22:30 @agent_ppo2.py:185][0m |          -0.0086 |         150.3191 |          12.8124 |
[32m[20221213 21:22:30 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:22:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 762.80
[32m[20221213 21:22:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.00
[32m[20221213 21:22:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.00
[32m[20221213 21:22:30 @agent_ppo2.py:143][0m Total time:      26.92 min
[32m[20221213 21:22:30 @agent_ppo2.py:145][0m 2639872 total steps have happened
[32m[20221213 21:22:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1289 --------------------------#
[32m[20221213 21:22:30 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:22:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:30 @agent_ppo2.py:185][0m |           0.0040 |         158.7426 |          12.4875 |
[32m[20221213 21:22:30 @agent_ppo2.py:185][0m |           0.0034 |         156.8567 |          12.5016 |
[32m[20221213 21:22:30 @agent_ppo2.py:185][0m |           0.0116 |         176.1270 |          12.4941 |
[32m[20221213 21:22:30 @agent_ppo2.py:185][0m |          -0.0027 |         155.0170 |          12.5118 |
[32m[20221213 21:22:30 @agent_ppo2.py:185][0m |          -0.0031 |         154.5648 |          12.4580 |
[32m[20221213 21:22:31 @agent_ppo2.py:185][0m |          -0.0031 |         155.0472 |          12.4654 |
[32m[20221213 21:22:31 @agent_ppo2.py:185][0m |          -0.0054 |         154.2533 |          12.4477 |
[32m[20221213 21:22:31 @agent_ppo2.py:185][0m |          -0.0064 |         154.2047 |          12.4283 |
[32m[20221213 21:22:31 @agent_ppo2.py:185][0m |          -0.0061 |         153.9406 |          12.4516 |
[32m[20221213 21:22:31 @agent_ppo2.py:185][0m |          -0.0034 |         154.5605 |          12.4193 |
[32m[20221213 21:22:31 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:22:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.20
[32m[20221213 21:22:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 775.00
[32m[20221213 21:22:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.00
[32m[20221213 21:22:31 @agent_ppo2.py:143][0m Total time:      26.94 min
[32m[20221213 21:22:31 @agent_ppo2.py:145][0m 2641920 total steps have happened
[32m[20221213 21:22:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1290 --------------------------#
[32m[20221213 21:22:31 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:22:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:31 @agent_ppo2.py:185][0m |          -0.0014 |         155.4612 |          12.6589 |
[32m[20221213 21:22:31 @agent_ppo2.py:185][0m |          -0.0002 |         155.4536 |          12.6833 |
[32m[20221213 21:22:32 @agent_ppo2.py:185][0m |          -0.0033 |         153.4811 |          12.7055 |
[32m[20221213 21:22:32 @agent_ppo2.py:185][0m |          -0.0031 |         152.5846 |          12.7234 |
[32m[20221213 21:22:32 @agent_ppo2.py:185][0m |          -0.0054 |         151.9131 |          12.7210 |
[32m[20221213 21:22:32 @agent_ppo2.py:185][0m |           0.0016 |         158.3948 |          12.7293 |
[32m[20221213 21:22:32 @agent_ppo2.py:185][0m |          -0.0042 |         150.9951 |          12.7640 |
[32m[20221213 21:22:32 @agent_ppo2.py:185][0m |          -0.0069 |         150.8052 |          12.7550 |
[32m[20221213 21:22:32 @agent_ppo2.py:185][0m |          -0.0075 |         150.2819 |          12.7780 |
[32m[20221213 21:22:32 @agent_ppo2.py:185][0m |           0.0037 |         162.5624 |          12.7622 |
[32m[20221213 21:22:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:22:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.20
[32m[20221213 21:22:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.00
[32m[20221213 21:22:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.00
[32m[20221213 21:22:32 @agent_ppo2.py:143][0m Total time:      26.96 min
[32m[20221213 21:22:32 @agent_ppo2.py:145][0m 2643968 total steps have happened
[32m[20221213 21:22:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1291 --------------------------#
[32m[20221213 21:22:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:33 @agent_ppo2.py:185][0m |          -0.0012 |         155.0032 |          12.6259 |
[32m[20221213 21:22:33 @agent_ppo2.py:185][0m |           0.0148 |         171.8306 |          12.6079 |
[32m[20221213 21:22:33 @agent_ppo2.py:185][0m |           0.0084 |         165.6939 |          12.6086 |
[32m[20221213 21:22:33 @agent_ppo2.py:185][0m |          -0.0052 |         151.9478 |          12.6021 |
[32m[20221213 21:22:33 @agent_ppo2.py:185][0m |           0.0045 |         164.0466 |          12.5756 |
[32m[20221213 21:22:33 @agent_ppo2.py:185][0m |          -0.0067 |         150.9302 |          12.5591 |
[32m[20221213 21:22:33 @agent_ppo2.py:185][0m |          -0.0094 |         150.7127 |          12.5323 |
[32m[20221213 21:22:33 @agent_ppo2.py:185][0m |          -0.0105 |         150.5524 |          12.5314 |
[32m[20221213 21:22:33 @agent_ppo2.py:185][0m |          -0.0011 |         159.1903 |          12.5032 |
[32m[20221213 21:22:33 @agent_ppo2.py:185][0m |          -0.0080 |         149.9356 |          12.5442 |
[32m[20221213 21:22:33 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:22:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 751.80
[32m[20221213 21:22:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.00
[32m[20221213 21:22:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.00
[32m[20221213 21:22:34 @agent_ppo2.py:143][0m Total time:      26.98 min
[32m[20221213 21:22:34 @agent_ppo2.py:145][0m 2646016 total steps have happened
[32m[20221213 21:22:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1292 --------------------------#
[32m[20221213 21:22:34 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:22:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:34 @agent_ppo2.py:185][0m |           0.0088 |         173.2230 |          12.4458 |
[32m[20221213 21:22:34 @agent_ppo2.py:185][0m |          -0.0039 |         156.7302 |          12.5421 |
[32m[20221213 21:22:34 @agent_ppo2.py:185][0m |          -0.0063 |         156.2576 |          12.4327 |
[32m[20221213 21:22:34 @agent_ppo2.py:185][0m |          -0.0068 |         155.9259 |          12.4241 |
[32m[20221213 21:22:34 @agent_ppo2.py:185][0m |          -0.0095 |         155.7172 |          12.4205 |
[32m[20221213 21:22:34 @agent_ppo2.py:185][0m |          -0.0072 |         155.4814 |          12.3908 |
[32m[20221213 21:22:34 @agent_ppo2.py:185][0m |          -0.0068 |         155.1757 |          12.3927 |
[32m[20221213 21:22:35 @agent_ppo2.py:185][0m |          -0.0104 |         154.9966 |          12.4010 |
[32m[20221213 21:22:35 @agent_ppo2.py:185][0m |          -0.0094 |         154.8933 |          12.4118 |
[32m[20221213 21:22:35 @agent_ppo2.py:185][0m |          -0.0102 |         154.6479 |          12.3839 |
[32m[20221213 21:22:35 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:22:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.20
[32m[20221213 21:22:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.00
[32m[20221213 21:22:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.00
[32m[20221213 21:22:35 @agent_ppo2.py:143][0m Total time:      27.00 min
[32m[20221213 21:22:35 @agent_ppo2.py:145][0m 2648064 total steps have happened
[32m[20221213 21:22:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1293 --------------------------#
[32m[20221213 21:22:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:22:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:35 @agent_ppo2.py:185][0m |          -0.0016 |         157.9259 |          12.5393 |
[32m[20221213 21:22:35 @agent_ppo2.py:185][0m |          -0.0058 |         156.0356 |          12.5711 |
[32m[20221213 21:22:35 @agent_ppo2.py:185][0m |          -0.0055 |         154.6906 |          12.5181 |
[32m[20221213 21:22:35 @agent_ppo2.py:185][0m |           0.0074 |         168.2651 |          12.5592 |
[32m[20221213 21:22:36 @agent_ppo2.py:185][0m |           0.0102 |         164.9047 |          12.5339 |
[32m[20221213 21:22:36 @agent_ppo2.py:185][0m |          -0.0057 |         152.8856 |          12.5220 |
[32m[20221213 21:22:36 @agent_ppo2.py:185][0m |          -0.0087 |         152.0003 |          12.5391 |
[32m[20221213 21:22:36 @agent_ppo2.py:185][0m |          -0.0089 |         151.5750 |          12.5019 |
[32m[20221213 21:22:36 @agent_ppo2.py:185][0m |          -0.0088 |         151.3214 |          12.5348 |
[32m[20221213 21:22:36 @agent_ppo2.py:185][0m |          -0.0094 |         151.0866 |          12.5178 |
[32m[20221213 21:22:36 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 21:22:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 728.60
[32m[20221213 21:22:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.00
[32m[20221213 21:22:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 21:22:36 @agent_ppo2.py:143][0m Total time:      27.03 min
[32m[20221213 21:22:36 @agent_ppo2.py:145][0m 2650112 total steps have happened
[32m[20221213 21:22:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1294 --------------------------#
[32m[20221213 21:22:36 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:22:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:37 @agent_ppo2.py:185][0m |          -0.0025 |         159.2388 |          12.7861 |
[32m[20221213 21:22:37 @agent_ppo2.py:185][0m |          -0.0062 |         157.0430 |          12.7976 |
[32m[20221213 21:22:37 @agent_ppo2.py:185][0m |          -0.0070 |         155.4821 |          12.7826 |
[32m[20221213 21:22:37 @agent_ppo2.py:185][0m |          -0.0069 |         153.2125 |          12.7980 |
[32m[20221213 21:22:37 @agent_ppo2.py:185][0m |          -0.0085 |         151.0736 |          12.8107 |
[32m[20221213 21:22:37 @agent_ppo2.py:185][0m |          -0.0096 |         149.8863 |          12.7751 |
[32m[20221213 21:22:37 @agent_ppo2.py:185][0m |          -0.0080 |         149.1292 |          12.7814 |
[32m[20221213 21:22:37 @agent_ppo2.py:185][0m |          -0.0053 |         150.1049 |          12.7776 |
[32m[20221213 21:22:37 @agent_ppo2.py:185][0m |          -0.0094 |         147.0180 |          12.7443 |
[32m[20221213 21:22:37 @agent_ppo2.py:185][0m |          -0.0067 |         148.2200 |          12.7598 |
[32m[20221213 21:22:37 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:22:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.00
[32m[20221213 21:22:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:22:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.00
[32m[20221213 21:22:37 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 852.00
[32m[20221213 21:22:37 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 852.00
[32m[20221213 21:22:37 @agent_ppo2.py:143][0m Total time:      27.05 min
[32m[20221213 21:22:37 @agent_ppo2.py:145][0m 2652160 total steps have happened
[32m[20221213 21:22:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1295 --------------------------#
[32m[20221213 21:22:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:38 @agent_ppo2.py:185][0m |          -0.0013 |         163.3533 |          12.5236 |
[32m[20221213 21:22:38 @agent_ppo2.py:185][0m |          -0.0039 |         160.6626 |          12.5978 |
[32m[20221213 21:22:38 @agent_ppo2.py:185][0m |          -0.0044 |         159.5753 |          12.5812 |
[32m[20221213 21:22:38 @agent_ppo2.py:185][0m |           0.0094 |         181.3900 |          12.5942 |
[32m[20221213 21:22:38 @agent_ppo2.py:185][0m |          -0.0011 |         159.6367 |          12.6186 |
[32m[20221213 21:22:38 @agent_ppo2.py:185][0m |          -0.0049 |         158.1261 |          12.5938 |
[32m[20221213 21:22:38 @agent_ppo2.py:185][0m |          -0.0095 |         157.9973 |          12.5979 |
[32m[20221213 21:22:38 @agent_ppo2.py:185][0m |          -0.0058 |         157.8113 |          12.6018 |
[32m[20221213 21:22:38 @agent_ppo2.py:185][0m |          -0.0078 |         157.5395 |          12.5847 |
[32m[20221213 21:22:39 @agent_ppo2.py:185][0m |          -0.0078 |         157.2670 |          12.5811 |
[32m[20221213 21:22:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:22:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.20
[32m[20221213 21:22:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:22:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.00
[32m[20221213 21:22:39 @agent_ppo2.py:143][0m Total time:      27.07 min
[32m[20221213 21:22:39 @agent_ppo2.py:145][0m 2654208 total steps have happened
[32m[20221213 21:22:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1296 --------------------------#
[32m[20221213 21:22:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:39 @agent_ppo2.py:185][0m |          -0.0016 |         157.4057 |          12.4529 |
[32m[20221213 21:22:39 @agent_ppo2.py:185][0m |          -0.0054 |         156.6505 |          12.4834 |
[32m[20221213 21:22:39 @agent_ppo2.py:185][0m |          -0.0055 |         156.3797 |          12.4597 |
[32m[20221213 21:22:39 @agent_ppo2.py:185][0m |          -0.0074 |         156.0636 |          12.4699 |
[32m[20221213 21:22:39 @agent_ppo2.py:185][0m |          -0.0059 |         155.6489 |          12.4866 |
[32m[20221213 21:22:39 @agent_ppo2.py:185][0m |          -0.0089 |         155.5459 |          12.4870 |
[32m[20221213 21:22:40 @agent_ppo2.py:185][0m |          -0.0089 |         155.4344 |          12.4552 |
[32m[20221213 21:22:40 @agent_ppo2.py:185][0m |          -0.0008 |         165.4087 |          12.4722 |
[32m[20221213 21:22:40 @agent_ppo2.py:185][0m |          -0.0058 |         155.3850 |          12.4795 |
[32m[20221213 21:22:40 @agent_ppo2.py:185][0m |          -0.0084 |         155.1271 |          12.4866 |
[32m[20221213 21:22:40 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:22:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.20
[32m[20221213 21:22:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.00
[32m[20221213 21:22:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.00
[32m[20221213 21:22:40 @agent_ppo2.py:143][0m Total time:      27.09 min
[32m[20221213 21:22:40 @agent_ppo2.py:145][0m 2656256 total steps have happened
[32m[20221213 21:22:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1297 --------------------------#
[32m[20221213 21:22:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:22:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:40 @agent_ppo2.py:185][0m |           0.0007 |         159.0900 |          12.5168 |
[32m[20221213 21:22:40 @agent_ppo2.py:185][0m |          -0.0057 |         157.7158 |          12.5376 |
[32m[20221213 21:22:40 @agent_ppo2.py:185][0m |          -0.0044 |         157.0027 |          12.5668 |
[32m[20221213 21:22:41 @agent_ppo2.py:185][0m |          -0.0054 |         156.4814 |          12.5510 |
[32m[20221213 21:22:41 @agent_ppo2.py:185][0m |          -0.0069 |         156.4759 |          12.5135 |
[32m[20221213 21:22:41 @agent_ppo2.py:185][0m |          -0.0081 |         155.9117 |          12.4885 |
[32m[20221213 21:22:41 @agent_ppo2.py:185][0m |          -0.0092 |         155.7004 |          12.4900 |
[32m[20221213 21:22:41 @agent_ppo2.py:185][0m |          -0.0064 |         155.6065 |          12.4941 |
[32m[20221213 21:22:41 @agent_ppo2.py:185][0m |          -0.0065 |         156.0921 |          12.4729 |
[32m[20221213 21:22:41 @agent_ppo2.py:185][0m |          -0.0073 |         155.3395 |          12.5019 |
[32m[20221213 21:22:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:22:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.60
[32m[20221213 21:22:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.00
[32m[20221213 21:22:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 789.00
[32m[20221213 21:22:41 @agent_ppo2.py:143][0m Total time:      27.11 min
[32m[20221213 21:22:41 @agent_ppo2.py:145][0m 2658304 total steps have happened
[32m[20221213 21:22:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1298 --------------------------#
[32m[20221213 21:22:41 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:22:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:41 @agent_ppo2.py:185][0m |          -0.0035 |         156.0570 |          12.5659 |
[32m[20221213 21:22:42 @agent_ppo2.py:185][0m |          -0.0063 |         155.6210 |          12.5970 |
[32m[20221213 21:22:42 @agent_ppo2.py:185][0m |          -0.0072 |         154.9564 |          12.5900 |
[32m[20221213 21:22:42 @agent_ppo2.py:185][0m |          -0.0070 |         154.7063 |          12.6093 |
[32m[20221213 21:22:42 @agent_ppo2.py:185][0m |          -0.0055 |         154.4350 |          12.6041 |
[32m[20221213 21:22:42 @agent_ppo2.py:185][0m |          -0.0013 |         157.4348 |          12.6198 |
[32m[20221213 21:22:42 @agent_ppo2.py:185][0m |          -0.0076 |         153.8618 |          12.6070 |
[32m[20221213 21:22:42 @agent_ppo2.py:185][0m |          -0.0076 |         153.9444 |          12.6228 |
[32m[20221213 21:22:42 @agent_ppo2.py:185][0m |          -0.0023 |         158.4251 |          12.6318 |
[32m[20221213 21:22:42 @agent_ppo2.py:185][0m |          -0.0067 |         154.2987 |          12.6653 |
[32m[20221213 21:22:42 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:22:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 751.60
[32m[20221213 21:22:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.00
[32m[20221213 21:22:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.00
[32m[20221213 21:22:42 @agent_ppo2.py:143][0m Total time:      27.13 min
[32m[20221213 21:22:42 @agent_ppo2.py:145][0m 2660352 total steps have happened
[32m[20221213 21:22:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1299 --------------------------#
[32m[20221213 21:22:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:43 @agent_ppo2.py:185][0m |           0.0050 |         158.4301 |          12.6924 |
[32m[20221213 21:22:43 @agent_ppo2.py:185][0m |          -0.0042 |         156.1392 |          12.6944 |
[32m[20221213 21:22:43 @agent_ppo2.py:185][0m |          -0.0039 |         155.5637 |          12.7396 |
[32m[20221213 21:22:43 @agent_ppo2.py:185][0m |          -0.0048 |         155.0828 |          12.7388 |
[32m[20221213 21:22:43 @agent_ppo2.py:185][0m |          -0.0054 |         154.6898 |          12.7213 |
[32m[20221213 21:22:43 @agent_ppo2.py:185][0m |          -0.0074 |         154.4476 |          12.7198 |
[32m[20221213 21:22:43 @agent_ppo2.py:185][0m |          -0.0070 |         154.1890 |          12.7319 |
[32m[20221213 21:22:43 @agent_ppo2.py:185][0m |          -0.0099 |         154.1257 |          12.7225 |
[32m[20221213 21:22:43 @agent_ppo2.py:185][0m |          -0.0074 |         153.7757 |          12.7599 |
[32m[20221213 21:22:44 @agent_ppo2.py:185][0m |          -0.0070 |         153.7298 |          12.7193 |
[32m[20221213 21:22:44 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:22:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.00
[32m[20221213 21:22:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.00
[32m[20221213 21:22:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.00
[32m[20221213 21:22:44 @agent_ppo2.py:143][0m Total time:      27.15 min
[32m[20221213 21:22:44 @agent_ppo2.py:145][0m 2662400 total steps have happened
[32m[20221213 21:22:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1300 --------------------------#
[32m[20221213 21:22:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:22:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:44 @agent_ppo2.py:185][0m |          -0.0014 |         160.9455 |          12.5296 |
[32m[20221213 21:22:44 @agent_ppo2.py:185][0m |          -0.0060 |         154.6244 |          12.5538 |
[32m[20221213 21:22:44 @agent_ppo2.py:185][0m |          -0.0066 |         152.1472 |          12.5620 |
[32m[20221213 21:22:44 @agent_ppo2.py:185][0m |           0.0021 |         154.4539 |          12.5879 |
[32m[20221213 21:22:44 @agent_ppo2.py:185][0m |           0.0099 |         159.7797 |          12.5938 |
[32m[20221213 21:22:44 @agent_ppo2.py:185][0m |          -0.0076 |         149.5451 |          12.6059 |
[32m[20221213 21:22:45 @agent_ppo2.py:185][0m |          -0.0096 |         148.8197 |          12.6111 |
[32m[20221213 21:22:45 @agent_ppo2.py:185][0m |          -0.0094 |         148.3472 |          12.6047 |
[32m[20221213 21:22:45 @agent_ppo2.py:185][0m |          -0.0080 |         147.8087 |          12.6039 |
[32m[20221213 21:22:45 @agent_ppo2.py:185][0m |          -0.0075 |         147.5545 |          12.6202 |
[32m[20221213 21:22:45 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:22:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 695.60
[32m[20221213 21:22:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 775.00
[32m[20221213 21:22:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.00
[32m[20221213 21:22:45 @agent_ppo2.py:143][0m Total time:      27.17 min
[32m[20221213 21:22:45 @agent_ppo2.py:145][0m 2664448 total steps have happened
[32m[20221213 21:22:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1301 --------------------------#
[32m[20221213 21:22:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:45 @agent_ppo2.py:185][0m |          -0.0022 |         159.8324 |          12.3964 |
[32m[20221213 21:22:45 @agent_ppo2.py:185][0m |          -0.0046 |         158.3470 |          12.4222 |
[32m[20221213 21:22:45 @agent_ppo2.py:185][0m |          -0.0049 |         157.9193 |          12.4414 |
[32m[20221213 21:22:46 @agent_ppo2.py:185][0m |          -0.0069 |         156.9664 |          12.4285 |
[32m[20221213 21:22:46 @agent_ppo2.py:185][0m |           0.0111 |         178.2632 |          12.4225 |
[32m[20221213 21:22:46 @agent_ppo2.py:185][0m |          -0.0059 |         156.8275 |          12.4043 |
[32m[20221213 21:22:46 @agent_ppo2.py:185][0m |          -0.0063 |         155.9433 |          12.3992 |
[32m[20221213 21:22:46 @agent_ppo2.py:185][0m |          -0.0072 |         155.8635 |          12.4292 |
[32m[20221213 21:22:46 @agent_ppo2.py:185][0m |          -0.0102 |         155.6213 |          12.4003 |
[32m[20221213 21:22:46 @agent_ppo2.py:185][0m |          -0.0082 |         155.4704 |          12.3814 |
[32m[20221213 21:22:46 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:22:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.60
[32m[20221213 21:22:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 758.00
[32m[20221213 21:22:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.00
[32m[20221213 21:22:46 @agent_ppo2.py:143][0m Total time:      27.19 min
[32m[20221213 21:22:46 @agent_ppo2.py:145][0m 2666496 total steps have happened
[32m[20221213 21:22:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1302 --------------------------#
[32m[20221213 21:22:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:46 @agent_ppo2.py:185][0m |          -0.0011 |         157.4375 |          12.3837 |
[32m[20221213 21:22:47 @agent_ppo2.py:185][0m |          -0.0043 |         156.6536 |          12.3834 |
[32m[20221213 21:22:47 @agent_ppo2.py:185][0m |          -0.0066 |         156.2153 |          12.3713 |
[32m[20221213 21:22:47 @agent_ppo2.py:185][0m |          -0.0073 |         155.7526 |          12.3643 |
[32m[20221213 21:22:47 @agent_ppo2.py:185][0m |          -0.0058 |         155.6930 |          12.3353 |
[32m[20221213 21:22:47 @agent_ppo2.py:185][0m |           0.0048 |         166.7424 |          12.3016 |
[32m[20221213 21:22:47 @agent_ppo2.py:185][0m |           0.0020 |         157.8203 |          12.3252 |
[32m[20221213 21:22:47 @agent_ppo2.py:185][0m |          -0.0048 |         154.8789 |          12.3053 |
[32m[20221213 21:22:47 @agent_ppo2.py:185][0m |          -0.0075 |         154.7819 |          12.2780 |
[32m[20221213 21:22:47 @agent_ppo2.py:185][0m |          -0.0086 |         154.7606 |          12.2637 |
[32m[20221213 21:22:47 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:22:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 761.00
[32m[20221213 21:22:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 784.00
[32m[20221213 21:22:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.00
[32m[20221213 21:22:47 @agent_ppo2.py:143][0m Total time:      27.21 min
[32m[20221213 21:22:47 @agent_ppo2.py:145][0m 2668544 total steps have happened
[32m[20221213 21:22:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1303 --------------------------#
[32m[20221213 21:22:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:48 @agent_ppo2.py:185][0m |           0.0013 |         156.5276 |          12.2122 |
[32m[20221213 21:22:48 @agent_ppo2.py:185][0m |          -0.0021 |         155.3335 |          12.2157 |
[32m[20221213 21:22:48 @agent_ppo2.py:185][0m |          -0.0053 |         154.6515 |          12.2258 |
[32m[20221213 21:22:48 @agent_ppo2.py:185][0m |          -0.0023 |         154.4006 |          12.1887 |
[32m[20221213 21:22:48 @agent_ppo2.py:185][0m |          -0.0027 |         154.1103 |          12.1844 |
[32m[20221213 21:22:48 @agent_ppo2.py:185][0m |          -0.0067 |         152.7794 |          12.1472 |
[32m[20221213 21:22:48 @agent_ppo2.py:185][0m |          -0.0050 |         152.3977 |          12.0659 |
[32m[20221213 21:22:48 @agent_ppo2.py:185][0m |          -0.0033 |         154.0301 |          12.1396 |
[32m[20221213 21:22:48 @agent_ppo2.py:185][0m |          -0.0073 |         151.5997 |          12.0903 |
[32m[20221213 21:22:49 @agent_ppo2.py:185][0m |          -0.0049 |         152.1042 |          12.0657 |
[32m[20221213 21:22:49 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:22:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 741.40
[32m[20221213 21:22:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 784.00
[32m[20221213 21:22:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.00
[32m[20221213 21:22:49 @agent_ppo2.py:143][0m Total time:      27.23 min
[32m[20221213 21:22:49 @agent_ppo2.py:145][0m 2670592 total steps have happened
[32m[20221213 21:22:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1304 --------------------------#
[32m[20221213 21:22:49 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:22:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:49 @agent_ppo2.py:185][0m |          -0.0013 |         157.1854 |          12.0565 |
[32m[20221213 21:22:49 @agent_ppo2.py:185][0m |          -0.0028 |         156.6640 |          12.0805 |
[32m[20221213 21:22:49 @agent_ppo2.py:185][0m |          -0.0050 |         156.1435 |          12.1009 |
[32m[20221213 21:22:49 @agent_ppo2.py:185][0m |           0.0021 |         158.9888 |          12.1089 |
[32m[20221213 21:22:49 @agent_ppo2.py:185][0m |          -0.0055 |         155.6723 |          12.1592 |
[32m[20221213 21:22:49 @agent_ppo2.py:185][0m |           0.0041 |         159.3755 |          12.1593 |
[32m[20221213 21:22:50 @agent_ppo2.py:185][0m |          -0.0052 |         155.4187 |          12.1933 |
[32m[20221213 21:22:50 @agent_ppo2.py:185][0m |          -0.0075 |         155.1555 |          12.2161 |
[32m[20221213 21:22:50 @agent_ppo2.py:185][0m |           0.0048 |         164.2338 |          12.2149 |
[32m[20221213 21:22:50 @agent_ppo2.py:185][0m |          -0.0056 |         154.9263 |          12.2195 |
[32m[20221213 21:22:50 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:22:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.40
[32m[20221213 21:22:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:22:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.00
[32m[20221213 21:22:50 @agent_ppo2.py:143][0m Total time:      27.25 min
[32m[20221213 21:22:50 @agent_ppo2.py:145][0m 2672640 total steps have happened
[32m[20221213 21:22:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1305 --------------------------#
[32m[20221213 21:22:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:50 @agent_ppo2.py:185][0m |           0.0051 |         161.5250 |          12.2170 |
[32m[20221213 21:22:50 @agent_ppo2.py:185][0m |          -0.0032 |         156.1205 |          12.2303 |
[32m[20221213 21:22:50 @agent_ppo2.py:185][0m |           0.0020 |         159.1890 |          12.2582 |
[32m[20221213 21:22:51 @agent_ppo2.py:185][0m |          -0.0067 |         154.7154 |          12.2535 |
[32m[20221213 21:22:51 @agent_ppo2.py:185][0m |          -0.0062 |         154.0056 |          12.2482 |
[32m[20221213 21:22:51 @agent_ppo2.py:185][0m |          -0.0042 |         153.3015 |          12.2481 |
[32m[20221213 21:22:51 @agent_ppo2.py:185][0m |          -0.0062 |         152.9542 |          12.2311 |
[32m[20221213 21:22:51 @agent_ppo2.py:185][0m |          -0.0082 |         152.4677 |          12.2242 |
[32m[20221213 21:22:51 @agent_ppo2.py:185][0m |          -0.0047 |         153.4169 |          12.2054 |
[32m[20221213 21:22:51 @agent_ppo2.py:185][0m |          -0.0086 |         151.8905 |          12.2287 |
[32m[20221213 21:22:51 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:22:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 753.20
[32m[20221213 21:22:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.00
[32m[20221213 21:22:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.00
[32m[20221213 21:22:51 @agent_ppo2.py:143][0m Total time:      27.27 min
[32m[20221213 21:22:51 @agent_ppo2.py:145][0m 2674688 total steps have happened
[32m[20221213 21:22:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1306 --------------------------#
[32m[20221213 21:22:51 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:22:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:51 @agent_ppo2.py:185][0m |          -0.0028 |         159.6280 |          12.1952 |
[32m[20221213 21:22:52 @agent_ppo2.py:185][0m |          -0.0067 |         158.4640 |          12.1535 |
[32m[20221213 21:22:52 @agent_ppo2.py:185][0m |          -0.0072 |         158.1254 |          12.1440 |
[32m[20221213 21:22:52 @agent_ppo2.py:185][0m |          -0.0070 |         157.8573 |          12.0926 |
[32m[20221213 21:22:52 @agent_ppo2.py:185][0m |          -0.0075 |         157.7267 |          12.1315 |
[32m[20221213 21:22:52 @agent_ppo2.py:185][0m |          -0.0068 |         158.3930 |          12.1105 |
[32m[20221213 21:22:52 @agent_ppo2.py:185][0m |          -0.0093 |         157.2235 |          12.1326 |
[32m[20221213 21:22:52 @agent_ppo2.py:185][0m |          -0.0026 |         161.4780 |          12.1097 |
[32m[20221213 21:22:52 @agent_ppo2.py:185][0m |          -0.0047 |         160.2087 |          12.1174 |
[32m[20221213 21:22:52 @agent_ppo2.py:185][0m |          -0.0081 |         156.7605 |          12.1134 |
[32m[20221213 21:22:52 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:22:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 732.60
[32m[20221213 21:22:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.00
[32m[20221213 21:22:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 21:22:52 @agent_ppo2.py:143][0m Total time:      27.30 min
[32m[20221213 21:22:52 @agent_ppo2.py:145][0m 2676736 total steps have happened
[32m[20221213 21:22:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1307 --------------------------#
[32m[20221213 21:22:53 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:22:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:53 @agent_ppo2.py:185][0m |          -0.0014 |         157.0672 |          11.9616 |
[32m[20221213 21:22:53 @agent_ppo2.py:185][0m |          -0.0019 |         156.0441 |          11.9230 |
[32m[20221213 21:22:53 @agent_ppo2.py:185][0m |          -0.0072 |         154.5140 |          11.9089 |
[32m[20221213 21:22:53 @agent_ppo2.py:185][0m |          -0.0028 |         155.1397 |          11.8990 |
[32m[20221213 21:22:53 @agent_ppo2.py:185][0m |          -0.0067 |         153.5788 |          11.9236 |
[32m[20221213 21:22:53 @agent_ppo2.py:185][0m |          -0.0041 |         153.8445 |          11.9162 |
[32m[20221213 21:22:53 @agent_ppo2.py:185][0m |          -0.0015 |         157.8587 |          11.9136 |
[32m[20221213 21:22:53 @agent_ppo2.py:185][0m |           0.0092 |         169.1914 |          11.8835 |
[32m[20221213 21:22:53 @agent_ppo2.py:185][0m |          -0.0066 |         152.5829 |          11.9157 |
[32m[20221213 21:22:54 @agent_ppo2.py:185][0m |          -0.0083 |         152.3678 |          11.9295 |
[32m[20221213 21:22:54 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:22:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 746.80
[32m[20221213 21:22:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.00
[32m[20221213 21:22:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 21:22:54 @agent_ppo2.py:143][0m Total time:      27.32 min
[32m[20221213 21:22:54 @agent_ppo2.py:145][0m 2678784 total steps have happened
[32m[20221213 21:22:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1308 --------------------------#
[32m[20221213 21:22:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:54 @agent_ppo2.py:185][0m |          -0.0001 |         158.1040 |          12.1528 |
[32m[20221213 21:22:54 @agent_ppo2.py:185][0m |          -0.0044 |         156.4350 |          12.1666 |
[32m[20221213 21:22:54 @agent_ppo2.py:185][0m |           0.0054 |         168.1985 |          12.1573 |
[32m[20221213 21:22:54 @agent_ppo2.py:185][0m |          -0.0045 |         155.1427 |          12.1443 |
[32m[20221213 21:22:54 @agent_ppo2.py:185][0m |          -0.0053 |         154.9475 |          12.1339 |
[32m[20221213 21:22:55 @agent_ppo2.py:185][0m |          -0.0069 |         154.8934 |          12.1147 |
[32m[20221213 21:22:55 @agent_ppo2.py:185][0m |          -0.0046 |         154.9398 |          12.1188 |
[32m[20221213 21:22:55 @agent_ppo2.py:185][0m |           0.0060 |         159.2789 |          12.1160 |
[32m[20221213 21:22:55 @agent_ppo2.py:185][0m |          -0.0060 |         154.1792 |          12.0911 |
[32m[20221213 21:22:55 @agent_ppo2.py:185][0m |           0.0001 |         163.1170 |          12.1241 |
[32m[20221213 21:22:55 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 21:22:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 763.80
[32m[20221213 21:22:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.00
[32m[20221213 21:22:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.00
[32m[20221213 21:22:55 @agent_ppo2.py:143][0m Total time:      27.34 min
[32m[20221213 21:22:55 @agent_ppo2.py:145][0m 2680832 total steps have happened
[32m[20221213 21:22:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1309 --------------------------#
[32m[20221213 21:22:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:22:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:55 @agent_ppo2.py:185][0m |          -0.0028 |         158.8663 |          11.8698 |
[32m[20221213 21:22:55 @agent_ppo2.py:185][0m |           0.0105 |         176.6202 |          11.8644 |
[32m[20221213 21:22:56 @agent_ppo2.py:185][0m |          -0.0036 |         158.1133 |          11.8572 |
[32m[20221213 21:22:56 @agent_ppo2.py:185][0m |          -0.0007 |         158.1445 |          11.8627 |
[32m[20221213 21:22:56 @agent_ppo2.py:185][0m |          -0.0007 |         158.3611 |          11.8549 |
[32m[20221213 21:22:56 @agent_ppo2.py:185][0m |          -0.0062 |         156.9294 |          11.8690 |
[32m[20221213 21:22:56 @agent_ppo2.py:185][0m |           0.0109 |         182.2802 |          11.8571 |
[32m[20221213 21:22:56 @agent_ppo2.py:185][0m |           0.0028 |         166.4646 |          11.8551 |
[32m[20221213 21:22:56 @agent_ppo2.py:185][0m |           0.0014 |         164.4524 |          11.8782 |
[32m[20221213 21:22:56 @agent_ppo2.py:185][0m |          -0.0071 |         156.4833 |          11.8519 |
[32m[20221213 21:22:56 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 21:22:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 744.20
[32m[20221213 21:22:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.00
[32m[20221213 21:22:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.00
[32m[20221213 21:22:56 @agent_ppo2.py:143][0m Total time:      27.36 min
[32m[20221213 21:22:56 @agent_ppo2.py:145][0m 2682880 total steps have happened
[32m[20221213 21:22:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1310 --------------------------#
[32m[20221213 21:22:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:22:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:57 @agent_ppo2.py:185][0m |           0.0011 |         156.2588 |          12.0245 |
[32m[20221213 21:22:57 @agent_ppo2.py:185][0m |          -0.0011 |         154.9375 |          12.0097 |
[32m[20221213 21:22:57 @agent_ppo2.py:185][0m |           0.0071 |         168.3279 |          12.0470 |
[32m[20221213 21:22:57 @agent_ppo2.py:185][0m |          -0.0034 |         154.2229 |          12.0460 |
[32m[20221213 21:22:57 @agent_ppo2.py:185][0m |          -0.0051 |         154.0228 |          12.0404 |
[32m[20221213 21:22:57 @agent_ppo2.py:185][0m |           0.0058 |         168.7721 |          12.0539 |
[32m[20221213 21:22:58 @agent_ppo2.py:185][0m |          -0.0072 |         153.4699 |          12.0334 |
[32m[20221213 21:22:58 @agent_ppo2.py:185][0m |          -0.0067 |         153.0817 |          12.0794 |
[32m[20221213 21:22:58 @agent_ppo2.py:185][0m |          -0.0012 |         154.7682 |          12.0247 |
[32m[20221213 21:22:58 @agent_ppo2.py:185][0m |          -0.0064 |         152.7669 |          12.0751 |
[32m[20221213 21:22:58 @agent_ppo2.py:130][0m Policy update time: 1.63 s
[32m[20221213 21:22:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.20
[32m[20221213 21:22:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.00
[32m[20221213 21:22:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 791.00
[32m[20221213 21:22:58 @agent_ppo2.py:143][0m Total time:      27.39 min
[32m[20221213 21:22:58 @agent_ppo2.py:145][0m 2684928 total steps have happened
[32m[20221213 21:22:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1311 --------------------------#
[32m[20221213 21:22:58 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:22:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:22:59 @agent_ppo2.py:185][0m |           0.0001 |         154.7337 |          12.1525 |
[32m[20221213 21:22:59 @agent_ppo2.py:185][0m |          -0.0060 |         153.3605 |          12.1015 |
[32m[20221213 21:22:59 @agent_ppo2.py:185][0m |          -0.0046 |         154.7600 |          12.0842 |
[32m[20221213 21:22:59 @agent_ppo2.py:185][0m |          -0.0093 |         152.0632 |          12.0695 |
[32m[20221213 21:22:59 @agent_ppo2.py:185][0m |          -0.0092 |         151.7129 |          12.0622 |
[32m[20221213 21:22:59 @agent_ppo2.py:185][0m |          -0.0097 |         151.3420 |          12.0456 |
[32m[20221213 21:22:59 @agent_ppo2.py:185][0m |           0.0045 |         169.1764 |          12.0134 |
[32m[20221213 21:22:59 @agent_ppo2.py:185][0m |          -0.0085 |         151.0295 |          12.0201 |
[32m[20221213 21:23:00 @agent_ppo2.py:185][0m |          -0.0087 |         150.5522 |          12.0137 |
[32m[20221213 21:23:00 @agent_ppo2.py:185][0m |          -0.0089 |         150.3342 |          11.9742 |
[32m[20221213 21:23:00 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221213 21:23:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 723.40
[32m[20221213 21:23:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 763.00
[32m[20221213 21:23:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.00
[32m[20221213 21:23:00 @agent_ppo2.py:143][0m Total time:      27.42 min
[32m[20221213 21:23:00 @agent_ppo2.py:145][0m 2686976 total steps have happened
[32m[20221213 21:23:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1312 --------------------------#
[32m[20221213 21:23:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:23:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:00 @agent_ppo2.py:185][0m |          -0.0026 |         155.6599 |          11.8911 |
[32m[20221213 21:23:00 @agent_ppo2.py:185][0m |          -0.0032 |         155.1236 |          11.8941 |
[32m[20221213 21:23:00 @agent_ppo2.py:185][0m |          -0.0066 |         154.4137 |          11.9261 |
[32m[20221213 21:23:01 @agent_ppo2.py:185][0m |          -0.0057 |         154.0672 |          11.9332 |
[32m[20221213 21:23:01 @agent_ppo2.py:185][0m |          -0.0070 |         153.7842 |          11.9597 |
[32m[20221213 21:23:01 @agent_ppo2.py:185][0m |           0.0007 |         160.4693 |          11.9892 |
[32m[20221213 21:23:01 @agent_ppo2.py:185][0m |          -0.0066 |         153.5864 |          11.9852 |
[32m[20221213 21:23:01 @agent_ppo2.py:185][0m |          -0.0049 |         153.5541 |          11.9803 |
[32m[20221213 21:23:01 @agent_ppo2.py:185][0m |          -0.0091 |         153.2205 |          11.9929 |
[32m[20221213 21:23:01 @agent_ppo2.py:185][0m |          -0.0061 |         153.9048 |          12.0141 |
[32m[20221213 21:23:01 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 21:23:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.60
[32m[20221213 21:23:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.00
[32m[20221213 21:23:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.00
[32m[20221213 21:23:01 @agent_ppo2.py:143][0m Total time:      27.44 min
[32m[20221213 21:23:01 @agent_ppo2.py:145][0m 2689024 total steps have happened
[32m[20221213 21:23:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1313 --------------------------#
[32m[20221213 21:23:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 21:23:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:02 @agent_ppo2.py:185][0m |           0.0091 |         166.2893 |          11.9430 |
[32m[20221213 21:23:02 @agent_ppo2.py:185][0m |          -0.0028 |         155.8605 |          11.9695 |
[32m[20221213 21:23:02 @agent_ppo2.py:185][0m |          -0.0022 |         156.5255 |          11.9980 |
[32m[20221213 21:23:02 @agent_ppo2.py:185][0m |          -0.0002 |         157.1873 |          12.0105 |
[32m[20221213 21:23:02 @agent_ppo2.py:185][0m |          -0.0057 |         154.6640 |          12.0515 |
[32m[20221213 21:23:02 @agent_ppo2.py:185][0m |          -0.0037 |         155.8578 |          12.0682 |
[32m[20221213 21:23:02 @agent_ppo2.py:185][0m |          -0.0062 |         154.0287 |          12.1006 |
[32m[20221213 21:23:02 @agent_ppo2.py:185][0m |          -0.0067 |         153.8656 |          12.1212 |
[32m[20221213 21:23:03 @agent_ppo2.py:185][0m |          -0.0106 |         153.7852 |          12.1505 |
[32m[20221213 21:23:03 @agent_ppo2.py:185][0m |          -0.0072 |         153.7163 |          12.1774 |
[32m[20221213 21:23:03 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 21:23:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 755.40
[32m[20221213 21:23:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 777.00
[32m[20221213 21:23:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 788.00
[32m[20221213 21:23:03 @agent_ppo2.py:143][0m Total time:      27.47 min
[32m[20221213 21:23:03 @agent_ppo2.py:145][0m 2691072 total steps have happened
[32m[20221213 21:23:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1314 --------------------------#
[32m[20221213 21:23:03 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:03 @agent_ppo2.py:185][0m |           0.0104 |         161.8939 |          12.4215 |
[32m[20221213 21:23:03 @agent_ppo2.py:185][0m |          -0.0050 |         153.3508 |          12.3957 |
[32m[20221213 21:23:03 @agent_ppo2.py:185][0m |          -0.0064 |         152.3164 |          12.4215 |
[32m[20221213 21:23:03 @agent_ppo2.py:185][0m |          -0.0079 |         151.1671 |          12.4018 |
[32m[20221213 21:23:04 @agent_ppo2.py:185][0m |          -0.0046 |         152.2858 |          12.4227 |
[32m[20221213 21:23:04 @agent_ppo2.py:185][0m |          -0.0099 |         149.2742 |          12.4003 |
[32m[20221213 21:23:04 @agent_ppo2.py:185][0m |          -0.0034 |         150.2126 |          12.4351 |
[32m[20221213 21:23:04 @agent_ppo2.py:185][0m |          -0.0086 |         147.8901 |          12.4215 |
[32m[20221213 21:23:04 @agent_ppo2.py:185][0m |          -0.0085 |         147.3542 |          12.4232 |
[32m[20221213 21:23:04 @agent_ppo2.py:185][0m |          -0.0100 |         147.1695 |          12.4036 |
[32m[20221213 21:23:04 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 21:23:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.60
[32m[20221213 21:23:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.00
[32m[20221213 21:23:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.00
[32m[20221213 21:23:04 @agent_ppo2.py:143][0m Total time:      27.49 min
[32m[20221213 21:23:04 @agent_ppo2.py:145][0m 2693120 total steps have happened
[32m[20221213 21:23:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1315 --------------------------#
[32m[20221213 21:23:04 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:05 @agent_ppo2.py:185][0m |          -0.0007 |         162.2389 |          12.4390 |
[32m[20221213 21:23:05 @agent_ppo2.py:185][0m |          -0.0054 |         159.5614 |          12.4641 |
[32m[20221213 21:23:05 @agent_ppo2.py:185][0m |          -0.0076 |         158.5957 |          12.4526 |
[32m[20221213 21:23:05 @agent_ppo2.py:185][0m |          -0.0075 |         159.2249 |          12.4397 |
[32m[20221213 21:23:05 @agent_ppo2.py:185][0m |          -0.0094 |         157.7731 |          12.4555 |
[32m[20221213 21:23:05 @agent_ppo2.py:185][0m |          -0.0133 |         157.5374 |          12.4449 |
[32m[20221213 21:23:05 @agent_ppo2.py:185][0m |          -0.0035 |         168.2114 |          12.4467 |
[32m[20221213 21:23:05 @agent_ppo2.py:185][0m |          -0.0121 |         157.0908 |          12.4684 |
[32m[20221213 21:23:06 @agent_ppo2.py:185][0m |          -0.0142 |         156.7696 |          12.4494 |
[32m[20221213 21:23:06 @agent_ppo2.py:185][0m |          -0.0116 |         156.6845 |          12.4563 |
[32m[20221213 21:23:06 @agent_ppo2.py:130][0m Policy update time: 1.24 s
[32m[20221213 21:23:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 746.00
[32m[20221213 21:23:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.00
[32m[20221213 21:23:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.00
[32m[20221213 21:23:06 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 856.00
[32m[20221213 21:23:06 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 856.00
[32m[20221213 21:23:06 @agent_ppo2.py:143][0m Total time:      27.52 min
[32m[20221213 21:23:06 @agent_ppo2.py:145][0m 2695168 total steps have happened
[32m[20221213 21:23:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1316 --------------------------#
[32m[20221213 21:23:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:23:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:06 @agent_ppo2.py:185][0m |          -0.0026 |         160.6224 |          12.0711 |
[32m[20221213 21:23:06 @agent_ppo2.py:185][0m |          -0.0067 |         158.6798 |          12.0857 |
[32m[20221213 21:23:06 @agent_ppo2.py:185][0m |          -0.0039 |         159.8372 |          12.0444 |
[32m[20221213 21:23:07 @agent_ppo2.py:185][0m |          -0.0072 |         157.4348 |          12.0285 |
[32m[20221213 21:23:07 @agent_ppo2.py:185][0m |          -0.0088 |         156.8160 |          12.0312 |
[32m[20221213 21:23:07 @agent_ppo2.py:185][0m |          -0.0101 |         156.4903 |          12.0064 |
[32m[20221213 21:23:07 @agent_ppo2.py:185][0m |          -0.0083 |         155.9140 |          12.0199 |
[32m[20221213 21:23:07 @agent_ppo2.py:185][0m |          -0.0050 |         156.3928 |          11.9794 |
[32m[20221213 21:23:07 @agent_ppo2.py:185][0m |          -0.0092 |         155.6671 |          11.9971 |
[32m[20221213 21:23:07 @agent_ppo2.py:185][0m |          -0.0046 |         156.4115 |          11.9648 |
[32m[20221213 21:23:07 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 21:23:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 705.60
[32m[20221213 21:23:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.00
[32m[20221213 21:23:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:23:07 @agent_ppo2.py:143][0m Total time:      27.54 min
[32m[20221213 21:23:07 @agent_ppo2.py:145][0m 2697216 total steps have happened
[32m[20221213 21:23:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1317 --------------------------#
[32m[20221213 21:23:07 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:08 @agent_ppo2.py:185][0m |          -0.0011 |         158.2812 |          11.9337 |
[32m[20221213 21:23:08 @agent_ppo2.py:185][0m |          -0.0062 |         156.9583 |          11.9781 |
[32m[20221213 21:23:08 @agent_ppo2.py:185][0m |          -0.0061 |         156.4749 |          11.9510 |
[32m[20221213 21:23:08 @agent_ppo2.py:185][0m |          -0.0058 |         156.5895 |          11.9765 |
[32m[20221213 21:23:08 @agent_ppo2.py:185][0m |          -0.0082 |         155.7149 |          11.9474 |
[32m[20221213 21:23:08 @agent_ppo2.py:185][0m |          -0.0100 |         155.5522 |          11.9439 |
[32m[20221213 21:23:08 @agent_ppo2.py:185][0m |          -0.0082 |         155.3902 |          11.9435 |
[32m[20221213 21:23:08 @agent_ppo2.py:185][0m |          -0.0081 |         155.2958 |          11.9304 |
[32m[20221213 21:23:09 @agent_ppo2.py:185][0m |          -0.0078 |         156.1259 |          11.9441 |
[32m[20221213 21:23:09 @agent_ppo2.py:185][0m |          -0.0100 |         155.0028 |          11.8961 |
[32m[20221213 21:23:09 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221213 21:23:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 753.40
[32m[20221213 21:23:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 773.00
[32m[20221213 21:23:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:23:09 @agent_ppo2.py:143][0m Total time:      27.57 min
[32m[20221213 21:23:09 @agent_ppo2.py:145][0m 2699264 total steps have happened
[32m[20221213 21:23:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1318 --------------------------#
[32m[20221213 21:23:09 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:09 @agent_ppo2.py:185][0m |           0.0095 |         168.4920 |          11.9617 |
[32m[20221213 21:23:09 @agent_ppo2.py:185][0m |          -0.0026 |         158.7178 |          11.9455 |
[32m[20221213 21:23:09 @agent_ppo2.py:185][0m |          -0.0068 |         157.1916 |          11.9619 |
[32m[20221213 21:23:10 @agent_ppo2.py:185][0m |          -0.0038 |         156.9253 |          11.9521 |
[32m[20221213 21:23:10 @agent_ppo2.py:185][0m |          -0.0076 |         155.8394 |          11.9349 |
[32m[20221213 21:23:10 @agent_ppo2.py:185][0m |          -0.0077 |         154.9634 |          11.9453 |
[32m[20221213 21:23:10 @agent_ppo2.py:185][0m |          -0.0075 |         154.6060 |          11.9221 |
[32m[20221213 21:23:10 @agent_ppo2.py:185][0m |          -0.0101 |         154.1640 |          11.9153 |
[32m[20221213 21:23:10 @agent_ppo2.py:185][0m |          -0.0084 |         153.8060 |          11.9198 |
[32m[20221213 21:23:10 @agent_ppo2.py:185][0m |          -0.0088 |         153.5150 |          11.8861 |
[32m[20221213 21:23:10 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:23:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.20
[32m[20221213 21:23:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 772.00
[32m[20221213 21:23:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.00
[32m[20221213 21:23:10 @agent_ppo2.py:143][0m Total time:      27.59 min
[32m[20221213 21:23:10 @agent_ppo2.py:145][0m 2701312 total steps have happened
[32m[20221213 21:23:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1319 --------------------------#
[32m[20221213 21:23:10 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:11 @agent_ppo2.py:185][0m |          -0.0021 |         163.0577 |          11.9855 |
[32m[20221213 21:23:11 @agent_ppo2.py:185][0m |          -0.0018 |         160.4218 |          12.0316 |
[32m[20221213 21:23:11 @agent_ppo2.py:185][0m |          -0.0058 |         159.6149 |          12.0021 |
[32m[20221213 21:23:11 @agent_ppo2.py:185][0m |          -0.0061 |         158.5031 |          12.0083 |
[32m[20221213 21:23:11 @agent_ppo2.py:185][0m |          -0.0017 |         161.8640 |          11.9952 |
[32m[20221213 21:23:11 @agent_ppo2.py:185][0m |          -0.0056 |         157.2825 |          12.0172 |
[32m[20221213 21:23:11 @agent_ppo2.py:185][0m |          -0.0011 |         159.3890 |          11.9967 |
[32m[20221213 21:23:11 @agent_ppo2.py:185][0m |          -0.0056 |         157.5534 |          11.9969 |
[32m[20221213 21:23:12 @agent_ppo2.py:185][0m |          -0.0024 |         157.5546 |          11.9897 |
[32m[20221213 21:23:12 @agent_ppo2.py:185][0m |          -0.0084 |         155.6569 |          11.9921 |
[32m[20221213 21:23:12 @agent_ppo2.py:130][0m Policy update time: 1.24 s
[32m[20221213 21:23:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 699.80
[32m[20221213 21:23:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.00
[32m[20221213 21:23:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.00
[32m[20221213 21:23:12 @agent_ppo2.py:143][0m Total time:      27.62 min
[32m[20221213 21:23:12 @agent_ppo2.py:145][0m 2703360 total steps have happened
[32m[20221213 21:23:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1320 --------------------------#
[32m[20221213 21:23:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:23:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:12 @agent_ppo2.py:185][0m |           0.0044 |         161.7713 |          11.5348 |
[32m[20221213 21:23:12 @agent_ppo2.py:185][0m |          -0.0051 |         156.9851 |          11.5264 |
[32m[20221213 21:23:12 @agent_ppo2.py:185][0m |          -0.0071 |         156.1922 |          11.4550 |
[32m[20221213 21:23:13 @agent_ppo2.py:185][0m |          -0.0096 |         155.6573 |          11.5194 |
[32m[20221213 21:23:13 @agent_ppo2.py:185][0m |           0.0027 |         158.9992 |          11.4401 |
[32m[20221213 21:23:13 @agent_ppo2.py:185][0m |          -0.0079 |         155.2377 |          11.4758 |
[32m[20221213 21:23:13 @agent_ppo2.py:185][0m |          -0.0078 |         155.0847 |          11.4159 |
[32m[20221213 21:23:13 @agent_ppo2.py:185][0m |          -0.0068 |         155.0317 |          11.4025 |
[32m[20221213 21:23:13 @agent_ppo2.py:185][0m |          -0.0088 |         154.7696 |          11.3778 |
[32m[20221213 21:23:13 @agent_ppo2.py:185][0m |          -0.0130 |         154.5201 |          11.3420 |
[32m[20221213 21:23:13 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 21:23:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 755.80
[32m[20221213 21:23:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.00
[32m[20221213 21:23:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.00
[32m[20221213 21:23:13 @agent_ppo2.py:143][0m Total time:      27.64 min
[32m[20221213 21:23:13 @agent_ppo2.py:145][0m 2705408 total steps have happened
[32m[20221213 21:23:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1321 --------------------------#
[32m[20221213 21:23:13 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:14 @agent_ppo2.py:185][0m |          -0.0017 |         155.8671 |          11.5141 |
[32m[20221213 21:23:14 @agent_ppo2.py:185][0m |          -0.0023 |         154.6688 |          11.4919 |
[32m[20221213 21:23:14 @agent_ppo2.py:185][0m |          -0.0034 |         153.8504 |          11.4947 |
[32m[20221213 21:23:14 @agent_ppo2.py:185][0m |          -0.0063 |         153.3252 |          11.5098 |
[32m[20221213 21:23:14 @agent_ppo2.py:185][0m |          -0.0054 |         152.7175 |          11.4941 |
[32m[20221213 21:23:14 @agent_ppo2.py:185][0m |          -0.0071 |         152.4569 |          11.4685 |
[32m[20221213 21:23:14 @agent_ppo2.py:185][0m |          -0.0067 |         152.4121 |          11.4785 |
[32m[20221213 21:23:14 @agent_ppo2.py:185][0m |          -0.0068 |         152.0568 |          11.4645 |
[32m[20221213 21:23:15 @agent_ppo2.py:185][0m |          -0.0081 |         152.0717 |          11.4888 |
[32m[20221213 21:23:15 @agent_ppo2.py:185][0m |          -0.0094 |         151.6799 |          11.4714 |
[32m[20221213 21:23:15 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221213 21:23:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 736.60
[32m[20221213 21:23:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.00
[32m[20221213 21:23:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.00
[32m[20221213 21:23:15 @agent_ppo2.py:143][0m Total time:      27.67 min
[32m[20221213 21:23:15 @agent_ppo2.py:145][0m 2707456 total steps have happened
[32m[20221213 21:23:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1322 --------------------------#
[32m[20221213 21:23:15 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:15 @agent_ppo2.py:185][0m |          -0.0007 |         158.0607 |          11.7452 |
[32m[20221213 21:23:15 @agent_ppo2.py:185][0m |          -0.0033 |         157.2631 |          11.7643 |
[32m[20221213 21:23:15 @agent_ppo2.py:185][0m |          -0.0047 |         156.7294 |          11.7368 |
[32m[20221213 21:23:15 @agent_ppo2.py:185][0m |          -0.0058 |         156.3510 |          11.7078 |
[32m[20221213 21:23:16 @agent_ppo2.py:185][0m |          -0.0048 |         155.9783 |          11.6660 |
[32m[20221213 21:23:16 @agent_ppo2.py:185][0m |          -0.0044 |         155.6758 |          11.6917 |
[32m[20221213 21:23:16 @agent_ppo2.py:185][0m |          -0.0042 |         155.3809 |          11.6790 |
[32m[20221213 21:23:16 @agent_ppo2.py:185][0m |          -0.0045 |         155.1161 |          11.6723 |
[32m[20221213 21:23:16 @agent_ppo2.py:185][0m |           0.0029 |         160.1786 |          11.6497 |
[32m[20221213 21:23:16 @agent_ppo2.py:185][0m |          -0.0070 |         155.0771 |          11.6459 |
[32m[20221213 21:23:16 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 21:23:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.60
[32m[20221213 21:23:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.00
[32m[20221213 21:23:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.00
[32m[20221213 21:23:16 @agent_ppo2.py:143][0m Total time:      27.69 min
[32m[20221213 21:23:16 @agent_ppo2.py:145][0m 2709504 total steps have happened
[32m[20221213 21:23:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1323 --------------------------#
[32m[20221213 21:23:16 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 21:23:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:17 @agent_ppo2.py:185][0m |           0.0038 |         157.0639 |          11.4984 |
[32m[20221213 21:23:17 @agent_ppo2.py:185][0m |           0.0020 |         155.0587 |          11.4989 |
[32m[20221213 21:23:17 @agent_ppo2.py:185][0m |           0.0061 |         162.4326 |          11.5367 |
[32m[20221213 21:23:17 @agent_ppo2.py:185][0m |          -0.0042 |         153.3124 |          11.5595 |
[32m[20221213 21:23:17 @agent_ppo2.py:185][0m |           0.0003 |         155.6546 |          11.5427 |
[32m[20221213 21:23:17 @agent_ppo2.py:185][0m |          -0.0030 |         153.0216 |          11.5725 |
[32m[20221213 21:23:17 @agent_ppo2.py:185][0m |           0.0061 |         168.6616 |          11.5325 |
[32m[20221213 21:23:17 @agent_ppo2.py:185][0m |           0.0096 |         172.6597 |          11.5484 |
[32m[20221213 21:23:18 @agent_ppo2.py:185][0m |          -0.0071 |         152.4747 |          11.6032 |
[32m[20221213 21:23:18 @agent_ppo2.py:185][0m |          -0.0072 |         152.1924 |          11.6071 |
[32m[20221213 21:23:18 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 21:23:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.80
[32m[20221213 21:23:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 21:23:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.00
[32m[20221213 21:23:18 @agent_ppo2.py:143][0m Total time:      27.72 min
[32m[20221213 21:23:18 @agent_ppo2.py:145][0m 2711552 total steps have happened
[32m[20221213 21:23:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1324 --------------------------#
[32m[20221213 21:23:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:23:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:18 @agent_ppo2.py:185][0m |          -0.0017 |         159.6418 |          11.5631 |
[32m[20221213 21:23:18 @agent_ppo2.py:185][0m |          -0.0066 |         157.2692 |          11.6135 |
[32m[20221213 21:23:18 @agent_ppo2.py:185][0m |          -0.0059 |         156.2871 |          11.6085 |
[32m[20221213 21:23:19 @agent_ppo2.py:185][0m |          -0.0056 |         155.6971 |          11.5897 |
[32m[20221213 21:23:19 @agent_ppo2.py:185][0m |          -0.0077 |         155.3134 |          11.5878 |
[32m[20221213 21:23:19 @agent_ppo2.py:185][0m |          -0.0092 |         154.8937 |          11.5950 |
[32m[20221213 21:23:19 @agent_ppo2.py:185][0m |          -0.0096 |         154.5312 |          11.5594 |
[32m[20221213 21:23:19 @agent_ppo2.py:185][0m |          -0.0032 |         156.2461 |          11.5774 |
[32m[20221213 21:23:19 @agent_ppo2.py:185][0m |          -0.0108 |         153.5240 |          11.5718 |
[32m[20221213 21:23:19 @agent_ppo2.py:185][0m |          -0.0115 |         153.0754 |          11.5796 |
[32m[20221213 21:23:19 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 21:23:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 779.80
[32m[20221213 21:23:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.00
[32m[20221213 21:23:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 779.00
[32m[20221213 21:23:19 @agent_ppo2.py:143][0m Total time:      27.74 min
[32m[20221213 21:23:19 @agent_ppo2.py:145][0m 2713600 total steps have happened
[32m[20221213 21:23:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1325 --------------------------#
[32m[20221213 21:23:19 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:20 @agent_ppo2.py:185][0m |          -0.0007 |         160.8022 |          11.3615 |
[32m[20221213 21:23:20 @agent_ppo2.py:185][0m |          -0.0011 |         160.5877 |          11.4219 |
[32m[20221213 21:23:20 @agent_ppo2.py:185][0m |          -0.0075 |         159.1871 |          11.4423 |
[32m[20221213 21:23:20 @agent_ppo2.py:185][0m |          -0.0047 |         158.8146 |          11.4494 |
[32m[20221213 21:23:20 @agent_ppo2.py:185][0m |          -0.0067 |         158.4180 |          11.4389 |
[32m[20221213 21:23:20 @agent_ppo2.py:185][0m |           0.0008 |         169.2729 |          11.4430 |
[32m[20221213 21:23:20 @agent_ppo2.py:185][0m |          -0.0054 |         158.0589 |          11.5201 |
[32m[20221213 21:23:20 @agent_ppo2.py:185][0m |          -0.0087 |         157.7129 |          11.4812 |
[32m[20221213 21:23:21 @agent_ppo2.py:185][0m |          -0.0083 |         157.5640 |          11.5027 |
[32m[20221213 21:23:21 @agent_ppo2.py:185][0m |          -0.0053 |         159.7142 |          11.5174 |
[32m[20221213 21:23:21 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221213 21:23:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.00
[32m[20221213 21:23:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.00
[32m[20221213 21:23:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.00
[32m[20221213 21:23:21 @agent_ppo2.py:143][0m Total time:      27.77 min
[32m[20221213 21:23:21 @agent_ppo2.py:145][0m 2715648 total steps have happened
[32m[20221213 21:23:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1326 --------------------------#
[32m[20221213 21:23:21 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 21:23:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:21 @agent_ppo2.py:185][0m |          -0.0022 |         157.4855 |          11.7125 |
[32m[20221213 21:23:21 @agent_ppo2.py:185][0m |          -0.0042 |         156.3251 |          11.7639 |
[32m[20221213 21:23:21 @agent_ppo2.py:185][0m |          -0.0049 |         155.7537 |          11.7626 |
[32m[20221213 21:23:22 @agent_ppo2.py:185][0m |          -0.0058 |         155.4171 |          11.7637 |
[32m[20221213 21:23:22 @agent_ppo2.py:185][0m |          -0.0070 |         155.1023 |          11.8009 |
[32m[20221213 21:23:22 @agent_ppo2.py:185][0m |          -0.0104 |         155.0147 |          11.8021 |
[32m[20221213 21:23:22 @agent_ppo2.py:185][0m |          -0.0064 |         154.5407 |          11.8260 |
[32m[20221213 21:23:22 @agent_ppo2.py:185][0m |          -0.0074 |         154.4387 |          11.8184 |
[32m[20221213 21:23:22 @agent_ppo2.py:185][0m |           0.0033 |         177.2258 |          11.8496 |
[32m[20221213 21:23:22 @agent_ppo2.py:185][0m |          -0.0086 |         154.1937 |          11.8475 |
[32m[20221213 21:23:22 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 21:23:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.40
[32m[20221213 21:23:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 765.00
[32m[20221213 21:23:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.00
[32m[20221213 21:23:22 @agent_ppo2.py:143][0m Total time:      27.79 min
[32m[20221213 21:23:22 @agent_ppo2.py:145][0m 2717696 total steps have happened
[32m[20221213 21:23:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1327 --------------------------#
[32m[20221213 21:23:22 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:23 @agent_ppo2.py:185][0m |           0.0043 |         159.5734 |          11.6594 |
[32m[20221213 21:23:23 @agent_ppo2.py:185][0m |          -0.0044 |         155.3275 |          11.7007 |
[32m[20221213 21:23:23 @agent_ppo2.py:185][0m |          -0.0035 |         154.6914 |          11.7623 |
[32m[20221213 21:23:23 @agent_ppo2.py:185][0m |           0.0059 |         163.7167 |          11.7757 |
[32m[20221213 21:23:23 @agent_ppo2.py:185][0m |          -0.0058 |         154.2811 |          11.7599 |
[32m[20221213 21:23:23 @agent_ppo2.py:185][0m |          -0.0068 |         153.9631 |          11.8208 |
[32m[20221213 21:23:23 @agent_ppo2.py:185][0m |          -0.0073 |         153.8013 |          11.8583 |
[32m[20221213 21:23:23 @agent_ppo2.py:185][0m |          -0.0070 |         153.6166 |          11.8546 |
[32m[20221213 21:23:24 @agent_ppo2.py:185][0m |          -0.0022 |         154.6367 |          11.9214 |
[32m[20221213 21:23:24 @agent_ppo2.py:185][0m |          -0.0076 |         153.3239 |          11.9447 |
[32m[20221213 21:23:24 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 21:23:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 772.60
[32m[20221213 21:23:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.00
[32m[20221213 21:23:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.00
[32m[20221213 21:23:24 @agent_ppo2.py:143][0m Total time:      27.82 min
[32m[20221213 21:23:24 @agent_ppo2.py:145][0m 2719744 total steps have happened
[32m[20221213 21:23:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1328 --------------------------#
[32m[20221213 21:23:24 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:24 @agent_ppo2.py:185][0m |          -0.0015 |         157.6841 |          12.1267 |
[32m[20221213 21:23:24 @agent_ppo2.py:185][0m |          -0.0040 |         155.9186 |          12.1029 |
[32m[20221213 21:23:24 @agent_ppo2.py:185][0m |          -0.0024 |         155.3778 |          12.1054 |
[32m[20221213 21:23:24 @agent_ppo2.py:185][0m |          -0.0033 |         154.8690 |          12.1090 |
[32m[20221213 21:23:25 @agent_ppo2.py:185][0m |          -0.0001 |         156.0632 |          12.1035 |
[32m[20221213 21:23:25 @agent_ppo2.py:185][0m |          -0.0029 |         154.4059 |          12.1186 |
[32m[20221213 21:23:25 @agent_ppo2.py:185][0m |          -0.0066 |         153.9419 |          12.1281 |
[32m[20221213 21:23:25 @agent_ppo2.py:185][0m |           0.0042 |         164.2090 |          12.1055 |
[32m[20221213 21:23:25 @agent_ppo2.py:185][0m |          -0.0046 |         153.6428 |          12.1312 |
[32m[20221213 21:23:25 @agent_ppo2.py:185][0m |          -0.0019 |         155.4945 |          12.0855 |
[32m[20221213 21:23:25 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:23:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 724.20
[32m[20221213 21:23:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.00
[32m[20221213 21:23:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.00
[32m[20221213 21:23:25 @agent_ppo2.py:143][0m Total time:      27.84 min
[32m[20221213 21:23:25 @agent_ppo2.py:145][0m 2721792 total steps have happened
[32m[20221213 21:23:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1329 --------------------------#
[32m[20221213 21:23:25 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:26 @agent_ppo2.py:185][0m |          -0.0010 |         157.8724 |          11.8914 |
[32m[20221213 21:23:26 @agent_ppo2.py:185][0m |          -0.0038 |         157.2284 |          11.9223 |
[32m[20221213 21:23:26 @agent_ppo2.py:185][0m |          -0.0070 |         157.0164 |          11.9435 |
[32m[20221213 21:23:26 @agent_ppo2.py:185][0m |          -0.0063 |         156.6709 |          11.9694 |
[32m[20221213 21:23:26 @agent_ppo2.py:185][0m |          -0.0019 |         158.6505 |          12.0056 |
[32m[20221213 21:23:26 @agent_ppo2.py:185][0m |          -0.0049 |         156.3270 |          11.9971 |
[32m[20221213 21:23:26 @agent_ppo2.py:185][0m |          -0.0069 |         156.2578 |          11.9863 |
[32m[20221213 21:23:26 @agent_ppo2.py:185][0m |          -0.0077 |         156.1312 |          12.0241 |
[32m[20221213 21:23:26 @agent_ppo2.py:185][0m |           0.0068 |         176.0501 |          12.0536 |
[32m[20221213 21:23:26 @agent_ppo2.py:185][0m |          -0.0061 |         156.1130 |          12.0604 |
[32m[20221213 21:23:26 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:23:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 728.40
[32m[20221213 21:23:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.00
[32m[20221213 21:23:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.00
[32m[20221213 21:23:27 @agent_ppo2.py:143][0m Total time:      27.87 min
[32m[20221213 21:23:27 @agent_ppo2.py:145][0m 2723840 total steps have happened
[32m[20221213 21:23:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1330 --------------------------#
[32m[20221213 21:23:27 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:27 @agent_ppo2.py:185][0m |          -0.0011 |         154.6893 |          12.1441 |
[32m[20221213 21:23:27 @agent_ppo2.py:185][0m |          -0.0041 |         153.5555 |          12.1374 |
[32m[20221213 21:23:27 @agent_ppo2.py:185][0m |          -0.0000 |         156.2591 |          12.1573 |
[32m[20221213 21:23:27 @agent_ppo2.py:185][0m |          -0.0082 |         152.6535 |          12.1952 |
[32m[20221213 21:23:27 @agent_ppo2.py:185][0m |          -0.0098 |         152.3017 |          12.1781 |
[32m[20221213 21:23:28 @agent_ppo2.py:185][0m |          -0.0093 |         152.0255 |          12.1787 |
[32m[20221213 21:23:28 @agent_ppo2.py:185][0m |          -0.0089 |         151.8112 |          12.1869 |
[32m[20221213 21:23:28 @agent_ppo2.py:185][0m |          -0.0091 |         151.6978 |          12.1830 |
[32m[20221213 21:23:28 @agent_ppo2.py:185][0m |           0.0022 |         158.6271 |          12.2041 |
[32m[20221213 21:23:28 @agent_ppo2.py:185][0m |          -0.0099 |         151.3469 |          12.1769 |
[32m[20221213 21:23:28 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:23:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 755.20
[32m[20221213 21:23:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 775.00
[32m[20221213 21:23:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.00
[32m[20221213 21:23:28 @agent_ppo2.py:143][0m Total time:      27.89 min
[32m[20221213 21:23:28 @agent_ppo2.py:145][0m 2725888 total steps have happened
[32m[20221213 21:23:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1331 --------------------------#
[32m[20221213 21:23:28 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:28 @agent_ppo2.py:185][0m |          -0.0001 |         157.2720 |          12.2707 |
[32m[20221213 21:23:29 @agent_ppo2.py:185][0m |           0.0051 |         163.5284 |          12.2790 |
[32m[20221213 21:23:29 @agent_ppo2.py:185][0m |          -0.0021 |         157.3509 |          12.2847 |
[32m[20221213 21:23:29 @agent_ppo2.py:185][0m |          -0.0092 |         154.8975 |          12.2941 |
[32m[20221213 21:23:29 @agent_ppo2.py:185][0m |          -0.0092 |         154.5464 |          12.3272 |
[32m[20221213 21:23:29 @agent_ppo2.py:185][0m |          -0.0126 |         154.3464 |          12.2762 |
[32m[20221213 21:23:29 @agent_ppo2.py:185][0m |          -0.0051 |         156.5296 |          12.2767 |
[32m[20221213 21:23:29 @agent_ppo2.py:185][0m |          -0.0128 |         153.7516 |          12.2756 |
[32m[20221213 21:23:29 @agent_ppo2.py:185][0m |          -0.0121 |         153.5141 |          12.2655 |
[32m[20221213 21:23:29 @agent_ppo2.py:185][0m |          -0.0132 |         153.2661 |          12.2470 |
[32m[20221213 21:23:29 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:23:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 688.60
[32m[20221213 21:23:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 736.00
[32m[20221213 21:23:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.00
[32m[20221213 21:23:29 @agent_ppo2.py:143][0m Total time:      27.91 min
[32m[20221213 21:23:29 @agent_ppo2.py:145][0m 2727936 total steps have happened
[32m[20221213 21:23:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1332 --------------------------#
[32m[20221213 21:23:30 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:30 @agent_ppo2.py:185][0m |           0.0010 |         157.7161 |          12.2499 |
[32m[20221213 21:23:30 @agent_ppo2.py:185][0m |          -0.0029 |         156.2937 |          12.2503 |
[32m[20221213 21:23:30 @agent_ppo2.py:185][0m |          -0.0048 |         155.7024 |          12.2710 |
[32m[20221213 21:23:30 @agent_ppo2.py:185][0m |           0.0039 |         163.6115 |          12.2900 |
[32m[20221213 21:23:30 @agent_ppo2.py:185][0m |          -0.0068 |         155.2399 |          12.2715 |
[32m[20221213 21:23:30 @agent_ppo2.py:185][0m |          -0.0052 |         154.7628 |          12.3118 |
[32m[20221213 21:23:30 @agent_ppo2.py:185][0m |          -0.0062 |         154.5088 |          12.3235 |
[32m[20221213 21:23:31 @agent_ppo2.py:185][0m |          -0.0072 |         154.2118 |          12.3045 |
[32m[20221213 21:23:31 @agent_ppo2.py:185][0m |          -0.0066 |         154.0961 |          12.3278 |
[32m[20221213 21:23:31 @agent_ppo2.py:185][0m |          -0.0084 |         153.8591 |          12.3699 |
[32m[20221213 21:23:31 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:23:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.80
[32m[20221213 21:23:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.00
[32m[20221213 21:23:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.00
[32m[20221213 21:23:31 @agent_ppo2.py:143][0m Total time:      27.94 min
[32m[20221213 21:23:31 @agent_ppo2.py:145][0m 2729984 total steps have happened
[32m[20221213 21:23:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1333 --------------------------#
[32m[20221213 21:23:31 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:31 @agent_ppo2.py:185][0m |          -0.0004 |         156.1479 |          12.1927 |
[32m[20221213 21:23:31 @agent_ppo2.py:185][0m |          -0.0070 |         155.3622 |          12.2305 |
[32m[20221213 21:23:32 @agent_ppo2.py:185][0m |          -0.0073 |         154.6092 |          12.2400 |
[32m[20221213 21:23:32 @agent_ppo2.py:185][0m |           0.0020 |         159.8924 |          12.2540 |
[32m[20221213 21:23:32 @agent_ppo2.py:185][0m |          -0.0055 |         154.0254 |          12.1986 |
[32m[20221213 21:23:32 @agent_ppo2.py:185][0m |           0.0003 |         165.7702 |          12.2214 |
[32m[20221213 21:23:32 @agent_ppo2.py:185][0m |           0.0007 |         163.3220 |          12.2888 |
[32m[20221213 21:23:32 @agent_ppo2.py:185][0m |          -0.0086 |         153.4537 |          12.2036 |
[32m[20221213 21:23:32 @agent_ppo2.py:185][0m |          -0.0093 |         153.2448 |          12.2095 |
[32m[20221213 21:23:32 @agent_ppo2.py:185][0m |          -0.0082 |         153.5101 |          12.1647 |
[32m[20221213 21:23:32 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:23:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 772.00
[32m[20221213 21:23:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.00
[32m[20221213 21:23:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.00
[32m[20221213 21:23:32 @agent_ppo2.py:143][0m Total time:      27.96 min
[32m[20221213 21:23:32 @agent_ppo2.py:145][0m 2732032 total steps have happened
[32m[20221213 21:23:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1334 --------------------------#
[32m[20221213 21:23:33 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:33 @agent_ppo2.py:185][0m |          -0.0041 |         154.9596 |          12.0118 |
[32m[20221213 21:23:33 @agent_ppo2.py:185][0m |          -0.0055 |         154.4378 |          12.0209 |
[32m[20221213 21:23:33 @agent_ppo2.py:185][0m |          -0.0093 |         153.9247 |          12.0425 |
[32m[20221213 21:23:33 @agent_ppo2.py:185][0m |          -0.0058 |         154.0143 |          12.0949 |
[32m[20221213 21:23:33 @agent_ppo2.py:185][0m |          -0.0080 |         153.6345 |          12.0941 |
[32m[20221213 21:23:33 @agent_ppo2.py:185][0m |          -0.0041 |         155.3583 |          12.0695 |
[32m[20221213 21:23:33 @agent_ppo2.py:185][0m |           0.0011 |         164.4606 |          12.0803 |
[32m[20221213 21:23:33 @agent_ppo2.py:185][0m |          -0.0098 |         153.3198 |          12.1726 |
[32m[20221213 21:23:34 @agent_ppo2.py:185][0m |          -0.0110 |         153.2819 |          12.1172 |
[32m[20221213 21:23:34 @agent_ppo2.py:185][0m |          -0.0092 |         153.2240 |          12.1472 |
[32m[20221213 21:23:34 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:23:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.60
[32m[20221213 21:23:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:23:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.00
[32m[20221213 21:23:34 @agent_ppo2.py:143][0m Total time:      27.98 min
[32m[20221213 21:23:34 @agent_ppo2.py:145][0m 2734080 total steps have happened
[32m[20221213 21:23:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1335 --------------------------#
[32m[20221213 21:23:34 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:34 @agent_ppo2.py:185][0m |          -0.0014 |         157.0042 |          12.3980 |
[32m[20221213 21:23:34 @agent_ppo2.py:185][0m |          -0.0043 |         156.4561 |          12.4301 |
[32m[20221213 21:23:34 @agent_ppo2.py:185][0m |          -0.0022 |         157.9938 |          12.4428 |
[32m[20221213 21:23:34 @agent_ppo2.py:185][0m |          -0.0049 |         155.7530 |          12.4522 |
[32m[20221213 21:23:35 @agent_ppo2.py:185][0m |           0.0007 |         157.8617 |          12.4583 |
[32m[20221213 21:23:35 @agent_ppo2.py:185][0m |          -0.0078 |         155.3218 |          12.4544 |
[32m[20221213 21:23:35 @agent_ppo2.py:185][0m |          -0.0089 |         155.2621 |          12.4348 |
[32m[20221213 21:23:35 @agent_ppo2.py:185][0m |          -0.0078 |         155.2650 |          12.4471 |
[32m[20221213 21:23:35 @agent_ppo2.py:185][0m |          -0.0074 |         155.0691 |          12.4189 |
[32m[20221213 21:23:35 @agent_ppo2.py:185][0m |          -0.0091 |         154.7725 |          12.4455 |
[32m[20221213 21:23:35 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:23:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 747.80
[32m[20221213 21:23:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.00
[32m[20221213 21:23:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.00
[32m[20221213 21:23:35 @agent_ppo2.py:143][0m Total time:      28.01 min
[32m[20221213 21:23:35 @agent_ppo2.py:145][0m 2736128 total steps have happened
[32m[20221213 21:23:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1336 --------------------------#
[32m[20221213 21:23:35 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 21:23:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:36 @agent_ppo2.py:185][0m |           0.0151 |         167.0256 |          12.3953 |
[32m[20221213 21:23:36 @agent_ppo2.py:185][0m |           0.0014 |         157.9749 |          12.3391 |
[32m[20221213 21:23:36 @agent_ppo2.py:185][0m |          -0.0040 |         155.1675 |          12.3268 |
[32m[20221213 21:23:36 @agent_ppo2.py:185][0m |          -0.0070 |         154.6775 |          12.3109 |
[32m[20221213 21:23:36 @agent_ppo2.py:185][0m |           0.0015 |         163.2432 |          12.3455 |
[32m[20221213 21:23:36 @agent_ppo2.py:185][0m |          -0.0061 |         154.3708 |          12.3260 |
[32m[20221213 21:23:36 @agent_ppo2.py:185][0m |          -0.0079 |         154.0158 |          12.3517 |
[32m[20221213 21:23:36 @agent_ppo2.py:185][0m |           0.0030 |         161.4239 |          12.3257 |
[32m[20221213 21:23:36 @agent_ppo2.py:185][0m |          -0.0084 |         153.7298 |          12.3625 |
[32m[20221213 21:23:37 @agent_ppo2.py:185][0m |          -0.0093 |         153.5313 |          12.3513 |
[32m[20221213 21:23:37 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:23:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 741.00
[32m[20221213 21:23:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.00
[32m[20221213 21:23:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.00
[32m[20221213 21:23:37 @agent_ppo2.py:143][0m Total time:      28.03 min
[32m[20221213 21:23:37 @agent_ppo2.py:145][0m 2738176 total steps have happened
[32m[20221213 21:23:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1337 --------------------------#
[32m[20221213 21:23:37 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:37 @agent_ppo2.py:185][0m |           0.0006 |         155.3596 |          12.1594 |
[32m[20221213 21:23:37 @agent_ppo2.py:185][0m |          -0.0031 |         154.5865 |          12.1277 |
[32m[20221213 21:23:37 @agent_ppo2.py:185][0m |          -0.0022 |         153.9999 |          12.1371 |
[32m[20221213 21:23:37 @agent_ppo2.py:185][0m |          -0.0054 |         153.5495 |          12.1090 |
[32m[20221213 21:23:37 @agent_ppo2.py:185][0m |          -0.0058 |         153.2699 |          12.1181 |
[32m[20221213 21:23:38 @agent_ppo2.py:185][0m |          -0.0083 |         153.0053 |          12.1203 |
[32m[20221213 21:23:38 @agent_ppo2.py:185][0m |           0.0010 |         159.0494 |          12.1092 |
[32m[20221213 21:23:38 @agent_ppo2.py:185][0m |          -0.0072 |         152.6392 |          12.0766 |
[32m[20221213 21:23:38 @agent_ppo2.py:185][0m |          -0.0088 |         152.4661 |          12.0850 |
[32m[20221213 21:23:38 @agent_ppo2.py:185][0m |          -0.0093 |         152.2849 |          12.0571 |
[32m[20221213 21:23:38 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:23:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.40
[32m[20221213 21:23:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 21:23:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.00
[32m[20221213 21:23:38 @agent_ppo2.py:143][0m Total time:      28.06 min
[32m[20221213 21:23:38 @agent_ppo2.py:145][0m 2740224 total steps have happened
[32m[20221213 21:23:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1338 --------------------------#
[32m[20221213 21:23:38 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:38 @agent_ppo2.py:185][0m |          -0.0011 |         156.0286 |          12.1754 |
[32m[20221213 21:23:39 @agent_ppo2.py:185][0m |          -0.0054 |         154.5690 |          12.2126 |
[32m[20221213 21:23:39 @agent_ppo2.py:185][0m |          -0.0059 |         153.7845 |          12.1679 |
[32m[20221213 21:23:39 @agent_ppo2.py:185][0m |          -0.0036 |         153.2505 |          12.1949 |
[32m[20221213 21:23:39 @agent_ppo2.py:185][0m |          -0.0062 |         152.2739 |          12.1869 |
[32m[20221213 21:23:39 @agent_ppo2.py:185][0m |          -0.0073 |         151.7903 |          12.1863 |
[32m[20221213 21:23:39 @agent_ppo2.py:185][0m |          -0.0090 |         151.0919 |          12.2065 |
[32m[20221213 21:23:39 @agent_ppo2.py:185][0m |          -0.0075 |         150.5561 |          12.2132 |
[32m[20221213 21:23:39 @agent_ppo2.py:185][0m |          -0.0092 |         149.8617 |          12.2254 |
[32m[20221213 21:23:39 @agent_ppo2.py:185][0m |          -0.0090 |         149.3720 |          12.1951 |
[32m[20221213 21:23:39 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:23:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 761.00
[32m[20221213 21:23:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.00
[32m[20221213 21:23:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 709.00
[32m[20221213 21:23:40 @agent_ppo2.py:143][0m Total time:      28.08 min
[32m[20221213 21:23:40 @agent_ppo2.py:145][0m 2742272 total steps have happened
[32m[20221213 21:23:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1339 --------------------------#
[32m[20221213 21:23:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:23:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:40 @agent_ppo2.py:185][0m |           0.0001 |         159.4392 |          12.1323 |
[32m[20221213 21:23:40 @agent_ppo2.py:185][0m |          -0.0043 |         156.9706 |          12.1822 |
[32m[20221213 21:23:40 @agent_ppo2.py:185][0m |          -0.0063 |         155.8977 |          12.2024 |
[32m[20221213 21:23:40 @agent_ppo2.py:185][0m |          -0.0083 |         155.0909 |          12.2079 |
[32m[20221213 21:23:40 @agent_ppo2.py:185][0m |          -0.0075 |         154.4916 |          12.2038 |
[32m[20221213 21:23:40 @agent_ppo2.py:185][0m |          -0.0076 |         154.2208 |          12.2175 |
[32m[20221213 21:23:41 @agent_ppo2.py:185][0m |          -0.0076 |         153.8742 |          12.2469 |
[32m[20221213 21:23:41 @agent_ppo2.py:185][0m |          -0.0042 |         156.6615 |          12.2314 |
[32m[20221213 21:23:41 @agent_ppo2.py:185][0m |          -0.0077 |         153.5385 |          12.2612 |
[32m[20221213 21:23:41 @agent_ppo2.py:185][0m |          -0.0080 |         153.2917 |          12.2768 |
[32m[20221213 21:23:41 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:23:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 753.00
[32m[20221213 21:23:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.00
[32m[20221213 21:23:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.00
[32m[20221213 21:23:41 @agent_ppo2.py:143][0m Total time:      28.10 min
[32m[20221213 21:23:41 @agent_ppo2.py:145][0m 2744320 total steps have happened
[32m[20221213 21:23:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1340 --------------------------#
[32m[20221213 21:23:41 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:41 @agent_ppo2.py:185][0m |          -0.0010 |         159.7238 |          12.0194 |
[32m[20221213 21:23:41 @agent_ppo2.py:185][0m |          -0.0041 |         158.2769 |          12.0865 |
[32m[20221213 21:23:42 @agent_ppo2.py:185][0m |          -0.0030 |         157.5038 |          12.0271 |
[32m[20221213 21:23:42 @agent_ppo2.py:185][0m |          -0.0048 |         156.8780 |          12.0457 |
[32m[20221213 21:23:42 @agent_ppo2.py:185][0m |          -0.0051 |         156.6706 |          11.9984 |
[32m[20221213 21:23:42 @agent_ppo2.py:185][0m |          -0.0014 |         159.4115 |          12.0013 |
[32m[20221213 21:23:42 @agent_ppo2.py:185][0m |          -0.0109 |         155.7887 |          12.0171 |
[32m[20221213 21:23:42 @agent_ppo2.py:185][0m |          -0.0073 |         155.5955 |          11.9852 |
[32m[20221213 21:23:42 @agent_ppo2.py:185][0m |          -0.0094 |         155.2923 |          11.9552 |
[32m[20221213 21:23:42 @agent_ppo2.py:185][0m |          -0.0084 |         155.2637 |          11.9350 |
[32m[20221213 21:23:42 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 21:23:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.00
[32m[20221213 21:23:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 779.00
[32m[20221213 21:23:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.00
[32m[20221213 21:23:42 @agent_ppo2.py:143][0m Total time:      28.13 min
[32m[20221213 21:23:42 @agent_ppo2.py:145][0m 2746368 total steps have happened
[32m[20221213 21:23:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1341 --------------------------#
[32m[20221213 21:23:43 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:23:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:43 @agent_ppo2.py:185][0m |          -0.0009 |         162.4937 |          12.0323 |
[32m[20221213 21:23:43 @agent_ppo2.py:185][0m |          -0.0039 |         160.0834 |          12.0368 |
[32m[20221213 21:23:43 @agent_ppo2.py:185][0m |          -0.0047 |         157.9981 |          11.9865 |
[32m[20221213 21:23:43 @agent_ppo2.py:185][0m |          -0.0069 |         157.0490 |          11.9854 |
[32m[20221213 21:23:43 @agent_ppo2.py:185][0m |          -0.0059 |         156.6083 |          11.9979 |
[32m[20221213 21:23:43 @agent_ppo2.py:185][0m |          -0.0083 |         155.3799 |          11.9937 |
[32m[20221213 21:23:43 @agent_ppo2.py:185][0m |          -0.0084 |         154.8570 |          11.9778 |
[32m[20221213 21:23:43 @agent_ppo2.py:185][0m |          -0.0101 |         154.3110 |          11.9639 |
[32m[20221213 21:23:44 @agent_ppo2.py:185][0m |          -0.0032 |         160.7818 |          11.9602 |
[32m[20221213 21:23:44 @agent_ppo2.py:185][0m |          -0.0055 |         156.9772 |          11.9498 |
[32m[20221213 21:23:44 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:23:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.20
[32m[20221213 21:23:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.00
[32m[20221213 21:23:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.00
[32m[20221213 21:23:44 @agent_ppo2.py:143][0m Total time:      28.15 min
[32m[20221213 21:23:44 @agent_ppo2.py:145][0m 2748416 total steps have happened
[32m[20221213 21:23:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1342 --------------------------#
[32m[20221213 21:23:44 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:23:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:44 @agent_ppo2.py:185][0m |          -0.0004 |         163.4677 |          11.8915 |
[32m[20221213 21:23:44 @agent_ppo2.py:185][0m |          -0.0054 |         161.3070 |          11.8883 |
[32m[20221213 21:23:44 @agent_ppo2.py:185][0m |          -0.0072 |         160.1167 |          11.9149 |
[32m[20221213 21:23:44 @agent_ppo2.py:185][0m |          -0.0048 |         161.0714 |          11.9064 |
[32m[20221213 21:23:44 @agent_ppo2.py:185][0m |          -0.0080 |         158.4181 |          11.9277 |
[32m[20221213 21:23:44 @agent_ppo2.py:185][0m |          -0.0072 |         157.8891 |          11.9411 |
[32m[20221213 21:23:45 @agent_ppo2.py:185][0m |          -0.0013 |         163.0926 |          11.9508 |
[32m[20221213 21:23:45 @agent_ppo2.py:185][0m |          -0.0098 |         157.1598 |          11.9167 |
[32m[20221213 21:23:45 @agent_ppo2.py:185][0m |          -0.0054 |         157.1332 |          11.9432 |
[32m[20221213 21:23:45 @agent_ppo2.py:185][0m |          -0.0105 |         156.4766 |          11.9638 |
[32m[20221213 21:23:45 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:23:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.00
[32m[20221213 21:23:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.00
[32m[20221213 21:23:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.00
[32m[20221213 21:23:45 @agent_ppo2.py:143][0m Total time:      28.17 min
[32m[20221213 21:23:45 @agent_ppo2.py:145][0m 2750464 total steps have happened
[32m[20221213 21:23:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1343 --------------------------#
[32m[20221213 21:23:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:23:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:45 @agent_ppo2.py:185][0m |           0.0008 |         161.1370 |          12.1008 |
[32m[20221213 21:23:45 @agent_ppo2.py:185][0m |          -0.0053 |         158.8881 |          12.0932 |
[32m[20221213 21:23:45 @agent_ppo2.py:185][0m |          -0.0068 |         158.1178 |          12.1129 |
[32m[20221213 21:23:46 @agent_ppo2.py:185][0m |          -0.0060 |         158.1609 |          12.0842 |
[32m[20221213 21:23:46 @agent_ppo2.py:185][0m |          -0.0079 |         157.4035 |          12.0999 |
[32m[20221213 21:23:46 @agent_ppo2.py:185][0m |          -0.0078 |         157.3301 |          12.1023 |
[32m[20221213 21:23:46 @agent_ppo2.py:185][0m |          -0.0024 |         161.4768 |          12.1115 |
[32m[20221213 21:23:46 @agent_ppo2.py:185][0m |          -0.0106 |         155.9222 |          12.0923 |
[32m[20221213 21:23:46 @agent_ppo2.py:185][0m |          -0.0107 |         155.2803 |          12.1262 |
[32m[20221213 21:23:46 @agent_ppo2.py:185][0m |          -0.0112 |         154.8303 |          12.1030 |
[32m[20221213 21:23:46 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:23:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.60
[32m[20221213 21:23:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 21:23:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.00
[32m[20221213 21:23:46 @agent_ppo2.py:143][0m Total time:      28.19 min
[32m[20221213 21:23:46 @agent_ppo2.py:145][0m 2752512 total steps have happened
[32m[20221213 21:23:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1344 --------------------------#
[32m[20221213 21:23:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:23:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:47 @agent_ppo2.py:185][0m |          -0.0040 |         167.7346 |          12.0975 |
[32m[20221213 21:23:47 @agent_ppo2.py:185][0m |          -0.0041 |         163.0164 |          12.0276 |
[32m[20221213 21:23:47 @agent_ppo2.py:185][0m |          -0.0045 |         161.4863 |          12.0355 |
[32m[20221213 21:23:47 @agent_ppo2.py:185][0m |          -0.0057 |         160.3277 |          12.0830 |
[32m[20221213 21:23:47 @agent_ppo2.py:185][0m |          -0.0063 |         159.6835 |          12.0274 |
[32m[20221213 21:23:47 @agent_ppo2.py:185][0m |          -0.0081 |         158.6861 |          12.0145 |
[32m[20221213 21:23:47 @agent_ppo2.py:185][0m |          -0.0098 |         158.1201 |          12.0298 |
[32m[20221213 21:23:47 @agent_ppo2.py:185][0m |          -0.0073 |         157.9541 |          12.0459 |
[32m[20221213 21:23:47 @agent_ppo2.py:185][0m |          -0.0128 |         157.2000 |          12.0132 |
[32m[20221213 21:23:47 @agent_ppo2.py:185][0m |          -0.0132 |         156.8674 |          11.9867 |
[32m[20221213 21:23:47 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:23:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.80
[32m[20221213 21:23:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.00
[32m[20221213 21:23:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.00
[32m[20221213 21:23:47 @agent_ppo2.py:143][0m Total time:      28.21 min
[32m[20221213 21:23:47 @agent_ppo2.py:145][0m 2754560 total steps have happened
[32m[20221213 21:23:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1345 --------------------------#
[32m[20221213 21:23:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:23:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:48 @agent_ppo2.py:185][0m |          -0.0004 |         162.9634 |          12.0389 |
[32m[20221213 21:23:48 @agent_ppo2.py:185][0m |          -0.0027 |         161.4478 |          12.0338 |
[32m[20221213 21:23:48 @agent_ppo2.py:185][0m |          -0.0033 |         161.1950 |          12.0358 |
[32m[20221213 21:23:48 @agent_ppo2.py:185][0m |          -0.0062 |         159.7550 |          12.0141 |
[32m[20221213 21:23:48 @agent_ppo2.py:185][0m |          -0.0090 |         159.4048 |          12.0198 |
[32m[20221213 21:23:48 @agent_ppo2.py:185][0m |          -0.0070 |         159.0296 |          12.0113 |
[32m[20221213 21:23:48 @agent_ppo2.py:185][0m |          -0.0100 |         158.8434 |          11.9947 |
[32m[20221213 21:23:48 @agent_ppo2.py:185][0m |          -0.0099 |         158.5066 |          11.9797 |
[32m[20221213 21:23:48 @agent_ppo2.py:185][0m |          -0.0086 |         158.3375 |          11.9738 |
[32m[20221213 21:23:49 @agent_ppo2.py:185][0m |          -0.0101 |         158.1352 |          11.9653 |
[32m[20221213 21:23:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:23:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.40
[32m[20221213 21:23:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:23:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 21:23:49 @agent_ppo2.py:143][0m Total time:      28.23 min
[32m[20221213 21:23:49 @agent_ppo2.py:145][0m 2756608 total steps have happened
[32m[20221213 21:23:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1346 --------------------------#
[32m[20221213 21:23:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:23:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:49 @agent_ppo2.py:185][0m |           0.0001 |         162.2686 |          11.9679 |
[32m[20221213 21:23:49 @agent_ppo2.py:185][0m |          -0.0056 |         160.7570 |          11.9481 |
[32m[20221213 21:23:49 @agent_ppo2.py:185][0m |          -0.0052 |         159.5285 |          11.9593 |
[32m[20221213 21:23:49 @agent_ppo2.py:185][0m |          -0.0055 |         158.8393 |          11.9451 |
[32m[20221213 21:23:49 @agent_ppo2.py:185][0m |          -0.0046 |         158.1809 |          11.9333 |
[32m[20221213 21:23:49 @agent_ppo2.py:185][0m |          -0.0061 |         157.7586 |          11.9605 |
[32m[20221213 21:23:50 @agent_ppo2.py:185][0m |          -0.0037 |         157.9773 |          11.9380 |
[32m[20221213 21:23:50 @agent_ppo2.py:185][0m |          -0.0057 |         157.2027 |          11.9265 |
[32m[20221213 21:23:50 @agent_ppo2.py:185][0m |          -0.0060 |         157.0023 |          11.9863 |
[32m[20221213 21:23:50 @agent_ppo2.py:185][0m |          -0.0064 |         156.7050 |          11.9684 |
[32m[20221213 21:23:50 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:23:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.80
[32m[20221213 21:23:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.00
[32m[20221213 21:23:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.00
[32m[20221213 21:23:50 @agent_ppo2.py:143][0m Total time:      28.25 min
[32m[20221213 21:23:50 @agent_ppo2.py:145][0m 2758656 total steps have happened
[32m[20221213 21:23:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1347 --------------------------#
[32m[20221213 21:23:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:23:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:50 @agent_ppo2.py:185][0m |          -0.0010 |         162.3747 |          12.1986 |
[32m[20221213 21:23:50 @agent_ppo2.py:185][0m |          -0.0033 |         161.1249 |          12.1475 |
[32m[20221213 21:23:50 @agent_ppo2.py:185][0m |          -0.0016 |         161.4696 |          12.1327 |
[32m[20221213 21:23:51 @agent_ppo2.py:185][0m |          -0.0026 |         160.5091 |          12.1159 |
[32m[20221213 21:23:51 @agent_ppo2.py:185][0m |          -0.0083 |         159.7501 |          12.1280 |
[32m[20221213 21:23:51 @agent_ppo2.py:185][0m |          -0.0081 |         159.4641 |          12.0807 |
[32m[20221213 21:23:51 @agent_ppo2.py:185][0m |          -0.0083 |         159.1900 |          12.0733 |
[32m[20221213 21:23:51 @agent_ppo2.py:185][0m |          -0.0037 |         161.2934 |          12.0892 |
[32m[20221213 21:23:51 @agent_ppo2.py:185][0m |          -0.0065 |         158.9682 |          12.0647 |
[32m[20221213 21:23:51 @agent_ppo2.py:185][0m |          -0.0108 |         158.7922 |          12.0796 |
[32m[20221213 21:23:51 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:23:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 763.80
[32m[20221213 21:23:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 21:23:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.00
[32m[20221213 21:23:51 @agent_ppo2.py:143][0m Total time:      28.27 min
[32m[20221213 21:23:51 @agent_ppo2.py:145][0m 2760704 total steps have happened
[32m[20221213 21:23:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1348 --------------------------#
[32m[20221213 21:23:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:23:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:51 @agent_ppo2.py:185][0m |           0.0034 |         161.2027 |          11.8955 |
[32m[20221213 21:23:52 @agent_ppo2.py:185][0m |          -0.0028 |         160.4803 |          11.8823 |
[32m[20221213 21:23:52 @agent_ppo2.py:185][0m |           0.0020 |         166.5979 |          11.8639 |
[32m[20221213 21:23:52 @agent_ppo2.py:185][0m |          -0.0026 |         160.4755 |          11.8127 |
[32m[20221213 21:23:52 @agent_ppo2.py:185][0m |           0.0032 |         164.3781 |          11.7901 |
[32m[20221213 21:23:52 @agent_ppo2.py:185][0m |          -0.0078 |         159.6409 |          11.7950 |
[32m[20221213 21:23:52 @agent_ppo2.py:185][0m |          -0.0072 |         159.4349 |          11.8116 |
[32m[20221213 21:23:52 @agent_ppo2.py:185][0m |          -0.0050 |         159.2539 |          11.7601 |
[32m[20221213 21:23:52 @agent_ppo2.py:185][0m |          -0.0048 |         159.7918 |          11.7456 |
[32m[20221213 21:23:52 @agent_ppo2.py:185][0m |          -0.0034 |         160.4732 |          11.7271 |
[32m[20221213 21:23:52 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:23:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 762.60
[32m[20221213 21:23:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.00
[32m[20221213 21:23:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.00
[32m[20221213 21:23:52 @agent_ppo2.py:143][0m Total time:      28.30 min
[32m[20221213 21:23:52 @agent_ppo2.py:145][0m 2762752 total steps have happened
[32m[20221213 21:23:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1349 --------------------------#
[32m[20221213 21:23:53 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 21:23:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:53 @agent_ppo2.py:185][0m |           0.0036 |         161.4004 |          11.2713 |
[32m[20221213 21:23:53 @agent_ppo2.py:185][0m |          -0.0056 |         160.1324 |          11.2359 |
[32m[20221213 21:23:53 @agent_ppo2.py:185][0m |          -0.0030 |         160.1402 |          11.2440 |
[32m[20221213 21:23:53 @agent_ppo2.py:185][0m |          -0.0056 |         159.5079 |          11.2552 |
[32m[20221213 21:23:53 @agent_ppo2.py:185][0m |           0.0045 |         169.3068 |          11.1930 |
[32m[20221213 21:23:53 @agent_ppo2.py:185][0m |          -0.0046 |         159.0795 |          11.2461 |
[32m[20221213 21:23:53 @agent_ppo2.py:185][0m |          -0.0065 |         158.8637 |          11.2503 |
[32m[20221213 21:23:54 @agent_ppo2.py:185][0m |          -0.0023 |         160.3711 |          11.2385 |
[32m[20221213 21:23:54 @agent_ppo2.py:185][0m |          -0.0082 |         158.6330 |          11.2315 |
[32m[20221213 21:23:54 @agent_ppo2.py:185][0m |          -0.0070 |         158.7195 |          11.2447 |
[32m[20221213 21:23:54 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:23:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 770.20
[32m[20221213 21:23:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.00
[32m[20221213 21:23:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.00
[32m[20221213 21:23:54 @agent_ppo2.py:143][0m Total time:      28.32 min
[32m[20221213 21:23:54 @agent_ppo2.py:145][0m 2764800 total steps have happened
[32m[20221213 21:23:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1350 --------------------------#
[32m[20221213 21:23:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:23:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:54 @agent_ppo2.py:185][0m |          -0.0020 |         159.0379 |          11.6471 |
[32m[20221213 21:23:54 @agent_ppo2.py:185][0m |          -0.0047 |         158.1036 |          11.6615 |
[32m[20221213 21:23:54 @agent_ppo2.py:185][0m |          -0.0072 |         157.6683 |          11.6547 |
[32m[20221213 21:23:55 @agent_ppo2.py:185][0m |          -0.0081 |         157.3153 |          11.6183 |
[32m[20221213 21:23:55 @agent_ppo2.py:185][0m |          -0.0064 |         157.1466 |          11.6321 |
[32m[20221213 21:23:55 @agent_ppo2.py:185][0m |          -0.0096 |         156.8758 |          11.6300 |
[32m[20221213 21:23:55 @agent_ppo2.py:185][0m |          -0.0078 |         156.6706 |          11.5915 |
[32m[20221213 21:23:55 @agent_ppo2.py:185][0m |          -0.0085 |         156.4492 |          11.6242 |
[32m[20221213 21:23:55 @agent_ppo2.py:185][0m |          -0.0090 |         156.4412 |          11.6253 |
[32m[20221213 21:23:55 @agent_ppo2.py:185][0m |          -0.0056 |         156.6175 |          11.6361 |
[32m[20221213 21:23:55 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:23:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.80
[32m[20221213 21:23:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.00
[32m[20221213 21:23:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 788.00
[32m[20221213 21:23:55 @agent_ppo2.py:143][0m Total time:      28.34 min
[32m[20221213 21:23:55 @agent_ppo2.py:145][0m 2766848 total steps have happened
[32m[20221213 21:23:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1351 --------------------------#
[32m[20221213 21:23:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:23:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:55 @agent_ppo2.py:185][0m |           0.0031 |         163.9105 |          11.5651 |
[32m[20221213 21:23:56 @agent_ppo2.py:185][0m |          -0.0022 |         162.2137 |          11.5543 |
[32m[20221213 21:23:56 @agent_ppo2.py:185][0m |          -0.0036 |         161.4459 |          11.5306 |
[32m[20221213 21:23:56 @agent_ppo2.py:185][0m |          -0.0050 |         160.9529 |          11.5344 |
[32m[20221213 21:23:56 @agent_ppo2.py:185][0m |           0.0062 |         181.1248 |          11.5553 |
[32m[20221213 21:23:56 @agent_ppo2.py:185][0m |           0.0031 |         170.1455 |          11.5355 |
[32m[20221213 21:23:56 @agent_ppo2.py:185][0m |          -0.0071 |         159.7792 |          11.5475 |
[32m[20221213 21:23:56 @agent_ppo2.py:185][0m |          -0.0049 |         159.8131 |          11.5284 |
[32m[20221213 21:23:56 @agent_ppo2.py:185][0m |          -0.0084 |         159.2824 |          11.5110 |
[32m[20221213 21:23:56 @agent_ppo2.py:185][0m |          -0.0075 |         158.9935 |          11.5137 |
[32m[20221213 21:23:56 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:23:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.60
[32m[20221213 21:23:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.00
[32m[20221213 21:23:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 762.00
[32m[20221213 21:23:56 @agent_ppo2.py:143][0m Total time:      28.36 min
[32m[20221213 21:23:56 @agent_ppo2.py:145][0m 2768896 total steps have happened
[32m[20221213 21:23:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1352 --------------------------#
[32m[20221213 21:23:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:23:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:57 @agent_ppo2.py:185][0m |          -0.0001 |         162.3241 |          11.1907 |
[32m[20221213 21:23:57 @agent_ppo2.py:185][0m |          -0.0001 |         162.5644 |          11.2514 |
[32m[20221213 21:23:57 @agent_ppo2.py:185][0m |          -0.0038 |         161.2378 |          11.2197 |
[32m[20221213 21:23:57 @agent_ppo2.py:185][0m |          -0.0051 |         160.9283 |          11.2188 |
[32m[20221213 21:23:57 @agent_ppo2.py:185][0m |          -0.0044 |         160.5632 |          11.2835 |
[32m[20221213 21:23:57 @agent_ppo2.py:185][0m |          -0.0048 |         160.1696 |          11.2387 |
[32m[20221213 21:23:57 @agent_ppo2.py:185][0m |          -0.0047 |         159.8352 |          11.2223 |
[32m[20221213 21:23:57 @agent_ppo2.py:185][0m |           0.0059 |         171.0830 |          11.2151 |
[32m[20221213 21:23:57 @agent_ppo2.py:185][0m |          -0.0051 |         160.0498 |          11.2421 |
[32m[20221213 21:23:58 @agent_ppo2.py:185][0m |          -0.0056 |         159.1163 |          11.2456 |
[32m[20221213 21:23:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:23:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 753.00
[32m[20221213 21:23:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.00
[32m[20221213 21:23:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.00
[32m[20221213 21:23:58 @agent_ppo2.py:143][0m Total time:      28.38 min
[32m[20221213 21:23:58 @agent_ppo2.py:145][0m 2770944 total steps have happened
[32m[20221213 21:23:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1353 --------------------------#
[32m[20221213 21:23:58 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:23:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:58 @agent_ppo2.py:185][0m |          -0.0032 |         163.0870 |          11.4098 |
[32m[20221213 21:23:58 @agent_ppo2.py:185][0m |          -0.0040 |         162.2558 |          11.4302 |
[32m[20221213 21:23:58 @agent_ppo2.py:185][0m |          -0.0046 |         162.2517 |          11.3986 |
[32m[20221213 21:23:58 @agent_ppo2.py:185][0m |          -0.0090 |         160.6996 |          11.4264 |
[32m[20221213 21:23:58 @agent_ppo2.py:185][0m |          -0.0062 |         161.0166 |          11.3840 |
[32m[20221213 21:23:58 @agent_ppo2.py:185][0m |          -0.0096 |         159.8910 |          11.3781 |
[32m[20221213 21:23:58 @agent_ppo2.py:185][0m |          -0.0103 |         159.5127 |          11.3859 |
[32m[20221213 21:23:59 @agent_ppo2.py:185][0m |          -0.0044 |         162.6633 |          11.3688 |
[32m[20221213 21:23:59 @agent_ppo2.py:185][0m |          -0.0090 |         158.8190 |          11.3795 |
[32m[20221213 21:23:59 @agent_ppo2.py:185][0m |          -0.0098 |         158.6119 |          11.3722 |
[32m[20221213 21:23:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:23:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 772.80
[32m[20221213 21:23:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:23:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.00
[32m[20221213 21:23:59 @agent_ppo2.py:143][0m Total time:      28.40 min
[32m[20221213 21:23:59 @agent_ppo2.py:145][0m 2772992 total steps have happened
[32m[20221213 21:23:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1354 --------------------------#
[32m[20221213 21:23:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:23:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:23:59 @agent_ppo2.py:185][0m |          -0.0034 |         162.4767 |          11.1852 |
[32m[20221213 21:23:59 @agent_ppo2.py:185][0m |          -0.0045 |         161.0981 |          11.2989 |
[32m[20221213 21:23:59 @agent_ppo2.py:185][0m |          -0.0082 |         160.2755 |          11.2668 |
[32m[20221213 21:23:59 @agent_ppo2.py:185][0m |          -0.0050 |         160.2171 |          11.2264 |
[32m[20221213 21:24:00 @agent_ppo2.py:185][0m |          -0.0064 |         159.3560 |          11.2098 |
[32m[20221213 21:24:00 @agent_ppo2.py:185][0m |          -0.0077 |         159.3761 |          11.2263 |
[32m[20221213 21:24:00 @agent_ppo2.py:185][0m |          -0.0039 |         161.1187 |          11.2366 |
[32m[20221213 21:24:00 @agent_ppo2.py:185][0m |          -0.0083 |         158.4785 |          11.1692 |
[32m[20221213 21:24:00 @agent_ppo2.py:185][0m |          -0.0040 |         160.9049 |          11.1881 |
[32m[20221213 21:24:00 @agent_ppo2.py:185][0m |          -0.0102 |         158.1516 |          11.2190 |
[32m[20221213 21:24:00 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:24:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.20
[32m[20221213 21:24:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 851.00
[32m[20221213 21:24:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 21:24:00 @agent_ppo2.py:143][0m Total time:      28.42 min
[32m[20221213 21:24:00 @agent_ppo2.py:145][0m 2775040 total steps have happened
[32m[20221213 21:24:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1355 --------------------------#
[32m[20221213 21:24:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:00 @agent_ppo2.py:185][0m |          -0.0007 |         162.4492 |          11.3746 |
[32m[20221213 21:24:00 @agent_ppo2.py:185][0m |          -0.0066 |         161.2636 |          11.3865 |
[32m[20221213 21:24:01 @agent_ppo2.py:185][0m |          -0.0048 |         160.5066 |          11.3737 |
[32m[20221213 21:24:01 @agent_ppo2.py:185][0m |          -0.0073 |         160.0770 |          11.3509 |
[32m[20221213 21:24:01 @agent_ppo2.py:185][0m |          -0.0078 |         159.5011 |          11.3555 |
[32m[20221213 21:24:01 @agent_ppo2.py:185][0m |          -0.0069 |         159.5291 |          11.3168 |
[32m[20221213 21:24:01 @agent_ppo2.py:185][0m |          -0.0050 |         160.1808 |          11.3363 |
[32m[20221213 21:24:01 @agent_ppo2.py:185][0m |          -0.0039 |         159.2613 |          11.3160 |
[32m[20221213 21:24:01 @agent_ppo2.py:185][0m |          -0.0033 |         162.5388 |          11.3040 |
[32m[20221213 21:24:01 @agent_ppo2.py:185][0m |          -0.0087 |         158.2918 |          11.2837 |
[32m[20221213 21:24:01 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:24:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 734.20
[32m[20221213 21:24:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.00
[32m[20221213 21:24:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.00
[32m[20221213 21:24:01 @agent_ppo2.py:143][0m Total time:      28.45 min
[32m[20221213 21:24:01 @agent_ppo2.py:145][0m 2777088 total steps have happened
[32m[20221213 21:24:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1356 --------------------------#
[32m[20221213 21:24:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:02 @agent_ppo2.py:185][0m |           0.0138 |         183.7441 |          11.0508 |
[32m[20221213 21:24:02 @agent_ppo2.py:185][0m |          -0.0044 |         162.3608 |          11.1014 |
[32m[20221213 21:24:02 @agent_ppo2.py:185][0m |          -0.0043 |         162.3696 |          11.1434 |
[32m[20221213 21:24:02 @agent_ppo2.py:185][0m |          -0.0051 |         161.4625 |          11.1155 |
[32m[20221213 21:24:02 @agent_ppo2.py:185][0m |          -0.0070 |         161.2100 |          11.1410 |
[32m[20221213 21:24:02 @agent_ppo2.py:185][0m |          -0.0091 |         161.1691 |          11.1920 |
[32m[20221213 21:24:02 @agent_ppo2.py:185][0m |          -0.0078 |         160.9630 |          11.1836 |
[32m[20221213 21:24:02 @agent_ppo2.py:185][0m |           0.0002 |         164.3598 |          11.2044 |
[32m[20221213 21:24:02 @agent_ppo2.py:185][0m |          -0.0043 |         162.7797 |          11.2197 |
[32m[20221213 21:24:03 @agent_ppo2.py:185][0m |          -0.0093 |         160.6400 |          11.2416 |
[32m[20221213 21:24:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:24:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.80
[32m[20221213 21:24:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:24:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 21:24:03 @agent_ppo2.py:143][0m Total time:      28.47 min
[32m[20221213 21:24:03 @agent_ppo2.py:145][0m 2779136 total steps have happened
[32m[20221213 21:24:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1357 --------------------------#
[32m[20221213 21:24:03 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:24:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:03 @agent_ppo2.py:185][0m |           0.0019 |         162.1343 |          11.0757 |
[32m[20221213 21:24:03 @agent_ppo2.py:185][0m |          -0.0029 |         161.2787 |          11.0825 |
[32m[20221213 21:24:03 @agent_ppo2.py:185][0m |          -0.0044 |         161.0812 |          11.0779 |
[32m[20221213 21:24:03 @agent_ppo2.py:185][0m |          -0.0038 |         160.6332 |          11.0862 |
[32m[20221213 21:24:03 @agent_ppo2.py:185][0m |          -0.0055 |         160.1754 |          11.0640 |
[32m[20221213 21:24:03 @agent_ppo2.py:185][0m |          -0.0046 |         160.0741 |          11.0723 |
[32m[20221213 21:24:04 @agent_ppo2.py:185][0m |          -0.0056 |         159.8735 |          11.0704 |
[32m[20221213 21:24:04 @agent_ppo2.py:185][0m |          -0.0079 |         159.8387 |          11.0479 |
[32m[20221213 21:24:04 @agent_ppo2.py:185][0m |          -0.0060 |         159.3687 |          11.0805 |
[32m[20221213 21:24:04 @agent_ppo2.py:185][0m |          -0.0054 |         159.5313 |          11.0688 |
[32m[20221213 21:24:04 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:24:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.40
[32m[20221213 21:24:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.00
[32m[20221213 21:24:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.00
[32m[20221213 21:24:04 @agent_ppo2.py:143][0m Total time:      28.49 min
[32m[20221213 21:24:04 @agent_ppo2.py:145][0m 2781184 total steps have happened
[32m[20221213 21:24:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1358 --------------------------#
[32m[20221213 21:24:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:04 @agent_ppo2.py:185][0m |           0.0001 |         165.3787 |          11.0432 |
[32m[20221213 21:24:04 @agent_ppo2.py:185][0m |          -0.0038 |         164.2332 |          10.9863 |
[32m[20221213 21:24:04 @agent_ppo2.py:185][0m |          -0.0034 |         163.3624 |          11.0115 |
[32m[20221213 21:24:05 @agent_ppo2.py:185][0m |          -0.0056 |         162.6470 |          10.9846 |
[32m[20221213 21:24:05 @agent_ppo2.py:185][0m |          -0.0034 |         163.1065 |          10.9585 |
[32m[20221213 21:24:05 @agent_ppo2.py:185][0m |          -0.0072 |         162.2189 |          10.9813 |
[32m[20221213 21:24:05 @agent_ppo2.py:185][0m |          -0.0075 |         161.6779 |          10.9811 |
[32m[20221213 21:24:05 @agent_ppo2.py:185][0m |          -0.0090 |         161.7674 |          10.9656 |
[32m[20221213 21:24:05 @agent_ppo2.py:185][0m |          -0.0028 |         163.2278 |          10.9660 |
[32m[20221213 21:24:05 @agent_ppo2.py:185][0m |          -0.0106 |         161.2334 |          10.9538 |
[32m[20221213 21:24:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:24:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.20
[32m[20221213 21:24:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.00
[32m[20221213 21:24:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.00
[32m[20221213 21:24:05 @agent_ppo2.py:143][0m Total time:      28.51 min
[32m[20221213 21:24:05 @agent_ppo2.py:145][0m 2783232 total steps have happened
[32m[20221213 21:24:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1359 --------------------------#
[32m[20221213 21:24:05 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:24:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:06 @agent_ppo2.py:185][0m |          -0.0014 |         163.2422 |          11.3171 |
[32m[20221213 21:24:06 @agent_ppo2.py:185][0m |           0.0001 |         165.0336 |          11.3522 |
[32m[20221213 21:24:06 @agent_ppo2.py:185][0m |          -0.0058 |         158.4047 |          11.3633 |
[32m[20221213 21:24:06 @agent_ppo2.py:185][0m |          -0.0015 |         158.7454 |          11.3567 |
[32m[20221213 21:24:06 @agent_ppo2.py:185][0m |          -0.0006 |         159.5129 |          11.3521 |
[32m[20221213 21:24:06 @agent_ppo2.py:185][0m |          -0.0022 |         157.6871 |          11.3488 |
[32m[20221213 21:24:06 @agent_ppo2.py:185][0m |          -0.0011 |         156.2242 |          11.3343 |
[32m[20221213 21:24:06 @agent_ppo2.py:185][0m |          -0.0097 |         154.0873 |          11.3028 |
[32m[20221213 21:24:06 @agent_ppo2.py:185][0m |          -0.0042 |         157.1528 |          11.2973 |
[32m[20221213 21:24:06 @agent_ppo2.py:185][0m |           0.0008 |         158.6205 |          11.2841 |
[32m[20221213 21:24:06 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:24:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.20
[32m[20221213 21:24:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.00
[32m[20221213 21:24:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.00
[32m[20221213 21:24:06 @agent_ppo2.py:143][0m Total time:      28.53 min
[32m[20221213 21:24:06 @agent_ppo2.py:145][0m 2785280 total steps have happened
[32m[20221213 21:24:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1360 --------------------------#
[32m[20221213 21:24:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:07 @agent_ppo2.py:185][0m |          -0.0023 |         170.8388 |          11.0041 |
[32m[20221213 21:24:07 @agent_ppo2.py:185][0m |          -0.0075 |         165.8345 |          11.0328 |
[32m[20221213 21:24:07 @agent_ppo2.py:185][0m |          -0.0028 |         168.6388 |          11.0136 |
[32m[20221213 21:24:07 @agent_ppo2.py:185][0m |           0.0046 |         174.1096 |          11.0556 |
[32m[20221213 21:24:07 @agent_ppo2.py:185][0m |          -0.0088 |         163.2682 |          11.1360 |
[32m[20221213 21:24:07 @agent_ppo2.py:185][0m |          -0.0082 |         162.4209 |          11.1689 |
[32m[20221213 21:24:07 @agent_ppo2.py:185][0m |          -0.0049 |         164.0973 |          11.1714 |
[32m[20221213 21:24:07 @agent_ppo2.py:185][0m |          -0.0112 |         161.9412 |          11.1909 |
[32m[20221213 21:24:07 @agent_ppo2.py:185][0m |          -0.0113 |         161.4706 |          11.1928 |
[32m[20221213 21:24:08 @agent_ppo2.py:185][0m |          -0.0103 |         161.5724 |          11.2122 |
[32m[20221213 21:24:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:24:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 763.20
[32m[20221213 21:24:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.00
[32m[20221213 21:24:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.00
[32m[20221213 21:24:08 @agent_ppo2.py:143][0m Total time:      28.55 min
[32m[20221213 21:24:08 @agent_ppo2.py:145][0m 2787328 total steps have happened
[32m[20221213 21:24:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1361 --------------------------#
[32m[20221213 21:24:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:08 @agent_ppo2.py:185][0m |           0.0010 |         165.8769 |          10.9715 |
[32m[20221213 21:24:08 @agent_ppo2.py:185][0m |          -0.0044 |         163.4278 |          10.9585 |
[32m[20221213 21:24:08 @agent_ppo2.py:185][0m |          -0.0049 |         162.3155 |          10.9681 |
[32m[20221213 21:24:08 @agent_ppo2.py:185][0m |          -0.0060 |         161.7068 |          10.9820 |
[32m[20221213 21:24:08 @agent_ppo2.py:185][0m |          -0.0040 |         161.3706 |          10.9378 |
[32m[20221213 21:24:08 @agent_ppo2.py:185][0m |          -0.0083 |         161.1710 |          10.9289 |
[32m[20221213 21:24:08 @agent_ppo2.py:185][0m |           0.0032 |         178.3522 |          10.8785 |
[32m[20221213 21:24:09 @agent_ppo2.py:185][0m |          -0.0029 |         164.9373 |          10.8821 |
[32m[20221213 21:24:09 @agent_ppo2.py:185][0m |          -0.0074 |         160.5122 |          10.8601 |
[32m[20221213 21:24:09 @agent_ppo2.py:185][0m |          -0.0068 |         160.4181 |          10.8871 |
[32m[20221213 21:24:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:24:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.40
[32m[20221213 21:24:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.00
[32m[20221213 21:24:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.00
[32m[20221213 21:24:09 @agent_ppo2.py:143][0m Total time:      28.57 min
[32m[20221213 21:24:09 @agent_ppo2.py:145][0m 2789376 total steps have happened
[32m[20221213 21:24:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1362 --------------------------#
[32m[20221213 21:24:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:24:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:09 @agent_ppo2.py:185][0m |          -0.0040 |         164.2559 |          11.1238 |
[32m[20221213 21:24:09 @agent_ppo2.py:185][0m |          -0.0047 |         163.0895 |          11.1525 |
[32m[20221213 21:24:09 @agent_ppo2.py:185][0m |          -0.0085 |         162.9905 |          11.1630 |
[32m[20221213 21:24:09 @agent_ppo2.py:185][0m |          -0.0080 |         162.6356 |          11.2093 |
[32m[20221213 21:24:10 @agent_ppo2.py:185][0m |           0.0009 |         167.0128 |          11.2173 |
[32m[20221213 21:24:10 @agent_ppo2.py:185][0m |          -0.0059 |         162.2536 |          11.2148 |
[32m[20221213 21:24:10 @agent_ppo2.py:185][0m |          -0.0006 |         168.6335 |          11.2618 |
[32m[20221213 21:24:10 @agent_ppo2.py:185][0m |          -0.0103 |         162.0346 |          11.2749 |
[32m[20221213 21:24:10 @agent_ppo2.py:185][0m |          -0.0087 |         161.6100 |          11.2396 |
[32m[20221213 21:24:10 @agent_ppo2.py:185][0m |          -0.0063 |         162.2257 |          11.2834 |
[32m[20221213 21:24:10 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:24:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.00
[32m[20221213 21:24:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.00
[32m[20221213 21:24:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.00
[32m[20221213 21:24:10 @agent_ppo2.py:143][0m Total time:      28.59 min
[32m[20221213 21:24:10 @agent_ppo2.py:145][0m 2791424 total steps have happened
[32m[20221213 21:24:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1363 --------------------------#
[32m[20221213 21:24:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:10 @agent_ppo2.py:185][0m |          -0.0018 |         162.9952 |          11.2159 |
[32m[20221213 21:24:11 @agent_ppo2.py:185][0m |           0.0042 |         170.3268 |          11.2071 |
[32m[20221213 21:24:11 @agent_ppo2.py:185][0m |          -0.0056 |         161.8679 |          11.1876 |
[32m[20221213 21:24:11 @agent_ppo2.py:185][0m |          -0.0038 |         161.6306 |          11.1829 |
[32m[20221213 21:24:11 @agent_ppo2.py:185][0m |          -0.0038 |         161.9974 |          11.2071 |
[32m[20221213 21:24:11 @agent_ppo2.py:185][0m |          -0.0078 |         160.6768 |          11.1911 |
[32m[20221213 21:24:11 @agent_ppo2.py:185][0m |          -0.0056 |         160.4021 |          11.2472 |
[32m[20221213 21:24:11 @agent_ppo2.py:185][0m |          -0.0096 |         160.2784 |          11.1995 |
[32m[20221213 21:24:11 @agent_ppo2.py:185][0m |          -0.0105 |         160.1452 |          11.2084 |
[32m[20221213 21:24:11 @agent_ppo2.py:185][0m |          -0.0004 |         170.0778 |          11.2033 |
[32m[20221213 21:24:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:24:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 746.00
[32m[20221213 21:24:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 777.00
[32m[20221213 21:24:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.00
[32m[20221213 21:24:11 @agent_ppo2.py:143][0m Total time:      28.61 min
[32m[20221213 21:24:11 @agent_ppo2.py:145][0m 2793472 total steps have happened
[32m[20221213 21:24:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1364 --------------------------#
[32m[20221213 21:24:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:12 @agent_ppo2.py:185][0m |           0.0031 |         162.7070 |          11.3695 |
[32m[20221213 21:24:12 @agent_ppo2.py:185][0m |          -0.0062 |         162.3213 |          11.3095 |
[32m[20221213 21:24:12 @agent_ppo2.py:185][0m |          -0.0029 |         162.6436 |          11.2775 |
[32m[20221213 21:24:12 @agent_ppo2.py:185][0m |          -0.0078 |         161.7082 |          11.2689 |
[32m[20221213 21:24:12 @agent_ppo2.py:185][0m |          -0.0087 |         161.5995 |          11.2642 |
[32m[20221213 21:24:12 @agent_ppo2.py:185][0m |           0.0022 |         175.8068 |          11.2804 |
[32m[20221213 21:24:12 @agent_ppo2.py:185][0m |          -0.0086 |         161.3380 |          11.2355 |
[32m[20221213 21:24:12 @agent_ppo2.py:185][0m |          -0.0100 |         161.1179 |          11.2159 |
[32m[20221213 21:24:12 @agent_ppo2.py:185][0m |          -0.0098 |         161.3155 |          11.2219 |
[32m[20221213 21:24:12 @agent_ppo2.py:185][0m |          -0.0035 |         165.2162 |          11.2069 |
[32m[20221213 21:24:12 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:24:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 747.80
[32m[20221213 21:24:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.00
[32m[20221213 21:24:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.00
[32m[20221213 21:24:13 @agent_ppo2.py:143][0m Total time:      28.63 min
[32m[20221213 21:24:13 @agent_ppo2.py:145][0m 2795520 total steps have happened
[32m[20221213 21:24:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1365 --------------------------#
[32m[20221213 21:24:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:24:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:13 @agent_ppo2.py:185][0m |           0.0023 |         162.8017 |          10.7971 |
[32m[20221213 21:24:13 @agent_ppo2.py:185][0m |           0.0087 |         159.6853 |          10.8188 |
[32m[20221213 21:24:13 @agent_ppo2.py:185][0m |          -0.0024 |         149.2808 |          10.8633 |
[32m[20221213 21:24:13 @agent_ppo2.py:185][0m |          -0.0066 |         147.1398 |          10.8415 |
[32m[20221213 21:24:13 @agent_ppo2.py:185][0m |          -0.0056 |         145.7556 |          10.8703 |
[32m[20221213 21:24:13 @agent_ppo2.py:185][0m |          -0.0040 |         148.2425 |          10.8338 |
[32m[20221213 21:24:13 @agent_ppo2.py:185][0m |          -0.0075 |         143.1614 |          10.8634 |
[32m[20221213 21:24:14 @agent_ppo2.py:185][0m |          -0.0062 |         143.4302 |          10.8857 |
[32m[20221213 21:24:14 @agent_ppo2.py:185][0m |          -0.0097 |         142.7330 |          10.8679 |
[32m[20221213 21:24:14 @agent_ppo2.py:185][0m |          -0.0107 |         140.6407 |          10.8845 |
[32m[20221213 21:24:14 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:24:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.60
[32m[20221213 21:24:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:24:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.00
[32m[20221213 21:24:14 @agent_ppo2.py:143][0m Total time:      28.65 min
[32m[20221213 21:24:14 @agent_ppo2.py:145][0m 2797568 total steps have happened
[32m[20221213 21:24:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1366 --------------------------#
[32m[20221213 21:24:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:14 @agent_ppo2.py:185][0m |           0.0023 |         178.2619 |          11.1626 |
[32m[20221213 21:24:14 @agent_ppo2.py:185][0m |          -0.0051 |         169.0484 |          11.1662 |
[32m[20221213 21:24:14 @agent_ppo2.py:185][0m |          -0.0059 |         166.7898 |          11.1509 |
[32m[20221213 21:24:14 @agent_ppo2.py:185][0m |          -0.0036 |         167.0228 |          11.1513 |
[32m[20221213 21:24:14 @agent_ppo2.py:185][0m |          -0.0045 |         170.0325 |          11.1631 |
[32m[20221213 21:24:15 @agent_ppo2.py:185][0m |          -0.0104 |         163.8095 |          11.1360 |
[32m[20221213 21:24:15 @agent_ppo2.py:185][0m |          -0.0093 |         163.3265 |          11.1402 |
[32m[20221213 21:24:15 @agent_ppo2.py:185][0m |          -0.0122 |         163.9566 |          11.1373 |
[32m[20221213 21:24:15 @agent_ppo2.py:185][0m |          -0.0113 |         162.8441 |          11.1399 |
[32m[20221213 21:24:15 @agent_ppo2.py:185][0m |          -0.0067 |         167.3264 |          11.1541 |
[32m[20221213 21:24:15 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:24:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.80
[32m[20221213 21:24:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.00
[32m[20221213 21:24:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.00
[32m[20221213 21:24:15 @agent_ppo2.py:143][0m Total time:      28.67 min
[32m[20221213 21:24:15 @agent_ppo2.py:145][0m 2799616 total steps have happened
[32m[20221213 21:24:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1367 --------------------------#
[32m[20221213 21:24:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:15 @agent_ppo2.py:185][0m |          -0.0006 |         171.3552 |          11.0831 |
[32m[20221213 21:24:15 @agent_ppo2.py:185][0m |           0.0036 |         170.4231 |          11.0606 |
[32m[20221213 21:24:16 @agent_ppo2.py:185][0m |          -0.0070 |         166.4569 |          11.0543 |
[32m[20221213 21:24:16 @agent_ppo2.py:185][0m |          -0.0056 |         165.7129 |          11.0454 |
[32m[20221213 21:24:16 @agent_ppo2.py:185][0m |          -0.0052 |         165.5660 |          11.0709 |
[32m[20221213 21:24:16 @agent_ppo2.py:185][0m |          -0.0027 |         165.1570 |          11.0546 |
[32m[20221213 21:24:16 @agent_ppo2.py:185][0m |          -0.0041 |         164.6456 |          11.0388 |
[32m[20221213 21:24:16 @agent_ppo2.py:185][0m |          -0.0058 |         164.3917 |          11.0569 |
[32m[20221213 21:24:16 @agent_ppo2.py:185][0m |          -0.0095 |         164.2761 |          11.0261 |
[32m[20221213 21:24:16 @agent_ppo2.py:185][0m |          -0.0027 |         165.9142 |          11.0325 |
[32m[20221213 21:24:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:24:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.40
[32m[20221213 21:24:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.00
[32m[20221213 21:24:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.00
[32m[20221213 21:24:16 @agent_ppo2.py:143][0m Total time:      28.69 min
[32m[20221213 21:24:16 @agent_ppo2.py:145][0m 2801664 total steps have happened
[32m[20221213 21:24:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1368 --------------------------#
[32m[20221213 21:24:16 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:24:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:17 @agent_ppo2.py:185][0m |           0.0002 |         159.8230 |          10.7064 |
[32m[20221213 21:24:17 @agent_ppo2.py:185][0m |          -0.0028 |         159.0934 |          10.7105 |
[32m[20221213 21:24:17 @agent_ppo2.py:185][0m |          -0.0034 |         158.7431 |          10.6913 |
[32m[20221213 21:24:17 @agent_ppo2.py:185][0m |           0.0040 |         162.9860 |          10.7095 |
[32m[20221213 21:24:17 @agent_ppo2.py:185][0m |          -0.0053 |         158.2681 |          10.7418 |
[32m[20221213 21:24:17 @agent_ppo2.py:185][0m |          -0.0040 |         158.0191 |          10.7455 |
[32m[20221213 21:24:17 @agent_ppo2.py:185][0m |           0.0131 |         181.2701 |          10.7590 |
[32m[20221213 21:24:17 @agent_ppo2.py:185][0m |          -0.0031 |         158.0476 |          10.7947 |
[32m[20221213 21:24:17 @agent_ppo2.py:185][0m |          -0.0019 |         157.9146 |          10.7416 |
[32m[20221213 21:24:17 @agent_ppo2.py:185][0m |           0.0043 |         166.5671 |          10.7704 |
[32m[20221213 21:24:17 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:24:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 746.00
[32m[20221213 21:24:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 757.00
[32m[20221213 21:24:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.00
[32m[20221213 21:24:17 @agent_ppo2.py:143][0m Total time:      28.71 min
[32m[20221213 21:24:17 @agent_ppo2.py:145][0m 2803712 total steps have happened
[32m[20221213 21:24:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1369 --------------------------#
[32m[20221213 21:24:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:18 @agent_ppo2.py:185][0m |          -0.0013 |         161.4655 |          11.0866 |
[32m[20221213 21:24:18 @agent_ppo2.py:185][0m |          -0.0027 |         158.7513 |          11.0396 |
[32m[20221213 21:24:18 @agent_ppo2.py:185][0m |          -0.0023 |         157.0301 |          11.0110 |
[32m[20221213 21:24:18 @agent_ppo2.py:185][0m |          -0.0046 |         155.5849 |          10.9929 |
[32m[20221213 21:24:18 @agent_ppo2.py:185][0m |          -0.0054 |         154.7682 |          10.9428 |
[32m[20221213 21:24:18 @agent_ppo2.py:185][0m |           0.0041 |         167.3241 |          10.9445 |
[32m[20221213 21:24:18 @agent_ppo2.py:185][0m |          -0.0036 |         153.9532 |          10.9607 |
[32m[20221213 21:24:18 @agent_ppo2.py:185][0m |          -0.0068 |         152.8741 |          10.9091 |
[32m[20221213 21:24:18 @agent_ppo2.py:185][0m |          -0.0100 |         152.6070 |          10.9341 |
[32m[20221213 21:24:19 @agent_ppo2.py:185][0m |          -0.0089 |         152.1763 |          10.8900 |
[32m[20221213 21:24:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:24:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 752.40
[32m[20221213 21:24:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.00
[32m[20221213 21:24:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.00
[32m[20221213 21:24:19 @agent_ppo2.py:143][0m Total time:      28.73 min
[32m[20221213 21:24:19 @agent_ppo2.py:145][0m 2805760 total steps have happened
[32m[20221213 21:24:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1370 --------------------------#
[32m[20221213 21:24:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:19 @agent_ppo2.py:185][0m |          -0.0016 |         161.6341 |          10.5276 |
[32m[20221213 21:24:19 @agent_ppo2.py:185][0m |          -0.0011 |         160.3174 |          10.4600 |
[32m[20221213 21:24:19 @agent_ppo2.py:185][0m |          -0.0048 |         159.8600 |          10.4139 |
[32m[20221213 21:24:19 @agent_ppo2.py:185][0m |          -0.0031 |         159.5101 |          10.3927 |
[32m[20221213 21:24:19 @agent_ppo2.py:185][0m |          -0.0045 |         159.5423 |          10.3761 |
[32m[20221213 21:24:19 @agent_ppo2.py:185][0m |          -0.0051 |         159.0281 |          10.3377 |
[32m[20221213 21:24:20 @agent_ppo2.py:185][0m |          -0.0062 |         158.9149 |          10.3424 |
[32m[20221213 21:24:20 @agent_ppo2.py:185][0m |          -0.0072 |         158.6306 |          10.3065 |
[32m[20221213 21:24:20 @agent_ppo2.py:185][0m |          -0.0030 |         158.9014 |          10.2886 |
[32m[20221213 21:24:20 @agent_ppo2.py:185][0m |          -0.0050 |         158.3857 |          10.2467 |
[32m[20221213 21:24:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:24:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.40
[32m[20221213 21:24:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.00
[32m[20221213 21:24:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 775.00
[32m[20221213 21:24:20 @agent_ppo2.py:143][0m Total time:      28.75 min
[32m[20221213 21:24:20 @agent_ppo2.py:145][0m 2807808 total steps have happened
[32m[20221213 21:24:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1371 --------------------------#
[32m[20221213 21:24:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:20 @agent_ppo2.py:185][0m |           0.0014 |         161.0209 |          10.6777 |
[32m[20221213 21:24:20 @agent_ppo2.py:185][0m |          -0.0005 |         157.5489 |          10.6437 |
[32m[20221213 21:24:20 @agent_ppo2.py:185][0m |          -0.0040 |         156.1160 |          10.6520 |
[32m[20221213 21:24:20 @agent_ppo2.py:185][0m |           0.0021 |         159.8207 |          10.6625 |
[32m[20221213 21:24:21 @agent_ppo2.py:185][0m |          -0.0051 |         154.6858 |          10.6897 |
[32m[20221213 21:24:21 @agent_ppo2.py:185][0m |          -0.0029 |         154.9553 |          10.6784 |
[32m[20221213 21:24:21 @agent_ppo2.py:185][0m |          -0.0061 |         153.6101 |          10.6489 |
[32m[20221213 21:24:21 @agent_ppo2.py:185][0m |          -0.0057 |         153.7207 |          10.6866 |
[32m[20221213 21:24:21 @agent_ppo2.py:185][0m |           0.0044 |         161.4879 |          10.7136 |
[32m[20221213 21:24:21 @agent_ppo2.py:185][0m |          -0.0043 |         153.1140 |          10.7312 |
[32m[20221213 21:24:21 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:24:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.00
[32m[20221213 21:24:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 781.00
[32m[20221213 21:24:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.00
[32m[20221213 21:24:21 @agent_ppo2.py:143][0m Total time:      28.77 min
[32m[20221213 21:24:21 @agent_ppo2.py:145][0m 2809856 total steps have happened
[32m[20221213 21:24:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1372 --------------------------#
[32m[20221213 21:24:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:21 @agent_ppo2.py:185][0m |           0.0064 |         170.6089 |          10.5808 |
[32m[20221213 21:24:22 @agent_ppo2.py:185][0m |          -0.0044 |         162.1999 |          10.6327 |
[32m[20221213 21:24:22 @agent_ppo2.py:185][0m |          -0.0082 |         161.5823 |          10.6110 |
[32m[20221213 21:24:22 @agent_ppo2.py:185][0m |          -0.0074 |         161.0229 |          10.6170 |
[32m[20221213 21:24:22 @agent_ppo2.py:185][0m |          -0.0056 |         162.6810 |          10.6519 |
[32m[20221213 21:24:22 @agent_ppo2.py:185][0m |          -0.0081 |         160.3959 |          10.6551 |
[32m[20221213 21:24:22 @agent_ppo2.py:185][0m |          -0.0092 |         160.3440 |          10.6743 |
[32m[20221213 21:24:22 @agent_ppo2.py:185][0m |          -0.0081 |         160.1355 |          10.7102 |
[32m[20221213 21:24:22 @agent_ppo2.py:185][0m |          -0.0106 |         160.0195 |          10.7266 |
[32m[20221213 21:24:22 @agent_ppo2.py:185][0m |          -0.0076 |         160.0496 |          10.7655 |
[32m[20221213 21:24:22 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:24:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.20
[32m[20221213 21:24:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221213 21:24:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.00
[32m[20221213 21:24:22 @agent_ppo2.py:143][0m Total time:      28.80 min
[32m[20221213 21:24:22 @agent_ppo2.py:145][0m 2811904 total steps have happened
[32m[20221213 21:24:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1373 --------------------------#
[32m[20221213 21:24:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:23 @agent_ppo2.py:185][0m |           0.0013 |         162.2332 |          10.7420 |
[32m[20221213 21:24:23 @agent_ppo2.py:185][0m |          -0.0003 |         162.7047 |          10.7381 |
[32m[20221213 21:24:23 @agent_ppo2.py:185][0m |          -0.0040 |         160.6845 |          10.7588 |
[32m[20221213 21:24:23 @agent_ppo2.py:185][0m |          -0.0062 |         160.4338 |          10.7187 |
[32m[20221213 21:24:23 @agent_ppo2.py:185][0m |          -0.0060 |         160.1905 |          10.7236 |
[32m[20221213 21:24:23 @agent_ppo2.py:185][0m |          -0.0073 |         159.8858 |          10.7294 |
[32m[20221213 21:24:23 @agent_ppo2.py:185][0m |          -0.0063 |         159.9088 |          10.6994 |
[32m[20221213 21:24:23 @agent_ppo2.py:185][0m |          -0.0067 |         160.0142 |          10.6553 |
[32m[20221213 21:24:24 @agent_ppo2.py:185][0m |          -0.0083 |         159.7065 |          10.6659 |
[32m[20221213 21:24:24 @agent_ppo2.py:185][0m |          -0.0092 |         159.5246 |          10.6792 |
[32m[20221213 21:24:24 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:24:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.20
[32m[20221213 21:24:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.00
[32m[20221213 21:24:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.00
[32m[20221213 21:24:24 @agent_ppo2.py:143][0m Total time:      28.82 min
[32m[20221213 21:24:24 @agent_ppo2.py:145][0m 2813952 total steps have happened
[32m[20221213 21:24:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1374 --------------------------#
[32m[20221213 21:24:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:24 @agent_ppo2.py:185][0m |          -0.0018 |         159.9245 |          10.5784 |
[32m[20221213 21:24:24 @agent_ppo2.py:185][0m |          -0.0040 |         158.9370 |          10.5875 |
[32m[20221213 21:24:24 @agent_ppo2.py:185][0m |          -0.0053 |         158.4633 |          10.6089 |
[32m[20221213 21:24:24 @agent_ppo2.py:185][0m |          -0.0004 |         160.8805 |          10.5805 |
[32m[20221213 21:24:24 @agent_ppo2.py:185][0m |          -0.0073 |         157.9345 |          10.5541 |
[32m[20221213 21:24:25 @agent_ppo2.py:185][0m |          -0.0072 |         157.4614 |          10.5832 |
[32m[20221213 21:24:25 @agent_ppo2.py:185][0m |          -0.0038 |         158.6432 |          10.5777 |
[32m[20221213 21:24:25 @agent_ppo2.py:185][0m |           0.0060 |         174.6598 |          10.5642 |
[32m[20221213 21:24:25 @agent_ppo2.py:185][0m |          -0.0080 |         156.8926 |          10.5743 |
[32m[20221213 21:24:25 @agent_ppo2.py:185][0m |          -0.0103 |         156.8140 |          10.5603 |
[32m[20221213 21:24:25 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:24:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.00
[32m[20221213 21:24:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.00
[32m[20221213 21:24:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 21:24:25 @agent_ppo2.py:143][0m Total time:      28.84 min
[32m[20221213 21:24:25 @agent_ppo2.py:145][0m 2816000 total steps have happened
[32m[20221213 21:24:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1375 --------------------------#
[32m[20221213 21:24:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:24:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:25 @agent_ppo2.py:185][0m |          -0.0021 |         160.5591 |          10.4972 |
[32m[20221213 21:24:25 @agent_ppo2.py:185][0m |          -0.0031 |         159.7763 |          10.4916 |
[32m[20221213 21:24:26 @agent_ppo2.py:185][0m |          -0.0042 |         159.5194 |          10.5409 |
[32m[20221213 21:24:26 @agent_ppo2.py:185][0m |          -0.0057 |         158.9113 |          10.4685 |
[32m[20221213 21:24:26 @agent_ppo2.py:185][0m |          -0.0062 |         158.7139 |          10.4990 |
[32m[20221213 21:24:26 @agent_ppo2.py:185][0m |          -0.0005 |         165.3627 |          10.4889 |
[32m[20221213 21:24:26 @agent_ppo2.py:185][0m |          -0.0080 |         158.3177 |          10.4806 |
[32m[20221213 21:24:26 @agent_ppo2.py:185][0m |          -0.0045 |         158.4790 |          10.5018 |
[32m[20221213 21:24:26 @agent_ppo2.py:185][0m |          -0.0055 |         158.9204 |          10.4894 |
[32m[20221213 21:24:26 @agent_ppo2.py:185][0m |          -0.0074 |         158.0889 |          10.4658 |
[32m[20221213 21:24:26 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:24:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 739.00
[32m[20221213 21:24:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.00
[32m[20221213 21:24:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 783.00
[32m[20221213 21:24:26 @agent_ppo2.py:143][0m Total time:      28.86 min
[32m[20221213 21:24:26 @agent_ppo2.py:145][0m 2818048 total steps have happened
[32m[20221213 21:24:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1376 --------------------------#
[32m[20221213 21:24:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:27 @agent_ppo2.py:185][0m |          -0.0012 |         164.3167 |          10.3559 |
[32m[20221213 21:24:27 @agent_ppo2.py:185][0m |           0.0203 |         193.4058 |          10.4215 |
[32m[20221213 21:24:27 @agent_ppo2.py:185][0m |          -0.0032 |         163.5455 |          10.4340 |
[32m[20221213 21:24:27 @agent_ppo2.py:185][0m |          -0.0083 |         161.4659 |          10.4404 |
[32m[20221213 21:24:27 @agent_ppo2.py:185][0m |          -0.0089 |         161.0456 |          10.4119 |
[32m[20221213 21:24:27 @agent_ppo2.py:185][0m |          -0.0077 |         160.5946 |          10.4147 |
[32m[20221213 21:24:27 @agent_ppo2.py:185][0m |          -0.0114 |         160.6514 |          10.3952 |
[32m[20221213 21:24:27 @agent_ppo2.py:185][0m |          -0.0096 |         160.0967 |          10.4005 |
[32m[20221213 21:24:27 @agent_ppo2.py:185][0m |          -0.0091 |         160.0006 |          10.4287 |
[32m[20221213 21:24:27 @agent_ppo2.py:185][0m |          -0.0103 |         159.6471 |          10.3896 |
[32m[20221213 21:24:27 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:24:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 706.80
[32m[20221213 21:24:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.00
[32m[20221213 21:24:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.00
[32m[20221213 21:24:27 @agent_ppo2.py:143][0m Total time:      28.88 min
[32m[20221213 21:24:27 @agent_ppo2.py:145][0m 2820096 total steps have happened
[32m[20221213 21:24:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1377 --------------------------#
[32m[20221213 21:24:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:28 @agent_ppo2.py:185][0m |           0.0020 |         160.0624 |          10.3560 |
[32m[20221213 21:24:28 @agent_ppo2.py:185][0m |          -0.0040 |         157.8527 |          10.3765 |
[32m[20221213 21:24:28 @agent_ppo2.py:185][0m |          -0.0060 |         157.2517 |          10.3853 |
[32m[20221213 21:24:28 @agent_ppo2.py:185][0m |          -0.0069 |         157.0354 |          10.4069 |
[32m[20221213 21:24:28 @agent_ppo2.py:185][0m |          -0.0063 |         156.7364 |          10.3795 |
[32m[20221213 21:24:28 @agent_ppo2.py:185][0m |          -0.0084 |         156.5543 |          10.4380 |
[32m[20221213 21:24:28 @agent_ppo2.py:185][0m |           0.0004 |         161.5752 |          10.4407 |
[32m[20221213 21:24:28 @agent_ppo2.py:185][0m |          -0.0045 |         156.0239 |          10.4058 |
[32m[20221213 21:24:28 @agent_ppo2.py:185][0m |           0.0323 |         202.0155 |          10.4297 |
[32m[20221213 21:24:29 @agent_ppo2.py:185][0m |           0.0041 |         174.1113 |          10.4607 |
[32m[20221213 21:24:29 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:24:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.20
[32m[20221213 21:24:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.00
[32m[20221213 21:24:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.00
[32m[20221213 21:24:29 @agent_ppo2.py:143][0m Total time:      28.90 min
[32m[20221213 21:24:29 @agent_ppo2.py:145][0m 2822144 total steps have happened
[32m[20221213 21:24:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1378 --------------------------#
[32m[20221213 21:24:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:24:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:29 @agent_ppo2.py:185][0m |          -0.0013 |         157.4962 |          10.3667 |
[32m[20221213 21:24:29 @agent_ppo2.py:185][0m |          -0.0052 |         156.9454 |          10.3495 |
[32m[20221213 21:24:29 @agent_ppo2.py:185][0m |          -0.0054 |         156.4647 |          10.4077 |
[32m[20221213 21:24:29 @agent_ppo2.py:185][0m |          -0.0054 |         156.6392 |          10.3835 |
[32m[20221213 21:24:29 @agent_ppo2.py:185][0m |          -0.0089 |         156.0636 |          10.4067 |
[32m[20221213 21:24:29 @agent_ppo2.py:185][0m |          -0.0064 |         155.7901 |          10.4286 |
[32m[20221213 21:24:30 @agent_ppo2.py:185][0m |          -0.0081 |         155.6143 |          10.4254 |
[32m[20221213 21:24:30 @agent_ppo2.py:185][0m |          -0.0073 |         155.5483 |          10.4538 |
[32m[20221213 21:24:30 @agent_ppo2.py:185][0m |          -0.0096 |         155.3075 |          10.4653 |
[32m[20221213 21:24:30 @agent_ppo2.py:185][0m |          -0.0090 |         155.0584 |          10.4850 |
[32m[20221213 21:24:30 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:24:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 769.00
[32m[20221213 21:24:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.00
[32m[20221213 21:24:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.00
[32m[20221213 21:24:30 @agent_ppo2.py:143][0m Total time:      28.92 min
[32m[20221213 21:24:30 @agent_ppo2.py:145][0m 2824192 total steps have happened
[32m[20221213 21:24:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1379 --------------------------#
[32m[20221213 21:24:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:30 @agent_ppo2.py:185][0m |          -0.0005 |         161.0420 |          10.6459 |
[32m[20221213 21:24:30 @agent_ppo2.py:185][0m |          -0.0001 |         162.2216 |          10.6353 |
[32m[20221213 21:24:30 @agent_ppo2.py:185][0m |          -0.0019 |         160.6039 |          10.6161 |
[32m[20221213 21:24:31 @agent_ppo2.py:185][0m |          -0.0001 |         164.0079 |          10.6777 |
[32m[20221213 21:24:31 @agent_ppo2.py:185][0m |          -0.0082 |         159.5853 |          10.6723 |
[32m[20221213 21:24:31 @agent_ppo2.py:185][0m |          -0.0082 |         159.3907 |          10.6897 |
[32m[20221213 21:24:31 @agent_ppo2.py:185][0m |          -0.0050 |         161.2315 |          10.6699 |
[32m[20221213 21:24:31 @agent_ppo2.py:185][0m |          -0.0095 |         158.8938 |          10.6770 |
[32m[20221213 21:24:31 @agent_ppo2.py:185][0m |           0.0153 |         189.5356 |          10.6811 |
[32m[20221213 21:24:31 @agent_ppo2.py:185][0m |          -0.0100 |         158.8459 |          10.7206 |
[32m[20221213 21:24:31 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:24:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.00
[32m[20221213 21:24:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.00
[32m[20221213 21:24:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.00
[32m[20221213 21:24:31 @agent_ppo2.py:143][0m Total time:      28.94 min
[32m[20221213 21:24:31 @agent_ppo2.py:145][0m 2826240 total steps have happened
[32m[20221213 21:24:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1380 --------------------------#
[32m[20221213 21:24:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:31 @agent_ppo2.py:185][0m |           0.0049 |         160.1169 |          10.5071 |
[32m[20221213 21:24:32 @agent_ppo2.py:185][0m |          -0.0038 |         157.9043 |          10.5229 |
[32m[20221213 21:24:32 @agent_ppo2.py:185][0m |          -0.0056 |         157.4818 |          10.5255 |
[32m[20221213 21:24:32 @agent_ppo2.py:185][0m |          -0.0064 |         156.9857 |          10.5285 |
[32m[20221213 21:24:32 @agent_ppo2.py:185][0m |          -0.0055 |         156.9141 |          10.5004 |
[32m[20221213 21:24:32 @agent_ppo2.py:185][0m |          -0.0070 |         156.4468 |          10.5039 |
[32m[20221213 21:24:32 @agent_ppo2.py:185][0m |           0.0073 |         175.8852 |          10.5412 |
[32m[20221213 21:24:32 @agent_ppo2.py:185][0m |           0.0111 |         173.2486 |          10.4926 |
[32m[20221213 21:24:32 @agent_ppo2.py:185][0m |          -0.0036 |         156.3949 |          10.5488 |
[32m[20221213 21:24:32 @agent_ppo2.py:185][0m |           0.0005 |         158.7494 |          10.5084 |
[32m[20221213 21:24:32 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:24:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.80
[32m[20221213 21:24:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.00
[32m[20221213 21:24:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.00
[32m[20221213 21:24:32 @agent_ppo2.py:143][0m Total time:      28.96 min
[32m[20221213 21:24:32 @agent_ppo2.py:145][0m 2828288 total steps have happened
[32m[20221213 21:24:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1381 --------------------------#
[32m[20221213 21:24:33 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:24:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:33 @agent_ppo2.py:185][0m |          -0.0001 |         160.0516 |          10.6580 |
[32m[20221213 21:24:33 @agent_ppo2.py:185][0m |          -0.0031 |         159.8346 |          10.6650 |
[32m[20221213 21:24:33 @agent_ppo2.py:185][0m |          -0.0033 |         159.4686 |          10.6641 |
[32m[20221213 21:24:33 @agent_ppo2.py:185][0m |           0.0021 |         171.4242 |          10.7074 |
[32m[20221213 21:24:33 @agent_ppo2.py:185][0m |          -0.0019 |         160.2997 |          10.6956 |
[32m[20221213 21:24:33 @agent_ppo2.py:185][0m |          -0.0050 |         158.7851 |          10.6804 |
[32m[20221213 21:24:33 @agent_ppo2.py:185][0m |          -0.0081 |         158.4338 |          10.6889 |
[32m[20221213 21:24:33 @agent_ppo2.py:185][0m |          -0.0069 |         158.2464 |          10.6991 |
[32m[20221213 21:24:33 @agent_ppo2.py:185][0m |          -0.0058 |         158.2308 |          10.6680 |
[32m[20221213 21:24:33 @agent_ppo2.py:185][0m |          -0.0068 |         158.0504 |          10.6809 |
[32m[20221213 21:24:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:24:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.00
[32m[20221213 21:24:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.00
[32m[20221213 21:24:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 21:24:34 @agent_ppo2.py:143][0m Total time:      28.98 min
[32m[20221213 21:24:34 @agent_ppo2.py:145][0m 2830336 total steps have happened
[32m[20221213 21:24:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1382 --------------------------#
[32m[20221213 21:24:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:34 @agent_ppo2.py:185][0m |          -0.0017 |         162.3629 |          10.8357 |
[32m[20221213 21:24:34 @agent_ppo2.py:185][0m |          -0.0036 |         161.4625 |          10.8484 |
[32m[20221213 21:24:34 @agent_ppo2.py:185][0m |           0.0036 |         166.4473 |          10.9147 |
[32m[20221213 21:24:34 @agent_ppo2.py:185][0m |          -0.0038 |         161.4406 |          10.9435 |
[32m[20221213 21:24:34 @agent_ppo2.py:185][0m |          -0.0058 |         160.3553 |          10.9796 |
[32m[20221213 21:24:34 @agent_ppo2.py:185][0m |          -0.0057 |         160.1640 |          10.9747 |
[32m[20221213 21:24:34 @agent_ppo2.py:185][0m |          -0.0020 |         163.6599 |          11.0344 |
[32m[20221213 21:24:35 @agent_ppo2.py:185][0m |          -0.0047 |         159.9934 |          11.0555 |
[32m[20221213 21:24:35 @agent_ppo2.py:185][0m |          -0.0104 |         159.6475 |          11.0359 |
[32m[20221213 21:24:35 @agent_ppo2.py:185][0m |          -0.0085 |         159.3373 |          11.0732 |
[32m[20221213 21:24:35 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:24:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.00
[32m[20221213 21:24:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 773.00
[32m[20221213 21:24:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.00
[32m[20221213 21:24:35 @agent_ppo2.py:143][0m Total time:      29.00 min
[32m[20221213 21:24:35 @agent_ppo2.py:145][0m 2832384 total steps have happened
[32m[20221213 21:24:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1383 --------------------------#
[32m[20221213 21:24:35 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:24:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:35 @agent_ppo2.py:185][0m |           0.0025 |         162.3099 |          10.7585 |
[32m[20221213 21:24:35 @agent_ppo2.py:185][0m |          -0.0026 |         159.9280 |          10.8150 |
[32m[20221213 21:24:35 @agent_ppo2.py:185][0m |          -0.0021 |         159.9112 |          10.8097 |
[32m[20221213 21:24:35 @agent_ppo2.py:185][0m |          -0.0032 |         159.6274 |          10.7928 |
[32m[20221213 21:24:36 @agent_ppo2.py:185][0m |          -0.0052 |         159.2668 |          10.8360 |
[32m[20221213 21:24:36 @agent_ppo2.py:185][0m |          -0.0044 |         159.2959 |          10.8160 |
[32m[20221213 21:24:36 @agent_ppo2.py:185][0m |          -0.0046 |         158.7898 |          10.8518 |
[32m[20221213 21:24:36 @agent_ppo2.py:185][0m |           0.0010 |         164.4927 |          10.8738 |
[32m[20221213 21:24:36 @agent_ppo2.py:185][0m |          -0.0054 |         158.5871 |          10.8919 |
[32m[20221213 21:24:36 @agent_ppo2.py:185][0m |          -0.0053 |         158.1162 |          10.8550 |
[32m[20221213 21:24:36 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:24:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 762.40
[32m[20221213 21:24:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.00
[32m[20221213 21:24:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.00
[32m[20221213 21:24:36 @agent_ppo2.py:143][0m Total time:      29.02 min
[32m[20221213 21:24:36 @agent_ppo2.py:145][0m 2834432 total steps have happened
[32m[20221213 21:24:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1384 --------------------------#
[32m[20221213 21:24:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:36 @agent_ppo2.py:185][0m |          -0.0031 |         165.5156 |          11.0335 |
[32m[20221213 21:24:36 @agent_ppo2.py:185][0m |          -0.0041 |         162.6079 |          11.0278 |
[32m[20221213 21:24:37 @agent_ppo2.py:185][0m |          -0.0062 |         160.5245 |          11.0351 |
[32m[20221213 21:24:37 @agent_ppo2.py:185][0m |          -0.0054 |         159.4053 |          11.0157 |
[32m[20221213 21:24:37 @agent_ppo2.py:185][0m |          -0.0017 |         160.0698 |          11.0344 |
[32m[20221213 21:24:37 @agent_ppo2.py:185][0m |          -0.0071 |         158.2847 |          11.0271 |
[32m[20221213 21:24:37 @agent_ppo2.py:185][0m |          -0.0072 |         157.5395 |          11.0172 |
[32m[20221213 21:24:37 @agent_ppo2.py:185][0m |          -0.0098 |         156.9575 |          10.9839 |
[32m[20221213 21:24:37 @agent_ppo2.py:185][0m |          -0.0022 |         165.7873 |          10.9829 |
[32m[20221213 21:24:37 @agent_ppo2.py:185][0m |          -0.0021 |         165.5768 |          10.9611 |
[32m[20221213 21:24:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:24:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 704.80
[32m[20221213 21:24:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.00
[32m[20221213 21:24:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.00
[32m[20221213 21:24:37 @agent_ppo2.py:143][0m Total time:      29.04 min
[32m[20221213 21:24:37 @agent_ppo2.py:145][0m 2836480 total steps have happened
[32m[20221213 21:24:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1385 --------------------------#
[32m[20221213 21:24:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:38 @agent_ppo2.py:185][0m |           0.0108 |         173.4203 |          10.9903 |
[32m[20221213 21:24:38 @agent_ppo2.py:185][0m |          -0.0026 |         164.4202 |          11.0093 |
[32m[20221213 21:24:38 @agent_ppo2.py:185][0m |          -0.0038 |         164.1877 |          10.9658 |
[32m[20221213 21:24:38 @agent_ppo2.py:185][0m |          -0.0050 |         163.2076 |          10.9579 |
[32m[20221213 21:24:38 @agent_ppo2.py:185][0m |          -0.0077 |         163.1185 |          10.9702 |
[32m[20221213 21:24:38 @agent_ppo2.py:185][0m |          -0.0083 |         162.6102 |          10.8788 |
[32m[20221213 21:24:38 @agent_ppo2.py:185][0m |          -0.0055 |         162.4509 |          10.9015 |
[32m[20221213 21:24:38 @agent_ppo2.py:185][0m |           0.0008 |         175.2103 |          10.8810 |
[32m[20221213 21:24:38 @agent_ppo2.py:185][0m |          -0.0056 |         162.1595 |          10.8646 |
[32m[20221213 21:24:38 @agent_ppo2.py:185][0m |          -0.0099 |         161.9583 |          10.8287 |
[32m[20221213 21:24:38 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:24:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 739.60
[32m[20221213 21:24:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.00
[32m[20221213 21:24:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.00
[32m[20221213 21:24:39 @agent_ppo2.py:143][0m Total time:      29.06 min
[32m[20221213 21:24:39 @agent_ppo2.py:145][0m 2838528 total steps have happened
[32m[20221213 21:24:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1386 --------------------------#
[32m[20221213 21:24:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:39 @agent_ppo2.py:185][0m |           0.0050 |         160.6563 |          10.6096 |
[32m[20221213 21:24:39 @agent_ppo2.py:185][0m |          -0.0034 |         157.1115 |          10.6403 |
[32m[20221213 21:24:39 @agent_ppo2.py:185][0m |          -0.0029 |         156.0187 |          10.6404 |
[32m[20221213 21:24:39 @agent_ppo2.py:185][0m |          -0.0036 |         155.3023 |          10.6428 |
[32m[20221213 21:24:39 @agent_ppo2.py:185][0m |          -0.0065 |         154.6487 |          10.6709 |
[32m[20221213 21:24:39 @agent_ppo2.py:185][0m |          -0.0085 |         154.0903 |          10.6228 |
[32m[20221213 21:24:39 @agent_ppo2.py:185][0m |          -0.0078 |         153.9370 |          10.6614 |
[32m[20221213 21:24:39 @agent_ppo2.py:185][0m |          -0.0070 |         153.7049 |          10.6609 |
[32m[20221213 21:24:40 @agent_ppo2.py:185][0m |          -0.0053 |         156.0352 |          10.6708 |
[32m[20221213 21:24:40 @agent_ppo2.py:185][0m |          -0.0092 |         153.0522 |          10.6541 |
[32m[20221213 21:24:40 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:24:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.80
[32m[20221213 21:24:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 772.00
[32m[20221213 21:24:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.00
[32m[20221213 21:24:40 @agent_ppo2.py:143][0m Total time:      29.09 min
[32m[20221213 21:24:40 @agent_ppo2.py:145][0m 2840576 total steps have happened
[32m[20221213 21:24:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1387 --------------------------#
[32m[20221213 21:24:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:40 @agent_ppo2.py:185][0m |          -0.0009 |         161.1751 |          10.8529 |
[32m[20221213 21:24:40 @agent_ppo2.py:185][0m |          -0.0026 |         158.2872 |          10.9162 |
[32m[20221213 21:24:40 @agent_ppo2.py:185][0m |          -0.0051 |         156.6806 |          10.9370 |
[32m[20221213 21:24:40 @agent_ppo2.py:185][0m |          -0.0009 |         157.0807 |          10.9474 |
[32m[20221213 21:24:40 @agent_ppo2.py:185][0m |          -0.0054 |         155.1125 |          10.9591 |
[32m[20221213 21:24:41 @agent_ppo2.py:185][0m |          -0.0009 |         155.2915 |          11.0052 |
[32m[20221213 21:24:41 @agent_ppo2.py:185][0m |          -0.0067 |         154.3365 |          11.0439 |
[32m[20221213 21:24:41 @agent_ppo2.py:185][0m |           0.0010 |         164.0260 |          11.0581 |
[32m[20221213 21:24:41 @agent_ppo2.py:185][0m |          -0.0083 |         153.7068 |          11.1215 |
[32m[20221213 21:24:41 @agent_ppo2.py:185][0m |           0.0017 |         157.7975 |          11.1055 |
[32m[20221213 21:24:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:24:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 772.60
[32m[20221213 21:24:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.00
[32m[20221213 21:24:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 21:24:41 @agent_ppo2.py:143][0m Total time:      29.11 min
[32m[20221213 21:24:41 @agent_ppo2.py:145][0m 2842624 total steps have happened
[32m[20221213 21:24:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1388 --------------------------#
[32m[20221213 21:24:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:24:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:41 @agent_ppo2.py:185][0m |           0.0001 |         162.4152 |          11.1009 |
[32m[20221213 21:24:41 @agent_ppo2.py:185][0m |          -0.0044 |         161.6042 |          11.0916 |
[32m[20221213 21:24:42 @agent_ppo2.py:185][0m |          -0.0051 |         161.1522 |          11.0781 |
[32m[20221213 21:24:42 @agent_ppo2.py:185][0m |          -0.0070 |         160.9083 |          11.0795 |
[32m[20221213 21:24:42 @agent_ppo2.py:185][0m |          -0.0088 |         160.5147 |          11.0828 |
[32m[20221213 21:24:42 @agent_ppo2.py:185][0m |          -0.0067 |         160.3506 |          11.0990 |
[32m[20221213 21:24:42 @agent_ppo2.py:185][0m |          -0.0083 |         160.1846 |          11.1248 |
[32m[20221213 21:24:42 @agent_ppo2.py:185][0m |          -0.0067 |         160.2206 |          11.1081 |
[32m[20221213 21:24:42 @agent_ppo2.py:185][0m |          -0.0054 |         159.9732 |          11.1526 |
[32m[20221213 21:24:42 @agent_ppo2.py:185][0m |           0.0024 |         182.3234 |          11.1381 |
[32m[20221213 21:24:42 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:24:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.00
[32m[20221213 21:24:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.00
[32m[20221213 21:24:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 784.00
[32m[20221213 21:24:42 @agent_ppo2.py:143][0m Total time:      29.13 min
[32m[20221213 21:24:42 @agent_ppo2.py:145][0m 2844672 total steps have happened
[32m[20221213 21:24:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1389 --------------------------#
[32m[20221213 21:24:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:43 @agent_ppo2.py:185][0m |           0.0022 |         162.3340 |          10.8791 |
[32m[20221213 21:24:43 @agent_ppo2.py:185][0m |          -0.0056 |         161.1410 |          10.8790 |
[32m[20221213 21:24:43 @agent_ppo2.py:185][0m |           0.0029 |         170.9779 |          10.8741 |
[32m[20221213 21:24:43 @agent_ppo2.py:185][0m |          -0.0056 |         160.2982 |          10.8944 |
[32m[20221213 21:24:43 @agent_ppo2.py:185][0m |          -0.0066 |         160.0012 |          10.8294 |
[32m[20221213 21:24:43 @agent_ppo2.py:185][0m |          -0.0081 |         159.7517 |          10.8146 |
[32m[20221213 21:24:43 @agent_ppo2.py:185][0m |          -0.0072 |         159.6113 |          10.8114 |
[32m[20221213 21:24:43 @agent_ppo2.py:185][0m |           0.0005 |         175.8533 |          10.8399 |
[32m[20221213 21:24:43 @agent_ppo2.py:185][0m |          -0.0082 |         159.3920 |          10.8229 |
[32m[20221213 21:24:43 @agent_ppo2.py:185][0m |           0.0085 |         179.2216 |          10.8176 |
[32m[20221213 21:24:43 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:24:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.40
[32m[20221213 21:24:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.00
[32m[20221213 21:24:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.00
[32m[20221213 21:24:43 @agent_ppo2.py:143][0m Total time:      29.15 min
[32m[20221213 21:24:43 @agent_ppo2.py:145][0m 2846720 total steps have happened
[32m[20221213 21:24:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1390 --------------------------#
[32m[20221213 21:24:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:44 @agent_ppo2.py:185][0m |           0.0019 |         161.4096 |          10.7828 |
[32m[20221213 21:24:44 @agent_ppo2.py:185][0m |           0.0035 |         166.1997 |          10.6944 |
[32m[20221213 21:24:44 @agent_ppo2.py:185][0m |           0.0026 |         162.0435 |          10.6252 |
[32m[20221213 21:24:44 @agent_ppo2.py:185][0m |          -0.0069 |         158.2823 |          10.6051 |
[32m[20221213 21:24:44 @agent_ppo2.py:185][0m |          -0.0052 |         158.0226 |          10.5892 |
[32m[20221213 21:24:44 @agent_ppo2.py:185][0m |          -0.0066 |         158.0341 |          10.5531 |
[32m[20221213 21:24:44 @agent_ppo2.py:185][0m |          -0.0097 |         157.6074 |          10.5726 |
[32m[20221213 21:24:44 @agent_ppo2.py:185][0m |          -0.0101 |         157.5524 |          10.5118 |
[32m[20221213 21:24:44 @agent_ppo2.py:185][0m |          -0.0108 |         157.5146 |          10.5118 |
[32m[20221213 21:24:45 @agent_ppo2.py:185][0m |          -0.0102 |         157.2602 |          10.4857 |
[32m[20221213 21:24:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:24:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 717.80
[32m[20221213 21:24:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.00
[32m[20221213 21:24:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 763.00
[32m[20221213 21:24:45 @agent_ppo2.py:143][0m Total time:      29.17 min
[32m[20221213 21:24:45 @agent_ppo2.py:145][0m 2848768 total steps have happened
[32m[20221213 21:24:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1391 --------------------------#
[32m[20221213 21:24:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:24:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:45 @agent_ppo2.py:185][0m |           0.0013 |         163.2367 |          10.3926 |
[32m[20221213 21:24:45 @agent_ppo2.py:185][0m |          -0.0045 |         160.2266 |          10.3865 |
[32m[20221213 21:24:45 @agent_ppo2.py:185][0m |          -0.0062 |         159.4755 |          10.3738 |
[32m[20221213 21:24:45 @agent_ppo2.py:185][0m |          -0.0070 |         159.1231 |          10.3741 |
[32m[20221213 21:24:45 @agent_ppo2.py:185][0m |          -0.0034 |         160.2268 |          10.3586 |
[32m[20221213 21:24:45 @agent_ppo2.py:185][0m |          -0.0092 |         158.8398 |          10.3231 |
[32m[20221213 21:24:46 @agent_ppo2.py:185][0m |          -0.0089 |         158.7086 |          10.3510 |
[32m[20221213 21:24:46 @agent_ppo2.py:185][0m |           0.0030 |         173.1848 |          10.3306 |
[32m[20221213 21:24:46 @agent_ppo2.py:185][0m |          -0.0023 |         162.7000 |          10.3197 |
[32m[20221213 21:24:46 @agent_ppo2.py:185][0m |          -0.0094 |         158.6792 |          10.3063 |
[32m[20221213 21:24:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:24:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.00
[32m[20221213 21:24:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:24:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.00
[32m[20221213 21:24:46 @agent_ppo2.py:143][0m Total time:      29.19 min
[32m[20221213 21:24:46 @agent_ppo2.py:145][0m 2850816 total steps have happened
[32m[20221213 21:24:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1392 --------------------------#
[32m[20221213 21:24:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:46 @agent_ppo2.py:185][0m |          -0.0026 |         158.7390 |          10.0983 |
[32m[20221213 21:24:46 @agent_ppo2.py:185][0m |          -0.0061 |         157.9829 |          10.0823 |
[32m[20221213 21:24:46 @agent_ppo2.py:185][0m |          -0.0005 |         162.3350 |          10.1318 |
[32m[20221213 21:24:46 @agent_ppo2.py:185][0m |          -0.0070 |         157.2126 |          10.0319 |
[32m[20221213 21:24:47 @agent_ppo2.py:185][0m |          -0.0063 |         156.7032 |          10.1063 |
[32m[20221213 21:24:47 @agent_ppo2.py:185][0m |          -0.0084 |         156.4136 |          10.0414 |
[32m[20221213 21:24:47 @agent_ppo2.py:185][0m |          -0.0086 |         156.0919 |          10.0539 |
[32m[20221213 21:24:47 @agent_ppo2.py:185][0m |          -0.0082 |         155.8008 |          10.0587 |
[32m[20221213 21:24:47 @agent_ppo2.py:185][0m |          -0.0102 |         155.5288 |          10.0523 |
[32m[20221213 21:24:47 @agent_ppo2.py:185][0m |          -0.0106 |         155.1471 |          10.0605 |
[32m[20221213 21:24:47 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:24:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.60
[32m[20221213 21:24:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:24:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 21:24:47 @agent_ppo2.py:143][0m Total time:      29.21 min
[32m[20221213 21:24:47 @agent_ppo2.py:145][0m 2852864 total steps have happened
[32m[20221213 21:24:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1393 --------------------------#
[32m[20221213 21:24:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:47 @agent_ppo2.py:185][0m |          -0.0019 |         164.5915 |          10.2171 |
[32m[20221213 21:24:48 @agent_ppo2.py:185][0m |          -0.0056 |         161.4946 |          10.2205 |
[32m[20221213 21:24:48 @agent_ppo2.py:185][0m |          -0.0043 |         159.9602 |          10.2030 |
[32m[20221213 21:24:48 @agent_ppo2.py:185][0m |          -0.0077 |         159.0694 |          10.2088 |
[32m[20221213 21:24:48 @agent_ppo2.py:185][0m |          -0.0053 |         158.9621 |          10.2144 |
[32m[20221213 21:24:48 @agent_ppo2.py:185][0m |          -0.0095 |         157.7772 |          10.1888 |
[32m[20221213 21:24:48 @agent_ppo2.py:185][0m |          -0.0102 |         157.4257 |          10.2010 |
[32m[20221213 21:24:48 @agent_ppo2.py:185][0m |          -0.0102 |         157.0074 |          10.1886 |
[32m[20221213 21:24:48 @agent_ppo2.py:185][0m |          -0.0113 |         156.7085 |          10.1825 |
[32m[20221213 21:24:48 @agent_ppo2.py:185][0m |          -0.0043 |         158.9514 |          10.2592 |
[32m[20221213 21:24:48 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:24:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.40
[32m[20221213 21:24:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:24:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.00
[32m[20221213 21:24:48 @agent_ppo2.py:143][0m Total time:      29.23 min
[32m[20221213 21:24:48 @agent_ppo2.py:145][0m 2854912 total steps have happened
[32m[20221213 21:24:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1394 --------------------------#
[32m[20221213 21:24:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:49 @agent_ppo2.py:185][0m |           0.0053 |         165.0664 |          10.0826 |
[32m[20221213 21:24:49 @agent_ppo2.py:185][0m |          -0.0022 |         162.7149 |          10.0098 |
[32m[20221213 21:24:49 @agent_ppo2.py:185][0m |          -0.0029 |         162.0087 |           9.9717 |
[32m[20221213 21:24:49 @agent_ppo2.py:185][0m |           0.0015 |         166.1693 |           9.9556 |
[32m[20221213 21:24:49 @agent_ppo2.py:185][0m |          -0.0035 |         161.0638 |           9.8767 |
[32m[20221213 21:24:49 @agent_ppo2.py:185][0m |          -0.0050 |         161.0738 |           9.8916 |
[32m[20221213 21:24:49 @agent_ppo2.py:185][0m |          -0.0071 |         160.6946 |           9.7852 |
[32m[20221213 21:24:49 @agent_ppo2.py:185][0m |          -0.0077 |         160.4941 |           9.7775 |
[32m[20221213 21:24:49 @agent_ppo2.py:185][0m |          -0.0074 |         160.3534 |           9.7543 |
[32m[20221213 21:24:49 @agent_ppo2.py:185][0m |          -0.0042 |         161.8524 |           9.7340 |
[32m[20221213 21:24:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:24:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.20
[32m[20221213 21:24:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.00
[32m[20221213 21:24:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 784.00
[32m[20221213 21:24:50 @agent_ppo2.py:143][0m Total time:      29.25 min
[32m[20221213 21:24:50 @agent_ppo2.py:145][0m 2856960 total steps have happened
[32m[20221213 21:24:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1395 --------------------------#
[32m[20221213 21:24:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:50 @agent_ppo2.py:185][0m |          -0.0002 |         161.4629 |           9.5348 |
[32m[20221213 21:24:50 @agent_ppo2.py:185][0m |          -0.0039 |         160.9754 |           9.5464 |
[32m[20221213 21:24:50 @agent_ppo2.py:185][0m |           0.0015 |         164.0756 |           9.5304 |
[32m[20221213 21:24:50 @agent_ppo2.py:185][0m |          -0.0065 |         160.4930 |           9.5184 |
[32m[20221213 21:24:50 @agent_ppo2.py:185][0m |          -0.0068 |         160.2335 |           9.5196 |
[32m[20221213 21:24:50 @agent_ppo2.py:185][0m |          -0.0023 |         161.0464 |           9.4977 |
[32m[20221213 21:24:50 @agent_ppo2.py:185][0m |          -0.0060 |         159.9100 |           9.5095 |
[32m[20221213 21:24:50 @agent_ppo2.py:185][0m |          -0.0044 |         160.0048 |           9.4713 |
[32m[20221213 21:24:51 @agent_ppo2.py:185][0m |           0.0061 |         177.8563 |           9.4769 |
[32m[20221213 21:24:51 @agent_ppo2.py:185][0m |          -0.0010 |         164.7070 |           9.4733 |
[32m[20221213 21:24:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:24:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.80
[32m[20221213 21:24:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.00
[32m[20221213 21:24:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 653.00
[32m[20221213 21:24:51 @agent_ppo2.py:143][0m Total time:      29.27 min
[32m[20221213 21:24:51 @agent_ppo2.py:145][0m 2859008 total steps have happened
[32m[20221213 21:24:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1396 --------------------------#
[32m[20221213 21:24:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:51 @agent_ppo2.py:185][0m |           0.0040 |         165.7284 |           9.8308 |
[32m[20221213 21:24:51 @agent_ppo2.py:185][0m |          -0.0031 |         160.9622 |           9.8413 |
[32m[20221213 21:24:51 @agent_ppo2.py:185][0m |          -0.0040 |         161.0530 |           9.8276 |
[32m[20221213 21:24:51 @agent_ppo2.py:185][0m |          -0.0048 |         159.5668 |           9.8089 |
[32m[20221213 21:24:51 @agent_ppo2.py:185][0m |          -0.0043 |         159.1790 |           9.8744 |
[32m[20221213 21:24:52 @agent_ppo2.py:185][0m |          -0.0060 |         158.9403 |           9.8047 |
[32m[20221213 21:24:52 @agent_ppo2.py:185][0m |          -0.0069 |         158.6330 |           9.8157 |
[32m[20221213 21:24:52 @agent_ppo2.py:185][0m |          -0.0073 |         158.4504 |           9.8315 |
[32m[20221213 21:24:52 @agent_ppo2.py:185][0m |          -0.0070 |         158.1648 |           9.8585 |
[32m[20221213 21:24:52 @agent_ppo2.py:185][0m |          -0.0082 |         158.3656 |           9.8518 |
[32m[20221213 21:24:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:24:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 763.60
[32m[20221213 21:24:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.00
[32m[20221213 21:24:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 21:24:52 @agent_ppo2.py:143][0m Total time:      29.29 min
[32m[20221213 21:24:52 @agent_ppo2.py:145][0m 2861056 total steps have happened
[32m[20221213 21:24:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1397 --------------------------#
[32m[20221213 21:24:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:52 @agent_ppo2.py:185][0m |          -0.0027 |         165.6364 |           9.2136 |
[32m[20221213 21:24:52 @agent_ppo2.py:185][0m |          -0.0007 |         165.5196 |           9.1714 |
[32m[20221213 21:24:52 @agent_ppo2.py:185][0m |          -0.0057 |         162.9543 |           9.2153 |
[32m[20221213 21:24:53 @agent_ppo2.py:185][0m |          -0.0083 |         161.8742 |           9.1639 |
[32m[20221213 21:24:53 @agent_ppo2.py:185][0m |           0.0035 |         180.1524 |           9.1697 |
[32m[20221213 21:24:53 @agent_ppo2.py:185][0m |          -0.0036 |         166.8878 |           9.1918 |
[32m[20221213 21:24:53 @agent_ppo2.py:185][0m |          -0.0097 |         160.3983 |           9.1870 |
[32m[20221213 21:24:53 @agent_ppo2.py:185][0m |          -0.0097 |         160.2343 |           9.1794 |
[32m[20221213 21:24:53 @agent_ppo2.py:185][0m |          -0.0101 |         160.0228 |           9.1661 |
[32m[20221213 21:24:53 @agent_ppo2.py:185][0m |          -0.0123 |         159.4104 |           9.1860 |
[32m[20221213 21:24:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:24:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 779.40
[32m[20221213 21:24:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.00
[32m[20221213 21:24:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.00
[32m[20221213 21:24:53 @agent_ppo2.py:143][0m Total time:      29.31 min
[32m[20221213 21:24:53 @agent_ppo2.py:145][0m 2863104 total steps have happened
[32m[20221213 21:24:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1398 --------------------------#
[32m[20221213 21:24:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:54 @agent_ppo2.py:185][0m |           0.0028 |         167.3535 |           9.3558 |
[32m[20221213 21:24:54 @agent_ppo2.py:185][0m |           0.0007 |         166.0921 |           9.4294 |
[32m[20221213 21:24:54 @agent_ppo2.py:185][0m |           0.0044 |         171.5771 |           9.4167 |
[32m[20221213 21:24:54 @agent_ppo2.py:185][0m |          -0.0056 |         165.5841 |           9.4163 |
[32m[20221213 21:24:54 @agent_ppo2.py:185][0m |          -0.0019 |         165.2209 |           9.4123 |
[32m[20221213 21:24:54 @agent_ppo2.py:185][0m |           0.0040 |         170.3385 |           9.4954 |
[32m[20221213 21:24:54 @agent_ppo2.py:185][0m |          -0.0054 |         164.4093 |           9.4642 |
[32m[20221213 21:24:54 @agent_ppo2.py:185][0m |          -0.0068 |         164.1381 |           9.4440 |
[32m[20221213 21:24:54 @agent_ppo2.py:185][0m |          -0.0059 |         164.0307 |           9.4964 |
[32m[20221213 21:24:54 @agent_ppo2.py:185][0m |          -0.0047 |         163.8314 |           9.4274 |
[32m[20221213 21:24:54 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:24:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.60
[32m[20221213 21:24:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 21:24:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.00
[32m[20221213 21:24:54 @agent_ppo2.py:143][0m Total time:      29.33 min
[32m[20221213 21:24:54 @agent_ppo2.py:145][0m 2865152 total steps have happened
[32m[20221213 21:24:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1399 --------------------------#
[32m[20221213 21:24:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:55 @agent_ppo2.py:185][0m |           0.0008 |         165.2010 |           9.7254 |
[32m[20221213 21:24:55 @agent_ppo2.py:185][0m |          -0.0056 |         163.9942 |           9.7162 |
[32m[20221213 21:24:55 @agent_ppo2.py:185][0m |          -0.0022 |         163.4484 |           9.7071 |
[32m[20221213 21:24:55 @agent_ppo2.py:185][0m |          -0.0032 |         163.5524 |           9.7056 |
[32m[20221213 21:24:55 @agent_ppo2.py:185][0m |          -0.0048 |         162.8191 |           9.7282 |
[32m[20221213 21:24:55 @agent_ppo2.py:185][0m |          -0.0013 |         163.9296 |           9.7013 |
[32m[20221213 21:24:55 @agent_ppo2.py:185][0m |          -0.0051 |         162.5096 |           9.7029 |
[32m[20221213 21:24:55 @agent_ppo2.py:185][0m |          -0.0055 |         162.2707 |           9.7168 |
[32m[20221213 21:24:55 @agent_ppo2.py:185][0m |          -0.0067 |         162.3943 |           9.6901 |
[32m[20221213 21:24:56 @agent_ppo2.py:185][0m |          -0.0072 |         162.1161 |           9.7225 |
[32m[20221213 21:24:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:24:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.20
[32m[20221213 21:24:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:24:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.00
[32m[20221213 21:24:56 @agent_ppo2.py:143][0m Total time:      29.35 min
[32m[20221213 21:24:56 @agent_ppo2.py:145][0m 2867200 total steps have happened
[32m[20221213 21:24:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1400 --------------------------#
[32m[20221213 21:24:56 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:24:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:56 @agent_ppo2.py:185][0m |           0.0005 |         165.5120 |           9.3043 |
[32m[20221213 21:24:56 @agent_ppo2.py:185][0m |          -0.0043 |         164.5459 |           9.3750 |
[32m[20221213 21:24:56 @agent_ppo2.py:185][0m |          -0.0053 |         163.9929 |           9.3909 |
[32m[20221213 21:24:56 @agent_ppo2.py:185][0m |          -0.0071 |         163.6650 |           9.4087 |
[32m[20221213 21:24:56 @agent_ppo2.py:185][0m |          -0.0071 |         163.3920 |           9.4140 |
[32m[20221213 21:24:56 @agent_ppo2.py:185][0m |          -0.0070 |         163.1784 |           9.4161 |
[32m[20221213 21:24:57 @agent_ppo2.py:185][0m |           0.0021 |         175.0879 |           9.4463 |
[32m[20221213 21:24:57 @agent_ppo2.py:185][0m |           0.0021 |         173.2837 |           9.5025 |
[32m[20221213 21:24:57 @agent_ppo2.py:185][0m |          -0.0085 |         162.7704 |           9.4857 |
[32m[20221213 21:24:57 @agent_ppo2.py:185][0m |          -0.0086 |         162.6437 |           9.4616 |
[32m[20221213 21:24:57 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:24:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 753.60
[32m[20221213 21:24:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:24:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.00
[32m[20221213 21:24:57 @agent_ppo2.py:143][0m Total time:      29.37 min
[32m[20221213 21:24:57 @agent_ppo2.py:145][0m 2869248 total steps have happened
[32m[20221213 21:24:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1401 --------------------------#
[32m[20221213 21:24:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:24:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:57 @agent_ppo2.py:185][0m |           0.0046 |         166.9478 |           9.7392 |
[32m[20221213 21:24:57 @agent_ppo2.py:185][0m |          -0.0035 |         163.1951 |           9.7380 |
[32m[20221213 21:24:57 @agent_ppo2.py:185][0m |          -0.0040 |         162.4990 |           9.6995 |
[32m[20221213 21:24:58 @agent_ppo2.py:185][0m |          -0.0058 |         161.9230 |           9.7525 |
[32m[20221213 21:24:58 @agent_ppo2.py:185][0m |          -0.0058 |         161.7136 |           9.7477 |
[32m[20221213 21:24:58 @agent_ppo2.py:185][0m |          -0.0032 |         162.6094 |           9.7772 |
[32m[20221213 21:24:58 @agent_ppo2.py:185][0m |           0.0077 |         179.2970 |           9.7576 |
[32m[20221213 21:24:58 @agent_ppo2.py:185][0m |           0.0010 |         163.3038 |           9.8407 |
[32m[20221213 21:24:58 @agent_ppo2.py:185][0m |          -0.0068 |         160.8799 |           9.7658 |
[32m[20221213 21:24:58 @agent_ppo2.py:185][0m |          -0.0075 |         160.8230 |           9.8196 |
[32m[20221213 21:24:58 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:24:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 755.60
[32m[20221213 21:24:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.00
[32m[20221213 21:24:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.00
[32m[20221213 21:24:58 @agent_ppo2.py:143][0m Total time:      29.39 min
[32m[20221213 21:24:58 @agent_ppo2.py:145][0m 2871296 total steps have happened
[32m[20221213 21:24:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1402 --------------------------#
[32m[20221213 21:24:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:24:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:24:59 @agent_ppo2.py:185][0m |          -0.0001 |         166.0779 |           9.7049 |
[32m[20221213 21:24:59 @agent_ppo2.py:185][0m |          -0.0016 |         166.4918 |           9.7194 |
[32m[20221213 21:24:59 @agent_ppo2.py:185][0m |          -0.0021 |         164.7266 |           9.7173 |
[32m[20221213 21:24:59 @agent_ppo2.py:185][0m |          -0.0011 |         165.1592 |           9.6731 |
[32m[20221213 21:24:59 @agent_ppo2.py:185][0m |           0.0025 |         172.4924 |           9.7406 |
[32m[20221213 21:24:59 @agent_ppo2.py:185][0m |          -0.0040 |         164.9516 |           9.6942 |
[32m[20221213 21:24:59 @agent_ppo2.py:185][0m |          -0.0045 |         163.9291 |           9.7335 |
[32m[20221213 21:24:59 @agent_ppo2.py:185][0m |          -0.0060 |         163.8602 |           9.7097 |
[32m[20221213 21:24:59 @agent_ppo2.py:185][0m |          -0.0100 |         163.8040 |           9.6877 |
[32m[20221213 21:24:59 @agent_ppo2.py:185][0m |           0.0025 |         170.0737 |           9.6798 |
[32m[20221213 21:24:59 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:25:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.00
[32m[20221213 21:25:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.00
[32m[20221213 21:25:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 21:25:00 @agent_ppo2.py:143][0m Total time:      29.42 min
[32m[20221213 21:25:00 @agent_ppo2.py:145][0m 2873344 total steps have happened
[32m[20221213 21:25:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1403 --------------------------#
[32m[20221213 21:25:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:00 @agent_ppo2.py:185][0m |          -0.0012 |         161.2930 |           9.4699 |
[32m[20221213 21:25:00 @agent_ppo2.py:185][0m |          -0.0047 |         160.5717 |           9.4864 |
[32m[20221213 21:25:00 @agent_ppo2.py:185][0m |          -0.0060 |         160.2268 |           9.5243 |
[32m[20221213 21:25:00 @agent_ppo2.py:185][0m |          -0.0049 |         160.0636 |           9.4714 |
[32m[20221213 21:25:00 @agent_ppo2.py:185][0m |          -0.0049 |         159.9156 |           9.4837 |
[32m[20221213 21:25:00 @agent_ppo2.py:185][0m |          -0.0048 |         161.1663 |           9.4647 |
[32m[20221213 21:25:00 @agent_ppo2.py:185][0m |           0.0037 |         180.6043 |           9.4630 |
[32m[20221213 21:25:01 @agent_ppo2.py:185][0m |          -0.0083 |         159.5894 |           9.4699 |
[32m[20221213 21:25:01 @agent_ppo2.py:185][0m |          -0.0072 |         159.5644 |           9.4964 |
[32m[20221213 21:25:01 @agent_ppo2.py:185][0m |          -0.0087 |         159.5196 |           9.4582 |
[32m[20221213 21:25:01 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:25:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.40
[32m[20221213 21:25:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 21:25:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 791.00
[32m[20221213 21:25:01 @agent_ppo2.py:143][0m Total time:      29.44 min
[32m[20221213 21:25:01 @agent_ppo2.py:145][0m 2875392 total steps have happened
[32m[20221213 21:25:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1404 --------------------------#
[32m[20221213 21:25:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:25:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:01 @agent_ppo2.py:185][0m |           0.0023 |         162.0704 |           9.5120 |
[32m[20221213 21:25:01 @agent_ppo2.py:185][0m |          -0.0055 |         160.6549 |           9.5633 |
[32m[20221213 21:25:01 @agent_ppo2.py:185][0m |           0.0094 |         174.6616 |           9.4927 |
[32m[20221213 21:25:02 @agent_ppo2.py:185][0m |          -0.0063 |         159.6194 |           9.5219 |
[32m[20221213 21:25:02 @agent_ppo2.py:185][0m |          -0.0069 |         159.2396 |           9.4762 |
[32m[20221213 21:25:02 @agent_ppo2.py:185][0m |          -0.0064 |         159.2971 |           9.5373 |
[32m[20221213 21:25:02 @agent_ppo2.py:185][0m |          -0.0058 |         159.0834 |           9.5140 |
[32m[20221213 21:25:02 @agent_ppo2.py:185][0m |          -0.0087 |         158.6528 |           9.4940 |
[32m[20221213 21:25:02 @agent_ppo2.py:185][0m |          -0.0079 |         158.4348 |           9.5233 |
[32m[20221213 21:25:02 @agent_ppo2.py:185][0m |          -0.0063 |         158.5285 |           9.4729 |
[32m[20221213 21:25:02 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 21:25:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.60
[32m[20221213 21:25:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:25:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.00
[32m[20221213 21:25:02 @agent_ppo2.py:143][0m Total time:      29.46 min
[32m[20221213 21:25:02 @agent_ppo2.py:145][0m 2877440 total steps have happened
[32m[20221213 21:25:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1405 --------------------------#
[32m[20221213 21:25:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:03 @agent_ppo2.py:185][0m |           0.0012 |         167.2974 |           9.3617 |
[32m[20221213 21:25:03 @agent_ppo2.py:185][0m |          -0.0055 |         165.2788 |           9.2610 |
[32m[20221213 21:25:03 @agent_ppo2.py:185][0m |          -0.0045 |         164.2518 |           9.2755 |
[32m[20221213 21:25:03 @agent_ppo2.py:185][0m |           0.0005 |         170.7901 |           9.2512 |
[32m[20221213 21:25:03 @agent_ppo2.py:185][0m |          -0.0083 |         162.7902 |           9.2725 |
[32m[20221213 21:25:03 @agent_ppo2.py:185][0m |           0.0002 |         175.2286 |           9.2056 |
[32m[20221213 21:25:03 @agent_ppo2.py:185][0m |          -0.0101 |         162.0396 |           9.2096 |
[32m[20221213 21:25:03 @agent_ppo2.py:185][0m |          -0.0090 |         161.6325 |           9.2256 |
[32m[20221213 21:25:03 @agent_ppo2.py:185][0m |          -0.0105 |         161.2954 |           9.1813 |
[32m[20221213 21:25:03 @agent_ppo2.py:185][0m |          -0.0127 |         161.1442 |           9.1779 |
[32m[20221213 21:25:03 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:25:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 754.40
[32m[20221213 21:25:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:25:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.00
[32m[20221213 21:25:04 @agent_ppo2.py:143][0m Total time:      29.48 min
[32m[20221213 21:25:04 @agent_ppo2.py:145][0m 2879488 total steps have happened
[32m[20221213 21:25:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1406 --------------------------#
[32m[20221213 21:25:04 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:25:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:04 @agent_ppo2.py:185][0m |          -0.0017 |         167.0292 |           9.4642 |
[32m[20221213 21:25:04 @agent_ppo2.py:185][0m |          -0.0058 |         162.8702 |           9.3980 |
[32m[20221213 21:25:04 @agent_ppo2.py:185][0m |          -0.0086 |         158.9314 |           9.4081 |
[32m[20221213 21:25:04 @agent_ppo2.py:185][0m |          -0.0079 |         156.5696 |           9.4067 |
[32m[20221213 21:25:04 @agent_ppo2.py:185][0m |          -0.0096 |         155.0908 |           9.3715 |
[32m[20221213 21:25:04 @agent_ppo2.py:185][0m |          -0.0093 |         153.9752 |           9.3330 |
[32m[20221213 21:25:04 @agent_ppo2.py:185][0m |          -0.0122 |         153.2153 |           9.3404 |
[32m[20221213 21:25:05 @agent_ppo2.py:185][0m |          -0.0071 |         156.8893 |           9.2850 |
[32m[20221213 21:25:05 @agent_ppo2.py:185][0m |          -0.0125 |         151.9034 |           9.3171 |
[32m[20221213 21:25:05 @agent_ppo2.py:185][0m |          -0.0003 |         168.4402 |           9.2797 |
[32m[20221213 21:25:05 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 21:25:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.00
[32m[20221213 21:25:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.00
[32m[20221213 21:25:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 702.00
[32m[20221213 21:25:05 @agent_ppo2.py:143][0m Total time:      29.50 min
[32m[20221213 21:25:05 @agent_ppo2.py:145][0m 2881536 total steps have happened
[32m[20221213 21:25:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1407 --------------------------#
[32m[20221213 21:25:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:05 @agent_ppo2.py:185][0m |          -0.0007 |         175.4954 |           9.2003 |
[32m[20221213 21:25:05 @agent_ppo2.py:185][0m |          -0.0063 |         172.1377 |           9.2030 |
[32m[20221213 21:25:05 @agent_ppo2.py:185][0m |          -0.0064 |         170.2248 |           9.1715 |
[32m[20221213 21:25:05 @agent_ppo2.py:185][0m |          -0.0083 |         169.3774 |           9.2195 |
[32m[20221213 21:25:06 @agent_ppo2.py:185][0m |          -0.0075 |         168.3110 |           9.2003 |
[32m[20221213 21:25:06 @agent_ppo2.py:185][0m |          -0.0098 |         167.5249 |           9.2126 |
[32m[20221213 21:25:06 @agent_ppo2.py:185][0m |          -0.0110 |         167.0074 |           9.1957 |
[32m[20221213 21:25:06 @agent_ppo2.py:185][0m |          -0.0089 |         166.3890 |           9.2007 |
[32m[20221213 21:25:06 @agent_ppo2.py:185][0m |          -0.0077 |         168.1841 |           9.2128 |
[32m[20221213 21:25:06 @agent_ppo2.py:185][0m |          -0.0063 |         167.6301 |           9.2085 |
[32m[20221213 21:25:06 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:25:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 728.40
[32m[20221213 21:25:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.00
[32m[20221213 21:25:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.00
[32m[20221213 21:25:06 @agent_ppo2.py:143][0m Total time:      29.52 min
[32m[20221213 21:25:06 @agent_ppo2.py:145][0m 2883584 total steps have happened
[32m[20221213 21:25:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1408 --------------------------#
[32m[20221213 21:25:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:07 @agent_ppo2.py:185][0m |          -0.0004 |         169.4866 |           8.7905 |
[32m[20221213 21:25:07 @agent_ppo2.py:185][0m |          -0.0051 |         167.6913 |           8.7629 |
[32m[20221213 21:25:07 @agent_ppo2.py:185][0m |          -0.0007 |         168.4063 |           8.6901 |
[32m[20221213 21:25:07 @agent_ppo2.py:185][0m |          -0.0055 |         166.4497 |           8.6787 |
[32m[20221213 21:25:07 @agent_ppo2.py:185][0m |           0.0085 |         173.1820 |           8.6151 |
[32m[20221213 21:25:07 @agent_ppo2.py:185][0m |           0.0003 |         169.2561 |           8.6302 |
[32m[20221213 21:25:07 @agent_ppo2.py:185][0m |          -0.0057 |         165.7132 |           8.5540 |
[32m[20221213 21:25:07 @agent_ppo2.py:185][0m |          -0.0046 |         165.9872 |           8.5221 |
[32m[20221213 21:25:07 @agent_ppo2.py:185][0m |          -0.0056 |         165.3823 |           8.4435 |
[32m[20221213 21:25:07 @agent_ppo2.py:185][0m |          -0.0071 |         165.3335 |           8.4309 |
[32m[20221213 21:25:07 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:25:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.80
[32m[20221213 21:25:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:25:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.00
[32m[20221213 21:25:08 @agent_ppo2.py:143][0m Total time:      29.55 min
[32m[20221213 21:25:08 @agent_ppo2.py:145][0m 2885632 total steps have happened
[32m[20221213 21:25:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1409 --------------------------#
[32m[20221213 21:25:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:08 @agent_ppo2.py:185][0m |          -0.0016 |         166.5517 |           8.9154 |
[32m[20221213 21:25:08 @agent_ppo2.py:185][0m |          -0.0040 |         165.4465 |           8.8889 |
[32m[20221213 21:25:08 @agent_ppo2.py:185][0m |          -0.0062 |         164.5202 |           8.8586 |
[32m[20221213 21:25:08 @agent_ppo2.py:185][0m |          -0.0074 |         164.1383 |           8.8678 |
[32m[20221213 21:25:08 @agent_ppo2.py:185][0m |          -0.0052 |         163.6975 |           8.8668 |
[32m[20221213 21:25:08 @agent_ppo2.py:185][0m |          -0.0059 |         163.4126 |           8.8239 |
[32m[20221213 21:25:08 @agent_ppo2.py:185][0m |          -0.0084 |         163.1671 |           8.7906 |
[32m[20221213 21:25:08 @agent_ppo2.py:185][0m |          -0.0075 |         162.8862 |           8.7826 |
[32m[20221213 21:25:09 @agent_ppo2.py:185][0m |          -0.0063 |         162.5905 |           8.7967 |
[32m[20221213 21:25:09 @agent_ppo2.py:185][0m |          -0.0026 |         168.6662 |           8.8027 |
[32m[20221213 21:25:09 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:25:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 763.00
[32m[20221213 21:25:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.00
[32m[20221213 21:25:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.00
[32m[20221213 21:25:09 @agent_ppo2.py:143][0m Total time:      29.57 min
[32m[20221213 21:25:09 @agent_ppo2.py:145][0m 2887680 total steps have happened
[32m[20221213 21:25:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1410 --------------------------#
[32m[20221213 21:25:09 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:25:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:09 @agent_ppo2.py:185][0m |           0.0032 |         167.0942 |           8.5325 |
[32m[20221213 21:25:09 @agent_ppo2.py:185][0m |          -0.0047 |         164.0954 |           8.5889 |
[32m[20221213 21:25:09 @agent_ppo2.py:185][0m |          -0.0054 |         163.5687 |           8.5347 |
[32m[20221213 21:25:09 @agent_ppo2.py:185][0m |          -0.0071 |         163.1585 |           8.5651 |
[32m[20221213 21:25:09 @agent_ppo2.py:185][0m |          -0.0066 |         163.3685 |           8.5302 |
[32m[20221213 21:25:09 @agent_ppo2.py:185][0m |          -0.0086 |         162.8609 |           8.5549 |
[32m[20221213 21:25:10 @agent_ppo2.py:185][0m |          -0.0066 |         162.6242 |           8.4896 |
[32m[20221213 21:25:10 @agent_ppo2.py:185][0m |          -0.0097 |         162.4272 |           8.4862 |
[32m[20221213 21:25:10 @agent_ppo2.py:185][0m |          -0.0098 |         162.3571 |           8.4444 |
[32m[20221213 21:25:10 @agent_ppo2.py:185][0m |          -0.0091 |         162.2308 |           8.4463 |
[32m[20221213 21:25:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.80
[32m[20221213 21:25:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.00
[32m[20221213 21:25:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 783.00
[32m[20221213 21:25:10 @agent_ppo2.py:143][0m Total time:      29.59 min
[32m[20221213 21:25:10 @agent_ppo2.py:145][0m 2889728 total steps have happened
[32m[20221213 21:25:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1411 --------------------------#
[32m[20221213 21:25:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:10 @agent_ppo2.py:185][0m |          -0.0035 |         165.8952 |           8.4871 |
[32m[20221213 21:25:10 @agent_ppo2.py:185][0m |          -0.0037 |         164.7480 |           8.4675 |
[32m[20221213 21:25:10 @agent_ppo2.py:185][0m |           0.0135 |         185.3972 |           8.4431 |
[32m[20221213 21:25:10 @agent_ppo2.py:185][0m |          -0.0031 |         164.3914 |           8.5545 |
[32m[20221213 21:25:11 @agent_ppo2.py:185][0m |          -0.0066 |         163.5918 |           8.4874 |
[32m[20221213 21:25:11 @agent_ppo2.py:185][0m |          -0.0054 |         163.4698 |           8.5032 |
[32m[20221213 21:25:11 @agent_ppo2.py:185][0m |          -0.0080 |         163.0988 |           8.4988 |
[32m[20221213 21:25:11 @agent_ppo2.py:185][0m |          -0.0081 |         162.8439 |           8.4954 |
[32m[20221213 21:25:11 @agent_ppo2.py:185][0m |           0.0012 |         172.1773 |           8.4943 |
[32m[20221213 21:25:11 @agent_ppo2.py:185][0m |          -0.0092 |         162.7409 |           8.4986 |
[32m[20221213 21:25:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:25:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.00
[32m[20221213 21:25:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.00
[32m[20221213 21:25:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.00
[32m[20221213 21:25:11 @agent_ppo2.py:143][0m Total time:      29.61 min
[32m[20221213 21:25:11 @agent_ppo2.py:145][0m 2891776 total steps have happened
[32m[20221213 21:25:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1412 --------------------------#
[32m[20221213 21:25:11 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:25:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:11 @agent_ppo2.py:185][0m |          -0.0023 |         165.4439 |           8.5636 |
[32m[20221213 21:25:12 @agent_ppo2.py:185][0m |           0.0089 |         188.3566 |           8.5753 |
[32m[20221213 21:25:12 @agent_ppo2.py:185][0m |          -0.0035 |         163.5512 |           8.5946 |
[32m[20221213 21:25:12 @agent_ppo2.py:185][0m |          -0.0054 |         163.2443 |           8.5271 |
[32m[20221213 21:25:12 @agent_ppo2.py:185][0m |          -0.0046 |         163.1913 |           8.5141 |
[32m[20221213 21:25:12 @agent_ppo2.py:185][0m |          -0.0079 |         162.7141 |           8.4712 |
[32m[20221213 21:25:12 @agent_ppo2.py:185][0m |          -0.0071 |         162.5552 |           8.4840 |
[32m[20221213 21:25:12 @agent_ppo2.py:185][0m |          -0.0075 |         162.5235 |           8.4917 |
[32m[20221213 21:25:12 @agent_ppo2.py:185][0m |          -0.0082 |         162.2467 |           8.5158 |
[32m[20221213 21:25:12 @agent_ppo2.py:185][0m |          -0.0078 |         162.0149 |           8.4761 |
[32m[20221213 21:25:12 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:25:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.60
[32m[20221213 21:25:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:25:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.00
[32m[20221213 21:25:12 @agent_ppo2.py:143][0m Total time:      29.63 min
[32m[20221213 21:25:12 @agent_ppo2.py:145][0m 2893824 total steps have happened
[32m[20221213 21:25:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1413 --------------------------#
[32m[20221213 21:25:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:13 @agent_ppo2.py:185][0m |          -0.0025 |         164.2848 |           8.3312 |
[32m[20221213 21:25:13 @agent_ppo2.py:185][0m |          -0.0032 |         163.1424 |           8.3568 |
[32m[20221213 21:25:13 @agent_ppo2.py:185][0m |          -0.0056 |         162.1526 |           8.2887 |
[32m[20221213 21:25:13 @agent_ppo2.py:185][0m |          -0.0084 |         161.5445 |           8.2941 |
[32m[20221213 21:25:13 @agent_ppo2.py:185][0m |          -0.0076 |         161.2451 |           8.2942 |
[32m[20221213 21:25:13 @agent_ppo2.py:185][0m |          -0.0052 |         162.3411 |           8.2531 |
[32m[20221213 21:25:13 @agent_ppo2.py:185][0m |          -0.0099 |         160.4991 |           8.3001 |
[32m[20221213 21:25:13 @agent_ppo2.py:185][0m |          -0.0107 |         160.3732 |           8.2701 |
[32m[20221213 21:25:13 @agent_ppo2.py:185][0m |          -0.0001 |         178.9790 |           8.2565 |
[32m[20221213 21:25:13 @agent_ppo2.py:185][0m |          -0.0067 |         163.1073 |           8.2533 |
[32m[20221213 21:25:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:25:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.40
[32m[20221213 21:25:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 784.00
[32m[20221213 21:25:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.00
[32m[20221213 21:25:14 @agent_ppo2.py:143][0m Total time:      29.65 min
[32m[20221213 21:25:14 @agent_ppo2.py:145][0m 2895872 total steps have happened
[32m[20221213 21:25:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1414 --------------------------#
[32m[20221213 21:25:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:25:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:14 @agent_ppo2.py:185][0m |           0.0043 |         168.3182 |           7.9811 |
[32m[20221213 21:25:14 @agent_ppo2.py:185][0m |          -0.0049 |         164.2755 |           7.9805 |
[32m[20221213 21:25:14 @agent_ppo2.py:185][0m |          -0.0065 |         163.5110 |           8.0028 |
[32m[20221213 21:25:14 @agent_ppo2.py:185][0m |          -0.0075 |         162.8734 |           7.9347 |
[32m[20221213 21:25:14 @agent_ppo2.py:185][0m |          -0.0082 |         162.2754 |           7.8830 |
[32m[20221213 21:25:14 @agent_ppo2.py:185][0m |          -0.0080 |         161.9452 |           7.8779 |
[32m[20221213 21:25:14 @agent_ppo2.py:185][0m |           0.0012 |         173.9142 |           7.8875 |
[32m[20221213 21:25:15 @agent_ppo2.py:185][0m |          -0.0086 |         161.6372 |           7.9153 |
[32m[20221213 21:25:15 @agent_ppo2.py:185][0m |          -0.0084 |         161.2144 |           7.8052 |
[32m[20221213 21:25:15 @agent_ppo2.py:185][0m |          -0.0095 |         160.8799 |           7.7894 |
[32m[20221213 21:25:15 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:25:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.40
[32m[20221213 21:25:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 21:25:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.00
[32m[20221213 21:25:15 @agent_ppo2.py:143][0m Total time:      29.67 min
[32m[20221213 21:25:15 @agent_ppo2.py:145][0m 2897920 total steps have happened
[32m[20221213 21:25:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1415 --------------------------#
[32m[20221213 21:25:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:15 @agent_ppo2.py:185][0m |           0.0012 |         167.4045 |           8.0050 |
[32m[20221213 21:25:15 @agent_ppo2.py:185][0m |          -0.0045 |         166.1014 |           7.9591 |
[32m[20221213 21:25:15 @agent_ppo2.py:185][0m |          -0.0072 |         165.3635 |           7.9368 |
[32m[20221213 21:25:15 @agent_ppo2.py:185][0m |          -0.0066 |         164.8964 |           7.8957 |
[32m[20221213 21:25:15 @agent_ppo2.py:185][0m |          -0.0076 |         164.7000 |           7.9229 |
[32m[20221213 21:25:16 @agent_ppo2.py:185][0m |          -0.0083 |         164.2577 |           7.9241 |
[32m[20221213 21:25:16 @agent_ppo2.py:185][0m |          -0.0068 |         164.0753 |           7.9459 |
[32m[20221213 21:25:16 @agent_ppo2.py:185][0m |          -0.0099 |         163.9563 |           7.9475 |
[32m[20221213 21:25:16 @agent_ppo2.py:185][0m |          -0.0098 |         163.5969 |           7.9192 |
[32m[20221213 21:25:16 @agent_ppo2.py:185][0m |          -0.0089 |         163.5076 |           7.9348 |
[32m[20221213 21:25:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:25:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.80
[32m[20221213 21:25:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:25:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.00
[32m[20221213 21:25:16 @agent_ppo2.py:143][0m Total time:      29.69 min
[32m[20221213 21:25:16 @agent_ppo2.py:145][0m 2899968 total steps have happened
[32m[20221213 21:25:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1416 --------------------------#
[32m[20221213 21:25:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:16 @agent_ppo2.py:185][0m |          -0.0024 |         165.5646 |           7.3794 |
[32m[20221213 21:25:16 @agent_ppo2.py:185][0m |           0.0009 |         167.4058 |           7.3704 |
[32m[20221213 21:25:17 @agent_ppo2.py:185][0m |          -0.0078 |         164.2917 |           7.3381 |
[32m[20221213 21:25:17 @agent_ppo2.py:185][0m |          -0.0001 |         174.6696 |           7.3023 |
[32m[20221213 21:25:17 @agent_ppo2.py:185][0m |           0.0005 |         175.4675 |           7.3699 |
[32m[20221213 21:25:17 @agent_ppo2.py:185][0m |          -0.0071 |         163.2851 |           7.3583 |
[32m[20221213 21:25:17 @agent_ppo2.py:185][0m |          -0.0015 |         169.6776 |           7.4104 |
[32m[20221213 21:25:17 @agent_ppo2.py:185][0m |          -0.0096 |         162.6122 |           7.3298 |
[32m[20221213 21:25:17 @agent_ppo2.py:185][0m |          -0.0061 |         162.8824 |           7.3910 |
[32m[20221213 21:25:17 @agent_ppo2.py:185][0m |          -0.0069 |         162.4528 |           7.3526 |
[32m[20221213 21:25:17 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:25:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 762.80
[32m[20221213 21:25:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.00
[32m[20221213 21:25:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.00
[32m[20221213 21:25:17 @agent_ppo2.py:143][0m Total time:      29.71 min
[32m[20221213 21:25:17 @agent_ppo2.py:145][0m 2902016 total steps have happened
[32m[20221213 21:25:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1417 --------------------------#
[32m[20221213 21:25:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:25:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:18 @agent_ppo2.py:185][0m |           0.0047 |         172.4291 |           7.5778 |
[32m[20221213 21:25:18 @agent_ppo2.py:185][0m |           0.0023 |         167.0656 |           7.6150 |
[32m[20221213 21:25:18 @agent_ppo2.py:185][0m |           0.0025 |         170.2055 |           7.6280 |
[32m[20221213 21:25:18 @agent_ppo2.py:185][0m |          -0.0046 |         165.1108 |           7.6000 |
[32m[20221213 21:25:18 @agent_ppo2.py:185][0m |          -0.0083 |         164.7170 |           7.5568 |
[32m[20221213 21:25:18 @agent_ppo2.py:185][0m |          -0.0087 |         164.4599 |           7.5924 |
[32m[20221213 21:25:18 @agent_ppo2.py:185][0m |          -0.0065 |         164.3430 |           7.5502 |
[32m[20221213 21:25:18 @agent_ppo2.py:185][0m |          -0.0062 |         163.8308 |           7.5504 |
[32m[20221213 21:25:18 @agent_ppo2.py:185][0m |          -0.0083 |         163.7346 |           7.5283 |
[32m[20221213 21:25:18 @agent_ppo2.py:185][0m |          -0.0094 |         163.4889 |           7.4796 |
[32m[20221213 21:25:18 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:25:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 725.20
[32m[20221213 21:25:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.00
[32m[20221213 21:25:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.00
[32m[20221213 21:25:19 @agent_ppo2.py:143][0m Total time:      29.73 min
[32m[20221213 21:25:19 @agent_ppo2.py:145][0m 2904064 total steps have happened
[32m[20221213 21:25:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1418 --------------------------#
[32m[20221213 21:25:19 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:25:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:19 @agent_ppo2.py:185][0m |           0.0030 |         167.8721 |           7.4584 |
[32m[20221213 21:25:19 @agent_ppo2.py:185][0m |          -0.0047 |         164.3482 |           7.5109 |
[32m[20221213 21:25:19 @agent_ppo2.py:185][0m |           0.0065 |         175.8077 |           7.6154 |
[32m[20221213 21:25:19 @agent_ppo2.py:185][0m |           0.0030 |         168.2098 |           7.6585 |
[32m[20221213 21:25:19 @agent_ppo2.py:185][0m |          -0.0021 |         163.7531 |           7.5602 |
[32m[20221213 21:25:19 @agent_ppo2.py:185][0m |           0.0033 |         171.2810 |           7.6112 |
[32m[20221213 21:25:19 @agent_ppo2.py:185][0m |           0.0053 |         176.1126 |           7.7369 |
[32m[20221213 21:25:19 @agent_ppo2.py:185][0m |          -0.0070 |         162.6499 |           7.7507 |
[32m[20221213 21:25:20 @agent_ppo2.py:185][0m |          -0.0061 |         162.5829 |           7.7757 |
[32m[20221213 21:25:20 @agent_ppo2.py:185][0m |          -0.0090 |         162.2878 |           7.7820 |
[32m[20221213 21:25:20 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:25:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 779.60
[32m[20221213 21:25:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:25:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.00
[32m[20221213 21:25:20 @agent_ppo2.py:143][0m Total time:      29.75 min
[32m[20221213 21:25:20 @agent_ppo2.py:145][0m 2906112 total steps have happened
[32m[20221213 21:25:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1419 --------------------------#
[32m[20221213 21:25:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:20 @agent_ppo2.py:185][0m |           0.0018 |         166.2008 |           7.7022 |
[32m[20221213 21:25:20 @agent_ppo2.py:185][0m |           0.0031 |         169.2992 |           7.6821 |
[32m[20221213 21:25:20 @agent_ppo2.py:185][0m |          -0.0006 |         166.0765 |           7.6782 |
[32m[20221213 21:25:20 @agent_ppo2.py:185][0m |          -0.0067 |         164.8899 |           7.7239 |
[32m[20221213 21:25:20 @agent_ppo2.py:185][0m |          -0.0073 |         164.6977 |           7.6960 |
[32m[20221213 21:25:21 @agent_ppo2.py:185][0m |          -0.0048 |         165.0224 |           7.7265 |
[32m[20221213 21:25:21 @agent_ppo2.py:185][0m |          -0.0058 |         164.6595 |           7.7220 |
[32m[20221213 21:25:21 @agent_ppo2.py:185][0m |          -0.0104 |         164.2451 |           7.6938 |
[32m[20221213 21:25:21 @agent_ppo2.py:185][0m |          -0.0101 |         164.1103 |           7.7142 |
[32m[20221213 21:25:21 @agent_ppo2.py:185][0m |          -0.0105 |         163.9738 |           7.7654 |
[32m[20221213 21:25:21 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:25:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.20
[32m[20221213 21:25:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.00
[32m[20221213 21:25:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.00
[32m[20221213 21:25:21 @agent_ppo2.py:143][0m Total time:      29.77 min
[32m[20221213 21:25:21 @agent_ppo2.py:145][0m 2908160 total steps have happened
[32m[20221213 21:25:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1420 --------------------------#
[32m[20221213 21:25:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:21 @agent_ppo2.py:185][0m |          -0.0029 |         162.4565 |           7.7366 |
[32m[20221213 21:25:21 @agent_ppo2.py:185][0m |          -0.0079 |         161.8553 |           7.7546 |
[32m[20221213 21:25:21 @agent_ppo2.py:185][0m |          -0.0093 |         161.6679 |           7.6744 |
[32m[20221213 21:25:22 @agent_ppo2.py:185][0m |          -0.0071 |         161.4536 |           7.6750 |
[32m[20221213 21:25:22 @agent_ppo2.py:185][0m |          -0.0095 |         161.1385 |           7.6829 |
[32m[20221213 21:25:22 @agent_ppo2.py:185][0m |          -0.0091 |         160.8974 |           7.7058 |
[32m[20221213 21:25:22 @agent_ppo2.py:185][0m |          -0.0086 |         160.6944 |           7.6733 |
[32m[20221213 21:25:22 @agent_ppo2.py:185][0m |          -0.0024 |         174.8423 |           7.6696 |
[32m[20221213 21:25:22 @agent_ppo2.py:185][0m |          -0.0085 |         160.8304 |           7.6547 |
[32m[20221213 21:25:22 @agent_ppo2.py:185][0m |          -0.0122 |         160.3372 |           7.6392 |
[32m[20221213 21:25:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:25:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.40
[32m[20221213 21:25:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:25:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.00
[32m[20221213 21:25:22 @agent_ppo2.py:143][0m Total time:      29.79 min
[32m[20221213 21:25:22 @agent_ppo2.py:145][0m 2910208 total steps have happened
[32m[20221213 21:25:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1421 --------------------------#
[32m[20221213 21:25:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:23 @agent_ppo2.py:185][0m |          -0.0024 |         164.9099 |           7.4530 |
[32m[20221213 21:25:23 @agent_ppo2.py:185][0m |          -0.0069 |         164.2890 |           7.3509 |
[32m[20221213 21:25:23 @agent_ppo2.py:185][0m |          -0.0011 |         170.1211 |           7.4495 |
[32m[20221213 21:25:23 @agent_ppo2.py:185][0m |          -0.0027 |         167.8179 |           7.3988 |
[32m[20221213 21:25:23 @agent_ppo2.py:185][0m |          -0.0035 |         164.1695 |           7.5124 |
[32m[20221213 21:25:23 @agent_ppo2.py:185][0m |          -0.0089 |         163.3791 |           7.4427 |
[32m[20221213 21:25:23 @agent_ppo2.py:185][0m |          -0.0054 |         165.3997 |           7.4311 |
[32m[20221213 21:25:23 @agent_ppo2.py:185][0m |          -0.0085 |         163.1644 |           7.4648 |
[32m[20221213 21:25:23 @agent_ppo2.py:185][0m |          -0.0108 |         163.0078 |           7.4926 |
[32m[20221213 21:25:23 @agent_ppo2.py:185][0m |          -0.0112 |         162.8967 |           7.4030 |
[32m[20221213 21:25:23 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:25:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 763.80
[32m[20221213 21:25:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.00
[32m[20221213 21:25:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.00
[32m[20221213 21:25:24 @agent_ppo2.py:143][0m Total time:      29.81 min
[32m[20221213 21:25:24 @agent_ppo2.py:145][0m 2912256 total steps have happened
[32m[20221213 21:25:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1422 --------------------------#
[32m[20221213 21:25:24 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:25:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:24 @agent_ppo2.py:185][0m |          -0.0008 |         165.6837 |           7.6881 |
[32m[20221213 21:25:24 @agent_ppo2.py:185][0m |          -0.0016 |         165.3124 |           7.6927 |
[32m[20221213 21:25:24 @agent_ppo2.py:185][0m |          -0.0043 |         164.5502 |           7.6626 |
[32m[20221213 21:25:24 @agent_ppo2.py:185][0m |          -0.0055 |         164.0654 |           7.7160 |
[32m[20221213 21:25:24 @agent_ppo2.py:185][0m |          -0.0069 |         163.7546 |           7.6922 |
[32m[20221213 21:25:24 @agent_ppo2.py:185][0m |          -0.0076 |         163.6199 |           7.6999 |
[32m[20221213 21:25:24 @agent_ppo2.py:185][0m |          -0.0069 |         163.3298 |           7.6976 |
[32m[20221213 21:25:24 @agent_ppo2.py:185][0m |          -0.0075 |         163.1593 |           7.6629 |
[32m[20221213 21:25:25 @agent_ppo2.py:185][0m |          -0.0073 |         163.0667 |           7.6635 |
[32m[20221213 21:25:25 @agent_ppo2.py:185][0m |          -0.0039 |         163.4142 |           7.7343 |
[32m[20221213 21:25:25 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:25:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.80
[32m[20221213 21:25:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:25:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.00
[32m[20221213 21:25:25 @agent_ppo2.py:143][0m Total time:      29.83 min
[32m[20221213 21:25:25 @agent_ppo2.py:145][0m 2914304 total steps have happened
[32m[20221213 21:25:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1423 --------------------------#
[32m[20221213 21:25:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:25 @agent_ppo2.py:185][0m |           0.0011 |         169.3760 |           7.4796 |
[32m[20221213 21:25:25 @agent_ppo2.py:185][0m |          -0.0046 |         167.1545 |           7.5487 |
[32m[20221213 21:25:25 @agent_ppo2.py:185][0m |          -0.0018 |         167.2129 |           7.4932 |
[32m[20221213 21:25:25 @agent_ppo2.py:185][0m |          -0.0063 |         165.2319 |           7.4882 |
[32m[20221213 21:25:25 @agent_ppo2.py:185][0m |          -0.0058 |         164.6444 |           7.4668 |
[32m[20221213 21:25:25 @agent_ppo2.py:185][0m |          -0.0052 |         164.1288 |           7.4521 |
[32m[20221213 21:25:26 @agent_ppo2.py:185][0m |          -0.0069 |         163.8286 |           7.4585 |
[32m[20221213 21:25:26 @agent_ppo2.py:185][0m |           0.0035 |         181.7201 |           7.4000 |
[32m[20221213 21:25:26 @agent_ppo2.py:185][0m |          -0.0076 |         163.0919 |           7.4210 |
[32m[20221213 21:25:26 @agent_ppo2.py:185][0m |          -0.0071 |         163.0157 |           7.3658 |
[32m[20221213 21:25:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 762.40
[32m[20221213 21:25:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:25:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.00
[32m[20221213 21:25:26 @agent_ppo2.py:143][0m Total time:      29.85 min
[32m[20221213 21:25:26 @agent_ppo2.py:145][0m 2916352 total steps have happened
[32m[20221213 21:25:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1424 --------------------------#
[32m[20221213 21:25:26 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:25:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:26 @agent_ppo2.py:185][0m |          -0.0003 |         168.3049 |           7.4572 |
[32m[20221213 21:25:26 @agent_ppo2.py:185][0m |          -0.0049 |         167.1028 |           7.4661 |
[32m[20221213 21:25:26 @agent_ppo2.py:185][0m |          -0.0057 |         166.2694 |           7.5265 |
[32m[20221213 21:25:27 @agent_ppo2.py:185][0m |          -0.0074 |         165.7981 |           7.5812 |
[32m[20221213 21:25:27 @agent_ppo2.py:185][0m |           0.0024 |         176.7928 |           7.5973 |
[32m[20221213 21:25:27 @agent_ppo2.py:185][0m |          -0.0082 |         165.2325 |           7.6259 |
[32m[20221213 21:25:27 @agent_ppo2.py:185][0m |          -0.0089 |         164.5846 |           7.6859 |
[32m[20221213 21:25:27 @agent_ppo2.py:185][0m |          -0.0087 |         164.3028 |           7.7061 |
[32m[20221213 21:25:27 @agent_ppo2.py:185][0m |          -0.0107 |         164.1206 |           7.7418 |
[32m[20221213 21:25:27 @agent_ppo2.py:185][0m |          -0.0090 |         163.8480 |           7.7341 |
[32m[20221213 21:25:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.60
[32m[20221213 21:25:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.00
[32m[20221213 21:25:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.00
[32m[20221213 21:25:27 @agent_ppo2.py:143][0m Total time:      29.87 min
[32m[20221213 21:25:27 @agent_ppo2.py:145][0m 2918400 total steps have happened
[32m[20221213 21:25:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1425 --------------------------#
[32m[20221213 21:25:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:27 @agent_ppo2.py:185][0m |          -0.0033 |         164.2770 |           7.9488 |
[32m[20221213 21:25:28 @agent_ppo2.py:185][0m |          -0.0071 |         163.6507 |           8.0600 |
[32m[20221213 21:25:28 @agent_ppo2.py:185][0m |          -0.0064 |         163.8013 |           8.0169 |
[32m[20221213 21:25:28 @agent_ppo2.py:185][0m |          -0.0078 |         162.8380 |           7.9975 |
[32m[20221213 21:25:28 @agent_ppo2.py:185][0m |          -0.0056 |         164.9817 |           7.9741 |
[32m[20221213 21:25:28 @agent_ppo2.py:185][0m |          -0.0084 |         162.3344 |           7.9794 |
[32m[20221213 21:25:28 @agent_ppo2.py:185][0m |          -0.0097 |         162.2774 |           7.9483 |
[32m[20221213 21:25:28 @agent_ppo2.py:185][0m |          -0.0086 |         162.0631 |           7.9190 |
[32m[20221213 21:25:28 @agent_ppo2.py:185][0m |          -0.0112 |         161.9580 |           7.9135 |
[32m[20221213 21:25:28 @agent_ppo2.py:185][0m |          -0.0113 |         161.7792 |           7.9181 |
[32m[20221213 21:25:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.20
[32m[20221213 21:25:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.00
[32m[20221213 21:25:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.00
[32m[20221213 21:25:28 @agent_ppo2.py:143][0m Total time:      29.89 min
[32m[20221213 21:25:28 @agent_ppo2.py:145][0m 2920448 total steps have happened
[32m[20221213 21:25:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1426 --------------------------#
[32m[20221213 21:25:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:29 @agent_ppo2.py:185][0m |          -0.0018 |         167.9787 |           7.8175 |
[32m[20221213 21:25:29 @agent_ppo2.py:185][0m |           0.0033 |         177.0805 |           7.7905 |
[32m[20221213 21:25:29 @agent_ppo2.py:185][0m |          -0.0069 |         165.9453 |           7.7675 |
[32m[20221213 21:25:29 @agent_ppo2.py:185][0m |          -0.0062 |         165.4864 |           7.7869 |
[32m[20221213 21:25:29 @agent_ppo2.py:185][0m |          -0.0053 |         165.2178 |           7.7954 |
[32m[20221213 21:25:29 @agent_ppo2.py:185][0m |          -0.0092 |         164.9630 |           7.7543 |
[32m[20221213 21:25:29 @agent_ppo2.py:185][0m |          -0.0081 |         164.7366 |           7.7269 |
[32m[20221213 21:25:29 @agent_ppo2.py:185][0m |          -0.0087 |         164.5649 |           7.7480 |
[32m[20221213 21:25:29 @agent_ppo2.py:185][0m |          -0.0062 |         164.6365 |           7.7599 |
[32m[20221213 21:25:29 @agent_ppo2.py:185][0m |          -0.0102 |         164.2613 |           7.7638 |
[32m[20221213 21:25:29 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:25:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.40
[32m[20221213 21:25:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:25:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.00
[32m[20221213 21:25:30 @agent_ppo2.py:143][0m Total time:      29.91 min
[32m[20221213 21:25:30 @agent_ppo2.py:145][0m 2922496 total steps have happened
[32m[20221213 21:25:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1427 --------------------------#
[32m[20221213 21:25:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:25:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:30 @agent_ppo2.py:185][0m |           0.0009 |         166.1045 |           7.7846 |
[32m[20221213 21:25:30 @agent_ppo2.py:185][0m |          -0.0017 |         165.5103 |           7.7225 |
[32m[20221213 21:25:30 @agent_ppo2.py:185][0m |          -0.0021 |         165.0650 |           7.8180 |
[32m[20221213 21:25:30 @agent_ppo2.py:185][0m |          -0.0016 |         164.7769 |           7.7953 |
[32m[20221213 21:25:30 @agent_ppo2.py:185][0m |          -0.0034 |         164.7012 |           7.7958 |
[32m[20221213 21:25:30 @agent_ppo2.py:185][0m |          -0.0052 |         164.5265 |           7.7906 |
[32m[20221213 21:25:30 @agent_ppo2.py:185][0m |          -0.0045 |         164.4102 |           7.7942 |
[32m[20221213 21:25:30 @agent_ppo2.py:185][0m |          -0.0061 |         164.1032 |           7.7956 |
[32m[20221213 21:25:31 @agent_ppo2.py:185][0m |          -0.0015 |         168.5237 |           7.8033 |
[32m[20221213 21:25:31 @agent_ppo2.py:185][0m |          -0.0033 |         163.9026 |           7.8545 |
[32m[20221213 21:25:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:25:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 784.40
[32m[20221213 21:25:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.00
[32m[20221213 21:25:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.00
[32m[20221213 21:25:31 @agent_ppo2.py:143][0m Total time:      29.93 min
[32m[20221213 21:25:31 @agent_ppo2.py:145][0m 2924544 total steps have happened
[32m[20221213 21:25:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1428 --------------------------#
[32m[20221213 21:25:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:31 @agent_ppo2.py:185][0m |           0.0035 |         166.8561 |           7.6984 |
[32m[20221213 21:25:31 @agent_ppo2.py:185][0m |           0.0001 |         166.6920 |           7.7067 |
[32m[20221213 21:25:31 @agent_ppo2.py:185][0m |          -0.0050 |         163.1453 |           7.6506 |
[32m[20221213 21:25:31 @agent_ppo2.py:185][0m |          -0.0045 |         162.3730 |           7.5823 |
[32m[20221213 21:25:31 @agent_ppo2.py:185][0m |          -0.0075 |         161.9437 |           7.6474 |
[32m[20221213 21:25:31 @agent_ppo2.py:185][0m |          -0.0109 |         161.8523 |           7.5720 |
[32m[20221213 21:25:32 @agent_ppo2.py:185][0m |          -0.0087 |         161.6426 |           7.6044 |
[32m[20221213 21:25:32 @agent_ppo2.py:185][0m |          -0.0077 |         162.3696 |           7.6265 |
[32m[20221213 21:25:32 @agent_ppo2.py:185][0m |          -0.0121 |         161.2626 |           7.5912 |
[32m[20221213 21:25:32 @agent_ppo2.py:185][0m |          -0.0113 |         160.9286 |           7.6025 |
[32m[20221213 21:25:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 721.00
[32m[20221213 21:25:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 773.00
[32m[20221213 21:25:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.00
[32m[20221213 21:25:32 @agent_ppo2.py:143][0m Total time:      29.95 min
[32m[20221213 21:25:32 @agent_ppo2.py:145][0m 2926592 total steps have happened
[32m[20221213 21:25:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1429 --------------------------#
[32m[20221213 21:25:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:32 @agent_ppo2.py:185][0m |          -0.0007 |         164.0401 |           7.4883 |
[32m[20221213 21:25:32 @agent_ppo2.py:185][0m |          -0.0033 |         163.1387 |           7.4073 |
[32m[20221213 21:25:32 @agent_ppo2.py:185][0m |          -0.0021 |         162.7560 |           7.3648 |
[32m[20221213 21:25:33 @agent_ppo2.py:185][0m |          -0.0069 |         162.3006 |           7.3483 |
[32m[20221213 21:25:33 @agent_ppo2.py:185][0m |          -0.0070 |         162.2040 |           7.3212 |
[32m[20221213 21:25:33 @agent_ppo2.py:185][0m |          -0.0031 |         163.9853 |           7.3049 |
[32m[20221213 21:25:33 @agent_ppo2.py:185][0m |          -0.0066 |         161.9007 |           7.2959 |
[32m[20221213 21:25:33 @agent_ppo2.py:185][0m |          -0.0012 |         165.7783 |           7.2613 |
[32m[20221213 21:25:33 @agent_ppo2.py:185][0m |          -0.0054 |         162.0109 |           7.2596 |
[32m[20221213 21:25:33 @agent_ppo2.py:185][0m |          -0.0089 |         161.4571 |           7.2623 |
[32m[20221213 21:25:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 750.40
[32m[20221213 21:25:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.00
[32m[20221213 21:25:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 21:25:33 @agent_ppo2.py:143][0m Total time:      29.97 min
[32m[20221213 21:25:33 @agent_ppo2.py:145][0m 2928640 total steps have happened
[32m[20221213 21:25:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1430 --------------------------#
[32m[20221213 21:25:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:25:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:33 @agent_ppo2.py:185][0m |          -0.0028 |         165.3121 |           6.7954 |
[32m[20221213 21:25:34 @agent_ppo2.py:185][0m |          -0.0055 |         162.3292 |           6.8519 |
[32m[20221213 21:25:34 @agent_ppo2.py:185][0m |          -0.0058 |         161.0057 |           6.8665 |
[32m[20221213 21:25:34 @agent_ppo2.py:185][0m |           0.0077 |         169.5103 |           6.9316 |
[32m[20221213 21:25:34 @agent_ppo2.py:185][0m |          -0.0076 |         159.9177 |           6.8378 |
[32m[20221213 21:25:34 @agent_ppo2.py:185][0m |          -0.0099 |         159.4327 |           6.9126 |
[32m[20221213 21:25:34 @agent_ppo2.py:185][0m |          -0.0058 |         159.1451 |           6.8899 |
[32m[20221213 21:25:34 @agent_ppo2.py:185][0m |          -0.0086 |         159.1573 |           6.8970 |
[32m[20221213 21:25:34 @agent_ppo2.py:185][0m |          -0.0096 |         158.6892 |           6.8905 |
[32m[20221213 21:25:34 @agent_ppo2.py:185][0m |          -0.0084 |         158.3618 |           6.8743 |
[32m[20221213 21:25:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.40
[32m[20221213 21:25:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.00
[32m[20221213 21:25:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.00
[32m[20221213 21:25:34 @agent_ppo2.py:143][0m Total time:      29.99 min
[32m[20221213 21:25:34 @agent_ppo2.py:145][0m 2930688 total steps have happened
[32m[20221213 21:25:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1431 --------------------------#
[32m[20221213 21:25:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:35 @agent_ppo2.py:185][0m |           0.0026 |         166.5685 |           6.9049 |
[32m[20221213 21:25:35 @agent_ppo2.py:185][0m |          -0.0002 |         166.0973 |           6.9113 |
[32m[20221213 21:25:35 @agent_ppo2.py:185][0m |          -0.0029 |         164.6845 |           6.9013 |
[32m[20221213 21:25:35 @agent_ppo2.py:185][0m |          -0.0047 |         164.2409 |           6.8791 |
[32m[20221213 21:25:35 @agent_ppo2.py:185][0m |           0.0013 |         173.9587 |           6.9298 |
[32m[20221213 21:25:35 @agent_ppo2.py:185][0m |          -0.0081 |         163.5238 |           6.9094 |
[32m[20221213 21:25:35 @agent_ppo2.py:185][0m |          -0.0054 |         163.3034 |           6.9092 |
[32m[20221213 21:25:35 @agent_ppo2.py:185][0m |          -0.0062 |         162.9731 |           6.8908 |
[32m[20221213 21:25:35 @agent_ppo2.py:185][0m |          -0.0031 |         166.5324 |           6.8875 |
[32m[20221213 21:25:35 @agent_ppo2.py:185][0m |          -0.0076 |         162.6364 |           6.8927 |
[32m[20221213 21:25:35 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:25:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.80
[32m[20221213 21:25:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.00
[32m[20221213 21:25:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.00
[32m[20221213 21:25:36 @agent_ppo2.py:143][0m Total time:      30.01 min
[32m[20221213 21:25:36 @agent_ppo2.py:145][0m 2932736 total steps have happened
[32m[20221213 21:25:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1432 --------------------------#
[32m[20221213 21:25:36 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:25:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:36 @agent_ppo2.py:185][0m |           0.0003 |         164.3158 |           7.0501 |
[32m[20221213 21:25:36 @agent_ppo2.py:185][0m |           0.0098 |         187.6893 |           7.0468 |
[32m[20221213 21:25:36 @agent_ppo2.py:185][0m |          -0.0059 |         162.7154 |           7.1447 |
[32m[20221213 21:25:36 @agent_ppo2.py:185][0m |          -0.0066 |         162.1852 |           7.1013 |
[32m[20221213 21:25:36 @agent_ppo2.py:185][0m |          -0.0072 |         161.5536 |           7.1772 |
[32m[20221213 21:25:36 @agent_ppo2.py:185][0m |          -0.0042 |         161.5777 |           7.1617 |
[32m[20221213 21:25:36 @agent_ppo2.py:185][0m |          -0.0079 |         160.7503 |           7.1940 |
[32m[20221213 21:25:36 @agent_ppo2.py:185][0m |          -0.0096 |         160.4774 |           7.2219 |
[32m[20221213 21:25:37 @agent_ppo2.py:185][0m |          -0.0077 |         160.0870 |           7.2170 |
[32m[20221213 21:25:37 @agent_ppo2.py:185][0m |          -0.0101 |         160.0384 |           7.2228 |
[32m[20221213 21:25:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:25:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.60
[32m[20221213 21:25:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221213 21:25:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:25:37 @agent_ppo2.py:143][0m Total time:      30.03 min
[32m[20221213 21:25:37 @agent_ppo2.py:145][0m 2934784 total steps have happened
[32m[20221213 21:25:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1433 --------------------------#
[32m[20221213 21:25:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:37 @agent_ppo2.py:185][0m |           0.0032 |         173.7455 |           7.4046 |
[32m[20221213 21:25:37 @agent_ppo2.py:185][0m |          -0.0043 |         170.7796 |           7.4496 |
[32m[20221213 21:25:37 @agent_ppo2.py:185][0m |          -0.0062 |         170.1648 |           7.4269 |
[32m[20221213 21:25:37 @agent_ppo2.py:185][0m |          -0.0063 |         169.6673 |           7.4575 |
[32m[20221213 21:25:37 @agent_ppo2.py:185][0m |          -0.0069 |         169.4900 |           7.4490 |
[32m[20221213 21:25:37 @agent_ppo2.py:185][0m |          -0.0079 |         169.1285 |           7.5099 |
[32m[20221213 21:25:38 @agent_ppo2.py:185][0m |          -0.0076 |         169.0807 |           7.5011 |
[32m[20221213 21:25:38 @agent_ppo2.py:185][0m |          -0.0074 |         168.8416 |           7.4826 |
[32m[20221213 21:25:38 @agent_ppo2.py:185][0m |          -0.0080 |         168.7365 |           7.5394 |
[32m[20221213 21:25:38 @agent_ppo2.py:185][0m |          -0.0098 |         168.5670 |           7.5295 |
[32m[20221213 21:25:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 745.40
[32m[20221213 21:25:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.00
[32m[20221213 21:25:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.00
[32m[20221213 21:25:38 @agent_ppo2.py:143][0m Total time:      30.05 min
[32m[20221213 21:25:38 @agent_ppo2.py:145][0m 2936832 total steps have happened
[32m[20221213 21:25:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1434 --------------------------#
[32m[20221213 21:25:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:38 @agent_ppo2.py:185][0m |           0.0041 |         167.5241 |           7.2232 |
[32m[20221213 21:25:38 @agent_ppo2.py:185][0m |           0.0014 |         168.0182 |           7.2611 |
[32m[20221213 21:25:38 @agent_ppo2.py:185][0m |          -0.0067 |         163.6111 |           7.2389 |
[32m[20221213 21:25:39 @agent_ppo2.py:185][0m |           0.0013 |         167.6556 |           7.2376 |
[32m[20221213 21:25:39 @agent_ppo2.py:185][0m |          -0.0020 |         163.6478 |           7.2006 |
[32m[20221213 21:25:39 @agent_ppo2.py:185][0m |          -0.0082 |         163.0724 |           7.2192 |
[32m[20221213 21:25:39 @agent_ppo2.py:185][0m |          -0.0087 |         162.8943 |           7.1960 |
[32m[20221213 21:25:39 @agent_ppo2.py:185][0m |          -0.0078 |         162.7184 |           7.2123 |
[32m[20221213 21:25:39 @agent_ppo2.py:185][0m |           0.0030 |         173.4668 |           7.2223 |
[32m[20221213 21:25:39 @agent_ppo2.py:185][0m |          -0.0085 |         162.4836 |           7.1620 |
[32m[20221213 21:25:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.40
[32m[20221213 21:25:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:25:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 21:25:39 @agent_ppo2.py:143][0m Total time:      30.07 min
[32m[20221213 21:25:39 @agent_ppo2.py:145][0m 2938880 total steps have happened
[32m[20221213 21:25:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1435 --------------------------#
[32m[20221213 21:25:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:39 @agent_ppo2.py:185][0m |           0.0003 |         166.6989 |           7.4780 |
[32m[20221213 21:25:40 @agent_ppo2.py:185][0m |           0.0024 |         167.8565 |           7.4877 |
[32m[20221213 21:25:40 @agent_ppo2.py:185][0m |           0.0001 |         166.6726 |           7.5014 |
[32m[20221213 21:25:40 @agent_ppo2.py:185][0m |          -0.0080 |         164.4971 |           7.5581 |
[32m[20221213 21:25:40 @agent_ppo2.py:185][0m |          -0.0090 |         164.0981 |           7.5795 |
[32m[20221213 21:25:40 @agent_ppo2.py:185][0m |          -0.0060 |         164.4643 |           7.5977 |
[32m[20221213 21:25:40 @agent_ppo2.py:185][0m |          -0.0083 |         163.6726 |           7.6419 |
[32m[20221213 21:25:40 @agent_ppo2.py:185][0m |          -0.0089 |         163.6187 |           7.6195 |
[32m[20221213 21:25:40 @agent_ppo2.py:185][0m |          -0.0088 |         163.2685 |           7.6561 |
[32m[20221213 21:25:40 @agent_ppo2.py:185][0m |          -0.0087 |         163.2119 |           7.6502 |
[32m[20221213 21:25:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.40
[32m[20221213 21:25:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:25:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.00
[32m[20221213 21:25:40 @agent_ppo2.py:143][0m Total time:      30.09 min
[32m[20221213 21:25:40 @agent_ppo2.py:145][0m 2940928 total steps have happened
[32m[20221213 21:25:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1436 --------------------------#
[32m[20221213 21:25:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:41 @agent_ppo2.py:185][0m |          -0.0044 |         165.6668 |           7.5229 |
[32m[20221213 21:25:41 @agent_ppo2.py:185][0m |          -0.0055 |         163.8881 |           7.5336 |
[32m[20221213 21:25:41 @agent_ppo2.py:185][0m |          -0.0063 |         162.6324 |           7.5471 |
[32m[20221213 21:25:41 @agent_ppo2.py:185][0m |           0.0015 |         167.6426 |           7.5675 |
[32m[20221213 21:25:41 @agent_ppo2.py:185][0m |           0.0039 |         181.9581 |           7.5457 |
[32m[20221213 21:25:41 @agent_ppo2.py:185][0m |           0.0084 |         180.7689 |           7.7339 |
[32m[20221213 21:25:41 @agent_ppo2.py:185][0m |          -0.0099 |         158.0010 |           7.5513 |
[32m[20221213 21:25:41 @agent_ppo2.py:185][0m |          -0.0109 |         157.0449 |           7.5691 |
[32m[20221213 21:25:41 @agent_ppo2.py:185][0m |          -0.0090 |         155.8931 |           7.5596 |
[32m[20221213 21:25:41 @agent_ppo2.py:185][0m |          -0.0112 |         155.0235 |           7.5915 |
[32m[20221213 21:25:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:25:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.80
[32m[20221213 21:25:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:25:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.00
[32m[20221213 21:25:42 @agent_ppo2.py:143][0m Total time:      30.11 min
[32m[20221213 21:25:42 @agent_ppo2.py:145][0m 2942976 total steps have happened
[32m[20221213 21:25:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1437 --------------------------#
[32m[20221213 21:25:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:42 @agent_ppo2.py:185][0m |          -0.0003 |         168.8696 |           7.4930 |
[32m[20221213 21:25:42 @agent_ppo2.py:185][0m |          -0.0038 |         166.0887 |           7.5188 |
[32m[20221213 21:25:42 @agent_ppo2.py:185][0m |          -0.0048 |         165.3276 |           7.4787 |
[32m[20221213 21:25:42 @agent_ppo2.py:185][0m |          -0.0070 |         164.6762 |           7.4841 |
[32m[20221213 21:25:42 @agent_ppo2.py:185][0m |          -0.0079 |         164.2844 |           7.4740 |
[32m[20221213 21:25:42 @agent_ppo2.py:185][0m |          -0.0069 |         164.0927 |           7.4830 |
[32m[20221213 21:25:42 @agent_ppo2.py:185][0m |          -0.0141 |         163.8695 |           7.4483 |
[32m[20221213 21:25:42 @agent_ppo2.py:185][0m |          -0.0095 |         163.6469 |           7.4359 |
[32m[20221213 21:25:43 @agent_ppo2.py:185][0m |          -0.0072 |         163.4269 |           7.3917 |
[32m[20221213 21:25:43 @agent_ppo2.py:185][0m |          -0.0054 |         166.1860 |           7.4344 |
[32m[20221213 21:25:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:25:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.40
[32m[20221213 21:25:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.00
[32m[20221213 21:25:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.00
[32m[20221213 21:25:43 @agent_ppo2.py:143][0m Total time:      30.13 min
[32m[20221213 21:25:43 @agent_ppo2.py:145][0m 2945024 total steps have happened
[32m[20221213 21:25:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1438 --------------------------#
[32m[20221213 21:25:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:43 @agent_ppo2.py:185][0m |          -0.0016 |         168.2446 |           7.2933 |
[32m[20221213 21:25:43 @agent_ppo2.py:185][0m |          -0.0008 |         169.4615 |           7.2743 |
[32m[20221213 21:25:43 @agent_ppo2.py:185][0m |          -0.0050 |         165.6727 |           7.2623 |
[32m[20221213 21:25:43 @agent_ppo2.py:185][0m |          -0.0074 |         165.2241 |           7.2983 |
[32m[20221213 21:25:43 @agent_ppo2.py:185][0m |          -0.0075 |         164.8411 |           7.3069 |
[32m[20221213 21:25:43 @agent_ppo2.py:185][0m |          -0.0088 |         164.5873 |           7.3047 |
[32m[20221213 21:25:44 @agent_ppo2.py:185][0m |          -0.0109 |         164.4112 |           7.3795 |
[32m[20221213 21:25:44 @agent_ppo2.py:185][0m |          -0.0080 |         164.7533 |           7.4131 |
[32m[20221213 21:25:44 @agent_ppo2.py:185][0m |          -0.0104 |         164.1121 |           7.4221 |
[32m[20221213 21:25:44 @agent_ppo2.py:185][0m |          -0.0076 |         164.0448 |           7.4548 |
[32m[20221213 21:25:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.40
[32m[20221213 21:25:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.00
[32m[20221213 21:25:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.00
[32m[20221213 21:25:44 @agent_ppo2.py:143][0m Total time:      30.15 min
[32m[20221213 21:25:44 @agent_ppo2.py:145][0m 2947072 total steps have happened
[32m[20221213 21:25:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1439 --------------------------#
[32m[20221213 21:25:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:44 @agent_ppo2.py:185][0m |          -0.0032 |         166.8646 |           7.1724 |
[32m[20221213 21:25:44 @agent_ppo2.py:185][0m |          -0.0068 |         165.4485 |           7.1659 |
[32m[20221213 21:25:44 @agent_ppo2.py:185][0m |           0.0026 |         177.8531 |           7.1161 |
[32m[20221213 21:25:45 @agent_ppo2.py:185][0m |          -0.0064 |         164.1865 |           7.1664 |
[32m[20221213 21:25:45 @agent_ppo2.py:185][0m |          -0.0090 |         163.5960 |           7.2179 |
[32m[20221213 21:25:45 @agent_ppo2.py:185][0m |          -0.0042 |         164.6574 |           7.2035 |
[32m[20221213 21:25:45 @agent_ppo2.py:185][0m |          -0.0065 |         163.0547 |           7.2005 |
[32m[20221213 21:25:45 @agent_ppo2.py:185][0m |          -0.0064 |         162.7403 |           7.2089 |
[32m[20221213 21:25:45 @agent_ppo2.py:185][0m |          -0.0011 |         167.5233 |           7.2245 |
[32m[20221213 21:25:45 @agent_ppo2.py:185][0m |          -0.0101 |         162.2608 |           7.2422 |
[32m[20221213 21:25:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.00
[32m[20221213 21:25:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 768.00
[32m[20221213 21:25:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.00
[32m[20221213 21:25:45 @agent_ppo2.py:143][0m Total time:      30.17 min
[32m[20221213 21:25:45 @agent_ppo2.py:145][0m 2949120 total steps have happened
[32m[20221213 21:25:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1440 --------------------------#
[32m[20221213 21:25:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:25:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:45 @agent_ppo2.py:185][0m |           0.0003 |         165.1189 |           7.2059 |
[32m[20221213 21:25:46 @agent_ppo2.py:185][0m |          -0.0024 |         164.4839 |           7.3064 |
[32m[20221213 21:25:46 @agent_ppo2.py:185][0m |          -0.0033 |         164.0764 |           7.2200 |
[32m[20221213 21:25:46 @agent_ppo2.py:185][0m |          -0.0041 |         163.9007 |           7.2704 |
[32m[20221213 21:25:46 @agent_ppo2.py:185][0m |          -0.0017 |         163.5261 |           7.2618 |
[32m[20221213 21:25:46 @agent_ppo2.py:185][0m |          -0.0060 |         163.3545 |           7.2549 |
[32m[20221213 21:25:46 @agent_ppo2.py:185][0m |          -0.0070 |         163.3141 |           7.2948 |
[32m[20221213 21:25:46 @agent_ppo2.py:185][0m |          -0.0071 |         163.3010 |           7.2979 |
[32m[20221213 21:25:46 @agent_ppo2.py:185][0m |          -0.0068 |         163.0330 |           7.3028 |
[32m[20221213 21:25:46 @agent_ppo2.py:185][0m |          -0.0070 |         162.8974 |           7.3047 |
[32m[20221213 21:25:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:25:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.20
[32m[20221213 21:25:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:25:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.00
[32m[20221213 21:25:46 @agent_ppo2.py:143][0m Total time:      30.19 min
[32m[20221213 21:25:46 @agent_ppo2.py:145][0m 2951168 total steps have happened
[32m[20221213 21:25:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1441 --------------------------#
[32m[20221213 21:25:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:47 @agent_ppo2.py:185][0m |           0.0019 |         163.5099 |           7.6488 |
[32m[20221213 21:25:47 @agent_ppo2.py:185][0m |           0.0000 |         163.3217 |           7.6343 |
[32m[20221213 21:25:47 @agent_ppo2.py:185][0m |          -0.0052 |         160.9959 |           7.6085 |
[32m[20221213 21:25:47 @agent_ppo2.py:185][0m |          -0.0079 |         160.1468 |           7.5555 |
[32m[20221213 21:25:47 @agent_ppo2.py:185][0m |           0.0004 |         162.3348 |           7.5878 |
[32m[20221213 21:25:47 @agent_ppo2.py:185][0m |          -0.0072 |         159.1832 |           7.5235 |
[32m[20221213 21:25:47 @agent_ppo2.py:185][0m |          -0.0097 |         158.8678 |           7.5284 |
[32m[20221213 21:25:47 @agent_ppo2.py:185][0m |          -0.0087 |         158.4555 |           7.5501 |
[32m[20221213 21:25:47 @agent_ppo2.py:185][0m |          -0.0066 |         158.4629 |           7.5460 |
[32m[20221213 21:25:47 @agent_ppo2.py:185][0m |          -0.0084 |         157.9500 |           7.5179 |
[32m[20221213 21:25:47 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:25:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 727.60
[32m[20221213 21:25:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.00
[32m[20221213 21:25:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.00
[32m[20221213 21:25:48 @agent_ppo2.py:143][0m Total time:      30.21 min
[32m[20221213 21:25:48 @agent_ppo2.py:145][0m 2953216 total steps have happened
[32m[20221213 21:25:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1442 --------------------------#
[32m[20221213 21:25:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:48 @agent_ppo2.py:185][0m |           0.0004 |         164.3383 |           7.5166 |
[32m[20221213 21:25:48 @agent_ppo2.py:185][0m |          -0.0001 |         162.3583 |           7.5294 |
[32m[20221213 21:25:48 @agent_ppo2.py:185][0m |          -0.0058 |         161.0793 |           7.5429 |
[32m[20221213 21:25:48 @agent_ppo2.py:185][0m |          -0.0037 |         160.7966 |           7.4856 |
[32m[20221213 21:25:48 @agent_ppo2.py:185][0m |          -0.0060 |         159.7547 |           7.5517 |
[32m[20221213 21:25:48 @agent_ppo2.py:185][0m |          -0.0069 |         159.4365 |           7.4706 |
[32m[20221213 21:25:48 @agent_ppo2.py:185][0m |          -0.0079 |         159.0643 |           7.5033 |
[32m[20221213 21:25:48 @agent_ppo2.py:185][0m |          -0.0085 |         158.8350 |           7.4945 |
[32m[20221213 21:25:49 @agent_ppo2.py:185][0m |           0.0021 |         172.6921 |           7.5034 |
[32m[20221213 21:25:49 @agent_ppo2.py:185][0m |          -0.0060 |         158.4638 |           7.5127 |
[32m[20221213 21:25:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:25:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.20
[32m[20221213 21:25:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.00
[32m[20221213 21:25:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.00
[32m[20221213 21:25:49 @agent_ppo2.py:143][0m Total time:      30.23 min
[32m[20221213 21:25:49 @agent_ppo2.py:145][0m 2955264 total steps have happened
[32m[20221213 21:25:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1443 --------------------------#
[32m[20221213 21:25:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:25:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:49 @agent_ppo2.py:185][0m |           0.0013 |         166.1948 |           7.4241 |
[32m[20221213 21:25:49 @agent_ppo2.py:185][0m |          -0.0033 |         163.1033 |           7.3984 |
[32m[20221213 21:25:49 @agent_ppo2.py:185][0m |          -0.0056 |         161.4116 |           7.4348 |
[32m[20221213 21:25:49 @agent_ppo2.py:185][0m |           0.0025 |         169.4098 |           7.3961 |
[32m[20221213 21:25:49 @agent_ppo2.py:185][0m |          -0.0072 |         159.6815 |           7.4291 |
[32m[20221213 21:25:50 @agent_ppo2.py:185][0m |          -0.0013 |         167.9600 |           7.4061 |
[32m[20221213 21:25:50 @agent_ppo2.py:185][0m |          -0.0042 |         160.1891 |           7.4072 |
[32m[20221213 21:25:50 @agent_ppo2.py:185][0m |          -0.0083 |         158.2033 |           7.3622 |
[32m[20221213 21:25:50 @agent_ppo2.py:185][0m |          -0.0088 |         158.0850 |           7.4284 |
[32m[20221213 21:25:50 @agent_ppo2.py:185][0m |          -0.0074 |         158.3204 |           7.3820 |
[32m[20221213 21:25:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.60
[32m[20221213 21:25:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.00
[32m[20221213 21:25:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:25:50 @agent_ppo2.py:143][0m Total time:      30.25 min
[32m[20221213 21:25:50 @agent_ppo2.py:145][0m 2957312 total steps have happened
[32m[20221213 21:25:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1444 --------------------------#
[32m[20221213 21:25:50 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:25:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:50 @agent_ppo2.py:185][0m |           0.0001 |         176.7437 |           7.3865 |
[32m[20221213 21:25:50 @agent_ppo2.py:185][0m |           0.0018 |         175.6371 |           7.3863 |
[32m[20221213 21:25:50 @agent_ppo2.py:185][0m |          -0.0045 |         169.9790 |           7.3048 |
[32m[20221213 21:25:51 @agent_ppo2.py:185][0m |          -0.0036 |         168.7490 |           7.3211 |
[32m[20221213 21:25:51 @agent_ppo2.py:185][0m |          -0.0036 |         169.5558 |           7.2663 |
[32m[20221213 21:25:51 @agent_ppo2.py:185][0m |          -0.0066 |         166.9552 |           7.2176 |
[32m[20221213 21:25:51 @agent_ppo2.py:185][0m |          -0.0069 |         166.4366 |           7.2733 |
[32m[20221213 21:25:51 @agent_ppo2.py:185][0m |          -0.0086 |         165.9734 |           7.2166 |
[32m[20221213 21:25:51 @agent_ppo2.py:185][0m |          -0.0098 |         165.7527 |           7.2081 |
[32m[20221213 21:25:51 @agent_ppo2.py:185][0m |          -0.0080 |         165.2296 |           7.2141 |
[32m[20221213 21:25:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.80
[32m[20221213 21:25:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.00
[32m[20221213 21:25:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:25:51 @agent_ppo2.py:143][0m Total time:      30.27 min
[32m[20221213 21:25:51 @agent_ppo2.py:145][0m 2959360 total steps have happened
[32m[20221213 21:25:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1445 --------------------------#
[32m[20221213 21:25:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:51 @agent_ppo2.py:185][0m |          -0.0002 |         168.2353 |           7.1475 |
[32m[20221213 21:25:52 @agent_ppo2.py:185][0m |           0.0021 |         170.3779 |           7.1214 |
[32m[20221213 21:25:52 @agent_ppo2.py:185][0m |          -0.0049 |         165.6454 |           7.1256 |
[32m[20221213 21:25:52 @agent_ppo2.py:185][0m |          -0.0053 |         165.0612 |           7.1795 |
[32m[20221213 21:25:52 @agent_ppo2.py:185][0m |           0.0066 |         176.6655 |           7.2231 |
[32m[20221213 21:25:52 @agent_ppo2.py:185][0m |          -0.0041 |         164.3788 |           7.2957 |
[32m[20221213 21:25:52 @agent_ppo2.py:185][0m |          -0.0068 |         164.2451 |           7.1960 |
[32m[20221213 21:25:52 @agent_ppo2.py:185][0m |          -0.0056 |         163.9803 |           7.2138 |
[32m[20221213 21:25:52 @agent_ppo2.py:185][0m |          -0.0066 |         163.8637 |           7.2363 |
[32m[20221213 21:25:52 @agent_ppo2.py:185][0m |          -0.0007 |         165.6232 |           7.2706 |
[32m[20221213 21:25:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.00
[32m[20221213 21:25:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 21:25:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.00
[32m[20221213 21:25:52 @agent_ppo2.py:143][0m Total time:      30.29 min
[32m[20221213 21:25:52 @agent_ppo2.py:145][0m 2961408 total steps have happened
[32m[20221213 21:25:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1446 --------------------------#
[32m[20221213 21:25:52 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:25:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:53 @agent_ppo2.py:185][0m |          -0.0030 |         165.7793 |           7.5854 |
[32m[20221213 21:25:53 @agent_ppo2.py:185][0m |          -0.0037 |         165.1861 |           7.5895 |
[32m[20221213 21:25:53 @agent_ppo2.py:185][0m |          -0.0072 |         164.8063 |           7.5958 |
[32m[20221213 21:25:53 @agent_ppo2.py:185][0m |          -0.0003 |         169.5707 |           7.5651 |
[32m[20221213 21:25:53 @agent_ppo2.py:185][0m |          -0.0048 |         165.2094 |           7.5426 |
[32m[20221213 21:25:53 @agent_ppo2.py:185][0m |          -0.0094 |         163.8388 |           7.5822 |
[32m[20221213 21:25:53 @agent_ppo2.py:185][0m |          -0.0072 |         163.5361 |           7.5720 |
[32m[20221213 21:25:53 @agent_ppo2.py:185][0m |          -0.0092 |         163.4569 |           7.5631 |
[32m[20221213 21:25:53 @agent_ppo2.py:185][0m |          -0.0108 |         163.1257 |           7.5124 |
[32m[20221213 21:25:53 @agent_ppo2.py:185][0m |          -0.0092 |         163.1212 |           7.5736 |
[32m[20221213 21:25:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:25:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.00
[32m[20221213 21:25:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.00
[32m[20221213 21:25:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 791.00
[32m[20221213 21:25:54 @agent_ppo2.py:143][0m Total time:      30.31 min
[32m[20221213 21:25:54 @agent_ppo2.py:145][0m 2963456 total steps have happened
[32m[20221213 21:25:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1447 --------------------------#
[32m[20221213 21:25:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:54 @agent_ppo2.py:185][0m |           0.0043 |         167.4131 |           6.8830 |
[32m[20221213 21:25:54 @agent_ppo2.py:185][0m |           0.0003 |         161.9630 |           7.0526 |
[32m[20221213 21:25:54 @agent_ppo2.py:185][0m |          -0.0045 |         161.4689 |           6.9647 |
[32m[20221213 21:25:54 @agent_ppo2.py:185][0m |          -0.0055 |         161.0760 |           6.9931 |
[32m[20221213 21:25:54 @agent_ppo2.py:185][0m |          -0.0057 |         160.7771 |           6.9825 |
[32m[20221213 21:25:54 @agent_ppo2.py:185][0m |          -0.0012 |         167.8847 |           6.9969 |
[32m[20221213 21:25:54 @agent_ppo2.py:185][0m |          -0.0019 |         162.4673 |           7.0672 |
[32m[20221213 21:25:54 @agent_ppo2.py:185][0m |          -0.0078 |         160.2239 |           7.0039 |
[32m[20221213 21:25:55 @agent_ppo2.py:185][0m |          -0.0088 |         159.9547 |           7.0342 |
[32m[20221213 21:25:55 @agent_ppo2.py:185][0m |          -0.0021 |         165.8220 |           7.0036 |
[32m[20221213 21:25:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:25:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.60
[32m[20221213 21:25:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.00
[32m[20221213 21:25:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.00
[32m[20221213 21:25:55 @agent_ppo2.py:143][0m Total time:      30.33 min
[32m[20221213 21:25:55 @agent_ppo2.py:145][0m 2965504 total steps have happened
[32m[20221213 21:25:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1448 --------------------------#
[32m[20221213 21:25:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:55 @agent_ppo2.py:185][0m |           0.0006 |         162.4033 |           7.0372 |
[32m[20221213 21:25:55 @agent_ppo2.py:185][0m |          -0.0033 |         154.1630 |           7.0952 |
[32m[20221213 21:25:55 @agent_ppo2.py:185][0m |          -0.0083 |         150.3518 |           7.0932 |
[32m[20221213 21:25:55 @agent_ppo2.py:185][0m |          -0.0083 |         145.8883 |           7.1495 |
[32m[20221213 21:25:55 @agent_ppo2.py:185][0m |          -0.0087 |         144.0651 |           7.0789 |
[32m[20221213 21:25:56 @agent_ppo2.py:185][0m |          -0.0107 |         141.5091 |           7.0560 |
[32m[20221213 21:25:56 @agent_ppo2.py:185][0m |          -0.0135 |         139.0106 |           7.0399 |
[32m[20221213 21:25:56 @agent_ppo2.py:185][0m |          -0.0111 |         137.2848 |           7.0523 |
[32m[20221213 21:25:56 @agent_ppo2.py:185][0m |          -0.0106 |         136.2577 |           7.0470 |
[32m[20221213 21:25:56 @agent_ppo2.py:185][0m |          -0.0125 |         135.7172 |           7.0662 |
[32m[20221213 21:25:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.20
[32m[20221213 21:25:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.00
[32m[20221213 21:25:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.00
[32m[20221213 21:25:56 @agent_ppo2.py:143][0m Total time:      30.35 min
[32m[20221213 21:25:56 @agent_ppo2.py:145][0m 2967552 total steps have happened
[32m[20221213 21:25:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1449 --------------------------#
[32m[20221213 21:25:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:56 @agent_ppo2.py:185][0m |           0.0014 |         185.3641 |           7.1890 |
[32m[20221213 21:25:56 @agent_ppo2.py:185][0m |          -0.0059 |         179.4888 |           7.2255 |
[32m[20221213 21:25:56 @agent_ppo2.py:185][0m |          -0.0027 |         178.1296 |           7.2137 |
[32m[20221213 21:25:57 @agent_ppo2.py:185][0m |          -0.0061 |         176.8590 |           7.2491 |
[32m[20221213 21:25:57 @agent_ppo2.py:185][0m |          -0.0030 |         177.3074 |           7.2804 |
[32m[20221213 21:25:57 @agent_ppo2.py:185][0m |          -0.0053 |         175.5944 |           7.3440 |
[32m[20221213 21:25:57 @agent_ppo2.py:185][0m |          -0.0088 |         175.5467 |           7.2939 |
[32m[20221213 21:25:57 @agent_ppo2.py:185][0m |          -0.0076 |         174.8570 |           7.3414 |
[32m[20221213 21:25:57 @agent_ppo2.py:185][0m |          -0.0090 |         174.6802 |           7.3839 |
[32m[20221213 21:25:57 @agent_ppo2.py:185][0m |          -0.0082 |         174.2885 |           7.4265 |
[32m[20221213 21:25:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 752.80
[32m[20221213 21:25:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.00
[32m[20221213 21:25:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 21:25:57 @agent_ppo2.py:143][0m Total time:      30.37 min
[32m[20221213 21:25:57 @agent_ppo2.py:145][0m 2969600 total steps have happened
[32m[20221213 21:25:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1450 --------------------------#
[32m[20221213 21:25:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:25:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:57 @agent_ppo2.py:185][0m |          -0.0001 |         168.6249 |           7.2691 |
[32m[20221213 21:25:58 @agent_ppo2.py:185][0m |           0.0021 |         169.9407 |           7.2235 |
[32m[20221213 21:25:58 @agent_ppo2.py:185][0m |          -0.0054 |         166.0090 |           7.2304 |
[32m[20221213 21:25:58 @agent_ppo2.py:185][0m |          -0.0064 |         165.4133 |           7.1908 |
[32m[20221213 21:25:58 @agent_ppo2.py:185][0m |          -0.0063 |         165.0042 |           7.2030 |
[32m[20221213 21:25:58 @agent_ppo2.py:185][0m |          -0.0066 |         164.4848 |           7.1712 |
[32m[20221213 21:25:58 @agent_ppo2.py:185][0m |          -0.0098 |         164.2224 |           7.1549 |
[32m[20221213 21:25:58 @agent_ppo2.py:185][0m |          -0.0092 |         164.0538 |           7.1366 |
[32m[20221213 21:25:58 @agent_ppo2.py:185][0m |          -0.0107 |         163.8338 |           7.1697 |
[32m[20221213 21:25:58 @agent_ppo2.py:185][0m |          -0.0058 |         164.9894 |           7.1016 |
[32m[20221213 21:25:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:25:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.20
[32m[20221213 21:25:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.00
[32m[20221213 21:25:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 21:25:58 @agent_ppo2.py:143][0m Total time:      30.39 min
[32m[20221213 21:25:58 @agent_ppo2.py:145][0m 2971648 total steps have happened
[32m[20221213 21:25:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1451 --------------------------#
[32m[20221213 21:25:58 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:25:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:25:59 @agent_ppo2.py:185][0m |           0.0003 |         164.9104 |           6.9780 |
[32m[20221213 21:25:59 @agent_ppo2.py:185][0m |           0.0011 |         163.5765 |           6.8747 |
[32m[20221213 21:25:59 @agent_ppo2.py:185][0m |          -0.0034 |         162.7607 |           6.8916 |
[32m[20221213 21:25:59 @agent_ppo2.py:185][0m |          -0.0057 |         162.3133 |           6.8757 |
[32m[20221213 21:25:59 @agent_ppo2.py:185][0m |          -0.0039 |         162.2127 |           6.9229 |
[32m[20221213 21:25:59 @agent_ppo2.py:185][0m |          -0.0039 |         161.8327 |           6.9486 |
[32m[20221213 21:25:59 @agent_ppo2.py:185][0m |          -0.0052 |         161.6767 |           6.9940 |
[32m[20221213 21:25:59 @agent_ppo2.py:185][0m |          -0.0043 |         162.0412 |           6.9290 |
[32m[20221213 21:25:59 @agent_ppo2.py:185][0m |           0.0012 |         168.7093 |           6.9353 |
[32m[20221213 21:25:59 @agent_ppo2.py:185][0m |           0.0015 |         170.4616 |           7.0067 |
[32m[20221213 21:25:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:26:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.40
[32m[20221213 21:26:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.00
[32m[20221213 21:26:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.00
[32m[20221213 21:26:00 @agent_ppo2.py:143][0m Total time:      30.41 min
[32m[20221213 21:26:00 @agent_ppo2.py:145][0m 2973696 total steps have happened
[32m[20221213 21:26:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1452 --------------------------#
[32m[20221213 21:26:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:00 @agent_ppo2.py:185][0m |          -0.0004 |         165.1247 |           7.1325 |
[32m[20221213 21:26:00 @agent_ppo2.py:185][0m |           0.0061 |         179.6806 |           7.2336 |
[32m[20221213 21:26:00 @agent_ppo2.py:185][0m |          -0.0077 |         162.2615 |           7.3169 |
[32m[20221213 21:26:00 @agent_ppo2.py:185][0m |          -0.0069 |         161.6349 |           7.3149 |
[32m[20221213 21:26:00 @agent_ppo2.py:185][0m |          -0.0073 |         161.3588 |           7.2508 |
[32m[20221213 21:26:00 @agent_ppo2.py:185][0m |          -0.0102 |         161.1748 |           7.3075 |
[32m[20221213 21:26:00 @agent_ppo2.py:185][0m |          -0.0066 |         162.8161 |           7.3138 |
[32m[20221213 21:26:00 @agent_ppo2.py:185][0m |          -0.0083 |         160.7796 |           7.3362 |
[32m[20221213 21:26:01 @agent_ppo2.py:185][0m |          -0.0097 |         160.6508 |           7.3090 |
[32m[20221213 21:26:01 @agent_ppo2.py:185][0m |          -0.0108 |         160.5492 |           7.3087 |
[32m[20221213 21:26:01 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:26:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.40
[32m[20221213 21:26:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:26:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:26:01 @agent_ppo2.py:143][0m Total time:      30.43 min
[32m[20221213 21:26:01 @agent_ppo2.py:145][0m 2975744 total steps have happened
[32m[20221213 21:26:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1453 --------------------------#
[32m[20221213 21:26:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:26:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:01 @agent_ppo2.py:185][0m |          -0.0006 |         166.0434 |           7.0624 |
[32m[20221213 21:26:01 @agent_ppo2.py:185][0m |          -0.0056 |         164.8713 |           7.0519 |
[32m[20221213 21:26:01 @agent_ppo2.py:185][0m |           0.0008 |         171.6278 |           7.1304 |
[32m[20221213 21:26:01 @agent_ppo2.py:185][0m |          -0.0050 |         164.1631 |           7.1085 |
[32m[20221213 21:26:01 @agent_ppo2.py:185][0m |          -0.0065 |         163.8324 |           7.1419 |
[32m[20221213 21:26:02 @agent_ppo2.py:185][0m |          -0.0049 |         163.6001 |           7.2084 |
[32m[20221213 21:26:02 @agent_ppo2.py:185][0m |          -0.0083 |         163.3783 |           7.2242 |
[32m[20221213 21:26:02 @agent_ppo2.py:185][0m |          -0.0086 |         163.4186 |           7.2389 |
[32m[20221213 21:26:02 @agent_ppo2.py:185][0m |          -0.0082 |         163.1583 |           7.2938 |
[32m[20221213 21:26:02 @agent_ppo2.py:185][0m |          -0.0080 |         162.9904 |           7.2571 |
[32m[20221213 21:26:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 720.80
[32m[20221213 21:26:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.00
[32m[20221213 21:26:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 783.00
[32m[20221213 21:26:02 @agent_ppo2.py:143][0m Total time:      30.45 min
[32m[20221213 21:26:02 @agent_ppo2.py:145][0m 2977792 total steps have happened
[32m[20221213 21:26:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1454 --------------------------#
[32m[20221213 21:26:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:02 @agent_ppo2.py:185][0m |          -0.0021 |         166.4785 |           7.8844 |
[32m[20221213 21:26:02 @agent_ppo2.py:185][0m |          -0.0031 |         165.7043 |           7.8601 |
[32m[20221213 21:26:02 @agent_ppo2.py:185][0m |          -0.0064 |         165.2065 |           7.8658 |
[32m[20221213 21:26:03 @agent_ppo2.py:185][0m |          -0.0080 |         164.8298 |           7.8658 |
[32m[20221213 21:26:03 @agent_ppo2.py:185][0m |          -0.0073 |         164.6883 |           7.8896 |
[32m[20221213 21:26:03 @agent_ppo2.py:185][0m |          -0.0047 |         165.3195 |           7.9302 |
[32m[20221213 21:26:03 @agent_ppo2.py:185][0m |          -0.0077 |         164.3400 |           7.9136 |
[32m[20221213 21:26:03 @agent_ppo2.py:185][0m |          -0.0091 |         164.0752 |           7.9228 |
[32m[20221213 21:26:03 @agent_ppo2.py:185][0m |          -0.0079 |         163.9400 |           7.9185 |
[32m[20221213 21:26:03 @agent_ppo2.py:185][0m |          -0.0055 |         164.8815 |           7.9316 |
[32m[20221213 21:26:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 763.20
[32m[20221213 21:26:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.00
[32m[20221213 21:26:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.00
[32m[20221213 21:26:03 @agent_ppo2.py:143][0m Total time:      30.47 min
[32m[20221213 21:26:03 @agent_ppo2.py:145][0m 2979840 total steps have happened
[32m[20221213 21:26:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1455 --------------------------#
[32m[20221213 21:26:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:03 @agent_ppo2.py:185][0m |          -0.0014 |         164.1590 |           7.1194 |
[32m[20221213 21:26:04 @agent_ppo2.py:185][0m |          -0.0058 |         161.9221 |           7.1565 |
[32m[20221213 21:26:04 @agent_ppo2.py:185][0m |          -0.0070 |         161.3033 |           7.1544 |
[32m[20221213 21:26:04 @agent_ppo2.py:185][0m |          -0.0018 |         164.4141 |           7.1144 |
[32m[20221213 21:26:04 @agent_ppo2.py:185][0m |          -0.0081 |         160.4423 |           7.0803 |
[32m[20221213 21:26:04 @agent_ppo2.py:185][0m |          -0.0071 |         160.1120 |           7.0484 |
[32m[20221213 21:26:04 @agent_ppo2.py:185][0m |          -0.0069 |         159.8867 |           7.0631 |
[32m[20221213 21:26:04 @agent_ppo2.py:185][0m |          -0.0095 |         159.6901 |           7.0105 |
[32m[20221213 21:26:04 @agent_ppo2.py:185][0m |          -0.0098 |         159.6060 |           6.9838 |
[32m[20221213 21:26:04 @agent_ppo2.py:185][0m |          -0.0081 |         159.4418 |           6.9892 |
[32m[20221213 21:26:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.00
[32m[20221213 21:26:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.00
[32m[20221213 21:26:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 21:26:04 @agent_ppo2.py:143][0m Total time:      30.49 min
[32m[20221213 21:26:04 @agent_ppo2.py:145][0m 2981888 total steps have happened
[32m[20221213 21:26:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1456 --------------------------#
[32m[20221213 21:26:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:26:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:05 @agent_ppo2.py:185][0m |          -0.0004 |         162.5678 |           7.1263 |
[32m[20221213 21:26:05 @agent_ppo2.py:185][0m |           0.0114 |         174.6770 |           7.1458 |
[32m[20221213 21:26:05 @agent_ppo2.py:185][0m |           0.0121 |         183.2869 |           7.1511 |
[32m[20221213 21:26:05 @agent_ppo2.py:185][0m |          -0.0047 |         159.0338 |           7.1632 |
[32m[20221213 21:26:05 @agent_ppo2.py:185][0m |          -0.0041 |         158.6032 |           7.1160 |
[32m[20221213 21:26:05 @agent_ppo2.py:185][0m |           0.0043 |         161.9024 |           7.1830 |
[32m[20221213 21:26:05 @agent_ppo2.py:185][0m |          -0.0034 |         158.1571 |           7.1904 |
[32m[20221213 21:26:05 @agent_ppo2.py:185][0m |          -0.0074 |         157.8494 |           7.1796 |
[32m[20221213 21:26:05 @agent_ppo2.py:185][0m |          -0.0066 |         157.7225 |           7.1580 |
[32m[20221213 21:26:05 @agent_ppo2.py:185][0m |          -0.0089 |         157.5880 |           7.0940 |
[32m[20221213 21:26:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.80
[32m[20221213 21:26:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.00
[32m[20221213 21:26:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.00
[32m[20221213 21:26:06 @agent_ppo2.py:143][0m Total time:      30.51 min
[32m[20221213 21:26:06 @agent_ppo2.py:145][0m 2983936 total steps have happened
[32m[20221213 21:26:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1457 --------------------------#
[32m[20221213 21:26:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:06 @agent_ppo2.py:185][0m |          -0.0015 |         168.6687 |           7.5410 |
[32m[20221213 21:26:06 @agent_ppo2.py:185][0m |          -0.0045 |         165.6582 |           7.5232 |
[32m[20221213 21:26:06 @agent_ppo2.py:185][0m |          -0.0028 |         164.9908 |           7.5420 |
[32m[20221213 21:26:06 @agent_ppo2.py:185][0m |          -0.0036 |         164.1233 |           7.5698 |
[32m[20221213 21:26:06 @agent_ppo2.py:185][0m |          -0.0030 |         163.5417 |           7.6113 |
[32m[20221213 21:26:06 @agent_ppo2.py:185][0m |          -0.0045 |         163.3398 |           7.6006 |
[32m[20221213 21:26:06 @agent_ppo2.py:185][0m |          -0.0068 |         162.8182 |           7.6386 |
[32m[20221213 21:26:06 @agent_ppo2.py:185][0m |           0.0011 |         164.0780 |           7.6749 |
[32m[20221213 21:26:07 @agent_ppo2.py:185][0m |          -0.0084 |         162.4928 |           7.7005 |
[32m[20221213 21:26:07 @agent_ppo2.py:185][0m |          -0.0104 |         162.1893 |           7.6975 |
[32m[20221213 21:26:07 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:26:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 696.20
[32m[20221213 21:26:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.00
[32m[20221213 21:26:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:26:07 @agent_ppo2.py:143][0m Total time:      30.53 min
[32m[20221213 21:26:07 @agent_ppo2.py:145][0m 2985984 total steps have happened
[32m[20221213 21:26:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1458 --------------------------#
[32m[20221213 21:26:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:07 @agent_ppo2.py:185][0m |          -0.0005 |         163.9216 |           7.8419 |
[32m[20221213 21:26:07 @agent_ppo2.py:185][0m |          -0.0029 |         162.6740 |           7.7282 |
[32m[20221213 21:26:07 @agent_ppo2.py:185][0m |          -0.0082 |         161.6004 |           7.6971 |
[32m[20221213 21:26:07 @agent_ppo2.py:185][0m |          -0.0074 |         161.0469 |           7.7198 |
[32m[20221213 21:26:07 @agent_ppo2.py:185][0m |          -0.0089 |         160.6441 |           7.6993 |
[32m[20221213 21:26:08 @agent_ppo2.py:185][0m |          -0.0087 |         160.2402 |           7.7201 |
[32m[20221213 21:26:08 @agent_ppo2.py:185][0m |          -0.0109 |         160.0952 |           7.7053 |
[32m[20221213 21:26:08 @agent_ppo2.py:185][0m |          -0.0094 |         159.7019 |           7.7081 |
[32m[20221213 21:26:08 @agent_ppo2.py:185][0m |          -0.0093 |         159.5870 |           7.7307 |
[32m[20221213 21:26:08 @agent_ppo2.py:185][0m |          -0.0089 |         159.3261 |           7.7055 |
[32m[20221213 21:26:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.00
[32m[20221213 21:26:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:26:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.00
[32m[20221213 21:26:08 @agent_ppo2.py:143][0m Total time:      30.55 min
[32m[20221213 21:26:08 @agent_ppo2.py:145][0m 2988032 total steps have happened
[32m[20221213 21:26:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1459 --------------------------#
[32m[20221213 21:26:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:08 @agent_ppo2.py:185][0m |          -0.0009 |         164.7285 |           7.8042 |
[32m[20221213 21:26:08 @agent_ppo2.py:185][0m |          -0.0043 |         163.5832 |           7.8450 |
[32m[20221213 21:26:08 @agent_ppo2.py:185][0m |          -0.0064 |         163.0954 |           7.8426 |
[32m[20221213 21:26:09 @agent_ppo2.py:185][0m |          -0.0003 |         165.5551 |           7.8216 |
[32m[20221213 21:26:09 @agent_ppo2.py:185][0m |          -0.0088 |         162.7285 |           7.8546 |
[32m[20221213 21:26:09 @agent_ppo2.py:185][0m |          -0.0065 |         162.5155 |           7.8803 |
[32m[20221213 21:26:09 @agent_ppo2.py:185][0m |          -0.0076 |         162.3543 |           7.8943 |
[32m[20221213 21:26:09 @agent_ppo2.py:185][0m |          -0.0013 |         170.7690 |           7.9248 |
[32m[20221213 21:26:09 @agent_ppo2.py:185][0m |          -0.0072 |         162.0921 |           7.9631 |
[32m[20221213 21:26:09 @agent_ppo2.py:185][0m |          -0.0038 |         165.6027 |           7.9330 |
[32m[20221213 21:26:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:26:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.80
[32m[20221213 21:26:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.00
[32m[20221213 21:26:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.00
[32m[20221213 21:26:09 @agent_ppo2.py:143][0m Total time:      30.57 min
[32m[20221213 21:26:09 @agent_ppo2.py:145][0m 2990080 total steps have happened
[32m[20221213 21:26:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1460 --------------------------#
[32m[20221213 21:26:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:09 @agent_ppo2.py:185][0m |          -0.0004 |         161.8552 |           7.9181 |
[32m[20221213 21:26:10 @agent_ppo2.py:185][0m |          -0.0038 |         160.2125 |           7.9901 |
[32m[20221213 21:26:10 @agent_ppo2.py:185][0m |          -0.0055 |         159.7541 |           7.9515 |
[32m[20221213 21:26:10 @agent_ppo2.py:185][0m |          -0.0060 |         159.3101 |           7.9363 |
[32m[20221213 21:26:10 @agent_ppo2.py:185][0m |          -0.0065 |         159.1454 |           7.8975 |
[32m[20221213 21:26:10 @agent_ppo2.py:185][0m |          -0.0009 |         161.9820 |           7.9186 |
[32m[20221213 21:26:10 @agent_ppo2.py:185][0m |          -0.0100 |         159.0224 |           7.9138 |
[32m[20221213 21:26:10 @agent_ppo2.py:185][0m |          -0.0082 |         158.6836 |           7.9132 |
[32m[20221213 21:26:10 @agent_ppo2.py:185][0m |          -0.0094 |         158.5641 |           7.8742 |
[32m[20221213 21:26:10 @agent_ppo2.py:185][0m |          -0.0087 |         158.4783 |           7.8870 |
[32m[20221213 21:26:10 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:26:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 772.20
[32m[20221213 21:26:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.00
[32m[20221213 21:26:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:26:10 @agent_ppo2.py:143][0m Total time:      30.59 min
[32m[20221213 21:26:10 @agent_ppo2.py:145][0m 2992128 total steps have happened
[32m[20221213 21:26:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1461 --------------------------#
[32m[20221213 21:26:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:11 @agent_ppo2.py:185][0m |           0.0153 |         181.5806 |           7.9109 |
[32m[20221213 21:26:11 @agent_ppo2.py:185][0m |           0.0073 |         171.1791 |           8.0329 |
[32m[20221213 21:26:11 @agent_ppo2.py:185][0m |           0.0038 |         171.3299 |           7.9590 |
[32m[20221213 21:26:11 @agent_ppo2.py:185][0m |          -0.0056 |         160.5812 |           7.9651 |
[32m[20221213 21:26:11 @agent_ppo2.py:185][0m |          -0.0073 |         160.3534 |           8.0449 |
[32m[20221213 21:26:11 @agent_ppo2.py:185][0m |          -0.0079 |         160.1081 |           8.0037 |
[32m[20221213 21:26:11 @agent_ppo2.py:185][0m |          -0.0074 |         159.9275 |           8.0567 |
[32m[20221213 21:26:11 @agent_ppo2.py:185][0m |          -0.0083 |         159.7914 |           8.0545 |
[32m[20221213 21:26:11 @agent_ppo2.py:185][0m |           0.0012 |         171.8413 |           8.0905 |
[32m[20221213 21:26:11 @agent_ppo2.py:185][0m |          -0.0071 |         159.8511 |           8.0424 |
[32m[20221213 21:26:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.00
[32m[20221213 21:26:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 21:26:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.00
[32m[20221213 21:26:12 @agent_ppo2.py:143][0m Total time:      30.61 min
[32m[20221213 21:26:12 @agent_ppo2.py:145][0m 2994176 total steps have happened
[32m[20221213 21:26:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1462 --------------------------#
[32m[20221213 21:26:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:12 @agent_ppo2.py:185][0m |          -0.0031 |         163.5365 |           7.9703 |
[32m[20221213 21:26:12 @agent_ppo2.py:185][0m |          -0.0054 |         162.6320 |           8.0752 |
[32m[20221213 21:26:12 @agent_ppo2.py:185][0m |          -0.0069 |         162.1224 |           8.1163 |
[32m[20221213 21:26:12 @agent_ppo2.py:185][0m |          -0.0048 |         162.6223 |           8.1182 |
[32m[20221213 21:26:12 @agent_ppo2.py:185][0m |          -0.0068 |         161.1985 |           8.1377 |
[32m[20221213 21:26:12 @agent_ppo2.py:185][0m |          -0.0076 |         160.8707 |           8.1962 |
[32m[20221213 21:26:12 @agent_ppo2.py:185][0m |           0.0040 |         183.9711 |           8.1815 |
[32m[20221213 21:26:12 @agent_ppo2.py:185][0m |          -0.0087 |         160.5835 |           8.2387 |
[32m[20221213 21:26:13 @agent_ppo2.py:185][0m |          -0.0075 |         162.0728 |           8.2613 |
[32m[20221213 21:26:13 @agent_ppo2.py:185][0m |          -0.0091 |         160.0225 |           8.3157 |
[32m[20221213 21:26:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:26:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.80
[32m[20221213 21:26:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.00
[32m[20221213 21:26:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.00
[32m[20221213 21:26:13 @agent_ppo2.py:143][0m Total time:      30.63 min
[32m[20221213 21:26:13 @agent_ppo2.py:145][0m 2996224 total steps have happened
[32m[20221213 21:26:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1463 --------------------------#
[32m[20221213 21:26:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:13 @agent_ppo2.py:185][0m |           0.0010 |         163.4232 |           8.4109 |
[32m[20221213 21:26:13 @agent_ppo2.py:185][0m |          -0.0049 |         162.9342 |           8.3896 |
[32m[20221213 21:26:13 @agent_ppo2.py:185][0m |          -0.0055 |         162.7899 |           8.3971 |
[32m[20221213 21:26:13 @agent_ppo2.py:185][0m |          -0.0064 |         162.5410 |           8.3609 |
[32m[20221213 21:26:13 @agent_ppo2.py:185][0m |          -0.0033 |         165.1967 |           8.3595 |
[32m[20221213 21:26:14 @agent_ppo2.py:185][0m |          -0.0078 |         162.1802 |           8.3450 |
[32m[20221213 21:26:14 @agent_ppo2.py:185][0m |          -0.0047 |         162.6663 |           8.3400 |
[32m[20221213 21:26:14 @agent_ppo2.py:185][0m |          -0.0093 |         162.0803 |           8.3178 |
[32m[20221213 21:26:14 @agent_ppo2.py:185][0m |          -0.0073 |         161.9736 |           8.3692 |
[32m[20221213 21:26:14 @agent_ppo2.py:185][0m |          -0.0082 |         161.9378 |           8.3674 |
[32m[20221213 21:26:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.20
[32m[20221213 21:26:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.00
[32m[20221213 21:26:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.00
[32m[20221213 21:26:14 @agent_ppo2.py:143][0m Total time:      30.65 min
[32m[20221213 21:26:14 @agent_ppo2.py:145][0m 2998272 total steps have happened
[32m[20221213 21:26:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1464 --------------------------#
[32m[20221213 21:26:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:14 @agent_ppo2.py:185][0m |           0.0005 |         164.9316 |           8.0894 |
[32m[20221213 21:26:14 @agent_ppo2.py:185][0m |           0.0037 |         169.8732 |           8.1432 |
[32m[20221213 21:26:14 @agent_ppo2.py:185][0m |          -0.0032 |         163.1184 |           8.1331 |
[32m[20221213 21:26:15 @agent_ppo2.py:185][0m |          -0.0055 |         162.9584 |           8.0750 |
[32m[20221213 21:26:15 @agent_ppo2.py:185][0m |          -0.0053 |         162.6458 |           8.0760 |
[32m[20221213 21:26:15 @agent_ppo2.py:185][0m |          -0.0074 |         162.3062 |           8.0770 |
[32m[20221213 21:26:15 @agent_ppo2.py:185][0m |          -0.0098 |         162.2265 |           8.0748 |
[32m[20221213 21:26:15 @agent_ppo2.py:185][0m |          -0.0091 |         161.9399 |           8.0365 |
[32m[20221213 21:26:15 @agent_ppo2.py:185][0m |          -0.0085 |         161.8394 |           8.0550 |
[32m[20221213 21:26:15 @agent_ppo2.py:185][0m |          -0.0072 |         161.7319 |           8.0257 |
[32m[20221213 21:26:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.40
[32m[20221213 21:26:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.00
[32m[20221213 21:26:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.00
[32m[20221213 21:26:15 @agent_ppo2.py:143][0m Total time:      30.67 min
[32m[20221213 21:26:15 @agent_ppo2.py:145][0m 3000320 total steps have happened
[32m[20221213 21:26:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1465 --------------------------#
[32m[20221213 21:26:15 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:26:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:15 @agent_ppo2.py:185][0m |          -0.0042 |         164.0471 |           7.9223 |
[32m[20221213 21:26:16 @agent_ppo2.py:185][0m |          -0.0010 |         167.3971 |           7.9345 |
[32m[20221213 21:26:16 @agent_ppo2.py:185][0m |          -0.0082 |         163.0335 |           7.8461 |
[32m[20221213 21:26:16 @agent_ppo2.py:185][0m |          -0.0074 |         162.7816 |           7.8289 |
[32m[20221213 21:26:16 @agent_ppo2.py:185][0m |          -0.0089 |         162.6025 |           7.8227 |
[32m[20221213 21:26:16 @agent_ppo2.py:185][0m |          -0.0091 |         162.3741 |           7.7748 |
[32m[20221213 21:26:16 @agent_ppo2.py:185][0m |          -0.0094 |         162.3324 |           7.7816 |
[32m[20221213 21:26:16 @agent_ppo2.py:185][0m |          -0.0080 |         162.1451 |           7.8078 |
[32m[20221213 21:26:16 @agent_ppo2.py:185][0m |           0.0028 |         185.8681 |           7.7151 |
[32m[20221213 21:26:16 @agent_ppo2.py:185][0m |          -0.0082 |         162.0065 |           7.7559 |
[32m[20221213 21:26:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.00
[32m[20221213 21:26:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:26:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.00
[32m[20221213 21:26:16 @agent_ppo2.py:143][0m Total time:      30.69 min
[32m[20221213 21:26:16 @agent_ppo2.py:145][0m 3002368 total steps have happened
[32m[20221213 21:26:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1466 --------------------------#
[32m[20221213 21:26:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:26:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:17 @agent_ppo2.py:185][0m |           0.0074 |         173.9386 |           7.5057 |
[32m[20221213 21:26:17 @agent_ppo2.py:185][0m |           0.0042 |         173.4828 |           7.5703 |
[32m[20221213 21:26:17 @agent_ppo2.py:185][0m |          -0.0070 |         164.5400 |           7.5852 |
[32m[20221213 21:26:17 @agent_ppo2.py:185][0m |          -0.0070 |         164.3883 |           7.5801 |
[32m[20221213 21:26:17 @agent_ppo2.py:185][0m |          -0.0077 |         164.1628 |           7.6343 |
[32m[20221213 21:26:17 @agent_ppo2.py:185][0m |          -0.0078 |         164.0278 |           7.6029 |
[32m[20221213 21:26:17 @agent_ppo2.py:185][0m |          -0.0075 |         164.3415 |           7.6249 |
[32m[20221213 21:26:17 @agent_ppo2.py:185][0m |          -0.0093 |         163.8054 |           7.6422 |
[32m[20221213 21:26:17 @agent_ppo2.py:185][0m |          -0.0094 |         163.6869 |           7.6318 |
[32m[20221213 21:26:17 @agent_ppo2.py:185][0m |          -0.0100 |         163.6792 |           7.5994 |
[32m[20221213 21:26:17 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:26:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 784.40
[32m[20221213 21:26:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.00
[32m[20221213 21:26:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:26:18 @agent_ppo2.py:143][0m Total time:      30.72 min
[32m[20221213 21:26:18 @agent_ppo2.py:145][0m 3004416 total steps have happened
[32m[20221213 21:26:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1467 --------------------------#
[32m[20221213 21:26:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:18 @agent_ppo2.py:185][0m |           0.0087 |         171.7497 |           7.7110 |
[32m[20221213 21:26:18 @agent_ppo2.py:185][0m |           0.0033 |         176.6645 |           7.7889 |
[32m[20221213 21:26:18 @agent_ppo2.py:185][0m |          -0.0049 |         165.8552 |           7.7807 |
[32m[20221213 21:26:18 @agent_ppo2.py:185][0m |           0.0015 |         175.7140 |           7.8349 |
[32m[20221213 21:26:18 @agent_ppo2.py:185][0m |          -0.0060 |         165.2111 |           7.8483 |
[32m[20221213 21:26:18 @agent_ppo2.py:185][0m |          -0.0072 |         164.9027 |           7.8815 |
[32m[20221213 21:26:18 @agent_ppo2.py:185][0m |          -0.0083 |         164.8944 |           7.9145 |
[32m[20221213 21:26:19 @agent_ppo2.py:185][0m |          -0.0088 |         164.7279 |           7.9086 |
[32m[20221213 21:26:19 @agent_ppo2.py:185][0m |          -0.0075 |         164.5723 |           7.9392 |
[32m[20221213 21:26:19 @agent_ppo2.py:185][0m |          -0.0100 |         164.5494 |           7.9760 |
[32m[20221213 21:26:19 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:26:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.40
[32m[20221213 21:26:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.00
[32m[20221213 21:26:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.00
[32m[20221213 21:26:19 @agent_ppo2.py:143][0m Total time:      30.74 min
[32m[20221213 21:26:19 @agent_ppo2.py:145][0m 3006464 total steps have happened
[32m[20221213 21:26:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1468 --------------------------#
[32m[20221213 21:26:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:19 @agent_ppo2.py:185][0m |          -0.0019 |         164.4727 |           8.1122 |
[32m[20221213 21:26:19 @agent_ppo2.py:185][0m |          -0.0070 |         162.9826 |           8.1010 |
[32m[20221213 21:26:19 @agent_ppo2.py:185][0m |          -0.0074 |         161.6936 |           8.0695 |
[32m[20221213 21:26:19 @agent_ppo2.py:185][0m |          -0.0087 |         161.0633 |           8.1060 |
[32m[20221213 21:26:19 @agent_ppo2.py:185][0m |           0.0013 |         169.9620 |           8.0538 |
[32m[20221213 21:26:20 @agent_ppo2.py:185][0m |          -0.0095 |         159.4112 |           8.0185 |
[32m[20221213 21:26:20 @agent_ppo2.py:185][0m |          -0.0103 |         159.1263 |           8.0643 |
[32m[20221213 21:26:20 @agent_ppo2.py:185][0m |          -0.0114 |         158.8316 |           8.0652 |
[32m[20221213 21:26:20 @agent_ppo2.py:185][0m |          -0.0093 |         158.4525 |           8.0450 |
[32m[20221213 21:26:20 @agent_ppo2.py:185][0m |          -0.0110 |         158.1928 |           7.9974 |
[32m[20221213 21:26:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:26:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.60
[32m[20221213 21:26:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.00
[32m[20221213 21:26:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 744.00
[32m[20221213 21:26:20 @agent_ppo2.py:143][0m Total time:      30.76 min
[32m[20221213 21:26:20 @agent_ppo2.py:145][0m 3008512 total steps have happened
[32m[20221213 21:26:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1469 --------------------------#
[32m[20221213 21:26:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:26:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:20 @agent_ppo2.py:185][0m |           0.0008 |         168.5640 |           7.4676 |
[32m[20221213 21:26:21 @agent_ppo2.py:185][0m |           0.0002 |         167.8057 |           7.5275 |
[32m[20221213 21:26:21 @agent_ppo2.py:185][0m |          -0.0035 |         166.7362 |           7.4945 |
[32m[20221213 21:26:21 @agent_ppo2.py:185][0m |          -0.0046 |         166.0614 |           7.5096 |
[32m[20221213 21:26:21 @agent_ppo2.py:185][0m |           0.0110 |         187.4161 |           7.5167 |
[32m[20221213 21:26:21 @agent_ppo2.py:185][0m |          -0.0053 |         165.2883 |           7.5085 |
[32m[20221213 21:26:21 @agent_ppo2.py:185][0m |          -0.0041 |         165.1742 |           7.4931 |
[32m[20221213 21:26:21 @agent_ppo2.py:185][0m |          -0.0072 |         164.7138 |           7.4980 |
[32m[20221213 21:26:22 @agent_ppo2.py:185][0m |          -0.0067 |         164.3858 |           7.5449 |
[32m[20221213 21:26:22 @agent_ppo2.py:185][0m |          -0.0069 |         164.2274 |           7.5275 |
[32m[20221213 21:26:22 @agent_ppo2.py:130][0m Policy update time: 1.39 s
[32m[20221213 21:26:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.60
[32m[20221213 21:26:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221213 21:26:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 21:26:22 @agent_ppo2.py:143][0m Total time:      30.78 min
[32m[20221213 21:26:22 @agent_ppo2.py:145][0m 3010560 total steps have happened
[32m[20221213 21:26:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1470 --------------------------#
[32m[20221213 21:26:22 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:26:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:22 @agent_ppo2.py:185][0m |           0.0047 |         171.6289 |           7.7811 |
[32m[20221213 21:26:22 @agent_ppo2.py:185][0m |          -0.0028 |         167.3055 |           7.8521 |
[32m[20221213 21:26:22 @agent_ppo2.py:185][0m |          -0.0051 |         166.2874 |           7.9069 |
[32m[20221213 21:26:22 @agent_ppo2.py:185][0m |          -0.0051 |         165.6524 |           7.8536 |
[32m[20221213 21:26:22 @agent_ppo2.py:185][0m |          -0.0069 |         165.1655 |           7.8925 |
[32m[20221213 21:26:22 @agent_ppo2.py:185][0m |          -0.0076 |         164.6670 |           7.8522 |
[32m[20221213 21:26:23 @agent_ppo2.py:185][0m |          -0.0064 |         164.3890 |           7.9091 |
[32m[20221213 21:26:23 @agent_ppo2.py:185][0m |           0.0081 |         182.6280 |           7.9177 |
[32m[20221213 21:26:23 @agent_ppo2.py:185][0m |          -0.0052 |         163.8485 |           7.9821 |
[32m[20221213 21:26:23 @agent_ppo2.py:185][0m |          -0.0075 |         163.7085 |           7.9231 |
[32m[20221213 21:26:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.20
[32m[20221213 21:26:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:26:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 772.00
[32m[20221213 21:26:23 @agent_ppo2.py:143][0m Total time:      30.80 min
[32m[20221213 21:26:23 @agent_ppo2.py:145][0m 3012608 total steps have happened
[32m[20221213 21:26:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1471 --------------------------#
[32m[20221213 21:26:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:23 @agent_ppo2.py:185][0m |          -0.0003 |         165.5508 |           7.6291 |
[32m[20221213 21:26:23 @agent_ppo2.py:185][0m |          -0.0046 |         163.7736 |           7.5485 |
[32m[20221213 21:26:23 @agent_ppo2.py:185][0m |           0.0012 |         166.5145 |           7.5444 |
[32m[20221213 21:26:24 @agent_ppo2.py:185][0m |          -0.0057 |         162.7446 |           7.5372 |
[32m[20221213 21:26:24 @agent_ppo2.py:185][0m |           0.0036 |         179.3730 |           7.5316 |
[32m[20221213 21:26:24 @agent_ppo2.py:185][0m |          -0.0065 |         162.1586 |           7.5297 |
[32m[20221213 21:26:24 @agent_ppo2.py:185][0m |          -0.0079 |         161.9249 |           7.4690 |
[32m[20221213 21:26:24 @agent_ppo2.py:185][0m |          -0.0081 |         161.7443 |           7.4644 |
[32m[20221213 21:26:24 @agent_ppo2.py:185][0m |          -0.0044 |         163.9599 |           7.5126 |
[32m[20221213 21:26:24 @agent_ppo2.py:185][0m |          -0.0079 |         161.6477 |           7.4129 |
[32m[20221213 21:26:24 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:26:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.60
[32m[20221213 21:26:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 21:26:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 737.00
[32m[20221213 21:26:24 @agent_ppo2.py:143][0m Total time:      30.83 min
[32m[20221213 21:26:24 @agent_ppo2.py:145][0m 3014656 total steps have happened
[32m[20221213 21:26:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1472 --------------------------#
[32m[20221213 21:26:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:25 @agent_ppo2.py:185][0m |           0.0026 |         167.1717 |           7.4474 |
[32m[20221213 21:26:25 @agent_ppo2.py:185][0m |          -0.0006 |         166.7726 |           7.4427 |
[32m[20221213 21:26:25 @agent_ppo2.py:185][0m |          -0.0031 |         165.0280 |           7.4619 |
[32m[20221213 21:26:25 @agent_ppo2.py:185][0m |          -0.0056 |         164.6455 |           7.4889 |
[32m[20221213 21:26:25 @agent_ppo2.py:185][0m |          -0.0060 |         164.4922 |           7.4793 |
[32m[20221213 21:26:25 @agent_ppo2.py:185][0m |          -0.0066 |         164.2918 |           7.5205 |
[32m[20221213 21:26:25 @agent_ppo2.py:185][0m |          -0.0024 |         164.3144 |           7.4912 |
[32m[20221213 21:26:25 @agent_ppo2.py:185][0m |          -0.0066 |         163.8510 |           7.5220 |
[32m[20221213 21:26:25 @agent_ppo2.py:185][0m |           0.0063 |         173.7293 |           7.5140 |
[32m[20221213 21:26:25 @agent_ppo2.py:185][0m |          -0.0068 |         163.6211 |           7.5467 |
[32m[20221213 21:26:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:26:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.80
[32m[20221213 21:26:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.00
[32m[20221213 21:26:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.00
[32m[20221213 21:26:25 @agent_ppo2.py:143][0m Total time:      30.85 min
[32m[20221213 21:26:25 @agent_ppo2.py:145][0m 3016704 total steps have happened
[32m[20221213 21:26:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1473 --------------------------#
[32m[20221213 21:26:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 21:26:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:26 @agent_ppo2.py:185][0m |           0.0023 |         166.6074 |           7.6498 |
[32m[20221213 21:26:26 @agent_ppo2.py:185][0m |           0.0016 |         165.0315 |           7.6673 |
[32m[20221213 21:26:26 @agent_ppo2.py:185][0m |          -0.0047 |         163.4921 |           7.6187 |
[32m[20221213 21:26:26 @agent_ppo2.py:185][0m |          -0.0047 |         163.2386 |           7.6604 |
[32m[20221213 21:26:26 @agent_ppo2.py:185][0m |          -0.0059 |         163.1461 |           7.5769 |
[32m[20221213 21:26:26 @agent_ppo2.py:185][0m |          -0.0054 |         162.8621 |           7.5842 |
[32m[20221213 21:26:26 @agent_ppo2.py:185][0m |          -0.0064 |         162.6585 |           7.5071 |
[32m[20221213 21:26:26 @agent_ppo2.py:185][0m |          -0.0082 |         162.6288 |           7.5352 |
[32m[20221213 21:26:26 @agent_ppo2.py:185][0m |          -0.0062 |         162.8412 |           7.4625 |
[32m[20221213 21:26:27 @agent_ppo2.py:185][0m |          -0.0075 |         162.4405 |           7.4639 |
[32m[20221213 21:26:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:26:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.80
[32m[20221213 21:26:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 21:26:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.00
[32m[20221213 21:26:27 @agent_ppo2.py:143][0m Total time:      30.87 min
[32m[20221213 21:26:27 @agent_ppo2.py:145][0m 3018752 total steps have happened
[32m[20221213 21:26:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1474 --------------------------#
[32m[20221213 21:26:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:27 @agent_ppo2.py:185][0m |          -0.0007 |         165.9366 |           7.4848 |
[32m[20221213 21:26:27 @agent_ppo2.py:185][0m |           0.0028 |         170.6440 |           7.4676 |
[32m[20221213 21:26:27 @agent_ppo2.py:185][0m |          -0.0065 |         163.1081 |           7.5321 |
[32m[20221213 21:26:27 @agent_ppo2.py:185][0m |          -0.0077 |         161.4728 |           7.5079 |
[32m[20221213 21:26:27 @agent_ppo2.py:185][0m |          -0.0094 |         160.1402 |           7.4951 |
[32m[20221213 21:26:27 @agent_ppo2.py:185][0m |          -0.0089 |         159.1697 |           7.5117 |
[32m[20221213 21:26:28 @agent_ppo2.py:185][0m |          -0.0069 |         158.3068 |           7.4870 |
[32m[20221213 21:26:28 @agent_ppo2.py:185][0m |          -0.0097 |         157.6122 |           7.4856 |
[32m[20221213 21:26:28 @agent_ppo2.py:185][0m |          -0.0031 |         160.1635 |           7.4493 |
[32m[20221213 21:26:28 @agent_ppo2.py:185][0m |          -0.0108 |         156.7247 |           7.4480 |
[32m[20221213 21:26:28 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:26:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.20
[32m[20221213 21:26:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:26:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.00
[32m[20221213 21:26:28 @agent_ppo2.py:143][0m Total time:      30.89 min
[32m[20221213 21:26:28 @agent_ppo2.py:145][0m 3020800 total steps have happened
[32m[20221213 21:26:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1475 --------------------------#
[32m[20221213 21:26:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:28 @agent_ppo2.py:185][0m |          -0.0011 |         170.0513 |           7.3533 |
[32m[20221213 21:26:28 @agent_ppo2.py:185][0m |           0.0013 |         170.9186 |           7.3227 |
[32m[20221213 21:26:28 @agent_ppo2.py:185][0m |           0.0018 |         174.2542 |           7.3359 |
[32m[20221213 21:26:29 @agent_ppo2.py:185][0m |          -0.0061 |         166.0850 |           7.3349 |
[32m[20221213 21:26:29 @agent_ppo2.py:185][0m |          -0.0086 |         165.9608 |           7.3518 |
[32m[20221213 21:26:29 @agent_ppo2.py:185][0m |          -0.0076 |         165.4945 |           7.3622 |
[32m[20221213 21:26:29 @agent_ppo2.py:185][0m |          -0.0087 |         165.4228 |           7.3191 |
[32m[20221213 21:26:29 @agent_ppo2.py:185][0m |          -0.0097 |         165.2219 |           7.2766 |
[32m[20221213 21:26:29 @agent_ppo2.py:185][0m |          -0.0085 |         165.1414 |           7.3128 |
[32m[20221213 21:26:29 @agent_ppo2.py:185][0m |          -0.0104 |         165.0240 |           7.2913 |
[32m[20221213 21:26:29 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:26:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.40
[32m[20221213 21:26:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.00
[32m[20221213 21:26:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.00
[32m[20221213 21:26:29 @agent_ppo2.py:143][0m Total time:      30.91 min
[32m[20221213 21:26:29 @agent_ppo2.py:145][0m 3022848 total steps have happened
[32m[20221213 21:26:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1476 --------------------------#
[32m[20221213 21:26:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:30 @agent_ppo2.py:185][0m |           0.0027 |         166.5980 |           7.2753 |
[32m[20221213 21:26:30 @agent_ppo2.py:185][0m |           0.0044 |         169.5009 |           7.2808 |
[32m[20221213 21:26:30 @agent_ppo2.py:185][0m |          -0.0031 |         164.3658 |           7.2540 |
[32m[20221213 21:26:30 @agent_ppo2.py:185][0m |          -0.0021 |         163.9365 |           7.2500 |
[32m[20221213 21:26:30 @agent_ppo2.py:185][0m |          -0.0032 |         163.5019 |           7.2353 |
[32m[20221213 21:26:30 @agent_ppo2.py:185][0m |          -0.0024 |         163.1620 |           7.2193 |
[32m[20221213 21:26:30 @agent_ppo2.py:185][0m |          -0.0003 |         166.3169 |           7.2586 |
[32m[20221213 21:26:30 @agent_ppo2.py:185][0m |          -0.0049 |         162.6873 |           7.1935 |
[32m[20221213 21:26:30 @agent_ppo2.py:185][0m |          -0.0060 |         162.2737 |           7.2017 |
[32m[20221213 21:26:30 @agent_ppo2.py:185][0m |          -0.0072 |         162.1267 |           7.1732 |
[32m[20221213 21:26:30 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:26:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.00
[32m[20221213 21:26:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:26:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.00
[32m[20221213 21:26:31 @agent_ppo2.py:143][0m Total time:      30.93 min
[32m[20221213 21:26:31 @agent_ppo2.py:145][0m 3024896 total steps have happened
[32m[20221213 21:26:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1477 --------------------------#
[32m[20221213 21:26:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:31 @agent_ppo2.py:185][0m |          -0.0026 |         167.5018 |           6.9003 |
[32m[20221213 21:26:31 @agent_ppo2.py:185][0m |          -0.0041 |         166.5842 |           6.9480 |
[32m[20221213 21:26:31 @agent_ppo2.py:185][0m |          -0.0036 |         166.1317 |           7.0062 |
[32m[20221213 21:26:31 @agent_ppo2.py:185][0m |          -0.0087 |         165.5212 |           6.9896 |
[32m[20221213 21:26:31 @agent_ppo2.py:185][0m |          -0.0083 |         165.0503 |           7.0821 |
[32m[20221213 21:26:31 @agent_ppo2.py:185][0m |          -0.0064 |         164.7647 |           7.0860 |
[32m[20221213 21:26:31 @agent_ppo2.py:185][0m |          -0.0078 |         164.5951 |           7.1106 |
[32m[20221213 21:26:31 @agent_ppo2.py:185][0m |          -0.0099 |         164.1262 |           7.1559 |
[32m[20221213 21:26:32 @agent_ppo2.py:185][0m |          -0.0083 |         163.8754 |           7.1193 |
[32m[20221213 21:26:32 @agent_ppo2.py:185][0m |          -0.0007 |         176.5293 |           7.1746 |
[32m[20221213 21:26:32 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:26:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.40
[32m[20221213 21:26:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.00
[32m[20221213 21:26:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.00
[32m[20221213 21:26:32 @agent_ppo2.py:143][0m Total time:      30.95 min
[32m[20221213 21:26:32 @agent_ppo2.py:145][0m 3026944 total steps have happened
[32m[20221213 21:26:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1478 --------------------------#
[32m[20221213 21:26:32 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:26:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:32 @agent_ppo2.py:185][0m |           0.0081 |         172.3430 |           6.9023 |
[32m[20221213 21:26:32 @agent_ppo2.py:185][0m |          -0.0014 |         164.6173 |           6.9612 |
[32m[20221213 21:26:32 @agent_ppo2.py:185][0m |          -0.0045 |         163.6891 |           6.7986 |
[32m[20221213 21:26:32 @agent_ppo2.py:185][0m |          -0.0050 |         163.8886 |           6.7360 |
[32m[20221213 21:26:32 @agent_ppo2.py:185][0m |          -0.0085 |         162.4949 |           6.6985 |
[32m[20221213 21:26:33 @agent_ppo2.py:185][0m |          -0.0017 |         166.4346 |           6.6952 |
[32m[20221213 21:26:33 @agent_ppo2.py:185][0m |           0.0002 |         172.8654 |           6.6720 |
[32m[20221213 21:26:33 @agent_ppo2.py:185][0m |          -0.0084 |         161.5700 |           6.6368 |
[32m[20221213 21:26:33 @agent_ppo2.py:185][0m |          -0.0086 |         161.5088 |           6.5933 |
[32m[20221213 21:26:33 @agent_ppo2.py:185][0m |          -0.0086 |         161.0497 |           6.5861 |
[32m[20221213 21:26:33 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:26:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.40
[32m[20221213 21:26:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.00
[32m[20221213 21:26:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 21:26:33 @agent_ppo2.py:143][0m Total time:      30.97 min
[32m[20221213 21:26:33 @agent_ppo2.py:145][0m 3028992 total steps have happened
[32m[20221213 21:26:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1479 --------------------------#
[32m[20221213 21:26:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:26:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:33 @agent_ppo2.py:185][0m |           0.0016 |         167.1744 |           6.6016 |
[32m[20221213 21:26:33 @agent_ppo2.py:185][0m |          -0.0061 |         165.9399 |           6.6453 |
[32m[20221213 21:26:34 @agent_ppo2.py:185][0m |          -0.0058 |         165.4179 |           6.5767 |
[32m[20221213 21:26:34 @agent_ppo2.py:185][0m |          -0.0078 |         165.2093 |           6.6167 |
[32m[20221213 21:26:34 @agent_ppo2.py:185][0m |          -0.0074 |         164.9702 |           6.6290 |
[32m[20221213 21:26:34 @agent_ppo2.py:185][0m |          -0.0073 |         164.6393 |           6.6177 |
[32m[20221213 21:26:34 @agent_ppo2.py:185][0m |          -0.0086 |         164.6150 |           6.6166 |
[32m[20221213 21:26:34 @agent_ppo2.py:185][0m |          -0.0095 |         164.4235 |           6.6048 |
[32m[20221213 21:26:34 @agent_ppo2.py:185][0m |          -0.0098 |         164.3385 |           6.6141 |
[32m[20221213 21:26:34 @agent_ppo2.py:185][0m |          -0.0013 |         169.6695 |           6.6353 |
[32m[20221213 21:26:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:26:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.20
[32m[20221213 21:26:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.00
[32m[20221213 21:26:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.00
[32m[20221213 21:26:34 @agent_ppo2.py:143][0m Total time:      30.99 min
[32m[20221213 21:26:34 @agent_ppo2.py:145][0m 3031040 total steps have happened
[32m[20221213 21:26:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1480 --------------------------#
[32m[20221213 21:26:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:35 @agent_ppo2.py:185][0m |           0.0069 |         171.2971 |           6.8149 |
[32m[20221213 21:26:35 @agent_ppo2.py:185][0m |          -0.0021 |         165.7660 |           6.8524 |
[32m[20221213 21:26:35 @agent_ppo2.py:185][0m |          -0.0050 |         164.3645 |           6.8806 |
[32m[20221213 21:26:35 @agent_ppo2.py:185][0m |          -0.0076 |         163.9837 |           6.9694 |
[32m[20221213 21:26:35 @agent_ppo2.py:185][0m |          -0.0030 |         164.5748 |           6.9683 |
[32m[20221213 21:26:35 @agent_ppo2.py:185][0m |          -0.0083 |         163.4470 |           7.0059 |
[32m[20221213 21:26:35 @agent_ppo2.py:185][0m |           0.0064 |         179.0704 |           7.0482 |
[32m[20221213 21:26:35 @agent_ppo2.py:185][0m |          -0.0060 |         162.8595 |           7.0906 |
[32m[20221213 21:26:35 @agent_ppo2.py:185][0m |          -0.0082 |         162.9530 |           7.0753 |
[32m[20221213 21:26:35 @agent_ppo2.py:185][0m |          -0.0096 |         162.5277 |           7.1198 |
[32m[20221213 21:26:35 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:26:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.80
[32m[20221213 21:26:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.00
[32m[20221213 21:26:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 702.00
[32m[20221213 21:26:35 @agent_ppo2.py:143][0m Total time:      31.01 min
[32m[20221213 21:26:35 @agent_ppo2.py:145][0m 3033088 total steps have happened
[32m[20221213 21:26:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1481 --------------------------#
[32m[20221213 21:26:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:36 @agent_ppo2.py:185][0m |          -0.0010 |         166.0602 |           7.2058 |
[32m[20221213 21:26:36 @agent_ppo2.py:185][0m |          -0.0044 |         165.0761 |           7.2152 |
[32m[20221213 21:26:36 @agent_ppo2.py:185][0m |          -0.0059 |         164.6673 |           7.1586 |
[32m[20221213 21:26:36 @agent_ppo2.py:185][0m |          -0.0024 |         165.2596 |           7.1459 |
[32m[20221213 21:26:36 @agent_ppo2.py:185][0m |          -0.0055 |         164.3258 |           7.1241 |
[32m[20221213 21:26:36 @agent_ppo2.py:185][0m |          -0.0082 |         164.2676 |           7.1641 |
[32m[20221213 21:26:36 @agent_ppo2.py:185][0m |          -0.0071 |         164.0869 |           7.0920 |
[32m[20221213 21:26:36 @agent_ppo2.py:185][0m |          -0.0089 |         164.1917 |           7.1315 |
[32m[20221213 21:26:36 @agent_ppo2.py:185][0m |          -0.0109 |         164.0523 |           7.0634 |
[32m[20221213 21:26:37 @agent_ppo2.py:185][0m |          -0.0088 |         163.8952 |           7.0679 |
[32m[20221213 21:26:37 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:26:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 747.20
[32m[20221213 21:26:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 765.00
[32m[20221213 21:26:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.00
[32m[20221213 21:26:37 @agent_ppo2.py:143][0m Total time:      31.03 min
[32m[20221213 21:26:37 @agent_ppo2.py:145][0m 3035136 total steps have happened
[32m[20221213 21:26:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1482 --------------------------#
[32m[20221213 21:26:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:26:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:37 @agent_ppo2.py:185][0m |          -0.0010 |         168.3411 |           7.0159 |
[32m[20221213 21:26:37 @agent_ppo2.py:185][0m |          -0.0059 |         167.5463 |           7.1247 |
[32m[20221213 21:26:37 @agent_ppo2.py:185][0m |          -0.0004 |         169.2711 |           7.1204 |
[32m[20221213 21:26:37 @agent_ppo2.py:185][0m |          -0.0070 |         167.0474 |           7.1893 |
[32m[20221213 21:26:37 @agent_ppo2.py:185][0m |          -0.0055 |         166.8701 |           7.2151 |
[32m[20221213 21:26:37 @agent_ppo2.py:185][0m |           0.0060 |         188.6463 |           7.2700 |
[32m[20221213 21:26:38 @agent_ppo2.py:185][0m |          -0.0040 |         166.7266 |           7.3172 |
[32m[20221213 21:26:38 @agent_ppo2.py:185][0m |          -0.0082 |         166.6329 |           7.3372 |
[32m[20221213 21:26:38 @agent_ppo2.py:185][0m |           0.0012 |         178.5302 |           7.3498 |
[32m[20221213 21:26:38 @agent_ppo2.py:185][0m |          -0.0091 |         166.3912 |           7.3440 |
[32m[20221213 21:26:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:26:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 772.40
[32m[20221213 21:26:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 21:26:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.00
[32m[20221213 21:26:38 @agent_ppo2.py:143][0m Total time:      31.05 min
[32m[20221213 21:26:38 @agent_ppo2.py:145][0m 3037184 total steps have happened
[32m[20221213 21:26:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1483 --------------------------#
[32m[20221213 21:26:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:38 @agent_ppo2.py:185][0m |           0.0003 |         168.8284 |           7.3028 |
[32m[20221213 21:26:38 @agent_ppo2.py:185][0m |          -0.0033 |         167.6940 |           7.2644 |
[32m[20221213 21:26:38 @agent_ppo2.py:185][0m |          -0.0067 |         166.9532 |           7.2283 |
[32m[20221213 21:26:38 @agent_ppo2.py:185][0m |          -0.0048 |         166.5456 |           7.1676 |
[32m[20221213 21:26:39 @agent_ppo2.py:185][0m |          -0.0081 |         166.1465 |           7.1000 |
[32m[20221213 21:26:39 @agent_ppo2.py:185][0m |          -0.0088 |         165.9739 |           7.1011 |
[32m[20221213 21:26:39 @agent_ppo2.py:185][0m |          -0.0068 |         166.0843 |           7.0740 |
[32m[20221213 21:26:39 @agent_ppo2.py:185][0m |          -0.0078 |         165.9258 |           7.0620 |
[32m[20221213 21:26:39 @agent_ppo2.py:185][0m |          -0.0099 |         165.4367 |           7.0373 |
[32m[20221213 21:26:39 @agent_ppo2.py:185][0m |          -0.0104 |         165.2121 |           7.0163 |
[32m[20221213 21:26:39 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:26:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.60
[32m[20221213 21:26:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.00
[32m[20221213 21:26:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.00
[32m[20221213 21:26:39 @agent_ppo2.py:143][0m Total time:      31.07 min
[32m[20221213 21:26:39 @agent_ppo2.py:145][0m 3039232 total steps have happened
[32m[20221213 21:26:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1484 --------------------------#
[32m[20221213 21:26:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:39 @agent_ppo2.py:185][0m |           0.0051 |         172.9228 |           6.4016 |
[32m[20221213 21:26:40 @agent_ppo2.py:185][0m |          -0.0020 |         168.9672 |           6.4524 |
[32m[20221213 21:26:40 @agent_ppo2.py:185][0m |          -0.0070 |         166.5135 |           6.4439 |
[32m[20221213 21:26:40 @agent_ppo2.py:185][0m |           0.0032 |         183.3751 |           6.5110 |
[32m[20221213 21:26:40 @agent_ppo2.py:185][0m |           0.0027 |         185.3135 |           6.5698 |
[32m[20221213 21:26:40 @agent_ppo2.py:185][0m |          -0.0087 |         165.6615 |           6.5682 |
[32m[20221213 21:26:40 @agent_ppo2.py:185][0m |          -0.0087 |         165.3899 |           6.5067 |
[32m[20221213 21:26:40 @agent_ppo2.py:185][0m |          -0.0107 |         165.1300 |           6.5545 |
[32m[20221213 21:26:40 @agent_ppo2.py:185][0m |          -0.0085 |         164.9696 |           6.5698 |
[32m[20221213 21:26:40 @agent_ppo2.py:185][0m |          -0.0025 |         178.3520 |           6.6013 |
[32m[20221213 21:26:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:26:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.00
[32m[20221213 21:26:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.00
[32m[20221213 21:26:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.00
[32m[20221213 21:26:40 @agent_ppo2.py:143][0m Total time:      31.09 min
[32m[20221213 21:26:40 @agent_ppo2.py:145][0m 3041280 total steps have happened
[32m[20221213 21:26:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1485 --------------------------#
[32m[20221213 21:26:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:41 @agent_ppo2.py:185][0m |           0.0023 |         167.9684 |           6.7987 |
[32m[20221213 21:26:41 @agent_ppo2.py:185][0m |          -0.0051 |         166.7150 |           6.8893 |
[32m[20221213 21:26:41 @agent_ppo2.py:185][0m |           0.0002 |         172.7517 |           6.9272 |
[32m[20221213 21:26:41 @agent_ppo2.py:185][0m |          -0.0066 |         165.5811 |           6.9686 |
[32m[20221213 21:26:41 @agent_ppo2.py:185][0m |          -0.0073 |         165.2192 |           7.0244 |
[32m[20221213 21:26:41 @agent_ppo2.py:185][0m |          -0.0037 |         166.7890 |           7.0310 |
[32m[20221213 21:26:41 @agent_ppo2.py:185][0m |          -0.0065 |         164.7409 |           7.0865 |
[32m[20221213 21:26:41 @agent_ppo2.py:185][0m |          -0.0050 |         164.7524 |           7.1322 |
[32m[20221213 21:26:41 @agent_ppo2.py:185][0m |          -0.0001 |         172.7499 |           7.1686 |
[32m[20221213 21:26:41 @agent_ppo2.py:185][0m |          -0.0094 |         164.2178 |           7.2231 |
[32m[20221213 21:26:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:26:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.20
[32m[20221213 21:26:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 21:26:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.00
[32m[20221213 21:26:42 @agent_ppo2.py:143][0m Total time:      31.11 min
[32m[20221213 21:26:42 @agent_ppo2.py:145][0m 3043328 total steps have happened
[32m[20221213 21:26:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1486 --------------------------#
[32m[20221213 21:26:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:42 @agent_ppo2.py:185][0m |           0.0024 |         167.1313 |           7.3748 |
[32m[20221213 21:26:42 @agent_ppo2.py:185][0m |          -0.0028 |         165.8799 |           7.3953 |
[32m[20221213 21:26:42 @agent_ppo2.py:185][0m |          -0.0026 |         165.2890 |           7.3856 |
[32m[20221213 21:26:42 @agent_ppo2.py:185][0m |          -0.0029 |         164.6348 |           7.3493 |
[32m[20221213 21:26:42 @agent_ppo2.py:185][0m |           0.0023 |         169.1382 |           7.3625 |
[32m[20221213 21:26:42 @agent_ppo2.py:185][0m |          -0.0039 |         164.1583 |           7.3442 |
[32m[20221213 21:26:42 @agent_ppo2.py:185][0m |          -0.0055 |         163.8824 |           7.3616 |
[32m[20221213 21:26:42 @agent_ppo2.py:185][0m |          -0.0065 |         163.7347 |           7.3330 |
[32m[20221213 21:26:43 @agent_ppo2.py:185][0m |          -0.0089 |         163.5470 |           7.3061 |
[32m[20221213 21:26:43 @agent_ppo2.py:185][0m |          -0.0033 |         165.8679 |           7.3363 |
[32m[20221213 21:26:43 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:26:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.80
[32m[20221213 21:26:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.00
[32m[20221213 21:26:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 21:26:43 @agent_ppo2.py:143][0m Total time:      31.13 min
[32m[20221213 21:26:43 @agent_ppo2.py:145][0m 3045376 total steps have happened
[32m[20221213 21:26:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1487 --------------------------#
[32m[20221213 21:26:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:43 @agent_ppo2.py:185][0m |          -0.0022 |         166.6959 |           7.3250 |
[32m[20221213 21:26:43 @agent_ppo2.py:185][0m |          -0.0022 |         165.8437 |           7.3763 |
[32m[20221213 21:26:43 @agent_ppo2.py:185][0m |          -0.0060 |         164.5778 |           7.3492 |
[32m[20221213 21:26:43 @agent_ppo2.py:185][0m |          -0.0042 |         164.7331 |           7.3469 |
[32m[20221213 21:26:43 @agent_ppo2.py:185][0m |          -0.0029 |         165.8472 |           7.4159 |
[32m[20221213 21:26:43 @agent_ppo2.py:185][0m |          -0.0057 |         163.7926 |           7.3841 |
[32m[20221213 21:26:44 @agent_ppo2.py:185][0m |          -0.0082 |         163.1371 |           7.4197 |
[32m[20221213 21:26:44 @agent_ppo2.py:185][0m |          -0.0076 |         162.9295 |           7.3660 |
[32m[20221213 21:26:44 @agent_ppo2.py:185][0m |          -0.0072 |         162.9463 |           7.3805 |
[32m[20221213 21:26:44 @agent_ppo2.py:185][0m |          -0.0101 |         162.4445 |           7.4103 |
[32m[20221213 21:26:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.60
[32m[20221213 21:26:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:26:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.00
[32m[20221213 21:26:44 @agent_ppo2.py:143][0m Total time:      31.15 min
[32m[20221213 21:26:44 @agent_ppo2.py:145][0m 3047424 total steps have happened
[32m[20221213 21:26:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1488 --------------------------#
[32m[20221213 21:26:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:44 @agent_ppo2.py:185][0m |          -0.0014 |         169.0351 |           7.3713 |
[32m[20221213 21:26:44 @agent_ppo2.py:185][0m |          -0.0051 |         168.3231 |           7.4412 |
[32m[20221213 21:26:44 @agent_ppo2.py:185][0m |          -0.0035 |         168.6840 |           7.4630 |
[32m[20221213 21:26:45 @agent_ppo2.py:185][0m |          -0.0009 |         168.8498 |           7.4780 |
[32m[20221213 21:26:45 @agent_ppo2.py:185][0m |          -0.0085 |         167.0970 |           7.5220 |
[32m[20221213 21:26:45 @agent_ppo2.py:185][0m |          -0.0067 |         166.9646 |           7.5445 |
[32m[20221213 21:26:45 @agent_ppo2.py:185][0m |           0.0046 |         178.2588 |           7.5774 |
[32m[20221213 21:26:45 @agent_ppo2.py:185][0m |          -0.0060 |         166.4736 |           7.6222 |
[32m[20221213 21:26:45 @agent_ppo2.py:185][0m |           0.0094 |         189.6822 |           7.5667 |
[32m[20221213 21:26:45 @agent_ppo2.py:185][0m |          -0.0077 |         166.5069 |           7.6436 |
[32m[20221213 21:26:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:26:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.00
[32m[20221213 21:26:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:26:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.00
[32m[20221213 21:26:45 @agent_ppo2.py:143][0m Total time:      31.17 min
[32m[20221213 21:26:45 @agent_ppo2.py:145][0m 3049472 total steps have happened
[32m[20221213 21:26:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1489 --------------------------#
[32m[20221213 21:26:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:45 @agent_ppo2.py:185][0m |           0.0046 |         171.4951 |           7.8327 |
[32m[20221213 21:26:46 @agent_ppo2.py:185][0m |          -0.0036 |         165.5172 |           7.8004 |
[32m[20221213 21:26:46 @agent_ppo2.py:185][0m |          -0.0037 |         165.0744 |           7.8009 |
[32m[20221213 21:26:46 @agent_ppo2.py:185][0m |          -0.0058 |         164.3424 |           7.7982 |
[32m[20221213 21:26:46 @agent_ppo2.py:185][0m |          -0.0060 |         163.8658 |           7.8011 |
[32m[20221213 21:26:46 @agent_ppo2.py:185][0m |          -0.0084 |         163.6038 |           7.7945 |
[32m[20221213 21:26:46 @agent_ppo2.py:185][0m |          -0.0058 |         163.8846 |           7.7955 |
[32m[20221213 21:26:46 @agent_ppo2.py:185][0m |          -0.0067 |         163.2615 |           7.8165 |
[32m[20221213 21:26:46 @agent_ppo2.py:185][0m |          -0.0083 |         162.5795 |           7.8084 |
[32m[20221213 21:26:46 @agent_ppo2.py:185][0m |           0.0025 |         170.0496 |           7.8194 |
[32m[20221213 21:26:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:26:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.20
[32m[20221213 21:26:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:26:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 860.00
[32m[20221213 21:26:46 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 860.00
[32m[20221213 21:26:46 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 860.00
[32m[20221213 21:26:46 @agent_ppo2.py:143][0m Total time:      31.20 min
[32m[20221213 21:26:46 @agent_ppo2.py:145][0m 3051520 total steps have happened
[32m[20221213 21:26:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1490 --------------------------#
[32m[20221213 21:26:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:47 @agent_ppo2.py:185][0m |          -0.0012 |         172.5947 |           7.3224 |
[32m[20221213 21:26:47 @agent_ppo2.py:185][0m |          -0.0023 |         171.3142 |           7.4052 |
[32m[20221213 21:26:47 @agent_ppo2.py:185][0m |          -0.0046 |         170.6245 |           7.3279 |
[32m[20221213 21:26:47 @agent_ppo2.py:185][0m |          -0.0034 |         169.9329 |           7.3616 |
[32m[20221213 21:26:47 @agent_ppo2.py:185][0m |           0.0029 |         179.3045 |           7.3328 |
[32m[20221213 21:26:47 @agent_ppo2.py:185][0m |          -0.0072 |         169.2982 |           7.3705 |
[32m[20221213 21:26:47 @agent_ppo2.py:185][0m |          -0.0043 |         172.3466 |           7.3094 |
[32m[20221213 21:26:47 @agent_ppo2.py:185][0m |          -0.0069 |         168.9502 |           7.3520 |
[32m[20221213 21:26:47 @agent_ppo2.py:185][0m |          -0.0086 |         168.7910 |           7.3897 |
[32m[20221213 21:26:48 @agent_ppo2.py:185][0m |          -0.0063 |         168.7514 |           7.3435 |
[32m[20221213 21:26:48 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:26:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.80
[32m[20221213 21:26:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:26:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.00
[32m[20221213 21:26:48 @agent_ppo2.py:143][0m Total time:      31.22 min
[32m[20221213 21:26:48 @agent_ppo2.py:145][0m 3053568 total steps have happened
[32m[20221213 21:26:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1491 --------------------------#
[32m[20221213 21:26:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:48 @agent_ppo2.py:185][0m |           0.0066 |         175.8506 |           7.6073 |
[32m[20221213 21:26:48 @agent_ppo2.py:185][0m |           0.0017 |         171.3504 |           7.6584 |
[32m[20221213 21:26:48 @agent_ppo2.py:185][0m |          -0.0034 |         169.0081 |           7.6170 |
[32m[20221213 21:26:48 @agent_ppo2.py:185][0m |          -0.0002 |         169.3222 |           7.6284 |
[32m[20221213 21:26:48 @agent_ppo2.py:185][0m |          -0.0062 |         168.1825 |           7.6039 |
[32m[20221213 21:26:48 @agent_ppo2.py:185][0m |          -0.0029 |         168.8073 |           7.6331 |
[32m[20221213 21:26:48 @agent_ppo2.py:185][0m |          -0.0058 |         167.6828 |           7.5976 |
[32m[20221213 21:26:49 @agent_ppo2.py:185][0m |          -0.0069 |         167.5276 |           7.6271 |
[32m[20221213 21:26:49 @agent_ppo2.py:185][0m |           0.0004 |         169.1106 |           7.5932 |
[32m[20221213 21:26:49 @agent_ppo2.py:185][0m |          -0.0036 |         167.7597 |           7.6104 |
[32m[20221213 21:26:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:26:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.60
[32m[20221213 21:26:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:26:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.00
[32m[20221213 21:26:49 @agent_ppo2.py:143][0m Total time:      31.24 min
[32m[20221213 21:26:49 @agent_ppo2.py:145][0m 3055616 total steps have happened
[32m[20221213 21:26:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1492 --------------------------#
[32m[20221213 21:26:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:26:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:49 @agent_ppo2.py:185][0m |           0.0087 |         181.3313 |           7.3370 |
[32m[20221213 21:26:49 @agent_ppo2.py:185][0m |          -0.0064 |         168.4914 |           7.5494 |
[32m[20221213 21:26:49 @agent_ppo2.py:185][0m |          -0.0062 |         167.7541 |           7.3999 |
[32m[20221213 21:26:49 @agent_ppo2.py:185][0m |          -0.0067 |         167.1540 |           7.4330 |
[32m[20221213 21:26:50 @agent_ppo2.py:185][0m |          -0.0083 |         166.7568 |           7.3241 |
[32m[20221213 21:26:50 @agent_ppo2.py:185][0m |          -0.0086 |         166.4244 |           7.3838 |
[32m[20221213 21:26:50 @agent_ppo2.py:185][0m |          -0.0084 |         166.0999 |           7.3776 |
[32m[20221213 21:26:50 @agent_ppo2.py:185][0m |          -0.0097 |         165.9415 |           7.3645 |
[32m[20221213 21:26:50 @agent_ppo2.py:185][0m |           0.0037 |         183.2442 |           7.3600 |
[32m[20221213 21:26:50 @agent_ppo2.py:185][0m |          -0.0079 |         166.7236 |           7.3713 |
[32m[20221213 21:26:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:26:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.80
[32m[20221213 21:26:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.00
[32m[20221213 21:26:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.00
[32m[20221213 21:26:50 @agent_ppo2.py:143][0m Total time:      31.26 min
[32m[20221213 21:26:50 @agent_ppo2.py:145][0m 3057664 total steps have happened
[32m[20221213 21:26:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1493 --------------------------#
[32m[20221213 21:26:50 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:26:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:50 @agent_ppo2.py:185][0m |          -0.0034 |         169.0840 |           7.0852 |
[32m[20221213 21:26:50 @agent_ppo2.py:185][0m |           0.0171 |         192.6554 |           7.0546 |
[32m[20221213 21:26:51 @agent_ppo2.py:185][0m |          -0.0059 |         167.6652 |           7.0711 |
[32m[20221213 21:26:51 @agent_ppo2.py:185][0m |          -0.0016 |         170.2964 |           7.0459 |
[32m[20221213 21:26:51 @agent_ppo2.py:185][0m |          -0.0072 |         167.0509 |           7.0394 |
[32m[20221213 21:26:51 @agent_ppo2.py:185][0m |          -0.0018 |         171.3442 |           7.0545 |
[32m[20221213 21:26:51 @agent_ppo2.py:185][0m |          -0.0059 |         166.8915 |           7.0709 |
[32m[20221213 21:26:51 @agent_ppo2.py:185][0m |          -0.0050 |         168.1269 |           7.0422 |
[32m[20221213 21:26:51 @agent_ppo2.py:185][0m |          -0.0056 |         167.4808 |           7.0227 |
[32m[20221213 21:26:51 @agent_ppo2.py:185][0m |          -0.0087 |         166.1720 |           7.0029 |
[32m[20221213 21:26:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.40
[32m[20221213 21:26:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:26:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:26:51 @agent_ppo2.py:143][0m Total time:      31.28 min
[32m[20221213 21:26:51 @agent_ppo2.py:145][0m 3059712 total steps have happened
[32m[20221213 21:26:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1494 --------------------------#
[32m[20221213 21:26:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:52 @agent_ppo2.py:185][0m |           0.0019 |         168.0883 |           6.9299 |
[32m[20221213 21:26:52 @agent_ppo2.py:185][0m |          -0.0039 |         166.9687 |           6.9439 |
[32m[20221213 21:26:52 @agent_ppo2.py:185][0m |          -0.0040 |         166.3040 |           6.9994 |
[32m[20221213 21:26:52 @agent_ppo2.py:185][0m |          -0.0033 |         166.3393 |           7.0062 |
[32m[20221213 21:26:52 @agent_ppo2.py:185][0m |          -0.0080 |         165.6204 |           7.0425 |
[32m[20221213 21:26:52 @agent_ppo2.py:185][0m |          -0.0073 |         165.4756 |           7.0390 |
[32m[20221213 21:26:52 @agent_ppo2.py:185][0m |          -0.0063 |         165.2183 |           7.0620 |
[32m[20221213 21:26:52 @agent_ppo2.py:185][0m |          -0.0088 |         165.1028 |           7.0395 |
[32m[20221213 21:26:52 @agent_ppo2.py:185][0m |          -0.0028 |         169.6712 |           7.0929 |
[32m[20221213 21:26:52 @agent_ppo2.py:185][0m |           0.0052 |         177.7457 |           7.1475 |
[32m[20221213 21:26:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 779.80
[32m[20221213 21:26:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.00
[32m[20221213 21:26:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.00
[32m[20221213 21:26:52 @agent_ppo2.py:143][0m Total time:      31.30 min
[32m[20221213 21:26:52 @agent_ppo2.py:145][0m 3061760 total steps have happened
[32m[20221213 21:26:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1495 --------------------------#
[32m[20221213 21:26:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:26:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:53 @agent_ppo2.py:185][0m |          -0.0018 |         168.5620 |           7.0602 |
[32m[20221213 21:26:53 @agent_ppo2.py:185][0m |           0.0106 |         186.4003 |           7.0813 |
[32m[20221213 21:26:53 @agent_ppo2.py:185][0m |          -0.0065 |         162.8219 |           7.0816 |
[32m[20221213 21:26:53 @agent_ppo2.py:185][0m |          -0.0094 |         160.8023 |           7.0664 |
[32m[20221213 21:26:53 @agent_ppo2.py:185][0m |          -0.0089 |         159.9710 |           7.0347 |
[32m[20221213 21:26:53 @agent_ppo2.py:185][0m |          -0.0100 |         159.2387 |           7.0009 |
[32m[20221213 21:26:53 @agent_ppo2.py:185][0m |          -0.0042 |         163.9127 |           7.0340 |
[32m[20221213 21:26:53 @agent_ppo2.py:185][0m |          -0.0005 |         164.7319 |           7.0445 |
[32m[20221213 21:26:53 @agent_ppo2.py:185][0m |          -0.0092 |         157.1413 |           7.0238 |
[32m[20221213 21:26:54 @agent_ppo2.py:185][0m |          -0.0050 |         161.5726 |           7.0528 |
[32m[20221213 21:26:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.60
[32m[20221213 21:26:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 21:26:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.00
[32m[20221213 21:26:54 @agent_ppo2.py:143][0m Total time:      31.32 min
[32m[20221213 21:26:54 @agent_ppo2.py:145][0m 3063808 total steps have happened
[32m[20221213 21:26:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1496 --------------------------#
[32m[20221213 21:26:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:54 @agent_ppo2.py:185][0m |           0.0001 |         172.7471 |           7.4233 |
[32m[20221213 21:26:54 @agent_ppo2.py:185][0m |          -0.0000 |         172.9995 |           7.4617 |
[32m[20221213 21:26:54 @agent_ppo2.py:185][0m |           0.0080 |         193.2621 |           7.4594 |
[32m[20221213 21:26:54 @agent_ppo2.py:185][0m |          -0.0001 |         173.4644 |           7.4630 |
[32m[20221213 21:26:54 @agent_ppo2.py:185][0m |          -0.0034 |         170.7649 |           7.6020 |
[32m[20221213 21:26:54 @agent_ppo2.py:185][0m |          -0.0086 |         170.0358 |           7.5355 |
[32m[20221213 21:26:54 @agent_ppo2.py:185][0m |          -0.0084 |         169.7923 |           7.5297 |
[32m[20221213 21:26:55 @agent_ppo2.py:185][0m |          -0.0088 |         169.6205 |           7.5656 |
[32m[20221213 21:26:55 @agent_ppo2.py:185][0m |          -0.0088 |         169.5017 |           7.5936 |
[32m[20221213 21:26:55 @agent_ppo2.py:185][0m |          -0.0081 |         169.4019 |           7.6312 |
[32m[20221213 21:26:55 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:26:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.40
[32m[20221213 21:26:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.00
[32m[20221213 21:26:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.00
[32m[20221213 21:26:55 @agent_ppo2.py:143][0m Total time:      31.34 min
[32m[20221213 21:26:55 @agent_ppo2.py:145][0m 3065856 total steps have happened
[32m[20221213 21:26:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1497 --------------------------#
[32m[20221213 21:26:55 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:26:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:55 @agent_ppo2.py:185][0m |          -0.0012 |         171.2034 |           7.1460 |
[32m[20221213 21:26:55 @agent_ppo2.py:185][0m |          -0.0027 |         169.5842 |           7.1569 |
[32m[20221213 21:26:55 @agent_ppo2.py:185][0m |          -0.0039 |         168.9319 |           7.1640 |
[32m[20221213 21:26:55 @agent_ppo2.py:185][0m |          -0.0051 |         168.5155 |           7.2053 |
[32m[20221213 21:26:56 @agent_ppo2.py:185][0m |           0.0052 |         181.4488 |           7.2300 |
[32m[20221213 21:26:56 @agent_ppo2.py:185][0m |           0.0002 |         172.9223 |           7.2579 |
[32m[20221213 21:26:56 @agent_ppo2.py:185][0m |          -0.0020 |         169.2693 |           7.2733 |
[32m[20221213 21:26:56 @agent_ppo2.py:185][0m |          -0.0080 |         167.6139 |           7.2478 |
[32m[20221213 21:26:56 @agent_ppo2.py:185][0m |          -0.0073 |         167.3547 |           7.2872 |
[32m[20221213 21:26:56 @agent_ppo2.py:185][0m |          -0.0017 |         171.0669 |           7.2600 |
[32m[20221213 21:26:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.60
[32m[20221213 21:26:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:26:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.00
[32m[20221213 21:26:56 @agent_ppo2.py:143][0m Total time:      31.36 min
[32m[20221213 21:26:56 @agent_ppo2.py:145][0m 3067904 total steps have happened
[32m[20221213 21:26:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1498 --------------------------#
[32m[20221213 21:26:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:56 @agent_ppo2.py:185][0m |           0.0003 |         169.9688 |           7.1028 |
[32m[20221213 21:26:56 @agent_ppo2.py:185][0m |          -0.0023 |         168.3905 |           7.0968 |
[32m[20221213 21:26:57 @agent_ppo2.py:185][0m |          -0.0046 |         167.5792 |           7.0968 |
[32m[20221213 21:26:57 @agent_ppo2.py:185][0m |          -0.0052 |         167.1300 |           7.0576 |
[32m[20221213 21:26:57 @agent_ppo2.py:185][0m |          -0.0073 |         166.7271 |           7.0734 |
[32m[20221213 21:26:57 @agent_ppo2.py:185][0m |          -0.0088 |         166.4450 |           7.0682 |
[32m[20221213 21:26:57 @agent_ppo2.py:185][0m |           0.0177 |         206.3053 |           7.0417 |
[32m[20221213 21:26:57 @agent_ppo2.py:185][0m |          -0.0067 |         165.8141 |           7.0692 |
[32m[20221213 21:26:57 @agent_ppo2.py:185][0m |          -0.0079 |         165.8139 |           7.0942 |
[32m[20221213 21:26:57 @agent_ppo2.py:185][0m |          -0.0088 |         165.6792 |           7.0218 |
[32m[20221213 21:26:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 760.20
[32m[20221213 21:26:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.00
[32m[20221213 21:26:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.00
[32m[20221213 21:26:57 @agent_ppo2.py:143][0m Total time:      31.38 min
[32m[20221213 21:26:57 @agent_ppo2.py:145][0m 3069952 total steps have happened
[32m[20221213 21:26:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1499 --------------------------#
[32m[20221213 21:26:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:58 @agent_ppo2.py:185][0m |          -0.0010 |         166.4479 |           7.1636 |
[32m[20221213 21:26:58 @agent_ppo2.py:185][0m |          -0.0042 |         165.7238 |           7.2626 |
[32m[20221213 21:26:58 @agent_ppo2.py:185][0m |           0.0028 |         170.4573 |           7.2846 |
[32m[20221213 21:26:58 @agent_ppo2.py:185][0m |          -0.0059 |         164.4820 |           7.3380 |
[32m[20221213 21:26:58 @agent_ppo2.py:185][0m |          -0.0046 |         165.3678 |           7.3644 |
[32m[20221213 21:26:58 @agent_ppo2.py:185][0m |           0.0069 |         184.9428 |           7.4128 |
[32m[20221213 21:26:58 @agent_ppo2.py:185][0m |          -0.0046 |         165.0979 |           7.4745 |
[32m[20221213 21:26:58 @agent_ppo2.py:185][0m |          -0.0087 |         163.1996 |           7.4937 |
[32m[20221213 21:26:58 @agent_ppo2.py:185][0m |          -0.0089 |         163.1167 |           7.4969 |
[32m[20221213 21:26:58 @agent_ppo2.py:185][0m |          -0.0088 |         162.8767 |           7.5376 |
[32m[20221213 21:26:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:26:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.20
[32m[20221213 21:26:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:26:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 640.00
[32m[20221213 21:26:58 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 860.00
[32m[20221213 21:26:58 @agent_ppo2.py:143][0m Total time:      31.40 min
[32m[20221213 21:26:58 @agent_ppo2.py:145][0m 3072000 total steps have happened
[32m[20221213 21:26:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1500 --------------------------#
[32m[20221213 21:26:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:26:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:26:59 @agent_ppo2.py:185][0m |          -0.0010 |         168.8172 |           7.7326 |
[32m[20221213 21:26:59 @agent_ppo2.py:185][0m |          -0.0036 |         167.3702 |           7.7770 |
[32m[20221213 21:26:59 @agent_ppo2.py:185][0m |          -0.0077 |         166.6324 |           7.7709 |
[32m[20221213 21:26:59 @agent_ppo2.py:185][0m |          -0.0080 |         165.9910 |           7.7879 |
[32m[20221213 21:26:59 @agent_ppo2.py:185][0m |          -0.0029 |         166.7015 |           7.8047 |
[32m[20221213 21:26:59 @agent_ppo2.py:185][0m |          -0.0093 |         165.6347 |           7.8283 |
[32m[20221213 21:26:59 @agent_ppo2.py:185][0m |          -0.0087 |         165.3869 |           7.9137 |
[32m[20221213 21:26:59 @agent_ppo2.py:185][0m |          -0.0088 |         165.1907 |           7.8981 |
[32m[20221213 21:26:59 @agent_ppo2.py:185][0m |          -0.0102 |         165.2066 |           7.8968 |
[32m[20221213 21:27:00 @agent_ppo2.py:185][0m |          -0.0085 |         164.8700 |           7.9442 |
[32m[20221213 21:27:00 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:27:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.60
[32m[20221213 21:27:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:27:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.00
[32m[20221213 21:27:00 @agent_ppo2.py:143][0m Total time:      31.42 min
[32m[20221213 21:27:00 @agent_ppo2.py:145][0m 3074048 total steps have happened
[32m[20221213 21:27:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1501 --------------------------#
[32m[20221213 21:27:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:00 @agent_ppo2.py:185][0m |          -0.0016 |         167.9385 |           7.2397 |
[32m[20221213 21:27:00 @agent_ppo2.py:185][0m |          -0.0026 |         164.6607 |           7.1919 |
[32m[20221213 21:27:00 @agent_ppo2.py:185][0m |          -0.0091 |         162.4383 |           7.1700 |
[32m[20221213 21:27:00 @agent_ppo2.py:185][0m |           0.0110 |         185.0796 |           7.1719 |
[32m[20221213 21:27:00 @agent_ppo2.py:185][0m |          -0.0099 |         160.1826 |           7.1722 |
[32m[20221213 21:27:00 @agent_ppo2.py:185][0m |          -0.0043 |         164.4247 |           7.1524 |
[32m[20221213 21:27:01 @agent_ppo2.py:185][0m |          -0.0070 |         160.0083 |           7.1028 |
[32m[20221213 21:27:01 @agent_ppo2.py:185][0m |          -0.0114 |         158.1772 |           7.1070 |
[32m[20221213 21:27:01 @agent_ppo2.py:185][0m |          -0.0137 |         157.8298 |           7.0791 |
[32m[20221213 21:27:01 @agent_ppo2.py:185][0m |          -0.0132 |         157.2273 |           7.0306 |
[32m[20221213 21:27:01 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:27:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.60
[32m[20221213 21:27:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:27:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.00
[32m[20221213 21:27:01 @agent_ppo2.py:143][0m Total time:      31.44 min
[32m[20221213 21:27:01 @agent_ppo2.py:145][0m 3076096 total steps have happened
[32m[20221213 21:27:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1502 --------------------------#
[32m[20221213 21:27:01 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:27:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:01 @agent_ppo2.py:185][0m |          -0.0017 |         169.9399 |           7.4357 |
[32m[20221213 21:27:01 @agent_ppo2.py:185][0m |          -0.0057 |         168.5014 |           7.4886 |
[32m[20221213 21:27:02 @agent_ppo2.py:185][0m |          -0.0068 |         167.5299 |           7.4767 |
[32m[20221213 21:27:02 @agent_ppo2.py:185][0m |          -0.0079 |         166.9077 |           7.5589 |
[32m[20221213 21:27:02 @agent_ppo2.py:185][0m |          -0.0068 |         166.2751 |           7.5768 |
[32m[20221213 21:27:02 @agent_ppo2.py:185][0m |          -0.0091 |         165.8826 |           7.6441 |
[32m[20221213 21:27:02 @agent_ppo2.py:185][0m |          -0.0031 |         169.6109 |           7.6141 |
[32m[20221213 21:27:02 @agent_ppo2.py:185][0m |          -0.0078 |         165.3070 |           7.6855 |
[32m[20221213 21:27:02 @agent_ppo2.py:185][0m |          -0.0092 |         165.0023 |           7.6977 |
[32m[20221213 21:27:02 @agent_ppo2.py:185][0m |          -0.0101 |         165.0971 |           7.7434 |
[32m[20221213 21:27:02 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:27:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 779.40
[32m[20221213 21:27:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.00
[32m[20221213 21:27:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.00
[32m[20221213 21:27:02 @agent_ppo2.py:143][0m Total time:      31.46 min
[32m[20221213 21:27:02 @agent_ppo2.py:145][0m 3078144 total steps have happened
[32m[20221213 21:27:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1503 --------------------------#
[32m[20221213 21:27:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:27:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:03 @agent_ppo2.py:185][0m |           0.0004 |         171.6286 |           7.6814 |
[32m[20221213 21:27:03 @agent_ppo2.py:185][0m |          -0.0016 |         170.3834 |           7.7204 |
[32m[20221213 21:27:03 @agent_ppo2.py:185][0m |          -0.0069 |         165.3109 |           7.7452 |
[32m[20221213 21:27:03 @agent_ppo2.py:185][0m |          -0.0083 |         164.0511 |           7.7296 |
[32m[20221213 21:27:03 @agent_ppo2.py:185][0m |          -0.0113 |         163.3661 |           7.6991 |
[32m[20221213 21:27:03 @agent_ppo2.py:185][0m |           0.0090 |         193.5195 |           7.7212 |
[32m[20221213 21:27:03 @agent_ppo2.py:185][0m |          -0.0103 |         162.3599 |           7.8195 |
[32m[20221213 21:27:03 @agent_ppo2.py:185][0m |          -0.0123 |         161.6088 |           7.7483 |
[32m[20221213 21:27:03 @agent_ppo2.py:185][0m |          -0.0125 |         161.1991 |           7.7503 |
[32m[20221213 21:27:04 @agent_ppo2.py:185][0m |          -0.0117 |         160.6533 |           7.7437 |
[32m[20221213 21:27:04 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:27:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.80
[32m[20221213 21:27:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:27:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.00
[32m[20221213 21:27:04 @agent_ppo2.py:143][0m Total time:      31.48 min
[32m[20221213 21:27:04 @agent_ppo2.py:145][0m 3080192 total steps have happened
[32m[20221213 21:27:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1504 --------------------------#
[32m[20221213 21:27:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:04 @agent_ppo2.py:185][0m |          -0.0008 |         174.3919 |           7.5138 |
[32m[20221213 21:27:04 @agent_ppo2.py:185][0m |          -0.0041 |         171.1719 |           7.5364 |
[32m[20221213 21:27:04 @agent_ppo2.py:185][0m |           0.0013 |         180.9134 |           7.4967 |
[32m[20221213 21:27:04 @agent_ppo2.py:185][0m |          -0.0079 |         169.1187 |           7.5549 |
[32m[20221213 21:27:04 @agent_ppo2.py:185][0m |          -0.0038 |         169.2737 |           7.5734 |
[32m[20221213 21:27:04 @agent_ppo2.py:185][0m |          -0.0098 |         168.0715 |           7.5578 |
[32m[20221213 21:27:04 @agent_ppo2.py:185][0m |          -0.0082 |         167.9974 |           7.5823 |
[32m[20221213 21:27:05 @agent_ppo2.py:185][0m |          -0.0108 |         167.6460 |           7.5646 |
[32m[20221213 21:27:05 @agent_ppo2.py:185][0m |          -0.0073 |         167.4935 |           7.5779 |
[32m[20221213 21:27:05 @agent_ppo2.py:185][0m |          -0.0084 |         167.2848 |           7.6231 |
[32m[20221213 21:27:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.60
[32m[20221213 21:27:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.00
[32m[20221213 21:27:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 21:27:05 @agent_ppo2.py:143][0m Total time:      31.50 min
[32m[20221213 21:27:05 @agent_ppo2.py:145][0m 3082240 total steps have happened
[32m[20221213 21:27:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1505 --------------------------#
[32m[20221213 21:27:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:27:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:05 @agent_ppo2.py:185][0m |          -0.0032 |         170.6716 |           7.9329 |
[32m[20221213 21:27:05 @agent_ppo2.py:185][0m |          -0.0043 |         170.0433 |           7.9683 |
[32m[20221213 21:27:05 @agent_ppo2.py:185][0m |          -0.0057 |         169.3716 |           7.9869 |
[32m[20221213 21:27:05 @agent_ppo2.py:185][0m |          -0.0075 |         168.9954 |           7.9949 |
[32m[20221213 21:27:06 @agent_ppo2.py:185][0m |          -0.0052 |         168.9075 |           8.0480 |
[32m[20221213 21:27:06 @agent_ppo2.py:185][0m |           0.0084 |         188.3965 |           8.1022 |
[32m[20221213 21:27:06 @agent_ppo2.py:185][0m |          -0.0061 |         168.4477 |           8.1650 |
[32m[20221213 21:27:06 @agent_ppo2.py:185][0m |          -0.0019 |         173.4800 |           8.1026 |
[32m[20221213 21:27:06 @agent_ppo2.py:185][0m |          -0.0095 |         167.9970 |           8.1439 |
[32m[20221213 21:27:06 @agent_ppo2.py:185][0m |          -0.0063 |         169.4700 |           8.1505 |
[32m[20221213 21:27:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:27:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.20
[32m[20221213 21:27:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:27:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.00
[32m[20221213 21:27:06 @agent_ppo2.py:143][0m Total time:      31.52 min
[32m[20221213 21:27:06 @agent_ppo2.py:145][0m 3084288 total steps have happened
[32m[20221213 21:27:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1506 --------------------------#
[32m[20221213 21:27:06 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:27:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:06 @agent_ppo2.py:185][0m |          -0.0002 |         167.3813 |           8.0280 |
[32m[20221213 21:27:06 @agent_ppo2.py:185][0m |          -0.0042 |         168.2229 |           7.9698 |
[32m[20221213 21:27:07 @agent_ppo2.py:185][0m |          -0.0050 |         166.6568 |           8.0217 |
[32m[20221213 21:27:07 @agent_ppo2.py:185][0m |          -0.0059 |         166.3096 |           7.9784 |
[32m[20221213 21:27:07 @agent_ppo2.py:185][0m |          -0.0070 |         166.1879 |           8.0301 |
[32m[20221213 21:27:07 @agent_ppo2.py:185][0m |           0.0023 |         172.0702 |           7.9871 |
[32m[20221213 21:27:07 @agent_ppo2.py:185][0m |          -0.0087 |         166.1110 |           7.9948 |
[32m[20221213 21:27:07 @agent_ppo2.py:185][0m |          -0.0035 |         168.0664 |           7.9580 |
[32m[20221213 21:27:07 @agent_ppo2.py:185][0m |          -0.0076 |         165.8497 |           7.9875 |
[32m[20221213 21:27:07 @agent_ppo2.py:185][0m |           0.0041 |         189.5289 |           7.9676 |
[32m[20221213 21:27:07 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:27:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 779.60
[32m[20221213 21:27:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.00
[32m[20221213 21:27:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.00
[32m[20221213 21:27:07 @agent_ppo2.py:143][0m Total time:      31.54 min
[32m[20221213 21:27:07 @agent_ppo2.py:145][0m 3086336 total steps have happened
[32m[20221213 21:27:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1507 --------------------------#
[32m[20221213 21:27:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:27:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:08 @agent_ppo2.py:185][0m |          -0.0018 |         166.1516 |           7.6882 |
[32m[20221213 21:27:08 @agent_ppo2.py:185][0m |          -0.0021 |         165.8737 |           7.7128 |
[32m[20221213 21:27:08 @agent_ppo2.py:185][0m |          -0.0071 |         164.6456 |           7.7329 |
[32m[20221213 21:27:08 @agent_ppo2.py:185][0m |          -0.0087 |         164.1896 |           7.7037 |
[32m[20221213 21:27:08 @agent_ppo2.py:185][0m |          -0.0078 |         163.9190 |           7.7166 |
[32m[20221213 21:27:08 @agent_ppo2.py:185][0m |          -0.0055 |         164.3699 |           7.7607 |
[32m[20221213 21:27:08 @agent_ppo2.py:185][0m |          -0.0083 |         163.4291 |           7.7280 |
[32m[20221213 21:27:09 @agent_ppo2.py:185][0m |           0.0039 |         184.6714 |           7.7633 |
[32m[20221213 21:27:09 @agent_ppo2.py:185][0m |          -0.0099 |         163.1087 |           7.7510 |
[32m[20221213 21:27:09 @agent_ppo2.py:185][0m |          -0.0089 |         163.2610 |           7.7648 |
[32m[20221213 21:27:09 @agent_ppo2.py:130][0m Policy update time: 1.24 s
[32m[20221213 21:27:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.40
[32m[20221213 21:27:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.00
[32m[20221213 21:27:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 21:27:09 @agent_ppo2.py:143][0m Total time:      31.57 min
[32m[20221213 21:27:09 @agent_ppo2.py:145][0m 3088384 total steps have happened
[32m[20221213 21:27:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1508 --------------------------#
[32m[20221213 21:27:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:27:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:09 @agent_ppo2.py:185][0m |          -0.0026 |         166.6612 |           7.7713 |
[32m[20221213 21:27:09 @agent_ppo2.py:185][0m |          -0.0054 |         166.1023 |           7.7230 |
[32m[20221213 21:27:09 @agent_ppo2.py:185][0m |          -0.0046 |         165.6815 |           7.7231 |
[32m[20221213 21:27:09 @agent_ppo2.py:185][0m |          -0.0076 |         165.3707 |           7.6948 |
[32m[20221213 21:27:09 @agent_ppo2.py:185][0m |          -0.0081 |         165.1861 |           7.7212 |
[32m[20221213 21:27:10 @agent_ppo2.py:185][0m |          -0.0064 |         165.0251 |           7.6997 |
[32m[20221213 21:27:10 @agent_ppo2.py:185][0m |          -0.0083 |         164.7714 |           7.6933 |
[32m[20221213 21:27:10 @agent_ppo2.py:185][0m |          -0.0096 |         164.8849 |           7.6553 |
[32m[20221213 21:27:10 @agent_ppo2.py:185][0m |          -0.0101 |         164.6420 |           7.6430 |
[32m[20221213 21:27:10 @agent_ppo2.py:185][0m |          -0.0097 |         164.4610 |           7.6404 |
[32m[20221213 21:27:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.60
[32m[20221213 21:27:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:27:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.00
[32m[20221213 21:27:10 @agent_ppo2.py:143][0m Total time:      31.59 min
[32m[20221213 21:27:10 @agent_ppo2.py:145][0m 3090432 total steps have happened
[32m[20221213 21:27:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1509 --------------------------#
[32m[20221213 21:27:10 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:27:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:10 @agent_ppo2.py:185][0m |           0.0008 |         167.9267 |           7.9470 |
[32m[20221213 21:27:10 @agent_ppo2.py:185][0m |          -0.0030 |         166.9579 |           7.9418 |
[32m[20221213 21:27:10 @agent_ppo2.py:185][0m |           0.0018 |         170.6578 |           7.9345 |
[32m[20221213 21:27:11 @agent_ppo2.py:185][0m |          -0.0035 |         166.2654 |           7.8895 |
[32m[20221213 21:27:11 @agent_ppo2.py:185][0m |          -0.0063 |         166.1103 |           7.8633 |
[32m[20221213 21:27:11 @agent_ppo2.py:185][0m |           0.0037 |         175.7435 |           7.8054 |
[32m[20221213 21:27:11 @agent_ppo2.py:185][0m |           0.0002 |         168.0373 |           7.8640 |
[32m[20221213 21:27:11 @agent_ppo2.py:185][0m |          -0.0016 |         168.0139 |           7.7620 |
[32m[20221213 21:27:11 @agent_ppo2.py:185][0m |          -0.0091 |         165.6947 |           7.7279 |
[32m[20221213 21:27:11 @agent_ppo2.py:185][0m |          -0.0036 |         165.8471 |           7.6997 |
[32m[20221213 21:27:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.20
[32m[20221213 21:27:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:27:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.00
[32m[20221213 21:27:11 @agent_ppo2.py:143][0m Total time:      31.61 min
[32m[20221213 21:27:11 @agent_ppo2.py:145][0m 3092480 total steps have happened
[32m[20221213 21:27:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1510 --------------------------#
[32m[20221213 21:27:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:12 @agent_ppo2.py:185][0m |           0.0002 |         170.5773 |           7.4986 |
[32m[20221213 21:27:12 @agent_ppo2.py:185][0m |           0.0029 |         171.6850 |           7.5248 |
[32m[20221213 21:27:12 @agent_ppo2.py:185][0m |          -0.0025 |         169.3908 |           7.4981 |
[32m[20221213 21:27:12 @agent_ppo2.py:185][0m |          -0.0003 |         168.8770 |           7.5473 |
[32m[20221213 21:27:12 @agent_ppo2.py:185][0m |          -0.0036 |         168.7674 |           7.5720 |
[32m[20221213 21:27:12 @agent_ppo2.py:185][0m |           0.0094 |         183.6350 |           7.5538 |
[32m[20221213 21:27:12 @agent_ppo2.py:185][0m |          -0.0035 |         168.1174 |           7.5104 |
[32m[20221213 21:27:12 @agent_ppo2.py:185][0m |          -0.0048 |         167.9107 |           7.5214 |
[32m[20221213 21:27:12 @agent_ppo2.py:185][0m |          -0.0055 |         167.6834 |           7.5297 |
[32m[20221213 21:27:12 @agent_ppo2.py:185][0m |          -0.0008 |         169.7870 |           7.5400 |
[32m[20221213 21:27:12 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:27:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.20
[32m[20221213 21:27:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.00
[32m[20221213 21:27:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.00
[32m[20221213 21:27:12 @agent_ppo2.py:143][0m Total time:      31.63 min
[32m[20221213 21:27:12 @agent_ppo2.py:145][0m 3094528 total steps have happened
[32m[20221213 21:27:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1511 --------------------------#
[32m[20221213 21:27:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:13 @agent_ppo2.py:185][0m |           0.0241 |         201.6368 |           7.1262 |
[32m[20221213 21:27:13 @agent_ppo2.py:185][0m |          -0.0032 |         168.0199 |           7.1788 |
[32m[20221213 21:27:13 @agent_ppo2.py:185][0m |          -0.0043 |         167.9684 |           7.2084 |
[32m[20221213 21:27:13 @agent_ppo2.py:185][0m |          -0.0088 |         166.7494 |           7.2117 |
[32m[20221213 21:27:13 @agent_ppo2.py:185][0m |          -0.0089 |         166.4631 |           7.1912 |
[32m[20221213 21:27:13 @agent_ppo2.py:185][0m |          -0.0073 |         166.2962 |           7.1633 |
[32m[20221213 21:27:13 @agent_ppo2.py:185][0m |          -0.0088 |         165.9154 |           7.1622 |
[32m[20221213 21:27:13 @agent_ppo2.py:185][0m |          -0.0120 |         165.8509 |           7.1807 |
[32m[20221213 21:27:13 @agent_ppo2.py:185][0m |          -0.0089 |         165.8621 |           7.1655 |
[32m[20221213 21:27:14 @agent_ppo2.py:185][0m |          -0.0008 |         177.6139 |           7.1215 |
[32m[20221213 21:27:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:27:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.60
[32m[20221213 21:27:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:27:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.00
[32m[20221213 21:27:14 @agent_ppo2.py:143][0m Total time:      31.65 min
[32m[20221213 21:27:14 @agent_ppo2.py:145][0m 3096576 total steps have happened
[32m[20221213 21:27:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1512 --------------------------#
[32m[20221213 21:27:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:14 @agent_ppo2.py:185][0m |          -0.0059 |         169.4125 |           7.0126 |
[32m[20221213 21:27:14 @agent_ppo2.py:185][0m |          -0.0089 |         168.4658 |           6.9882 |
[32m[20221213 21:27:14 @agent_ppo2.py:185][0m |          -0.0083 |         167.8993 |           6.9663 |
[32m[20221213 21:27:14 @agent_ppo2.py:185][0m |          -0.0104 |         167.6825 |           6.9806 |
[32m[20221213 21:27:14 @agent_ppo2.py:185][0m |          -0.0080 |         167.8453 |           6.9973 |
[32m[20221213 21:27:14 @agent_ppo2.py:185][0m |          -0.0106 |         167.3385 |           6.9750 |
[32m[20221213 21:27:14 @agent_ppo2.py:185][0m |          -0.0102 |         167.0371 |           6.9728 |
[32m[20221213 21:27:15 @agent_ppo2.py:185][0m |          -0.0110 |         166.9117 |           6.9597 |
[32m[20221213 21:27:15 @agent_ppo2.py:185][0m |          -0.0100 |         166.7471 |           6.9802 |
[32m[20221213 21:27:15 @agent_ppo2.py:185][0m |          -0.0106 |         166.6013 |           6.9586 |
[32m[20221213 21:27:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.40
[32m[20221213 21:27:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:27:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:27:15 @agent_ppo2.py:143][0m Total time:      31.67 min
[32m[20221213 21:27:15 @agent_ppo2.py:145][0m 3098624 total steps have happened
[32m[20221213 21:27:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1513 --------------------------#
[32m[20221213 21:27:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:15 @agent_ppo2.py:185][0m |          -0.0013 |         169.1162 |           7.1453 |
[32m[20221213 21:27:15 @agent_ppo2.py:185][0m |           0.0003 |         170.0393 |           7.1862 |
[32m[20221213 21:27:15 @agent_ppo2.py:185][0m |          -0.0060 |         166.9289 |           7.2113 |
[32m[20221213 21:27:15 @agent_ppo2.py:185][0m |          -0.0049 |         166.5285 |           7.2426 |
[32m[20221213 21:27:15 @agent_ppo2.py:185][0m |          -0.0014 |         169.7709 |           7.2633 |
[32m[20221213 21:27:16 @agent_ppo2.py:185][0m |          -0.0070 |         165.6779 |           7.2961 |
[32m[20221213 21:27:16 @agent_ppo2.py:185][0m |          -0.0069 |         165.4739 |           7.3156 |
[32m[20221213 21:27:16 @agent_ppo2.py:185][0m |          -0.0038 |         167.6797 |           7.3300 |
[32m[20221213 21:27:16 @agent_ppo2.py:185][0m |          -0.0094 |         164.8754 |           7.4025 |
[32m[20221213 21:27:16 @agent_ppo2.py:185][0m |           0.0014 |         177.2824 |           7.3962 |
[32m[20221213 21:27:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.60
[32m[20221213 21:27:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:27:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.00
[32m[20221213 21:27:16 @agent_ppo2.py:143][0m Total time:      31.69 min
[32m[20221213 21:27:16 @agent_ppo2.py:145][0m 3100672 total steps have happened
[32m[20221213 21:27:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1514 --------------------------#
[32m[20221213 21:27:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:16 @agent_ppo2.py:185][0m |           0.0039 |         169.7712 |           7.3577 |
[32m[20221213 21:27:16 @agent_ppo2.py:185][0m |          -0.0035 |         168.3945 |           7.2476 |
[32m[20221213 21:27:16 @agent_ppo2.py:185][0m |          -0.0020 |         168.2307 |           7.2287 |
[32m[20221213 21:27:17 @agent_ppo2.py:185][0m |          -0.0050 |         167.6388 |           7.2366 |
[32m[20221213 21:27:17 @agent_ppo2.py:185][0m |          -0.0039 |         167.3452 |           7.1915 |
[32m[20221213 21:27:17 @agent_ppo2.py:185][0m |           0.0026 |         180.3695 |           7.2287 |
[32m[20221213 21:27:17 @agent_ppo2.py:185][0m |          -0.0072 |         167.0032 |           7.1910 |
[32m[20221213 21:27:17 @agent_ppo2.py:185][0m |          -0.0079 |         166.6546 |           7.1919 |
[32m[20221213 21:27:17 @agent_ppo2.py:185][0m |          -0.0091 |         166.5832 |           7.1980 |
[32m[20221213 21:27:17 @agent_ppo2.py:185][0m |          -0.0008 |         172.4245 |           7.1840 |
[32m[20221213 21:27:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:27:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.00
[32m[20221213 21:27:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.00
[32m[20221213 21:27:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.00
[32m[20221213 21:27:17 @agent_ppo2.py:143][0m Total time:      31.71 min
[32m[20221213 21:27:17 @agent_ppo2.py:145][0m 3102720 total steps have happened
[32m[20221213 21:27:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1515 --------------------------#
[32m[20221213 21:27:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:18 @agent_ppo2.py:185][0m |          -0.0009 |         171.2640 |           7.5317 |
[32m[20221213 21:27:18 @agent_ppo2.py:185][0m |          -0.0035 |         169.4502 |           7.5396 |
[32m[20221213 21:27:18 @agent_ppo2.py:185][0m |          -0.0026 |         168.6885 |           7.5327 |
[32m[20221213 21:27:18 @agent_ppo2.py:185][0m |          -0.0036 |         168.4283 |           7.4619 |
[32m[20221213 21:27:18 @agent_ppo2.py:185][0m |          -0.0076 |         168.0783 |           7.5314 |
[32m[20221213 21:27:18 @agent_ppo2.py:185][0m |          -0.0033 |         168.8227 |           7.4645 |
[32m[20221213 21:27:18 @agent_ppo2.py:185][0m |          -0.0037 |         167.6662 |           7.5128 |
[32m[20221213 21:27:18 @agent_ppo2.py:185][0m |          -0.0074 |         167.3989 |           7.5183 |
[32m[20221213 21:27:18 @agent_ppo2.py:185][0m |          -0.0086 |         167.0378 |           7.5110 |
[32m[20221213 21:27:18 @agent_ppo2.py:185][0m |          -0.0092 |         166.8970 |           7.5082 |
[32m[20221213 21:27:18 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:27:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 757.40
[32m[20221213 21:27:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 775.00
[32m[20221213 21:27:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.00
[32m[20221213 21:27:18 @agent_ppo2.py:143][0m Total time:      31.73 min
[32m[20221213 21:27:18 @agent_ppo2.py:145][0m 3104768 total steps have happened
[32m[20221213 21:27:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1516 --------------------------#
[32m[20221213 21:27:19 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:27:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:19 @agent_ppo2.py:185][0m |          -0.0032 |         171.2730 |           7.1925 |
[32m[20221213 21:27:19 @agent_ppo2.py:185][0m |          -0.0024 |         170.4842 |           7.2953 |
[32m[20221213 21:27:19 @agent_ppo2.py:185][0m |           0.0004 |         174.1195 |           7.2406 |
[32m[20221213 21:27:19 @agent_ppo2.py:185][0m |          -0.0084 |         169.7479 |           7.2146 |
[32m[20221213 21:27:19 @agent_ppo2.py:185][0m |          -0.0054 |         169.6634 |           7.2684 |
[32m[20221213 21:27:19 @agent_ppo2.py:185][0m |          -0.0099 |         169.2209 |           7.2903 |
[32m[20221213 21:27:19 @agent_ppo2.py:185][0m |          -0.0095 |         168.9559 |           7.2579 |
[32m[20221213 21:27:19 @agent_ppo2.py:185][0m |          -0.0053 |         169.2252 |           7.2734 |
[32m[20221213 21:27:19 @agent_ppo2.py:185][0m |           0.0025 |         179.9232 |           7.2140 |
[32m[20221213 21:27:20 @agent_ppo2.py:185][0m |           0.0144 |         193.5538 |           7.2677 |
[32m[20221213 21:27:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.00
[32m[20221213 21:27:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:27:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.00
[32m[20221213 21:27:20 @agent_ppo2.py:143][0m Total time:      31.75 min
[32m[20221213 21:27:20 @agent_ppo2.py:145][0m 3106816 total steps have happened
[32m[20221213 21:27:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1517 --------------------------#
[32m[20221213 21:27:20 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:27:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:20 @agent_ppo2.py:185][0m |          -0.0008 |         169.6553 |           7.2725 |
[32m[20221213 21:27:20 @agent_ppo2.py:185][0m |          -0.0039 |         168.8287 |           7.2680 |
[32m[20221213 21:27:20 @agent_ppo2.py:185][0m |           0.0121 |         188.7920 |           7.3069 |
[32m[20221213 21:27:20 @agent_ppo2.py:185][0m |          -0.0046 |         168.2684 |           7.3071 |
[32m[20221213 21:27:20 @agent_ppo2.py:185][0m |          -0.0058 |         168.2557 |           7.3967 |
[32m[20221213 21:27:20 @agent_ppo2.py:185][0m |          -0.0071 |         168.1307 |           7.4161 |
[32m[20221213 21:27:20 @agent_ppo2.py:185][0m |          -0.0070 |         167.9304 |           7.4555 |
[32m[20221213 21:27:21 @agent_ppo2.py:185][0m |          -0.0058 |         167.8525 |           7.4468 |
[32m[20221213 21:27:21 @agent_ppo2.py:185][0m |          -0.0075 |         167.8635 |           7.5491 |
[32m[20221213 21:27:21 @agent_ppo2.py:185][0m |          -0.0070 |         167.7344 |           7.5391 |
[32m[20221213 21:27:21 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:27:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.80
[32m[20221213 21:27:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:27:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:27:21 @agent_ppo2.py:143][0m Total time:      31.77 min
[32m[20221213 21:27:21 @agent_ppo2.py:145][0m 3108864 total steps have happened
[32m[20221213 21:27:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1518 --------------------------#
[32m[20221213 21:27:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:27:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:21 @agent_ppo2.py:185][0m |           0.0054 |         172.6588 |           7.7982 |
[32m[20221213 21:27:21 @agent_ppo2.py:185][0m |          -0.0030 |         169.0161 |           7.7615 |
[32m[20221213 21:27:21 @agent_ppo2.py:185][0m |           0.0097 |         185.7131 |           7.8169 |
[32m[20221213 21:27:21 @agent_ppo2.py:185][0m |          -0.0043 |         168.1080 |           7.8676 |
[32m[20221213 21:27:22 @agent_ppo2.py:185][0m |          -0.0015 |         168.8965 |           7.8433 |
[32m[20221213 21:27:22 @agent_ppo2.py:185][0m |           0.0009 |         176.2991 |           7.8089 |
[32m[20221213 21:27:22 @agent_ppo2.py:185][0m |          -0.0048 |         167.8572 |           7.8240 |
[32m[20221213 21:27:22 @agent_ppo2.py:185][0m |          -0.0062 |         167.3972 |           7.7979 |
[32m[20221213 21:27:22 @agent_ppo2.py:185][0m |          -0.0078 |         167.3155 |           7.8025 |
[32m[20221213 21:27:22 @agent_ppo2.py:185][0m |          -0.0056 |         167.5454 |           7.8157 |
[32m[20221213 21:27:22 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:27:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.00
[32m[20221213 21:27:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:27:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.00
[32m[20221213 21:27:22 @agent_ppo2.py:143][0m Total time:      31.79 min
[32m[20221213 21:27:22 @agent_ppo2.py:145][0m 3110912 total steps have happened
[32m[20221213 21:27:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1519 --------------------------#
[32m[20221213 21:27:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:22 @agent_ppo2.py:185][0m |          -0.0008 |         168.0298 |           7.9151 |
[32m[20221213 21:27:23 @agent_ppo2.py:185][0m |          -0.0024 |         167.5966 |           7.8498 |
[32m[20221213 21:27:23 @agent_ppo2.py:185][0m |          -0.0038 |         167.2692 |           7.8688 |
[32m[20221213 21:27:23 @agent_ppo2.py:185][0m |          -0.0063 |         167.0092 |           7.8446 |
[32m[20221213 21:27:23 @agent_ppo2.py:185][0m |          -0.0056 |         166.8202 |           7.8590 |
[32m[20221213 21:27:23 @agent_ppo2.py:185][0m |          -0.0047 |         166.6986 |           7.8717 |
[32m[20221213 21:27:23 @agent_ppo2.py:185][0m |          -0.0073 |         166.4737 |           7.8430 |
[32m[20221213 21:27:23 @agent_ppo2.py:185][0m |           0.0057 |         179.6919 |           7.8118 |
[32m[20221213 21:27:23 @agent_ppo2.py:185][0m |          -0.0080 |         166.3622 |           7.8148 |
[32m[20221213 21:27:23 @agent_ppo2.py:185][0m |          -0.0081 |         166.0465 |           7.7865 |
[32m[20221213 21:27:23 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:27:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.40
[32m[20221213 21:27:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.00
[32m[20221213 21:27:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.00
[32m[20221213 21:27:23 @agent_ppo2.py:143][0m Total time:      31.81 min
[32m[20221213 21:27:23 @agent_ppo2.py:145][0m 3112960 total steps have happened
[32m[20221213 21:27:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1520 --------------------------#
[32m[20221213 21:27:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:24 @agent_ppo2.py:185][0m |           0.0044 |         169.4287 |           7.3906 |
[32m[20221213 21:27:24 @agent_ppo2.py:185][0m |          -0.0061 |         167.3924 |           7.4426 |
[32m[20221213 21:27:24 @agent_ppo2.py:185][0m |          -0.0052 |         166.7823 |           7.4191 |
[32m[20221213 21:27:24 @agent_ppo2.py:185][0m |          -0.0080 |         166.3153 |           7.3635 |
[32m[20221213 21:27:24 @agent_ppo2.py:185][0m |          -0.0065 |         165.8738 |           7.4213 |
[32m[20221213 21:27:24 @agent_ppo2.py:185][0m |          -0.0085 |         165.6283 |           7.3661 |
[32m[20221213 21:27:24 @agent_ppo2.py:185][0m |          -0.0036 |         167.3250 |           7.3802 |
[32m[20221213 21:27:24 @agent_ppo2.py:185][0m |          -0.0088 |         165.4344 |           7.3947 |
[32m[20221213 21:27:24 @agent_ppo2.py:185][0m |          -0.0085 |         165.3987 |           7.3237 |
[32m[20221213 21:27:25 @agent_ppo2.py:185][0m |          -0.0091 |         165.3029 |           7.3797 |
[32m[20221213 21:27:25 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:27:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.00
[32m[20221213 21:27:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:27:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.00
[32m[20221213 21:27:25 @agent_ppo2.py:143][0m Total time:      31.83 min
[32m[20221213 21:27:25 @agent_ppo2.py:145][0m 3115008 total steps have happened
[32m[20221213 21:27:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1521 --------------------------#
[32m[20221213 21:27:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:27:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:25 @agent_ppo2.py:185][0m |          -0.0013 |         171.1143 |           7.2887 |
[32m[20221213 21:27:25 @agent_ppo2.py:185][0m |          -0.0062 |         170.2827 |           7.3652 |
[32m[20221213 21:27:25 @agent_ppo2.py:185][0m |          -0.0061 |         169.7662 |           7.3529 |
[32m[20221213 21:27:25 @agent_ppo2.py:185][0m |          -0.0078 |         169.5333 |           7.4461 |
[32m[20221213 21:27:25 @agent_ppo2.py:185][0m |           0.0026 |         184.8711 |           7.4452 |
[32m[20221213 21:27:25 @agent_ppo2.py:185][0m |          -0.0081 |         169.3951 |           7.4734 |
[32m[20221213 21:27:26 @agent_ppo2.py:185][0m |           0.0007 |         183.0013 |           7.5366 |
[32m[20221213 21:27:26 @agent_ppo2.py:185][0m |           0.0025 |         186.3066 |           7.5405 |
[32m[20221213 21:27:26 @agent_ppo2.py:185][0m |          -0.0075 |         168.8333 |           7.6409 |
[32m[20221213 21:27:26 @agent_ppo2.py:185][0m |          -0.0096 |         168.6312 |           7.5845 |
[32m[20221213 21:27:26 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:27:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.80
[32m[20221213 21:27:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.00
[32m[20221213 21:27:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.00
[32m[20221213 21:27:26 @agent_ppo2.py:143][0m Total time:      31.85 min
[32m[20221213 21:27:26 @agent_ppo2.py:145][0m 3117056 total steps have happened
[32m[20221213 21:27:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1522 --------------------------#
[32m[20221213 21:27:26 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:27:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:26 @agent_ppo2.py:185][0m |           0.0114 |         189.4287 |           7.7465 |
[32m[20221213 21:27:26 @agent_ppo2.py:185][0m |           0.0046 |         172.2526 |           7.7978 |
[32m[20221213 21:27:26 @agent_ppo2.py:185][0m |          -0.0038 |         167.5846 |           7.7185 |
[32m[20221213 21:27:26 @agent_ppo2.py:185][0m |          -0.0059 |         167.1714 |           7.7600 |
[32m[20221213 21:27:27 @agent_ppo2.py:185][0m |          -0.0062 |         166.9435 |           7.7972 |
[32m[20221213 21:27:27 @agent_ppo2.py:185][0m |           0.0040 |         172.4374 |           7.7159 |
[32m[20221213 21:27:27 @agent_ppo2.py:185][0m |          -0.0043 |         166.6454 |           7.7441 |
[32m[20221213 21:27:27 @agent_ppo2.py:185][0m |          -0.0047 |         166.2631 |           7.7378 |
[32m[20221213 21:27:27 @agent_ppo2.py:185][0m |          -0.0064 |         166.1093 |           7.6986 |
[32m[20221213 21:27:27 @agent_ppo2.py:185][0m |          -0.0071 |         165.9163 |           7.7031 |
[32m[20221213 21:27:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 755.00
[32m[20221213 21:27:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:27:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.00
[32m[20221213 21:27:27 @agent_ppo2.py:143][0m Total time:      31.87 min
[32m[20221213 21:27:27 @agent_ppo2.py:145][0m 3119104 total steps have happened
[32m[20221213 21:27:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1523 --------------------------#
[32m[20221213 21:27:27 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:27:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:27 @agent_ppo2.py:185][0m |          -0.0028 |         169.0850 |           7.7906 |
[32m[20221213 21:27:27 @agent_ppo2.py:185][0m |          -0.0055 |         168.2351 |           7.7958 |
[32m[20221213 21:27:28 @agent_ppo2.py:185][0m |          -0.0052 |         167.5880 |           7.7846 |
[32m[20221213 21:27:28 @agent_ppo2.py:185][0m |          -0.0062 |         167.3256 |           7.7804 |
[32m[20221213 21:27:28 @agent_ppo2.py:185][0m |          -0.0061 |         167.0975 |           7.7797 |
[32m[20221213 21:27:28 @agent_ppo2.py:185][0m |          -0.0078 |         166.8354 |           7.7696 |
[32m[20221213 21:27:28 @agent_ppo2.py:185][0m |          -0.0082 |         166.6036 |           7.7580 |
[32m[20221213 21:27:28 @agent_ppo2.py:185][0m |          -0.0095 |         166.5058 |           7.7853 |
[32m[20221213 21:27:28 @agent_ppo2.py:185][0m |          -0.0064 |         166.4067 |           7.7442 |
[32m[20221213 21:27:28 @agent_ppo2.py:185][0m |          -0.0060 |         167.5314 |           7.7161 |
[32m[20221213 21:27:28 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:27:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 772.60
[32m[20221213 21:27:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.00
[32m[20221213 21:27:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.00
[32m[20221213 21:27:28 @agent_ppo2.py:143][0m Total time:      31.89 min
[32m[20221213 21:27:28 @agent_ppo2.py:145][0m 3121152 total steps have happened
[32m[20221213 21:27:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1524 --------------------------#
[32m[20221213 21:27:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:29 @agent_ppo2.py:185][0m |          -0.0012 |         171.7793 |           7.3506 |
[32m[20221213 21:27:29 @agent_ppo2.py:185][0m |          -0.0038 |         170.7378 |           7.3422 |
[32m[20221213 21:27:29 @agent_ppo2.py:185][0m |          -0.0040 |         170.0935 |           7.3761 |
[32m[20221213 21:27:29 @agent_ppo2.py:185][0m |          -0.0064 |         169.5120 |           7.2746 |
[32m[20221213 21:27:29 @agent_ppo2.py:185][0m |          -0.0065 |         169.1650 |           7.3113 |
[32m[20221213 21:27:29 @agent_ppo2.py:185][0m |          -0.0018 |         171.6765 |           7.3470 |
[32m[20221213 21:27:29 @agent_ppo2.py:185][0m |          -0.0076 |         168.7346 |           7.3318 |
[32m[20221213 21:27:29 @agent_ppo2.py:185][0m |          -0.0040 |         171.0284 |           7.3220 |
[32m[20221213 21:27:29 @agent_ppo2.py:185][0m |          -0.0063 |         168.2536 |           7.3429 |
[32m[20221213 21:27:29 @agent_ppo2.py:185][0m |          -0.0076 |         168.2628 |           7.3458 |
[32m[20221213 21:27:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:27:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.00
[32m[20221213 21:27:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.00
[32m[20221213 21:27:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.00
[32m[20221213 21:27:29 @agent_ppo2.py:143][0m Total time:      31.91 min
[32m[20221213 21:27:29 @agent_ppo2.py:145][0m 3123200 total steps have happened
[32m[20221213 21:27:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1525 --------------------------#
[32m[20221213 21:27:30 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:27:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:30 @agent_ppo2.py:185][0m |           0.0028 |         172.3917 |           7.2198 |
[32m[20221213 21:27:30 @agent_ppo2.py:185][0m |           0.0099 |         184.6402 |           7.2565 |
[32m[20221213 21:27:30 @agent_ppo2.py:185][0m |          -0.0018 |         169.4327 |           7.4062 |
[32m[20221213 21:27:30 @agent_ppo2.py:185][0m |           0.0031 |         176.6820 |           7.2755 |
[32m[20221213 21:27:30 @agent_ppo2.py:185][0m |          -0.0035 |         169.0021 |           7.2948 |
[32m[20221213 21:27:30 @agent_ppo2.py:185][0m |          -0.0056 |         168.8944 |           7.2484 |
[32m[20221213 21:27:30 @agent_ppo2.py:185][0m |          -0.0029 |         169.9284 |           7.2752 |
[32m[20221213 21:27:30 @agent_ppo2.py:185][0m |           0.0049 |         184.1069 |           7.3032 |
[32m[20221213 21:27:31 @agent_ppo2.py:185][0m |          -0.0008 |         170.8846 |           7.2481 |
[32m[20221213 21:27:31 @agent_ppo2.py:185][0m |          -0.0072 |         168.3449 |           7.2989 |
[32m[20221213 21:27:31 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:27:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.60
[32m[20221213 21:27:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.00
[32m[20221213 21:27:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.00
[32m[20221213 21:27:31 @agent_ppo2.py:143][0m Total time:      31.93 min
[32m[20221213 21:27:31 @agent_ppo2.py:145][0m 3125248 total steps have happened
[32m[20221213 21:27:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1526 --------------------------#
[32m[20221213 21:27:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:31 @agent_ppo2.py:185][0m |          -0.0036 |         170.5881 |           7.0639 |
[32m[20221213 21:27:31 @agent_ppo2.py:185][0m |           0.0043 |         186.3598 |           7.0956 |
[32m[20221213 21:27:31 @agent_ppo2.py:185][0m |          -0.0082 |         168.9112 |           7.1267 |
[32m[20221213 21:27:31 @agent_ppo2.py:185][0m |          -0.0089 |         168.0612 |           7.1162 |
[32m[20221213 21:27:31 @agent_ppo2.py:185][0m |          -0.0078 |         167.7491 |           7.1846 |
[32m[20221213 21:27:31 @agent_ppo2.py:185][0m |          -0.0093 |         167.9816 |           7.1641 |
[32m[20221213 21:27:32 @agent_ppo2.py:185][0m |          -0.0121 |         167.0941 |           7.1677 |
[32m[20221213 21:27:32 @agent_ppo2.py:185][0m |          -0.0097 |         166.7510 |           7.1796 |
[32m[20221213 21:27:32 @agent_ppo2.py:185][0m |           0.0015 |         187.4500 |           7.2252 |
[32m[20221213 21:27:32 @agent_ppo2.py:185][0m |          -0.0096 |         166.4339 |           7.2210 |
[32m[20221213 21:27:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 758.40
[32m[20221213 21:27:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.00
[32m[20221213 21:27:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.00
[32m[20221213 21:27:32 @agent_ppo2.py:143][0m Total time:      31.95 min
[32m[20221213 21:27:32 @agent_ppo2.py:145][0m 3127296 total steps have happened
[32m[20221213 21:27:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1527 --------------------------#
[32m[20221213 21:27:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:32 @agent_ppo2.py:185][0m |           0.0003 |         168.2980 |           7.3156 |
[32m[20221213 21:27:32 @agent_ppo2.py:185][0m |          -0.0010 |         167.8945 |           7.3181 |
[32m[20221213 21:27:32 @agent_ppo2.py:185][0m |          -0.0045 |         167.4950 |           7.3676 |
[32m[20221213 21:27:32 @agent_ppo2.py:185][0m |           0.0014 |         172.5250 |           7.3389 |
[32m[20221213 21:27:33 @agent_ppo2.py:185][0m |          -0.0024 |         167.0810 |           7.3375 |
[32m[20221213 21:27:33 @agent_ppo2.py:185][0m |          -0.0061 |         166.9934 |           7.3588 |
[32m[20221213 21:27:33 @agent_ppo2.py:185][0m |          -0.0064 |         166.8418 |           7.3635 |
[32m[20221213 21:27:33 @agent_ppo2.py:185][0m |          -0.0003 |         168.2073 |           7.3197 |
[32m[20221213 21:27:33 @agent_ppo2.py:185][0m |          -0.0058 |         166.6892 |           7.3737 |
[32m[20221213 21:27:33 @agent_ppo2.py:185][0m |          -0.0034 |         169.5251 |           7.3209 |
[32m[20221213 21:27:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.00
[32m[20221213 21:27:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.00
[32m[20221213 21:27:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 801.00
[32m[20221213 21:27:33 @agent_ppo2.py:143][0m Total time:      31.97 min
[32m[20221213 21:27:33 @agent_ppo2.py:145][0m 3129344 total steps have happened
[32m[20221213 21:27:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1528 --------------------------#
[32m[20221213 21:27:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:33 @agent_ppo2.py:185][0m |          -0.0010 |         169.9029 |           7.8633 |
[32m[20221213 21:27:33 @agent_ppo2.py:185][0m |          -0.0044 |         169.1853 |           7.9231 |
[32m[20221213 21:27:34 @agent_ppo2.py:185][0m |          -0.0048 |         168.8662 |           7.9120 |
[32m[20221213 21:27:34 @agent_ppo2.py:185][0m |           0.0004 |         170.8999 |           7.9925 |
[32m[20221213 21:27:34 @agent_ppo2.py:185][0m |          -0.0016 |         173.0103 |           7.9942 |
[32m[20221213 21:27:34 @agent_ppo2.py:185][0m |          -0.0066 |         167.9182 |           8.0129 |
[32m[20221213 21:27:34 @agent_ppo2.py:185][0m |          -0.0052 |         168.2832 |           8.0324 |
[32m[20221213 21:27:34 @agent_ppo2.py:185][0m |          -0.0069 |         167.7393 |           8.0289 |
[32m[20221213 21:27:34 @agent_ppo2.py:185][0m |          -0.0002 |         181.0084 |           8.0505 |
[32m[20221213 21:27:34 @agent_ppo2.py:185][0m |          -0.0084 |         167.5953 |           8.0685 |
[32m[20221213 21:27:34 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:27:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 769.20
[32m[20221213 21:27:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.00
[32m[20221213 21:27:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.00
[32m[20221213 21:27:34 @agent_ppo2.py:143][0m Total time:      31.99 min
[32m[20221213 21:27:34 @agent_ppo2.py:145][0m 3131392 total steps have happened
[32m[20221213 21:27:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1529 --------------------------#
[32m[20221213 21:27:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:35 @agent_ppo2.py:185][0m |          -0.0017 |         169.4088 |           7.3038 |
[32m[20221213 21:27:35 @agent_ppo2.py:185][0m |          -0.0034 |         168.6543 |           7.2898 |
[32m[20221213 21:27:35 @agent_ppo2.py:185][0m |          -0.0078 |         168.2886 |           7.3125 |
[32m[20221213 21:27:35 @agent_ppo2.py:185][0m |          -0.0053 |         169.8680 |           7.2885 |
[32m[20221213 21:27:35 @agent_ppo2.py:185][0m |          -0.0079 |         167.9156 |           7.3044 |
[32m[20221213 21:27:35 @agent_ppo2.py:185][0m |          -0.0088 |         167.7531 |           7.2872 |
[32m[20221213 21:27:35 @agent_ppo2.py:185][0m |          -0.0102 |         167.6693 |           7.3046 |
[32m[20221213 21:27:35 @agent_ppo2.py:185][0m |          -0.0082 |         167.5710 |           7.2605 |
[32m[20221213 21:27:35 @agent_ppo2.py:185][0m |           0.0037 |         182.0923 |           7.2948 |
[32m[20221213 21:27:35 @agent_ppo2.py:185][0m |          -0.0064 |         167.3937 |           7.2969 |
[32m[20221213 21:27:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:27:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.00
[32m[20221213 21:27:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.00
[32m[20221213 21:27:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.00
[32m[20221213 21:27:35 @agent_ppo2.py:143][0m Total time:      32.01 min
[32m[20221213 21:27:35 @agent_ppo2.py:145][0m 3133440 total steps have happened
[32m[20221213 21:27:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1530 --------------------------#
[32m[20221213 21:27:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:36 @agent_ppo2.py:185][0m |          -0.0002 |         167.1937 |           7.3896 |
[32m[20221213 21:27:36 @agent_ppo2.py:185][0m |           0.0166 |         183.4357 |           7.4520 |
[32m[20221213 21:27:36 @agent_ppo2.py:185][0m |          -0.0035 |         166.6865 |           7.4667 |
[32m[20221213 21:27:36 @agent_ppo2.py:185][0m |           0.0046 |         169.3984 |           7.4562 |
[32m[20221213 21:27:36 @agent_ppo2.py:185][0m |           0.0091 |         176.8258 |           7.5038 |
[32m[20221213 21:27:36 @agent_ppo2.py:185][0m |           0.0007 |         172.5904 |           7.5178 |
[32m[20221213 21:27:36 @agent_ppo2.py:185][0m |          -0.0051 |         166.0427 |           7.5249 |
[32m[20221213 21:27:36 @agent_ppo2.py:185][0m |          -0.0081 |         165.8481 |           7.5493 |
[32m[20221213 21:27:37 @agent_ppo2.py:185][0m |          -0.0069 |         165.6414 |           7.5501 |
[32m[20221213 21:27:37 @agent_ppo2.py:185][0m |           0.0056 |         183.2350 |           7.5619 |
[32m[20221213 21:27:37 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:27:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.20
[32m[20221213 21:27:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.00
[32m[20221213 21:27:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 864.00
[32m[20221213 21:27:37 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 864.00
[32m[20221213 21:27:37 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 864.00
[32m[20221213 21:27:37 @agent_ppo2.py:143][0m Total time:      32.03 min
[32m[20221213 21:27:37 @agent_ppo2.py:145][0m 3135488 total steps have happened
[32m[20221213 21:27:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1531 --------------------------#
[32m[20221213 21:27:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:27:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:37 @agent_ppo2.py:185][0m |          -0.0004 |         169.4886 |           7.5859 |
[32m[20221213 21:27:37 @agent_ppo2.py:185][0m |          -0.0039 |         168.0360 |           7.5958 |
[32m[20221213 21:27:37 @agent_ppo2.py:185][0m |          -0.0048 |         167.4112 |           7.5069 |
[32m[20221213 21:27:37 @agent_ppo2.py:185][0m |          -0.0042 |         167.5114 |           7.4958 |
[32m[20221213 21:27:37 @agent_ppo2.py:185][0m |          -0.0077 |         166.3697 |           7.4739 |
[32m[20221213 21:27:37 @agent_ppo2.py:185][0m |          -0.0093 |         166.1685 |           7.4869 |
[32m[20221213 21:27:38 @agent_ppo2.py:185][0m |          -0.0098 |         165.8664 |           7.4170 |
[32m[20221213 21:27:38 @agent_ppo2.py:185][0m |          -0.0093 |         165.5297 |           7.4170 |
[32m[20221213 21:27:38 @agent_ppo2.py:185][0m |          -0.0113 |         165.2419 |           7.3889 |
[32m[20221213 21:27:38 @agent_ppo2.py:185][0m |          -0.0085 |         165.4090 |           7.3766 |
[32m[20221213 21:27:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.60
[32m[20221213 21:27:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:27:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.00
[32m[20221213 21:27:38 @agent_ppo2.py:143][0m Total time:      32.05 min
[32m[20221213 21:27:38 @agent_ppo2.py:145][0m 3137536 total steps have happened
[32m[20221213 21:27:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1532 --------------------------#
[32m[20221213 21:27:38 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:27:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:38 @agent_ppo2.py:185][0m |          -0.0001 |         171.1025 |           6.9110 |
[32m[20221213 21:27:38 @agent_ppo2.py:185][0m |          -0.0034 |         167.4757 |           6.8574 |
[32m[20221213 21:27:38 @agent_ppo2.py:185][0m |          -0.0020 |         166.7578 |           6.9090 |
[32m[20221213 21:27:38 @agent_ppo2.py:185][0m |          -0.0028 |         165.8670 |           6.8662 |
[32m[20221213 21:27:39 @agent_ppo2.py:185][0m |           0.0066 |         181.2405 |           6.8886 |
[32m[20221213 21:27:39 @agent_ppo2.py:185][0m |          -0.0022 |         164.8095 |           6.8955 |
[32m[20221213 21:27:39 @agent_ppo2.py:185][0m |          -0.0056 |         164.5882 |           6.9186 |
[32m[20221213 21:27:39 @agent_ppo2.py:185][0m |           0.0043 |         178.7377 |           6.8412 |
[32m[20221213 21:27:39 @agent_ppo2.py:185][0m |          -0.0095 |         164.1739 |           6.8402 |
[32m[20221213 21:27:39 @agent_ppo2.py:185][0m |          -0.0019 |         165.3153 |           6.8175 |
[32m[20221213 21:27:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:27:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 632.20
[32m[20221213 21:27:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 672.00
[32m[20221213 21:27:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.00
[32m[20221213 21:27:39 @agent_ppo2.py:143][0m Total time:      32.07 min
[32m[20221213 21:27:39 @agent_ppo2.py:145][0m 3139584 total steps have happened
[32m[20221213 21:27:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1533 --------------------------#
[32m[20221213 21:27:39 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:27:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:39 @agent_ppo2.py:185][0m |          -0.0014 |         169.8149 |           7.3960 |
[32m[20221213 21:27:39 @agent_ppo2.py:185][0m |          -0.0059 |         168.6746 |           7.4259 |
[32m[20221213 21:27:40 @agent_ppo2.py:185][0m |          -0.0075 |         167.9888 |           7.4511 |
[32m[20221213 21:27:40 @agent_ppo2.py:185][0m |          -0.0095 |         167.6867 |           7.4293 |
[32m[20221213 21:27:40 @agent_ppo2.py:185][0m |          -0.0071 |         167.5880 |           7.4539 |
[32m[20221213 21:27:40 @agent_ppo2.py:185][0m |          -0.0079 |         168.4651 |           7.4904 |
[32m[20221213 21:27:40 @agent_ppo2.py:185][0m |          -0.0052 |         171.5994 |           7.5113 |
[32m[20221213 21:27:40 @agent_ppo2.py:185][0m |          -0.0121 |         166.9219 |           7.4958 |
[32m[20221213 21:27:40 @agent_ppo2.py:185][0m |          -0.0110 |         166.8054 |           7.5347 |
[32m[20221213 21:27:40 @agent_ppo2.py:185][0m |          -0.0104 |         166.6461 |           7.5674 |
[32m[20221213 21:27:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.80
[32m[20221213 21:27:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:27:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.00
[32m[20221213 21:27:40 @agent_ppo2.py:143][0m Total time:      32.09 min
[32m[20221213 21:27:40 @agent_ppo2.py:145][0m 3141632 total steps have happened
[32m[20221213 21:27:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1534 --------------------------#
[32m[20221213 21:27:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:27:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:41 @agent_ppo2.py:185][0m |           0.0015 |         166.2274 |           7.0690 |
[32m[20221213 21:27:41 @agent_ppo2.py:185][0m |          -0.0043 |         165.0257 |           7.0623 |
[32m[20221213 21:27:41 @agent_ppo2.py:185][0m |          -0.0032 |         164.2950 |           7.0682 |
[32m[20221213 21:27:41 @agent_ppo2.py:185][0m |          -0.0019 |         167.0833 |           7.0891 |
[32m[20221213 21:27:41 @agent_ppo2.py:185][0m |          -0.0048 |         163.7532 |           7.0754 |
[32m[20221213 21:27:41 @agent_ppo2.py:185][0m |          -0.0071 |         163.4765 |           7.1752 |
[32m[20221213 21:27:41 @agent_ppo2.py:185][0m |           0.0088 |         175.1076 |           7.1207 |
[32m[20221213 21:27:41 @agent_ppo2.py:185][0m |          -0.0032 |         164.1978 |           7.0917 |
[32m[20221213 21:27:41 @agent_ppo2.py:185][0m |          -0.0011 |         165.1978 |           7.1021 |
[32m[20221213 21:27:41 @agent_ppo2.py:185][0m |          -0.0037 |         163.1429 |           7.1411 |
[32m[20221213 21:27:41 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:27:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.60
[32m[20221213 21:27:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 21:27:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 21:27:42 @agent_ppo2.py:143][0m Total time:      32.11 min
[32m[20221213 21:27:42 @agent_ppo2.py:145][0m 3143680 total steps have happened
[32m[20221213 21:27:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1535 --------------------------#
[32m[20221213 21:27:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:42 @agent_ppo2.py:185][0m |           0.0015 |         168.6395 |           7.6475 |
[32m[20221213 21:27:42 @agent_ppo2.py:185][0m |          -0.0030 |         165.9951 |           7.6979 |
[32m[20221213 21:27:42 @agent_ppo2.py:185][0m |           0.0057 |         181.0890 |           7.6074 |
[32m[20221213 21:27:42 @agent_ppo2.py:185][0m |          -0.0042 |         163.7731 |           7.6300 |
[32m[20221213 21:27:42 @agent_ppo2.py:185][0m |          -0.0099 |         163.4516 |           7.5620 |
[32m[20221213 21:27:42 @agent_ppo2.py:185][0m |          -0.0084 |         162.9710 |           7.6207 |
[32m[20221213 21:27:42 @agent_ppo2.py:185][0m |          -0.0091 |         162.7571 |           7.5342 |
[32m[20221213 21:27:42 @agent_ppo2.py:185][0m |          -0.0090 |         162.5404 |           7.5443 |
[32m[20221213 21:27:43 @agent_ppo2.py:185][0m |          -0.0097 |         162.3139 |           7.5652 |
[32m[20221213 21:27:43 @agent_ppo2.py:185][0m |          -0.0067 |         164.1103 |           7.5412 |
[32m[20221213 21:27:43 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:27:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.40
[32m[20221213 21:27:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.00
[32m[20221213 21:27:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.00
[32m[20221213 21:27:43 @agent_ppo2.py:143][0m Total time:      32.13 min
[32m[20221213 21:27:43 @agent_ppo2.py:145][0m 3145728 total steps have happened
[32m[20221213 21:27:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1536 --------------------------#
[32m[20221213 21:27:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:43 @agent_ppo2.py:185][0m |          -0.0015 |         167.6888 |           7.4773 |
[32m[20221213 21:27:43 @agent_ppo2.py:185][0m |          -0.0049 |         165.8154 |           7.5549 |
[32m[20221213 21:27:43 @agent_ppo2.py:185][0m |          -0.0081 |         164.4717 |           7.5048 |
[32m[20221213 21:27:43 @agent_ppo2.py:185][0m |          -0.0088 |         163.7980 |           7.5283 |
[32m[20221213 21:27:44 @agent_ppo2.py:185][0m |          -0.0001 |         172.0882 |           7.5269 |
[32m[20221213 21:27:44 @agent_ppo2.py:185][0m |          -0.0038 |         163.7094 |           7.5341 |
[32m[20221213 21:27:44 @agent_ppo2.py:185][0m |          -0.0100 |         162.1533 |           7.5402 |
[32m[20221213 21:27:44 @agent_ppo2.py:185][0m |          -0.0112 |         161.5305 |           7.5704 |
[32m[20221213 21:27:44 @agent_ppo2.py:185][0m |          -0.0098 |         161.3316 |           7.5553 |
[32m[20221213 21:27:44 @agent_ppo2.py:185][0m |          -0.0094 |         161.2838 |           7.5599 |
[32m[20221213 21:27:44 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:27:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.80
[32m[20221213 21:27:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.00
[32m[20221213 21:27:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.00
[32m[20221213 21:27:44 @agent_ppo2.py:143][0m Total time:      32.16 min
[32m[20221213 21:27:44 @agent_ppo2.py:145][0m 3147776 total steps have happened
[32m[20221213 21:27:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1537 --------------------------#
[32m[20221213 21:27:44 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:27:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:44 @agent_ppo2.py:185][0m |          -0.0004 |         175.1576 |           7.2528 |
[32m[20221213 21:27:45 @agent_ppo2.py:185][0m |          -0.0030 |         172.6107 |           7.2978 |
[32m[20221213 21:27:45 @agent_ppo2.py:185][0m |          -0.0002 |         171.8235 |           7.3209 |
[32m[20221213 21:27:45 @agent_ppo2.py:185][0m |          -0.0044 |         169.7328 |           7.3643 |
[32m[20221213 21:27:45 @agent_ppo2.py:185][0m |          -0.0000 |         170.9252 |           7.4025 |
[32m[20221213 21:27:45 @agent_ppo2.py:185][0m |          -0.0086 |         168.8763 |           7.4157 |
[32m[20221213 21:27:45 @agent_ppo2.py:185][0m |          -0.0086 |         168.7065 |           7.4593 |
[32m[20221213 21:27:45 @agent_ppo2.py:185][0m |          -0.0069 |         168.5665 |           7.4915 |
[32m[20221213 21:27:45 @agent_ppo2.py:185][0m |          -0.0051 |         168.7070 |           7.4804 |
[32m[20221213 21:27:45 @agent_ppo2.py:185][0m |          -0.0074 |         168.1260 |           7.5562 |
[32m[20221213 21:27:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.80
[32m[20221213 21:27:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:27:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.00
[32m[20221213 21:27:45 @agent_ppo2.py:143][0m Total time:      32.18 min
[32m[20221213 21:27:45 @agent_ppo2.py:145][0m 3149824 total steps have happened
[32m[20221213 21:27:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1538 --------------------------#
[32m[20221213 21:27:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:46 @agent_ppo2.py:185][0m |           0.0028 |         168.4259 |           7.9001 |
[32m[20221213 21:27:46 @agent_ppo2.py:185][0m |          -0.0012 |         166.7473 |           7.9035 |
[32m[20221213 21:27:46 @agent_ppo2.py:185][0m |          -0.0039 |         166.3430 |           7.9416 |
[32m[20221213 21:27:46 @agent_ppo2.py:185][0m |           0.0002 |         170.2383 |           7.9367 |
[32m[20221213 21:27:46 @agent_ppo2.py:185][0m |          -0.0058 |         165.6912 |           8.0198 |
[32m[20221213 21:27:46 @agent_ppo2.py:185][0m |          -0.0066 |         165.5752 |           8.0186 |
[32m[20221213 21:27:46 @agent_ppo2.py:185][0m |           0.0018 |         176.2228 |           8.0604 |
[32m[20221213 21:27:46 @agent_ppo2.py:185][0m |          -0.0060 |         165.1401 |           8.0675 |
[32m[20221213 21:27:46 @agent_ppo2.py:185][0m |          -0.0074 |         165.0533 |           8.0966 |
[32m[20221213 21:27:47 @agent_ppo2.py:185][0m |          -0.0048 |         165.9372 |           8.0860 |
[32m[20221213 21:27:47 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:27:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 765.20
[32m[20221213 21:27:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.00
[32m[20221213 21:27:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.00
[32m[20221213 21:27:47 @agent_ppo2.py:143][0m Total time:      32.20 min
[32m[20221213 21:27:47 @agent_ppo2.py:145][0m 3151872 total steps have happened
[32m[20221213 21:27:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1539 --------------------------#
[32m[20221213 21:27:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:47 @agent_ppo2.py:185][0m |          -0.0023 |         173.3829 |           7.8232 |
[32m[20221213 21:27:47 @agent_ppo2.py:185][0m |          -0.0051 |         170.8334 |           7.8020 |
[32m[20221213 21:27:47 @agent_ppo2.py:185][0m |          -0.0050 |         170.2449 |           7.7772 |
[32m[20221213 21:27:47 @agent_ppo2.py:185][0m |          -0.0051 |         170.6601 |           7.7829 |
[32m[20221213 21:27:47 @agent_ppo2.py:185][0m |          -0.0058 |         169.1871 |           7.7972 |
[32m[20221213 21:27:47 @agent_ppo2.py:185][0m |          -0.0086 |         169.0023 |           7.7218 |
[32m[20221213 21:27:47 @agent_ppo2.py:185][0m |           0.0007 |         182.2059 |           7.7365 |
[32m[20221213 21:27:48 @agent_ppo2.py:185][0m |          -0.0020 |         168.9944 |           7.8143 |
[32m[20221213 21:27:48 @agent_ppo2.py:185][0m |          -0.0081 |         169.2156 |           7.6780 |
[32m[20221213 21:27:48 @agent_ppo2.py:185][0m |          -0.0110 |         168.0868 |           7.6917 |
[32m[20221213 21:27:48 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:27:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.40
[32m[20221213 21:27:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:27:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.00
[32m[20221213 21:27:48 @agent_ppo2.py:143][0m Total time:      32.22 min
[32m[20221213 21:27:48 @agent_ppo2.py:145][0m 3153920 total steps have happened
[32m[20221213 21:27:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1540 --------------------------#
[32m[20221213 21:27:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:48 @agent_ppo2.py:185][0m |          -0.0021 |         170.1351 |           7.5677 |
[32m[20221213 21:27:48 @agent_ppo2.py:185][0m |          -0.0075 |         169.4326 |           7.6225 |
[32m[20221213 21:27:48 @agent_ppo2.py:185][0m |          -0.0069 |         168.8799 |           7.6503 |
[32m[20221213 21:27:48 @agent_ppo2.py:185][0m |           0.0004 |         177.8975 |           7.7287 |
[32m[20221213 21:27:49 @agent_ppo2.py:185][0m |          -0.0079 |         167.8787 |           7.7894 |
[32m[20221213 21:27:49 @agent_ppo2.py:185][0m |          -0.0093 |         167.7037 |           7.8452 |
[32m[20221213 21:27:49 @agent_ppo2.py:185][0m |          -0.0086 |         167.2556 |           7.8597 |
[32m[20221213 21:27:49 @agent_ppo2.py:185][0m |          -0.0089 |         167.1467 |           7.8797 |
[32m[20221213 21:27:49 @agent_ppo2.py:185][0m |          -0.0091 |         166.7858 |           7.9504 |
[32m[20221213 21:27:49 @agent_ppo2.py:185][0m |          -0.0101 |         166.7442 |           7.9457 |
[32m[20221213 21:27:49 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:27:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.00
[32m[20221213 21:27:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:27:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 21:27:49 @agent_ppo2.py:143][0m Total time:      32.24 min
[32m[20221213 21:27:49 @agent_ppo2.py:145][0m 3155968 total steps have happened
[32m[20221213 21:27:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1541 --------------------------#
[32m[20221213 21:27:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:49 @agent_ppo2.py:185][0m |           0.0099 |         184.5832 |           8.2277 |
[32m[20221213 21:27:50 @agent_ppo2.py:185][0m |           0.0081 |         184.0520 |           8.1999 |
[32m[20221213 21:27:50 @agent_ppo2.py:185][0m |          -0.0080 |         168.2624 |           8.2269 |
[32m[20221213 21:27:50 @agent_ppo2.py:185][0m |          -0.0083 |         168.0603 |           8.1894 |
[32m[20221213 21:27:50 @agent_ppo2.py:185][0m |          -0.0095 |         167.8112 |           8.2294 |
[32m[20221213 21:27:50 @agent_ppo2.py:185][0m |          -0.0105 |         167.5879 |           8.2171 |
[32m[20221213 21:27:50 @agent_ppo2.py:185][0m |          -0.0110 |         167.4671 |           8.1475 |
[32m[20221213 21:27:50 @agent_ppo2.py:185][0m |          -0.0097 |         167.2198 |           8.1782 |
[32m[20221213 21:27:50 @agent_ppo2.py:185][0m |          -0.0125 |         167.0348 |           8.1739 |
[32m[20221213 21:27:50 @agent_ppo2.py:185][0m |          -0.0122 |         166.9823 |           8.1374 |
[32m[20221213 21:27:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 761.00
[32m[20221213 21:27:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 781.00
[32m[20221213 21:27:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 21:27:50 @agent_ppo2.py:143][0m Total time:      32.26 min
[32m[20221213 21:27:50 @agent_ppo2.py:145][0m 3158016 total steps have happened
[32m[20221213 21:27:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1542 --------------------------#
[32m[20221213 21:27:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:51 @agent_ppo2.py:185][0m |          -0.0014 |         171.1655 |           7.9594 |
[32m[20221213 21:27:51 @agent_ppo2.py:185][0m |          -0.0062 |         169.6789 |           7.9263 |
[32m[20221213 21:27:51 @agent_ppo2.py:185][0m |           0.0014 |         171.1119 |           7.9380 |
[32m[20221213 21:27:51 @agent_ppo2.py:185][0m |           0.0012 |         171.0268 |           7.9788 |
[32m[20221213 21:27:51 @agent_ppo2.py:185][0m |           0.0004 |         180.1179 |           7.9773 |
[32m[20221213 21:27:51 @agent_ppo2.py:185][0m |          -0.0026 |         167.6729 |           8.0023 |
[32m[20221213 21:27:51 @agent_ppo2.py:185][0m |          -0.0084 |         167.0785 |           8.0007 |
[32m[20221213 21:27:51 @agent_ppo2.py:185][0m |          -0.0101 |         167.0766 |           8.0156 |
[32m[20221213 21:27:51 @agent_ppo2.py:185][0m |          -0.0089 |         166.5416 |           8.0525 |
[32m[20221213 21:27:51 @agent_ppo2.py:185][0m |          -0.0080 |         166.2536 |           8.0464 |
[32m[20221213 21:27:51 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:27:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 752.20
[32m[20221213 21:27:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221213 21:27:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:27:52 @agent_ppo2.py:143][0m Total time:      32.28 min
[32m[20221213 21:27:52 @agent_ppo2.py:145][0m 3160064 total steps have happened
[32m[20221213 21:27:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1543 --------------------------#
[32m[20221213 21:27:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:52 @agent_ppo2.py:185][0m |          -0.0011 |         168.5902 |           8.0755 |
[32m[20221213 21:27:52 @agent_ppo2.py:185][0m |           0.0078 |         187.1209 |           8.0793 |
[32m[20221213 21:27:52 @agent_ppo2.py:185][0m |          -0.0045 |         167.6113 |           8.0522 |
[32m[20221213 21:27:52 @agent_ppo2.py:185][0m |          -0.0016 |         169.0502 |           8.0663 |
[32m[20221213 21:27:52 @agent_ppo2.py:185][0m |          -0.0042 |         167.4236 |           8.0706 |
[32m[20221213 21:27:52 @agent_ppo2.py:185][0m |          -0.0064 |         167.0602 |           8.0458 |
[32m[20221213 21:27:52 @agent_ppo2.py:185][0m |          -0.0084 |         167.0027 |           8.0067 |
[32m[20221213 21:27:52 @agent_ppo2.py:185][0m |          -0.0055 |         167.3220 |           8.0116 |
[32m[20221213 21:27:53 @agent_ppo2.py:185][0m |          -0.0085 |         166.7910 |           8.0318 |
[32m[20221213 21:27:53 @agent_ppo2.py:185][0m |          -0.0091 |         166.7943 |           7.9962 |
[32m[20221213 21:27:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.40
[32m[20221213 21:27:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.00
[32m[20221213 21:27:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.00
[32m[20221213 21:27:53 @agent_ppo2.py:143][0m Total time:      32.30 min
[32m[20221213 21:27:53 @agent_ppo2.py:145][0m 3162112 total steps have happened
[32m[20221213 21:27:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1544 --------------------------#
[32m[20221213 21:27:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:27:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:53 @agent_ppo2.py:185][0m |           0.0106 |         182.4249 |           7.9246 |
[32m[20221213 21:27:53 @agent_ppo2.py:185][0m |          -0.0015 |         166.4823 |           7.9786 |
[32m[20221213 21:27:53 @agent_ppo2.py:185][0m |          -0.0044 |         166.2329 |           7.9399 |
[32m[20221213 21:27:53 @agent_ppo2.py:185][0m |          -0.0018 |         167.4733 |           8.0230 |
[32m[20221213 21:27:53 @agent_ppo2.py:185][0m |          -0.0063 |         165.6525 |           8.0322 |
[32m[20221213 21:27:54 @agent_ppo2.py:185][0m |          -0.0061 |         165.5260 |           8.0694 |
[32m[20221213 21:27:54 @agent_ppo2.py:185][0m |          -0.0056 |         165.4824 |           8.0859 |
[32m[20221213 21:27:54 @agent_ppo2.py:185][0m |          -0.0085 |         165.3935 |           8.1070 |
[32m[20221213 21:27:54 @agent_ppo2.py:185][0m |          -0.0062 |         165.3853 |           8.1637 |
[32m[20221213 21:27:54 @agent_ppo2.py:185][0m |          -0.0078 |         165.1292 |           8.1686 |
[32m[20221213 21:27:54 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:27:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.20
[32m[20221213 21:27:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 21:27:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.00
[32m[20221213 21:27:54 @agent_ppo2.py:143][0m Total time:      32.32 min
[32m[20221213 21:27:54 @agent_ppo2.py:145][0m 3164160 total steps have happened
[32m[20221213 21:27:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1545 --------------------------#
[32m[20221213 21:27:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:54 @agent_ppo2.py:185][0m |          -0.0029 |         165.8843 |           8.0538 |
[32m[20221213 21:27:54 @agent_ppo2.py:185][0m |           0.0029 |         188.2930 |           8.0528 |
[32m[20221213 21:27:55 @agent_ppo2.py:185][0m |           0.0022 |         177.5220 |           7.9581 |
[32m[20221213 21:27:55 @agent_ppo2.py:185][0m |          -0.0078 |         163.3106 |           7.9929 |
[32m[20221213 21:27:55 @agent_ppo2.py:185][0m |          -0.0072 |         162.8531 |           7.9946 |
[32m[20221213 21:27:55 @agent_ppo2.py:185][0m |          -0.0104 |         162.4182 |           7.9662 |
[32m[20221213 21:27:55 @agent_ppo2.py:185][0m |          -0.0092 |         162.1203 |           7.9643 |
[32m[20221213 21:27:55 @agent_ppo2.py:185][0m |           0.0089 |         187.8749 |           7.9664 |
[32m[20221213 21:27:55 @agent_ppo2.py:185][0m |          -0.0103 |         161.6421 |           8.0341 |
[32m[20221213 21:27:55 @agent_ppo2.py:185][0m |          -0.0104 |         161.3315 |           8.0155 |
[32m[20221213 21:27:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:27:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.20
[32m[20221213 21:27:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:27:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.00
[32m[20221213 21:27:55 @agent_ppo2.py:143][0m Total time:      32.34 min
[32m[20221213 21:27:55 @agent_ppo2.py:145][0m 3166208 total steps have happened
[32m[20221213 21:27:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1546 --------------------------#
[32m[20221213 21:27:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:56 @agent_ppo2.py:185][0m |          -0.0008 |         167.5107 |           8.3404 |
[32m[20221213 21:27:56 @agent_ppo2.py:185][0m |          -0.0062 |         166.5974 |           8.2798 |
[32m[20221213 21:27:56 @agent_ppo2.py:185][0m |          -0.0048 |         165.9170 |           8.2854 |
[32m[20221213 21:27:56 @agent_ppo2.py:185][0m |          -0.0070 |         165.0457 |           8.2879 |
[32m[20221213 21:27:56 @agent_ppo2.py:185][0m |          -0.0067 |         164.4682 |           8.3216 |
[32m[20221213 21:27:56 @agent_ppo2.py:185][0m |          -0.0077 |         164.0479 |           8.3028 |
[32m[20221213 21:27:56 @agent_ppo2.py:185][0m |          -0.0092 |         163.8051 |           8.3172 |
[32m[20221213 21:27:56 @agent_ppo2.py:185][0m |          -0.0083 |         163.4399 |           8.3043 |
[32m[20221213 21:27:56 @agent_ppo2.py:185][0m |          -0.0053 |         164.1168 |           8.3647 |
[32m[20221213 21:27:56 @agent_ppo2.py:185][0m |          -0.0101 |         162.9951 |           8.3754 |
[32m[20221213 21:27:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:27:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.40
[32m[20221213 21:27:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.00
[32m[20221213 21:27:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 21:27:56 @agent_ppo2.py:143][0m Total time:      32.36 min
[32m[20221213 21:27:56 @agent_ppo2.py:145][0m 3168256 total steps have happened
[32m[20221213 21:27:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1547 --------------------------#
[32m[20221213 21:27:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:27:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:57 @agent_ppo2.py:185][0m |           0.0020 |         170.2577 |           8.1973 |
[32m[20221213 21:27:57 @agent_ppo2.py:185][0m |          -0.0059 |         167.7077 |           8.2067 |
[32m[20221213 21:27:57 @agent_ppo2.py:185][0m |           0.0003 |         178.2886 |           8.2413 |
[32m[20221213 21:27:57 @agent_ppo2.py:185][0m |          -0.0083 |         167.1430 |           8.2287 |
[32m[20221213 21:27:57 @agent_ppo2.py:185][0m |          -0.0017 |         172.9898 |           8.2142 |
[32m[20221213 21:27:57 @agent_ppo2.py:185][0m |          -0.0017 |         171.1118 |           8.2915 |
[32m[20221213 21:27:57 @agent_ppo2.py:185][0m |          -0.0079 |         166.4516 |           8.2909 |
[32m[20221213 21:27:57 @agent_ppo2.py:185][0m |          -0.0097 |         166.2632 |           8.2654 |
[32m[20221213 21:27:57 @agent_ppo2.py:185][0m |          -0.0083 |         166.8344 |           8.3002 |
[32m[20221213 21:27:58 @agent_ppo2.py:185][0m |          -0.0045 |         171.6121 |           8.2804 |
[32m[20221213 21:27:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:27:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.40
[32m[20221213 21:27:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:27:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 21:27:58 @agent_ppo2.py:143][0m Total time:      32.38 min
[32m[20221213 21:27:58 @agent_ppo2.py:145][0m 3170304 total steps have happened
[32m[20221213 21:27:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1548 --------------------------#
[32m[20221213 21:27:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:58 @agent_ppo2.py:185][0m |          -0.0007 |         174.1244 |           8.3255 |
[32m[20221213 21:27:58 @agent_ppo2.py:185][0m |          -0.0043 |         172.8388 |           8.3093 |
[32m[20221213 21:27:58 @agent_ppo2.py:185][0m |          -0.0032 |         172.4175 |           8.3749 |
[32m[20221213 21:27:58 @agent_ppo2.py:185][0m |          -0.0084 |         171.8431 |           8.3582 |
[32m[20221213 21:27:58 @agent_ppo2.py:185][0m |          -0.0057 |         171.3913 |           8.3567 |
[32m[20221213 21:27:58 @agent_ppo2.py:185][0m |           0.0008 |         177.7775 |           8.3401 |
[32m[20221213 21:27:58 @agent_ppo2.py:185][0m |          -0.0019 |         173.0994 |           8.3773 |
[32m[20221213 21:27:59 @agent_ppo2.py:185][0m |          -0.0095 |         170.6590 |           8.3731 |
[32m[20221213 21:27:59 @agent_ppo2.py:185][0m |          -0.0086 |         170.5477 |           8.4336 |
[32m[20221213 21:27:59 @agent_ppo2.py:185][0m |          -0.0083 |         170.2296 |           8.3885 |
[32m[20221213 21:27:59 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:27:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 745.40
[32m[20221213 21:27:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.00
[32m[20221213 21:27:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 864.00
[32m[20221213 21:27:59 @agent_ppo2.py:143][0m Total time:      32.40 min
[32m[20221213 21:27:59 @agent_ppo2.py:145][0m 3172352 total steps have happened
[32m[20221213 21:27:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1549 --------------------------#
[32m[20221213 21:27:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:27:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:27:59 @agent_ppo2.py:185][0m |          -0.0004 |         169.7487 |           8.4364 |
[32m[20221213 21:27:59 @agent_ppo2.py:185][0m |          -0.0052 |         168.6660 |           8.4613 |
[32m[20221213 21:27:59 @agent_ppo2.py:185][0m |          -0.0032 |         168.3208 |           8.4753 |
[32m[20221213 21:27:59 @agent_ppo2.py:185][0m |          -0.0085 |         167.7279 |           8.4410 |
[32m[20221213 21:28:00 @agent_ppo2.py:185][0m |          -0.0073 |         167.3017 |           8.4907 |
[32m[20221213 21:28:00 @agent_ppo2.py:185][0m |          -0.0090 |         167.1075 |           8.4344 |
[32m[20221213 21:28:00 @agent_ppo2.py:185][0m |          -0.0076 |         167.0081 |           8.4537 |
[32m[20221213 21:28:00 @agent_ppo2.py:185][0m |          -0.0080 |         166.8983 |           8.4386 |
[32m[20221213 21:28:00 @agent_ppo2.py:185][0m |          -0.0099 |         166.6422 |           8.4341 |
[32m[20221213 21:28:00 @agent_ppo2.py:185][0m |          -0.0008 |         177.4948 |           8.4457 |
[32m[20221213 21:28:00 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:28:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.00
[32m[20221213 21:28:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.00
[32m[20221213 21:28:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.00
[32m[20221213 21:28:00 @agent_ppo2.py:143][0m Total time:      32.42 min
[32m[20221213 21:28:00 @agent_ppo2.py:145][0m 3174400 total steps have happened
[32m[20221213 21:28:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1550 --------------------------#
[32m[20221213 21:28:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:00 @agent_ppo2.py:185][0m |           0.0080 |         177.5372 |           8.1977 |
[32m[20221213 21:28:01 @agent_ppo2.py:185][0m |          -0.0035 |         168.0953 |           8.2672 |
[32m[20221213 21:28:01 @agent_ppo2.py:185][0m |           0.0017 |         172.3808 |           8.2329 |
[32m[20221213 21:28:01 @agent_ppo2.py:185][0m |          -0.0066 |         167.2545 |           8.2256 |
[32m[20221213 21:28:01 @agent_ppo2.py:185][0m |          -0.0047 |         167.3583 |           8.2906 |
[32m[20221213 21:28:01 @agent_ppo2.py:185][0m |          -0.0075 |         166.7803 |           8.3070 |
[32m[20221213 21:28:01 @agent_ppo2.py:185][0m |           0.0006 |         176.1101 |           8.3503 |
[32m[20221213 21:28:01 @agent_ppo2.py:185][0m |          -0.0068 |         166.7498 |           8.3995 |
[32m[20221213 21:28:01 @agent_ppo2.py:185][0m |          -0.0085 |         166.3600 |           8.4176 |
[32m[20221213 21:28:01 @agent_ppo2.py:185][0m |          -0.0026 |         168.6121 |           8.4123 |
[32m[20221213 21:28:01 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:28:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.80
[32m[20221213 21:28:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:28:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:28:01 @agent_ppo2.py:143][0m Total time:      32.44 min
[32m[20221213 21:28:01 @agent_ppo2.py:145][0m 3176448 total steps have happened
[32m[20221213 21:28:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1551 --------------------------#
[32m[20221213 21:28:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:02 @agent_ppo2.py:185][0m |          -0.0001 |         171.3952 |           8.7171 |
[32m[20221213 21:28:02 @agent_ppo2.py:185][0m |          -0.0037 |         167.7553 |           8.7554 |
[32m[20221213 21:28:02 @agent_ppo2.py:185][0m |          -0.0063 |         166.1228 |           8.7331 |
[32m[20221213 21:28:02 @agent_ppo2.py:185][0m |          -0.0080 |         165.1536 |           8.8196 |
[32m[20221213 21:28:02 @agent_ppo2.py:185][0m |          -0.0083 |         164.7162 |           8.8017 |
[32m[20221213 21:28:02 @agent_ppo2.py:185][0m |           0.0014 |         173.9533 |           8.7567 |
[32m[20221213 21:28:02 @agent_ppo2.py:185][0m |          -0.0026 |         166.9067 |           8.8668 |
[32m[20221213 21:28:02 @agent_ppo2.py:185][0m |          -0.0096 |         163.5430 |           8.8462 |
[32m[20221213 21:28:02 @agent_ppo2.py:185][0m |          -0.0097 |         163.2683 |           8.8671 |
[32m[20221213 21:28:02 @agent_ppo2.py:185][0m |          -0.0088 |         163.2319 |           8.8812 |
[32m[20221213 21:28:02 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:28:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.00
[32m[20221213 21:28:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:28:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.00
[32m[20221213 21:28:03 @agent_ppo2.py:143][0m Total time:      32.46 min
[32m[20221213 21:28:03 @agent_ppo2.py:145][0m 3178496 total steps have happened
[32m[20221213 21:28:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1552 --------------------------#
[32m[20221213 21:28:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:03 @agent_ppo2.py:185][0m |           0.0118 |         180.3865 |           8.7815 |
[32m[20221213 21:28:03 @agent_ppo2.py:185][0m |           0.0037 |         176.2425 |           8.7626 |
[32m[20221213 21:28:03 @agent_ppo2.py:185][0m |          -0.0013 |         174.8711 |           8.7902 |
[32m[20221213 21:28:03 @agent_ppo2.py:185][0m |           0.0152 |         206.4787 |           8.7698 |
[32m[20221213 21:28:03 @agent_ppo2.py:185][0m |          -0.0073 |         170.7627 |           8.8400 |
[32m[20221213 21:28:03 @agent_ppo2.py:185][0m |          -0.0076 |         170.0351 |           8.7793 |
[32m[20221213 21:28:03 @agent_ppo2.py:185][0m |          -0.0089 |         169.9867 |           8.8045 |
[32m[20221213 21:28:03 @agent_ppo2.py:185][0m |          -0.0094 |         169.6120 |           8.7970 |
[32m[20221213 21:28:04 @agent_ppo2.py:185][0m |          -0.0112 |         169.5449 |           8.7781 |
[32m[20221213 21:28:04 @agent_ppo2.py:185][0m |          -0.0100 |         169.2172 |           8.8052 |
[32m[20221213 21:28:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:28:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.20
[32m[20221213 21:28:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221213 21:28:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.00
[32m[20221213 21:28:04 @agent_ppo2.py:143][0m Total time:      32.48 min
[32m[20221213 21:28:04 @agent_ppo2.py:145][0m 3180544 total steps have happened
[32m[20221213 21:28:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1553 --------------------------#
[32m[20221213 21:28:04 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:28:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:04 @agent_ppo2.py:185][0m |           0.0055 |         177.6673 |           8.4323 |
[32m[20221213 21:28:04 @agent_ppo2.py:185][0m |           0.0015 |         173.1321 |           8.5284 |
[32m[20221213 21:28:04 @agent_ppo2.py:185][0m |           0.0002 |         167.8416 |           8.5828 |
[32m[20221213 21:28:04 @agent_ppo2.py:185][0m |          -0.0063 |         167.5065 |           8.6552 |
[32m[20221213 21:28:04 @agent_ppo2.py:185][0m |          -0.0068 |         167.1123 |           8.6547 |
[32m[20221213 21:28:05 @agent_ppo2.py:185][0m |          -0.0058 |         166.7140 |           8.6885 |
[32m[20221213 21:28:05 @agent_ppo2.py:185][0m |          -0.0078 |         166.6092 |           8.7307 |
[32m[20221213 21:28:05 @agent_ppo2.py:185][0m |          -0.0000 |         174.9127 |           8.7385 |
[32m[20221213 21:28:05 @agent_ppo2.py:185][0m |          -0.0074 |         166.1633 |           8.7934 |
[32m[20221213 21:28:05 @agent_ppo2.py:185][0m |           0.0004 |         169.9858 |           8.8090 |
[32m[20221213 21:28:05 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:28:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 695.40
[32m[20221213 21:28:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.00
[32m[20221213 21:28:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 21:28:05 @agent_ppo2.py:143][0m Total time:      32.51 min
[32m[20221213 21:28:05 @agent_ppo2.py:145][0m 3182592 total steps have happened
[32m[20221213 21:28:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1554 --------------------------#
[32m[20221213 21:28:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:05 @agent_ppo2.py:185][0m |          -0.0009 |         169.8794 |           9.3133 |
[32m[20221213 21:28:05 @agent_ppo2.py:185][0m |          -0.0062 |         168.9821 |           9.3485 |
[32m[20221213 21:28:06 @agent_ppo2.py:185][0m |          -0.0072 |         168.4033 |           9.3614 |
[32m[20221213 21:28:06 @agent_ppo2.py:185][0m |          -0.0076 |         167.9896 |           9.3307 |
[32m[20221213 21:28:06 @agent_ppo2.py:185][0m |           0.0011 |         176.0626 |           9.3482 |
[32m[20221213 21:28:06 @agent_ppo2.py:185][0m |          -0.0047 |         168.1155 |           9.4220 |
[32m[20221213 21:28:06 @agent_ppo2.py:185][0m |          -0.0072 |         167.6075 |           9.3496 |
[32m[20221213 21:28:06 @agent_ppo2.py:185][0m |          -0.0104 |         167.0650 |           9.3140 |
[32m[20221213 21:28:06 @agent_ppo2.py:185][0m |          -0.0072 |         166.9338 |           9.3615 |
[32m[20221213 21:28:06 @agent_ppo2.py:185][0m |          -0.0043 |         168.7810 |           9.3340 |
[32m[20221213 21:28:06 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:28:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.00
[32m[20221213 21:28:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:28:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.00
[32m[20221213 21:28:06 @agent_ppo2.py:143][0m Total time:      32.53 min
[32m[20221213 21:28:06 @agent_ppo2.py:145][0m 3184640 total steps have happened
[32m[20221213 21:28:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1555 --------------------------#
[32m[20221213 21:28:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:07 @agent_ppo2.py:185][0m |          -0.0008 |         172.8412 |           9.1039 |
[32m[20221213 21:28:07 @agent_ppo2.py:185][0m |          -0.0034 |         171.0294 |           9.1105 |
[32m[20221213 21:28:07 @agent_ppo2.py:185][0m |           0.0047 |         180.8079 |           9.0984 |
[32m[20221213 21:28:07 @agent_ppo2.py:185][0m |          -0.0014 |         170.4180 |           9.1560 |
[32m[20221213 21:28:07 @agent_ppo2.py:185][0m |          -0.0064 |         168.7605 |           9.1560 |
[32m[20221213 21:28:07 @agent_ppo2.py:185][0m |          -0.0064 |         168.3371 |           9.1105 |
[32m[20221213 21:28:07 @agent_ppo2.py:185][0m |          -0.0057 |         168.1224 |           9.1446 |
[32m[20221213 21:28:07 @agent_ppo2.py:185][0m |          -0.0086 |         167.7312 |           9.1205 |
[32m[20221213 21:28:07 @agent_ppo2.py:185][0m |          -0.0084 |         167.4633 |           9.1625 |
[32m[20221213 21:28:07 @agent_ppo2.py:185][0m |          -0.0099 |         167.3016 |           9.1580 |
[32m[20221213 21:28:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:28:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.00
[32m[20221213 21:28:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:28:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:28:08 @agent_ppo2.py:143][0m Total time:      32.55 min
[32m[20221213 21:28:08 @agent_ppo2.py:145][0m 3186688 total steps have happened
[32m[20221213 21:28:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1556 --------------------------#
[32m[20221213 21:28:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:08 @agent_ppo2.py:185][0m |          -0.0004 |         173.5978 |           9.4398 |
[32m[20221213 21:28:08 @agent_ppo2.py:185][0m |           0.0018 |         173.2156 |           9.4489 |
[32m[20221213 21:28:08 @agent_ppo2.py:185][0m |          -0.0040 |         169.6148 |           9.4439 |
[32m[20221213 21:28:08 @agent_ppo2.py:185][0m |          -0.0032 |         168.9720 |           9.4135 |
[32m[20221213 21:28:08 @agent_ppo2.py:185][0m |          -0.0055 |         168.4037 |           9.4036 |
[32m[20221213 21:28:08 @agent_ppo2.py:185][0m |          -0.0050 |         168.1353 |           9.4218 |
[32m[20221213 21:28:08 @agent_ppo2.py:185][0m |          -0.0042 |         167.7011 |           9.4433 |
[32m[20221213 21:28:08 @agent_ppo2.py:185][0m |          -0.0077 |         167.6246 |           9.3700 |
[32m[20221213 21:28:09 @agent_ppo2.py:185][0m |           0.0097 |         192.5469 |           9.4190 |
[32m[20221213 21:28:09 @agent_ppo2.py:185][0m |          -0.0065 |         167.3219 |           9.4512 |
[32m[20221213 21:28:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:28:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.40
[32m[20221213 21:28:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:28:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.00
[32m[20221213 21:28:09 @agent_ppo2.py:143][0m Total time:      32.57 min
[32m[20221213 21:28:09 @agent_ppo2.py:145][0m 3188736 total steps have happened
[32m[20221213 21:28:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1557 --------------------------#
[32m[20221213 21:28:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:28:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:09 @agent_ppo2.py:185][0m |          -0.0009 |         170.4769 |           9.1951 |
[32m[20221213 21:28:09 @agent_ppo2.py:185][0m |          -0.0028 |         169.7179 |           9.1904 |
[32m[20221213 21:28:09 @agent_ppo2.py:185][0m |          -0.0017 |         170.9427 |           9.1759 |
[32m[20221213 21:28:09 @agent_ppo2.py:185][0m |          -0.0009 |         177.9554 |           9.2095 |
[32m[20221213 21:28:09 @agent_ppo2.py:185][0m |           0.0178 |         202.7460 |           9.1935 |
[32m[20221213 21:28:09 @agent_ppo2.py:185][0m |          -0.0062 |         168.7539 |           9.1793 |
[32m[20221213 21:28:10 @agent_ppo2.py:185][0m |          -0.0063 |         168.6293 |           9.1514 |
[32m[20221213 21:28:10 @agent_ppo2.py:185][0m |           0.0025 |         188.9023 |           9.1557 |
[32m[20221213 21:28:10 @agent_ppo2.py:185][0m |           0.0034 |         190.4340 |           9.1674 |
[32m[20221213 21:28:10 @agent_ppo2.py:185][0m |          -0.0031 |         173.6389 |           9.2132 |
[32m[20221213 21:28:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:28:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.60
[32m[20221213 21:28:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:28:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.00
[32m[20221213 21:28:10 @agent_ppo2.py:143][0m Total time:      32.59 min
[32m[20221213 21:28:10 @agent_ppo2.py:145][0m 3190784 total steps have happened
[32m[20221213 21:28:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1558 --------------------------#
[32m[20221213 21:28:10 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:28:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:10 @agent_ppo2.py:185][0m |           0.0190 |         202.8366 |           9.2488 |
[32m[20221213 21:28:10 @agent_ppo2.py:185][0m |          -0.0050 |         172.7089 |           9.3083 |
[32m[20221213 21:28:10 @agent_ppo2.py:185][0m |          -0.0063 |         172.1990 |           9.2946 |
[32m[20221213 21:28:10 @agent_ppo2.py:185][0m |          -0.0063 |         171.8536 |           9.3546 |
[32m[20221213 21:28:11 @agent_ppo2.py:185][0m |          -0.0068 |         171.9840 |           9.3729 |
[32m[20221213 21:28:11 @agent_ppo2.py:185][0m |          -0.0046 |         172.9697 |           9.3900 |
[32m[20221213 21:28:11 @agent_ppo2.py:185][0m |          -0.0092 |         171.2337 |           9.4465 |
[32m[20221213 21:28:11 @agent_ppo2.py:185][0m |          -0.0090 |         171.1379 |           9.4697 |
[32m[20221213 21:28:11 @agent_ppo2.py:185][0m |          -0.0094 |         170.9884 |           9.5578 |
[32m[20221213 21:28:11 @agent_ppo2.py:185][0m |          -0.0074 |         171.2018 |           9.5628 |
[32m[20221213 21:28:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:28:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.40
[32m[20221213 21:28:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.00
[32m[20221213 21:28:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.00
[32m[20221213 21:28:11 @agent_ppo2.py:143][0m Total time:      32.61 min
[32m[20221213 21:28:11 @agent_ppo2.py:145][0m 3192832 total steps have happened
[32m[20221213 21:28:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1559 --------------------------#
[32m[20221213 21:28:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:11 @agent_ppo2.py:185][0m |           0.0153 |         196.2418 |           9.3258 |
[32m[20221213 21:28:11 @agent_ppo2.py:185][0m |          -0.0008 |         174.8927 |           9.3323 |
[32m[20221213 21:28:12 @agent_ppo2.py:185][0m |          -0.0039 |         174.5222 |           9.2497 |
[32m[20221213 21:28:12 @agent_ppo2.py:185][0m |          -0.0062 |         174.2436 |           9.2288 |
[32m[20221213 21:28:12 @agent_ppo2.py:185][0m |          -0.0049 |         173.9996 |           9.2032 |
[32m[20221213 21:28:12 @agent_ppo2.py:185][0m |          -0.0062 |         173.9375 |           9.1825 |
[32m[20221213 21:28:12 @agent_ppo2.py:185][0m |          -0.0058 |         173.7434 |           9.1291 |
[32m[20221213 21:28:12 @agent_ppo2.py:185][0m |          -0.0054 |         173.6548 |           9.1545 |
[32m[20221213 21:28:12 @agent_ppo2.py:185][0m |          -0.0056 |         173.7325 |           9.0817 |
[32m[20221213 21:28:12 @agent_ppo2.py:185][0m |          -0.0092 |         173.3004 |           9.1005 |
[32m[20221213 21:28:12 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:28:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 763.00
[32m[20221213 21:28:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:28:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 865.00
[32m[20221213 21:28:12 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 865.00
[32m[20221213 21:28:12 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 865.00
[32m[20221213 21:28:12 @agent_ppo2.py:143][0m Total time:      32.63 min
[32m[20221213 21:28:12 @agent_ppo2.py:145][0m 3194880 total steps have happened
[32m[20221213 21:28:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1560 --------------------------#
[32m[20221213 21:28:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:28:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:13 @agent_ppo2.py:185][0m |           0.0009 |         172.9462 |           9.0531 |
[32m[20221213 21:28:13 @agent_ppo2.py:185][0m |          -0.0020 |         171.9427 |           9.0690 |
[32m[20221213 21:28:13 @agent_ppo2.py:185][0m |          -0.0030 |         171.3856 |           9.1169 |
[32m[20221213 21:28:13 @agent_ppo2.py:185][0m |          -0.0045 |         171.1858 |           9.0859 |
[32m[20221213 21:28:13 @agent_ppo2.py:185][0m |          -0.0028 |         171.0057 |           9.0997 |
[32m[20221213 21:28:13 @agent_ppo2.py:185][0m |           0.0000 |         174.7225 |           9.0882 |
[32m[20221213 21:28:13 @agent_ppo2.py:185][0m |          -0.0016 |         172.2714 |           9.0843 |
[32m[20221213 21:28:13 @agent_ppo2.py:185][0m |          -0.0065 |         170.6257 |           9.0598 |
[32m[20221213 21:28:13 @agent_ppo2.py:185][0m |          -0.0064 |         170.6033 |           9.0764 |
[32m[20221213 21:28:13 @agent_ppo2.py:185][0m |          -0.0072 |         170.4902 |           9.0967 |
[32m[20221213 21:28:13 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:28:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.00
[32m[20221213 21:28:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:28:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.00
[32m[20221213 21:28:14 @agent_ppo2.py:143][0m Total time:      32.65 min
[32m[20221213 21:28:14 @agent_ppo2.py:145][0m 3196928 total steps have happened
[32m[20221213 21:28:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1561 --------------------------#
[32m[20221213 21:28:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:14 @agent_ppo2.py:185][0m |           0.0072 |         177.3293 |           9.0322 |
[32m[20221213 21:28:14 @agent_ppo2.py:185][0m |          -0.0045 |         171.3174 |           9.0476 |
[32m[20221213 21:28:14 @agent_ppo2.py:185][0m |          -0.0043 |         171.0370 |           9.0530 |
[32m[20221213 21:28:14 @agent_ppo2.py:185][0m |           0.0041 |         185.7826 |           9.0133 |
[32m[20221213 21:28:14 @agent_ppo2.py:185][0m |          -0.0046 |         170.4507 |           9.0431 |
[32m[20221213 21:28:14 @agent_ppo2.py:185][0m |          -0.0083 |         169.7393 |           8.9595 |
[32m[20221213 21:28:14 @agent_ppo2.py:185][0m |          -0.0086 |         169.5306 |           8.9517 |
[32m[20221213 21:28:14 @agent_ppo2.py:185][0m |          -0.0092 |         169.3550 |           8.9725 |
[32m[20221213 21:28:15 @agent_ppo2.py:185][0m |          -0.0058 |         170.1303 |           8.9421 |
[32m[20221213 21:28:15 @agent_ppo2.py:185][0m |          -0.0105 |         169.1853 |           8.9289 |
[32m[20221213 21:28:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:28:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.60
[32m[20221213 21:28:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:28:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.00
[32m[20221213 21:28:15 @agent_ppo2.py:143][0m Total time:      32.67 min
[32m[20221213 21:28:15 @agent_ppo2.py:145][0m 3198976 total steps have happened
[32m[20221213 21:28:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1562 --------------------------#
[32m[20221213 21:28:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:15 @agent_ppo2.py:185][0m |          -0.0015 |         171.4742 |           8.6935 |
[32m[20221213 21:28:15 @agent_ppo2.py:185][0m |          -0.0062 |         170.3421 |           8.7486 |
[32m[20221213 21:28:15 @agent_ppo2.py:185][0m |          -0.0052 |         169.7221 |           8.7347 |
[32m[20221213 21:28:15 @agent_ppo2.py:185][0m |          -0.0050 |         169.3870 |           8.7614 |
[32m[20221213 21:28:15 @agent_ppo2.py:185][0m |          -0.0063 |         169.0576 |           8.7059 |
[32m[20221213 21:28:16 @agent_ppo2.py:185][0m |          -0.0062 |         168.8083 |           8.6930 |
[32m[20221213 21:28:16 @agent_ppo2.py:185][0m |          -0.0070 |         168.7015 |           8.7190 |
[32m[20221213 21:28:16 @agent_ppo2.py:185][0m |          -0.0094 |         168.4086 |           8.6600 |
[32m[20221213 21:28:16 @agent_ppo2.py:185][0m |          -0.0072 |         168.2822 |           8.6897 |
[32m[20221213 21:28:16 @agent_ppo2.py:185][0m |          -0.0089 |         168.3283 |           8.6671 |
[32m[20221213 21:28:16 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:28:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.20
[32m[20221213 21:28:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.00
[32m[20221213 21:28:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.00
[32m[20221213 21:28:16 @agent_ppo2.py:143][0m Total time:      32.69 min
[32m[20221213 21:28:16 @agent_ppo2.py:145][0m 3201024 total steps have happened
[32m[20221213 21:28:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1563 --------------------------#
[32m[20221213 21:28:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:16 @agent_ppo2.py:185][0m |          -0.0018 |         171.1651 |           8.8551 |
[32m[20221213 21:28:16 @agent_ppo2.py:185][0m |          -0.0036 |         170.2871 |           8.9101 |
[32m[20221213 21:28:17 @agent_ppo2.py:185][0m |          -0.0058 |         169.9140 |           8.9090 |
[32m[20221213 21:28:17 @agent_ppo2.py:185][0m |          -0.0085 |         169.6539 |           8.8531 |
[32m[20221213 21:28:17 @agent_ppo2.py:185][0m |          -0.0080 |         169.4351 |           8.8621 |
[32m[20221213 21:28:17 @agent_ppo2.py:185][0m |          -0.0072 |         169.3992 |           8.8504 |
[32m[20221213 21:28:17 @agent_ppo2.py:185][0m |          -0.0098 |         169.1574 |           8.8158 |
[32m[20221213 21:28:17 @agent_ppo2.py:185][0m |          -0.0085 |         169.2299 |           8.8394 |
[32m[20221213 21:28:17 @agent_ppo2.py:185][0m |          -0.0112 |         168.9109 |           8.8043 |
[32m[20221213 21:28:17 @agent_ppo2.py:185][0m |          -0.0111 |         168.7722 |           8.8508 |
[32m[20221213 21:28:17 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:28:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.20
[32m[20221213 21:28:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:28:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 786.00
[32m[20221213 21:28:17 @agent_ppo2.py:143][0m Total time:      32.71 min
[32m[20221213 21:28:17 @agent_ppo2.py:145][0m 3203072 total steps have happened
[32m[20221213 21:28:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1564 --------------------------#
[32m[20221213 21:28:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:18 @agent_ppo2.py:185][0m |          -0.0010 |         170.8180 |           8.4010 |
[32m[20221213 21:28:18 @agent_ppo2.py:185][0m |           0.0034 |         173.3601 |           8.4633 |
[32m[20221213 21:28:18 @agent_ppo2.py:185][0m |          -0.0055 |         169.6737 |           8.4997 |
[32m[20221213 21:28:18 @agent_ppo2.py:185][0m |          -0.0070 |         169.3935 |           8.4729 |
[32m[20221213 21:28:18 @agent_ppo2.py:185][0m |          -0.0056 |         169.8259 |           8.5132 |
[32m[20221213 21:28:18 @agent_ppo2.py:185][0m |          -0.0104 |         169.1875 |           8.5065 |
[32m[20221213 21:28:18 @agent_ppo2.py:185][0m |          -0.0096 |         169.1393 |           8.5256 |
[32m[20221213 21:28:18 @agent_ppo2.py:185][0m |          -0.0095 |         169.0198 |           8.4690 |
[32m[20221213 21:28:18 @agent_ppo2.py:185][0m |          -0.0001 |         176.8619 |           8.5073 |
[32m[20221213 21:28:18 @agent_ppo2.py:185][0m |          -0.0094 |         168.9437 |           8.5463 |
[32m[20221213 21:28:18 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:28:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.40
[32m[20221213 21:28:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.00
[32m[20221213 21:28:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.00
[32m[20221213 21:28:19 @agent_ppo2.py:143][0m Total time:      32.73 min
[32m[20221213 21:28:19 @agent_ppo2.py:145][0m 3205120 total steps have happened
[32m[20221213 21:28:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1565 --------------------------#
[32m[20221213 21:28:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:28:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:19 @agent_ppo2.py:185][0m |           0.0062 |         183.2474 |           8.6109 |
[32m[20221213 21:28:19 @agent_ppo2.py:185][0m |          -0.0053 |         173.0052 |           8.5877 |
[32m[20221213 21:28:19 @agent_ppo2.py:185][0m |          -0.0049 |         172.2765 |           8.6153 |
[32m[20221213 21:28:19 @agent_ppo2.py:185][0m |          -0.0027 |         175.7759 |           8.5339 |
[32m[20221213 21:28:19 @agent_ppo2.py:185][0m |          -0.0065 |         171.4820 |           8.5456 |
[32m[20221213 21:28:19 @agent_ppo2.py:185][0m |          -0.0073 |         170.8519 |           8.5406 |
[32m[20221213 21:28:19 @agent_ppo2.py:185][0m |          -0.0072 |         170.6457 |           8.5162 |
[32m[20221213 21:28:20 @agent_ppo2.py:185][0m |          -0.0100 |         170.3870 |           8.5232 |
[32m[20221213 21:28:20 @agent_ppo2.py:185][0m |          -0.0075 |         170.2598 |           8.5027 |
[32m[20221213 21:28:20 @agent_ppo2.py:185][0m |          -0.0105 |         169.8417 |           8.5122 |
[32m[20221213 21:28:20 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:28:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.40
[32m[20221213 21:28:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.00
[32m[20221213 21:28:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.00
[32m[20221213 21:28:20 @agent_ppo2.py:143][0m Total time:      32.75 min
[32m[20221213 21:28:20 @agent_ppo2.py:145][0m 3207168 total steps have happened
[32m[20221213 21:28:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1566 --------------------------#
[32m[20221213 21:28:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:20 @agent_ppo2.py:185][0m |          -0.0011 |         174.4681 |           8.1863 |
[32m[20221213 21:28:20 @agent_ppo2.py:185][0m |          -0.0032 |         173.6342 |           8.2354 |
[32m[20221213 21:28:20 @agent_ppo2.py:185][0m |           0.0072 |         183.2576 |           8.2298 |
[32m[20221213 21:28:20 @agent_ppo2.py:185][0m |          -0.0030 |         172.8021 |           8.3108 |
[32m[20221213 21:28:21 @agent_ppo2.py:185][0m |           0.0061 |         190.3333 |           8.2850 |
[32m[20221213 21:28:21 @agent_ppo2.py:185][0m |          -0.0058 |         172.4301 |           8.3222 |
[32m[20221213 21:28:21 @agent_ppo2.py:185][0m |          -0.0079 |         172.3322 |           8.3083 |
[32m[20221213 21:28:21 @agent_ppo2.py:185][0m |          -0.0075 |         172.1593 |           8.2922 |
[32m[20221213 21:28:21 @agent_ppo2.py:185][0m |          -0.0086 |         172.1376 |           8.2837 |
[32m[20221213 21:28:21 @agent_ppo2.py:185][0m |           0.0020 |         187.9043 |           8.3245 |
[32m[20221213 21:28:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:28:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.20
[32m[20221213 21:28:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.00
[32m[20221213 21:28:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.00
[32m[20221213 21:28:21 @agent_ppo2.py:143][0m Total time:      32.77 min
[32m[20221213 21:28:21 @agent_ppo2.py:145][0m 3209216 total steps have happened
[32m[20221213 21:28:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1567 --------------------------#
[32m[20221213 21:28:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:21 @agent_ppo2.py:185][0m |          -0.0017 |         172.3967 |           8.5608 |
[32m[20221213 21:28:21 @agent_ppo2.py:185][0m |          -0.0059 |         171.7695 |           8.5017 |
[32m[20221213 21:28:22 @agent_ppo2.py:185][0m |          -0.0058 |         171.3917 |           8.4898 |
[32m[20221213 21:28:22 @agent_ppo2.py:185][0m |           0.0068 |         184.9953 |           8.4796 |
[32m[20221213 21:28:22 @agent_ppo2.py:185][0m |          -0.0082 |         170.7145 |           8.5122 |
[32m[20221213 21:28:22 @agent_ppo2.py:185][0m |          -0.0083 |         170.1461 |           8.4626 |
[32m[20221213 21:28:22 @agent_ppo2.py:185][0m |          -0.0104 |         169.9944 |           8.4812 |
[32m[20221213 21:28:22 @agent_ppo2.py:185][0m |          -0.0098 |         169.6215 |           8.4805 |
[32m[20221213 21:28:22 @agent_ppo2.py:185][0m |          -0.0114 |         169.3912 |           8.4970 |
[32m[20221213 21:28:22 @agent_ppo2.py:185][0m |          -0.0103 |         169.1099 |           8.4397 |
[32m[20221213 21:28:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:28:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.80
[32m[20221213 21:28:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:28:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.00
[32m[20221213 21:28:22 @agent_ppo2.py:143][0m Total time:      32.79 min
[32m[20221213 21:28:22 @agent_ppo2.py:145][0m 3211264 total steps have happened
[32m[20221213 21:28:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1568 --------------------------#
[32m[20221213 21:28:22 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:28:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:23 @agent_ppo2.py:185][0m |          -0.0013 |         175.6315 |           8.2297 |
[32m[20221213 21:28:23 @agent_ppo2.py:185][0m |          -0.0047 |         173.8591 |           8.2493 |
[32m[20221213 21:28:23 @agent_ppo2.py:185][0m |          -0.0055 |         172.9431 |           8.3150 |
[32m[20221213 21:28:23 @agent_ppo2.py:185][0m |          -0.0052 |         172.2840 |           8.3030 |
[32m[20221213 21:28:23 @agent_ppo2.py:185][0m |          -0.0056 |         171.8108 |           8.3682 |
[32m[20221213 21:28:23 @agent_ppo2.py:185][0m |          -0.0080 |         171.6728 |           8.3641 |
[32m[20221213 21:28:23 @agent_ppo2.py:185][0m |          -0.0076 |         171.3932 |           8.3720 |
[32m[20221213 21:28:23 @agent_ppo2.py:185][0m |          -0.0079 |         171.2185 |           8.4524 |
[32m[20221213 21:28:23 @agent_ppo2.py:185][0m |          -0.0078 |         171.0391 |           8.4333 |
[32m[20221213 21:28:23 @agent_ppo2.py:185][0m |          -0.0010 |         174.0651 |           8.4673 |
[32m[20221213 21:28:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:28:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.80
[32m[20221213 21:28:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.00
[32m[20221213 21:28:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.00
[32m[20221213 21:28:23 @agent_ppo2.py:143][0m Total time:      32.81 min
[32m[20221213 21:28:23 @agent_ppo2.py:145][0m 3213312 total steps have happened
[32m[20221213 21:28:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1569 --------------------------#
[32m[20221213 21:28:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:24 @agent_ppo2.py:185][0m |          -0.0016 |         176.5742 |           8.9514 |
[32m[20221213 21:28:24 @agent_ppo2.py:185][0m |          -0.0060 |         175.8837 |           8.9801 |
[32m[20221213 21:28:24 @agent_ppo2.py:185][0m |          -0.0063 |         175.5728 |           8.9999 |
[32m[20221213 21:28:24 @agent_ppo2.py:185][0m |          -0.0076 |         175.2908 |           8.9923 |
[32m[20221213 21:28:24 @agent_ppo2.py:185][0m |          -0.0076 |         175.0995 |           8.9975 |
[32m[20221213 21:28:24 @agent_ppo2.py:185][0m |          -0.0082 |         174.9523 |           8.9937 |
[32m[20221213 21:28:24 @agent_ppo2.py:185][0m |          -0.0085 |         174.8892 |           9.0210 |
[32m[20221213 21:28:24 @agent_ppo2.py:185][0m |          -0.0027 |         177.8244 |           8.9724 |
[32m[20221213 21:28:25 @agent_ppo2.py:185][0m |          -0.0082 |         174.6905 |           9.0686 |
[32m[20221213 21:28:25 @agent_ppo2.py:185][0m |          -0.0078 |         174.4997 |           8.9811 |
[32m[20221213 21:28:25 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:28:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.00
[32m[20221213 21:28:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.00
[32m[20221213 21:28:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 861.00
[32m[20221213 21:28:25 @agent_ppo2.py:143][0m Total time:      32.83 min
[32m[20221213 21:28:25 @agent_ppo2.py:145][0m 3215360 total steps have happened
[32m[20221213 21:28:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1570 --------------------------#
[32m[20221213 21:28:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:28:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:25 @agent_ppo2.py:185][0m |          -0.0007 |         175.3822 |           8.9619 |
[32m[20221213 21:28:25 @agent_ppo2.py:185][0m |          -0.0048 |         173.9961 |           8.9829 |
[32m[20221213 21:28:25 @agent_ppo2.py:185][0m |           0.0018 |         178.7938 |           8.9983 |
[32m[20221213 21:28:25 @agent_ppo2.py:185][0m |          -0.0069 |         172.3821 |           8.9861 |
[32m[20221213 21:28:25 @agent_ppo2.py:185][0m |          -0.0009 |         177.0373 |           9.0667 |
[32m[20221213 21:28:25 @agent_ppo2.py:185][0m |          -0.0077 |         171.3820 |           9.0298 |
[32m[20221213 21:28:26 @agent_ppo2.py:185][0m |          -0.0095 |         171.0283 |           9.0844 |
[32m[20221213 21:28:26 @agent_ppo2.py:185][0m |          -0.0082 |         170.8784 |           9.0950 |
[32m[20221213 21:28:26 @agent_ppo2.py:185][0m |          -0.0097 |         170.5468 |           9.0594 |
[32m[20221213 21:28:26 @agent_ppo2.py:185][0m |          -0.0064 |         171.1089 |           9.0837 |
[32m[20221213 21:28:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:28:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.00
[32m[20221213 21:28:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221213 21:28:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.00
[32m[20221213 21:28:26 @agent_ppo2.py:143][0m Total time:      32.85 min
[32m[20221213 21:28:26 @agent_ppo2.py:145][0m 3217408 total steps have happened
[32m[20221213 21:28:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1571 --------------------------#
[32m[20221213 21:28:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:26 @agent_ppo2.py:185][0m |          -0.0017 |         176.4324 |           8.5667 |
[32m[20221213 21:28:26 @agent_ppo2.py:185][0m |          -0.0024 |         174.9629 |           8.6354 |
[32m[20221213 21:28:26 @agent_ppo2.py:185][0m |          -0.0051 |         174.3934 |           8.6487 |
[32m[20221213 21:28:27 @agent_ppo2.py:185][0m |          -0.0066 |         173.9575 |           8.6878 |
[32m[20221213 21:28:27 @agent_ppo2.py:185][0m |          -0.0052 |         173.5324 |           8.6335 |
[32m[20221213 21:28:27 @agent_ppo2.py:185][0m |          -0.0012 |         178.6171 |           8.6733 |
[32m[20221213 21:28:27 @agent_ppo2.py:185][0m |          -0.0015 |         174.7733 |           8.7376 |
[32m[20221213 21:28:27 @agent_ppo2.py:185][0m |          -0.0079 |         173.1042 |           8.6317 |
[32m[20221213 21:28:27 @agent_ppo2.py:185][0m |          -0.0080 |         172.8600 |           8.6448 |
[32m[20221213 21:28:27 @agent_ppo2.py:185][0m |           0.0018 |         183.9841 |           8.6603 |
[32m[20221213 21:28:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:28:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.80
[32m[20221213 21:28:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:28:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.00
[32m[20221213 21:28:27 @agent_ppo2.py:143][0m Total time:      32.87 min
[32m[20221213 21:28:27 @agent_ppo2.py:145][0m 3219456 total steps have happened
[32m[20221213 21:28:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1572 --------------------------#
[32m[20221213 21:28:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:27 @agent_ppo2.py:185][0m |           0.0002 |         173.9985 |           9.0886 |
[32m[20221213 21:28:28 @agent_ppo2.py:185][0m |           0.0001 |         173.5190 |           9.0908 |
[32m[20221213 21:28:28 @agent_ppo2.py:185][0m |          -0.0031 |         173.0564 |           9.0876 |
[32m[20221213 21:28:28 @agent_ppo2.py:185][0m |          -0.0025 |         172.8125 |           9.1415 |
[32m[20221213 21:28:28 @agent_ppo2.py:185][0m |          -0.0037 |         172.6471 |           9.0888 |
[32m[20221213 21:28:28 @agent_ppo2.py:185][0m |          -0.0009 |         173.9993 |           9.1324 |
[32m[20221213 21:28:28 @agent_ppo2.py:185][0m |           0.0047 |         184.5234 |           9.2051 |
[32m[20221213 21:28:28 @agent_ppo2.py:185][0m |           0.0058 |         183.9547 |           9.2023 |
[32m[20221213 21:28:28 @agent_ppo2.py:185][0m |          -0.0052 |         172.2420 |           9.2310 |
[32m[20221213 21:28:28 @agent_ppo2.py:185][0m |          -0.0053 |         172.0234 |           9.3196 |
[32m[20221213 21:28:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:28:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.20
[32m[20221213 21:28:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:28:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:28:28 @agent_ppo2.py:143][0m Total time:      32.89 min
[32m[20221213 21:28:28 @agent_ppo2.py:145][0m 3221504 total steps have happened
[32m[20221213 21:28:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1573 --------------------------#
[32m[20221213 21:28:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:28:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:29 @agent_ppo2.py:185][0m |          -0.0006 |         173.9825 |           9.4006 |
[32m[20221213 21:28:29 @agent_ppo2.py:185][0m |          -0.0041 |         173.0819 |           9.4258 |
[32m[20221213 21:28:29 @agent_ppo2.py:185][0m |          -0.0038 |         172.6085 |           9.4363 |
[32m[20221213 21:28:29 @agent_ppo2.py:185][0m |          -0.0036 |         172.3847 |           9.3693 |
[32m[20221213 21:28:29 @agent_ppo2.py:185][0m |           0.0033 |         183.3819 |           9.4122 |
[32m[20221213 21:28:29 @agent_ppo2.py:185][0m |          -0.0056 |         172.0263 |           9.3969 |
[32m[20221213 21:28:29 @agent_ppo2.py:185][0m |          -0.0092 |         171.9807 |           9.4073 |
[32m[20221213 21:28:29 @agent_ppo2.py:185][0m |          -0.0078 |         171.7501 |           9.3553 |
[32m[20221213 21:28:29 @agent_ppo2.py:185][0m |          -0.0087 |         171.5998 |           9.3974 |
[32m[20221213 21:28:29 @agent_ppo2.py:185][0m |          -0.0036 |         172.7745 |           9.3843 |
[32m[20221213 21:28:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:28:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.80
[32m[20221213 21:28:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:28:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.00
[32m[20221213 21:28:30 @agent_ppo2.py:143][0m Total time:      32.91 min
[32m[20221213 21:28:30 @agent_ppo2.py:145][0m 3223552 total steps have happened
[32m[20221213 21:28:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1574 --------------------------#
[32m[20221213 21:28:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:30 @agent_ppo2.py:185][0m |           0.0068 |         179.3031 |           9.1730 |
[32m[20221213 21:28:30 @agent_ppo2.py:185][0m |          -0.0038 |         173.4237 |           9.1553 |
[32m[20221213 21:28:30 @agent_ppo2.py:185][0m |           0.0064 |         181.4875 |           9.1550 |
[32m[20221213 21:28:30 @agent_ppo2.py:185][0m |          -0.0059 |         172.8761 |           9.1681 |
[32m[20221213 21:28:30 @agent_ppo2.py:185][0m |          -0.0061 |         172.6141 |           9.0354 |
[32m[20221213 21:28:30 @agent_ppo2.py:185][0m |          -0.0059 |         172.5161 |           9.0047 |
[32m[20221213 21:28:30 @agent_ppo2.py:185][0m |           0.0130 |         199.0808 |           8.9803 |
[32m[20221213 21:28:30 @agent_ppo2.py:185][0m |           0.0063 |         181.3640 |           9.0275 |
[32m[20221213 21:28:31 @agent_ppo2.py:185][0m |          -0.0058 |         172.1837 |           8.9619 |
[32m[20221213 21:28:31 @agent_ppo2.py:185][0m |          -0.0067 |         172.1033 |           8.9119 |
[32m[20221213 21:28:31 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:28:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.60
[32m[20221213 21:28:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:28:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.00
[32m[20221213 21:28:31 @agent_ppo2.py:143][0m Total time:      32.93 min
[32m[20221213 21:28:31 @agent_ppo2.py:145][0m 3225600 total steps have happened
[32m[20221213 21:28:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1575 --------------------------#
[32m[20221213 21:28:31 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:28:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:31 @agent_ppo2.py:185][0m |           0.0002 |         172.4585 |           8.8575 |
[32m[20221213 21:28:31 @agent_ppo2.py:185][0m |          -0.0031 |         172.0274 |           8.8816 |
[32m[20221213 21:28:31 @agent_ppo2.py:185][0m |          -0.0055 |         171.4949 |           8.8866 |
[32m[20221213 21:28:31 @agent_ppo2.py:185][0m |          -0.0033 |         171.2436 |           8.9313 |
[32m[20221213 21:28:31 @agent_ppo2.py:185][0m |          -0.0056 |         171.0197 |           8.9267 |
[32m[20221213 21:28:31 @agent_ppo2.py:185][0m |          -0.0062 |         170.9102 |           8.9456 |
[32m[20221213 21:28:32 @agent_ppo2.py:185][0m |          -0.0051 |         170.7740 |           8.9941 |
[32m[20221213 21:28:32 @agent_ppo2.py:185][0m |          -0.0066 |         170.6852 |           8.9946 |
[32m[20221213 21:28:32 @agent_ppo2.py:185][0m |          -0.0043 |         172.0285 |           8.9954 |
[32m[20221213 21:28:32 @agent_ppo2.py:185][0m |          -0.0087 |         170.3721 |           9.0053 |
[32m[20221213 21:28:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:28:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.80
[32m[20221213 21:28:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:28:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.00
[32m[20221213 21:28:32 @agent_ppo2.py:143][0m Total time:      32.95 min
[32m[20221213 21:28:32 @agent_ppo2.py:145][0m 3227648 total steps have happened
[32m[20221213 21:28:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1576 --------------------------#
[32m[20221213 21:28:32 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:28:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:32 @agent_ppo2.py:185][0m |          -0.0012 |         172.5633 |           9.0814 |
[32m[20221213 21:28:32 @agent_ppo2.py:185][0m |           0.0071 |         179.9564 |           9.0396 |
[32m[20221213 21:28:32 @agent_ppo2.py:185][0m |          -0.0054 |         171.4376 |           9.0506 |
[32m[20221213 21:28:33 @agent_ppo2.py:185][0m |          -0.0056 |         171.1050 |           9.0423 |
[32m[20221213 21:28:33 @agent_ppo2.py:185][0m |          -0.0058 |         170.9446 |           9.0596 |
[32m[20221213 21:28:33 @agent_ppo2.py:185][0m |           0.0020 |         177.7690 |           9.0306 |
[32m[20221213 21:28:33 @agent_ppo2.py:185][0m |          -0.0061 |         170.7101 |           9.0112 |
[32m[20221213 21:28:33 @agent_ppo2.py:185][0m |          -0.0019 |         172.2124 |           9.0050 |
[32m[20221213 21:28:33 @agent_ppo2.py:185][0m |           0.0018 |         175.6071 |           9.0019 |
[32m[20221213 21:28:33 @agent_ppo2.py:185][0m |          -0.0074 |         170.2667 |           9.0284 |
[32m[20221213 21:28:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:28:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.80
[32m[20221213 21:28:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:28:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.00
[32m[20221213 21:28:33 @agent_ppo2.py:143][0m Total time:      32.97 min
[32m[20221213 21:28:33 @agent_ppo2.py:145][0m 3229696 total steps have happened
[32m[20221213 21:28:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1577 --------------------------#
[32m[20221213 21:28:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:33 @agent_ppo2.py:185][0m |           0.0003 |         174.3518 |           9.0933 |
[32m[20221213 21:28:34 @agent_ppo2.py:185][0m |          -0.0043 |         171.9605 |           9.1002 |
[32m[20221213 21:28:34 @agent_ppo2.py:185][0m |          -0.0071 |         170.7722 |           9.0902 |
[32m[20221213 21:28:34 @agent_ppo2.py:185][0m |           0.0041 |         180.7704 |           9.0840 |
[32m[20221213 21:28:34 @agent_ppo2.py:185][0m |          -0.0084 |         169.5536 |           9.0806 |
[32m[20221213 21:28:34 @agent_ppo2.py:185][0m |          -0.0062 |         169.2678 |           9.0527 |
[32m[20221213 21:28:34 @agent_ppo2.py:185][0m |          -0.0075 |         168.8766 |           9.0695 |
[32m[20221213 21:28:34 @agent_ppo2.py:185][0m |          -0.0078 |         168.5528 |           9.0593 |
[32m[20221213 21:28:34 @agent_ppo2.py:185][0m |          -0.0070 |         168.7703 |           8.9937 |
[32m[20221213 21:28:34 @agent_ppo2.py:185][0m |          -0.0070 |         168.1667 |           9.0300 |
[32m[20221213 21:28:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:28:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.00
[32m[20221213 21:28:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.00
[32m[20221213 21:28:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:28:34 @agent_ppo2.py:143][0m Total time:      32.99 min
[32m[20221213 21:28:34 @agent_ppo2.py:145][0m 3231744 total steps have happened
[32m[20221213 21:28:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1578 --------------------------#
[32m[20221213 21:28:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:35 @agent_ppo2.py:185][0m |           0.0030 |         175.9716 |           8.6406 |
[32m[20221213 21:28:35 @agent_ppo2.py:185][0m |          -0.0021 |         175.4188 |           8.6167 |
[32m[20221213 21:28:35 @agent_ppo2.py:185][0m |           0.0080 |         192.4144 |           8.6213 |
[32m[20221213 21:28:35 @agent_ppo2.py:185][0m |          -0.0034 |         174.3141 |           8.6153 |
[32m[20221213 21:28:35 @agent_ppo2.py:185][0m |          -0.0057 |         173.8568 |           8.6139 |
[32m[20221213 21:28:35 @agent_ppo2.py:185][0m |          -0.0075 |         173.6507 |           8.5877 |
[32m[20221213 21:28:35 @agent_ppo2.py:185][0m |          -0.0016 |         175.4461 |           8.6118 |
[32m[20221213 21:28:35 @agent_ppo2.py:185][0m |          -0.0091 |         173.3983 |           8.5832 |
[32m[20221213 21:28:35 @agent_ppo2.py:185][0m |          -0.0062 |         173.1789 |           8.6195 |
[32m[20221213 21:28:35 @agent_ppo2.py:185][0m |          -0.0083 |         173.0005 |           8.5874 |
[32m[20221213 21:28:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:28:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.60
[32m[20221213 21:28:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:28:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.00
[32m[20221213 21:28:36 @agent_ppo2.py:143][0m Total time:      33.01 min
[32m[20221213 21:28:36 @agent_ppo2.py:145][0m 3233792 total steps have happened
[32m[20221213 21:28:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1579 --------------------------#
[32m[20221213 21:28:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:36 @agent_ppo2.py:185][0m |           0.0049 |         177.3730 |           8.7663 |
[32m[20221213 21:28:36 @agent_ppo2.py:185][0m |          -0.0001 |         171.6724 |           8.8239 |
[32m[20221213 21:28:36 @agent_ppo2.py:185][0m |          -0.0021 |         170.4345 |           8.7378 |
[32m[20221213 21:28:36 @agent_ppo2.py:185][0m |          -0.0054 |         169.8602 |           8.7359 |
[32m[20221213 21:28:36 @agent_ppo2.py:185][0m |           0.0031 |         183.7816 |           8.7330 |
[32m[20221213 21:28:36 @agent_ppo2.py:185][0m |           0.0044 |         181.8489 |           8.7212 |
[32m[20221213 21:28:36 @agent_ppo2.py:185][0m |          -0.0072 |         169.0047 |           8.7446 |
[32m[20221213 21:28:36 @agent_ppo2.py:185][0m |          -0.0081 |         168.6472 |           8.6784 |
[32m[20221213 21:28:37 @agent_ppo2.py:185][0m |          -0.0094 |         168.4953 |           8.6786 |
[32m[20221213 21:28:37 @agent_ppo2.py:185][0m |          -0.0090 |         168.0980 |           8.6923 |
[32m[20221213 21:28:37 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:28:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.00
[32m[20221213 21:28:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.00
[32m[20221213 21:28:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.00
[32m[20221213 21:28:37 @agent_ppo2.py:143][0m Total time:      33.04 min
[32m[20221213 21:28:37 @agent_ppo2.py:145][0m 3235840 total steps have happened
[32m[20221213 21:28:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1580 --------------------------#
[32m[20221213 21:28:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:37 @agent_ppo2.py:185][0m |           0.0008 |         175.5677 |           8.4188 |
[32m[20221213 21:28:37 @agent_ppo2.py:185][0m |          -0.0053 |         171.4109 |           8.4609 |
[32m[20221213 21:28:37 @agent_ppo2.py:185][0m |          -0.0044 |         170.3912 |           8.4288 |
[32m[20221213 21:28:37 @agent_ppo2.py:185][0m |          -0.0043 |         169.6158 |           8.4250 |
[32m[20221213 21:28:37 @agent_ppo2.py:185][0m |          -0.0030 |         168.9162 |           8.4353 |
[32m[20221213 21:28:38 @agent_ppo2.py:185][0m |          -0.0040 |         168.5922 |           8.4103 |
[32m[20221213 21:28:38 @agent_ppo2.py:185][0m |          -0.0022 |         174.1388 |           8.4249 |
[32m[20221213 21:28:38 @agent_ppo2.py:185][0m |          -0.0091 |         168.2188 |           8.4707 |
[32m[20221213 21:28:38 @agent_ppo2.py:185][0m |          -0.0062 |         167.8539 |           8.4646 |
[32m[20221213 21:28:38 @agent_ppo2.py:185][0m |           0.0104 |         192.9471 |           8.4306 |
[32m[20221213 21:28:38 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:28:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 610.60
[32m[20221213 21:28:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 761.00
[32m[20221213 21:28:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.00
[32m[20221213 21:28:38 @agent_ppo2.py:143][0m Total time:      33.06 min
[32m[20221213 21:28:38 @agent_ppo2.py:145][0m 3237888 total steps have happened
[32m[20221213 21:28:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1581 --------------------------#
[32m[20221213 21:28:38 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:28:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:38 @agent_ppo2.py:185][0m |          -0.0004 |         174.5150 |           8.6457 |
[32m[20221213 21:28:38 @agent_ppo2.py:185][0m |          -0.0062 |         172.9185 |           8.6721 |
[32m[20221213 21:28:39 @agent_ppo2.py:185][0m |          -0.0083 |         172.4420 |           8.6789 |
[32m[20221213 21:28:39 @agent_ppo2.py:185][0m |          -0.0061 |         172.6994 |           8.7363 |
[32m[20221213 21:28:39 @agent_ppo2.py:185][0m |          -0.0004 |         179.8451 |           8.7495 |
[32m[20221213 21:28:39 @agent_ppo2.py:185][0m |          -0.0081 |         171.6462 |           8.7089 |
[32m[20221213 21:28:39 @agent_ppo2.py:185][0m |           0.0013 |         191.4635 |           8.7338 |
[32m[20221213 21:28:39 @agent_ppo2.py:185][0m |          -0.0041 |         178.4045 |           8.7376 |
[32m[20221213 21:28:39 @agent_ppo2.py:185][0m |          -0.0061 |         173.5183 |           8.7799 |
[32m[20221213 21:28:39 @agent_ppo2.py:185][0m |          -0.0032 |         182.0135 |           8.8111 |
[32m[20221213 21:28:39 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:28:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.60
[32m[20221213 21:28:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:28:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:28:39 @agent_ppo2.py:143][0m Total time:      33.08 min
[32m[20221213 21:28:39 @agent_ppo2.py:145][0m 3239936 total steps have happened
[32m[20221213 21:28:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1582 --------------------------#
[32m[20221213 21:28:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:40 @agent_ppo2.py:185][0m |           0.0089 |         185.9021 |           9.1213 |
[32m[20221213 21:28:40 @agent_ppo2.py:185][0m |          -0.0037 |         172.0226 |           9.1755 |
[32m[20221213 21:28:40 @agent_ppo2.py:185][0m |          -0.0045 |         171.4639 |           9.1575 |
[32m[20221213 21:28:40 @agent_ppo2.py:185][0m |           0.0072 |         188.3688 |           9.1329 |
[32m[20221213 21:28:40 @agent_ppo2.py:185][0m |          -0.0044 |         170.5925 |           9.1345 |
[32m[20221213 21:28:40 @agent_ppo2.py:185][0m |          -0.0072 |         170.4199 |           9.1406 |
[32m[20221213 21:28:40 @agent_ppo2.py:185][0m |          -0.0066 |         169.9659 |           9.1410 |
[32m[20221213 21:28:40 @agent_ppo2.py:185][0m |          -0.0063 |         170.1279 |           9.1415 |
[32m[20221213 21:28:40 @agent_ppo2.py:185][0m |          -0.0111 |         169.7392 |           9.1750 |
[32m[20221213 21:28:40 @agent_ppo2.py:185][0m |          -0.0095 |         169.3579 |           9.1374 |
[32m[20221213 21:28:40 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:28:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.80
[32m[20221213 21:28:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:28:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 777.00
[32m[20221213 21:28:41 @agent_ppo2.py:143][0m Total time:      33.10 min
[32m[20221213 21:28:41 @agent_ppo2.py:145][0m 3241984 total steps have happened
[32m[20221213 21:28:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1583 --------------------------#
[32m[20221213 21:28:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:28:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:41 @agent_ppo2.py:185][0m |           0.0030 |         174.7019 |           8.8026 |
[32m[20221213 21:28:41 @agent_ppo2.py:185][0m |          -0.0035 |         171.8246 |           8.8365 |
[32m[20221213 21:28:41 @agent_ppo2.py:185][0m |          -0.0039 |         171.1889 |           8.8098 |
[32m[20221213 21:28:41 @agent_ppo2.py:185][0m |          -0.0063 |         170.4205 |           8.7363 |
[32m[20221213 21:28:41 @agent_ppo2.py:185][0m |          -0.0066 |         169.8423 |           8.7616 |
[32m[20221213 21:28:41 @agent_ppo2.py:185][0m |          -0.0030 |         174.6944 |           8.7202 |
[32m[20221213 21:28:41 @agent_ppo2.py:185][0m |          -0.0052 |         169.1062 |           8.7322 |
[32m[20221213 21:28:41 @agent_ppo2.py:185][0m |          -0.0079 |         169.0547 |           8.6901 |
[32m[20221213 21:28:42 @agent_ppo2.py:185][0m |          -0.0106 |         168.5679 |           8.7027 |
[32m[20221213 21:28:42 @agent_ppo2.py:185][0m |          -0.0102 |         168.3297 |           8.6894 |
[32m[20221213 21:28:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:28:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.60
[32m[20221213 21:28:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.00
[32m[20221213 21:28:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:28:42 @agent_ppo2.py:143][0m Total time:      33.12 min
[32m[20221213 21:28:42 @agent_ppo2.py:145][0m 3244032 total steps have happened
[32m[20221213 21:28:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1584 --------------------------#
[32m[20221213 21:28:42 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:28:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:42 @agent_ppo2.py:185][0m |          -0.0020 |         178.8786 |           8.5640 |
[32m[20221213 21:28:42 @agent_ppo2.py:185][0m |          -0.0002 |         176.8368 |           8.5728 |
[32m[20221213 21:28:42 @agent_ppo2.py:185][0m |          -0.0069 |         174.4751 |           8.5695 |
[32m[20221213 21:28:42 @agent_ppo2.py:185][0m |           0.0053 |         179.7200 |           8.5854 |
[32m[20221213 21:28:42 @agent_ppo2.py:185][0m |          -0.0008 |         178.3212 |           8.5747 |
[32m[20221213 21:28:43 @agent_ppo2.py:185][0m |          -0.0071 |         172.2303 |           8.6438 |
[32m[20221213 21:28:43 @agent_ppo2.py:185][0m |          -0.0080 |         172.0609 |           8.6289 |
[32m[20221213 21:28:43 @agent_ppo2.py:185][0m |          -0.0090 |         171.6726 |           8.6825 |
[32m[20221213 21:28:43 @agent_ppo2.py:185][0m |          -0.0098 |         171.4288 |           8.6302 |
[32m[20221213 21:28:43 @agent_ppo2.py:185][0m |          -0.0105 |         171.0193 |           8.6647 |
[32m[20221213 21:28:43 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:28:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.60
[32m[20221213 21:28:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.00
[32m[20221213 21:28:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 860.00
[32m[20221213 21:28:43 @agent_ppo2.py:143][0m Total time:      33.14 min
[32m[20221213 21:28:43 @agent_ppo2.py:145][0m 3246080 total steps have happened
[32m[20221213 21:28:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1585 --------------------------#
[32m[20221213 21:28:43 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:28:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:43 @agent_ppo2.py:185][0m |           0.0001 |         176.4546 |           8.5508 |
[32m[20221213 21:28:43 @agent_ppo2.py:185][0m |          -0.0003 |         174.2320 |           8.5746 |
[32m[20221213 21:28:43 @agent_ppo2.py:185][0m |           0.0031 |         175.4938 |           8.5742 |
[32m[20221213 21:28:44 @agent_ppo2.py:185][0m |          -0.0023 |         172.6928 |           8.5528 |
[32m[20221213 21:28:44 @agent_ppo2.py:185][0m |          -0.0062 |         172.2162 |           8.5777 |
[32m[20221213 21:28:44 @agent_ppo2.py:185][0m |          -0.0061 |         171.9362 |           8.6071 |
[32m[20221213 21:28:44 @agent_ppo2.py:185][0m |          -0.0064 |         171.7312 |           8.5865 |
[32m[20221213 21:28:44 @agent_ppo2.py:185][0m |           0.0031 |         181.5782 |           8.6059 |
[32m[20221213 21:28:44 @agent_ppo2.py:185][0m |          -0.0071 |         171.3659 |           8.5918 |
[32m[20221213 21:28:44 @agent_ppo2.py:185][0m |          -0.0074 |         171.2172 |           8.6202 |
[32m[20221213 21:28:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:28:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.40
[32m[20221213 21:28:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221213 21:28:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:28:44 @agent_ppo2.py:143][0m Total time:      33.16 min
[32m[20221213 21:28:44 @agent_ppo2.py:145][0m 3248128 total steps have happened
[32m[20221213 21:28:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1586 --------------------------#
[32m[20221213 21:28:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:28:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:44 @agent_ppo2.py:185][0m |           0.0007 |         171.9412 |           8.9057 |
[32m[20221213 21:28:45 @agent_ppo2.py:185][0m |           0.0152 |         194.1130 |           8.9023 |
[32m[20221213 21:28:45 @agent_ppo2.py:185][0m |          -0.0023 |         169.4370 |           8.8528 |
[32m[20221213 21:28:45 @agent_ppo2.py:185][0m |          -0.0057 |         168.6008 |           8.9287 |
[32m[20221213 21:28:45 @agent_ppo2.py:185][0m |          -0.0037 |         168.0253 |           8.9702 |
[32m[20221213 21:28:45 @agent_ppo2.py:185][0m |          -0.0045 |         167.8253 |           8.9615 |
[32m[20221213 21:28:45 @agent_ppo2.py:185][0m |           0.0043 |         178.3656 |           8.9959 |
[32m[20221213 21:28:45 @agent_ppo2.py:185][0m |          -0.0039 |         167.8965 |           8.9975 |
[32m[20221213 21:28:45 @agent_ppo2.py:185][0m |          -0.0062 |         166.9221 |           8.9837 |
[32m[20221213 21:28:45 @agent_ppo2.py:185][0m |           0.0043 |         175.6009 |           9.0127 |
[32m[20221213 21:28:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:28:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.40
[32m[20221213 21:28:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.00
[32m[20221213 21:28:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 870.00
[32m[20221213 21:28:45 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 870.00
[32m[20221213 21:28:45 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 870.00
[32m[20221213 21:28:45 @agent_ppo2.py:143][0m Total time:      33.18 min
[32m[20221213 21:28:45 @agent_ppo2.py:145][0m 3250176 total steps have happened
[32m[20221213 21:28:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1587 --------------------------#
[32m[20221213 21:28:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:46 @agent_ppo2.py:185][0m |           0.0021 |         173.3975 |           9.0315 |
[32m[20221213 21:28:46 @agent_ppo2.py:185][0m |          -0.0051 |         171.3838 |           9.0361 |
[32m[20221213 21:28:46 @agent_ppo2.py:185][0m |          -0.0062 |         170.7368 |           9.0574 |
[32m[20221213 21:28:46 @agent_ppo2.py:185][0m |          -0.0066 |         170.3190 |           9.0798 |
[32m[20221213 21:28:46 @agent_ppo2.py:185][0m |          -0.0089 |         170.0536 |           9.0479 |
[32m[20221213 21:28:46 @agent_ppo2.py:185][0m |          -0.0087 |         169.8436 |           9.0247 |
[32m[20221213 21:28:46 @agent_ppo2.py:185][0m |          -0.0086 |         169.4402 |           9.0926 |
[32m[20221213 21:28:46 @agent_ppo2.py:185][0m |          -0.0079 |         169.3632 |           9.0761 |
[32m[20221213 21:28:46 @agent_ppo2.py:185][0m |          -0.0104 |         169.4478 |           9.0610 |
[32m[20221213 21:28:46 @agent_ppo2.py:185][0m |          -0.0097 |         169.0121 |           9.0998 |
[32m[20221213 21:28:46 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:28:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.00
[32m[20221213 21:28:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.00
[32m[20221213 21:28:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 21:28:47 @agent_ppo2.py:143][0m Total time:      33.20 min
[32m[20221213 21:28:47 @agent_ppo2.py:145][0m 3252224 total steps have happened
[32m[20221213 21:28:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1588 --------------------------#
[32m[20221213 21:28:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:47 @agent_ppo2.py:185][0m |           0.0045 |         174.1884 |           8.9273 |
[32m[20221213 21:28:47 @agent_ppo2.py:185][0m |          -0.0005 |         171.2521 |           8.9404 |
[32m[20221213 21:28:47 @agent_ppo2.py:185][0m |          -0.0045 |         170.7478 |           8.9495 |
[32m[20221213 21:28:47 @agent_ppo2.py:185][0m |          -0.0041 |         170.4614 |           8.9541 |
[32m[20221213 21:28:47 @agent_ppo2.py:185][0m |          -0.0067 |         170.1772 |           8.9125 |
[32m[20221213 21:28:47 @agent_ppo2.py:185][0m |          -0.0052 |         170.0025 |           8.9546 |
[32m[20221213 21:28:47 @agent_ppo2.py:185][0m |          -0.0054 |         169.7850 |           8.9395 |
[32m[20221213 21:28:47 @agent_ppo2.py:185][0m |          -0.0061 |         169.6757 |           8.9376 |
[32m[20221213 21:28:48 @agent_ppo2.py:185][0m |          -0.0062 |         169.5413 |           8.9550 |
[32m[20221213 21:28:48 @agent_ppo2.py:185][0m |          -0.0042 |         169.9443 |           8.9522 |
[32m[20221213 21:28:48 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:28:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.40
[32m[20221213 21:28:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.00
[32m[20221213 21:28:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:28:48 @agent_ppo2.py:143][0m Total time:      33.22 min
[32m[20221213 21:28:48 @agent_ppo2.py:145][0m 3254272 total steps have happened
[32m[20221213 21:28:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1589 --------------------------#
[32m[20221213 21:28:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:48 @agent_ppo2.py:185][0m |          -0.0002 |         171.7746 |           9.2765 |
[32m[20221213 21:28:48 @agent_ppo2.py:185][0m |          -0.0046 |         170.8344 |           9.2745 |
[32m[20221213 21:28:48 @agent_ppo2.py:185][0m |           0.0070 |         186.2978 |           9.2308 |
[32m[20221213 21:28:48 @agent_ppo2.py:185][0m |          -0.0064 |         170.0326 |           9.2871 |
[32m[20221213 21:28:48 @agent_ppo2.py:185][0m |          -0.0075 |         169.8433 |           9.2436 |
[32m[20221213 21:28:49 @agent_ppo2.py:185][0m |          -0.0067 |         169.6452 |           9.2473 |
[32m[20221213 21:28:49 @agent_ppo2.py:185][0m |          -0.0064 |         169.5119 |           9.2583 |
[32m[20221213 21:28:49 @agent_ppo2.py:185][0m |          -0.0085 |         169.3667 |           9.2359 |
[32m[20221213 21:28:49 @agent_ppo2.py:185][0m |          -0.0086 |         169.3036 |           9.2148 |
[32m[20221213 21:28:49 @agent_ppo2.py:185][0m |          -0.0077 |         169.1506 |           9.1914 |
[32m[20221213 21:28:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:28:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.60
[32m[20221213 21:28:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.00
[32m[20221213 21:28:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.00
[32m[20221213 21:28:49 @agent_ppo2.py:143][0m Total time:      33.24 min
[32m[20221213 21:28:49 @agent_ppo2.py:145][0m 3256320 total steps have happened
[32m[20221213 21:28:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1590 --------------------------#
[32m[20221213 21:28:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:49 @agent_ppo2.py:185][0m |           0.0109 |         185.6378 |           8.9993 |
[32m[20221213 21:28:49 @agent_ppo2.py:185][0m |          -0.0043 |         170.8821 |           8.9784 |
[32m[20221213 21:28:49 @agent_ppo2.py:185][0m |          -0.0052 |         170.4629 |           8.9601 |
[32m[20221213 21:28:50 @agent_ppo2.py:185][0m |          -0.0058 |         170.3118 |           9.0109 |
[32m[20221213 21:28:50 @agent_ppo2.py:185][0m |          -0.0063 |         170.2287 |           8.9750 |
[32m[20221213 21:28:50 @agent_ppo2.py:185][0m |          -0.0072 |         170.0690 |           8.9555 |
[32m[20221213 21:28:50 @agent_ppo2.py:185][0m |          -0.0076 |         170.0004 |           9.0045 |
[32m[20221213 21:28:50 @agent_ppo2.py:185][0m |          -0.0074 |         169.8424 |           8.9923 |
[32m[20221213 21:28:50 @agent_ppo2.py:185][0m |          -0.0079 |         169.7569 |           9.0050 |
[32m[20221213 21:28:50 @agent_ppo2.py:185][0m |          -0.0092 |         169.7252 |           8.9964 |
[32m[20221213 21:28:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:28:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.40
[32m[20221213 21:28:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.00
[32m[20221213 21:28:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:28:50 @agent_ppo2.py:143][0m Total time:      33.26 min
[32m[20221213 21:28:50 @agent_ppo2.py:145][0m 3258368 total steps have happened
[32m[20221213 21:28:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1591 --------------------------#
[32m[20221213 21:28:50 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:28:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:51 @agent_ppo2.py:185][0m |          -0.0015 |         172.8748 |           8.9781 |
[32m[20221213 21:28:51 @agent_ppo2.py:185][0m |          -0.0046 |         169.0633 |           8.9626 |
[32m[20221213 21:28:51 @agent_ppo2.py:185][0m |          -0.0058 |         167.3228 |           8.9790 |
[32m[20221213 21:28:51 @agent_ppo2.py:185][0m |           0.0079 |         191.5432 |           8.9699 |
[32m[20221213 21:28:51 @agent_ppo2.py:185][0m |           0.0021 |         166.8583 |           8.9970 |
[32m[20221213 21:28:51 @agent_ppo2.py:185][0m |          -0.0069 |         165.1018 |           9.0388 |
[32m[20221213 21:28:51 @agent_ppo2.py:185][0m |          -0.0090 |         164.3724 |           9.0123 |
[32m[20221213 21:28:51 @agent_ppo2.py:185][0m |          -0.0093 |         164.0385 |           8.9676 |
[32m[20221213 21:28:51 @agent_ppo2.py:185][0m |          -0.0081 |         163.7084 |           8.9930 |
[32m[20221213 21:28:51 @agent_ppo2.py:185][0m |           0.0042 |         173.0514 |           8.9543 |
[32m[20221213 21:28:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:28:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.60
[32m[20221213 21:28:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.00
[32m[20221213 21:28:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 866.00
[32m[20221213 21:28:51 @agent_ppo2.py:143][0m Total time:      33.28 min
[32m[20221213 21:28:51 @agent_ppo2.py:145][0m 3260416 total steps have happened
[32m[20221213 21:28:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1592 --------------------------#
[32m[20221213 21:28:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:52 @agent_ppo2.py:185][0m |           0.0029 |         178.3518 |           8.9794 |
[32m[20221213 21:28:52 @agent_ppo2.py:185][0m |          -0.0059 |         177.0823 |           8.9765 |
[32m[20221213 21:28:52 @agent_ppo2.py:185][0m |          -0.0058 |         176.1416 |           9.0411 |
[32m[20221213 21:28:52 @agent_ppo2.py:185][0m |          -0.0028 |         180.5390 |           8.9946 |
[32m[20221213 21:28:52 @agent_ppo2.py:185][0m |          -0.0074 |         175.1718 |           9.0663 |
[32m[20221213 21:28:52 @agent_ppo2.py:185][0m |          -0.0084 |         174.9434 |           9.0743 |
[32m[20221213 21:28:52 @agent_ppo2.py:185][0m |          -0.0037 |         176.8568 |           9.1166 |
[32m[20221213 21:28:52 @agent_ppo2.py:185][0m |          -0.0066 |         174.2426 |           9.1137 |
[32m[20221213 21:28:52 @agent_ppo2.py:185][0m |          -0.0054 |         174.1926 |           9.1364 |
[32m[20221213 21:28:52 @agent_ppo2.py:185][0m |           0.0012 |         183.0527 |           9.1367 |
[32m[20221213 21:28:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:28:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.00
[32m[20221213 21:28:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:28:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:28:53 @agent_ppo2.py:143][0m Total time:      33.30 min
[32m[20221213 21:28:53 @agent_ppo2.py:145][0m 3262464 total steps have happened
[32m[20221213 21:28:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1593 --------------------------#
[32m[20221213 21:28:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:53 @agent_ppo2.py:185][0m |           0.0110 |         181.6373 |           9.1419 |
[32m[20221213 21:28:53 @agent_ppo2.py:185][0m |          -0.0030 |         174.8753 |           9.0900 |
[32m[20221213 21:28:53 @agent_ppo2.py:185][0m |          -0.0039 |         174.1262 |           9.0201 |
[32m[20221213 21:28:53 @agent_ppo2.py:185][0m |          -0.0059 |         173.8237 |           9.0499 |
[32m[20221213 21:28:53 @agent_ppo2.py:185][0m |          -0.0056 |         173.4421 |           9.0747 |
[32m[20221213 21:28:53 @agent_ppo2.py:185][0m |          -0.0084 |         173.2998 |           9.0663 |
[32m[20221213 21:28:53 @agent_ppo2.py:185][0m |          -0.0077 |         172.7350 |           9.0577 |
[32m[20221213 21:28:54 @agent_ppo2.py:185][0m |          -0.0089 |         172.6134 |           9.0834 |
[32m[20221213 21:28:54 @agent_ppo2.py:185][0m |          -0.0074 |         172.4505 |           9.0664 |
[32m[20221213 21:28:54 @agent_ppo2.py:185][0m |          -0.0071 |         172.4482 |           9.0861 |
[32m[20221213 21:28:54 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:28:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.40
[32m[20221213 21:28:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.00
[32m[20221213 21:28:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.00
[32m[20221213 21:28:54 @agent_ppo2.py:143][0m Total time:      33.32 min
[32m[20221213 21:28:54 @agent_ppo2.py:145][0m 3264512 total steps have happened
[32m[20221213 21:28:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1594 --------------------------#
[32m[20221213 21:28:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:54 @agent_ppo2.py:185][0m |          -0.0008 |         171.4362 |           9.1505 |
[32m[20221213 21:28:54 @agent_ppo2.py:185][0m |          -0.0049 |         170.5045 |           9.1956 |
[32m[20221213 21:28:54 @agent_ppo2.py:185][0m |          -0.0078 |         170.1207 |           9.1530 |
[32m[20221213 21:28:54 @agent_ppo2.py:185][0m |           0.0086 |         189.5357 |           9.1312 |
[32m[20221213 21:28:55 @agent_ppo2.py:185][0m |          -0.0087 |         169.7369 |           9.1199 |
[32m[20221213 21:28:55 @agent_ppo2.py:185][0m |          -0.0088 |         169.3526 |           9.0713 |
[32m[20221213 21:28:55 @agent_ppo2.py:185][0m |           0.0146 |         206.2690 |           9.0690 |
[32m[20221213 21:28:55 @agent_ppo2.py:185][0m |           0.0023 |         181.4795 |           9.0501 |
[32m[20221213 21:28:55 @agent_ppo2.py:185][0m |          -0.0064 |         169.4406 |           9.0931 |
[32m[20221213 21:28:55 @agent_ppo2.py:185][0m |           0.0012 |         180.9162 |           9.0322 |
[32m[20221213 21:28:55 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:28:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.60
[32m[20221213 21:28:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.00
[32m[20221213 21:28:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.00
[32m[20221213 21:28:55 @agent_ppo2.py:143][0m Total time:      33.34 min
[32m[20221213 21:28:55 @agent_ppo2.py:145][0m 3266560 total steps have happened
[32m[20221213 21:28:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1595 --------------------------#
[32m[20221213 21:28:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:55 @agent_ppo2.py:185][0m |          -0.0031 |         172.3019 |           9.1392 |
[32m[20221213 21:28:55 @agent_ppo2.py:185][0m |          -0.0037 |         171.2864 |           9.1734 |
[32m[20221213 21:28:56 @agent_ppo2.py:185][0m |           0.0025 |         177.9061 |           9.1451 |
[32m[20221213 21:28:56 @agent_ppo2.py:185][0m |          -0.0070 |         170.6281 |           9.1856 |
[32m[20221213 21:28:56 @agent_ppo2.py:185][0m |           0.0021 |         178.2265 |           9.1523 |
[32m[20221213 21:28:56 @agent_ppo2.py:185][0m |          -0.0081 |         170.3850 |           9.1639 |
[32m[20221213 21:28:56 @agent_ppo2.py:185][0m |          -0.0085 |         170.0419 |           9.1652 |
[32m[20221213 21:28:56 @agent_ppo2.py:185][0m |          -0.0092 |         170.0303 |           9.1729 |
[32m[20221213 21:28:56 @agent_ppo2.py:185][0m |           0.0048 |         184.2330 |           9.1731 |
[32m[20221213 21:28:56 @agent_ppo2.py:185][0m |          -0.0101 |         169.9196 |           9.2342 |
[32m[20221213 21:28:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:28:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.60
[32m[20221213 21:28:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:28:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.00
[32m[20221213 21:28:56 @agent_ppo2.py:143][0m Total time:      33.36 min
[32m[20221213 21:28:56 @agent_ppo2.py:145][0m 3268608 total steps have happened
[32m[20221213 21:28:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1596 --------------------------#
[32m[20221213 21:28:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:28:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:57 @agent_ppo2.py:185][0m |          -0.0016 |         174.7240 |           9.3549 |
[32m[20221213 21:28:57 @agent_ppo2.py:185][0m |          -0.0010 |         173.3097 |           9.3602 |
[32m[20221213 21:28:57 @agent_ppo2.py:185][0m |          -0.0050 |         172.7615 |           9.3871 |
[32m[20221213 21:28:57 @agent_ppo2.py:185][0m |          -0.0076 |         172.0942 |           9.3494 |
[32m[20221213 21:28:57 @agent_ppo2.py:185][0m |          -0.0063 |         171.6597 |           9.3794 |
[32m[20221213 21:28:57 @agent_ppo2.py:185][0m |           0.0076 |         183.3076 |           9.4212 |
[32m[20221213 21:28:57 @agent_ppo2.py:185][0m |          -0.0075 |         171.4050 |           9.3726 |
[32m[20221213 21:28:57 @agent_ppo2.py:185][0m |          -0.0082 |         171.2255 |           9.3769 |
[32m[20221213 21:28:57 @agent_ppo2.py:185][0m |          -0.0097 |         170.7300 |           9.3672 |
[32m[20221213 21:28:57 @agent_ppo2.py:185][0m |          -0.0088 |         170.4738 |           9.3737 |
[32m[20221213 21:28:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:28:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.20
[32m[20221213 21:28:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:28:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.00
[32m[20221213 21:28:58 @agent_ppo2.py:143][0m Total time:      33.38 min
[32m[20221213 21:28:58 @agent_ppo2.py:145][0m 3270656 total steps have happened
[32m[20221213 21:28:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1597 --------------------------#
[32m[20221213 21:28:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:58 @agent_ppo2.py:185][0m |          -0.0009 |         171.0765 |           9.1075 |
[32m[20221213 21:28:58 @agent_ppo2.py:185][0m |          -0.0034 |         170.5048 |           9.1170 |
[32m[20221213 21:28:58 @agent_ppo2.py:185][0m |          -0.0014 |         171.1053 |           9.1130 |
[32m[20221213 21:28:58 @agent_ppo2.py:185][0m |          -0.0046 |         170.1746 |           9.1930 |
[32m[20221213 21:28:58 @agent_ppo2.py:185][0m |          -0.0039 |         170.1030 |           9.1724 |
[32m[20221213 21:28:58 @agent_ppo2.py:185][0m |          -0.0065 |         169.9445 |           9.1997 |
[32m[20221213 21:28:58 @agent_ppo2.py:185][0m |          -0.0057 |         169.7264 |           9.2003 |
[32m[20221213 21:28:58 @agent_ppo2.py:185][0m |          -0.0026 |         171.3625 |           9.2378 |
[32m[20221213 21:28:59 @agent_ppo2.py:185][0m |          -0.0078 |         169.5767 |           9.2097 |
[32m[20221213 21:28:59 @agent_ppo2.py:185][0m |          -0.0087 |         169.5093 |           9.2408 |
[32m[20221213 21:28:59 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:28:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.60
[32m[20221213 21:28:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:28:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.00
[32m[20221213 21:28:59 @agent_ppo2.py:143][0m Total time:      33.40 min
[32m[20221213 21:28:59 @agent_ppo2.py:145][0m 3272704 total steps have happened
[32m[20221213 21:28:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1598 --------------------------#
[32m[20221213 21:28:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:28:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:28:59 @agent_ppo2.py:185][0m |          -0.0033 |         172.0522 |           9.2237 |
[32m[20221213 21:28:59 @agent_ppo2.py:185][0m |          -0.0052 |         171.2184 |           9.1725 |
[32m[20221213 21:28:59 @agent_ppo2.py:185][0m |           0.0053 |         188.0342 |           9.1906 |
[32m[20221213 21:28:59 @agent_ppo2.py:185][0m |          -0.0004 |         172.3705 |           9.2105 |
[32m[20221213 21:28:59 @agent_ppo2.py:185][0m |          -0.0090 |         170.1261 |           9.1529 |
[32m[20221213 21:28:59 @agent_ppo2.py:185][0m |          -0.0079 |         169.8035 |           9.1753 |
[32m[20221213 21:29:00 @agent_ppo2.py:185][0m |          -0.0087 |         170.0373 |           9.1348 |
[32m[20221213 21:29:00 @agent_ppo2.py:185][0m |          -0.0076 |         169.6816 |           9.1693 |
[32m[20221213 21:29:00 @agent_ppo2.py:185][0m |          -0.0102 |         169.5931 |           9.1739 |
[32m[20221213 21:29:00 @agent_ppo2.py:185][0m |          -0.0102 |         169.2436 |           9.1944 |
[32m[20221213 21:29:00 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:29:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.40
[32m[20221213 21:29:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:29:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.00
[32m[20221213 21:29:00 @agent_ppo2.py:143][0m Total time:      33.42 min
[32m[20221213 21:29:00 @agent_ppo2.py:145][0m 3274752 total steps have happened
[32m[20221213 21:29:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1599 --------------------------#
[32m[20221213 21:29:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:29:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:00 @agent_ppo2.py:185][0m |           0.0058 |         175.5078 |           9.4140 |
[32m[20221213 21:29:00 @agent_ppo2.py:185][0m |           0.0013 |         172.7811 |           9.4244 |
[32m[20221213 21:29:00 @agent_ppo2.py:185][0m |          -0.0030 |         171.1261 |           9.4534 |
[32m[20221213 21:29:01 @agent_ppo2.py:185][0m |          -0.0032 |         170.5336 |           9.4909 |
[32m[20221213 21:29:01 @agent_ppo2.py:185][0m |          -0.0034 |         170.2319 |           9.4851 |
[32m[20221213 21:29:01 @agent_ppo2.py:185][0m |          -0.0047 |         169.8913 |           9.5036 |
[32m[20221213 21:29:01 @agent_ppo2.py:185][0m |          -0.0058 |         169.6551 |           9.5287 |
[32m[20221213 21:29:01 @agent_ppo2.py:185][0m |          -0.0050 |         169.7813 |           9.5120 |
[32m[20221213 21:29:01 @agent_ppo2.py:185][0m |          -0.0044 |         169.4167 |           9.5588 |
[32m[20221213 21:29:01 @agent_ppo2.py:185][0m |           0.0117 |         194.1355 |           9.5757 |
[32m[20221213 21:29:01 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:29:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.60
[32m[20221213 21:29:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:29:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 864.00
[32m[20221213 21:29:01 @agent_ppo2.py:143][0m Total time:      33.44 min
[32m[20221213 21:29:01 @agent_ppo2.py:145][0m 3276800 total steps have happened
[32m[20221213 21:29:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1600 --------------------------#
[32m[20221213 21:29:01 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:29:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:01 @agent_ppo2.py:185][0m |          -0.0022 |         173.9757 |           9.0426 |
[32m[20221213 21:29:02 @agent_ppo2.py:185][0m |          -0.0021 |         173.4311 |           8.9670 |
[32m[20221213 21:29:02 @agent_ppo2.py:185][0m |           0.0007 |         174.3770 |           8.8876 |
[32m[20221213 21:29:02 @agent_ppo2.py:185][0m |          -0.0076 |         168.9927 |           8.8725 |
[32m[20221213 21:29:02 @agent_ppo2.py:185][0m |           0.0019 |         181.0568 |           8.8862 |
[32m[20221213 21:29:02 @agent_ppo2.py:185][0m |          -0.0047 |         170.1444 |           8.8821 |
[32m[20221213 21:29:02 @agent_ppo2.py:185][0m |          -0.0118 |         167.1830 |           8.8288 |
[32m[20221213 21:29:02 @agent_ppo2.py:185][0m |          -0.0011 |         180.9353 |           8.8524 |
[32m[20221213 21:29:02 @agent_ppo2.py:185][0m |          -0.0102 |         166.5216 |           8.8830 |
[32m[20221213 21:29:02 @agent_ppo2.py:185][0m |          -0.0133 |         166.4191 |           8.8528 |
[32m[20221213 21:29:02 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:29:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 736.60
[32m[20221213 21:29:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:29:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:29:02 @agent_ppo2.py:143][0m Total time:      33.46 min
[32m[20221213 21:29:02 @agent_ppo2.py:145][0m 3278848 total steps have happened
[32m[20221213 21:29:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1601 --------------------------#
[32m[20221213 21:29:03 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:29:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:03 @agent_ppo2.py:185][0m |          -0.0028 |         174.0423 |           8.9705 |
[32m[20221213 21:29:03 @agent_ppo2.py:185][0m |          -0.0019 |         172.9486 |           9.0099 |
[32m[20221213 21:29:03 @agent_ppo2.py:185][0m |          -0.0077 |         170.7601 |           9.1104 |
[32m[20221213 21:29:03 @agent_ppo2.py:185][0m |           0.0010 |         187.5717 |           9.0340 |
[32m[20221213 21:29:03 @agent_ppo2.py:185][0m |          -0.0048 |         169.7383 |           9.1341 |
[32m[20221213 21:29:03 @agent_ppo2.py:185][0m |          -0.0078 |         169.0574 |           9.1198 |
[32m[20221213 21:29:03 @agent_ppo2.py:185][0m |          -0.0085 |         168.5878 |           9.1197 |
[32m[20221213 21:29:03 @agent_ppo2.py:185][0m |          -0.0103 |         168.4353 |           9.1141 |
[32m[20221213 21:29:03 @agent_ppo2.py:185][0m |          -0.0109 |         168.0527 |           9.1313 |
[32m[20221213 21:29:04 @agent_ppo2.py:185][0m |          -0.0091 |         167.6811 |           9.1638 |
[32m[20221213 21:29:04 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:29:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 758.20
[32m[20221213 21:29:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:29:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.00
[32m[20221213 21:29:04 @agent_ppo2.py:143][0m Total time:      33.48 min
[32m[20221213 21:29:04 @agent_ppo2.py:145][0m 3280896 total steps have happened
[32m[20221213 21:29:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1602 --------------------------#
[32m[20221213 21:29:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:04 @agent_ppo2.py:185][0m |           0.0061 |         173.8672 |           9.1696 |
[32m[20221213 21:29:04 @agent_ppo2.py:185][0m |          -0.0025 |         171.8470 |           9.1902 |
[32m[20221213 21:29:04 @agent_ppo2.py:185][0m |          -0.0043 |         170.8977 |           9.1387 |
[32m[20221213 21:29:04 @agent_ppo2.py:185][0m |           0.0021 |         175.5644 |           9.1602 |
[32m[20221213 21:29:04 @agent_ppo2.py:185][0m |           0.0000 |         183.0331 |           9.0928 |
[32m[20221213 21:29:04 @agent_ppo2.py:185][0m |           0.0121 |         192.9186 |           9.1272 |
[32m[20221213 21:29:05 @agent_ppo2.py:185][0m |          -0.0081 |         170.2311 |           9.0952 |
[32m[20221213 21:29:05 @agent_ppo2.py:185][0m |          -0.0029 |         172.7379 |           9.1013 |
[32m[20221213 21:29:05 @agent_ppo2.py:185][0m |          -0.0071 |         170.0541 |           9.0781 |
[32m[20221213 21:29:05 @agent_ppo2.py:185][0m |          -0.0075 |         169.6890 |           9.0497 |
[32m[20221213 21:29:05 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:29:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.40
[32m[20221213 21:29:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.00
[32m[20221213 21:29:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.00
[32m[20221213 21:29:05 @agent_ppo2.py:143][0m Total time:      33.50 min
[32m[20221213 21:29:05 @agent_ppo2.py:145][0m 3282944 total steps have happened
[32m[20221213 21:29:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1603 --------------------------#
[32m[20221213 21:29:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:05 @agent_ppo2.py:185][0m |           0.0072 |         175.5466 |           9.0145 |
[32m[20221213 21:29:05 @agent_ppo2.py:185][0m |          -0.0026 |         170.1543 |           9.0096 |
[32m[20221213 21:29:05 @agent_ppo2.py:185][0m |          -0.0054 |         169.5010 |           9.0467 |
[32m[20221213 21:29:06 @agent_ppo2.py:185][0m |          -0.0065 |         169.3145 |           8.9574 |
[32m[20221213 21:29:06 @agent_ppo2.py:185][0m |          -0.0034 |         169.8222 |           8.9365 |
[32m[20221213 21:29:06 @agent_ppo2.py:185][0m |          -0.0026 |         172.0541 |           8.9200 |
[32m[20221213 21:29:06 @agent_ppo2.py:185][0m |          -0.0027 |         171.3025 |           8.9146 |
[32m[20221213 21:29:06 @agent_ppo2.py:185][0m |          -0.0092 |         168.5491 |           8.8770 |
[32m[20221213 21:29:06 @agent_ppo2.py:185][0m |          -0.0074 |         168.3368 |           8.8607 |
[32m[20221213 21:29:06 @agent_ppo2.py:185][0m |          -0.0007 |         183.2382 |           8.8584 |
[32m[20221213 21:29:06 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:29:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.00
[32m[20221213 21:29:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:29:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.00
[32m[20221213 21:29:06 @agent_ppo2.py:143][0m Total time:      33.53 min
[32m[20221213 21:29:06 @agent_ppo2.py:145][0m 3284992 total steps have happened
[32m[20221213 21:29:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1604 --------------------------#
[32m[20221213 21:29:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:07 @agent_ppo2.py:185][0m |          -0.0019 |         172.8037 |           8.6370 |
[32m[20221213 21:29:07 @agent_ppo2.py:185][0m |          -0.0023 |         170.9954 |           8.6679 |
[32m[20221213 21:29:07 @agent_ppo2.py:185][0m |           0.0086 |         189.8454 |           8.6842 |
[32m[20221213 21:29:07 @agent_ppo2.py:185][0m |          -0.0032 |         169.7702 |           8.6918 |
[32m[20221213 21:29:07 @agent_ppo2.py:185][0m |          -0.0057 |         169.0750 |           8.7230 |
[32m[20221213 21:29:07 @agent_ppo2.py:185][0m |          -0.0046 |         168.7053 |           8.7224 |
[32m[20221213 21:29:07 @agent_ppo2.py:185][0m |           0.0016 |         172.7823 |           8.6621 |
[32m[20221213 21:29:07 @agent_ppo2.py:185][0m |          -0.0068 |         168.1849 |           8.6813 |
[32m[20221213 21:29:07 @agent_ppo2.py:185][0m |          -0.0055 |         167.9461 |           8.7388 |
[32m[20221213 21:29:07 @agent_ppo2.py:185][0m |          -0.0078 |         167.8281 |           8.6805 |
[32m[20221213 21:29:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:29:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.40
[32m[20221213 21:29:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:29:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.00
[32m[20221213 21:29:07 @agent_ppo2.py:143][0m Total time:      33.55 min
[32m[20221213 21:29:07 @agent_ppo2.py:145][0m 3287040 total steps have happened
[32m[20221213 21:29:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1605 --------------------------#
[32m[20221213 21:29:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:08 @agent_ppo2.py:185][0m |          -0.0020 |         170.4037 |           8.3631 |
[32m[20221213 21:29:08 @agent_ppo2.py:185][0m |          -0.0028 |         170.2453 |           8.4380 |
[32m[20221213 21:29:08 @agent_ppo2.py:185][0m |          -0.0048 |         169.7826 |           8.4410 |
[32m[20221213 21:29:08 @agent_ppo2.py:185][0m |          -0.0063 |         169.6541 |           8.4071 |
[32m[20221213 21:29:08 @agent_ppo2.py:185][0m |          -0.0011 |         172.0026 |           8.3997 |
[32m[20221213 21:29:08 @agent_ppo2.py:185][0m |          -0.0060 |         169.2914 |           8.4062 |
[32m[20221213 21:29:08 @agent_ppo2.py:185][0m |           0.0019 |         175.6131 |           8.4569 |
[32m[20221213 21:29:08 @agent_ppo2.py:185][0m |          -0.0068 |         169.0736 |           8.4496 |
[32m[20221213 21:29:08 @agent_ppo2.py:185][0m |          -0.0079 |         169.0392 |           8.3918 |
[32m[20221213 21:29:08 @agent_ppo2.py:185][0m |          -0.0078 |         168.9188 |           8.3904 |
[32m[20221213 21:29:08 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:29:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.80
[32m[20221213 21:29:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:29:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 699.00
[32m[20221213 21:29:09 @agent_ppo2.py:143][0m Total time:      33.57 min
[32m[20221213 21:29:09 @agent_ppo2.py:145][0m 3289088 total steps have happened
[32m[20221213 21:29:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1606 --------------------------#
[32m[20221213 21:29:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:09 @agent_ppo2.py:185][0m |          -0.0003 |         171.9039 |           8.5492 |
[32m[20221213 21:29:09 @agent_ppo2.py:185][0m |          -0.0055 |         169.7570 |           8.6089 |
[32m[20221213 21:29:09 @agent_ppo2.py:185][0m |          -0.0071 |         168.3049 |           8.5791 |
[32m[20221213 21:29:09 @agent_ppo2.py:185][0m |          -0.0069 |         167.5377 |           8.5727 |
[32m[20221213 21:29:09 @agent_ppo2.py:185][0m |          -0.0069 |         167.0185 |           8.5408 |
[32m[20221213 21:29:09 @agent_ppo2.py:185][0m |          -0.0062 |         166.7191 |           8.5524 |
[32m[20221213 21:29:09 @agent_ppo2.py:185][0m |          -0.0086 |         166.2124 |           8.5089 |
[32m[20221213 21:29:10 @agent_ppo2.py:185][0m |          -0.0099 |         165.8774 |           8.5191 |
[32m[20221213 21:29:10 @agent_ppo2.py:185][0m |          -0.0112 |         165.5789 |           8.5076 |
[32m[20221213 21:29:10 @agent_ppo2.py:185][0m |           0.0015 |         171.9926 |           8.5790 |
[32m[20221213 21:29:10 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:29:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.80
[32m[20221213 21:29:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:29:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.00
[32m[20221213 21:29:10 @agent_ppo2.py:143][0m Total time:      33.59 min
[32m[20221213 21:29:10 @agent_ppo2.py:145][0m 3291136 total steps have happened
[32m[20221213 21:29:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1607 --------------------------#
[32m[20221213 21:29:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:29:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:10 @agent_ppo2.py:185][0m |          -0.0023 |         173.3530 |           8.4956 |
[32m[20221213 21:29:10 @agent_ppo2.py:185][0m |          -0.0058 |         171.6914 |           8.5475 |
[32m[20221213 21:29:11 @agent_ppo2.py:185][0m |           0.0003 |         174.1396 |           8.5654 |
[32m[20221213 21:29:11 @agent_ppo2.py:185][0m |          -0.0054 |         170.6891 |           8.5909 |
[32m[20221213 21:29:11 @agent_ppo2.py:185][0m |          -0.0006 |         172.8476 |           8.6168 |
[32m[20221213 21:29:11 @agent_ppo2.py:185][0m |          -0.0070 |         170.2670 |           8.6454 |
[32m[20221213 21:29:11 @agent_ppo2.py:185][0m |          -0.0070 |         169.9238 |           8.5986 |
[32m[20221213 21:29:11 @agent_ppo2.py:185][0m |          -0.0070 |         169.7094 |           8.6892 |
[32m[20221213 21:29:11 @agent_ppo2.py:185][0m |          -0.0084 |         169.7180 |           8.6474 |
[32m[20221213 21:29:11 @agent_ppo2.py:185][0m |          -0.0082 |         169.5424 |           8.6076 |
[32m[20221213 21:29:11 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221213 21:29:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.60
[32m[20221213 21:29:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:29:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 861.00
[32m[20221213 21:29:11 @agent_ppo2.py:143][0m Total time:      33.61 min
[32m[20221213 21:29:11 @agent_ppo2.py:145][0m 3293184 total steps have happened
[32m[20221213 21:29:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1608 --------------------------#
[32m[20221213 21:29:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:12 @agent_ppo2.py:185][0m |          -0.0004 |         171.0896 |           8.4835 |
[32m[20221213 21:29:12 @agent_ppo2.py:185][0m |          -0.0008 |         171.4978 |           8.5420 |
[32m[20221213 21:29:12 @agent_ppo2.py:185][0m |          -0.0057 |         170.0694 |           8.5029 |
[32m[20221213 21:29:12 @agent_ppo2.py:185][0m |           0.0027 |         178.8275 |           8.5070 |
[32m[20221213 21:29:12 @agent_ppo2.py:185][0m |          -0.0036 |         169.7066 |           8.5466 |
[32m[20221213 21:29:12 @agent_ppo2.py:185][0m |          -0.0066 |         169.4706 |           8.4601 |
[32m[20221213 21:29:12 @agent_ppo2.py:185][0m |          -0.0075 |         169.2459 |           8.4821 |
[32m[20221213 21:29:12 @agent_ppo2.py:185][0m |           0.0018 |         183.3366 |           8.4902 |
[32m[20221213 21:29:12 @agent_ppo2.py:185][0m |          -0.0071 |         168.9720 |           8.5144 |
[32m[20221213 21:29:13 @agent_ppo2.py:185][0m |          -0.0063 |         168.9733 |           8.4968 |
[32m[20221213 21:29:13 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:29:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.40
[32m[20221213 21:29:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.00
[32m[20221213 21:29:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:29:13 @agent_ppo2.py:143][0m Total time:      33.63 min
[32m[20221213 21:29:13 @agent_ppo2.py:145][0m 3295232 total steps have happened
[32m[20221213 21:29:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1609 --------------------------#
[32m[20221213 21:29:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:29:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:13 @agent_ppo2.py:185][0m |           0.0005 |         173.7920 |           8.4676 |
[32m[20221213 21:29:13 @agent_ppo2.py:185][0m |          -0.0035 |         171.9236 |           8.5399 |
[32m[20221213 21:29:13 @agent_ppo2.py:185][0m |          -0.0063 |         170.1873 |           8.5594 |
[32m[20221213 21:29:13 @agent_ppo2.py:185][0m |          -0.0068 |         169.2662 |           8.5615 |
[32m[20221213 21:29:13 @agent_ppo2.py:185][0m |          -0.0092 |         168.9399 |           8.5523 |
[32m[20221213 21:29:13 @agent_ppo2.py:185][0m |          -0.0111 |         168.4310 |           8.5589 |
[32m[20221213 21:29:14 @agent_ppo2.py:185][0m |          -0.0091 |         168.5999 |           8.5047 |
[32m[20221213 21:29:14 @agent_ppo2.py:185][0m |          -0.0108 |         167.8974 |           8.5290 |
[32m[20221213 21:29:14 @agent_ppo2.py:185][0m |          -0.0110 |         167.1711 |           8.5561 |
[32m[20221213 21:29:14 @agent_ppo2.py:185][0m |          -0.0112 |         166.8417 |           8.5386 |
[32m[20221213 21:29:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:29:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 709.40
[32m[20221213 21:29:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 784.00
[32m[20221213 21:29:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:29:14 @agent_ppo2.py:143][0m Total time:      33.65 min
[32m[20221213 21:29:14 @agent_ppo2.py:145][0m 3297280 total steps have happened
[32m[20221213 21:29:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1610 --------------------------#
[32m[20221213 21:29:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:14 @agent_ppo2.py:185][0m |          -0.0000 |         173.2780 |           8.2187 |
[32m[20221213 21:29:14 @agent_ppo2.py:185][0m |          -0.0040 |         171.8001 |           8.1947 |
[32m[20221213 21:29:14 @agent_ppo2.py:185][0m |          -0.0061 |         171.0953 |           8.2575 |
[32m[20221213 21:29:15 @agent_ppo2.py:185][0m |          -0.0061 |         170.5613 |           8.2879 |
[32m[20221213 21:29:15 @agent_ppo2.py:185][0m |          -0.0071 |         170.1721 |           8.3468 |
[32m[20221213 21:29:15 @agent_ppo2.py:185][0m |           0.0037 |         176.0396 |           8.3732 |
[32m[20221213 21:29:15 @agent_ppo2.py:185][0m |          -0.0042 |         170.7536 |           8.4036 |
[32m[20221213 21:29:15 @agent_ppo2.py:185][0m |          -0.0057 |         169.2986 |           8.4237 |
[32m[20221213 21:29:15 @agent_ppo2.py:185][0m |          -0.0076 |         168.9958 |           8.4432 |
[32m[20221213 21:29:15 @agent_ppo2.py:185][0m |          -0.0053 |         169.0864 |           8.4887 |
[32m[20221213 21:29:15 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:29:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.80
[32m[20221213 21:29:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.00
[32m[20221213 21:29:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.00
[32m[20221213 21:29:15 @agent_ppo2.py:143][0m Total time:      33.68 min
[32m[20221213 21:29:15 @agent_ppo2.py:145][0m 3299328 total steps have happened
[32m[20221213 21:29:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1611 --------------------------#
[32m[20221213 21:29:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:16 @agent_ppo2.py:185][0m |           0.0047 |         175.2264 |           8.8251 |
[32m[20221213 21:29:16 @agent_ppo2.py:185][0m |          -0.0017 |         171.2526 |           8.8115 |
[32m[20221213 21:29:16 @agent_ppo2.py:185][0m |           0.0033 |         176.8714 |           8.7728 |
[32m[20221213 21:29:16 @agent_ppo2.py:185][0m |          -0.0013 |         173.1546 |           8.7274 |
[32m[20221213 21:29:16 @agent_ppo2.py:185][0m |          -0.0064 |         170.0893 |           8.7112 |
[32m[20221213 21:29:16 @agent_ppo2.py:185][0m |          -0.0052 |         170.0525 |           8.6926 |
[32m[20221213 21:29:16 @agent_ppo2.py:185][0m |           0.0027 |         175.0545 |           8.6306 |
[32m[20221213 21:29:16 @agent_ppo2.py:185][0m |          -0.0058 |         169.6871 |           8.6242 |
[32m[20221213 21:29:16 @agent_ppo2.py:185][0m |          -0.0088 |         169.5394 |           8.5783 |
[32m[20221213 21:29:16 @agent_ppo2.py:185][0m |          -0.0085 |         169.4917 |           8.5602 |
[32m[20221213 21:29:16 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:29:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.60
[32m[20221213 21:29:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:29:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 21:29:17 @agent_ppo2.py:143][0m Total time:      33.70 min
[32m[20221213 21:29:17 @agent_ppo2.py:145][0m 3301376 total steps have happened
[32m[20221213 21:29:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1612 --------------------------#
[32m[20221213 21:29:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:29:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:17 @agent_ppo2.py:185][0m |          -0.0021 |         174.9981 |           7.7920 |
[32m[20221213 21:29:17 @agent_ppo2.py:185][0m |          -0.0007 |         176.5060 |           7.8363 |
[32m[20221213 21:29:17 @agent_ppo2.py:185][0m |           0.0035 |         191.2254 |           7.8678 |
[32m[20221213 21:29:17 @agent_ppo2.py:185][0m |           0.0042 |         193.1066 |           7.9134 |
[32m[20221213 21:29:17 @agent_ppo2.py:185][0m |          -0.0095 |         169.6657 |           7.9783 |
[32m[20221213 21:29:17 @agent_ppo2.py:185][0m |          -0.0078 |         169.2026 |           7.9281 |
[32m[20221213 21:29:17 @agent_ppo2.py:185][0m |          -0.0102 |         168.9962 |           7.9690 |
[32m[20221213 21:29:18 @agent_ppo2.py:185][0m |          -0.0106 |         168.1429 |           8.0199 |
[32m[20221213 21:29:18 @agent_ppo2.py:185][0m |          -0.0005 |         180.1224 |           8.0043 |
[32m[20221213 21:29:18 @agent_ppo2.py:185][0m |          -0.0089 |         167.2619 |           8.0095 |
[32m[20221213 21:29:18 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:29:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 784.20
[32m[20221213 21:29:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:29:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.00
[32m[20221213 21:29:18 @agent_ppo2.py:143][0m Total time:      33.72 min
[32m[20221213 21:29:18 @agent_ppo2.py:145][0m 3303424 total steps have happened
[32m[20221213 21:29:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1613 --------------------------#
[32m[20221213 21:29:18 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:29:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:18 @agent_ppo2.py:185][0m |          -0.0008 |         174.4781 |           8.5727 |
[32m[20221213 21:29:18 @agent_ppo2.py:185][0m |          -0.0049 |         173.0419 |           8.5100 |
[32m[20221213 21:29:18 @agent_ppo2.py:185][0m |          -0.0052 |         172.3948 |           8.5015 |
[32m[20221213 21:29:18 @agent_ppo2.py:185][0m |           0.0018 |         179.4344 |           8.4852 |
[32m[20221213 21:29:19 @agent_ppo2.py:185][0m |          -0.0055 |         171.8329 |           8.5175 |
[32m[20221213 21:29:19 @agent_ppo2.py:185][0m |           0.0028 |         188.7017 |           8.4732 |
[32m[20221213 21:29:19 @agent_ppo2.py:185][0m |          -0.0013 |         171.4115 |           8.5307 |
[32m[20221213 21:29:19 @agent_ppo2.py:185][0m |          -0.0092 |         171.1078 |           8.4595 |
[32m[20221213 21:29:19 @agent_ppo2.py:185][0m |          -0.0071 |         170.8894 |           8.4741 |
[32m[20221213 21:29:19 @agent_ppo2.py:185][0m |          -0.0077 |         170.9054 |           8.4667 |
[32m[20221213 21:29:19 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 21:29:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.80
[32m[20221213 21:29:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:29:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 779.00
[32m[20221213 21:29:19 @agent_ppo2.py:143][0m Total time:      33.74 min
[32m[20221213 21:29:19 @agent_ppo2.py:145][0m 3305472 total steps have happened
[32m[20221213 21:29:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1614 --------------------------#
[32m[20221213 21:29:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:19 @agent_ppo2.py:185][0m |           0.0005 |         173.9938 |           8.5020 |
[32m[20221213 21:29:20 @agent_ppo2.py:185][0m |          -0.0014 |         172.9757 |           8.5479 |
[32m[20221213 21:29:20 @agent_ppo2.py:185][0m |          -0.0057 |         170.2788 |           8.5328 |
[32m[20221213 21:29:20 @agent_ppo2.py:185][0m |          -0.0087 |         169.6962 |           8.5252 |
[32m[20221213 21:29:20 @agent_ppo2.py:185][0m |          -0.0092 |         169.2233 |           8.5493 |
[32m[20221213 21:29:20 @agent_ppo2.py:185][0m |          -0.0096 |         168.5511 |           8.5566 |
[32m[20221213 21:29:20 @agent_ppo2.py:185][0m |          -0.0114 |         168.1810 |           8.5856 |
[32m[20221213 21:29:20 @agent_ppo2.py:185][0m |          -0.0111 |         167.6225 |           8.6066 |
[32m[20221213 21:29:20 @agent_ppo2.py:185][0m |          -0.0114 |         167.2938 |           8.5873 |
[32m[20221213 21:29:20 @agent_ppo2.py:185][0m |           0.0023 |         178.3500 |           8.5845 |
[32m[20221213 21:29:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:29:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.80
[32m[20221213 21:29:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:29:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 21:29:20 @agent_ppo2.py:143][0m Total time:      33.76 min
[32m[20221213 21:29:20 @agent_ppo2.py:145][0m 3307520 total steps have happened
[32m[20221213 21:29:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1615 --------------------------#
[32m[20221213 21:29:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:21 @agent_ppo2.py:185][0m |          -0.0008 |         174.2427 |           8.2351 |
[32m[20221213 21:29:21 @agent_ppo2.py:185][0m |           0.0041 |         177.2381 |           8.2650 |
[32m[20221213 21:29:21 @agent_ppo2.py:185][0m |          -0.0048 |         171.2203 |           8.2286 |
[32m[20221213 21:29:21 @agent_ppo2.py:185][0m |          -0.0061 |         170.2504 |           8.2603 |
[32m[20221213 21:29:21 @agent_ppo2.py:185][0m |           0.0001 |         176.5120 |           8.2479 |
[32m[20221213 21:29:21 @agent_ppo2.py:185][0m |           0.0027 |         183.5174 |           8.2433 |
[32m[20221213 21:29:21 @agent_ppo2.py:185][0m |          -0.0015 |         181.8480 |           8.2294 |
[32m[20221213 21:29:21 @agent_ppo2.py:185][0m |          -0.0080 |         169.0017 |           8.2325 |
[32m[20221213 21:29:21 @agent_ppo2.py:185][0m |          -0.0102 |         168.6000 |           8.2148 |
[32m[20221213 21:29:21 @agent_ppo2.py:185][0m |          -0.0097 |         168.7832 |           8.2220 |
[32m[20221213 21:29:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:29:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.60
[32m[20221213 21:29:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:29:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 862.00
[32m[20221213 21:29:22 @agent_ppo2.py:143][0m Total time:      33.78 min
[32m[20221213 21:29:22 @agent_ppo2.py:145][0m 3309568 total steps have happened
[32m[20221213 21:29:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1616 --------------------------#
[32m[20221213 21:29:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:22 @agent_ppo2.py:185][0m |          -0.0030 |         177.0978 |           8.4091 |
[32m[20221213 21:29:22 @agent_ppo2.py:185][0m |          -0.0052 |         175.9117 |           8.5168 |
[32m[20221213 21:29:22 @agent_ppo2.py:185][0m |          -0.0055 |         175.0614 |           8.4511 |
[32m[20221213 21:29:22 @agent_ppo2.py:185][0m |          -0.0056 |         174.4683 |           8.4549 |
[32m[20221213 21:29:22 @agent_ppo2.py:185][0m |          -0.0066 |         173.8311 |           8.4493 |
[32m[20221213 21:29:22 @agent_ppo2.py:185][0m |          -0.0011 |         176.2695 |           8.4688 |
[32m[20221213 21:29:22 @agent_ppo2.py:185][0m |          -0.0083 |         173.1992 |           8.4294 |
[32m[20221213 21:29:23 @agent_ppo2.py:185][0m |          -0.0095 |         172.9176 |           8.4837 |
[32m[20221213 21:29:23 @agent_ppo2.py:185][0m |          -0.0083 |         172.4890 |           8.5089 |
[32m[20221213 21:29:23 @agent_ppo2.py:185][0m |          -0.0013 |         182.6491 |           8.4632 |
[32m[20221213 21:29:23 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:29:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.20
[32m[20221213 21:29:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:29:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.00
[32m[20221213 21:29:23 @agent_ppo2.py:143][0m Total time:      33.80 min
[32m[20221213 21:29:23 @agent_ppo2.py:145][0m 3311616 total steps have happened
[32m[20221213 21:29:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1617 --------------------------#
[32m[20221213 21:29:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:23 @agent_ppo2.py:185][0m |          -0.0005 |         172.8912 |           8.2405 |
[32m[20221213 21:29:23 @agent_ppo2.py:185][0m |          -0.0020 |         171.6765 |           8.2836 |
[32m[20221213 21:29:23 @agent_ppo2.py:185][0m |          -0.0041 |         171.2864 |           8.3307 |
[32m[20221213 21:29:23 @agent_ppo2.py:185][0m |          -0.0014 |         173.1727 |           8.3863 |
[32m[20221213 21:29:23 @agent_ppo2.py:185][0m |          -0.0049 |         170.6293 |           8.4256 |
[32m[20221213 21:29:24 @agent_ppo2.py:185][0m |           0.0031 |         180.3503 |           8.4718 |
[32m[20221213 21:29:24 @agent_ppo2.py:185][0m |          -0.0045 |         170.4074 |           8.6214 |
[32m[20221213 21:29:24 @agent_ppo2.py:185][0m |          -0.0020 |         172.2357 |           8.5727 |
[32m[20221213 21:29:24 @agent_ppo2.py:185][0m |          -0.0064 |         170.0465 |           8.5644 |
[32m[20221213 21:29:24 @agent_ppo2.py:185][0m |          -0.0064 |         169.9501 |           8.6003 |
[32m[20221213 21:29:24 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:29:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.40
[32m[20221213 21:29:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.00
[32m[20221213 21:29:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 672.00
[32m[20221213 21:29:24 @agent_ppo2.py:143][0m Total time:      33.82 min
[32m[20221213 21:29:24 @agent_ppo2.py:145][0m 3313664 total steps have happened
[32m[20221213 21:29:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1618 --------------------------#
[32m[20221213 21:29:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:24 @agent_ppo2.py:185][0m |          -0.0034 |         176.4682 |           8.5237 |
[32m[20221213 21:29:25 @agent_ppo2.py:185][0m |          -0.0050 |         175.0921 |           8.5158 |
[32m[20221213 21:29:25 @agent_ppo2.py:185][0m |          -0.0011 |         177.8718 |           8.5384 |
[32m[20221213 21:29:25 @agent_ppo2.py:185][0m |          -0.0079 |         173.1786 |           8.5395 |
[32m[20221213 21:29:25 @agent_ppo2.py:185][0m |          -0.0069 |         172.5402 |           8.5374 |
[32m[20221213 21:29:25 @agent_ppo2.py:185][0m |          -0.0091 |         172.1376 |           8.5998 |
[32m[20221213 21:29:25 @agent_ppo2.py:185][0m |          -0.0100 |         171.8098 |           8.5225 |
[32m[20221213 21:29:25 @agent_ppo2.py:185][0m |          -0.0100 |         171.2560 |           8.5611 |
[32m[20221213 21:29:25 @agent_ppo2.py:185][0m |          -0.0090 |         170.8252 |           8.5443 |
[32m[20221213 21:29:25 @agent_ppo2.py:185][0m |          -0.0096 |         170.5284 |           8.5602 |
[32m[20221213 21:29:25 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 21:29:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.60
[32m[20221213 21:29:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.00
[32m[20221213 21:29:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 862.00
[32m[20221213 21:29:25 @agent_ppo2.py:143][0m Total time:      33.85 min
[32m[20221213 21:29:25 @agent_ppo2.py:145][0m 3315712 total steps have happened
[32m[20221213 21:29:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1619 --------------------------#
[32m[20221213 21:29:26 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:29:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:26 @agent_ppo2.py:185][0m |           0.0117 |         196.8770 |           8.6769 |
[32m[20221213 21:29:26 @agent_ppo2.py:185][0m |          -0.0044 |         175.2191 |           8.7610 |
[32m[20221213 21:29:26 @agent_ppo2.py:185][0m |          -0.0048 |         173.9638 |           8.7983 |
[32m[20221213 21:29:26 @agent_ppo2.py:185][0m |          -0.0067 |         173.0028 |           8.8112 |
[32m[20221213 21:29:26 @agent_ppo2.py:185][0m |          -0.0067 |         172.1118 |           8.8818 |
[32m[20221213 21:29:26 @agent_ppo2.py:185][0m |          -0.0076 |         171.3787 |           8.8903 |
[32m[20221213 21:29:26 @agent_ppo2.py:185][0m |          -0.0022 |         173.8067 |           8.9247 |
[32m[20221213 21:29:26 @agent_ppo2.py:185][0m |          -0.0057 |         169.9823 |           9.0124 |
[32m[20221213 21:29:26 @agent_ppo2.py:185][0m |           0.0005 |         179.6870 |           8.9873 |
[32m[20221213 21:29:27 @agent_ppo2.py:185][0m |          -0.0079 |         168.9394 |           9.0438 |
[32m[20221213 21:29:27 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:29:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.00
[32m[20221213 21:29:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:29:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.00
[32m[20221213 21:29:27 @agent_ppo2.py:143][0m Total time:      33.87 min
[32m[20221213 21:29:27 @agent_ppo2.py:145][0m 3317760 total steps have happened
[32m[20221213 21:29:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1620 --------------------------#
[32m[20221213 21:29:27 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:29:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:27 @agent_ppo2.py:185][0m |           0.0007 |         177.0804 |           9.2876 |
[32m[20221213 21:29:27 @agent_ppo2.py:185][0m |          -0.0032 |         176.1438 |           9.3341 |
[32m[20221213 21:29:27 @agent_ppo2.py:185][0m |          -0.0049 |         175.5701 |           9.2934 |
[32m[20221213 21:29:27 @agent_ppo2.py:185][0m |          -0.0078 |         175.1858 |           9.2949 |
[32m[20221213 21:29:27 @agent_ppo2.py:185][0m |          -0.0061 |         174.8557 |           9.2860 |
[32m[20221213 21:29:27 @agent_ppo2.py:185][0m |           0.0087 |         188.0088 |           9.2744 |
[32m[20221213 21:29:28 @agent_ppo2.py:185][0m |          -0.0058 |         174.6038 |           9.2759 |
[32m[20221213 21:29:28 @agent_ppo2.py:185][0m |          -0.0062 |         174.2335 |           9.2464 |
[32m[20221213 21:29:28 @agent_ppo2.py:185][0m |          -0.0084 |         174.0208 |           9.2573 |
[32m[20221213 21:29:28 @agent_ppo2.py:185][0m |          -0.0078 |         173.7969 |           9.2572 |
[32m[20221213 21:29:28 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:29:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.00
[32m[20221213 21:29:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221213 21:29:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.00
[32m[20221213 21:29:28 @agent_ppo2.py:143][0m Total time:      33.89 min
[32m[20221213 21:29:28 @agent_ppo2.py:145][0m 3319808 total steps have happened
[32m[20221213 21:29:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1621 --------------------------#
[32m[20221213 21:29:28 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:29:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:28 @agent_ppo2.py:185][0m |          -0.0029 |         171.8699 |           8.8365 |
[32m[20221213 21:29:28 @agent_ppo2.py:185][0m |          -0.0052 |         169.4330 |           8.8527 |
[32m[20221213 21:29:28 @agent_ppo2.py:185][0m |          -0.0067 |         167.2505 |           8.8957 |
[32m[20221213 21:29:28 @agent_ppo2.py:185][0m |          -0.0061 |         167.3131 |           8.8584 |
[32m[20221213 21:29:29 @agent_ppo2.py:185][0m |          -0.0084 |         164.2775 |           8.8585 |
[32m[20221213 21:29:29 @agent_ppo2.py:185][0m |          -0.0035 |         166.1134 |           8.8756 |
[32m[20221213 21:29:29 @agent_ppo2.py:185][0m |          -0.0096 |         160.9472 |           8.8881 |
[32m[20221213 21:29:29 @agent_ppo2.py:185][0m |          -0.0095 |         158.4654 |           8.8881 |
[32m[20221213 21:29:29 @agent_ppo2.py:185][0m |          -0.0115 |         156.4213 |           8.8750 |
[32m[20221213 21:29:29 @agent_ppo2.py:185][0m |          -0.0117 |         155.1859 |           8.8947 |
[32m[20221213 21:29:29 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:29:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.80
[32m[20221213 21:29:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:29:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.00
[32m[20221213 21:29:29 @agent_ppo2.py:143][0m Total time:      33.91 min
[32m[20221213 21:29:29 @agent_ppo2.py:145][0m 3321856 total steps have happened
[32m[20221213 21:29:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1622 --------------------------#
[32m[20221213 21:29:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:29:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:29 @agent_ppo2.py:185][0m |           0.0001 |         183.0120 |           9.0563 |
[32m[20221213 21:29:30 @agent_ppo2.py:185][0m |          -0.0058 |         179.2216 |           9.0309 |
[32m[20221213 21:29:30 @agent_ppo2.py:185][0m |           0.0050 |         195.9546 |           9.0419 |
[32m[20221213 21:29:30 @agent_ppo2.py:185][0m |          -0.0054 |         177.4895 |           9.1359 |
[32m[20221213 21:29:30 @agent_ppo2.py:185][0m |          -0.0012 |         181.0673 |           9.0404 |
[32m[20221213 21:29:30 @agent_ppo2.py:185][0m |          -0.0080 |         176.6789 |           9.0326 |
[32m[20221213 21:29:30 @agent_ppo2.py:185][0m |          -0.0090 |         176.3111 |           9.0565 |
[32m[20221213 21:29:30 @agent_ppo2.py:185][0m |          -0.0118 |         176.2006 |           9.0422 |
[32m[20221213 21:29:30 @agent_ppo2.py:185][0m |          -0.0081 |         177.1928 |           9.0181 |
[32m[20221213 21:29:30 @agent_ppo2.py:185][0m |          -0.0103 |         175.9946 |           9.0253 |
[32m[20221213 21:29:30 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:29:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.60
[32m[20221213 21:29:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.00
[32m[20221213 21:29:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.00
[32m[20221213 21:29:30 @agent_ppo2.py:143][0m Total time:      33.93 min
[32m[20221213 21:29:30 @agent_ppo2.py:145][0m 3323904 total steps have happened
[32m[20221213 21:29:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1623 --------------------------#
[32m[20221213 21:29:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:31 @agent_ppo2.py:185][0m |          -0.0005 |         177.6142 |           9.1414 |
[32m[20221213 21:29:31 @agent_ppo2.py:185][0m |          -0.0057 |         176.1162 |           9.2075 |
[32m[20221213 21:29:31 @agent_ppo2.py:185][0m |           0.0079 |         199.9514 |           9.1876 |
[32m[20221213 21:29:31 @agent_ppo2.py:185][0m |          -0.0012 |         177.7221 |           9.1895 |
[32m[20221213 21:29:31 @agent_ppo2.py:185][0m |          -0.0096 |         173.1285 |           9.1779 |
[32m[20221213 21:29:31 @agent_ppo2.py:185][0m |           0.0006 |         188.6135 |           9.1775 |
[32m[20221213 21:29:31 @agent_ppo2.py:185][0m |          -0.0067 |         172.7455 |           9.2496 |
[32m[20221213 21:29:31 @agent_ppo2.py:185][0m |          -0.0109 |         171.6581 |           9.2388 |
[32m[20221213 21:29:31 @agent_ppo2.py:185][0m |          -0.0070 |         172.8068 |           9.2380 |
[32m[20221213 21:29:31 @agent_ppo2.py:185][0m |          -0.0141 |         170.8180 |           9.2365 |
[32m[20221213 21:29:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:29:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.80
[32m[20221213 21:29:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:29:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.00
[32m[20221213 21:29:32 @agent_ppo2.py:143][0m Total time:      33.95 min
[32m[20221213 21:29:32 @agent_ppo2.py:145][0m 3325952 total steps have happened
[32m[20221213 21:29:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1624 --------------------------#
[32m[20221213 21:29:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:32 @agent_ppo2.py:185][0m |          -0.0016 |         173.7350 |           8.6176 |
[32m[20221213 21:29:32 @agent_ppo2.py:185][0m |          -0.0034 |         172.9669 |           8.5768 |
[32m[20221213 21:29:32 @agent_ppo2.py:185][0m |          -0.0055 |         171.8935 |           8.5917 |
[32m[20221213 21:29:32 @agent_ppo2.py:185][0m |          -0.0078 |         171.3536 |           8.5502 |
[32m[20221213 21:29:32 @agent_ppo2.py:185][0m |          -0.0074 |         170.7242 |           8.5511 |
[32m[20221213 21:29:32 @agent_ppo2.py:185][0m |          -0.0075 |         170.2021 |           8.5096 |
[32m[20221213 21:29:32 @agent_ppo2.py:185][0m |          -0.0008 |         178.8738 |           8.5125 |
[32m[20221213 21:29:32 @agent_ppo2.py:185][0m |          -0.0074 |         169.8954 |           8.4821 |
[32m[20221213 21:29:33 @agent_ppo2.py:185][0m |          -0.0094 |         169.4160 |           8.4947 |
[32m[20221213 21:29:33 @agent_ppo2.py:185][0m |          -0.0100 |         169.1302 |           8.4272 |
[32m[20221213 21:29:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:29:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.40
[32m[20221213 21:29:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:29:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 865.00
[32m[20221213 21:29:33 @agent_ppo2.py:143][0m Total time:      33.97 min
[32m[20221213 21:29:33 @agent_ppo2.py:145][0m 3328000 total steps have happened
[32m[20221213 21:29:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1625 --------------------------#
[32m[20221213 21:29:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:29:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:33 @agent_ppo2.py:185][0m |           0.0006 |         175.8790 |           8.6829 |
[32m[20221213 21:29:33 @agent_ppo2.py:185][0m |          -0.0019 |         176.4297 |           8.7070 |
[32m[20221213 21:29:33 @agent_ppo2.py:185][0m |           0.0052 |         180.9356 |           8.6710 |
[32m[20221213 21:29:33 @agent_ppo2.py:185][0m |          -0.0059 |         173.8349 |           8.7060 |
[32m[20221213 21:29:33 @agent_ppo2.py:185][0m |          -0.0004 |         180.5559 |           8.6694 |
[32m[20221213 21:29:33 @agent_ppo2.py:185][0m |          -0.0038 |         174.2811 |           8.6653 |
[32m[20221213 21:29:34 @agent_ppo2.py:185][0m |          -0.0082 |         172.9342 |           8.6714 |
[32m[20221213 21:29:34 @agent_ppo2.py:185][0m |          -0.0084 |         172.9690 |           8.6202 |
[32m[20221213 21:29:34 @agent_ppo2.py:185][0m |          -0.0089 |         172.7305 |           8.6493 |
[32m[20221213 21:29:34 @agent_ppo2.py:185][0m |          -0.0046 |         178.8214 |           8.5859 |
[32m[20221213 21:29:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:29:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.20
[32m[20221213 21:29:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:29:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.00
[32m[20221213 21:29:34 @agent_ppo2.py:143][0m Total time:      33.99 min
[32m[20221213 21:29:34 @agent_ppo2.py:145][0m 3330048 total steps have happened
[32m[20221213 21:29:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1626 --------------------------#
[32m[20221213 21:29:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:34 @agent_ppo2.py:185][0m |          -0.0014 |         177.8610 |           8.5415 |
[32m[20221213 21:29:34 @agent_ppo2.py:185][0m |          -0.0032 |         176.6385 |           8.6146 |
[32m[20221213 21:29:34 @agent_ppo2.py:185][0m |           0.0005 |         178.7478 |           8.5475 |
[32m[20221213 21:29:35 @agent_ppo2.py:185][0m |           0.0015 |         178.3714 |           8.5177 |
[32m[20221213 21:29:35 @agent_ppo2.py:185][0m |           0.0031 |         184.4017 |           8.5767 |
[32m[20221213 21:29:35 @agent_ppo2.py:185][0m |          -0.0072 |         174.8375 |           8.4733 |
[32m[20221213 21:29:35 @agent_ppo2.py:185][0m |          -0.0057 |         175.5310 |           8.5148 |
[32m[20221213 21:29:35 @agent_ppo2.py:185][0m |          -0.0080 |         174.4242 |           8.5178 |
[32m[20221213 21:29:35 @agent_ppo2.py:185][0m |          -0.0088 |         174.0882 |           8.4890 |
[32m[20221213 21:29:35 @agent_ppo2.py:185][0m |          -0.0103 |         174.0329 |           8.4970 |
[32m[20221213 21:29:35 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:29:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.40
[32m[20221213 21:29:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:29:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.00
[32m[20221213 21:29:35 @agent_ppo2.py:143][0m Total time:      34.01 min
[32m[20221213 21:29:35 @agent_ppo2.py:145][0m 3332096 total steps have happened
[32m[20221213 21:29:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1627 --------------------------#
[32m[20221213 21:29:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:36 @agent_ppo2.py:185][0m |          -0.0014 |         174.5446 |           8.6638 |
[32m[20221213 21:29:36 @agent_ppo2.py:185][0m |          -0.0024 |         173.8711 |           8.6278 |
[32m[20221213 21:29:36 @agent_ppo2.py:185][0m |          -0.0035 |         173.4509 |           8.6057 |
[32m[20221213 21:29:36 @agent_ppo2.py:185][0m |           0.0054 |         188.4842 |           8.6377 |
[32m[20221213 21:29:36 @agent_ppo2.py:185][0m |          -0.0025 |         174.8682 |           8.6192 |
[32m[20221213 21:29:36 @agent_ppo2.py:185][0m |          -0.0043 |         172.8030 |           8.5869 |
[32m[20221213 21:29:36 @agent_ppo2.py:185][0m |          -0.0088 |         172.5487 |           8.6078 |
[32m[20221213 21:29:36 @agent_ppo2.py:185][0m |          -0.0081 |         172.4117 |           8.6512 |
[32m[20221213 21:29:36 @agent_ppo2.py:185][0m |          -0.0082 |         172.2811 |           8.6083 |
[32m[20221213 21:29:37 @agent_ppo2.py:185][0m |          -0.0093 |         172.0535 |           8.5837 |
[32m[20221213 21:29:37 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 21:29:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.80
[32m[20221213 21:29:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:29:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.00
[32m[20221213 21:29:37 @agent_ppo2.py:143][0m Total time:      34.03 min
[32m[20221213 21:29:37 @agent_ppo2.py:145][0m 3334144 total steps have happened
[32m[20221213 21:29:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1628 --------------------------#
[32m[20221213 21:29:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:29:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:37 @agent_ppo2.py:185][0m |          -0.0001 |         171.7357 |           8.3099 |
[32m[20221213 21:29:37 @agent_ppo2.py:185][0m |           0.0011 |         172.4953 |           8.3161 |
[32m[20221213 21:29:37 @agent_ppo2.py:185][0m |          -0.0043 |         170.7711 |           8.3140 |
[32m[20221213 21:29:37 @agent_ppo2.py:185][0m |          -0.0059 |         170.4241 |           8.3108 |
[32m[20221213 21:29:38 @agent_ppo2.py:185][0m |          -0.0053 |         170.1116 |           8.2809 |
[32m[20221213 21:29:38 @agent_ppo2.py:185][0m |          -0.0072 |         169.7830 |           8.2838 |
[32m[20221213 21:29:38 @agent_ppo2.py:185][0m |          -0.0040 |         169.9498 |           8.3243 |
[32m[20221213 21:29:38 @agent_ppo2.py:185][0m |          -0.0078 |         169.4247 |           8.3005 |
[32m[20221213 21:29:38 @agent_ppo2.py:185][0m |          -0.0071 |         169.4928 |           8.2863 |
[32m[20221213 21:29:38 @agent_ppo2.py:185][0m |          -0.0050 |         169.3315 |           8.3003 |
[32m[20221213 21:29:38 @agent_ppo2.py:130][0m Policy update time: 1.43 s
[32m[20221213 21:29:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.60
[32m[20221213 21:29:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:29:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.00
[32m[20221213 21:29:38 @agent_ppo2.py:143][0m Total time:      34.06 min
[32m[20221213 21:29:38 @agent_ppo2.py:145][0m 3336192 total steps have happened
[32m[20221213 21:29:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1629 --------------------------#
[32m[20221213 21:29:39 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:29:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:39 @agent_ppo2.py:185][0m |          -0.0010 |         172.5644 |           7.9999 |
[32m[20221213 21:29:39 @agent_ppo2.py:185][0m |          -0.0044 |         171.6900 |           7.9803 |
[32m[20221213 21:29:39 @agent_ppo2.py:185][0m |          -0.0060 |         171.3440 |           7.9724 |
[32m[20221213 21:29:39 @agent_ppo2.py:185][0m |          -0.0078 |         171.0022 |           7.9799 |
[32m[20221213 21:29:39 @agent_ppo2.py:185][0m |          -0.0074 |         170.6092 |           7.9494 |
[32m[20221213 21:29:39 @agent_ppo2.py:185][0m |          -0.0078 |         170.3706 |           7.9433 |
[32m[20221213 21:29:39 @agent_ppo2.py:185][0m |          -0.0098 |         170.0990 |           7.9545 |
[32m[20221213 21:29:40 @agent_ppo2.py:185][0m |          -0.0085 |         169.8578 |           7.9377 |
[32m[20221213 21:29:40 @agent_ppo2.py:185][0m |          -0.0109 |         169.5906 |           7.9385 |
[32m[20221213 21:29:40 @agent_ppo2.py:185][0m |          -0.0101 |         169.3092 |           7.8915 |
[32m[20221213 21:29:40 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:29:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.60
[32m[20221213 21:29:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.00
[32m[20221213 21:29:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 789.00
[32m[20221213 21:29:40 @agent_ppo2.py:143][0m Total time:      34.09 min
[32m[20221213 21:29:40 @agent_ppo2.py:145][0m 3338240 total steps have happened
[32m[20221213 21:29:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1630 --------------------------#
[32m[20221213 21:29:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:40 @agent_ppo2.py:185][0m |           0.0062 |         181.4612 |           8.1114 |
[32m[20221213 21:29:40 @agent_ppo2.py:185][0m |          -0.0010 |         176.5833 |           8.0987 |
[32m[20221213 21:29:40 @agent_ppo2.py:185][0m |           0.0058 |         195.9063 |           8.0297 |
[32m[20221213 21:29:41 @agent_ppo2.py:185][0m |          -0.0064 |         175.5806 |           8.0729 |
[32m[20221213 21:29:41 @agent_ppo2.py:185][0m |           0.0055 |         198.0494 |           8.0291 |
[32m[20221213 21:29:41 @agent_ppo2.py:185][0m |          -0.0045 |         174.9050 |           8.0187 |
[32m[20221213 21:29:41 @agent_ppo2.py:185][0m |          -0.0077 |         174.5003 |           8.0023 |
[32m[20221213 21:29:41 @agent_ppo2.py:185][0m |           0.0107 |         197.2755 |           7.9682 |
[32m[20221213 21:29:41 @agent_ppo2.py:185][0m |          -0.0103 |         174.2968 |           7.9880 |
[32m[20221213 21:29:41 @agent_ppo2.py:185][0m |          -0.0087 |         174.0341 |           7.9900 |
[32m[20221213 21:29:41 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:29:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.60
[32m[20221213 21:29:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:29:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.00
[32m[20221213 21:29:41 @agent_ppo2.py:143][0m Total time:      34.11 min
[32m[20221213 21:29:41 @agent_ppo2.py:145][0m 3340288 total steps have happened
[32m[20221213 21:29:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1631 --------------------------#
[32m[20221213 21:29:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:42 @agent_ppo2.py:185][0m |          -0.0020 |         176.3598 |           7.7410 |
[32m[20221213 21:29:42 @agent_ppo2.py:185][0m |          -0.0039 |         175.7119 |           7.8427 |
[32m[20221213 21:29:42 @agent_ppo2.py:185][0m |          -0.0043 |         175.4638 |           7.7871 |
[32m[20221213 21:29:42 @agent_ppo2.py:185][0m |          -0.0050 |         175.1395 |           7.8656 |
[32m[20221213 21:29:42 @agent_ppo2.py:185][0m |           0.0040 |         190.5053 |           7.8360 |
[32m[20221213 21:29:42 @agent_ppo2.py:185][0m |          -0.0067 |         175.0052 |           7.8880 |
[32m[20221213 21:29:42 @agent_ppo2.py:185][0m |          -0.0044 |         175.6437 |           7.9652 |
[32m[20221213 21:29:42 @agent_ppo2.py:185][0m |          -0.0073 |         174.5945 |           7.9197 |
[32m[20221213 21:29:43 @agent_ppo2.py:185][0m |          -0.0027 |         177.0616 |           8.0060 |
[32m[20221213 21:29:43 @agent_ppo2.py:185][0m |          -0.0042 |         174.5117 |           7.9967 |
[32m[20221213 21:29:43 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 21:29:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.20
[32m[20221213 21:29:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:29:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.00
[32m[20221213 21:29:43 @agent_ppo2.py:143][0m Total time:      34.13 min
[32m[20221213 21:29:43 @agent_ppo2.py:145][0m 3342336 total steps have happened
[32m[20221213 21:29:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1632 --------------------------#
[32m[20221213 21:29:43 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:29:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:43 @agent_ppo2.py:185][0m |           0.0006 |         175.7784 |           7.7993 |
[32m[20221213 21:29:43 @agent_ppo2.py:185][0m |          -0.0029 |         174.8689 |           7.8035 |
[32m[20221213 21:29:43 @agent_ppo2.py:185][0m |           0.0026 |         182.3863 |           7.8881 |
[32m[20221213 21:29:43 @agent_ppo2.py:185][0m |          -0.0057 |         173.7810 |           7.8423 |
[32m[20221213 21:29:44 @agent_ppo2.py:185][0m |          -0.0060 |         173.3222 |           7.8746 |
[32m[20221213 21:29:44 @agent_ppo2.py:185][0m |          -0.0088 |         173.0383 |           7.9023 |
[32m[20221213 21:29:44 @agent_ppo2.py:185][0m |          -0.0032 |         173.4173 |           7.9221 |
[32m[20221213 21:29:44 @agent_ppo2.py:185][0m |          -0.0074 |         172.5437 |           7.8770 |
[32m[20221213 21:29:44 @agent_ppo2.py:185][0m |          -0.0037 |         176.6002 |           7.9072 |
[32m[20221213 21:29:44 @agent_ppo2.py:185][0m |          -0.0038 |         173.4363 |           7.9141 |
[32m[20221213 21:29:44 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:29:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.60
[32m[20221213 21:29:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.00
[32m[20221213 21:29:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.00
[32m[20221213 21:29:44 @agent_ppo2.py:143][0m Total time:      34.16 min
[32m[20221213 21:29:44 @agent_ppo2.py:145][0m 3344384 total steps have happened
[32m[20221213 21:29:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1633 --------------------------#
[32m[20221213 21:29:44 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:29:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:45 @agent_ppo2.py:185][0m |          -0.0022 |         175.6724 |           8.2353 |
[32m[20221213 21:29:45 @agent_ppo2.py:185][0m |          -0.0060 |         174.8629 |           8.2287 |
[32m[20221213 21:29:45 @agent_ppo2.py:185][0m |          -0.0057 |         174.4969 |           8.2772 |
[32m[20221213 21:29:45 @agent_ppo2.py:185][0m |          -0.0073 |         174.1598 |           8.2007 |
[32m[20221213 21:29:45 @agent_ppo2.py:185][0m |          -0.0071 |         173.9991 |           8.2259 |
[32m[20221213 21:29:45 @agent_ppo2.py:185][0m |          -0.0071 |         173.8309 |           8.2261 |
[32m[20221213 21:29:45 @agent_ppo2.py:185][0m |          -0.0083 |         173.5532 |           8.2000 |
[32m[20221213 21:29:45 @agent_ppo2.py:185][0m |          -0.0076 |         173.8044 |           8.2095 |
[32m[20221213 21:29:45 @agent_ppo2.py:185][0m |          -0.0069 |         173.8430 |           8.1975 |
[32m[20221213 21:29:46 @agent_ppo2.py:185][0m |          -0.0092 |         173.0569 |           8.2124 |
[32m[20221213 21:29:46 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 21:29:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.80
[32m[20221213 21:29:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:29:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.00
[32m[20221213 21:29:46 @agent_ppo2.py:143][0m Total time:      34.18 min
[32m[20221213 21:29:46 @agent_ppo2.py:145][0m 3346432 total steps have happened
[32m[20221213 21:29:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1634 --------------------------#
[32m[20221213 21:29:46 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:29:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:46 @agent_ppo2.py:185][0m |           0.0009 |         172.6691 |           8.2157 |
[32m[20221213 21:29:46 @agent_ppo2.py:185][0m |           0.0062 |         184.2195 |           8.2400 |
[32m[20221213 21:29:46 @agent_ppo2.py:185][0m |           0.0023 |         172.4080 |           8.3043 |
[32m[20221213 21:29:46 @agent_ppo2.py:185][0m |          -0.0025 |         170.8779 |           8.2866 |
[32m[20221213 21:29:46 @agent_ppo2.py:185][0m |          -0.0014 |         171.3273 |           8.2782 |
[32m[20221213 21:29:47 @agent_ppo2.py:185][0m |          -0.0069 |         170.4096 |           8.3855 |
[32m[20221213 21:29:47 @agent_ppo2.py:185][0m |          -0.0061 |         170.3399 |           8.2805 |
[32m[20221213 21:29:47 @agent_ppo2.py:185][0m |          -0.0052 |         170.4758 |           8.3646 |
[32m[20221213 21:29:47 @agent_ppo2.py:185][0m |          -0.0084 |         170.0714 |           8.3364 |
[32m[20221213 21:29:47 @agent_ppo2.py:185][0m |          -0.0042 |         170.6020 |           8.3926 |
[32m[20221213 21:29:47 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 21:29:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.20
[32m[20221213 21:29:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.00
[32m[20221213 21:29:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.00
[32m[20221213 21:29:47 @agent_ppo2.py:143][0m Total time:      34.21 min
[32m[20221213 21:29:47 @agent_ppo2.py:145][0m 3348480 total steps have happened
[32m[20221213 21:29:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1635 --------------------------#
[32m[20221213 21:29:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:29:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:48 @agent_ppo2.py:185][0m |          -0.0006 |         172.3834 |           8.0802 |
[32m[20221213 21:29:48 @agent_ppo2.py:185][0m |          -0.0027 |         172.0132 |           8.0222 |
[32m[20221213 21:29:48 @agent_ppo2.py:185][0m |          -0.0037 |         171.7962 |           8.0072 |
[32m[20221213 21:29:48 @agent_ppo2.py:185][0m |          -0.0034 |         171.6941 |           7.9690 |
[32m[20221213 21:29:48 @agent_ppo2.py:185][0m |          -0.0049 |         172.1777 |           7.9189 |
[32m[20221213 21:29:48 @agent_ppo2.py:185][0m |          -0.0022 |         173.6013 |           7.9012 |
[32m[20221213 21:29:48 @agent_ppo2.py:185][0m |          -0.0068 |         171.2390 |           7.9195 |
[32m[20221213 21:29:48 @agent_ppo2.py:185][0m |          -0.0070 |         171.2337 |           7.9047 |
[32m[20221213 21:29:48 @agent_ppo2.py:185][0m |          -0.0004 |         180.0235 |           7.8951 |
[32m[20221213 21:29:49 @agent_ppo2.py:185][0m |          -0.0089 |         171.0579 |           7.9244 |
[32m[20221213 21:29:49 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221213 21:29:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.60
[32m[20221213 21:29:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.00
[32m[20221213 21:29:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.00
[32m[20221213 21:29:49 @agent_ppo2.py:143][0m Total time:      34.23 min
[32m[20221213 21:29:49 @agent_ppo2.py:145][0m 3350528 total steps have happened
[32m[20221213 21:29:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1636 --------------------------#
[32m[20221213 21:29:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:29:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:49 @agent_ppo2.py:185][0m |           0.0146 |         198.8282 |           8.2912 |
[32m[20221213 21:29:49 @agent_ppo2.py:185][0m |           0.0087 |         198.0263 |           8.2646 |
[32m[20221213 21:29:49 @agent_ppo2.py:185][0m |          -0.0061 |         172.9385 |           8.2838 |
[32m[20221213 21:29:49 @agent_ppo2.py:185][0m |          -0.0077 |         172.5440 |           8.2308 |
[32m[20221213 21:29:49 @agent_ppo2.py:185][0m |           0.0044 |         194.9013 |           8.2590 |
[32m[20221213 21:29:50 @agent_ppo2.py:185][0m |           0.0005 |         183.3751 |           8.2161 |
[32m[20221213 21:29:50 @agent_ppo2.py:185][0m |          -0.0072 |         172.1514 |           8.2936 |
[32m[20221213 21:29:50 @agent_ppo2.py:185][0m |          -0.0081 |         172.1178 |           8.1940 |
[32m[20221213 21:29:50 @agent_ppo2.py:185][0m |          -0.0091 |         171.8942 |           8.2276 |
[32m[20221213 21:29:50 @agent_ppo2.py:185][0m |          -0.0092 |         171.8790 |           8.2191 |
[32m[20221213 21:29:50 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:29:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.60
[32m[20221213 21:29:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:29:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.00
[32m[20221213 21:29:50 @agent_ppo2.py:143][0m Total time:      34.26 min
[32m[20221213 21:29:50 @agent_ppo2.py:145][0m 3352576 total steps have happened
[32m[20221213 21:29:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1637 --------------------------#
[32m[20221213 21:29:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:29:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:51 @agent_ppo2.py:185][0m |          -0.0007 |         175.5410 |           7.5223 |
[32m[20221213 21:29:51 @agent_ppo2.py:185][0m |           0.0091 |         183.0109 |           7.4679 |
[32m[20221213 21:29:51 @agent_ppo2.py:185][0m |          -0.0002 |         174.3371 |           7.6267 |
[32m[20221213 21:29:51 @agent_ppo2.py:185][0m |          -0.0000 |         177.2749 |           7.4898 |
[32m[20221213 21:29:51 @agent_ppo2.py:185][0m |          -0.0067 |         173.0826 |           7.5314 |
[32m[20221213 21:29:51 @agent_ppo2.py:185][0m |           0.0010 |         182.8679 |           7.5258 |
[32m[20221213 21:29:51 @agent_ppo2.py:185][0m |          -0.0066 |         172.6399 |           7.4998 |
[32m[20221213 21:29:51 @agent_ppo2.py:185][0m |          -0.0024 |         175.3170 |           7.5393 |
[32m[20221213 21:29:51 @agent_ppo2.py:185][0m |          -0.0052 |         172.3868 |           7.5246 |
[32m[20221213 21:29:52 @agent_ppo2.py:185][0m |          -0.0071 |         172.0768 |           7.4635 |
[32m[20221213 21:29:52 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221213 21:29:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.00
[32m[20221213 21:29:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:29:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.00
[32m[20221213 21:29:52 @agent_ppo2.py:143][0m Total time:      34.28 min
[32m[20221213 21:29:52 @agent_ppo2.py:145][0m 3354624 total steps have happened
[32m[20221213 21:29:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1638 --------------------------#
[32m[20221213 21:29:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 21:29:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:52 @agent_ppo2.py:185][0m |           0.0002 |         175.2265 |           8.0764 |
[32m[20221213 21:29:52 @agent_ppo2.py:185][0m |           0.0008 |         176.3003 |           8.0276 |
[32m[20221213 21:29:52 @agent_ppo2.py:185][0m |           0.0005 |         176.3511 |           7.9906 |
[32m[20221213 21:29:52 @agent_ppo2.py:185][0m |          -0.0063 |         172.9994 |           7.9940 |
[32m[20221213 21:29:52 @agent_ppo2.py:185][0m |          -0.0066 |         172.4201 |           7.9541 |
[32m[20221213 21:29:53 @agent_ppo2.py:185][0m |           0.0011 |         176.1063 |           7.8997 |
[32m[20221213 21:29:53 @agent_ppo2.py:185][0m |          -0.0043 |         172.6412 |           7.9197 |
[32m[20221213 21:29:53 @agent_ppo2.py:185][0m |          -0.0051 |         171.5996 |           7.9161 |
[32m[20221213 21:29:53 @agent_ppo2.py:185][0m |          -0.0088 |         170.5028 |           7.8977 |
[32m[20221213 21:29:53 @agent_ppo2.py:185][0m |          -0.0079 |         170.4140 |           7.9583 |
[32m[20221213 21:29:53 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:29:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.80
[32m[20221213 21:29:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:29:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 21:29:53 @agent_ppo2.py:143][0m Total time:      34.31 min
[32m[20221213 21:29:53 @agent_ppo2.py:145][0m 3356672 total steps have happened
[32m[20221213 21:29:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1639 --------------------------#
[32m[20221213 21:29:53 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:29:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:53 @agent_ppo2.py:185][0m |          -0.0026 |         180.9794 |           7.6299 |
[32m[20221213 21:29:54 @agent_ppo2.py:185][0m |          -0.0028 |         179.3829 |           7.5918 |
[32m[20221213 21:29:54 @agent_ppo2.py:185][0m |          -0.0067 |         178.3337 |           7.6029 |
[32m[20221213 21:29:54 @agent_ppo2.py:185][0m |          -0.0034 |         178.1368 |           7.6280 |
[32m[20221213 21:29:54 @agent_ppo2.py:185][0m |          -0.0067 |         178.1736 |           7.5900 |
[32m[20221213 21:29:54 @agent_ppo2.py:185][0m |           0.0029 |         187.2263 |           7.5853 |
[32m[20221213 21:29:54 @agent_ppo2.py:185][0m |          -0.0075 |         176.8456 |           7.5876 |
[32m[20221213 21:29:54 @agent_ppo2.py:185][0m |           0.0011 |         182.7674 |           7.6137 |
[32m[20221213 21:29:54 @agent_ppo2.py:185][0m |          -0.0066 |         176.3275 |           7.5610 |
[32m[20221213 21:29:54 @agent_ppo2.py:185][0m |          -0.0105 |         176.1876 |           7.5597 |
[32m[20221213 21:29:54 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:29:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.20
[32m[20221213 21:29:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.00
[32m[20221213 21:29:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:29:55 @agent_ppo2.py:143][0m Total time:      34.33 min
[32m[20221213 21:29:55 @agent_ppo2.py:145][0m 3358720 total steps have happened
[32m[20221213 21:29:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1640 --------------------------#
[32m[20221213 21:29:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:55 @agent_ppo2.py:185][0m |          -0.0004 |         175.1588 |           7.7329 |
[32m[20221213 21:29:55 @agent_ppo2.py:185][0m |          -0.0048 |         173.9307 |           7.7368 |
[32m[20221213 21:29:55 @agent_ppo2.py:185][0m |          -0.0052 |         173.0205 |           7.7219 |
[32m[20221213 21:29:55 @agent_ppo2.py:185][0m |          -0.0064 |         172.3478 |           7.7382 |
[32m[20221213 21:29:55 @agent_ppo2.py:185][0m |          -0.0074 |         171.9509 |           7.7689 |
[32m[20221213 21:29:55 @agent_ppo2.py:185][0m |          -0.0064 |         171.5907 |           7.7379 |
[32m[20221213 21:29:56 @agent_ppo2.py:185][0m |          -0.0090 |         171.0929 |           7.7452 |
[32m[20221213 21:29:56 @agent_ppo2.py:185][0m |          -0.0098 |         170.9587 |           7.7525 |
[32m[20221213 21:29:56 @agent_ppo2.py:185][0m |          -0.0045 |         174.5201 |           7.7644 |
[32m[20221213 21:29:56 @agent_ppo2.py:185][0m |          -0.0095 |         170.2837 |           7.7607 |
[32m[20221213 21:29:56 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:29:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.60
[32m[20221213 21:29:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.00
[32m[20221213 21:29:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.00
[32m[20221213 21:29:56 @agent_ppo2.py:143][0m Total time:      34.35 min
[32m[20221213 21:29:56 @agent_ppo2.py:145][0m 3360768 total steps have happened
[32m[20221213 21:29:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1641 --------------------------#
[32m[20221213 21:29:56 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:29:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:56 @agent_ppo2.py:185][0m |           0.0012 |         181.7121 |           7.6903 |
[32m[20221213 21:29:56 @agent_ppo2.py:185][0m |          -0.0021 |         180.1266 |           7.7388 |
[32m[20221213 21:29:57 @agent_ppo2.py:185][0m |           0.0089 |         192.5866 |           7.6931 |
[32m[20221213 21:29:57 @agent_ppo2.py:185][0m |          -0.0063 |         178.5627 |           7.7356 |
[32m[20221213 21:29:57 @agent_ppo2.py:185][0m |          -0.0054 |         178.1691 |           7.7018 |
[32m[20221213 21:29:57 @agent_ppo2.py:185][0m |          -0.0070 |         177.6945 |           7.7268 |
[32m[20221213 21:29:57 @agent_ppo2.py:185][0m |          -0.0071 |         177.7891 |           7.7037 |
[32m[20221213 21:29:57 @agent_ppo2.py:185][0m |          -0.0087 |         177.4715 |           7.7220 |
[32m[20221213 21:29:57 @agent_ppo2.py:185][0m |           0.0081 |         197.5992 |           7.6852 |
[32m[20221213 21:29:57 @agent_ppo2.py:185][0m |          -0.0071 |         177.2180 |           7.6666 |
[32m[20221213 21:29:57 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 21:29:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.80
[32m[20221213 21:29:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:29:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.00
[32m[20221213 21:29:57 @agent_ppo2.py:143][0m Total time:      34.38 min
[32m[20221213 21:29:57 @agent_ppo2.py:145][0m 3362816 total steps have happened
[32m[20221213 21:29:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1642 --------------------------#
[32m[20221213 21:29:58 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:29:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:58 @agent_ppo2.py:185][0m |          -0.0016 |         174.0218 |           7.1997 |
[32m[20221213 21:29:58 @agent_ppo2.py:185][0m |          -0.0041 |         173.3186 |           7.2533 |
[32m[20221213 21:29:58 @agent_ppo2.py:185][0m |          -0.0039 |         173.2201 |           7.2201 |
[32m[20221213 21:29:58 @agent_ppo2.py:185][0m |           0.0091 |         189.2959 |           7.1815 |
[32m[20221213 21:29:58 @agent_ppo2.py:185][0m |          -0.0040 |         172.9750 |           7.1672 |
[32m[20221213 21:29:58 @agent_ppo2.py:185][0m |          -0.0045 |         172.6363 |           7.1950 |
[32m[20221213 21:29:58 @agent_ppo2.py:185][0m |          -0.0074 |         172.4876 |           7.1638 |
[32m[20221213 21:29:58 @agent_ppo2.py:185][0m |          -0.0072 |         172.3405 |           7.1534 |
[32m[20221213 21:29:59 @agent_ppo2.py:185][0m |          -0.0065 |         172.2560 |           7.1226 |
[32m[20221213 21:29:59 @agent_ppo2.py:185][0m |          -0.0073 |         172.1206 |           7.1108 |
[32m[20221213 21:29:59 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:29:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.20
[32m[20221213 21:29:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:29:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.00
[32m[20221213 21:29:59 @agent_ppo2.py:143][0m Total time:      34.40 min
[32m[20221213 21:29:59 @agent_ppo2.py:145][0m 3364864 total steps have happened
[32m[20221213 21:29:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1643 --------------------------#
[32m[20221213 21:29:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:29:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:29:59 @agent_ppo2.py:185][0m |          -0.0015 |         174.6056 |           7.4508 |
[32m[20221213 21:29:59 @agent_ppo2.py:185][0m |          -0.0035 |         173.5268 |           7.4260 |
[32m[20221213 21:29:59 @agent_ppo2.py:185][0m |           0.0051 |         188.1157 |           7.4276 |
[32m[20221213 21:29:59 @agent_ppo2.py:185][0m |          -0.0039 |         172.3853 |           7.4521 |
[32m[20221213 21:30:00 @agent_ppo2.py:185][0m |          -0.0062 |         172.1110 |           7.3853 |
[32m[20221213 21:30:00 @agent_ppo2.py:185][0m |          -0.0067 |         172.0964 |           7.4226 |
[32m[20221213 21:30:00 @agent_ppo2.py:185][0m |          -0.0068 |         171.7023 |           7.3650 |
[32m[20221213 21:30:00 @agent_ppo2.py:185][0m |          -0.0065 |         171.4575 |           7.4198 |
[32m[20221213 21:30:00 @agent_ppo2.py:185][0m |          -0.0060 |         171.6827 |           7.3501 |
[32m[20221213 21:30:00 @agent_ppo2.py:185][0m |          -0.0074 |         171.1839 |           7.3569 |
[32m[20221213 21:30:00 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:30:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.20
[32m[20221213 21:30:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.00
[32m[20221213 21:30:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.00
[32m[20221213 21:30:00 @agent_ppo2.py:143][0m Total time:      34.43 min
[32m[20221213 21:30:00 @agent_ppo2.py:145][0m 3366912 total steps have happened
[32m[20221213 21:30:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1644 --------------------------#
[32m[20221213 21:30:00 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:01 @agent_ppo2.py:185][0m |          -0.0000 |         176.5260 |           7.3249 |
[32m[20221213 21:30:01 @agent_ppo2.py:185][0m |          -0.0034 |         175.7555 |           7.3500 |
[32m[20221213 21:30:01 @agent_ppo2.py:185][0m |           0.0033 |         180.1481 |           7.3380 |
[32m[20221213 21:30:01 @agent_ppo2.py:185][0m |          -0.0075 |         174.9518 |           7.4069 |
[32m[20221213 21:30:01 @agent_ppo2.py:185][0m |          -0.0033 |         174.8127 |           7.3552 |
[32m[20221213 21:30:01 @agent_ppo2.py:185][0m |          -0.0080 |         174.4522 |           7.3146 |
[32m[20221213 21:30:01 @agent_ppo2.py:185][0m |          -0.0075 |         174.3057 |           7.3352 |
[32m[20221213 21:30:01 @agent_ppo2.py:185][0m |          -0.0055 |         174.5002 |           7.3069 |
[32m[20221213 21:30:01 @agent_ppo2.py:185][0m |          -0.0069 |         174.0705 |           7.3727 |
[32m[20221213 21:30:01 @agent_ppo2.py:185][0m |          -0.0084 |         173.8081 |           7.3402 |
[32m[20221213 21:30:01 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:30:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.80
[32m[20221213 21:30:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:30:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:30:02 @agent_ppo2.py:143][0m Total time:      34.45 min
[32m[20221213 21:30:02 @agent_ppo2.py:145][0m 3368960 total steps have happened
[32m[20221213 21:30:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1645 --------------------------#
[32m[20221213 21:30:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:30:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:02 @agent_ppo2.py:185][0m |          -0.0003 |         175.3569 |           7.3491 |
[32m[20221213 21:30:02 @agent_ppo2.py:185][0m |          -0.0043 |         174.6014 |           7.3809 |
[32m[20221213 21:30:02 @agent_ppo2.py:185][0m |          -0.0040 |         174.1959 |           7.3972 |
[32m[20221213 21:30:02 @agent_ppo2.py:185][0m |          -0.0053 |         174.0492 |           7.3771 |
[32m[20221213 21:30:02 @agent_ppo2.py:185][0m |          -0.0061 |         173.9479 |           7.4320 |
[32m[20221213 21:30:02 @agent_ppo2.py:185][0m |          -0.0053 |         173.7663 |           7.4910 |
[32m[20221213 21:30:03 @agent_ppo2.py:185][0m |          -0.0067 |         173.5776 |           7.4855 |
[32m[20221213 21:30:03 @agent_ppo2.py:185][0m |           0.0004 |         176.8453 |           7.5118 |
[32m[20221213 21:30:03 @agent_ppo2.py:185][0m |          -0.0065 |         173.7913 |           7.5762 |
[32m[20221213 21:30:03 @agent_ppo2.py:185][0m |          -0.0035 |         173.7014 |           7.5209 |
[32m[20221213 21:30:03 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:30:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.80
[32m[20221213 21:30:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:30:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.00
[32m[20221213 21:30:03 @agent_ppo2.py:143][0m Total time:      34.47 min
[32m[20221213 21:30:03 @agent_ppo2.py:145][0m 3371008 total steps have happened
[32m[20221213 21:30:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1646 --------------------------#
[32m[20221213 21:30:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:30:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:03 @agent_ppo2.py:185][0m |           0.0142 |         200.1349 |           7.6000 |
[32m[20221213 21:30:03 @agent_ppo2.py:185][0m |          -0.0034 |         173.9511 |           7.6221 |
[32m[20221213 21:30:04 @agent_ppo2.py:185][0m |          -0.0023 |         173.7457 |           7.5810 |
[32m[20221213 21:30:04 @agent_ppo2.py:185][0m |          -0.0049 |         173.3753 |           7.5433 |
[32m[20221213 21:30:04 @agent_ppo2.py:185][0m |          -0.0065 |         173.3358 |           7.4981 |
[32m[20221213 21:30:04 @agent_ppo2.py:185][0m |          -0.0059 |         173.1671 |           7.4806 |
[32m[20221213 21:30:04 @agent_ppo2.py:185][0m |          -0.0054 |         173.0691 |           7.5143 |
[32m[20221213 21:30:04 @agent_ppo2.py:185][0m |          -0.0075 |         172.9064 |           7.4744 |
[32m[20221213 21:30:04 @agent_ppo2.py:185][0m |          -0.0040 |         173.5748 |           7.4501 |
[32m[20221213 21:30:04 @agent_ppo2.py:185][0m |          -0.0079 |         173.0430 |           7.3994 |
[32m[20221213 21:30:04 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 21:30:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.80
[32m[20221213 21:30:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:30:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.00
[32m[20221213 21:30:04 @agent_ppo2.py:143][0m Total time:      34.49 min
[32m[20221213 21:30:04 @agent_ppo2.py:145][0m 3373056 total steps have happened
[32m[20221213 21:30:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1647 --------------------------#
[32m[20221213 21:30:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:30:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:05 @agent_ppo2.py:185][0m |           0.0038 |         182.7210 |           7.2963 |
[32m[20221213 21:30:05 @agent_ppo2.py:185][0m |          -0.0025 |         176.6248 |           7.3751 |
[32m[20221213 21:30:05 @agent_ppo2.py:185][0m |          -0.0078 |         175.0785 |           7.3697 |
[32m[20221213 21:30:05 @agent_ppo2.py:185][0m |          -0.0089 |         174.8593 |           7.4302 |
[32m[20221213 21:30:05 @agent_ppo2.py:185][0m |          -0.0030 |         177.2417 |           7.4422 |
[32m[20221213 21:30:05 @agent_ppo2.py:185][0m |          -0.0094 |         174.4827 |           7.5202 |
[32m[20221213 21:30:05 @agent_ppo2.py:185][0m |          -0.0099 |         174.3223 |           7.5282 |
[32m[20221213 21:30:05 @agent_ppo2.py:185][0m |          -0.0097 |         174.3060 |           7.5358 |
[32m[20221213 21:30:05 @agent_ppo2.py:185][0m |          -0.0097 |         174.1367 |           7.5628 |
[32m[20221213 21:30:06 @agent_ppo2.py:185][0m |          -0.0101 |         173.9072 |           7.5646 |
[32m[20221213 21:30:06 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:30:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.60
[32m[20221213 21:30:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.00
[32m[20221213 21:30:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:30:06 @agent_ppo2.py:143][0m Total time:      34.52 min
[32m[20221213 21:30:06 @agent_ppo2.py:145][0m 3375104 total steps have happened
[32m[20221213 21:30:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1648 --------------------------#
[32m[20221213 21:30:06 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 21:30:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:06 @agent_ppo2.py:185][0m |          -0.0004 |         175.8391 |           7.0448 |
[32m[20221213 21:30:06 @agent_ppo2.py:185][0m |           0.0082 |         181.5867 |           7.0622 |
[32m[20221213 21:30:06 @agent_ppo2.py:185][0m |          -0.0004 |         174.0051 |           7.0324 |
[32m[20221213 21:30:06 @agent_ppo2.py:185][0m |           0.0078 |         184.4849 |           7.0710 |
[32m[20221213 21:30:07 @agent_ppo2.py:185][0m |          -0.0044 |         170.2915 |           7.0750 |
[32m[20221213 21:30:07 @agent_ppo2.py:185][0m |          -0.0053 |         169.5893 |           7.0596 |
[32m[20221213 21:30:07 @agent_ppo2.py:185][0m |           0.0009 |         177.1262 |           7.0373 |
[32m[20221213 21:30:07 @agent_ppo2.py:185][0m |          -0.0077 |         168.8979 |           7.0421 |
[32m[20221213 21:30:07 @agent_ppo2.py:185][0m |          -0.0056 |         168.5931 |           7.0339 |
[32m[20221213 21:30:07 @agent_ppo2.py:185][0m |          -0.0045 |         170.1792 |           7.0148 |
[32m[20221213 21:30:07 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:30:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.80
[32m[20221213 21:30:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:30:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:30:07 @agent_ppo2.py:143][0m Total time:      34.54 min
[32m[20221213 21:30:07 @agent_ppo2.py:145][0m 3377152 total steps have happened
[32m[20221213 21:30:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1649 --------------------------#
[32m[20221213 21:30:07 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:08 @agent_ppo2.py:185][0m |          -0.0031 |         175.1367 |           7.2088 |
[32m[20221213 21:30:08 @agent_ppo2.py:185][0m |          -0.0019 |         175.2091 |           7.2279 |
[32m[20221213 21:30:08 @agent_ppo2.py:185][0m |          -0.0007 |         176.8073 |           7.2711 |
[32m[20221213 21:30:08 @agent_ppo2.py:185][0m |          -0.0015 |         177.4471 |           7.3033 |
[32m[20221213 21:30:08 @agent_ppo2.py:185][0m |          -0.0084 |         172.2647 |           7.2471 |
[32m[20221213 21:30:08 @agent_ppo2.py:185][0m |          -0.0109 |         171.9758 |           7.2033 |
[32m[20221213 21:30:08 @agent_ppo2.py:185][0m |          -0.0021 |         181.2541 |           7.2492 |
[32m[20221213 21:30:08 @agent_ppo2.py:185][0m |          -0.0068 |         171.8606 |           7.2527 |
[32m[20221213 21:30:08 @agent_ppo2.py:185][0m |          -0.0084 |         171.5834 |           7.2530 |
[32m[20221213 21:30:08 @agent_ppo2.py:185][0m |          -0.0087 |         171.5666 |           7.2756 |
[32m[20221213 21:30:08 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:30:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.00
[32m[20221213 21:30:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:30:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.00
[32m[20221213 21:30:09 @agent_ppo2.py:143][0m Total time:      34.56 min
[32m[20221213 21:30:09 @agent_ppo2.py:145][0m 3379200 total steps have happened
[32m[20221213 21:30:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1650 --------------------------#
[32m[20221213 21:30:09 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:09 @agent_ppo2.py:185][0m |          -0.0007 |         177.1300 |           7.3680 |
[32m[20221213 21:30:09 @agent_ppo2.py:185][0m |          -0.0003 |         175.3619 |           7.3301 |
[32m[20221213 21:30:09 @agent_ppo2.py:185][0m |           0.0003 |         175.0223 |           7.4153 |
[32m[20221213 21:30:09 @agent_ppo2.py:185][0m |          -0.0009 |         173.9921 |           7.4062 |
[32m[20221213 21:30:09 @agent_ppo2.py:185][0m |          -0.0028 |         173.6232 |           7.4053 |
[32m[20221213 21:30:09 @agent_ppo2.py:185][0m |          -0.0040 |         173.5077 |           7.3753 |
[32m[20221213 21:30:10 @agent_ppo2.py:185][0m |           0.0101 |         198.3809 |           7.3839 |
[32m[20221213 21:30:10 @agent_ppo2.py:185][0m |           0.0031 |         177.1925 |           7.4358 |
[32m[20221213 21:30:10 @agent_ppo2.py:185][0m |          -0.0057 |         173.1360 |           7.4255 |
[32m[20221213 21:30:10 @agent_ppo2.py:185][0m |          -0.0050 |         173.0703 |           7.4038 |
[32m[20221213 21:30:10 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:30:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.60
[32m[20221213 21:30:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:30:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.00
[32m[20221213 21:30:10 @agent_ppo2.py:143][0m Total time:      34.59 min
[32m[20221213 21:30:10 @agent_ppo2.py:145][0m 3381248 total steps have happened
[32m[20221213 21:30:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1651 --------------------------#
[32m[20221213 21:30:10 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 21:30:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:10 @agent_ppo2.py:185][0m |          -0.0003 |         172.2855 |           7.6826 |
[32m[20221213 21:30:10 @agent_ppo2.py:185][0m |          -0.0032 |         171.7515 |           7.6523 |
[32m[20221213 21:30:11 @agent_ppo2.py:185][0m |          -0.0050 |         171.0427 |           7.6120 |
[32m[20221213 21:30:11 @agent_ppo2.py:185][0m |          -0.0051 |         170.8251 |           7.6030 |
[32m[20221213 21:30:11 @agent_ppo2.py:185][0m |          -0.0053 |         170.6971 |           7.6044 |
[32m[20221213 21:30:11 @agent_ppo2.py:185][0m |          -0.0062 |         170.2116 |           7.5295 |
[32m[20221213 21:30:11 @agent_ppo2.py:185][0m |          -0.0067 |         170.1520 |           7.4603 |
[32m[20221213 21:30:11 @agent_ppo2.py:185][0m |           0.0084 |         189.4769 |           7.4652 |
[32m[20221213 21:30:11 @agent_ppo2.py:185][0m |          -0.0059 |         169.7083 |           7.5651 |
[32m[20221213 21:30:11 @agent_ppo2.py:185][0m |          -0.0069 |         169.5767 |           7.4387 |
[32m[20221213 21:30:11 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:30:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.80
[32m[20221213 21:30:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:30:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 863.00
[32m[20221213 21:30:11 @agent_ppo2.py:143][0m Total time:      34.61 min
[32m[20221213 21:30:11 @agent_ppo2.py:145][0m 3383296 total steps have happened
[32m[20221213 21:30:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1652 --------------------------#
[32m[20221213 21:30:12 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:12 @agent_ppo2.py:185][0m |          -0.0014 |         175.0415 |           7.4275 |
[32m[20221213 21:30:12 @agent_ppo2.py:185][0m |          -0.0052 |         174.0674 |           7.4854 |
[32m[20221213 21:30:12 @agent_ppo2.py:185][0m |          -0.0055 |         173.5271 |           7.4334 |
[32m[20221213 21:30:12 @agent_ppo2.py:185][0m |          -0.0065 |         172.9999 |           7.4793 |
[32m[20221213 21:30:12 @agent_ppo2.py:185][0m |          -0.0063 |         172.6225 |           7.4844 |
[32m[20221213 21:30:12 @agent_ppo2.py:185][0m |          -0.0078 |         172.2446 |           7.4883 |
[32m[20221213 21:30:12 @agent_ppo2.py:185][0m |          -0.0076 |         172.0873 |           7.4515 |
[32m[20221213 21:30:12 @agent_ppo2.py:185][0m |          -0.0029 |         179.8877 |           7.4917 |
[32m[20221213 21:30:13 @agent_ppo2.py:185][0m |          -0.0102 |         171.6894 |           7.4741 |
[32m[20221213 21:30:13 @agent_ppo2.py:185][0m |          -0.0093 |         171.5100 |           7.5161 |
[32m[20221213 21:30:13 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 21:30:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.80
[32m[20221213 21:30:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 21:30:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.00
[32m[20221213 21:30:13 @agent_ppo2.py:143][0m Total time:      34.64 min
[32m[20221213 21:30:13 @agent_ppo2.py:145][0m 3385344 total steps have happened
[32m[20221213 21:30:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1653 --------------------------#
[32m[20221213 21:30:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:30:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:13 @agent_ppo2.py:185][0m |           0.0019 |         173.5371 |           7.2272 |
[32m[20221213 21:30:13 @agent_ppo2.py:185][0m |          -0.0016 |         172.8827 |           7.0892 |
[32m[20221213 21:30:13 @agent_ppo2.py:185][0m |           0.0038 |         181.8037 |           7.1071 |
[32m[20221213 21:30:13 @agent_ppo2.py:185][0m |          -0.0036 |         172.5144 |           7.1028 |
[32m[20221213 21:30:14 @agent_ppo2.py:185][0m |          -0.0063 |         172.3530 |           7.0924 |
[32m[20221213 21:30:14 @agent_ppo2.py:185][0m |          -0.0056 |         172.3032 |           7.1068 |
[32m[20221213 21:30:14 @agent_ppo2.py:185][0m |          -0.0046 |         172.8122 |           7.0876 |
[32m[20221213 21:30:14 @agent_ppo2.py:185][0m |          -0.0049 |         172.5595 |           7.0595 |
[32m[20221213 21:30:14 @agent_ppo2.py:185][0m |          -0.0066 |         171.9673 |           7.0324 |
[32m[20221213 21:30:14 @agent_ppo2.py:185][0m |           0.0033 |         187.9045 |           7.0711 |
[32m[20221213 21:30:14 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:30:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.40
[32m[20221213 21:30:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.00
[32m[20221213 21:30:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.00
[32m[20221213 21:30:14 @agent_ppo2.py:143][0m Total time:      34.66 min
[32m[20221213 21:30:14 @agent_ppo2.py:145][0m 3387392 total steps have happened
[32m[20221213 21:30:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1654 --------------------------#
[32m[20221213 21:30:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:30:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:15 @agent_ppo2.py:185][0m |           0.0002 |         175.3200 |           6.9821 |
[32m[20221213 21:30:15 @agent_ppo2.py:185][0m |          -0.0052 |         174.4262 |           7.0238 |
[32m[20221213 21:30:15 @agent_ppo2.py:185][0m |          -0.0020 |         176.7095 |           6.9702 |
[32m[20221213 21:30:15 @agent_ppo2.py:185][0m |          -0.0052 |         173.8187 |           7.0174 |
[32m[20221213 21:30:15 @agent_ppo2.py:185][0m |          -0.0082 |         173.0885 |           6.9878 |
[32m[20221213 21:30:15 @agent_ppo2.py:185][0m |           0.0106 |         202.7722 |           7.0168 |
[32m[20221213 21:30:15 @agent_ppo2.py:185][0m |          -0.0098 |         172.6683 |           6.9866 |
[32m[20221213 21:30:15 @agent_ppo2.py:185][0m |          -0.0095 |         172.4160 |           6.9918 |
[32m[20221213 21:30:15 @agent_ppo2.py:185][0m |          -0.0108 |         172.2046 |           6.9839 |
[32m[20221213 21:30:15 @agent_ppo2.py:185][0m |          -0.0106 |         172.1535 |           6.9961 |
[32m[20221213 21:30:15 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:30:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.80
[32m[20221213 21:30:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.00
[32m[20221213 21:30:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.00
[32m[20221213 21:30:16 @agent_ppo2.py:143][0m Total time:      34.68 min
[32m[20221213 21:30:16 @agent_ppo2.py:145][0m 3389440 total steps have happened
[32m[20221213 21:30:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1655 --------------------------#
[32m[20221213 21:30:16 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:16 @agent_ppo2.py:185][0m |           0.0098 |         202.3349 |           6.5829 |
[32m[20221213 21:30:16 @agent_ppo2.py:185][0m |          -0.0058 |         176.3888 |           6.5803 |
[32m[20221213 21:30:16 @agent_ppo2.py:185][0m |          -0.0072 |         175.7990 |           6.5981 |
[32m[20221213 21:30:16 @agent_ppo2.py:185][0m |          -0.0080 |         175.5848 |           6.5818 |
[32m[20221213 21:30:16 @agent_ppo2.py:185][0m |          -0.0051 |         175.9267 |           6.5656 |
[32m[20221213 21:30:16 @agent_ppo2.py:185][0m |          -0.0084 |         175.2465 |           6.5632 |
[32m[20221213 21:30:17 @agent_ppo2.py:185][0m |          -0.0081 |         174.9353 |           6.5247 |
[32m[20221213 21:30:17 @agent_ppo2.py:185][0m |          -0.0102 |         174.8715 |           6.5057 |
[32m[20221213 21:30:17 @agent_ppo2.py:185][0m |          -0.0093 |         174.8928 |           6.4958 |
[32m[20221213 21:30:17 @agent_ppo2.py:185][0m |          -0.0112 |         174.5440 |           6.4997 |
[32m[20221213 21:30:17 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:30:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.60
[32m[20221213 21:30:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.00
[32m[20221213 21:30:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.00
[32m[20221213 21:30:17 @agent_ppo2.py:143][0m Total time:      34.71 min
[32m[20221213 21:30:17 @agent_ppo2.py:145][0m 3391488 total steps have happened
[32m[20221213 21:30:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1656 --------------------------#
[32m[20221213 21:30:17 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:17 @agent_ppo2.py:185][0m |          -0.0018 |         176.7489 |           7.2708 |
[32m[20221213 21:30:18 @agent_ppo2.py:185][0m |          -0.0068 |         175.6627 |           7.3062 |
[32m[20221213 21:30:18 @agent_ppo2.py:185][0m |          -0.0064 |         174.7074 |           7.2955 |
[32m[20221213 21:30:18 @agent_ppo2.py:185][0m |          -0.0071 |         174.2110 |           7.2609 |
[32m[20221213 21:30:18 @agent_ppo2.py:185][0m |           0.0045 |         194.2619 |           7.3338 |
[32m[20221213 21:30:18 @agent_ppo2.py:185][0m |          -0.0070 |         173.6448 |           7.3294 |
[32m[20221213 21:30:18 @agent_ppo2.py:185][0m |          -0.0077 |         173.2774 |           7.2855 |
[32m[20221213 21:30:18 @agent_ppo2.py:185][0m |          -0.0107 |         173.0688 |           7.3482 |
[32m[20221213 21:30:18 @agent_ppo2.py:185][0m |          -0.0066 |         172.9842 |           7.3243 |
[32m[20221213 21:30:18 @agent_ppo2.py:185][0m |          -0.0090 |         172.8124 |           7.3078 |
[32m[20221213 21:30:18 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221213 21:30:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 765.80
[32m[20221213 21:30:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:30:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:30:19 @agent_ppo2.py:143][0m Total time:      34.73 min
[32m[20221213 21:30:19 @agent_ppo2.py:145][0m 3393536 total steps have happened
[32m[20221213 21:30:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1657 --------------------------#
[32m[20221213 21:30:19 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:19 @agent_ppo2.py:185][0m |           0.0007 |         174.9214 |           7.1862 |
[32m[20221213 21:30:19 @agent_ppo2.py:185][0m |          -0.0046 |         173.8141 |           7.2031 |
[32m[20221213 21:30:19 @agent_ppo2.py:185][0m |          -0.0043 |         173.5258 |           7.2424 |
[32m[20221213 21:30:19 @agent_ppo2.py:185][0m |          -0.0075 |         172.3803 |           7.2196 |
[32m[20221213 21:30:19 @agent_ppo2.py:185][0m |          -0.0087 |         171.9817 |           7.2356 |
[32m[20221213 21:30:19 @agent_ppo2.py:185][0m |          -0.0069 |         171.7623 |           7.2437 |
[32m[20221213 21:30:20 @agent_ppo2.py:185][0m |          -0.0032 |         173.9013 |           7.2458 |
[32m[20221213 21:30:20 @agent_ppo2.py:185][0m |          -0.0094 |         171.2247 |           7.2620 |
[32m[20221213 21:30:20 @agent_ppo2.py:185][0m |          -0.0042 |         171.7173 |           7.2629 |
[32m[20221213 21:30:20 @agent_ppo2.py:185][0m |          -0.0077 |         171.2503 |           7.2671 |
[32m[20221213 21:30:20 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:30:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.20
[32m[20221213 21:30:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.00
[32m[20221213 21:30:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:30:20 @agent_ppo2.py:143][0m Total time:      34.76 min
[32m[20221213 21:30:20 @agent_ppo2.py:145][0m 3395584 total steps have happened
[32m[20221213 21:30:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1658 --------------------------#
[32m[20221213 21:30:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:30:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:20 @agent_ppo2.py:185][0m |          -0.0022 |         177.6419 |           6.9673 |
[32m[20221213 21:30:20 @agent_ppo2.py:185][0m |          -0.0008 |         176.0996 |           7.0179 |
[32m[20221213 21:30:21 @agent_ppo2.py:185][0m |          -0.0011 |         176.1663 |           7.0413 |
[32m[20221213 21:30:21 @agent_ppo2.py:185][0m |          -0.0065 |         174.3159 |           7.0467 |
[32m[20221213 21:30:21 @agent_ppo2.py:185][0m |          -0.0062 |         173.6497 |           7.0949 |
[32m[20221213 21:30:21 @agent_ppo2.py:185][0m |          -0.0075 |         173.3627 |           7.0966 |
[32m[20221213 21:30:21 @agent_ppo2.py:185][0m |          -0.0063 |         173.6747 |           7.1380 |
[32m[20221213 21:30:21 @agent_ppo2.py:185][0m |          -0.0004 |         176.2818 |           7.1163 |
[32m[20221213 21:30:21 @agent_ppo2.py:185][0m |           0.0041 |         186.7798 |           7.0918 |
[32m[20221213 21:30:21 @agent_ppo2.py:185][0m |          -0.0063 |         172.7478 |           7.0723 |
[32m[20221213 21:30:21 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:30:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.80
[32m[20221213 21:30:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:30:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.00
[32m[20221213 21:30:22 @agent_ppo2.py:143][0m Total time:      34.78 min
[32m[20221213 21:30:22 @agent_ppo2.py:145][0m 3397632 total steps have happened
[32m[20221213 21:30:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1659 --------------------------#
[32m[20221213 21:30:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:30:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:22 @agent_ppo2.py:185][0m |          -0.0011 |         177.3224 |           6.4982 |
[32m[20221213 21:30:22 @agent_ppo2.py:185][0m |          -0.0003 |         176.6076 |           6.5504 |
[32m[20221213 21:30:22 @agent_ppo2.py:185][0m |          -0.0037 |         175.6093 |           6.5761 |
[32m[20221213 21:30:22 @agent_ppo2.py:185][0m |          -0.0012 |         175.6980 |           6.5243 |
[32m[20221213 21:30:22 @agent_ppo2.py:185][0m |          -0.0048 |         175.0667 |           6.5523 |
[32m[20221213 21:30:22 @agent_ppo2.py:185][0m |          -0.0062 |         175.1123 |           6.5639 |
[32m[20221213 21:30:23 @agent_ppo2.py:185][0m |          -0.0043 |         175.0730 |           6.5919 |
[32m[20221213 21:30:23 @agent_ppo2.py:185][0m |           0.0068 |         194.3265 |           6.5350 |
[32m[20221213 21:30:23 @agent_ppo2.py:185][0m |          -0.0057 |         174.4807 |           6.5502 |
[32m[20221213 21:30:23 @agent_ppo2.py:185][0m |          -0.0064 |         174.2803 |           6.5970 |
[32m[20221213 21:30:23 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221213 21:30:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.00
[32m[20221213 21:30:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:30:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.00
[32m[20221213 21:30:23 @agent_ppo2.py:143][0m Total time:      34.81 min
[32m[20221213 21:30:23 @agent_ppo2.py:145][0m 3399680 total steps have happened
[32m[20221213 21:30:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1660 --------------------------#
[32m[20221213 21:30:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:30:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:23 @agent_ppo2.py:185][0m |          -0.0012 |         177.0028 |           6.9577 |
[32m[20221213 21:30:24 @agent_ppo2.py:185][0m |          -0.0029 |         175.2178 |           6.9615 |
[32m[20221213 21:30:24 @agent_ppo2.py:185][0m |          -0.0056 |         174.5859 |           6.9640 |
[32m[20221213 21:30:24 @agent_ppo2.py:185][0m |          -0.0030 |         174.7067 |           7.0334 |
[32m[20221213 21:30:24 @agent_ppo2.py:185][0m |          -0.0058 |         174.1148 |           7.0531 |
[32m[20221213 21:30:24 @agent_ppo2.py:185][0m |          -0.0079 |         173.0495 |           7.0837 |
[32m[20221213 21:30:24 @agent_ppo2.py:185][0m |          -0.0084 |         172.8015 |           7.1609 |
[32m[20221213 21:30:24 @agent_ppo2.py:185][0m |          -0.0097 |         172.3249 |           7.1481 |
[32m[20221213 21:30:24 @agent_ppo2.py:185][0m |          -0.0092 |         172.3673 |           7.1798 |
[32m[20221213 21:30:24 @agent_ppo2.py:185][0m |          -0.0040 |         173.1831 |           7.2466 |
[32m[20221213 21:30:24 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 21:30:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.20
[32m[20221213 21:30:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:30:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 773.00
[32m[20221213 21:30:25 @agent_ppo2.py:143][0m Total time:      34.83 min
[32m[20221213 21:30:25 @agent_ppo2.py:145][0m 3401728 total steps have happened
[32m[20221213 21:30:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1661 --------------------------#
[32m[20221213 21:30:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 21:30:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:25 @agent_ppo2.py:185][0m |          -0.0003 |         178.8715 |           7.4067 |
[32m[20221213 21:30:25 @agent_ppo2.py:185][0m |          -0.0019 |         178.0635 |           7.3982 |
[32m[20221213 21:30:25 @agent_ppo2.py:185][0m |          -0.0024 |         178.3335 |           7.3497 |
[32m[20221213 21:30:25 @agent_ppo2.py:185][0m |          -0.0042 |         177.0717 |           7.3999 |
[32m[20221213 21:30:25 @agent_ppo2.py:185][0m |          -0.0050 |         176.8683 |           7.4177 |
[32m[20221213 21:30:26 @agent_ppo2.py:185][0m |          -0.0063 |         176.4804 |           7.4362 |
[32m[20221213 21:30:26 @agent_ppo2.py:185][0m |          -0.0048 |         176.5449 |           7.3995 |
[32m[20221213 21:30:26 @agent_ppo2.py:185][0m |          -0.0081 |         176.1116 |           7.4411 |
[32m[20221213 21:30:26 @agent_ppo2.py:185][0m |          -0.0026 |         181.5579 |           7.4600 |
[32m[20221213 21:30:26 @agent_ppo2.py:185][0m |          -0.0078 |         175.8245 |           7.4031 |
[32m[20221213 21:30:26 @agent_ppo2.py:130][0m Policy update time: 1.44 s
[32m[20221213 21:30:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.00
[32m[20221213 21:30:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.00
[32m[20221213 21:30:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.00
[32m[20221213 21:30:26 @agent_ppo2.py:143][0m Total time:      34.86 min
[32m[20221213 21:30:26 @agent_ppo2.py:145][0m 3403776 total steps have happened
[32m[20221213 21:30:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1662 --------------------------#
[32m[20221213 21:30:26 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:27 @agent_ppo2.py:185][0m |           0.0120 |         191.6358 |           7.3195 |
[32m[20221213 21:30:27 @agent_ppo2.py:185][0m |          -0.0038 |         178.0172 |           7.3740 |
[32m[20221213 21:30:27 @agent_ppo2.py:185][0m |          -0.0069 |         177.2020 |           7.3364 |
[32m[20221213 21:30:27 @agent_ppo2.py:185][0m |          -0.0062 |         176.5878 |           7.3311 |
[32m[20221213 21:30:27 @agent_ppo2.py:185][0m |          -0.0020 |         178.6582 |           7.3266 |
[32m[20221213 21:30:27 @agent_ppo2.py:185][0m |          -0.0092 |         176.0501 |           7.3702 |
[32m[20221213 21:30:27 @agent_ppo2.py:185][0m |          -0.0057 |         176.0997 |           7.3396 |
[32m[20221213 21:30:27 @agent_ppo2.py:185][0m |          -0.0081 |         175.5639 |           7.3441 |
[32m[20221213 21:30:27 @agent_ppo2.py:185][0m |          -0.0060 |         177.1782 |           7.3478 |
[32m[20221213 21:30:28 @agent_ppo2.py:185][0m |          -0.0084 |         175.0447 |           7.3531 |
[32m[20221213 21:30:28 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:30:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.40
[32m[20221213 21:30:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:30:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.00
[32m[20221213 21:30:28 @agent_ppo2.py:143][0m Total time:      34.88 min
[32m[20221213 21:30:28 @agent_ppo2.py:145][0m 3405824 total steps have happened
[32m[20221213 21:30:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1663 --------------------------#
[32m[20221213 21:30:28 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:28 @agent_ppo2.py:185][0m |           0.0020 |         173.5054 |           7.0579 |
[32m[20221213 21:30:28 @agent_ppo2.py:185][0m |           0.0098 |         189.6064 |           7.1056 |
[32m[20221213 21:30:28 @agent_ppo2.py:185][0m |          -0.0032 |         171.3697 |           7.1763 |
[32m[20221213 21:30:28 @agent_ppo2.py:185][0m |          -0.0065 |         170.8381 |           7.1403 |
[32m[20221213 21:30:28 @agent_ppo2.py:185][0m |          -0.0002 |         173.2881 |           7.1274 |
[32m[20221213 21:30:29 @agent_ppo2.py:185][0m |          -0.0073 |         170.3484 |           7.1228 |
[32m[20221213 21:30:29 @agent_ppo2.py:185][0m |          -0.0037 |         172.1103 |           7.0746 |
[32m[20221213 21:30:29 @agent_ppo2.py:185][0m |          -0.0103 |         170.0303 |           7.0565 |
[32m[20221213 21:30:29 @agent_ppo2.py:185][0m |          -0.0066 |         170.1548 |           7.0404 |
[32m[20221213 21:30:29 @agent_ppo2.py:185][0m |          -0.0033 |         173.6474 |           7.0607 |
[32m[20221213 21:30:29 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:30:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.60
[32m[20221213 21:30:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 21:30:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.00
[32m[20221213 21:30:29 @agent_ppo2.py:143][0m Total time:      34.91 min
[32m[20221213 21:30:29 @agent_ppo2.py:145][0m 3407872 total steps have happened
[32m[20221213 21:30:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1664 --------------------------#
[32m[20221213 21:30:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 21:30:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:30 @agent_ppo2.py:185][0m |           0.0001 |         173.5252 |           7.1815 |
[32m[20221213 21:30:30 @agent_ppo2.py:185][0m |          -0.0046 |         172.8749 |           7.2353 |
[32m[20221213 21:30:30 @agent_ppo2.py:185][0m |          -0.0035 |         172.5781 |           7.2041 |
[32m[20221213 21:30:30 @agent_ppo2.py:185][0m |          -0.0037 |         172.2792 |           7.2095 |
[32m[20221213 21:30:30 @agent_ppo2.py:185][0m |           0.0033 |         180.0844 |           7.1480 |
[32m[20221213 21:30:30 @agent_ppo2.py:185][0m |          -0.0057 |         172.1465 |           7.1744 |
[32m[20221213 21:30:30 @agent_ppo2.py:185][0m |          -0.0069 |         171.9914 |           7.1338 |
[32m[20221213 21:30:30 @agent_ppo2.py:185][0m |          -0.0008 |         175.9271 |           7.1180 |
[32m[20221213 21:30:30 @agent_ppo2.py:185][0m |          -0.0065 |         171.9674 |           7.1173 |
[32m[20221213 21:30:31 @agent_ppo2.py:185][0m |          -0.0074 |         171.7826 |           7.1225 |
[32m[20221213 21:30:31 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:30:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.00
[32m[20221213 21:30:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:30:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.00
[32m[20221213 21:30:31 @agent_ppo2.py:143][0m Total time:      34.93 min
[32m[20221213 21:30:31 @agent_ppo2.py:145][0m 3409920 total steps have happened
[32m[20221213 21:30:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1665 --------------------------#
[32m[20221213 21:30:31 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:31 @agent_ppo2.py:185][0m |          -0.0013 |         175.6441 |           7.0237 |
[32m[20221213 21:30:31 @agent_ppo2.py:185][0m |          -0.0044 |         174.9714 |           7.0873 |
[32m[20221213 21:30:31 @agent_ppo2.py:185][0m |           0.0019 |         186.9253 |           7.0987 |
[32m[20221213 21:30:31 @agent_ppo2.py:185][0m |          -0.0021 |         178.3129 |           7.1044 |
[32m[20221213 21:30:31 @agent_ppo2.py:185][0m |           0.0056 |         186.2630 |           7.0922 |
[32m[20221213 21:30:31 @agent_ppo2.py:185][0m |          -0.0077 |         174.1015 |           7.1309 |
[32m[20221213 21:30:32 @agent_ppo2.py:185][0m |          -0.0075 |         173.9447 |           7.1159 |
[32m[20221213 21:30:32 @agent_ppo2.py:185][0m |          -0.0026 |         176.6062 |           7.1588 |
[32m[20221213 21:30:32 @agent_ppo2.py:185][0m |          -0.0090 |         173.6429 |           7.1913 |
[32m[20221213 21:30:32 @agent_ppo2.py:185][0m |           0.0072 |         192.8431 |           7.1539 |
[32m[20221213 21:30:32 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:30:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.00
[32m[20221213 21:30:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:30:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.00
[32m[20221213 21:30:32 @agent_ppo2.py:143][0m Total time:      34.96 min
[32m[20221213 21:30:32 @agent_ppo2.py:145][0m 3411968 total steps have happened
[32m[20221213 21:30:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1666 --------------------------#
[32m[20221213 21:30:32 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:32 @agent_ppo2.py:185][0m |           0.0006 |         172.6411 |           6.8233 |
[32m[20221213 21:30:33 @agent_ppo2.py:185][0m |          -0.0014 |         171.7633 |           6.8418 |
[32m[20221213 21:30:33 @agent_ppo2.py:185][0m |          -0.0008 |         171.5932 |           6.7976 |
[32m[20221213 21:30:33 @agent_ppo2.py:185][0m |          -0.0053 |         170.9688 |           6.8323 |
[32m[20221213 21:30:33 @agent_ppo2.py:185][0m |           0.0051 |         186.6114 |           6.7677 |
[32m[20221213 21:30:33 @agent_ppo2.py:185][0m |           0.0053 |         177.5775 |           6.8292 |
[32m[20221213 21:30:33 @agent_ppo2.py:185][0m |          -0.0065 |         170.3669 |           6.7232 |
[32m[20221213 21:30:33 @agent_ppo2.py:185][0m |          -0.0063 |         170.3093 |           6.7210 |
[32m[20221213 21:30:33 @agent_ppo2.py:185][0m |          -0.0029 |         170.8912 |           6.6990 |
[32m[20221213 21:30:33 @agent_ppo2.py:185][0m |           0.0002 |         174.8680 |           6.6674 |
[32m[20221213 21:30:33 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:30:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.80
[32m[20221213 21:30:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:30:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.00
[32m[20221213 21:30:33 @agent_ppo2.py:143][0m Total time:      34.98 min
[32m[20221213 21:30:33 @agent_ppo2.py:145][0m 3414016 total steps have happened
[32m[20221213 21:30:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1667 --------------------------#
[32m[20221213 21:30:34 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:34 @agent_ppo2.py:185][0m |          -0.0002 |         172.5106 |           6.8677 |
[32m[20221213 21:30:34 @agent_ppo2.py:185][0m |          -0.0070 |         171.0257 |           6.9494 |
[32m[20221213 21:30:34 @agent_ppo2.py:185][0m |          -0.0036 |         170.8968 |           6.9380 |
[32m[20221213 21:30:34 @agent_ppo2.py:185][0m |           0.0026 |         185.0832 |           6.9690 |
[32m[20221213 21:30:34 @agent_ppo2.py:185][0m |          -0.0085 |         169.5289 |           7.0251 |
[32m[20221213 21:30:34 @agent_ppo2.py:185][0m |           0.0036 |         187.3279 |           7.0640 |
[32m[20221213 21:30:34 @agent_ppo2.py:185][0m |          -0.0086 |         168.7866 |           7.0639 |
[32m[20221213 21:30:35 @agent_ppo2.py:185][0m |          -0.0108 |         168.7540 |           7.0966 |
[32m[20221213 21:30:35 @agent_ppo2.py:185][0m |          -0.0111 |         168.6119 |           7.1236 |
[32m[20221213 21:30:35 @agent_ppo2.py:185][0m |          -0.0103 |         168.3579 |           7.1436 |
[32m[20221213 21:30:35 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:30:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.40
[32m[20221213 21:30:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.00
[32m[20221213 21:30:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.00
[32m[20221213 21:30:35 @agent_ppo2.py:143][0m Total time:      35.00 min
[32m[20221213 21:30:35 @agent_ppo2.py:145][0m 3416064 total steps have happened
[32m[20221213 21:30:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1668 --------------------------#
[32m[20221213 21:30:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:30:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:35 @agent_ppo2.py:185][0m |           0.0042 |         178.7986 |           7.1077 |
[32m[20221213 21:30:35 @agent_ppo2.py:185][0m |          -0.0024 |         173.6283 |           7.0934 |
[32m[20221213 21:30:36 @agent_ppo2.py:185][0m |          -0.0052 |         173.1402 |           7.0768 |
[32m[20221213 21:30:36 @agent_ppo2.py:185][0m |          -0.0041 |         175.3428 |           7.0786 |
[32m[20221213 21:30:36 @agent_ppo2.py:185][0m |          -0.0020 |         174.6478 |           7.0982 |
[32m[20221213 21:30:36 @agent_ppo2.py:185][0m |           0.0047 |         186.2698 |           7.1316 |
[32m[20221213 21:30:36 @agent_ppo2.py:185][0m |          -0.0065 |         171.9073 |           7.1395 |
[32m[20221213 21:30:36 @agent_ppo2.py:185][0m |          -0.0102 |         171.8626 |           7.1225 |
[32m[20221213 21:30:36 @agent_ppo2.py:185][0m |          -0.0087 |         171.6265 |           7.1069 |
[32m[20221213 21:30:36 @agent_ppo2.py:185][0m |           0.0049 |         196.8409 |           7.1456 |
[32m[20221213 21:30:36 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:30:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.80
[32m[20221213 21:30:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:30:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:30:36 @agent_ppo2.py:143][0m Total time:      35.03 min
[32m[20221213 21:30:36 @agent_ppo2.py:145][0m 3418112 total steps have happened
[32m[20221213 21:30:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1669 --------------------------#
[32m[20221213 21:30:37 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:37 @agent_ppo2.py:185][0m |          -0.0016 |         174.3087 |           6.9935 |
[32m[20221213 21:30:37 @agent_ppo2.py:185][0m |          -0.0040 |         173.3210 |           6.9707 |
[32m[20221213 21:30:37 @agent_ppo2.py:185][0m |          -0.0033 |         172.7582 |           6.9301 |
[32m[20221213 21:30:37 @agent_ppo2.py:185][0m |          -0.0072 |         172.6381 |           6.9625 |
[32m[20221213 21:30:37 @agent_ppo2.py:185][0m |          -0.0048 |         173.0164 |           6.9718 |
[32m[20221213 21:30:37 @agent_ppo2.py:185][0m |          -0.0009 |         177.4656 |           6.9191 |
[32m[20221213 21:30:37 @agent_ppo2.py:185][0m |          -0.0093 |         171.6004 |           6.9818 |
[32m[20221213 21:30:37 @agent_ppo2.py:185][0m |          -0.0083 |         171.6709 |           6.9237 |
[32m[20221213 21:30:38 @agent_ppo2.py:185][0m |          -0.0101 |         171.2653 |           6.9638 |
[32m[20221213 21:30:38 @agent_ppo2.py:185][0m |          -0.0101 |         171.3248 |           6.9426 |
[32m[20221213 21:30:38 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 21:30:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.80
[32m[20221213 21:30:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.00
[32m[20221213 21:30:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:30:38 @agent_ppo2.py:143][0m Total time:      35.05 min
[32m[20221213 21:30:38 @agent_ppo2.py:145][0m 3420160 total steps have happened
[32m[20221213 21:30:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1670 --------------------------#
[32m[20221213 21:30:38 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:38 @agent_ppo2.py:185][0m |           0.0002 |         174.1011 |           6.7759 |
[32m[20221213 21:30:38 @agent_ppo2.py:185][0m |          -0.0020 |         173.5829 |           6.7334 |
[32m[20221213 21:30:38 @agent_ppo2.py:185][0m |          -0.0038 |         173.1016 |           6.7728 |
[32m[20221213 21:30:39 @agent_ppo2.py:185][0m |          -0.0052 |         172.6695 |           6.7495 |
[32m[20221213 21:30:39 @agent_ppo2.py:185][0m |          -0.0056 |         172.3450 |           6.7484 |
[32m[20221213 21:30:39 @agent_ppo2.py:185][0m |          -0.0081 |         172.4022 |           6.7125 |
[32m[20221213 21:30:39 @agent_ppo2.py:185][0m |          -0.0079 |         172.1437 |           6.7069 |
[32m[20221213 21:30:39 @agent_ppo2.py:185][0m |          -0.0012 |         177.0088 |           6.6873 |
[32m[20221213 21:30:39 @agent_ppo2.py:185][0m |          -0.0065 |         171.6004 |           6.6825 |
[32m[20221213 21:30:39 @agent_ppo2.py:185][0m |          -0.0001 |         179.4332 |           6.7065 |
[32m[20221213 21:30:39 @agent_ppo2.py:130][0m Policy update time: 1.24 s
[32m[20221213 21:30:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.00
[32m[20221213 21:30:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 853.00
[32m[20221213 21:30:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.00
[32m[20221213 21:30:39 @agent_ppo2.py:143][0m Total time:      35.08 min
[32m[20221213 21:30:39 @agent_ppo2.py:145][0m 3422208 total steps have happened
[32m[20221213 21:30:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1671 --------------------------#
[32m[20221213 21:30:40 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:40 @agent_ppo2.py:185][0m |           0.0004 |         178.4718 |           6.8631 |
[32m[20221213 21:30:40 @agent_ppo2.py:185][0m |          -0.0032 |         175.2775 |           6.8463 |
[32m[20221213 21:30:40 @agent_ppo2.py:185][0m |          -0.0030 |         174.2344 |           6.8912 |
[32m[20221213 21:30:40 @agent_ppo2.py:185][0m |          -0.0054 |         172.9219 |           6.9471 |
[32m[20221213 21:30:40 @agent_ppo2.py:185][0m |          -0.0055 |         172.3563 |           6.8764 |
[32m[20221213 21:30:40 @agent_ppo2.py:185][0m |          -0.0069 |         172.3439 |           6.9833 |
[32m[20221213 21:30:40 @agent_ppo2.py:185][0m |          -0.0030 |         178.3811 |           6.9324 |
[32m[20221213 21:30:40 @agent_ppo2.py:185][0m |          -0.0079 |         171.8436 |           6.9685 |
[32m[20221213 21:30:41 @agent_ppo2.py:185][0m |          -0.0099 |         171.7880 |           6.9468 |
[32m[20221213 21:30:41 @agent_ppo2.py:185][0m |          -0.0108 |         171.6109 |           6.9473 |
[32m[20221213 21:30:41 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 21:30:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.60
[32m[20221213 21:30:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:30:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.00
[32m[20221213 21:30:41 @agent_ppo2.py:143][0m Total time:      35.10 min
[32m[20221213 21:30:41 @agent_ppo2.py:145][0m 3424256 total steps have happened
[32m[20221213 21:30:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1672 --------------------------#
[32m[20221213 21:30:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:30:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:41 @agent_ppo2.py:185][0m |          -0.0021 |         174.8889 |           6.5144 |
[32m[20221213 21:30:41 @agent_ppo2.py:185][0m |          -0.0053 |         173.9999 |           6.5742 |
[32m[20221213 21:30:41 @agent_ppo2.py:185][0m |          -0.0067 |         173.2368 |           6.6165 |
[32m[20221213 21:30:42 @agent_ppo2.py:185][0m |          -0.0086 |         172.9023 |           6.6758 |
[32m[20221213 21:30:42 @agent_ppo2.py:185][0m |          -0.0070 |         172.6042 |           6.7212 |
[32m[20221213 21:30:42 @agent_ppo2.py:185][0m |          -0.0079 |         172.3652 |           6.7398 |
[32m[20221213 21:30:42 @agent_ppo2.py:185][0m |          -0.0076 |         172.3047 |           6.7438 |
[32m[20221213 21:30:42 @agent_ppo2.py:185][0m |          -0.0075 |         172.1598 |           6.7843 |
[32m[20221213 21:30:42 @agent_ppo2.py:185][0m |          -0.0064 |         172.1844 |           6.7857 |
[32m[20221213 21:30:42 @agent_ppo2.py:185][0m |          -0.0107 |         171.9480 |           6.8294 |
[32m[20221213 21:30:42 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 21:30:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.60
[32m[20221213 21:30:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.00
[32m[20221213 21:30:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.00
[32m[20221213 21:30:42 @agent_ppo2.py:143][0m Total time:      35.13 min
[32m[20221213 21:30:42 @agent_ppo2.py:145][0m 3426304 total steps have happened
[32m[20221213 21:30:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1673 --------------------------#
[32m[20221213 21:30:42 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:43 @agent_ppo2.py:185][0m |          -0.0017 |         177.8593 |           7.0669 |
[32m[20221213 21:30:43 @agent_ppo2.py:185][0m |          -0.0035 |         172.1786 |           7.1184 |
[32m[20221213 21:30:43 @agent_ppo2.py:185][0m |          -0.0031 |         170.6600 |           7.2002 |
[32m[20221213 21:30:43 @agent_ppo2.py:185][0m |          -0.0055 |         169.3230 |           7.1703 |
[32m[20221213 21:30:43 @agent_ppo2.py:185][0m |           0.0035 |         181.6653 |           7.2054 |
[32m[20221213 21:30:43 @agent_ppo2.py:185][0m |          -0.0067 |         168.0329 |           7.2200 |
[32m[20221213 21:30:43 @agent_ppo2.py:185][0m |          -0.0044 |         167.7738 |           7.3187 |
[32m[20221213 21:30:43 @agent_ppo2.py:185][0m |          -0.0086 |         166.8102 |           7.3110 |
[32m[20221213 21:30:44 @agent_ppo2.py:185][0m |          -0.0082 |         166.1750 |           7.3669 |
[32m[20221213 21:30:44 @agent_ppo2.py:185][0m |          -0.0094 |         165.1772 |           7.3768 |
[32m[20221213 21:30:44 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:30:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.00
[32m[20221213 21:30:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:30:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 21:30:44 @agent_ppo2.py:143][0m Total time:      35.15 min
[32m[20221213 21:30:44 @agent_ppo2.py:145][0m 3428352 total steps have happened
[32m[20221213 21:30:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1674 --------------------------#
[32m[20221213 21:30:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:30:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:44 @agent_ppo2.py:185][0m |           0.0069 |         183.4832 |           7.2512 |
[32m[20221213 21:30:44 @agent_ppo2.py:185][0m |          -0.0027 |         176.5630 |           7.3253 |
[32m[20221213 21:30:44 @agent_ppo2.py:185][0m |          -0.0049 |         175.8806 |           7.3405 |
[32m[20221213 21:30:44 @agent_ppo2.py:185][0m |          -0.0045 |         175.6198 |           7.3796 |
[32m[20221213 21:30:45 @agent_ppo2.py:185][0m |          -0.0046 |         175.4128 |           7.3564 |
[32m[20221213 21:30:45 @agent_ppo2.py:185][0m |          -0.0056 |         175.2098 |           7.3738 |
[32m[20221213 21:30:45 @agent_ppo2.py:185][0m |          -0.0064 |         175.1048 |           7.4249 |
[32m[20221213 21:30:45 @agent_ppo2.py:185][0m |          -0.0073 |         175.0707 |           7.3988 |
[32m[20221213 21:30:45 @agent_ppo2.py:185][0m |          -0.0054 |         174.9942 |           7.4709 |
[32m[20221213 21:30:45 @agent_ppo2.py:185][0m |          -0.0070 |         174.9564 |           7.4358 |
[32m[20221213 21:30:45 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:30:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.40
[32m[20221213 21:30:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:30:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.00
[32m[20221213 21:30:45 @agent_ppo2.py:143][0m Total time:      35.18 min
[32m[20221213 21:30:45 @agent_ppo2.py:145][0m 3430400 total steps have happened
[32m[20221213 21:30:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1675 --------------------------#
[32m[20221213 21:30:45 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:46 @agent_ppo2.py:185][0m |           0.0047 |         181.3896 |           7.7028 |
[32m[20221213 21:30:46 @agent_ppo2.py:185][0m |          -0.0018 |         177.8801 |           7.6805 |
[32m[20221213 21:30:46 @agent_ppo2.py:185][0m |          -0.0054 |         177.5698 |           7.7575 |
[32m[20221213 21:30:46 @agent_ppo2.py:185][0m |          -0.0033 |         177.3782 |           7.6864 |
[32m[20221213 21:30:46 @agent_ppo2.py:185][0m |          -0.0050 |         177.1063 |           7.6970 |
[32m[20221213 21:30:46 @agent_ppo2.py:185][0m |          -0.0059 |         176.9424 |           7.6473 |
[32m[20221213 21:30:46 @agent_ppo2.py:185][0m |          -0.0060 |         176.6098 |           7.6839 |
[32m[20221213 21:30:46 @agent_ppo2.py:185][0m |          -0.0003 |         182.8067 |           7.7009 |
[32m[20221213 21:30:46 @agent_ppo2.py:185][0m |          -0.0049 |         176.4421 |           7.7273 |
[32m[20221213 21:30:47 @agent_ppo2.py:185][0m |          -0.0066 |         176.3076 |           7.7111 |
[32m[20221213 21:30:47 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 21:30:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.80
[32m[20221213 21:30:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.00
[32m[20221213 21:30:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.00
[32m[20221213 21:30:47 @agent_ppo2.py:143][0m Total time:      35.20 min
[32m[20221213 21:30:47 @agent_ppo2.py:145][0m 3432448 total steps have happened
[32m[20221213 21:30:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1676 --------------------------#
[32m[20221213 21:30:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:30:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:47 @agent_ppo2.py:185][0m |           0.0064 |         184.1461 |           7.6586 |
[32m[20221213 21:30:47 @agent_ppo2.py:185][0m |          -0.0027 |         178.7681 |           7.6212 |
[32m[20221213 21:30:47 @agent_ppo2.py:185][0m |          -0.0059 |         178.3183 |           7.6238 |
[32m[20221213 21:30:47 @agent_ppo2.py:185][0m |          -0.0088 |         177.8090 |           7.6013 |
[32m[20221213 21:30:48 @agent_ppo2.py:185][0m |          -0.0062 |         177.3424 |           7.5775 |
[32m[20221213 21:30:48 @agent_ppo2.py:185][0m |          -0.0075 |         177.2331 |           7.5603 |
[32m[20221213 21:30:48 @agent_ppo2.py:185][0m |          -0.0081 |         177.0467 |           7.5925 |
[32m[20221213 21:30:48 @agent_ppo2.py:185][0m |          -0.0079 |         177.0568 |           7.5645 |
[32m[20221213 21:30:48 @agent_ppo2.py:185][0m |          -0.0093 |         176.7888 |           7.5755 |
[32m[20221213 21:30:48 @agent_ppo2.py:185][0m |          -0.0095 |         176.9573 |           7.5409 |
[32m[20221213 21:30:48 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 21:30:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.40
[32m[20221213 21:30:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.00
[32m[20221213 21:30:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.00
[32m[20221213 21:30:48 @agent_ppo2.py:143][0m Total time:      35.22 min
[32m[20221213 21:30:48 @agent_ppo2.py:145][0m 3434496 total steps have happened
[32m[20221213 21:30:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1677 --------------------------#
[32m[20221213 21:30:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:30:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:49 @agent_ppo2.py:185][0m |          -0.0003 |         179.1152 |           7.2590 |
[32m[20221213 21:30:49 @agent_ppo2.py:185][0m |          -0.0035 |         177.2745 |           7.2626 |
[32m[20221213 21:30:49 @agent_ppo2.py:185][0m |          -0.0036 |         176.7182 |           7.3368 |
[32m[20221213 21:30:49 @agent_ppo2.py:185][0m |          -0.0052 |         175.8793 |           7.3812 |
[32m[20221213 21:30:49 @agent_ppo2.py:185][0m |          -0.0049 |         175.6015 |           7.4004 |
[32m[20221213 21:30:49 @agent_ppo2.py:185][0m |          -0.0072 |         175.2431 |           7.4137 |
[32m[20221213 21:30:49 @agent_ppo2.py:185][0m |          -0.0051 |         175.0434 |           7.4481 |
[32m[20221213 21:30:49 @agent_ppo2.py:185][0m |          -0.0076 |         174.8517 |           7.4615 |
[32m[20221213 21:30:49 @agent_ppo2.py:185][0m |          -0.0063 |         174.7373 |           7.5082 |
[32m[20221213 21:30:50 @agent_ppo2.py:185][0m |           0.0031 |         188.8049 |           7.4674 |
[32m[20221213 21:30:50 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:30:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.40
[32m[20221213 21:30:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.00
[32m[20221213 21:30:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.00
[32m[20221213 21:30:50 @agent_ppo2.py:143][0m Total time:      35.25 min
[32m[20221213 21:30:50 @agent_ppo2.py:145][0m 3436544 total steps have happened
[32m[20221213 21:30:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1678 --------------------------#
[32m[20221213 21:30:50 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:30:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:50 @agent_ppo2.py:185][0m |          -0.0018 |         176.0667 |           7.9947 |
[32m[20221213 21:30:50 @agent_ppo2.py:185][0m |          -0.0045 |         175.4882 |           7.9970 |
[32m[20221213 21:30:50 @agent_ppo2.py:185][0m |           0.0055 |         195.4816 |           7.9934 |
[32m[20221213 21:30:50 @agent_ppo2.py:185][0m |          -0.0068 |         174.9233 |           8.0211 |
[32m[20221213 21:30:50 @agent_ppo2.py:185][0m |           0.0090 |         191.6944 |           8.0318 |
[32m[20221213 21:30:51 @agent_ppo2.py:185][0m |          -0.0062 |         174.3701 |           8.0529 |
[32m[20221213 21:30:51 @agent_ppo2.py:185][0m |          -0.0083 |         174.3112 |           8.0646 |
[32m[20221213 21:30:51 @agent_ppo2.py:185][0m |          -0.0083 |         174.1203 |           8.0679 |
[32m[20221213 21:30:51 @agent_ppo2.py:185][0m |          -0.0015 |         177.4153 |           8.1077 |
[32m[20221213 21:30:51 @agent_ppo2.py:185][0m |          -0.0093 |         173.9821 |           8.0879 |
[32m[20221213 21:30:51 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 21:30:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.20
[32m[20221213 21:30:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:30:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.00
[32m[20221213 21:30:51 @agent_ppo2.py:143][0m Total time:      35.27 min
[32m[20221213 21:30:51 @agent_ppo2.py:145][0m 3438592 total steps have happened
[32m[20221213 21:30:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1679 --------------------------#
[32m[20221213 21:30:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:30:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:51 @agent_ppo2.py:185][0m |           0.0010 |         179.5022 |           7.9772 |
[32m[20221213 21:30:51 @agent_ppo2.py:185][0m |          -0.0010 |         180.7699 |           8.0403 |
[32m[20221213 21:30:52 @agent_ppo2.py:185][0m |          -0.0073 |         177.8620 |           8.0216 |
[32m[20221213 21:30:52 @agent_ppo2.py:185][0m |          -0.0072 |         178.4280 |           8.0565 |
[32m[20221213 21:30:52 @agent_ppo2.py:185][0m |           0.0008 |         184.7141 |           8.0406 |
[32m[20221213 21:30:52 @agent_ppo2.py:185][0m |          -0.0088 |         177.2505 |           8.0796 |
[32m[20221213 21:30:52 @agent_ppo2.py:185][0m |          -0.0111 |         176.8940 |           8.0448 |
[32m[20221213 21:30:52 @agent_ppo2.py:185][0m |           0.0012 |         199.9388 |           8.0337 |
[32m[20221213 21:30:52 @agent_ppo2.py:185][0m |           0.0030 |         199.1428 |           8.0627 |
[32m[20221213 21:30:52 @agent_ppo2.py:185][0m |          -0.0115 |         176.6391 |           8.1100 |
[32m[20221213 21:30:52 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:30:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.80
[32m[20221213 21:30:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.00
[32m[20221213 21:30:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 21:30:52 @agent_ppo2.py:143][0m Total time:      35.30 min
[32m[20221213 21:30:52 @agent_ppo2.py:145][0m 3440640 total steps have happened
[32m[20221213 21:30:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1680 --------------------------#
[32m[20221213 21:30:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:30:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:53 @agent_ppo2.py:185][0m |          -0.0007 |         175.7560 |           7.7494 |
[32m[20221213 21:30:53 @agent_ppo2.py:185][0m |           0.0024 |         177.6536 |           7.7835 |
[32m[20221213 21:30:53 @agent_ppo2.py:185][0m |          -0.0065 |         175.0229 |           7.7838 |
[32m[20221213 21:30:53 @agent_ppo2.py:185][0m |          -0.0076 |         175.0258 |           7.8162 |
[32m[20221213 21:30:53 @agent_ppo2.py:185][0m |          -0.0050 |         175.6655 |           7.8418 |
[32m[20221213 21:30:53 @agent_ppo2.py:185][0m |          -0.0020 |         177.9290 |           7.8699 |
[32m[20221213 21:30:53 @agent_ppo2.py:185][0m |          -0.0092 |         174.4415 |           7.8787 |
[32m[20221213 21:30:53 @agent_ppo2.py:185][0m |          -0.0091 |         174.1884 |           7.9197 |
[32m[20221213 21:30:53 @agent_ppo2.py:185][0m |          -0.0091 |         174.0295 |           7.9358 |
[32m[20221213 21:30:54 @agent_ppo2.py:185][0m |          -0.0093 |         173.8446 |           7.9103 |
[32m[20221213 21:30:54 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:30:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.20
[32m[20221213 21:30:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:30:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 21:30:54 @agent_ppo2.py:143][0m Total time:      35.32 min
[32m[20221213 21:30:54 @agent_ppo2.py:145][0m 3442688 total steps have happened
[32m[20221213 21:30:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1681 --------------------------#
[32m[20221213 21:30:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:30:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:54 @agent_ppo2.py:185][0m |           0.0028 |         178.0576 |           8.4227 |
[32m[20221213 21:30:54 @agent_ppo2.py:185][0m |          -0.0057 |         174.4854 |           8.3758 |
[32m[20221213 21:30:54 @agent_ppo2.py:185][0m |           0.0085 |         185.5422 |           8.4090 |
[32m[20221213 21:30:54 @agent_ppo2.py:185][0m |          -0.0060 |         173.6850 |           8.3784 |
[32m[20221213 21:30:54 @agent_ppo2.py:185][0m |          -0.0059 |         173.2387 |           8.4195 |
[32m[20221213 21:30:54 @agent_ppo2.py:185][0m |           0.0011 |         187.7313 |           8.3708 |
[32m[20221213 21:30:55 @agent_ppo2.py:185][0m |          -0.0044 |         172.7250 |           8.3791 |
[32m[20221213 21:30:55 @agent_ppo2.py:185][0m |           0.0002 |         184.1674 |           8.3872 |
[32m[20221213 21:30:55 @agent_ppo2.py:185][0m |           0.0057 |         188.3646 |           8.4167 |
[32m[20221213 21:30:55 @agent_ppo2.py:185][0m |          -0.0086 |         172.1678 |           8.4537 |
[32m[20221213 21:30:55 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:30:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.20
[32m[20221213 21:30:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:30:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:30:55 @agent_ppo2.py:143][0m Total time:      35.34 min
[32m[20221213 21:30:55 @agent_ppo2.py:145][0m 3444736 total steps have happened
[32m[20221213 21:30:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1682 --------------------------#
[32m[20221213 21:30:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:30:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:55 @agent_ppo2.py:185][0m |          -0.0015 |         179.5826 |           8.0033 |
[32m[20221213 21:30:55 @agent_ppo2.py:185][0m |          -0.0047 |         178.8302 |           8.0427 |
[32m[20221213 21:30:55 @agent_ppo2.py:185][0m |          -0.0056 |         177.8692 |           8.0439 |
[32m[20221213 21:30:56 @agent_ppo2.py:185][0m |          -0.0052 |         177.3225 |           8.0376 |
[32m[20221213 21:30:56 @agent_ppo2.py:185][0m |          -0.0056 |         177.5785 |           8.0386 |
[32m[20221213 21:30:56 @agent_ppo2.py:185][0m |           0.0012 |         186.9870 |           8.0850 |
[32m[20221213 21:30:56 @agent_ppo2.py:185][0m |          -0.0064 |         176.2639 |           8.0670 |
[32m[20221213 21:30:56 @agent_ppo2.py:185][0m |          -0.0071 |         176.1119 |           8.0376 |
[32m[20221213 21:30:56 @agent_ppo2.py:185][0m |          -0.0085 |         175.8679 |           8.0812 |
[32m[20221213 21:30:56 @agent_ppo2.py:185][0m |          -0.0101 |         175.6224 |           8.0373 |
[32m[20221213 21:30:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:30:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.80
[32m[20221213 21:30:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.00
[32m[20221213 21:30:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 868.00
[32m[20221213 21:30:56 @agent_ppo2.py:143][0m Total time:      35.36 min
[32m[20221213 21:30:56 @agent_ppo2.py:145][0m 3446784 total steps have happened
[32m[20221213 21:30:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1683 --------------------------#
[32m[20221213 21:30:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:30:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:56 @agent_ppo2.py:185][0m |           0.0027 |         182.6819 |           7.5347 |
[32m[20221213 21:30:57 @agent_ppo2.py:185][0m |          -0.0007 |         179.4696 |           7.4571 |
[32m[20221213 21:30:57 @agent_ppo2.py:185][0m |           0.0023 |         186.9886 |           7.4626 |
[32m[20221213 21:30:57 @agent_ppo2.py:185][0m |          -0.0056 |         178.1653 |           7.5841 |
[32m[20221213 21:30:57 @agent_ppo2.py:185][0m |          -0.0092 |         178.0717 |           7.5332 |
[32m[20221213 21:30:57 @agent_ppo2.py:185][0m |          -0.0081 |         177.7545 |           7.5133 |
[32m[20221213 21:30:57 @agent_ppo2.py:185][0m |           0.0001 |         187.0786 |           7.4842 |
[32m[20221213 21:30:57 @agent_ppo2.py:185][0m |          -0.0079 |         177.6101 |           7.5102 |
[32m[20221213 21:30:57 @agent_ppo2.py:185][0m |          -0.0077 |         177.3471 |           7.4694 |
[32m[20221213 21:30:57 @agent_ppo2.py:185][0m |          -0.0103 |         177.2537 |           7.4944 |
[32m[20221213 21:30:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:30:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.40
[32m[20221213 21:30:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:30:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:30:57 @agent_ppo2.py:143][0m Total time:      35.38 min
[32m[20221213 21:30:57 @agent_ppo2.py:145][0m 3448832 total steps have happened
[32m[20221213 21:30:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1684 --------------------------#
[32m[20221213 21:30:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:30:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:58 @agent_ppo2.py:185][0m |          -0.0006 |         181.0137 |           7.7406 |
[32m[20221213 21:30:58 @agent_ppo2.py:185][0m |          -0.0048 |         179.7361 |           7.6457 |
[32m[20221213 21:30:58 @agent_ppo2.py:185][0m |           0.0049 |         185.9426 |           7.6540 |
[32m[20221213 21:30:58 @agent_ppo2.py:185][0m |          -0.0036 |         179.0034 |           7.6333 |
[32m[20221213 21:30:58 @agent_ppo2.py:185][0m |          -0.0054 |         178.7259 |           7.6391 |
[32m[20221213 21:30:58 @agent_ppo2.py:185][0m |          -0.0059 |         178.3630 |           7.6487 |
[32m[20221213 21:30:58 @agent_ppo2.py:185][0m |          -0.0075 |         178.4357 |           7.6482 |
[32m[20221213 21:30:58 @agent_ppo2.py:185][0m |          -0.0068 |         178.1391 |           7.6367 |
[32m[20221213 21:30:58 @agent_ppo2.py:185][0m |          -0.0008 |         179.9803 |           7.6872 |
[32m[20221213 21:30:58 @agent_ppo2.py:185][0m |          -0.0069 |         178.0274 |           7.7078 |
[32m[20221213 21:30:58 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:30:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.60
[32m[20221213 21:30:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.00
[32m[20221213 21:30:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.00
[32m[20221213 21:30:59 @agent_ppo2.py:143][0m Total time:      35.40 min
[32m[20221213 21:30:59 @agent_ppo2.py:145][0m 3450880 total steps have happened
[32m[20221213 21:30:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1685 --------------------------#
[32m[20221213 21:30:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:30:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:30:59 @agent_ppo2.py:185][0m |           0.0120 |         185.5753 |           7.9246 |
[32m[20221213 21:30:59 @agent_ppo2.py:185][0m |          -0.0020 |         176.2402 |           7.9287 |
[32m[20221213 21:30:59 @agent_ppo2.py:185][0m |          -0.0032 |         175.6886 |           7.9705 |
[32m[20221213 21:30:59 @agent_ppo2.py:185][0m |          -0.0054 |         175.3414 |           8.0096 |
[32m[20221213 21:30:59 @agent_ppo2.py:185][0m |          -0.0084 |         175.2139 |           8.0233 |
[32m[20221213 21:30:59 @agent_ppo2.py:185][0m |          -0.0055 |         175.0171 |           7.9935 |
[32m[20221213 21:30:59 @agent_ppo2.py:185][0m |           0.0034 |         185.9004 |           8.0835 |
[32m[20221213 21:30:59 @agent_ppo2.py:185][0m |          -0.0067 |         174.6456 |           8.0223 |
[32m[20221213 21:31:00 @agent_ppo2.py:185][0m |          -0.0075 |         174.5755 |           8.0472 |
[32m[20221213 21:31:00 @agent_ppo2.py:185][0m |          -0.0065 |         174.6675 |           8.0762 |
[32m[20221213 21:31:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:31:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 784.00
[32m[20221213 21:31:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.00
[32m[20221213 21:31:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.00
[32m[20221213 21:31:00 @agent_ppo2.py:143][0m Total time:      35.42 min
[32m[20221213 21:31:00 @agent_ppo2.py:145][0m 3452928 total steps have happened
[32m[20221213 21:31:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1686 --------------------------#
[32m[20221213 21:31:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:00 @agent_ppo2.py:185][0m |           0.0010 |         173.1826 |           7.8595 |
[32m[20221213 21:31:00 @agent_ppo2.py:185][0m |          -0.0010 |         172.8863 |           7.8490 |
[32m[20221213 21:31:00 @agent_ppo2.py:185][0m |          -0.0046 |         172.3319 |           7.9275 |
[32m[20221213 21:31:00 @agent_ppo2.py:185][0m |           0.0003 |         176.2139 |           7.8785 |
[32m[20221213 21:31:00 @agent_ppo2.py:185][0m |          -0.0063 |         172.0653 |           7.9372 |
[32m[20221213 21:31:01 @agent_ppo2.py:185][0m |           0.0017 |         176.7509 |           7.9347 |
[32m[20221213 21:31:01 @agent_ppo2.py:185][0m |          -0.0061 |         171.9038 |           7.9447 |
[32m[20221213 21:31:01 @agent_ppo2.py:185][0m |           0.0034 |         175.7882 |           7.9332 |
[32m[20221213 21:31:01 @agent_ppo2.py:185][0m |          -0.0061 |         171.7695 |           7.9540 |
[32m[20221213 21:31:01 @agent_ppo2.py:185][0m |          -0.0060 |         171.5450 |           7.9793 |
[32m[20221213 21:31:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:31:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.80
[32m[20221213 21:31:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:31:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.00
[32m[20221213 21:31:01 @agent_ppo2.py:143][0m Total time:      35.44 min
[32m[20221213 21:31:01 @agent_ppo2.py:145][0m 3454976 total steps have happened
[32m[20221213 21:31:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1687 --------------------------#
[32m[20221213 21:31:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:31:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:01 @agent_ppo2.py:185][0m |           0.0022 |         177.0733 |           8.3759 |
[32m[20221213 21:31:01 @agent_ppo2.py:185][0m |          -0.0017 |         176.4513 |           8.4056 |
[32m[20221213 21:31:01 @agent_ppo2.py:185][0m |          -0.0068 |         175.0495 |           8.4038 |
[32m[20221213 21:31:02 @agent_ppo2.py:185][0m |          -0.0070 |         174.7790 |           8.3860 |
[32m[20221213 21:31:02 @agent_ppo2.py:185][0m |          -0.0070 |         174.5168 |           8.3209 |
[32m[20221213 21:31:02 @agent_ppo2.py:185][0m |          -0.0077 |         174.3695 |           8.3598 |
[32m[20221213 21:31:02 @agent_ppo2.py:185][0m |          -0.0013 |         176.9771 |           8.3548 |
[32m[20221213 21:31:02 @agent_ppo2.py:185][0m |          -0.0006 |         186.8408 |           8.3572 |
[32m[20221213 21:31:02 @agent_ppo2.py:185][0m |          -0.0094 |         173.8731 |           8.3512 |
[32m[20221213 21:31:02 @agent_ppo2.py:185][0m |          -0.0092 |         173.6548 |           8.3517 |
[32m[20221213 21:31:02 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:31:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.60
[32m[20221213 21:31:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:31:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:31:02 @agent_ppo2.py:143][0m Total time:      35.46 min
[32m[20221213 21:31:02 @agent_ppo2.py:145][0m 3457024 total steps have happened
[32m[20221213 21:31:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1688 --------------------------#
[32m[20221213 21:31:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:03 @agent_ppo2.py:185][0m |           0.0088 |         189.1113 |           7.8260 |
[32m[20221213 21:31:03 @agent_ppo2.py:185][0m |          -0.0017 |         178.8461 |           7.7783 |
[32m[20221213 21:31:03 @agent_ppo2.py:185][0m |          -0.0026 |         179.8675 |           7.8193 |
[32m[20221213 21:31:03 @agent_ppo2.py:185][0m |          -0.0096 |         176.1572 |           7.8142 |
[32m[20221213 21:31:03 @agent_ppo2.py:185][0m |          -0.0115 |         175.7814 |           7.8438 |
[32m[20221213 21:31:03 @agent_ppo2.py:185][0m |          -0.0094 |         175.5834 |           7.8590 |
[32m[20221213 21:31:03 @agent_ppo2.py:185][0m |          -0.0116 |         175.0944 |           7.8248 |
[32m[20221213 21:31:03 @agent_ppo2.py:185][0m |          -0.0064 |         177.4240 |           7.8352 |
[32m[20221213 21:31:03 @agent_ppo2.py:185][0m |          -0.0099 |         174.8134 |           7.8283 |
[32m[20221213 21:31:04 @agent_ppo2.py:185][0m |          -0.0130 |         174.4869 |           7.8594 |
[32m[20221213 21:31:04 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221213 21:31:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.20
[32m[20221213 21:31:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:31:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 21:31:04 @agent_ppo2.py:143][0m Total time:      35.48 min
[32m[20221213 21:31:04 @agent_ppo2.py:145][0m 3459072 total steps have happened
[32m[20221213 21:31:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1689 --------------------------#
[32m[20221213 21:31:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:04 @agent_ppo2.py:185][0m |           0.0109 |         196.9560 |           7.7229 |
[32m[20221213 21:31:04 @agent_ppo2.py:185][0m |          -0.0035 |         176.9493 |           7.8172 |
[32m[20221213 21:31:04 @agent_ppo2.py:185][0m |          -0.0028 |         176.3288 |           7.8570 |
[32m[20221213 21:31:04 @agent_ppo2.py:185][0m |           0.0060 |         196.6245 |           7.8848 |
[32m[20221213 21:31:04 @agent_ppo2.py:185][0m |          -0.0067 |         175.8336 |           7.8985 |
[32m[20221213 21:31:05 @agent_ppo2.py:185][0m |          -0.0060 |         175.4408 |           7.8904 |
[32m[20221213 21:31:05 @agent_ppo2.py:185][0m |          -0.0100 |         175.3986 |           7.9109 |
[32m[20221213 21:31:05 @agent_ppo2.py:185][0m |           0.0001 |         186.5689 |           7.9692 |
[32m[20221213 21:31:05 @agent_ppo2.py:185][0m |           0.0007 |         187.7412 |           8.0464 |
[32m[20221213 21:31:05 @agent_ppo2.py:185][0m |          -0.0083 |         174.8384 |           8.0296 |
[32m[20221213 21:31:05 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:31:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.60
[32m[20221213 21:31:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 851.00
[32m[20221213 21:31:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.00
[32m[20221213 21:31:05 @agent_ppo2.py:143][0m Total time:      35.50 min
[32m[20221213 21:31:05 @agent_ppo2.py:145][0m 3461120 total steps have happened
[32m[20221213 21:31:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1690 --------------------------#
[32m[20221213 21:31:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:31:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:05 @agent_ppo2.py:185][0m |          -0.0025 |         177.6350 |           7.8047 |
[32m[20221213 21:31:05 @agent_ppo2.py:185][0m |          -0.0055 |         176.7964 |           7.7814 |
[32m[20221213 21:31:06 @agent_ppo2.py:185][0m |          -0.0064 |         176.3744 |           7.8035 |
[32m[20221213 21:31:06 @agent_ppo2.py:185][0m |           0.0036 |         194.5175 |           7.8140 |
[32m[20221213 21:31:06 @agent_ppo2.py:185][0m |          -0.0071 |         175.7256 |           7.8742 |
[32m[20221213 21:31:06 @agent_ppo2.py:185][0m |          -0.0099 |         175.6070 |           7.8523 |
[32m[20221213 21:31:06 @agent_ppo2.py:185][0m |          -0.0110 |         175.1879 |           7.8815 |
[32m[20221213 21:31:06 @agent_ppo2.py:185][0m |          -0.0118 |         175.2035 |           7.9176 |
[32m[20221213 21:31:06 @agent_ppo2.py:185][0m |          -0.0080 |         174.9474 |           7.8988 |
[32m[20221213 21:31:06 @agent_ppo2.py:185][0m |          -0.0111 |         174.9074 |           7.9001 |
[32m[20221213 21:31:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:31:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.80
[32m[20221213 21:31:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:31:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 875.00
[32m[20221213 21:31:06 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 875.00
[32m[20221213 21:31:06 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 875.00
[32m[20221213 21:31:06 @agent_ppo2.py:143][0m Total time:      35.53 min
[32m[20221213 21:31:06 @agent_ppo2.py:145][0m 3463168 total steps have happened
[32m[20221213 21:31:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1691 --------------------------#
[32m[20221213 21:31:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:07 @agent_ppo2.py:185][0m |          -0.0002 |         178.5435 |           7.9045 |
[32m[20221213 21:31:07 @agent_ppo2.py:185][0m |          -0.0058 |         175.5496 |           7.9215 |
[32m[20221213 21:31:07 @agent_ppo2.py:185][0m |          -0.0054 |         174.1494 |           7.8611 |
[32m[20221213 21:31:07 @agent_ppo2.py:185][0m |          -0.0055 |         173.1798 |           7.9360 |
[32m[20221213 21:31:07 @agent_ppo2.py:185][0m |          -0.0082 |         171.1857 |           7.8987 |
[32m[20221213 21:31:07 @agent_ppo2.py:185][0m |          -0.0085 |         170.3495 |           7.8743 |
[32m[20221213 21:31:07 @agent_ppo2.py:185][0m |          -0.0084 |         169.0865 |           7.8667 |
[32m[20221213 21:31:07 @agent_ppo2.py:185][0m |          -0.0103 |         167.8861 |           7.8614 |
[32m[20221213 21:31:07 @agent_ppo2.py:185][0m |          -0.0056 |         168.6231 |           7.8639 |
[32m[20221213 21:31:07 @agent_ppo2.py:185][0m |          -0.0110 |         166.3588 |           7.8527 |
[32m[20221213 21:31:07 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:31:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.80
[32m[20221213 21:31:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.00
[32m[20221213 21:31:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:31:08 @agent_ppo2.py:143][0m Total time:      35.55 min
[32m[20221213 21:31:08 @agent_ppo2.py:145][0m 3465216 total steps have happened
[32m[20221213 21:31:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1692 --------------------------#
[32m[20221213 21:31:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:08 @agent_ppo2.py:185][0m |          -0.0011 |         179.9776 |           8.2951 |
[32m[20221213 21:31:08 @agent_ppo2.py:185][0m |           0.0063 |         190.1624 |           8.2569 |
[32m[20221213 21:31:08 @agent_ppo2.py:185][0m |          -0.0046 |         177.8858 |           8.2917 |
[32m[20221213 21:31:08 @agent_ppo2.py:185][0m |          -0.0040 |         178.2918 |           8.2459 |
[32m[20221213 21:31:08 @agent_ppo2.py:185][0m |          -0.0081 |         176.9686 |           8.2142 |
[32m[20221213 21:31:08 @agent_ppo2.py:185][0m |          -0.0094 |         176.9610 |           8.2127 |
[32m[20221213 21:31:08 @agent_ppo2.py:185][0m |          -0.0094 |         176.5477 |           8.2016 |
[32m[20221213 21:31:08 @agent_ppo2.py:185][0m |          -0.0109 |         176.2752 |           8.1784 |
[32m[20221213 21:31:09 @agent_ppo2.py:185][0m |          -0.0083 |         176.8433 |           8.1952 |
[32m[20221213 21:31:09 @agent_ppo2.py:185][0m |          -0.0105 |         176.0872 |           8.1967 |
[32m[20221213 21:31:09 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:31:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.40
[32m[20221213 21:31:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.00
[32m[20221213 21:31:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:31:09 @agent_ppo2.py:143][0m Total time:      35.57 min
[32m[20221213 21:31:09 @agent_ppo2.py:145][0m 3467264 total steps have happened
[32m[20221213 21:31:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1693 --------------------------#
[32m[20221213 21:31:09 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:31:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:09 @agent_ppo2.py:185][0m |           0.0140 |         202.1695 |           7.9996 |
[32m[20221213 21:31:09 @agent_ppo2.py:185][0m |          -0.0022 |         178.4072 |           8.1883 |
[32m[20221213 21:31:09 @agent_ppo2.py:185][0m |          -0.0076 |         177.5719 |           8.0614 |
[32m[20221213 21:31:09 @agent_ppo2.py:185][0m |          -0.0024 |         179.4018 |           8.0949 |
[32m[20221213 21:31:09 @agent_ppo2.py:185][0m |          -0.0085 |         176.8622 |           8.1177 |
[32m[20221213 21:31:09 @agent_ppo2.py:185][0m |          -0.0103 |         176.6884 |           8.1089 |
[32m[20221213 21:31:10 @agent_ppo2.py:185][0m |          -0.0083 |         176.4268 |           8.0892 |
[32m[20221213 21:31:10 @agent_ppo2.py:185][0m |          -0.0015 |         185.4584 |           8.1485 |
[32m[20221213 21:31:10 @agent_ppo2.py:185][0m |          -0.0105 |         176.0437 |           8.1179 |
[32m[20221213 21:31:10 @agent_ppo2.py:185][0m |          -0.0113 |         175.7890 |           8.1652 |
[32m[20221213 21:31:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:31:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.20
[32m[20221213 21:31:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:31:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 870.00
[32m[20221213 21:31:10 @agent_ppo2.py:143][0m Total time:      35.59 min
[32m[20221213 21:31:10 @agent_ppo2.py:145][0m 3469312 total steps have happened
[32m[20221213 21:31:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1694 --------------------------#
[32m[20221213 21:31:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:10 @agent_ppo2.py:185][0m |           0.0115 |         197.7224 |           7.7799 |
[32m[20221213 21:31:10 @agent_ppo2.py:185][0m |          -0.0038 |         176.2598 |           7.8903 |
[32m[20221213 21:31:10 @agent_ppo2.py:185][0m |          -0.0012 |         175.8450 |           7.7930 |
[32m[20221213 21:31:11 @agent_ppo2.py:185][0m |          -0.0040 |         175.6496 |           7.8151 |
[32m[20221213 21:31:11 @agent_ppo2.py:185][0m |          -0.0055 |         175.3788 |           7.8910 |
[32m[20221213 21:31:11 @agent_ppo2.py:185][0m |           0.0060 |         187.9224 |           7.8977 |
[32m[20221213 21:31:11 @agent_ppo2.py:185][0m |          -0.0042 |         175.0819 |           8.0020 |
[32m[20221213 21:31:11 @agent_ppo2.py:185][0m |          -0.0058 |         174.9599 |           7.9954 |
[32m[20221213 21:31:11 @agent_ppo2.py:185][0m |          -0.0018 |         178.1337 |           8.0066 |
[32m[20221213 21:31:11 @agent_ppo2.py:185][0m |          -0.0056 |         174.9862 |           8.0978 |
[32m[20221213 21:31:11 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:31:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.40
[32m[20221213 21:31:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.00
[32m[20221213 21:31:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.00
[32m[20221213 21:31:11 @agent_ppo2.py:143][0m Total time:      35.61 min
[32m[20221213 21:31:11 @agent_ppo2.py:145][0m 3471360 total steps have happened
[32m[20221213 21:31:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1695 --------------------------#
[32m[20221213 21:31:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:11 @agent_ppo2.py:185][0m |           0.0020 |         175.5693 |           8.2254 |
[32m[20221213 21:31:12 @agent_ppo2.py:185][0m |          -0.0065 |         172.9362 |           8.2186 |
[32m[20221213 21:31:12 @agent_ppo2.py:185][0m |          -0.0071 |         172.4564 |           8.2290 |
[32m[20221213 21:31:12 @agent_ppo2.py:185][0m |          -0.0079 |         172.0211 |           8.2501 |
[32m[20221213 21:31:12 @agent_ppo2.py:185][0m |          -0.0093 |         171.7569 |           8.2836 |
[32m[20221213 21:31:12 @agent_ppo2.py:185][0m |          -0.0096 |         171.4207 |           8.3016 |
[32m[20221213 21:31:12 @agent_ppo2.py:185][0m |          -0.0088 |         171.1941 |           8.2825 |
[32m[20221213 21:31:12 @agent_ppo2.py:185][0m |          -0.0103 |         171.0129 |           8.2801 |
[32m[20221213 21:31:12 @agent_ppo2.py:185][0m |          -0.0013 |         187.0711 |           8.3058 |
[32m[20221213 21:31:12 @agent_ppo2.py:185][0m |          -0.0120 |         170.6992 |           8.3221 |
[32m[20221213 21:31:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:31:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.40
[32m[20221213 21:31:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.00
[32m[20221213 21:31:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.00
[32m[20221213 21:31:12 @agent_ppo2.py:143][0m Total time:      35.63 min
[32m[20221213 21:31:12 @agent_ppo2.py:145][0m 3473408 total steps have happened
[32m[20221213 21:31:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1696 --------------------------#
[32m[20221213 21:31:12 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:31:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:13 @agent_ppo2.py:185][0m |          -0.0001 |         174.2837 |           8.1320 |
[32m[20221213 21:31:13 @agent_ppo2.py:185][0m |          -0.0029 |         173.6430 |           8.0539 |
[32m[20221213 21:31:13 @agent_ppo2.py:185][0m |           0.0173 |         191.5107 |           8.0729 |
[32m[20221213 21:31:13 @agent_ppo2.py:185][0m |          -0.0023 |         172.9964 |           8.0051 |
[32m[20221213 21:31:13 @agent_ppo2.py:185][0m |          -0.0046 |         172.8167 |           8.0109 |
[32m[20221213 21:31:13 @agent_ppo2.py:185][0m |          -0.0040 |         172.9678 |           8.0074 |
[32m[20221213 21:31:13 @agent_ppo2.py:185][0m |          -0.0022 |         177.2554 |           8.0315 |
[32m[20221213 21:31:13 @agent_ppo2.py:185][0m |          -0.0069 |         172.0914 |           7.9906 |
[32m[20221213 21:31:13 @agent_ppo2.py:185][0m |          -0.0086 |         172.0585 |           7.9813 |
[32m[20221213 21:31:13 @agent_ppo2.py:185][0m |          -0.0096 |         171.8084 |           7.9704 |
[32m[20221213 21:31:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:31:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.60
[32m[20221213 21:31:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:31:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:31:14 @agent_ppo2.py:143][0m Total time:      35.65 min
[32m[20221213 21:31:14 @agent_ppo2.py:145][0m 3475456 total steps have happened
[32m[20221213 21:31:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1697 --------------------------#
[32m[20221213 21:31:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:14 @agent_ppo2.py:185][0m |           0.0010 |         173.8235 |           7.7999 |
[32m[20221213 21:31:14 @agent_ppo2.py:185][0m |          -0.0033 |         173.3608 |           7.7649 |
[32m[20221213 21:31:14 @agent_ppo2.py:185][0m |           0.0008 |         175.6674 |           7.7972 |
[32m[20221213 21:31:14 @agent_ppo2.py:185][0m |          -0.0028 |         173.1375 |           7.7524 |
[32m[20221213 21:31:14 @agent_ppo2.py:185][0m |          -0.0053 |         172.8477 |           7.7716 |
[32m[20221213 21:31:14 @agent_ppo2.py:185][0m |          -0.0063 |         172.6476 |           7.8057 |
[32m[20221213 21:31:14 @agent_ppo2.py:185][0m |          -0.0053 |         172.4879 |           7.7680 |
[32m[20221213 21:31:15 @agent_ppo2.py:185][0m |          -0.0065 |         172.4196 |           7.7847 |
[32m[20221213 21:31:15 @agent_ppo2.py:185][0m |          -0.0088 |         172.2593 |           7.7508 |
[32m[20221213 21:31:15 @agent_ppo2.py:185][0m |          -0.0074 |         172.2566 |           7.7948 |
[32m[20221213 21:31:15 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:31:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.20
[32m[20221213 21:31:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.00
[32m[20221213 21:31:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.00
[32m[20221213 21:31:15 @agent_ppo2.py:143][0m Total time:      35.67 min
[32m[20221213 21:31:15 @agent_ppo2.py:145][0m 3477504 total steps have happened
[32m[20221213 21:31:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1698 --------------------------#
[32m[20221213 21:31:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:15 @agent_ppo2.py:185][0m |          -0.0025 |         174.6777 |           7.8049 |
[32m[20221213 21:31:15 @agent_ppo2.py:185][0m |          -0.0051 |         173.9377 |           7.6874 |
[32m[20221213 21:31:15 @agent_ppo2.py:185][0m |           0.0071 |         198.6912 |           7.6822 |
[32m[20221213 21:31:15 @agent_ppo2.py:185][0m |          -0.0082 |         173.1775 |           7.7114 |
[32m[20221213 21:31:16 @agent_ppo2.py:185][0m |          -0.0086 |         172.7841 |           7.6571 |
[32m[20221213 21:31:16 @agent_ppo2.py:185][0m |          -0.0045 |         173.8640 |           7.6685 |
[32m[20221213 21:31:16 @agent_ppo2.py:185][0m |          -0.0105 |         172.2346 |           7.6411 |
[32m[20221213 21:31:16 @agent_ppo2.py:185][0m |          -0.0081 |         172.0157 |           7.6157 |
[32m[20221213 21:31:16 @agent_ppo2.py:185][0m |          -0.0093 |         171.8358 |           7.5791 |
[32m[20221213 21:31:16 @agent_ppo2.py:185][0m |          -0.0067 |         172.6998 |           7.5472 |
[32m[20221213 21:31:16 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:31:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.80
[32m[20221213 21:31:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:31:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.00
[32m[20221213 21:31:16 @agent_ppo2.py:143][0m Total time:      35.69 min
[32m[20221213 21:31:16 @agent_ppo2.py:145][0m 3479552 total steps have happened
[32m[20221213 21:31:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1699 --------------------------#
[32m[20221213 21:31:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:16 @agent_ppo2.py:185][0m |          -0.0027 |         177.7591 |           7.6757 |
[32m[20221213 21:31:16 @agent_ppo2.py:185][0m |          -0.0034 |         176.1066 |           7.6892 |
[32m[20221213 21:31:17 @agent_ppo2.py:185][0m |          -0.0054 |         174.9505 |           7.6889 |
[32m[20221213 21:31:17 @agent_ppo2.py:185][0m |           0.0037 |         196.5024 |           7.6704 |
[32m[20221213 21:31:17 @agent_ppo2.py:185][0m |          -0.0075 |         173.1687 |           7.6353 |
[32m[20221213 21:31:17 @agent_ppo2.py:185][0m |          -0.0090 |         172.2883 |           7.6231 |
[32m[20221213 21:31:17 @agent_ppo2.py:185][0m |          -0.0090 |         171.7632 |           7.6107 |
[32m[20221213 21:31:17 @agent_ppo2.py:185][0m |          -0.0103 |         171.2143 |           7.6558 |
[32m[20221213 21:31:17 @agent_ppo2.py:185][0m |          -0.0013 |         176.6955 |           7.6409 |
[32m[20221213 21:31:17 @agent_ppo2.py:185][0m |          -0.0098 |         170.4470 |           7.6510 |
[32m[20221213 21:31:17 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:31:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.20
[32m[20221213 21:31:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:31:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.00
[32m[20221213 21:31:17 @agent_ppo2.py:143][0m Total time:      35.71 min
[32m[20221213 21:31:17 @agent_ppo2.py:145][0m 3481600 total steps have happened
[32m[20221213 21:31:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1700 --------------------------#
[32m[20221213 21:31:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:31:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:18 @agent_ppo2.py:185][0m |           0.0005 |         176.2024 |           7.9313 |
[32m[20221213 21:31:18 @agent_ppo2.py:185][0m |          -0.0040 |         175.4312 |           7.9125 |
[32m[20221213 21:31:18 @agent_ppo2.py:185][0m |           0.0037 |         186.3190 |           7.8863 |
[32m[20221213 21:31:18 @agent_ppo2.py:185][0m |          -0.0043 |         174.7077 |           7.9679 |
[32m[20221213 21:31:18 @agent_ppo2.py:185][0m |          -0.0073 |         174.6868 |           7.9273 |
[32m[20221213 21:31:18 @agent_ppo2.py:185][0m |          -0.0057 |         174.2423 |           7.9344 |
[32m[20221213 21:31:18 @agent_ppo2.py:185][0m |          -0.0073 |         174.0778 |           7.9222 |
[32m[20221213 21:31:18 @agent_ppo2.py:185][0m |          -0.0081 |         174.0642 |           7.9728 |
[32m[20221213 21:31:18 @agent_ppo2.py:185][0m |          -0.0077 |         173.8250 |           7.9973 |
[32m[20221213 21:31:18 @agent_ppo2.py:185][0m |          -0.0099 |         173.8058 |           7.9492 |
[32m[20221213 21:31:18 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:31:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.80
[32m[20221213 21:31:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:31:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.00
[32m[20221213 21:31:19 @agent_ppo2.py:143][0m Total time:      35.73 min
[32m[20221213 21:31:19 @agent_ppo2.py:145][0m 3483648 total steps have happened
[32m[20221213 21:31:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1701 --------------------------#
[32m[20221213 21:31:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:19 @agent_ppo2.py:185][0m |           0.0002 |         174.4364 |           7.5069 |
[32m[20221213 21:31:19 @agent_ppo2.py:185][0m |           0.0002 |         174.0261 |           7.5206 |
[32m[20221213 21:31:19 @agent_ppo2.py:185][0m |           0.0070 |         189.5580 |           7.4819 |
[32m[20221213 21:31:19 @agent_ppo2.py:185][0m |           0.0046 |         180.9751 |           7.4638 |
[32m[20221213 21:31:19 @agent_ppo2.py:185][0m |          -0.0051 |         173.0491 |           7.4059 |
[32m[20221213 21:31:19 @agent_ppo2.py:185][0m |          -0.0052 |         172.9540 |           7.4036 |
[32m[20221213 21:31:19 @agent_ppo2.py:185][0m |          -0.0066 |         172.6671 |           7.3673 |
[32m[20221213 21:31:19 @agent_ppo2.py:185][0m |          -0.0075 |         172.4730 |           7.3525 |
[32m[20221213 21:31:20 @agent_ppo2.py:185][0m |          -0.0068 |         172.9132 |           7.3360 |
[32m[20221213 21:31:20 @agent_ppo2.py:185][0m |          -0.0067 |         172.2807 |           7.3119 |
[32m[20221213 21:31:20 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:31:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.80
[32m[20221213 21:31:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:31:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 21:31:20 @agent_ppo2.py:143][0m Total time:      35.75 min
[32m[20221213 21:31:20 @agent_ppo2.py:145][0m 3485696 total steps have happened
[32m[20221213 21:31:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1702 --------------------------#
[32m[20221213 21:31:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:20 @agent_ppo2.py:185][0m |           0.0045 |         178.6046 |           7.7500 |
[32m[20221213 21:31:20 @agent_ppo2.py:185][0m |          -0.0037 |         176.6151 |           7.7272 |
[32m[20221213 21:31:20 @agent_ppo2.py:185][0m |          -0.0028 |         175.9220 |           7.7066 |
[32m[20221213 21:31:20 @agent_ppo2.py:185][0m |          -0.0060 |         175.4842 |           7.6830 |
[32m[20221213 21:31:20 @agent_ppo2.py:185][0m |          -0.0042 |         175.2052 |           7.6766 |
[32m[20221213 21:31:21 @agent_ppo2.py:185][0m |           0.0026 |         183.0780 |           7.6979 |
[32m[20221213 21:31:21 @agent_ppo2.py:185][0m |          -0.0062 |         174.7899 |           7.6845 |
[32m[20221213 21:31:21 @agent_ppo2.py:185][0m |          -0.0022 |         174.8001 |           7.6808 |
[32m[20221213 21:31:21 @agent_ppo2.py:185][0m |          -0.0092 |         174.3637 |           7.6388 |
[32m[20221213 21:31:21 @agent_ppo2.py:185][0m |          -0.0073 |         174.2915 |           7.6990 |
[32m[20221213 21:31:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:31:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.20
[32m[20221213 21:31:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:31:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 872.00
[32m[20221213 21:31:21 @agent_ppo2.py:143][0m Total time:      35.77 min
[32m[20221213 21:31:21 @agent_ppo2.py:145][0m 3487744 total steps have happened
[32m[20221213 21:31:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1703 --------------------------#
[32m[20221213 21:31:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:31:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:21 @agent_ppo2.py:185][0m |           0.0015 |         176.8497 |           7.4211 |
[32m[20221213 21:31:21 @agent_ppo2.py:185][0m |          -0.0040 |         174.9854 |           7.3764 |
[32m[20221213 21:31:21 @agent_ppo2.py:185][0m |          -0.0023 |         175.1163 |           7.4440 |
[32m[20221213 21:31:22 @agent_ppo2.py:185][0m |          -0.0033 |         175.5370 |           7.4300 |
[32m[20221213 21:31:22 @agent_ppo2.py:185][0m |          -0.0062 |         173.6431 |           7.4703 |
[32m[20221213 21:31:22 @agent_ppo2.py:185][0m |          -0.0069 |         173.8271 |           7.4378 |
[32m[20221213 21:31:22 @agent_ppo2.py:185][0m |          -0.0086 |         173.3552 |           7.4729 |
[32m[20221213 21:31:22 @agent_ppo2.py:185][0m |          -0.0090 |         173.2056 |           7.5029 |
[32m[20221213 21:31:22 @agent_ppo2.py:185][0m |          -0.0058 |         174.3369 |           7.4873 |
[32m[20221213 21:31:22 @agent_ppo2.py:185][0m |           0.0033 |         183.9992 |           7.5278 |
[32m[20221213 21:31:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:31:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.80
[32m[20221213 21:31:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:31:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.00
[32m[20221213 21:31:22 @agent_ppo2.py:143][0m Total time:      35.79 min
[32m[20221213 21:31:22 @agent_ppo2.py:145][0m 3489792 total steps have happened
[32m[20221213 21:31:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1704 --------------------------#
[32m[20221213 21:31:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:23 @agent_ppo2.py:185][0m |          -0.0001 |         175.6258 |           7.8703 |
[32m[20221213 21:31:23 @agent_ppo2.py:185][0m |          -0.0006 |         176.2402 |           7.8122 |
[32m[20221213 21:31:23 @agent_ppo2.py:185][0m |          -0.0056 |         174.9258 |           7.8638 |
[32m[20221213 21:31:23 @agent_ppo2.py:185][0m |          -0.0050 |         174.7601 |           7.8436 |
[32m[20221213 21:31:23 @agent_ppo2.py:185][0m |          -0.0071 |         174.5953 |           7.8513 |
[32m[20221213 21:31:23 @agent_ppo2.py:185][0m |          -0.0056 |         174.8479 |           7.8636 |
[32m[20221213 21:31:23 @agent_ppo2.py:185][0m |          -0.0070 |         174.3339 |           7.8895 |
[32m[20221213 21:31:23 @agent_ppo2.py:185][0m |          -0.0079 |         174.2370 |           7.8777 |
[32m[20221213 21:31:23 @agent_ppo2.py:185][0m |          -0.0069 |         174.0435 |           7.8553 |
[32m[20221213 21:31:23 @agent_ppo2.py:185][0m |          -0.0078 |         173.9927 |           7.8986 |
[32m[20221213 21:31:23 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:31:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.60
[32m[20221213 21:31:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 862.00
[32m[20221213 21:31:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.00
[32m[20221213 21:31:23 @agent_ppo2.py:143][0m Total time:      35.81 min
[32m[20221213 21:31:23 @agent_ppo2.py:145][0m 3491840 total steps have happened
[32m[20221213 21:31:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1705 --------------------------#
[32m[20221213 21:31:24 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:31:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:24 @agent_ppo2.py:185][0m |           0.0033 |         180.5444 |           7.1503 |
[32m[20221213 21:31:24 @agent_ppo2.py:185][0m |          -0.0040 |         178.0871 |           7.1459 |
[32m[20221213 21:31:24 @agent_ppo2.py:185][0m |          -0.0064 |         177.2212 |           7.1912 |
[32m[20221213 21:31:24 @agent_ppo2.py:185][0m |          -0.0081 |         176.6181 |           7.1701 |
[32m[20221213 21:31:24 @agent_ppo2.py:185][0m |          -0.0092 |         176.1346 |           7.1667 |
[32m[20221213 21:31:24 @agent_ppo2.py:185][0m |          -0.0078 |         175.8916 |           7.2595 |
[32m[20221213 21:31:24 @agent_ppo2.py:185][0m |          -0.0077 |         175.5282 |           7.2107 |
[32m[20221213 21:31:24 @agent_ppo2.py:185][0m |          -0.0105 |         175.3974 |           7.2392 |
[32m[20221213 21:31:24 @agent_ppo2.py:185][0m |          -0.0102 |         175.0944 |           7.1952 |
[32m[20221213 21:31:25 @agent_ppo2.py:185][0m |          -0.0082 |         175.1118 |           7.2327 |
[32m[20221213 21:31:25 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:31:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.60
[32m[20221213 21:31:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.00
[32m[20221213 21:31:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 865.00
[32m[20221213 21:31:25 @agent_ppo2.py:143][0m Total time:      35.83 min
[32m[20221213 21:31:25 @agent_ppo2.py:145][0m 3493888 total steps have happened
[32m[20221213 21:31:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1706 --------------------------#
[32m[20221213 21:31:25 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:31:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:25 @agent_ppo2.py:185][0m |          -0.0028 |         178.0255 |           7.4626 |
[32m[20221213 21:31:25 @agent_ppo2.py:185][0m |          -0.0058 |         174.6632 |           7.4788 |
[32m[20221213 21:31:25 @agent_ppo2.py:185][0m |           0.0053 |         199.3789 |           7.4783 |
[32m[20221213 21:31:25 @agent_ppo2.py:185][0m |          -0.0007 |         176.0327 |           7.4478 |
[32m[20221213 21:31:25 @agent_ppo2.py:185][0m |          -0.0098 |         173.3313 |           7.4476 |
[32m[20221213 21:31:25 @agent_ppo2.py:185][0m |          -0.0076 |         173.0595 |           7.3460 |
[32m[20221213 21:31:26 @agent_ppo2.py:185][0m |          -0.0055 |         172.7233 |           7.3407 |
[32m[20221213 21:31:26 @agent_ppo2.py:185][0m |           0.0003 |         179.9617 |           7.2954 |
[32m[20221213 21:31:26 @agent_ppo2.py:185][0m |          -0.0075 |         172.6755 |           7.2997 |
[32m[20221213 21:31:26 @agent_ppo2.py:185][0m |          -0.0059 |         172.5285 |           7.2668 |
[32m[20221213 21:31:26 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:31:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.80
[32m[20221213 21:31:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.00
[32m[20221213 21:31:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:31:26 @agent_ppo2.py:143][0m Total time:      35.85 min
[32m[20221213 21:31:26 @agent_ppo2.py:145][0m 3495936 total steps have happened
[32m[20221213 21:31:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1707 --------------------------#
[32m[20221213 21:31:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:26 @agent_ppo2.py:185][0m |           0.0052 |         177.5947 |           7.2231 |
[32m[20221213 21:31:26 @agent_ppo2.py:185][0m |          -0.0013 |         173.5748 |           7.2152 |
[32m[20221213 21:31:26 @agent_ppo2.py:185][0m |          -0.0081 |         172.0798 |           7.1738 |
[32m[20221213 21:31:27 @agent_ppo2.py:185][0m |          -0.0056 |         171.7926 |           7.2134 |
[32m[20221213 21:31:27 @agent_ppo2.py:185][0m |          -0.0061 |         171.7692 |           7.1866 |
[32m[20221213 21:31:27 @agent_ppo2.py:185][0m |          -0.0084 |         171.4558 |           7.2386 |
[32m[20221213 21:31:27 @agent_ppo2.py:185][0m |          -0.0045 |         172.1180 |           7.2155 |
[32m[20221213 21:31:27 @agent_ppo2.py:185][0m |          -0.0098 |         171.2259 |           7.2587 |
[32m[20221213 21:31:27 @agent_ppo2.py:185][0m |           0.0016 |         180.5987 |           7.1613 |
[32m[20221213 21:31:27 @agent_ppo2.py:185][0m |           0.0013 |         188.7261 |           7.2021 |
[32m[20221213 21:31:27 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 21:31:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.20
[32m[20221213 21:31:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.00
[32m[20221213 21:31:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.00
[32m[20221213 21:31:27 @agent_ppo2.py:143][0m Total time:      35.88 min
[32m[20221213 21:31:27 @agent_ppo2.py:145][0m 3497984 total steps have happened
[32m[20221213 21:31:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1708 --------------------------#
[32m[20221213 21:31:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:28 @agent_ppo2.py:185][0m |           0.0088 |         178.0719 |           6.9898 |
[32m[20221213 21:31:28 @agent_ppo2.py:185][0m |           0.0020 |         177.8832 |           7.0074 |
[32m[20221213 21:31:28 @agent_ppo2.py:185][0m |          -0.0076 |         170.9271 |           7.0252 |
[32m[20221213 21:31:28 @agent_ppo2.py:185][0m |          -0.0084 |         170.3776 |           7.0064 |
[32m[20221213 21:31:28 @agent_ppo2.py:185][0m |          -0.0037 |         171.2827 |           7.0226 |
[32m[20221213 21:31:28 @agent_ppo2.py:185][0m |          -0.0091 |         169.6778 |           7.0044 |
[32m[20221213 21:31:28 @agent_ppo2.py:185][0m |          -0.0106 |         169.4708 |           7.0403 |
[32m[20221213 21:31:28 @agent_ppo2.py:185][0m |          -0.0083 |         169.2237 |           7.0481 |
[32m[20221213 21:31:28 @agent_ppo2.py:185][0m |          -0.0091 |         168.9263 |           7.0576 |
[32m[20221213 21:31:28 @agent_ppo2.py:185][0m |          -0.0094 |         168.7800 |           7.0470 |
[32m[20221213 21:31:28 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:31:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.60
[32m[20221213 21:31:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.00
[32m[20221213 21:31:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.00
[32m[20221213 21:31:29 @agent_ppo2.py:143][0m Total time:      35.90 min
[32m[20221213 21:31:29 @agent_ppo2.py:145][0m 3500032 total steps have happened
[32m[20221213 21:31:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1709 --------------------------#
[32m[20221213 21:31:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:29 @agent_ppo2.py:185][0m |          -0.0009 |         176.1231 |           7.4330 |
[32m[20221213 21:31:29 @agent_ppo2.py:185][0m |          -0.0050 |         174.6772 |           7.4396 |
[32m[20221213 21:31:29 @agent_ppo2.py:185][0m |          -0.0027 |         176.0401 |           7.5371 |
[32m[20221213 21:31:29 @agent_ppo2.py:185][0m |           0.0057 |         181.6825 |           7.5406 |
[32m[20221213 21:31:29 @agent_ppo2.py:185][0m |          -0.0090 |         172.2181 |           7.5496 |
[32m[20221213 21:31:29 @agent_ppo2.py:185][0m |          -0.0095 |         171.8841 |           7.6049 |
[32m[20221213 21:31:29 @agent_ppo2.py:185][0m |          -0.0076 |         171.4178 |           7.5623 |
[32m[20221213 21:31:29 @agent_ppo2.py:185][0m |          -0.0085 |         170.9990 |           7.5676 |
[32m[20221213 21:31:30 @agent_ppo2.py:185][0m |          -0.0095 |         170.5902 |           7.6737 |
[32m[20221213 21:31:30 @agent_ppo2.py:185][0m |          -0.0092 |         170.3185 |           7.6401 |
[32m[20221213 21:31:30 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:31:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.20
[32m[20221213 21:31:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.00
[32m[20221213 21:31:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 867.00
[32m[20221213 21:31:30 @agent_ppo2.py:143][0m Total time:      35.92 min
[32m[20221213 21:31:30 @agent_ppo2.py:145][0m 3502080 total steps have happened
[32m[20221213 21:31:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1710 --------------------------#
[32m[20221213 21:31:30 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:30 @agent_ppo2.py:185][0m |          -0.0008 |         175.5299 |           7.4821 |
[32m[20221213 21:31:30 @agent_ppo2.py:185][0m |          -0.0052 |         174.8210 |           7.4733 |
[32m[20221213 21:31:30 @agent_ppo2.py:185][0m |          -0.0028 |         174.5721 |           7.4280 |
[32m[20221213 21:31:30 @agent_ppo2.py:185][0m |          -0.0063 |         174.1755 |           7.4391 |
[32m[20221213 21:31:30 @agent_ppo2.py:185][0m |          -0.0044 |         174.0682 |           7.4222 |
[32m[20221213 21:31:31 @agent_ppo2.py:185][0m |          -0.0085 |         173.8619 |           7.4147 |
[32m[20221213 21:31:31 @agent_ppo2.py:185][0m |           0.0008 |         185.2186 |           7.4139 |
[32m[20221213 21:31:31 @agent_ppo2.py:185][0m |          -0.0041 |         174.1382 |           7.4773 |
[32m[20221213 21:31:31 @agent_ppo2.py:185][0m |          -0.0054 |         173.4212 |           7.4075 |
[32m[20221213 21:31:31 @agent_ppo2.py:185][0m |          -0.0080 |         173.4127 |           7.3516 |
[32m[20221213 21:31:31 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:31:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.40
[32m[20221213 21:31:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:31:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.00
[32m[20221213 21:31:31 @agent_ppo2.py:143][0m Total time:      35.94 min
[32m[20221213 21:31:31 @agent_ppo2.py:145][0m 3504128 total steps have happened
[32m[20221213 21:31:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1711 --------------------------#
[32m[20221213 21:31:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:31 @agent_ppo2.py:185][0m |          -0.0002 |         174.7686 |           7.2043 |
[32m[20221213 21:31:31 @agent_ppo2.py:185][0m |          -0.0037 |         173.4384 |           7.1998 |
[32m[20221213 21:31:32 @agent_ppo2.py:185][0m |          -0.0045 |         172.7610 |           7.2340 |
[32m[20221213 21:31:32 @agent_ppo2.py:185][0m |          -0.0015 |         174.8610 |           7.1867 |
[32m[20221213 21:31:32 @agent_ppo2.py:185][0m |          -0.0075 |         171.7604 |           7.1925 |
[32m[20221213 21:31:32 @agent_ppo2.py:185][0m |          -0.0085 |         171.4382 |           7.1946 |
[32m[20221213 21:31:32 @agent_ppo2.py:185][0m |          -0.0079 |         171.3589 |           7.1904 |
[32m[20221213 21:31:32 @agent_ppo2.py:185][0m |          -0.0104 |         170.8704 |           7.2292 |
[32m[20221213 21:31:32 @agent_ppo2.py:185][0m |          -0.0098 |         170.5852 |           7.1836 |
[32m[20221213 21:31:32 @agent_ppo2.py:185][0m |          -0.0058 |         173.0047 |           7.2473 |
[32m[20221213 21:31:32 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:31:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.80
[32m[20221213 21:31:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:31:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 866.00
[32m[20221213 21:31:32 @agent_ppo2.py:143][0m Total time:      35.96 min
[32m[20221213 21:31:32 @agent_ppo2.py:145][0m 3506176 total steps have happened
[32m[20221213 21:31:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1712 --------------------------#
[32m[20221213 21:31:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:33 @agent_ppo2.py:185][0m |          -0.0006 |         174.8405 |           7.0973 |
[32m[20221213 21:31:33 @agent_ppo2.py:185][0m |          -0.0027 |         173.9091 |           7.0303 |
[32m[20221213 21:31:33 @agent_ppo2.py:185][0m |          -0.0058 |         173.4850 |           7.0014 |
[32m[20221213 21:31:33 @agent_ppo2.py:185][0m |          -0.0068 |         173.0393 |           6.9992 |
[32m[20221213 21:31:33 @agent_ppo2.py:185][0m |          -0.0080 |         172.7279 |           7.0099 |
[32m[20221213 21:31:33 @agent_ppo2.py:185][0m |          -0.0077 |         172.6442 |           6.9794 |
[32m[20221213 21:31:33 @agent_ppo2.py:185][0m |          -0.0052 |         174.7639 |           6.9408 |
[32m[20221213 21:31:33 @agent_ppo2.py:185][0m |          -0.0081 |         172.3355 |           6.9904 |
[32m[20221213 21:31:33 @agent_ppo2.py:185][0m |          -0.0101 |         172.2908 |           6.9656 |
[32m[20221213 21:31:33 @agent_ppo2.py:185][0m |          -0.0098 |         172.1636 |           6.9705 |
[32m[20221213 21:31:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:31:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.40
[32m[20221213 21:31:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:31:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.00
[32m[20221213 21:31:33 @agent_ppo2.py:143][0m Total time:      35.98 min
[32m[20221213 21:31:33 @agent_ppo2.py:145][0m 3508224 total steps have happened
[32m[20221213 21:31:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1713 --------------------------#
[32m[20221213 21:31:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:31:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:34 @agent_ppo2.py:185][0m |          -0.0017 |         176.5492 |           6.8483 |
[32m[20221213 21:31:34 @agent_ppo2.py:185][0m |          -0.0055 |         176.0549 |           6.8984 |
[32m[20221213 21:31:34 @agent_ppo2.py:185][0m |          -0.0029 |         178.8226 |           6.9047 |
[32m[20221213 21:31:34 @agent_ppo2.py:185][0m |           0.0132 |         199.9460 |           6.9492 |
[32m[20221213 21:31:34 @agent_ppo2.py:185][0m |          -0.0042 |         175.4507 |           6.9345 |
[32m[20221213 21:31:34 @agent_ppo2.py:185][0m |          -0.0077 |         175.1019 |           6.9216 |
[32m[20221213 21:31:34 @agent_ppo2.py:185][0m |          -0.0073 |         174.9295 |           6.9363 |
[32m[20221213 21:31:34 @agent_ppo2.py:185][0m |          -0.0077 |         174.9318 |           6.9552 |
[32m[20221213 21:31:34 @agent_ppo2.py:185][0m |          -0.0082 |         174.8200 |           6.9305 |
[32m[20221213 21:31:35 @agent_ppo2.py:185][0m |           0.0115 |         208.9183 |           6.9767 |
[32m[20221213 21:31:35 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:31:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.40
[32m[20221213 21:31:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.00
[32m[20221213 21:31:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:31:35 @agent_ppo2.py:143][0m Total time:      36.00 min
[32m[20221213 21:31:35 @agent_ppo2.py:145][0m 3510272 total steps have happened
[32m[20221213 21:31:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1714 --------------------------#
[32m[20221213 21:31:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:35 @agent_ppo2.py:185][0m |          -0.0011 |         175.8397 |           7.2089 |
[32m[20221213 21:31:35 @agent_ppo2.py:185][0m |          -0.0030 |         175.2459 |           7.2462 |
[32m[20221213 21:31:35 @agent_ppo2.py:185][0m |          -0.0046 |         174.5978 |           7.3222 |
[32m[20221213 21:31:35 @agent_ppo2.py:185][0m |          -0.0053 |         174.3808 |           7.3245 |
[32m[20221213 21:31:35 @agent_ppo2.py:185][0m |          -0.0054 |         174.3138 |           7.3751 |
[32m[20221213 21:31:35 @agent_ppo2.py:185][0m |          -0.0074 |         174.0355 |           7.4739 |
[32m[20221213 21:31:36 @agent_ppo2.py:185][0m |          -0.0071 |         173.9521 |           7.5294 |
[32m[20221213 21:31:36 @agent_ppo2.py:185][0m |          -0.0074 |         173.8323 |           7.5381 |
[32m[20221213 21:31:36 @agent_ppo2.py:185][0m |          -0.0083 |         173.7884 |           7.6124 |
[32m[20221213 21:31:36 @agent_ppo2.py:185][0m |           0.0028 |         191.6757 |           7.6220 |
[32m[20221213 21:31:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:31:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.00
[32m[20221213 21:31:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.00
[32m[20221213 21:31:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 21:31:36 @agent_ppo2.py:143][0m Total time:      36.02 min
[32m[20221213 21:31:36 @agent_ppo2.py:145][0m 3512320 total steps have happened
[32m[20221213 21:31:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1715 --------------------------#
[32m[20221213 21:31:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:36 @agent_ppo2.py:185][0m |           0.0108 |         183.0847 |           7.6288 |
[32m[20221213 21:31:36 @agent_ppo2.py:185][0m |          -0.0021 |         174.5702 |           7.5786 |
[32m[20221213 21:31:36 @agent_ppo2.py:185][0m |          -0.0064 |         174.1087 |           7.5757 |
[32m[20221213 21:31:36 @agent_ppo2.py:185][0m |           0.0094 |         189.8740 |           7.4993 |
[32m[20221213 21:31:37 @agent_ppo2.py:185][0m |          -0.0081 |         173.3555 |           7.5298 |
[32m[20221213 21:31:37 @agent_ppo2.py:185][0m |           0.0084 |         196.2177 |           7.5353 |
[32m[20221213 21:31:37 @agent_ppo2.py:185][0m |          -0.0090 |         173.0151 |           7.5507 |
[32m[20221213 21:31:37 @agent_ppo2.py:185][0m |          -0.0089 |         172.6699 |           7.5322 |
[32m[20221213 21:31:37 @agent_ppo2.py:185][0m |          -0.0107 |         172.3429 |           7.4703 |
[32m[20221213 21:31:37 @agent_ppo2.py:185][0m |          -0.0115 |         172.3187 |           7.4087 |
[32m[20221213 21:31:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:31:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.60
[32m[20221213 21:31:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.00
[32m[20221213 21:31:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.00
[32m[20221213 21:31:37 @agent_ppo2.py:143][0m Total time:      36.04 min
[32m[20221213 21:31:37 @agent_ppo2.py:145][0m 3514368 total steps have happened
[32m[20221213 21:31:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1716 --------------------------#
[32m[20221213 21:31:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:31:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:37 @agent_ppo2.py:185][0m |           0.0047 |         186.5599 |           7.5081 |
[32m[20221213 21:31:38 @agent_ppo2.py:185][0m |          -0.0018 |         176.5157 |           7.4792 |
[32m[20221213 21:31:38 @agent_ppo2.py:185][0m |          -0.0013 |         177.8920 |           7.4161 |
[32m[20221213 21:31:38 @agent_ppo2.py:185][0m |          -0.0050 |         178.1225 |           7.4007 |
[32m[20221213 21:31:38 @agent_ppo2.py:185][0m |          -0.0045 |         175.1368 |           7.4008 |
[32m[20221213 21:31:38 @agent_ppo2.py:185][0m |           0.0027 |         185.9668 |           7.3896 |
[32m[20221213 21:31:38 @agent_ppo2.py:185][0m |          -0.0080 |         174.6887 |           7.4267 |
[32m[20221213 21:31:38 @agent_ppo2.py:185][0m |          -0.0090 |         174.3966 |           7.3901 |
[32m[20221213 21:31:38 @agent_ppo2.py:185][0m |          -0.0082 |         174.3852 |           7.3857 |
[32m[20221213 21:31:38 @agent_ppo2.py:185][0m |          -0.0016 |         180.2565 |           7.3659 |
[32m[20221213 21:31:38 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:31:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.20
[32m[20221213 21:31:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 21:31:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.00
[32m[20221213 21:31:38 @agent_ppo2.py:143][0m Total time:      36.06 min
[32m[20221213 21:31:38 @agent_ppo2.py:145][0m 3516416 total steps have happened
[32m[20221213 21:31:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1717 --------------------------#
[32m[20221213 21:31:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:39 @agent_ppo2.py:185][0m |           0.0001 |         175.0352 |           7.2462 |
[32m[20221213 21:31:39 @agent_ppo2.py:185][0m |          -0.0027 |         174.2469 |           7.2380 |
[32m[20221213 21:31:39 @agent_ppo2.py:185][0m |          -0.0016 |         174.7251 |           7.2903 |
[32m[20221213 21:31:39 @agent_ppo2.py:185][0m |          -0.0064 |         173.0880 |           7.2873 |
[32m[20221213 21:31:39 @agent_ppo2.py:185][0m |          -0.0073 |         172.7262 |           7.3881 |
[32m[20221213 21:31:39 @agent_ppo2.py:185][0m |          -0.0057 |         172.9590 |           7.2852 |
[32m[20221213 21:31:39 @agent_ppo2.py:185][0m |          -0.0063 |         172.4011 |           7.3398 |
[32m[20221213 21:31:39 @agent_ppo2.py:185][0m |           0.0019 |         179.4114 |           7.3856 |
[32m[20221213 21:31:39 @agent_ppo2.py:185][0m |          -0.0087 |         171.6340 |           7.3591 |
[32m[20221213 21:31:39 @agent_ppo2.py:185][0m |          -0.0080 |         171.5089 |           7.3863 |
[32m[20221213 21:31:39 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:31:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.40
[32m[20221213 21:31:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.00
[32m[20221213 21:31:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 870.00
[32m[20221213 21:31:40 @agent_ppo2.py:143][0m Total time:      36.08 min
[32m[20221213 21:31:40 @agent_ppo2.py:145][0m 3518464 total steps have happened
[32m[20221213 21:31:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1718 --------------------------#
[32m[20221213 21:31:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:40 @agent_ppo2.py:185][0m |           0.0046 |         183.3054 |           7.2356 |
[32m[20221213 21:31:40 @agent_ppo2.py:185][0m |          -0.0037 |         177.1721 |           7.2942 |
[32m[20221213 21:31:40 @agent_ppo2.py:185][0m |           0.0072 |         190.7245 |           7.2408 |
[32m[20221213 21:31:40 @agent_ppo2.py:185][0m |           0.0009 |         177.7719 |           7.2938 |
[32m[20221213 21:31:40 @agent_ppo2.py:185][0m |          -0.0063 |         175.7065 |           7.2915 |
[32m[20221213 21:31:40 @agent_ppo2.py:185][0m |          -0.0075 |         175.3415 |           7.3150 |
[32m[20221213 21:31:40 @agent_ppo2.py:185][0m |          -0.0083 |         175.2416 |           7.3576 |
[32m[20221213 21:31:40 @agent_ppo2.py:185][0m |          -0.0071 |         175.0795 |           7.3518 |
[32m[20221213 21:31:41 @agent_ppo2.py:185][0m |          -0.0077 |         174.8921 |           7.3642 |
[32m[20221213 21:31:41 @agent_ppo2.py:185][0m |          -0.0082 |         174.7830 |           7.3752 |
[32m[20221213 21:31:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:31:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.40
[32m[20221213 21:31:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.00
[32m[20221213 21:31:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.00
[32m[20221213 21:31:41 @agent_ppo2.py:143][0m Total time:      36.10 min
[32m[20221213 21:31:41 @agent_ppo2.py:145][0m 3520512 total steps have happened
[32m[20221213 21:31:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1719 --------------------------#
[32m[20221213 21:31:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:41 @agent_ppo2.py:185][0m |          -0.0011 |         176.1983 |           7.0597 |
[32m[20221213 21:31:41 @agent_ppo2.py:185][0m |          -0.0016 |         176.1623 |           7.0132 |
[32m[20221213 21:31:41 @agent_ppo2.py:185][0m |          -0.0051 |         175.1956 |           7.0076 |
[32m[20221213 21:31:41 @agent_ppo2.py:185][0m |          -0.0074 |         174.6351 |           6.9406 |
[32m[20221213 21:31:41 @agent_ppo2.py:185][0m |          -0.0065 |         174.7446 |           6.9275 |
[32m[20221213 21:31:42 @agent_ppo2.py:185][0m |          -0.0084 |         174.2745 |           6.8267 |
[32m[20221213 21:31:42 @agent_ppo2.py:185][0m |          -0.0090 |         174.0690 |           6.8182 |
[32m[20221213 21:31:42 @agent_ppo2.py:185][0m |          -0.0089 |         174.0211 |           6.8115 |
[32m[20221213 21:31:42 @agent_ppo2.py:185][0m |          -0.0078 |         173.9386 |           6.7760 |
[32m[20221213 21:31:42 @agent_ppo2.py:185][0m |          -0.0111 |         173.7429 |           6.6933 |
[32m[20221213 21:31:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:31:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.00
[32m[20221213 21:31:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:31:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 21:31:42 @agent_ppo2.py:143][0m Total time:      36.12 min
[32m[20221213 21:31:42 @agent_ppo2.py:145][0m 3522560 total steps have happened
[32m[20221213 21:31:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1720 --------------------------#
[32m[20221213 21:31:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:42 @agent_ppo2.py:185][0m |          -0.0009 |         174.4803 |           6.5587 |
[32m[20221213 21:31:42 @agent_ppo2.py:185][0m |          -0.0024 |         173.8490 |           6.6222 |
[32m[20221213 21:31:42 @agent_ppo2.py:185][0m |          -0.0052 |         173.4904 |           6.6146 |
[32m[20221213 21:31:43 @agent_ppo2.py:185][0m |          -0.0047 |         173.3955 |           6.5829 |
[32m[20221213 21:31:43 @agent_ppo2.py:185][0m |          -0.0054 |         173.0111 |           6.6652 |
[32m[20221213 21:31:43 @agent_ppo2.py:185][0m |          -0.0057 |         172.9244 |           6.6251 |
[32m[20221213 21:31:43 @agent_ppo2.py:185][0m |          -0.0009 |         177.3055 |           6.6128 |
[32m[20221213 21:31:43 @agent_ppo2.py:185][0m |          -0.0075 |         172.6133 |           6.6034 |
[32m[20221213 21:31:43 @agent_ppo2.py:185][0m |          -0.0070 |         172.4882 |           6.5955 |
[32m[20221213 21:31:43 @agent_ppo2.py:185][0m |          -0.0076 |         172.4796 |           6.6218 |
[32m[20221213 21:31:43 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:31:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.20
[32m[20221213 21:31:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.00
[32m[20221213 21:31:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.00
[32m[20221213 21:31:43 @agent_ppo2.py:143][0m Total time:      36.14 min
[32m[20221213 21:31:43 @agent_ppo2.py:145][0m 3524608 total steps have happened
[32m[20221213 21:31:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1721 --------------------------#
[32m[20221213 21:31:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:44 @agent_ppo2.py:185][0m |           0.0003 |         174.9447 |           6.7025 |
[32m[20221213 21:31:44 @agent_ppo2.py:185][0m |          -0.0044 |         174.0446 |           6.6701 |
[32m[20221213 21:31:44 @agent_ppo2.py:185][0m |          -0.0056 |         173.5007 |           6.6144 |
[32m[20221213 21:31:44 @agent_ppo2.py:185][0m |          -0.0077 |         173.2865 |           6.6954 |
[32m[20221213 21:31:44 @agent_ppo2.py:185][0m |          -0.0076 |         173.0416 |           6.6886 |
[32m[20221213 21:31:44 @agent_ppo2.py:185][0m |          -0.0071 |         173.4109 |           6.6976 |
[32m[20221213 21:31:44 @agent_ppo2.py:185][0m |          -0.0082 |         172.7771 |           6.6814 |
[32m[20221213 21:31:44 @agent_ppo2.py:185][0m |          -0.0087 |         172.7076 |           6.6718 |
[32m[20221213 21:31:44 @agent_ppo2.py:185][0m |          -0.0072 |         172.4734 |           6.7321 |
[32m[20221213 21:31:44 @agent_ppo2.py:185][0m |          -0.0108 |         172.3292 |           6.6611 |
[32m[20221213 21:31:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:31:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.80
[32m[20221213 21:31:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 854.00
[32m[20221213 21:31:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.00
[32m[20221213 21:31:44 @agent_ppo2.py:143][0m Total time:      36.16 min
[32m[20221213 21:31:44 @agent_ppo2.py:145][0m 3526656 total steps have happened
[32m[20221213 21:31:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1722 --------------------------#
[32m[20221213 21:31:45 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:31:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:45 @agent_ppo2.py:185][0m |           0.0075 |         179.9206 |           6.4041 |
[32m[20221213 21:31:45 @agent_ppo2.py:185][0m |           0.0017 |         173.6298 |           6.4285 |
[32m[20221213 21:31:45 @agent_ppo2.py:185][0m |          -0.0052 |         173.0814 |           6.3997 |
[32m[20221213 21:31:45 @agent_ppo2.py:185][0m |          -0.0056 |         172.8359 |           6.3709 |
[32m[20221213 21:31:45 @agent_ppo2.py:185][0m |          -0.0061 |         172.8678 |           6.3327 |
[32m[20221213 21:31:45 @agent_ppo2.py:185][0m |          -0.0006 |         178.7503 |           6.3673 |
[32m[20221213 21:31:45 @agent_ppo2.py:185][0m |          -0.0068 |         172.4235 |           6.3657 |
[32m[20221213 21:31:45 @agent_ppo2.py:185][0m |          -0.0079 |         172.4542 |           6.3150 |
[32m[20221213 21:31:45 @agent_ppo2.py:185][0m |          -0.0084 |         172.3591 |           6.3324 |
[32m[20221213 21:31:46 @agent_ppo2.py:185][0m |          -0.0074 |         172.2125 |           6.2994 |
[32m[20221213 21:31:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:31:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.00
[32m[20221213 21:31:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.00
[32m[20221213 21:31:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:31:46 @agent_ppo2.py:143][0m Total time:      36.18 min
[32m[20221213 21:31:46 @agent_ppo2.py:145][0m 3528704 total steps have happened
[32m[20221213 21:31:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1723 --------------------------#
[32m[20221213 21:31:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:46 @agent_ppo2.py:185][0m |           0.0104 |         179.9067 |           6.2695 |
[32m[20221213 21:31:46 @agent_ppo2.py:185][0m |          -0.0015 |         172.3114 |           6.2899 |
[32m[20221213 21:31:46 @agent_ppo2.py:185][0m |          -0.0041 |         171.9331 |           6.2898 |
[32m[20221213 21:31:46 @agent_ppo2.py:185][0m |          -0.0066 |         171.6880 |           6.3755 |
[32m[20221213 21:31:46 @agent_ppo2.py:185][0m |           0.0012 |         173.4818 |           6.3856 |
[32m[20221213 21:31:46 @agent_ppo2.py:185][0m |          -0.0075 |         171.4944 |           6.4147 |
[32m[20221213 21:31:47 @agent_ppo2.py:185][0m |          -0.0073 |         171.3635 |           6.4246 |
[32m[20221213 21:31:47 @agent_ppo2.py:185][0m |          -0.0078 |         171.1727 |           6.4727 |
[32m[20221213 21:31:47 @agent_ppo2.py:185][0m |          -0.0042 |         171.9982 |           6.5010 |
[32m[20221213 21:31:47 @agent_ppo2.py:185][0m |          -0.0064 |         171.1005 |           6.5076 |
[32m[20221213 21:31:47 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:31:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.00
[32m[20221213 21:31:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:31:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.00
[32m[20221213 21:31:47 @agent_ppo2.py:143][0m Total time:      36.20 min
[32m[20221213 21:31:47 @agent_ppo2.py:145][0m 3530752 total steps have happened
[32m[20221213 21:31:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1724 --------------------------#
[32m[20221213 21:31:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:47 @agent_ppo2.py:185][0m |          -0.0016 |         173.2517 |           6.5826 |
[32m[20221213 21:31:47 @agent_ppo2.py:185][0m |          -0.0067 |         172.2856 |           6.5466 |
[32m[20221213 21:31:47 @agent_ppo2.py:185][0m |          -0.0068 |         171.6202 |           6.6360 |
[32m[20221213 21:31:47 @agent_ppo2.py:185][0m |          -0.0094 |         171.1248 |           6.6369 |
[32m[20221213 21:31:48 @agent_ppo2.py:185][0m |          -0.0058 |         171.7070 |           6.6139 |
[32m[20221213 21:31:48 @agent_ppo2.py:185][0m |          -0.0071 |         172.8893 |           6.6070 |
[32m[20221213 21:31:48 @agent_ppo2.py:185][0m |          -0.0103 |         170.1603 |           6.6611 |
[32m[20221213 21:31:48 @agent_ppo2.py:185][0m |          -0.0013 |         182.5147 |           6.6724 |
[32m[20221213 21:31:48 @agent_ppo2.py:185][0m |          -0.0087 |         169.8047 |           6.6522 |
[32m[20221213 21:31:48 @agent_ppo2.py:185][0m |          -0.0112 |         169.6889 |           6.7431 |
[32m[20221213 21:31:48 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:31:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.00
[32m[20221213 21:31:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.00
[32m[20221213 21:31:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 21:31:48 @agent_ppo2.py:143][0m Total time:      36.22 min
[32m[20221213 21:31:48 @agent_ppo2.py:145][0m 3532800 total steps have happened
[32m[20221213 21:31:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1725 --------------------------#
[32m[20221213 21:31:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:48 @agent_ppo2.py:185][0m |           0.0007 |         174.6620 |           6.7122 |
[32m[20221213 21:31:49 @agent_ppo2.py:185][0m |          -0.0018 |         173.5658 |           6.6902 |
[32m[20221213 21:31:49 @agent_ppo2.py:185][0m |          -0.0070 |         172.6537 |           6.7102 |
[32m[20221213 21:31:49 @agent_ppo2.py:185][0m |          -0.0082 |         172.1167 |           6.7213 |
[32m[20221213 21:31:49 @agent_ppo2.py:185][0m |          -0.0071 |         171.8165 |           6.6941 |
[32m[20221213 21:31:49 @agent_ppo2.py:185][0m |          -0.0089 |         171.3840 |           6.6844 |
[32m[20221213 21:31:49 @agent_ppo2.py:185][0m |          -0.0049 |         172.1599 |           6.6883 |
[32m[20221213 21:31:49 @agent_ppo2.py:185][0m |          -0.0078 |         171.0924 |           6.7385 |
[32m[20221213 21:31:49 @agent_ppo2.py:185][0m |          -0.0072 |         170.8286 |           6.7324 |
[32m[20221213 21:31:49 @agent_ppo2.py:185][0m |          -0.0095 |         170.8125 |           6.7375 |
[32m[20221213 21:31:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:31:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.80
[32m[20221213 21:31:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.00
[32m[20221213 21:31:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.00
[32m[20221213 21:31:49 @agent_ppo2.py:143][0m Total time:      36.24 min
[32m[20221213 21:31:49 @agent_ppo2.py:145][0m 3534848 total steps have happened
[32m[20221213 21:31:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1726 --------------------------#
[32m[20221213 21:31:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:31:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:50 @agent_ppo2.py:185][0m |          -0.0007 |         174.3939 |           6.5898 |
[32m[20221213 21:31:50 @agent_ppo2.py:185][0m |          -0.0039 |         174.0611 |           6.6448 |
[32m[20221213 21:31:50 @agent_ppo2.py:185][0m |          -0.0048 |         173.7516 |           6.6768 |
[32m[20221213 21:31:50 @agent_ppo2.py:185][0m |          -0.0065 |         172.5681 |           6.6419 |
[32m[20221213 21:31:50 @agent_ppo2.py:185][0m |          -0.0088 |         172.0965 |           6.6807 |
[32m[20221213 21:31:50 @agent_ppo2.py:185][0m |          -0.0072 |         171.8475 |           6.6377 |
[32m[20221213 21:31:50 @agent_ppo2.py:185][0m |          -0.0058 |         172.3449 |           6.6666 |
[32m[20221213 21:31:50 @agent_ppo2.py:185][0m |          -0.0098 |         171.3999 |           6.7037 |
[32m[20221213 21:31:50 @agent_ppo2.py:185][0m |          -0.0061 |         172.5506 |           6.6866 |
[32m[20221213 21:31:50 @agent_ppo2.py:185][0m |          -0.0097 |         171.0439 |           6.6961 |
[32m[20221213 21:31:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:31:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.60
[32m[20221213 21:31:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:31:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 865.00
[32m[20221213 21:31:51 @agent_ppo2.py:143][0m Total time:      36.26 min
[32m[20221213 21:31:51 @agent_ppo2.py:145][0m 3536896 total steps have happened
[32m[20221213 21:31:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1727 --------------------------#
[32m[20221213 21:31:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:51 @agent_ppo2.py:185][0m |          -0.0033 |         178.5338 |           6.7227 |
[32m[20221213 21:31:51 @agent_ppo2.py:185][0m |          -0.0068 |         177.5749 |           6.7997 |
[32m[20221213 21:31:51 @agent_ppo2.py:185][0m |          -0.0082 |         177.1178 |           6.8559 |
[32m[20221213 21:31:51 @agent_ppo2.py:185][0m |          -0.0084 |         176.7900 |           6.8713 |
[32m[20221213 21:31:51 @agent_ppo2.py:185][0m |          -0.0105 |         176.5922 |           6.9391 |
[32m[20221213 21:31:51 @agent_ppo2.py:185][0m |          -0.0098 |         176.2254 |           6.9419 |
[32m[20221213 21:31:51 @agent_ppo2.py:185][0m |          -0.0068 |         176.6864 |           6.9874 |
[32m[20221213 21:31:51 @agent_ppo2.py:185][0m |          -0.0089 |         175.8582 |           7.0205 |
[32m[20221213 21:31:52 @agent_ppo2.py:185][0m |          -0.0111 |         175.8950 |           7.0340 |
[32m[20221213 21:31:52 @agent_ppo2.py:185][0m |          -0.0096 |         176.2939 |           7.0373 |
[32m[20221213 21:31:52 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:31:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.80
[32m[20221213 21:31:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.00
[32m[20221213 21:31:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.00
[32m[20221213 21:31:52 @agent_ppo2.py:143][0m Total time:      36.28 min
[32m[20221213 21:31:52 @agent_ppo2.py:145][0m 3538944 total steps have happened
[32m[20221213 21:31:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1728 --------------------------#
[32m[20221213 21:31:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:52 @agent_ppo2.py:185][0m |          -0.0008 |         174.8478 |           7.2812 |
[32m[20221213 21:31:52 @agent_ppo2.py:185][0m |          -0.0035 |         174.2251 |           7.3003 |
[32m[20221213 21:31:52 @agent_ppo2.py:185][0m |          -0.0069 |         173.9933 |           7.2968 |
[32m[20221213 21:31:52 @agent_ppo2.py:185][0m |          -0.0080 |         173.7724 |           7.3002 |
[32m[20221213 21:31:52 @agent_ppo2.py:185][0m |          -0.0065 |         173.7189 |           7.2470 |
[32m[20221213 21:31:53 @agent_ppo2.py:185][0m |          -0.0088 |         173.4086 |           7.2704 |
[32m[20221213 21:31:53 @agent_ppo2.py:185][0m |          -0.0085 |         173.1716 |           7.3183 |
[32m[20221213 21:31:53 @agent_ppo2.py:185][0m |          -0.0088 |         173.1513 |           7.3203 |
[32m[20221213 21:31:53 @agent_ppo2.py:185][0m |          -0.0105 |         173.1734 |           7.3209 |
[32m[20221213 21:31:53 @agent_ppo2.py:185][0m |          -0.0071 |         172.8774 |           7.3320 |
[32m[20221213 21:31:53 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:31:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.80
[32m[20221213 21:31:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:31:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.00
[32m[20221213 21:31:53 @agent_ppo2.py:143][0m Total time:      36.31 min
[32m[20221213 21:31:53 @agent_ppo2.py:145][0m 3540992 total steps have happened
[32m[20221213 21:31:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1729 --------------------------#
[32m[20221213 21:31:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:31:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:53 @agent_ppo2.py:185][0m |          -0.0013 |         174.6835 |           7.0151 |
[32m[20221213 21:31:53 @agent_ppo2.py:185][0m |          -0.0032 |         173.4369 |           6.9964 |
[32m[20221213 21:31:54 @agent_ppo2.py:185][0m |          -0.0032 |         173.0129 |           6.9679 |
[32m[20221213 21:31:54 @agent_ppo2.py:185][0m |          -0.0029 |         172.6021 |           6.9734 |
[32m[20221213 21:31:54 @agent_ppo2.py:185][0m |          -0.0024 |         172.7162 |           6.9855 |
[32m[20221213 21:31:54 @agent_ppo2.py:185][0m |          -0.0072 |         172.1414 |           6.9796 |
[32m[20221213 21:31:54 @agent_ppo2.py:185][0m |          -0.0053 |         172.3444 |           7.0537 |
[32m[20221213 21:31:54 @agent_ppo2.py:185][0m |          -0.0069 |         171.7931 |           7.0192 |
[32m[20221213 21:31:54 @agent_ppo2.py:185][0m |          -0.0062 |         171.6513 |           7.0521 |
[32m[20221213 21:31:54 @agent_ppo2.py:185][0m |           0.0013 |         181.6081 |           7.0581 |
[32m[20221213 21:31:54 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:31:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.00
[32m[20221213 21:31:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.00
[32m[20221213 21:31:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 21:31:54 @agent_ppo2.py:143][0m Total time:      36.33 min
[32m[20221213 21:31:54 @agent_ppo2.py:145][0m 3543040 total steps have happened
[32m[20221213 21:31:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1730 --------------------------#
[32m[20221213 21:31:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:55 @agent_ppo2.py:185][0m |           0.0032 |         172.4657 |           7.3874 |
[32m[20221213 21:31:55 @agent_ppo2.py:185][0m |           0.0135 |         191.9236 |           7.3828 |
[32m[20221213 21:31:55 @agent_ppo2.py:185][0m |          -0.0021 |         171.6240 |           7.3730 |
[32m[20221213 21:31:55 @agent_ppo2.py:185][0m |          -0.0034 |         171.3688 |           7.3729 |
[32m[20221213 21:31:55 @agent_ppo2.py:185][0m |          -0.0034 |         171.3020 |           7.3449 |
[32m[20221213 21:31:55 @agent_ppo2.py:185][0m |          -0.0048 |         171.1433 |           7.3445 |
[32m[20221213 21:31:55 @agent_ppo2.py:185][0m |          -0.0039 |         171.1562 |           7.3802 |
[32m[20221213 21:31:55 @agent_ppo2.py:185][0m |          -0.0046 |         170.9165 |           7.3690 |
[32m[20221213 21:31:55 @agent_ppo2.py:185][0m |          -0.0038 |         171.2568 |           7.3838 |
[32m[20221213 21:31:56 @agent_ppo2.py:185][0m |          -0.0051 |         170.7847 |           7.3363 |
[32m[20221213 21:31:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:31:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.80
[32m[20221213 21:31:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:31:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.00
[32m[20221213 21:31:56 @agent_ppo2.py:143][0m Total time:      36.35 min
[32m[20221213 21:31:56 @agent_ppo2.py:145][0m 3545088 total steps have happened
[32m[20221213 21:31:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1731 --------------------------#
[32m[20221213 21:31:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:56 @agent_ppo2.py:185][0m |           0.0001 |         174.8243 |           7.0957 |
[32m[20221213 21:31:56 @agent_ppo2.py:185][0m |          -0.0047 |         172.4791 |           7.1105 |
[32m[20221213 21:31:56 @agent_ppo2.py:185][0m |           0.0040 |         188.4194 |           7.0952 |
[32m[20221213 21:31:56 @agent_ppo2.py:185][0m |          -0.0021 |         169.8997 |           7.1197 |
[32m[20221213 21:31:56 @agent_ppo2.py:185][0m |          -0.0077 |         168.1332 |           7.1681 |
[32m[20221213 21:31:56 @agent_ppo2.py:185][0m |          -0.0084 |         167.3683 |           7.1550 |
[32m[20221213 21:31:57 @agent_ppo2.py:185][0m |          -0.0088 |         166.7156 |           7.1274 |
[32m[20221213 21:31:57 @agent_ppo2.py:185][0m |          -0.0038 |         168.3986 |           7.1476 |
[32m[20221213 21:31:57 @agent_ppo2.py:185][0m |          -0.0062 |         168.9643 |           7.1779 |
[32m[20221213 21:31:57 @agent_ppo2.py:185][0m |          -0.0108 |         165.2681 |           7.1949 |
[32m[20221213 21:31:57 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:31:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.00
[32m[20221213 21:31:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:31:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 21:31:57 @agent_ppo2.py:143][0m Total time:      36.37 min
[32m[20221213 21:31:57 @agent_ppo2.py:145][0m 3547136 total steps have happened
[32m[20221213 21:31:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1732 --------------------------#
[32m[20221213 21:31:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:57 @agent_ppo2.py:185][0m |           0.0045 |         180.1141 |           7.3843 |
[32m[20221213 21:31:57 @agent_ppo2.py:185][0m |          -0.0035 |         176.7858 |           7.3762 |
[32m[20221213 21:31:57 @agent_ppo2.py:185][0m |          -0.0047 |         176.1605 |           7.3834 |
[32m[20221213 21:31:57 @agent_ppo2.py:185][0m |          -0.0034 |         175.8550 |           7.3946 |
[32m[20221213 21:31:58 @agent_ppo2.py:185][0m |          -0.0051 |         175.7539 |           7.3750 |
[32m[20221213 21:31:58 @agent_ppo2.py:185][0m |          -0.0071 |         175.3897 |           7.3970 |
[32m[20221213 21:31:58 @agent_ppo2.py:185][0m |          -0.0020 |         178.0240 |           7.3891 |
[32m[20221213 21:31:58 @agent_ppo2.py:185][0m |          -0.0076 |         175.0351 |           7.3571 |
[32m[20221213 21:31:58 @agent_ppo2.py:185][0m |          -0.0094 |         175.0293 |           7.3004 |
[32m[20221213 21:31:58 @agent_ppo2.py:185][0m |          -0.0068 |         174.8178 |           7.4183 |
[32m[20221213 21:31:58 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:31:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.40
[32m[20221213 21:31:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.00
[32m[20221213 21:31:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.00
[32m[20221213 21:31:58 @agent_ppo2.py:143][0m Total time:      36.39 min
[32m[20221213 21:31:58 @agent_ppo2.py:145][0m 3549184 total steps have happened
[32m[20221213 21:31:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1733 --------------------------#
[32m[20221213 21:31:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:31:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:31:58 @agent_ppo2.py:185][0m |           0.0098 |         184.4680 |           7.0716 |
[32m[20221213 21:31:59 @agent_ppo2.py:185][0m |           0.0008 |         176.2347 |           7.0229 |
[32m[20221213 21:31:59 @agent_ppo2.py:185][0m |          -0.0056 |         175.6381 |           6.9597 |
[32m[20221213 21:31:59 @agent_ppo2.py:185][0m |          -0.0050 |         175.3302 |           6.9443 |
[32m[20221213 21:31:59 @agent_ppo2.py:185][0m |          -0.0069 |         175.0032 |           6.9447 |
[32m[20221213 21:31:59 @agent_ppo2.py:185][0m |          -0.0079 |         174.8170 |           6.9323 |
[32m[20221213 21:31:59 @agent_ppo2.py:185][0m |          -0.0060 |         174.7574 |           6.9218 |
[32m[20221213 21:31:59 @agent_ppo2.py:185][0m |          -0.0069 |         174.4530 |           6.9343 |
[32m[20221213 21:31:59 @agent_ppo2.py:185][0m |          -0.0095 |         174.3644 |           6.8694 |
[32m[20221213 21:31:59 @agent_ppo2.py:185][0m |          -0.0087 |         174.1819 |           6.8560 |
[32m[20221213 21:31:59 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:31:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.40
[32m[20221213 21:31:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.00
[32m[20221213 21:31:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.00
[32m[20221213 21:31:59 @agent_ppo2.py:143][0m Total time:      36.41 min
[32m[20221213 21:31:59 @agent_ppo2.py:145][0m 3551232 total steps have happened
[32m[20221213 21:31:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1734 --------------------------#
[32m[20221213 21:32:00 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:32:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:00 @agent_ppo2.py:185][0m |           0.0004 |         175.9453 |           6.7591 |
[32m[20221213 21:32:00 @agent_ppo2.py:185][0m |          -0.0047 |         175.0086 |           6.7447 |
[32m[20221213 21:32:00 @agent_ppo2.py:185][0m |          -0.0046 |         174.3726 |           6.7715 |
[32m[20221213 21:32:00 @agent_ppo2.py:185][0m |          -0.0016 |         175.7639 |           6.7537 |
[32m[20221213 21:32:00 @agent_ppo2.py:185][0m |          -0.0053 |         173.6313 |           6.7700 |
[32m[20221213 21:32:00 @agent_ppo2.py:185][0m |          -0.0068 |         173.3868 |           6.7386 |
[32m[20221213 21:32:00 @agent_ppo2.py:185][0m |          -0.0080 |         173.2680 |           6.7595 |
[32m[20221213 21:32:00 @agent_ppo2.py:185][0m |          -0.0040 |         173.9438 |           6.7587 |
[32m[20221213 21:32:00 @agent_ppo2.py:185][0m |          -0.0061 |         173.1482 |           6.6880 |
[32m[20221213 21:32:00 @agent_ppo2.py:185][0m |          -0.0089 |         172.7782 |           6.7399 |
[32m[20221213 21:32:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.60
[32m[20221213 21:32:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.00
[32m[20221213 21:32:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.00
[32m[20221213 21:32:01 @agent_ppo2.py:143][0m Total time:      36.43 min
[32m[20221213 21:32:01 @agent_ppo2.py:145][0m 3553280 total steps have happened
[32m[20221213 21:32:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1735 --------------------------#
[32m[20221213 21:32:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:01 @agent_ppo2.py:185][0m |          -0.0004 |         176.2080 |           7.1531 |
[32m[20221213 21:32:01 @agent_ppo2.py:185][0m |          -0.0011 |         173.8762 |           7.2761 |
[32m[20221213 21:32:01 @agent_ppo2.py:185][0m |          -0.0017 |         172.0659 |           7.2405 |
[32m[20221213 21:32:01 @agent_ppo2.py:185][0m |           0.0029 |         178.6202 |           7.2619 |
[32m[20221213 21:32:01 @agent_ppo2.py:185][0m |          -0.0062 |         168.3422 |           7.2440 |
[32m[20221213 21:32:01 @agent_ppo2.py:185][0m |          -0.0075 |         167.3955 |           7.2409 |
[32m[20221213 21:32:01 @agent_ppo2.py:185][0m |          -0.0090 |         166.8525 |           7.2966 |
[32m[20221213 21:32:02 @agent_ppo2.py:185][0m |          -0.0082 |         166.2958 |           7.2654 |
[32m[20221213 21:32:02 @agent_ppo2.py:185][0m |          -0.0079 |         165.8893 |           7.2954 |
[32m[20221213 21:32:02 @agent_ppo2.py:185][0m |          -0.0095 |         165.5812 |           7.2911 |
[32m[20221213 21:32:02 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:32:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.80
[32m[20221213 21:32:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.00
[32m[20221213 21:32:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.00
[32m[20221213 21:32:02 @agent_ppo2.py:143][0m Total time:      36.45 min
[32m[20221213 21:32:02 @agent_ppo2.py:145][0m 3555328 total steps have happened
[32m[20221213 21:32:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1736 --------------------------#
[32m[20221213 21:32:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:02 @agent_ppo2.py:185][0m |          -0.0015 |         177.3185 |           6.5480 |
[32m[20221213 21:32:02 @agent_ppo2.py:185][0m |          -0.0044 |         176.3067 |           6.5699 |
[32m[20221213 21:32:02 @agent_ppo2.py:185][0m |          -0.0069 |         175.9565 |           6.5203 |
[32m[20221213 21:32:02 @agent_ppo2.py:185][0m |          -0.0023 |         177.7956 |           6.5281 |
[32m[20221213 21:32:02 @agent_ppo2.py:185][0m |          -0.0088 |         175.3819 |           6.5894 |
[32m[20221213 21:32:03 @agent_ppo2.py:185][0m |          -0.0075 |         175.3023 |           6.5952 |
[32m[20221213 21:32:03 @agent_ppo2.py:185][0m |          -0.0069 |         176.1701 |           6.5940 |
[32m[20221213 21:32:03 @agent_ppo2.py:185][0m |          -0.0089 |         174.9943 |           6.5945 |
[32m[20221213 21:32:03 @agent_ppo2.py:185][0m |          -0.0100 |         174.8853 |           6.5864 |
[32m[20221213 21:32:03 @agent_ppo2.py:185][0m |          -0.0095 |         174.8115 |           6.6629 |
[32m[20221213 21:32:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:32:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.80
[32m[20221213 21:32:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.00
[32m[20221213 21:32:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.00
[32m[20221213 21:32:03 @agent_ppo2.py:143][0m Total time:      36.47 min
[32m[20221213 21:32:03 @agent_ppo2.py:145][0m 3557376 total steps have happened
[32m[20221213 21:32:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1737 --------------------------#
[32m[20221213 21:32:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:03 @agent_ppo2.py:185][0m |          -0.0011 |         173.5366 |           6.7213 |
[32m[20221213 21:32:03 @agent_ppo2.py:185][0m |          -0.0059 |         172.6848 |           6.7029 |
[32m[20221213 21:32:04 @agent_ppo2.py:185][0m |          -0.0067 |         172.4796 |           6.6947 |
[32m[20221213 21:32:04 @agent_ppo2.py:185][0m |          -0.0077 |         172.2240 |           6.7280 |
[32m[20221213 21:32:04 @agent_ppo2.py:185][0m |          -0.0077 |         171.9678 |           6.7300 |
[32m[20221213 21:32:04 @agent_ppo2.py:185][0m |          -0.0093 |         171.8986 |           6.7459 |
[32m[20221213 21:32:04 @agent_ppo2.py:185][0m |          -0.0085 |         171.6162 |           6.7067 |
[32m[20221213 21:32:04 @agent_ppo2.py:185][0m |          -0.0096 |         171.5116 |           6.6885 |
[32m[20221213 21:32:04 @agent_ppo2.py:185][0m |          -0.0105 |         171.5216 |           6.7050 |
[32m[20221213 21:32:04 @agent_ppo2.py:185][0m |          -0.0098 |         171.4412 |           6.6833 |
[32m[20221213 21:32:04 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:32:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.60
[32m[20221213 21:32:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:32:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.00
[32m[20221213 21:32:04 @agent_ppo2.py:143][0m Total time:      36.49 min
[32m[20221213 21:32:04 @agent_ppo2.py:145][0m 3559424 total steps have happened
[32m[20221213 21:32:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1738 --------------------------#
[32m[20221213 21:32:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:05 @agent_ppo2.py:185][0m |           0.0001 |         174.8961 |           6.8392 |
[32m[20221213 21:32:05 @agent_ppo2.py:185][0m |          -0.0014 |         175.1156 |           6.8768 |
[32m[20221213 21:32:05 @agent_ppo2.py:185][0m |          -0.0059 |         173.4510 |           6.9224 |
[32m[20221213 21:32:05 @agent_ppo2.py:185][0m |          -0.0083 |         172.9863 |           6.8737 |
[32m[20221213 21:32:05 @agent_ppo2.py:185][0m |          -0.0069 |         172.6800 |           6.8895 |
[32m[20221213 21:32:05 @agent_ppo2.py:185][0m |          -0.0089 |         172.4731 |           6.8829 |
[32m[20221213 21:32:05 @agent_ppo2.py:185][0m |          -0.0082 |         172.3524 |           6.8662 |
[32m[20221213 21:32:05 @agent_ppo2.py:185][0m |          -0.0087 |         172.2535 |           6.8342 |
[32m[20221213 21:32:05 @agent_ppo2.py:185][0m |          -0.0081 |         171.8988 |           6.8714 |
[32m[20221213 21:32:05 @agent_ppo2.py:185][0m |          -0.0111 |         171.7102 |           6.8944 |
[32m[20221213 21:32:05 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:32:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.40
[32m[20221213 21:32:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.00
[32m[20221213 21:32:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.00
[32m[20221213 21:32:06 @agent_ppo2.py:143][0m Total time:      36.51 min
[32m[20221213 21:32:06 @agent_ppo2.py:145][0m 3561472 total steps have happened
[32m[20221213 21:32:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1739 --------------------------#
[32m[20221213 21:32:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:32:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:06 @agent_ppo2.py:185][0m |           0.0033 |         173.9988 |           6.6451 |
[32m[20221213 21:32:06 @agent_ppo2.py:185][0m |          -0.0014 |         173.3503 |           6.6428 |
[32m[20221213 21:32:06 @agent_ppo2.py:185][0m |           0.0020 |         176.4152 |           6.6646 |
[32m[20221213 21:32:06 @agent_ppo2.py:185][0m |          -0.0049 |         172.6527 |           6.6004 |
[32m[20221213 21:32:06 @agent_ppo2.py:185][0m |          -0.0065 |         172.5182 |           6.6476 |
[32m[20221213 21:32:06 @agent_ppo2.py:185][0m |          -0.0053 |         172.2067 |           6.6079 |
[32m[20221213 21:32:06 @agent_ppo2.py:185][0m |          -0.0062 |         172.0736 |           6.5845 |
[32m[20221213 21:32:07 @agent_ppo2.py:185][0m |          -0.0070 |         171.9644 |           6.6103 |
[32m[20221213 21:32:07 @agent_ppo2.py:185][0m |          -0.0060 |         171.8559 |           6.6558 |
[32m[20221213 21:32:07 @agent_ppo2.py:185][0m |          -0.0041 |         171.7572 |           6.6161 |
[32m[20221213 21:32:07 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:32:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.80
[32m[20221213 21:32:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:32:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.00
[32m[20221213 21:32:07 @agent_ppo2.py:143][0m Total time:      36.54 min
[32m[20221213 21:32:07 @agent_ppo2.py:145][0m 3563520 total steps have happened
[32m[20221213 21:32:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1740 --------------------------#
[32m[20221213 21:32:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:07 @agent_ppo2.py:185][0m |           0.0103 |         190.0720 |           6.7904 |
[32m[20221213 21:32:07 @agent_ppo2.py:185][0m |          -0.0040 |         175.3046 |           6.8913 |
[32m[20221213 21:32:07 @agent_ppo2.py:185][0m |           0.0119 |         195.8129 |           6.9888 |
[32m[20221213 21:32:07 @agent_ppo2.py:185][0m |          -0.0044 |         174.7459 |           6.9443 |
[32m[20221213 21:32:08 @agent_ppo2.py:185][0m |          -0.0041 |         175.3275 |           6.9841 |
[32m[20221213 21:32:08 @agent_ppo2.py:185][0m |          -0.0004 |         180.7841 |           7.0217 |
[32m[20221213 21:32:08 @agent_ppo2.py:185][0m |          -0.0079 |         174.0583 |           7.0499 |
[32m[20221213 21:32:08 @agent_ppo2.py:185][0m |           0.0010 |         185.1267 |           7.0842 |
[32m[20221213 21:32:08 @agent_ppo2.py:185][0m |          -0.0076 |         173.8144 |           7.1375 |
[32m[20221213 21:32:08 @agent_ppo2.py:185][0m |          -0.0089 |         173.6513 |           7.1298 |
[32m[20221213 21:32:08 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:32:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.60
[32m[20221213 21:32:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.00
[32m[20221213 21:32:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.00
[32m[20221213 21:32:08 @agent_ppo2.py:143][0m Total time:      36.56 min
[32m[20221213 21:32:08 @agent_ppo2.py:145][0m 3565568 total steps have happened
[32m[20221213 21:32:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1741 --------------------------#
[32m[20221213 21:32:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:08 @agent_ppo2.py:185][0m |          -0.0013 |         175.9912 |           7.3660 |
[32m[20221213 21:32:08 @agent_ppo2.py:185][0m |          -0.0039 |         175.5137 |           7.3727 |
[32m[20221213 21:32:09 @agent_ppo2.py:185][0m |          -0.0001 |         178.3380 |           7.3652 |
[32m[20221213 21:32:09 @agent_ppo2.py:185][0m |          -0.0053 |         175.0226 |           7.3494 |
[32m[20221213 21:32:09 @agent_ppo2.py:185][0m |          -0.0013 |         179.3498 |           7.3656 |
[32m[20221213 21:32:09 @agent_ppo2.py:185][0m |          -0.0051 |         175.5559 |           7.3545 |
[32m[20221213 21:32:09 @agent_ppo2.py:185][0m |          -0.0003 |         185.3851 |           7.3371 |
[32m[20221213 21:32:09 @agent_ppo2.py:185][0m |          -0.0040 |         176.4460 |           7.3600 |
[32m[20221213 21:32:09 @agent_ppo2.py:185][0m |          -0.0087 |         174.1111 |           7.3623 |
[32m[20221213 21:32:09 @agent_ppo2.py:185][0m |          -0.0093 |         173.9439 |           7.3350 |
[32m[20221213 21:32:09 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:32:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.00
[32m[20221213 21:32:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:32:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.00
[32m[20221213 21:32:09 @agent_ppo2.py:143][0m Total time:      36.58 min
[32m[20221213 21:32:09 @agent_ppo2.py:145][0m 3567616 total steps have happened
[32m[20221213 21:32:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1742 --------------------------#
[32m[20221213 21:32:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:32:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:10 @agent_ppo2.py:185][0m |          -0.0017 |         175.8735 |           6.3861 |
[32m[20221213 21:32:10 @agent_ppo2.py:185][0m |          -0.0060 |         175.2332 |           6.3515 |
[32m[20221213 21:32:10 @agent_ppo2.py:185][0m |          -0.0070 |         174.8294 |           6.3401 |
[32m[20221213 21:32:10 @agent_ppo2.py:185][0m |          -0.0078 |         174.4607 |           6.3542 |
[32m[20221213 21:32:10 @agent_ppo2.py:185][0m |          -0.0084 |         174.2776 |           6.3261 |
[32m[20221213 21:32:10 @agent_ppo2.py:185][0m |          -0.0094 |         174.0488 |           6.3731 |
[32m[20221213 21:32:10 @agent_ppo2.py:185][0m |          -0.0052 |         176.8928 |           6.3480 |
[32m[20221213 21:32:10 @agent_ppo2.py:185][0m |          -0.0094 |         173.6436 |           6.4188 |
[32m[20221213 21:32:10 @agent_ppo2.py:185][0m |          -0.0094 |         173.5692 |           6.3667 |
[32m[20221213 21:32:10 @agent_ppo2.py:185][0m |          -0.0099 |         173.4157 |           6.4235 |
[32m[20221213 21:32:10 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:32:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.00
[32m[20221213 21:32:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:32:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.00
[32m[20221213 21:32:11 @agent_ppo2.py:143][0m Total time:      36.60 min
[32m[20221213 21:32:11 @agent_ppo2.py:145][0m 3569664 total steps have happened
[32m[20221213 21:32:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1743 --------------------------#
[32m[20221213 21:32:11 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:32:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:11 @agent_ppo2.py:185][0m |          -0.0007 |         176.2289 |           7.0770 |
[32m[20221213 21:32:11 @agent_ppo2.py:185][0m |          -0.0055 |         175.5675 |           7.1094 |
[32m[20221213 21:32:11 @agent_ppo2.py:185][0m |          -0.0036 |         175.4813 |           7.0534 |
[32m[20221213 21:32:11 @agent_ppo2.py:185][0m |          -0.0057 |         174.9365 |           7.0669 |
[32m[20221213 21:32:11 @agent_ppo2.py:185][0m |          -0.0074 |         174.7582 |           7.0518 |
[32m[20221213 21:32:11 @agent_ppo2.py:185][0m |          -0.0070 |         174.6111 |           7.0704 |
[32m[20221213 21:32:11 @agent_ppo2.py:185][0m |          -0.0093 |         174.3802 |           7.0586 |
[32m[20221213 21:32:11 @agent_ppo2.py:185][0m |          -0.0086 |         174.3179 |           7.0350 |
[32m[20221213 21:32:12 @agent_ppo2.py:185][0m |          -0.0091 |         174.1946 |           7.0509 |
[32m[20221213 21:32:12 @agent_ppo2.py:185][0m |           0.0030 |         196.9485 |           7.0103 |
[32m[20221213 21:32:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:32:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.00
[32m[20221213 21:32:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.00
[32m[20221213 21:32:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.00
[32m[20221213 21:32:12 @agent_ppo2.py:143][0m Total time:      36.62 min
[32m[20221213 21:32:12 @agent_ppo2.py:145][0m 3571712 total steps have happened
[32m[20221213 21:32:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1744 --------------------------#
[32m[20221213 21:32:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:12 @agent_ppo2.py:185][0m |          -0.0022 |         174.6412 |           6.8337 |
[32m[20221213 21:32:12 @agent_ppo2.py:185][0m |          -0.0038 |         174.3180 |           6.9038 |
[32m[20221213 21:32:12 @agent_ppo2.py:185][0m |          -0.0043 |         173.9853 |           6.9018 |
[32m[20221213 21:32:12 @agent_ppo2.py:185][0m |          -0.0062 |         173.9462 |           6.9591 |
[32m[20221213 21:32:12 @agent_ppo2.py:185][0m |          -0.0064 |         173.8569 |           6.9664 |
[32m[20221213 21:32:12 @agent_ppo2.py:185][0m |          -0.0060 |         173.5945 |           6.9925 |
[32m[20221213 21:32:13 @agent_ppo2.py:185][0m |          -0.0037 |         174.4196 |           7.0366 |
[32m[20221213 21:32:13 @agent_ppo2.py:185][0m |          -0.0063 |         173.4618 |           7.0353 |
[32m[20221213 21:32:13 @agent_ppo2.py:185][0m |          -0.0062 |         173.3711 |           7.0243 |
[32m[20221213 21:32:13 @agent_ppo2.py:185][0m |          -0.0079 |         173.3933 |           7.0858 |
[32m[20221213 21:32:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.00
[32m[20221213 21:32:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221213 21:32:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.00
[32m[20221213 21:32:13 @agent_ppo2.py:143][0m Total time:      36.64 min
[32m[20221213 21:32:13 @agent_ppo2.py:145][0m 3573760 total steps have happened
[32m[20221213 21:32:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1745 --------------------------#
[32m[20221213 21:32:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:13 @agent_ppo2.py:185][0m |          -0.0007 |         177.2077 |           6.9901 |
[32m[20221213 21:32:13 @agent_ppo2.py:185][0m |          -0.0027 |         177.1128 |           6.9557 |
[32m[20221213 21:32:13 @agent_ppo2.py:185][0m |          -0.0078 |         175.3733 |           6.9895 |
[32m[20221213 21:32:13 @agent_ppo2.py:185][0m |          -0.0076 |         175.1259 |           6.9862 |
[32m[20221213 21:32:14 @agent_ppo2.py:185][0m |          -0.0063 |         174.8523 |           6.9437 |
[32m[20221213 21:32:14 @agent_ppo2.py:185][0m |          -0.0081 |         174.6438 |           6.9780 |
[32m[20221213 21:32:14 @agent_ppo2.py:185][0m |          -0.0084 |         174.6151 |           7.0149 |
[32m[20221213 21:32:14 @agent_ppo2.py:185][0m |          -0.0096 |         174.3646 |           6.9813 |
[32m[20221213 21:32:14 @agent_ppo2.py:185][0m |          -0.0096 |         174.2897 |           6.9941 |
[32m[20221213 21:32:14 @agent_ppo2.py:185][0m |          -0.0018 |         182.9369 |           6.9754 |
[32m[20221213 21:32:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.80
[32m[20221213 21:32:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:32:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 860.00
[32m[20221213 21:32:14 @agent_ppo2.py:143][0m Total time:      36.66 min
[32m[20221213 21:32:14 @agent_ppo2.py:145][0m 3575808 total steps have happened
[32m[20221213 21:32:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1746 --------------------------#
[32m[20221213 21:32:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:14 @agent_ppo2.py:185][0m |           0.0082 |         188.4599 |           7.0980 |
[32m[20221213 21:32:15 @agent_ppo2.py:185][0m |           0.0094 |         193.8824 |           7.1018 |
[32m[20221213 21:32:15 @agent_ppo2.py:185][0m |          -0.0016 |         176.5338 |           7.0804 |
[32m[20221213 21:32:15 @agent_ppo2.py:185][0m |          -0.0034 |         176.1160 |           7.0493 |
[32m[20221213 21:32:15 @agent_ppo2.py:185][0m |          -0.0068 |         175.6033 |           7.0381 |
[32m[20221213 21:32:15 @agent_ppo2.py:185][0m |          -0.0066 |         175.0736 |           7.0735 |
[32m[20221213 21:32:15 @agent_ppo2.py:185][0m |          -0.0065 |         174.6826 |           7.0358 |
[32m[20221213 21:32:15 @agent_ppo2.py:185][0m |           0.0067 |         187.1061 |           6.9897 |
[32m[20221213 21:32:15 @agent_ppo2.py:185][0m |          -0.0064 |         174.9896 |           7.0867 |
[32m[20221213 21:32:15 @agent_ppo2.py:185][0m |          -0.0077 |         174.0041 |           7.0252 |
[32m[20221213 21:32:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.60
[32m[20221213 21:32:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.00
[32m[20221213 21:32:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.00
[32m[20221213 21:32:15 @agent_ppo2.py:143][0m Total time:      36.68 min
[32m[20221213 21:32:15 @agent_ppo2.py:145][0m 3577856 total steps have happened
[32m[20221213 21:32:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1747 --------------------------#
[32m[20221213 21:32:15 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:32:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:16 @agent_ppo2.py:185][0m |           0.0027 |         179.8541 |           7.1275 |
[32m[20221213 21:32:16 @agent_ppo2.py:185][0m |          -0.0032 |         174.1658 |           7.1212 |
[32m[20221213 21:32:16 @agent_ppo2.py:185][0m |          -0.0062 |         172.6789 |           7.1536 |
[32m[20221213 21:32:16 @agent_ppo2.py:185][0m |          -0.0076 |         171.3284 |           7.0774 |
[32m[20221213 21:32:16 @agent_ppo2.py:185][0m |          -0.0071 |         170.5526 |           7.0783 |
[32m[20221213 21:32:16 @agent_ppo2.py:185][0m |          -0.0037 |         173.7888 |           7.0946 |
[32m[20221213 21:32:16 @agent_ppo2.py:185][0m |          -0.0083 |         169.3014 |           7.0304 |
[32m[20221213 21:32:16 @agent_ppo2.py:185][0m |          -0.0085 |         168.7952 |           7.0851 |
[32m[20221213 21:32:16 @agent_ppo2.py:185][0m |          -0.0100 |         168.4998 |           6.9746 |
[32m[20221213 21:32:16 @agent_ppo2.py:185][0m |          -0.0114 |         168.0563 |           7.0025 |
[32m[20221213 21:32:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:32:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.00
[32m[20221213 21:32:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.00
[32m[20221213 21:32:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.00
[32m[20221213 21:32:17 @agent_ppo2.py:143][0m Total time:      36.70 min
[32m[20221213 21:32:17 @agent_ppo2.py:145][0m 3579904 total steps have happened
[32m[20221213 21:32:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1748 --------------------------#
[32m[20221213 21:32:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:17 @agent_ppo2.py:185][0m |           0.0008 |         174.7240 |           6.6663 |
[32m[20221213 21:32:17 @agent_ppo2.py:185][0m |          -0.0026 |         173.5511 |           6.7181 |
[32m[20221213 21:32:17 @agent_ppo2.py:185][0m |          -0.0059 |         172.5019 |           6.7272 |
[32m[20221213 21:32:17 @agent_ppo2.py:185][0m |          -0.0068 |         171.9115 |           6.7523 |
[32m[20221213 21:32:17 @agent_ppo2.py:185][0m |           0.0022 |         179.7927 |           6.7247 |
[32m[20221213 21:32:17 @agent_ppo2.py:185][0m |          -0.0028 |         173.2667 |           6.7350 |
[32m[20221213 21:32:17 @agent_ppo2.py:185][0m |          -0.0072 |         171.5080 |           6.7312 |
[32m[20221213 21:32:17 @agent_ppo2.py:185][0m |          -0.0024 |         175.9452 |           6.7338 |
[32m[20221213 21:32:18 @agent_ppo2.py:185][0m |          -0.0069 |         170.9251 |           6.8313 |
[32m[20221213 21:32:18 @agent_ppo2.py:185][0m |          -0.0076 |         170.6900 |           6.7582 |
[32m[20221213 21:32:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:32:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.20
[32m[20221213 21:32:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.00
[32m[20221213 21:32:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.00
[32m[20221213 21:32:18 @agent_ppo2.py:143][0m Total time:      36.72 min
[32m[20221213 21:32:18 @agent_ppo2.py:145][0m 3581952 total steps have happened
[32m[20221213 21:32:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1749 --------------------------#
[32m[20221213 21:32:18 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:32:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:18 @agent_ppo2.py:185][0m |          -0.0002 |         179.2765 |           6.8154 |
[32m[20221213 21:32:18 @agent_ppo2.py:185][0m |          -0.0035 |         176.6974 |           6.8734 |
[32m[20221213 21:32:18 @agent_ppo2.py:185][0m |          -0.0036 |         175.1069 |           6.8854 |
[32m[20221213 21:32:18 @agent_ppo2.py:185][0m |          -0.0061 |         174.1133 |           6.8010 |
[32m[20221213 21:32:18 @agent_ppo2.py:185][0m |           0.0056 |         193.6661 |           6.8296 |
[32m[20221213 21:32:18 @agent_ppo2.py:185][0m |          -0.0074 |         172.9978 |           6.8756 |
[32m[20221213 21:32:19 @agent_ppo2.py:185][0m |          -0.0080 |         172.7972 |           6.8559 |
[32m[20221213 21:32:19 @agent_ppo2.py:185][0m |          -0.0068 |         172.7185 |           6.8600 |
[32m[20221213 21:32:19 @agent_ppo2.py:185][0m |          -0.0007 |         177.9263 |           6.8428 |
[32m[20221213 21:32:19 @agent_ppo2.py:185][0m |          -0.0050 |         172.5171 |           6.8362 |
[32m[20221213 21:32:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.00
[32m[20221213 21:32:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:32:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 864.00
[32m[20221213 21:32:19 @agent_ppo2.py:143][0m Total time:      36.74 min
[32m[20221213 21:32:19 @agent_ppo2.py:145][0m 3584000 total steps have happened
[32m[20221213 21:32:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1750 --------------------------#
[32m[20221213 21:32:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:19 @agent_ppo2.py:185][0m |           0.0012 |         174.8694 |           6.5039 |
[32m[20221213 21:32:19 @agent_ppo2.py:185][0m |           0.0037 |         182.8950 |           6.5521 |
[32m[20221213 21:32:19 @agent_ppo2.py:185][0m |          -0.0045 |         173.1933 |           6.5413 |
[32m[20221213 21:32:20 @agent_ppo2.py:185][0m |           0.0076 |         195.5279 |           6.4782 |
[32m[20221213 21:32:20 @agent_ppo2.py:185][0m |          -0.0059 |         172.5094 |           6.5279 |
[32m[20221213 21:32:20 @agent_ppo2.py:185][0m |          -0.0061 |         172.4042 |           6.5159 |
[32m[20221213 21:32:20 @agent_ppo2.py:185][0m |           0.0057 |         197.0327 |           6.5171 |
[32m[20221213 21:32:20 @agent_ppo2.py:185][0m |          -0.0084 |         172.2437 |           6.5206 |
[32m[20221213 21:32:20 @agent_ppo2.py:185][0m |          -0.0043 |         172.6020 |           6.5419 |
[32m[20221213 21:32:20 @agent_ppo2.py:185][0m |          -0.0048 |         173.4766 |           6.4652 |
[32m[20221213 21:32:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.20
[32m[20221213 21:32:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.00
[32m[20221213 21:32:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.00
[32m[20221213 21:32:20 @agent_ppo2.py:143][0m Total time:      36.76 min
[32m[20221213 21:32:20 @agent_ppo2.py:145][0m 3586048 total steps have happened
[32m[20221213 21:32:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1751 --------------------------#
[32m[20221213 21:32:20 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:32:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:20 @agent_ppo2.py:185][0m |          -0.0027 |         175.0413 |           6.5125 |
[32m[20221213 21:32:21 @agent_ppo2.py:185][0m |          -0.0054 |         173.8266 |           6.5818 |
[32m[20221213 21:32:21 @agent_ppo2.py:185][0m |          -0.0068 |         172.9568 |           6.5255 |
[32m[20221213 21:32:21 @agent_ppo2.py:185][0m |          -0.0074 |         172.3704 |           6.5477 |
[32m[20221213 21:32:21 @agent_ppo2.py:185][0m |          -0.0091 |         171.8825 |           6.5397 |
[32m[20221213 21:32:21 @agent_ppo2.py:185][0m |          -0.0102 |         171.3580 |           6.5912 |
[32m[20221213 21:32:21 @agent_ppo2.py:185][0m |          -0.0092 |         171.0038 |           6.5482 |
[32m[20221213 21:32:21 @agent_ppo2.py:185][0m |          -0.0113 |         170.9004 |           6.5430 |
[32m[20221213 21:32:21 @agent_ppo2.py:185][0m |          -0.0095 |         171.0573 |           6.5534 |
[32m[20221213 21:32:21 @agent_ppo2.py:185][0m |          -0.0126 |         170.3865 |           6.5475 |
[32m[20221213 21:32:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.80
[32m[20221213 21:32:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:32:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 867.00
[32m[20221213 21:32:21 @agent_ppo2.py:143][0m Total time:      36.78 min
[32m[20221213 21:32:21 @agent_ppo2.py:145][0m 3588096 total steps have happened
[32m[20221213 21:32:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1752 --------------------------#
[32m[20221213 21:32:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:32:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:22 @agent_ppo2.py:185][0m |           0.0000 |         176.9274 |           7.0242 |
[32m[20221213 21:32:22 @agent_ppo2.py:185][0m |          -0.0034 |         175.2220 |           7.0405 |
[32m[20221213 21:32:22 @agent_ppo2.py:185][0m |          -0.0047 |         173.7707 |           7.0669 |
[32m[20221213 21:32:22 @agent_ppo2.py:185][0m |          -0.0059 |         172.6717 |           7.0815 |
[32m[20221213 21:32:22 @agent_ppo2.py:185][0m |          -0.0039 |         173.5059 |           7.1254 |
[32m[20221213 21:32:22 @agent_ppo2.py:185][0m |          -0.0008 |         181.1184 |           7.1257 |
[32m[20221213 21:32:22 @agent_ppo2.py:185][0m |          -0.0092 |         169.8903 |           7.1585 |
[32m[20221213 21:32:22 @agent_ppo2.py:185][0m |          -0.0007 |         176.0548 |           7.1543 |
[32m[20221213 21:32:22 @agent_ppo2.py:185][0m |          -0.0109 |         166.7015 |           7.2201 |
[32m[20221213 21:32:22 @agent_ppo2.py:185][0m |          -0.0100 |         165.9618 |           7.2160 |
[32m[20221213 21:32:22 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:32:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.20
[32m[20221213 21:32:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 21:32:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:32:23 @agent_ppo2.py:143][0m Total time:      36.80 min
[32m[20221213 21:32:23 @agent_ppo2.py:145][0m 3590144 total steps have happened
[32m[20221213 21:32:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1753 --------------------------#
[32m[20221213 21:32:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:23 @agent_ppo2.py:185][0m |           0.0128 |         194.4588 |           6.5970 |
[32m[20221213 21:32:23 @agent_ppo2.py:185][0m |          -0.0018 |         178.3899 |           6.6640 |
[32m[20221213 21:32:23 @agent_ppo2.py:185][0m |          -0.0023 |         177.8995 |           6.6309 |
[32m[20221213 21:32:23 @agent_ppo2.py:185][0m |          -0.0016 |         177.4543 |           6.6561 |
[32m[20221213 21:32:23 @agent_ppo2.py:185][0m |          -0.0056 |         176.7769 |           6.7253 |
[32m[20221213 21:32:23 @agent_ppo2.py:185][0m |          -0.0055 |         176.6746 |           6.7155 |
[32m[20221213 21:32:23 @agent_ppo2.py:185][0m |          -0.0035 |         177.3382 |           6.7460 |
[32m[20221213 21:32:24 @agent_ppo2.py:185][0m |          -0.0067 |         176.3639 |           6.7219 |
[32m[20221213 21:32:24 @agent_ppo2.py:185][0m |          -0.0081 |         175.8999 |           6.7349 |
[32m[20221213 21:32:24 @agent_ppo2.py:185][0m |          -0.0070 |         175.8799 |           6.7870 |
[32m[20221213 21:32:24 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:32:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.40
[32m[20221213 21:32:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:32:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.00
[32m[20221213 21:32:24 @agent_ppo2.py:143][0m Total time:      36.82 min
[32m[20221213 21:32:24 @agent_ppo2.py:145][0m 3592192 total steps have happened
[32m[20221213 21:32:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1754 --------------------------#
[32m[20221213 21:32:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:24 @agent_ppo2.py:185][0m |           0.0002 |         174.8789 |           6.9336 |
[32m[20221213 21:32:24 @agent_ppo2.py:185][0m |          -0.0032 |         174.0256 |           6.9215 |
[32m[20221213 21:32:24 @agent_ppo2.py:185][0m |          -0.0035 |         173.4911 |           6.8792 |
[32m[20221213 21:32:25 @agent_ppo2.py:185][0m |          -0.0069 |         173.2332 |           6.8066 |
[32m[20221213 21:32:25 @agent_ppo2.py:185][0m |          -0.0062 |         172.8082 |           6.7635 |
[32m[20221213 21:32:25 @agent_ppo2.py:185][0m |          -0.0099 |         172.4953 |           6.8087 |
[32m[20221213 21:32:25 @agent_ppo2.py:185][0m |          -0.0083 |         172.4114 |           6.7600 |
[32m[20221213 21:32:25 @agent_ppo2.py:185][0m |          -0.0067 |         172.2886 |           6.6732 |
[32m[20221213 21:32:25 @agent_ppo2.py:185][0m |          -0.0111 |         171.9170 |           6.7159 |
[32m[20221213 21:32:25 @agent_ppo2.py:185][0m |          -0.0091 |         172.0573 |           6.6794 |
[32m[20221213 21:32:25 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:32:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.00
[32m[20221213 21:32:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.00
[32m[20221213 21:32:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 21:32:25 @agent_ppo2.py:143][0m Total time:      36.84 min
[32m[20221213 21:32:25 @agent_ppo2.py:145][0m 3594240 total steps have happened
[32m[20221213 21:32:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1755 --------------------------#
[32m[20221213 21:32:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 21:32:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:26 @agent_ppo2.py:185][0m |          -0.0008 |         174.8860 |           6.5696 |
[32m[20221213 21:32:26 @agent_ppo2.py:185][0m |          -0.0035 |         173.6850 |           6.6034 |
[32m[20221213 21:32:26 @agent_ppo2.py:185][0m |          -0.0075 |         172.1909 |           6.6049 |
[32m[20221213 21:32:26 @agent_ppo2.py:185][0m |          -0.0071 |         171.4835 |           6.6025 |
[32m[20221213 21:32:26 @agent_ppo2.py:185][0m |          -0.0079 |         171.0456 |           6.6049 |
[32m[20221213 21:32:26 @agent_ppo2.py:185][0m |          -0.0072 |         170.4969 |           6.5756 |
[32m[20221213 21:32:26 @agent_ppo2.py:185][0m |          -0.0023 |         175.9725 |           6.5703 |
[32m[20221213 21:32:26 @agent_ppo2.py:185][0m |          -0.0062 |         169.9922 |           6.6824 |
[32m[20221213 21:32:26 @agent_ppo2.py:185][0m |          -0.0077 |         172.9074 |           6.5481 |
[32m[20221213 21:32:26 @agent_ppo2.py:185][0m |          -0.0053 |         170.7508 |           6.5425 |
[32m[20221213 21:32:26 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 21:32:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.20
[32m[20221213 21:32:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.00
[32m[20221213 21:32:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.00
[32m[20221213 21:32:27 @agent_ppo2.py:143][0m Total time:      36.86 min
[32m[20221213 21:32:27 @agent_ppo2.py:145][0m 3596288 total steps have happened
[32m[20221213 21:32:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1756 --------------------------#
[32m[20221213 21:32:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:27 @agent_ppo2.py:185][0m |          -0.0003 |         174.8790 |           6.0887 |
[32m[20221213 21:32:27 @agent_ppo2.py:185][0m |           0.0059 |         187.9932 |           6.0416 |
[32m[20221213 21:32:27 @agent_ppo2.py:185][0m |          -0.0030 |         172.4751 |           6.0760 |
[32m[20221213 21:32:27 @agent_ppo2.py:185][0m |          -0.0056 |         171.2691 |           6.0614 |
[32m[20221213 21:32:27 @agent_ppo2.py:185][0m |          -0.0062 |         169.4160 |           6.0520 |
[32m[20221213 21:32:27 @agent_ppo2.py:185][0m |          -0.0075 |         168.5185 |           6.0519 |
[32m[20221213 21:32:27 @agent_ppo2.py:185][0m |          -0.0083 |         167.8331 |           6.0430 |
[32m[20221213 21:32:27 @agent_ppo2.py:185][0m |          -0.0098 |         167.0275 |           6.1102 |
[32m[20221213 21:32:28 @agent_ppo2.py:185][0m |          -0.0085 |         166.0494 |           6.1063 |
[32m[20221213 21:32:28 @agent_ppo2.py:185][0m |          -0.0086 |         165.3916 |           6.1340 |
[32m[20221213 21:32:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:32:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.40
[32m[20221213 21:32:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:32:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.00
[32m[20221213 21:32:28 @agent_ppo2.py:143][0m Total time:      36.89 min
[32m[20221213 21:32:28 @agent_ppo2.py:145][0m 3598336 total steps have happened
[32m[20221213 21:32:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1757 --------------------------#
[32m[20221213 21:32:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:28 @agent_ppo2.py:185][0m |           0.0005 |         177.3657 |           6.4336 |
[32m[20221213 21:32:28 @agent_ppo2.py:185][0m |          -0.0035 |         175.7779 |           6.4460 |
[32m[20221213 21:32:28 @agent_ppo2.py:185][0m |           0.0057 |         191.0964 |           6.4096 |
[32m[20221213 21:32:28 @agent_ppo2.py:185][0m |           0.0008 |         178.2376 |           6.3909 |
[32m[20221213 21:32:29 @agent_ppo2.py:185][0m |          -0.0021 |         176.8749 |           6.3965 |
[32m[20221213 21:32:29 @agent_ppo2.py:185][0m |          -0.0072 |         173.4598 |           6.3079 |
[32m[20221213 21:32:29 @agent_ppo2.py:185][0m |          -0.0080 |         173.0731 |           6.2630 |
[32m[20221213 21:32:29 @agent_ppo2.py:185][0m |           0.0022 |         180.8608 |           6.2427 |
[32m[20221213 21:32:29 @agent_ppo2.py:185][0m |          -0.0068 |         172.5392 |           6.2344 |
[32m[20221213 21:32:29 @agent_ppo2.py:185][0m |          -0.0087 |         172.0407 |           6.2010 |
[32m[20221213 21:32:29 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:32:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.00
[32m[20221213 21:32:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:32:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.00
[32m[20221213 21:32:29 @agent_ppo2.py:143][0m Total time:      36.91 min
[32m[20221213 21:32:29 @agent_ppo2.py:145][0m 3600384 total steps have happened
[32m[20221213 21:32:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1758 --------------------------#
[32m[20221213 21:32:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:29 @agent_ppo2.py:185][0m |           0.0003 |         180.7819 |           6.1856 |
[32m[20221213 21:32:30 @agent_ppo2.py:185][0m |           0.0063 |         190.5537 |           6.2185 |
[32m[20221213 21:32:30 @agent_ppo2.py:185][0m |          -0.0073 |         177.0469 |           6.3261 |
[32m[20221213 21:32:30 @agent_ppo2.py:185][0m |          -0.0079 |         176.4422 |           6.2754 |
[32m[20221213 21:32:30 @agent_ppo2.py:185][0m |          -0.0084 |         175.4778 |           6.2922 |
[32m[20221213 21:32:30 @agent_ppo2.py:185][0m |           0.0037 |         185.8605 |           6.3133 |
[32m[20221213 21:32:30 @agent_ppo2.py:185][0m |          -0.0071 |         174.7652 |           6.3985 |
[32m[20221213 21:32:30 @agent_ppo2.py:185][0m |          -0.0068 |         174.3715 |           6.3294 |
[32m[20221213 21:32:30 @agent_ppo2.py:185][0m |          -0.0092 |         173.9773 |           6.3853 |
[32m[20221213 21:32:30 @agent_ppo2.py:185][0m |          -0.0015 |         181.5187 |           6.3657 |
[32m[20221213 21:32:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:32:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.00
[32m[20221213 21:32:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 851.00
[32m[20221213 21:32:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 21:32:30 @agent_ppo2.py:143][0m Total time:      36.93 min
[32m[20221213 21:32:30 @agent_ppo2.py:145][0m 3602432 total steps have happened
[32m[20221213 21:32:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1759 --------------------------#
[32m[20221213 21:32:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:31 @agent_ppo2.py:185][0m |           0.0089 |         185.8031 |           5.8254 |
[32m[20221213 21:32:31 @agent_ppo2.py:185][0m |           0.0038 |         175.6047 |           5.8879 |
[32m[20221213 21:32:31 @agent_ppo2.py:185][0m |          -0.0045 |         174.2680 |           5.8707 |
[32m[20221213 21:32:31 @agent_ppo2.py:185][0m |          -0.0045 |         173.8506 |           5.8326 |
[32m[20221213 21:32:31 @agent_ppo2.py:185][0m |          -0.0060 |         173.4741 |           5.9343 |
[32m[20221213 21:32:31 @agent_ppo2.py:185][0m |          -0.0063 |         173.2206 |           5.8815 |
[32m[20221213 21:32:31 @agent_ppo2.py:185][0m |          -0.0058 |         172.9622 |           5.8940 |
[32m[20221213 21:32:31 @agent_ppo2.py:185][0m |          -0.0076 |         172.8822 |           5.9029 |
[32m[20221213 21:32:31 @agent_ppo2.py:185][0m |          -0.0065 |         172.6264 |           5.9448 |
[32m[20221213 21:32:31 @agent_ppo2.py:185][0m |          -0.0063 |         172.6271 |           5.9587 |
[32m[20221213 21:32:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.60
[32m[20221213 21:32:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.00
[32m[20221213 21:32:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.00
[32m[20221213 21:32:32 @agent_ppo2.py:143][0m Total time:      36.95 min
[32m[20221213 21:32:32 @agent_ppo2.py:145][0m 3604480 total steps have happened
[32m[20221213 21:32:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1760 --------------------------#
[32m[20221213 21:32:32 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:32:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:32 @agent_ppo2.py:185][0m |           0.0022 |         176.2034 |           5.8208 |
[32m[20221213 21:32:32 @agent_ppo2.py:185][0m |          -0.0038 |         174.6271 |           5.8071 |
[32m[20221213 21:32:32 @agent_ppo2.py:185][0m |          -0.0038 |         173.8608 |           5.7931 |
[32m[20221213 21:32:32 @agent_ppo2.py:185][0m |          -0.0054 |         173.2335 |           5.8325 |
[32m[20221213 21:32:32 @agent_ppo2.py:185][0m |           0.0062 |         195.4966 |           5.8037 |
[32m[20221213 21:32:32 @agent_ppo2.py:185][0m |          -0.0054 |         172.6700 |           5.7830 |
[32m[20221213 21:32:32 @agent_ppo2.py:185][0m |          -0.0090 |         172.2794 |           5.8049 |
[32m[20221213 21:32:32 @agent_ppo2.py:185][0m |          -0.0044 |         173.2844 |           5.8101 |
[32m[20221213 21:32:33 @agent_ppo2.py:185][0m |          -0.0088 |         172.1768 |           5.7994 |
[32m[20221213 21:32:33 @agent_ppo2.py:185][0m |          -0.0075 |         171.9015 |           5.8281 |
[32m[20221213 21:32:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.60
[32m[20221213 21:32:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:32:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 864.00
[32m[20221213 21:32:33 @agent_ppo2.py:143][0m Total time:      36.97 min
[32m[20221213 21:32:33 @agent_ppo2.py:145][0m 3606528 total steps have happened
[32m[20221213 21:32:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1761 --------------------------#
[32m[20221213 21:32:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:33 @agent_ppo2.py:185][0m |          -0.0011 |         176.9036 |           6.3565 |
[32m[20221213 21:32:33 @agent_ppo2.py:185][0m |          -0.0018 |         176.7944 |           6.3656 |
[32m[20221213 21:32:33 @agent_ppo2.py:185][0m |           0.0003 |         178.2020 |           6.4176 |
[32m[20221213 21:32:33 @agent_ppo2.py:185][0m |           0.0020 |         186.1284 |           6.3819 |
[32m[20221213 21:32:33 @agent_ppo2.py:185][0m |          -0.0038 |         174.7113 |           6.4118 |
[32m[20221213 21:32:33 @agent_ppo2.py:185][0m |          -0.0083 |         174.4652 |           6.3834 |
[32m[20221213 21:32:34 @agent_ppo2.py:185][0m |           0.0013 |         191.7088 |           6.3861 |
[32m[20221213 21:32:34 @agent_ppo2.py:185][0m |          -0.0092 |         173.6815 |           6.4444 |
[32m[20221213 21:32:34 @agent_ppo2.py:185][0m |           0.0037 |         197.9908 |           6.4015 |
[32m[20221213 21:32:34 @agent_ppo2.py:185][0m |          -0.0097 |         173.4286 |           6.4198 |
[32m[20221213 21:32:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.60
[32m[20221213 21:32:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:32:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:32:34 @agent_ppo2.py:143][0m Total time:      36.99 min
[32m[20221213 21:32:34 @agent_ppo2.py:145][0m 3608576 total steps have happened
[32m[20221213 21:32:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1762 --------------------------#
[32m[20221213 21:32:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:34 @agent_ppo2.py:185][0m |           0.0005 |         174.7176 |           5.7393 |
[32m[20221213 21:32:34 @agent_ppo2.py:185][0m |          -0.0012 |         173.5832 |           5.6888 |
[32m[20221213 21:32:34 @agent_ppo2.py:185][0m |          -0.0041 |         172.7680 |           5.8556 |
[32m[20221213 21:32:35 @agent_ppo2.py:185][0m |           0.0053 |         180.2365 |           5.7959 |
[32m[20221213 21:32:35 @agent_ppo2.py:185][0m |          -0.0068 |         172.1327 |           5.7444 |
[32m[20221213 21:32:35 @agent_ppo2.py:185][0m |          -0.0042 |         173.8538 |           5.7760 |
[32m[20221213 21:32:35 @agent_ppo2.py:185][0m |          -0.0035 |         173.3723 |           5.7899 |
[32m[20221213 21:32:35 @agent_ppo2.py:185][0m |          -0.0099 |         171.2512 |           5.7258 |
[32m[20221213 21:32:35 @agent_ppo2.py:185][0m |          -0.0079 |         171.0485 |           5.7653 |
[32m[20221213 21:32:35 @agent_ppo2.py:185][0m |          -0.0049 |         172.3968 |           5.7190 |
[32m[20221213 21:32:35 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:32:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.20
[32m[20221213 21:32:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.00
[32m[20221213 21:32:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.00
[32m[20221213 21:32:35 @agent_ppo2.py:143][0m Total time:      37.01 min
[32m[20221213 21:32:35 @agent_ppo2.py:145][0m 3610624 total steps have happened
[32m[20221213 21:32:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1763 --------------------------#
[32m[20221213 21:32:35 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:32:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:35 @agent_ppo2.py:185][0m |          -0.0011 |         173.7859 |           5.0971 |
[32m[20221213 21:32:36 @agent_ppo2.py:185][0m |          -0.0037 |         172.8927 |           5.1322 |
[32m[20221213 21:32:36 @agent_ppo2.py:185][0m |           0.0028 |         178.3557 |           5.1932 |
[32m[20221213 21:32:36 @agent_ppo2.py:185][0m |          -0.0055 |         172.0796 |           5.1397 |
[32m[20221213 21:32:36 @agent_ppo2.py:185][0m |          -0.0073 |         171.9453 |           5.2285 |
[32m[20221213 21:32:36 @agent_ppo2.py:185][0m |          -0.0059 |         172.2683 |           5.2098 |
[32m[20221213 21:32:36 @agent_ppo2.py:185][0m |          -0.0093 |         171.4632 |           5.2570 |
[32m[20221213 21:32:36 @agent_ppo2.py:185][0m |          -0.0080 |         171.3148 |           5.2381 |
[32m[20221213 21:32:36 @agent_ppo2.py:185][0m |          -0.0020 |         174.6299 |           5.2736 |
[32m[20221213 21:32:36 @agent_ppo2.py:185][0m |          -0.0088 |         171.0298 |           5.2675 |
[32m[20221213 21:32:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.60
[32m[20221213 21:32:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.00
[32m[20221213 21:32:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.00
[32m[20221213 21:32:36 @agent_ppo2.py:143][0m Total time:      37.03 min
[32m[20221213 21:32:36 @agent_ppo2.py:145][0m 3612672 total steps have happened
[32m[20221213 21:32:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1764 --------------------------#
[32m[20221213 21:32:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:37 @agent_ppo2.py:185][0m |           0.0020 |         176.3708 |           5.3797 |
[32m[20221213 21:32:37 @agent_ppo2.py:185][0m |          -0.0015 |         175.2619 |           5.3163 |
[32m[20221213 21:32:37 @agent_ppo2.py:185][0m |           0.0005 |         176.4130 |           5.3037 |
[32m[20221213 21:32:37 @agent_ppo2.py:185][0m |           0.0025 |         184.1414 |           5.3125 |
[32m[20221213 21:32:37 @agent_ppo2.py:185][0m |          -0.0037 |         173.5582 |           5.3716 |
[32m[20221213 21:32:37 @agent_ppo2.py:185][0m |          -0.0063 |         173.1919 |           5.3258 |
[32m[20221213 21:32:37 @agent_ppo2.py:185][0m |           0.0047 |         192.9635 |           5.3133 |
[32m[20221213 21:32:37 @agent_ppo2.py:185][0m |           0.0055 |         186.0949 |           5.2817 |
[32m[20221213 21:32:37 @agent_ppo2.py:185][0m |          -0.0055 |         172.5637 |           5.3177 |
[32m[20221213 21:32:37 @agent_ppo2.py:185][0m |          -0.0068 |         172.6949 |           5.2494 |
[32m[20221213 21:32:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.20
[32m[20221213 21:32:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.00
[32m[20221213 21:32:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 861.00
[32m[20221213 21:32:38 @agent_ppo2.py:143][0m Total time:      37.05 min
[32m[20221213 21:32:38 @agent_ppo2.py:145][0m 3614720 total steps have happened
[32m[20221213 21:32:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1765 --------------------------#
[32m[20221213 21:32:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:32:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:38 @agent_ppo2.py:185][0m |          -0.0002 |         178.5914 |           5.9096 |
[32m[20221213 21:32:38 @agent_ppo2.py:185][0m |          -0.0042 |         177.8222 |           5.9243 |
[32m[20221213 21:32:38 @agent_ppo2.py:185][0m |          -0.0048 |         177.3743 |           5.8524 |
[32m[20221213 21:32:38 @agent_ppo2.py:185][0m |           0.0030 |         183.2017 |           5.9511 |
[32m[20221213 21:32:38 @agent_ppo2.py:185][0m |          -0.0032 |         179.0801 |           5.9220 |
[32m[20221213 21:32:38 @agent_ppo2.py:185][0m |          -0.0067 |         176.5040 |           5.8582 |
[32m[20221213 21:32:38 @agent_ppo2.py:185][0m |          -0.0085 |         176.1824 |           5.8821 |
[32m[20221213 21:32:38 @agent_ppo2.py:185][0m |          -0.0090 |         176.2527 |           5.9174 |
[32m[20221213 21:32:39 @agent_ppo2.py:185][0m |           0.0007 |         182.5917 |           5.9240 |
[32m[20221213 21:32:39 @agent_ppo2.py:185][0m |          -0.0070 |         176.0462 |           5.9567 |
[32m[20221213 21:32:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.60
[32m[20221213 21:32:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:32:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.00
[32m[20221213 21:32:39 @agent_ppo2.py:143][0m Total time:      37.07 min
[32m[20221213 21:32:39 @agent_ppo2.py:145][0m 3616768 total steps have happened
[32m[20221213 21:32:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1766 --------------------------#
[32m[20221213 21:32:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:39 @agent_ppo2.py:185][0m |          -0.0025 |         177.0387 |           5.5699 |
[32m[20221213 21:32:39 @agent_ppo2.py:185][0m |          -0.0046 |         176.2165 |           5.5834 |
[32m[20221213 21:32:39 @agent_ppo2.py:185][0m |          -0.0038 |         175.8989 |           5.6387 |
[32m[20221213 21:32:39 @agent_ppo2.py:185][0m |          -0.0061 |         175.4215 |           5.5982 |
[32m[20221213 21:32:39 @agent_ppo2.py:185][0m |          -0.0078 |         175.1532 |           5.5894 |
[32m[20221213 21:32:39 @agent_ppo2.py:185][0m |          -0.0068 |         174.9339 |           5.6052 |
[32m[20221213 21:32:40 @agent_ppo2.py:185][0m |          -0.0054 |         175.3351 |           5.5741 |
[32m[20221213 21:32:40 @agent_ppo2.py:185][0m |           0.0080 |         191.9767 |           5.5597 |
[32m[20221213 21:32:40 @agent_ppo2.py:185][0m |          -0.0053 |         174.1690 |           5.6545 |
[32m[20221213 21:32:40 @agent_ppo2.py:185][0m |          -0.0071 |         173.8080 |           5.5273 |
[32m[20221213 21:32:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.20
[32m[20221213 21:32:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.00
[32m[20221213 21:32:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.00
[32m[20221213 21:32:40 @agent_ppo2.py:143][0m Total time:      37.09 min
[32m[20221213 21:32:40 @agent_ppo2.py:145][0m 3618816 total steps have happened
[32m[20221213 21:32:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1767 --------------------------#
[32m[20221213 21:32:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:40 @agent_ppo2.py:185][0m |          -0.0021 |         175.8351 |           5.2751 |
[32m[20221213 21:32:40 @agent_ppo2.py:185][0m |          -0.0065 |         175.2268 |           5.2322 |
[32m[20221213 21:32:40 @agent_ppo2.py:185][0m |           0.0024 |         183.9264 |           5.3174 |
[32m[20221213 21:32:41 @agent_ppo2.py:185][0m |          -0.0090 |         174.6228 |           5.3617 |
[32m[20221213 21:32:41 @agent_ppo2.py:185][0m |          -0.0083 |         174.4197 |           5.4171 |
[32m[20221213 21:32:41 @agent_ppo2.py:185][0m |          -0.0088 |         174.3023 |           5.4324 |
[32m[20221213 21:32:41 @agent_ppo2.py:185][0m |           0.0028 |         194.4136 |           5.4688 |
[32m[20221213 21:32:41 @agent_ppo2.py:185][0m |          -0.0105 |         174.0584 |           5.5213 |
[32m[20221213 21:32:41 @agent_ppo2.py:185][0m |          -0.0100 |         173.9466 |           5.5604 |
[32m[20221213 21:32:41 @agent_ppo2.py:185][0m |          -0.0073 |         174.1447 |           5.6232 |
[32m[20221213 21:32:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:32:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.20
[32m[20221213 21:32:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.00
[32m[20221213 21:32:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.00
[32m[20221213 21:32:41 @agent_ppo2.py:143][0m Total time:      37.11 min
[32m[20221213 21:32:41 @agent_ppo2.py:145][0m 3620864 total steps have happened
[32m[20221213 21:32:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1768 --------------------------#
[32m[20221213 21:32:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:32:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:42 @agent_ppo2.py:185][0m |           0.0017 |         176.3967 |           6.2512 |
[32m[20221213 21:32:42 @agent_ppo2.py:185][0m |          -0.0056 |         174.7959 |           6.2238 |
[32m[20221213 21:32:42 @agent_ppo2.py:185][0m |          -0.0004 |         179.0372 |           6.1871 |
[32m[20221213 21:32:42 @agent_ppo2.py:185][0m |          -0.0057 |         173.9372 |           6.2126 |
[32m[20221213 21:32:42 @agent_ppo2.py:185][0m |           0.0036 |         181.3397 |           6.2101 |
[32m[20221213 21:32:42 @agent_ppo2.py:185][0m |          -0.0078 |         173.1230 |           6.1692 |
[32m[20221213 21:32:42 @agent_ppo2.py:185][0m |          -0.0065 |         172.8417 |           6.1970 |
[32m[20221213 21:32:42 @agent_ppo2.py:185][0m |           0.0018 |         182.3132 |           6.2590 |
[32m[20221213 21:32:42 @agent_ppo2.py:185][0m |          -0.0084 |         172.2501 |           6.2462 |
[32m[20221213 21:32:42 @agent_ppo2.py:185][0m |          -0.0095 |         172.2753 |           6.2348 |
[32m[20221213 21:32:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.20
[32m[20221213 21:32:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:32:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.00
[32m[20221213 21:32:42 @agent_ppo2.py:143][0m Total time:      37.13 min
[32m[20221213 21:32:42 @agent_ppo2.py:145][0m 3622912 total steps have happened
[32m[20221213 21:32:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1769 --------------------------#
[32m[20221213 21:32:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:43 @agent_ppo2.py:185][0m |          -0.0016 |         179.1268 |           6.0511 |
[32m[20221213 21:32:43 @agent_ppo2.py:185][0m |          -0.0044 |         176.4156 |           6.1242 |
[32m[20221213 21:32:43 @agent_ppo2.py:185][0m |          -0.0033 |         174.9395 |           6.1092 |
[32m[20221213 21:32:43 @agent_ppo2.py:185][0m |          -0.0043 |         174.2642 |           6.1207 |
[32m[20221213 21:32:43 @agent_ppo2.py:185][0m |           0.0075 |         193.0598 |           6.0946 |
[32m[20221213 21:32:43 @agent_ppo2.py:185][0m |          -0.0076 |         170.0527 |           6.1403 |
[32m[20221213 21:32:43 @agent_ppo2.py:185][0m |          -0.0083 |         167.1412 |           6.1318 |
[32m[20221213 21:32:43 @agent_ppo2.py:185][0m |          -0.0000 |         174.8701 |           6.1353 |
[32m[20221213 21:32:43 @agent_ppo2.py:185][0m |          -0.0084 |         164.3265 |           6.1757 |
[32m[20221213 21:32:43 @agent_ppo2.py:185][0m |          -0.0090 |         163.5168 |           6.1383 |
[32m[20221213 21:32:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.20
[32m[20221213 21:32:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.00
[32m[20221213 21:32:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 21:32:44 @agent_ppo2.py:143][0m Total time:      37.15 min
[32m[20221213 21:32:44 @agent_ppo2.py:145][0m 3624960 total steps have happened
[32m[20221213 21:32:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1770 --------------------------#
[32m[20221213 21:32:44 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:32:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:44 @agent_ppo2.py:185][0m |          -0.0011 |         191.0952 |           5.7054 |
[32m[20221213 21:32:44 @agent_ppo2.py:185][0m |          -0.0050 |         182.2346 |           5.6996 |
[32m[20221213 21:32:44 @agent_ppo2.py:185][0m |          -0.0158 |         181.5800 |           5.7879 |
[32m[20221213 21:32:44 @agent_ppo2.py:185][0m |           0.0026 |         191.7951 |           5.8393 |
[32m[20221213 21:32:44 @agent_ppo2.py:185][0m |          -0.0069 |         180.1309 |           5.8742 |
[32m[20221213 21:32:44 @agent_ppo2.py:185][0m |          -0.0081 |         179.5042 |           5.8810 |
[32m[20221213 21:32:44 @agent_ppo2.py:185][0m |          -0.0097 |         179.1935 |           5.8993 |
[32m[20221213 21:32:44 @agent_ppo2.py:185][0m |          -0.0081 |         179.1765 |           5.9409 |
[32m[20221213 21:32:45 @agent_ppo2.py:185][0m |          -0.0111 |         178.5241 |           5.9173 |
[32m[20221213 21:32:45 @agent_ppo2.py:185][0m |           0.0037 |         204.3595 |           5.9792 |
[32m[20221213 21:32:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.60
[32m[20221213 21:32:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.00
[32m[20221213 21:32:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.00
[32m[20221213 21:32:45 @agent_ppo2.py:143][0m Total time:      37.17 min
[32m[20221213 21:32:45 @agent_ppo2.py:145][0m 3627008 total steps have happened
[32m[20221213 21:32:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1771 --------------------------#
[32m[20221213 21:32:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:45 @agent_ppo2.py:185][0m |           0.0020 |         179.7721 |           5.8273 |
[32m[20221213 21:32:45 @agent_ppo2.py:185][0m |          -0.0036 |         178.2975 |           5.7415 |
[32m[20221213 21:32:45 @agent_ppo2.py:185][0m |           0.0005 |         181.0723 |           5.7013 |
[32m[20221213 21:32:45 @agent_ppo2.py:185][0m |           0.0002 |         186.2995 |           5.6345 |
[32m[20221213 21:32:45 @agent_ppo2.py:185][0m |          -0.0042 |         176.9649 |           5.7026 |
[32m[20221213 21:32:45 @agent_ppo2.py:185][0m |          -0.0092 |         176.4929 |           5.6332 |
[32m[20221213 21:32:46 @agent_ppo2.py:185][0m |          -0.0091 |         176.2478 |           5.6594 |
[32m[20221213 21:32:46 @agent_ppo2.py:185][0m |          -0.0098 |         176.1509 |           5.6829 |
[32m[20221213 21:32:46 @agent_ppo2.py:185][0m |          -0.0114 |         175.9648 |           5.6511 |
[32m[20221213 21:32:46 @agent_ppo2.py:185][0m |          -0.0118 |         175.8658 |           5.6513 |
[32m[20221213 21:32:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.20
[32m[20221213 21:32:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:32:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 741.00
[32m[20221213 21:32:46 @agent_ppo2.py:143][0m Total time:      37.19 min
[32m[20221213 21:32:46 @agent_ppo2.py:145][0m 3629056 total steps have happened
[32m[20221213 21:32:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1772 --------------------------#
[32m[20221213 21:32:46 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:46 @agent_ppo2.py:185][0m |           0.0101 |         190.9756 |           5.7701 |
[32m[20221213 21:32:46 @agent_ppo2.py:185][0m |          -0.0027 |         176.0465 |           5.8257 |
[32m[20221213 21:32:46 @agent_ppo2.py:185][0m |          -0.0075 |         175.3293 |           5.8061 |
[32m[20221213 21:32:47 @agent_ppo2.py:185][0m |          -0.0074 |         174.8970 |           5.8021 |
[32m[20221213 21:32:47 @agent_ppo2.py:185][0m |          -0.0079 |         174.5069 |           5.7958 |
[32m[20221213 21:32:47 @agent_ppo2.py:185][0m |          -0.0080 |         174.3713 |           5.7959 |
[32m[20221213 21:32:47 @agent_ppo2.py:185][0m |          -0.0050 |         175.8337 |           5.7539 |
[32m[20221213 21:32:47 @agent_ppo2.py:185][0m |          -0.0057 |         174.4099 |           5.6913 |
[32m[20221213 21:32:47 @agent_ppo2.py:185][0m |          -0.0099 |         173.8476 |           5.6854 |
[32m[20221213 21:32:47 @agent_ppo2.py:185][0m |          -0.0109 |         173.4141 |           5.7135 |
[32m[20221213 21:32:47 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:32:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.00
[32m[20221213 21:32:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.00
[32m[20221213 21:32:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.00
[32m[20221213 21:32:47 @agent_ppo2.py:143][0m Total time:      37.21 min
[32m[20221213 21:32:47 @agent_ppo2.py:145][0m 3631104 total steps have happened
[32m[20221213 21:32:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1773 --------------------------#
[32m[20221213 21:32:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:48 @agent_ppo2.py:185][0m |           0.0046 |         176.5587 |           6.1832 |
[32m[20221213 21:32:48 @agent_ppo2.py:185][0m |          -0.0014 |         174.6932 |           6.2676 |
[32m[20221213 21:32:48 @agent_ppo2.py:185][0m |          -0.0027 |         173.4993 |           6.3483 |
[32m[20221213 21:32:48 @agent_ppo2.py:185][0m |          -0.0028 |         173.1444 |           6.3433 |
[32m[20221213 21:32:48 @agent_ppo2.py:185][0m |          -0.0007 |         175.5403 |           6.3943 |
[32m[20221213 21:32:48 @agent_ppo2.py:185][0m |          -0.0056 |         172.5675 |           6.4374 |
[32m[20221213 21:32:48 @agent_ppo2.py:185][0m |          -0.0056 |         172.3594 |           6.3914 |
[32m[20221213 21:32:48 @agent_ppo2.py:185][0m |          -0.0075 |         172.2366 |           6.5220 |
[32m[20221213 21:32:48 @agent_ppo2.py:185][0m |          -0.0075 |         172.0170 |           6.5032 |
[32m[20221213 21:32:48 @agent_ppo2.py:185][0m |          -0.0074 |         171.8794 |           6.5208 |
[32m[20221213 21:32:48 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:32:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.00
[32m[20221213 21:32:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:32:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:32:48 @agent_ppo2.py:143][0m Total time:      37.23 min
[32m[20221213 21:32:48 @agent_ppo2.py:145][0m 3633152 total steps have happened
[32m[20221213 21:32:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1774 --------------------------#
[32m[20221213 21:32:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:49 @agent_ppo2.py:185][0m |           0.0018 |         175.9305 |           6.3054 |
[32m[20221213 21:32:49 @agent_ppo2.py:185][0m |          -0.0040 |         175.1920 |           6.4284 |
[32m[20221213 21:32:49 @agent_ppo2.py:185][0m |          -0.0041 |         174.8585 |           6.3754 |
[32m[20221213 21:32:49 @agent_ppo2.py:185][0m |          -0.0051 |         174.5119 |           6.3978 |
[32m[20221213 21:32:49 @agent_ppo2.py:185][0m |          -0.0047 |         174.9328 |           6.3957 |
[32m[20221213 21:32:49 @agent_ppo2.py:185][0m |          -0.0057 |         174.3440 |           6.4307 |
[32m[20221213 21:32:49 @agent_ppo2.py:185][0m |           0.0012 |         182.0328 |           6.4106 |
[32m[20221213 21:32:50 @agent_ppo2.py:185][0m |           0.0033 |         186.6129 |           6.3953 |
[32m[20221213 21:32:50 @agent_ppo2.py:185][0m |          -0.0081 |         173.8911 |           6.4072 |
[32m[20221213 21:32:50 @agent_ppo2.py:185][0m |          -0.0087 |         173.7755 |           6.3887 |
[32m[20221213 21:32:50 @agent_ppo2.py:130][0m Policy update time: 1.24 s
[32m[20221213 21:32:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.00
[32m[20221213 21:32:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:32:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.00
[32m[20221213 21:32:50 @agent_ppo2.py:143][0m Total time:      37.26 min
[32m[20221213 21:32:50 @agent_ppo2.py:145][0m 3635200 total steps have happened
[32m[20221213 21:32:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1775 --------------------------#
[32m[20221213 21:32:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:32:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:50 @agent_ppo2.py:185][0m |          -0.0008 |         173.6766 |           6.2542 |
[32m[20221213 21:32:51 @agent_ppo2.py:185][0m |          -0.0027 |         173.0682 |           6.2760 |
[32m[20221213 21:32:51 @agent_ppo2.py:185][0m |          -0.0045 |         172.7739 |           6.2874 |
[32m[20221213 21:32:51 @agent_ppo2.py:185][0m |          -0.0065 |         172.4863 |           6.2790 |
[32m[20221213 21:32:51 @agent_ppo2.py:185][0m |          -0.0065 |         172.2511 |           6.3134 |
[32m[20221213 21:32:51 @agent_ppo2.py:185][0m |          -0.0079 |         172.1904 |           6.2697 |
[32m[20221213 21:32:51 @agent_ppo2.py:185][0m |          -0.0068 |         171.9723 |           6.2518 |
[32m[20221213 21:32:51 @agent_ppo2.py:185][0m |           0.0077 |         189.4280 |           6.2258 |
[32m[20221213 21:32:51 @agent_ppo2.py:185][0m |          -0.0083 |         171.7322 |           6.2013 |
[32m[20221213 21:32:51 @agent_ppo2.py:185][0m |          -0.0091 |         171.6092 |           6.2291 |
[32m[20221213 21:32:51 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:32:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.80
[32m[20221213 21:32:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 853.00
[32m[20221213 21:32:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 864.00
[32m[20221213 21:32:51 @agent_ppo2.py:143][0m Total time:      37.28 min
[32m[20221213 21:32:51 @agent_ppo2.py:145][0m 3637248 total steps have happened
[32m[20221213 21:32:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1776 --------------------------#
[32m[20221213 21:32:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:52 @agent_ppo2.py:185][0m |           0.0175 |         201.7191 |           6.0175 |
[32m[20221213 21:32:52 @agent_ppo2.py:185][0m |           0.0051 |         183.8473 |           6.1682 |
[32m[20221213 21:32:52 @agent_ppo2.py:185][0m |          -0.0041 |         175.0119 |           6.1155 |
[32m[20221213 21:32:52 @agent_ppo2.py:185][0m |          -0.0076 |         174.3216 |           6.1456 |
[32m[20221213 21:32:52 @agent_ppo2.py:185][0m |          -0.0066 |         174.0419 |           6.2149 |
[32m[20221213 21:32:52 @agent_ppo2.py:185][0m |          -0.0083 |         173.6313 |           6.1795 |
[32m[20221213 21:32:52 @agent_ppo2.py:185][0m |          -0.0099 |         173.5847 |           6.2133 |
[32m[20221213 21:32:52 @agent_ppo2.py:185][0m |          -0.0030 |         178.3430 |           6.2415 |
[32m[20221213 21:32:52 @agent_ppo2.py:185][0m |          -0.0091 |         173.0600 |           6.2758 |
[32m[20221213 21:32:53 @agent_ppo2.py:185][0m |          -0.0054 |         176.0287 |           6.3635 |
[32m[20221213 21:32:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:32:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.00
[32m[20221213 21:32:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.00
[32m[20221213 21:32:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.00
[32m[20221213 21:32:53 @agent_ppo2.py:143][0m Total time:      37.30 min
[32m[20221213 21:32:53 @agent_ppo2.py:145][0m 3639296 total steps have happened
[32m[20221213 21:32:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1777 --------------------------#
[32m[20221213 21:32:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:53 @agent_ppo2.py:185][0m |           0.0014 |         178.6833 |           6.2455 |
[32m[20221213 21:32:53 @agent_ppo2.py:185][0m |          -0.0002 |         180.4854 |           6.1974 |
[32m[20221213 21:32:53 @agent_ppo2.py:185][0m |          -0.0078 |         175.0287 |           6.2106 |
[32m[20221213 21:32:53 @agent_ppo2.py:185][0m |           0.0027 |         192.1048 |           6.2188 |
[32m[20221213 21:32:53 @agent_ppo2.py:185][0m |          -0.0067 |         173.7273 |           6.2222 |
[32m[20221213 21:32:53 @agent_ppo2.py:185][0m |          -0.0086 |         172.8412 |           6.1756 |
[32m[20221213 21:32:54 @agent_ppo2.py:185][0m |          -0.0080 |         173.5340 |           6.1889 |
[32m[20221213 21:32:54 @agent_ppo2.py:185][0m |          -0.0107 |         172.0602 |           6.1843 |
[32m[20221213 21:32:54 @agent_ppo2.py:185][0m |           0.0038 |         197.4547 |           6.1864 |
[32m[20221213 21:32:54 @agent_ppo2.py:185][0m |          -0.0084 |         171.7168 |           6.2575 |
[32m[20221213 21:32:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.00
[32m[20221213 21:32:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.00
[32m[20221213 21:32:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.00
[32m[20221213 21:32:54 @agent_ppo2.py:143][0m Total time:      37.32 min
[32m[20221213 21:32:54 @agent_ppo2.py:145][0m 3641344 total steps have happened
[32m[20221213 21:32:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1778 --------------------------#
[32m[20221213 21:32:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:32:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:54 @agent_ppo2.py:185][0m |           0.0145 |         199.3068 |           5.9161 |
[32m[20221213 21:32:54 @agent_ppo2.py:185][0m |          -0.0019 |         174.7291 |           6.0418 |
[32m[20221213 21:32:54 @agent_ppo2.py:185][0m |          -0.0057 |         174.1130 |           5.9382 |
[32m[20221213 21:32:54 @agent_ppo2.py:185][0m |           0.0019 |         183.8002 |           5.8627 |
[32m[20221213 21:32:55 @agent_ppo2.py:185][0m |          -0.0056 |         173.4354 |           5.8862 |
[32m[20221213 21:32:55 @agent_ppo2.py:185][0m |          -0.0058 |         173.1898 |           5.9188 |
[32m[20221213 21:32:55 @agent_ppo2.py:185][0m |          -0.0067 |         173.0088 |           5.9726 |
[32m[20221213 21:32:55 @agent_ppo2.py:185][0m |          -0.0073 |         172.8276 |           5.8299 |
[32m[20221213 21:32:55 @agent_ppo2.py:185][0m |          -0.0071 |         172.7095 |           5.8705 |
[32m[20221213 21:32:55 @agent_ppo2.py:185][0m |          -0.0072 |         172.5104 |           5.8832 |
[32m[20221213 21:32:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:32:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.80
[32m[20221213 21:32:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:32:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.00
[32m[20221213 21:32:55 @agent_ppo2.py:143][0m Total time:      37.34 min
[32m[20221213 21:32:55 @agent_ppo2.py:145][0m 3643392 total steps have happened
[32m[20221213 21:32:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1779 --------------------------#
[32m[20221213 21:32:55 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:32:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:55 @agent_ppo2.py:185][0m |          -0.0039 |         175.1564 |           5.9373 |
[32m[20221213 21:32:56 @agent_ppo2.py:185][0m |          -0.0017 |         174.6561 |           5.9246 |
[32m[20221213 21:32:56 @agent_ppo2.py:185][0m |          -0.0020 |         174.4156 |           5.9498 |
[32m[20221213 21:32:56 @agent_ppo2.py:185][0m |          -0.0071 |         174.0604 |           6.0243 |
[32m[20221213 21:32:56 @agent_ppo2.py:185][0m |          -0.0074 |         173.7423 |           5.9257 |
[32m[20221213 21:32:56 @agent_ppo2.py:185][0m |           0.0015 |         181.2870 |           5.9768 |
[32m[20221213 21:32:56 @agent_ppo2.py:185][0m |          -0.0081 |         173.3541 |           5.9433 |
[32m[20221213 21:32:56 @agent_ppo2.py:185][0m |          -0.0085 |         173.2829 |           5.9343 |
[32m[20221213 21:32:56 @agent_ppo2.py:185][0m |          -0.0080 |         173.0452 |           5.9291 |
[32m[20221213 21:32:56 @agent_ppo2.py:185][0m |          -0.0083 |         173.0036 |           5.9603 |
[32m[20221213 21:32:56 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:32:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.60
[32m[20221213 21:32:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.00
[32m[20221213 21:32:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.00
[32m[20221213 21:32:56 @agent_ppo2.py:143][0m Total time:      37.36 min
[32m[20221213 21:32:56 @agent_ppo2.py:145][0m 3645440 total steps have happened
[32m[20221213 21:32:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1780 --------------------------#
[32m[20221213 21:32:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:57 @agent_ppo2.py:185][0m |          -0.0015 |         173.5834 |           5.5289 |
[32m[20221213 21:32:57 @agent_ppo2.py:185][0m |          -0.0042 |         173.3684 |           5.4991 |
[32m[20221213 21:32:57 @agent_ppo2.py:185][0m |          -0.0032 |         173.6028 |           5.5376 |
[32m[20221213 21:32:57 @agent_ppo2.py:185][0m |          -0.0057 |         172.7358 |           5.5445 |
[32m[20221213 21:32:57 @agent_ppo2.py:185][0m |          -0.0054 |         172.6841 |           5.5525 |
[32m[20221213 21:32:57 @agent_ppo2.py:185][0m |          -0.0078 |         172.4862 |           5.5456 |
[32m[20221213 21:32:57 @agent_ppo2.py:185][0m |          -0.0082 |         172.4360 |           5.6022 |
[32m[20221213 21:32:57 @agent_ppo2.py:185][0m |          -0.0075 |         172.3159 |           5.6091 |
[32m[20221213 21:32:57 @agent_ppo2.py:185][0m |          -0.0038 |         172.5968 |           5.6048 |
[32m[20221213 21:32:58 @agent_ppo2.py:185][0m |          -0.0091 |         171.9773 |           5.6268 |
[32m[20221213 21:32:58 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:32:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.20
[32m[20221213 21:32:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.00
[32m[20221213 21:32:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.00
[32m[20221213 21:32:58 @agent_ppo2.py:143][0m Total time:      37.38 min
[32m[20221213 21:32:58 @agent_ppo2.py:145][0m 3647488 total steps have happened
[32m[20221213 21:32:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1781 --------------------------#
[32m[20221213 21:32:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:32:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:58 @agent_ppo2.py:185][0m |          -0.0014 |         174.0179 |           5.8796 |
[32m[20221213 21:32:58 @agent_ppo2.py:185][0m |          -0.0053 |         173.2974 |           5.9477 |
[32m[20221213 21:32:58 @agent_ppo2.py:185][0m |          -0.0069 |         173.3049 |           5.9647 |
[32m[20221213 21:32:58 @agent_ppo2.py:185][0m |          -0.0066 |         172.7943 |           6.0216 |
[32m[20221213 21:32:58 @agent_ppo2.py:185][0m |          -0.0078 |         172.5417 |           6.0349 |
[32m[20221213 21:32:58 @agent_ppo2.py:185][0m |          -0.0087 |         172.3675 |           6.1219 |
[32m[20221213 21:32:59 @agent_ppo2.py:185][0m |          -0.0099 |         172.2343 |           6.0802 |
[32m[20221213 21:32:59 @agent_ppo2.py:185][0m |          -0.0088 |         172.1205 |           6.1038 |
[32m[20221213 21:32:59 @agent_ppo2.py:185][0m |          -0.0094 |         172.0612 |           6.1376 |
[32m[20221213 21:32:59 @agent_ppo2.py:185][0m |          -0.0101 |         172.0515 |           6.1560 |
[32m[20221213 21:32:59 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:32:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.00
[32m[20221213 21:32:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:32:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.00
[32m[20221213 21:32:59 @agent_ppo2.py:143][0m Total time:      37.40 min
[32m[20221213 21:32:59 @agent_ppo2.py:145][0m 3649536 total steps have happened
[32m[20221213 21:32:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1782 --------------------------#
[32m[20221213 21:32:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:32:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:32:59 @agent_ppo2.py:185][0m |           0.0015 |         172.9871 |           5.8206 |
[32m[20221213 21:32:59 @agent_ppo2.py:185][0m |          -0.0017 |         172.3680 |           5.9133 |
[32m[20221213 21:32:59 @agent_ppo2.py:185][0m |          -0.0052 |         170.9555 |           5.9476 |
[32m[20221213 21:32:59 @agent_ppo2.py:185][0m |          -0.0063 |         170.2778 |           5.8949 |
[32m[20221213 21:33:00 @agent_ppo2.py:185][0m |          -0.0089 |         169.8161 |           5.9931 |
[32m[20221213 21:33:00 @agent_ppo2.py:185][0m |          -0.0034 |         171.5430 |           6.0516 |
[32m[20221213 21:33:00 @agent_ppo2.py:185][0m |           0.0017 |         185.0791 |           6.0771 |
[32m[20221213 21:33:00 @agent_ppo2.py:185][0m |           0.0038 |         190.1028 |           6.0610 |
[32m[20221213 21:33:00 @agent_ppo2.py:185][0m |          -0.0070 |         168.5224 |           6.1840 |
[32m[20221213 21:33:00 @agent_ppo2.py:185][0m |          -0.0108 |         168.0289 |           6.1594 |
[32m[20221213 21:33:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:33:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.40
[32m[20221213 21:33:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.00
[32m[20221213 21:33:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.00
[32m[20221213 21:33:00 @agent_ppo2.py:143][0m Total time:      37.42 min
[32m[20221213 21:33:00 @agent_ppo2.py:145][0m 3651584 total steps have happened
[32m[20221213 21:33:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1783 --------------------------#
[32m[20221213 21:33:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:00 @agent_ppo2.py:185][0m |           0.0063 |         176.9995 |           6.4720 |
[32m[20221213 21:33:01 @agent_ppo2.py:185][0m |          -0.0029 |         173.3916 |           6.5319 |
[32m[20221213 21:33:01 @agent_ppo2.py:185][0m |          -0.0041 |         172.7832 |           6.5126 |
[32m[20221213 21:33:01 @agent_ppo2.py:185][0m |          -0.0056 |         172.3941 |           6.5456 |
[32m[20221213 21:33:01 @agent_ppo2.py:185][0m |          -0.0025 |         173.1105 |           6.4621 |
[32m[20221213 21:33:01 @agent_ppo2.py:185][0m |          -0.0048 |         172.0313 |           6.5240 |
[32m[20221213 21:33:01 @agent_ppo2.py:185][0m |          -0.0061 |         171.8381 |           6.5275 |
[32m[20221213 21:33:01 @agent_ppo2.py:185][0m |          -0.0076 |         171.6120 |           6.5168 |
[32m[20221213 21:33:01 @agent_ppo2.py:185][0m |          -0.0078 |         171.5092 |           6.5076 |
[32m[20221213 21:33:01 @agent_ppo2.py:185][0m |          -0.0076 |         171.4535 |           6.5484 |
[32m[20221213 21:33:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:33:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.80
[32m[20221213 21:33:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.00
[32m[20221213 21:33:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 867.00
[32m[20221213 21:33:01 @agent_ppo2.py:143][0m Total time:      37.44 min
[32m[20221213 21:33:01 @agent_ppo2.py:145][0m 3653632 total steps have happened
[32m[20221213 21:33:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1784 --------------------------#
[32m[20221213 21:33:01 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:33:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:02 @agent_ppo2.py:185][0m |          -0.0007 |         173.3651 |           5.7555 |
[32m[20221213 21:33:02 @agent_ppo2.py:185][0m |          -0.0054 |         172.7418 |           5.7566 |
[32m[20221213 21:33:02 @agent_ppo2.py:185][0m |          -0.0042 |         172.6665 |           5.8368 |
[32m[20221213 21:33:02 @agent_ppo2.py:185][0m |          -0.0058 |         172.1565 |           5.9485 |
[32m[20221213 21:33:02 @agent_ppo2.py:185][0m |          -0.0075 |         171.8893 |           5.9428 |
[32m[20221213 21:33:02 @agent_ppo2.py:185][0m |          -0.0063 |         171.5033 |           5.9884 |
[32m[20221213 21:33:02 @agent_ppo2.py:185][0m |          -0.0062 |         171.3650 |           6.0774 |
[32m[20221213 21:33:02 @agent_ppo2.py:185][0m |          -0.0069 |         171.1225 |           6.1489 |
[32m[20221213 21:33:02 @agent_ppo2.py:185][0m |          -0.0084 |         170.9211 |           6.1510 |
[32m[20221213 21:33:02 @agent_ppo2.py:185][0m |          -0.0023 |         175.5163 |           6.1815 |
[32m[20221213 21:33:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:33:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.20
[32m[20221213 21:33:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:33:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 861.00
[32m[20221213 21:33:03 @agent_ppo2.py:143][0m Total time:      37.46 min
[32m[20221213 21:33:03 @agent_ppo2.py:145][0m 3655680 total steps have happened
[32m[20221213 21:33:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1785 --------------------------#
[32m[20221213 21:33:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:03 @agent_ppo2.py:185][0m |          -0.0002 |         175.9216 |           7.0748 |
[32m[20221213 21:33:03 @agent_ppo2.py:185][0m |          -0.0029 |         174.9878 |           7.2040 |
[32m[20221213 21:33:03 @agent_ppo2.py:185][0m |          -0.0029 |         174.5208 |           7.1527 |
[32m[20221213 21:33:03 @agent_ppo2.py:185][0m |          -0.0042 |         174.2774 |           7.1225 |
[32m[20221213 21:33:03 @agent_ppo2.py:185][0m |          -0.0045 |         173.8760 |           7.0998 |
[32m[20221213 21:33:03 @agent_ppo2.py:185][0m |          -0.0060 |         173.6097 |           7.1267 |
[32m[20221213 21:33:03 @agent_ppo2.py:185][0m |          -0.0074 |         173.3989 |           7.1097 |
[32m[20221213 21:33:03 @agent_ppo2.py:185][0m |          -0.0057 |         173.1915 |           7.1485 |
[32m[20221213 21:33:04 @agent_ppo2.py:185][0m |          -0.0062 |         173.1064 |           7.1105 |
[32m[20221213 21:33:04 @agent_ppo2.py:185][0m |          -0.0058 |         173.4537 |           7.1512 |
[32m[20221213 21:33:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:33:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.40
[32m[20221213 21:33:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:33:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.00
[32m[20221213 21:33:04 @agent_ppo2.py:143][0m Total time:      37.48 min
[32m[20221213 21:33:04 @agent_ppo2.py:145][0m 3657728 total steps have happened
[32m[20221213 21:33:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1786 --------------------------#
[32m[20221213 21:33:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:04 @agent_ppo2.py:185][0m |          -0.0025 |         176.7128 |           6.9269 |
[32m[20221213 21:33:04 @agent_ppo2.py:185][0m |           0.0036 |         182.9647 |           6.9658 |
[32m[20221213 21:33:04 @agent_ppo2.py:185][0m |          -0.0050 |         174.6149 |           6.9318 |
[32m[20221213 21:33:04 @agent_ppo2.py:185][0m |          -0.0060 |         174.4589 |           7.0859 |
[32m[20221213 21:33:04 @agent_ppo2.py:185][0m |          -0.0082 |         174.0426 |           6.9851 |
[32m[20221213 21:33:04 @agent_ppo2.py:185][0m |          -0.0058 |         173.7404 |           7.0353 |
[32m[20221213 21:33:05 @agent_ppo2.py:185][0m |          -0.0061 |         174.1849 |           7.0230 |
[32m[20221213 21:33:05 @agent_ppo2.py:185][0m |          -0.0074 |         173.1732 |           7.0408 |
[32m[20221213 21:33:05 @agent_ppo2.py:185][0m |          -0.0087 |         173.0010 |           7.0199 |
[32m[20221213 21:33:05 @agent_ppo2.py:185][0m |          -0.0040 |         175.1223 |           7.0050 |
[32m[20221213 21:33:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:33:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.60
[32m[20221213 21:33:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.00
[32m[20221213 21:33:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.00
[32m[20221213 21:33:05 @agent_ppo2.py:143][0m Total time:      37.50 min
[32m[20221213 21:33:05 @agent_ppo2.py:145][0m 3659776 total steps have happened
[32m[20221213 21:33:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1787 --------------------------#
[32m[20221213 21:33:05 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:33:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:05 @agent_ppo2.py:185][0m |          -0.0026 |         174.9736 |           7.0139 |
[32m[20221213 21:33:05 @agent_ppo2.py:185][0m |          -0.0067 |         173.4271 |           7.0143 |
[32m[20221213 21:33:05 @agent_ppo2.py:185][0m |          -0.0051 |         173.6087 |           6.9583 |
[32m[20221213 21:33:06 @agent_ppo2.py:185][0m |          -0.0055 |         173.4914 |           7.0173 |
[32m[20221213 21:33:06 @agent_ppo2.py:185][0m |          -0.0107 |         172.1335 |           7.0825 |
[32m[20221213 21:33:06 @agent_ppo2.py:185][0m |          -0.0049 |         175.8044 |           7.1054 |
[32m[20221213 21:33:06 @agent_ppo2.py:185][0m |          -0.0103 |         171.6932 |           7.1130 |
[32m[20221213 21:33:06 @agent_ppo2.py:185][0m |          -0.0092 |         171.7807 |           7.1373 |
[32m[20221213 21:33:06 @agent_ppo2.py:185][0m |          -0.0023 |         177.5383 |           7.1360 |
[32m[20221213 21:33:06 @agent_ppo2.py:185][0m |          -0.0108 |         171.5301 |           7.1913 |
[32m[20221213 21:33:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:33:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.80
[32m[20221213 21:33:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.00
[32m[20221213 21:33:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:33:06 @agent_ppo2.py:143][0m Total time:      37.53 min
[32m[20221213 21:33:06 @agent_ppo2.py:145][0m 3661824 total steps have happened
[32m[20221213 21:33:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1788 --------------------------#
[32m[20221213 21:33:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:06 @agent_ppo2.py:185][0m |          -0.0019 |         173.7099 |           7.3347 |
[32m[20221213 21:33:07 @agent_ppo2.py:185][0m |          -0.0020 |         173.4843 |           7.3664 |
[32m[20221213 21:33:07 @agent_ppo2.py:185][0m |          -0.0041 |         172.8144 |           7.2992 |
[32m[20221213 21:33:07 @agent_ppo2.py:185][0m |          -0.0040 |         173.1237 |           7.3538 |
[32m[20221213 21:33:07 @agent_ppo2.py:185][0m |          -0.0067 |         172.5010 |           7.3477 |
[32m[20221213 21:33:07 @agent_ppo2.py:185][0m |           0.0020 |         178.7754 |           7.3322 |
[32m[20221213 21:33:07 @agent_ppo2.py:185][0m |          -0.0022 |         174.4649 |           7.4024 |
[32m[20221213 21:33:07 @agent_ppo2.py:185][0m |          -0.0083 |         172.1520 |           7.4445 |
[32m[20221213 21:33:07 @agent_ppo2.py:185][0m |          -0.0032 |         174.8043 |           7.4177 |
[32m[20221213 21:33:07 @agent_ppo2.py:185][0m |          -0.0070 |         171.9849 |           7.4798 |
[32m[20221213 21:33:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:33:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.40
[32m[20221213 21:33:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:33:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:33:07 @agent_ppo2.py:143][0m Total time:      37.55 min
[32m[20221213 21:33:07 @agent_ppo2.py:145][0m 3663872 total steps have happened
[32m[20221213 21:33:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1789 --------------------------#
[32m[20221213 21:33:08 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:33:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:08 @agent_ppo2.py:185][0m |          -0.0004 |         176.7314 |           7.8679 |
[32m[20221213 21:33:08 @agent_ppo2.py:185][0m |          -0.0044 |         176.0323 |           7.8959 |
[32m[20221213 21:33:08 @agent_ppo2.py:185][0m |          -0.0038 |         175.7118 |           7.9011 |
[32m[20221213 21:33:08 @agent_ppo2.py:185][0m |           0.0021 |         180.4310 |           7.9390 |
[32m[20221213 21:33:08 @agent_ppo2.py:185][0m |          -0.0067 |         175.0631 |           8.0203 |
[32m[20221213 21:33:08 @agent_ppo2.py:185][0m |          -0.0074 |         174.9742 |           8.0415 |
[32m[20221213 21:33:08 @agent_ppo2.py:185][0m |          -0.0086 |         174.7412 |           8.0856 |
[32m[20221213 21:33:08 @agent_ppo2.py:185][0m |          -0.0019 |         179.8366 |           8.1148 |
[32m[20221213 21:33:08 @agent_ppo2.py:185][0m |           0.0023 |         186.5261 |           8.1202 |
[32m[20221213 21:33:08 @agent_ppo2.py:185][0m |          -0.0093 |         174.3089 |           8.1209 |
[32m[20221213 21:33:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:33:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.60
[32m[20221213 21:33:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.00
[32m[20221213 21:33:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 877.00
[32m[20221213 21:33:09 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 877.00
[32m[20221213 21:33:09 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 877.00
[32m[20221213 21:33:09 @agent_ppo2.py:143][0m Total time:      37.57 min
[32m[20221213 21:33:09 @agent_ppo2.py:145][0m 3665920 total steps have happened
[32m[20221213 21:33:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1790 --------------------------#
[32m[20221213 21:33:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:09 @agent_ppo2.py:185][0m |          -0.0010 |         178.1282 |           7.5184 |
[32m[20221213 21:33:09 @agent_ppo2.py:185][0m |          -0.0030 |         177.5180 |           7.4267 |
[32m[20221213 21:33:09 @agent_ppo2.py:185][0m |          -0.0011 |         180.0718 |           7.3907 |
[32m[20221213 21:33:09 @agent_ppo2.py:185][0m |          -0.0071 |         176.6815 |           7.3461 |
[32m[20221213 21:33:09 @agent_ppo2.py:185][0m |           0.0007 |         186.0782 |           7.3347 |
[32m[20221213 21:33:09 @agent_ppo2.py:185][0m |          -0.0061 |         176.8389 |           7.3982 |
[32m[20221213 21:33:09 @agent_ppo2.py:185][0m |          -0.0080 |         176.3705 |           7.3081 |
[32m[20221213 21:33:10 @agent_ppo2.py:185][0m |          -0.0095 |         176.3176 |           7.3047 |
[32m[20221213 21:33:10 @agent_ppo2.py:185][0m |          -0.0093 |         176.1405 |           7.2705 |
[32m[20221213 21:33:10 @agent_ppo2.py:185][0m |           0.0006 |         187.1999 |           7.2766 |
[32m[20221213 21:33:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:33:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.00
[32m[20221213 21:33:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:33:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.00
[32m[20221213 21:33:10 @agent_ppo2.py:143][0m Total time:      37.59 min
[32m[20221213 21:33:10 @agent_ppo2.py:145][0m 3667968 total steps have happened
[32m[20221213 21:33:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1791 --------------------------#
[32m[20221213 21:33:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:33:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:10 @agent_ppo2.py:185][0m |           0.0016 |         176.9349 |           7.2822 |
[32m[20221213 21:33:10 @agent_ppo2.py:185][0m |          -0.0018 |         175.8460 |           7.3229 |
[32m[20221213 21:33:10 @agent_ppo2.py:185][0m |          -0.0052 |         175.1391 |           7.3274 |
[32m[20221213 21:33:10 @agent_ppo2.py:185][0m |          -0.0052 |         174.5733 |           7.3171 |
[32m[20221213 21:33:10 @agent_ppo2.py:185][0m |          -0.0068 |         174.0653 |           7.3170 |
[32m[20221213 21:33:11 @agent_ppo2.py:185][0m |           0.0004 |         180.6420 |           7.3316 |
[32m[20221213 21:33:11 @agent_ppo2.py:185][0m |           0.0046 |         189.3239 |           7.3668 |
[32m[20221213 21:33:11 @agent_ppo2.py:185][0m |          -0.0074 |         173.1756 |           7.3773 |
[32m[20221213 21:33:11 @agent_ppo2.py:185][0m |          -0.0005 |         176.3883 |           7.4030 |
[32m[20221213 21:33:11 @agent_ppo2.py:185][0m |          -0.0067 |         172.8364 |           7.3458 |
[32m[20221213 21:33:11 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:33:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.20
[32m[20221213 21:33:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:33:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.00
[32m[20221213 21:33:11 @agent_ppo2.py:143][0m Total time:      37.61 min
[32m[20221213 21:33:11 @agent_ppo2.py:145][0m 3670016 total steps have happened
[32m[20221213 21:33:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1792 --------------------------#
[32m[20221213 21:33:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:11 @agent_ppo2.py:185][0m |          -0.0032 |         175.8886 |           7.6361 |
[32m[20221213 21:33:11 @agent_ppo2.py:185][0m |          -0.0046 |         175.2217 |           7.6998 |
[32m[20221213 21:33:12 @agent_ppo2.py:185][0m |          -0.0033 |         174.6783 |           7.6364 |
[32m[20221213 21:33:12 @agent_ppo2.py:185][0m |          -0.0074 |         174.2290 |           7.6025 |
[32m[20221213 21:33:12 @agent_ppo2.py:185][0m |          -0.0055 |         174.4533 |           7.6429 |
[32m[20221213 21:33:12 @agent_ppo2.py:185][0m |          -0.0029 |         182.0420 |           7.6053 |
[32m[20221213 21:33:12 @agent_ppo2.py:185][0m |          -0.0041 |         176.4514 |           7.6093 |
[32m[20221213 21:33:12 @agent_ppo2.py:185][0m |           0.0003 |         179.7997 |           7.6339 |
[32m[20221213 21:33:12 @agent_ppo2.py:185][0m |          -0.0016 |         181.0722 |           7.5777 |
[32m[20221213 21:33:12 @agent_ppo2.py:185][0m |          -0.0093 |         173.2232 |           7.6148 |
[32m[20221213 21:33:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:33:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.00
[32m[20221213 21:33:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:33:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.00
[32m[20221213 21:33:12 @agent_ppo2.py:143][0m Total time:      37.63 min
[32m[20221213 21:33:12 @agent_ppo2.py:145][0m 3672064 total steps have happened
[32m[20221213 21:33:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1793 --------------------------#
[32m[20221213 21:33:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:13 @agent_ppo2.py:185][0m |          -0.0016 |         174.6904 |           7.3068 |
[32m[20221213 21:33:13 @agent_ppo2.py:185][0m |          -0.0031 |         173.7964 |           7.2845 |
[32m[20221213 21:33:13 @agent_ppo2.py:185][0m |          -0.0042 |         173.2210 |           7.3328 |
[32m[20221213 21:33:13 @agent_ppo2.py:185][0m |          -0.0034 |         172.8366 |           7.3148 |
[32m[20221213 21:33:13 @agent_ppo2.py:185][0m |          -0.0036 |         172.8472 |           7.3282 |
[32m[20221213 21:33:13 @agent_ppo2.py:185][0m |          -0.0042 |         172.3528 |           7.2178 |
[32m[20221213 21:33:13 @agent_ppo2.py:185][0m |          -0.0071 |         172.1138 |           7.3390 |
[32m[20221213 21:33:13 @agent_ppo2.py:185][0m |           0.0028 |         178.5881 |           7.2374 |
[32m[20221213 21:33:13 @agent_ppo2.py:185][0m |          -0.0062 |         171.8548 |           7.3084 |
[32m[20221213 21:33:13 @agent_ppo2.py:185][0m |          -0.0069 |         171.5544 |           7.2504 |
[32m[20221213 21:33:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:33:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.60
[32m[20221213 21:33:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:33:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:33:13 @agent_ppo2.py:143][0m Total time:      37.65 min
[32m[20221213 21:33:13 @agent_ppo2.py:145][0m 3674112 total steps have happened
[32m[20221213 21:33:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1794 --------------------------#
[32m[20221213 21:33:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:33:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:14 @agent_ppo2.py:185][0m |          -0.0023 |         175.6629 |           7.3558 |
[32m[20221213 21:33:14 @agent_ppo2.py:185][0m |          -0.0051 |         169.8395 |           7.3422 |
[32m[20221213 21:33:14 @agent_ppo2.py:185][0m |          -0.0054 |         167.2802 |           7.3503 |
[32m[20221213 21:33:14 @agent_ppo2.py:185][0m |          -0.0070 |         165.0987 |           7.3389 |
[32m[20221213 21:33:14 @agent_ppo2.py:185][0m |          -0.0090 |         163.4779 |           7.3869 |
[32m[20221213 21:33:14 @agent_ppo2.py:185][0m |          -0.0032 |         163.7687 |           7.3560 |
[32m[20221213 21:33:14 @agent_ppo2.py:185][0m |          -0.0101 |         161.7077 |           7.3504 |
[32m[20221213 21:33:14 @agent_ppo2.py:185][0m |          -0.0056 |         162.6133 |           7.3622 |
[32m[20221213 21:33:14 @agent_ppo2.py:185][0m |          -0.0101 |         160.4020 |           7.3889 |
[32m[20221213 21:33:15 @agent_ppo2.py:185][0m |          -0.0124 |         159.4567 |           7.4170 |
[32m[20221213 21:33:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:33:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.60
[32m[20221213 21:33:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.00
[32m[20221213 21:33:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.00
[32m[20221213 21:33:15 @agent_ppo2.py:143][0m Total time:      37.67 min
[32m[20221213 21:33:15 @agent_ppo2.py:145][0m 3676160 total steps have happened
[32m[20221213 21:33:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1795 --------------------------#
[32m[20221213 21:33:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:15 @agent_ppo2.py:185][0m |           0.0028 |         189.1994 |           7.2615 |
[32m[20221213 21:33:15 @agent_ppo2.py:185][0m |          -0.0054 |         183.5780 |           7.3339 |
[32m[20221213 21:33:15 @agent_ppo2.py:185][0m |          -0.0065 |         182.1490 |           7.3385 |
[32m[20221213 21:33:15 @agent_ppo2.py:185][0m |          -0.0077 |         181.4155 |           7.3160 |
[32m[20221213 21:33:15 @agent_ppo2.py:185][0m |          -0.0112 |         180.6157 |           7.3340 |
[32m[20221213 21:33:15 @agent_ppo2.py:185][0m |          -0.0063 |         180.1664 |           7.3121 |
[32m[20221213 21:33:15 @agent_ppo2.py:185][0m |          -0.0048 |         181.1954 |           7.3542 |
[32m[20221213 21:33:16 @agent_ppo2.py:185][0m |          -0.0073 |         179.7868 |           7.3729 |
[32m[20221213 21:33:16 @agent_ppo2.py:185][0m |          -0.0100 |         178.9356 |           7.3778 |
[32m[20221213 21:33:16 @agent_ppo2.py:185][0m |          -0.0114 |         178.7713 |           7.3851 |
[32m[20221213 21:33:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:33:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.80
[32m[20221213 21:33:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:33:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.00
[32m[20221213 21:33:16 @agent_ppo2.py:143][0m Total time:      37.69 min
[32m[20221213 21:33:16 @agent_ppo2.py:145][0m 3678208 total steps have happened
[32m[20221213 21:33:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1796 --------------------------#
[32m[20221213 21:33:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:16 @agent_ppo2.py:185][0m |           0.0004 |         174.9940 |           7.3400 |
[32m[20221213 21:33:16 @agent_ppo2.py:185][0m |          -0.0020 |         173.8595 |           7.2287 |
[32m[20221213 21:33:16 @agent_ppo2.py:185][0m |          -0.0078 |         170.9384 |           7.2588 |
[32m[20221213 21:33:16 @agent_ppo2.py:185][0m |          -0.0085 |         169.7559 |           7.2051 |
[32m[20221213 21:33:17 @agent_ppo2.py:185][0m |          -0.0100 |         168.6617 |           7.2265 |
[32m[20221213 21:33:17 @agent_ppo2.py:185][0m |          -0.0096 |         168.0835 |           7.2087 |
[32m[20221213 21:33:17 @agent_ppo2.py:185][0m |          -0.0095 |         167.4679 |           7.1524 |
[32m[20221213 21:33:17 @agent_ppo2.py:185][0m |           0.0035 |         185.6928 |           7.1264 |
[32m[20221213 21:33:17 @agent_ppo2.py:185][0m |          -0.0125 |         166.6912 |           7.1320 |
[32m[20221213 21:33:17 @agent_ppo2.py:185][0m |          -0.0111 |         166.0208 |           7.1517 |
[32m[20221213 21:33:17 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:33:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.00
[32m[20221213 21:33:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.00
[32m[20221213 21:33:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.00
[32m[20221213 21:33:17 @agent_ppo2.py:143][0m Total time:      37.71 min
[32m[20221213 21:33:17 @agent_ppo2.py:145][0m 3680256 total steps have happened
[32m[20221213 21:33:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1797 --------------------------#
[32m[20221213 21:33:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:17 @agent_ppo2.py:185][0m |           0.0007 |         181.4104 |           6.9458 |
[32m[20221213 21:33:17 @agent_ppo2.py:185][0m |           0.0056 |         189.3501 |           7.0062 |
[32m[20221213 21:33:18 @agent_ppo2.py:185][0m |          -0.0049 |         175.9098 |           7.0343 |
[32m[20221213 21:33:18 @agent_ppo2.py:185][0m |          -0.0045 |         175.9033 |           6.9927 |
[32m[20221213 21:33:18 @agent_ppo2.py:185][0m |           0.0147 |         192.5802 |           7.0121 |
[32m[20221213 21:33:18 @agent_ppo2.py:185][0m |           0.0000 |         179.5896 |           6.9941 |
[32m[20221213 21:33:18 @agent_ppo2.py:185][0m |          -0.0039 |         177.5363 |           7.0584 |
[32m[20221213 21:33:18 @agent_ppo2.py:185][0m |          -0.0092 |         174.1476 |           7.0084 |
[32m[20221213 21:33:18 @agent_ppo2.py:185][0m |          -0.0089 |         173.9843 |           7.0198 |
[32m[20221213 21:33:18 @agent_ppo2.py:185][0m |          -0.0069 |         173.9968 |           7.0538 |
[32m[20221213 21:33:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:33:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.00
[32m[20221213 21:33:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:33:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 860.00
[32m[20221213 21:33:18 @agent_ppo2.py:143][0m Total time:      37.73 min
[32m[20221213 21:33:18 @agent_ppo2.py:145][0m 3682304 total steps have happened
[32m[20221213 21:33:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1798 --------------------------#
[32m[20221213 21:33:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:19 @agent_ppo2.py:185][0m |          -0.0020 |         175.8146 |           7.4340 |
[32m[20221213 21:33:19 @agent_ppo2.py:185][0m |          -0.0081 |         175.2893 |           7.3430 |
[32m[20221213 21:33:19 @agent_ppo2.py:185][0m |           0.0026 |         197.7356 |           7.3369 |
[32m[20221213 21:33:19 @agent_ppo2.py:185][0m |          -0.0096 |         174.5331 |           7.3041 |
[32m[20221213 21:33:19 @agent_ppo2.py:185][0m |          -0.0092 |         174.2018 |           7.2533 |
[32m[20221213 21:33:19 @agent_ppo2.py:185][0m |          -0.0099 |         174.0783 |           7.2488 |
[32m[20221213 21:33:19 @agent_ppo2.py:185][0m |          -0.0106 |         173.9685 |           7.2262 |
[32m[20221213 21:33:19 @agent_ppo2.py:185][0m |          -0.0107 |         173.5989 |           7.1741 |
[32m[20221213 21:33:19 @agent_ppo2.py:185][0m |          -0.0104 |         173.7743 |           7.1816 |
[32m[20221213 21:33:19 @agent_ppo2.py:185][0m |          -0.0097 |         173.5393 |           7.1364 |
[32m[20221213 21:33:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:33:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.20
[32m[20221213 21:33:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 851.00
[32m[20221213 21:33:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:33:19 @agent_ppo2.py:143][0m Total time:      37.75 min
[32m[20221213 21:33:19 @agent_ppo2.py:145][0m 3684352 total steps have happened
[32m[20221213 21:33:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1799 --------------------------#
[32m[20221213 21:33:20 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:33:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:20 @agent_ppo2.py:185][0m |           0.0105 |         192.6639 |           7.0879 |
[32m[20221213 21:33:20 @agent_ppo2.py:185][0m |          -0.0031 |         176.1225 |           7.1714 |
[32m[20221213 21:33:20 @agent_ppo2.py:185][0m |           0.0071 |         194.2401 |           7.2503 |
[32m[20221213 21:33:20 @agent_ppo2.py:185][0m |          -0.0071 |         173.3272 |           7.2576 |
[32m[20221213 21:33:20 @agent_ppo2.py:185][0m |          -0.0048 |         172.5582 |           7.2758 |
[32m[20221213 21:33:20 @agent_ppo2.py:185][0m |          -0.0074 |         172.0627 |           7.3141 |
[32m[20221213 21:33:20 @agent_ppo2.py:185][0m |          -0.0089 |         171.8563 |           7.2778 |
[32m[20221213 21:33:20 @agent_ppo2.py:185][0m |          -0.0068 |         171.4255 |           7.3308 |
[32m[20221213 21:33:20 @agent_ppo2.py:185][0m |          -0.0073 |         171.2356 |           7.3007 |
[32m[20221213 21:33:21 @agent_ppo2.py:185][0m |          -0.0092 |         170.8162 |           7.3430 |
[32m[20221213 21:33:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:33:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.00
[32m[20221213 21:33:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.00
[32m[20221213 21:33:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.00
[32m[20221213 21:33:21 @agent_ppo2.py:143][0m Total time:      37.77 min
[32m[20221213 21:33:21 @agent_ppo2.py:145][0m 3686400 total steps have happened
[32m[20221213 21:33:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1800 --------------------------#
[32m[20221213 21:33:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:21 @agent_ppo2.py:185][0m |          -0.0019 |         177.9467 |           7.3315 |
[32m[20221213 21:33:21 @agent_ppo2.py:185][0m |           0.0031 |         184.9651 |           7.3334 |
[32m[20221213 21:33:21 @agent_ppo2.py:185][0m |          -0.0034 |         176.5149 |           7.3819 |
[32m[20221213 21:33:21 @agent_ppo2.py:185][0m |          -0.0049 |         175.7693 |           7.4310 |
[32m[20221213 21:33:21 @agent_ppo2.py:185][0m |           0.0048 |         195.6759 |           7.4400 |
[32m[20221213 21:33:21 @agent_ppo2.py:185][0m |          -0.0040 |         175.1073 |           7.5386 |
[32m[20221213 21:33:21 @agent_ppo2.py:185][0m |          -0.0081 |         174.9320 |           7.4723 |
[32m[20221213 21:33:22 @agent_ppo2.py:185][0m |          -0.0078 |         174.6973 |           7.4721 |
[32m[20221213 21:33:22 @agent_ppo2.py:185][0m |          -0.0087 |         174.5679 |           7.5270 |
[32m[20221213 21:33:22 @agent_ppo2.py:185][0m |          -0.0090 |         174.4976 |           7.5362 |
[32m[20221213 21:33:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:33:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.80
[32m[20221213 21:33:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.00
[32m[20221213 21:33:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.00
[32m[20221213 21:33:22 @agent_ppo2.py:143][0m Total time:      37.79 min
[32m[20221213 21:33:22 @agent_ppo2.py:145][0m 3688448 total steps have happened
[32m[20221213 21:33:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1801 --------------------------#
[32m[20221213 21:33:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:22 @agent_ppo2.py:185][0m |           0.0012 |         178.3019 |           7.0581 |
[32m[20221213 21:33:22 @agent_ppo2.py:185][0m |          -0.0056 |         176.3446 |           7.1402 |
[32m[20221213 21:33:22 @agent_ppo2.py:185][0m |          -0.0019 |         178.9218 |           7.0877 |
[32m[20221213 21:33:22 @agent_ppo2.py:185][0m |          -0.0091 |         175.6639 |           7.1204 |
[32m[20221213 21:33:23 @agent_ppo2.py:185][0m |          -0.0106 |         175.4715 |           7.0958 |
[32m[20221213 21:33:23 @agent_ppo2.py:185][0m |          -0.0096 |         175.2290 |           7.1221 |
[32m[20221213 21:33:23 @agent_ppo2.py:185][0m |          -0.0086 |         175.3252 |           7.1389 |
[32m[20221213 21:33:23 @agent_ppo2.py:185][0m |          -0.0102 |         174.9725 |           7.0815 |
[32m[20221213 21:33:23 @agent_ppo2.py:185][0m |          -0.0049 |         176.6416 |           7.0976 |
[32m[20221213 21:33:23 @agent_ppo2.py:185][0m |          -0.0106 |         174.7764 |           7.0490 |
[32m[20221213 21:33:23 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:33:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.20
[32m[20221213 21:33:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.00
[32m[20221213 21:33:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.00
[32m[20221213 21:33:23 @agent_ppo2.py:143][0m Total time:      37.81 min
[32m[20221213 21:33:23 @agent_ppo2.py:145][0m 3690496 total steps have happened
[32m[20221213 21:33:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1802 --------------------------#
[32m[20221213 21:33:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:23 @agent_ppo2.py:185][0m |          -0.0005 |         176.8515 |           7.1303 |
[32m[20221213 21:33:24 @agent_ppo2.py:185][0m |          -0.0027 |         175.8102 |           7.0812 |
[32m[20221213 21:33:24 @agent_ppo2.py:185][0m |          -0.0004 |         176.6614 |           7.0085 |
[32m[20221213 21:33:24 @agent_ppo2.py:185][0m |          -0.0063 |         174.6769 |           7.0132 |
[32m[20221213 21:33:24 @agent_ppo2.py:185][0m |           0.0078 |         189.0855 |           6.9715 |
[32m[20221213 21:33:24 @agent_ppo2.py:185][0m |          -0.0032 |         174.9518 |           7.0583 |
[32m[20221213 21:33:24 @agent_ppo2.py:185][0m |          -0.0087 |         173.6893 |           7.0264 |
[32m[20221213 21:33:24 @agent_ppo2.py:185][0m |          -0.0095 |         173.5237 |           6.9989 |
[32m[20221213 21:33:24 @agent_ppo2.py:185][0m |          -0.0106 |         173.3639 |           6.9973 |
[32m[20221213 21:33:24 @agent_ppo2.py:185][0m |          -0.0098 |         173.0061 |           6.9785 |
[32m[20221213 21:33:24 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:33:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.80
[32m[20221213 21:33:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:33:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:33:24 @agent_ppo2.py:143][0m Total time:      37.83 min
[32m[20221213 21:33:24 @agent_ppo2.py:145][0m 3692544 total steps have happened
[32m[20221213 21:33:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1803 --------------------------#
[32m[20221213 21:33:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:25 @agent_ppo2.py:185][0m |           0.0006 |         179.8565 |           7.1375 |
[32m[20221213 21:33:25 @agent_ppo2.py:185][0m |           0.0088 |         199.4982 |           7.0732 |
[32m[20221213 21:33:25 @agent_ppo2.py:185][0m |          -0.0074 |         177.7672 |           7.0454 |
[32m[20221213 21:33:25 @agent_ppo2.py:185][0m |          -0.0096 |         177.0936 |           7.0982 |
[32m[20221213 21:33:25 @agent_ppo2.py:185][0m |          -0.0106 |         176.6067 |           7.0498 |
[32m[20221213 21:33:25 @agent_ppo2.py:185][0m |          -0.0026 |         182.5695 |           7.0617 |
[32m[20221213 21:33:25 @agent_ppo2.py:185][0m |          -0.0092 |         176.3101 |           7.0327 |
[32m[20221213 21:33:25 @agent_ppo2.py:185][0m |          -0.0083 |         175.9580 |           7.0626 |
[32m[20221213 21:33:25 @agent_ppo2.py:185][0m |          -0.0115 |         175.7061 |           7.0589 |
[32m[20221213 21:33:25 @agent_ppo2.py:185][0m |          -0.0111 |         175.6180 |           7.0213 |
[32m[20221213 21:33:25 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:33:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.40
[32m[20221213 21:33:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.00
[32m[20221213 21:33:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.00
[32m[20221213 21:33:26 @agent_ppo2.py:143][0m Total time:      37.85 min
[32m[20221213 21:33:26 @agent_ppo2.py:145][0m 3694592 total steps have happened
[32m[20221213 21:33:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1804 --------------------------#
[32m[20221213 21:33:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:33:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:26 @agent_ppo2.py:185][0m |          -0.0026 |         177.6066 |           6.7960 |
[32m[20221213 21:33:26 @agent_ppo2.py:185][0m |          -0.0032 |         176.4691 |           6.8280 |
[32m[20221213 21:33:26 @agent_ppo2.py:185][0m |          -0.0033 |         176.0682 |           6.8118 |
[32m[20221213 21:33:26 @agent_ppo2.py:185][0m |           0.0007 |         179.7802 |           6.8114 |
[32m[20221213 21:33:26 @agent_ppo2.py:185][0m |          -0.0017 |         182.3323 |           6.7686 |
[32m[20221213 21:33:26 @agent_ppo2.py:185][0m |           0.0043 |         200.2456 |           6.7823 |
[32m[20221213 21:33:26 @agent_ppo2.py:185][0m |          -0.0074 |         174.7802 |           6.7529 |
[32m[20221213 21:33:27 @agent_ppo2.py:185][0m |          -0.0072 |         174.5486 |           6.7123 |
[32m[20221213 21:33:27 @agent_ppo2.py:185][0m |          -0.0093 |         174.3665 |           6.7017 |
[32m[20221213 21:33:27 @agent_ppo2.py:185][0m |          -0.0079 |         174.0854 |           6.6865 |
[32m[20221213 21:33:27 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:33:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.20
[32m[20221213 21:33:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:33:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.00
[32m[20221213 21:33:27 @agent_ppo2.py:143][0m Total time:      37.87 min
[32m[20221213 21:33:27 @agent_ppo2.py:145][0m 3696640 total steps have happened
[32m[20221213 21:33:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1805 --------------------------#
[32m[20221213 21:33:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:27 @agent_ppo2.py:185][0m |           0.0013 |         175.9889 |           6.4184 |
[32m[20221213 21:33:27 @agent_ppo2.py:185][0m |          -0.0021 |         174.9781 |           6.4370 |
[32m[20221213 21:33:27 @agent_ppo2.py:185][0m |          -0.0029 |         174.5798 |           6.4642 |
[32m[20221213 21:33:27 @agent_ppo2.py:185][0m |           0.0117 |         188.4624 |           6.4577 |
[32m[20221213 21:33:28 @agent_ppo2.py:185][0m |          -0.0031 |         173.9246 |           6.4201 |
[32m[20221213 21:33:28 @agent_ppo2.py:185][0m |          -0.0063 |         173.6266 |           6.4430 |
[32m[20221213 21:33:28 @agent_ppo2.py:185][0m |          -0.0056 |         173.3682 |           6.4198 |
[32m[20221213 21:33:28 @agent_ppo2.py:185][0m |          -0.0042 |         174.7521 |           6.4264 |
[32m[20221213 21:33:28 @agent_ppo2.py:185][0m |           0.0011 |         183.3402 |           6.4977 |
[32m[20221213 21:33:28 @agent_ppo2.py:185][0m |          -0.0065 |         172.9292 |           6.4616 |
[32m[20221213 21:33:28 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:33:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.40
[32m[20221213 21:33:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:33:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 21:33:28 @agent_ppo2.py:143][0m Total time:      37.89 min
[32m[20221213 21:33:28 @agent_ppo2.py:145][0m 3698688 total steps have happened
[32m[20221213 21:33:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1806 --------------------------#
[32m[20221213 21:33:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:28 @agent_ppo2.py:185][0m |           0.0032 |         177.7346 |           7.3426 |
[32m[20221213 21:33:29 @agent_ppo2.py:185][0m |          -0.0051 |         175.6696 |           7.3842 |
[32m[20221213 21:33:29 @agent_ppo2.py:185][0m |          -0.0075 |         175.5222 |           7.3385 |
[32m[20221213 21:33:29 @agent_ppo2.py:185][0m |          -0.0068 |         175.3597 |           7.3904 |
[32m[20221213 21:33:29 @agent_ppo2.py:185][0m |          -0.0064 |         175.2934 |           7.3888 |
[32m[20221213 21:33:29 @agent_ppo2.py:185][0m |          -0.0093 |         175.1110 |           7.3603 |
[32m[20221213 21:33:29 @agent_ppo2.py:185][0m |          -0.0091 |         174.8810 |           7.3731 |
[32m[20221213 21:33:29 @agent_ppo2.py:185][0m |          -0.0085 |         174.8285 |           7.3618 |
[32m[20221213 21:33:29 @agent_ppo2.py:185][0m |          -0.0096 |         174.8375 |           7.3777 |
[32m[20221213 21:33:29 @agent_ppo2.py:185][0m |          -0.0103 |         174.9208 |           7.3340 |
[32m[20221213 21:33:29 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 21:33:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.00
[32m[20221213 21:33:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.00
[32m[20221213 21:33:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.00
[32m[20221213 21:33:29 @agent_ppo2.py:143][0m Total time:      37.91 min
[32m[20221213 21:33:29 @agent_ppo2.py:145][0m 3700736 total steps have happened
[32m[20221213 21:33:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1807 --------------------------#
[32m[20221213 21:33:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:33:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:30 @agent_ppo2.py:185][0m |          -0.0018 |         174.3559 |           6.3405 |
[32m[20221213 21:33:30 @agent_ppo2.py:185][0m |          -0.0036 |         174.5822 |           6.4956 |
[32m[20221213 21:33:30 @agent_ppo2.py:185][0m |          -0.0032 |         175.0661 |           6.4857 |
[32m[20221213 21:33:30 @agent_ppo2.py:185][0m |           0.0055 |         191.1801 |           6.5091 |
[32m[20221213 21:33:30 @agent_ppo2.py:185][0m |           0.0017 |         178.6609 |           6.4980 |
[32m[20221213 21:33:30 @agent_ppo2.py:185][0m |          -0.0067 |         172.9276 |           6.4868 |
[32m[20221213 21:33:30 @agent_ppo2.py:185][0m |          -0.0087 |         172.4515 |           6.4961 |
[32m[20221213 21:33:30 @agent_ppo2.py:185][0m |          -0.0097 |         172.1499 |           6.5350 |
[32m[20221213 21:33:30 @agent_ppo2.py:185][0m |          -0.0097 |         172.2293 |           6.5144 |
[32m[20221213 21:33:31 @agent_ppo2.py:185][0m |          -0.0095 |         171.9034 |           6.5143 |
[32m[20221213 21:33:31 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:33:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.80
[32m[20221213 21:33:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:33:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.00
[32m[20221213 21:33:31 @agent_ppo2.py:143][0m Total time:      37.93 min
[32m[20221213 21:33:31 @agent_ppo2.py:145][0m 3702784 total steps have happened
[32m[20221213 21:33:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1808 --------------------------#
[32m[20221213 21:33:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:31 @agent_ppo2.py:185][0m |           0.0004 |         173.8390 |           6.7213 |
[32m[20221213 21:33:31 @agent_ppo2.py:185][0m |           0.0010 |         175.4753 |           6.7700 |
[32m[20221213 21:33:31 @agent_ppo2.py:185][0m |          -0.0030 |         172.5786 |           6.8430 |
[32m[20221213 21:33:31 @agent_ppo2.py:185][0m |          -0.0041 |         172.3688 |           6.8564 |
[32m[20221213 21:33:31 @agent_ppo2.py:185][0m |          -0.0058 |         172.0078 |           6.8825 |
[32m[20221213 21:33:31 @agent_ppo2.py:185][0m |           0.0021 |         175.0328 |           6.9817 |
[32m[20221213 21:33:32 @agent_ppo2.py:185][0m |          -0.0050 |         171.7081 |           7.0292 |
[32m[20221213 21:33:32 @agent_ppo2.py:185][0m |          -0.0012 |         176.4760 |           7.0845 |
[32m[20221213 21:33:32 @agent_ppo2.py:185][0m |          -0.0075 |         171.4797 |           7.1320 |
[32m[20221213 21:33:32 @agent_ppo2.py:185][0m |          -0.0067 |         171.0642 |           7.1766 |
[32m[20221213 21:33:32 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:33:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.00
[32m[20221213 21:33:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.00
[32m[20221213 21:33:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 21:33:32 @agent_ppo2.py:143][0m Total time:      37.95 min
[32m[20221213 21:33:32 @agent_ppo2.py:145][0m 3704832 total steps have happened
[32m[20221213 21:33:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1809 --------------------------#
[32m[20221213 21:33:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:32 @agent_ppo2.py:185][0m |           0.0105 |         190.0070 |           7.1344 |
[32m[20221213 21:33:32 @agent_ppo2.py:185][0m |           0.0109 |         200.6893 |           7.1910 |
[32m[20221213 21:33:32 @agent_ppo2.py:185][0m |          -0.0057 |         174.1783 |           7.2477 |
[32m[20221213 21:33:33 @agent_ppo2.py:185][0m |          -0.0054 |         174.0357 |           7.2290 |
[32m[20221213 21:33:33 @agent_ppo2.py:185][0m |          -0.0073 |         173.5944 |           7.2548 |
[32m[20221213 21:33:33 @agent_ppo2.py:185][0m |          -0.0076 |         173.5263 |           7.2974 |
[32m[20221213 21:33:33 @agent_ppo2.py:185][0m |          -0.0084 |         173.3420 |           7.2940 |
[32m[20221213 21:33:33 @agent_ppo2.py:185][0m |           0.0018 |         187.5557 |           7.2947 |
[32m[20221213 21:33:33 @agent_ppo2.py:185][0m |          -0.0084 |         173.1097 |           7.3608 |
[32m[20221213 21:33:33 @agent_ppo2.py:185][0m |          -0.0084 |         172.9160 |           7.3350 |
[32m[20221213 21:33:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:33:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.40
[32m[20221213 21:33:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:33:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 21:33:33 @agent_ppo2.py:143][0m Total time:      37.97 min
[32m[20221213 21:33:33 @agent_ppo2.py:145][0m 3706880 total steps have happened
[32m[20221213 21:33:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1810 --------------------------#
[32m[20221213 21:33:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:33 @agent_ppo2.py:185][0m |           0.0021 |         177.0257 |           7.4730 |
[32m[20221213 21:33:34 @agent_ppo2.py:185][0m |          -0.0032 |         176.2026 |           7.5104 |
[32m[20221213 21:33:34 @agent_ppo2.py:185][0m |          -0.0051 |         175.9356 |           7.5464 |
[32m[20221213 21:33:34 @agent_ppo2.py:185][0m |          -0.0063 |         175.6062 |           7.5858 |
[32m[20221213 21:33:34 @agent_ppo2.py:185][0m |          -0.0069 |         175.3445 |           7.5941 |
[32m[20221213 21:33:34 @agent_ppo2.py:185][0m |          -0.0085 |         175.0714 |           7.5758 |
[32m[20221213 21:33:34 @agent_ppo2.py:185][0m |          -0.0032 |         179.3512 |           7.5580 |
[32m[20221213 21:33:34 @agent_ppo2.py:185][0m |          -0.0085 |         174.6348 |           7.6113 |
[32m[20221213 21:33:34 @agent_ppo2.py:185][0m |          -0.0080 |         174.3751 |           7.6097 |
[32m[20221213 21:33:34 @agent_ppo2.py:185][0m |          -0.0083 |         174.2782 |           7.6297 |
[32m[20221213 21:33:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:33:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.00
[32m[20221213 21:33:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.00
[32m[20221213 21:33:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.00
[32m[20221213 21:33:34 @agent_ppo2.py:143][0m Total time:      37.99 min
[32m[20221213 21:33:34 @agent_ppo2.py:145][0m 3708928 total steps have happened
[32m[20221213 21:33:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1811 --------------------------#
[32m[20221213 21:33:34 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:35 @agent_ppo2.py:185][0m |          -0.0022 |         180.2465 |           7.8962 |
[32m[20221213 21:33:35 @agent_ppo2.py:185][0m |          -0.0022 |         179.8389 |           7.9482 |
[32m[20221213 21:33:35 @agent_ppo2.py:185][0m |           0.0014 |         182.8986 |           7.9743 |
[32m[20221213 21:33:35 @agent_ppo2.py:185][0m |          -0.0003 |         185.7386 |           8.0315 |
[32m[20221213 21:33:35 @agent_ppo2.py:185][0m |          -0.0077 |         177.4898 |           7.9694 |
[32m[20221213 21:33:35 @agent_ppo2.py:185][0m |          -0.0042 |         179.1738 |           8.0124 |
[32m[20221213 21:33:35 @agent_ppo2.py:185][0m |          -0.0100 |         176.8402 |           8.0251 |
[32m[20221213 21:33:35 @agent_ppo2.py:185][0m |          -0.0073 |         176.5780 |           8.0722 |
[32m[20221213 21:33:35 @agent_ppo2.py:185][0m |          -0.0059 |         179.4193 |           8.0681 |
[32m[20221213 21:33:35 @agent_ppo2.py:185][0m |          -0.0107 |         176.2894 |           8.0851 |
[32m[20221213 21:33:35 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:33:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.80
[32m[20221213 21:33:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:33:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 21:33:36 @agent_ppo2.py:143][0m Total time:      38.01 min
[32m[20221213 21:33:36 @agent_ppo2.py:145][0m 3710976 total steps have happened
[32m[20221213 21:33:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1812 --------------------------#
[32m[20221213 21:33:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:36 @agent_ppo2.py:185][0m |           0.0084 |         181.7009 |           7.9273 |
[32m[20221213 21:33:36 @agent_ppo2.py:185][0m |          -0.0037 |         178.3719 |           7.9569 |
[32m[20221213 21:33:36 @agent_ppo2.py:185][0m |           0.0052 |         187.3033 |           7.9588 |
[32m[20221213 21:33:36 @agent_ppo2.py:185][0m |          -0.0062 |         177.6010 |           7.9563 |
[32m[20221213 21:33:36 @agent_ppo2.py:185][0m |          -0.0055 |         177.3949 |           7.9501 |
[32m[20221213 21:33:36 @agent_ppo2.py:185][0m |          -0.0028 |         177.4082 |           7.9856 |
[32m[20221213 21:33:36 @agent_ppo2.py:185][0m |          -0.0068 |         177.3048 |           7.9970 |
[32m[20221213 21:33:36 @agent_ppo2.py:185][0m |          -0.0074 |         176.8582 |           7.9680 |
[32m[20221213 21:33:37 @agent_ppo2.py:185][0m |          -0.0079 |         176.9441 |           7.9648 |
[32m[20221213 21:33:37 @agent_ppo2.py:185][0m |          -0.0076 |         176.8275 |           7.9557 |
[32m[20221213 21:33:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:33:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.60
[32m[20221213 21:33:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.00
[32m[20221213 21:33:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.00
[32m[20221213 21:33:37 @agent_ppo2.py:143][0m Total time:      38.03 min
[32m[20221213 21:33:37 @agent_ppo2.py:145][0m 3713024 total steps have happened
[32m[20221213 21:33:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1813 --------------------------#
[32m[20221213 21:33:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:37 @agent_ppo2.py:185][0m |           0.0030 |         177.6416 |           7.5388 |
[32m[20221213 21:33:37 @agent_ppo2.py:185][0m |          -0.0027 |         175.7766 |           7.6263 |
[32m[20221213 21:33:37 @agent_ppo2.py:185][0m |          -0.0036 |         175.3756 |           7.6363 |
[32m[20221213 21:33:37 @agent_ppo2.py:185][0m |          -0.0047 |         175.1922 |           7.7095 |
[32m[20221213 21:33:37 @agent_ppo2.py:185][0m |          -0.0055 |         175.0838 |           7.7540 |
[32m[20221213 21:33:38 @agent_ppo2.py:185][0m |          -0.0061 |         174.8800 |           7.7769 |
[32m[20221213 21:33:38 @agent_ppo2.py:185][0m |          -0.0072 |         174.7345 |           7.8120 |
[32m[20221213 21:33:38 @agent_ppo2.py:185][0m |          -0.0061 |         174.7239 |           7.8133 |
[32m[20221213 21:33:38 @agent_ppo2.py:185][0m |          -0.0082 |         174.6100 |           7.8601 |
[32m[20221213 21:33:38 @agent_ppo2.py:185][0m |          -0.0090 |         174.3393 |           7.9131 |
[32m[20221213 21:33:38 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:33:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.20
[32m[20221213 21:33:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.00
[32m[20221213 21:33:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.00
[32m[20221213 21:33:38 @agent_ppo2.py:143][0m Total time:      38.06 min
[32m[20221213 21:33:38 @agent_ppo2.py:145][0m 3715072 total steps have happened
[32m[20221213 21:33:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1814 --------------------------#
[32m[20221213 21:33:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:38 @agent_ppo2.py:185][0m |          -0.0012 |         178.0081 |           8.0035 |
[32m[20221213 21:33:38 @agent_ppo2.py:185][0m |          -0.0032 |         176.9066 |           7.9799 |
[32m[20221213 21:33:39 @agent_ppo2.py:185][0m |          -0.0070 |         176.2401 |           7.9901 |
[32m[20221213 21:33:39 @agent_ppo2.py:185][0m |          -0.0074 |         175.4835 |           7.9819 |
[32m[20221213 21:33:39 @agent_ppo2.py:185][0m |          -0.0085 |         174.7408 |           8.0039 |
[32m[20221213 21:33:39 @agent_ppo2.py:185][0m |          -0.0080 |         174.2626 |           7.9667 |
[32m[20221213 21:33:39 @agent_ppo2.py:185][0m |          -0.0102 |         173.9714 |           8.0084 |
[32m[20221213 21:33:39 @agent_ppo2.py:185][0m |          -0.0105 |         173.6483 |           7.9926 |
[32m[20221213 21:33:39 @agent_ppo2.py:185][0m |          -0.0097 |         173.4060 |           8.0201 |
[32m[20221213 21:33:39 @agent_ppo2.py:185][0m |          -0.0110 |         173.1708 |           8.0182 |
[32m[20221213 21:33:39 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:33:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.80
[32m[20221213 21:33:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.00
[32m[20221213 21:33:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.00
[32m[20221213 21:33:39 @agent_ppo2.py:143][0m Total time:      38.08 min
[32m[20221213 21:33:39 @agent_ppo2.py:145][0m 3717120 total steps have happened
[32m[20221213 21:33:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1815 --------------------------#
[32m[20221213 21:33:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:40 @agent_ppo2.py:185][0m |           0.0036 |         179.7435 |           7.8551 |
[32m[20221213 21:33:40 @agent_ppo2.py:185][0m |           0.0078 |         194.3085 |           7.8528 |
[32m[20221213 21:33:40 @agent_ppo2.py:185][0m |          -0.0057 |         178.0761 |           7.8440 |
[32m[20221213 21:33:40 @agent_ppo2.py:185][0m |          -0.0013 |         179.6985 |           7.8734 |
[32m[20221213 21:33:40 @agent_ppo2.py:185][0m |          -0.0056 |         177.7351 |           7.8056 |
[32m[20221213 21:33:40 @agent_ppo2.py:185][0m |          -0.0052 |         177.8897 |           7.8010 |
[32m[20221213 21:33:40 @agent_ppo2.py:185][0m |          -0.0082 |         177.5208 |           7.7974 |
[32m[20221213 21:33:40 @agent_ppo2.py:185][0m |          -0.0084 |         177.2711 |           7.7371 |
[32m[20221213 21:33:40 @agent_ppo2.py:185][0m |          -0.0088 |         177.2016 |           7.7124 |
[32m[20221213 21:33:40 @agent_ppo2.py:185][0m |          -0.0084 |         177.3524 |           7.6717 |
[32m[20221213 21:33:40 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:33:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.40
[32m[20221213 21:33:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:33:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.00
[32m[20221213 21:33:41 @agent_ppo2.py:143][0m Total time:      38.10 min
[32m[20221213 21:33:41 @agent_ppo2.py:145][0m 3719168 total steps have happened
[32m[20221213 21:33:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1816 --------------------------#
[32m[20221213 21:33:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:41 @agent_ppo2.py:185][0m |           0.0006 |         179.1356 |           7.8286 |
[32m[20221213 21:33:41 @agent_ppo2.py:185][0m |          -0.0031 |         178.4555 |           7.9024 |
[32m[20221213 21:33:41 @agent_ppo2.py:185][0m |          -0.0033 |         177.7252 |           7.8880 |
[32m[20221213 21:33:41 @agent_ppo2.py:185][0m |          -0.0058 |         177.4439 |           7.8794 |
[32m[20221213 21:33:41 @agent_ppo2.py:185][0m |          -0.0043 |         176.9135 |           7.8625 |
[32m[20221213 21:33:41 @agent_ppo2.py:185][0m |          -0.0050 |         176.5171 |           7.8747 |
[32m[20221213 21:33:41 @agent_ppo2.py:185][0m |          -0.0064 |         176.2313 |           7.9161 |
[32m[20221213 21:33:41 @agent_ppo2.py:185][0m |           0.0053 |         198.6107 |           7.8977 |
[32m[20221213 21:33:42 @agent_ppo2.py:185][0m |          -0.0066 |         175.8594 |           7.9128 |
[32m[20221213 21:33:42 @agent_ppo2.py:185][0m |          -0.0086 |         175.4792 |           7.9488 |
[32m[20221213 21:33:42 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:33:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.80
[32m[20221213 21:33:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221213 21:33:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.00
[32m[20221213 21:33:42 @agent_ppo2.py:143][0m Total time:      38.12 min
[32m[20221213 21:33:42 @agent_ppo2.py:145][0m 3721216 total steps have happened
[32m[20221213 21:33:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1817 --------------------------#
[32m[20221213 21:33:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:33:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:42 @agent_ppo2.py:185][0m |           0.0123 |         185.5340 |           7.6254 |
[32m[20221213 21:33:42 @agent_ppo2.py:185][0m |          -0.0041 |         175.1184 |           7.6943 |
[32m[20221213 21:33:42 @agent_ppo2.py:185][0m |          -0.0060 |         174.6587 |           7.7052 |
[32m[20221213 21:33:42 @agent_ppo2.py:185][0m |          -0.0060 |         174.1732 |           7.6702 |
[32m[20221213 21:33:42 @agent_ppo2.py:185][0m |          -0.0010 |         176.0430 |           7.7302 |
[32m[20221213 21:33:43 @agent_ppo2.py:185][0m |           0.0084 |         192.9639 |           7.7594 |
[32m[20221213 21:33:43 @agent_ppo2.py:185][0m |          -0.0094 |         173.0248 |           7.6635 |
[32m[20221213 21:33:43 @agent_ppo2.py:185][0m |          -0.0087 |         172.5732 |           7.7389 |
[32m[20221213 21:33:43 @agent_ppo2.py:185][0m |          -0.0096 |         172.2630 |           7.7458 |
[32m[20221213 21:33:43 @agent_ppo2.py:185][0m |           0.0007 |         176.7251 |           7.7609 |
[32m[20221213 21:33:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:33:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.60
[32m[20221213 21:33:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:33:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.00
[32m[20221213 21:33:43 @agent_ppo2.py:143][0m Total time:      38.14 min
[32m[20221213 21:33:43 @agent_ppo2.py:145][0m 3723264 total steps have happened
[32m[20221213 21:33:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1818 --------------------------#
[32m[20221213 21:33:43 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:33:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:43 @agent_ppo2.py:185][0m |           0.0023 |         174.9141 |           7.8742 |
[32m[20221213 21:33:43 @agent_ppo2.py:185][0m |           0.0000 |         173.0054 |           7.8824 |
[32m[20221213 21:33:43 @agent_ppo2.py:185][0m |          -0.0029 |         172.1832 |           7.9397 |
[32m[20221213 21:33:44 @agent_ppo2.py:185][0m |          -0.0047 |         171.9538 |           7.9364 |
[32m[20221213 21:33:44 @agent_ppo2.py:185][0m |          -0.0009 |         177.1428 |           7.9116 |
[32m[20221213 21:33:44 @agent_ppo2.py:185][0m |          -0.0043 |         171.6132 |           7.9974 |
[32m[20221213 21:33:44 @agent_ppo2.py:185][0m |          -0.0028 |         172.3774 |           7.9976 |
[32m[20221213 21:33:44 @agent_ppo2.py:185][0m |          -0.0040 |         171.1930 |           7.9934 |
[32m[20221213 21:33:44 @agent_ppo2.py:185][0m |          -0.0060 |         171.1607 |           7.9640 |
[32m[20221213 21:33:44 @agent_ppo2.py:185][0m |          -0.0057 |         170.9511 |           7.9664 |
[32m[20221213 21:33:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:33:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.80
[32m[20221213 21:33:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:33:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.00
[32m[20221213 21:33:44 @agent_ppo2.py:143][0m Total time:      38.16 min
[32m[20221213 21:33:44 @agent_ppo2.py:145][0m 3725312 total steps have happened
[32m[20221213 21:33:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1819 --------------------------#
[32m[20221213 21:33:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:44 @agent_ppo2.py:185][0m |          -0.0038 |         177.9706 |           8.2182 |
[32m[20221213 21:33:45 @agent_ppo2.py:185][0m |          -0.0086 |         174.7996 |           8.2482 |
[32m[20221213 21:33:45 @agent_ppo2.py:185][0m |          -0.0030 |         174.6820 |           8.2890 |
[32m[20221213 21:33:45 @agent_ppo2.py:185][0m |          -0.0085 |         172.3445 |           8.2799 |
[32m[20221213 21:33:45 @agent_ppo2.py:185][0m |          -0.0096 |         171.8875 |           8.2747 |
[32m[20221213 21:33:45 @agent_ppo2.py:185][0m |          -0.0086 |         171.7060 |           8.3219 |
[32m[20221213 21:33:45 @agent_ppo2.py:185][0m |          -0.0100 |         171.4558 |           8.3122 |
[32m[20221213 21:33:45 @agent_ppo2.py:185][0m |          -0.0107 |         171.3086 |           8.3365 |
[32m[20221213 21:33:45 @agent_ppo2.py:185][0m |          -0.0104 |         171.0893 |           8.3284 |
[32m[20221213 21:33:45 @agent_ppo2.py:185][0m |          -0.0115 |         170.9894 |           8.3747 |
[32m[20221213 21:33:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:33:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.80
[32m[20221213 21:33:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.00
[32m[20221213 21:33:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.00
[32m[20221213 21:33:45 @agent_ppo2.py:143][0m Total time:      38.18 min
[32m[20221213 21:33:45 @agent_ppo2.py:145][0m 3727360 total steps have happened
[32m[20221213 21:33:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1820 --------------------------#
[32m[20221213 21:33:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:33:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:46 @agent_ppo2.py:185][0m |          -0.0020 |         173.8204 |           7.9439 |
[32m[20221213 21:33:46 @agent_ppo2.py:185][0m |           0.0217 |         203.9763 |           7.9558 |
[32m[20221213 21:33:46 @agent_ppo2.py:185][0m |          -0.0007 |         170.2886 |           8.0806 |
[32m[20221213 21:33:46 @agent_ppo2.py:185][0m |          -0.0041 |         169.5746 |           7.9471 |
[32m[20221213 21:33:46 @agent_ppo2.py:185][0m |          -0.0079 |         169.0881 |           7.9158 |
[32m[20221213 21:33:46 @agent_ppo2.py:185][0m |          -0.0083 |         168.8949 |           7.8938 |
[32m[20221213 21:33:46 @agent_ppo2.py:185][0m |          -0.0077 |         168.5310 |           7.9312 |
[32m[20221213 21:33:46 @agent_ppo2.py:185][0m |          -0.0067 |         168.1823 |           7.8632 |
[32m[20221213 21:33:46 @agent_ppo2.py:185][0m |          -0.0091 |         167.7280 |           7.8313 |
[32m[20221213 21:33:46 @agent_ppo2.py:185][0m |          -0.0029 |         171.6936 |           7.8554 |
[32m[20221213 21:33:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:33:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.80
[32m[20221213 21:33:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.00
[32m[20221213 21:33:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.00
[32m[20221213 21:33:47 @agent_ppo2.py:143][0m Total time:      38.20 min
[32m[20221213 21:33:47 @agent_ppo2.py:145][0m 3729408 total steps have happened
[32m[20221213 21:33:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1821 --------------------------#
[32m[20221213 21:33:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:47 @agent_ppo2.py:185][0m |           0.0037 |         175.7972 |           8.0582 |
[32m[20221213 21:33:47 @agent_ppo2.py:185][0m |          -0.0006 |         176.0823 |           8.0593 |
[32m[20221213 21:33:47 @agent_ppo2.py:185][0m |           0.0030 |         189.4330 |           8.0848 |
[32m[20221213 21:33:47 @agent_ppo2.py:185][0m |           0.0012 |         182.1670 |           8.0724 |
[32m[20221213 21:33:47 @agent_ppo2.py:185][0m |          -0.0069 |         172.6136 |           8.0614 |
[32m[20221213 21:33:47 @agent_ppo2.py:185][0m |          -0.0074 |         172.3222 |           8.0189 |
[32m[20221213 21:33:47 @agent_ppo2.py:185][0m |          -0.0054 |         172.5570 |           8.0097 |
[32m[20221213 21:33:48 @agent_ppo2.py:185][0m |          -0.0092 |         172.0029 |           8.0339 |
[32m[20221213 21:33:48 @agent_ppo2.py:185][0m |          -0.0098 |         171.7760 |           8.0359 |
[32m[20221213 21:33:48 @agent_ppo2.py:185][0m |          -0.0097 |         171.6476 |           8.0473 |
[32m[20221213 21:33:48 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:33:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.60
[32m[20221213 21:33:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.00
[32m[20221213 21:33:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.00
[32m[20221213 21:33:48 @agent_ppo2.py:143][0m Total time:      38.22 min
[32m[20221213 21:33:48 @agent_ppo2.py:145][0m 3731456 total steps have happened
[32m[20221213 21:33:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1822 --------------------------#
[32m[20221213 21:33:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:48 @agent_ppo2.py:185][0m |           0.0002 |         174.0181 |           8.0660 |
[32m[20221213 21:33:48 @agent_ppo2.py:185][0m |          -0.0041 |         173.3507 |           8.0819 |
[32m[20221213 21:33:48 @agent_ppo2.py:185][0m |           0.0048 |         182.7758 |           8.1413 |
[32m[20221213 21:33:48 @agent_ppo2.py:185][0m |          -0.0057 |         172.5025 |           8.1393 |
[32m[20221213 21:33:49 @agent_ppo2.py:185][0m |          -0.0059 |         172.2655 |           8.1694 |
[32m[20221213 21:33:49 @agent_ppo2.py:185][0m |          -0.0055 |         171.8147 |           8.1527 |
[32m[20221213 21:33:49 @agent_ppo2.py:185][0m |          -0.0052 |         171.8224 |           8.1471 |
[32m[20221213 21:33:49 @agent_ppo2.py:185][0m |          -0.0065 |         171.4155 |           8.2493 |
[32m[20221213 21:33:49 @agent_ppo2.py:185][0m |          -0.0085 |         171.2732 |           8.1579 |
[32m[20221213 21:33:49 @agent_ppo2.py:185][0m |          -0.0045 |         171.2922 |           8.2333 |
[32m[20221213 21:33:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:33:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.60
[32m[20221213 21:33:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.00
[32m[20221213 21:33:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.00
[32m[20221213 21:33:49 @agent_ppo2.py:143][0m Total time:      38.24 min
[32m[20221213 21:33:49 @agent_ppo2.py:145][0m 3733504 total steps have happened
[32m[20221213 21:33:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1823 --------------------------#
[32m[20221213 21:33:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:49 @agent_ppo2.py:185][0m |           0.0006 |         176.1677 |           7.9225 |
[32m[20221213 21:33:49 @agent_ppo2.py:185][0m |          -0.0011 |         176.0016 |           7.9209 |
[32m[20221213 21:33:50 @agent_ppo2.py:185][0m |           0.0041 |         185.6802 |           7.9553 |
[32m[20221213 21:33:50 @agent_ppo2.py:185][0m |          -0.0060 |         173.6527 |           7.9604 |
[32m[20221213 21:33:50 @agent_ppo2.py:185][0m |          -0.0089 |         173.2765 |           7.9840 |
[32m[20221213 21:33:50 @agent_ppo2.py:185][0m |          -0.0080 |         173.1122 |           8.0041 |
[32m[20221213 21:33:50 @agent_ppo2.py:185][0m |          -0.0092 |         173.1294 |           8.0454 |
[32m[20221213 21:33:50 @agent_ppo2.py:185][0m |           0.0137 |         214.7650 |           8.0341 |
[32m[20221213 21:33:50 @agent_ppo2.py:185][0m |          -0.0049 |         173.5245 |           8.1288 |
[32m[20221213 21:33:50 @agent_ppo2.py:185][0m |          -0.0110 |         172.7076 |           8.0514 |
[32m[20221213 21:33:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:33:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.80
[32m[20221213 21:33:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.00
[32m[20221213 21:33:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.00
[32m[20221213 21:33:50 @agent_ppo2.py:143][0m Total time:      38.26 min
[32m[20221213 21:33:50 @agent_ppo2.py:145][0m 3735552 total steps have happened
[32m[20221213 21:33:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1824 --------------------------#
[32m[20221213 21:33:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:51 @agent_ppo2.py:185][0m |          -0.0041 |         175.4654 |           8.2468 |
[32m[20221213 21:33:51 @agent_ppo2.py:185][0m |          -0.0052 |         174.0222 |           8.2915 |
[32m[20221213 21:33:51 @agent_ppo2.py:185][0m |          -0.0054 |         173.3957 |           8.3495 |
[32m[20221213 21:33:51 @agent_ppo2.py:185][0m |          -0.0058 |         172.5800 |           8.3621 |
[32m[20221213 21:33:51 @agent_ppo2.py:185][0m |          -0.0082 |         172.2135 |           8.3890 |
[32m[20221213 21:33:51 @agent_ppo2.py:185][0m |          -0.0074 |         171.6700 |           8.4013 |
[32m[20221213 21:33:51 @agent_ppo2.py:185][0m |          -0.0085 |         171.4397 |           8.4228 |
[32m[20221213 21:33:51 @agent_ppo2.py:185][0m |          -0.0103 |         171.0358 |           8.3958 |
[32m[20221213 21:33:51 @agent_ppo2.py:185][0m |          -0.0095 |         170.9413 |           8.4738 |
[32m[20221213 21:33:51 @agent_ppo2.py:185][0m |          -0.0076 |         170.5746 |           8.4472 |
[32m[20221213 21:33:51 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:33:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.00
[32m[20221213 21:33:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.00
[32m[20221213 21:33:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.00
[32m[20221213 21:33:52 @agent_ppo2.py:143][0m Total time:      38.28 min
[32m[20221213 21:33:52 @agent_ppo2.py:145][0m 3737600 total steps have happened
[32m[20221213 21:33:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1825 --------------------------#
[32m[20221213 21:33:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:52 @agent_ppo2.py:185][0m |           0.0113 |         190.2299 |           8.5418 |
[32m[20221213 21:33:52 @agent_ppo2.py:185][0m |          -0.0032 |         176.5737 |           8.5878 |
[32m[20221213 21:33:52 @agent_ppo2.py:185][0m |          -0.0044 |         175.9472 |           8.5925 |
[32m[20221213 21:33:52 @agent_ppo2.py:185][0m |          -0.0060 |         175.5711 |           8.5798 |
[32m[20221213 21:33:52 @agent_ppo2.py:185][0m |          -0.0061 |         175.1661 |           8.5709 |
[32m[20221213 21:33:52 @agent_ppo2.py:185][0m |          -0.0074 |         174.7897 |           8.5767 |
[32m[20221213 21:33:52 @agent_ppo2.py:185][0m |          -0.0046 |         175.8621 |           8.5856 |
[32m[20221213 21:33:52 @agent_ppo2.py:185][0m |          -0.0082 |         174.4280 |           8.6227 |
[32m[20221213 21:33:53 @agent_ppo2.py:185][0m |           0.0112 |         201.7503 |           8.6029 |
[32m[20221213 21:33:53 @agent_ppo2.py:185][0m |          -0.0059 |         174.1440 |           8.6727 |
[32m[20221213 21:33:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:33:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.80
[32m[20221213 21:33:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 850.00
[32m[20221213 21:33:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 21:33:53 @agent_ppo2.py:143][0m Total time:      38.30 min
[32m[20221213 21:33:53 @agent_ppo2.py:145][0m 3739648 total steps have happened
[32m[20221213 21:33:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1826 --------------------------#
[32m[20221213 21:33:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:53 @agent_ppo2.py:185][0m |          -0.0007 |         176.5811 |           8.1936 |
[32m[20221213 21:33:53 @agent_ppo2.py:185][0m |          -0.0054 |         175.7271 |           8.2667 |
[32m[20221213 21:33:53 @agent_ppo2.py:185][0m |          -0.0083 |         175.1438 |           8.2806 |
[32m[20221213 21:33:53 @agent_ppo2.py:185][0m |          -0.0071 |         174.6283 |           8.2868 |
[32m[20221213 21:33:53 @agent_ppo2.py:185][0m |          -0.0085 |         174.1839 |           8.2533 |
[32m[20221213 21:33:53 @agent_ppo2.py:185][0m |          -0.0101 |         173.9846 |           8.2634 |
[32m[20221213 21:33:54 @agent_ppo2.py:185][0m |          -0.0109 |         173.6048 |           8.3010 |
[32m[20221213 21:33:54 @agent_ppo2.py:185][0m |          -0.0099 |         174.0221 |           8.2969 |
[32m[20221213 21:33:54 @agent_ppo2.py:185][0m |          -0.0009 |         191.4850 |           8.3475 |
[32m[20221213 21:33:54 @agent_ppo2.py:185][0m |          -0.0098 |         173.0876 |           8.3262 |
[32m[20221213 21:33:54 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:33:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.20
[32m[20221213 21:33:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.00
[32m[20221213 21:33:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.00
[32m[20221213 21:33:54 @agent_ppo2.py:143][0m Total time:      38.32 min
[32m[20221213 21:33:54 @agent_ppo2.py:145][0m 3741696 total steps have happened
[32m[20221213 21:33:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1827 --------------------------#
[32m[20221213 21:33:54 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:33:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:54 @agent_ppo2.py:185][0m |          -0.0022 |         176.9778 |           8.8405 |
[32m[20221213 21:33:54 @agent_ppo2.py:185][0m |           0.0062 |         191.9424 |           8.8708 |
[32m[20221213 21:33:54 @agent_ppo2.py:185][0m |          -0.0029 |         176.3352 |           8.9063 |
[32m[20221213 21:33:55 @agent_ppo2.py:185][0m |          -0.0078 |         175.0072 |           8.8709 |
[32m[20221213 21:33:55 @agent_ppo2.py:185][0m |          -0.0090 |         174.7653 |           8.9342 |
[32m[20221213 21:33:55 @agent_ppo2.py:185][0m |           0.0000 |         180.0142 |           8.9284 |
[32m[20221213 21:33:55 @agent_ppo2.py:185][0m |          -0.0097 |         174.0785 |           8.9427 |
[32m[20221213 21:33:55 @agent_ppo2.py:185][0m |          -0.0091 |         174.0225 |           8.9473 |
[32m[20221213 21:33:55 @agent_ppo2.py:185][0m |          -0.0084 |         173.8072 |           9.0316 |
[32m[20221213 21:33:55 @agent_ppo2.py:185][0m |          -0.0113 |         173.7409 |           9.0048 |
[32m[20221213 21:33:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:33:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.20
[32m[20221213 21:33:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:33:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.00
[32m[20221213 21:33:55 @agent_ppo2.py:143][0m Total time:      38.34 min
[32m[20221213 21:33:55 @agent_ppo2.py:145][0m 3743744 total steps have happened
[32m[20221213 21:33:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1828 --------------------------#
[32m[20221213 21:33:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:55 @agent_ppo2.py:185][0m |          -0.0002 |         174.6198 |           8.7982 |
[32m[20221213 21:33:56 @agent_ppo2.py:185][0m |          -0.0025 |         173.9238 |           8.8455 |
[32m[20221213 21:33:56 @agent_ppo2.py:185][0m |          -0.0054 |         173.4840 |           8.8612 |
[32m[20221213 21:33:56 @agent_ppo2.py:185][0m |          -0.0038 |         173.2257 |           8.8318 |
[32m[20221213 21:33:56 @agent_ppo2.py:185][0m |          -0.0074 |         172.9494 |           8.8888 |
[32m[20221213 21:33:56 @agent_ppo2.py:185][0m |          -0.0082 |         172.7767 |           8.9012 |
[32m[20221213 21:33:56 @agent_ppo2.py:185][0m |          -0.0055 |         172.4555 |           8.9190 |
[32m[20221213 21:33:56 @agent_ppo2.py:185][0m |           0.0012 |         181.3096 |           8.9543 |
[32m[20221213 21:33:56 @agent_ppo2.py:185][0m |          -0.0074 |         172.1654 |           8.9434 |
[32m[20221213 21:33:56 @agent_ppo2.py:185][0m |          -0.0077 |         171.9531 |           8.9524 |
[32m[20221213 21:33:56 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:33:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.40
[32m[20221213 21:33:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.00
[32m[20221213 21:33:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.00
[32m[20221213 21:33:56 @agent_ppo2.py:143][0m Total time:      38.36 min
[32m[20221213 21:33:56 @agent_ppo2.py:145][0m 3745792 total steps have happened
[32m[20221213 21:33:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1829 --------------------------#
[32m[20221213 21:33:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:57 @agent_ppo2.py:185][0m |           0.0025 |         176.2917 |           8.7970 |
[32m[20221213 21:33:57 @agent_ppo2.py:185][0m |          -0.0039 |         173.6919 |           8.8389 |
[32m[20221213 21:33:57 @agent_ppo2.py:185][0m |          -0.0065 |         172.9565 |           8.8381 |
[32m[20221213 21:33:57 @agent_ppo2.py:185][0m |           0.0023 |         175.3716 |           8.7958 |
[32m[20221213 21:33:57 @agent_ppo2.py:185][0m |           0.0083 |         188.1894 |           8.8045 |
[32m[20221213 21:33:57 @agent_ppo2.py:185][0m |           0.0171 |         209.0000 |           8.8492 |
[32m[20221213 21:33:57 @agent_ppo2.py:185][0m |           0.0032 |         188.3087 |           8.9218 |
[32m[20221213 21:33:57 @agent_ppo2.py:185][0m |          -0.0063 |         171.3551 |           8.8445 |
[32m[20221213 21:33:57 @agent_ppo2.py:185][0m |          -0.0058 |         171.1316 |           8.8592 |
[32m[20221213 21:33:58 @agent_ppo2.py:185][0m |          -0.0069 |         171.0104 |           8.8764 |
[32m[20221213 21:33:58 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 21:33:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.40
[32m[20221213 21:33:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 865.00
[32m[20221213 21:33:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 873.00
[32m[20221213 21:33:58 @agent_ppo2.py:143][0m Total time:      38.38 min
[32m[20221213 21:33:58 @agent_ppo2.py:145][0m 3747840 total steps have happened
[32m[20221213 21:33:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1830 --------------------------#
[32m[20221213 21:33:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:33:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:58 @agent_ppo2.py:185][0m |          -0.0016 |         176.8197 |           8.9226 |
[32m[20221213 21:33:58 @agent_ppo2.py:185][0m |          -0.0055 |         175.8969 |           8.8258 |
[32m[20221213 21:33:58 @agent_ppo2.py:185][0m |          -0.0066 |         175.4272 |           8.8484 |
[32m[20221213 21:33:58 @agent_ppo2.py:185][0m |          -0.0070 |         175.1739 |           8.8930 |
[32m[20221213 21:33:58 @agent_ppo2.py:185][0m |          -0.0067 |         174.8974 |           8.8978 |
[32m[20221213 21:33:59 @agent_ppo2.py:185][0m |          -0.0087 |         174.6471 |           8.9127 |
[32m[20221213 21:33:59 @agent_ppo2.py:185][0m |          -0.0083 |         174.2799 |           8.9257 |
[32m[20221213 21:33:59 @agent_ppo2.py:185][0m |          -0.0087 |         174.1036 |           8.9183 |
[32m[20221213 21:33:59 @agent_ppo2.py:185][0m |          -0.0091 |         174.1148 |           8.9416 |
[32m[20221213 21:33:59 @agent_ppo2.py:185][0m |          -0.0032 |         179.8104 |           8.9353 |
[32m[20221213 21:33:59 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 21:33:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.20
[32m[20221213 21:33:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:33:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.00
[32m[20221213 21:33:59 @agent_ppo2.py:143][0m Total time:      38.41 min
[32m[20221213 21:33:59 @agent_ppo2.py:145][0m 3749888 total steps have happened
[32m[20221213 21:33:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1831 --------------------------#
[32m[20221213 21:33:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:33:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:33:59 @agent_ppo2.py:185][0m |          -0.0025 |         178.9854 |           8.7152 |
[32m[20221213 21:34:00 @agent_ppo2.py:185][0m |           0.0084 |         202.0257 |           8.7749 |
[32m[20221213 21:34:00 @agent_ppo2.py:185][0m |          -0.0040 |         177.6493 |           8.8403 |
[32m[20221213 21:34:00 @agent_ppo2.py:185][0m |           0.0032 |         183.3969 |           8.8528 |
[32m[20221213 21:34:00 @agent_ppo2.py:185][0m |          -0.0063 |         177.2690 |           8.8248 |
[32m[20221213 21:34:00 @agent_ppo2.py:185][0m |          -0.0058 |         176.8949 |           8.8630 |
[32m[20221213 21:34:00 @agent_ppo2.py:185][0m |          -0.0064 |         176.8140 |           8.8787 |
[32m[20221213 21:34:00 @agent_ppo2.py:185][0m |           0.0050 |         203.5897 |           8.9176 |
[32m[20221213 21:34:00 @agent_ppo2.py:185][0m |          -0.0003 |         183.5756 |           8.8497 |
[32m[20221213 21:34:00 @agent_ppo2.py:185][0m |          -0.0014 |         179.1159 |           8.9551 |
[32m[20221213 21:34:00 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 21:34:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.20
[32m[20221213 21:34:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 859.00
[32m[20221213 21:34:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.00
[32m[20221213 21:34:00 @agent_ppo2.py:143][0m Total time:      38.43 min
[32m[20221213 21:34:00 @agent_ppo2.py:145][0m 3751936 total steps have happened
[32m[20221213 21:34:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1832 --------------------------#
[32m[20221213 21:34:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:01 @agent_ppo2.py:185][0m |           0.0239 |         212.5633 |           9.5188 |
[32m[20221213 21:34:01 @agent_ppo2.py:185][0m |           0.0006 |         177.8937 |           9.4528 |
[32m[20221213 21:34:01 @agent_ppo2.py:185][0m |          -0.0038 |         177.3014 |           9.4600 |
[32m[20221213 21:34:01 @agent_ppo2.py:185][0m |          -0.0057 |         177.0309 |           9.4559 |
[32m[20221213 21:34:01 @agent_ppo2.py:185][0m |           0.0077 |         200.0703 |           9.4278 |
[32m[20221213 21:34:01 @agent_ppo2.py:185][0m |          -0.0045 |         176.7443 |           9.3852 |
[32m[20221213 21:34:01 @agent_ppo2.py:185][0m |          -0.0020 |         181.0849 |           9.3860 |
[32m[20221213 21:34:01 @agent_ppo2.py:185][0m |          -0.0066 |         176.2309 |           9.3716 |
[32m[20221213 21:34:02 @agent_ppo2.py:185][0m |          -0.0070 |         176.1382 |           9.3352 |
[32m[20221213 21:34:02 @agent_ppo2.py:185][0m |          -0.0081 |         175.9945 |           9.3613 |
[32m[20221213 21:34:02 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 21:34:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.00
[32m[20221213 21:34:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:34:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 21:34:02 @agent_ppo2.py:143][0m Total time:      38.45 min
[32m[20221213 21:34:02 @agent_ppo2.py:145][0m 3753984 total steps have happened
[32m[20221213 21:34:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1833 --------------------------#
[32m[20221213 21:34:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:34:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:02 @agent_ppo2.py:185][0m |          -0.0012 |         178.0899 |           8.7411 |
[32m[20221213 21:34:02 @agent_ppo2.py:185][0m |           0.0027 |         181.0474 |           8.8280 |
[32m[20221213 21:34:02 @agent_ppo2.py:185][0m |           0.0015 |         183.4649 |           8.8198 |
[32m[20221213 21:34:02 @agent_ppo2.py:185][0m |          -0.0072 |         176.3605 |           8.9040 |
[32m[20221213 21:34:02 @agent_ppo2.py:185][0m |           0.0049 |         200.7709 |           8.8910 |
[32m[20221213 21:34:03 @agent_ppo2.py:185][0m |          -0.0083 |         176.0012 |           8.8897 |
[32m[20221213 21:34:03 @agent_ppo2.py:185][0m |          -0.0091 |         175.5523 |           8.9259 |
[32m[20221213 21:34:03 @agent_ppo2.py:185][0m |          -0.0103 |         175.5240 |           8.9247 |
[32m[20221213 21:34:03 @agent_ppo2.py:185][0m |          -0.0107 |         175.3003 |           8.9098 |
[32m[20221213 21:34:03 @agent_ppo2.py:185][0m |          -0.0062 |         178.3811 |           8.9579 |
[32m[20221213 21:34:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:34:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.80
[32m[20221213 21:34:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:34:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 862.00
[32m[20221213 21:34:03 @agent_ppo2.py:143][0m Total time:      38.47 min
[32m[20221213 21:34:03 @agent_ppo2.py:145][0m 3756032 total steps have happened
[32m[20221213 21:34:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1834 --------------------------#
[32m[20221213 21:34:03 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:34:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:03 @agent_ppo2.py:185][0m |          -0.0007 |         179.9360 |           9.2129 |
[32m[20221213 21:34:03 @agent_ppo2.py:185][0m |          -0.0045 |         177.7621 |           9.2505 |
[32m[20221213 21:34:04 @agent_ppo2.py:185][0m |           0.0008 |         181.1127 |           9.2338 |
[32m[20221213 21:34:04 @agent_ppo2.py:185][0m |          -0.0050 |         177.2924 |           9.2079 |
[32m[20221213 21:34:04 @agent_ppo2.py:185][0m |          -0.0053 |         177.0597 |           9.2201 |
[32m[20221213 21:34:04 @agent_ppo2.py:185][0m |          -0.0093 |         173.8750 |           9.2199 |
[32m[20221213 21:34:04 @agent_ppo2.py:185][0m |           0.0008 |         183.6134 |           9.2502 |
[32m[20221213 21:34:04 @agent_ppo2.py:185][0m |          -0.0085 |         172.9514 |           9.2170 |
[32m[20221213 21:34:04 @agent_ppo2.py:185][0m |          -0.0106 |         172.5730 |           9.2446 |
[32m[20221213 21:34:04 @agent_ppo2.py:185][0m |          -0.0074 |         174.3966 |           9.1980 |
[32m[20221213 21:34:04 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 21:34:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.40
[32m[20221213 21:34:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:34:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 21:34:04 @agent_ppo2.py:143][0m Total time:      38.49 min
[32m[20221213 21:34:04 @agent_ppo2.py:145][0m 3758080 total steps have happened
[32m[20221213 21:34:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1835 --------------------------#
[32m[20221213 21:34:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:05 @agent_ppo2.py:185][0m |          -0.0006 |         180.9326 |           8.8452 |
[32m[20221213 21:34:05 @agent_ppo2.py:185][0m |          -0.0040 |         179.3527 |           8.9010 |
[32m[20221213 21:34:05 @agent_ppo2.py:185][0m |          -0.0053 |         178.7599 |           8.8845 |
[32m[20221213 21:34:05 @agent_ppo2.py:185][0m |          -0.0004 |         180.5489 |           8.9077 |
[32m[20221213 21:34:05 @agent_ppo2.py:185][0m |          -0.0057 |         178.1635 |           8.9952 |
[32m[20221213 21:34:05 @agent_ppo2.py:185][0m |          -0.0075 |         178.0700 |           8.9206 |
[32m[20221213 21:34:05 @agent_ppo2.py:185][0m |          -0.0066 |         178.7804 |           8.9695 |
[32m[20221213 21:34:05 @agent_ppo2.py:185][0m |          -0.0092 |         177.4543 |           8.9836 |
[32m[20221213 21:34:05 @agent_ppo2.py:185][0m |           0.0002 |         190.7438 |           8.9868 |
[32m[20221213 21:34:05 @agent_ppo2.py:185][0m |          -0.0050 |         178.2340 |           9.1020 |
[32m[20221213 21:34:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:34:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.40
[32m[20221213 21:34:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:34:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 863.00
[32m[20221213 21:34:06 @agent_ppo2.py:143][0m Total time:      38.51 min
[32m[20221213 21:34:06 @agent_ppo2.py:145][0m 3760128 total steps have happened
[32m[20221213 21:34:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1836 --------------------------#
[32m[20221213 21:34:06 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:34:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:06 @agent_ppo2.py:185][0m |          -0.0008 |         180.7201 |           8.8858 |
[32m[20221213 21:34:06 @agent_ppo2.py:185][0m |          -0.0042 |         178.9239 |           8.8390 |
[32m[20221213 21:34:06 @agent_ppo2.py:185][0m |          -0.0059 |         177.9570 |           8.8712 |
[32m[20221213 21:34:06 @agent_ppo2.py:185][0m |          -0.0059 |         177.4445 |           8.9151 |
[32m[20221213 21:34:06 @agent_ppo2.py:185][0m |          -0.0036 |         178.2440 |           8.9046 |
[32m[20221213 21:34:06 @agent_ppo2.py:185][0m |          -0.0040 |         177.8921 |           8.9710 |
[32m[20221213 21:34:06 @agent_ppo2.py:185][0m |          -0.0083 |         176.1318 |           8.9203 |
[32m[20221213 21:34:06 @agent_ppo2.py:185][0m |          -0.0087 |         175.7850 |           8.9114 |
[32m[20221213 21:34:07 @agent_ppo2.py:185][0m |          -0.0101 |         175.5825 |           8.9346 |
[32m[20221213 21:34:07 @agent_ppo2.py:185][0m |          -0.0084 |         175.3276 |           8.9618 |
[32m[20221213 21:34:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:34:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.80
[32m[20221213 21:34:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 857.00
[32m[20221213 21:34:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:34:07 @agent_ppo2.py:143][0m Total time:      38.54 min
[32m[20221213 21:34:07 @agent_ppo2.py:145][0m 3762176 total steps have happened
[32m[20221213 21:34:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1837 --------------------------#
[32m[20221213 21:34:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:07 @agent_ppo2.py:185][0m |           0.0030 |         177.8520 |           8.6492 |
[32m[20221213 21:34:07 @agent_ppo2.py:185][0m |          -0.0023 |         176.7703 |           8.6879 |
[32m[20221213 21:34:07 @agent_ppo2.py:185][0m |          -0.0029 |         177.1220 |           8.6577 |
[32m[20221213 21:34:07 @agent_ppo2.py:185][0m |          -0.0055 |         176.0029 |           8.6730 |
[32m[20221213 21:34:07 @agent_ppo2.py:185][0m |          -0.0071 |         175.8563 |           8.6731 |
[32m[20221213 21:34:08 @agent_ppo2.py:185][0m |          -0.0069 |         175.5356 |           8.6988 |
[32m[20221213 21:34:08 @agent_ppo2.py:185][0m |          -0.0079 |         175.4929 |           8.7531 |
[32m[20221213 21:34:08 @agent_ppo2.py:185][0m |          -0.0066 |         175.1940 |           8.7509 |
[32m[20221213 21:34:08 @agent_ppo2.py:185][0m |           0.0019 |         189.5251 |           8.7424 |
[32m[20221213 21:34:08 @agent_ppo2.py:185][0m |           0.0082 |         193.7499 |           8.7646 |
[32m[20221213 21:34:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:34:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.00
[32m[20221213 21:34:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.00
[32m[20221213 21:34:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.00
[32m[20221213 21:34:08 @agent_ppo2.py:143][0m Total time:      38.56 min
[32m[20221213 21:34:08 @agent_ppo2.py:145][0m 3764224 total steps have happened
[32m[20221213 21:34:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1838 --------------------------#
[32m[20221213 21:34:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:08 @agent_ppo2.py:185][0m |           0.0081 |         192.2414 |           8.8603 |
[32m[20221213 21:34:08 @agent_ppo2.py:185][0m |          -0.0001 |         180.8083 |           8.9375 |
[32m[20221213 21:34:08 @agent_ppo2.py:185][0m |           0.0082 |         189.4288 |           8.9564 |
[32m[20221213 21:34:09 @agent_ppo2.py:185][0m |          -0.0080 |         176.3424 |           8.9171 |
[32m[20221213 21:34:09 @agent_ppo2.py:185][0m |          -0.0085 |         176.1239 |           8.9629 |
[32m[20221213 21:34:09 @agent_ppo2.py:185][0m |          -0.0092 |         175.8302 |           8.9228 |
[32m[20221213 21:34:09 @agent_ppo2.py:185][0m |          -0.0088 |         175.6001 |           8.9351 |
[32m[20221213 21:34:09 @agent_ppo2.py:185][0m |          -0.0085 |         175.4934 |           8.9540 |
[32m[20221213 21:34:09 @agent_ppo2.py:185][0m |          -0.0108 |         175.1348 |           8.9113 |
[32m[20221213 21:34:09 @agent_ppo2.py:185][0m |          -0.0119 |         175.0571 |           8.9018 |
[32m[20221213 21:34:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:34:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.60
[32m[20221213 21:34:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:34:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 862.00
[32m[20221213 21:34:09 @agent_ppo2.py:143][0m Total time:      38.58 min
[32m[20221213 21:34:09 @agent_ppo2.py:145][0m 3766272 total steps have happened
[32m[20221213 21:34:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1839 --------------------------#
[32m[20221213 21:34:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:10 @agent_ppo2.py:185][0m |          -0.0016 |         179.6001 |           9.1156 |
[32m[20221213 21:34:10 @agent_ppo2.py:185][0m |          -0.0034 |         176.6098 |           9.0992 |
[32m[20221213 21:34:10 @agent_ppo2.py:185][0m |          -0.0051 |         175.0664 |           9.0950 |
[32m[20221213 21:34:10 @agent_ppo2.py:185][0m |          -0.0037 |         175.1253 |           9.1009 |
[32m[20221213 21:34:10 @agent_ppo2.py:185][0m |          -0.0070 |         173.5435 |           9.0599 |
[32m[20221213 21:34:10 @agent_ppo2.py:185][0m |          -0.0086 |         173.0324 |           9.0737 |
[32m[20221213 21:34:10 @agent_ppo2.py:185][0m |          -0.0075 |         172.5948 |           9.0977 |
[32m[20221213 21:34:10 @agent_ppo2.py:185][0m |           0.0060 |         181.9972 |           9.0817 |
[32m[20221213 21:34:10 @agent_ppo2.py:185][0m |          -0.0106 |         171.9640 |           9.0715 |
[32m[20221213 21:34:10 @agent_ppo2.py:185][0m |          -0.0102 |         171.0844 |           9.1200 |
[32m[20221213 21:34:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:34:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.60
[32m[20221213 21:34:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:34:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.00
[32m[20221213 21:34:10 @agent_ppo2.py:143][0m Total time:      38.60 min
[32m[20221213 21:34:10 @agent_ppo2.py:145][0m 3768320 total steps have happened
[32m[20221213 21:34:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1840 --------------------------#
[32m[20221213 21:34:11 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:34:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:11 @agent_ppo2.py:185][0m |          -0.0015 |         182.6405 |           9.2445 |
[32m[20221213 21:34:11 @agent_ppo2.py:185][0m |          -0.0029 |         180.8738 |           9.2170 |
[32m[20221213 21:34:11 @agent_ppo2.py:185][0m |           0.0097 |         190.2741 |           9.2167 |
[32m[20221213 21:34:11 @agent_ppo2.py:185][0m |          -0.0046 |         179.3837 |           9.2382 |
[32m[20221213 21:34:11 @agent_ppo2.py:185][0m |          -0.0069 |         178.8513 |           9.2481 |
[32m[20221213 21:34:11 @agent_ppo2.py:185][0m |          -0.0055 |         178.9911 |           9.1817 |
[32m[20221213 21:34:11 @agent_ppo2.py:185][0m |          -0.0091 |         178.2440 |           9.2087 |
[32m[20221213 21:34:11 @agent_ppo2.py:185][0m |           0.0057 |         201.0314 |           9.2160 |
[32m[20221213 21:34:11 @agent_ppo2.py:185][0m |          -0.0090 |         178.1595 |           9.2395 |
[32m[20221213 21:34:12 @agent_ppo2.py:185][0m |           0.0003 |         186.9890 |           9.2113 |
[32m[20221213 21:34:12 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:34:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.40
[32m[20221213 21:34:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.00
[32m[20221213 21:34:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.00
[32m[20221213 21:34:12 @agent_ppo2.py:143][0m Total time:      38.62 min
[32m[20221213 21:34:12 @agent_ppo2.py:145][0m 3770368 total steps have happened
[32m[20221213 21:34:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1841 --------------------------#
[32m[20221213 21:34:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:12 @agent_ppo2.py:185][0m |          -0.0006 |         177.6921 |           9.1053 |
[32m[20221213 21:34:12 @agent_ppo2.py:185][0m |          -0.0076 |         174.8036 |           9.0883 |
[32m[20221213 21:34:12 @agent_ppo2.py:185][0m |          -0.0105 |         173.5853 |           9.0911 |
[32m[20221213 21:34:12 @agent_ppo2.py:185][0m |          -0.0123 |         172.6018 |           9.0656 |
[32m[20221213 21:34:12 @agent_ppo2.py:185][0m |          -0.0122 |         171.9930 |           9.0600 |
[32m[20221213 21:34:12 @agent_ppo2.py:185][0m |          -0.0116 |         171.4679 |           9.0646 |
[32m[20221213 21:34:12 @agent_ppo2.py:185][0m |          -0.0123 |         171.0201 |           9.0423 |
[32m[20221213 21:34:13 @agent_ppo2.py:185][0m |          -0.0131 |         170.6371 |           9.0636 |
[32m[20221213 21:34:13 @agent_ppo2.py:185][0m |          -0.0124 |         170.3121 |           9.0604 |
[32m[20221213 21:34:13 @agent_ppo2.py:185][0m |          -0.0135 |         170.1229 |           9.0818 |
[32m[20221213 21:34:13 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:34:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.40
[32m[20221213 21:34:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.00
[32m[20221213 21:34:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 779.00
[32m[20221213 21:34:13 @agent_ppo2.py:143][0m Total time:      38.64 min
[32m[20221213 21:34:13 @agent_ppo2.py:145][0m 3772416 total steps have happened
[32m[20221213 21:34:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1842 --------------------------#
[32m[20221213 21:34:13 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:34:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:13 @agent_ppo2.py:185][0m |           0.0023 |         184.4276 |           9.0453 |
[32m[20221213 21:34:13 @agent_ppo2.py:185][0m |          -0.0046 |         181.9548 |           9.0145 |
[32m[20221213 21:34:13 @agent_ppo2.py:185][0m |          -0.0046 |         180.9100 |           8.9617 |
[32m[20221213 21:34:13 @agent_ppo2.py:185][0m |          -0.0077 |         180.5590 |           8.9426 |
[32m[20221213 21:34:13 @agent_ppo2.py:185][0m |          -0.0084 |         180.1758 |           8.9560 |
[32m[20221213 21:34:14 @agent_ppo2.py:185][0m |          -0.0070 |         179.8447 |           8.9358 |
[32m[20221213 21:34:14 @agent_ppo2.py:185][0m |          -0.0057 |         179.5413 |           8.8895 |
[32m[20221213 21:34:14 @agent_ppo2.py:185][0m |          -0.0101 |         179.2423 |           8.8807 |
[32m[20221213 21:34:14 @agent_ppo2.py:185][0m |          -0.0077 |         178.9996 |           8.9354 |
[32m[20221213 21:34:14 @agent_ppo2.py:185][0m |          -0.0097 |         179.0679 |           8.8965 |
[32m[20221213 21:34:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.20
[32m[20221213 21:34:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.00
[32m[20221213 21:34:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 21:34:14 @agent_ppo2.py:143][0m Total time:      38.66 min
[32m[20221213 21:34:14 @agent_ppo2.py:145][0m 3774464 total steps have happened
[32m[20221213 21:34:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1843 --------------------------#
[32m[20221213 21:34:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:34:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:14 @agent_ppo2.py:185][0m |           0.0012 |         180.6002 |           9.0515 |
[32m[20221213 21:34:14 @agent_ppo2.py:185][0m |           0.0003 |         181.0327 |           9.1266 |
[32m[20221213 21:34:14 @agent_ppo2.py:185][0m |          -0.0061 |         179.2014 |           9.1209 |
[32m[20221213 21:34:15 @agent_ppo2.py:185][0m |          -0.0054 |         178.8895 |           9.1344 |
[32m[20221213 21:34:15 @agent_ppo2.py:185][0m |          -0.0081 |         178.4313 |           9.2141 |
[32m[20221213 21:34:15 @agent_ppo2.py:185][0m |          -0.0076 |         178.3082 |           9.1931 |
[32m[20221213 21:34:15 @agent_ppo2.py:185][0m |          -0.0032 |         182.2593 |           9.2279 |
[32m[20221213 21:34:15 @agent_ppo2.py:185][0m |          -0.0045 |         179.0976 |           9.2490 |
[32m[20221213 21:34:15 @agent_ppo2.py:185][0m |           0.0022 |         190.5997 |           9.2659 |
[32m[20221213 21:34:15 @agent_ppo2.py:185][0m |          -0.0096 |         177.5764 |           9.2623 |
[32m[20221213 21:34:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.40
[32m[20221213 21:34:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:34:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.00
[32m[20221213 21:34:15 @agent_ppo2.py:143][0m Total time:      38.68 min
[32m[20221213 21:34:15 @agent_ppo2.py:145][0m 3776512 total steps have happened
[32m[20221213 21:34:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1844 --------------------------#
[32m[20221213 21:34:15 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:34:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:16 @agent_ppo2.py:185][0m |          -0.0008 |         179.3701 |           9.2802 |
[32m[20221213 21:34:16 @agent_ppo2.py:185][0m |          -0.0051 |         178.2957 |           9.2558 |
[32m[20221213 21:34:16 @agent_ppo2.py:185][0m |          -0.0051 |         177.6539 |           9.3168 |
[32m[20221213 21:34:16 @agent_ppo2.py:185][0m |          -0.0034 |         177.5296 |           9.2816 |
[32m[20221213 21:34:16 @agent_ppo2.py:185][0m |          -0.0064 |         176.9186 |           9.3099 |
[32m[20221213 21:34:16 @agent_ppo2.py:185][0m |          -0.0080 |         176.4753 |           9.2869 |
[32m[20221213 21:34:16 @agent_ppo2.py:185][0m |          -0.0050 |         176.8962 |           9.3098 |
[32m[20221213 21:34:16 @agent_ppo2.py:185][0m |          -0.0081 |         176.1030 |           9.3055 |
[32m[20221213 21:34:16 @agent_ppo2.py:185][0m |          -0.0078 |         176.2035 |           9.2812 |
[32m[20221213 21:34:16 @agent_ppo2.py:185][0m |          -0.0080 |         175.7016 |           9.2629 |
[32m[20221213 21:34:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.80
[32m[20221213 21:34:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:34:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.00
[32m[20221213 21:34:16 @agent_ppo2.py:143][0m Total time:      38.70 min
[32m[20221213 21:34:16 @agent_ppo2.py:145][0m 3778560 total steps have happened
[32m[20221213 21:34:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1845 --------------------------#
[32m[20221213 21:34:17 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:34:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:17 @agent_ppo2.py:185][0m |           0.0010 |         181.7315 |           9.0540 |
[32m[20221213 21:34:17 @agent_ppo2.py:185][0m |          -0.0069 |         179.9937 |           9.1449 |
[32m[20221213 21:34:17 @agent_ppo2.py:185][0m |          -0.0021 |         183.3233 |           9.1146 |
[32m[20221213 21:34:17 @agent_ppo2.py:185][0m |          -0.0057 |         177.7679 |           9.0857 |
[32m[20221213 21:34:17 @agent_ppo2.py:185][0m |          -0.0099 |         177.2761 |           9.0911 |
[32m[20221213 21:34:17 @agent_ppo2.py:185][0m |          -0.0067 |         176.9928 |           9.1125 |
[32m[20221213 21:34:17 @agent_ppo2.py:185][0m |          -0.0108 |         176.2271 |           9.1187 |
[32m[20221213 21:34:17 @agent_ppo2.py:185][0m |          -0.0110 |         175.7929 |           9.0470 |
[32m[20221213 21:34:17 @agent_ppo2.py:185][0m |          -0.0102 |         175.4948 |           9.0841 |
[32m[20221213 21:34:18 @agent_ppo2.py:185][0m |          -0.0106 |         175.4654 |           9.0581 |
[32m[20221213 21:34:18 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:34:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.60
[32m[20221213 21:34:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.00
[32m[20221213 21:34:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.00
[32m[20221213 21:34:18 @agent_ppo2.py:143][0m Total time:      38.72 min
[32m[20221213 21:34:18 @agent_ppo2.py:145][0m 3780608 total steps have happened
[32m[20221213 21:34:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1846 --------------------------#
[32m[20221213 21:34:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:34:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:18 @agent_ppo2.py:185][0m |          -0.0026 |         179.6599 |           8.3822 |
[32m[20221213 21:34:18 @agent_ppo2.py:185][0m |           0.0004 |         190.2936 |           8.4328 |
[32m[20221213 21:34:18 @agent_ppo2.py:185][0m |          -0.0041 |         177.7660 |           8.4796 |
[32m[20221213 21:34:18 @agent_ppo2.py:185][0m |          -0.0104 |         177.0098 |           8.4574 |
[32m[20221213 21:34:18 @agent_ppo2.py:185][0m |          -0.0083 |         177.4244 |           8.4474 |
[32m[20221213 21:34:18 @agent_ppo2.py:185][0m |          -0.0006 |         191.4759 |           8.4411 |
[32m[20221213 21:34:18 @agent_ppo2.py:185][0m |          -0.0134 |         176.0742 |           8.4243 |
[32m[20221213 21:34:19 @agent_ppo2.py:185][0m |          -0.0051 |         197.8766 |           8.4432 |
[32m[20221213 21:34:19 @agent_ppo2.py:185][0m |          -0.0139 |         175.2875 |           8.4149 |
[32m[20221213 21:34:19 @agent_ppo2.py:185][0m |          -0.0113 |         175.3270 |           8.4010 |
[32m[20221213 21:34:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 769.40
[32m[20221213 21:34:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.00
[32m[20221213 21:34:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.00
[32m[20221213 21:34:19 @agent_ppo2.py:143][0m Total time:      38.74 min
[32m[20221213 21:34:19 @agent_ppo2.py:145][0m 3782656 total steps have happened
[32m[20221213 21:34:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1847 --------------------------#
[32m[20221213 21:34:19 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:34:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:19 @agent_ppo2.py:185][0m |           0.0000 |         177.1746 |           8.8104 |
[32m[20221213 21:34:19 @agent_ppo2.py:185][0m |          -0.0029 |         176.8405 |           8.7874 |
[32m[20221213 21:34:19 @agent_ppo2.py:185][0m |          -0.0074 |         175.3662 |           8.7659 |
[32m[20221213 21:34:19 @agent_ppo2.py:185][0m |          -0.0080 |         175.0311 |           8.7474 |
[32m[20221213 21:34:19 @agent_ppo2.py:185][0m |          -0.0058 |         177.3438 |           8.7202 |
[32m[20221213 21:34:20 @agent_ppo2.py:185][0m |          -0.0070 |         174.5072 |           8.7157 |
[32m[20221213 21:34:20 @agent_ppo2.py:185][0m |          -0.0116 |         174.2744 |           8.6698 |
[32m[20221213 21:34:20 @agent_ppo2.py:185][0m |          -0.0106 |         174.0080 |           8.6425 |
[32m[20221213 21:34:20 @agent_ppo2.py:185][0m |          -0.0117 |         173.9763 |           8.6397 |
[32m[20221213 21:34:20 @agent_ppo2.py:185][0m |          -0.0119 |         173.7699 |           8.5870 |
[32m[20221213 21:34:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:34:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.80
[32m[20221213 21:34:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:34:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.00
[32m[20221213 21:34:20 @agent_ppo2.py:143][0m Total time:      38.76 min
[32m[20221213 21:34:20 @agent_ppo2.py:145][0m 3784704 total steps have happened
[32m[20221213 21:34:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1848 --------------------------#
[32m[20221213 21:34:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:20 @agent_ppo2.py:185][0m |           0.0039 |         182.6147 |           8.8347 |
[32m[20221213 21:34:20 @agent_ppo2.py:185][0m |          -0.0026 |         178.0011 |           8.7190 |
[32m[20221213 21:34:21 @agent_ppo2.py:185][0m |          -0.0067 |         177.1626 |           8.7553 |
[32m[20221213 21:34:21 @agent_ppo2.py:185][0m |          -0.0081 |         176.7454 |           8.7015 |
[32m[20221213 21:34:21 @agent_ppo2.py:185][0m |          -0.0080 |         176.4241 |           8.6821 |
[32m[20221213 21:34:21 @agent_ppo2.py:185][0m |          -0.0061 |         179.0511 |           8.7025 |
[32m[20221213 21:34:21 @agent_ppo2.py:185][0m |          -0.0095 |         175.9922 |           8.6526 |
[32m[20221213 21:34:21 @agent_ppo2.py:185][0m |          -0.0099 |         175.8378 |           8.6747 |
[32m[20221213 21:34:21 @agent_ppo2.py:185][0m |          -0.0030 |         180.6782 |           8.6651 |
[32m[20221213 21:34:21 @agent_ppo2.py:185][0m |          -0.0113 |         175.4581 |           8.6674 |
[32m[20221213 21:34:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.80
[32m[20221213 21:34:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.00
[32m[20221213 21:34:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 879.00
[32m[20221213 21:34:21 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 879.00
[32m[20221213 21:34:21 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 879.00
[32m[20221213 21:34:21 @agent_ppo2.py:143][0m Total time:      38.78 min
[32m[20221213 21:34:21 @agent_ppo2.py:145][0m 3786752 total steps have happened
[32m[20221213 21:34:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1849 --------------------------#
[32m[20221213 21:34:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:22 @agent_ppo2.py:185][0m |           0.0015 |         175.8573 |           8.3512 |
[32m[20221213 21:34:22 @agent_ppo2.py:185][0m |          -0.0025 |         175.1606 |           8.3476 |
[32m[20221213 21:34:22 @agent_ppo2.py:185][0m |          -0.0035 |         174.7919 |           8.3715 |
[32m[20221213 21:34:22 @agent_ppo2.py:185][0m |          -0.0063 |         174.2005 |           8.3212 |
[32m[20221213 21:34:22 @agent_ppo2.py:185][0m |          -0.0011 |         176.4007 |           8.3658 |
[32m[20221213 21:34:22 @agent_ppo2.py:185][0m |          -0.0076 |         173.6716 |           8.3019 |
[32m[20221213 21:34:22 @agent_ppo2.py:185][0m |          -0.0059 |         173.8674 |           8.2820 |
[32m[20221213 21:34:22 @agent_ppo2.py:185][0m |          -0.0077 |         172.9069 |           8.3044 |
[32m[20221213 21:34:22 @agent_ppo2.py:185][0m |          -0.0093 |         172.5617 |           8.3240 |
[32m[20221213 21:34:22 @agent_ppo2.py:185][0m |          -0.0096 |         172.5124 |           8.2720 |
[32m[20221213 21:34:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.60
[32m[20221213 21:34:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:34:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:34:22 @agent_ppo2.py:143][0m Total time:      38.80 min
[32m[20221213 21:34:22 @agent_ppo2.py:145][0m 3788800 total steps have happened
[32m[20221213 21:34:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1850 --------------------------#
[32m[20221213 21:34:23 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:34:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:23 @agent_ppo2.py:185][0m |          -0.0013 |         179.5257 |           8.5097 |
[32m[20221213 21:34:23 @agent_ppo2.py:185][0m |          -0.0058 |         177.8460 |           8.5068 |
[32m[20221213 21:34:23 @agent_ppo2.py:185][0m |          -0.0044 |         177.4985 |           8.6149 |
[32m[20221213 21:34:23 @agent_ppo2.py:185][0m |          -0.0088 |         176.9824 |           8.6054 |
[32m[20221213 21:34:23 @agent_ppo2.py:185][0m |           0.0029 |         196.0912 |           8.6769 |
[32m[20221213 21:34:23 @agent_ppo2.py:185][0m |           0.0065 |         186.2431 |           8.6802 |
[32m[20221213 21:34:23 @agent_ppo2.py:185][0m |          -0.0088 |         175.8955 |           8.7254 |
[32m[20221213 21:34:23 @agent_ppo2.py:185][0m |          -0.0113 |         175.6389 |           8.7192 |
[32m[20221213 21:34:23 @agent_ppo2.py:185][0m |           0.0004 |         198.0757 |           8.7441 |
[32m[20221213 21:34:24 @agent_ppo2.py:185][0m |          -0.0093 |         175.4309 |           8.8352 |
[32m[20221213 21:34:24 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:34:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.20
[32m[20221213 21:34:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:34:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.00
[32m[20221213 21:34:24 @agent_ppo2.py:143][0m Total time:      38.82 min
[32m[20221213 21:34:24 @agent_ppo2.py:145][0m 3790848 total steps have happened
[32m[20221213 21:34:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1851 --------------------------#
[32m[20221213 21:34:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:24 @agent_ppo2.py:185][0m |           0.0070 |         185.2631 |           8.5089 |
[32m[20221213 21:34:24 @agent_ppo2.py:185][0m |          -0.0019 |         176.1635 |           8.6313 |
[32m[20221213 21:34:24 @agent_ppo2.py:185][0m |          -0.0051 |         175.5452 |           8.5020 |
[32m[20221213 21:34:24 @agent_ppo2.py:185][0m |           0.0056 |         181.6969 |           8.5177 |
[32m[20221213 21:34:24 @agent_ppo2.py:185][0m |          -0.0056 |         174.6765 |           8.4657 |
[32m[20221213 21:34:24 @agent_ppo2.py:185][0m |          -0.0004 |         182.6531 |           8.4708 |
[32m[20221213 21:34:24 @agent_ppo2.py:185][0m |          -0.0069 |         174.0906 |           8.4420 |
[32m[20221213 21:34:25 @agent_ppo2.py:185][0m |          -0.0016 |         178.0122 |           8.3997 |
[32m[20221213 21:34:25 @agent_ppo2.py:185][0m |          -0.0066 |         173.7114 |           8.4009 |
[32m[20221213 21:34:25 @agent_ppo2.py:185][0m |          -0.0080 |         173.5315 |           8.3196 |
[32m[20221213 21:34:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:34:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.60
[32m[20221213 21:34:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.00
[32m[20221213 21:34:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.00
[32m[20221213 21:34:25 @agent_ppo2.py:143][0m Total time:      38.84 min
[32m[20221213 21:34:25 @agent_ppo2.py:145][0m 3792896 total steps have happened
[32m[20221213 21:34:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1852 --------------------------#
[32m[20221213 21:34:25 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:34:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:25 @agent_ppo2.py:185][0m |           0.0028 |         174.9102 |           7.9920 |
[32m[20221213 21:34:25 @agent_ppo2.py:185][0m |          -0.0043 |         173.8162 |           8.0027 |
[32m[20221213 21:34:25 @agent_ppo2.py:185][0m |           0.0000 |         174.2344 |           8.0459 |
[32m[20221213 21:34:25 @agent_ppo2.py:185][0m |          -0.0048 |         172.8726 |           8.0858 |
[32m[20221213 21:34:25 @agent_ppo2.py:185][0m |          -0.0059 |         172.5056 |           8.0303 |
[32m[20221213 21:34:26 @agent_ppo2.py:185][0m |          -0.0080 |         172.4224 |           8.0729 |
[32m[20221213 21:34:26 @agent_ppo2.py:185][0m |          -0.0055 |         172.3944 |           8.0858 |
[32m[20221213 21:34:26 @agent_ppo2.py:185][0m |          -0.0063 |         171.9748 |           8.0975 |
[32m[20221213 21:34:26 @agent_ppo2.py:185][0m |          -0.0042 |         172.1483 |           8.0716 |
[32m[20221213 21:34:26 @agent_ppo2.py:185][0m |          -0.0075 |         171.5328 |           8.1034 |
[32m[20221213 21:34:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.00
[32m[20221213 21:34:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:34:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 21:34:26 @agent_ppo2.py:143][0m Total time:      38.86 min
[32m[20221213 21:34:26 @agent_ppo2.py:145][0m 3794944 total steps have happened
[32m[20221213 21:34:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1853 --------------------------#
[32m[20221213 21:34:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:26 @agent_ppo2.py:185][0m |          -0.0020 |         176.9625 |           8.4245 |
[32m[20221213 21:34:26 @agent_ppo2.py:185][0m |          -0.0057 |         175.6580 |           8.4695 |
[32m[20221213 21:34:27 @agent_ppo2.py:185][0m |          -0.0030 |         175.3843 |           8.4218 |
[32m[20221213 21:34:27 @agent_ppo2.py:185][0m |           0.0052 |         197.2548 |           8.4236 |
[32m[20221213 21:34:27 @agent_ppo2.py:185][0m |           0.0001 |         184.6634 |           8.4397 |
[32m[20221213 21:34:27 @agent_ppo2.py:185][0m |          -0.0098 |         174.0985 |           8.4700 |
[32m[20221213 21:34:27 @agent_ppo2.py:185][0m |           0.0040 |         193.4949 |           8.4541 |
[32m[20221213 21:34:27 @agent_ppo2.py:185][0m |          -0.0091 |         173.8844 |           8.4754 |
[32m[20221213 21:34:27 @agent_ppo2.py:185][0m |          -0.0098 |         173.3927 |           8.4755 |
[32m[20221213 21:34:27 @agent_ppo2.py:185][0m |           0.0065 |         196.8878 |           8.4569 |
[32m[20221213 21:34:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.40
[32m[20221213 21:34:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.00
[32m[20221213 21:34:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 867.00
[32m[20221213 21:34:27 @agent_ppo2.py:143][0m Total time:      38.88 min
[32m[20221213 21:34:27 @agent_ppo2.py:145][0m 3796992 total steps have happened
[32m[20221213 21:34:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1854 --------------------------#
[32m[20221213 21:34:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:28 @agent_ppo2.py:185][0m |           0.0032 |         177.3237 |           8.3572 |
[32m[20221213 21:34:28 @agent_ppo2.py:185][0m |          -0.0017 |         175.7306 |           8.3795 |
[32m[20221213 21:34:28 @agent_ppo2.py:185][0m |          -0.0032 |         175.3025 |           8.4076 |
[32m[20221213 21:34:28 @agent_ppo2.py:185][0m |          -0.0047 |         174.8374 |           8.4428 |
[32m[20221213 21:34:28 @agent_ppo2.py:185][0m |          -0.0065 |         174.6008 |           8.4860 |
[32m[20221213 21:34:28 @agent_ppo2.py:185][0m |          -0.0059 |         174.3731 |           8.4272 |
[32m[20221213 21:34:28 @agent_ppo2.py:185][0m |          -0.0082 |         174.1001 |           8.5021 |
[32m[20221213 21:34:28 @agent_ppo2.py:185][0m |          -0.0083 |         173.8762 |           8.5254 |
[32m[20221213 21:34:28 @agent_ppo2.py:185][0m |          -0.0098 |         173.8353 |           8.4917 |
[32m[20221213 21:34:28 @agent_ppo2.py:185][0m |          -0.0079 |         173.5542 |           8.4997 |
[32m[20221213 21:34:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.60
[32m[20221213 21:34:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.00
[32m[20221213 21:34:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 766.00
[32m[20221213 21:34:28 @agent_ppo2.py:143][0m Total time:      38.90 min
[32m[20221213 21:34:28 @agent_ppo2.py:145][0m 3799040 total steps have happened
[32m[20221213 21:34:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1855 --------------------------#
[32m[20221213 21:34:29 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:34:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:29 @agent_ppo2.py:185][0m |           0.0042 |         178.2227 |           8.8363 |
[32m[20221213 21:34:29 @agent_ppo2.py:185][0m |           0.0083 |         197.4616 |           8.8720 |
[32m[20221213 21:34:29 @agent_ppo2.py:185][0m |          -0.0073 |         173.9367 |           8.8368 |
[32m[20221213 21:34:29 @agent_ppo2.py:185][0m |          -0.0098 |         173.5812 |           8.8550 |
[32m[20221213 21:34:29 @agent_ppo2.py:185][0m |          -0.0085 |         173.3816 |           8.8825 |
[32m[20221213 21:34:29 @agent_ppo2.py:185][0m |          -0.0074 |         173.1907 |           8.8727 |
[32m[20221213 21:34:29 @agent_ppo2.py:185][0m |          -0.0124 |         172.9132 |           8.8749 |
[32m[20221213 21:34:29 @agent_ppo2.py:185][0m |          -0.0116 |         172.7428 |           8.8839 |
[32m[20221213 21:34:29 @agent_ppo2.py:185][0m |          -0.0047 |         178.4640 |           8.9104 |
[32m[20221213 21:34:30 @agent_ppo2.py:185][0m |          -0.0104 |         172.9768 |           8.8988 |
[32m[20221213 21:34:30 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:34:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.80
[32m[20221213 21:34:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:34:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.00
[32m[20221213 21:34:30 @agent_ppo2.py:143][0m Total time:      38.92 min
[32m[20221213 21:34:30 @agent_ppo2.py:145][0m 3801088 total steps have happened
[32m[20221213 21:34:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1856 --------------------------#
[32m[20221213 21:34:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:34:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:30 @agent_ppo2.py:185][0m |          -0.0009 |         176.9323 |           8.3457 |
[32m[20221213 21:34:30 @agent_ppo2.py:185][0m |           0.0018 |         180.4185 |           8.3500 |
[32m[20221213 21:34:30 @agent_ppo2.py:185][0m |          -0.0059 |         175.3663 |           8.3556 |
[32m[20221213 21:34:30 @agent_ppo2.py:185][0m |          -0.0055 |         175.0652 |           8.3222 |
[32m[20221213 21:34:30 @agent_ppo2.py:185][0m |          -0.0062 |         174.6243 |           8.3333 |
[32m[20221213 21:34:30 @agent_ppo2.py:185][0m |          -0.0089 |         174.7027 |           8.3109 |
[32m[20221213 21:34:30 @agent_ppo2.py:185][0m |          -0.0072 |         174.5743 |           8.3065 |
[32m[20221213 21:34:31 @agent_ppo2.py:185][0m |          -0.0076 |         174.2999 |           8.3499 |
[32m[20221213 21:34:31 @agent_ppo2.py:185][0m |          -0.0075 |         174.0344 |           8.3297 |
[32m[20221213 21:34:31 @agent_ppo2.py:185][0m |          -0.0078 |         173.7712 |           8.3097 |
[32m[20221213 21:34:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.80
[32m[20221213 21:34:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.00
[32m[20221213 21:34:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.00
[32m[20221213 21:34:31 @agent_ppo2.py:143][0m Total time:      38.94 min
[32m[20221213 21:34:31 @agent_ppo2.py:145][0m 3803136 total steps have happened
[32m[20221213 21:34:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1857 --------------------------#
[32m[20221213 21:34:31 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:34:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:31 @agent_ppo2.py:185][0m |           0.0047 |         179.4626 |           8.5989 |
[32m[20221213 21:34:31 @agent_ppo2.py:185][0m |           0.0007 |         176.0534 |           8.6201 |
[32m[20221213 21:34:31 @agent_ppo2.py:185][0m |          -0.0032 |         172.0688 |           8.6229 |
[32m[20221213 21:34:31 @agent_ppo2.py:185][0m |           0.0001 |         178.5524 |           8.6183 |
[32m[20221213 21:34:32 @agent_ppo2.py:185][0m |          -0.0063 |         170.9409 |           8.6472 |
[32m[20221213 21:34:32 @agent_ppo2.py:185][0m |          -0.0006 |         173.9198 |           8.6057 |
[32m[20221213 21:34:32 @agent_ppo2.py:185][0m |          -0.0032 |         174.2714 |           8.5869 |
[32m[20221213 21:34:32 @agent_ppo2.py:185][0m |          -0.0100 |         169.6025 |           8.5881 |
[32m[20221213 21:34:32 @agent_ppo2.py:185][0m |          -0.0000 |         186.5278 |           8.6218 |
[32m[20221213 21:34:32 @agent_ppo2.py:185][0m |          -0.0106 |         169.2907 |           8.6049 |
[32m[20221213 21:34:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.40
[32m[20221213 21:34:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.00
[32m[20221213 21:34:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.00
[32m[20221213 21:34:32 @agent_ppo2.py:143][0m Total time:      38.96 min
[32m[20221213 21:34:32 @agent_ppo2.py:145][0m 3805184 total steps have happened
[32m[20221213 21:34:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1858 --------------------------#
[32m[20221213 21:34:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:32 @agent_ppo2.py:185][0m |          -0.0009 |         178.0001 |           8.7754 |
[32m[20221213 21:34:32 @agent_ppo2.py:185][0m |          -0.0047 |         177.2399 |           8.7351 |
[32m[20221213 21:34:33 @agent_ppo2.py:185][0m |          -0.0031 |         177.8359 |           8.7749 |
[32m[20221213 21:34:33 @agent_ppo2.py:185][0m |          -0.0056 |         176.3465 |           8.7636 |
[32m[20221213 21:34:33 @agent_ppo2.py:185][0m |          -0.0073 |         176.1254 |           8.7915 |
[32m[20221213 21:34:33 @agent_ppo2.py:185][0m |          -0.0053 |         176.2509 |           8.7965 |
[32m[20221213 21:34:33 @agent_ppo2.py:185][0m |          -0.0073 |         175.6407 |           8.7872 |
[32m[20221213 21:34:33 @agent_ppo2.py:185][0m |          -0.0091 |         175.3918 |           8.7783 |
[32m[20221213 21:34:33 @agent_ppo2.py:185][0m |          -0.0089 |         175.6672 |           8.8548 |
[32m[20221213 21:34:33 @agent_ppo2.py:185][0m |          -0.0094 |         174.9853 |           8.7955 |
[32m[20221213 21:34:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.80
[32m[20221213 21:34:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:34:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.00
[32m[20221213 21:34:33 @agent_ppo2.py:143][0m Total time:      38.98 min
[32m[20221213 21:34:33 @agent_ppo2.py:145][0m 3807232 total steps have happened
[32m[20221213 21:34:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1859 --------------------------#
[32m[20221213 21:34:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:34:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:34 @agent_ppo2.py:185][0m |           0.0009 |         176.2278 |           8.6301 |
[32m[20221213 21:34:34 @agent_ppo2.py:185][0m |          -0.0002 |         175.0424 |           8.5765 |
[32m[20221213 21:34:34 @agent_ppo2.py:185][0m |          -0.0047 |         174.1514 |           8.5704 |
[32m[20221213 21:34:34 @agent_ppo2.py:185][0m |          -0.0040 |         173.6422 |           8.5644 |
[32m[20221213 21:34:34 @agent_ppo2.py:185][0m |          -0.0067 |         173.3407 |           8.5117 |
[32m[20221213 21:34:34 @agent_ppo2.py:185][0m |          -0.0034 |         173.2233 |           8.5230 |
[32m[20221213 21:34:34 @agent_ppo2.py:185][0m |          -0.0069 |         172.9205 |           8.4875 |
[32m[20221213 21:34:34 @agent_ppo2.py:185][0m |          -0.0053 |         172.8036 |           8.4612 |
[32m[20221213 21:34:34 @agent_ppo2.py:185][0m |          -0.0089 |         172.7152 |           8.4776 |
[32m[20221213 21:34:34 @agent_ppo2.py:185][0m |          -0.0072 |         172.5573 |           8.4296 |
[32m[20221213 21:34:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.00
[32m[20221213 21:34:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.00
[32m[20221213 21:34:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:34:34 @agent_ppo2.py:143][0m Total time:      39.00 min
[32m[20221213 21:34:34 @agent_ppo2.py:145][0m 3809280 total steps have happened
[32m[20221213 21:34:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1860 --------------------------#
[32m[20221213 21:34:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:35 @agent_ppo2.py:185][0m |           0.0001 |         170.8614 |           8.2995 |
[32m[20221213 21:34:35 @agent_ppo2.py:185][0m |           0.0078 |         184.3174 |           8.2603 |
[32m[20221213 21:34:35 @agent_ppo2.py:185][0m |          -0.0025 |         170.0745 |           8.2422 |
[32m[20221213 21:34:35 @agent_ppo2.py:185][0m |          -0.0044 |         169.6066 |           8.2720 |
[32m[20221213 21:34:35 @agent_ppo2.py:185][0m |          -0.0065 |         169.4822 |           8.2543 |
[32m[20221213 21:34:35 @agent_ppo2.py:185][0m |          -0.0061 |         169.3088 |           8.2772 |
[32m[20221213 21:34:35 @agent_ppo2.py:185][0m |          -0.0076 |         169.2088 |           8.2867 |
[32m[20221213 21:34:35 @agent_ppo2.py:185][0m |          -0.0085 |         169.1244 |           8.2894 |
[32m[20221213 21:34:35 @agent_ppo2.py:185][0m |          -0.0077 |         169.0166 |           8.3009 |
[32m[20221213 21:34:36 @agent_ppo2.py:185][0m |          -0.0071 |         169.2243 |           8.2918 |
[32m[20221213 21:34:36 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:34:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.00
[32m[20221213 21:34:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.00
[32m[20221213 21:34:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.00
[32m[20221213 21:34:36 @agent_ppo2.py:143][0m Total time:      39.02 min
[32m[20221213 21:34:36 @agent_ppo2.py:145][0m 3811328 total steps have happened
[32m[20221213 21:34:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1861 --------------------------#
[32m[20221213 21:34:36 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:34:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:36 @agent_ppo2.py:185][0m |          -0.0027 |         173.9501 |           8.3845 |
[32m[20221213 21:34:36 @agent_ppo2.py:185][0m |          -0.0058 |         173.0072 |           8.3590 |
[32m[20221213 21:34:36 @agent_ppo2.py:185][0m |          -0.0057 |         172.5737 |           8.3596 |
[32m[20221213 21:34:36 @agent_ppo2.py:185][0m |          -0.0082 |         172.3028 |           8.4002 |
[32m[20221213 21:34:36 @agent_ppo2.py:185][0m |          -0.0076 |         171.9236 |           8.4678 |
[32m[20221213 21:34:36 @agent_ppo2.py:185][0m |          -0.0071 |         171.8161 |           8.4680 |
[32m[20221213 21:34:36 @agent_ppo2.py:185][0m |           0.0057 |         196.7815 |           8.5156 |
[32m[20221213 21:34:37 @agent_ppo2.py:185][0m |          -0.0034 |         173.7567 |           8.5627 |
[32m[20221213 21:34:37 @agent_ppo2.py:185][0m |          -0.0080 |         171.4234 |           8.5727 |
[32m[20221213 21:34:37 @agent_ppo2.py:185][0m |          -0.0077 |         171.4612 |           8.5905 |
[32m[20221213 21:34:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:34:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.40
[32m[20221213 21:34:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:34:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 21:34:37 @agent_ppo2.py:143][0m Total time:      39.04 min
[32m[20221213 21:34:37 @agent_ppo2.py:145][0m 3813376 total steps have happened
[32m[20221213 21:34:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1862 --------------------------#
[32m[20221213 21:34:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:37 @agent_ppo2.py:185][0m |          -0.0020 |         175.9903 |           8.8330 |
[32m[20221213 21:34:37 @agent_ppo2.py:185][0m |          -0.0063 |         175.5001 |           8.8478 |
[32m[20221213 21:34:37 @agent_ppo2.py:185][0m |          -0.0070 |         175.0682 |           8.8045 |
[32m[20221213 21:34:37 @agent_ppo2.py:185][0m |          -0.0074 |         174.6944 |           8.8963 |
[32m[20221213 21:34:38 @agent_ppo2.py:185][0m |          -0.0075 |         174.8193 |           8.8847 |
[32m[20221213 21:34:38 @agent_ppo2.py:185][0m |          -0.0100 |         174.4219 |           8.9072 |
[32m[20221213 21:34:38 @agent_ppo2.py:185][0m |          -0.0097 |         174.1945 |           8.9057 |
[32m[20221213 21:34:38 @agent_ppo2.py:185][0m |          -0.0089 |         173.9073 |           8.9116 |
[32m[20221213 21:34:38 @agent_ppo2.py:185][0m |          -0.0100 |         173.9507 |           8.9396 |
[32m[20221213 21:34:38 @agent_ppo2.py:185][0m |          -0.0102 |         173.7501 |           8.9476 |
[32m[20221213 21:34:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.60
[32m[20221213 21:34:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.00
[32m[20221213 21:34:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.00
[32m[20221213 21:34:38 @agent_ppo2.py:143][0m Total time:      39.06 min
[32m[20221213 21:34:38 @agent_ppo2.py:145][0m 3815424 total steps have happened
[32m[20221213 21:34:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1863 --------------------------#
[32m[20221213 21:34:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:38 @agent_ppo2.py:185][0m |          -0.0010 |         174.6342 |           8.7110 |
[32m[20221213 21:34:38 @agent_ppo2.py:185][0m |          -0.0049 |         174.3738 |           8.7286 |
[32m[20221213 21:34:39 @agent_ppo2.py:185][0m |          -0.0050 |         174.1191 |           8.7015 |
[32m[20221213 21:34:39 @agent_ppo2.py:185][0m |           0.0026 |         178.2181 |           8.7343 |
[32m[20221213 21:34:39 @agent_ppo2.py:185][0m |          -0.0044 |         173.7654 |           8.7099 |
[32m[20221213 21:34:39 @agent_ppo2.py:185][0m |          -0.0077 |         173.7554 |           8.6905 |
[32m[20221213 21:34:39 @agent_ppo2.py:185][0m |          -0.0086 |         173.5241 |           8.6422 |
[32m[20221213 21:34:39 @agent_ppo2.py:185][0m |          -0.0070 |         173.4831 |           8.6225 |
[32m[20221213 21:34:39 @agent_ppo2.py:185][0m |          -0.0061 |         174.4948 |           8.6089 |
[32m[20221213 21:34:39 @agent_ppo2.py:185][0m |          -0.0086 |         173.3500 |           8.6245 |
[32m[20221213 21:34:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:34:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.60
[32m[20221213 21:34:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.00
[32m[20221213 21:34:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 21:34:39 @agent_ppo2.py:143][0m Total time:      39.08 min
[32m[20221213 21:34:39 @agent_ppo2.py:145][0m 3817472 total steps have happened
[32m[20221213 21:34:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1864 --------------------------#
[32m[20221213 21:34:39 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:34:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:40 @agent_ppo2.py:185][0m |          -0.0023 |         174.8455 |           8.7707 |
[32m[20221213 21:34:40 @agent_ppo2.py:185][0m |           0.0032 |         176.6509 |           8.7678 |
[32m[20221213 21:34:40 @agent_ppo2.py:185][0m |          -0.0060 |         174.0641 |           8.8923 |
[32m[20221213 21:34:40 @agent_ppo2.py:185][0m |          -0.0077 |         173.5795 |           8.8246 |
[32m[20221213 21:34:40 @agent_ppo2.py:185][0m |          -0.0022 |         179.0532 |           8.8231 |
[32m[20221213 21:34:40 @agent_ppo2.py:185][0m |          -0.0078 |         173.3244 |           8.8881 |
[32m[20221213 21:34:40 @agent_ppo2.py:185][0m |          -0.0088 |         173.2072 |           8.8784 |
[32m[20221213 21:34:40 @agent_ppo2.py:185][0m |          -0.0053 |         173.8224 |           8.9070 |
[32m[20221213 21:34:40 @agent_ppo2.py:185][0m |          -0.0089 |         173.0490 |           8.9456 |
[32m[20221213 21:34:40 @agent_ppo2.py:185][0m |          -0.0095 |         172.9967 |           8.9309 |
[32m[20221213 21:34:40 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:34:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.20
[32m[20221213 21:34:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:34:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.00
[32m[20221213 21:34:41 @agent_ppo2.py:143][0m Total time:      39.10 min
[32m[20221213 21:34:41 @agent_ppo2.py:145][0m 3819520 total steps have happened
[32m[20221213 21:34:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1865 --------------------------#
[32m[20221213 21:34:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:41 @agent_ppo2.py:185][0m |          -0.0007 |         173.6789 |           8.9277 |
[32m[20221213 21:34:41 @agent_ppo2.py:185][0m |          -0.0069 |         172.8227 |           8.9506 |
[32m[20221213 21:34:41 @agent_ppo2.py:185][0m |          -0.0094 |         172.4077 |           8.9454 |
[32m[20221213 21:34:41 @agent_ppo2.py:185][0m |           0.0129 |         199.0781 |           8.9198 |
[32m[20221213 21:34:41 @agent_ppo2.py:185][0m |          -0.0086 |         171.3171 |           8.9822 |
[32m[20221213 21:34:41 @agent_ppo2.py:185][0m |           0.0050 |         197.3941 |           8.9190 |
[32m[20221213 21:34:41 @agent_ppo2.py:185][0m |          -0.0094 |         170.1622 |           8.8485 |
[32m[20221213 21:34:42 @agent_ppo2.py:185][0m |          -0.0100 |         169.8294 |           8.8715 |
[32m[20221213 21:34:42 @agent_ppo2.py:185][0m |          -0.0090 |         169.8245 |           8.8586 |
[32m[20221213 21:34:42 @agent_ppo2.py:185][0m |          -0.0102 |         169.4534 |           8.8174 |
[32m[20221213 21:34:42 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 21:34:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.80
[32m[20221213 21:34:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:34:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 761.00
[32m[20221213 21:34:42 @agent_ppo2.py:143][0m Total time:      39.12 min
[32m[20221213 21:34:42 @agent_ppo2.py:145][0m 3821568 total steps have happened
[32m[20221213 21:34:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1866 --------------------------#
[32m[20221213 21:34:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:42 @agent_ppo2.py:185][0m |          -0.0002 |         178.9340 |           8.7004 |
[32m[20221213 21:34:42 @agent_ppo2.py:185][0m |          -0.0056 |         177.6729 |           8.7572 |
[32m[20221213 21:34:42 @agent_ppo2.py:185][0m |          -0.0028 |         179.5505 |           8.7428 |
[32m[20221213 21:34:43 @agent_ppo2.py:185][0m |          -0.0083 |         176.7127 |           8.7108 |
[32m[20221213 21:34:43 @agent_ppo2.py:185][0m |          -0.0087 |         176.1551 |           8.7557 |
[32m[20221213 21:34:43 @agent_ppo2.py:185][0m |          -0.0113 |         175.7782 |           8.7445 |
[32m[20221213 21:34:43 @agent_ppo2.py:185][0m |          -0.0089 |         175.2977 |           8.7304 |
[32m[20221213 21:34:43 @agent_ppo2.py:185][0m |          -0.0120 |         175.1059 |           8.7533 |
[32m[20221213 21:34:43 @agent_ppo2.py:185][0m |          -0.0110 |         174.8489 |           8.7574 |
[32m[20221213 21:34:43 @agent_ppo2.py:185][0m |          -0.0121 |         174.5679 |           8.7313 |
[32m[20221213 21:34:43 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:34:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.20
[32m[20221213 21:34:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:34:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.00
[32m[20221213 21:34:43 @agent_ppo2.py:143][0m Total time:      39.14 min
[32m[20221213 21:34:43 @agent_ppo2.py:145][0m 3823616 total steps have happened
[32m[20221213 21:34:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1867 --------------------------#
[32m[20221213 21:34:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:43 @agent_ppo2.py:185][0m |           0.0111 |         191.8480 |           8.6147 |
[32m[20221213 21:34:44 @agent_ppo2.py:185][0m |          -0.0053 |         176.5319 |           8.6100 |
[32m[20221213 21:34:44 @agent_ppo2.py:185][0m |          -0.0077 |         175.4063 |           8.5832 |
[32m[20221213 21:34:44 @agent_ppo2.py:185][0m |          -0.0084 |         174.4054 |           8.5856 |
[32m[20221213 21:34:44 @agent_ppo2.py:185][0m |          -0.0068 |         174.9130 |           8.5886 |
[32m[20221213 21:34:44 @agent_ppo2.py:185][0m |          -0.0041 |         175.8929 |           8.5931 |
[32m[20221213 21:34:44 @agent_ppo2.py:185][0m |          -0.0105 |         173.1307 |           8.5631 |
[32m[20221213 21:34:44 @agent_ppo2.py:185][0m |          -0.0117 |         173.0024 |           8.5537 |
[32m[20221213 21:34:44 @agent_ppo2.py:185][0m |          -0.0099 |         172.7005 |           8.5395 |
[32m[20221213 21:34:44 @agent_ppo2.py:185][0m |          -0.0095 |         172.6833 |           8.5340 |
[32m[20221213 21:34:44 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:34:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.00
[32m[20221213 21:34:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:34:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.00
[32m[20221213 21:34:44 @agent_ppo2.py:143][0m Total time:      39.16 min
[32m[20221213 21:34:44 @agent_ppo2.py:145][0m 3825664 total steps have happened
[32m[20221213 21:34:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1868 --------------------------#
[32m[20221213 21:34:45 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:34:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:45 @agent_ppo2.py:185][0m |           0.0003 |         181.2798 |           8.8224 |
[32m[20221213 21:34:45 @agent_ppo2.py:185][0m |          -0.0038 |         178.8715 |           8.8198 |
[32m[20221213 21:34:45 @agent_ppo2.py:185][0m |          -0.0055 |         178.0342 |           8.8427 |
[32m[20221213 21:34:45 @agent_ppo2.py:185][0m |          -0.0083 |         177.3934 |           8.8526 |
[32m[20221213 21:34:45 @agent_ppo2.py:185][0m |          -0.0084 |         177.0112 |           8.8183 |
[32m[20221213 21:34:45 @agent_ppo2.py:185][0m |           0.0026 |         194.6526 |           8.8351 |
[32m[20221213 21:34:45 @agent_ppo2.py:185][0m |          -0.0048 |         178.4002 |           8.8737 |
[32m[20221213 21:34:45 @agent_ppo2.py:185][0m |          -0.0103 |         176.2689 |           8.8735 |
[32m[20221213 21:34:45 @agent_ppo2.py:185][0m |          -0.0115 |         176.0729 |           8.8424 |
[32m[20221213 21:34:46 @agent_ppo2.py:185][0m |          -0.0118 |         175.9581 |           8.8370 |
[32m[20221213 21:34:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:34:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.00
[32m[20221213 21:34:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:34:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 861.00
[32m[20221213 21:34:46 @agent_ppo2.py:143][0m Total time:      39.18 min
[32m[20221213 21:34:46 @agent_ppo2.py:145][0m 3827712 total steps have happened
[32m[20221213 21:34:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1869 --------------------------#
[32m[20221213 21:34:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:34:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:46 @agent_ppo2.py:185][0m |           0.0140 |         187.1611 |           8.2654 |
[32m[20221213 21:34:46 @agent_ppo2.py:185][0m |          -0.0049 |         172.9906 |           8.2898 |
[32m[20221213 21:34:46 @agent_ppo2.py:185][0m |          -0.0071 |         172.5674 |           8.2414 |
[32m[20221213 21:34:46 @agent_ppo2.py:185][0m |          -0.0081 |         172.1637 |           8.3026 |
[32m[20221213 21:34:46 @agent_ppo2.py:185][0m |          -0.0062 |         171.7818 |           8.2756 |
[32m[20221213 21:34:46 @agent_ppo2.py:185][0m |          -0.0073 |         171.7937 |           8.2524 |
[32m[20221213 21:34:46 @agent_ppo2.py:185][0m |          -0.0103 |         171.2558 |           8.2779 |
[32m[20221213 21:34:47 @agent_ppo2.py:185][0m |          -0.0064 |         171.0880 |           8.2690 |
[32m[20221213 21:34:47 @agent_ppo2.py:185][0m |          -0.0042 |         173.9118 |           8.2666 |
[32m[20221213 21:34:47 @agent_ppo2.py:185][0m |           0.0011 |         181.5033 |           8.3287 |
[32m[20221213 21:34:47 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:34:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.60
[32m[20221213 21:34:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.00
[32m[20221213 21:34:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 781.00
[32m[20221213 21:34:47 @agent_ppo2.py:143][0m Total time:      39.20 min
[32m[20221213 21:34:47 @agent_ppo2.py:145][0m 3829760 total steps have happened
[32m[20221213 21:34:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1870 --------------------------#
[32m[20221213 21:34:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:47 @agent_ppo2.py:185][0m |          -0.0018 |         178.1293 |           8.5885 |
[32m[20221213 21:34:47 @agent_ppo2.py:185][0m |          -0.0062 |         175.8889 |           8.6674 |
[32m[20221213 21:34:47 @agent_ppo2.py:185][0m |          -0.0058 |         174.1894 |           8.7149 |
[32m[20221213 21:34:47 @agent_ppo2.py:185][0m |          -0.0077 |         173.4128 |           8.6802 |
[32m[20221213 21:34:48 @agent_ppo2.py:185][0m |          -0.0073 |         173.1459 |           8.7265 |
[32m[20221213 21:34:48 @agent_ppo2.py:185][0m |          -0.0103 |         172.8006 |           8.6726 |
[32m[20221213 21:34:48 @agent_ppo2.py:185][0m |          -0.0078 |         172.6084 |           8.7159 |
[32m[20221213 21:34:48 @agent_ppo2.py:185][0m |          -0.0066 |         172.3038 |           8.7432 |
[32m[20221213 21:34:48 @agent_ppo2.py:185][0m |           0.0012 |         190.6719 |           8.6759 |
[32m[20221213 21:34:48 @agent_ppo2.py:185][0m |          -0.0088 |         172.4190 |           8.6868 |
[32m[20221213 21:34:48 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:34:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.00
[32m[20221213 21:34:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.00
[32m[20221213 21:34:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.00
[32m[20221213 21:34:48 @agent_ppo2.py:143][0m Total time:      39.22 min
[32m[20221213 21:34:48 @agent_ppo2.py:145][0m 3831808 total steps have happened
[32m[20221213 21:34:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1871 --------------------------#
[32m[20221213 21:34:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:48 @agent_ppo2.py:185][0m |           0.0003 |         175.2294 |           8.6560 |
[32m[20221213 21:34:49 @agent_ppo2.py:185][0m |          -0.0044 |         174.5284 |           8.6677 |
[32m[20221213 21:34:49 @agent_ppo2.py:185][0m |           0.0016 |         178.1038 |           8.7342 |
[32m[20221213 21:34:49 @agent_ppo2.py:185][0m |          -0.0022 |         175.9184 |           8.7364 |
[32m[20221213 21:34:49 @agent_ppo2.py:185][0m |          -0.0066 |         173.1923 |           8.7311 |
[32m[20221213 21:34:49 @agent_ppo2.py:185][0m |          -0.0083 |         172.9829 |           8.7160 |
[32m[20221213 21:34:49 @agent_ppo2.py:185][0m |          -0.0051 |         173.6037 |           8.7148 |
[32m[20221213 21:34:49 @agent_ppo2.py:185][0m |          -0.0068 |         173.0006 |           8.7534 |
[32m[20221213 21:34:49 @agent_ppo2.py:185][0m |          -0.0062 |         174.4649 |           8.7142 |
[32m[20221213 21:34:49 @agent_ppo2.py:185][0m |          -0.0101 |         172.2974 |           8.7527 |
[32m[20221213 21:34:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:34:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.80
[32m[20221213 21:34:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:34:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:34:49 @agent_ppo2.py:143][0m Total time:      39.24 min
[32m[20221213 21:34:49 @agent_ppo2.py:145][0m 3833856 total steps have happened
[32m[20221213 21:34:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1872 --------------------------#
[32m[20221213 21:34:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:34:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:50 @agent_ppo2.py:185][0m |           0.0044 |         180.4988 |           8.3413 |
[32m[20221213 21:34:50 @agent_ppo2.py:185][0m |          -0.0013 |         176.1313 |           8.3101 |
[32m[20221213 21:34:50 @agent_ppo2.py:185][0m |          -0.0025 |         175.3013 |           8.3470 |
[32m[20221213 21:34:50 @agent_ppo2.py:185][0m |          -0.0036 |         174.8066 |           8.2976 |
[32m[20221213 21:34:50 @agent_ppo2.py:185][0m |          -0.0050 |         174.5439 |           8.2875 |
[32m[20221213 21:34:50 @agent_ppo2.py:185][0m |          -0.0057 |         174.1272 |           8.2554 |
[32m[20221213 21:34:50 @agent_ppo2.py:185][0m |          -0.0024 |         173.8837 |           8.2619 |
[32m[20221213 21:34:50 @agent_ppo2.py:185][0m |           0.0057 |         187.3397 |           8.2572 |
[32m[20221213 21:34:50 @agent_ppo2.py:185][0m |          -0.0068 |         173.7992 |           8.2135 |
[32m[20221213 21:34:50 @agent_ppo2.py:185][0m |           0.0098 |         192.6741 |           8.2308 |
[32m[20221213 21:34:50 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:34:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.40
[32m[20221213 21:34:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.00
[32m[20221213 21:34:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:34:51 @agent_ppo2.py:143][0m Total time:      39.26 min
[32m[20221213 21:34:51 @agent_ppo2.py:145][0m 3835904 total steps have happened
[32m[20221213 21:34:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1873 --------------------------#
[32m[20221213 21:34:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:51 @agent_ppo2.py:185][0m |          -0.0008 |         172.9105 |           8.2385 |
[32m[20221213 21:34:51 @agent_ppo2.py:185][0m |          -0.0057 |         172.0673 |           8.2163 |
[32m[20221213 21:34:51 @agent_ppo2.py:185][0m |           0.0035 |         176.6520 |           8.3003 |
[32m[20221213 21:34:51 @agent_ppo2.py:185][0m |          -0.0070 |         171.4816 |           8.2662 |
[32m[20221213 21:34:51 @agent_ppo2.py:185][0m |          -0.0076 |         171.1263 |           8.3521 |
[32m[20221213 21:34:51 @agent_ppo2.py:185][0m |           0.0048 |         189.0447 |           8.3817 |
[32m[20221213 21:34:51 @agent_ppo2.py:185][0m |          -0.0093 |         170.7991 |           8.4206 |
[32m[20221213 21:34:52 @agent_ppo2.py:185][0m |          -0.0097 |         170.6903 |           8.4716 |
[32m[20221213 21:34:52 @agent_ppo2.py:185][0m |          -0.0078 |         170.5766 |           8.4579 |
[32m[20221213 21:34:52 @agent_ppo2.py:185][0m |          -0.0106 |         170.4357 |           8.4991 |
[32m[20221213 21:34:52 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:34:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.20
[32m[20221213 21:34:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:34:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:34:52 @agent_ppo2.py:143][0m Total time:      39.29 min
[32m[20221213 21:34:52 @agent_ppo2.py:145][0m 3837952 total steps have happened
[32m[20221213 21:34:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1874 --------------------------#
[32m[20221213 21:34:52 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:34:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:52 @agent_ppo2.py:185][0m |          -0.0022 |         173.9140 |           8.8066 |
[32m[20221213 21:34:52 @agent_ppo2.py:185][0m |          -0.0057 |         172.6559 |           8.8048 |
[32m[20221213 21:34:52 @agent_ppo2.py:185][0m |          -0.0079 |         172.0016 |           8.7755 |
[32m[20221213 21:34:52 @agent_ppo2.py:185][0m |           0.0014 |         180.4691 |           8.8331 |
[32m[20221213 21:34:52 @agent_ppo2.py:185][0m |          -0.0092 |         171.2999 |           8.7940 |
[32m[20221213 21:34:53 @agent_ppo2.py:185][0m |          -0.0117 |         170.8451 |           8.8132 |
[32m[20221213 21:34:53 @agent_ppo2.py:185][0m |          -0.0113 |         170.5768 |           8.7795 |
[32m[20221213 21:34:53 @agent_ppo2.py:185][0m |          -0.0129 |         170.2560 |           8.7740 |
[32m[20221213 21:34:53 @agent_ppo2.py:185][0m |          -0.0009 |         177.2469 |           8.8031 |
[32m[20221213 21:34:53 @agent_ppo2.py:185][0m |          -0.0110 |         169.9281 |           8.7789 |
[32m[20221213 21:34:53 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:34:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.00
[32m[20221213 21:34:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:34:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 785.00
[32m[20221213 21:34:53 @agent_ppo2.py:143][0m Total time:      39.31 min
[32m[20221213 21:34:53 @agent_ppo2.py:145][0m 3840000 total steps have happened
[32m[20221213 21:34:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1875 --------------------------#
[32m[20221213 21:34:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:53 @agent_ppo2.py:185][0m |          -0.0016 |         175.7965 |           8.9123 |
[32m[20221213 21:34:53 @agent_ppo2.py:185][0m |          -0.0021 |         174.7689 |           8.8690 |
[32m[20221213 21:34:53 @agent_ppo2.py:185][0m |          -0.0039 |         174.5400 |           8.8524 |
[32m[20221213 21:34:54 @agent_ppo2.py:185][0m |          -0.0062 |         174.1287 |           8.8402 |
[32m[20221213 21:34:54 @agent_ppo2.py:185][0m |          -0.0049 |         173.9183 |           8.8320 |
[32m[20221213 21:34:54 @agent_ppo2.py:185][0m |          -0.0056 |         173.7826 |           8.7867 |
[32m[20221213 21:34:54 @agent_ppo2.py:185][0m |          -0.0051 |         173.5613 |           8.8203 |
[32m[20221213 21:34:54 @agent_ppo2.py:185][0m |          -0.0074 |         173.2802 |           8.7574 |
[32m[20221213 21:34:54 @agent_ppo2.py:185][0m |           0.0052 |         195.0225 |           8.7841 |
[32m[20221213 21:34:54 @agent_ppo2.py:185][0m |          -0.0096 |         173.1888 |           8.7663 |
[32m[20221213 21:34:54 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:34:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 754.60
[32m[20221213 21:34:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 21:34:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.00
[32m[20221213 21:34:54 @agent_ppo2.py:143][0m Total time:      39.33 min
[32m[20221213 21:34:54 @agent_ppo2.py:145][0m 3842048 total steps have happened
[32m[20221213 21:34:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1876 --------------------------#
[32m[20221213 21:34:54 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:55 @agent_ppo2.py:185][0m |          -0.0010 |         174.1419 |           8.6443 |
[32m[20221213 21:34:55 @agent_ppo2.py:185][0m |          -0.0050 |         173.0387 |           8.6538 |
[32m[20221213 21:34:55 @agent_ppo2.py:185][0m |          -0.0030 |         174.4015 |           8.6818 |
[32m[20221213 21:34:55 @agent_ppo2.py:185][0m |           0.0007 |         183.2994 |           8.6552 |
[32m[20221213 21:34:55 @agent_ppo2.py:185][0m |          -0.0059 |         171.7294 |           8.7353 |
[32m[20221213 21:34:55 @agent_ppo2.py:185][0m |          -0.0098 |         171.2915 |           8.7113 |
[32m[20221213 21:34:55 @agent_ppo2.py:185][0m |          -0.0074 |         172.9986 |           8.6542 |
[32m[20221213 21:34:55 @agent_ppo2.py:185][0m |          -0.0052 |         176.0633 |           8.6987 |
[32m[20221213 21:34:55 @agent_ppo2.py:185][0m |          -0.0100 |         171.0367 |           8.6708 |
[32m[20221213 21:34:55 @agent_ppo2.py:185][0m |          -0.0097 |         171.2141 |           8.6783 |
[32m[20221213 21:34:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:34:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.40
[32m[20221213 21:34:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:34:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.00
[32m[20221213 21:34:55 @agent_ppo2.py:143][0m Total time:      39.35 min
[32m[20221213 21:34:55 @agent_ppo2.py:145][0m 3844096 total steps have happened
[32m[20221213 21:34:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1877 --------------------------#
[32m[20221213 21:34:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:56 @agent_ppo2.py:185][0m |          -0.0026 |         175.4353 |           8.1499 |
[32m[20221213 21:34:56 @agent_ppo2.py:185][0m |          -0.0064 |         174.5693 |           8.2005 |
[32m[20221213 21:34:56 @agent_ppo2.py:185][0m |          -0.0057 |         173.8775 |           8.2167 |
[32m[20221213 21:34:56 @agent_ppo2.py:185][0m |          -0.0070 |         173.2646 |           8.2065 |
[32m[20221213 21:34:56 @agent_ppo2.py:185][0m |          -0.0097 |         172.8152 |           8.2470 |
[32m[20221213 21:34:56 @agent_ppo2.py:185][0m |          -0.0094 |         172.6259 |           8.2037 |
[32m[20221213 21:34:56 @agent_ppo2.py:185][0m |          -0.0079 |         172.2704 |           8.2163 |
[32m[20221213 21:34:56 @agent_ppo2.py:185][0m |          -0.0099 |         171.8946 |           8.2190 |
[32m[20221213 21:34:56 @agent_ppo2.py:185][0m |          -0.0104 |         171.6718 |           8.2641 |
[32m[20221213 21:34:57 @agent_ppo2.py:185][0m |          -0.0080 |         171.4590 |           8.2108 |
[32m[20221213 21:34:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:34:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.20
[32m[20221213 21:34:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.00
[32m[20221213 21:34:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 863.00
[32m[20221213 21:34:57 @agent_ppo2.py:143][0m Total time:      39.37 min
[32m[20221213 21:34:57 @agent_ppo2.py:145][0m 3846144 total steps have happened
[32m[20221213 21:34:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1878 --------------------------#
[32m[20221213 21:34:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:57 @agent_ppo2.py:185][0m |          -0.0002 |         173.7515 |           8.7342 |
[32m[20221213 21:34:57 @agent_ppo2.py:185][0m |          -0.0049 |         172.6284 |           8.7650 |
[32m[20221213 21:34:57 @agent_ppo2.py:185][0m |          -0.0015 |         171.4172 |           8.8140 |
[32m[20221213 21:34:57 @agent_ppo2.py:185][0m |          -0.0009 |         177.8579 |           8.8675 |
[32m[20221213 21:34:57 @agent_ppo2.py:185][0m |          -0.0052 |         166.7178 |           8.8980 |
[32m[20221213 21:34:57 @agent_ppo2.py:185][0m |          -0.0077 |         163.5432 |           8.9319 |
[32m[20221213 21:34:57 @agent_ppo2.py:185][0m |           0.0036 |         171.9271 |           8.9078 |
[32m[20221213 21:34:58 @agent_ppo2.py:185][0m |          -0.0052 |         163.0449 |           8.9517 |
[32m[20221213 21:34:58 @agent_ppo2.py:185][0m |          -0.0091 |         159.3520 |           8.9622 |
[32m[20221213 21:34:58 @agent_ppo2.py:185][0m |          -0.0111 |         158.8619 |           9.0085 |
[32m[20221213 21:34:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:34:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.40
[32m[20221213 21:34:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:34:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.00
[32m[20221213 21:34:58 @agent_ppo2.py:143][0m Total time:      39.39 min
[32m[20221213 21:34:58 @agent_ppo2.py:145][0m 3848192 total steps have happened
[32m[20221213 21:34:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1879 --------------------------#
[32m[20221213 21:34:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:58 @agent_ppo2.py:185][0m |           0.0034 |         178.4012 |           8.5713 |
[32m[20221213 21:34:58 @agent_ppo2.py:185][0m |          -0.0022 |         174.9291 |           8.5572 |
[32m[20221213 21:34:58 @agent_ppo2.py:185][0m |           0.0007 |         177.0721 |           8.5343 |
[32m[20221213 21:34:58 @agent_ppo2.py:185][0m |          -0.0080 |         174.7082 |           8.5027 |
[32m[20221213 21:34:59 @agent_ppo2.py:185][0m |          -0.0024 |         174.6734 |           8.4234 |
[32m[20221213 21:34:59 @agent_ppo2.py:185][0m |           0.0067 |         195.7099 |           8.3975 |
[32m[20221213 21:34:59 @agent_ppo2.py:185][0m |          -0.0059 |         173.8255 |           8.4047 |
[32m[20221213 21:34:59 @agent_ppo2.py:185][0m |          -0.0064 |         173.5536 |           8.3718 |
[32m[20221213 21:34:59 @agent_ppo2.py:185][0m |          -0.0061 |         173.4882 |           8.3465 |
[32m[20221213 21:34:59 @agent_ppo2.py:185][0m |          -0.0083 |         173.3240 |           8.3108 |
[32m[20221213 21:34:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:34:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.20
[32m[20221213 21:34:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.00
[32m[20221213 21:34:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.00
[32m[20221213 21:34:59 @agent_ppo2.py:143][0m Total time:      39.41 min
[32m[20221213 21:34:59 @agent_ppo2.py:145][0m 3850240 total steps have happened
[32m[20221213 21:34:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1880 --------------------------#
[32m[20221213 21:34:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:34:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:34:59 @agent_ppo2.py:185][0m |           0.0023 |         179.7241 |           8.3090 |
[32m[20221213 21:34:59 @agent_ppo2.py:185][0m |           0.0079 |         189.1689 |           8.2443 |
[32m[20221213 21:35:00 @agent_ppo2.py:185][0m |          -0.0092 |         175.9289 |           8.2595 |
[32m[20221213 21:35:00 @agent_ppo2.py:185][0m |          -0.0064 |         175.1018 |           8.2712 |
[32m[20221213 21:35:00 @agent_ppo2.py:185][0m |          -0.0104 |         174.8109 |           8.2521 |
[32m[20221213 21:35:00 @agent_ppo2.py:185][0m |          -0.0085 |         174.1071 |           8.2217 |
[32m[20221213 21:35:00 @agent_ppo2.py:185][0m |          -0.0096 |         173.7679 |           8.2189 |
[32m[20221213 21:35:00 @agent_ppo2.py:185][0m |          -0.0040 |         174.8709 |           8.1857 |
[32m[20221213 21:35:00 @agent_ppo2.py:185][0m |          -0.0021 |         185.2778 |           8.2065 |
[32m[20221213 21:35:00 @agent_ppo2.py:185][0m |          -0.0083 |         172.8482 |           8.1874 |
[32m[20221213 21:35:00 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:35:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.60
[32m[20221213 21:35:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.00
[32m[20221213 21:35:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.00
[32m[20221213 21:35:00 @agent_ppo2.py:143][0m Total time:      39.43 min
[32m[20221213 21:35:00 @agent_ppo2.py:145][0m 3852288 total steps have happened
[32m[20221213 21:35:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1881 --------------------------#
[32m[20221213 21:35:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:01 @agent_ppo2.py:185][0m |          -0.0013 |         174.8677 |           8.0971 |
[32m[20221213 21:35:01 @agent_ppo2.py:185][0m |          -0.0015 |         175.1944 |           8.0985 |
[32m[20221213 21:35:01 @agent_ppo2.py:185][0m |          -0.0027 |         173.4688 |           8.1560 |
[32m[20221213 21:35:01 @agent_ppo2.py:185][0m |          -0.0070 |         172.5330 |           8.1418 |
[32m[20221213 21:35:01 @agent_ppo2.py:185][0m |          -0.0071 |         171.9521 |           8.1767 |
[32m[20221213 21:35:01 @agent_ppo2.py:185][0m |          -0.0080 |         171.6649 |           8.1919 |
[32m[20221213 21:35:01 @agent_ppo2.py:185][0m |          -0.0009 |         174.7994 |           8.1801 |
[32m[20221213 21:35:01 @agent_ppo2.py:185][0m |          -0.0089 |         171.3675 |           8.1971 |
[32m[20221213 21:35:01 @agent_ppo2.py:185][0m |          -0.0093 |         170.9752 |           8.2290 |
[32m[20221213 21:35:01 @agent_ppo2.py:185][0m |          -0.0088 |         171.0886 |           8.2906 |
[32m[20221213 21:35:01 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:35:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.60
[32m[20221213 21:35:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.00
[32m[20221213 21:35:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.00
[32m[20221213 21:35:02 @agent_ppo2.py:143][0m Total time:      39.45 min
[32m[20221213 21:35:02 @agent_ppo2.py:145][0m 3854336 total steps have happened
[32m[20221213 21:35:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1882 --------------------------#
[32m[20221213 21:35:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:35:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:02 @agent_ppo2.py:185][0m |          -0.0030 |         175.8969 |           7.9629 |
[32m[20221213 21:35:02 @agent_ppo2.py:185][0m |          -0.0063 |         175.2323 |           7.9106 |
[32m[20221213 21:35:02 @agent_ppo2.py:185][0m |          -0.0049 |         174.2300 |           7.9200 |
[32m[20221213 21:35:02 @agent_ppo2.py:185][0m |          -0.0009 |         176.6311 |           7.9597 |
[32m[20221213 21:35:02 @agent_ppo2.py:185][0m |          -0.0047 |         173.7582 |           7.9327 |
[32m[20221213 21:35:02 @agent_ppo2.py:185][0m |          -0.0070 |         173.3123 |           8.0128 |
[32m[20221213 21:35:02 @agent_ppo2.py:185][0m |          -0.0078 |         173.1586 |           7.9874 |
[32m[20221213 21:35:02 @agent_ppo2.py:185][0m |           0.0107 |         193.8960 |           8.0027 |
[32m[20221213 21:35:03 @agent_ppo2.py:185][0m |          -0.0077 |         172.9727 |           7.9738 |
[32m[20221213 21:35:03 @agent_ppo2.py:185][0m |          -0.0086 |         172.7971 |           8.0128 |
[32m[20221213 21:35:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:35:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.60
[32m[20221213 21:35:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:35:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.00
[32m[20221213 21:35:03 @agent_ppo2.py:143][0m Total time:      39.47 min
[32m[20221213 21:35:03 @agent_ppo2.py:145][0m 3856384 total steps have happened
[32m[20221213 21:35:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1883 --------------------------#
[32m[20221213 21:35:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:03 @agent_ppo2.py:185][0m |          -0.0028 |         175.2942 |           8.3467 |
[32m[20221213 21:35:03 @agent_ppo2.py:185][0m |          -0.0055 |         174.1596 |           8.3525 |
[32m[20221213 21:35:03 @agent_ppo2.py:185][0m |          -0.0067 |         173.4281 |           8.3097 |
[32m[20221213 21:35:03 @agent_ppo2.py:185][0m |          -0.0076 |         173.0028 |           8.3130 |
[32m[20221213 21:35:03 @agent_ppo2.py:185][0m |          -0.0088 |         172.8867 |           8.3229 |
[32m[20221213 21:35:03 @agent_ppo2.py:185][0m |          -0.0084 |         172.4049 |           8.3273 |
[32m[20221213 21:35:04 @agent_ppo2.py:185][0m |           0.0035 |         194.4405 |           8.3186 |
[32m[20221213 21:35:04 @agent_ppo2.py:185][0m |          -0.0077 |         172.0962 |           8.2908 |
[32m[20221213 21:35:04 @agent_ppo2.py:185][0m |          -0.0087 |         171.6333 |           8.3217 |
[32m[20221213 21:35:04 @agent_ppo2.py:185][0m |          -0.0053 |         172.0812 |           8.3282 |
[32m[20221213 21:35:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:35:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.00
[32m[20221213 21:35:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.00
[32m[20221213 21:35:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:35:04 @agent_ppo2.py:143][0m Total time:      39.49 min
[32m[20221213 21:35:04 @agent_ppo2.py:145][0m 3858432 total steps have happened
[32m[20221213 21:35:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1884 --------------------------#
[32m[20221213 21:35:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:04 @agent_ppo2.py:185][0m |           0.0039 |         172.4895 |           8.3690 |
[32m[20221213 21:35:04 @agent_ppo2.py:185][0m |          -0.0036 |         169.6761 |           8.3556 |
[32m[20221213 21:35:04 @agent_ppo2.py:185][0m |          -0.0047 |         168.5086 |           8.3006 |
[32m[20221213 21:35:04 @agent_ppo2.py:185][0m |          -0.0045 |         167.5954 |           8.2239 |
[32m[20221213 21:35:05 @agent_ppo2.py:185][0m |          -0.0052 |         166.4101 |           8.2231 |
[32m[20221213 21:35:05 @agent_ppo2.py:185][0m |           0.0007 |         171.2161 |           8.2471 |
[32m[20221213 21:35:05 @agent_ppo2.py:185][0m |          -0.0052 |         165.3130 |           8.1897 |
[32m[20221213 21:35:05 @agent_ppo2.py:185][0m |          -0.0042 |         166.6925 |           8.2207 |
[32m[20221213 21:35:05 @agent_ppo2.py:185][0m |          -0.0076 |         164.4079 |           8.1916 |
[32m[20221213 21:35:05 @agent_ppo2.py:185][0m |          -0.0086 |         163.3757 |           8.1560 |
[32m[20221213 21:35:05 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:35:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.40
[32m[20221213 21:35:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:35:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.00
[32m[20221213 21:35:05 @agent_ppo2.py:143][0m Total time:      39.51 min
[32m[20221213 21:35:05 @agent_ppo2.py:145][0m 3860480 total steps have happened
[32m[20221213 21:35:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1885 --------------------------#
[32m[20221213 21:35:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:35:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:05 @agent_ppo2.py:185][0m |           0.0018 |         173.6279 |           8.0570 |
[32m[20221213 21:35:06 @agent_ppo2.py:185][0m |          -0.0045 |         171.7953 |           8.0716 |
[32m[20221213 21:35:06 @agent_ppo2.py:185][0m |          -0.0061 |         170.9788 |           8.0769 |
[32m[20221213 21:35:06 @agent_ppo2.py:185][0m |          -0.0079 |         170.3680 |           8.0609 |
[32m[20221213 21:35:06 @agent_ppo2.py:185][0m |          -0.0078 |         170.0774 |           8.0233 |
[32m[20221213 21:35:06 @agent_ppo2.py:185][0m |          -0.0072 |         169.8056 |           8.0271 |
[32m[20221213 21:35:06 @agent_ppo2.py:185][0m |          -0.0072 |         169.6734 |           8.0137 |
[32m[20221213 21:35:06 @agent_ppo2.py:185][0m |          -0.0089 |         169.2470 |           8.0169 |
[32m[20221213 21:35:06 @agent_ppo2.py:185][0m |          -0.0091 |         169.0463 |           8.0264 |
[32m[20221213 21:35:06 @agent_ppo2.py:185][0m |          -0.0108 |         168.7910 |           8.0028 |
[32m[20221213 21:35:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:35:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.80
[32m[20221213 21:35:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:35:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.00
[32m[20221213 21:35:06 @agent_ppo2.py:143][0m Total time:      39.53 min
[32m[20221213 21:35:06 @agent_ppo2.py:145][0m 3862528 total steps have happened
[32m[20221213 21:35:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1886 --------------------------#
[32m[20221213 21:35:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:07 @agent_ppo2.py:185][0m |          -0.0012 |         173.0488 |           7.6397 |
[32m[20221213 21:35:07 @agent_ppo2.py:185][0m |           0.0075 |         187.5653 |           7.6514 |
[32m[20221213 21:35:07 @agent_ppo2.py:185][0m |           0.0042 |         186.2216 |           7.6273 |
[32m[20221213 21:35:07 @agent_ppo2.py:185][0m |           0.0047 |         188.0864 |           7.5950 |
[32m[20221213 21:35:07 @agent_ppo2.py:185][0m |          -0.0058 |         170.4178 |           7.6074 |
[32m[20221213 21:35:07 @agent_ppo2.py:185][0m |          -0.0072 |         170.0441 |           7.5844 |
[32m[20221213 21:35:07 @agent_ppo2.py:185][0m |          -0.0076 |         169.9324 |           7.5599 |
[32m[20221213 21:35:07 @agent_ppo2.py:185][0m |          -0.0080 |         169.8922 |           7.5881 |
[32m[20221213 21:35:07 @agent_ppo2.py:185][0m |          -0.0056 |         169.6773 |           7.5748 |
[32m[20221213 21:35:07 @agent_ppo2.py:185][0m |          -0.0078 |         169.5609 |           7.5550 |
[32m[20221213 21:35:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:35:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.60
[32m[20221213 21:35:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.00
[32m[20221213 21:35:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.00
[32m[20221213 21:35:08 @agent_ppo2.py:143][0m Total time:      39.55 min
[32m[20221213 21:35:08 @agent_ppo2.py:145][0m 3864576 total steps have happened
[32m[20221213 21:35:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1887 --------------------------#
[32m[20221213 21:35:08 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:08 @agent_ppo2.py:185][0m |          -0.0010 |         174.4405 |           7.6472 |
[32m[20221213 21:35:08 @agent_ppo2.py:185][0m |          -0.0052 |         171.9683 |           7.7090 |
[32m[20221213 21:35:08 @agent_ppo2.py:185][0m |           0.0005 |         176.9185 |           7.7087 |
[32m[20221213 21:35:08 @agent_ppo2.py:185][0m |          -0.0064 |         170.1486 |           7.6894 |
[32m[20221213 21:35:08 @agent_ppo2.py:185][0m |          -0.0065 |         169.6209 |           7.6464 |
[32m[20221213 21:35:08 @agent_ppo2.py:185][0m |          -0.0099 |         169.1384 |           7.6753 |
[32m[20221213 21:35:08 @agent_ppo2.py:185][0m |           0.0052 |         192.5224 |           7.6651 |
[32m[20221213 21:35:08 @agent_ppo2.py:185][0m |          -0.0081 |         168.3097 |           7.7309 |
[32m[20221213 21:35:09 @agent_ppo2.py:185][0m |          -0.0104 |         168.3487 |           7.7284 |
[32m[20221213 21:35:09 @agent_ppo2.py:185][0m |          -0.0102 |         167.7920 |           7.6823 |
[32m[20221213 21:35:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:35:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.80
[32m[20221213 21:35:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:35:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.00
[32m[20221213 21:35:09 @agent_ppo2.py:143][0m Total time:      39.57 min
[32m[20221213 21:35:09 @agent_ppo2.py:145][0m 3866624 total steps have happened
[32m[20221213 21:35:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1888 --------------------------#
[32m[20221213 21:35:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:09 @agent_ppo2.py:185][0m |           0.0060 |         173.8988 |           7.4909 |
[32m[20221213 21:35:09 @agent_ppo2.py:185][0m |          -0.0042 |         170.7060 |           7.4176 |
[32m[20221213 21:35:09 @agent_ppo2.py:185][0m |          -0.0047 |         169.8649 |           7.4397 |
[32m[20221213 21:35:09 @agent_ppo2.py:185][0m |          -0.0038 |         170.3212 |           7.3871 |
[32m[20221213 21:35:09 @agent_ppo2.py:185][0m |          -0.0052 |         169.3584 |           7.4104 |
[32m[20221213 21:35:10 @agent_ppo2.py:185][0m |          -0.0044 |         168.7278 |           7.4217 |
[32m[20221213 21:35:10 @agent_ppo2.py:185][0m |          -0.0091 |         168.2227 |           7.4143 |
[32m[20221213 21:35:10 @agent_ppo2.py:185][0m |          -0.0078 |         168.0975 |           7.4103 |
[32m[20221213 21:35:10 @agent_ppo2.py:185][0m |          -0.0086 |         167.8299 |           7.3688 |
[32m[20221213 21:35:10 @agent_ppo2.py:185][0m |          -0.0104 |         167.7436 |           7.3791 |
[32m[20221213 21:35:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:35:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 772.00
[32m[20221213 21:35:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.00
[32m[20221213 21:35:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.00
[32m[20221213 21:35:10 @agent_ppo2.py:143][0m Total time:      39.59 min
[32m[20221213 21:35:10 @agent_ppo2.py:145][0m 3868672 total steps have happened
[32m[20221213 21:35:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1889 --------------------------#
[32m[20221213 21:35:10 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:10 @agent_ppo2.py:185][0m |          -0.0005 |         171.8162 |           7.7775 |
[32m[20221213 21:35:10 @agent_ppo2.py:185][0m |           0.0026 |         174.3448 |           7.7395 |
[32m[20221213 21:35:10 @agent_ppo2.py:185][0m |          -0.0023 |         167.9026 |           7.7226 |
[32m[20221213 21:35:11 @agent_ppo2.py:185][0m |          -0.0054 |         167.0956 |           7.6896 |
[32m[20221213 21:35:11 @agent_ppo2.py:185][0m |          -0.0078 |         166.2403 |           7.6432 |
[32m[20221213 21:35:11 @agent_ppo2.py:185][0m |           0.0172 |         191.6028 |           7.6270 |
[32m[20221213 21:35:11 @agent_ppo2.py:185][0m |          -0.0092 |         165.1865 |           7.6350 |
[32m[20221213 21:35:11 @agent_ppo2.py:185][0m |          -0.0065 |         164.9631 |           7.6410 |
[32m[20221213 21:35:11 @agent_ppo2.py:185][0m |          -0.0101 |         164.2885 |           7.5916 |
[32m[20221213 21:35:11 @agent_ppo2.py:185][0m |          -0.0102 |         163.9017 |           7.5556 |
[32m[20221213 21:35:11 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:35:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 784.40
[32m[20221213 21:35:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:35:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.00
[32m[20221213 21:35:11 @agent_ppo2.py:143][0m Total time:      39.61 min
[32m[20221213 21:35:11 @agent_ppo2.py:145][0m 3870720 total steps have happened
[32m[20221213 21:35:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1890 --------------------------#
[32m[20221213 21:35:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:12 @agent_ppo2.py:185][0m |          -0.0008 |         174.6978 |           7.5930 |
[32m[20221213 21:35:12 @agent_ppo2.py:185][0m |           0.0007 |         176.7347 |           7.5982 |
[32m[20221213 21:35:12 @agent_ppo2.py:185][0m |           0.0076 |         190.7458 |           7.6093 |
[32m[20221213 21:35:12 @agent_ppo2.py:185][0m |          -0.0061 |         172.7295 |           7.6132 |
[32m[20221213 21:35:12 @agent_ppo2.py:185][0m |          -0.0058 |         173.2834 |           7.6154 |
[32m[20221213 21:35:12 @agent_ppo2.py:185][0m |          -0.0095 |         172.0682 |           7.6183 |
[32m[20221213 21:35:12 @agent_ppo2.py:185][0m |           0.0097 |         194.9230 |           7.6477 |
[32m[20221213 21:35:12 @agent_ppo2.py:185][0m |          -0.0099 |         172.0414 |           7.6516 |
[32m[20221213 21:35:12 @agent_ppo2.py:185][0m |          -0.0090 |         171.4991 |           7.6558 |
[32m[20221213 21:35:12 @agent_ppo2.py:185][0m |          -0.0103 |         171.3236 |           7.6496 |
[32m[20221213 21:35:12 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:35:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.00
[32m[20221213 21:35:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.00
[32m[20221213 21:35:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.00
[32m[20221213 21:35:12 @agent_ppo2.py:143][0m Total time:      39.63 min
[32m[20221213 21:35:12 @agent_ppo2.py:145][0m 3872768 total steps have happened
[32m[20221213 21:35:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1891 --------------------------#
[32m[20221213 21:35:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:13 @agent_ppo2.py:185][0m |          -0.0003 |         172.9992 |           7.3961 |
[32m[20221213 21:35:13 @agent_ppo2.py:185][0m |          -0.0032 |         171.7776 |           7.3935 |
[32m[20221213 21:35:13 @agent_ppo2.py:185][0m |          -0.0052 |         171.1603 |           7.4064 |
[32m[20221213 21:35:13 @agent_ppo2.py:185][0m |          -0.0067 |         170.8114 |           7.4104 |
[32m[20221213 21:35:13 @agent_ppo2.py:185][0m |          -0.0048 |         170.3376 |           7.3915 |
[32m[20221213 21:35:13 @agent_ppo2.py:185][0m |          -0.0067 |         170.2785 |           7.4045 |
[32m[20221213 21:35:13 @agent_ppo2.py:185][0m |          -0.0068 |         169.9762 |           7.3517 |
[32m[20221213 21:35:13 @agent_ppo2.py:185][0m |          -0.0077 |         169.8371 |           7.3714 |
[32m[20221213 21:35:13 @agent_ppo2.py:185][0m |           0.0006 |         177.4994 |           7.3552 |
[32m[20221213 21:35:14 @agent_ppo2.py:185][0m |          -0.0059 |         169.7714 |           7.4096 |
[32m[20221213 21:35:14 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:35:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.00
[32m[20221213 21:35:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:35:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 861.00
[32m[20221213 21:35:14 @agent_ppo2.py:143][0m Total time:      39.65 min
[32m[20221213 21:35:14 @agent_ppo2.py:145][0m 3874816 total steps have happened
[32m[20221213 21:35:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1892 --------------------------#
[32m[20221213 21:35:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:14 @agent_ppo2.py:185][0m |          -0.0014 |         171.2745 |           7.5558 |
[32m[20221213 21:35:14 @agent_ppo2.py:185][0m |          -0.0005 |         172.0469 |           7.5917 |
[32m[20221213 21:35:14 @agent_ppo2.py:185][0m |          -0.0037 |         170.1411 |           7.6179 |
[32m[20221213 21:35:14 @agent_ppo2.py:185][0m |          -0.0012 |         172.1598 |           7.6121 |
[32m[20221213 21:35:14 @agent_ppo2.py:185][0m |           0.0062 |         184.7652 |           7.6150 |
[32m[20221213 21:35:14 @agent_ppo2.py:185][0m |           0.0041 |         180.3544 |           7.6728 |
[32m[20221213 21:35:15 @agent_ppo2.py:185][0m |           0.0032 |         184.8387 |           7.6942 |
[32m[20221213 21:35:15 @agent_ppo2.py:185][0m |          -0.0088 |         169.3246 |           7.7874 |
[32m[20221213 21:35:15 @agent_ppo2.py:185][0m |          -0.0068 |         170.2813 |           7.7465 |
[32m[20221213 21:35:15 @agent_ppo2.py:185][0m |          -0.0061 |         170.0863 |           7.7948 |
[32m[20221213 21:35:15 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:35:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.20
[32m[20221213 21:35:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.00
[32m[20221213 21:35:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.00
[32m[20221213 21:35:15 @agent_ppo2.py:143][0m Total time:      39.67 min
[32m[20221213 21:35:15 @agent_ppo2.py:145][0m 3876864 total steps have happened
[32m[20221213 21:35:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1893 --------------------------#
[32m[20221213 21:35:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:15 @agent_ppo2.py:185][0m |           0.0018 |         169.9985 |           7.6643 |
[32m[20221213 21:35:15 @agent_ppo2.py:185][0m |          -0.0038 |         169.2892 |           7.6676 |
[32m[20221213 21:35:15 @agent_ppo2.py:185][0m |          -0.0044 |         168.8907 |           7.7304 |
[32m[20221213 21:35:16 @agent_ppo2.py:185][0m |          -0.0062 |         168.6706 |           7.6940 |
[32m[20221213 21:35:16 @agent_ppo2.py:185][0m |          -0.0078 |         168.5887 |           7.6686 |
[32m[20221213 21:35:16 @agent_ppo2.py:185][0m |          -0.0082 |         168.3904 |           7.7156 |
[32m[20221213 21:35:16 @agent_ppo2.py:185][0m |          -0.0078 |         168.2353 |           7.7013 |
[32m[20221213 21:35:16 @agent_ppo2.py:185][0m |          -0.0083 |         168.1653 |           7.7322 |
[32m[20221213 21:35:16 @agent_ppo2.py:185][0m |          -0.0063 |         167.9370 |           7.6868 |
[32m[20221213 21:35:16 @agent_ppo2.py:185][0m |           0.0018 |         175.9957 |           7.7299 |
[32m[20221213 21:35:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:35:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.80
[32m[20221213 21:35:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.00
[32m[20221213 21:35:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 21:35:16 @agent_ppo2.py:143][0m Total time:      39.69 min
[32m[20221213 21:35:16 @agent_ppo2.py:145][0m 3878912 total steps have happened
[32m[20221213 21:35:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1894 --------------------------#
[32m[20221213 21:35:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:16 @agent_ppo2.py:185][0m |          -0.0004 |         172.6577 |           7.5128 |
[32m[20221213 21:35:17 @agent_ppo2.py:185][0m |          -0.0044 |         171.9114 |           7.5422 |
[32m[20221213 21:35:17 @agent_ppo2.py:185][0m |          -0.0052 |         171.3040 |           7.6156 |
[32m[20221213 21:35:17 @agent_ppo2.py:185][0m |          -0.0040 |         171.7239 |           7.5925 |
[32m[20221213 21:35:17 @agent_ppo2.py:185][0m |          -0.0057 |         170.5245 |           7.5925 |
[32m[20221213 21:35:17 @agent_ppo2.py:185][0m |           0.0074 |         186.0705 |           7.6372 |
[32m[20221213 21:35:17 @agent_ppo2.py:185][0m |          -0.0085 |         170.0982 |           7.6424 |
[32m[20221213 21:35:17 @agent_ppo2.py:185][0m |          -0.0051 |         170.1041 |           7.6631 |
[32m[20221213 21:35:17 @agent_ppo2.py:185][0m |          -0.0087 |         169.7400 |           7.6830 |
[32m[20221213 21:35:17 @agent_ppo2.py:185][0m |          -0.0082 |         169.4484 |           7.7115 |
[32m[20221213 21:35:17 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:35:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.00
[32m[20221213 21:35:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:35:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 21:35:17 @agent_ppo2.py:143][0m Total time:      39.71 min
[32m[20221213 21:35:17 @agent_ppo2.py:145][0m 3880960 total steps have happened
[32m[20221213 21:35:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1895 --------------------------#
[32m[20221213 21:35:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:35:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:18 @agent_ppo2.py:185][0m |          -0.0006 |         172.7242 |           7.4704 |
[32m[20221213 21:35:18 @agent_ppo2.py:185][0m |          -0.0031 |         172.1476 |           7.5447 |
[32m[20221213 21:35:18 @agent_ppo2.py:185][0m |          -0.0045 |         171.6362 |           7.5618 |
[32m[20221213 21:35:18 @agent_ppo2.py:185][0m |          -0.0057 |         171.2628 |           7.5624 |
[32m[20221213 21:35:18 @agent_ppo2.py:185][0m |          -0.0074 |         171.2024 |           7.5638 |
[32m[20221213 21:35:18 @agent_ppo2.py:185][0m |          -0.0084 |         171.1707 |           7.6213 |
[32m[20221213 21:35:18 @agent_ppo2.py:185][0m |          -0.0004 |         181.1966 |           7.6370 |
[32m[20221213 21:35:18 @agent_ppo2.py:185][0m |          -0.0035 |         171.7815 |           7.6521 |
[32m[20221213 21:35:18 @agent_ppo2.py:185][0m |          -0.0097 |         170.5152 |           7.6641 |
[32m[20221213 21:35:18 @agent_ppo2.py:185][0m |           0.0055 |         193.7909 |           7.6541 |
[32m[20221213 21:35:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:35:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.80
[32m[20221213 21:35:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:35:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.00
[32m[20221213 21:35:19 @agent_ppo2.py:143][0m Total time:      39.73 min
[32m[20221213 21:35:19 @agent_ppo2.py:145][0m 3883008 total steps have happened
[32m[20221213 21:35:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1896 --------------------------#
[32m[20221213 21:35:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:19 @agent_ppo2.py:185][0m |          -0.0016 |         172.3614 |           7.8405 |
[32m[20221213 21:35:19 @agent_ppo2.py:185][0m |           0.0038 |         180.0717 |           7.9102 |
[32m[20221213 21:35:19 @agent_ppo2.py:185][0m |          -0.0046 |         168.1010 |           7.8451 |
[32m[20221213 21:35:19 @agent_ppo2.py:185][0m |          -0.0039 |         167.8612 |           7.8067 |
[32m[20221213 21:35:19 @agent_ppo2.py:185][0m |          -0.0083 |         165.1369 |           7.8206 |
[32m[20221213 21:35:19 @agent_ppo2.py:185][0m |          -0.0096 |         164.7296 |           7.8290 |
[32m[20221213 21:35:19 @agent_ppo2.py:185][0m |          -0.0093 |         164.2871 |           7.8301 |
[32m[20221213 21:35:20 @agent_ppo2.py:185][0m |           0.0003 |         182.1215 |           7.8289 |
[32m[20221213 21:35:20 @agent_ppo2.py:185][0m |          -0.0081 |         163.8730 |           7.8473 |
[32m[20221213 21:35:20 @agent_ppo2.py:185][0m |          -0.0088 |         163.8684 |           7.8316 |
[32m[20221213 21:35:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:35:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.80
[32m[20221213 21:35:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.00
[32m[20221213 21:35:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.00
[32m[20221213 21:35:20 @agent_ppo2.py:143][0m Total time:      39.75 min
[32m[20221213 21:35:20 @agent_ppo2.py:145][0m 3885056 total steps have happened
[32m[20221213 21:35:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1897 --------------------------#
[32m[20221213 21:35:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:20 @agent_ppo2.py:185][0m |           0.0019 |         183.7823 |           7.6074 |
[32m[20221213 21:35:20 @agent_ppo2.py:185][0m |          -0.0033 |         177.9012 |           7.7106 |
[32m[20221213 21:35:20 @agent_ppo2.py:185][0m |          -0.0037 |         177.0167 |           7.6392 |
[32m[20221213 21:35:20 @agent_ppo2.py:185][0m |          -0.0098 |         175.9902 |           7.7014 |
[32m[20221213 21:35:20 @agent_ppo2.py:185][0m |          -0.0073 |         175.5100 |           7.6539 |
[32m[20221213 21:35:21 @agent_ppo2.py:185][0m |          -0.0088 |         175.0585 |           7.6658 |
[32m[20221213 21:35:21 @agent_ppo2.py:185][0m |          -0.0092 |         174.9445 |           7.6278 |
[32m[20221213 21:35:21 @agent_ppo2.py:185][0m |          -0.0082 |         174.6577 |           7.6144 |
[32m[20221213 21:35:21 @agent_ppo2.py:185][0m |          -0.0119 |         174.2972 |           7.6280 |
[32m[20221213 21:35:21 @agent_ppo2.py:185][0m |          -0.0094 |         174.1360 |           7.6614 |
[32m[20221213 21:35:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:35:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.40
[32m[20221213 21:35:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.00
[32m[20221213 21:35:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.00
[32m[20221213 21:35:21 @agent_ppo2.py:143][0m Total time:      39.77 min
[32m[20221213 21:35:21 @agent_ppo2.py:145][0m 3887104 total steps have happened
[32m[20221213 21:35:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1898 --------------------------#
[32m[20221213 21:35:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:35:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:21 @agent_ppo2.py:185][0m |           0.0005 |         171.5419 |           7.9751 |
[32m[20221213 21:35:21 @agent_ppo2.py:185][0m |          -0.0011 |         170.9196 |           7.9983 |
[32m[20221213 21:35:21 @agent_ppo2.py:185][0m |          -0.0029 |         169.6873 |           7.9655 |
[32m[20221213 21:35:22 @agent_ppo2.py:185][0m |          -0.0023 |         169.2676 |           7.9997 |
[32m[20221213 21:35:22 @agent_ppo2.py:185][0m |          -0.0056 |         168.6695 |           7.9230 |
[32m[20221213 21:35:22 @agent_ppo2.py:185][0m |          -0.0081 |         168.3275 |           7.9595 |
[32m[20221213 21:35:22 @agent_ppo2.py:185][0m |          -0.0013 |         173.4788 |           7.9477 |
[32m[20221213 21:35:22 @agent_ppo2.py:185][0m |          -0.0007 |         170.8750 |           7.9752 |
[32m[20221213 21:35:22 @agent_ppo2.py:185][0m |          -0.0072 |         167.7163 |           7.9382 |
[32m[20221213 21:35:22 @agent_ppo2.py:185][0m |          -0.0082 |         167.3630 |           7.9267 |
[32m[20221213 21:35:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:35:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.00
[32m[20221213 21:35:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.00
[32m[20221213 21:35:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.00
[32m[20221213 21:35:22 @agent_ppo2.py:143][0m Total time:      39.79 min
[32m[20221213 21:35:22 @agent_ppo2.py:145][0m 3889152 total steps have happened
[32m[20221213 21:35:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1899 --------------------------#
[32m[20221213 21:35:22 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:35:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:23 @agent_ppo2.py:185][0m |          -0.0036 |         171.9561 |           7.5522 |
[32m[20221213 21:35:23 @agent_ppo2.py:185][0m |          -0.0056 |         171.1906 |           7.5339 |
[32m[20221213 21:35:23 @agent_ppo2.py:185][0m |          -0.0061 |         170.8967 |           7.5864 |
[32m[20221213 21:35:23 @agent_ppo2.py:185][0m |          -0.0018 |         175.6672 |           7.5796 |
[32m[20221213 21:35:23 @agent_ppo2.py:185][0m |          -0.0045 |         172.7849 |           7.6317 |
[32m[20221213 21:35:23 @agent_ppo2.py:185][0m |          -0.0092 |         170.4648 |           7.6289 |
[32m[20221213 21:35:23 @agent_ppo2.py:185][0m |          -0.0089 |         170.2652 |           7.5802 |
[32m[20221213 21:35:23 @agent_ppo2.py:185][0m |          -0.0097 |         170.1688 |           7.5826 |
[32m[20221213 21:35:23 @agent_ppo2.py:185][0m |          -0.0108 |         170.0643 |           7.5933 |
[32m[20221213 21:35:23 @agent_ppo2.py:185][0m |          -0.0112 |         169.8739 |           7.5839 |
[32m[20221213 21:35:23 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:35:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.00
[32m[20221213 21:35:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.00
[32m[20221213 21:35:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.00
[32m[20221213 21:35:23 @agent_ppo2.py:143][0m Total time:      39.81 min
[32m[20221213 21:35:23 @agent_ppo2.py:145][0m 3891200 total steps have happened
[32m[20221213 21:35:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1900 --------------------------#
[32m[20221213 21:35:24 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:24 @agent_ppo2.py:185][0m |           0.0090 |         176.7999 |           7.4359 |
[32m[20221213 21:35:24 @agent_ppo2.py:185][0m |          -0.0039 |         165.8584 |           7.4349 |
[32m[20221213 21:35:24 @agent_ppo2.py:185][0m |          -0.0050 |         164.8122 |           7.4596 |
[32m[20221213 21:35:24 @agent_ppo2.py:185][0m |          -0.0040 |         164.2714 |           7.4079 |
[32m[20221213 21:35:24 @agent_ppo2.py:185][0m |          -0.0072 |         163.7276 |           7.3749 |
[32m[20221213 21:35:24 @agent_ppo2.py:185][0m |          -0.0052 |         163.3616 |           7.4054 |
[32m[20221213 21:35:24 @agent_ppo2.py:185][0m |          -0.0087 |         163.2075 |           7.3276 |
[32m[20221213 21:35:24 @agent_ppo2.py:185][0m |          -0.0086 |         163.0182 |           7.3181 |
[32m[20221213 21:35:24 @agent_ppo2.py:185][0m |          -0.0087 |         162.6298 |           7.3414 |
[32m[20221213 21:35:25 @agent_ppo2.py:185][0m |          -0.0060 |         162.4384 |           7.3771 |
[32m[20221213 21:35:25 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:35:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 752.00
[32m[20221213 21:35:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.00
[32m[20221213 21:35:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.00
[32m[20221213 21:35:25 @agent_ppo2.py:143][0m Total time:      39.83 min
[32m[20221213 21:35:25 @agent_ppo2.py:145][0m 3893248 total steps have happened
[32m[20221213 21:35:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1901 --------------------------#
[32m[20221213 21:35:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:25 @agent_ppo2.py:185][0m |          -0.0008 |         168.5126 |           7.6109 |
[32m[20221213 21:35:25 @agent_ppo2.py:185][0m |           0.0015 |         169.6763 |           7.6526 |
[32m[20221213 21:35:25 @agent_ppo2.py:185][0m |          -0.0031 |         167.5517 |           7.6885 |
[32m[20221213 21:35:25 @agent_ppo2.py:185][0m |           0.0012 |         171.9588 |           7.7232 |
[32m[20221213 21:35:25 @agent_ppo2.py:185][0m |          -0.0061 |         167.0459 |           7.6734 |
[32m[20221213 21:35:25 @agent_ppo2.py:185][0m |          -0.0059 |         167.0188 |           7.7309 |
[32m[20221213 21:35:25 @agent_ppo2.py:185][0m |          -0.0045 |         166.7702 |           7.7647 |
[32m[20221213 21:35:26 @agent_ppo2.py:185][0m |          -0.0080 |         166.6763 |           7.7340 |
[32m[20221213 21:35:26 @agent_ppo2.py:185][0m |          -0.0079 |         166.5254 |           7.7514 |
[32m[20221213 21:35:26 @agent_ppo2.py:185][0m |          -0.0073 |         166.6118 |           7.7217 |
[32m[20221213 21:35:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:35:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.80
[32m[20221213 21:35:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 853.00
[32m[20221213 21:35:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.00
[32m[20221213 21:35:26 @agent_ppo2.py:143][0m Total time:      39.85 min
[32m[20221213 21:35:26 @agent_ppo2.py:145][0m 3895296 total steps have happened
[32m[20221213 21:35:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1902 --------------------------#
[32m[20221213 21:35:26 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:26 @agent_ppo2.py:185][0m |           0.0005 |         169.6263 |           7.3120 |
[32m[20221213 21:35:26 @agent_ppo2.py:185][0m |          -0.0029 |         168.9740 |           7.2436 |
[32m[20221213 21:35:26 @agent_ppo2.py:185][0m |          -0.0032 |         168.5193 |           7.2293 |
[32m[20221213 21:35:26 @agent_ppo2.py:185][0m |          -0.0049 |         168.2626 |           7.2062 |
[32m[20221213 21:35:26 @agent_ppo2.py:185][0m |          -0.0042 |         168.0780 |           7.1697 |
[32m[20221213 21:35:27 @agent_ppo2.py:185][0m |          -0.0044 |         167.8896 |           7.1345 |
[32m[20221213 21:35:27 @agent_ppo2.py:185][0m |           0.0021 |         173.7837 |           7.1032 |
[32m[20221213 21:35:27 @agent_ppo2.py:185][0m |          -0.0050 |         168.8813 |           7.0429 |
[32m[20221213 21:35:27 @agent_ppo2.py:185][0m |          -0.0030 |         170.1538 |           7.0323 |
[32m[20221213 21:35:27 @agent_ppo2.py:185][0m |           0.0028 |         175.6293 |           6.9918 |
[32m[20221213 21:35:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:35:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.00
[32m[20221213 21:35:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.00
[32m[20221213 21:35:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.00
[32m[20221213 21:35:27 @agent_ppo2.py:143][0m Total time:      39.87 min
[32m[20221213 21:35:27 @agent_ppo2.py:145][0m 3897344 total steps have happened
[32m[20221213 21:35:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1903 --------------------------#
[32m[20221213 21:35:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:27 @agent_ppo2.py:185][0m |           0.0078 |         179.7212 |           7.0479 |
[32m[20221213 21:35:27 @agent_ppo2.py:185][0m |          -0.0037 |         170.0504 |           7.0468 |
[32m[20221213 21:35:28 @agent_ppo2.py:185][0m |          -0.0034 |         168.9614 |           7.0119 |
[32m[20221213 21:35:28 @agent_ppo2.py:185][0m |          -0.0077 |         168.6249 |           7.0526 |
[32m[20221213 21:35:28 @agent_ppo2.py:185][0m |          -0.0019 |         169.3580 |           7.0605 |
[32m[20221213 21:35:28 @agent_ppo2.py:185][0m |          -0.0028 |         167.8171 |           7.0425 |
[32m[20221213 21:35:28 @agent_ppo2.py:185][0m |          -0.0025 |         170.4345 |           7.0319 |
[32m[20221213 21:35:28 @agent_ppo2.py:185][0m |          -0.0080 |         167.2239 |           7.0283 |
[32m[20221213 21:35:28 @agent_ppo2.py:185][0m |          -0.0051 |         167.7946 |           7.0512 |
[32m[20221213 21:35:28 @agent_ppo2.py:185][0m |          -0.0082 |         167.0547 |           7.0651 |
[32m[20221213 21:35:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:35:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.40
[32m[20221213 21:35:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:35:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.00
[32m[20221213 21:35:28 @agent_ppo2.py:143][0m Total time:      39.89 min
[32m[20221213 21:35:28 @agent_ppo2.py:145][0m 3899392 total steps have happened
[32m[20221213 21:35:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1904 --------------------------#
[32m[20221213 21:35:28 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:35:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:29 @agent_ppo2.py:185][0m |           0.0048 |         174.8753 |           7.0522 |
[32m[20221213 21:35:29 @agent_ppo2.py:185][0m |          -0.0046 |         170.9957 |           7.1264 |
[32m[20221213 21:35:29 @agent_ppo2.py:185][0m |          -0.0073 |         170.0426 |           7.1542 |
[32m[20221213 21:35:29 @agent_ppo2.py:185][0m |          -0.0076 |         169.2623 |           7.1505 |
[32m[20221213 21:35:29 @agent_ppo2.py:185][0m |          -0.0087 |         168.7735 |           7.1744 |
[32m[20221213 21:35:29 @agent_ppo2.py:185][0m |          -0.0083 |         168.1531 |           7.2051 |
[32m[20221213 21:35:29 @agent_ppo2.py:185][0m |          -0.0078 |         168.2364 |           7.2153 |
[32m[20221213 21:35:29 @agent_ppo2.py:185][0m |          -0.0109 |         167.7326 |           7.2246 |
[32m[20221213 21:35:29 @agent_ppo2.py:185][0m |          -0.0098 |         167.4209 |           7.2396 |
[32m[20221213 21:35:29 @agent_ppo2.py:185][0m |          -0.0060 |         172.5842 |           7.2576 |
[32m[20221213 21:35:29 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:35:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.60
[32m[20221213 21:35:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:35:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.00
[32m[20221213 21:35:30 @agent_ppo2.py:143][0m Total time:      39.91 min
[32m[20221213 21:35:30 @agent_ppo2.py:145][0m 3901440 total steps have happened
[32m[20221213 21:35:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1905 --------------------------#
[32m[20221213 21:35:30 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:35:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:30 @agent_ppo2.py:185][0m |           0.0035 |         176.8391 |           6.2445 |
[32m[20221213 21:35:30 @agent_ppo2.py:185][0m |           0.0042 |         181.0833 |           6.2296 |
[32m[20221213 21:35:30 @agent_ppo2.py:185][0m |           0.0007 |         173.3240 |           6.2509 |
[32m[20221213 21:35:30 @agent_ppo2.py:185][0m |          -0.0075 |         169.4308 |           6.2794 |
[32m[20221213 21:35:30 @agent_ppo2.py:185][0m |          -0.0030 |         172.8623 |           6.2931 |
[32m[20221213 21:35:30 @agent_ppo2.py:185][0m |          -0.0095 |         168.5782 |           6.3119 |
[32m[20221213 21:35:30 @agent_ppo2.py:185][0m |          -0.0111 |         168.4471 |           6.2826 |
[32m[20221213 21:35:30 @agent_ppo2.py:185][0m |          -0.0092 |         168.0601 |           6.3280 |
[32m[20221213 21:35:31 @agent_ppo2.py:185][0m |          -0.0111 |         168.0623 |           6.3801 |
[32m[20221213 21:35:31 @agent_ppo2.py:185][0m |          -0.0117 |         167.7814 |           6.3736 |
[32m[20221213 21:35:31 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:35:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.40
[32m[20221213 21:35:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.00
[32m[20221213 21:35:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.00
[32m[20221213 21:35:31 @agent_ppo2.py:143][0m Total time:      39.93 min
[32m[20221213 21:35:31 @agent_ppo2.py:145][0m 3903488 total steps have happened
[32m[20221213 21:35:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1906 --------------------------#
[32m[20221213 21:35:31 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:35:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:31 @agent_ppo2.py:185][0m |          -0.0021 |         170.6675 |           7.2443 |
[32m[20221213 21:35:31 @agent_ppo2.py:185][0m |          -0.0068 |         169.8651 |           7.2775 |
[32m[20221213 21:35:31 @agent_ppo2.py:185][0m |          -0.0078 |         169.3784 |           7.3141 |
[32m[20221213 21:35:31 @agent_ppo2.py:185][0m |          -0.0093 |         168.9390 |           7.3641 |
[32m[20221213 21:35:31 @agent_ppo2.py:185][0m |          -0.0092 |         168.6463 |           7.3851 |
[32m[20221213 21:35:32 @agent_ppo2.py:185][0m |          -0.0073 |         168.6782 |           7.4557 |
[32m[20221213 21:35:32 @agent_ppo2.py:185][0m |          -0.0111 |         168.0731 |           7.4387 |
[32m[20221213 21:35:32 @agent_ppo2.py:185][0m |           0.0020 |         177.0618 |           7.5165 |
[32m[20221213 21:35:32 @agent_ppo2.py:185][0m |          -0.0099 |         167.7659 |           7.5187 |
[32m[20221213 21:35:32 @agent_ppo2.py:185][0m |          -0.0106 |         167.5651 |           7.5797 |
[32m[20221213 21:35:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:35:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.20
[32m[20221213 21:35:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:35:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.00
[32m[20221213 21:35:32 @agent_ppo2.py:143][0m Total time:      39.96 min
[32m[20221213 21:35:32 @agent_ppo2.py:145][0m 3905536 total steps have happened
[32m[20221213 21:35:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1907 --------------------------#
[32m[20221213 21:35:32 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:35:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:32 @agent_ppo2.py:185][0m |          -0.0010 |         173.2988 |           7.6170 |
[32m[20221213 21:35:32 @agent_ppo2.py:185][0m |           0.0034 |         180.5828 |           7.6608 |
[32m[20221213 21:35:33 @agent_ppo2.py:185][0m |          -0.0063 |         171.3135 |           7.7215 |
[32m[20221213 21:35:33 @agent_ppo2.py:185][0m |          -0.0051 |         170.9145 |           7.7510 |
[32m[20221213 21:35:33 @agent_ppo2.py:185][0m |           0.0009 |         175.1792 |           7.7543 |
[32m[20221213 21:35:33 @agent_ppo2.py:185][0m |          -0.0063 |         170.1931 |           7.7696 |
[32m[20221213 21:35:33 @agent_ppo2.py:185][0m |          -0.0083 |         170.0688 |           7.8370 |
[32m[20221213 21:35:33 @agent_ppo2.py:185][0m |          -0.0075 |         169.8631 |           7.8614 |
[32m[20221213 21:35:33 @agent_ppo2.py:185][0m |          -0.0077 |         169.9591 |           7.8834 |
[32m[20221213 21:35:33 @agent_ppo2.py:185][0m |           0.0028 |         192.1671 |           7.8810 |
[32m[20221213 21:35:33 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:35:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.00
[32m[20221213 21:35:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:35:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.00
[32m[20221213 21:35:33 @agent_ppo2.py:143][0m Total time:      39.98 min
[32m[20221213 21:35:33 @agent_ppo2.py:145][0m 3907584 total steps have happened
[32m[20221213 21:35:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1908 --------------------------#
[32m[20221213 21:35:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:35:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:34 @agent_ppo2.py:185][0m |           0.0109 |         180.6466 |           7.7495 |
[32m[20221213 21:35:34 @agent_ppo2.py:185][0m |          -0.0013 |         170.9111 |           7.7839 |
[32m[20221213 21:35:34 @agent_ppo2.py:185][0m |           0.0024 |         175.0744 |           7.7890 |
[32m[20221213 21:35:34 @agent_ppo2.py:185][0m |          -0.0046 |         169.1379 |           7.8301 |
[32m[20221213 21:35:34 @agent_ppo2.py:185][0m |           0.0038 |         182.8596 |           7.8359 |
[32m[20221213 21:35:34 @agent_ppo2.py:185][0m |          -0.0042 |         168.3703 |           7.8533 |
[32m[20221213 21:35:34 @agent_ppo2.py:185][0m |          -0.0081 |         168.0555 |           7.7926 |
[32m[20221213 21:35:34 @agent_ppo2.py:185][0m |           0.0015 |         183.2774 |           7.8286 |
[32m[20221213 21:35:34 @agent_ppo2.py:185][0m |          -0.0069 |         168.2356 |           7.8250 |
[32m[20221213 21:35:34 @agent_ppo2.py:185][0m |          -0.0091 |         167.3783 |           7.8625 |
[32m[20221213 21:35:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:35:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.40
[32m[20221213 21:35:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.00
[32m[20221213 21:35:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:35:35 @agent_ppo2.py:143][0m Total time:      40.00 min
[32m[20221213 21:35:35 @agent_ppo2.py:145][0m 3909632 total steps have happened
[32m[20221213 21:35:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1909 --------------------------#
[32m[20221213 21:35:35 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:35 @agent_ppo2.py:185][0m |           0.0000 |         171.4915 |           7.7884 |
[32m[20221213 21:35:35 @agent_ppo2.py:185][0m |          -0.0043 |         170.6342 |           7.7809 |
[32m[20221213 21:35:35 @agent_ppo2.py:185][0m |          -0.0063 |         170.1731 |           7.7586 |
[32m[20221213 21:35:35 @agent_ppo2.py:185][0m |          -0.0014 |         175.6912 |           7.7630 |
[32m[20221213 21:35:35 @agent_ppo2.py:185][0m |           0.0038 |         188.4781 |           7.7515 |
[32m[20221213 21:35:35 @agent_ppo2.py:185][0m |           0.0045 |         185.7812 |           7.8017 |
[32m[20221213 21:35:35 @agent_ppo2.py:185][0m |          -0.0075 |         169.1126 |           7.7983 |
[32m[20221213 21:35:35 @agent_ppo2.py:185][0m |          -0.0082 |         168.8231 |           7.7926 |
[32m[20221213 21:35:36 @agent_ppo2.py:185][0m |          -0.0091 |         168.7005 |           7.7949 |
[32m[20221213 21:35:36 @agent_ppo2.py:185][0m |          -0.0105 |         168.5332 |           7.7847 |
[32m[20221213 21:35:36 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:35:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.00
[32m[20221213 21:35:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:35:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.00
[32m[20221213 21:35:36 @agent_ppo2.py:143][0m Total time:      40.02 min
[32m[20221213 21:35:36 @agent_ppo2.py:145][0m 3911680 total steps have happened
[32m[20221213 21:35:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1910 --------------------------#
[32m[20221213 21:35:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:36 @agent_ppo2.py:185][0m |          -0.0014 |         171.7930 |           8.0412 |
[32m[20221213 21:35:36 @agent_ppo2.py:185][0m |          -0.0034 |         171.3026 |           7.9752 |
[32m[20221213 21:35:36 @agent_ppo2.py:185][0m |           0.0011 |         174.7326 |           8.0140 |
[32m[20221213 21:35:36 @agent_ppo2.py:185][0m |          -0.0057 |         170.5874 |           7.9940 |
[32m[20221213 21:35:36 @agent_ppo2.py:185][0m |          -0.0063 |         170.2879 |           8.0149 |
[32m[20221213 21:35:36 @agent_ppo2.py:185][0m |           0.0084 |         184.2035 |           8.0093 |
[32m[20221213 21:35:37 @agent_ppo2.py:185][0m |          -0.0055 |         169.8659 |           8.0170 |
[32m[20221213 21:35:37 @agent_ppo2.py:185][0m |          -0.0055 |         169.9356 |           7.9656 |
[32m[20221213 21:35:37 @agent_ppo2.py:185][0m |          -0.0076 |         169.4750 |           8.0412 |
[32m[20221213 21:35:37 @agent_ppo2.py:185][0m |          -0.0080 |         169.2222 |           7.9382 |
[32m[20221213 21:35:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:35:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.80
[32m[20221213 21:35:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:35:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 21:35:37 @agent_ppo2.py:143][0m Total time:      40.04 min
[32m[20221213 21:35:37 @agent_ppo2.py:145][0m 3913728 total steps have happened
[32m[20221213 21:35:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1911 --------------------------#
[32m[20221213 21:35:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:35:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:37 @agent_ppo2.py:185][0m |           0.0027 |         172.2714 |           7.8740 |
[32m[20221213 21:35:37 @agent_ppo2.py:185][0m |          -0.0040 |         169.5343 |           7.9147 |
[32m[20221213 21:35:37 @agent_ppo2.py:185][0m |          -0.0051 |         168.3010 |           7.9431 |
[32m[20221213 21:35:38 @agent_ppo2.py:185][0m |          -0.0038 |         167.5844 |           7.9379 |
[32m[20221213 21:35:38 @agent_ppo2.py:185][0m |          -0.0011 |         168.4763 |           7.9564 |
[32m[20221213 21:35:38 @agent_ppo2.py:185][0m |          -0.0070 |         167.0242 |           8.0310 |
[32m[20221213 21:35:38 @agent_ppo2.py:185][0m |          -0.0045 |         166.8506 |           8.0816 |
[32m[20221213 21:35:38 @agent_ppo2.py:185][0m |          -0.0055 |         166.4295 |           8.0376 |
[32m[20221213 21:35:38 @agent_ppo2.py:185][0m |          -0.0085 |         166.1357 |           8.1177 |
[32m[20221213 21:35:38 @agent_ppo2.py:185][0m |          -0.0075 |         165.9781 |           8.1365 |
[32m[20221213 21:35:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:35:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.20
[32m[20221213 21:35:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.00
[32m[20221213 21:35:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.00
[32m[20221213 21:35:38 @agent_ppo2.py:143][0m Total time:      40.06 min
[32m[20221213 21:35:38 @agent_ppo2.py:145][0m 3915776 total steps have happened
[32m[20221213 21:35:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1912 --------------------------#
[32m[20221213 21:35:38 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:38 @agent_ppo2.py:185][0m |          -0.0043 |         169.0909 |           8.0578 |
[32m[20221213 21:35:39 @agent_ppo2.py:185][0m |           0.0031 |         175.0622 |           8.1555 |
[32m[20221213 21:35:39 @agent_ppo2.py:185][0m |          -0.0062 |         167.2282 |           8.1489 |
[32m[20221213 21:35:39 @agent_ppo2.py:185][0m |          -0.0071 |         166.6531 |           8.2146 |
[32m[20221213 21:35:39 @agent_ppo2.py:185][0m |          -0.0070 |         166.6419 |           8.2609 |
[32m[20221213 21:35:39 @agent_ppo2.py:185][0m |          -0.0089 |         165.7882 |           8.2607 |
[32m[20221213 21:35:39 @agent_ppo2.py:185][0m |          -0.0079 |         165.4683 |           8.2422 |
[32m[20221213 21:35:39 @agent_ppo2.py:185][0m |          -0.0084 |         165.3506 |           8.2652 |
[32m[20221213 21:35:39 @agent_ppo2.py:185][0m |          -0.0098 |         165.0342 |           8.3291 |
[32m[20221213 21:35:39 @agent_ppo2.py:185][0m |          -0.0100 |         164.9290 |           8.2842 |
[32m[20221213 21:35:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:35:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.80
[32m[20221213 21:35:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:35:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.00
[32m[20221213 21:35:39 @agent_ppo2.py:143][0m Total time:      40.08 min
[32m[20221213 21:35:39 @agent_ppo2.py:145][0m 3917824 total steps have happened
[32m[20221213 21:35:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1913 --------------------------#
[32m[20221213 21:35:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:40 @agent_ppo2.py:185][0m |           0.0096 |         193.1520 |           8.3229 |
[32m[20221213 21:35:40 @agent_ppo2.py:185][0m |           0.0027 |         179.7032 |           8.3248 |
[32m[20221213 21:35:40 @agent_ppo2.py:185][0m |          -0.0058 |         172.4660 |           8.3018 |
[32m[20221213 21:35:40 @agent_ppo2.py:185][0m |          -0.0082 |         171.6193 |           8.3029 |
[32m[20221213 21:35:40 @agent_ppo2.py:185][0m |          -0.0083 |         171.0764 |           8.2864 |
[32m[20221213 21:35:40 @agent_ppo2.py:185][0m |          -0.0094 |         170.4676 |           8.2777 |
[32m[20221213 21:35:40 @agent_ppo2.py:185][0m |          -0.0071 |         170.7643 |           8.2830 |
[32m[20221213 21:35:40 @agent_ppo2.py:185][0m |          -0.0055 |         174.0593 |           8.2766 |
[32m[20221213 21:35:40 @agent_ppo2.py:185][0m |          -0.0072 |         169.6745 |           8.2667 |
[32m[20221213 21:35:40 @agent_ppo2.py:185][0m |          -0.0097 |         168.8865 |           8.2949 |
[32m[20221213 21:35:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:35:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.80
[32m[20221213 21:35:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221213 21:35:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.00
[32m[20221213 21:35:41 @agent_ppo2.py:143][0m Total time:      40.10 min
[32m[20221213 21:35:41 @agent_ppo2.py:145][0m 3919872 total steps have happened
[32m[20221213 21:35:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1914 --------------------------#
[32m[20221213 21:35:41 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:41 @agent_ppo2.py:185][0m |          -0.0018 |         173.0846 |           8.3815 |
[32m[20221213 21:35:41 @agent_ppo2.py:185][0m |           0.0008 |         172.0225 |           8.4591 |
[32m[20221213 21:35:41 @agent_ppo2.py:185][0m |          -0.0035 |         171.4714 |           8.4332 |
[32m[20221213 21:35:41 @agent_ppo2.py:185][0m |          -0.0032 |         171.0075 |           8.3776 |
[32m[20221213 21:35:41 @agent_ppo2.py:185][0m |          -0.0050 |         170.7253 |           8.3608 |
[32m[20221213 21:35:41 @agent_ppo2.py:185][0m |          -0.0051 |         170.6028 |           8.3577 |
[32m[20221213 21:35:41 @agent_ppo2.py:185][0m |          -0.0057 |         170.3929 |           8.3337 |
[32m[20221213 21:35:41 @agent_ppo2.py:185][0m |          -0.0061 |         170.2703 |           8.3244 |
[32m[20221213 21:35:42 @agent_ppo2.py:185][0m |           0.0037 |         174.8261 |           8.3430 |
[32m[20221213 21:35:42 @agent_ppo2.py:185][0m |          -0.0066 |         170.2652 |           8.3091 |
[32m[20221213 21:35:42 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:35:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.40
[32m[20221213 21:35:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:35:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:35:42 @agent_ppo2.py:143][0m Total time:      40.12 min
[32m[20221213 21:35:42 @agent_ppo2.py:145][0m 3921920 total steps have happened
[32m[20221213 21:35:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1915 --------------------------#
[32m[20221213 21:35:42 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:42 @agent_ppo2.py:185][0m |          -0.0016 |         170.6523 |           8.1708 |
[32m[20221213 21:35:42 @agent_ppo2.py:185][0m |          -0.0051 |         169.0002 |           8.1532 |
[32m[20221213 21:35:42 @agent_ppo2.py:185][0m |          -0.0065 |         168.1703 |           8.1273 |
[32m[20221213 21:35:42 @agent_ppo2.py:185][0m |          -0.0086 |         167.6337 |           8.2105 |
[32m[20221213 21:35:42 @agent_ppo2.py:185][0m |          -0.0079 |         167.4097 |           8.1531 |
[32m[20221213 21:35:43 @agent_ppo2.py:185][0m |           0.0029 |         184.1424 |           8.1349 |
[32m[20221213 21:35:43 @agent_ppo2.py:185][0m |          -0.0103 |         166.6435 |           8.1220 |
[32m[20221213 21:35:43 @agent_ppo2.py:185][0m |          -0.0108 |         166.3596 |           8.1555 |
[32m[20221213 21:35:43 @agent_ppo2.py:185][0m |          -0.0093 |         166.4600 |           8.1543 |
[32m[20221213 21:35:43 @agent_ppo2.py:185][0m |          -0.0102 |         165.9251 |           8.1455 |
[32m[20221213 21:35:43 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:35:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.40
[32m[20221213 21:35:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:35:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:35:43 @agent_ppo2.py:143][0m Total time:      40.14 min
[32m[20221213 21:35:43 @agent_ppo2.py:145][0m 3923968 total steps have happened
[32m[20221213 21:35:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1916 --------------------------#
[32m[20221213 21:35:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:43 @agent_ppo2.py:185][0m |          -0.0005 |         171.1778 |           8.2571 |
[32m[20221213 21:35:43 @agent_ppo2.py:185][0m |          -0.0041 |         169.7914 |           8.1830 |
[32m[20221213 21:35:43 @agent_ppo2.py:185][0m |          -0.0045 |         168.6817 |           8.2079 |
[32m[20221213 21:35:44 @agent_ppo2.py:185][0m |          -0.0030 |         168.0255 |           8.2125 |
[32m[20221213 21:35:44 @agent_ppo2.py:185][0m |          -0.0060 |         167.3023 |           8.1808 |
[32m[20221213 21:35:44 @agent_ppo2.py:185][0m |          -0.0058 |         166.7514 |           8.2351 |
[32m[20221213 21:35:44 @agent_ppo2.py:185][0m |           0.0031 |         188.3357 |           8.2115 |
[32m[20221213 21:35:44 @agent_ppo2.py:185][0m |          -0.0076 |         165.7246 |           8.2394 |
[32m[20221213 21:35:44 @agent_ppo2.py:185][0m |          -0.0104 |         165.1807 |           8.2772 |
[32m[20221213 21:35:44 @agent_ppo2.py:185][0m |          -0.0105 |         164.4305 |           8.2030 |
[32m[20221213 21:35:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:35:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.40
[32m[20221213 21:35:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:35:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221213 21:35:44 @agent_ppo2.py:143][0m Total time:      40.16 min
[32m[20221213 21:35:44 @agent_ppo2.py:145][0m 3926016 total steps have happened
[32m[20221213 21:35:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1917 --------------------------#
[32m[20221213 21:35:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:44 @agent_ppo2.py:185][0m |          -0.0006 |         170.5871 |           7.9145 |
[32m[20221213 21:35:45 @agent_ppo2.py:185][0m |          -0.0025 |         169.4660 |           7.9906 |
[32m[20221213 21:35:45 @agent_ppo2.py:185][0m |          -0.0068 |         168.9524 |           7.9380 |
[32m[20221213 21:35:45 @agent_ppo2.py:185][0m |          -0.0063 |         168.5632 |           7.9548 |
[32m[20221213 21:35:45 @agent_ppo2.py:185][0m |          -0.0091 |         168.3204 |           7.9412 |
[32m[20221213 21:35:45 @agent_ppo2.py:185][0m |          -0.0077 |         168.2271 |           7.8838 |
[32m[20221213 21:35:45 @agent_ppo2.py:185][0m |          -0.0061 |         168.5593 |           7.9369 |
[32m[20221213 21:35:45 @agent_ppo2.py:185][0m |          -0.0082 |         167.8157 |           7.9047 |
[32m[20221213 21:35:45 @agent_ppo2.py:185][0m |          -0.0088 |         167.6314 |           7.9181 |
[32m[20221213 21:35:45 @agent_ppo2.py:185][0m |          -0.0082 |         167.7796 |           7.9113 |
[32m[20221213 21:35:45 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:35:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.80
[32m[20221213 21:35:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:35:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.00
[32m[20221213 21:35:45 @agent_ppo2.py:143][0m Total time:      40.18 min
[32m[20221213 21:35:45 @agent_ppo2.py:145][0m 3928064 total steps have happened
[32m[20221213 21:35:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1918 --------------------------#
[32m[20221213 21:35:46 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:35:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:46 @agent_ppo2.py:185][0m |          -0.0025 |         172.9352 |           7.9604 |
[32m[20221213 21:35:46 @agent_ppo2.py:185][0m |          -0.0051 |         171.2486 |           7.9620 |
[32m[20221213 21:35:46 @agent_ppo2.py:185][0m |           0.0031 |         178.1415 |           8.0017 |
[32m[20221213 21:35:46 @agent_ppo2.py:185][0m |          -0.0078 |         170.4815 |           7.9969 |
[32m[20221213 21:35:46 @agent_ppo2.py:185][0m |          -0.0082 |         169.9341 |           7.9997 |
[32m[20221213 21:35:46 @agent_ppo2.py:185][0m |          -0.0109 |         169.8822 |           7.9681 |
[32m[20221213 21:35:46 @agent_ppo2.py:185][0m |          -0.0026 |         173.0311 |           7.9935 |
[32m[20221213 21:35:46 @agent_ppo2.py:185][0m |          -0.0106 |         169.3123 |           8.0266 |
[32m[20221213 21:35:47 @agent_ppo2.py:185][0m |          -0.0072 |         169.2874 |           7.9436 |
[32m[20221213 21:35:47 @agent_ppo2.py:185][0m |          -0.0093 |         169.2768 |           7.9641 |
[32m[20221213 21:35:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:35:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.80
[32m[20221213 21:35:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.00
[32m[20221213 21:35:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.00
[32m[20221213 21:35:47 @agent_ppo2.py:143][0m Total time:      40.20 min
[32m[20221213 21:35:47 @agent_ppo2.py:145][0m 3930112 total steps have happened
[32m[20221213 21:35:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1919 --------------------------#
[32m[20221213 21:35:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:47 @agent_ppo2.py:185][0m |           0.0008 |         168.9283 |           7.9242 |
[32m[20221213 21:35:47 @agent_ppo2.py:185][0m |           0.0106 |         187.2648 |           8.0235 |
[32m[20221213 21:35:47 @agent_ppo2.py:185][0m |          -0.0002 |         169.3838 |           7.9477 |
[32m[20221213 21:35:47 @agent_ppo2.py:185][0m |          -0.0075 |         167.0403 |           8.0614 |
[32m[20221213 21:35:47 @agent_ppo2.py:185][0m |          -0.0042 |         169.9325 |           8.0619 |
[32m[20221213 21:35:48 @agent_ppo2.py:185][0m |          -0.0023 |         174.2946 |           8.1138 |
[32m[20221213 21:35:48 @agent_ppo2.py:185][0m |          -0.0067 |         166.1425 |           8.1519 |
[32m[20221213 21:35:48 @agent_ppo2.py:185][0m |           0.0043 |         184.9422 |           8.1535 |
[32m[20221213 21:35:48 @agent_ppo2.py:185][0m |          -0.0064 |         167.3673 |           8.1810 |
[32m[20221213 21:35:48 @agent_ppo2.py:185][0m |          -0.0107 |         165.5317 |           8.2168 |
[32m[20221213 21:35:48 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:35:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.60
[32m[20221213 21:35:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:35:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:35:48 @agent_ppo2.py:143][0m Total time:      40.22 min
[32m[20221213 21:35:48 @agent_ppo2.py:145][0m 3932160 total steps have happened
[32m[20221213 21:35:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1920 --------------------------#
[32m[20221213 21:35:48 @agent_ppo2.py:127][0m Sampling time: 0.14 s by 5 slaves
[32m[20221213 21:35:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:48 @agent_ppo2.py:185][0m |          -0.0006 |         173.1827 |           8.0192 |
[32m[20221213 21:35:48 @agent_ppo2.py:185][0m |          -0.0041 |         171.4848 |           8.0686 |
[32m[20221213 21:35:49 @agent_ppo2.py:185][0m |          -0.0007 |         171.5208 |           8.0883 |
[32m[20221213 21:35:49 @agent_ppo2.py:185][0m |          -0.0060 |         169.8246 |           8.0582 |
[32m[20221213 21:35:49 @agent_ppo2.py:185][0m |          -0.0000 |         173.5563 |           8.0648 |
[32m[20221213 21:35:49 @agent_ppo2.py:185][0m |          -0.0072 |         169.0855 |           8.0716 |
[32m[20221213 21:35:49 @agent_ppo2.py:185][0m |          -0.0075 |         168.5918 |           8.0679 |
[32m[20221213 21:35:49 @agent_ppo2.py:185][0m |          -0.0062 |         168.2509 |           8.0035 |
[32m[20221213 21:35:49 @agent_ppo2.py:185][0m |          -0.0073 |         167.8534 |           8.0447 |
[32m[20221213 21:35:49 @agent_ppo2.py:185][0m |          -0.0088 |         167.5874 |           7.9806 |
[32m[20221213 21:35:49 @agent_ppo2.py:130][0m Policy update time: 1.35 s
[32m[20221213 21:35:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.80
[32m[20221213 21:35:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:35:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 707.00
[32m[20221213 21:35:50 @agent_ppo2.py:143][0m Total time:      40.25 min
[32m[20221213 21:35:50 @agent_ppo2.py:145][0m 3934208 total steps have happened
[32m[20221213 21:35:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1921 --------------------------#
[32m[20221213 21:35:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 21:35:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:50 @agent_ppo2.py:185][0m |          -0.0015 |         175.8442 |           7.7043 |
[32m[20221213 21:35:50 @agent_ppo2.py:185][0m |          -0.0031 |         174.7489 |           7.7597 |
[32m[20221213 21:35:50 @agent_ppo2.py:185][0m |          -0.0065 |         174.2587 |           7.7329 |
[32m[20221213 21:35:50 @agent_ppo2.py:185][0m |          -0.0050 |         173.8932 |           7.7214 |
[32m[20221213 21:35:50 @agent_ppo2.py:185][0m |          -0.0085 |         173.2730 |           7.7339 |
[32m[20221213 21:35:50 @agent_ppo2.py:185][0m |          -0.0057 |         173.5883 |           7.7704 |
[32m[20221213 21:35:51 @agent_ppo2.py:185][0m |          -0.0006 |         183.0064 |           7.7257 |
[32m[20221213 21:35:51 @agent_ppo2.py:185][0m |          -0.0084 |         172.6651 |           7.7159 |
[32m[20221213 21:35:51 @agent_ppo2.py:185][0m |          -0.0036 |         175.6131 |           7.7385 |
[32m[20221213 21:35:51 @agent_ppo2.py:185][0m |          -0.0094 |         172.5845 |           7.7341 |
[32m[20221213 21:35:51 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:35:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.20
[32m[20221213 21:35:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.00
[32m[20221213 21:35:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.00
[32m[20221213 21:35:51 @agent_ppo2.py:143][0m Total time:      40.27 min
[32m[20221213 21:35:51 @agent_ppo2.py:145][0m 3936256 total steps have happened
[32m[20221213 21:35:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1922 --------------------------#
[32m[20221213 21:35:51 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:35:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:51 @agent_ppo2.py:185][0m |           0.0005 |         174.2747 |           7.5685 |
[32m[20221213 21:35:51 @agent_ppo2.py:185][0m |          -0.0034 |         172.4631 |           7.5746 |
[32m[20221213 21:35:51 @agent_ppo2.py:185][0m |           0.0089 |         197.2046 |           7.5878 |
[32m[20221213 21:35:51 @agent_ppo2.py:185][0m |           0.0005 |         171.4136 |           7.6735 |
[32m[20221213 21:35:52 @agent_ppo2.py:185][0m |           0.0066 |         188.5389 |           7.5964 |
[32m[20221213 21:35:52 @agent_ppo2.py:185][0m |           0.0037 |         183.3691 |           7.6218 |
[32m[20221213 21:35:52 @agent_ppo2.py:185][0m |           0.0056 |         179.1110 |           7.6087 |
[32m[20221213 21:35:52 @agent_ppo2.py:185][0m |          -0.0073 |         170.7554 |           7.5717 |
[32m[20221213 21:35:52 @agent_ppo2.py:185][0m |          -0.0058 |         170.4493 |           7.5910 |
[32m[20221213 21:35:52 @agent_ppo2.py:185][0m |          -0.0053 |         170.1761 |           7.6259 |
[32m[20221213 21:35:52 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 21:35:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.80
[32m[20221213 21:35:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.00
[32m[20221213 21:35:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 21:35:52 @agent_ppo2.py:143][0m Total time:      40.29 min
[32m[20221213 21:35:52 @agent_ppo2.py:145][0m 3938304 total steps have happened
[32m[20221213 21:35:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1923 --------------------------#
[32m[20221213 21:35:52 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:52 @agent_ppo2.py:185][0m |           0.0009 |         169.5024 |           7.7978 |
[32m[20221213 21:35:53 @agent_ppo2.py:185][0m |          -0.0043 |         168.7746 |           7.8453 |
[32m[20221213 21:35:53 @agent_ppo2.py:185][0m |          -0.0038 |         168.4227 |           7.9024 |
[32m[20221213 21:35:53 @agent_ppo2.py:185][0m |          -0.0058 |         168.1012 |           7.8505 |
[32m[20221213 21:35:53 @agent_ppo2.py:185][0m |          -0.0049 |         167.7949 |           7.8494 |
[32m[20221213 21:35:53 @agent_ppo2.py:185][0m |          -0.0048 |         167.5483 |           7.8518 |
[32m[20221213 21:35:53 @agent_ppo2.py:185][0m |          -0.0064 |         167.3930 |           7.8812 |
[32m[20221213 21:35:53 @agent_ppo2.py:185][0m |          -0.0081 |         167.1155 |           7.8978 |
[32m[20221213 21:35:53 @agent_ppo2.py:185][0m |          -0.0060 |         166.8470 |           7.9152 |
[32m[20221213 21:35:53 @agent_ppo2.py:185][0m |           0.0020 |         176.6614 |           7.8934 |
[32m[20221213 21:35:53 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:35:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.00
[32m[20221213 21:35:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:35:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 758.00
[32m[20221213 21:35:53 @agent_ppo2.py:143][0m Total time:      40.31 min
[32m[20221213 21:35:53 @agent_ppo2.py:145][0m 3940352 total steps have happened
[32m[20221213 21:35:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1924 --------------------------#
[32m[20221213 21:35:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 21:35:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:54 @agent_ppo2.py:185][0m |           0.0009 |         170.4926 |           7.8928 |
[32m[20221213 21:35:54 @agent_ppo2.py:185][0m |           0.0027 |         174.4120 |           7.8853 |
[32m[20221213 21:35:54 @agent_ppo2.py:185][0m |          -0.0055 |         168.5900 |           7.9288 |
[32m[20221213 21:35:54 @agent_ppo2.py:185][0m |          -0.0015 |         168.8921 |           7.9231 |
[32m[20221213 21:35:54 @agent_ppo2.py:185][0m |          -0.0066 |         167.8695 |           7.9569 |
[32m[20221213 21:35:54 @agent_ppo2.py:185][0m |          -0.0062 |         167.5562 |           7.9458 |
[32m[20221213 21:35:54 @agent_ppo2.py:185][0m |          -0.0047 |         167.4703 |           7.9364 |
[32m[20221213 21:35:54 @agent_ppo2.py:185][0m |          -0.0048 |         168.8708 |           7.9358 |
[32m[20221213 21:35:55 @agent_ppo2.py:185][0m |          -0.0074 |         166.8521 |           7.9850 |
[32m[20221213 21:35:55 @agent_ppo2.py:185][0m |          -0.0053 |         168.1667 |           8.0326 |
[32m[20221213 21:35:55 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:35:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.60
[32m[20221213 21:35:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.00
[32m[20221213 21:35:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.00
[32m[20221213 21:35:55 @agent_ppo2.py:143][0m Total time:      40.33 min
[32m[20221213 21:35:55 @agent_ppo2.py:145][0m 3942400 total steps have happened
[32m[20221213 21:35:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1925 --------------------------#
[32m[20221213 21:35:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:55 @agent_ppo2.py:185][0m |          -0.0025 |         175.8242 |           8.0363 |
[32m[20221213 21:35:55 @agent_ppo2.py:185][0m |          -0.0015 |         176.6663 |           8.0833 |
[32m[20221213 21:35:55 @agent_ppo2.py:185][0m |          -0.0087 |         174.2077 |           8.0526 |
[32m[20221213 21:35:55 @agent_ppo2.py:185][0m |          -0.0021 |         179.1669 |           7.9744 |
[32m[20221213 21:35:55 @agent_ppo2.py:185][0m |          -0.0108 |         173.3530 |           8.0077 |
[32m[20221213 21:35:55 @agent_ppo2.py:185][0m |          -0.0057 |         179.1191 |           7.9686 |
[32m[20221213 21:35:56 @agent_ppo2.py:185][0m |          -0.0100 |         172.9058 |           7.9457 |
[32m[20221213 21:35:56 @agent_ppo2.py:185][0m |          -0.0109 |         172.7507 |           7.9628 |
[32m[20221213 21:35:56 @agent_ppo2.py:185][0m |          -0.0108 |         172.4276 |           7.9195 |
[32m[20221213 21:35:56 @agent_ppo2.py:185][0m |          -0.0113 |         172.3235 |           7.9350 |
[32m[20221213 21:35:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:35:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.80
[32m[20221213 21:35:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.00
[32m[20221213 21:35:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.00
[32m[20221213 21:35:56 @agent_ppo2.py:143][0m Total time:      40.35 min
[32m[20221213 21:35:56 @agent_ppo2.py:145][0m 3944448 total steps have happened
[32m[20221213 21:35:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1926 --------------------------#
[32m[20221213 21:35:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:56 @agent_ppo2.py:185][0m |           0.0058 |         176.5147 |           7.7011 |
[32m[20221213 21:35:56 @agent_ppo2.py:185][0m |          -0.0035 |         173.0210 |           7.7250 |
[32m[20221213 21:35:56 @agent_ppo2.py:185][0m |          -0.0043 |         172.4583 |           7.7313 |
[32m[20221213 21:35:57 @agent_ppo2.py:185][0m |          -0.0055 |         171.9557 |           7.7619 |
[32m[20221213 21:35:57 @agent_ppo2.py:185][0m |          -0.0060 |         171.7943 |           7.7505 |
[32m[20221213 21:35:57 @agent_ppo2.py:185][0m |          -0.0066 |         171.5231 |           7.7799 |
[32m[20221213 21:35:57 @agent_ppo2.py:185][0m |          -0.0080 |         171.2213 |           7.8211 |
[32m[20221213 21:35:57 @agent_ppo2.py:185][0m |          -0.0087 |         171.0628 |           7.8055 |
[32m[20221213 21:35:57 @agent_ppo2.py:185][0m |          -0.0005 |         183.0140 |           7.8510 |
[32m[20221213 21:35:57 @agent_ppo2.py:185][0m |          -0.0077 |         170.7592 |           7.8667 |
[32m[20221213 21:35:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:35:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.60
[32m[20221213 21:35:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:35:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 21:35:57 @agent_ppo2.py:143][0m Total time:      40.37 min
[32m[20221213 21:35:57 @agent_ppo2.py:145][0m 3946496 total steps have happened
[32m[20221213 21:35:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1927 --------------------------#
[32m[20221213 21:35:57 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:57 @agent_ppo2.py:185][0m |           0.0002 |         175.3814 |           7.7666 |
[32m[20221213 21:35:58 @agent_ppo2.py:185][0m |           0.0022 |         176.2617 |           7.7615 |
[32m[20221213 21:35:58 @agent_ppo2.py:185][0m |          -0.0054 |         171.3274 |           7.7635 |
[32m[20221213 21:35:58 @agent_ppo2.py:185][0m |          -0.0059 |         170.2651 |           7.8015 |
[32m[20221213 21:35:58 @agent_ppo2.py:185][0m |          -0.0041 |         169.3970 |           7.7874 |
[32m[20221213 21:35:58 @agent_ppo2.py:185][0m |           0.0021 |         175.3804 |           7.8084 |
[32m[20221213 21:35:58 @agent_ppo2.py:185][0m |          -0.0044 |         168.5692 |           7.8076 |
[32m[20221213 21:35:58 @agent_ppo2.py:185][0m |          -0.0071 |         167.8135 |           7.8039 |
[32m[20221213 21:35:58 @agent_ppo2.py:185][0m |          -0.0095 |         167.5287 |           7.7632 |
[32m[20221213 21:35:58 @agent_ppo2.py:185][0m |          -0.0083 |         167.2503 |           7.7392 |
[32m[20221213 21:35:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:35:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.60
[32m[20221213 21:35:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:35:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:35:58 @agent_ppo2.py:143][0m Total time:      40.39 min
[32m[20221213 21:35:58 @agent_ppo2.py:145][0m 3948544 total steps have happened
[32m[20221213 21:35:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1928 --------------------------#
[32m[20221213 21:35:58 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:35:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:35:59 @agent_ppo2.py:185][0m |          -0.0002 |         178.0756 |           7.8313 |
[32m[20221213 21:35:59 @agent_ppo2.py:185][0m |           0.0067 |         184.6340 |           7.8459 |
[32m[20221213 21:35:59 @agent_ppo2.py:185][0m |           0.0066 |         196.2231 |           7.7740 |
[32m[20221213 21:35:59 @agent_ppo2.py:185][0m |          -0.0053 |         172.6616 |           7.8311 |
[32m[20221213 21:35:59 @agent_ppo2.py:185][0m |          -0.0118 |         171.6941 |           7.8315 |
[32m[20221213 21:35:59 @agent_ppo2.py:185][0m |          -0.0115 |         170.8919 |           7.8256 |
[32m[20221213 21:35:59 @agent_ppo2.py:185][0m |          -0.0098 |         170.3720 |           7.8221 |
[32m[20221213 21:35:59 @agent_ppo2.py:185][0m |          -0.0075 |         172.6859 |           7.7743 |
[32m[20221213 21:35:59 @agent_ppo2.py:185][0m |          -0.0128 |         169.7390 |           7.8266 |
[32m[20221213 21:35:59 @agent_ppo2.py:185][0m |          -0.0089 |         170.6794 |           7.8020 |
[32m[20221213 21:35:59 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:36:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.80
[32m[20221213 21:36:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:36:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.00
[32m[20221213 21:36:00 @agent_ppo2.py:143][0m Total time:      40.41 min
[32m[20221213 21:36:00 @agent_ppo2.py:145][0m 3950592 total steps have happened
[32m[20221213 21:36:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1929 --------------------------#
[32m[20221213 21:36:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:00 @agent_ppo2.py:185][0m |          -0.0011 |         177.2664 |           7.4401 |
[32m[20221213 21:36:00 @agent_ppo2.py:185][0m |          -0.0050 |         176.2957 |           7.3969 |
[32m[20221213 21:36:00 @agent_ppo2.py:185][0m |           0.0025 |         180.8976 |           7.3819 |
[32m[20221213 21:36:00 @agent_ppo2.py:185][0m |          -0.0058 |         175.8159 |           7.4657 |
[32m[20221213 21:36:00 @agent_ppo2.py:185][0m |          -0.0080 |         175.4808 |           7.3877 |
[32m[20221213 21:36:00 @agent_ppo2.py:185][0m |           0.0068 |         202.0681 |           7.4287 |
[32m[20221213 21:36:00 @agent_ppo2.py:185][0m |          -0.0067 |         175.1985 |           7.5198 |
[32m[20221213 21:36:00 @agent_ppo2.py:185][0m |          -0.0094 |         174.8963 |           7.4839 |
[32m[20221213 21:36:01 @agent_ppo2.py:185][0m |          -0.0100 |         174.8648 |           7.4471 |
[32m[20221213 21:36:01 @agent_ppo2.py:185][0m |          -0.0103 |         174.7073 |           7.5070 |
[32m[20221213 21:36:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:36:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.20
[32m[20221213 21:36:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:36:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.00
[32m[20221213 21:36:01 @agent_ppo2.py:143][0m Total time:      40.43 min
[32m[20221213 21:36:01 @agent_ppo2.py:145][0m 3952640 total steps have happened
[32m[20221213 21:36:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1930 --------------------------#
[32m[20221213 21:36:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:01 @agent_ppo2.py:185][0m |          -0.0016 |         178.1925 |           8.1716 |
[32m[20221213 21:36:01 @agent_ppo2.py:185][0m |          -0.0012 |         177.3624 |           8.1976 |
[32m[20221213 21:36:01 @agent_ppo2.py:185][0m |          -0.0008 |         180.0859 |           8.1845 |
[32m[20221213 21:36:01 @agent_ppo2.py:185][0m |          -0.0020 |         176.4669 |           8.2026 |
[32m[20221213 21:36:01 @agent_ppo2.py:185][0m |           0.0011 |         177.7486 |           8.1376 |
[32m[20221213 21:36:02 @agent_ppo2.py:185][0m |          -0.0072 |         175.8636 |           8.1543 |
[32m[20221213 21:36:02 @agent_ppo2.py:185][0m |          -0.0073 |         175.6385 |           8.1912 |
[32m[20221213 21:36:02 @agent_ppo2.py:185][0m |          -0.0083 |         175.5522 |           8.1485 |
[32m[20221213 21:36:02 @agent_ppo2.py:185][0m |          -0.0080 |         175.6240 |           8.2024 |
[32m[20221213 21:36:02 @agent_ppo2.py:185][0m |          -0.0043 |         177.0745 |           8.1590 |
[32m[20221213 21:36:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:36:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.00
[32m[20221213 21:36:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 850.00
[32m[20221213 21:36:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 862.00
[32m[20221213 21:36:02 @agent_ppo2.py:143][0m Total time:      40.46 min
[32m[20221213 21:36:02 @agent_ppo2.py:145][0m 3954688 total steps have happened
[32m[20221213 21:36:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1931 --------------------------#
[32m[20221213 21:36:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:02 @agent_ppo2.py:185][0m |          -0.0020 |         172.1423 |           8.2720 |
[32m[20221213 21:36:02 @agent_ppo2.py:185][0m |          -0.0056 |         171.3255 |           8.2747 |
[32m[20221213 21:36:02 @agent_ppo2.py:185][0m |          -0.0047 |         170.5967 |           8.2916 |
[32m[20221213 21:36:03 @agent_ppo2.py:185][0m |          -0.0059 |         169.9552 |           8.2989 |
[32m[20221213 21:36:03 @agent_ppo2.py:185][0m |          -0.0063 |         169.3061 |           8.2802 |
[32m[20221213 21:36:03 @agent_ppo2.py:185][0m |          -0.0072 |         169.0239 |           8.3174 |
[32m[20221213 21:36:03 @agent_ppo2.py:185][0m |          -0.0084 |         168.5660 |           8.3403 |
[32m[20221213 21:36:03 @agent_ppo2.py:185][0m |          -0.0087 |         168.1250 |           8.3469 |
[32m[20221213 21:36:03 @agent_ppo2.py:185][0m |          -0.0075 |         167.6036 |           8.3446 |
[32m[20221213 21:36:03 @agent_ppo2.py:185][0m |          -0.0035 |         169.0749 |           8.4156 |
[32m[20221213 21:36:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:36:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.40
[32m[20221213 21:36:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.00
[32m[20221213 21:36:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.00
[32m[20221213 21:36:03 @agent_ppo2.py:143][0m Total time:      40.48 min
[32m[20221213 21:36:03 @agent_ppo2.py:145][0m 3956736 total steps have happened
[32m[20221213 21:36:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1932 --------------------------#
[32m[20221213 21:36:03 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:04 @agent_ppo2.py:185][0m |          -0.0018 |         177.3372 |           8.0581 |
[32m[20221213 21:36:04 @agent_ppo2.py:185][0m |          -0.0050 |         174.9778 |           8.0429 |
[32m[20221213 21:36:04 @agent_ppo2.py:185][0m |           0.0013 |         178.8538 |           8.0491 |
[32m[20221213 21:36:04 @agent_ppo2.py:185][0m |          -0.0072 |         172.8298 |           8.0673 |
[32m[20221213 21:36:04 @agent_ppo2.py:185][0m |          -0.0076 |         172.1772 |           8.0819 |
[32m[20221213 21:36:04 @agent_ppo2.py:185][0m |          -0.0088 |         172.0472 |           8.0702 |
[32m[20221213 21:36:04 @agent_ppo2.py:185][0m |          -0.0103 |         171.1214 |           8.0442 |
[32m[20221213 21:36:04 @agent_ppo2.py:185][0m |          -0.0085 |         171.2534 |           8.1071 |
[32m[20221213 21:36:04 @agent_ppo2.py:185][0m |          -0.0009 |         175.9116 |           8.1098 |
[32m[20221213 21:36:04 @agent_ppo2.py:185][0m |          -0.0032 |         173.1146 |           8.1386 |
[32m[20221213 21:36:04 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 21:36:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.80
[32m[20221213 21:36:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:36:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.00
[32m[20221213 21:36:05 @agent_ppo2.py:143][0m Total time:      40.50 min
[32m[20221213 21:36:05 @agent_ppo2.py:145][0m 3958784 total steps have happened
[32m[20221213 21:36:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1933 --------------------------#
[32m[20221213 21:36:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:05 @agent_ppo2.py:185][0m |          -0.0013 |         181.3474 |           8.2579 |
[32m[20221213 21:36:05 @agent_ppo2.py:185][0m |          -0.0040 |         179.0716 |           8.1985 |
[32m[20221213 21:36:05 @agent_ppo2.py:185][0m |          -0.0053 |         178.1747 |           8.2309 |
[32m[20221213 21:36:05 @agent_ppo2.py:185][0m |          -0.0054 |         177.8609 |           8.2002 |
[32m[20221213 21:36:05 @agent_ppo2.py:185][0m |          -0.0067 |         177.5626 |           8.2008 |
[32m[20221213 21:36:05 @agent_ppo2.py:185][0m |           0.0055 |         188.1174 |           8.1794 |
[32m[20221213 21:36:06 @agent_ppo2.py:185][0m |          -0.0065 |         176.8876 |           8.1682 |
[32m[20221213 21:36:06 @agent_ppo2.py:185][0m |           0.0139 |         199.1478 |           8.1894 |
[32m[20221213 21:36:06 @agent_ppo2.py:185][0m |          -0.0075 |         176.6373 |           8.2000 |
[32m[20221213 21:36:06 @agent_ppo2.py:185][0m |          -0.0050 |         176.3270 |           8.1925 |
[32m[20221213 21:36:06 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:36:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.80
[32m[20221213 21:36:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:36:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.00
[32m[20221213 21:36:06 @agent_ppo2.py:143][0m Total time:      40.52 min
[32m[20221213 21:36:06 @agent_ppo2.py:145][0m 3960832 total steps have happened
[32m[20221213 21:36:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1934 --------------------------#
[32m[20221213 21:36:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:36:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:06 @agent_ppo2.py:185][0m |          -0.0010 |         173.3999 |           7.9844 |
[32m[20221213 21:36:06 @agent_ppo2.py:185][0m |          -0.0013 |         173.8307 |           7.9807 |
[32m[20221213 21:36:06 @agent_ppo2.py:185][0m |          -0.0071 |         171.6055 |           7.9954 |
[32m[20221213 21:36:07 @agent_ppo2.py:185][0m |          -0.0088 |         171.1639 |           8.0513 |
[32m[20221213 21:36:07 @agent_ppo2.py:185][0m |          -0.0078 |         170.6119 |           8.0556 |
[32m[20221213 21:36:07 @agent_ppo2.py:185][0m |          -0.0098 |         170.1575 |           8.0785 |
[32m[20221213 21:36:07 @agent_ppo2.py:185][0m |          -0.0091 |         169.8425 |           8.0749 |
[32m[20221213 21:36:07 @agent_ppo2.py:185][0m |          -0.0096 |         169.4535 |           8.0881 |
[32m[20221213 21:36:07 @agent_ppo2.py:185][0m |          -0.0108 |         169.2102 |           8.0655 |
[32m[20221213 21:36:07 @agent_ppo2.py:185][0m |          -0.0109 |         168.8679 |           8.1179 |
[32m[20221213 21:36:07 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:36:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.80
[32m[20221213 21:36:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.00
[32m[20221213 21:36:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.00
[32m[20221213 21:36:07 @agent_ppo2.py:143][0m Total time:      40.54 min
[32m[20221213 21:36:07 @agent_ppo2.py:145][0m 3962880 total steps have happened
[32m[20221213 21:36:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1935 --------------------------#
[32m[20221213 21:36:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:08 @agent_ppo2.py:185][0m |           0.0056 |         182.9623 |           8.1358 |
[32m[20221213 21:36:08 @agent_ppo2.py:185][0m |          -0.0065 |         174.7464 |           8.1234 |
[32m[20221213 21:36:08 @agent_ppo2.py:185][0m |          -0.0083 |         173.2433 |           8.1346 |
[32m[20221213 21:36:08 @agent_ppo2.py:185][0m |          -0.0092 |         172.2560 |           8.1540 |
[32m[20221213 21:36:08 @agent_ppo2.py:185][0m |          -0.0095 |         171.4401 |           8.1988 |
[32m[20221213 21:36:08 @agent_ppo2.py:185][0m |          -0.0092 |         171.3196 |           8.1582 |
[32m[20221213 21:36:08 @agent_ppo2.py:185][0m |          -0.0084 |         170.4738 |           8.1793 |
[32m[20221213 21:36:08 @agent_ppo2.py:185][0m |          -0.0110 |         170.0749 |           8.2129 |
[32m[20221213 21:36:08 @agent_ppo2.py:185][0m |          -0.0072 |         170.4834 |           8.1911 |
[32m[20221213 21:36:08 @agent_ppo2.py:185][0m |          -0.0110 |         169.3840 |           8.2142 |
[32m[20221213 21:36:08 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 21:36:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.40
[32m[20221213 21:36:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.00
[32m[20221213 21:36:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.00
[32m[20221213 21:36:09 @agent_ppo2.py:143][0m Total time:      40.57 min
[32m[20221213 21:36:09 @agent_ppo2.py:145][0m 3964928 total steps have happened
[32m[20221213 21:36:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1936 --------------------------#
[32m[20221213 21:36:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:09 @agent_ppo2.py:185][0m |           0.0081 |         177.9081 |           8.5998 |
[32m[20221213 21:36:09 @agent_ppo2.py:185][0m |           0.0022 |         176.4148 |           8.5514 |
[32m[20221213 21:36:09 @agent_ppo2.py:185][0m |           0.0011 |         176.0148 |           8.5252 |
[32m[20221213 21:36:09 @agent_ppo2.py:185][0m |          -0.0011 |         174.4426 |           8.4885 |
[32m[20221213 21:36:09 @agent_ppo2.py:185][0m |          -0.0088 |         170.1509 |           8.4564 |
[32m[20221213 21:36:09 @agent_ppo2.py:185][0m |          -0.0091 |         169.4825 |           8.4594 |
[32m[20221213 21:36:09 @agent_ppo2.py:185][0m |          -0.0092 |         169.1112 |           8.4377 |
[32m[20221213 21:36:10 @agent_ppo2.py:185][0m |          -0.0066 |         170.1642 |           8.3722 |
[32m[20221213 21:36:10 @agent_ppo2.py:185][0m |          -0.0062 |         171.8784 |           8.4015 |
[32m[20221213 21:36:10 @agent_ppo2.py:185][0m |          -0.0077 |         169.1989 |           8.3475 |
[32m[20221213 21:36:10 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:36:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.60
[32m[20221213 21:36:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.00
[32m[20221213 21:36:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:36:10 @agent_ppo2.py:143][0m Total time:      40.59 min
[32m[20221213 21:36:10 @agent_ppo2.py:145][0m 3966976 total steps have happened
[32m[20221213 21:36:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1937 --------------------------#
[32m[20221213 21:36:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:36:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:10 @agent_ppo2.py:185][0m |          -0.0006 |         173.3600 |           8.0816 |
[32m[20221213 21:36:10 @agent_ppo2.py:185][0m |           0.0077 |         194.6175 |           8.1655 |
[32m[20221213 21:36:10 @agent_ppo2.py:185][0m |          -0.0059 |         170.8473 |           8.1853 |
[32m[20221213 21:36:10 @agent_ppo2.py:185][0m |          -0.0046 |         170.4033 |           8.1427 |
[32m[20221213 21:36:11 @agent_ppo2.py:185][0m |          -0.0061 |         170.0643 |           8.0739 |
[32m[20221213 21:36:11 @agent_ppo2.py:185][0m |          -0.0092 |         169.7055 |           8.1066 |
[32m[20221213 21:36:11 @agent_ppo2.py:185][0m |          -0.0084 |         169.4699 |           8.0105 |
[32m[20221213 21:36:11 @agent_ppo2.py:185][0m |          -0.0096 |         169.2955 |           8.0163 |
[32m[20221213 21:36:11 @agent_ppo2.py:185][0m |          -0.0076 |         169.2013 |           8.0138 |
[32m[20221213 21:36:11 @agent_ppo2.py:185][0m |          -0.0096 |         169.0140 |           7.9840 |
[32m[20221213 21:36:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:36:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.00
[32m[20221213 21:36:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:36:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.00
[32m[20221213 21:36:11 @agent_ppo2.py:143][0m Total time:      40.61 min
[32m[20221213 21:36:11 @agent_ppo2.py:145][0m 3969024 total steps have happened
[32m[20221213 21:36:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1938 --------------------------#
[32m[20221213 21:36:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:11 @agent_ppo2.py:185][0m |          -0.0001 |         171.8709 |           7.4990 |
[32m[20221213 21:36:11 @agent_ppo2.py:185][0m |          -0.0037 |         170.6047 |           7.5232 |
[32m[20221213 21:36:12 @agent_ppo2.py:185][0m |          -0.0030 |         170.0369 |           7.4670 |
[32m[20221213 21:36:12 @agent_ppo2.py:185][0m |          -0.0050 |         169.6607 |           7.4694 |
[32m[20221213 21:36:12 @agent_ppo2.py:185][0m |          -0.0051 |         168.8658 |           7.4352 |
[32m[20221213 21:36:12 @agent_ppo2.py:185][0m |          -0.0057 |         168.9223 |           7.4310 |
[32m[20221213 21:36:12 @agent_ppo2.py:185][0m |          -0.0092 |         168.4176 |           7.3944 |
[32m[20221213 21:36:12 @agent_ppo2.py:185][0m |          -0.0102 |         168.3339 |           7.4125 |
[32m[20221213 21:36:12 @agent_ppo2.py:185][0m |          -0.0021 |         170.8214 |           7.4364 |
[32m[20221213 21:36:12 @agent_ppo2.py:185][0m |          -0.0067 |         167.7731 |           7.3880 |
[32m[20221213 21:36:12 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:36:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 763.20
[32m[20221213 21:36:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.00
[32m[20221213 21:36:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.00
[32m[20221213 21:36:12 @agent_ppo2.py:143][0m Total time:      40.63 min
[32m[20221213 21:36:12 @agent_ppo2.py:145][0m 3971072 total steps have happened
[32m[20221213 21:36:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1939 --------------------------#
[32m[20221213 21:36:12 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:13 @agent_ppo2.py:185][0m |           0.0047 |         174.2400 |           7.3217 |
[32m[20221213 21:36:13 @agent_ppo2.py:185][0m |          -0.0026 |         169.2162 |           7.3577 |
[32m[20221213 21:36:13 @agent_ppo2.py:185][0m |          -0.0056 |         167.2529 |           7.3546 |
[32m[20221213 21:36:13 @agent_ppo2.py:185][0m |          -0.0048 |         166.4612 |           7.3595 |
[32m[20221213 21:36:13 @agent_ppo2.py:185][0m |          -0.0031 |         166.3491 |           7.4059 |
[32m[20221213 21:36:13 @agent_ppo2.py:185][0m |          -0.0068 |         165.4831 |           7.4073 |
[32m[20221213 21:36:13 @agent_ppo2.py:185][0m |          -0.0078 |         165.2766 |           7.3589 |
[32m[20221213 21:36:13 @agent_ppo2.py:185][0m |          -0.0090 |         164.7849 |           7.3959 |
[32m[20221213 21:36:13 @agent_ppo2.py:185][0m |          -0.0104 |         165.1030 |           7.4342 |
[32m[20221213 21:36:13 @agent_ppo2.py:185][0m |          -0.0103 |         164.5991 |           7.4511 |
[32m[20221213 21:36:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:36:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.60
[32m[20221213 21:36:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:36:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.00
[32m[20221213 21:36:13 @agent_ppo2.py:143][0m Total time:      40.65 min
[32m[20221213 21:36:13 @agent_ppo2.py:145][0m 3973120 total steps have happened
[32m[20221213 21:36:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1940 --------------------------#
[32m[20221213 21:36:14 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:14 @agent_ppo2.py:185][0m |          -0.0022 |         173.0849 |           7.5618 |
[32m[20221213 21:36:14 @agent_ppo2.py:185][0m |           0.0005 |         169.9279 |           7.5613 |
[32m[20221213 21:36:14 @agent_ppo2.py:185][0m |          -0.0053 |         168.3044 |           7.5397 |
[32m[20221213 21:36:14 @agent_ppo2.py:185][0m |          -0.0060 |         167.4502 |           7.5712 |
[32m[20221213 21:36:14 @agent_ppo2.py:185][0m |          -0.0055 |         166.8077 |           7.5819 |
[32m[20221213 21:36:14 @agent_ppo2.py:185][0m |          -0.0062 |         166.5635 |           7.5664 |
[32m[20221213 21:36:14 @agent_ppo2.py:185][0m |          -0.0073 |         166.2888 |           7.5560 |
[32m[20221213 21:36:14 @agent_ppo2.py:185][0m |          -0.0065 |         166.2103 |           7.5745 |
[32m[20221213 21:36:14 @agent_ppo2.py:185][0m |           0.0034 |         184.9881 |           7.5480 |
[32m[20221213 21:36:15 @agent_ppo2.py:185][0m |          -0.0059 |         166.1157 |           7.6501 |
[32m[20221213 21:36:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:36:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.80
[32m[20221213 21:36:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.00
[32m[20221213 21:36:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.00
[32m[20221213 21:36:15 @agent_ppo2.py:143][0m Total time:      40.67 min
[32m[20221213 21:36:15 @agent_ppo2.py:145][0m 3975168 total steps have happened
[32m[20221213 21:36:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1941 --------------------------#
[32m[20221213 21:36:15 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:15 @agent_ppo2.py:185][0m |          -0.0006 |         171.7764 |           7.4370 |
[32m[20221213 21:36:15 @agent_ppo2.py:185][0m |          -0.0035 |         171.3260 |           7.4848 |
[32m[20221213 21:36:15 @agent_ppo2.py:185][0m |           0.0008 |         171.5265 |           7.5573 |
[32m[20221213 21:36:15 @agent_ppo2.py:185][0m |           0.0074 |         192.2708 |           7.6004 |
[32m[20221213 21:36:15 @agent_ppo2.py:185][0m |          -0.0038 |         170.4936 |           7.7138 |
[32m[20221213 21:36:15 @agent_ppo2.py:185][0m |          -0.0024 |         171.9165 |           7.6792 |
[32m[20221213 21:36:15 @agent_ppo2.py:185][0m |          -0.0062 |         170.1298 |           7.6737 |
[32m[20221213 21:36:16 @agent_ppo2.py:185][0m |           0.0014 |         172.8651 |           7.7048 |
[32m[20221213 21:36:16 @agent_ppo2.py:185][0m |          -0.0072 |         169.8424 |           7.7455 |
[32m[20221213 21:36:16 @agent_ppo2.py:185][0m |          -0.0040 |         170.8856 |           7.7473 |
[32m[20221213 21:36:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:36:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.20
[32m[20221213 21:36:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221213 21:36:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 860.00
[32m[20221213 21:36:16 @agent_ppo2.py:143][0m Total time:      40.69 min
[32m[20221213 21:36:16 @agent_ppo2.py:145][0m 3977216 total steps have happened
[32m[20221213 21:36:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1942 --------------------------#
[32m[20221213 21:36:16 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:16 @agent_ppo2.py:185][0m |          -0.0018 |         173.5295 |           7.4175 |
[32m[20221213 21:36:16 @agent_ppo2.py:185][0m |          -0.0053 |         170.4425 |           7.3989 |
[32m[20221213 21:36:16 @agent_ppo2.py:185][0m |          -0.0073 |         168.8258 |           7.4059 |
[32m[20221213 21:36:16 @agent_ppo2.py:185][0m |          -0.0064 |         167.5362 |           7.4032 |
[32m[20221213 21:36:17 @agent_ppo2.py:185][0m |          -0.0058 |         167.6212 |           7.4304 |
[32m[20221213 21:36:17 @agent_ppo2.py:185][0m |           0.0017 |         181.1666 |           7.3829 |
[32m[20221213 21:36:17 @agent_ppo2.py:185][0m |          -0.0059 |         164.8788 |           7.4432 |
[32m[20221213 21:36:17 @agent_ppo2.py:185][0m |          -0.0094 |         163.1707 |           7.3985 |
[32m[20221213 21:36:17 @agent_ppo2.py:185][0m |          -0.0079 |         162.6943 |           7.4322 |
[32m[20221213 21:36:17 @agent_ppo2.py:185][0m |          -0.0092 |         161.9956 |           7.3734 |
[32m[20221213 21:36:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:36:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.20
[32m[20221213 21:36:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.00
[32m[20221213 21:36:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 750.00
[32m[20221213 21:36:17 @agent_ppo2.py:143][0m Total time:      40.71 min
[32m[20221213 21:36:17 @agent_ppo2.py:145][0m 3979264 total steps have happened
[32m[20221213 21:36:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1943 --------------------------#
[32m[20221213 21:36:17 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:17 @agent_ppo2.py:185][0m |          -0.0008 |         172.6437 |           7.7870 |
[32m[20221213 21:36:17 @agent_ppo2.py:185][0m |          -0.0029 |         168.7202 |           7.8796 |
[32m[20221213 21:36:18 @agent_ppo2.py:185][0m |          -0.0065 |         166.2787 |           7.9174 |
[32m[20221213 21:36:18 @agent_ppo2.py:185][0m |          -0.0042 |         165.1252 |           7.9215 |
[32m[20221213 21:36:18 @agent_ppo2.py:185][0m |          -0.0083 |         162.1750 |           7.9491 |
[32m[20221213 21:36:18 @agent_ppo2.py:185][0m |          -0.0058 |         161.4040 |           7.9780 |
[32m[20221213 21:36:18 @agent_ppo2.py:185][0m |          -0.0119 |         159.5330 |           8.0084 |
[32m[20221213 21:36:18 @agent_ppo2.py:185][0m |          -0.0101 |         159.0254 |           8.0394 |
[32m[20221213 21:36:18 @agent_ppo2.py:185][0m |          -0.0025 |         168.3497 |           8.0367 |
[32m[20221213 21:36:18 @agent_ppo2.py:185][0m |          -0.0122 |         157.6105 |           8.1088 |
[32m[20221213 21:36:18 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:36:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.80
[32m[20221213 21:36:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.00
[32m[20221213 21:36:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.00
[32m[20221213 21:36:18 @agent_ppo2.py:143][0m Total time:      40.73 min
[32m[20221213 21:36:18 @agent_ppo2.py:145][0m 3981312 total steps have happened
[32m[20221213 21:36:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1944 --------------------------#
[32m[20221213 21:36:18 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:36:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:19 @agent_ppo2.py:185][0m |          -0.0011 |         183.7144 |           7.9664 |
[32m[20221213 21:36:19 @agent_ppo2.py:185][0m |          -0.0037 |         177.1935 |           8.0349 |
[32m[20221213 21:36:19 @agent_ppo2.py:185][0m |          -0.0100 |         175.7227 |           7.9684 |
[32m[20221213 21:36:19 @agent_ppo2.py:185][0m |          -0.0061 |         174.9641 |           7.9583 |
[32m[20221213 21:36:19 @agent_ppo2.py:185][0m |           0.0049 |         191.7606 |           7.9471 |
[32m[20221213 21:36:19 @agent_ppo2.py:185][0m |          -0.0076 |         174.1807 |           7.9971 |
[32m[20221213 21:36:19 @agent_ppo2.py:185][0m |          -0.0005 |         182.5124 |           7.9366 |
[32m[20221213 21:36:19 @agent_ppo2.py:185][0m |          -0.0060 |         173.7394 |           7.9339 |
[32m[20221213 21:36:19 @agent_ppo2.py:185][0m |          -0.0120 |         173.4974 |           7.8900 |
[32m[20221213 21:36:19 @agent_ppo2.py:185][0m |          -0.0109 |         173.4326 |           7.8705 |
[32m[20221213 21:36:19 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:36:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 779.20
[32m[20221213 21:36:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 21:36:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.00
[32m[20221213 21:36:20 @agent_ppo2.py:143][0m Total time:      40.75 min
[32m[20221213 21:36:20 @agent_ppo2.py:145][0m 3983360 total steps have happened
[32m[20221213 21:36:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1945 --------------------------#
[32m[20221213 21:36:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:20 @agent_ppo2.py:185][0m |          -0.0024 |         173.9044 |           7.6994 |
[32m[20221213 21:36:20 @agent_ppo2.py:185][0m |          -0.0052 |         171.0593 |           7.7206 |
[32m[20221213 21:36:20 @agent_ppo2.py:185][0m |           0.0011 |         173.4043 |           7.6915 |
[32m[20221213 21:36:20 @agent_ppo2.py:185][0m |          -0.0073 |         169.3932 |           7.7441 |
[32m[20221213 21:36:20 @agent_ppo2.py:185][0m |          -0.0092 |         168.9492 |           7.7216 |
[32m[20221213 21:36:20 @agent_ppo2.py:185][0m |          -0.0088 |         168.5885 |           7.7352 |
[32m[20221213 21:36:20 @agent_ppo2.py:185][0m |          -0.0095 |         168.2868 |           7.6475 |
[32m[20221213 21:36:20 @agent_ppo2.py:185][0m |          -0.0087 |         168.0480 |           7.6633 |
[32m[20221213 21:36:21 @agent_ppo2.py:185][0m |           0.0054 |         179.7745 |           7.6378 |
[32m[20221213 21:36:21 @agent_ppo2.py:185][0m |           0.0067 |         182.4710 |           7.6646 |
[32m[20221213 21:36:21 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:36:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 678.00
[32m[20221213 21:36:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 779.00
[32m[20221213 21:36:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.00
[32m[20221213 21:36:21 @agent_ppo2.py:143][0m Total time:      40.77 min
[32m[20221213 21:36:21 @agent_ppo2.py:145][0m 3985408 total steps have happened
[32m[20221213 21:36:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1946 --------------------------#
[32m[20221213 21:36:21 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:21 @agent_ppo2.py:185][0m |           0.0015 |         168.3391 |           7.6726 |
[32m[20221213 21:36:21 @agent_ppo2.py:185][0m |          -0.0045 |         166.7807 |           7.7051 |
[32m[20221213 21:36:21 @agent_ppo2.py:185][0m |           0.0033 |         176.1982 |           7.7303 |
[32m[20221213 21:36:21 @agent_ppo2.py:185][0m |           0.0007 |         170.4417 |           7.7840 |
[32m[20221213 21:36:21 @agent_ppo2.py:185][0m |          -0.0055 |         165.9490 |           7.8589 |
[32m[20221213 21:36:22 @agent_ppo2.py:185][0m |          -0.0078 |         165.7910 |           7.8279 |
[32m[20221213 21:36:22 @agent_ppo2.py:185][0m |          -0.0057 |         165.8015 |           7.8891 |
[32m[20221213 21:36:22 @agent_ppo2.py:185][0m |          -0.0064 |         166.2673 |           7.9415 |
[32m[20221213 21:36:22 @agent_ppo2.py:185][0m |          -0.0069 |         165.6688 |           7.9460 |
[32m[20221213 21:36:22 @agent_ppo2.py:185][0m |          -0.0072 |         165.5017 |           8.0090 |
[32m[20221213 21:36:22 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 21:36:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.00
[32m[20221213 21:36:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.00
[32m[20221213 21:36:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 873.00
[32m[20221213 21:36:22 @agent_ppo2.py:143][0m Total time:      40.79 min
[32m[20221213 21:36:22 @agent_ppo2.py:145][0m 3987456 total steps have happened
[32m[20221213 21:36:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1947 --------------------------#
[32m[20221213 21:36:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:36:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:22 @agent_ppo2.py:185][0m |           0.0037 |         171.2865 |           8.0418 |
[32m[20221213 21:36:22 @agent_ppo2.py:185][0m |          -0.0035 |         168.5779 |           8.0318 |
[32m[20221213 21:36:23 @agent_ppo2.py:185][0m |          -0.0045 |         168.0460 |           8.0103 |
[32m[20221213 21:36:23 @agent_ppo2.py:185][0m |          -0.0034 |         167.8537 |           7.9723 |
[32m[20221213 21:36:23 @agent_ppo2.py:185][0m |          -0.0072 |         167.4553 |           7.9917 |
[32m[20221213 21:36:23 @agent_ppo2.py:185][0m |          -0.0089 |         167.3447 |           7.9510 |
[32m[20221213 21:36:23 @agent_ppo2.py:185][0m |          -0.0087 |         167.0845 |           7.9401 |
[32m[20221213 21:36:23 @agent_ppo2.py:185][0m |          -0.0086 |         166.9908 |           7.9224 |
[32m[20221213 21:36:23 @agent_ppo2.py:185][0m |          -0.0093 |         166.7557 |           7.9283 |
[32m[20221213 21:36:23 @agent_ppo2.py:185][0m |           0.0005 |         186.4909 |           7.8794 |
[32m[20221213 21:36:23 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:36:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.40
[32m[20221213 21:36:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:36:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221213 21:36:23 @agent_ppo2.py:143][0m Total time:      40.81 min
[32m[20221213 21:36:23 @agent_ppo2.py:145][0m 3989504 total steps have happened
[32m[20221213 21:36:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1948 --------------------------#
[32m[20221213 21:36:23 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:24 @agent_ppo2.py:185][0m |           0.0009 |         170.3949 |           7.8711 |
[32m[20221213 21:36:24 @agent_ppo2.py:185][0m |          -0.0001 |         170.6602 |           7.8886 |
[32m[20221213 21:36:24 @agent_ppo2.py:185][0m |          -0.0038 |         168.8795 |           7.9138 |
[32m[20221213 21:36:24 @agent_ppo2.py:185][0m |          -0.0043 |         168.4738 |           7.8334 |
[32m[20221213 21:36:24 @agent_ppo2.py:185][0m |          -0.0060 |         167.8320 |           7.9679 |
[32m[20221213 21:36:24 @agent_ppo2.py:185][0m |          -0.0079 |         167.7224 |           7.9421 |
[32m[20221213 21:36:24 @agent_ppo2.py:185][0m |          -0.0068 |         167.3615 |           7.9801 |
[32m[20221213 21:36:24 @agent_ppo2.py:185][0m |          -0.0076 |         167.1784 |           7.9238 |
[32m[20221213 21:36:24 @agent_ppo2.py:185][0m |          -0.0077 |         166.8513 |           7.9803 |
[32m[20221213 21:36:24 @agent_ppo2.py:185][0m |          -0.0082 |         166.7416 |           8.0199 |
[32m[20221213 21:36:24 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:36:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.00
[32m[20221213 21:36:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 757.00
[32m[20221213 21:36:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.00
[32m[20221213 21:36:24 @agent_ppo2.py:143][0m Total time:      40.83 min
[32m[20221213 21:36:24 @agent_ppo2.py:145][0m 3991552 total steps have happened
[32m[20221213 21:36:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1949 --------------------------#
[32m[20221213 21:36:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:25 @agent_ppo2.py:185][0m |          -0.0001 |         168.3117 |           7.7375 |
[32m[20221213 21:36:25 @agent_ppo2.py:185][0m |          -0.0022 |         168.8771 |           7.6521 |
[32m[20221213 21:36:25 @agent_ppo2.py:185][0m |          -0.0023 |         169.0049 |           7.6721 |
[32m[20221213 21:36:25 @agent_ppo2.py:185][0m |          -0.0073 |         166.3405 |           7.6295 |
[32m[20221213 21:36:25 @agent_ppo2.py:185][0m |          -0.0091 |         166.0297 |           7.5966 |
[32m[20221213 21:36:25 @agent_ppo2.py:185][0m |          -0.0105 |         165.7889 |           7.6035 |
[32m[20221213 21:36:25 @agent_ppo2.py:185][0m |          -0.0083 |         165.4449 |           7.5738 |
[32m[20221213 21:36:25 @agent_ppo2.py:185][0m |          -0.0074 |         165.2042 |           7.6149 |
[32m[20221213 21:36:25 @agent_ppo2.py:185][0m |           0.0077 |         181.7304 |           7.5426 |
[32m[20221213 21:36:26 @agent_ppo2.py:185][0m |          -0.0090 |         165.2514 |           7.5654 |
[32m[20221213 21:36:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:36:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.80
[32m[20221213 21:36:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.00
[32m[20221213 21:36:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 865.00
[32m[20221213 21:36:26 @agent_ppo2.py:143][0m Total time:      40.85 min
[32m[20221213 21:36:26 @agent_ppo2.py:145][0m 3993600 total steps have happened
[32m[20221213 21:36:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1950 --------------------------#
[32m[20221213 21:36:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:36:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:26 @agent_ppo2.py:185][0m |           0.0017 |         122.2222 |           7.6799 |
[32m[20221213 21:36:26 @agent_ppo2.py:185][0m |           0.0064 |         117.2467 |           7.6307 |
[32m[20221213 21:36:26 @agent_ppo2.py:185][0m |           0.0064 |         114.3555 |           7.6811 |
[32m[20221213 21:36:26 @agent_ppo2.py:185][0m |          -0.0000 |         107.2420 |           7.8234 |
[32m[20221213 21:36:26 @agent_ppo2.py:185][0m |          -0.0021 |         106.1161 |           7.7440 |
[32m[20221213 21:36:26 @agent_ppo2.py:185][0m |           0.0002 |         105.6008 |           7.6985 |
[32m[20221213 21:36:27 @agent_ppo2.py:185][0m |          -0.0053 |         105.0001 |           7.7831 |
[32m[20221213 21:36:27 @agent_ppo2.py:185][0m |          -0.0011 |         104.7001 |           7.6752 |
[32m[20221213 21:36:27 @agent_ppo2.py:185][0m |          -0.0070 |         104.1420 |           7.8614 |
[32m[20221213 21:36:27 @agent_ppo2.py:185][0m |          -0.0026 |         103.8746 |           7.8013 |
[32m[20221213 21:36:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:36:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 116.60
[32m[20221213 21:36:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 583.00
[32m[20221213 21:36:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.00
[32m[20221213 21:36:27 @agent_ppo2.py:143][0m Total time:      40.87 min
[32m[20221213 21:36:27 @agent_ppo2.py:145][0m 3995648 total steps have happened
[32m[20221213 21:36:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1951 --------------------------#
[32m[20221213 21:36:27 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:27 @agent_ppo2.py:185][0m |          -0.0007 |         167.6831 |           7.8897 |
[32m[20221213 21:36:27 @agent_ppo2.py:185][0m |           0.0043 |         174.5752 |           7.9178 |
[32m[20221213 21:36:27 @agent_ppo2.py:185][0m |          -0.0002 |         167.2903 |           7.9120 |
[32m[20221213 21:36:27 @agent_ppo2.py:185][0m |          -0.0052 |         164.5214 |           7.8960 |
[32m[20221213 21:36:28 @agent_ppo2.py:185][0m |           0.0042 |         169.9608 |           7.8985 |
[32m[20221213 21:36:28 @agent_ppo2.py:185][0m |          -0.0054 |         163.9463 |           7.8944 |
[32m[20221213 21:36:28 @agent_ppo2.py:185][0m |          -0.0069 |         163.6900 |           7.9132 |
[32m[20221213 21:36:28 @agent_ppo2.py:185][0m |          -0.0082 |         163.5195 |           7.8734 |
[32m[20221213 21:36:28 @agent_ppo2.py:185][0m |          -0.0081 |         163.1769 |           7.9051 |
[32m[20221213 21:36:28 @agent_ppo2.py:185][0m |          -0.0082 |         163.0847 |           7.8946 |
[32m[20221213 21:36:28 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:36:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.40
[32m[20221213 21:36:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:36:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 864.00
[32m[20221213 21:36:28 @agent_ppo2.py:143][0m Total time:      40.89 min
[32m[20221213 21:36:28 @agent_ppo2.py:145][0m 3997696 total steps have happened
[32m[20221213 21:36:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1952 --------------------------#
[32m[20221213 21:36:28 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:28 @agent_ppo2.py:185][0m |           0.0002 |         166.5973 |           7.9845 |
[32m[20221213 21:36:29 @agent_ppo2.py:185][0m |          -0.0047 |         165.3935 |           8.0167 |
[32m[20221213 21:36:29 @agent_ppo2.py:185][0m |          -0.0003 |         170.1078 |           8.0023 |
[32m[20221213 21:36:29 @agent_ppo2.py:185][0m |          -0.0064 |         164.5597 |           8.0042 |
[32m[20221213 21:36:29 @agent_ppo2.py:185][0m |           0.0076 |         183.2215 |           7.9880 |
[32m[20221213 21:36:29 @agent_ppo2.py:185][0m |          -0.0039 |         164.2705 |           8.0137 |
[32m[20221213 21:36:29 @agent_ppo2.py:185][0m |          -0.0059 |         165.1830 |           7.9740 |
[32m[20221213 21:36:29 @agent_ppo2.py:185][0m |          -0.0053 |         164.9720 |           7.9868 |
[32m[20221213 21:36:29 @agent_ppo2.py:185][0m |          -0.0081 |         163.5061 |           7.9571 |
[32m[20221213 21:36:29 @agent_ppo2.py:185][0m |          -0.0097 |         163.3546 |           7.9496 |
[32m[20221213 21:36:29 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:36:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.80
[32m[20221213 21:36:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:36:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.00
[32m[20221213 21:36:29 @agent_ppo2.py:143][0m Total time:      40.91 min
[32m[20221213 21:36:29 @agent_ppo2.py:145][0m 3999744 total steps have happened
[32m[20221213 21:36:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1953 --------------------------#
[32m[20221213 21:36:29 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:30 @agent_ppo2.py:185][0m |           0.0001 |         167.3695 |           7.5509 |
[32m[20221213 21:36:30 @agent_ppo2.py:185][0m |          -0.0036 |         166.4596 |           7.5398 |
[32m[20221213 21:36:30 @agent_ppo2.py:185][0m |           0.0090 |         185.3346 |           7.4960 |
[32m[20221213 21:36:30 @agent_ppo2.py:185][0m |          -0.0055 |         165.6522 |           7.5736 |
[32m[20221213 21:36:30 @agent_ppo2.py:185][0m |          -0.0060 |         165.6516 |           7.5982 |
[32m[20221213 21:36:30 @agent_ppo2.py:185][0m |          -0.0036 |         165.2441 |           7.6294 |
[32m[20221213 21:36:30 @agent_ppo2.py:185][0m |          -0.0050 |         165.4725 |           7.6183 |
[32m[20221213 21:36:30 @agent_ppo2.py:185][0m |          -0.0084 |         165.0912 |           7.7004 |
[32m[20221213 21:36:30 @agent_ppo2.py:185][0m |          -0.0084 |         164.9215 |           7.6901 |
[32m[20221213 21:36:30 @agent_ppo2.py:185][0m |          -0.0059 |         164.9432 |           7.7192 |
[32m[20221213 21:36:30 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:36:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.40
[32m[20221213 21:36:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:36:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 868.00
[32m[20221213 21:36:31 @agent_ppo2.py:143][0m Total time:      40.93 min
[32m[20221213 21:36:31 @agent_ppo2.py:145][0m 4001792 total steps have happened
[32m[20221213 21:36:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1954 --------------------------#
[32m[20221213 21:36:31 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:31 @agent_ppo2.py:185][0m |          -0.0006 |         170.3631 |           7.9870 |
[32m[20221213 21:36:31 @agent_ppo2.py:185][0m |          -0.0038 |         168.5005 |           7.9366 |
[32m[20221213 21:36:31 @agent_ppo2.py:185][0m |          -0.0066 |         167.3554 |           7.9631 |
[32m[20221213 21:36:31 @agent_ppo2.py:185][0m |          -0.0063 |         166.4587 |           7.9238 |
[32m[20221213 21:36:31 @agent_ppo2.py:185][0m |          -0.0062 |         166.0448 |           7.9094 |
[32m[20221213 21:36:31 @agent_ppo2.py:185][0m |          -0.0088 |         165.6738 |           7.8456 |
[32m[20221213 21:36:31 @agent_ppo2.py:185][0m |          -0.0086 |         165.0765 |           7.8879 |
[32m[20221213 21:36:32 @agent_ppo2.py:185][0m |          -0.0088 |         164.6710 |           7.8933 |
[32m[20221213 21:36:32 @agent_ppo2.py:185][0m |          -0.0081 |         164.8898 |           7.8831 |
[32m[20221213 21:36:32 @agent_ppo2.py:185][0m |          -0.0028 |         167.8082 |           7.9234 |
[32m[20221213 21:36:32 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:36:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.40
[32m[20221213 21:36:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221213 21:36:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.00
[32m[20221213 21:36:32 @agent_ppo2.py:143][0m Total time:      40.95 min
[32m[20221213 21:36:32 @agent_ppo2.py:145][0m 4003840 total steps have happened
[32m[20221213 21:36:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1955 --------------------------#
[32m[20221213 21:36:32 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:32 @agent_ppo2.py:185][0m |           0.0091 |         190.7640 |           7.7098 |
[32m[20221213 21:36:32 @agent_ppo2.py:185][0m |          -0.0054 |         170.3969 |           7.7896 |
[32m[20221213 21:36:32 @agent_ppo2.py:185][0m |          -0.0055 |         169.6773 |           7.7752 |
[32m[20221213 21:36:32 @agent_ppo2.py:185][0m |          -0.0061 |         168.6699 |           7.8163 |
[32m[20221213 21:36:32 @agent_ppo2.py:185][0m |           0.0004 |         179.4337 |           7.8076 |
[32m[20221213 21:36:33 @agent_ppo2.py:185][0m |          -0.0094 |         168.3626 |           7.7908 |
[32m[20221213 21:36:33 @agent_ppo2.py:185][0m |          -0.0068 |         167.3953 |           7.8190 |
[32m[20221213 21:36:33 @agent_ppo2.py:185][0m |          -0.0063 |         167.2415 |           7.8711 |
[32m[20221213 21:36:33 @agent_ppo2.py:185][0m |          -0.0103 |         166.9413 |           7.8782 |
[32m[20221213 21:36:33 @agent_ppo2.py:185][0m |           0.0007 |         176.5732 |           7.9282 |
[32m[20221213 21:36:33 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:36:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 769.00
[32m[20221213 21:36:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.00
[32m[20221213 21:36:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.00
[32m[20221213 21:36:33 @agent_ppo2.py:143][0m Total time:      40.97 min
[32m[20221213 21:36:33 @agent_ppo2.py:145][0m 4005888 total steps have happened
[32m[20221213 21:36:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1956 --------------------------#
[32m[20221213 21:36:33 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:33 @agent_ppo2.py:185][0m |           0.0003 |         175.1529 |           7.9431 |
[32m[20221213 21:36:33 @agent_ppo2.py:185][0m |          -0.0014 |         173.8225 |           7.9694 |
[32m[20221213 21:36:34 @agent_ppo2.py:185][0m |          -0.0044 |         174.0223 |           7.9455 |
[32m[20221213 21:36:34 @agent_ppo2.py:185][0m |           0.0008 |         177.1642 |           7.9788 |
[32m[20221213 21:36:34 @agent_ppo2.py:185][0m |          -0.0080 |         172.2507 |           7.9450 |
[32m[20221213 21:36:34 @agent_ppo2.py:185][0m |          -0.0019 |         172.6262 |           7.8991 |
[32m[20221213 21:36:34 @agent_ppo2.py:185][0m |          -0.0087 |         172.0483 |           7.9119 |
[32m[20221213 21:36:34 @agent_ppo2.py:185][0m |          -0.0103 |         171.7514 |           7.8956 |
[32m[20221213 21:36:34 @agent_ppo2.py:185][0m |           0.0031 |         195.6632 |           7.9024 |
[32m[20221213 21:36:34 @agent_ppo2.py:185][0m |          -0.0057 |         171.6947 |           7.9530 |
[32m[20221213 21:36:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:36:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.20
[32m[20221213 21:36:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.00
[32m[20221213 21:36:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.00
[32m[20221213 21:36:34 @agent_ppo2.py:143][0m Total time:      40.99 min
[32m[20221213 21:36:34 @agent_ppo2.py:145][0m 4007936 total steps have happened
[32m[20221213 21:36:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1957 --------------------------#
[32m[20221213 21:36:34 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:36:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:35 @agent_ppo2.py:185][0m |           0.0041 |         178.5184 |           7.7914 |
[32m[20221213 21:36:35 @agent_ppo2.py:185][0m |           0.0001 |         172.9023 |           7.8290 |
[32m[20221213 21:36:35 @agent_ppo2.py:185][0m |          -0.0045 |         172.4663 |           7.8952 |
[32m[20221213 21:36:35 @agent_ppo2.py:185][0m |          -0.0041 |         172.4300 |           7.9454 |
[32m[20221213 21:36:35 @agent_ppo2.py:185][0m |          -0.0082 |         171.4439 |           7.9539 |
[32m[20221213 21:36:35 @agent_ppo2.py:185][0m |          -0.0072 |         171.2000 |           7.9884 |
[32m[20221213 21:36:35 @agent_ppo2.py:185][0m |          -0.0079 |         171.1448 |           8.0390 |
[32m[20221213 21:36:35 @agent_ppo2.py:185][0m |          -0.0113 |         171.1170 |           8.0278 |
[32m[20221213 21:36:35 @agent_ppo2.py:185][0m |          -0.0096 |         170.9955 |           8.0637 |
[32m[20221213 21:36:35 @agent_ppo2.py:185][0m |          -0.0088 |         170.9104 |           8.1074 |
[32m[20221213 21:36:35 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:36:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.80
[32m[20221213 21:36:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.00
[32m[20221213 21:36:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.00
[32m[20221213 21:36:35 @agent_ppo2.py:143][0m Total time:      41.01 min
[32m[20221213 21:36:35 @agent_ppo2.py:145][0m 4009984 total steps have happened
[32m[20221213 21:36:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1958 --------------------------#
[32m[20221213 21:36:36 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:36 @agent_ppo2.py:185][0m |          -0.0023 |         174.8299 |           8.2573 |
[32m[20221213 21:36:36 @agent_ppo2.py:185][0m |          -0.0007 |         176.9766 |           8.2474 |
[32m[20221213 21:36:36 @agent_ppo2.py:185][0m |          -0.0073 |         173.1097 |           8.2818 |
[32m[20221213 21:36:36 @agent_ppo2.py:185][0m |          -0.0092 |         172.7156 |           8.2880 |
[32m[20221213 21:36:36 @agent_ppo2.py:185][0m |          -0.0098 |         172.2462 |           8.3032 |
[32m[20221213 21:36:36 @agent_ppo2.py:185][0m |          -0.0041 |         174.4335 |           8.3520 |
[32m[20221213 21:36:36 @agent_ppo2.py:185][0m |          -0.0068 |         172.8737 |           8.3546 |
[32m[20221213 21:36:36 @agent_ppo2.py:185][0m |          -0.0094 |         171.4483 |           8.3288 |
[32m[20221213 21:36:36 @agent_ppo2.py:185][0m |          -0.0120 |         171.2983 |           8.3522 |
[32m[20221213 21:36:37 @agent_ppo2.py:185][0m |          -0.0084 |         171.2315 |           8.3923 |
[32m[20221213 21:36:37 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:36:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.60
[32m[20221213 21:36:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.00
[32m[20221213 21:36:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.00
[32m[20221213 21:36:37 @agent_ppo2.py:143][0m Total time:      41.03 min
[32m[20221213 21:36:37 @agent_ppo2.py:145][0m 4012032 total steps have happened
[32m[20221213 21:36:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1959 --------------------------#
[32m[20221213 21:36:37 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:37 @agent_ppo2.py:185][0m |          -0.0001 |         170.3811 |           8.6634 |
[32m[20221213 21:36:37 @agent_ppo2.py:185][0m |           0.0213 |         202.6210 |           8.6729 |
[32m[20221213 21:36:37 @agent_ppo2.py:185][0m |          -0.0021 |         167.0941 |           8.7680 |
[32m[20221213 21:36:37 @agent_ppo2.py:185][0m |          -0.0062 |         165.1243 |           8.6809 |
[32m[20221213 21:36:37 @agent_ppo2.py:185][0m |          -0.0060 |         164.2129 |           8.6848 |
[32m[20221213 21:36:37 @agent_ppo2.py:185][0m |          -0.0074 |         163.8346 |           8.6540 |
[32m[20221213 21:36:37 @agent_ppo2.py:185][0m |          -0.0076 |         163.2323 |           8.6727 |
[32m[20221213 21:36:38 @agent_ppo2.py:185][0m |          -0.0075 |         162.8991 |           8.6304 |
[32m[20221213 21:36:38 @agent_ppo2.py:185][0m |          -0.0086 |         162.3547 |           8.6530 |
[32m[20221213 21:36:38 @agent_ppo2.py:185][0m |          -0.0071 |         162.2245 |           8.6836 |
[32m[20221213 21:36:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:36:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.20
[32m[20221213 21:36:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:36:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.00
[32m[20221213 21:36:38 @agent_ppo2.py:143][0m Total time:      41.05 min
[32m[20221213 21:36:38 @agent_ppo2.py:145][0m 4014080 total steps have happened
[32m[20221213 21:36:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1960 --------------------------#
[32m[20221213 21:36:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:36:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:38 @agent_ppo2.py:185][0m |          -0.0029 |         180.1605 |           8.2064 |
[32m[20221213 21:36:38 @agent_ppo2.py:185][0m |          -0.0029 |         177.4799 |           8.2165 |
[32m[20221213 21:36:38 @agent_ppo2.py:185][0m |          -0.0065 |         176.3737 |           8.2245 |
[32m[20221213 21:36:38 @agent_ppo2.py:185][0m |          -0.0091 |         175.3595 |           8.2054 |
[32m[20221213 21:36:39 @agent_ppo2.py:185][0m |          -0.0061 |         175.0898 |           8.2555 |
[32m[20221213 21:36:39 @agent_ppo2.py:185][0m |          -0.0060 |         175.2380 |           8.2608 |
[32m[20221213 21:36:39 @agent_ppo2.py:185][0m |          -0.0096 |         174.6195 |           8.2497 |
[32m[20221213 21:36:39 @agent_ppo2.py:185][0m |          -0.0086 |         174.2585 |           8.3157 |
[32m[20221213 21:36:39 @agent_ppo2.py:185][0m |          -0.0085 |         174.0618 |           8.2605 |
[32m[20221213 21:36:39 @agent_ppo2.py:185][0m |          -0.0102 |         173.6330 |           8.3190 |
[32m[20221213 21:36:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:36:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.60
[32m[20221213 21:36:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:36:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 21:36:39 @agent_ppo2.py:143][0m Total time:      41.07 min
[32m[20221213 21:36:39 @agent_ppo2.py:145][0m 4016128 total steps have happened
[32m[20221213 21:36:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1961 --------------------------#
[32m[20221213 21:36:39 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:39 @agent_ppo2.py:185][0m |          -0.0017 |         176.3102 |           8.3387 |
[32m[20221213 21:36:39 @agent_ppo2.py:185][0m |          -0.0045 |         174.6676 |           8.3721 |
[32m[20221213 21:36:40 @agent_ppo2.py:185][0m |          -0.0041 |         173.3946 |           8.4209 |
[32m[20221213 21:36:40 @agent_ppo2.py:185][0m |          -0.0063 |         172.5354 |           8.4832 |
[32m[20221213 21:36:40 @agent_ppo2.py:185][0m |          -0.0050 |         171.8187 |           8.5121 |
[32m[20221213 21:36:40 @agent_ppo2.py:185][0m |          -0.0077 |         171.2405 |           8.5215 |
[32m[20221213 21:36:40 @agent_ppo2.py:185][0m |          -0.0086 |         171.1790 |           8.5152 |
[32m[20221213 21:36:40 @agent_ppo2.py:185][0m |          -0.0100 |         170.7479 |           8.5059 |
[32m[20221213 21:36:40 @agent_ppo2.py:185][0m |          -0.0081 |         170.7873 |           8.5226 |
[32m[20221213 21:36:40 @agent_ppo2.py:185][0m |          -0.0049 |         173.2207 |           8.4973 |
[32m[20221213 21:36:40 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:36:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.00
[32m[20221213 21:36:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.00
[32m[20221213 21:36:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.00
[32m[20221213 21:36:40 @agent_ppo2.py:143][0m Total time:      41.09 min
[32m[20221213 21:36:40 @agent_ppo2.py:145][0m 4018176 total steps have happened
[32m[20221213 21:36:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1962 --------------------------#
[32m[20221213 21:36:40 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:41 @agent_ppo2.py:185][0m |           0.0013 |         169.7003 |           8.3285 |
[32m[20221213 21:36:41 @agent_ppo2.py:185][0m |          -0.0050 |         168.8872 |           8.2636 |
[32m[20221213 21:36:41 @agent_ppo2.py:185][0m |          -0.0051 |         168.3437 |           8.2112 |
[32m[20221213 21:36:41 @agent_ppo2.py:185][0m |           0.0041 |         187.2876 |           8.2007 |
[32m[20221213 21:36:41 @agent_ppo2.py:185][0m |          -0.0057 |         168.0597 |           8.1871 |
[32m[20221213 21:36:41 @agent_ppo2.py:185][0m |          -0.0092 |         167.1209 |           8.1664 |
[32m[20221213 21:36:41 @agent_ppo2.py:185][0m |          -0.0083 |         169.4194 |           8.1135 |
[32m[20221213 21:36:41 @agent_ppo2.py:185][0m |          -0.0082 |         166.7675 |           8.1217 |
[32m[20221213 21:36:41 @agent_ppo2.py:185][0m |          -0.0120 |         166.5870 |           8.1076 |
[32m[20221213 21:36:41 @agent_ppo2.py:185][0m |          -0.0052 |         170.2348 |           8.1166 |
[32m[20221213 21:36:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:36:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.80
[32m[20221213 21:36:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.00
[32m[20221213 21:36:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 759.00
[32m[20221213 21:36:42 @agent_ppo2.py:143][0m Total time:      41.11 min
[32m[20221213 21:36:42 @agent_ppo2.py:145][0m 4020224 total steps have happened
[32m[20221213 21:36:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1963 --------------------------#
[32m[20221213 21:36:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:36:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:42 @agent_ppo2.py:185][0m |          -0.0014 |         170.2388 |           8.3185 |
[32m[20221213 21:36:42 @agent_ppo2.py:185][0m |           0.0038 |         182.9300 |           8.2561 |
[32m[20221213 21:36:42 @agent_ppo2.py:185][0m |          -0.0015 |         166.8361 |           8.2958 |
[32m[20221213 21:36:42 @agent_ppo2.py:185][0m |          -0.0072 |         165.0829 |           8.2815 |
[32m[20221213 21:36:42 @agent_ppo2.py:185][0m |          -0.0088 |         164.5470 |           8.3000 |
[32m[20221213 21:36:42 @agent_ppo2.py:185][0m |          -0.0078 |         163.9894 |           8.2917 |
[32m[20221213 21:36:42 @agent_ppo2.py:185][0m |          -0.0088 |         163.6592 |           8.2839 |
[32m[20221213 21:36:42 @agent_ppo2.py:185][0m |          -0.0113 |         163.2827 |           8.3257 |
[32m[20221213 21:36:43 @agent_ppo2.py:185][0m |          -0.0092 |         163.2856 |           8.2991 |
[32m[20221213 21:36:43 @agent_ppo2.py:185][0m |          -0.0089 |         162.6091 |           8.3519 |
[32m[20221213 21:36:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:36:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.80
[32m[20221213 21:36:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.00
[32m[20221213 21:36:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:36:43 @agent_ppo2.py:143][0m Total time:      41.13 min
[32m[20221213 21:36:43 @agent_ppo2.py:145][0m 4022272 total steps have happened
[32m[20221213 21:36:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1964 --------------------------#
[32m[20221213 21:36:43 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:43 @agent_ppo2.py:185][0m |           0.0055 |         172.0758 |           8.4931 |
[32m[20221213 21:36:43 @agent_ppo2.py:185][0m |          -0.0018 |         170.0415 |           8.4259 |
[32m[20221213 21:36:43 @agent_ppo2.py:185][0m |          -0.0037 |         169.3383 |           8.4861 |
[32m[20221213 21:36:43 @agent_ppo2.py:185][0m |          -0.0020 |         169.8888 |           8.4236 |
[32m[20221213 21:36:43 @agent_ppo2.py:185][0m |          -0.0045 |         169.1392 |           8.4150 |
[32m[20221213 21:36:43 @agent_ppo2.py:185][0m |          -0.0052 |         168.7708 |           8.4037 |
[32m[20221213 21:36:44 @agent_ppo2.py:185][0m |          -0.0049 |         168.8656 |           8.3605 |
[32m[20221213 21:36:44 @agent_ppo2.py:185][0m |          -0.0061 |         168.6087 |           8.3837 |
[32m[20221213 21:36:44 @agent_ppo2.py:185][0m |          -0.0079 |         168.4995 |           8.3587 |
[32m[20221213 21:36:44 @agent_ppo2.py:185][0m |          -0.0043 |         168.6733 |           8.2882 |
[32m[20221213 21:36:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:36:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.60
[32m[20221213 21:36:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.00
[32m[20221213 21:36:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221213 21:36:44 @agent_ppo2.py:143][0m Total time:      41.15 min
[32m[20221213 21:36:44 @agent_ppo2.py:145][0m 4024320 total steps have happened
[32m[20221213 21:36:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1965 --------------------------#
[32m[20221213 21:36:44 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:44 @agent_ppo2.py:185][0m |          -0.0012 |         170.3936 |           8.3762 |
[32m[20221213 21:36:44 @agent_ppo2.py:185][0m |           0.0044 |         177.2743 |           8.3499 |
[32m[20221213 21:36:44 @agent_ppo2.py:185][0m |          -0.0042 |         168.8382 |           8.3968 |
[32m[20221213 21:36:45 @agent_ppo2.py:185][0m |          -0.0052 |         169.0081 |           8.4094 |
[32m[20221213 21:36:45 @agent_ppo2.py:185][0m |          -0.0066 |         168.1340 |           8.4566 |
[32m[20221213 21:36:45 @agent_ppo2.py:185][0m |          -0.0079 |         167.9286 |           8.4346 |
[32m[20221213 21:36:45 @agent_ppo2.py:185][0m |          -0.0010 |         173.6457 |           8.4459 |
[32m[20221213 21:36:45 @agent_ppo2.py:185][0m |          -0.0067 |         167.5395 |           8.4615 |
[32m[20221213 21:36:45 @agent_ppo2.py:185][0m |          -0.0097 |         167.4031 |           8.4634 |
[32m[20221213 21:36:45 @agent_ppo2.py:185][0m |          -0.0100 |         167.2513 |           8.4956 |
[32m[20221213 21:36:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:36:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.80
[32m[20221213 21:36:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.00
[32m[20221213 21:36:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.00
[32m[20221213 21:36:45 @agent_ppo2.py:143][0m Total time:      41.17 min
[32m[20221213 21:36:45 @agent_ppo2.py:145][0m 4026368 total steps have happened
[32m[20221213 21:36:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1966 --------------------------#
[32m[20221213 21:36:45 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:45 @agent_ppo2.py:185][0m |           0.0038 |         174.8191 |           7.8660 |
[32m[20221213 21:36:46 @agent_ppo2.py:185][0m |          -0.0023 |         172.1589 |           7.8548 |
[32m[20221213 21:36:46 @agent_ppo2.py:185][0m |          -0.0071 |         171.6896 |           7.8006 |
[32m[20221213 21:36:46 @agent_ppo2.py:185][0m |          -0.0048 |         171.4490 |           7.7531 |
[32m[20221213 21:36:46 @agent_ppo2.py:185][0m |           0.0000 |         176.0952 |           7.7372 |
[32m[20221213 21:36:46 @agent_ppo2.py:185][0m |          -0.0049 |         170.8015 |           7.7777 |
[32m[20221213 21:36:46 @agent_ppo2.py:185][0m |          -0.0083 |         170.7490 |           7.7786 |
[32m[20221213 21:36:46 @agent_ppo2.py:185][0m |          -0.0067 |         170.6353 |           7.7439 |
[32m[20221213 21:36:46 @agent_ppo2.py:185][0m |          -0.0066 |         171.2074 |           7.7612 |
[32m[20221213 21:36:46 @agent_ppo2.py:185][0m |          -0.0082 |         170.3719 |           7.7469 |
[32m[20221213 21:36:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:36:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.40
[32m[20221213 21:36:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:36:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.00
[32m[20221213 21:36:46 @agent_ppo2.py:143][0m Total time:      41.20 min
[32m[20221213 21:36:46 @agent_ppo2.py:145][0m 4028416 total steps have happened
[32m[20221213 21:36:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1967 --------------------------#
[32m[20221213 21:36:47 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:47 @agent_ppo2.py:185][0m |           0.0128 |         185.6494 |           8.0076 |
[32m[20221213 21:36:47 @agent_ppo2.py:185][0m |          -0.0000 |         169.0091 |           8.0363 |
[32m[20221213 21:36:47 @agent_ppo2.py:185][0m |          -0.0037 |         168.5106 |           8.0194 |
[32m[20221213 21:36:47 @agent_ppo2.py:185][0m |          -0.0081 |         168.1948 |           8.0108 |
[32m[20221213 21:36:47 @agent_ppo2.py:185][0m |          -0.0009 |         170.1678 |           7.9927 |
[32m[20221213 21:36:47 @agent_ppo2.py:185][0m |          -0.0035 |         168.0874 |           8.0081 |
[32m[20221213 21:36:47 @agent_ppo2.py:185][0m |          -0.0058 |         167.6254 |           7.9630 |
[32m[20221213 21:36:47 @agent_ppo2.py:185][0m |           0.0072 |         189.5792 |           7.9886 |
[32m[20221213 21:36:47 @agent_ppo2.py:185][0m |           0.0081 |         186.0111 |           7.9804 |
[32m[20221213 21:36:48 @agent_ppo2.py:185][0m |          -0.0054 |         167.2738 |           8.0066 |
[32m[20221213 21:36:48 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:36:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 659.60
[32m[20221213 21:36:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 697.00
[32m[20221213 21:36:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 872.00
[32m[20221213 21:36:48 @agent_ppo2.py:143][0m Total time:      41.22 min
[32m[20221213 21:36:48 @agent_ppo2.py:145][0m 4030464 total steps have happened
[32m[20221213 21:36:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1968 --------------------------#
[32m[20221213 21:36:48 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:48 @agent_ppo2.py:185][0m |          -0.0003 |         170.9888 |           8.1827 |
[32m[20221213 21:36:48 @agent_ppo2.py:185][0m |          -0.0024 |         170.3441 |           8.2898 |
[32m[20221213 21:36:48 @agent_ppo2.py:185][0m |          -0.0059 |         169.7212 |           8.2571 |
[32m[20221213 21:36:48 @agent_ppo2.py:185][0m |          -0.0070 |         169.3211 |           8.2284 |
[32m[20221213 21:36:48 @agent_ppo2.py:185][0m |          -0.0029 |         170.7469 |           8.2497 |
[32m[20221213 21:36:48 @agent_ppo2.py:185][0m |          -0.0069 |         168.9344 |           8.3057 |
[32m[20221213 21:36:48 @agent_ppo2.py:185][0m |           0.0010 |         182.0138 |           8.3470 |
[32m[20221213 21:36:49 @agent_ppo2.py:185][0m |          -0.0094 |         168.4202 |           8.3544 |
[32m[20221213 21:36:49 @agent_ppo2.py:185][0m |          -0.0089 |         168.0690 |           8.3047 |
[32m[20221213 21:36:49 @agent_ppo2.py:185][0m |          -0.0077 |         167.7780 |           8.3893 |
[32m[20221213 21:36:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:36:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 770.00
[32m[20221213 21:36:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.00
[32m[20221213 21:36:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.00
[32m[20221213 21:36:49 @agent_ppo2.py:143][0m Total time:      41.24 min
[32m[20221213 21:36:49 @agent_ppo2.py:145][0m 4032512 total steps have happened
[32m[20221213 21:36:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1969 --------------------------#
[32m[20221213 21:36:49 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:49 @agent_ppo2.py:185][0m |           0.0006 |         170.7458 |           8.2409 |
[32m[20221213 21:36:49 @agent_ppo2.py:185][0m |          -0.0017 |         169.9769 |           8.2106 |
[32m[20221213 21:36:49 @agent_ppo2.py:185][0m |          -0.0032 |         169.6836 |           8.1763 |
[32m[20221213 21:36:49 @agent_ppo2.py:185][0m |          -0.0041 |         169.3088 |           8.1614 |
[32m[20221213 21:36:50 @agent_ppo2.py:185][0m |          -0.0051 |         168.8052 |           8.1677 |
[32m[20221213 21:36:50 @agent_ppo2.py:185][0m |          -0.0047 |         168.5589 |           8.1410 |
[32m[20221213 21:36:50 @agent_ppo2.py:185][0m |          -0.0012 |         169.8237 |           8.1489 |
[32m[20221213 21:36:50 @agent_ppo2.py:185][0m |          -0.0046 |         168.1451 |           8.0899 |
[32m[20221213 21:36:50 @agent_ppo2.py:185][0m |          -0.0068 |         167.9106 |           8.1272 |
[32m[20221213 21:36:50 @agent_ppo2.py:185][0m |          -0.0082 |         167.7920 |           8.0994 |
[32m[20221213 21:36:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:36:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.00
[32m[20221213 21:36:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:36:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.00
[32m[20221213 21:36:50 @agent_ppo2.py:143][0m Total time:      41.26 min
[32m[20221213 21:36:50 @agent_ppo2.py:145][0m 4034560 total steps have happened
[32m[20221213 21:36:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1970 --------------------------#
[32m[20221213 21:36:50 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:50 @agent_ppo2.py:185][0m |           0.0016 |         173.3611 |           7.6905 |
[32m[20221213 21:36:50 @agent_ppo2.py:185][0m |          -0.0025 |         172.1877 |           7.7191 |
[32m[20221213 21:36:51 @agent_ppo2.py:185][0m |          -0.0050 |         171.5373 |           7.7065 |
[32m[20221213 21:36:51 @agent_ppo2.py:185][0m |          -0.0067 |         171.0217 |           7.6913 |
[32m[20221213 21:36:51 @agent_ppo2.py:185][0m |           0.0001 |         176.6655 |           7.7521 |
[32m[20221213 21:36:51 @agent_ppo2.py:185][0m |          -0.0059 |         170.2432 |           7.8043 |
[32m[20221213 21:36:51 @agent_ppo2.py:185][0m |           0.0017 |         178.7755 |           7.7166 |
[32m[20221213 21:36:51 @agent_ppo2.py:185][0m |          -0.0024 |         174.7246 |           7.7902 |
[32m[20221213 21:36:51 @agent_ppo2.py:185][0m |          -0.0084 |         169.5051 |           7.7584 |
[32m[20221213 21:36:51 @agent_ppo2.py:185][0m |          -0.0082 |         169.3326 |           7.7485 |
[32m[20221213 21:36:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:36:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.60
[32m[20221213 21:36:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221213 21:36:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.00
[32m[20221213 21:36:51 @agent_ppo2.py:143][0m Total time:      41.28 min
[32m[20221213 21:36:51 @agent_ppo2.py:145][0m 4036608 total steps have happened
[32m[20221213 21:36:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1971 --------------------------#
[32m[20221213 21:36:51 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:52 @agent_ppo2.py:185][0m |          -0.0004 |         172.0257 |           8.2775 |
[32m[20221213 21:36:52 @agent_ppo2.py:185][0m |          -0.0048 |         170.6096 |           8.2732 |
[32m[20221213 21:36:52 @agent_ppo2.py:185][0m |           0.0007 |         171.0799 |           8.2567 |
[32m[20221213 21:36:52 @agent_ppo2.py:185][0m |           0.0046 |         179.2555 |           8.2997 |
[32m[20221213 21:36:52 @agent_ppo2.py:185][0m |          -0.0038 |         168.9328 |           8.3568 |
[32m[20221213 21:36:52 @agent_ppo2.py:185][0m |          -0.0069 |         168.5648 |           8.2746 |
[32m[20221213 21:36:52 @agent_ppo2.py:185][0m |          -0.0023 |         174.0002 |           8.3240 |
[32m[20221213 21:36:52 @agent_ppo2.py:185][0m |          -0.0007 |         170.7175 |           8.3194 |
[32m[20221213 21:36:52 @agent_ppo2.py:185][0m |          -0.0065 |         168.1988 |           8.3467 |
[32m[20221213 21:36:52 @agent_ppo2.py:185][0m |          -0.0087 |         168.0426 |           8.3345 |
[32m[20221213 21:36:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:36:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.20
[32m[20221213 21:36:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.00
[32m[20221213 21:36:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 864.00
[32m[20221213 21:36:52 @agent_ppo2.py:143][0m Total time:      41.30 min
[32m[20221213 21:36:52 @agent_ppo2.py:145][0m 4038656 total steps have happened
[32m[20221213 21:36:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1972 --------------------------#
[32m[20221213 21:36:53 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:53 @agent_ppo2.py:185][0m |          -0.0051 |         175.0913 |           8.0057 |
[32m[20221213 21:36:53 @agent_ppo2.py:185][0m |          -0.0062 |         174.2108 |           7.9907 |
[32m[20221213 21:36:53 @agent_ppo2.py:185][0m |          -0.0094 |         173.7295 |           7.9679 |
[32m[20221213 21:36:53 @agent_ppo2.py:185][0m |          -0.0095 |         173.2856 |           7.9612 |
[32m[20221213 21:36:53 @agent_ppo2.py:185][0m |          -0.0103 |         173.0527 |           7.9495 |
[32m[20221213 21:36:53 @agent_ppo2.py:185][0m |           0.0043 |         194.7757 |           7.9369 |
[32m[20221213 21:36:53 @agent_ppo2.py:185][0m |          -0.0083 |         172.8478 |           7.9778 |
[32m[20221213 21:36:53 @agent_ppo2.py:185][0m |          -0.0109 |         172.4492 |           7.9173 |
[32m[20221213 21:36:54 @agent_ppo2.py:185][0m |          -0.0117 |         172.3356 |           7.9112 |
[32m[20221213 21:36:54 @agent_ppo2.py:185][0m |          -0.0028 |         184.9160 |           7.8821 |
[32m[20221213 21:36:54 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:36:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.20
[32m[20221213 21:36:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.00
[32m[20221213 21:36:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 862.00
[32m[20221213 21:36:54 @agent_ppo2.py:143][0m Total time:      41.32 min
[32m[20221213 21:36:54 @agent_ppo2.py:145][0m 4040704 total steps have happened
[32m[20221213 21:36:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1973 --------------------------#
[32m[20221213 21:36:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:36:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:54 @agent_ppo2.py:185][0m |           0.0167 |         197.3991 |           7.7389 |
[32m[20221213 21:36:54 @agent_ppo2.py:185][0m |           0.0004 |         174.4619 |           7.7851 |
[32m[20221213 21:36:54 @agent_ppo2.py:185][0m |          -0.0033 |         174.0215 |           7.7515 |
[32m[20221213 21:36:54 @agent_ppo2.py:185][0m |          -0.0024 |         173.8562 |           7.6863 |
[32m[20221213 21:36:54 @agent_ppo2.py:185][0m |          -0.0040 |         173.5579 |           7.7383 |
[32m[20221213 21:36:55 @agent_ppo2.py:185][0m |          -0.0020 |         174.6242 |           7.7157 |
[32m[20221213 21:36:55 @agent_ppo2.py:185][0m |          -0.0036 |         173.1265 |           7.6439 |
[32m[20221213 21:36:55 @agent_ppo2.py:185][0m |          -0.0016 |         174.4936 |           7.6824 |
[32m[20221213 21:36:55 @agent_ppo2.py:185][0m |          -0.0065 |         172.8571 |           7.6549 |
[32m[20221213 21:36:55 @agent_ppo2.py:185][0m |           0.0013 |         174.2458 |           7.6689 |
[32m[20221213 21:36:55 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:36:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.80
[32m[20221213 21:36:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221213 21:36:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.00
[32m[20221213 21:36:55 @agent_ppo2.py:143][0m Total time:      41.34 min
[32m[20221213 21:36:55 @agent_ppo2.py:145][0m 4042752 total steps have happened
[32m[20221213 21:36:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1974 --------------------------#
[32m[20221213 21:36:55 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:55 @agent_ppo2.py:185][0m |          -0.0015 |         174.4148 |           7.8951 |
[32m[20221213 21:36:55 @agent_ppo2.py:185][0m |           0.0003 |         174.4264 |           7.9266 |
[32m[20221213 21:36:55 @agent_ppo2.py:185][0m |          -0.0050 |         173.5288 |           7.9462 |
[32m[20221213 21:36:56 @agent_ppo2.py:185][0m |          -0.0063 |         173.2958 |           7.9555 |
[32m[20221213 21:36:56 @agent_ppo2.py:185][0m |          -0.0066 |         173.0685 |           7.9327 |
[32m[20221213 21:36:56 @agent_ppo2.py:185][0m |          -0.0064 |         172.7582 |           7.9638 |
[32m[20221213 21:36:56 @agent_ppo2.py:185][0m |          -0.0071 |         172.6367 |           7.9657 |
[32m[20221213 21:36:56 @agent_ppo2.py:185][0m |          -0.0075 |         172.5555 |           7.9434 |
[32m[20221213 21:36:56 @agent_ppo2.py:185][0m |          -0.0092 |         172.4325 |           7.9625 |
[32m[20221213 21:36:56 @agent_ppo2.py:185][0m |          -0.0094 |         172.2558 |           7.9172 |
[32m[20221213 21:36:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:36:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.80
[32m[20221213 21:36:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.00
[32m[20221213 21:36:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.00
[32m[20221213 21:36:56 @agent_ppo2.py:143][0m Total time:      41.36 min
[32m[20221213 21:36:56 @agent_ppo2.py:145][0m 4044800 total steps have happened
[32m[20221213 21:36:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1975 --------------------------#
[32m[20221213 21:36:56 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:57 @agent_ppo2.py:185][0m |           0.0101 |         184.5590 |           7.7407 |
[32m[20221213 21:36:57 @agent_ppo2.py:185][0m |           0.0021 |         164.5680 |           7.7450 |
[32m[20221213 21:36:57 @agent_ppo2.py:185][0m |          -0.0036 |         160.1628 |           7.7685 |
[32m[20221213 21:36:57 @agent_ppo2.py:185][0m |          -0.0043 |         159.5566 |           7.7217 |
[32m[20221213 21:36:57 @agent_ppo2.py:185][0m |          -0.0038 |         159.0607 |           7.7187 |
[32m[20221213 21:36:57 @agent_ppo2.py:185][0m |          -0.0051 |         158.8755 |           7.6654 |
[32m[20221213 21:36:57 @agent_ppo2.py:185][0m |          -0.0068 |         158.7588 |           7.6976 |
[32m[20221213 21:36:57 @agent_ppo2.py:185][0m |          -0.0069 |         158.2760 |           7.7054 |
[32m[20221213 21:36:57 @agent_ppo2.py:185][0m |          -0.0023 |         166.1082 |           7.7360 |
[32m[20221213 21:36:57 @agent_ppo2.py:185][0m |          -0.0086 |         158.1995 |           7.6752 |
[32m[20221213 21:36:57 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:36:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 640.20
[32m[20221213 21:36:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.00
[32m[20221213 21:36:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.00
[32m[20221213 21:36:57 @agent_ppo2.py:143][0m Total time:      41.38 min
[32m[20221213 21:36:57 @agent_ppo2.py:145][0m 4046848 total steps have happened
[32m[20221213 21:36:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1976 --------------------------#
[32m[20221213 21:36:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:36:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:58 @agent_ppo2.py:185][0m |           0.0017 |         175.8451 |           7.5173 |
[32m[20221213 21:36:58 @agent_ppo2.py:185][0m |          -0.0019 |         174.6467 |           7.4901 |
[32m[20221213 21:36:58 @agent_ppo2.py:185][0m |          -0.0060 |         173.4235 |           7.5336 |
[32m[20221213 21:36:58 @agent_ppo2.py:185][0m |          -0.0082 |         173.2881 |           7.5134 |
[32m[20221213 21:36:58 @agent_ppo2.py:185][0m |          -0.0055 |         173.1202 |           7.5419 |
[32m[20221213 21:36:58 @agent_ppo2.py:185][0m |          -0.0076 |         173.0104 |           7.5012 |
[32m[20221213 21:36:58 @agent_ppo2.py:185][0m |          -0.0089 |         172.8323 |           7.4661 |
[32m[20221213 21:36:58 @agent_ppo2.py:185][0m |          -0.0095 |         172.8257 |           7.4873 |
[32m[20221213 21:36:58 @agent_ppo2.py:185][0m |          -0.0103 |         172.7643 |           7.4645 |
[32m[20221213 21:36:59 @agent_ppo2.py:185][0m |          -0.0035 |         177.5570 |           7.4598 |
[32m[20221213 21:36:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:36:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.60
[32m[20221213 21:36:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.00
[32m[20221213 21:36:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 865.00
[32m[20221213 21:36:59 @agent_ppo2.py:143][0m Total time:      41.40 min
[32m[20221213 21:36:59 @agent_ppo2.py:145][0m 4048896 total steps have happened
[32m[20221213 21:36:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1977 --------------------------#
[32m[20221213 21:36:59 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:36:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:36:59 @agent_ppo2.py:185][0m |           0.0028 |         176.9765 |           7.4828 |
[32m[20221213 21:36:59 @agent_ppo2.py:185][0m |          -0.0033 |         174.7912 |           7.5005 |
[32m[20221213 21:36:59 @agent_ppo2.py:185][0m |          -0.0053 |         173.0888 |           7.5104 |
[32m[20221213 21:36:59 @agent_ppo2.py:185][0m |           0.0183 |         199.9450 |           7.5075 |
[32m[20221213 21:36:59 @agent_ppo2.py:185][0m |          -0.0020 |         172.0299 |           7.5703 |
[32m[20221213 21:36:59 @agent_ppo2.py:185][0m |          -0.0069 |         171.7432 |           7.5591 |
[32m[20221213 21:37:00 @agent_ppo2.py:185][0m |          -0.0061 |         171.8461 |           7.5324 |
[32m[20221213 21:37:00 @agent_ppo2.py:185][0m |          -0.0072 |         171.2193 |           7.5848 |
[32m[20221213 21:37:00 @agent_ppo2.py:185][0m |          -0.0087 |         170.8850 |           7.6016 |
[32m[20221213 21:37:00 @agent_ppo2.py:185][0m |          -0.0080 |         170.5158 |           7.6396 |
[32m[20221213 21:37:00 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 21:37:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.20
[32m[20221213 21:37:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.00
[32m[20221213 21:37:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.00
[32m[20221213 21:37:00 @agent_ppo2.py:143][0m Total time:      41.42 min
[32m[20221213 21:37:00 @agent_ppo2.py:145][0m 4050944 total steps have happened
[32m[20221213 21:37:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1978 --------------------------#
[32m[20221213 21:37:00 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:00 @agent_ppo2.py:185][0m |          -0.0007 |         175.3661 |           7.6216 |
[32m[20221213 21:37:00 @agent_ppo2.py:185][0m |          -0.0040 |         174.5185 |           7.6804 |
[32m[20221213 21:37:00 @agent_ppo2.py:185][0m |          -0.0049 |         174.0238 |           7.6973 |
[32m[20221213 21:37:00 @agent_ppo2.py:185][0m |           0.0010 |         181.7094 |           7.7236 |
[32m[20221213 21:37:01 @agent_ppo2.py:185][0m |          -0.0032 |         173.3621 |           7.7980 |
[32m[20221213 21:37:01 @agent_ppo2.py:185][0m |          -0.0098 |         173.2031 |           7.7415 |
[32m[20221213 21:37:01 @agent_ppo2.py:185][0m |          -0.0081 |         172.9338 |           7.8139 |
[32m[20221213 21:37:01 @agent_ppo2.py:185][0m |          -0.0092 |         172.7322 |           7.7707 |
[32m[20221213 21:37:01 @agent_ppo2.py:185][0m |          -0.0097 |         172.5454 |           7.8236 |
[32m[20221213 21:37:01 @agent_ppo2.py:185][0m |          -0.0080 |         172.6362 |           7.8299 |
[32m[20221213 21:37:01 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:37:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.00
[32m[20221213 21:37:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.00
[32m[20221213 21:37:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.00
[32m[20221213 21:37:01 @agent_ppo2.py:143][0m Total time:      41.44 min
[32m[20221213 21:37:01 @agent_ppo2.py:145][0m 4052992 total steps have happened
[32m[20221213 21:37:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1979 --------------------------#
[32m[20221213 21:37:01 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:01 @agent_ppo2.py:185][0m |          -0.0015 |         177.2328 |           7.9331 |
[32m[20221213 21:37:02 @agent_ppo2.py:185][0m |           0.0046 |         195.6453 |           7.9728 |
[32m[20221213 21:37:02 @agent_ppo2.py:185][0m |          -0.0075 |         174.7307 |           7.9701 |
[32m[20221213 21:37:02 @agent_ppo2.py:185][0m |          -0.0035 |         177.0823 |           7.9958 |
[32m[20221213 21:37:02 @agent_ppo2.py:185][0m |          -0.0023 |         175.8710 |           7.9940 |
[32m[20221213 21:37:02 @agent_ppo2.py:185][0m |          -0.0091 |         173.2674 |           7.9757 |
[32m[20221213 21:37:02 @agent_ppo2.py:185][0m |          -0.0110 |         173.0161 |           7.9786 |
[32m[20221213 21:37:02 @agent_ppo2.py:185][0m |          -0.0100 |         172.7250 |           8.0217 |
[32m[20221213 21:37:02 @agent_ppo2.py:185][0m |          -0.0085 |         173.2577 |           8.0098 |
[32m[20221213 21:37:02 @agent_ppo2.py:185][0m |          -0.0113 |         172.2177 |           8.0364 |
[32m[20221213 21:37:02 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:37:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.40
[32m[20221213 21:37:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221213 21:37:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.00
[32m[20221213 21:37:02 @agent_ppo2.py:143][0m Total time:      41.46 min
[32m[20221213 21:37:02 @agent_ppo2.py:145][0m 4055040 total steps have happened
[32m[20221213 21:37:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1980 --------------------------#
[32m[20221213 21:37:02 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:03 @agent_ppo2.py:185][0m |           0.0025 |         176.9450 |           7.9685 |
[32m[20221213 21:37:03 @agent_ppo2.py:185][0m |          -0.0051 |         173.6641 |           7.9743 |
[32m[20221213 21:37:03 @agent_ppo2.py:185][0m |          -0.0063 |         171.9732 |           7.9872 |
[32m[20221213 21:37:03 @agent_ppo2.py:185][0m |          -0.0062 |         170.6134 |           7.9673 |
[32m[20221213 21:37:03 @agent_ppo2.py:185][0m |          -0.0068 |         169.8937 |           7.9249 |
[32m[20221213 21:37:03 @agent_ppo2.py:185][0m |          -0.0075 |         169.1235 |           7.9466 |
[32m[20221213 21:37:03 @agent_ppo2.py:185][0m |          -0.0075 |         168.4384 |           7.9650 |
[32m[20221213 21:37:03 @agent_ppo2.py:185][0m |          -0.0074 |         167.8767 |           7.9264 |
[32m[20221213 21:37:03 @agent_ppo2.py:185][0m |          -0.0040 |         169.3404 |           7.9297 |
[32m[20221213 21:37:03 @agent_ppo2.py:185][0m |          -0.0093 |         166.5582 |           7.9136 |
[32m[20221213 21:37:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:37:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.00
[32m[20221213 21:37:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.00
[32m[20221213 21:37:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.00
[32m[20221213 21:37:04 @agent_ppo2.py:143][0m Total time:      41.48 min
[32m[20221213 21:37:04 @agent_ppo2.py:145][0m 4057088 total steps have happened
[32m[20221213 21:37:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1981 --------------------------#
[32m[20221213 21:37:04 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:04 @agent_ppo2.py:185][0m |           0.0061 |         176.8751 |           8.0787 |
[32m[20221213 21:37:04 @agent_ppo2.py:185][0m |          -0.0029 |         175.6464 |           8.0063 |
[32m[20221213 21:37:04 @agent_ppo2.py:185][0m |          -0.0038 |         174.7523 |           7.9663 |
[32m[20221213 21:37:04 @agent_ppo2.py:185][0m |          -0.0039 |         174.2223 |           7.9876 |
[32m[20221213 21:37:04 @agent_ppo2.py:185][0m |          -0.0092 |         174.1358 |           8.0059 |
[32m[20221213 21:37:04 @agent_ppo2.py:185][0m |          -0.0049 |         173.7683 |           7.9539 |
[32m[20221213 21:37:04 @agent_ppo2.py:185][0m |          -0.0072 |         173.6200 |           7.9661 |
[32m[20221213 21:37:04 @agent_ppo2.py:185][0m |          -0.0066 |         173.3541 |           7.9604 |
[32m[20221213 21:37:05 @agent_ppo2.py:185][0m |          -0.0043 |         176.1372 |           7.9449 |
[32m[20221213 21:37:05 @agent_ppo2.py:185][0m |          -0.0062 |         173.2758 |           7.8914 |
[32m[20221213 21:37:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:37:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.40
[32m[20221213 21:37:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.00
[32m[20221213 21:37:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.00
[32m[20221213 21:37:05 @agent_ppo2.py:143][0m Total time:      41.50 min
[32m[20221213 21:37:05 @agent_ppo2.py:145][0m 4059136 total steps have happened
[32m[20221213 21:37:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1982 --------------------------#
[32m[20221213 21:37:05 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:05 @agent_ppo2.py:185][0m |           0.0019 |         177.6695 |           7.9576 |
[32m[20221213 21:37:05 @agent_ppo2.py:185][0m |          -0.0061 |         175.1247 |           7.9429 |
[32m[20221213 21:37:05 @agent_ppo2.py:185][0m |          -0.0047 |         174.7096 |           7.9483 |
[32m[20221213 21:37:05 @agent_ppo2.py:185][0m |          -0.0070 |         174.4424 |           7.9612 |
[32m[20221213 21:37:05 @agent_ppo2.py:185][0m |          -0.0053 |         174.5101 |           8.0031 |
[32m[20221213 21:37:06 @agent_ppo2.py:185][0m |          -0.0028 |         177.0535 |           7.9742 |
[32m[20221213 21:37:06 @agent_ppo2.py:185][0m |          -0.0056 |         174.1919 |           8.0154 |
[32m[20221213 21:37:06 @agent_ppo2.py:185][0m |          -0.0074 |         173.9392 |           7.9903 |
[32m[20221213 21:37:06 @agent_ppo2.py:185][0m |          -0.0088 |         173.6502 |           7.9953 |
[32m[20221213 21:37:06 @agent_ppo2.py:185][0m |          -0.0096 |         173.5833 |           8.0024 |
[32m[20221213 21:37:06 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 21:37:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.60
[32m[20221213 21:37:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.00
[32m[20221213 21:37:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.00
[32m[20221213 21:37:06 @agent_ppo2.py:143][0m Total time:      41.52 min
[32m[20221213 21:37:06 @agent_ppo2.py:145][0m 4061184 total steps have happened
[32m[20221213 21:37:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1983 --------------------------#
[32m[20221213 21:37:06 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:06 @agent_ppo2.py:185][0m |           0.0045 |         176.4096 |           7.8873 |
[32m[20221213 21:37:06 @agent_ppo2.py:185][0m |          -0.0038 |         174.1541 |           7.8450 |
[32m[20221213 21:37:07 @agent_ppo2.py:185][0m |          -0.0051 |         173.6805 |           7.7788 |
[32m[20221213 21:37:07 @agent_ppo2.py:185][0m |          -0.0070 |         173.3902 |           7.7600 |
[32m[20221213 21:37:07 @agent_ppo2.py:185][0m |          -0.0025 |         176.1817 |           7.7878 |
[32m[20221213 21:37:07 @agent_ppo2.py:185][0m |           0.0043 |         180.4031 |           7.7742 |
[32m[20221213 21:37:07 @agent_ppo2.py:185][0m |          -0.0071 |         172.9508 |           7.7377 |
[32m[20221213 21:37:07 @agent_ppo2.py:185][0m |          -0.0029 |         174.4231 |           7.7704 |
[32m[20221213 21:37:07 @agent_ppo2.py:185][0m |          -0.0094 |         172.5794 |           7.7464 |
[32m[20221213 21:37:07 @agent_ppo2.py:185][0m |           0.0092 |         199.4081 |           7.7613 |
[32m[20221213 21:37:07 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 21:37:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.20
[32m[20221213 21:37:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.00
[32m[20221213 21:37:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.00
[32m[20221213 21:37:07 @agent_ppo2.py:143][0m Total time:      41.54 min
[32m[20221213 21:37:07 @agent_ppo2.py:145][0m 4063232 total steps have happened
[32m[20221213 21:37:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1984 --------------------------#
[32m[20221213 21:37:07 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:08 @agent_ppo2.py:185][0m |          -0.0019 |         176.5888 |           7.8925 |
[32m[20221213 21:37:08 @agent_ppo2.py:185][0m |           0.0005 |         176.7590 |           7.8459 |
[32m[20221213 21:37:08 @agent_ppo2.py:185][0m |          -0.0047 |         174.1226 |           7.8923 |
[32m[20221213 21:37:08 @agent_ppo2.py:185][0m |          -0.0074 |         173.8195 |           7.9280 |
[32m[20221213 21:37:08 @agent_ppo2.py:185][0m |          -0.0018 |         176.5928 |           7.9402 |
[32m[20221213 21:37:08 @agent_ppo2.py:185][0m |           0.0002 |         178.2039 |           8.0030 |
[32m[20221213 21:37:08 @agent_ppo2.py:185][0m |          -0.0077 |         173.1830 |           8.0223 |
[32m[20221213 21:37:08 @agent_ppo2.py:185][0m |          -0.0049 |         173.4842 |           8.0392 |
[32m[20221213 21:37:08 @agent_ppo2.py:185][0m |          -0.0025 |         177.0614 |           8.0511 |
[32m[20221213 21:37:09 @agent_ppo2.py:185][0m |          -0.0039 |         173.9115 |           8.0560 |
[32m[20221213 21:37:09 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 21:37:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.20
[32m[20221213 21:37:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.00
[32m[20221213 21:37:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.00
[32m[20221213 21:37:09 @agent_ppo2.py:143][0m Total time:      41.57 min
[32m[20221213 21:37:09 @agent_ppo2.py:145][0m 4065280 total steps have happened
[32m[20221213 21:37:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1985 --------------------------#
[32m[20221213 21:37:09 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:09 @agent_ppo2.py:185][0m |           0.0010 |         175.0770 |           7.9670 |
[32m[20221213 21:37:09 @agent_ppo2.py:185][0m |          -0.0033 |         174.4001 |           7.9313 |
[32m[20221213 21:37:09 @agent_ppo2.py:185][0m |          -0.0044 |         174.1057 |           7.8995 |
[32m[20221213 21:37:09 @agent_ppo2.py:185][0m |          -0.0063 |         173.8757 |           7.8665 |
[32m[20221213 21:37:09 @agent_ppo2.py:185][0m |          -0.0062 |         173.6984 |           7.8230 |
[32m[20221213 21:37:09 @agent_ppo2.py:185][0m |          -0.0066 |         173.5782 |           7.8332 |
[32m[20221213 21:37:10 @agent_ppo2.py:185][0m |          -0.0083 |         173.4309 |           7.8097 |
[32m[20221213 21:37:10 @agent_ppo2.py:185][0m |          -0.0081 |         173.2531 |           7.7895 |
[32m[20221213 21:37:10 @agent_ppo2.py:185][0m |          -0.0086 |         173.2606 |           7.7566 |
[32m[20221213 21:37:10 @agent_ppo2.py:185][0m |          -0.0068 |         173.2980 |           7.7471 |
[32m[20221213 21:37:10 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 21:37:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.40
[32m[20221213 21:37:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.00
[32m[20221213 21:37:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.00
[32m[20221213 21:37:10 @agent_ppo2.py:143][0m Total time:      41.59 min
[32m[20221213 21:37:10 @agent_ppo2.py:145][0m 4067328 total steps have happened
[32m[20221213 21:37:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1986 --------------------------#
[32m[20221213 21:37:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 21:37:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:10 @agent_ppo2.py:185][0m |          -0.0027 |         174.9216 |           7.7747 |
[32m[20221213 21:37:10 @agent_ppo2.py:185][0m |          -0.0063 |         173.1968 |           7.8179 |
[32m[20221213 21:37:10 @agent_ppo2.py:185][0m |          -0.0074 |         172.2072 |           7.8073 |
[32m[20221213 21:37:11 @agent_ppo2.py:185][0m |          -0.0085 |         171.6528 |           7.8585 |
[32m[20221213 21:37:11 @agent_ppo2.py:185][0m |          -0.0105 |         170.6318 |           7.8219 |
[32m[20221213 21:37:11 @agent_ppo2.py:185][0m |          -0.0104 |         170.3585 |           7.8088 |
[32m[20221213 21:37:11 @agent_ppo2.py:185][0m |          -0.0104 |         169.7125 |           7.8146 |
[32m[20221213 21:37:11 @agent_ppo2.py:185][0m |           0.0121 |         193.8814 |           7.8213 |
[32m[20221213 21:37:11 @agent_ppo2.py:185][0m |          -0.0102 |         169.2018 |           7.8312 |
[32m[20221213 21:37:11 @agent_ppo2.py:185][0m |          -0.0130 |         168.8019 |           7.7713 |
[32m[20221213 21:37:11 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 21:37:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.40
[32m[20221213 21:37:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.00
[32m[20221213 21:37:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.00
[32m[20221213 21:37:11 @agent_ppo2.py:143][0m Total time:      41.61 min
[32m[20221213 21:37:11 @agent_ppo2.py:145][0m 4069376 total steps have happened
[32m[20221213 21:37:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1987 --------------------------#
[32m[20221213 21:37:11 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:12 @agent_ppo2.py:185][0m |          -0.0022 |         176.4853 |           7.3646 |
[32m[20221213 21:37:12 @agent_ppo2.py:185][0m |          -0.0024 |         176.2333 |           7.3437 |
[32m[20221213 21:37:12 @agent_ppo2.py:185][0m |          -0.0050 |         175.5020 |           7.3854 |
[32m[20221213 21:37:12 @agent_ppo2.py:185][0m |          -0.0074 |         175.0860 |           7.3553 |
[32m[20221213 21:37:12 @agent_ppo2.py:185][0m |          -0.0080 |         174.7582 |           7.3904 |
[32m[20221213 21:37:12 @agent_ppo2.py:185][0m |          -0.0046 |         175.1583 |           7.3680 |
[32m[20221213 21:37:12 @agent_ppo2.py:185][0m |          -0.0094 |         174.3512 |           7.3317 |
[32m[20221213 21:37:12 @agent_ppo2.py:185][0m |          -0.0083 |         174.1112 |           7.3531 |
[32m[20221213 21:37:12 @agent_ppo2.py:185][0m |          -0.0082 |         174.1222 |           7.3284 |
[32m[20221213 21:37:12 @agent_ppo2.py:185][0m |          -0.0099 |         173.9637 |           7.3317 |
[32m[20221213 21:37:12 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 21:37:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.40
[32m[20221213 21:37:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:37:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.00
[32m[20221213 21:37:13 @agent_ppo2.py:143][0m Total time:      41.63 min
[32m[20221213 21:37:13 @agent_ppo2.py:145][0m 4071424 total steps have happened
[32m[20221213 21:37:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1988 --------------------------#
[32m[20221213 21:37:13 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:13 @agent_ppo2.py:185][0m |           0.0048 |         179.3859 |           7.3560 |
[32m[20221213 21:37:13 @agent_ppo2.py:185][0m |          -0.0021 |         176.0511 |           7.4627 |
[32m[20221213 21:37:13 @agent_ppo2.py:185][0m |          -0.0040 |         175.1115 |           7.4085 |
[32m[20221213 21:37:13 @agent_ppo2.py:185][0m |          -0.0022 |         174.9888 |           7.4448 |
[32m[20221213 21:37:13 @agent_ppo2.py:185][0m |          -0.0037 |         174.0871 |           7.4307 |
[32m[20221213 21:37:13 @agent_ppo2.py:185][0m |          -0.0042 |         173.2795 |           7.4655 |
[32m[20221213 21:37:14 @agent_ppo2.py:185][0m |          -0.0065 |         172.6108 |           7.4399 |
[32m[20221213 21:37:14 @agent_ppo2.py:185][0m |          -0.0089 |         172.4932 |           7.4639 |
[32m[20221213 21:37:14 @agent_ppo2.py:185][0m |          -0.0075 |         171.9829 |           7.4698 |
[32m[20221213 21:37:14 @agent_ppo2.py:185][0m |          -0.0094 |         171.7131 |           7.4641 |
[32m[20221213 21:37:14 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 21:37:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 758.20
[32m[20221213 21:37:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.00
[32m[20221213 21:37:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 866.00
[32m[20221213 21:37:14 @agent_ppo2.py:143][0m Total time:      41.65 min
[32m[20221213 21:37:14 @agent_ppo2.py:145][0m 4073472 total steps have happened
[32m[20221213 21:37:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1989 --------------------------#
[32m[20221213 21:37:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:37:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:14 @agent_ppo2.py:185][0m |          -0.0004 |         178.6352 |           7.6914 |
[32m[20221213 21:37:14 @agent_ppo2.py:185][0m |          -0.0056 |         176.4334 |           7.6782 |
[32m[20221213 21:37:14 @agent_ppo2.py:185][0m |          -0.0017 |         178.4041 |           7.6425 |
[32m[20221213 21:37:14 @agent_ppo2.py:185][0m |          -0.0089 |         174.9904 |           7.6487 |
[32m[20221213 21:37:15 @agent_ppo2.py:185][0m |          -0.0018 |         179.0567 |           7.7109 |
[32m[20221213 21:37:15 @agent_ppo2.py:185][0m |           0.0058 |         193.2687 |           7.6832 |
[32m[20221213 21:37:15 @agent_ppo2.py:185][0m |          -0.0090 |         174.0650 |           7.6528 |
[32m[20221213 21:37:15 @agent_ppo2.py:185][0m |          -0.0072 |         173.7148 |           7.6331 |
[32m[20221213 21:37:15 @agent_ppo2.py:185][0m |          -0.0119 |         173.4405 |           7.6387 |
[32m[20221213 21:37:15 @agent_ppo2.py:185][0m |          -0.0109 |         173.5898 |           7.6004 |
[32m[20221213 21:37:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:37:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.80
[32m[20221213 21:37:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:37:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.00
[32m[20221213 21:37:15 @agent_ppo2.py:143][0m Total time:      41.67 min
[32m[20221213 21:37:15 @agent_ppo2.py:145][0m 4075520 total steps have happened
[32m[20221213 21:37:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1990 --------------------------#
[32m[20221213 21:37:15 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:37:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:15 @agent_ppo2.py:185][0m |          -0.0002 |         174.9400 |           7.3510 |
[32m[20221213 21:37:15 @agent_ppo2.py:185][0m |           0.0051 |         179.9026 |           7.3336 |
[32m[20221213 21:37:16 @agent_ppo2.py:185][0m |           0.0065 |         185.1102 |           7.3659 |
[32m[20221213 21:37:16 @agent_ppo2.py:185][0m |          -0.0046 |         170.8513 |           7.3686 |
[32m[20221213 21:37:16 @agent_ppo2.py:185][0m |          -0.0063 |         170.3091 |           7.3580 |
[32m[20221213 21:37:16 @agent_ppo2.py:185][0m |          -0.0063 |         169.9703 |           7.3927 |
[32m[20221213 21:37:16 @agent_ppo2.py:185][0m |          -0.0040 |         171.5060 |           7.4405 |
[32m[20221213 21:37:16 @agent_ppo2.py:185][0m |          -0.0068 |         169.3355 |           7.3556 |
[32m[20221213 21:37:16 @agent_ppo2.py:185][0m |           0.0007 |         174.4463 |           7.3307 |
[32m[20221213 21:37:16 @agent_ppo2.py:185][0m |          -0.0101 |         169.2393 |           7.3925 |
[32m[20221213 21:37:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:37:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.60
[32m[20221213 21:37:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221213 21:37:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 712.00
[32m[20221213 21:37:16 @agent_ppo2.py:143][0m Total time:      41.69 min
[32m[20221213 21:37:16 @agent_ppo2.py:145][0m 4077568 total steps have happened
[32m[20221213 21:37:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1991 --------------------------#
[32m[20221213 21:37:16 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:37:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:17 @agent_ppo2.py:185][0m |          -0.0001 |         174.0834 |           7.6611 |
[32m[20221213 21:37:17 @agent_ppo2.py:185][0m |          -0.0001 |         173.8965 |           7.6629 |
[32m[20221213 21:37:17 @agent_ppo2.py:185][0m |          -0.0006 |         175.2325 |           7.6481 |
[32m[20221213 21:37:17 @agent_ppo2.py:185][0m |          -0.0061 |         172.4317 |           7.6512 |
[32m[20221213 21:37:17 @agent_ppo2.py:185][0m |          -0.0058 |         172.5448 |           7.6599 |
[32m[20221213 21:37:17 @agent_ppo2.py:185][0m |          -0.0057 |         172.0163 |           7.6367 |
[32m[20221213 21:37:17 @agent_ppo2.py:185][0m |          -0.0090 |         172.0249 |           7.6433 |
[32m[20221213 21:37:17 @agent_ppo2.py:185][0m |          -0.0104 |         171.8490 |           7.6901 |
[32m[20221213 21:37:17 @agent_ppo2.py:185][0m |          -0.0091 |         171.7614 |           7.6264 |
[32m[20221213 21:37:17 @agent_ppo2.py:185][0m |          -0.0108 |         171.5724 |           7.6585 |
[32m[20221213 21:37:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:37:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.80
[32m[20221213 21:37:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.00
[32m[20221213 21:37:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.00
[32m[20221213 21:37:17 @agent_ppo2.py:143][0m Total time:      41.71 min
[32m[20221213 21:37:17 @agent_ppo2.py:145][0m 4079616 total steps have happened
[32m[20221213 21:37:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1992 --------------------------#
[32m[20221213 21:37:18 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:18 @agent_ppo2.py:185][0m |          -0.0013 |         171.4093 |           6.8819 |
[32m[20221213 21:37:18 @agent_ppo2.py:185][0m |          -0.0035 |         171.5826 |           6.9568 |
[32m[20221213 21:37:18 @agent_ppo2.py:185][0m |           0.0014 |         173.7006 |           6.9655 |
[32m[20221213 21:37:18 @agent_ppo2.py:185][0m |          -0.0079 |         170.2982 |           7.0096 |
[32m[20221213 21:37:18 @agent_ppo2.py:185][0m |           0.0036 |         191.1283 |           7.0094 |
[32m[20221213 21:37:18 @agent_ppo2.py:185][0m |          -0.0052 |         170.2917 |           7.1320 |
[32m[20221213 21:37:18 @agent_ppo2.py:185][0m |          -0.0075 |         169.4455 |           7.1241 |
[32m[20221213 21:37:18 @agent_ppo2.py:185][0m |          -0.0090 |         169.3434 |           7.1729 |
[32m[20221213 21:37:18 @agent_ppo2.py:185][0m |          -0.0102 |         169.2077 |           7.1477 |
[32m[20221213 21:37:19 @agent_ppo2.py:185][0m |          -0.0093 |         169.0165 |           7.2073 |
[32m[20221213 21:37:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:37:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.20
[32m[20221213 21:37:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.00
[32m[20221213 21:37:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 870.00
[32m[20221213 21:37:19 @agent_ppo2.py:143][0m Total time:      41.73 min
[32m[20221213 21:37:19 @agent_ppo2.py:145][0m 4081664 total steps have happened
[32m[20221213 21:37:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1993 --------------------------#
[32m[20221213 21:37:19 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:19 @agent_ppo2.py:185][0m |           0.0016 |         172.3080 |           7.5569 |
[32m[20221213 21:37:19 @agent_ppo2.py:185][0m |          -0.0031 |         171.3516 |           7.5127 |
[32m[20221213 21:37:19 @agent_ppo2.py:185][0m |          -0.0020 |         170.7772 |           7.5215 |
[32m[20221213 21:37:19 @agent_ppo2.py:185][0m |          -0.0037 |         170.4194 |           7.4276 |
[32m[20221213 21:37:19 @agent_ppo2.py:185][0m |          -0.0044 |         170.3467 |           7.4114 |
[32m[20221213 21:37:19 @agent_ppo2.py:185][0m |          -0.0052 |         170.3059 |           7.3870 |
[32m[20221213 21:37:20 @agent_ppo2.py:185][0m |          -0.0069 |         170.2279 |           7.3303 |
[32m[20221213 21:37:20 @agent_ppo2.py:185][0m |          -0.0065 |         169.7590 |           7.3289 |
[32m[20221213 21:37:20 @agent_ppo2.py:185][0m |          -0.0079 |         169.7601 |           7.2972 |
[32m[20221213 21:37:20 @agent_ppo2.py:185][0m |          -0.0085 |         169.6232 |           7.2143 |
[32m[20221213 21:37:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:37:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.80
[32m[20221213 21:37:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.00
[32m[20221213 21:37:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 871.00
[32m[20221213 21:37:20 @agent_ppo2.py:143][0m Total time:      41.75 min
[32m[20221213 21:37:20 @agent_ppo2.py:145][0m 4083712 total steps have happened
[32m[20221213 21:37:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1994 --------------------------#
[32m[20221213 21:37:20 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:20 @agent_ppo2.py:185][0m |          -0.0030 |         172.7015 |           7.1188 |
[32m[20221213 21:37:20 @agent_ppo2.py:185][0m |           0.0043 |         185.8484 |           7.1814 |
[32m[20221213 21:37:20 @agent_ppo2.py:185][0m |          -0.0023 |         171.5357 |           7.3210 |
[32m[20221213 21:37:20 @agent_ppo2.py:185][0m |          -0.0070 |         171.1173 |           7.2444 |
[32m[20221213 21:37:21 @agent_ppo2.py:185][0m |          -0.0043 |         171.6815 |           7.2522 |
[32m[20221213 21:37:21 @agent_ppo2.py:185][0m |          -0.0080 |         170.7055 |           7.2861 |
[32m[20221213 21:37:21 @agent_ppo2.py:185][0m |          -0.0035 |         170.8578 |           7.2488 |
[32m[20221213 21:37:21 @agent_ppo2.py:185][0m |          -0.0092 |         170.3332 |           7.2916 |
[32m[20221213 21:37:21 @agent_ppo2.py:185][0m |          -0.0089 |         169.9982 |           7.3356 |
[32m[20221213 21:37:21 @agent_ppo2.py:185][0m |          -0.0098 |         169.8486 |           7.3358 |
[32m[20221213 21:37:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:37:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.20
[32m[20221213 21:37:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.00
[32m[20221213 21:37:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 702.00
[32m[20221213 21:37:21 @agent_ppo2.py:143][0m Total time:      41.77 min
[32m[20221213 21:37:21 @agent_ppo2.py:145][0m 4085760 total steps have happened
[32m[20221213 21:37:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1995 --------------------------#
[32m[20221213 21:37:21 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:37:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:21 @agent_ppo2.py:185][0m |           0.0005 |         173.2166 |           7.2147 |
[32m[20221213 21:37:21 @agent_ppo2.py:185][0m |          -0.0058 |         170.6619 |           7.3249 |
[32m[20221213 21:37:22 @agent_ppo2.py:185][0m |          -0.0041 |         169.4067 |           7.3005 |
[32m[20221213 21:37:22 @agent_ppo2.py:185][0m |          -0.0063 |         167.8229 |           7.2764 |
[32m[20221213 21:37:22 @agent_ppo2.py:185][0m |          -0.0078 |         167.0128 |           7.2772 |
[32m[20221213 21:37:22 @agent_ppo2.py:185][0m |          -0.0081 |         166.1994 |           7.3291 |
[32m[20221213 21:37:22 @agent_ppo2.py:185][0m |           0.0019 |         176.6663 |           7.2794 |
[32m[20221213 21:37:22 @agent_ppo2.py:185][0m |          -0.0087 |         165.1546 |           7.2927 |
[32m[20221213 21:37:22 @agent_ppo2.py:185][0m |          -0.0064 |         165.1669 |           7.2550 |
[32m[20221213 21:37:22 @agent_ppo2.py:185][0m |          -0.0101 |         164.0411 |           7.2703 |
[32m[20221213 21:37:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:37:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.20
[32m[20221213 21:37:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.00
[32m[20221213 21:37:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.00
[32m[20221213 21:37:22 @agent_ppo2.py:143][0m Total time:      41.79 min
[32m[20221213 21:37:22 @agent_ppo2.py:145][0m 4087808 total steps have happened
[32m[20221213 21:37:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1996 --------------------------#
[32m[20221213 21:37:22 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:23 @agent_ppo2.py:185][0m |          -0.0017 |         180.4640 |           7.1777 |
[32m[20221213 21:37:23 @agent_ppo2.py:185][0m |          -0.0028 |         176.6308 |           7.1391 |
[32m[20221213 21:37:23 @agent_ppo2.py:185][0m |          -0.0009 |         176.0969 |           7.2091 |
[32m[20221213 21:37:23 @agent_ppo2.py:185][0m |          -0.0049 |         175.0479 |           7.2210 |
[32m[20221213 21:37:23 @agent_ppo2.py:185][0m |          -0.0056 |         174.5763 |           7.1927 |
[32m[20221213 21:37:23 @agent_ppo2.py:185][0m |          -0.0070 |         174.4464 |           7.1948 |
[32m[20221213 21:37:23 @agent_ppo2.py:185][0m |          -0.0039 |         174.7311 |           7.1944 |
[32m[20221213 21:37:23 @agent_ppo2.py:185][0m |          -0.0083 |         173.6617 |           7.1706 |
[32m[20221213 21:37:23 @agent_ppo2.py:185][0m |          -0.0056 |         174.1888 |           7.2343 |
[32m[20221213 21:37:23 @agent_ppo2.py:185][0m |          -0.0104 |         173.5263 |           7.1902 |
[32m[20221213 21:37:23 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 21:37:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.60
[32m[20221213 21:37:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.00
[32m[20221213 21:37:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.00
[32m[20221213 21:37:24 @agent_ppo2.py:143][0m Total time:      41.81 min
[32m[20221213 21:37:24 @agent_ppo2.py:145][0m 4089856 total steps have happened
[32m[20221213 21:37:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1997 --------------------------#
[32m[20221213 21:37:24 @agent_ppo2.py:127][0m Sampling time: 0.12 s by 5 slaves
[32m[20221213 21:37:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:24 @agent_ppo2.py:185][0m |           0.0005 |         173.6967 |           7.5936 |
[32m[20221213 21:37:24 @agent_ppo2.py:185][0m |          -0.0020 |         173.0415 |           7.6636 |
[32m[20221213 21:37:24 @agent_ppo2.py:185][0m |          -0.0053 |         172.1997 |           7.5844 |
[32m[20221213 21:37:24 @agent_ppo2.py:185][0m |           0.0029 |         185.5054 |           7.5786 |
[32m[20221213 21:37:24 @agent_ppo2.py:185][0m |          -0.0067 |         171.6354 |           7.5232 |
[32m[20221213 21:37:24 @agent_ppo2.py:185][0m |           0.0044 |         189.1624 |           7.5535 |
[32m[20221213 21:37:24 @agent_ppo2.py:185][0m |          -0.0059 |         171.2182 |           7.6381 |
[32m[20221213 21:37:24 @agent_ppo2.py:185][0m |          -0.0049 |         170.9954 |           7.5476 |
[32m[20221213 21:37:24 @agent_ppo2.py:185][0m |          -0.0084 |         170.9608 |           7.5122 |
[32m[20221213 21:37:25 @agent_ppo2.py:185][0m |          -0.0090 |         170.7987 |           7.4968 |
[32m[20221213 21:37:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 21:37:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.40
[32m[20221213 21:37:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221213 21:37:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.00
[32m[20221213 21:37:25 @agent_ppo2.py:143][0m Total time:      41.83 min
[32m[20221213 21:37:25 @agent_ppo2.py:145][0m 4091904 total steps have happened
[32m[20221213 21:37:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1998 --------------------------#
[32m[20221213 21:37:25 @agent_ppo2.py:127][0m Sampling time: 0.13 s by 5 slaves
[32m[20221213 21:37:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:25 @agent_ppo2.py:185][0m |          -0.0022 |         175.7732 |           7.4260 |
[32m[20221213 21:37:25 @agent_ppo2.py:185][0m |          -0.0044 |         174.8593 |           7.4773 |
[32m[20221213 21:37:25 @agent_ppo2.py:185][0m |          -0.0038 |         174.8971 |           7.5039 |
[32m[20221213 21:37:25 @agent_ppo2.py:185][0m |          -0.0054 |         174.3024 |           7.4756 |
[32m[20221213 21:37:25 @agent_ppo2.py:185][0m |          -0.0068 |         174.1474 |           7.4361 |
[32m[20221213 21:37:25 @agent_ppo2.py:185][0m |          -0.0064 |         174.0591 |           7.4010 |
[32m[20221213 21:37:26 @agent_ppo2.py:185][0m |          -0.0077 |         173.8356 |           7.4258 |
[32m[20221213 21:37:26 @agent_ppo2.py:185][0m |           0.0030 |         181.6404 |           7.3610 |
[32m[20221213 21:37:26 @agent_ppo2.py:185][0m |          -0.0073 |         174.0221 |           7.3716 |
[32m[20221213 21:37:26 @agent_ppo2.py:185][0m |          -0.0085 |         173.5499 |           7.3479 |
[32m[20221213 21:37:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 21:37:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.40
[32m[20221213 21:37:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221213 21:37:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.00
[32m[20221213 21:37:26 @agent_ppo2.py:143][0m Total time:      41.85 min
[32m[20221213 21:37:26 @agent_ppo2.py:145][0m 4093952 total steps have happened
[32m[20221213 21:37:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1999 --------------------------#
[32m[20221213 21:37:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 21:37:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 21:37:26 @agent_ppo2.py:185][0m |          -0.0017 |         173.7817 |           7.0154 |
[32m[20221213 21:37:26 @agent_ppo2.py:185][0m |          -0.0059 |         173.3306 |           7.0180 |
[32m[20221213 21:37:26 @agent_ppo2.py:185][0m |          -0.0049 |         173.1327 |           7.0033 |
[32m[20221213 21:37:26 @agent_ppo2.py:185][0m |          -0.0067 |         172.9028 |           7.0474 |
[32m[20221213 21:37:27 @agent_ppo2.py:185][0m |          -0.0055 |         172.6809 |           7.0749 |
[32m[20221213 21:37:27 @agent_ppo2.py:185][0m |          -0.0061 |         172.8177 |           7.1354 |
[32m[20221213 21:37:27 @agent_ppo2.py:185][0m |           0.0012 |         190.3412 |           7.1718 |
[32m[20221213 21:37:27 @agent_ppo2.py:185][0m |          -0.0056 |         176.3801 |           7.2096 |
[32m[20221213 21:37:27 @agent_ppo2.py:185][0m |          -0.0089 |         172.2997 |           7.2298 |
[32m[20221213 21:37:27 @agent_ppo2.py:185][0m |           0.0021 |         182.4614 |           7.2376 |
[32m[20221213 21:37:27 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 21:37:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.60
[32m[20221213 21:37:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.00
[32m[20221213 21:37:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.00
[32m[20221213 21:37:27 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 879.00
[32m[20221213 21:37:27 @agent_ppo2.py:143][0m Total time:      41.87 min
[32m[20221213 21:37:27 @agent_ppo2.py:145][0m 4096000 total steps have happened
[32m[20221213 21:37:27 @train.py:55][0m [4m[34mCRITICAL[0m Training completed!
