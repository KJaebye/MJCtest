[32m[20221214 13:58:02 @logger.py:105][0m Log file set to ./tmp/cartpole/swingup/20221214_135802/log/cartpole_swingup-20221214_135802.log
[32m[20221214 13:58:02 @agent_ppo2.py:121][0m #------------------------ Iteration 0 --------------------------#
[32m[20221214 13:58:02 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 13:58:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:03 @agent_ppo2.py:185][0m |          -0.0020 |           0.1016 |          20.9326 |
[32m[20221214 13:58:03 @agent_ppo2.py:185][0m |          -0.0092 |           0.0761 |          20.9330 |
[32m[20221214 13:58:03 @agent_ppo2.py:185][0m |           0.0058 |           0.0691 |          20.9332 |
[32m[20221214 13:58:03 @agent_ppo2.py:185][0m |          -0.0033 |           0.0640 |          20.9327 |
[32m[20221214 13:58:03 @agent_ppo2.py:185][0m |           0.0043 |           0.0597 |          20.9318 |
[32m[20221214 13:58:03 @agent_ppo2.py:185][0m |          -0.0059 |           0.0565 |          20.9306 |
[32m[20221214 13:58:03 @agent_ppo2.py:185][0m |           0.0048 |           0.0535 |          20.9309 |
[32m[20221214 13:58:03 @agent_ppo2.py:185][0m |          -0.0043 |           0.0514 |          20.9282 |
[32m[20221214 13:58:03 @agent_ppo2.py:185][0m |          -0.0030 |           0.0491 |          20.9276 |
[32m[20221214 13:58:03 @agent_ppo2.py:185][0m |          -0.0036 |           0.0479 |          20.9245 |
[32m[20221214 13:58:03 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 13:58:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 12.01
[32m[20221214 13:58:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 17.96
[32m[20221214 13:58:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.07
[32m[20221214 13:58:03 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 0.07
[32m[20221214 13:58:03 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 0.07
[32m[20221214 13:58:03 @agent_ppo2.py:143][0m Total time:       0.02 min
[32m[20221214 13:58:03 @agent_ppo2.py:145][0m 2048 total steps have happened
[32m[20221214 13:58:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1 --------------------------#
[32m[20221214 13:58:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:04 @agent_ppo2.py:185][0m |           0.0000 |           1.6015 |          20.8125 |
[32m[20221214 13:58:04 @agent_ppo2.py:185][0m |          -0.0100 |           1.1916 |          20.8129 |
[32m[20221214 13:58:04 @agent_ppo2.py:185][0m |           0.0026 |           0.9299 |          20.8119 |
[32m[20221214 13:58:04 @agent_ppo2.py:185][0m |          -0.0019 |           0.8008 |          20.8114 |
[32m[20221214 13:58:04 @agent_ppo2.py:185][0m |          -0.0054 |           0.6961 |          20.8082 |
[32m[20221214 13:58:04 @agent_ppo2.py:185][0m |          -0.0112 |           0.6135 |          20.8054 |
[32m[20221214 13:58:04 @agent_ppo2.py:185][0m |          -0.0030 |           0.5172 |          20.8003 |
[32m[20221214 13:58:04 @agent_ppo2.py:185][0m |           0.0006 |           0.4463 |          20.7970 |
[32m[20221214 13:58:04 @agent_ppo2.py:185][0m |          -0.0034 |           0.3975 |          20.7943 |
[32m[20221214 13:58:05 @agent_ppo2.py:185][0m |           0.0010 |           0.3512 |          20.7908 |
[32m[20221214 13:58:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 13:58:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.03
[32m[20221214 13:58:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 90.77
[32m[20221214 13:58:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.57
[32m[20221214 13:58:05 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 0.57
[32m[20221214 13:58:05 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 0.57
[32m[20221214 13:58:05 @agent_ppo2.py:143][0m Total time:       0.04 min
[32m[20221214 13:58:05 @agent_ppo2.py:145][0m 4096 total steps have happened
[32m[20221214 13:58:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2 --------------------------#
[32m[20221214 13:58:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:05 @agent_ppo2.py:185][0m |           0.0049 |           3.5735 |          20.8930 |
[32m[20221214 13:58:05 @agent_ppo2.py:185][0m |          -0.0012 |           1.5599 |          20.8943 |
[32m[20221214 13:58:05 @agent_ppo2.py:185][0m |          -0.0087 |           1.0891 |          20.8916 |
[32m[20221214 13:58:05 @agent_ppo2.py:185][0m |           0.0025 |           0.8643 |          20.8885 |
[32m[20221214 13:58:05 @agent_ppo2.py:185][0m |          -0.0061 |           0.7411 |          20.8858 |
[32m[20221214 13:58:05 @agent_ppo2.py:185][0m |          -0.0001 |           0.6470 |          20.8820 |
[32m[20221214 13:58:05 @agent_ppo2.py:185][0m |          -0.0091 |           0.6002 |          20.8796 |
[32m[20221214 13:58:06 @agent_ppo2.py:185][0m |          -0.0034 |           0.5364 |          20.8744 |
[32m[20221214 13:58:06 @agent_ppo2.py:185][0m |           0.0009 |           0.4905 |          20.8720 |
[32m[20221214 13:58:06 @agent_ppo2.py:185][0m |           0.0004 |           0.4597 |          20.8698 |
[32m[20221214 13:58:06 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 13:58:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 137.29
[32m[20221214 13:58:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 167.70
[32m[20221214 13:58:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.88
[32m[20221214 13:58:06 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 57.88
[32m[20221214 13:58:06 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 57.88
[32m[20221214 13:58:06 @agent_ppo2.py:143][0m Total time:       0.06 min
[32m[20221214 13:58:06 @agent_ppo2.py:145][0m 6144 total steps have happened
[32m[20221214 13:58:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3 --------------------------#
[32m[20221214 13:58:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:06 @agent_ppo2.py:185][0m |           0.0058 |           3.5200 |          20.8500 |
[32m[20221214 13:58:06 @agent_ppo2.py:185][0m |           0.0017 |           2.1732 |          20.8504 |
[32m[20221214 13:58:06 @agent_ppo2.py:185][0m |           0.0029 |           1.7625 |          20.8501 |
[32m[20221214 13:58:06 @agent_ppo2.py:185][0m |           0.0045 |           1.5516 |          20.8505 |
[32m[20221214 13:58:07 @agent_ppo2.py:185][0m |           0.0002 |           1.4091 |          20.8502 |
[32m[20221214 13:58:07 @agent_ppo2.py:185][0m |           0.0094 |           1.4742 |          20.8488 |
[32m[20221214 13:58:07 @agent_ppo2.py:185][0m |          -0.0027 |           1.2608 |          20.8491 |
[32m[20221214 13:58:07 @agent_ppo2.py:185][0m |           0.0051 |           1.1821 |          20.8483 |
[32m[20221214 13:58:07 @agent_ppo2.py:185][0m |          -0.0022 |           1.1163 |          20.8475 |
[32m[20221214 13:58:07 @agent_ppo2.py:185][0m |           0.0029 |           1.0972 |          20.8446 |
[32m[20221214 13:58:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 13:58:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.47
[32m[20221214 13:58:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 184.98
[32m[20221214 13:58:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 154.97
[32m[20221214 13:58:07 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 154.97
[32m[20221214 13:58:07 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 154.97
[32m[20221214 13:58:07 @agent_ppo2.py:143][0m Total time:       0.08 min
[32m[20221214 13:58:07 @agent_ppo2.py:145][0m 8192 total steps have happened
[32m[20221214 13:58:07 @agent_ppo2.py:121][0m #------------------------ Iteration 4 --------------------------#
[32m[20221214 13:58:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:08 @agent_ppo2.py:185][0m |          -0.0043 |           1.3038 |          20.9423 |
[32m[20221214 13:58:08 @agent_ppo2.py:185][0m |          -0.0017 |           0.8042 |          20.9329 |
[32m[20221214 13:58:08 @agent_ppo2.py:185][0m |          -0.0058 |           0.7265 |          20.9225 |
[32m[20221214 13:58:08 @agent_ppo2.py:185][0m |          -0.0010 |           0.6869 |          20.9120 |
[32m[20221214 13:58:08 @agent_ppo2.py:185][0m |          -0.0059 |           0.6649 |          20.9026 |
[32m[20221214 13:58:08 @agent_ppo2.py:185][0m |          -0.0075 |           0.6473 |          20.8961 |
[32m[20221214 13:58:08 @agent_ppo2.py:185][0m |          -0.0082 |           0.6205 |          20.8902 |
[32m[20221214 13:58:08 @agent_ppo2.py:185][0m |          -0.0046 |           0.6221 |          20.8814 |
[32m[20221214 13:58:08 @agent_ppo2.py:185][0m |          -0.0117 |           0.5957 |          20.8766 |
[32m[20221214 13:58:08 @agent_ppo2.py:185][0m |          -0.0060 |           0.5792 |          20.8701 |
[32m[20221214 13:58:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 13:58:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 66.78
[32m[20221214 13:58:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 104.24
[32m[20221214 13:58:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 118.93
[32m[20221214 13:58:08 @agent_ppo2.py:143][0m Total time:       0.11 min
[32m[20221214 13:58:08 @agent_ppo2.py:145][0m 10240 total steps have happened
[32m[20221214 13:58:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5 --------------------------#
[32m[20221214 13:58:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:09 @agent_ppo2.py:185][0m |          -0.0001 |           5.2971 |          20.9036 |
[32m[20221214 13:58:09 @agent_ppo2.py:185][0m |          -0.0013 |           3.5565 |          20.9046 |
[32m[20221214 13:58:09 @agent_ppo2.py:185][0m |           0.0005 |           3.1414 |          20.9038 |
[32m[20221214 13:58:09 @agent_ppo2.py:185][0m |          -0.0005 |           2.8501 |          20.9012 |
[32m[20221214 13:58:09 @agent_ppo2.py:185][0m |           0.0036 |           2.7500 |          20.8999 |
[32m[20221214 13:58:09 @agent_ppo2.py:185][0m |           0.0042 |           2.6356 |          20.8999 |
[32m[20221214 13:58:09 @agent_ppo2.py:185][0m |           0.0045 |           2.5504 |          20.8981 |
[32m[20221214 13:58:09 @agent_ppo2.py:185][0m |          -0.0043 |           2.4316 |          20.8962 |
[32m[20221214 13:58:09 @agent_ppo2.py:185][0m |          -0.0013 |           2.4200 |          20.8965 |
[32m[20221214 13:58:10 @agent_ppo2.py:185][0m |          -0.0005 |           2.3090 |          20.8962 |
[32m[20221214 13:58:10 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 13:58:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 117.14
[32m[20221214 13:58:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 144.38
[32m[20221214 13:58:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 120.68
[32m[20221214 13:58:10 @agent_ppo2.py:143][0m Total time:       0.13 min
[32m[20221214 13:58:10 @agent_ppo2.py:145][0m 12288 total steps have happened
[32m[20221214 13:58:10 @agent_ppo2.py:121][0m #------------------------ Iteration 6 --------------------------#
[32m[20221214 13:58:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:10 @agent_ppo2.py:185][0m |          -0.0008 |           6.0777 |          20.9285 |
[32m[20221214 13:58:10 @agent_ppo2.py:185][0m |          -0.0060 |           4.3995 |          20.9246 |
[32m[20221214 13:58:10 @agent_ppo2.py:185][0m |          -0.0008 |           3.7747 |          20.9145 |
[32m[20221214 13:58:10 @agent_ppo2.py:185][0m |          -0.0013 |           3.4869 |          20.9120 |
[32m[20221214 13:58:10 @agent_ppo2.py:185][0m |           0.0042 |           3.4645 |          20.9073 |
[32m[20221214 13:58:10 @agent_ppo2.py:185][0m |          -0.0026 |           3.1217 |          20.9054 |
[32m[20221214 13:58:11 @agent_ppo2.py:185][0m |          -0.0053 |           2.9972 |          20.9021 |
[32m[20221214 13:58:11 @agent_ppo2.py:185][0m |           0.0020 |           3.1168 |          20.9001 |
[32m[20221214 13:58:11 @agent_ppo2.py:185][0m |          -0.0037 |           2.8572 |          20.8977 |
[32m[20221214 13:58:11 @agent_ppo2.py:185][0m |          -0.0050 |           2.6946 |          20.8943 |
[32m[20221214 13:58:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 13:58:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 134.00
[32m[20221214 13:58:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 202.84
[32m[20221214 13:58:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 143.51
[32m[20221214 13:58:11 @agent_ppo2.py:143][0m Total time:       0.15 min
[32m[20221214 13:58:11 @agent_ppo2.py:145][0m 14336 total steps have happened
[32m[20221214 13:58:11 @agent_ppo2.py:121][0m #------------------------ Iteration 7 --------------------------#
[32m[20221214 13:58:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:11 @agent_ppo2.py:185][0m |          -0.0004 |           7.4985 |          20.9800 |
[32m[20221214 13:58:11 @agent_ppo2.py:185][0m |           0.0006 |           5.1316 |          20.9792 |
[32m[20221214 13:58:11 @agent_ppo2.py:185][0m |          -0.0016 |           4.4890 |          20.9755 |
[32m[20221214 13:58:12 @agent_ppo2.py:185][0m |           0.0022 |           4.0789 |          20.9741 |
[32m[20221214 13:58:12 @agent_ppo2.py:185][0m |          -0.0054 |           3.7624 |          20.9704 |
[32m[20221214 13:58:12 @agent_ppo2.py:185][0m |          -0.0063 |           3.5662 |          20.9661 |
[32m[20221214 13:58:12 @agent_ppo2.py:185][0m |          -0.0018 |           3.3407 |          20.9630 |
[32m[20221214 13:58:12 @agent_ppo2.py:185][0m |           0.0000 |           3.2423 |          20.9595 |
[32m[20221214 13:58:12 @agent_ppo2.py:185][0m |           0.0032 |           3.0602 |          20.9569 |
[32m[20221214 13:58:12 @agent_ppo2.py:185][0m |          -0.0080 |           3.0782 |          20.9538 |
[32m[20221214 13:58:12 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 13:58:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 173.07
[32m[20221214 13:58:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 239.25
[32m[20221214 13:58:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 147.69
[32m[20221214 13:58:12 @agent_ppo2.py:143][0m Total time:       0.17 min
[32m[20221214 13:58:12 @agent_ppo2.py:145][0m 16384 total steps have happened
[32m[20221214 13:58:12 @agent_ppo2.py:121][0m #------------------------ Iteration 8 --------------------------#
[32m[20221214 13:58:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:13 @agent_ppo2.py:185][0m |          -0.0010 |           8.6091 |          21.0102 |
[32m[20221214 13:58:13 @agent_ppo2.py:185][0m |          -0.0084 |           6.8442 |          21.0078 |
[32m[20221214 13:58:13 @agent_ppo2.py:185][0m |           0.0004 |           6.1878 |          21.0039 |
[32m[20221214 13:58:13 @agent_ppo2.py:185][0m |          -0.0076 |           5.8221 |          20.9998 |
[32m[20221214 13:58:13 @agent_ppo2.py:185][0m |          -0.0031 |           5.4497 |          20.9939 |
[32m[20221214 13:58:13 @agent_ppo2.py:185][0m |          -0.0012 |           5.1803 |          20.9888 |
[32m[20221214 13:58:13 @agent_ppo2.py:185][0m |          -0.0058 |           4.9786 |          20.9868 |
[32m[20221214 13:58:13 @agent_ppo2.py:185][0m |           0.0005 |           4.8542 |          20.9817 |
[32m[20221214 13:58:13 @agent_ppo2.py:185][0m |          -0.0066 |           4.6326 |          20.9801 |
[32m[20221214 13:58:13 @agent_ppo2.py:185][0m |          -0.0001 |           4.5144 |          20.9788 |
[32m[20221214 13:58:13 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 13:58:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 182.67
[32m[20221214 13:58:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 223.55
[32m[20221214 13:58:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 140.09
[32m[20221214 13:58:13 @agent_ppo2.py:143][0m Total time:       0.19 min
[32m[20221214 13:58:13 @agent_ppo2.py:145][0m 18432 total steps have happened
[32m[20221214 13:58:13 @agent_ppo2.py:121][0m #------------------------ Iteration 9 --------------------------#
[32m[20221214 13:58:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:14 @agent_ppo2.py:185][0m |          -0.0099 |           5.6115 |          21.0282 |
[32m[20221214 13:58:14 @agent_ppo2.py:185][0m |          -0.0003 |           4.3913 |          21.0271 |
[32m[20221214 13:58:14 @agent_ppo2.py:185][0m |           0.0043 |           4.1135 |          21.0264 |
[32m[20221214 13:58:14 @agent_ppo2.py:185][0m |          -0.0041 |           3.6752 |          21.0238 |
[32m[20221214 13:58:14 @agent_ppo2.py:185][0m |          -0.0009 |           3.4823 |          21.0205 |
[32m[20221214 13:58:14 @agent_ppo2.py:185][0m |          -0.0048 |           3.2539 |          21.0161 |
[32m[20221214 13:58:14 @agent_ppo2.py:185][0m |          -0.0040 |           3.1009 |          21.0117 |
[32m[20221214 13:58:14 @agent_ppo2.py:185][0m |          -0.0003 |           2.9608 |          21.0088 |
[32m[20221214 13:58:14 @agent_ppo2.py:185][0m |          -0.0061 |           2.8584 |          21.0043 |
[32m[20221214 13:58:15 @agent_ppo2.py:185][0m |          -0.0062 |           2.7789 |          21.0026 |
[32m[20221214 13:58:15 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 13:58:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 145.08
[32m[20221214 13:58:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 160.73
[32m[20221214 13:58:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 168.07
[32m[20221214 13:58:15 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 168.07
[32m[20221214 13:58:15 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 168.07
[32m[20221214 13:58:15 @agent_ppo2.py:143][0m Total time:       0.21 min
[32m[20221214 13:58:15 @agent_ppo2.py:145][0m 20480 total steps have happened
[32m[20221214 13:58:15 @agent_ppo2.py:121][0m #------------------------ Iteration 10 --------------------------#
[32m[20221214 13:58:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 13:58:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:15 @agent_ppo2.py:185][0m |           0.0027 |           6.5967 |          20.8456 |
[32m[20221214 13:58:15 @agent_ppo2.py:185][0m |          -0.0033 |           5.2196 |          20.8430 |
[32m[20221214 13:58:15 @agent_ppo2.py:185][0m |           0.0012 |           4.9103 |          20.8393 |
[32m[20221214 13:58:15 @agent_ppo2.py:185][0m |           0.0071 |           4.6875 |          20.8357 |
[32m[20221214 13:58:15 @agent_ppo2.py:185][0m |           0.0122 |           5.4586 |          20.8325 |
[32m[20221214 13:58:15 @agent_ppo2.py:185][0m |           0.0054 |           4.6107 |          20.8239 |
[32m[20221214 13:58:16 @agent_ppo2.py:185][0m |          -0.0049 |           4.1908 |          20.8217 |
[32m[20221214 13:58:16 @agent_ppo2.py:185][0m |           0.0064 |           4.2582 |          20.8190 |
[32m[20221214 13:58:16 @agent_ppo2.py:185][0m |          -0.0003 |           4.0423 |          20.8172 |
[32m[20221214 13:58:16 @agent_ppo2.py:185][0m |          -0.0058 |           3.9503 |          20.8172 |
[32m[20221214 13:58:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 13:58:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 165.49
[32m[20221214 13:58:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 210.37
[32m[20221214 13:58:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 153.86
[32m[20221214 13:58:16 @agent_ppo2.py:143][0m Total time:       0.23 min
[32m[20221214 13:58:16 @agent_ppo2.py:145][0m 22528 total steps have happened
[32m[20221214 13:58:16 @agent_ppo2.py:121][0m #------------------------ Iteration 11 --------------------------#
[32m[20221214 13:58:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:16 @agent_ppo2.py:185][0m |           0.0029 |           7.7861 |          20.9460 |
[32m[20221214 13:58:16 @agent_ppo2.py:185][0m |           0.0014 |           6.3635 |          20.9440 |
[32m[20221214 13:58:16 @agent_ppo2.py:185][0m |          -0.0021 |           5.7727 |          20.9397 |
[32m[20221214 13:58:17 @agent_ppo2.py:185][0m |          -0.0038 |           5.4167 |          20.9319 |
[32m[20221214 13:58:17 @agent_ppo2.py:185][0m |          -0.0071 |           5.2230 |          20.9300 |
[32m[20221214 13:58:17 @agent_ppo2.py:185][0m |          -0.0042 |           5.0230 |          20.9247 |
[32m[20221214 13:58:17 @agent_ppo2.py:185][0m |          -0.0068 |           4.8479 |          20.9235 |
[32m[20221214 13:58:17 @agent_ppo2.py:185][0m |          -0.0037 |           4.6756 |          20.9222 |
[32m[20221214 13:58:17 @agent_ppo2.py:185][0m |          -0.0029 |           4.5910 |          20.9199 |
[32m[20221214 13:58:17 @agent_ppo2.py:185][0m |          -0.0043 |           4.4811 |          20.9181 |
[32m[20221214 13:58:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 13:58:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 162.13
[32m[20221214 13:58:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 192.11
[32m[20221214 13:58:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 200.44
[32m[20221214 13:58:17 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 200.44
[32m[20221214 13:58:17 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 200.44
[32m[20221214 13:58:17 @agent_ppo2.py:143][0m Total time:       0.25 min
[32m[20221214 13:58:17 @agent_ppo2.py:145][0m 24576 total steps have happened
[32m[20221214 13:58:17 @agent_ppo2.py:121][0m #------------------------ Iteration 12 --------------------------#
[32m[20221214 13:58:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:18 @agent_ppo2.py:185][0m |           0.0025 |           9.9843 |          21.0004 |
[32m[20221214 13:58:18 @agent_ppo2.py:185][0m |           0.0025 |           8.0572 |          21.0006 |
[32m[20221214 13:58:18 @agent_ppo2.py:185][0m |          -0.0023 |           7.4894 |          20.9991 |
[32m[20221214 13:58:18 @agent_ppo2.py:185][0m |          -0.0029 |           7.1359 |          20.9983 |
[32m[20221214 13:58:18 @agent_ppo2.py:185][0m |           0.0058 |           7.0027 |          20.9990 |
[32m[20221214 13:58:18 @agent_ppo2.py:185][0m |          -0.0043 |           6.5735 |          20.9976 |
[32m[20221214 13:58:18 @agent_ppo2.py:185][0m |          -0.0012 |           6.3649 |          20.9943 |
[32m[20221214 13:58:18 @agent_ppo2.py:185][0m |          -0.0044 |           6.2080 |          20.9944 |
[32m[20221214 13:58:18 @agent_ppo2.py:185][0m |          -0.0002 |           6.0620 |          20.9916 |
[32m[20221214 13:58:18 @agent_ppo2.py:185][0m |           0.0029 |           5.9640 |          20.9907 |
[32m[20221214 13:58:18 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 13:58:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 191.16
[32m[20221214 13:58:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 212.16
[32m[20221214 13:58:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 202.93
[32m[20221214 13:58:18 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 202.93
[32m[20221214 13:58:18 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 202.93
[32m[20221214 13:58:18 @agent_ppo2.py:143][0m Total time:       0.27 min
[32m[20221214 13:58:18 @agent_ppo2.py:145][0m 26624 total steps have happened
[32m[20221214 13:58:18 @agent_ppo2.py:121][0m #------------------------ Iteration 13 --------------------------#
[32m[20221214 13:58:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:19 @agent_ppo2.py:185][0m |          -0.0016 |           8.2577 |          21.0331 |
[32m[20221214 13:58:19 @agent_ppo2.py:185][0m |           0.0019 |           6.9965 |          21.0305 |
[32m[20221214 13:58:19 @agent_ppo2.py:185][0m |           0.0054 |           6.5756 |          21.0293 |
[32m[20221214 13:58:19 @agent_ppo2.py:185][0m |          -0.0093 |           6.3346 |          21.0229 |
[32m[20221214 13:58:19 @agent_ppo2.py:185][0m |          -0.0040 |           6.0240 |          21.0180 |
[32m[20221214 13:58:19 @agent_ppo2.py:185][0m |          -0.0071 |           5.9241 |          21.0125 |
[32m[20221214 13:58:19 @agent_ppo2.py:185][0m |           0.0001 |           5.8096 |          21.0102 |
[32m[20221214 13:58:19 @agent_ppo2.py:185][0m |          -0.0002 |           5.7862 |          21.0033 |
[32m[20221214 13:58:19 @agent_ppo2.py:185][0m |          -0.0044 |           5.6628 |          21.0012 |
[32m[20221214 13:58:20 @agent_ppo2.py:185][0m |          -0.0022 |           5.5838 |          20.9987 |
[32m[20221214 13:58:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 13:58:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 184.05
[32m[20221214 13:58:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 213.92
[32m[20221214 13:58:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 193.57
[32m[20221214 13:58:20 @agent_ppo2.py:143][0m Total time:       0.29 min
[32m[20221214 13:58:20 @agent_ppo2.py:145][0m 28672 total steps have happened
[32m[20221214 13:58:20 @agent_ppo2.py:121][0m #------------------------ Iteration 14 --------------------------#
[32m[20221214 13:58:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:20 @agent_ppo2.py:185][0m |           0.0127 |           8.0238 |          21.0184 |
[32m[20221214 13:58:20 @agent_ppo2.py:185][0m |           0.0012 |           6.4520 |          21.0088 |
[32m[20221214 13:58:20 @agent_ppo2.py:185][0m |          -0.0023 |           6.0384 |          21.0010 |
[32m[20221214 13:58:20 @agent_ppo2.py:185][0m |          -0.0005 |           5.8283 |          20.9935 |
[32m[20221214 13:58:20 @agent_ppo2.py:185][0m |          -0.0018 |           5.6348 |          20.9864 |
[32m[20221214 13:58:21 @agent_ppo2.py:185][0m |          -0.0010 |           5.5339 |          20.9789 |
[32m[20221214 13:58:21 @agent_ppo2.py:185][0m |          -0.0003 |           5.3782 |          20.9719 |
[32m[20221214 13:58:21 @agent_ppo2.py:185][0m |          -0.0075 |           5.3239 |          20.9666 |
[32m[20221214 13:58:21 @agent_ppo2.py:185][0m |          -0.0010 |           5.2007 |          20.9613 |
[32m[20221214 13:58:21 @agent_ppo2.py:185][0m |          -0.0012 |           5.1453 |          20.9556 |
[32m[20221214 13:58:21 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 13:58:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 186.75
[32m[20221214 13:58:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 232.13
[32m[20221214 13:58:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 183.40
[32m[20221214 13:58:21 @agent_ppo2.py:143][0m Total time:       0.32 min
[32m[20221214 13:58:21 @agent_ppo2.py:145][0m 30720 total steps have happened
[32m[20221214 13:58:21 @agent_ppo2.py:121][0m #------------------------ Iteration 15 --------------------------#
[32m[20221214 13:58:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:21 @agent_ppo2.py:185][0m |           0.0071 |           6.8652 |          21.0719 |
[32m[20221214 13:58:21 @agent_ppo2.py:185][0m |           0.0022 |           5.7495 |          21.0649 |
[32m[20221214 13:58:22 @agent_ppo2.py:185][0m |          -0.0026 |           5.5088 |          21.0550 |
[32m[20221214 13:58:22 @agent_ppo2.py:185][0m |           0.0034 |           5.5093 |          21.0515 |
[32m[20221214 13:58:22 @agent_ppo2.py:185][0m |          -0.0031 |           5.2692 |          21.0462 |
[32m[20221214 13:58:22 @agent_ppo2.py:185][0m |           0.0042 |           5.4400 |          21.0441 |
[32m[20221214 13:58:22 @agent_ppo2.py:185][0m |          -0.0009 |           5.1199 |          21.0499 |
[32m[20221214 13:58:22 @agent_ppo2.py:185][0m |           0.0027 |           5.0599 |          21.0387 |
[32m[20221214 13:58:22 @agent_ppo2.py:185][0m |          -0.0023 |           4.9834 |          21.0434 |
[32m[20221214 13:58:22 @agent_ppo2.py:185][0m |          -0.0042 |           4.9432 |          21.0394 |
[32m[20221214 13:58:22 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 13:58:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 173.47
[32m[20221214 13:58:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 197.53
[32m[20221214 13:58:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 166.46
[32m[20221214 13:58:22 @agent_ppo2.py:143][0m Total time:       0.34 min
[32m[20221214 13:58:22 @agent_ppo2.py:145][0m 32768 total steps have happened
[32m[20221214 13:58:22 @agent_ppo2.py:121][0m #------------------------ Iteration 16 --------------------------#
[32m[20221214 13:58:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:23 @agent_ppo2.py:185][0m |          -0.0004 |           8.1135 |          21.0589 |
[32m[20221214 13:58:23 @agent_ppo2.py:185][0m |          -0.0009 |           6.1807 |          21.0554 |
[32m[20221214 13:58:23 @agent_ppo2.py:185][0m |          -0.0043 |           5.8383 |          21.0516 |
[32m[20221214 13:58:23 @agent_ppo2.py:185][0m |          -0.0093 |           5.6786 |          21.0438 |
[32m[20221214 13:58:23 @agent_ppo2.py:185][0m |           0.0001 |           5.4954 |          21.0450 |
[32m[20221214 13:58:23 @agent_ppo2.py:185][0m |           0.0054 |           5.9036 |          21.0434 |
[32m[20221214 13:58:23 @agent_ppo2.py:185][0m |          -0.0021 |           5.2983 |          21.0384 |
[32m[20221214 13:58:23 @agent_ppo2.py:185][0m |          -0.0007 |           5.1645 |          21.0428 |
[32m[20221214 13:58:23 @agent_ppo2.py:185][0m |           0.0015 |           5.1384 |          21.0411 |
[32m[20221214 13:58:23 @agent_ppo2.py:185][0m |          -0.0016 |           5.0542 |          21.0382 |
[32m[20221214 13:58:23 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 13:58:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 182.19
[32m[20221214 13:58:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 255.60
[32m[20221214 13:58:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 170.81
[32m[20221214 13:58:24 @agent_ppo2.py:143][0m Total time:       0.36 min
[32m[20221214 13:58:24 @agent_ppo2.py:145][0m 34816 total steps have happened
[32m[20221214 13:58:24 @agent_ppo2.py:121][0m #------------------------ Iteration 17 --------------------------#
[32m[20221214 13:58:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:24 @agent_ppo2.py:185][0m |          -0.0038 |          10.1054 |          21.0140 |
[32m[20221214 13:58:24 @agent_ppo2.py:185][0m |          -0.0070 |           8.6700 |          21.0126 |
[32m[20221214 13:58:24 @agent_ppo2.py:185][0m |          -0.0065 |           8.1888 |          21.0066 |
[32m[20221214 13:58:24 @agent_ppo2.py:185][0m |          -0.0035 |           7.8425 |          20.9995 |
[32m[20221214 13:58:24 @agent_ppo2.py:185][0m |          -0.0045 |           7.6996 |          20.9965 |
[32m[20221214 13:58:24 @agent_ppo2.py:185][0m |          -0.0030 |           7.3493 |          20.9947 |
[32m[20221214 13:58:24 @agent_ppo2.py:185][0m |          -0.0052 |           7.1830 |          20.9891 |
[32m[20221214 13:58:25 @agent_ppo2.py:185][0m |          -0.0083 |           6.9935 |          20.9874 |
[32m[20221214 13:58:25 @agent_ppo2.py:185][0m |          -0.0050 |           6.8595 |          20.9861 |
[32m[20221214 13:58:25 @agent_ppo2.py:185][0m |          -0.0025 |           6.7088 |          20.9830 |
[32m[20221214 13:58:25 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 13:58:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 217.04
[32m[20221214 13:58:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.17
[32m[20221214 13:58:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 170.40
[32m[20221214 13:58:25 @agent_ppo2.py:143][0m Total time:       0.38 min
[32m[20221214 13:58:25 @agent_ppo2.py:145][0m 36864 total steps have happened
[32m[20221214 13:58:25 @agent_ppo2.py:121][0m #------------------------ Iteration 18 --------------------------#
[32m[20221214 13:58:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:25 @agent_ppo2.py:185][0m |           0.0069 |          10.1157 |          21.0150 |
[32m[20221214 13:58:25 @agent_ppo2.py:185][0m |          -0.0037 |           8.0839 |          21.0045 |
[32m[20221214 13:58:26 @agent_ppo2.py:185][0m |          -0.0004 |           7.6378 |          21.0012 |
[32m[20221214 13:58:26 @agent_ppo2.py:185][0m |          -0.0035 |           7.4409 |          21.0019 |
[32m[20221214 13:58:26 @agent_ppo2.py:185][0m |          -0.0020 |           7.3272 |          20.9976 |
[32m[20221214 13:58:26 @agent_ppo2.py:185][0m |          -0.0033 |           7.2349 |          21.0011 |
[32m[20221214 13:58:26 @agent_ppo2.py:185][0m |          -0.0062 |           7.1383 |          21.0020 |
[32m[20221214 13:58:26 @agent_ppo2.py:185][0m |          -0.0022 |           7.0813 |          20.9953 |
[32m[20221214 13:58:26 @agent_ppo2.py:185][0m |          -0.0090 |           6.9580 |          20.9995 |
[32m[20221214 13:58:26 @agent_ppo2.py:185][0m |           0.0054 |           7.1198 |          20.9988 |
[32m[20221214 13:58:26 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 13:58:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 189.15
[32m[20221214 13:58:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 211.40
[32m[20221214 13:58:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 180.90
[32m[20221214 13:58:26 @agent_ppo2.py:143][0m Total time:       0.40 min
[32m[20221214 13:58:26 @agent_ppo2.py:145][0m 38912 total steps have happened
[32m[20221214 13:58:26 @agent_ppo2.py:121][0m #------------------------ Iteration 19 --------------------------#
[32m[20221214 13:58:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:27 @agent_ppo2.py:185][0m |          -0.0020 |          10.9795 |          21.1699 |
[32m[20221214 13:58:27 @agent_ppo2.py:185][0m |          -0.0021 |           9.8455 |          21.1679 |
[32m[20221214 13:58:27 @agent_ppo2.py:185][0m |          -0.0052 |           9.4120 |          21.1563 |
[32m[20221214 13:58:27 @agent_ppo2.py:185][0m |           0.0004 |           9.3335 |          21.1558 |
[32m[20221214 13:58:27 @agent_ppo2.py:185][0m |          -0.0026 |           8.9483 |          21.1539 |
[32m[20221214 13:58:27 @agent_ppo2.py:185][0m |          -0.0063 |           8.7881 |          21.1557 |
[32m[20221214 13:58:27 @agent_ppo2.py:185][0m |          -0.0024 |           8.8475 |          21.1567 |
[32m[20221214 13:58:27 @agent_ppo2.py:185][0m |          -0.0098 |           8.5962 |          21.1547 |
[32m[20221214 13:58:27 @agent_ppo2.py:185][0m |          -0.0007 |           8.4892 |          21.1539 |
[32m[20221214 13:58:27 @agent_ppo2.py:185][0m |          -0.0026 |           8.4454 |          21.1566 |
[32m[20221214 13:58:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 13:58:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 219.85
[32m[20221214 13:58:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.06
[32m[20221214 13:58:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 182.77
[32m[20221214 13:58:28 @agent_ppo2.py:143][0m Total time:       0.42 min
[32m[20221214 13:58:28 @agent_ppo2.py:145][0m 40960 total steps have happened
[32m[20221214 13:58:28 @agent_ppo2.py:121][0m #------------------------ Iteration 20 --------------------------#
[32m[20221214 13:58:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 13:58:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:28 @agent_ppo2.py:185][0m |          -0.0037 |          12.0244 |          20.9938 |
[32m[20221214 13:58:28 @agent_ppo2.py:185][0m |           0.0028 |          10.8524 |          20.9832 |
[32m[20221214 13:58:28 @agent_ppo2.py:185][0m |           0.0051 |          10.7200 |          20.9804 |
[32m[20221214 13:58:28 @agent_ppo2.py:185][0m |          -0.0048 |           9.5482 |          20.9814 |
[32m[20221214 13:58:28 @agent_ppo2.py:185][0m |          -0.0057 |           9.1777 |          20.9791 |
[32m[20221214 13:58:28 @agent_ppo2.py:185][0m |          -0.0065 |           8.9433 |          20.9770 |
[32m[20221214 13:58:28 @agent_ppo2.py:185][0m |          -0.0022 |           8.7341 |          20.9802 |
[32m[20221214 13:58:29 @agent_ppo2.py:185][0m |          -0.0008 |           8.5367 |          20.9773 |
[32m[20221214 13:58:29 @agent_ppo2.py:185][0m |           0.0031 |           9.0888 |          20.9764 |
[32m[20221214 13:58:29 @agent_ppo2.py:185][0m |          -0.0050 |           8.2686 |          20.9756 |
[32m[20221214 13:58:29 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 13:58:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 217.47
[32m[20221214 13:58:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 241.08
[32m[20221214 13:58:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 195.72
[32m[20221214 13:58:29 @agent_ppo2.py:143][0m Total time:       0.45 min
[32m[20221214 13:58:29 @agent_ppo2.py:145][0m 43008 total steps have happened
[32m[20221214 13:58:29 @agent_ppo2.py:121][0m #------------------------ Iteration 21 --------------------------#
[32m[20221214 13:58:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:29 @agent_ppo2.py:185][0m |           0.0038 |          10.2173 |          21.1177 |
[32m[20221214 13:58:29 @agent_ppo2.py:185][0m |          -0.0011 |           8.5731 |          21.1023 |
[32m[20221214 13:58:29 @agent_ppo2.py:185][0m |          -0.0057 |           8.1476 |          21.0853 |
[32m[20221214 13:58:29 @agent_ppo2.py:185][0m |          -0.0056 |           7.8846 |          21.0774 |
[32m[20221214 13:58:30 @agent_ppo2.py:185][0m |          -0.0098 |           7.7476 |          21.0781 |
[32m[20221214 13:58:30 @agent_ppo2.py:185][0m |          -0.0102 |           7.6232 |          21.0735 |
[32m[20221214 13:58:30 @agent_ppo2.py:185][0m |          -0.0040 |           7.5041 |          21.0753 |
[32m[20221214 13:58:30 @agent_ppo2.py:185][0m |          -0.0069 |           7.4011 |          21.0708 |
[32m[20221214 13:58:30 @agent_ppo2.py:185][0m |          -0.0068 |           7.3136 |          21.0699 |
[32m[20221214 13:58:30 @agent_ppo2.py:185][0m |          -0.0075 |           7.2596 |          21.0639 |
[32m[20221214 13:58:30 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 13:58:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.57
[32m[20221214 13:58:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.97
[32m[20221214 13:58:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 214.91
[32m[20221214 13:58:30 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 214.91
[32m[20221214 13:58:30 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 214.91
[32m[20221214 13:58:30 @agent_ppo2.py:143][0m Total time:       0.47 min
[32m[20221214 13:58:30 @agent_ppo2.py:145][0m 45056 total steps have happened
[32m[20221214 13:58:30 @agent_ppo2.py:121][0m #------------------------ Iteration 22 --------------------------#
[32m[20221214 13:58:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:31 @agent_ppo2.py:185][0m |          -0.0021 |          15.0679 |          20.9693 |
[32m[20221214 13:58:31 @agent_ppo2.py:185][0m |           0.0007 |          12.8185 |          20.9633 |
[32m[20221214 13:58:31 @agent_ppo2.py:185][0m |          -0.0026 |          11.9059 |          20.9524 |
[32m[20221214 13:58:31 @agent_ppo2.py:185][0m |          -0.0020 |          11.3721 |          20.9465 |
[32m[20221214 13:58:31 @agent_ppo2.py:185][0m |          -0.0043 |          10.9722 |          20.9431 |
[32m[20221214 13:58:31 @agent_ppo2.py:185][0m |          -0.0036 |          10.6702 |          20.9397 |
[32m[20221214 13:58:31 @agent_ppo2.py:185][0m |          -0.0047 |          10.3352 |          20.9338 |
[32m[20221214 13:58:31 @agent_ppo2.py:185][0m |          -0.0020 |          10.0476 |          20.9334 |
[32m[20221214 13:58:31 @agent_ppo2.py:185][0m |          -0.0054 |           9.8487 |          20.9275 |
[32m[20221214 13:58:31 @agent_ppo2.py:185][0m |          -0.0022 |           9.6674 |          20.9302 |
[32m[20221214 13:58:31 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 13:58:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.92
[32m[20221214 13:58:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 312.61
[32m[20221214 13:58:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 224.06
[32m[20221214 13:58:32 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 224.06
[32m[20221214 13:58:32 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 224.06
[32m[20221214 13:58:32 @agent_ppo2.py:143][0m Total time:       0.49 min
[32m[20221214 13:58:32 @agent_ppo2.py:145][0m 47104 total steps have happened
[32m[20221214 13:58:32 @agent_ppo2.py:121][0m #------------------------ Iteration 23 --------------------------#
[32m[20221214 13:58:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:32 @agent_ppo2.py:185][0m |          -0.0061 |          14.5633 |          21.1159 |
[32m[20221214 13:58:32 @agent_ppo2.py:185][0m |          -0.0006 |          12.6654 |          21.0977 |
[32m[20221214 13:58:32 @agent_ppo2.py:185][0m |           0.0048 |          13.1976 |          21.0895 |
[32m[20221214 13:58:32 @agent_ppo2.py:185][0m |          -0.0082 |          11.6837 |          21.0779 |
[32m[20221214 13:58:32 @agent_ppo2.py:185][0m |          -0.0050 |          11.2335 |          21.0750 |
[32m[20221214 13:58:32 @agent_ppo2.py:185][0m |          -0.0047 |          11.0697 |          21.0723 |
[32m[20221214 13:58:32 @agent_ppo2.py:185][0m |          -0.0087 |          10.9194 |          21.0627 |
[32m[20221214 13:58:33 @agent_ppo2.py:185][0m |          -0.0082 |          10.7738 |          21.0637 |
[32m[20221214 13:58:33 @agent_ppo2.py:185][0m |          -0.0045 |          10.5717 |          21.0639 |
[32m[20221214 13:58:33 @agent_ppo2.py:185][0m |          -0.0082 |          10.4487 |          21.0580 |
[32m[20221214 13:58:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 13:58:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 212.30
[32m[20221214 13:58:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 235.94
[32m[20221214 13:58:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 220.18
[32m[20221214 13:58:33 @agent_ppo2.py:143][0m Total time:       0.51 min
[32m[20221214 13:58:33 @agent_ppo2.py:145][0m 49152 total steps have happened
[32m[20221214 13:58:33 @agent_ppo2.py:121][0m #------------------------ Iteration 24 --------------------------#
[32m[20221214 13:58:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:33 @agent_ppo2.py:185][0m |          -0.0013 |           9.5557 |          21.0559 |
[32m[20221214 13:58:33 @agent_ppo2.py:185][0m |          -0.0027 |           8.2991 |          21.0522 |
[32m[20221214 13:58:33 @agent_ppo2.py:185][0m |          -0.0053 |           7.8404 |          21.0411 |
[32m[20221214 13:58:34 @agent_ppo2.py:185][0m |          -0.0041 |           7.5571 |          21.0439 |
[32m[20221214 13:58:34 @agent_ppo2.py:185][0m |          -0.0019 |           7.4016 |          21.0456 |
[32m[20221214 13:58:34 @agent_ppo2.py:185][0m |          -0.0045 |           7.1842 |          21.0433 |
[32m[20221214 13:58:34 @agent_ppo2.py:185][0m |          -0.0067 |           7.1067 |          21.0468 |
[32m[20221214 13:58:34 @agent_ppo2.py:185][0m |          -0.0023 |           7.0131 |          21.0446 |
[32m[20221214 13:58:34 @agent_ppo2.py:185][0m |          -0.0056 |           6.9987 |          21.0482 |
[32m[20221214 13:58:34 @agent_ppo2.py:185][0m |           0.0021 |           6.9987 |          21.0516 |
[32m[20221214 13:58:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 13:58:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.32
[32m[20221214 13:58:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 240.52
[32m[20221214 13:58:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 223.85
[32m[20221214 13:58:34 @agent_ppo2.py:143][0m Total time:       0.54 min
[32m[20221214 13:58:34 @agent_ppo2.py:145][0m 51200 total steps have happened
[32m[20221214 13:58:34 @agent_ppo2.py:121][0m #------------------------ Iteration 25 --------------------------#
[32m[20221214 13:58:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:35 @agent_ppo2.py:185][0m |          -0.0000 |           8.4248 |          20.9926 |
[32m[20221214 13:58:35 @agent_ppo2.py:185][0m |          -0.0030 |           7.4847 |          20.9878 |
[32m[20221214 13:58:35 @agent_ppo2.py:185][0m |          -0.0062 |           7.1591 |          20.9767 |
[32m[20221214 13:58:35 @agent_ppo2.py:185][0m |          -0.0035 |           6.9421 |          20.9663 |
[32m[20221214 13:58:35 @agent_ppo2.py:185][0m |          -0.0026 |           6.8010 |          20.9664 |
[32m[20221214 13:58:35 @agent_ppo2.py:185][0m |          -0.0043 |           6.6519 |          20.9601 |
[32m[20221214 13:58:35 @agent_ppo2.py:185][0m |           0.0095 |           6.9814 |          20.9557 |
[32m[20221214 13:58:35 @agent_ppo2.py:185][0m |          -0.0033 |           6.5276 |          20.9551 |
[32m[20221214 13:58:35 @agent_ppo2.py:185][0m |          -0.0073 |           6.5045 |          20.9511 |
[32m[20221214 13:58:35 @agent_ppo2.py:185][0m |          -0.0000 |           6.5021 |          20.9459 |
[32m[20221214 13:58:35 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 13:58:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.16
[32m[20221214 13:58:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.83
[32m[20221214 13:58:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.42
[32m[20221214 13:58:35 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 228.42
[32m[20221214 13:58:35 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 228.42
[32m[20221214 13:58:35 @agent_ppo2.py:143][0m Total time:       0.56 min
[32m[20221214 13:58:35 @agent_ppo2.py:145][0m 53248 total steps have happened
[32m[20221214 13:58:35 @agent_ppo2.py:121][0m #------------------------ Iteration 26 --------------------------#
[32m[20221214 13:58:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:36 @agent_ppo2.py:185][0m |          -0.0056 |           7.6907 |          21.0983 |
[32m[20221214 13:58:36 @agent_ppo2.py:185][0m |          -0.0070 |           7.0869 |          21.0831 |
[32m[20221214 13:58:36 @agent_ppo2.py:185][0m |          -0.0026 |           6.9306 |          21.0733 |
[32m[20221214 13:58:36 @agent_ppo2.py:185][0m |          -0.0077 |           6.8815 |          21.0631 |
[32m[20221214 13:58:36 @agent_ppo2.py:185][0m |          -0.0038 |           6.8342 |          21.0606 |
[32m[20221214 13:58:36 @agent_ppo2.py:185][0m |           0.0073 |           7.4760 |          21.0585 |
[32m[20221214 13:58:36 @agent_ppo2.py:185][0m |          -0.0044 |           6.7273 |          21.0551 |
[32m[20221214 13:58:36 @agent_ppo2.py:185][0m |          -0.0051 |           6.6187 |          21.0520 |
[32m[20221214 13:58:37 @agent_ppo2.py:185][0m |          -0.0044 |           6.6214 |          21.0488 |
[32m[20221214 13:58:37 @agent_ppo2.py:185][0m |          -0.0020 |           6.5618 |          21.0481 |
[32m[20221214 13:58:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 13:58:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 217.20
[32m[20221214 13:58:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 260.54
[32m[20221214 13:58:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.08
[32m[20221214 13:58:37 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 286.08
[32m[20221214 13:58:37 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 286.08
[32m[20221214 13:58:37 @agent_ppo2.py:143][0m Total time:       0.58 min
[32m[20221214 13:58:37 @agent_ppo2.py:145][0m 55296 total steps have happened
[32m[20221214 13:58:37 @agent_ppo2.py:121][0m #------------------------ Iteration 27 --------------------------#
[32m[20221214 13:58:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:37 @agent_ppo2.py:185][0m |           0.0012 |          13.4956 |          21.0966 |
[32m[20221214 13:58:37 @agent_ppo2.py:185][0m |           0.0018 |          12.6167 |          21.0963 |
[32m[20221214 13:58:37 @agent_ppo2.py:185][0m |          -0.0016 |          12.2670 |          21.0950 |
[32m[20221214 13:58:37 @agent_ppo2.py:185][0m |           0.0051 |          12.7044 |          21.0927 |
[32m[20221214 13:58:37 @agent_ppo2.py:185][0m |          -0.0042 |          12.0077 |          21.0896 |
[32m[20221214 13:58:38 @agent_ppo2.py:185][0m |           0.0070 |          12.9591 |          21.0874 |
[32m[20221214 13:58:38 @agent_ppo2.py:185][0m |          -0.0076 |          11.7277 |          21.0805 |
[32m[20221214 13:58:38 @agent_ppo2.py:185][0m |          -0.0035 |          11.6005 |          21.0818 |
[32m[20221214 13:58:38 @agent_ppo2.py:185][0m |          -0.0037 |          11.5355 |          21.0801 |
[32m[20221214 13:58:38 @agent_ppo2.py:185][0m |          -0.0034 |          11.4488 |          21.0780 |
[32m[20221214 13:58:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 13:58:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 236.35
[32m[20221214 13:58:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.61
[32m[20221214 13:58:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 324.46
[32m[20221214 13:58:38 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 324.46
[32m[20221214 13:58:38 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 324.46
[32m[20221214 13:58:38 @agent_ppo2.py:143][0m Total time:       0.60 min
[32m[20221214 13:58:38 @agent_ppo2.py:145][0m 57344 total steps have happened
[32m[20221214 13:58:38 @agent_ppo2.py:121][0m #------------------------ Iteration 28 --------------------------#
[32m[20221214 13:58:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:38 @agent_ppo2.py:185][0m |          -0.0021 |          12.4153 |          21.0415 |
[32m[20221214 13:58:39 @agent_ppo2.py:185][0m |           0.0015 |          11.6556 |          21.0421 |
[32m[20221214 13:58:39 @agent_ppo2.py:185][0m |           0.0001 |          11.0684 |          21.0355 |
[32m[20221214 13:58:39 @agent_ppo2.py:185][0m |          -0.0046 |          10.8190 |          21.0292 |
[32m[20221214 13:58:39 @agent_ppo2.py:185][0m |          -0.0053 |          10.5765 |          21.0257 |
[32m[20221214 13:58:39 @agent_ppo2.py:185][0m |          -0.0041 |          10.4571 |          21.0198 |
[32m[20221214 13:58:39 @agent_ppo2.py:185][0m |          -0.0004 |          10.4462 |          21.0201 |
[32m[20221214 13:58:39 @agent_ppo2.py:185][0m |          -0.0063 |          10.2998 |          21.0192 |
[32m[20221214 13:58:39 @agent_ppo2.py:185][0m |          -0.0054 |          10.1643 |          21.0196 |
[32m[20221214 13:58:39 @agent_ppo2.py:185][0m |          -0.0099 |          10.1353 |          21.0242 |
[32m[20221214 13:58:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 13:58:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 213.87
[32m[20221214 13:58:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.72
[32m[20221214 13:58:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.25
[32m[20221214 13:58:39 @agent_ppo2.py:143][0m Total time:       0.62 min
[32m[20221214 13:58:39 @agent_ppo2.py:145][0m 59392 total steps have happened
[32m[20221214 13:58:39 @agent_ppo2.py:121][0m #------------------------ Iteration 29 --------------------------#
[32m[20221214 13:58:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:40 @agent_ppo2.py:185][0m |          -0.0010 |          18.5959 |          21.1327 |
[32m[20221214 13:58:40 @agent_ppo2.py:185][0m |           0.0069 |          19.5170 |          21.1249 |
[32m[20221214 13:58:40 @agent_ppo2.py:185][0m |          -0.0044 |          17.2603 |          21.1154 |
[32m[20221214 13:58:40 @agent_ppo2.py:185][0m |          -0.0058 |          17.0201 |          21.1131 |
[32m[20221214 13:58:40 @agent_ppo2.py:185][0m |          -0.0000 |          17.1656 |          21.1136 |
[32m[20221214 13:58:40 @agent_ppo2.py:185][0m |          -0.0031 |          16.7595 |          21.1114 |
[32m[20221214 13:58:40 @agent_ppo2.py:185][0m |          -0.0035 |          16.6068 |          21.1090 |
[32m[20221214 13:58:40 @agent_ppo2.py:185][0m |          -0.0031 |          16.5372 |          21.1088 |
[32m[20221214 13:58:41 @agent_ppo2.py:185][0m |          -0.0060 |          16.4473 |          21.1081 |
[32m[20221214 13:58:41 @agent_ppo2.py:185][0m |          -0.0064 |          16.4148 |          21.1057 |
[32m[20221214 13:58:41 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 13:58:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.31
[32m[20221214 13:58:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.81
[32m[20221214 13:58:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 213.90
[32m[20221214 13:58:41 @agent_ppo2.py:143][0m Total time:       0.65 min
[32m[20221214 13:58:41 @agent_ppo2.py:145][0m 61440 total steps have happened
[32m[20221214 13:58:41 @agent_ppo2.py:121][0m #------------------------ Iteration 30 --------------------------#
[32m[20221214 13:58:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 13:58:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:41 @agent_ppo2.py:185][0m |          -0.0026 |          29.2279 |          21.0333 |
[32m[20221214 13:58:41 @agent_ppo2.py:185][0m |          -0.0016 |          27.2558 |          21.0286 |
[32m[20221214 13:58:41 @agent_ppo2.py:185][0m |          -0.0022 |          26.7400 |          21.0178 |
[32m[20221214 13:58:42 @agent_ppo2.py:185][0m |          -0.0021 |          26.0572 |          21.0154 |
[32m[20221214 13:58:42 @agent_ppo2.py:185][0m |          -0.0053 |          25.8230 |          21.0112 |
[32m[20221214 13:58:42 @agent_ppo2.py:185][0m |           0.0000 |          25.4087 |          21.0085 |
[32m[20221214 13:58:42 @agent_ppo2.py:185][0m |          -0.0044 |          25.1106 |          21.0048 |
[32m[20221214 13:58:42 @agent_ppo2.py:185][0m |          -0.0034 |          24.7434 |          21.0040 |
[32m[20221214 13:58:42 @agent_ppo2.py:185][0m |          -0.0047 |          24.6251 |          21.0017 |
[32m[20221214 13:58:42 @agent_ppo2.py:185][0m |          -0.0020 |          24.4395 |          21.0023 |
[32m[20221214 13:58:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 13:58:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 227.74
[32m[20221214 13:58:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 252.54
[32m[20221214 13:58:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 238.92
[32m[20221214 13:58:42 @agent_ppo2.py:143][0m Total time:       0.67 min
[32m[20221214 13:58:42 @agent_ppo2.py:145][0m 63488 total steps have happened
[32m[20221214 13:58:42 @agent_ppo2.py:121][0m #------------------------ Iteration 31 --------------------------#
[32m[20221214 13:58:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:43 @agent_ppo2.py:185][0m |          -0.0034 |          30.4112 |          20.9993 |
[32m[20221214 13:58:43 @agent_ppo2.py:185][0m |           0.0070 |          30.7826 |          21.0017 |
[32m[20221214 13:58:43 @agent_ppo2.py:185][0m |          -0.0030 |          28.5480 |          20.9996 |
[32m[20221214 13:58:43 @agent_ppo2.py:185][0m |           0.0001 |          28.3532 |          20.9994 |
[32m[20221214 13:58:43 @agent_ppo2.py:185][0m |           0.0099 |          30.7144 |          20.9986 |
[32m[20221214 13:58:43 @agent_ppo2.py:185][0m |          -0.0008 |          27.4195 |          21.0005 |
[32m[20221214 13:58:43 @agent_ppo2.py:185][0m |          -0.0018 |          27.1801 |          21.0044 |
[32m[20221214 13:58:43 @agent_ppo2.py:185][0m |          -0.0033 |          27.0676 |          21.0047 |
[32m[20221214 13:58:43 @agent_ppo2.py:185][0m |          -0.0014 |          26.8316 |          21.0113 |
[32m[20221214 13:58:43 @agent_ppo2.py:185][0m |           0.0096 |          28.5429 |          21.0158 |
[32m[20221214 13:58:43 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 13:58:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 223.33
[32m[20221214 13:58:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.78
[32m[20221214 13:58:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 239.82
[32m[20221214 13:58:44 @agent_ppo2.py:143][0m Total time:       0.69 min
[32m[20221214 13:58:44 @agent_ppo2.py:145][0m 65536 total steps have happened
[32m[20221214 13:58:44 @agent_ppo2.py:121][0m #------------------------ Iteration 32 --------------------------#
[32m[20221214 13:58:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:44 @agent_ppo2.py:185][0m |           0.0007 |          26.0590 |          21.1473 |
[32m[20221214 13:58:44 @agent_ppo2.py:185][0m |          -0.0037 |          24.7977 |          21.1463 |
[32m[20221214 13:58:44 @agent_ppo2.py:185][0m |          -0.0027 |          24.2275 |          21.1428 |
[32m[20221214 13:58:44 @agent_ppo2.py:185][0m |          -0.0010 |          23.6841 |          21.1411 |
[32m[20221214 13:58:44 @agent_ppo2.py:185][0m |          -0.0029 |          23.4766 |          21.1402 |
[32m[20221214 13:58:44 @agent_ppo2.py:185][0m |          -0.0048 |          23.0824 |          21.1376 |
[32m[20221214 13:58:45 @agent_ppo2.py:185][0m |          -0.0027 |          22.9079 |          21.1377 |
[32m[20221214 13:58:45 @agent_ppo2.py:185][0m |          -0.0029 |          22.6242 |          21.1367 |
[32m[20221214 13:58:45 @agent_ppo2.py:185][0m |          -0.0056 |          22.3781 |          21.1365 |
[32m[20221214 13:58:45 @agent_ppo2.py:185][0m |          -0.0030 |          22.2531 |          21.1338 |
[32m[20221214 13:58:45 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 13:58:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.28
[32m[20221214 13:58:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 266.79
[32m[20221214 13:58:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 241.85
[32m[20221214 13:58:45 @agent_ppo2.py:143][0m Total time:       0.72 min
[32m[20221214 13:58:45 @agent_ppo2.py:145][0m 67584 total steps have happened
[32m[20221214 13:58:45 @agent_ppo2.py:121][0m #------------------------ Iteration 33 --------------------------#
[32m[20221214 13:58:45 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 13:58:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:45 @agent_ppo2.py:185][0m |          -0.0002 |          25.6111 |          21.1547 |
[32m[20221214 13:58:46 @agent_ppo2.py:185][0m |           0.0000 |          23.9574 |          21.1472 |
[32m[20221214 13:58:46 @agent_ppo2.py:185][0m |          -0.0019 |          23.4507 |          21.1360 |
[32m[20221214 13:58:46 @agent_ppo2.py:185][0m |           0.0006 |          23.0409 |          21.1315 |
[32m[20221214 13:58:46 @agent_ppo2.py:185][0m |          -0.0054 |          22.5573 |          21.1308 |
[32m[20221214 13:58:46 @agent_ppo2.py:185][0m |          -0.0041 |          22.2138 |          21.1294 |
[32m[20221214 13:58:46 @agent_ppo2.py:185][0m |           0.0008 |          22.1867 |          21.1236 |
[32m[20221214 13:58:46 @agent_ppo2.py:185][0m |           0.0026 |          23.2335 |          21.1271 |
[32m[20221214 13:58:46 @agent_ppo2.py:185][0m |          -0.0034 |          21.5992 |          21.1254 |
[32m[20221214 13:58:46 @agent_ppo2.py:185][0m |           0.0006 |          22.6097 |          21.1226 |
[32m[20221214 13:58:46 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:58:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.91
[32m[20221214 13:58:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.73
[32m[20221214 13:58:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 237.33
[32m[20221214 13:58:47 @agent_ppo2.py:143][0m Total time:       0.74 min
[32m[20221214 13:58:47 @agent_ppo2.py:145][0m 69632 total steps have happened
[32m[20221214 13:58:47 @agent_ppo2.py:121][0m #------------------------ Iteration 34 --------------------------#
[32m[20221214 13:58:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:47 @agent_ppo2.py:185][0m |          -0.0034 |          30.3545 |          21.1321 |
[32m[20221214 13:58:47 @agent_ppo2.py:185][0m |           0.0019 |          27.8964 |          21.1292 |
[32m[20221214 13:58:47 @agent_ppo2.py:185][0m |          -0.0022 |          26.7489 |          21.1273 |
[32m[20221214 13:58:47 @agent_ppo2.py:185][0m |          -0.0002 |          26.0332 |          21.1249 |
[32m[20221214 13:58:47 @agent_ppo2.py:185][0m |           0.0024 |          25.8421 |          21.1206 |
[32m[20221214 13:58:47 @agent_ppo2.py:185][0m |          -0.0015 |          24.9654 |          21.1163 |
[32m[20221214 13:58:48 @agent_ppo2.py:185][0m |          -0.0037 |          24.6627 |          21.1157 |
[32m[20221214 13:58:48 @agent_ppo2.py:185][0m |          -0.0018 |          24.2543 |          21.1153 |
[32m[20221214 13:58:48 @agent_ppo2.py:185][0m |          -0.0042 |          24.0011 |          21.1129 |
[32m[20221214 13:58:48 @agent_ppo2.py:185][0m |           0.0007 |          23.7411 |          21.1101 |
[32m[20221214 13:58:48 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 13:58:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.88
[32m[20221214 13:58:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.53
[32m[20221214 13:58:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.09
[32m[20221214 13:58:48 @agent_ppo2.py:143][0m Total time:       0.77 min
[32m[20221214 13:58:48 @agent_ppo2.py:145][0m 71680 total steps have happened
[32m[20221214 13:58:48 @agent_ppo2.py:121][0m #------------------------ Iteration 35 --------------------------#
[32m[20221214 13:58:48 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 13:58:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:48 @agent_ppo2.py:185][0m |          -0.0040 |          35.0351 |          20.9846 |
[32m[20221214 13:58:48 @agent_ppo2.py:185][0m |          -0.0020 |          31.8403 |          20.9763 |
[32m[20221214 13:58:49 @agent_ppo2.py:185][0m |          -0.0060 |          30.6079 |          20.9732 |
[32m[20221214 13:58:49 @agent_ppo2.py:185][0m |          -0.0042 |          29.7297 |          20.9737 |
[32m[20221214 13:58:49 @agent_ppo2.py:185][0m |          -0.0062 |          29.2398 |          20.9711 |
[32m[20221214 13:58:49 @agent_ppo2.py:185][0m |          -0.0051 |          28.7879 |          20.9707 |
[32m[20221214 13:58:49 @agent_ppo2.py:185][0m |          -0.0077 |          28.3473 |          20.9694 |
[32m[20221214 13:58:49 @agent_ppo2.py:185][0m |           0.0014 |          28.6426 |          20.9727 |
[32m[20221214 13:58:49 @agent_ppo2.py:185][0m |          -0.0039 |          27.6399 |          20.9730 |
[32m[20221214 13:58:49 @agent_ppo2.py:185][0m |          -0.0068 |          27.4510 |          20.9723 |
[32m[20221214 13:58:49 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 13:58:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.84
[32m[20221214 13:58:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 316.63
[32m[20221214 13:58:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.59
[32m[20221214 13:58:49 @agent_ppo2.py:143][0m Total time:       0.79 min
[32m[20221214 13:58:49 @agent_ppo2.py:145][0m 73728 total steps have happened
[32m[20221214 13:58:49 @agent_ppo2.py:121][0m #------------------------ Iteration 36 --------------------------#
[32m[20221214 13:58:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 13:58:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:50 @agent_ppo2.py:185][0m |           0.0110 |          34.4110 |          21.0158 |
[32m[20221214 13:58:50 @agent_ppo2.py:185][0m |          -0.0030 |          29.9626 |          21.0154 |
[32m[20221214 13:58:50 @agent_ppo2.py:185][0m |          -0.0004 |          28.5115 |          21.0145 |
[32m[20221214 13:58:50 @agent_ppo2.py:185][0m |          -0.0030 |          27.8241 |          21.0107 |
[32m[20221214 13:58:50 @agent_ppo2.py:185][0m |          -0.0044 |          27.1201 |          21.0088 |
[32m[20221214 13:58:50 @agent_ppo2.py:185][0m |           0.0010 |          26.9957 |          21.0083 |
[32m[20221214 13:58:50 @agent_ppo2.py:185][0m |          -0.0048 |          26.4370 |          21.0064 |
[32m[20221214 13:58:51 @agent_ppo2.py:185][0m |           0.0000 |          26.2032 |          21.0055 |
[32m[20221214 13:58:51 @agent_ppo2.py:185][0m |          -0.0019 |          25.7720 |          21.0017 |
[32m[20221214 13:58:51 @agent_ppo2.py:185][0m |           0.0020 |          25.8798 |          21.0009 |
[32m[20221214 13:58:51 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 13:58:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.21
[32m[20221214 13:58:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.08
[32m[20221214 13:58:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 254.25
[32m[20221214 13:58:51 @agent_ppo2.py:143][0m Total time:       0.81 min
[32m[20221214 13:58:51 @agent_ppo2.py:145][0m 75776 total steps have happened
[32m[20221214 13:58:51 @agent_ppo2.py:121][0m #------------------------ Iteration 37 --------------------------#
[32m[20221214 13:58:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:51 @agent_ppo2.py:185][0m |          -0.0036 |          26.7795 |          21.0434 |
[32m[20221214 13:58:51 @agent_ppo2.py:185][0m |          -0.0041 |          23.2531 |          21.0395 |
[32m[20221214 13:58:51 @agent_ppo2.py:185][0m |          -0.0042 |          22.5359 |          21.0368 |
[32m[20221214 13:58:52 @agent_ppo2.py:185][0m |          -0.0016 |          21.8426 |          21.0311 |
[32m[20221214 13:58:52 @agent_ppo2.py:185][0m |          -0.0024 |          21.3726 |          21.0257 |
[32m[20221214 13:58:52 @agent_ppo2.py:185][0m |          -0.0028 |          21.1121 |          21.0220 |
[32m[20221214 13:58:52 @agent_ppo2.py:185][0m |          -0.0033 |          20.7902 |          21.0187 |
[32m[20221214 13:58:52 @agent_ppo2.py:185][0m |           0.0079 |          21.3107 |          21.0141 |
[32m[20221214 13:58:52 @agent_ppo2.py:185][0m |          -0.0007 |          20.4679 |          21.0094 |
[32m[20221214 13:58:52 @agent_ppo2.py:185][0m |          -0.0024 |          20.0538 |          21.0042 |
[32m[20221214 13:58:52 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:58:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.53
[32m[20221214 13:58:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.01
[32m[20221214 13:58:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.01
[32m[20221214 13:58:52 @agent_ppo2.py:143][0m Total time:       0.84 min
[32m[20221214 13:58:52 @agent_ppo2.py:145][0m 77824 total steps have happened
[32m[20221214 13:58:52 @agent_ppo2.py:121][0m #------------------------ Iteration 38 --------------------------#
[32m[20221214 13:58:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:53 @agent_ppo2.py:185][0m |          -0.0010 |          30.2438 |          21.1085 |
[32m[20221214 13:58:53 @agent_ppo2.py:185][0m |          -0.0067 |          27.9010 |          21.0972 |
[32m[20221214 13:58:53 @agent_ppo2.py:185][0m |          -0.0073 |          26.9989 |          21.0862 |
[32m[20221214 13:58:53 @agent_ppo2.py:185][0m |          -0.0089 |          26.4838 |          21.0819 |
[32m[20221214 13:58:53 @agent_ppo2.py:185][0m |          -0.0080 |          25.8350 |          21.0790 |
[32m[20221214 13:58:53 @agent_ppo2.py:185][0m |          -0.0075 |          25.4058 |          21.0736 |
[32m[20221214 13:58:53 @agent_ppo2.py:185][0m |          -0.0073 |          24.9836 |          21.0690 |
[32m[20221214 13:58:53 @agent_ppo2.py:185][0m |          -0.0003 |          25.1074 |          21.0698 |
[32m[20221214 13:58:53 @agent_ppo2.py:185][0m |          -0.0053 |          24.5017 |          21.0663 |
[32m[20221214 13:58:54 @agent_ppo2.py:185][0m |          -0.0055 |          24.2814 |          21.0617 |
[32m[20221214 13:58:54 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 13:58:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.26
[32m[20221214 13:58:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.46
[32m[20221214 13:58:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.13
[32m[20221214 13:58:54 @agent_ppo2.py:143][0m Total time:       0.86 min
[32m[20221214 13:58:54 @agent_ppo2.py:145][0m 79872 total steps have happened
[32m[20221214 13:58:54 @agent_ppo2.py:121][0m #------------------------ Iteration 39 --------------------------#
[32m[20221214 13:58:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:58:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:54 @agent_ppo2.py:185][0m |           0.0005 |          28.8990 |          20.9976 |
[32m[20221214 13:58:54 @agent_ppo2.py:185][0m |           0.0008 |          25.9159 |          20.9896 |
[32m[20221214 13:58:54 @agent_ppo2.py:185][0m |           0.0006 |          24.6811 |          20.9777 |
[32m[20221214 13:58:54 @agent_ppo2.py:185][0m |          -0.0025 |          23.9334 |          20.9743 |
[32m[20221214 13:58:55 @agent_ppo2.py:185][0m |          -0.0045 |          23.3733 |          20.9704 |
[32m[20221214 13:58:55 @agent_ppo2.py:185][0m |          -0.0001 |          22.7998 |          20.9647 |
[32m[20221214 13:58:55 @agent_ppo2.py:185][0m |          -0.0028 |          22.4520 |          20.9578 |
[32m[20221214 13:58:55 @agent_ppo2.py:185][0m |          -0.0027 |          22.0284 |          20.9524 |
[32m[20221214 13:58:55 @agent_ppo2.py:185][0m |          -0.0047 |          21.6137 |          20.9515 |
[32m[20221214 13:58:55 @agent_ppo2.py:185][0m |          -0.0005 |          21.2366 |          20.9462 |
[32m[20221214 13:58:55 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 13:58:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.56
[32m[20221214 13:58:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 319.52
[32m[20221214 13:58:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.19
[32m[20221214 13:58:55 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 385.19
[32m[20221214 13:58:55 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 385.19
[32m[20221214 13:58:55 @agent_ppo2.py:143][0m Total time:       0.88 min
[32m[20221214 13:58:55 @agent_ppo2.py:145][0m 81920 total steps have happened
[32m[20221214 13:58:55 @agent_ppo2.py:121][0m #------------------------ Iteration 40 --------------------------#
[32m[20221214 13:58:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 13:58:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:56 @agent_ppo2.py:185][0m |          -0.0037 |          32.9766 |          21.1506 |
[32m[20221214 13:58:56 @agent_ppo2.py:185][0m |          -0.0027 |          30.0433 |          21.1422 |
[32m[20221214 13:58:56 @agent_ppo2.py:185][0m |          -0.0037 |          28.8909 |          21.1363 |
[32m[20221214 13:58:56 @agent_ppo2.py:185][0m |          -0.0051 |          28.2080 |          21.1264 |
[32m[20221214 13:58:56 @agent_ppo2.py:185][0m |          -0.0040 |          27.7845 |          21.1304 |
[32m[20221214 13:58:56 @agent_ppo2.py:185][0m |           0.0021 |          28.0272 |          21.1364 |
[32m[20221214 13:58:56 @agent_ppo2.py:185][0m |          -0.0050 |          26.8566 |          21.1302 |
[32m[20221214 13:58:56 @agent_ppo2.py:185][0m |          -0.0027 |          26.4930 |          21.1323 |
[32m[20221214 13:58:56 @agent_ppo2.py:185][0m |           0.0011 |          26.5440 |          21.1369 |
[32m[20221214 13:58:56 @agent_ppo2.py:185][0m |          -0.0057 |          26.1229 |          21.1307 |
[32m[20221214 13:58:56 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 13:58:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.34
[32m[20221214 13:58:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.46
[32m[20221214 13:58:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.52
[32m[20221214 13:58:57 @agent_ppo2.py:143][0m Total time:       0.91 min
[32m[20221214 13:58:57 @agent_ppo2.py:145][0m 83968 total steps have happened
[32m[20221214 13:58:57 @agent_ppo2.py:121][0m #------------------------ Iteration 41 --------------------------#
[32m[20221214 13:58:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:57 @agent_ppo2.py:185][0m |          -0.0011 |          28.2634 |          21.0414 |
[32m[20221214 13:58:57 @agent_ppo2.py:185][0m |          -0.0053 |          25.1871 |          21.0381 |
[32m[20221214 13:58:57 @agent_ppo2.py:185][0m |           0.0021 |          23.7668 |          21.0353 |
[32m[20221214 13:58:57 @agent_ppo2.py:185][0m |          -0.0039 |          22.6450 |          21.0314 |
[32m[20221214 13:58:57 @agent_ppo2.py:185][0m |          -0.0027 |          22.0452 |          21.0248 |
[32m[20221214 13:58:57 @agent_ppo2.py:185][0m |          -0.0043 |          21.4965 |          21.0221 |
[32m[20221214 13:58:57 @agent_ppo2.py:185][0m |          -0.0017 |          20.9019 |          21.0171 |
[32m[20221214 13:58:58 @agent_ppo2.py:185][0m |          -0.0034 |          20.4974 |          21.0127 |
[32m[20221214 13:58:58 @agent_ppo2.py:185][0m |          -0.0052 |          20.2448 |          21.0128 |
[32m[20221214 13:58:58 @agent_ppo2.py:185][0m |          -0.0044 |          19.8979 |          21.0086 |
[32m[20221214 13:58:58 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 13:58:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.42
[32m[20221214 13:58:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.35
[32m[20221214 13:58:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.78
[32m[20221214 13:58:58 @agent_ppo2.py:143][0m Total time:       0.93 min
[32m[20221214 13:58:58 @agent_ppo2.py:145][0m 86016 total steps have happened
[32m[20221214 13:58:58 @agent_ppo2.py:121][0m #------------------------ Iteration 42 --------------------------#
[32m[20221214 13:58:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:58 @agent_ppo2.py:185][0m |          -0.0031 |          31.5001 |          21.1563 |
[32m[20221214 13:58:58 @agent_ppo2.py:185][0m |          -0.0017 |          29.0910 |          21.1490 |
[32m[20221214 13:58:58 @agent_ppo2.py:185][0m |           0.0001 |          28.1939 |          21.1401 |
[32m[20221214 13:58:58 @agent_ppo2.py:185][0m |           0.0044 |          28.2949 |          21.1344 |
[32m[20221214 13:58:59 @agent_ppo2.py:185][0m |          -0.0022 |          26.3314 |          21.1285 |
[32m[20221214 13:58:59 @agent_ppo2.py:185][0m |          -0.0038 |          25.7216 |          21.1220 |
[32m[20221214 13:58:59 @agent_ppo2.py:185][0m |          -0.0025 |          25.1914 |          21.1180 |
[32m[20221214 13:58:59 @agent_ppo2.py:185][0m |          -0.0045 |          24.7071 |          21.1117 |
[32m[20221214 13:58:59 @agent_ppo2.py:185][0m |          -0.0050 |          24.5110 |          21.1125 |
[32m[20221214 13:58:59 @agent_ppo2.py:185][0m |          -0.0070 |          24.1477 |          21.1094 |
[32m[20221214 13:58:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 13:58:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.29
[32m[20221214 13:58:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 314.53
[32m[20221214 13:58:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 190.99
[32m[20221214 13:58:59 @agent_ppo2.py:143][0m Total time:       0.95 min
[32m[20221214 13:58:59 @agent_ppo2.py:145][0m 88064 total steps have happened
[32m[20221214 13:58:59 @agent_ppo2.py:121][0m #------------------------ Iteration 43 --------------------------#
[32m[20221214 13:58:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:58:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:58:59 @agent_ppo2.py:185][0m |          -0.0038 |          35.2669 |          21.1537 |
[32m[20221214 13:59:00 @agent_ppo2.py:185][0m |          -0.0013 |          28.8093 |          21.1464 |
[32m[20221214 13:59:00 @agent_ppo2.py:185][0m |          -0.0046 |          26.2523 |          21.1373 |
[32m[20221214 13:59:00 @agent_ppo2.py:185][0m |           0.0004 |          24.7650 |          21.1318 |
[32m[20221214 13:59:00 @agent_ppo2.py:185][0m |          -0.0056 |          23.5034 |          21.1289 |
[32m[20221214 13:59:00 @agent_ppo2.py:185][0m |          -0.0024 |          22.5325 |          21.1297 |
[32m[20221214 13:59:00 @agent_ppo2.py:185][0m |          -0.0038 |          22.0515 |          21.1285 |
[32m[20221214 13:59:00 @agent_ppo2.py:185][0m |          -0.0043 |          21.2568 |          21.1267 |
[32m[20221214 13:59:00 @agent_ppo2.py:185][0m |          -0.0024 |          20.7444 |          21.1270 |
[32m[20221214 13:59:00 @agent_ppo2.py:185][0m |          -0.0068 |          20.2838 |          21.1246 |
[32m[20221214 13:59:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 13:59:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.87
[32m[20221214 13:59:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 321.84
[32m[20221214 13:59:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.76
[32m[20221214 13:59:00 @agent_ppo2.py:143][0m Total time:       0.97 min
[32m[20221214 13:59:00 @agent_ppo2.py:145][0m 90112 total steps have happened
[32m[20221214 13:59:00 @agent_ppo2.py:121][0m #------------------------ Iteration 44 --------------------------#
[32m[20221214 13:59:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:01 @agent_ppo2.py:185][0m |          -0.0001 |          35.1325 |          21.1205 |
[32m[20221214 13:59:01 @agent_ppo2.py:185][0m |          -0.0017 |          30.7416 |          21.1186 |
[32m[20221214 13:59:01 @agent_ppo2.py:185][0m |          -0.0011 |          29.2657 |          21.1154 |
[32m[20221214 13:59:01 @agent_ppo2.py:185][0m |          -0.0007 |          27.9587 |          21.1094 |
[32m[20221214 13:59:01 @agent_ppo2.py:185][0m |          -0.0045 |          27.1519 |          21.1073 |
[32m[20221214 13:59:01 @agent_ppo2.py:185][0m |          -0.0055 |          26.4726 |          21.1021 |
[32m[20221214 13:59:01 @agent_ppo2.py:185][0m |          -0.0017 |          25.9591 |          21.1048 |
[32m[20221214 13:59:01 @agent_ppo2.py:185][0m |          -0.0047 |          25.3309 |          21.0975 |
[32m[20221214 13:59:01 @agent_ppo2.py:185][0m |          -0.0005 |          24.9606 |          21.0988 |
[32m[20221214 13:59:02 @agent_ppo2.py:185][0m |           0.0073 |          26.5642 |          21.0983 |
[32m[20221214 13:59:02 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 13:59:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.30
[32m[20221214 13:59:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.85
[32m[20221214 13:59:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 241.54
[32m[20221214 13:59:02 @agent_ppo2.py:143][0m Total time:       0.99 min
[32m[20221214 13:59:02 @agent_ppo2.py:145][0m 92160 total steps have happened
[32m[20221214 13:59:02 @agent_ppo2.py:121][0m #------------------------ Iteration 45 --------------------------#
[32m[20221214 13:59:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:02 @agent_ppo2.py:185][0m |          -0.0003 |          30.1710 |          21.0898 |
[32m[20221214 13:59:02 @agent_ppo2.py:185][0m |           0.0018 |          26.1173 |          21.0874 |
[32m[20221214 13:59:02 @agent_ppo2.py:185][0m |          -0.0014 |          24.3343 |          21.0838 |
[32m[20221214 13:59:02 @agent_ppo2.py:185][0m |          -0.0027 |          23.2399 |          21.0817 |
[32m[20221214 13:59:02 @agent_ppo2.py:185][0m |          -0.0055 |          22.4812 |          21.0791 |
[32m[20221214 13:59:02 @agent_ppo2.py:185][0m |          -0.0034 |          21.8767 |          21.0804 |
[32m[20221214 13:59:03 @agent_ppo2.py:185][0m |          -0.0017 |          21.2893 |          21.0766 |
[32m[20221214 13:59:03 @agent_ppo2.py:185][0m |          -0.0029 |          20.7856 |          21.0797 |
[32m[20221214 13:59:03 @agent_ppo2.py:185][0m |          -0.0026 |          20.2826 |          21.0775 |
[32m[20221214 13:59:03 @agent_ppo2.py:185][0m |          -0.0005 |          19.9285 |          21.0787 |
[32m[20221214 13:59:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 13:59:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.65
[32m[20221214 13:59:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.51
[32m[20221214 13:59:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 237.90
[32m[20221214 13:59:03 @agent_ppo2.py:143][0m Total time:       1.02 min
[32m[20221214 13:59:03 @agent_ppo2.py:145][0m 94208 total steps have happened
[32m[20221214 13:59:03 @agent_ppo2.py:121][0m #------------------------ Iteration 46 --------------------------#
[32m[20221214 13:59:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:03 @agent_ppo2.py:185][0m |           0.0003 |          24.8810 |          21.1233 |
[32m[20221214 13:59:03 @agent_ppo2.py:185][0m |          -0.0006 |          21.9772 |          21.1130 |
[32m[20221214 13:59:04 @agent_ppo2.py:185][0m |           0.0091 |          21.4723 |          21.1016 |
[32m[20221214 13:59:04 @agent_ppo2.py:185][0m |          -0.0036 |          19.5372 |          21.0932 |
[32m[20221214 13:59:04 @agent_ppo2.py:185][0m |           0.0023 |          18.6972 |          21.0909 |
[32m[20221214 13:59:04 @agent_ppo2.py:185][0m |          -0.0019 |          18.1146 |          21.0924 |
[32m[20221214 13:59:04 @agent_ppo2.py:185][0m |          -0.0069 |          17.4450 |          21.0908 |
[32m[20221214 13:59:04 @agent_ppo2.py:185][0m |          -0.0042 |          16.8601 |          21.0881 |
[32m[20221214 13:59:04 @agent_ppo2.py:185][0m |          -0.0040 |          16.3088 |          21.0840 |
[32m[20221214 13:59:04 @agent_ppo2.py:185][0m |           0.0060 |          16.7387 |          21.0824 |
[32m[20221214 13:59:04 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 13:59:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.02
[32m[20221214 13:59:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 329.32
[32m[20221214 13:59:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 201.23
[32m[20221214 13:59:04 @agent_ppo2.py:143][0m Total time:       1.04 min
[32m[20221214 13:59:04 @agent_ppo2.py:145][0m 96256 total steps have happened
[32m[20221214 13:59:04 @agent_ppo2.py:121][0m #------------------------ Iteration 47 --------------------------#
[32m[20221214 13:59:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:59:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:05 @agent_ppo2.py:185][0m |          -0.0039 |          20.3784 |          21.0854 |
[32m[20221214 13:59:05 @agent_ppo2.py:185][0m |          -0.0042 |          16.6429 |          21.0683 |
[32m[20221214 13:59:05 @agent_ppo2.py:185][0m |          -0.0025 |          15.6494 |          21.0576 |
[32m[20221214 13:59:05 @agent_ppo2.py:185][0m |          -0.0036 |          14.9909 |          21.0498 |
[32m[20221214 13:59:05 @agent_ppo2.py:185][0m |          -0.0033 |          14.5859 |          21.0473 |
[32m[20221214 13:59:05 @agent_ppo2.py:185][0m |          -0.0062 |          14.1939 |          21.0458 |
[32m[20221214 13:59:05 @agent_ppo2.py:185][0m |          -0.0000 |          13.8404 |          21.0382 |
[32m[20221214 13:59:05 @agent_ppo2.py:185][0m |          -0.0027 |          13.5401 |          21.0389 |
[32m[20221214 13:59:05 @agent_ppo2.py:185][0m |          -0.0079 |          13.3020 |          21.0346 |
[32m[20221214 13:59:05 @agent_ppo2.py:185][0m |          -0.0079 |          13.1294 |          21.0362 |
[32m[20221214 13:59:05 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 13:59:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 229.60
[32m[20221214 13:59:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 317.37
[32m[20221214 13:59:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 239.96
[32m[20221214 13:59:06 @agent_ppo2.py:143][0m Total time:       1.06 min
[32m[20221214 13:59:06 @agent_ppo2.py:145][0m 98304 total steps have happened
[32m[20221214 13:59:06 @agent_ppo2.py:121][0m #------------------------ Iteration 48 --------------------------#
[32m[20221214 13:59:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:59:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:06 @agent_ppo2.py:185][0m |           0.0035 |          16.9601 |          21.1616 |
[32m[20221214 13:59:06 @agent_ppo2.py:185][0m |          -0.0030 |          13.3097 |          21.1571 |
[32m[20221214 13:59:06 @agent_ppo2.py:185][0m |           0.0007 |          12.0129 |          21.1500 |
[32m[20221214 13:59:06 @agent_ppo2.py:185][0m |          -0.0032 |          11.2542 |          21.1451 |
[32m[20221214 13:59:06 @agent_ppo2.py:185][0m |          -0.0038 |          10.7280 |          21.1395 |
[32m[20221214 13:59:06 @agent_ppo2.py:185][0m |           0.0004 |          10.2977 |          21.1377 |
[32m[20221214 13:59:06 @agent_ppo2.py:185][0m |          -0.0018 |          10.0069 |          21.1334 |
[32m[20221214 13:59:07 @agent_ppo2.py:185][0m |          -0.0028 |           9.6469 |          21.1264 |
[32m[20221214 13:59:07 @agent_ppo2.py:185][0m |          -0.0068 |           9.3772 |          21.1259 |
[32m[20221214 13:59:07 @agent_ppo2.py:185][0m |           0.0010 |           9.1573 |          21.1208 |
[32m[20221214 13:59:07 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 13:59:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 215.56
[32m[20221214 13:59:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 255.84
[32m[20221214 13:59:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 187.27
[32m[20221214 13:59:07 @agent_ppo2.py:143][0m Total time:       1.08 min
[32m[20221214 13:59:07 @agent_ppo2.py:145][0m 100352 total steps have happened
[32m[20221214 13:59:07 @agent_ppo2.py:121][0m #------------------------ Iteration 49 --------------------------#
[32m[20221214 13:59:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 13:59:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:07 @agent_ppo2.py:185][0m |          -0.0042 |          21.5739 |          21.0245 |
[32m[20221214 13:59:08 @agent_ppo2.py:185][0m |          -0.0034 |          17.2697 |          21.0185 |
[32m[20221214 13:59:08 @agent_ppo2.py:185][0m |          -0.0025 |          15.5518 |          21.0203 |
[32m[20221214 13:59:08 @agent_ppo2.py:185][0m |          -0.0006 |          14.8155 |          21.0198 |
[32m[20221214 13:59:08 @agent_ppo2.py:185][0m |          -0.0025 |          14.0190 |          21.0159 |
[32m[20221214 13:59:08 @agent_ppo2.py:185][0m |           0.0045 |          13.5663 |          21.0167 |
[32m[20221214 13:59:08 @agent_ppo2.py:185][0m |          -0.0028 |          12.9739 |          21.0154 |
[32m[20221214 13:59:08 @agent_ppo2.py:185][0m |          -0.0025 |          12.6774 |          21.0151 |
[32m[20221214 13:59:08 @agent_ppo2.py:185][0m |          -0.0012 |          12.2989 |          21.0134 |
[32m[20221214 13:59:08 @agent_ppo2.py:185][0m |          -0.0011 |          12.0298 |          21.0123 |
[32m[20221214 13:59:08 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221214 13:59:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 199.32
[32m[20221214 13:59:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 227.42
[32m[20221214 13:59:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 222.26
[32m[20221214 13:59:09 @agent_ppo2.py:143][0m Total time:       1.11 min
[32m[20221214 13:59:09 @agent_ppo2.py:145][0m 102400 total steps have happened
[32m[20221214 13:59:09 @agent_ppo2.py:121][0m #------------------------ Iteration 50 --------------------------#
[32m[20221214 13:59:09 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 13:59:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:09 @agent_ppo2.py:185][0m |           0.0069 |          20.2906 |          21.2137 |
[32m[20221214 13:59:09 @agent_ppo2.py:185][0m |          -0.0021 |          16.8576 |          21.2150 |
[32m[20221214 13:59:09 @agent_ppo2.py:185][0m |          -0.0007 |          15.5361 |          21.2058 |
[32m[20221214 13:59:09 @agent_ppo2.py:185][0m |          -0.0028 |          14.6103 |          21.2065 |
[32m[20221214 13:59:09 @agent_ppo2.py:185][0m |          -0.0053 |          13.8585 |          21.2053 |
[32m[20221214 13:59:09 @agent_ppo2.py:185][0m |          -0.0028 |          13.3557 |          21.2076 |
[32m[20221214 13:59:09 @agent_ppo2.py:185][0m |          -0.0053 |          12.8875 |          21.2034 |
[32m[20221214 13:59:10 @agent_ppo2.py:185][0m |           0.0000 |          12.4754 |          21.2086 |
[32m[20221214 13:59:10 @agent_ppo2.py:185][0m |          -0.0024 |          12.1026 |          21.2045 |
[32m[20221214 13:59:10 @agent_ppo2.py:185][0m |          -0.0054 |          11.7299 |          21.2092 |
[32m[20221214 13:59:10 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 13:59:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.93
[32m[20221214 13:59:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.99
[32m[20221214 13:59:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.02
[32m[20221214 13:59:10 @agent_ppo2.py:143][0m Total time:       1.13 min
[32m[20221214 13:59:10 @agent_ppo2.py:145][0m 104448 total steps have happened
[32m[20221214 13:59:10 @agent_ppo2.py:121][0m #------------------------ Iteration 51 --------------------------#
[32m[20221214 13:59:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:10 @agent_ppo2.py:185][0m |          -0.0031 |          15.7480 |          21.1573 |
[32m[20221214 13:59:10 @agent_ppo2.py:185][0m |          -0.0025 |          13.1434 |          21.1547 |
[32m[20221214 13:59:10 @agent_ppo2.py:185][0m |          -0.0019 |          11.9552 |          21.1450 |
[32m[20221214 13:59:11 @agent_ppo2.py:185][0m |          -0.0063 |          11.3663 |          21.1390 |
[32m[20221214 13:59:11 @agent_ppo2.py:185][0m |           0.0037 |          10.9659 |          21.1389 |
[32m[20221214 13:59:11 @agent_ppo2.py:185][0m |          -0.0055 |          10.5602 |          21.1349 |
[32m[20221214 13:59:11 @agent_ppo2.py:185][0m |          -0.0100 |          10.2967 |          21.1336 |
[32m[20221214 13:59:11 @agent_ppo2.py:185][0m |           0.0060 |          10.4554 |          21.1292 |
[32m[20221214 13:59:11 @agent_ppo2.py:185][0m |          -0.0086 |           9.7527 |          21.1321 |
[32m[20221214 13:59:11 @agent_ppo2.py:185][0m |          -0.0043 |           9.5138 |          21.1282 |
[32m[20221214 13:59:11 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 13:59:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 226.32
[32m[20221214 13:59:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.02
[32m[20221214 13:59:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 232.05
[32m[20221214 13:59:11 @agent_ppo2.py:143][0m Total time:       1.15 min
[32m[20221214 13:59:11 @agent_ppo2.py:145][0m 106496 total steps have happened
[32m[20221214 13:59:11 @agent_ppo2.py:121][0m #------------------------ Iteration 52 --------------------------#
[32m[20221214 13:59:11 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 13:59:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:12 @agent_ppo2.py:185][0m |          -0.0004 |          19.0959 |          21.1117 |
[32m[20221214 13:59:12 @agent_ppo2.py:185][0m |           0.0014 |          16.4163 |          21.1113 |
[32m[20221214 13:59:12 @agent_ppo2.py:185][0m |          -0.0010 |          15.4465 |          21.1119 |
[32m[20221214 13:59:12 @agent_ppo2.py:185][0m |          -0.0037 |          14.8319 |          21.1097 |
[32m[20221214 13:59:12 @agent_ppo2.py:185][0m |          -0.0018 |          14.2743 |          21.1071 |
[32m[20221214 13:59:12 @agent_ppo2.py:185][0m |           0.0069 |          15.4120 |          21.1050 |
[32m[20221214 13:59:12 @agent_ppo2.py:185][0m |          -0.0052 |          13.7693 |          21.1044 |
[32m[20221214 13:59:12 @agent_ppo2.py:185][0m |           0.0263 |          18.5405 |          21.1028 |
[32m[20221214 13:59:12 @agent_ppo2.py:185][0m |           0.0007 |          13.7801 |          21.0917 |
[32m[20221214 13:59:13 @agent_ppo2.py:185][0m |          -0.0009 |          12.9994 |          21.0916 |
[32m[20221214 13:59:13 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 13:59:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.92
[32m[20221214 13:59:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.68
[32m[20221214 13:59:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 203.56
[32m[20221214 13:59:13 @agent_ppo2.py:143][0m Total time:       1.18 min
[32m[20221214 13:59:13 @agent_ppo2.py:145][0m 108544 total steps have happened
[32m[20221214 13:59:13 @agent_ppo2.py:121][0m #------------------------ Iteration 53 --------------------------#
[32m[20221214 13:59:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:13 @agent_ppo2.py:185][0m |           0.0008 |          20.9079 |          21.0918 |
[32m[20221214 13:59:13 @agent_ppo2.py:185][0m |          -0.0067 |          16.8458 |          21.0910 |
[32m[20221214 13:59:13 @agent_ppo2.py:185][0m |           0.0049 |          15.6366 |          21.0866 |
[32m[20221214 13:59:13 @agent_ppo2.py:185][0m |          -0.0032 |          14.1751 |          21.0830 |
[32m[20221214 13:59:14 @agent_ppo2.py:185][0m |          -0.0017 |          13.4970 |          21.0783 |
[32m[20221214 13:59:14 @agent_ppo2.py:185][0m |          -0.0011 |          13.0005 |          21.0781 |
[32m[20221214 13:59:14 @agent_ppo2.py:185][0m |          -0.0075 |          12.6445 |          21.0753 |
[32m[20221214 13:59:14 @agent_ppo2.py:185][0m |          -0.0031 |          12.3044 |          21.0707 |
[32m[20221214 13:59:14 @agent_ppo2.py:185][0m |          -0.0049 |          11.9406 |          21.0711 |
[32m[20221214 13:59:14 @agent_ppo2.py:185][0m |          -0.0049 |          11.6542 |          21.0684 |
[32m[20221214 13:59:14 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 13:59:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 203.12
[32m[20221214 13:59:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 251.66
[32m[20221214 13:59:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 235.43
[32m[20221214 13:59:14 @agent_ppo2.py:143][0m Total time:       1.20 min
[32m[20221214 13:59:14 @agent_ppo2.py:145][0m 110592 total steps have happened
[32m[20221214 13:59:14 @agent_ppo2.py:121][0m #------------------------ Iteration 54 --------------------------#
[32m[20221214 13:59:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:15 @agent_ppo2.py:185][0m |           0.0004 |          19.2535 |          21.0845 |
[32m[20221214 13:59:15 @agent_ppo2.py:185][0m |          -0.0005 |          16.4821 |          21.0855 |
[32m[20221214 13:59:15 @agent_ppo2.py:185][0m |          -0.0034 |          15.2942 |          21.0838 |
[32m[20221214 13:59:15 @agent_ppo2.py:185][0m |          -0.0040 |          14.7071 |          21.0813 |
[32m[20221214 13:59:15 @agent_ppo2.py:185][0m |          -0.0008 |          14.2331 |          21.0829 |
[32m[20221214 13:59:15 @agent_ppo2.py:185][0m |          -0.0046 |          13.8067 |          21.0831 |
[32m[20221214 13:59:15 @agent_ppo2.py:185][0m |          -0.0048 |          13.6062 |          21.0828 |
[32m[20221214 13:59:15 @agent_ppo2.py:185][0m |          -0.0045 |          13.3558 |          21.0845 |
[32m[20221214 13:59:15 @agent_ppo2.py:185][0m |           0.0030 |          13.3843 |          21.0851 |
[32m[20221214 13:59:16 @agent_ppo2.py:185][0m |          -0.0061 |          13.0122 |          21.0856 |
[32m[20221214 13:59:16 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 13:59:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 221.87
[32m[20221214 13:59:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.38
[32m[20221214 13:59:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 187.64
[32m[20221214 13:59:16 @agent_ppo2.py:143][0m Total time:       1.23 min
[32m[20221214 13:59:16 @agent_ppo2.py:145][0m 112640 total steps have happened
[32m[20221214 13:59:16 @agent_ppo2.py:121][0m #------------------------ Iteration 55 --------------------------#
[32m[20221214 13:59:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:16 @agent_ppo2.py:185][0m |           0.0097 |          19.6096 |          21.1251 |
[32m[20221214 13:59:16 @agent_ppo2.py:185][0m |          -0.0029 |          15.9833 |          21.1207 |
[32m[20221214 13:59:16 @agent_ppo2.py:185][0m |          -0.0013 |          14.6863 |          21.1123 |
[32m[20221214 13:59:16 @agent_ppo2.py:185][0m |          -0.0042 |          14.0044 |          21.1048 |
[32m[20221214 13:59:16 @agent_ppo2.py:185][0m |          -0.0024 |          13.4329 |          21.1014 |
[32m[20221214 13:59:17 @agent_ppo2.py:185][0m |          -0.0012 |          12.9677 |          21.0959 |
[32m[20221214 13:59:17 @agent_ppo2.py:185][0m |          -0.0015 |          12.5983 |          21.0934 |
[32m[20221214 13:59:17 @agent_ppo2.py:185][0m |          -0.0056 |          12.3134 |          21.0916 |
[32m[20221214 13:59:17 @agent_ppo2.py:185][0m |          -0.0019 |          11.9600 |          21.0926 |
[32m[20221214 13:59:17 @agent_ppo2.py:185][0m |          -0.0049 |          11.7908 |          21.0905 |
[32m[20221214 13:59:17 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 13:59:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 208.18
[32m[20221214 13:59:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.62
[32m[20221214 13:59:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.30
[32m[20221214 13:59:17 @agent_ppo2.py:143][0m Total time:       1.25 min
[32m[20221214 13:59:17 @agent_ppo2.py:145][0m 114688 total steps have happened
[32m[20221214 13:59:17 @agent_ppo2.py:121][0m #------------------------ Iteration 56 --------------------------#
[32m[20221214 13:59:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:17 @agent_ppo2.py:185][0m |           0.0023 |          11.7787 |          21.1408 |
[32m[20221214 13:59:18 @agent_ppo2.py:185][0m |          -0.0013 |           9.7473 |          21.1439 |
[32m[20221214 13:59:18 @agent_ppo2.py:185][0m |          -0.0076 |           8.9614 |          21.1442 |
[32m[20221214 13:59:18 @agent_ppo2.py:185][0m |          -0.0036 |           8.4325 |          21.1392 |
[32m[20221214 13:59:18 @agent_ppo2.py:185][0m |          -0.0004 |           8.1470 |          21.1392 |
[32m[20221214 13:59:18 @agent_ppo2.py:185][0m |           0.0010 |           7.8183 |          21.1382 |
[32m[20221214 13:59:18 @agent_ppo2.py:185][0m |          -0.0043 |           7.5923 |          21.1348 |
[32m[20221214 13:59:18 @agent_ppo2.py:185][0m |          -0.0033 |           7.4259 |          21.1346 |
[32m[20221214 13:59:18 @agent_ppo2.py:185][0m |          -0.0045 |           7.2737 |          21.1339 |
[32m[20221214 13:59:18 @agent_ppo2.py:185][0m |           0.0014 |           7.1935 |          21.1328 |
[32m[20221214 13:59:18 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 200.35
[32m[20221214 13:59:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 206.33
[32m[20221214 13:59:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 235.49
[32m[20221214 13:59:19 @agent_ppo2.py:143][0m Total time:       1.27 min
[32m[20221214 13:59:19 @agent_ppo2.py:145][0m 116736 total steps have happened
[32m[20221214 13:59:19 @agent_ppo2.py:121][0m #------------------------ Iteration 57 --------------------------#
[32m[20221214 13:59:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:19 @agent_ppo2.py:185][0m |          -0.0019 |          15.8227 |          21.0360 |
[32m[20221214 13:59:19 @agent_ppo2.py:185][0m |          -0.0031 |          13.9905 |          21.0339 |
[32m[20221214 13:59:19 @agent_ppo2.py:185][0m |          -0.0029 |          13.1561 |          21.0278 |
[32m[20221214 13:59:19 @agent_ppo2.py:185][0m |          -0.0039 |          12.4712 |          21.0208 |
[32m[20221214 13:59:19 @agent_ppo2.py:185][0m |          -0.0054 |          12.0328 |          21.0210 |
[32m[20221214 13:59:19 @agent_ppo2.py:185][0m |           0.0011 |          11.5920 |          21.0189 |
[32m[20221214 13:59:20 @agent_ppo2.py:185][0m |          -0.0068 |          11.3758 |          21.0173 |
[32m[20221214 13:59:20 @agent_ppo2.py:185][0m |           0.0006 |          11.0396 |          21.0151 |
[32m[20221214 13:59:20 @agent_ppo2.py:185][0m |          -0.0020 |          10.8244 |          21.0156 |
[32m[20221214 13:59:20 @agent_ppo2.py:185][0m |          -0.0025 |          10.5602 |          21.0141 |
[32m[20221214 13:59:20 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 13:59:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 212.60
[32m[20221214 13:59:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.78
[32m[20221214 13:59:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.00
[32m[20221214 13:59:20 @agent_ppo2.py:143][0m Total time:       1.30 min
[32m[20221214 13:59:20 @agent_ppo2.py:145][0m 118784 total steps have happened
[32m[20221214 13:59:20 @agent_ppo2.py:121][0m #------------------------ Iteration 58 --------------------------#
[32m[20221214 13:59:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:20 @agent_ppo2.py:185][0m |          -0.0025 |          16.0972 |          21.0581 |
[32m[20221214 13:59:20 @agent_ppo2.py:185][0m |          -0.0045 |          13.5038 |          21.0511 |
[32m[20221214 13:59:21 @agent_ppo2.py:185][0m |          -0.0032 |          12.5355 |          21.0397 |
[32m[20221214 13:59:21 @agent_ppo2.py:185][0m |          -0.0064 |          11.9787 |          21.0412 |
[32m[20221214 13:59:21 @agent_ppo2.py:185][0m |           0.0007 |          11.5916 |          21.0459 |
[32m[20221214 13:59:21 @agent_ppo2.py:185][0m |          -0.0004 |          11.0538 |          21.0433 |
[32m[20221214 13:59:21 @agent_ppo2.py:185][0m |          -0.0109 |          10.7223 |          21.0543 |
[32m[20221214 13:59:21 @agent_ppo2.py:185][0m |          -0.0043 |          10.4443 |          21.0501 |
[32m[20221214 13:59:21 @agent_ppo2.py:185][0m |          -0.0036 |          10.2572 |          21.0577 |
[32m[20221214 13:59:21 @agent_ppo2.py:185][0m |          -0.0045 |           9.9797 |          21.0590 |
[32m[20221214 13:59:21 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 13:59:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.58
[32m[20221214 13:59:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 252.17
[32m[20221214 13:59:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.46
[32m[20221214 13:59:21 @agent_ppo2.py:143][0m Total time:       1.32 min
[32m[20221214 13:59:21 @agent_ppo2.py:145][0m 120832 total steps have happened
[32m[20221214 13:59:21 @agent_ppo2.py:121][0m #------------------------ Iteration 59 --------------------------#
[32m[20221214 13:59:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:22 @agent_ppo2.py:185][0m |          -0.0008 |          17.0694 |          21.1153 |
[32m[20221214 13:59:22 @agent_ppo2.py:185][0m |          -0.0027 |          14.6476 |          21.1023 |
[32m[20221214 13:59:22 @agent_ppo2.py:185][0m |          -0.0037 |          13.7421 |          21.0888 |
[32m[20221214 13:59:22 @agent_ppo2.py:185][0m |          -0.0101 |          13.2046 |          21.0877 |
[32m[20221214 13:59:22 @agent_ppo2.py:185][0m |          -0.0068 |          12.6818 |          21.0873 |
[32m[20221214 13:59:22 @agent_ppo2.py:185][0m |          -0.0068 |          12.3989 |          21.0827 |
[32m[20221214 13:59:22 @agent_ppo2.py:185][0m |          -0.0058 |          12.1459 |          21.0890 |
[32m[20221214 13:59:23 @agent_ppo2.py:185][0m |          -0.0068 |          11.8181 |          21.0844 |
[32m[20221214 13:59:23 @agent_ppo2.py:185][0m |           0.0015 |          11.6904 |          21.0887 |
[32m[20221214 13:59:23 @agent_ppo2.py:185][0m |          -0.0072 |          11.5066 |          21.0903 |
[32m[20221214 13:59:23 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 13:59:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 217.84
[32m[20221214 13:59:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 245.74
[32m[20221214 13:59:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.91
[32m[20221214 13:59:23 @agent_ppo2.py:143][0m Total time:       1.35 min
[32m[20221214 13:59:23 @agent_ppo2.py:145][0m 122880 total steps have happened
[32m[20221214 13:59:23 @agent_ppo2.py:121][0m #------------------------ Iteration 60 --------------------------#
[32m[20221214 13:59:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 13:59:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:23 @agent_ppo2.py:185][0m |          -0.0002 |          15.5398 |          21.0243 |
[32m[20221214 13:59:23 @agent_ppo2.py:185][0m |          -0.0047 |          14.0216 |          21.0227 |
[32m[20221214 13:59:23 @agent_ppo2.py:185][0m |          -0.0051 |          13.4616 |          21.0213 |
[32m[20221214 13:59:24 @agent_ppo2.py:185][0m |          -0.0084 |          13.0058 |          21.0217 |
[32m[20221214 13:59:24 @agent_ppo2.py:185][0m |          -0.0035 |          12.6926 |          21.0204 |
[32m[20221214 13:59:24 @agent_ppo2.py:185][0m |          -0.0109 |          12.4859 |          21.0194 |
[32m[20221214 13:59:24 @agent_ppo2.py:185][0m |          -0.0095 |          12.1954 |          21.0237 |
[32m[20221214 13:59:24 @agent_ppo2.py:185][0m |          -0.0051 |          11.9871 |          21.0242 |
[32m[20221214 13:59:24 @agent_ppo2.py:185][0m |          -0.0067 |          11.7210 |          21.0246 |
[32m[20221214 13:59:24 @agent_ppo2.py:185][0m |          -0.0074 |          11.6383 |          21.0268 |
[32m[20221214 13:59:24 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 239.46
[32m[20221214 13:59:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.05
[32m[20221214 13:59:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.58
[32m[20221214 13:59:24 @agent_ppo2.py:143][0m Total time:       1.37 min
[32m[20221214 13:59:24 @agent_ppo2.py:145][0m 124928 total steps have happened
[32m[20221214 13:59:24 @agent_ppo2.py:121][0m #------------------------ Iteration 61 --------------------------#
[32m[20221214 13:59:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:25 @agent_ppo2.py:185][0m |           0.0002 |          18.2221 |          21.1180 |
[32m[20221214 13:59:25 @agent_ppo2.py:185][0m |          -0.0029 |          15.3565 |          21.1176 |
[32m[20221214 13:59:25 @agent_ppo2.py:185][0m |          -0.0072 |          14.4859 |          21.1111 |
[32m[20221214 13:59:25 @agent_ppo2.py:185][0m |          -0.0023 |          13.9271 |          21.1093 |
[32m[20221214 13:59:25 @agent_ppo2.py:185][0m |           0.0106 |          14.0248 |          21.1060 |
[32m[20221214 13:59:25 @agent_ppo2.py:185][0m |           0.0042 |          13.8318 |          21.1068 |
[32m[20221214 13:59:25 @agent_ppo2.py:185][0m |           0.0033 |          12.8808 |          21.1118 |
[32m[20221214 13:59:25 @agent_ppo2.py:185][0m |          -0.0061 |          12.7760 |          21.1082 |
[32m[20221214 13:59:26 @agent_ppo2.py:185][0m |           0.0017 |          12.6146 |          21.1111 |
[32m[20221214 13:59:26 @agent_ppo2.py:185][0m |          -0.0046 |          12.1970 |          21.1095 |
[32m[20221214 13:59:26 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 13:59:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.19
[32m[20221214 13:59:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.58
[32m[20221214 13:59:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.95
[32m[20221214 13:59:26 @agent_ppo2.py:143][0m Total time:       1.40 min
[32m[20221214 13:59:26 @agent_ppo2.py:145][0m 126976 total steps have happened
[32m[20221214 13:59:26 @agent_ppo2.py:121][0m #------------------------ Iteration 62 --------------------------#
[32m[20221214 13:59:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:26 @agent_ppo2.py:185][0m |           0.0005 |          20.8406 |          21.0970 |
[32m[20221214 13:59:26 @agent_ppo2.py:185][0m |           0.0092 |          20.4265 |          21.0970 |
[32m[20221214 13:59:26 @agent_ppo2.py:185][0m |          -0.0020 |          17.0354 |          21.0916 |
[32m[20221214 13:59:26 @agent_ppo2.py:185][0m |          -0.0042 |          16.2383 |          21.0903 |
[32m[20221214 13:59:27 @agent_ppo2.py:185][0m |           0.0054 |          16.4877 |          21.0922 |
[32m[20221214 13:59:27 @agent_ppo2.py:185][0m |          -0.0035 |          15.5076 |          21.0877 |
[32m[20221214 13:59:27 @agent_ppo2.py:185][0m |          -0.0041 |          15.0391 |          21.0944 |
[32m[20221214 13:59:27 @agent_ppo2.py:185][0m |          -0.0026 |          14.7899 |          21.0911 |
[32m[20221214 13:59:27 @agent_ppo2.py:185][0m |          -0.0030 |          14.5500 |          21.0959 |
[32m[20221214 13:59:27 @agent_ppo2.py:185][0m |          -0.0027 |          14.3136 |          21.0943 |
[32m[20221214 13:59:27 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.12
[32m[20221214 13:59:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.76
[32m[20221214 13:59:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.41
[32m[20221214 13:59:27 @agent_ppo2.py:143][0m Total time:       1.42 min
[32m[20221214 13:59:27 @agent_ppo2.py:145][0m 129024 total steps have happened
[32m[20221214 13:59:27 @agent_ppo2.py:121][0m #------------------------ Iteration 63 --------------------------#
[32m[20221214 13:59:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:28 @agent_ppo2.py:185][0m |          -0.0020 |          19.7312 |          21.1562 |
[32m[20221214 13:59:28 @agent_ppo2.py:185][0m |           0.0031 |          16.4238 |          21.1557 |
[32m[20221214 13:59:28 @agent_ppo2.py:185][0m |          -0.0042 |          14.7313 |          21.1475 |
[32m[20221214 13:59:28 @agent_ppo2.py:185][0m |          -0.0048 |          13.5153 |          21.1411 |
[32m[20221214 13:59:28 @agent_ppo2.py:185][0m |          -0.0028 |          12.7095 |          21.1374 |
[32m[20221214 13:59:28 @agent_ppo2.py:185][0m |          -0.0087 |          12.1752 |          21.1336 |
[32m[20221214 13:59:28 @agent_ppo2.py:185][0m |          -0.0061 |          11.7595 |          21.1309 |
[32m[20221214 13:59:28 @agent_ppo2.py:185][0m |          -0.0069 |          11.3589 |          21.1300 |
[32m[20221214 13:59:28 @agent_ppo2.py:185][0m |          -0.0012 |          11.1034 |          21.1273 |
[32m[20221214 13:59:28 @agent_ppo2.py:185][0m |          -0.0039 |          10.8758 |          21.1250 |
[32m[20221214 13:59:28 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 221.41
[32m[20221214 13:59:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.20
[32m[20221214 13:59:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.11
[32m[20221214 13:59:29 @agent_ppo2.py:143][0m Total time:       1.44 min
[32m[20221214 13:59:29 @agent_ppo2.py:145][0m 131072 total steps have happened
[32m[20221214 13:59:29 @agent_ppo2.py:121][0m #------------------------ Iteration 64 --------------------------#
[32m[20221214 13:59:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:29 @agent_ppo2.py:185][0m |          -0.0032 |          23.9942 |          21.1912 |
[32m[20221214 13:59:29 @agent_ppo2.py:185][0m |          -0.0029 |          21.0601 |          21.1796 |
[32m[20221214 13:59:29 @agent_ppo2.py:185][0m |           0.0034 |          20.3030 |          21.1753 |
[32m[20221214 13:59:29 @agent_ppo2.py:185][0m |          -0.0055 |          19.0878 |          21.1740 |
[32m[20221214 13:59:29 @agent_ppo2.py:185][0m |          -0.0033 |          18.5448 |          21.1606 |
[32m[20221214 13:59:30 @agent_ppo2.py:185][0m |          -0.0027 |          18.1383 |          21.1605 |
[32m[20221214 13:59:30 @agent_ppo2.py:185][0m |          -0.0051 |          17.8066 |          21.1572 |
[32m[20221214 13:59:30 @agent_ppo2.py:185][0m |          -0.0057 |          17.4165 |          21.1538 |
[32m[20221214 13:59:30 @agent_ppo2.py:185][0m |          -0.0072 |          17.2251 |          21.1524 |
[32m[20221214 13:59:30 @agent_ppo2.py:185][0m |          -0.0048 |          17.1222 |          21.1464 |
[32m[20221214 13:59:30 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.14
[32m[20221214 13:59:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 322.74
[32m[20221214 13:59:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 318.49
[32m[20221214 13:59:30 @agent_ppo2.py:143][0m Total time:       1.47 min
[32m[20221214 13:59:30 @agent_ppo2.py:145][0m 133120 total steps have happened
[32m[20221214 13:59:30 @agent_ppo2.py:121][0m #------------------------ Iteration 65 --------------------------#
[32m[20221214 13:59:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:30 @agent_ppo2.py:185][0m |          -0.0021 |          21.2942 |          21.1339 |
[32m[20221214 13:59:31 @agent_ppo2.py:185][0m |           0.0044 |          18.0659 |          21.1331 |
[32m[20221214 13:59:31 @agent_ppo2.py:185][0m |          -0.0065 |          16.4423 |          21.1306 |
[32m[20221214 13:59:31 @agent_ppo2.py:185][0m |          -0.0085 |          15.4788 |          21.1302 |
[32m[20221214 13:59:31 @agent_ppo2.py:185][0m |          -0.0018 |          14.8492 |          21.1309 |
[32m[20221214 13:59:31 @agent_ppo2.py:185][0m |          -0.0014 |          14.2104 |          21.1333 |
[32m[20221214 13:59:31 @agent_ppo2.py:185][0m |          -0.0049 |          13.7059 |          21.1333 |
[32m[20221214 13:59:31 @agent_ppo2.py:185][0m |           0.0076 |          14.9333 |          21.1320 |
[32m[20221214 13:59:31 @agent_ppo2.py:185][0m |          -0.0029 |          13.0554 |          21.1363 |
[32m[20221214 13:59:31 @agent_ppo2.py:185][0m |          -0.0077 |          12.7329 |          21.1383 |
[32m[20221214 13:59:31 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.70
[32m[20221214 13:59:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 345.26
[32m[20221214 13:59:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 318.00
[32m[20221214 13:59:31 @agent_ppo2.py:143][0m Total time:       1.49 min
[32m[20221214 13:59:31 @agent_ppo2.py:145][0m 135168 total steps have happened
[32m[20221214 13:59:31 @agent_ppo2.py:121][0m #------------------------ Iteration 66 --------------------------#
[32m[20221214 13:59:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:32 @agent_ppo2.py:185][0m |           0.0004 |          26.3051 |          21.2294 |
[32m[20221214 13:59:32 @agent_ppo2.py:185][0m |          -0.0004 |          21.2768 |          21.2288 |
[32m[20221214 13:59:32 @agent_ppo2.py:185][0m |           0.0053 |          20.6581 |          21.2258 |
[32m[20221214 13:59:32 @agent_ppo2.py:185][0m |          -0.0023 |          18.5000 |          21.2255 |
[32m[20221214 13:59:32 @agent_ppo2.py:185][0m |          -0.0012 |          17.6210 |          21.2253 |
[32m[20221214 13:59:32 @agent_ppo2.py:185][0m |           0.0083 |          19.6175 |          21.2259 |
[32m[20221214 13:59:32 @agent_ppo2.py:185][0m |          -0.0018 |          16.7061 |          21.2294 |
[32m[20221214 13:59:33 @agent_ppo2.py:185][0m |          -0.0015 |          16.0256 |          21.2270 |
[32m[20221214 13:59:33 @agent_ppo2.py:185][0m |          -0.0029 |          15.8093 |          21.2301 |
[32m[20221214 13:59:33 @agent_ppo2.py:185][0m |          -0.0004 |          15.4391 |          21.2318 |
[32m[20221214 13:59:33 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.98
[32m[20221214 13:59:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 331.49
[32m[20221214 13:59:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 326.47
[32m[20221214 13:59:33 @agent_ppo2.py:143][0m Total time:       1.51 min
[32m[20221214 13:59:33 @agent_ppo2.py:145][0m 137216 total steps have happened
[32m[20221214 13:59:33 @agent_ppo2.py:121][0m #------------------------ Iteration 67 --------------------------#
[32m[20221214 13:59:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:33 @agent_ppo2.py:185][0m |           0.0002 |          22.1168 |          21.0465 |
[32m[20221214 13:59:33 @agent_ppo2.py:185][0m |          -0.0020 |          19.8704 |          21.0344 |
[32m[20221214 13:59:34 @agent_ppo2.py:185][0m |          -0.0016 |          18.9008 |          21.0306 |
[32m[20221214 13:59:34 @agent_ppo2.py:185][0m |          -0.0004 |          18.2489 |          21.0251 |
[32m[20221214 13:59:34 @agent_ppo2.py:185][0m |           0.0018 |          17.5187 |          21.0177 |
[32m[20221214 13:59:34 @agent_ppo2.py:185][0m |           0.0046 |          17.3404 |          21.0200 |
[32m[20221214 13:59:34 @agent_ppo2.py:185][0m |           0.0074 |          17.9723 |          21.0088 |
[32m[20221214 13:59:34 @agent_ppo2.py:185][0m |          -0.0006 |          16.4757 |          21.0123 |
[32m[20221214 13:59:34 @agent_ppo2.py:185][0m |          -0.0004 |          16.3399 |          21.0098 |
[32m[20221214 13:59:34 @agent_ppo2.py:185][0m |          -0.0036 |          16.0997 |          20.9997 |
[32m[20221214 13:59:34 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.77
[32m[20221214 13:59:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 321.67
[32m[20221214 13:59:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.61
[32m[20221214 13:59:34 @agent_ppo2.py:143][0m Total time:       1.54 min
[32m[20221214 13:59:34 @agent_ppo2.py:145][0m 139264 total steps have happened
[32m[20221214 13:59:34 @agent_ppo2.py:121][0m #------------------------ Iteration 68 --------------------------#
[32m[20221214 13:59:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:35 @agent_ppo2.py:185][0m |           0.0010 |          19.9620 |          21.0420 |
[32m[20221214 13:59:35 @agent_ppo2.py:185][0m |          -0.0032 |          17.7818 |          21.0430 |
[32m[20221214 13:59:35 @agent_ppo2.py:185][0m |          -0.0057 |          16.8116 |          21.0400 |
[32m[20221214 13:59:35 @agent_ppo2.py:185][0m |          -0.0077 |          16.1200 |          21.0374 |
[32m[20221214 13:59:35 @agent_ppo2.py:185][0m |          -0.0016 |          15.5940 |          21.0404 |
[32m[20221214 13:59:35 @agent_ppo2.py:185][0m |          -0.0033 |          15.1061 |          21.0370 |
[32m[20221214 13:59:35 @agent_ppo2.py:185][0m |          -0.0023 |          14.7942 |          21.0341 |
[32m[20221214 13:59:35 @agent_ppo2.py:185][0m |          -0.0023 |          14.4532 |          21.0407 |
[32m[20221214 13:59:36 @agent_ppo2.py:185][0m |           0.0084 |          15.4071 |          21.0370 |
[32m[20221214 13:59:36 @agent_ppo2.py:185][0m |          -0.0089 |          14.0117 |          21.0386 |
[32m[20221214 13:59:36 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.80
[32m[20221214 13:59:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.75
[32m[20221214 13:59:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.14
[32m[20221214 13:59:36 @agent_ppo2.py:143][0m Total time:       1.56 min
[32m[20221214 13:59:36 @agent_ppo2.py:145][0m 141312 total steps have happened
[32m[20221214 13:59:36 @agent_ppo2.py:121][0m #------------------------ Iteration 69 --------------------------#
[32m[20221214 13:59:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:36 @agent_ppo2.py:185][0m |           0.0052 |          24.1375 |          21.0846 |
[32m[20221214 13:59:36 @agent_ppo2.py:185][0m |          -0.0029 |          18.7993 |          21.0725 |
[32m[20221214 13:59:36 @agent_ppo2.py:185][0m |           0.0016 |          17.8638 |          21.0659 |
[32m[20221214 13:59:36 @agent_ppo2.py:185][0m |          -0.0061 |          16.2774 |          21.0633 |
[32m[20221214 13:59:37 @agent_ppo2.py:185][0m |           0.0114 |          17.0443 |          21.0509 |
[32m[20221214 13:59:37 @agent_ppo2.py:185][0m |           0.0096 |          15.7088 |          21.0484 |
[32m[20221214 13:59:37 @agent_ppo2.py:185][0m |          -0.0052 |          14.3773 |          21.0425 |
[32m[20221214 13:59:37 @agent_ppo2.py:185][0m |          -0.0005 |          13.9912 |          21.0370 |
[32m[20221214 13:59:37 @agent_ppo2.py:185][0m |          -0.0033 |          13.5800 |          21.0328 |
[32m[20221214 13:59:37 @agent_ppo2.py:185][0m |          -0.0052 |          13.2788 |          21.0268 |
[32m[20221214 13:59:37 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.21
[32m[20221214 13:59:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 323.18
[32m[20221214 13:59:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.97
[32m[20221214 13:59:37 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 400.97
[32m[20221214 13:59:37 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 400.97
[32m[20221214 13:59:37 @agent_ppo2.py:143][0m Total time:       1.59 min
[32m[20221214 13:59:37 @agent_ppo2.py:145][0m 143360 total steps have happened
[32m[20221214 13:59:37 @agent_ppo2.py:121][0m #------------------------ Iteration 70 --------------------------#
[32m[20221214 13:59:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 13:59:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:38 @agent_ppo2.py:185][0m |           0.0009 |          21.7571 |          21.0715 |
[32m[20221214 13:59:38 @agent_ppo2.py:185][0m |           0.0009 |          18.5183 |          21.0698 |
[32m[20221214 13:59:38 @agent_ppo2.py:185][0m |          -0.0064 |          17.3723 |          21.0657 |
[32m[20221214 13:59:38 @agent_ppo2.py:185][0m |          -0.0043 |          16.7370 |          21.0650 |
[32m[20221214 13:59:38 @agent_ppo2.py:185][0m |          -0.0031 |          16.1448 |          21.0645 |
[32m[20221214 13:59:38 @agent_ppo2.py:185][0m |          -0.0029 |          15.7986 |          21.0624 |
[32m[20221214 13:59:38 @agent_ppo2.py:185][0m |          -0.0018 |          15.4353 |          21.0599 |
[32m[20221214 13:59:38 @agent_ppo2.py:185][0m |          -0.0067 |          15.0780 |          21.0586 |
[32m[20221214 13:59:38 @agent_ppo2.py:185][0m |          -0.0001 |          14.9929 |          21.0603 |
[32m[20221214 13:59:39 @agent_ppo2.py:185][0m |          -0.0004 |          14.5958 |          21.0578 |
[32m[20221214 13:59:39 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 13:59:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 294.65
[32m[20221214 13:59:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 330.31
[32m[20221214 13:59:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.33
[32m[20221214 13:59:39 @agent_ppo2.py:143][0m Total time:       1.61 min
[32m[20221214 13:59:39 @agent_ppo2.py:145][0m 145408 total steps have happened
[32m[20221214 13:59:39 @agent_ppo2.py:121][0m #------------------------ Iteration 71 --------------------------#
[32m[20221214 13:59:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:39 @agent_ppo2.py:185][0m |          -0.0023 |          27.7403 |          21.2262 |
[32m[20221214 13:59:39 @agent_ppo2.py:185][0m |          -0.0020 |          24.5745 |          21.2219 |
[32m[20221214 13:59:39 @agent_ppo2.py:185][0m |          -0.0036 |          23.1315 |          21.2099 |
[32m[20221214 13:59:39 @agent_ppo2.py:185][0m |          -0.0063 |          22.2596 |          21.2074 |
[32m[20221214 13:59:39 @agent_ppo2.py:185][0m |          -0.0050 |          21.7507 |          21.2106 |
[32m[20221214 13:59:40 @agent_ppo2.py:185][0m |           0.0011 |          21.6126 |          21.2080 |
[32m[20221214 13:59:40 @agent_ppo2.py:185][0m |           0.0027 |          21.6916 |          21.2075 |
[32m[20221214 13:59:40 @agent_ppo2.py:185][0m |          -0.0034 |          20.8132 |          21.2051 |
[32m[20221214 13:59:40 @agent_ppo2.py:185][0m |          -0.0040 |          20.6243 |          21.2042 |
[32m[20221214 13:59:40 @agent_ppo2.py:185][0m |          -0.0031 |          20.2360 |          21.2051 |
[32m[20221214 13:59:40 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.33
[32m[20221214 13:59:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.79
[32m[20221214 13:59:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.32
[32m[20221214 13:59:40 @agent_ppo2.py:143][0m Total time:       1.63 min
[32m[20221214 13:59:40 @agent_ppo2.py:145][0m 147456 total steps have happened
[32m[20221214 13:59:40 @agent_ppo2.py:121][0m #------------------------ Iteration 72 --------------------------#
[32m[20221214 13:59:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:40 @agent_ppo2.py:185][0m |           0.0007 |          21.5532 |          21.0167 |
[32m[20221214 13:59:41 @agent_ppo2.py:185][0m |          -0.0035 |          17.2189 |          21.0112 |
[32m[20221214 13:59:41 @agent_ppo2.py:185][0m |           0.0031 |          16.4770 |          21.0028 |
[32m[20221214 13:59:41 @agent_ppo2.py:185][0m |          -0.0002 |          14.6889 |          20.9981 |
[32m[20221214 13:59:41 @agent_ppo2.py:185][0m |          -0.0050 |          13.9805 |          20.9974 |
[32m[20221214 13:59:41 @agent_ppo2.py:185][0m |          -0.0005 |          13.6042 |          20.9903 |
[32m[20221214 13:59:41 @agent_ppo2.py:185][0m |          -0.0045 |          12.9988 |          20.9842 |
[32m[20221214 13:59:41 @agent_ppo2.py:185][0m |          -0.0049 |          12.5936 |          20.9860 |
[32m[20221214 13:59:41 @agent_ppo2.py:185][0m |          -0.0046 |          12.2677 |          20.9821 |
[32m[20221214 13:59:41 @agent_ppo2.py:185][0m |          -0.0040 |          11.9110 |          20.9751 |
[32m[20221214 13:59:41 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.74
[32m[20221214 13:59:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 309.04
[32m[20221214 13:59:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.98
[32m[20221214 13:59:42 @agent_ppo2.py:143][0m Total time:       1.66 min
[32m[20221214 13:59:42 @agent_ppo2.py:145][0m 149504 total steps have happened
[32m[20221214 13:59:42 @agent_ppo2.py:121][0m #------------------------ Iteration 73 --------------------------#
[32m[20221214 13:59:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:42 @agent_ppo2.py:185][0m |           0.0003 |          21.2216 |          21.1354 |
[32m[20221214 13:59:42 @agent_ppo2.py:185][0m |          -0.0033 |          15.8913 |          21.1418 |
[32m[20221214 13:59:42 @agent_ppo2.py:185][0m |           0.0097 |          15.3443 |          21.1421 |
[32m[20221214 13:59:42 @agent_ppo2.py:185][0m |          -0.0043 |          13.2765 |          21.1470 |
[32m[20221214 13:59:42 @agent_ppo2.py:185][0m |          -0.0007 |          12.6475 |          21.1492 |
[32m[20221214 13:59:42 @agent_ppo2.py:185][0m |          -0.0043 |          12.2723 |          21.1509 |
[32m[20221214 13:59:42 @agent_ppo2.py:185][0m |           0.0051 |          11.7889 |          21.1525 |
[32m[20221214 13:59:43 @agent_ppo2.py:185][0m |           0.0008 |          11.4286 |          21.1562 |
[32m[20221214 13:59:43 @agent_ppo2.py:185][0m |          -0.0046 |          11.0105 |          21.1573 |
[32m[20221214 13:59:43 @agent_ppo2.py:185][0m |          -0.0041 |          10.7295 |          21.1585 |
[32m[20221214 13:59:43 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.56
[32m[20221214 13:59:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 308.90
[32m[20221214 13:59:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.19
[32m[20221214 13:59:43 @agent_ppo2.py:143][0m Total time:       1.68 min
[32m[20221214 13:59:43 @agent_ppo2.py:145][0m 151552 total steps have happened
[32m[20221214 13:59:43 @agent_ppo2.py:121][0m #------------------------ Iteration 74 --------------------------#
[32m[20221214 13:59:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:43 @agent_ppo2.py:185][0m |          -0.0029 |          15.5034 |          21.1379 |
[32m[20221214 13:59:43 @agent_ppo2.py:185][0m |           0.0004 |          13.1858 |          21.1329 |
[32m[20221214 13:59:44 @agent_ppo2.py:185][0m |          -0.0043 |          12.0576 |          21.1222 |
[32m[20221214 13:59:44 @agent_ppo2.py:185][0m |           0.0030 |          11.8682 |          21.1135 |
[32m[20221214 13:59:44 @agent_ppo2.py:185][0m |          -0.0036 |          10.9166 |          21.1072 |
[32m[20221214 13:59:44 @agent_ppo2.py:185][0m |          -0.0054 |          10.5992 |          21.1081 |
[32m[20221214 13:59:44 @agent_ppo2.py:185][0m |          -0.0047 |          10.2249 |          21.1041 |
[32m[20221214 13:59:44 @agent_ppo2.py:185][0m |          -0.0048 |           9.9666 |          21.1053 |
[32m[20221214 13:59:44 @agent_ppo2.py:185][0m |          -0.0084 |           9.7656 |          21.1045 |
[32m[20221214 13:59:44 @agent_ppo2.py:185][0m |          -0.0036 |           9.6003 |          21.1031 |
[32m[20221214 13:59:44 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.00
[32m[20221214 13:59:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.23
[32m[20221214 13:59:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.41
[32m[20221214 13:59:44 @agent_ppo2.py:143][0m Total time:       1.71 min
[32m[20221214 13:59:44 @agent_ppo2.py:145][0m 153600 total steps have happened
[32m[20221214 13:59:44 @agent_ppo2.py:121][0m #------------------------ Iteration 75 --------------------------#
[32m[20221214 13:59:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:45 @agent_ppo2.py:185][0m |           0.0069 |          29.8083 |          21.1256 |
[32m[20221214 13:59:45 @agent_ppo2.py:185][0m |          -0.0038 |          25.1960 |          21.1096 |
[32m[20221214 13:59:45 @agent_ppo2.py:185][0m |           0.0064 |          24.3450 |          21.1057 |
[32m[20221214 13:59:45 @agent_ppo2.py:185][0m |          -0.0051 |          22.2984 |          21.0962 |
[32m[20221214 13:59:45 @agent_ppo2.py:185][0m |           0.0031 |          21.7491 |          21.0930 |
[32m[20221214 13:59:45 @agent_ppo2.py:185][0m |          -0.0040 |          20.7018 |          21.0979 |
[32m[20221214 13:59:45 @agent_ppo2.py:185][0m |          -0.0023 |          20.2326 |          21.0949 |
[32m[20221214 13:59:45 @agent_ppo2.py:185][0m |          -0.0004 |          19.7097 |          21.0839 |
[32m[20221214 13:59:46 @agent_ppo2.py:185][0m |          -0.0069 |          19.3111 |          21.0886 |
[32m[20221214 13:59:46 @agent_ppo2.py:185][0m |          -0.0070 |          18.9008 |          21.0858 |
[32m[20221214 13:59:46 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 13:59:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.68
[32m[20221214 13:59:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.71
[32m[20221214 13:59:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 219.48
[32m[20221214 13:59:46 @agent_ppo2.py:143][0m Total time:       1.73 min
[32m[20221214 13:59:46 @agent_ppo2.py:145][0m 155648 total steps have happened
[32m[20221214 13:59:46 @agent_ppo2.py:121][0m #------------------------ Iteration 76 --------------------------#
[32m[20221214 13:59:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:46 @agent_ppo2.py:185][0m |          -0.0025 |          21.3926 |          20.9903 |
[32m[20221214 13:59:46 @agent_ppo2.py:185][0m |          -0.0001 |          17.5727 |          20.9839 |
[32m[20221214 13:59:46 @agent_ppo2.py:185][0m |          -0.0018 |          15.9178 |          20.9776 |
[32m[20221214 13:59:46 @agent_ppo2.py:185][0m |          -0.0071 |          14.9340 |          20.9739 |
[32m[20221214 13:59:47 @agent_ppo2.py:185][0m |           0.0016 |          14.1613 |          20.9704 |
[32m[20221214 13:59:47 @agent_ppo2.py:185][0m |           0.0085 |          14.0520 |          20.9711 |
[32m[20221214 13:59:47 @agent_ppo2.py:185][0m |          -0.0051 |          12.6946 |          20.9655 |
[32m[20221214 13:59:47 @agent_ppo2.py:185][0m |           0.0021 |          12.5441 |          20.9660 |
[32m[20221214 13:59:47 @agent_ppo2.py:185][0m |          -0.0031 |          11.8838 |          20.9662 |
[32m[20221214 13:59:47 @agent_ppo2.py:185][0m |          -0.0037 |          11.5295 |          20.9710 |
[32m[20221214 13:59:47 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 13:59:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.27
[32m[20221214 13:59:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 327.23
[32m[20221214 13:59:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.77
[32m[20221214 13:59:47 @agent_ppo2.py:143][0m Total time:       1.75 min
[32m[20221214 13:59:47 @agent_ppo2.py:145][0m 157696 total steps have happened
[32m[20221214 13:59:47 @agent_ppo2.py:121][0m #------------------------ Iteration 77 --------------------------#
[32m[20221214 13:59:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:48 @agent_ppo2.py:185][0m |           0.0038 |          18.8808 |          21.1506 |
[32m[20221214 13:59:48 @agent_ppo2.py:185][0m |          -0.0005 |          15.5179 |          21.1474 |
[32m[20221214 13:59:48 @agent_ppo2.py:185][0m |           0.0024 |          14.2443 |          21.1417 |
[32m[20221214 13:59:48 @agent_ppo2.py:185][0m |           0.0067 |          14.3321 |          21.1375 |
[32m[20221214 13:59:48 @agent_ppo2.py:185][0m |          -0.0036 |          12.9417 |          21.1303 |
[32m[20221214 13:59:48 @agent_ppo2.py:185][0m |          -0.0020 |          12.4387 |          21.1243 |
[32m[20221214 13:59:48 @agent_ppo2.py:185][0m |           0.0006 |          11.9910 |          21.1239 |
[32m[20221214 13:59:48 @agent_ppo2.py:185][0m |           0.0059 |          11.8832 |          21.1197 |
[32m[20221214 13:59:48 @agent_ppo2.py:185][0m |           0.0023 |          11.4354 |          21.1138 |
[32m[20221214 13:59:49 @agent_ppo2.py:185][0m |          -0.0001 |          11.1949 |          21.1095 |
[32m[20221214 13:59:49 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.83
[32m[20221214 13:59:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.35
[32m[20221214 13:59:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.65
[32m[20221214 13:59:49 @agent_ppo2.py:143][0m Total time:       1.78 min
[32m[20221214 13:59:49 @agent_ppo2.py:145][0m 159744 total steps have happened
[32m[20221214 13:59:49 @agent_ppo2.py:121][0m #------------------------ Iteration 78 --------------------------#
[32m[20221214 13:59:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:49 @agent_ppo2.py:185][0m |          -0.0015 |          25.3525 |          21.0864 |
[32m[20221214 13:59:49 @agent_ppo2.py:185][0m |          -0.0051 |          20.9242 |          21.0767 |
[32m[20221214 13:59:49 @agent_ppo2.py:185][0m |          -0.0022 |          19.1086 |          21.0714 |
[32m[20221214 13:59:49 @agent_ppo2.py:185][0m |          -0.0053 |          17.9339 |          21.0703 |
[32m[20221214 13:59:49 @agent_ppo2.py:185][0m |          -0.0060 |          17.0175 |          21.0704 |
[32m[20221214 13:59:50 @agent_ppo2.py:185][0m |          -0.0069 |          16.3991 |          21.0718 |
[32m[20221214 13:59:50 @agent_ppo2.py:185][0m |          -0.0065 |          15.8081 |          21.0671 |
[32m[20221214 13:59:50 @agent_ppo2.py:185][0m |          -0.0053 |          15.3391 |          21.0699 |
[32m[20221214 13:59:50 @agent_ppo2.py:185][0m |          -0.0051 |          15.0351 |          21.0679 |
[32m[20221214 13:59:50 @agent_ppo2.py:185][0m |          -0.0057 |          14.6705 |          21.0647 |
[32m[20221214 13:59:50 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.86
[32m[20221214 13:59:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.98
[32m[20221214 13:59:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.57
[32m[20221214 13:59:50 @agent_ppo2.py:143][0m Total time:       1.80 min
[32m[20221214 13:59:50 @agent_ppo2.py:145][0m 161792 total steps have happened
[32m[20221214 13:59:50 @agent_ppo2.py:121][0m #------------------------ Iteration 79 --------------------------#
[32m[20221214 13:59:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:50 @agent_ppo2.py:185][0m |           0.0010 |          23.7405 |          21.0943 |
[32m[20221214 13:59:51 @agent_ppo2.py:185][0m |           0.0006 |          18.4539 |          21.0880 |
[32m[20221214 13:59:51 @agent_ppo2.py:185][0m |          -0.0030 |          16.7516 |          21.0879 |
[32m[20221214 13:59:51 @agent_ppo2.py:185][0m |          -0.0009 |          15.7146 |          21.0876 |
[32m[20221214 13:59:51 @agent_ppo2.py:185][0m |          -0.0025 |          14.8956 |          21.0875 |
[32m[20221214 13:59:51 @agent_ppo2.py:185][0m |          -0.0015 |          14.1907 |          21.0870 |
[32m[20221214 13:59:51 @agent_ppo2.py:185][0m |          -0.0017 |          13.7244 |          21.0860 |
[32m[20221214 13:59:51 @agent_ppo2.py:185][0m |           0.0019 |          13.3008 |          21.0842 |
[32m[20221214 13:59:51 @agent_ppo2.py:185][0m |          -0.0002 |          12.8664 |          21.0881 |
[32m[20221214 13:59:51 @agent_ppo2.py:185][0m |          -0.0030 |          12.5098 |          21.0817 |
[32m[20221214 13:59:51 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 13:59:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.73
[32m[20221214 13:59:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.71
[32m[20221214 13:59:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 234.43
[32m[20221214 13:59:52 @agent_ppo2.py:143][0m Total time:       1.83 min
[32m[20221214 13:59:52 @agent_ppo2.py:145][0m 163840 total steps have happened
[32m[20221214 13:59:52 @agent_ppo2.py:121][0m #------------------------ Iteration 80 --------------------------#
[32m[20221214 13:59:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 13:59:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:52 @agent_ppo2.py:185][0m |           0.0008 |          19.1244 |          21.1496 |
[32m[20221214 13:59:52 @agent_ppo2.py:185][0m |          -0.0037 |          16.9753 |          21.1531 |
[32m[20221214 13:59:52 @agent_ppo2.py:185][0m |          -0.0059 |          16.0512 |          21.1548 |
[32m[20221214 13:59:52 @agent_ppo2.py:185][0m |           0.0010 |          15.4911 |          21.1528 |
[32m[20221214 13:59:52 @agent_ppo2.py:185][0m |          -0.0036 |          14.9798 |          21.1522 |
[32m[20221214 13:59:52 @agent_ppo2.py:185][0m |          -0.0026 |          14.5423 |          21.1507 |
[32m[20221214 13:59:53 @agent_ppo2.py:185][0m |          -0.0032 |          14.3002 |          21.1497 |
[32m[20221214 13:59:53 @agent_ppo2.py:185][0m |          -0.0021 |          13.9930 |          21.1509 |
[32m[20221214 13:59:53 @agent_ppo2.py:185][0m |          -0.0055 |          13.8822 |          21.1495 |
[32m[20221214 13:59:53 @agent_ppo2.py:185][0m |          -0.0029 |          13.7247 |          21.1476 |
[32m[20221214 13:59:53 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.12
[32m[20221214 13:59:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 308.12
[32m[20221214 13:59:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 346.98
[32m[20221214 13:59:53 @agent_ppo2.py:143][0m Total time:       1.85 min
[32m[20221214 13:59:53 @agent_ppo2.py:145][0m 165888 total steps have happened
[32m[20221214 13:59:53 @agent_ppo2.py:121][0m #------------------------ Iteration 81 --------------------------#
[32m[20221214 13:59:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 13:59:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:53 @agent_ppo2.py:185][0m |          -0.0043 |          21.1481 |          21.2059 |
[32m[20221214 13:59:53 @agent_ppo2.py:185][0m |           0.0002 |          16.4638 |          21.2058 |
[32m[20221214 13:59:54 @agent_ppo2.py:185][0m |           0.0003 |          14.6316 |          21.2011 |
[32m[20221214 13:59:54 @agent_ppo2.py:185][0m |          -0.0034 |          13.5761 |          21.1976 |
[32m[20221214 13:59:54 @agent_ppo2.py:185][0m |          -0.0017 |          12.8604 |          21.1962 |
[32m[20221214 13:59:54 @agent_ppo2.py:185][0m |           0.0015 |          12.3400 |          21.1965 |
[32m[20221214 13:59:54 @agent_ppo2.py:185][0m |           0.0020 |          11.7876 |          21.1942 |
[32m[20221214 13:59:54 @agent_ppo2.py:185][0m |           0.0002 |          11.3769 |          21.1971 |
[32m[20221214 13:59:54 @agent_ppo2.py:185][0m |          -0.0027 |          11.0350 |          21.1968 |
[32m[20221214 13:59:54 @agent_ppo2.py:185][0m |          -0.0067 |          10.8106 |          21.1946 |
[32m[20221214 13:59:54 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 312.11
[32m[20221214 13:59:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 341.66
[32m[20221214 13:59:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 270.53
[32m[20221214 13:59:54 @agent_ppo2.py:143][0m Total time:       1.87 min
[32m[20221214 13:59:54 @agent_ppo2.py:145][0m 167936 total steps have happened
[32m[20221214 13:59:54 @agent_ppo2.py:121][0m #------------------------ Iteration 82 --------------------------#
[32m[20221214 13:59:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:55 @agent_ppo2.py:185][0m |          -0.0015 |          25.5286 |          21.2222 |
[32m[20221214 13:59:55 @agent_ppo2.py:185][0m |           0.0001 |          22.0457 |          21.2153 |
[32m[20221214 13:59:55 @agent_ppo2.py:185][0m |          -0.0013 |          20.0989 |          21.2101 |
[32m[20221214 13:59:55 @agent_ppo2.py:185][0m |          -0.0010 |          18.9527 |          21.2046 |
[32m[20221214 13:59:55 @agent_ppo2.py:185][0m |          -0.0038 |          17.8325 |          21.2007 |
[32m[20221214 13:59:55 @agent_ppo2.py:185][0m |          -0.0029 |          17.0436 |          21.2047 |
[32m[20221214 13:59:55 @agent_ppo2.py:185][0m |           0.0000 |          16.3366 |          21.1991 |
[32m[20221214 13:59:56 @agent_ppo2.py:185][0m |          -0.0053 |          15.6885 |          21.1987 |
[32m[20221214 13:59:56 @agent_ppo2.py:185][0m |          -0.0017 |          15.2603 |          21.1977 |
[32m[20221214 13:59:56 @agent_ppo2.py:185][0m |          -0.0019 |          14.7065 |          21.1927 |
[32m[20221214 13:59:56 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 13:59:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.23
[32m[20221214 13:59:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.40
[32m[20221214 13:59:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.35
[32m[20221214 13:59:56 @agent_ppo2.py:143][0m Total time:       1.90 min
[32m[20221214 13:59:56 @agent_ppo2.py:145][0m 169984 total steps have happened
[32m[20221214 13:59:56 @agent_ppo2.py:121][0m #------------------------ Iteration 83 --------------------------#
[32m[20221214 13:59:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:56 @agent_ppo2.py:185][0m |           0.0070 |          22.4600 |          21.1867 |
[32m[20221214 13:59:56 @agent_ppo2.py:185][0m |          -0.0001 |          17.5545 |          21.1873 |
[32m[20221214 13:59:56 @agent_ppo2.py:185][0m |          -0.0046 |          16.0111 |          21.1864 |
[32m[20221214 13:59:57 @agent_ppo2.py:185][0m |          -0.0032 |          15.0916 |          21.1864 |
[32m[20221214 13:59:57 @agent_ppo2.py:185][0m |          -0.0001 |          14.4823 |          21.1888 |
[32m[20221214 13:59:57 @agent_ppo2.py:185][0m |          -0.0014 |          13.9605 |          21.1927 |
[32m[20221214 13:59:57 @agent_ppo2.py:185][0m |          -0.0039 |          13.5173 |          21.1896 |
[32m[20221214 13:59:57 @agent_ppo2.py:185][0m |          -0.0040 |          13.1615 |          21.1891 |
[32m[20221214 13:59:57 @agent_ppo2.py:185][0m |          -0.0050 |          12.8074 |          21.1915 |
[32m[20221214 13:59:57 @agent_ppo2.py:185][0m |           0.0001 |          12.5306 |          21.1922 |
[32m[20221214 13:59:57 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 13:59:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.57
[32m[20221214 13:59:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.30
[32m[20221214 13:59:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.67
[32m[20221214 13:59:57 @agent_ppo2.py:143][0m Total time:       1.92 min
[32m[20221214 13:59:57 @agent_ppo2.py:145][0m 172032 total steps have happened
[32m[20221214 13:59:57 @agent_ppo2.py:121][0m #------------------------ Iteration 84 --------------------------#
[32m[20221214 13:59:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:58 @agent_ppo2.py:185][0m |          -0.0040 |          21.3563 |          21.1642 |
[32m[20221214 13:59:58 @agent_ppo2.py:185][0m |          -0.0018 |          18.1101 |          21.1574 |
[32m[20221214 13:59:58 @agent_ppo2.py:185][0m |           0.0017 |          16.8693 |          21.1465 |
[32m[20221214 13:59:58 @agent_ppo2.py:185][0m |          -0.0010 |          15.8770 |          21.1450 |
[32m[20221214 13:59:58 @agent_ppo2.py:185][0m |          -0.0039 |          15.2358 |          21.1442 |
[32m[20221214 13:59:58 @agent_ppo2.py:185][0m |          -0.0068 |          14.6917 |          21.1406 |
[32m[20221214 13:59:58 @agent_ppo2.py:185][0m |          -0.0026 |          14.2512 |          21.1437 |
[32m[20221214 13:59:58 @agent_ppo2.py:185][0m |          -0.0041 |          13.9549 |          21.1400 |
[32m[20221214 13:59:58 @agent_ppo2.py:185][0m |          -0.0042 |          13.7389 |          21.1366 |
[32m[20221214 13:59:59 @agent_ppo2.py:185][0m |          -0.0035 |          13.3791 |          21.1364 |
[32m[20221214 13:59:59 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 13:59:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.20
[32m[20221214 13:59:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.77
[32m[20221214 13:59:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.20
[32m[20221214 13:59:59 @agent_ppo2.py:143][0m Total time:       1.94 min
[32m[20221214 13:59:59 @agent_ppo2.py:145][0m 174080 total steps have happened
[32m[20221214 13:59:59 @agent_ppo2.py:121][0m #------------------------ Iteration 85 --------------------------#
[32m[20221214 13:59:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 13:59:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 13:59:59 @agent_ppo2.py:185][0m |           0.0066 |          23.7960 |          21.1001 |
[32m[20221214 13:59:59 @agent_ppo2.py:185][0m |           0.0046 |          19.4600 |          21.1001 |
[32m[20221214 13:59:59 @agent_ppo2.py:185][0m |          -0.0011 |          17.8463 |          21.1018 |
[32m[20221214 13:59:59 @agent_ppo2.py:185][0m |          -0.0001 |          16.9911 |          21.0993 |
[32m[20221214 14:00:00 @agent_ppo2.py:185][0m |          -0.0014 |          16.3559 |          21.0978 |
[32m[20221214 14:00:00 @agent_ppo2.py:185][0m |           0.0013 |          16.0148 |          21.0961 |
[32m[20221214 14:00:00 @agent_ppo2.py:185][0m |          -0.0027 |          15.4589 |          21.0927 |
[32m[20221214 14:00:00 @agent_ppo2.py:185][0m |           0.0138 |          16.4222 |          21.0910 |
[32m[20221214 14:00:00 @agent_ppo2.py:185][0m |          -0.0002 |          14.9296 |          21.0831 |
[32m[20221214 14:00:00 @agent_ppo2.py:185][0m |           0.0054 |          15.0202 |          21.0798 |
[32m[20221214 14:00:00 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:00:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.59
[32m[20221214 14:00:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.88
[32m[20221214 14:00:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.40
[32m[20221214 14:00:00 @agent_ppo2.py:143][0m Total time:       1.97 min
[32m[20221214 14:00:00 @agent_ppo2.py:145][0m 176128 total steps have happened
[32m[20221214 14:00:00 @agent_ppo2.py:121][0m #------------------------ Iteration 86 --------------------------#
[32m[20221214 14:00:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:01 @agent_ppo2.py:185][0m |           0.0004 |          23.6007 |          21.0788 |
[32m[20221214 14:00:01 @agent_ppo2.py:185][0m |          -0.0005 |          18.5796 |          21.0718 |
[32m[20221214 14:00:01 @agent_ppo2.py:185][0m |          -0.0031 |          16.7310 |          21.0634 |
[32m[20221214 14:00:01 @agent_ppo2.py:185][0m |           0.0103 |          16.5140 |          21.0559 |
[32m[20221214 14:00:01 @agent_ppo2.py:185][0m |          -0.0051 |          14.8924 |          21.0570 |
[32m[20221214 14:00:01 @agent_ppo2.py:185][0m |          -0.0043 |          14.2869 |          21.0494 |
[32m[20221214 14:00:01 @agent_ppo2.py:185][0m |          -0.0040 |          13.8228 |          21.0423 |
[32m[20221214 14:00:01 @agent_ppo2.py:185][0m |          -0.0040 |          13.5270 |          21.0434 |
[32m[20221214 14:00:01 @agent_ppo2.py:185][0m |          -0.0011 |          12.9595 |          21.0394 |
[32m[20221214 14:00:01 @agent_ppo2.py:185][0m |          -0.0015 |          12.6717 |          21.0333 |
[32m[20221214 14:00:01 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:00:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.00
[32m[20221214 14:00:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 336.12
[32m[20221214 14:00:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.22
[32m[20221214 14:00:02 @agent_ppo2.py:143][0m Total time:       1.99 min
[32m[20221214 14:00:02 @agent_ppo2.py:145][0m 178176 total steps have happened
[32m[20221214 14:00:02 @agent_ppo2.py:121][0m #------------------------ Iteration 87 --------------------------#
[32m[20221214 14:00:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:02 @agent_ppo2.py:185][0m |           0.0014 |          28.0511 |          21.0902 |
[32m[20221214 14:00:02 @agent_ppo2.py:185][0m |           0.0005 |          23.3818 |          21.0855 |
[32m[20221214 14:00:02 @agent_ppo2.py:185][0m |           0.0014 |          21.5515 |          21.0787 |
[32m[20221214 14:00:02 @agent_ppo2.py:185][0m |          -0.0031 |          20.3218 |          21.0744 |
[32m[20221214 14:00:02 @agent_ppo2.py:185][0m |          -0.0029 |          19.3729 |          21.0694 |
[32m[20221214 14:00:02 @agent_ppo2.py:185][0m |           0.0166 |          22.1656 |          21.0664 |
[32m[20221214 14:00:03 @agent_ppo2.py:185][0m |          -0.0012 |          18.0958 |          21.0617 |
[32m[20221214 14:00:03 @agent_ppo2.py:185][0m |          -0.0023 |          17.4375 |          21.0581 |
[32m[20221214 14:00:03 @agent_ppo2.py:185][0m |          -0.0016 |          16.9796 |          21.0549 |
[32m[20221214 14:00:03 @agent_ppo2.py:185][0m |          -0.0018 |          16.5431 |          21.0533 |
[32m[20221214 14:00:03 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:00:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.30
[32m[20221214 14:00:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.20
[32m[20221214 14:00:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.34
[32m[20221214 14:00:03 @agent_ppo2.py:143][0m Total time:       2.02 min
[32m[20221214 14:00:03 @agent_ppo2.py:145][0m 180224 total steps have happened
[32m[20221214 14:00:03 @agent_ppo2.py:121][0m #------------------------ Iteration 88 --------------------------#
[32m[20221214 14:00:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:03 @agent_ppo2.py:185][0m |          -0.0021 |          24.5806 |          21.1759 |
[32m[20221214 14:00:04 @agent_ppo2.py:185][0m |          -0.0028 |          20.3523 |          21.1691 |
[32m[20221214 14:00:04 @agent_ppo2.py:185][0m |          -0.0045 |          18.3208 |          21.1610 |
[32m[20221214 14:00:04 @agent_ppo2.py:185][0m |          -0.0065 |          17.0070 |          21.1567 |
[32m[20221214 14:00:04 @agent_ppo2.py:185][0m |           0.0022 |          16.4641 |          21.1562 |
[32m[20221214 14:00:04 @agent_ppo2.py:185][0m |          -0.0021 |          15.2077 |          21.1538 |
[32m[20221214 14:00:04 @agent_ppo2.py:185][0m |           0.0020 |          15.1022 |          21.1584 |
[32m[20221214 14:00:04 @agent_ppo2.py:185][0m |          -0.0051 |          14.1014 |          21.1513 |
[32m[20221214 14:00:04 @agent_ppo2.py:185][0m |           0.0089 |          16.3342 |          21.1487 |
[32m[20221214 14:00:04 @agent_ppo2.py:185][0m |          -0.0045 |          13.2452 |          21.1516 |
[32m[20221214 14:00:04 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:00:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.85
[32m[20221214 14:00:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.22
[32m[20221214 14:00:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.97
[32m[20221214 14:00:04 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 425.97
[32m[20221214 14:00:04 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 425.97
[32m[20221214 14:00:04 @agent_ppo2.py:143][0m Total time:       2.04 min
[32m[20221214 14:00:04 @agent_ppo2.py:145][0m 182272 total steps have happened
[32m[20221214 14:00:04 @agent_ppo2.py:121][0m #------------------------ Iteration 89 --------------------------#
[32m[20221214 14:00:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:05 @agent_ppo2.py:185][0m |           0.0013 |          25.0396 |          21.1327 |
[32m[20221214 14:00:05 @agent_ppo2.py:185][0m |          -0.0001 |          20.3717 |          21.1312 |
[32m[20221214 14:00:05 @agent_ppo2.py:185][0m |          -0.0014 |          18.3884 |          21.1242 |
[32m[20221214 14:00:05 @agent_ppo2.py:185][0m |           0.0002 |          17.3316 |          21.1217 |
[32m[20221214 14:00:05 @agent_ppo2.py:185][0m |          -0.0071 |          16.4231 |          21.1194 |
[32m[20221214 14:00:05 @agent_ppo2.py:185][0m |          -0.0050 |          15.9562 |          21.1164 |
[32m[20221214 14:00:05 @agent_ppo2.py:185][0m |           0.0031 |          15.5676 |          21.1169 |
[32m[20221214 14:00:06 @agent_ppo2.py:185][0m |          -0.0017 |          15.0294 |          21.1165 |
[32m[20221214 14:00:06 @agent_ppo2.py:185][0m |           0.0021 |          14.9154 |          21.1141 |
[32m[20221214 14:00:06 @agent_ppo2.py:185][0m |          -0.0041 |          14.2073 |          21.1129 |
[32m[20221214 14:00:06 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:00:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.20
[32m[20221214 14:00:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 324.80
[32m[20221214 14:00:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.31
[32m[20221214 14:00:06 @agent_ppo2.py:143][0m Total time:       2.06 min
[32m[20221214 14:00:06 @agent_ppo2.py:145][0m 184320 total steps have happened
[32m[20221214 14:00:06 @agent_ppo2.py:121][0m #------------------------ Iteration 90 --------------------------#
[32m[20221214 14:00:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:00:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:06 @agent_ppo2.py:185][0m |          -0.0042 |          22.6783 |          21.1699 |
[32m[20221214 14:00:06 @agent_ppo2.py:185][0m |          -0.0024 |          18.7272 |          21.1645 |
[32m[20221214 14:00:06 @agent_ppo2.py:185][0m |          -0.0009 |          17.1449 |          21.1596 |
[32m[20221214 14:00:07 @agent_ppo2.py:185][0m |          -0.0036 |          16.1191 |          21.1536 |
[32m[20221214 14:00:07 @agent_ppo2.py:185][0m |          -0.0020 |          15.3054 |          21.1489 |
[32m[20221214 14:00:07 @agent_ppo2.py:185][0m |          -0.0025 |          14.6122 |          21.1456 |
[32m[20221214 14:00:07 @agent_ppo2.py:185][0m |           0.0100 |          16.1647 |          21.1441 |
[32m[20221214 14:00:07 @agent_ppo2.py:185][0m |          -0.0066 |          13.7451 |          21.1421 |
[32m[20221214 14:00:07 @agent_ppo2.py:185][0m |          -0.0037 |          13.3754 |          21.1396 |
[32m[20221214 14:00:07 @agent_ppo2.py:185][0m |          -0.0066 |          12.9967 |          21.1374 |
[32m[20221214 14:00:07 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:00:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.07
[32m[20221214 14:00:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 303.36
[32m[20221214 14:00:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 252.31
[32m[20221214 14:00:07 @agent_ppo2.py:143][0m Total time:       2.09 min
[32m[20221214 14:00:07 @agent_ppo2.py:145][0m 186368 total steps have happened
[32m[20221214 14:00:07 @agent_ppo2.py:121][0m #------------------------ Iteration 91 --------------------------#
[32m[20221214 14:00:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:08 @agent_ppo2.py:185][0m |           0.0002 |          26.5278 |          21.1470 |
[32m[20221214 14:00:08 @agent_ppo2.py:185][0m |           0.0042 |          21.2824 |          21.1496 |
[32m[20221214 14:00:08 @agent_ppo2.py:185][0m |          -0.0074 |          18.1181 |          21.1480 |
[32m[20221214 14:00:08 @agent_ppo2.py:185][0m |          -0.0029 |          16.7593 |          21.1474 |
[32m[20221214 14:00:08 @agent_ppo2.py:185][0m |          -0.0039 |          15.8864 |          21.1468 |
[32m[20221214 14:00:08 @agent_ppo2.py:185][0m |           0.0043 |          15.1961 |          21.1397 |
[32m[20221214 14:00:08 @agent_ppo2.py:185][0m |          -0.0002 |          14.5927 |          21.1435 |
[32m[20221214 14:00:08 @agent_ppo2.py:185][0m |           0.0001 |          14.2912 |          21.1439 |
[32m[20221214 14:00:09 @agent_ppo2.py:185][0m |           0.0039 |          14.1421 |          21.1430 |
[32m[20221214 14:00:09 @agent_ppo2.py:185][0m |           0.0015 |          13.3987 |          21.1391 |
[32m[20221214 14:00:09 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:00:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.92
[32m[20221214 14:00:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 330.70
[32m[20221214 14:00:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 186.60
[32m[20221214 14:00:09 @agent_ppo2.py:143][0m Total time:       2.11 min
[32m[20221214 14:00:09 @agent_ppo2.py:145][0m 188416 total steps have happened
[32m[20221214 14:00:09 @agent_ppo2.py:121][0m #------------------------ Iteration 92 --------------------------#
[32m[20221214 14:00:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:09 @agent_ppo2.py:185][0m |          -0.0023 |          25.0085 |          21.1558 |
[32m[20221214 14:00:09 @agent_ppo2.py:185][0m |          -0.0034 |          19.9465 |          21.1496 |
[32m[20221214 14:00:09 @agent_ppo2.py:185][0m |          -0.0046 |          17.4252 |          21.1483 |
[32m[20221214 14:00:09 @agent_ppo2.py:185][0m |          -0.0048 |          15.9003 |          21.1414 |
[32m[20221214 14:00:10 @agent_ppo2.py:185][0m |          -0.0051 |          14.6390 |          21.1362 |
[32m[20221214 14:00:10 @agent_ppo2.py:185][0m |          -0.0018 |          13.7912 |          21.1314 |
[32m[20221214 14:00:10 @agent_ppo2.py:185][0m |           0.0124 |          15.2409 |          21.1296 |
[32m[20221214 14:00:10 @agent_ppo2.py:185][0m |          -0.0046 |          12.5290 |          21.1298 |
[32m[20221214 14:00:10 @agent_ppo2.py:185][0m |          -0.0036 |          11.7821 |          21.1314 |
[32m[20221214 14:00:10 @agent_ppo2.py:185][0m |          -0.0050 |          11.3867 |          21.1286 |
[32m[20221214 14:00:10 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:00:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.16
[32m[20221214 14:00:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.25
[32m[20221214 14:00:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 218.05
[32m[20221214 14:00:10 @agent_ppo2.py:143][0m Total time:       2.14 min
[32m[20221214 14:00:10 @agent_ppo2.py:145][0m 190464 total steps have happened
[32m[20221214 14:00:10 @agent_ppo2.py:121][0m #------------------------ Iteration 93 --------------------------#
[32m[20221214 14:00:10 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:00:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:11 @agent_ppo2.py:185][0m |          -0.0046 |          17.7157 |          21.0793 |
[32m[20221214 14:00:11 @agent_ppo2.py:185][0m |           0.0002 |          14.3220 |          21.0679 |
[32m[20221214 14:00:11 @agent_ppo2.py:185][0m |          -0.0025 |          12.7767 |          21.0626 |
[32m[20221214 14:00:11 @agent_ppo2.py:185][0m |          -0.0035 |          11.7476 |          21.0555 |
[32m[20221214 14:00:11 @agent_ppo2.py:185][0m |          -0.0040 |          11.0263 |          21.0515 |
[32m[20221214 14:00:11 @agent_ppo2.py:185][0m |          -0.0026 |          10.4084 |          21.0490 |
[32m[20221214 14:00:11 @agent_ppo2.py:185][0m |          -0.0072 |           9.8987 |          21.0483 |
[32m[20221214 14:00:11 @agent_ppo2.py:185][0m |           0.0009 |           9.4927 |          21.0463 |
[32m[20221214 14:00:11 @agent_ppo2.py:185][0m |          -0.0063 |           9.1228 |          21.0437 |
[32m[20221214 14:00:12 @agent_ppo2.py:185][0m |          -0.0032 |           8.8256 |          21.0402 |
[32m[20221214 14:00:12 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 14:00:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.18
[32m[20221214 14:00:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.37
[32m[20221214 14:00:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 193.51
[32m[20221214 14:00:12 @agent_ppo2.py:143][0m Total time:       2.16 min
[32m[20221214 14:00:12 @agent_ppo2.py:145][0m 192512 total steps have happened
[32m[20221214 14:00:12 @agent_ppo2.py:121][0m #------------------------ Iteration 94 --------------------------#
[32m[20221214 14:00:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:12 @agent_ppo2.py:185][0m |          -0.0032 |          22.4353 |          21.0577 |
[32m[20221214 14:00:12 @agent_ppo2.py:185][0m |          -0.0022 |          18.6214 |          21.0502 |
[32m[20221214 14:00:12 @agent_ppo2.py:185][0m |           0.0005 |          16.8797 |          21.0378 |
[32m[20221214 14:00:12 @agent_ppo2.py:185][0m |          -0.0047 |          15.6974 |          21.0329 |
[32m[20221214 14:00:12 @agent_ppo2.py:185][0m |          -0.0028 |          15.0262 |          21.0298 |
[32m[20221214 14:00:13 @agent_ppo2.py:185][0m |          -0.0042 |          14.4250 |          21.0237 |
[32m[20221214 14:00:13 @agent_ppo2.py:185][0m |          -0.0067 |          14.0104 |          21.0206 |
[32m[20221214 14:00:13 @agent_ppo2.py:185][0m |          -0.0066 |          13.6662 |          21.0185 |
[32m[20221214 14:00:13 @agent_ppo2.py:185][0m |          -0.0120 |          13.4613 |          21.0132 |
[32m[20221214 14:00:13 @agent_ppo2.py:185][0m |          -0.0046 |          13.1729 |          21.0056 |
[32m[20221214 14:00:13 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:00:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.76
[32m[20221214 14:00:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.65
[32m[20221214 14:00:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.50
[32m[20221214 14:00:13 @agent_ppo2.py:143][0m Total time:       2.18 min
[32m[20221214 14:00:13 @agent_ppo2.py:145][0m 194560 total steps have happened
[32m[20221214 14:00:13 @agent_ppo2.py:121][0m #------------------------ Iteration 95 --------------------------#
[32m[20221214 14:00:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:00:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:13 @agent_ppo2.py:185][0m |           0.0037 |          21.1307 |          21.1015 |
[32m[20221214 14:00:14 @agent_ppo2.py:185][0m |           0.0028 |          17.3156 |          21.0952 |
[32m[20221214 14:00:14 @agent_ppo2.py:185][0m |          -0.0082 |          15.9883 |          21.0815 |
[32m[20221214 14:00:14 @agent_ppo2.py:185][0m |          -0.0015 |          15.1259 |          21.0813 |
[32m[20221214 14:00:14 @agent_ppo2.py:185][0m |          -0.0021 |          14.4777 |          21.0780 |
[32m[20221214 14:00:14 @agent_ppo2.py:185][0m |          -0.0051 |          14.0033 |          21.0801 |
[32m[20221214 14:00:14 @agent_ppo2.py:185][0m |          -0.0023 |          13.6762 |          21.0773 |
[32m[20221214 14:00:14 @agent_ppo2.py:185][0m |          -0.0062 |          13.3636 |          21.0785 |
[32m[20221214 14:00:14 @agent_ppo2.py:185][0m |          -0.0037 |          13.0550 |          21.0791 |
[32m[20221214 14:00:14 @agent_ppo2.py:185][0m |          -0.0005 |          12.8649 |          21.0786 |
[32m[20221214 14:00:14 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:00:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.98
[32m[20221214 14:00:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.96
[32m[20221214 14:00:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.11
[32m[20221214 14:00:15 @agent_ppo2.py:143][0m Total time:       2.21 min
[32m[20221214 14:00:15 @agent_ppo2.py:145][0m 196608 total steps have happened
[32m[20221214 14:00:15 @agent_ppo2.py:121][0m #------------------------ Iteration 96 --------------------------#
[32m[20221214 14:00:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:15 @agent_ppo2.py:185][0m |          -0.0044 |          21.9483 |          21.1461 |
[32m[20221214 14:00:15 @agent_ppo2.py:185][0m |           0.0019 |          18.9048 |          21.1406 |
[32m[20221214 14:00:15 @agent_ppo2.py:185][0m |           0.0172 |          20.3079 |          21.1390 |
[32m[20221214 14:00:15 @agent_ppo2.py:185][0m |          -0.0041 |          15.5872 |          21.1311 |
[32m[20221214 14:00:15 @agent_ppo2.py:185][0m |          -0.0032 |          14.4281 |          21.1337 |
[32m[20221214 14:00:15 @agent_ppo2.py:185][0m |          -0.0109 |          13.7744 |          21.1303 |
[32m[20221214 14:00:16 @agent_ppo2.py:185][0m |          -0.0060 |          13.2812 |          21.1287 |
[32m[20221214 14:00:16 @agent_ppo2.py:185][0m |          -0.0039 |          12.7757 |          21.1282 |
[32m[20221214 14:00:16 @agent_ppo2.py:185][0m |          -0.0025 |          12.2238 |          21.1257 |
[32m[20221214 14:00:16 @agent_ppo2.py:185][0m |          -0.0044 |          11.9266 |          21.1209 |
[32m[20221214 14:00:16 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:00:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.32
[32m[20221214 14:00:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.83
[32m[20221214 14:00:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.49
[32m[20221214 14:00:16 @agent_ppo2.py:143][0m Total time:       2.23 min
[32m[20221214 14:00:16 @agent_ppo2.py:145][0m 198656 total steps have happened
[32m[20221214 14:00:16 @agent_ppo2.py:121][0m #------------------------ Iteration 97 --------------------------#
[32m[20221214 14:00:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:16 @agent_ppo2.py:185][0m |          -0.0003 |          28.0324 |          21.1030 |
[32m[20221214 14:00:16 @agent_ppo2.py:185][0m |          -0.0018 |          22.1222 |          21.0949 |
[32m[20221214 14:00:17 @agent_ppo2.py:185][0m |          -0.0022 |          19.4778 |          21.0823 |
[32m[20221214 14:00:17 @agent_ppo2.py:185][0m |           0.0004 |          17.8545 |          21.0784 |
[32m[20221214 14:00:17 @agent_ppo2.py:185][0m |          -0.0010 |          16.9125 |          21.0763 |
[32m[20221214 14:00:17 @agent_ppo2.py:185][0m |          -0.0028 |          16.0667 |          21.0742 |
[32m[20221214 14:00:17 @agent_ppo2.py:185][0m |           0.0041 |          17.1561 |          21.0803 |
[32m[20221214 14:00:17 @agent_ppo2.py:185][0m |          -0.0071 |          15.0439 |          21.0802 |
[32m[20221214 14:00:17 @agent_ppo2.py:185][0m |          -0.0030 |          14.6318 |          21.0760 |
[32m[20221214 14:00:17 @agent_ppo2.py:185][0m |          -0.0055 |          14.2958 |          21.0815 |
[32m[20221214 14:00:17 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:00:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.95
[32m[20221214 14:00:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.88
[32m[20221214 14:00:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 232.20
[32m[20221214 14:00:17 @agent_ppo2.py:143][0m Total time:       2.26 min
[32m[20221214 14:00:17 @agent_ppo2.py:145][0m 200704 total steps have happened
[32m[20221214 14:00:17 @agent_ppo2.py:121][0m #------------------------ Iteration 98 --------------------------#
[32m[20221214 14:00:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:18 @agent_ppo2.py:185][0m |          -0.0021 |          23.2592 |          21.1006 |
[32m[20221214 14:00:18 @agent_ppo2.py:185][0m |          -0.0037 |          20.1717 |          21.0990 |
[32m[20221214 14:00:18 @agent_ppo2.py:185][0m |          -0.0059 |          18.8483 |          21.0993 |
[32m[20221214 14:00:18 @agent_ppo2.py:185][0m |           0.0038 |          18.2807 |          21.0967 |
[32m[20221214 14:00:18 @agent_ppo2.py:185][0m |          -0.0048 |          17.3332 |          21.0926 |
[32m[20221214 14:00:18 @agent_ppo2.py:185][0m |          -0.0017 |          16.8395 |          21.0916 |
[32m[20221214 14:00:18 @agent_ppo2.py:185][0m |          -0.0038 |          16.3450 |          21.0887 |
[32m[20221214 14:00:18 @agent_ppo2.py:185][0m |           0.0020 |          15.8934 |          21.0919 |
[32m[20221214 14:00:19 @agent_ppo2.py:185][0m |          -0.0032 |          15.5514 |          21.0884 |
[32m[20221214 14:00:19 @agent_ppo2.py:185][0m |          -0.0080 |          15.1300 |          21.0890 |
[32m[20221214 14:00:19 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:00:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.19
[32m[20221214 14:00:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.80
[32m[20221214 14:00:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 231.17
[32m[20221214 14:00:19 @agent_ppo2.py:143][0m Total time:       2.28 min
[32m[20221214 14:00:19 @agent_ppo2.py:145][0m 202752 total steps have happened
[32m[20221214 14:00:19 @agent_ppo2.py:121][0m #------------------------ Iteration 99 --------------------------#
[32m[20221214 14:00:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:19 @agent_ppo2.py:185][0m |          -0.0044 |          24.5565 |          21.1120 |
[32m[20221214 14:00:19 @agent_ppo2.py:185][0m |          -0.0025 |          20.4202 |          21.1095 |
[32m[20221214 14:00:19 @agent_ppo2.py:185][0m |          -0.0064 |          18.6914 |          21.1052 |
[32m[20221214 14:00:19 @agent_ppo2.py:185][0m |          -0.0070 |          17.8036 |          21.1041 |
[32m[20221214 14:00:19 @agent_ppo2.py:185][0m |          -0.0056 |          16.9521 |          21.1037 |
[32m[20221214 14:00:20 @agent_ppo2.py:185][0m |          -0.0042 |          16.3310 |          21.1031 |
[32m[20221214 14:00:20 @agent_ppo2.py:185][0m |          -0.0041 |          15.8856 |          21.1032 |
[32m[20221214 14:00:20 @agent_ppo2.py:185][0m |          -0.0070 |          15.4685 |          21.1043 |
[32m[20221214 14:00:20 @agent_ppo2.py:185][0m |          -0.0066 |          15.1985 |          21.1073 |
[32m[20221214 14:00:20 @agent_ppo2.py:185][0m |           0.0103 |          15.9599 |          21.1057 |
[32m[20221214 14:00:20 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:00:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.72
[32m[20221214 14:00:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 330.35
[32m[20221214 14:00:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.38
[32m[20221214 14:00:20 @agent_ppo2.py:143][0m Total time:       2.30 min
[32m[20221214 14:00:20 @agent_ppo2.py:145][0m 204800 total steps have happened
[32m[20221214 14:00:20 @agent_ppo2.py:121][0m #------------------------ Iteration 100 --------------------------#
[32m[20221214 14:00:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:00:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:20 @agent_ppo2.py:185][0m |          -0.0017 |          22.0722 |          21.1304 |
[32m[20221214 14:00:21 @agent_ppo2.py:185][0m |          -0.0040 |          17.6370 |          21.1235 |
[32m[20221214 14:00:21 @agent_ppo2.py:185][0m |          -0.0012 |          16.1491 |          21.1201 |
[32m[20221214 14:00:21 @agent_ppo2.py:185][0m |           0.0023 |          14.8268 |          21.1186 |
[32m[20221214 14:00:21 @agent_ppo2.py:185][0m |           0.0084 |          14.2584 |          21.1205 |
[32m[20221214 14:00:21 @agent_ppo2.py:185][0m |          -0.0058 |          13.3027 |          21.1156 |
[32m[20221214 14:00:21 @agent_ppo2.py:185][0m |          -0.0060 |          12.7895 |          21.1169 |
[32m[20221214 14:00:21 @agent_ppo2.py:185][0m |          -0.0063 |          12.4743 |          21.1248 |
[32m[20221214 14:00:21 @agent_ppo2.py:185][0m |          -0.0039 |          12.1787 |          21.1239 |
[32m[20221214 14:00:21 @agent_ppo2.py:185][0m |          -0.0058 |          11.9106 |          21.1184 |
[32m[20221214 14:00:21 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:00:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.22
[32m[20221214 14:00:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.15
[32m[20221214 14:00:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.61
[32m[20221214 14:00:21 @agent_ppo2.py:143][0m Total time:       2.32 min
[32m[20221214 14:00:21 @agent_ppo2.py:145][0m 206848 total steps have happened
[32m[20221214 14:00:21 @agent_ppo2.py:121][0m #------------------------ Iteration 101 --------------------------#
[32m[20221214 14:00:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:22 @agent_ppo2.py:185][0m |          -0.0053 |          13.0696 |          21.0206 |
[32m[20221214 14:00:22 @agent_ppo2.py:185][0m |          -0.0073 |          10.4656 |          21.0093 |
[32m[20221214 14:00:22 @agent_ppo2.py:185][0m |          -0.0029 |           9.5328 |          21.0013 |
[32m[20221214 14:00:22 @agent_ppo2.py:185][0m |           0.0068 |           9.4629 |          20.9914 |
[32m[20221214 14:00:22 @agent_ppo2.py:185][0m |          -0.0052 |           8.5382 |          20.9864 |
[32m[20221214 14:00:22 @agent_ppo2.py:185][0m |          -0.0042 |           8.1483 |          20.9803 |
[32m[20221214 14:00:22 @agent_ppo2.py:185][0m |          -0.0053 |           7.8861 |          20.9747 |
[32m[20221214 14:00:22 @agent_ppo2.py:185][0m |           0.0045 |           7.6823 |          20.9666 |
[32m[20221214 14:00:23 @agent_ppo2.py:185][0m |           0.0016 |           7.5281 |          20.9595 |
[32m[20221214 14:00:23 @agent_ppo2.py:185][0m |          -0.0070 |           7.2886 |          20.9594 |
[32m[20221214 14:00:23 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:00:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.04
[32m[20221214 14:00:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.73
[32m[20221214 14:00:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.66
[32m[20221214 14:00:23 @agent_ppo2.py:143][0m Total time:       2.35 min
[32m[20221214 14:00:23 @agent_ppo2.py:145][0m 208896 total steps have happened
[32m[20221214 14:00:23 @agent_ppo2.py:121][0m #------------------------ Iteration 102 --------------------------#
[32m[20221214 14:00:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:23 @agent_ppo2.py:185][0m |          -0.0029 |          16.9136 |          21.0486 |
[32m[20221214 14:00:23 @agent_ppo2.py:185][0m |          -0.0012 |          14.0516 |          21.0467 |
[32m[20221214 14:00:23 @agent_ppo2.py:185][0m |          -0.0039 |          12.9003 |          21.0388 |
[32m[20221214 14:00:23 @agent_ppo2.py:185][0m |           0.0006 |          12.1952 |          21.0336 |
[32m[20221214 14:00:24 @agent_ppo2.py:185][0m |          -0.0024 |          11.6236 |          21.0291 |
[32m[20221214 14:00:24 @agent_ppo2.py:185][0m |          -0.0056 |          11.2580 |          21.0284 |
[32m[20221214 14:00:24 @agent_ppo2.py:185][0m |           0.0015 |          10.8641 |          21.0243 |
[32m[20221214 14:00:24 @agent_ppo2.py:185][0m |          -0.0035 |          10.6523 |          21.0236 |
[32m[20221214 14:00:24 @agent_ppo2.py:185][0m |          -0.0047 |          10.4127 |          21.0215 |
[32m[20221214 14:00:24 @agent_ppo2.py:185][0m |          -0.0013 |          10.0983 |          21.0192 |
[32m[20221214 14:00:24 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:00:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.89
[32m[20221214 14:00:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.11
[32m[20221214 14:00:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.95
[32m[20221214 14:00:24 @agent_ppo2.py:143][0m Total time:       2.37 min
[32m[20221214 14:00:24 @agent_ppo2.py:145][0m 210944 total steps have happened
[32m[20221214 14:00:24 @agent_ppo2.py:121][0m #------------------------ Iteration 103 --------------------------#
[32m[20221214 14:00:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:25 @agent_ppo2.py:185][0m |           0.0022 |          15.9023 |          21.1267 |
[32m[20221214 14:00:25 @agent_ppo2.py:185][0m |          -0.0060 |          11.6319 |          21.1269 |
[32m[20221214 14:00:25 @agent_ppo2.py:185][0m |           0.0034 |          10.1881 |          21.1270 |
[32m[20221214 14:00:25 @agent_ppo2.py:185][0m |          -0.0035 |           9.3295 |          21.1271 |
[32m[20221214 14:00:25 @agent_ppo2.py:185][0m |          -0.0027 |           8.7004 |          21.1236 |
[32m[20221214 14:00:25 @agent_ppo2.py:185][0m |          -0.0022 |           8.1957 |          21.1193 |
[32m[20221214 14:00:25 @agent_ppo2.py:185][0m |          -0.0009 |           7.8532 |          21.1219 |
[32m[20221214 14:00:25 @agent_ppo2.py:185][0m |           0.0003 |           7.5013 |          21.1217 |
[32m[20221214 14:00:25 @agent_ppo2.py:185][0m |          -0.0055 |           7.0626 |          21.1214 |
[32m[20221214 14:00:25 @agent_ppo2.py:185][0m |          -0.0033 |           6.8276 |          21.1197 |
[32m[20221214 14:00:25 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:00:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.92
[32m[20221214 14:00:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 303.25
[32m[20221214 14:00:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.14
[32m[20221214 14:00:26 @agent_ppo2.py:143][0m Total time:       2.39 min
[32m[20221214 14:00:26 @agent_ppo2.py:145][0m 212992 total steps have happened
[32m[20221214 14:00:26 @agent_ppo2.py:121][0m #------------------------ Iteration 104 --------------------------#
[32m[20221214 14:00:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:26 @agent_ppo2.py:185][0m |          -0.0009 |          19.2323 |          21.0822 |
[32m[20221214 14:00:26 @agent_ppo2.py:185][0m |           0.0038 |          16.6045 |          21.0755 |
[32m[20221214 14:00:26 @agent_ppo2.py:185][0m |          -0.0037 |          14.4568 |          21.0675 |
[32m[20221214 14:00:26 @agent_ppo2.py:185][0m |          -0.0041 |          13.5293 |          21.0625 |
[32m[20221214 14:00:26 @agent_ppo2.py:185][0m |          -0.0067 |          12.8128 |          21.0600 |
[32m[20221214 14:00:26 @agent_ppo2.py:185][0m |          -0.0035 |          12.4987 |          21.0615 |
[32m[20221214 14:00:26 @agent_ppo2.py:185][0m |           0.0056 |          12.3037 |          21.0543 |
[32m[20221214 14:00:27 @agent_ppo2.py:185][0m |          -0.0082 |          11.6110 |          21.0545 |
[32m[20221214 14:00:27 @agent_ppo2.py:185][0m |          -0.0075 |          11.3292 |          21.0518 |
[32m[20221214 14:00:27 @agent_ppo2.py:185][0m |          -0.0034 |          11.2161 |          21.0469 |
[32m[20221214 14:00:27 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:00:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.50
[32m[20221214 14:00:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.91
[32m[20221214 14:00:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 235.83
[32m[20221214 14:00:27 @agent_ppo2.py:143][0m Total time:       2.41 min
[32m[20221214 14:00:27 @agent_ppo2.py:145][0m 215040 total steps have happened
[32m[20221214 14:00:27 @agent_ppo2.py:121][0m #------------------------ Iteration 105 --------------------------#
[32m[20221214 14:00:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:27 @agent_ppo2.py:185][0m |           0.0009 |          22.3334 |          21.0649 |
[32m[20221214 14:00:27 @agent_ppo2.py:185][0m |          -0.0036 |          17.8641 |          21.0624 |
[32m[20221214 14:00:27 @agent_ppo2.py:185][0m |          -0.0057 |          16.2128 |          21.0542 |
[32m[20221214 14:00:28 @agent_ppo2.py:185][0m |          -0.0006 |          15.4096 |          21.0463 |
[32m[20221214 14:00:28 @agent_ppo2.py:185][0m |           0.0001 |          14.6860 |          21.0465 |
[32m[20221214 14:00:28 @agent_ppo2.py:185][0m |          -0.0052 |          14.2044 |          21.0377 |
[32m[20221214 14:00:28 @agent_ppo2.py:185][0m |          -0.0030 |          13.8127 |          21.0340 |
[32m[20221214 14:00:28 @agent_ppo2.py:185][0m |          -0.0038 |          13.5306 |          21.0275 |
[32m[20221214 14:00:28 @agent_ppo2.py:185][0m |          -0.0039 |          13.1769 |          21.0224 |
[32m[20221214 14:00:28 @agent_ppo2.py:185][0m |           0.0069 |          13.4034 |          21.0195 |
[32m[20221214 14:00:28 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:00:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.97
[32m[20221214 14:00:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 335.20
[32m[20221214 14:00:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 235.02
[32m[20221214 14:00:28 @agent_ppo2.py:143][0m Total time:       2.44 min
[32m[20221214 14:00:28 @agent_ppo2.py:145][0m 217088 total steps have happened
[32m[20221214 14:00:28 @agent_ppo2.py:121][0m #------------------------ Iteration 106 --------------------------#
[32m[20221214 14:00:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:29 @agent_ppo2.py:185][0m |          -0.0007 |          27.7531 |          21.1585 |
[32m[20221214 14:00:29 @agent_ppo2.py:185][0m |          -0.0013 |          24.0743 |          21.1573 |
[32m[20221214 14:00:29 @agent_ppo2.py:185][0m |          -0.0023 |          22.5906 |          21.1502 |
[32m[20221214 14:00:29 @agent_ppo2.py:185][0m |          -0.0048 |          21.4467 |          21.1487 |
[32m[20221214 14:00:29 @agent_ppo2.py:185][0m |           0.0057 |          21.9081 |          21.1491 |
[32m[20221214 14:00:29 @agent_ppo2.py:185][0m |           0.0076 |          20.3531 |          21.1501 |
[32m[20221214 14:00:29 @agent_ppo2.py:185][0m |          -0.0017 |          19.5064 |          21.1540 |
[32m[20221214 14:00:29 @agent_ppo2.py:185][0m |           0.0065 |          20.1035 |          21.1527 |
[32m[20221214 14:00:29 @agent_ppo2.py:185][0m |          -0.0046 |          18.6115 |          21.1490 |
[32m[20221214 14:00:29 @agent_ppo2.py:185][0m |          -0.0055 |          18.2936 |          21.1520 |
[32m[20221214 14:00:29 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:00:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.31
[32m[20221214 14:00:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.84
[32m[20221214 14:00:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.51
[32m[20221214 14:00:30 @agent_ppo2.py:143][0m Total time:       2.46 min
[32m[20221214 14:00:30 @agent_ppo2.py:145][0m 219136 total steps have happened
[32m[20221214 14:00:30 @agent_ppo2.py:121][0m #------------------------ Iteration 107 --------------------------#
[32m[20221214 14:00:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:30 @agent_ppo2.py:185][0m |          -0.0005 |          24.6393 |          21.1151 |
[32m[20221214 14:00:30 @agent_ppo2.py:185][0m |           0.0010 |          21.2523 |          21.0999 |
[32m[20221214 14:00:30 @agent_ppo2.py:185][0m |          -0.0042 |          18.7516 |          21.0857 |
[32m[20221214 14:00:30 @agent_ppo2.py:185][0m |          -0.0039 |          17.7277 |          21.0817 |
[32m[20221214 14:00:30 @agent_ppo2.py:185][0m |          -0.0071 |          16.9344 |          21.0805 |
[32m[20221214 14:00:30 @agent_ppo2.py:185][0m |          -0.0084 |          16.3973 |          21.0796 |
[32m[20221214 14:00:31 @agent_ppo2.py:185][0m |          -0.0066 |          15.8400 |          21.0774 |
[32m[20221214 14:00:31 @agent_ppo2.py:185][0m |          -0.0028 |          15.3818 |          21.0798 |
[32m[20221214 14:00:31 @agent_ppo2.py:185][0m |          -0.0005 |          15.0870 |          21.0677 |
[32m[20221214 14:00:31 @agent_ppo2.py:185][0m |          -0.0026 |          14.6883 |          21.0695 |
[32m[20221214 14:00:31 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:00:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.84
[32m[20221214 14:00:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 357.88
[32m[20221214 14:00:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.43
[32m[20221214 14:00:31 @agent_ppo2.py:143][0m Total time:       2.48 min
[32m[20221214 14:00:31 @agent_ppo2.py:145][0m 221184 total steps have happened
[32m[20221214 14:00:31 @agent_ppo2.py:121][0m #------------------------ Iteration 108 --------------------------#
[32m[20221214 14:00:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:31 @agent_ppo2.py:185][0m |           0.0007 |          23.9083 |          21.1666 |
[32m[20221214 14:00:31 @agent_ppo2.py:185][0m |          -0.0002 |          21.0021 |          21.1539 |
[32m[20221214 14:00:32 @agent_ppo2.py:185][0m |          -0.0050 |          19.3517 |          21.1442 |
[32m[20221214 14:00:32 @agent_ppo2.py:185][0m |          -0.0037 |          18.3669 |          21.1411 |
[32m[20221214 14:00:32 @agent_ppo2.py:185][0m |          -0.0016 |          17.5828 |          21.1396 |
[32m[20221214 14:00:32 @agent_ppo2.py:185][0m |          -0.0031 |          17.2641 |          21.1384 |
[32m[20221214 14:00:32 @agent_ppo2.py:185][0m |          -0.0046 |          16.5239 |          21.1369 |
[32m[20221214 14:00:32 @agent_ppo2.py:185][0m |          -0.0042 |          16.1411 |          21.1364 |
[32m[20221214 14:00:32 @agent_ppo2.py:185][0m |          -0.0055 |          15.7729 |          21.1336 |
[32m[20221214 14:00:32 @agent_ppo2.py:185][0m |          -0.0040 |          15.4925 |          21.1320 |
[32m[20221214 14:00:32 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:00:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.26
[32m[20221214 14:00:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.32
[32m[20221214 14:00:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.61
[32m[20221214 14:00:32 @agent_ppo2.py:143][0m Total time:       2.50 min
[32m[20221214 14:00:32 @agent_ppo2.py:145][0m 223232 total steps have happened
[32m[20221214 14:00:32 @agent_ppo2.py:121][0m #------------------------ Iteration 109 --------------------------#
[32m[20221214 14:00:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:33 @agent_ppo2.py:185][0m |          -0.0001 |          23.8881 |          21.0450 |
[32m[20221214 14:00:33 @agent_ppo2.py:185][0m |          -0.0035 |          19.5979 |          21.0354 |
[32m[20221214 14:00:33 @agent_ppo2.py:185][0m |           0.0027 |          17.7698 |          21.0306 |
[32m[20221214 14:00:33 @agent_ppo2.py:185][0m |          -0.0026 |          16.6168 |          21.0250 |
[32m[20221214 14:00:33 @agent_ppo2.py:185][0m |          -0.0051 |          15.6264 |          21.0300 |
[32m[20221214 14:00:33 @agent_ppo2.py:185][0m |          -0.0085 |          15.0276 |          21.0299 |
[32m[20221214 14:00:33 @agent_ppo2.py:185][0m |           0.0014 |          14.4143 |          21.0279 |
[32m[20221214 14:00:33 @agent_ppo2.py:185][0m |          -0.0044 |          14.0088 |          21.0249 |
[32m[20221214 14:00:33 @agent_ppo2.py:185][0m |          -0.0059 |          13.4900 |          21.0256 |
[32m[20221214 14:00:34 @agent_ppo2.py:185][0m |          -0.0062 |          13.1555 |          21.0237 |
[32m[20221214 14:00:34 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:00:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.02
[32m[20221214 14:00:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 333.88
[32m[20221214 14:00:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.69
[32m[20221214 14:00:34 @agent_ppo2.py:143][0m Total time:       2.53 min
[32m[20221214 14:00:34 @agent_ppo2.py:145][0m 225280 total steps have happened
[32m[20221214 14:00:34 @agent_ppo2.py:121][0m #------------------------ Iteration 110 --------------------------#
[32m[20221214 14:00:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:00:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:34 @agent_ppo2.py:185][0m |          -0.0001 |          33.0847 |          21.1833 |
[32m[20221214 14:00:34 @agent_ppo2.py:185][0m |           0.0007 |          26.7424 |          21.1784 |
[32m[20221214 14:00:34 @agent_ppo2.py:185][0m |          -0.0020 |          23.5795 |          21.1709 |
[32m[20221214 14:00:34 @agent_ppo2.py:185][0m |           0.0026 |          21.9987 |          21.1613 |
[32m[20221214 14:00:34 @agent_ppo2.py:185][0m |           0.0002 |          20.3001 |          21.1573 |
[32m[20221214 14:00:35 @agent_ppo2.py:185][0m |          -0.0021 |          19.2555 |          21.1502 |
[32m[20221214 14:00:35 @agent_ppo2.py:185][0m |          -0.0046 |          18.3468 |          21.1439 |
[32m[20221214 14:00:35 @agent_ppo2.py:185][0m |           0.0118 |          20.2842 |          21.1365 |
[32m[20221214 14:00:35 @agent_ppo2.py:185][0m |          -0.0015 |          16.8101 |          21.1349 |
[32m[20221214 14:00:35 @agent_ppo2.py:185][0m |          -0.0028 |          16.2072 |          21.1303 |
[32m[20221214 14:00:35 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:00:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.66
[32m[20221214 14:00:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 310.39
[32m[20221214 14:00:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 216.68
[32m[20221214 14:00:35 @agent_ppo2.py:143][0m Total time:       2.55 min
[32m[20221214 14:00:35 @agent_ppo2.py:145][0m 227328 total steps have happened
[32m[20221214 14:00:35 @agent_ppo2.py:121][0m #------------------------ Iteration 111 --------------------------#
[32m[20221214 14:00:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:35 @agent_ppo2.py:185][0m |          -0.0046 |          24.3679 |          21.0943 |
[32m[20221214 14:00:36 @agent_ppo2.py:185][0m |          -0.0017 |          18.5999 |          21.0917 |
[32m[20221214 14:00:36 @agent_ppo2.py:185][0m |          -0.0036 |          16.2942 |          21.0860 |
[32m[20221214 14:00:36 @agent_ppo2.py:185][0m |          -0.0012 |          15.0092 |          21.0800 |
[32m[20221214 14:00:36 @agent_ppo2.py:185][0m |           0.0047 |          15.3635 |          21.0760 |
[32m[20221214 14:00:36 @agent_ppo2.py:185][0m |           0.0015 |          13.3398 |          21.0675 |
[32m[20221214 14:00:36 @agent_ppo2.py:185][0m |          -0.0011 |          12.7219 |          21.0632 |
[32m[20221214 14:00:36 @agent_ppo2.py:185][0m |          -0.0035 |          12.2123 |          21.0598 |
[32m[20221214 14:00:36 @agent_ppo2.py:185][0m |           0.0004 |          11.7630 |          21.0576 |
[32m[20221214 14:00:36 @agent_ppo2.py:185][0m |          -0.0025 |          11.4115 |          21.0530 |
[32m[20221214 14:00:36 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:00:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.58
[32m[20221214 14:00:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 325.81
[32m[20221214 14:00:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.41
[32m[20221214 14:00:36 @agent_ppo2.py:143][0m Total time:       2.57 min
[32m[20221214 14:00:36 @agent_ppo2.py:145][0m 229376 total steps have happened
[32m[20221214 14:00:36 @agent_ppo2.py:121][0m #------------------------ Iteration 112 --------------------------#
[32m[20221214 14:00:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:37 @agent_ppo2.py:185][0m |           0.0035 |          20.1926 |          21.1002 |
[32m[20221214 14:00:37 @agent_ppo2.py:185][0m |          -0.0011 |          14.5173 |          21.0968 |
[32m[20221214 14:00:37 @agent_ppo2.py:185][0m |          -0.0010 |          12.6906 |          21.0900 |
[32m[20221214 14:00:37 @agent_ppo2.py:185][0m |          -0.0040 |          11.5033 |          21.0892 |
[32m[20221214 14:00:37 @agent_ppo2.py:185][0m |           0.0045 |          10.8060 |          21.0825 |
[32m[20221214 14:00:37 @agent_ppo2.py:185][0m |          -0.0014 |           9.9783 |          21.0818 |
[32m[20221214 14:00:37 @agent_ppo2.py:185][0m |          -0.0054 |           9.4650 |          21.0786 |
[32m[20221214 14:00:37 @agent_ppo2.py:185][0m |          -0.0008 |           9.0922 |          21.0720 |
[32m[20221214 14:00:38 @agent_ppo2.py:185][0m |          -0.0092 |           8.9782 |          21.0723 |
[32m[20221214 14:00:38 @agent_ppo2.py:185][0m |           0.0031 |           8.7083 |          21.0735 |
[32m[20221214 14:00:38 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:00:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.41
[32m[20221214 14:00:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 312.77
[32m[20221214 14:00:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 204.33
[32m[20221214 14:00:38 @agent_ppo2.py:143][0m Total time:       2.60 min
[32m[20221214 14:00:38 @agent_ppo2.py:145][0m 231424 total steps have happened
[32m[20221214 14:00:38 @agent_ppo2.py:121][0m #------------------------ Iteration 113 --------------------------#
[32m[20221214 14:00:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:38 @agent_ppo2.py:185][0m |          -0.0019 |          19.2142 |          21.1052 |
[32m[20221214 14:00:38 @agent_ppo2.py:185][0m |          -0.0049 |          14.9516 |          21.1083 |
[32m[20221214 14:00:38 @agent_ppo2.py:185][0m |          -0.0015 |          13.2749 |          21.1063 |
[32m[20221214 14:00:38 @agent_ppo2.py:185][0m |          -0.0034 |          12.3289 |          21.0982 |
[32m[20221214 14:00:39 @agent_ppo2.py:185][0m |          -0.0067 |          11.6800 |          21.1005 |
[32m[20221214 14:00:39 @agent_ppo2.py:185][0m |          -0.0027 |          11.0891 |          21.1010 |
[32m[20221214 14:00:39 @agent_ppo2.py:185][0m |           0.0027 |          10.6094 |          21.1011 |
[32m[20221214 14:00:39 @agent_ppo2.py:185][0m |          -0.0009 |          10.2170 |          21.0985 |
[32m[20221214 14:00:39 @agent_ppo2.py:185][0m |          -0.0032 |           9.9349 |          21.1038 |
[32m[20221214 14:00:39 @agent_ppo2.py:185][0m |          -0.0012 |           9.6396 |          21.1000 |
[32m[20221214 14:00:39 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:00:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.55
[32m[20221214 14:00:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.67
[32m[20221214 14:00:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.34
[32m[20221214 14:00:39 @agent_ppo2.py:143][0m Total time:       2.62 min
[32m[20221214 14:00:39 @agent_ppo2.py:145][0m 233472 total steps have happened
[32m[20221214 14:00:39 @agent_ppo2.py:121][0m #------------------------ Iteration 114 --------------------------#
[32m[20221214 14:00:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:40 @agent_ppo2.py:185][0m |          -0.0006 |          21.9020 |          21.0839 |
[32m[20221214 14:00:40 @agent_ppo2.py:185][0m |          -0.0044 |          15.5645 |          21.0798 |
[32m[20221214 14:00:40 @agent_ppo2.py:185][0m |          -0.0027 |          13.5863 |          21.0790 |
[32m[20221214 14:00:40 @agent_ppo2.py:185][0m |           0.0019 |          12.4700 |          21.0797 |
[32m[20221214 14:00:40 @agent_ppo2.py:185][0m |          -0.0001 |          11.5586 |          21.0825 |
[32m[20221214 14:00:40 @agent_ppo2.py:185][0m |          -0.0040 |          11.0202 |          21.0823 |
[32m[20221214 14:00:40 @agent_ppo2.py:185][0m |           0.0021 |          11.0667 |          21.0845 |
[32m[20221214 14:00:40 @agent_ppo2.py:185][0m |          -0.0067 |           9.9866 |          21.0844 |
[32m[20221214 14:00:40 @agent_ppo2.py:185][0m |          -0.0015 |           9.6337 |          21.0855 |
[32m[20221214 14:00:40 @agent_ppo2.py:185][0m |          -0.0011 |           9.3100 |          21.0912 |
[32m[20221214 14:00:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:00:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.10
[32m[20221214 14:00:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.55
[32m[20221214 14:00:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 214.24
[32m[20221214 14:00:41 @agent_ppo2.py:143][0m Total time:       2.64 min
[32m[20221214 14:00:41 @agent_ppo2.py:145][0m 235520 total steps have happened
[32m[20221214 14:00:41 @agent_ppo2.py:121][0m #------------------------ Iteration 115 --------------------------#
[32m[20221214 14:00:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:41 @agent_ppo2.py:185][0m |          -0.0021 |          23.2142 |          21.0986 |
[32m[20221214 14:00:41 @agent_ppo2.py:185][0m |          -0.0022 |          18.6336 |          21.1007 |
[32m[20221214 14:00:41 @agent_ppo2.py:185][0m |           0.0000 |          16.7497 |          21.1004 |
[32m[20221214 14:00:41 @agent_ppo2.py:185][0m |           0.0015 |          15.2008 |          21.0989 |
[32m[20221214 14:00:41 @agent_ppo2.py:185][0m |          -0.0024 |          14.2253 |          21.0963 |
[32m[20221214 14:00:41 @agent_ppo2.py:185][0m |          -0.0032 |          13.4158 |          21.0962 |
[32m[20221214 14:00:41 @agent_ppo2.py:185][0m |           0.0113 |          13.1177 |          21.0948 |
[32m[20221214 14:00:42 @agent_ppo2.py:185][0m |          -0.0054 |          12.2795 |          21.0976 |
[32m[20221214 14:00:42 @agent_ppo2.py:185][0m |          -0.0023 |          11.8675 |          21.0974 |
[32m[20221214 14:00:42 @agent_ppo2.py:185][0m |           0.0001 |          11.4521 |          21.0952 |
[32m[20221214 14:00:42 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:00:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 223.46
[32m[20221214 14:00:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.31
[32m[20221214 14:00:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 200.50
[32m[20221214 14:00:42 @agent_ppo2.py:143][0m Total time:       2.66 min
[32m[20221214 14:00:42 @agent_ppo2.py:145][0m 237568 total steps have happened
[32m[20221214 14:00:42 @agent_ppo2.py:121][0m #------------------------ Iteration 116 --------------------------#
[32m[20221214 14:00:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:42 @agent_ppo2.py:185][0m |          -0.0022 |          24.8422 |          21.0695 |
[32m[20221214 14:00:42 @agent_ppo2.py:185][0m |           0.0070 |          20.4328 |          21.0568 |
[32m[20221214 14:00:42 @agent_ppo2.py:185][0m |          -0.0056 |          16.6677 |          21.0425 |
[32m[20221214 14:00:43 @agent_ppo2.py:185][0m |           0.0059 |          16.2025 |          21.0500 |
[32m[20221214 14:00:43 @agent_ppo2.py:185][0m |           0.0001 |          14.5259 |          21.0423 |
[32m[20221214 14:00:43 @agent_ppo2.py:185][0m |          -0.0037 |          13.6695 |          21.0450 |
[32m[20221214 14:00:43 @agent_ppo2.py:185][0m |           0.0071 |          14.4901 |          21.0380 |
[32m[20221214 14:00:43 @agent_ppo2.py:185][0m |          -0.0041 |          12.6229 |          21.0408 |
[32m[20221214 14:00:43 @agent_ppo2.py:185][0m |          -0.0059 |          12.1190 |          21.0410 |
[32m[20221214 14:00:43 @agent_ppo2.py:185][0m |          -0.0076 |          11.8164 |          21.0403 |
[32m[20221214 14:00:43 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:00:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.69
[32m[20221214 14:00:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.64
[32m[20221214 14:00:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.72
[32m[20221214 14:00:43 @agent_ppo2.py:143][0m Total time:       2.69 min
[32m[20221214 14:00:43 @agent_ppo2.py:145][0m 239616 total steps have happened
[32m[20221214 14:00:43 @agent_ppo2.py:121][0m #------------------------ Iteration 117 --------------------------#
[32m[20221214 14:00:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:44 @agent_ppo2.py:185][0m |           0.0009 |          22.6343 |          21.1326 |
[32m[20221214 14:00:44 @agent_ppo2.py:185][0m |          -0.0009 |          17.2699 |          21.1363 |
[32m[20221214 14:00:44 @agent_ppo2.py:185][0m |           0.0012 |          15.3935 |          21.1362 |
[32m[20221214 14:00:44 @agent_ppo2.py:185][0m |           0.0000 |          14.0766 |          21.1407 |
[32m[20221214 14:00:44 @agent_ppo2.py:185][0m |           0.0049 |          13.2606 |          21.1452 |
[32m[20221214 14:00:44 @agent_ppo2.py:185][0m |          -0.0065 |          12.5269 |          21.1468 |
[32m[20221214 14:00:44 @agent_ppo2.py:185][0m |          -0.0023 |          12.1308 |          21.1490 |
[32m[20221214 14:00:44 @agent_ppo2.py:185][0m |           0.0007 |          11.5408 |          21.1512 |
[32m[20221214 14:00:44 @agent_ppo2.py:185][0m |          -0.0028 |          11.1081 |          21.1543 |
[32m[20221214 14:00:45 @agent_ppo2.py:185][0m |          -0.0062 |          10.8550 |          21.1572 |
[32m[20221214 14:00:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:00:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.32
[32m[20221214 14:00:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 332.06
[32m[20221214 14:00:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.84
[32m[20221214 14:00:45 @agent_ppo2.py:143][0m Total time:       2.71 min
[32m[20221214 14:00:45 @agent_ppo2.py:145][0m 241664 total steps have happened
[32m[20221214 14:00:45 @agent_ppo2.py:121][0m #------------------------ Iteration 118 --------------------------#
[32m[20221214 14:00:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:45 @agent_ppo2.py:185][0m |           0.0028 |          19.1613 |          21.0793 |
[32m[20221214 14:00:45 @agent_ppo2.py:185][0m |           0.0034 |          15.9240 |          21.0696 |
[32m[20221214 14:00:45 @agent_ppo2.py:185][0m |           0.0109 |          14.2417 |          21.0657 |
[32m[20221214 14:00:45 @agent_ppo2.py:185][0m |          -0.0043 |          12.0763 |          21.0618 |
[32m[20221214 14:00:45 @agent_ppo2.py:185][0m |          -0.0002 |          11.1561 |          21.0543 |
[32m[20221214 14:00:45 @agent_ppo2.py:185][0m |          -0.0016 |          10.5937 |          21.0522 |
[32m[20221214 14:00:46 @agent_ppo2.py:185][0m |          -0.0026 |          10.0024 |          21.0476 |
[32m[20221214 14:00:46 @agent_ppo2.py:185][0m |          -0.0043 |           9.5675 |          21.0478 |
[32m[20221214 14:00:46 @agent_ppo2.py:185][0m |          -0.0031 |           9.2139 |          21.0457 |
[32m[20221214 14:00:46 @agent_ppo2.py:185][0m |          -0.0048 |           8.8715 |          21.0399 |
[32m[20221214 14:00:46 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:00:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.49
[32m[20221214 14:00:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 260.55
[32m[20221214 14:00:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.58
[32m[20221214 14:00:46 @agent_ppo2.py:143][0m Total time:       2.73 min
[32m[20221214 14:00:46 @agent_ppo2.py:145][0m 243712 total steps have happened
[32m[20221214 14:00:46 @agent_ppo2.py:121][0m #------------------------ Iteration 119 --------------------------#
[32m[20221214 14:00:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:46 @agent_ppo2.py:185][0m |          -0.0051 |          27.5129 |          21.1489 |
[32m[20221214 14:00:46 @agent_ppo2.py:185][0m |          -0.0044 |          22.1109 |          21.1488 |
[32m[20221214 14:00:47 @agent_ppo2.py:185][0m |          -0.0013 |          19.5632 |          21.1454 |
[32m[20221214 14:00:47 @agent_ppo2.py:185][0m |          -0.0010 |          17.8496 |          21.1426 |
[32m[20221214 14:00:47 @agent_ppo2.py:185][0m |          -0.0032 |          16.4005 |          21.1492 |
[32m[20221214 14:00:47 @agent_ppo2.py:185][0m |          -0.0047 |          15.3654 |          21.1491 |
[32m[20221214 14:00:47 @agent_ppo2.py:185][0m |           0.0049 |          15.4127 |          21.1510 |
[32m[20221214 14:00:47 @agent_ppo2.py:185][0m |          -0.0008 |          13.7272 |          21.1448 |
[32m[20221214 14:00:47 @agent_ppo2.py:185][0m |          -0.0026 |          13.0739 |          21.1557 |
[32m[20221214 14:00:47 @agent_ppo2.py:185][0m |           0.0030 |          13.2357 |          21.1616 |
[32m[20221214 14:00:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:00:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.59
[32m[20221214 14:00:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.80
[32m[20221214 14:00:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 242.61
[32m[20221214 14:00:47 @agent_ppo2.py:143][0m Total time:       2.76 min
[32m[20221214 14:00:47 @agent_ppo2.py:145][0m 245760 total steps have happened
[32m[20221214 14:00:47 @agent_ppo2.py:121][0m #------------------------ Iteration 120 --------------------------#
[32m[20221214 14:00:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:00:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:48 @agent_ppo2.py:185][0m |          -0.0011 |          24.1994 |          21.1010 |
[32m[20221214 14:00:48 @agent_ppo2.py:185][0m |          -0.0014 |          18.0809 |          21.0966 |
[32m[20221214 14:00:48 @agent_ppo2.py:185][0m |          -0.0052 |          15.8014 |          21.0871 |
[32m[20221214 14:00:48 @agent_ppo2.py:185][0m |          -0.0045 |          14.4712 |          21.0837 |
[32m[20221214 14:00:48 @agent_ppo2.py:185][0m |          -0.0039 |          13.4487 |          21.0782 |
[32m[20221214 14:00:48 @agent_ppo2.py:185][0m |          -0.0065 |          12.8299 |          21.0779 |
[32m[20221214 14:00:48 @agent_ppo2.py:185][0m |          -0.0039 |          12.3726 |          21.0734 |
[32m[20221214 14:00:48 @agent_ppo2.py:185][0m |          -0.0002 |          11.8024 |          21.0744 |
[32m[20221214 14:00:49 @agent_ppo2.py:185][0m |           0.0009 |          11.3788 |          21.0768 |
[32m[20221214 14:00:49 @agent_ppo2.py:185][0m |          -0.0003 |          11.1455 |          21.0731 |
[32m[20221214 14:00:49 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:00:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.08
[32m[20221214 14:00:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.18
[32m[20221214 14:00:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.62
[32m[20221214 14:00:49 @agent_ppo2.py:143][0m Total time:       2.78 min
[32m[20221214 14:00:49 @agent_ppo2.py:145][0m 247808 total steps have happened
[32m[20221214 14:00:49 @agent_ppo2.py:121][0m #------------------------ Iteration 121 --------------------------#
[32m[20221214 14:00:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:49 @agent_ppo2.py:185][0m |          -0.0008 |          25.9724 |          21.0660 |
[32m[20221214 14:00:49 @agent_ppo2.py:185][0m |           0.0148 |          22.2918 |          21.0577 |
[32m[20221214 14:00:49 @agent_ppo2.py:185][0m |          -0.0018 |          17.7506 |          21.0496 |
[32m[20221214 14:00:49 @agent_ppo2.py:185][0m |          -0.0001 |          16.4891 |          21.0422 |
[32m[20221214 14:00:50 @agent_ppo2.py:185][0m |           0.0012 |          15.5743 |          21.0367 |
[32m[20221214 14:00:50 @agent_ppo2.py:185][0m |           0.0031 |          14.9337 |          21.0331 |
[32m[20221214 14:00:50 @agent_ppo2.py:185][0m |          -0.0048 |          14.2311 |          21.0272 |
[32m[20221214 14:00:50 @agent_ppo2.py:185][0m |          -0.0043 |          13.6744 |          21.0239 |
[32m[20221214 14:00:50 @agent_ppo2.py:185][0m |          -0.0034 |          13.2057 |          21.0190 |
[32m[20221214 14:00:50 @agent_ppo2.py:185][0m |          -0.0009 |          12.8233 |          21.0173 |
[32m[20221214 14:00:50 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:00:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.62
[32m[20221214 14:00:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.26
[32m[20221214 14:00:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 371.86
[32m[20221214 14:00:50 @agent_ppo2.py:143][0m Total time:       2.80 min
[32m[20221214 14:00:50 @agent_ppo2.py:145][0m 249856 total steps have happened
[32m[20221214 14:00:50 @agent_ppo2.py:121][0m #------------------------ Iteration 122 --------------------------#
[32m[20221214 14:00:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:51 @agent_ppo2.py:185][0m |          -0.0044 |          22.9153 |          21.0473 |
[32m[20221214 14:00:51 @agent_ppo2.py:185][0m |          -0.0051 |          18.7021 |          21.0464 |
[32m[20221214 14:00:51 @agent_ppo2.py:185][0m |          -0.0000 |          16.9629 |          21.0377 |
[32m[20221214 14:00:51 @agent_ppo2.py:185][0m |          -0.0050 |          15.9858 |          21.0335 |
[32m[20221214 14:00:51 @agent_ppo2.py:185][0m |           0.0098 |          16.7750 |          21.0334 |
[32m[20221214 14:00:51 @agent_ppo2.py:185][0m |          -0.0019 |          14.7881 |          21.0325 |
[32m[20221214 14:00:51 @agent_ppo2.py:185][0m |          -0.0046 |          14.1827 |          21.0302 |
[32m[20221214 14:00:51 @agent_ppo2.py:185][0m |          -0.0035 |          13.7703 |          21.0256 |
[32m[20221214 14:00:51 @agent_ppo2.py:185][0m |          -0.0047 |          13.4229 |          21.0257 |
[32m[20221214 14:00:51 @agent_ppo2.py:185][0m |          -0.0033 |          13.1106 |          21.0248 |
[32m[20221214 14:00:51 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:00:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.10
[32m[20221214 14:00:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 342.81
[32m[20221214 14:00:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.50
[32m[20221214 14:00:52 @agent_ppo2.py:143][0m Total time:       2.82 min
[32m[20221214 14:00:52 @agent_ppo2.py:145][0m 251904 total steps have happened
[32m[20221214 14:00:52 @agent_ppo2.py:121][0m #------------------------ Iteration 123 --------------------------#
[32m[20221214 14:00:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:52 @agent_ppo2.py:185][0m |          -0.0055 |          19.6066 |          21.0542 |
[32m[20221214 14:00:52 @agent_ppo2.py:185][0m |           0.0107 |          16.9435 |          21.0518 |
[32m[20221214 14:00:52 @agent_ppo2.py:185][0m |          -0.0021 |          15.0098 |          21.0488 |
[32m[20221214 14:00:52 @agent_ppo2.py:185][0m |           0.0034 |          13.9286 |          21.0483 |
[32m[20221214 14:00:52 @agent_ppo2.py:185][0m |           0.0042 |          13.3240 |          21.0475 |
[32m[20221214 14:00:52 @agent_ppo2.py:185][0m |          -0.0027 |          12.5889 |          21.0451 |
[32m[20221214 14:00:52 @agent_ppo2.py:185][0m |          -0.0053 |          12.2585 |          21.0437 |
[32m[20221214 14:00:53 @agent_ppo2.py:185][0m |          -0.0033 |          11.5867 |          21.0448 |
[32m[20221214 14:00:53 @agent_ppo2.py:185][0m |           0.0017 |          11.1737 |          21.0447 |
[32m[20221214 14:00:53 @agent_ppo2.py:185][0m |           0.0027 |          10.8088 |          21.0444 |
[32m[20221214 14:00:53 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:00:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.20
[32m[20221214 14:00:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 341.93
[32m[20221214 14:00:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 323.12
[32m[20221214 14:00:53 @agent_ppo2.py:143][0m Total time:       2.85 min
[32m[20221214 14:00:53 @agent_ppo2.py:145][0m 253952 total steps have happened
[32m[20221214 14:00:53 @agent_ppo2.py:121][0m #------------------------ Iteration 124 --------------------------#
[32m[20221214 14:00:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:53 @agent_ppo2.py:185][0m |           0.0084 |          23.2915 |          21.0986 |
[32m[20221214 14:00:53 @agent_ppo2.py:185][0m |           0.0005 |          17.7229 |          21.0932 |
[32m[20221214 14:00:53 @agent_ppo2.py:185][0m |           0.0026 |          16.1088 |          21.0874 |
[32m[20221214 14:00:54 @agent_ppo2.py:185][0m |           0.0013 |          14.9647 |          21.0800 |
[32m[20221214 14:00:54 @agent_ppo2.py:185][0m |          -0.0031 |          14.2197 |          21.0764 |
[32m[20221214 14:00:54 @agent_ppo2.py:185][0m |          -0.0008 |          13.7735 |          21.0671 |
[32m[20221214 14:00:54 @agent_ppo2.py:185][0m |          -0.0021 |          13.1197 |          21.0670 |
[32m[20221214 14:00:54 @agent_ppo2.py:185][0m |           0.0036 |          13.2009 |          21.0658 |
[32m[20221214 14:00:54 @agent_ppo2.py:185][0m |          -0.0038 |          12.3518 |          21.0557 |
[32m[20221214 14:00:54 @agent_ppo2.py:185][0m |          -0.0017 |          12.1211 |          21.0541 |
[32m[20221214 14:00:54 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:00:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 313.76
[32m[20221214 14:00:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 350.81
[32m[20221214 14:00:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.95
[32m[20221214 14:00:54 @agent_ppo2.py:143][0m Total time:       2.87 min
[32m[20221214 14:00:54 @agent_ppo2.py:145][0m 256000 total steps have happened
[32m[20221214 14:00:54 @agent_ppo2.py:121][0m #------------------------ Iteration 125 --------------------------#
[32m[20221214 14:00:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:55 @agent_ppo2.py:185][0m |          -0.0029 |          26.6219 |          21.0283 |
[32m[20221214 14:00:55 @agent_ppo2.py:185][0m |           0.0003 |          20.1364 |          21.0257 |
[32m[20221214 14:00:55 @agent_ppo2.py:185][0m |           0.0016 |          18.1726 |          21.0193 |
[32m[20221214 14:00:55 @agent_ppo2.py:185][0m |          -0.0022 |          16.7076 |          21.0140 |
[32m[20221214 14:00:55 @agent_ppo2.py:185][0m |          -0.0066 |          15.8980 |          21.0079 |
[32m[20221214 14:00:55 @agent_ppo2.py:185][0m |          -0.0084 |          15.2845 |          21.0040 |
[32m[20221214 14:00:55 @agent_ppo2.py:185][0m |          -0.0012 |          14.8925 |          21.0065 |
[32m[20221214 14:00:55 @agent_ppo2.py:185][0m |          -0.0058 |          14.3715 |          21.0024 |
[32m[20221214 14:00:55 @agent_ppo2.py:185][0m |           0.0206 |          19.1958 |          21.0048 |
[32m[20221214 14:00:56 @agent_ppo2.py:185][0m |          -0.0026 |          13.9228 |          21.0036 |
[32m[20221214 14:00:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:00:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 301.99
[32m[20221214 14:00:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 329.18
[32m[20221214 14:00:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 351.92
[32m[20221214 14:00:56 @agent_ppo2.py:143][0m Total time:       2.89 min
[32m[20221214 14:00:56 @agent_ppo2.py:145][0m 258048 total steps have happened
[32m[20221214 14:00:56 @agent_ppo2.py:121][0m #------------------------ Iteration 126 --------------------------#
[32m[20221214 14:00:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:56 @agent_ppo2.py:185][0m |           0.0004 |          27.6115 |          21.1299 |
[32m[20221214 14:00:56 @agent_ppo2.py:185][0m |          -0.0030 |          22.1670 |          21.1265 |
[32m[20221214 14:00:56 @agent_ppo2.py:185][0m |          -0.0039 |          20.2546 |          21.1147 |
[32m[20221214 14:00:56 @agent_ppo2.py:185][0m |          -0.0039 |          18.9240 |          21.1074 |
[32m[20221214 14:00:56 @agent_ppo2.py:185][0m |          -0.0039 |          18.0025 |          21.1074 |
[32m[20221214 14:00:57 @agent_ppo2.py:185][0m |          -0.0043 |          17.1945 |          21.1054 |
[32m[20221214 14:00:57 @agent_ppo2.py:185][0m |           0.0007 |          16.5036 |          21.1027 |
[32m[20221214 14:00:57 @agent_ppo2.py:185][0m |          -0.0012 |          15.9814 |          21.0995 |
[32m[20221214 14:00:57 @agent_ppo2.py:185][0m |          -0.0013 |          15.5883 |          21.0982 |
[32m[20221214 14:00:57 @agent_ppo2.py:185][0m |          -0.0009 |          14.8597 |          21.0966 |
[32m[20221214 14:00:57 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:00:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.94
[32m[20221214 14:00:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 364.09
[32m[20221214 14:00:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 336.22
[32m[20221214 14:00:57 @agent_ppo2.py:143][0m Total time:       2.92 min
[32m[20221214 14:00:57 @agent_ppo2.py:145][0m 260096 total steps have happened
[32m[20221214 14:00:57 @agent_ppo2.py:121][0m #------------------------ Iteration 127 --------------------------#
[32m[20221214 14:00:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:57 @agent_ppo2.py:185][0m |          -0.0055 |          25.8256 |          21.0912 |
[32m[20221214 14:00:58 @agent_ppo2.py:185][0m |          -0.0008 |          23.0635 |          21.0869 |
[32m[20221214 14:00:58 @agent_ppo2.py:185][0m |          -0.0044 |          21.6379 |          21.0863 |
[32m[20221214 14:00:58 @agent_ppo2.py:185][0m |          -0.0062 |          20.5601 |          21.0915 |
[32m[20221214 14:00:58 @agent_ppo2.py:185][0m |          -0.0055 |          19.8382 |          21.0953 |
[32m[20221214 14:00:58 @agent_ppo2.py:185][0m |          -0.0063 |          19.0060 |          21.0984 |
[32m[20221214 14:00:58 @agent_ppo2.py:185][0m |          -0.0059 |          18.4858 |          21.1010 |
[32m[20221214 14:00:58 @agent_ppo2.py:185][0m |          -0.0078 |          17.9610 |          21.1024 |
[32m[20221214 14:00:58 @agent_ppo2.py:185][0m |          -0.0047 |          17.5818 |          21.1065 |
[32m[20221214 14:00:58 @agent_ppo2.py:185][0m |          -0.0049 |          17.2131 |          21.1124 |
[32m[20221214 14:00:58 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:00:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.59
[32m[20221214 14:00:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 319.77
[32m[20221214 14:00:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.82
[32m[20221214 14:00:58 @agent_ppo2.py:143][0m Total time:       2.94 min
[32m[20221214 14:00:58 @agent_ppo2.py:145][0m 262144 total steps have happened
[32m[20221214 14:00:58 @agent_ppo2.py:121][0m #------------------------ Iteration 128 --------------------------#
[32m[20221214 14:00:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:00:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:00:59 @agent_ppo2.py:185][0m |          -0.0017 |          30.7700 |          21.0425 |
[32m[20221214 14:00:59 @agent_ppo2.py:185][0m |          -0.0008 |          25.4246 |          21.0320 |
[32m[20221214 14:00:59 @agent_ppo2.py:185][0m |          -0.0045 |          23.1567 |          21.0279 |
[32m[20221214 14:00:59 @agent_ppo2.py:185][0m |          -0.0014 |          21.8210 |          21.0275 |
[32m[20221214 14:00:59 @agent_ppo2.py:185][0m |          -0.0030 |          20.5608 |          21.0282 |
[32m[20221214 14:00:59 @agent_ppo2.py:185][0m |          -0.0063 |          19.6497 |          21.0268 |
[32m[20221214 14:00:59 @agent_ppo2.py:185][0m |          -0.0038 |          18.9899 |          21.0354 |
[32m[20221214 14:00:59 @agent_ppo2.py:185][0m |           0.0079 |          18.8945 |          21.0373 |
[32m[20221214 14:01:00 @agent_ppo2.py:185][0m |          -0.0040 |          17.8741 |          21.0342 |
[32m[20221214 14:01:00 @agent_ppo2.py:185][0m |          -0.0039 |          17.4465 |          21.0364 |
[32m[20221214 14:01:00 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 288.39
[32m[20221214 14:01:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 322.28
[32m[20221214 14:01:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.39
[32m[20221214 14:01:00 @agent_ppo2.py:143][0m Total time:       2.96 min
[32m[20221214 14:01:00 @agent_ppo2.py:145][0m 264192 total steps have happened
[32m[20221214 14:01:00 @agent_ppo2.py:121][0m #------------------------ Iteration 129 --------------------------#
[32m[20221214 14:01:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:00 @agent_ppo2.py:185][0m |           0.0029 |          32.0966 |          21.1815 |
[32m[20221214 14:01:00 @agent_ppo2.py:185][0m |          -0.0002 |          25.8041 |          21.1749 |
[32m[20221214 14:01:00 @agent_ppo2.py:185][0m |          -0.0006 |          22.7506 |          21.1677 |
[32m[20221214 14:01:00 @agent_ppo2.py:185][0m |          -0.0011 |          20.5643 |          21.1628 |
[32m[20221214 14:01:00 @agent_ppo2.py:185][0m |          -0.0014 |          19.2003 |          21.1528 |
[32m[20221214 14:01:01 @agent_ppo2.py:185][0m |          -0.0004 |          18.4049 |          21.1478 |
[32m[20221214 14:01:01 @agent_ppo2.py:185][0m |           0.0039 |          18.0512 |          21.1416 |
[32m[20221214 14:01:01 @agent_ppo2.py:185][0m |           0.0024 |          17.1585 |          21.1380 |
[32m[20221214 14:01:01 @agent_ppo2.py:185][0m |          -0.0004 |          16.6709 |          21.1345 |
[32m[20221214 14:01:01 @agent_ppo2.py:185][0m |          -0.0003 |          16.2220 |          21.1297 |
[32m[20221214 14:01:01 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:01:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 317.96
[32m[20221214 14:01:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 347.15
[32m[20221214 14:01:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 319.60
[32m[20221214 14:01:01 @agent_ppo2.py:143][0m Total time:       2.98 min
[32m[20221214 14:01:01 @agent_ppo2.py:145][0m 266240 total steps have happened
[32m[20221214 14:01:01 @agent_ppo2.py:121][0m #------------------------ Iteration 130 --------------------------#
[32m[20221214 14:01:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:01:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:01 @agent_ppo2.py:185][0m |          -0.0006 |          32.1126 |          21.0202 |
[32m[20221214 14:01:02 @agent_ppo2.py:185][0m |          -0.0030 |          27.6216 |          21.0159 |
[32m[20221214 14:01:02 @agent_ppo2.py:185][0m |          -0.0010 |          25.7983 |          21.0151 |
[32m[20221214 14:01:02 @agent_ppo2.py:185][0m |          -0.0034 |          24.8319 |          21.0113 |
[32m[20221214 14:01:02 @agent_ppo2.py:185][0m |           0.0111 |          28.1314 |          21.0079 |
[32m[20221214 14:01:02 @agent_ppo2.py:185][0m |           0.0059 |          24.0666 |          21.0053 |
[32m[20221214 14:01:02 @agent_ppo2.py:185][0m |          -0.0069 |          22.5964 |          21.0014 |
[32m[20221214 14:01:02 @agent_ppo2.py:185][0m |           0.0012 |          22.3914 |          20.9978 |
[32m[20221214 14:01:02 @agent_ppo2.py:185][0m |           0.0176 |          22.8754 |          20.9983 |
[32m[20221214 14:01:02 @agent_ppo2.py:185][0m |           0.0013 |          21.7879 |          20.9894 |
[32m[20221214 14:01:02 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:01:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.41
[32m[20221214 14:01:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.06
[32m[20221214 14:01:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 325.18
[32m[20221214 14:01:03 @agent_ppo2.py:143][0m Total time:       3.01 min
[32m[20221214 14:01:03 @agent_ppo2.py:145][0m 268288 total steps have happened
[32m[20221214 14:01:03 @agent_ppo2.py:121][0m #------------------------ Iteration 131 --------------------------#
[32m[20221214 14:01:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:03 @agent_ppo2.py:185][0m |           0.0071 |          34.8765 |          21.1294 |
[32m[20221214 14:01:03 @agent_ppo2.py:185][0m |          -0.0029 |          26.7359 |          21.1191 |
[32m[20221214 14:01:03 @agent_ppo2.py:185][0m |          -0.0060 |          24.3493 |          21.1101 |
[32m[20221214 14:01:03 @agent_ppo2.py:185][0m |          -0.0033 |          22.3463 |          21.1045 |
[32m[20221214 14:01:03 @agent_ppo2.py:185][0m |          -0.0025 |          20.9184 |          21.0952 |
[32m[20221214 14:01:03 @agent_ppo2.py:185][0m |          -0.0042 |          19.7351 |          21.0963 |
[32m[20221214 14:01:03 @agent_ppo2.py:185][0m |          -0.0038 |          18.9284 |          21.0890 |
[32m[20221214 14:01:04 @agent_ppo2.py:185][0m |          -0.0047 |          18.3305 |          21.0892 |
[32m[20221214 14:01:04 @agent_ppo2.py:185][0m |          -0.0046 |          17.8299 |          21.0873 |
[32m[20221214 14:01:04 @agent_ppo2.py:185][0m |          -0.0028 |          17.6851 |          21.0853 |
[32m[20221214 14:01:04 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:01:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.97
[32m[20221214 14:01:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.10
[32m[20221214 14:01:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.25
[32m[20221214 14:01:04 @agent_ppo2.py:143][0m Total time:       3.03 min
[32m[20221214 14:01:04 @agent_ppo2.py:145][0m 270336 total steps have happened
[32m[20221214 14:01:04 @agent_ppo2.py:121][0m #------------------------ Iteration 132 --------------------------#
[32m[20221214 14:01:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:04 @agent_ppo2.py:185][0m |           0.0007 |          28.1394 |          21.1327 |
[32m[20221214 14:01:04 @agent_ppo2.py:185][0m |          -0.0042 |          22.8971 |          21.1266 |
[32m[20221214 14:01:04 @agent_ppo2.py:185][0m |           0.0013 |          21.1993 |          21.1231 |
[32m[20221214 14:01:04 @agent_ppo2.py:185][0m |          -0.0021 |          20.0100 |          21.1157 |
[32m[20221214 14:01:05 @agent_ppo2.py:185][0m |          -0.0013 |          19.2640 |          21.1111 |
[32m[20221214 14:01:05 @agent_ppo2.py:185][0m |           0.0008 |          18.7485 |          21.1118 |
[32m[20221214 14:01:05 @agent_ppo2.py:185][0m |          -0.0035 |          18.2526 |          21.1087 |
[32m[20221214 14:01:05 @agent_ppo2.py:185][0m |           0.0031 |          17.9818 |          21.1077 |
[32m[20221214 14:01:05 @agent_ppo2.py:185][0m |          -0.0041 |          17.5654 |          21.1043 |
[32m[20221214 14:01:05 @agent_ppo2.py:185][0m |          -0.0050 |          17.1732 |          21.1030 |
[32m[20221214 14:01:05 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:01:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 337.11
[32m[20221214 14:01:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 359.68
[32m[20221214 14:01:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.94
[32m[20221214 14:01:05 @agent_ppo2.py:143][0m Total time:       3.05 min
[32m[20221214 14:01:05 @agent_ppo2.py:145][0m 272384 total steps have happened
[32m[20221214 14:01:05 @agent_ppo2.py:121][0m #------------------------ Iteration 133 --------------------------#
[32m[20221214 14:01:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:06 @agent_ppo2.py:185][0m |          -0.0030 |          33.1405 |          21.1388 |
[32m[20221214 14:01:06 @agent_ppo2.py:185][0m |          -0.0024 |          28.2193 |          21.1292 |
[32m[20221214 14:01:06 @agent_ppo2.py:185][0m |          -0.0044 |          25.6983 |          21.1192 |
[32m[20221214 14:01:06 @agent_ppo2.py:185][0m |          -0.0048 |          23.8633 |          21.1157 |
[32m[20221214 14:01:06 @agent_ppo2.py:185][0m |          -0.0056 |          22.3278 |          21.1132 |
[32m[20221214 14:01:06 @agent_ppo2.py:185][0m |          -0.0072 |          21.0870 |          21.1084 |
[32m[20221214 14:01:06 @agent_ppo2.py:185][0m |           0.0008 |          20.3140 |          21.1052 |
[32m[20221214 14:01:06 @agent_ppo2.py:185][0m |          -0.0055 |          18.8938 |          21.1088 |
[32m[20221214 14:01:06 @agent_ppo2.py:185][0m |          -0.0042 |          18.0796 |          21.0992 |
[32m[20221214 14:01:06 @agent_ppo2.py:185][0m |          -0.0073 |          17.1734 |          21.1018 |
[32m[20221214 14:01:06 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:01:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 319.35
[32m[20221214 14:01:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 350.55
[32m[20221214 14:01:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.04
[32m[20221214 14:01:07 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 462.04
[32m[20221214 14:01:07 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 462.04
[32m[20221214 14:01:07 @agent_ppo2.py:143][0m Total time:       3.08 min
[32m[20221214 14:01:07 @agent_ppo2.py:145][0m 274432 total steps have happened
[32m[20221214 14:01:07 @agent_ppo2.py:121][0m #------------------------ Iteration 134 --------------------------#
[32m[20221214 14:01:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:07 @agent_ppo2.py:185][0m |          -0.0005 |          41.2245 |          21.0190 |
[32m[20221214 14:01:07 @agent_ppo2.py:185][0m |          -0.0057 |          35.3823 |          21.0113 |
[32m[20221214 14:01:07 @agent_ppo2.py:185][0m |          -0.0024 |          32.6897 |          21.0035 |
[32m[20221214 14:01:07 @agent_ppo2.py:185][0m |          -0.0012 |          31.0810 |          21.0012 |
[32m[20221214 14:01:07 @agent_ppo2.py:185][0m |          -0.0035 |          29.8744 |          21.0000 |
[32m[20221214 14:01:07 @agent_ppo2.py:185][0m |          -0.0060 |          28.8944 |          20.9937 |
[32m[20221214 14:01:08 @agent_ppo2.py:185][0m |          -0.0042 |          28.2442 |          20.9913 |
[32m[20221214 14:01:08 @agent_ppo2.py:185][0m |          -0.0042 |          27.4791 |          20.9941 |
[32m[20221214 14:01:08 @agent_ppo2.py:185][0m |          -0.0038 |          26.8547 |          20.9890 |
[32m[20221214 14:01:08 @agent_ppo2.py:185][0m |           0.0039 |          26.9691 |          20.9866 |
[32m[20221214 14:01:08 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.68
[32m[20221214 14:01:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 359.95
[32m[20221214 14:01:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.01
[32m[20221214 14:01:08 @agent_ppo2.py:143][0m Total time:       3.10 min
[32m[20221214 14:01:08 @agent_ppo2.py:145][0m 276480 total steps have happened
[32m[20221214 14:01:08 @agent_ppo2.py:121][0m #------------------------ Iteration 135 --------------------------#
[32m[20221214 14:01:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:08 @agent_ppo2.py:185][0m |          -0.0044 |          32.0586 |          21.0508 |
[32m[20221214 14:01:08 @agent_ppo2.py:185][0m |          -0.0041 |          28.2589 |          21.0436 |
[32m[20221214 14:01:09 @agent_ppo2.py:185][0m |          -0.0029 |          26.7840 |          21.0356 |
[32m[20221214 14:01:09 @agent_ppo2.py:185][0m |           0.0001 |          26.3335 |          21.0251 |
[32m[20221214 14:01:09 @agent_ppo2.py:185][0m |           0.0025 |          25.7531 |          21.0193 |
[32m[20221214 14:01:09 @agent_ppo2.py:185][0m |           0.0036 |          27.5100 |          21.0161 |
[32m[20221214 14:01:09 @agent_ppo2.py:185][0m |          -0.0013 |          24.3947 |          21.0049 |
[32m[20221214 14:01:09 @agent_ppo2.py:185][0m |           0.0065 |          25.2920 |          21.0162 |
[32m[20221214 14:01:09 @agent_ppo2.py:185][0m |           0.0019 |          23.6996 |          21.0041 |
[32m[20221214 14:01:09 @agent_ppo2.py:185][0m |           0.0001 |          23.4277 |          21.0116 |
[32m[20221214 14:01:09 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.05
[32m[20221214 14:01:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 315.80
[32m[20221214 14:01:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.27
[32m[20221214 14:01:09 @agent_ppo2.py:143][0m Total time:       3.12 min
[32m[20221214 14:01:09 @agent_ppo2.py:145][0m 278528 total steps have happened
[32m[20221214 14:01:09 @agent_ppo2.py:121][0m #------------------------ Iteration 136 --------------------------#
[32m[20221214 14:01:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:10 @agent_ppo2.py:185][0m |          -0.0045 |          22.3256 |          21.0928 |
[32m[20221214 14:01:10 @agent_ppo2.py:185][0m |           0.0008 |          17.9169 |          21.0869 |
[32m[20221214 14:01:10 @agent_ppo2.py:185][0m |          -0.0042 |          16.3102 |          21.0813 |
[32m[20221214 14:01:10 @agent_ppo2.py:185][0m |          -0.0038 |          15.3542 |          21.0809 |
[32m[20221214 14:01:10 @agent_ppo2.py:185][0m |          -0.0014 |          14.6096 |          21.0793 |
[32m[20221214 14:01:10 @agent_ppo2.py:185][0m |          -0.0019 |          13.9363 |          21.0758 |
[32m[20221214 14:01:10 @agent_ppo2.py:185][0m |          -0.0035 |          13.5870 |          21.0758 |
[32m[20221214 14:01:10 @agent_ppo2.py:185][0m |           0.0035 |          13.5887 |          21.0770 |
[32m[20221214 14:01:10 @agent_ppo2.py:185][0m |          -0.0067 |          12.7083 |          21.0769 |
[32m[20221214 14:01:11 @agent_ppo2.py:185][0m |          -0.0071 |          12.3714 |          21.0791 |
[32m[20221214 14:01:11 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:01:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.51
[32m[20221214 14:01:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 327.02
[32m[20221214 14:01:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.56
[32m[20221214 14:01:11 @agent_ppo2.py:143][0m Total time:       3.14 min
[32m[20221214 14:01:11 @agent_ppo2.py:145][0m 280576 total steps have happened
[32m[20221214 14:01:11 @agent_ppo2.py:121][0m #------------------------ Iteration 137 --------------------------#
[32m[20221214 14:01:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:11 @agent_ppo2.py:185][0m |           0.0069 |          28.5866 |          21.0324 |
[32m[20221214 14:01:11 @agent_ppo2.py:185][0m |          -0.0033 |          22.5547 |          21.0290 |
[32m[20221214 14:01:11 @agent_ppo2.py:185][0m |          -0.0058 |          19.9857 |          21.0276 |
[32m[20221214 14:01:11 @agent_ppo2.py:185][0m |          -0.0008 |          18.3836 |          21.0224 |
[32m[20221214 14:01:11 @agent_ppo2.py:185][0m |          -0.0042 |          17.3423 |          21.0190 |
[32m[20221214 14:01:12 @agent_ppo2.py:185][0m |          -0.0062 |          16.6099 |          21.0231 |
[32m[20221214 14:01:12 @agent_ppo2.py:185][0m |          -0.0058 |          15.9359 |          21.0183 |
[32m[20221214 14:01:12 @agent_ppo2.py:185][0m |          -0.0004 |          15.4190 |          21.0182 |
[32m[20221214 14:01:12 @agent_ppo2.py:185][0m |          -0.0032 |          14.8859 |          21.0203 |
[32m[20221214 14:01:12 @agent_ppo2.py:185][0m |          -0.0054 |          14.5104 |          21.0187 |
[32m[20221214 14:01:12 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:01:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 313.46
[32m[20221214 14:01:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 338.10
[32m[20221214 14:01:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.55
[32m[20221214 14:01:12 @agent_ppo2.py:143][0m Total time:       3.17 min
[32m[20221214 14:01:12 @agent_ppo2.py:145][0m 282624 total steps have happened
[32m[20221214 14:01:12 @agent_ppo2.py:121][0m #------------------------ Iteration 138 --------------------------#
[32m[20221214 14:01:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:12 @agent_ppo2.py:185][0m |          -0.0067 |          36.2128 |          21.1480 |
[32m[20221214 14:01:13 @agent_ppo2.py:185][0m |          -0.0050 |          30.3946 |          21.1340 |
[32m[20221214 14:01:13 @agent_ppo2.py:185][0m |           0.0029 |          28.6716 |          21.1227 |
[32m[20221214 14:01:13 @agent_ppo2.py:185][0m |          -0.0071 |          25.9695 |          21.1257 |
[32m[20221214 14:01:13 @agent_ppo2.py:185][0m |          -0.0031 |          24.6743 |          21.1233 |
[32m[20221214 14:01:13 @agent_ppo2.py:185][0m |          -0.0028 |          23.9963 |          21.1244 |
[32m[20221214 14:01:13 @agent_ppo2.py:185][0m |          -0.0066 |          23.1544 |          21.1180 |
[32m[20221214 14:01:13 @agent_ppo2.py:185][0m |          -0.0007 |          22.7701 |          21.1238 |
[32m[20221214 14:01:13 @agent_ppo2.py:185][0m |          -0.0020 |          22.1693 |          21.1235 |
[32m[20221214 14:01:13 @agent_ppo2.py:185][0m |          -0.0104 |          21.6956 |          21.1316 |
[32m[20221214 14:01:13 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:01:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.09
[32m[20221214 14:01:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.80
[32m[20221214 14:01:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.29
[32m[20221214 14:01:13 @agent_ppo2.py:143][0m Total time:       3.19 min
[32m[20221214 14:01:13 @agent_ppo2.py:145][0m 284672 total steps have happened
[32m[20221214 14:01:13 @agent_ppo2.py:121][0m #------------------------ Iteration 139 --------------------------#
[32m[20221214 14:01:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:14 @agent_ppo2.py:185][0m |          -0.0029 |          33.1570 |          21.0418 |
[32m[20221214 14:01:14 @agent_ppo2.py:185][0m |          -0.0056 |          27.4947 |          21.0331 |
[32m[20221214 14:01:14 @agent_ppo2.py:185][0m |          -0.0028 |          25.4266 |          21.0249 |
[32m[20221214 14:01:14 @agent_ppo2.py:185][0m |          -0.0047 |          24.3922 |          21.0188 |
[32m[20221214 14:01:14 @agent_ppo2.py:185][0m |           0.0039 |          24.3730 |          21.0204 |
[32m[20221214 14:01:14 @agent_ppo2.py:185][0m |          -0.0052 |          22.8773 |          21.0168 |
[32m[20221214 14:01:14 @agent_ppo2.py:185][0m |           0.0013 |          22.4997 |          21.0172 |
[32m[20221214 14:01:14 @agent_ppo2.py:185][0m |           0.0067 |          23.6399 |          21.0133 |
[32m[20221214 14:01:15 @agent_ppo2.py:185][0m |          -0.0038 |          21.6869 |          21.0024 |
[32m[20221214 14:01:15 @agent_ppo2.py:185][0m |          -0.0018 |          20.8810 |          21.0074 |
[32m[20221214 14:01:15 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 314.98
[32m[20221214 14:01:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 338.64
[32m[20221214 14:01:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.27
[32m[20221214 14:01:15 @agent_ppo2.py:143][0m Total time:       3.21 min
[32m[20221214 14:01:15 @agent_ppo2.py:145][0m 286720 total steps have happened
[32m[20221214 14:01:15 @agent_ppo2.py:121][0m #------------------------ Iteration 140 --------------------------#
[32m[20221214 14:01:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:01:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:15 @agent_ppo2.py:185][0m |          -0.0017 |          35.0294 |          21.0777 |
[32m[20221214 14:01:15 @agent_ppo2.py:185][0m |          -0.0024 |          26.6127 |          21.0771 |
[32m[20221214 14:01:15 @agent_ppo2.py:185][0m |          -0.0017 |          25.0523 |          21.0744 |
[32m[20221214 14:01:15 @agent_ppo2.py:185][0m |          -0.0036 |          24.1592 |          21.0659 |
[32m[20221214 14:01:16 @agent_ppo2.py:185][0m |          -0.0021 |          23.3961 |          21.0618 |
[32m[20221214 14:01:16 @agent_ppo2.py:185][0m |          -0.0015 |          22.8370 |          21.0579 |
[32m[20221214 14:01:16 @agent_ppo2.py:185][0m |          -0.0037 |          22.3878 |          21.0558 |
[32m[20221214 14:01:16 @agent_ppo2.py:185][0m |          -0.0068 |          21.8990 |          21.0486 |
[32m[20221214 14:01:16 @agent_ppo2.py:185][0m |           0.0007 |          21.5565 |          21.0453 |
[32m[20221214 14:01:16 @agent_ppo2.py:185][0m |          -0.0033 |          21.2875 |          21.0451 |
[32m[20221214 14:01:16 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.58
[32m[20221214 14:01:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.27
[32m[20221214 14:01:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.47
[32m[20221214 14:01:16 @agent_ppo2.py:143][0m Total time:       3.24 min
[32m[20221214 14:01:16 @agent_ppo2.py:145][0m 288768 total steps have happened
[32m[20221214 14:01:16 @agent_ppo2.py:121][0m #------------------------ Iteration 141 --------------------------#
[32m[20221214 14:01:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:17 @agent_ppo2.py:185][0m |          -0.0038 |          31.3633 |          20.9982 |
[32m[20221214 14:01:17 @agent_ppo2.py:185][0m |          -0.0034 |          24.4395 |          20.9937 |
[32m[20221214 14:01:17 @agent_ppo2.py:185][0m |           0.0007 |          21.5745 |          20.9844 |
[32m[20221214 14:01:17 @agent_ppo2.py:185][0m |          -0.0045 |          19.9260 |          20.9767 |
[32m[20221214 14:01:17 @agent_ppo2.py:185][0m |           0.0034 |          20.0265 |          20.9735 |
[32m[20221214 14:01:17 @agent_ppo2.py:185][0m |          -0.0034 |          18.0172 |          20.9748 |
[32m[20221214 14:01:17 @agent_ppo2.py:185][0m |          -0.0012 |          17.2777 |          20.9767 |
[32m[20221214 14:01:17 @agent_ppo2.py:185][0m |          -0.0022 |          16.7100 |          20.9762 |
[32m[20221214 14:01:17 @agent_ppo2.py:185][0m |          -0.0022 |          16.2938 |          20.9768 |
[32m[20221214 14:01:17 @agent_ppo2.py:185][0m |          -0.0083 |          15.6885 |          20.9785 |
[32m[20221214 14:01:17 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:01:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 316.99
[32m[20221214 14:01:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.97
[32m[20221214 14:01:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.81
[32m[20221214 14:01:18 @agent_ppo2.py:143][0m Total time:       3.26 min
[32m[20221214 14:01:18 @agent_ppo2.py:145][0m 290816 total steps have happened
[32m[20221214 14:01:18 @agent_ppo2.py:121][0m #------------------------ Iteration 142 --------------------------#
[32m[20221214 14:01:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:18 @agent_ppo2.py:185][0m |          -0.0014 |          36.9158 |          21.0395 |
[32m[20221214 14:01:18 @agent_ppo2.py:185][0m |          -0.0030 |          30.9456 |          21.0229 |
[32m[20221214 14:01:18 @agent_ppo2.py:185][0m |          -0.0035 |          28.1448 |          21.0251 |
[32m[20221214 14:01:18 @agent_ppo2.py:185][0m |          -0.0025 |          26.3649 |          21.0269 |
[32m[20221214 14:01:18 @agent_ppo2.py:185][0m |          -0.0006 |          25.3638 |          21.0272 |
[32m[20221214 14:01:18 @agent_ppo2.py:185][0m |          -0.0004 |          24.1407 |          21.0259 |
[32m[20221214 14:01:19 @agent_ppo2.py:185][0m |           0.0042 |          26.5357 |          21.0235 |
[32m[20221214 14:01:19 @agent_ppo2.py:185][0m |          -0.0047 |          22.7419 |          21.0232 |
[32m[20221214 14:01:19 @agent_ppo2.py:185][0m |          -0.0050 |          22.0102 |          21.0196 |
[32m[20221214 14:01:19 @agent_ppo2.py:185][0m |           0.0072 |          25.1815 |          21.0261 |
[32m[20221214 14:01:19 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:01:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.07
[32m[20221214 14:01:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 352.35
[32m[20221214 14:01:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 322.95
[32m[20221214 14:01:19 @agent_ppo2.py:143][0m Total time:       3.28 min
[32m[20221214 14:01:19 @agent_ppo2.py:145][0m 292864 total steps have happened
[32m[20221214 14:01:19 @agent_ppo2.py:121][0m #------------------------ Iteration 143 --------------------------#
[32m[20221214 14:01:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:01:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:19 @agent_ppo2.py:185][0m |           0.0008 |          36.1957 |          21.0862 |
[32m[20221214 14:01:19 @agent_ppo2.py:185][0m |           0.0008 |          31.0714 |          21.0818 |
[32m[20221214 14:01:20 @agent_ppo2.py:185][0m |          -0.0039 |          29.0392 |          21.0770 |
[32m[20221214 14:01:20 @agent_ppo2.py:185][0m |           0.0040 |          28.0144 |          21.0754 |
[32m[20221214 14:01:20 @agent_ppo2.py:185][0m |           0.0022 |          27.3598 |          21.0765 |
[32m[20221214 14:01:20 @agent_ppo2.py:185][0m |          -0.0014 |          25.8515 |          21.0673 |
[32m[20221214 14:01:20 @agent_ppo2.py:185][0m |          -0.0056 |          25.0723 |          21.0642 |
[32m[20221214 14:01:20 @agent_ppo2.py:185][0m |          -0.0042 |          24.3532 |          21.0647 |
[32m[20221214 14:01:20 @agent_ppo2.py:185][0m |          -0.0016 |          23.8652 |          21.0634 |
[32m[20221214 14:01:20 @agent_ppo2.py:185][0m |           0.0094 |          26.8912 |          21.0596 |
[32m[20221214 14:01:20 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.78
[32m[20221214 14:01:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 333.80
[32m[20221214 14:01:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.47
[32m[20221214 14:01:20 @agent_ppo2.py:143][0m Total time:       3.31 min
[32m[20221214 14:01:20 @agent_ppo2.py:145][0m 294912 total steps have happened
[32m[20221214 14:01:20 @agent_ppo2.py:121][0m #------------------------ Iteration 144 --------------------------#
[32m[20221214 14:01:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:21 @agent_ppo2.py:185][0m |           0.0089 |          36.4743 |          21.0318 |
[32m[20221214 14:01:21 @agent_ppo2.py:185][0m |          -0.0012 |          28.4703 |          21.0310 |
[32m[20221214 14:01:21 @agent_ppo2.py:185][0m |          -0.0043 |          25.9112 |          21.0267 |
[32m[20221214 14:01:21 @agent_ppo2.py:185][0m |          -0.0027 |          24.4893 |          21.0230 |
[32m[20221214 14:01:21 @agent_ppo2.py:185][0m |           0.0034 |          24.1067 |          21.0190 |
[32m[20221214 14:01:21 @agent_ppo2.py:185][0m |          -0.0056 |          22.7390 |          21.0164 |
[32m[20221214 14:01:21 @agent_ppo2.py:185][0m |           0.0078 |          22.8280 |          21.0112 |
[32m[20221214 14:01:21 @agent_ppo2.py:185][0m |          -0.0047 |          21.9836 |          21.0121 |
[32m[20221214 14:01:21 @agent_ppo2.py:185][0m |          -0.0052 |          21.0646 |          21.0095 |
[32m[20221214 14:01:22 @agent_ppo2.py:185][0m |          -0.0026 |          20.5531 |          21.0091 |
[32m[20221214 14:01:22 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.90
[32m[20221214 14:01:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 332.42
[32m[20221214 14:01:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.18
[32m[20221214 14:01:22 @agent_ppo2.py:143][0m Total time:       3.33 min
[32m[20221214 14:01:22 @agent_ppo2.py:145][0m 296960 total steps have happened
[32m[20221214 14:01:22 @agent_ppo2.py:121][0m #------------------------ Iteration 145 --------------------------#
[32m[20221214 14:01:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:01:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:22 @agent_ppo2.py:185][0m |          -0.0033 |          25.0622 |          21.0533 |
[32m[20221214 14:01:22 @agent_ppo2.py:185][0m |          -0.0077 |          19.9078 |          21.0518 |
[32m[20221214 14:01:22 @agent_ppo2.py:185][0m |          -0.0033 |          17.8755 |          21.0491 |
[32m[20221214 14:01:22 @agent_ppo2.py:185][0m |          -0.0017 |          16.5182 |          21.0404 |
[32m[20221214 14:01:23 @agent_ppo2.py:185][0m |          -0.0031 |          15.5101 |          21.0465 |
[32m[20221214 14:01:23 @agent_ppo2.py:185][0m |          -0.0015 |          14.7459 |          21.0455 |
[32m[20221214 14:01:23 @agent_ppo2.py:185][0m |          -0.0049 |          14.1044 |          21.0502 |
[32m[20221214 14:01:23 @agent_ppo2.py:185][0m |          -0.0078 |          13.5641 |          21.0500 |
[32m[20221214 14:01:23 @agent_ppo2.py:185][0m |          -0.0078 |          13.1724 |          21.0506 |
[32m[20221214 14:01:23 @agent_ppo2.py:185][0m |          -0.0055 |          12.7678 |          21.0545 |
[32m[20221214 14:01:23 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:01:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 302.62
[32m[20221214 14:01:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 333.49
[32m[20221214 14:01:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.95
[32m[20221214 14:01:23 @agent_ppo2.py:143][0m Total time:       3.35 min
[32m[20221214 14:01:23 @agent_ppo2.py:145][0m 299008 total steps have happened
[32m[20221214 14:01:23 @agent_ppo2.py:121][0m #------------------------ Iteration 146 --------------------------#
[32m[20221214 14:01:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:24 @agent_ppo2.py:185][0m |          -0.0043 |          34.0944 |          21.0786 |
[32m[20221214 14:01:24 @agent_ppo2.py:185][0m |           0.0010 |          29.4293 |          21.0727 |
[32m[20221214 14:01:24 @agent_ppo2.py:185][0m |          -0.0032 |          27.1821 |          21.0672 |
[32m[20221214 14:01:24 @agent_ppo2.py:185][0m |          -0.0033 |          25.7317 |          21.0600 |
[32m[20221214 14:01:24 @agent_ppo2.py:185][0m |          -0.0070 |          24.6812 |          21.0598 |
[32m[20221214 14:01:24 @agent_ppo2.py:185][0m |          -0.0053 |          24.0126 |          21.0640 |
[32m[20221214 14:01:24 @agent_ppo2.py:185][0m |          -0.0047 |          23.1066 |          21.0579 |
[32m[20221214 14:01:24 @agent_ppo2.py:185][0m |          -0.0045 |          22.4714 |          21.0620 |
[32m[20221214 14:01:24 @agent_ppo2.py:185][0m |          -0.0033 |          21.9817 |          21.0607 |
[32m[20221214 14:01:24 @agent_ppo2.py:185][0m |          -0.0053 |          21.5463 |          21.0560 |
[32m[20221214 14:01:24 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:01:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.99
[32m[20221214 14:01:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 312.58
[32m[20221214 14:01:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.85
[32m[20221214 14:01:25 @agent_ppo2.py:143][0m Total time:       3.37 min
[32m[20221214 14:01:25 @agent_ppo2.py:145][0m 301056 total steps have happened
[32m[20221214 14:01:25 @agent_ppo2.py:121][0m #------------------------ Iteration 147 --------------------------#
[32m[20221214 14:01:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:01:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:25 @agent_ppo2.py:185][0m |          -0.0032 |          27.2755 |          20.9854 |
[32m[20221214 14:01:25 @agent_ppo2.py:185][0m |          -0.0022 |          23.4351 |          20.9816 |
[32m[20221214 14:01:25 @agent_ppo2.py:185][0m |           0.0005 |          21.5199 |          20.9818 |
[32m[20221214 14:01:25 @agent_ppo2.py:185][0m |          -0.0035 |          19.8300 |          20.9802 |
[32m[20221214 14:01:25 @agent_ppo2.py:185][0m |          -0.0067 |          18.6582 |          20.9786 |
[32m[20221214 14:01:25 @agent_ppo2.py:185][0m |           0.0005 |          17.8054 |          20.9800 |
[32m[20221214 14:01:26 @agent_ppo2.py:185][0m |          -0.0055 |          17.2098 |          20.9811 |
[32m[20221214 14:01:26 @agent_ppo2.py:185][0m |          -0.0017 |          16.7778 |          20.9801 |
[32m[20221214 14:01:26 @agent_ppo2.py:185][0m |           0.0029 |          16.3740 |          20.9813 |
[32m[20221214 14:01:26 @agent_ppo2.py:185][0m |           0.0024 |          15.8936 |          20.9811 |
[32m[20221214 14:01:26 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 14:01:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.53
[32m[20221214 14:01:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 332.30
[32m[20221214 14:01:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.18
[32m[20221214 14:01:26 @agent_ppo2.py:143][0m Total time:       3.40 min
[32m[20221214 14:01:26 @agent_ppo2.py:145][0m 303104 total steps have happened
[32m[20221214 14:01:26 @agent_ppo2.py:121][0m #------------------------ Iteration 148 --------------------------#
[32m[20221214 14:01:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:26 @agent_ppo2.py:185][0m |          -0.0005 |          34.0948 |          21.0637 |
[32m[20221214 14:01:27 @agent_ppo2.py:185][0m |           0.0028 |          30.2731 |          21.0615 |
[32m[20221214 14:01:27 @agent_ppo2.py:185][0m |          -0.0058 |          28.1970 |          21.0619 |
[32m[20221214 14:01:27 @agent_ppo2.py:185][0m |          -0.0022 |          26.9518 |          21.0559 |
[32m[20221214 14:01:27 @agent_ppo2.py:185][0m |          -0.0008 |          26.0442 |          21.0522 |
[32m[20221214 14:01:27 @agent_ppo2.py:185][0m |           0.0019 |          25.2961 |          21.0528 |
[32m[20221214 14:01:27 @agent_ppo2.py:185][0m |          -0.0062 |          24.7819 |          21.0514 |
[32m[20221214 14:01:27 @agent_ppo2.py:185][0m |          -0.0069 |          24.3357 |          21.0471 |
[32m[20221214 14:01:27 @agent_ppo2.py:185][0m |          -0.0023 |          23.8192 |          21.0441 |
[32m[20221214 14:01:27 @agent_ppo2.py:185][0m |          -0.0028 |          23.4238 |          21.0404 |
[32m[20221214 14:01:27 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:01:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.39
[32m[20221214 14:01:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 327.89
[32m[20221214 14:01:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.11
[32m[20221214 14:01:27 @agent_ppo2.py:143][0m Total time:       3.42 min
[32m[20221214 14:01:27 @agent_ppo2.py:145][0m 305152 total steps have happened
[32m[20221214 14:01:27 @agent_ppo2.py:121][0m #------------------------ Iteration 149 --------------------------#
[32m[20221214 14:01:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:28 @agent_ppo2.py:185][0m |          -0.0030 |          27.3852 |          20.9889 |
[32m[20221214 14:01:28 @agent_ppo2.py:185][0m |          -0.0057 |          22.5992 |          20.9850 |
[32m[20221214 14:01:28 @agent_ppo2.py:185][0m |          -0.0022 |          20.5175 |          20.9829 |
[32m[20221214 14:01:28 @agent_ppo2.py:185][0m |          -0.0035 |          19.1426 |          20.9832 |
[32m[20221214 14:01:28 @agent_ppo2.py:185][0m |          -0.0022 |          18.2431 |          20.9868 |
[32m[20221214 14:01:28 @agent_ppo2.py:185][0m |           0.0023 |          17.7261 |          20.9849 |
[32m[20221214 14:01:28 @agent_ppo2.py:185][0m |           0.0004 |          17.1011 |          20.9875 |
[32m[20221214 14:01:28 @agent_ppo2.py:185][0m |          -0.0059 |          16.7038 |          20.9853 |
[32m[20221214 14:01:29 @agent_ppo2.py:185][0m |          -0.0036 |          16.2890 |          20.9871 |
[32m[20221214 14:01:29 @agent_ppo2.py:185][0m |          -0.0012 |          15.9878 |          20.9888 |
[32m[20221214 14:01:29 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:01:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.22
[32m[20221214 14:01:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.61
[32m[20221214 14:01:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.82
[32m[20221214 14:01:29 @agent_ppo2.py:143][0m Total time:       3.45 min
[32m[20221214 14:01:29 @agent_ppo2.py:145][0m 307200 total steps have happened
[32m[20221214 14:01:29 @agent_ppo2.py:121][0m #------------------------ Iteration 150 --------------------------#
[32m[20221214 14:01:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:01:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:29 @agent_ppo2.py:185][0m |          -0.0015 |          32.0082 |          21.1076 |
[32m[20221214 14:01:29 @agent_ppo2.py:185][0m |          -0.0078 |          25.7472 |          21.1065 |
[32m[20221214 14:01:29 @agent_ppo2.py:185][0m |          -0.0022 |          22.7449 |          21.1074 |
[32m[20221214 14:01:29 @agent_ppo2.py:185][0m |          -0.0054 |          20.9640 |          21.1053 |
[32m[20221214 14:01:30 @agent_ppo2.py:185][0m |          -0.0041 |          19.6377 |          21.1089 |
[32m[20221214 14:01:30 @agent_ppo2.py:185][0m |           0.0000 |          18.6573 |          21.1093 |
[32m[20221214 14:01:30 @agent_ppo2.py:185][0m |          -0.0031 |          17.7418 |          21.1103 |
[32m[20221214 14:01:30 @agent_ppo2.py:185][0m |          -0.0044 |          17.0498 |          21.1089 |
[32m[20221214 14:01:30 @agent_ppo2.py:185][0m |          -0.0030 |          16.4074 |          21.1111 |
[32m[20221214 14:01:30 @agent_ppo2.py:185][0m |          -0.0043 |          15.8539 |          21.1114 |
[32m[20221214 14:01:30 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:01:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 310.54
[32m[20221214 14:01:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 350.95
[32m[20221214 14:01:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.38
[32m[20221214 14:01:30 @agent_ppo2.py:143][0m Total time:       3.47 min
[32m[20221214 14:01:30 @agent_ppo2.py:145][0m 309248 total steps have happened
[32m[20221214 14:01:30 @agent_ppo2.py:121][0m #------------------------ Iteration 151 --------------------------#
[32m[20221214 14:01:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:30 @agent_ppo2.py:185][0m |          -0.0016 |          27.0261 |          21.0248 |
[32m[20221214 14:01:31 @agent_ppo2.py:185][0m |           0.0136 |          25.6380 |          21.0150 |
[32m[20221214 14:01:31 @agent_ppo2.py:185][0m |          -0.0066 |          19.5152 |          21.0066 |
[32m[20221214 14:01:31 @agent_ppo2.py:185][0m |          -0.0038 |          17.8764 |          21.0049 |
[32m[20221214 14:01:31 @agent_ppo2.py:185][0m |          -0.0049 |          16.6224 |          21.0051 |
[32m[20221214 14:01:31 @agent_ppo2.py:185][0m |          -0.0082 |          15.8826 |          21.0118 |
[32m[20221214 14:01:31 @agent_ppo2.py:185][0m |          -0.0065 |          14.9736 |          21.0149 |
[32m[20221214 14:01:31 @agent_ppo2.py:185][0m |          -0.0053 |          14.2767 |          21.0110 |
[32m[20221214 14:01:31 @agent_ppo2.py:185][0m |          -0.0085 |          13.7547 |          21.0140 |
[32m[20221214 14:01:31 @agent_ppo2.py:185][0m |          -0.0038 |          13.1353 |          21.0164 |
[32m[20221214 14:01:31 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 296.02
[32m[20221214 14:01:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 310.63
[32m[20221214 14:01:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 319.42
[32m[20221214 14:01:32 @agent_ppo2.py:143][0m Total time:       3.49 min
[32m[20221214 14:01:32 @agent_ppo2.py:145][0m 311296 total steps have happened
[32m[20221214 14:01:32 @agent_ppo2.py:121][0m #------------------------ Iteration 152 --------------------------#
[32m[20221214 14:01:32 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221214 14:01:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:32 @agent_ppo2.py:185][0m |           0.0004 |          31.5852 |          20.9921 |
[32m[20221214 14:01:32 @agent_ppo2.py:185][0m |          -0.0059 |          25.4735 |          20.9872 |
[32m[20221214 14:01:32 @agent_ppo2.py:185][0m |          -0.0060 |          22.9003 |          20.9856 |
[32m[20221214 14:01:32 @agent_ppo2.py:185][0m |          -0.0023 |          21.0163 |          20.9784 |
[32m[20221214 14:01:32 @agent_ppo2.py:185][0m |          -0.0008 |          19.8296 |          20.9835 |
[32m[20221214 14:01:32 @agent_ppo2.py:185][0m |          -0.0065 |          18.8640 |          20.9843 |
[32m[20221214 14:01:32 @agent_ppo2.py:185][0m |           0.0084 |          19.2836 |          20.9884 |
[32m[20221214 14:01:33 @agent_ppo2.py:185][0m |           0.0057 |          19.5369 |          20.9919 |
[32m[20221214 14:01:33 @agent_ppo2.py:185][0m |          -0.0023 |          16.9524 |          20.9926 |
[32m[20221214 14:01:33 @agent_ppo2.py:185][0m |           0.0015 |          16.4841 |          20.9948 |
[32m[20221214 14:01:33 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:01:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.51
[32m[20221214 14:01:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.89
[32m[20221214 14:01:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 318.08
[32m[20221214 14:01:33 @agent_ppo2.py:143][0m Total time:       3.51 min
[32m[20221214 14:01:33 @agent_ppo2.py:145][0m 313344 total steps have happened
[32m[20221214 14:01:33 @agent_ppo2.py:121][0m #------------------------ Iteration 153 --------------------------#
[32m[20221214 14:01:33 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221214 14:01:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:33 @agent_ppo2.py:185][0m |           0.0040 |          33.6815 |          20.9304 |
[32m[20221214 14:01:34 @agent_ppo2.py:185][0m |           0.0017 |          27.4955 |          20.9165 |
[32m[20221214 14:01:34 @agent_ppo2.py:185][0m |          -0.0035 |          24.6917 |          20.9137 |
[32m[20221214 14:01:34 @agent_ppo2.py:185][0m |          -0.0023 |          23.1674 |          20.9020 |
[32m[20221214 14:01:34 @agent_ppo2.py:185][0m |           0.0018 |          22.0155 |          20.9008 |
[32m[20221214 14:01:34 @agent_ppo2.py:185][0m |          -0.0003 |          21.1535 |          20.8853 |
[32m[20221214 14:01:34 @agent_ppo2.py:185][0m |          -0.0037 |          20.6381 |          20.8812 |
[32m[20221214 14:01:34 @agent_ppo2.py:185][0m |          -0.0010 |          19.8558 |          20.8718 |
[32m[20221214 14:01:34 @agent_ppo2.py:185][0m |          -0.0008 |          19.3740 |          20.8670 |
[32m[20221214 14:01:34 @agent_ppo2.py:185][0m |           0.0052 |          19.1633 |          20.8582 |
[32m[20221214 14:01:34 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221214 14:01:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.23
[32m[20221214 14:01:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 343.50
[32m[20221214 14:01:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.65
[32m[20221214 14:01:35 @agent_ppo2.py:143][0m Total time:       3.54 min
[32m[20221214 14:01:35 @agent_ppo2.py:145][0m 315392 total steps have happened
[32m[20221214 14:01:35 @agent_ppo2.py:121][0m #------------------------ Iteration 154 --------------------------#
[32m[20221214 14:01:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:35 @agent_ppo2.py:185][0m |           0.0042 |          26.3840 |          21.0782 |
[32m[20221214 14:01:35 @agent_ppo2.py:185][0m |          -0.0032 |          20.1467 |          21.0741 |
[32m[20221214 14:01:35 @agent_ppo2.py:185][0m |          -0.0045 |          17.9720 |          21.0672 |
[32m[20221214 14:01:35 @agent_ppo2.py:185][0m |          -0.0020 |          16.8635 |          21.0657 |
[32m[20221214 14:01:35 @agent_ppo2.py:185][0m |          -0.0042 |          16.0303 |          21.0605 |
[32m[20221214 14:01:35 @agent_ppo2.py:185][0m |          -0.0008 |          15.1640 |          21.0602 |
[32m[20221214 14:01:36 @agent_ppo2.py:185][0m |          -0.0033 |          14.6688 |          21.0627 |
[32m[20221214 14:01:36 @agent_ppo2.py:185][0m |          -0.0076 |          14.2659 |          21.0646 |
[32m[20221214 14:01:36 @agent_ppo2.py:185][0m |           0.0006 |          14.0096 |          21.0641 |
[32m[20221214 14:01:36 @agent_ppo2.py:185][0m |           0.0004 |          13.6972 |          21.0637 |
[32m[20221214 14:01:36 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:01:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.73
[32m[20221214 14:01:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 312.00
[32m[20221214 14:01:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.49
[32m[20221214 14:01:36 @agent_ppo2.py:143][0m Total time:       3.56 min
[32m[20221214 14:01:36 @agent_ppo2.py:145][0m 317440 total steps have happened
[32m[20221214 14:01:36 @agent_ppo2.py:121][0m #------------------------ Iteration 155 --------------------------#
[32m[20221214 14:01:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:36 @agent_ppo2.py:185][0m |          -0.0023 |          32.5416 |          20.9815 |
[32m[20221214 14:01:36 @agent_ppo2.py:185][0m |          -0.0083 |          27.5623 |          20.9814 |
[32m[20221214 14:01:37 @agent_ppo2.py:185][0m |          -0.0041 |          25.4566 |          20.9771 |
[32m[20221214 14:01:37 @agent_ppo2.py:185][0m |           0.0079 |          27.1067 |          20.9726 |
[32m[20221214 14:01:37 @agent_ppo2.py:185][0m |          -0.0048 |          23.0505 |          20.9718 |
[32m[20221214 14:01:37 @agent_ppo2.py:185][0m |          -0.0089 |          22.3270 |          20.9683 |
[32m[20221214 14:01:37 @agent_ppo2.py:185][0m |          -0.0043 |          21.4180 |          20.9671 |
[32m[20221214 14:01:37 @agent_ppo2.py:185][0m |          -0.0049 |          20.8789 |          20.9659 |
[32m[20221214 14:01:37 @agent_ppo2.py:185][0m |          -0.0064 |          20.3218 |          20.9646 |
[32m[20221214 14:01:37 @agent_ppo2.py:185][0m |          -0.0041 |          19.9572 |          20.9609 |
[32m[20221214 14:01:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:01:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.63
[32m[20221214 14:01:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 316.25
[32m[20221214 14:01:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.27
[32m[20221214 14:01:37 @agent_ppo2.py:143][0m Total time:       3.59 min
[32m[20221214 14:01:37 @agent_ppo2.py:145][0m 319488 total steps have happened
[32m[20221214 14:01:37 @agent_ppo2.py:121][0m #------------------------ Iteration 156 --------------------------#
[32m[20221214 14:01:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:38 @agent_ppo2.py:185][0m |           0.0055 |          33.6843 |          21.0593 |
[32m[20221214 14:01:38 @agent_ppo2.py:185][0m |          -0.0017 |          27.8851 |          21.0492 |
[32m[20221214 14:01:38 @agent_ppo2.py:185][0m |           0.0002 |          25.5631 |          21.0408 |
[32m[20221214 14:01:38 @agent_ppo2.py:185][0m |          -0.0022 |          23.4750 |          21.0333 |
[32m[20221214 14:01:38 @agent_ppo2.py:185][0m |          -0.0028 |          22.2041 |          21.0266 |
[32m[20221214 14:01:38 @agent_ppo2.py:185][0m |          -0.0023 |          21.0342 |          21.0220 |
[32m[20221214 14:01:38 @agent_ppo2.py:185][0m |          -0.0006 |          20.0186 |          21.0198 |
[32m[20221214 14:01:38 @agent_ppo2.py:185][0m |          -0.0028 |          19.1930 |          21.0189 |
[32m[20221214 14:01:38 @agent_ppo2.py:185][0m |          -0.0039 |          18.5685 |          21.0119 |
[32m[20221214 14:01:39 @agent_ppo2.py:185][0m |          -0.0030 |          17.9679 |          21.0070 |
[32m[20221214 14:01:39 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 288.99
[32m[20221214 14:01:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 322.37
[32m[20221214 14:01:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 243.31
[32m[20221214 14:01:39 @agent_ppo2.py:143][0m Total time:       3.61 min
[32m[20221214 14:01:39 @agent_ppo2.py:145][0m 321536 total steps have happened
[32m[20221214 14:01:39 @agent_ppo2.py:121][0m #------------------------ Iteration 157 --------------------------#
[32m[20221214 14:01:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:39 @agent_ppo2.py:185][0m |           0.0048 |          19.3480 |          21.0878 |
[32m[20221214 14:01:39 @agent_ppo2.py:185][0m |          -0.0040 |          14.5487 |          21.0800 |
[32m[20221214 14:01:39 @agent_ppo2.py:185][0m |          -0.0021 |          12.9909 |          21.0735 |
[32m[20221214 14:01:39 @agent_ppo2.py:185][0m |           0.0009 |          11.9382 |          21.0731 |
[32m[20221214 14:01:39 @agent_ppo2.py:185][0m |          -0.0033 |          10.8926 |          21.0679 |
[32m[20221214 14:01:40 @agent_ppo2.py:185][0m |          -0.0002 |          10.2399 |          21.0672 |
[32m[20221214 14:01:40 @agent_ppo2.py:185][0m |          -0.0057 |           9.7196 |          21.0597 |
[32m[20221214 14:01:40 @agent_ppo2.py:185][0m |          -0.0040 |           9.3097 |          21.0602 |
[32m[20221214 14:01:40 @agent_ppo2.py:185][0m |          -0.0021 |           8.8823 |          21.0545 |
[32m[20221214 14:01:40 @agent_ppo2.py:185][0m |          -0.0019 |           8.4902 |          21.0482 |
[32m[20221214 14:01:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:01:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.44
[32m[20221214 14:01:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.59
[32m[20221214 14:01:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 230.13
[32m[20221214 14:01:40 @agent_ppo2.py:143][0m Total time:       3.63 min
[32m[20221214 14:01:40 @agent_ppo2.py:145][0m 323584 total steps have happened
[32m[20221214 14:01:40 @agent_ppo2.py:121][0m #------------------------ Iteration 158 --------------------------#
[32m[20221214 14:01:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:40 @agent_ppo2.py:185][0m |          -0.0065 |          28.7041 |          21.0877 |
[32m[20221214 14:01:41 @agent_ppo2.py:185][0m |           0.0028 |          21.5767 |          21.0860 |
[32m[20221214 14:01:41 @agent_ppo2.py:185][0m |          -0.0047 |          18.9346 |          21.0802 |
[32m[20221214 14:01:41 @agent_ppo2.py:185][0m |          -0.0041 |          17.4679 |          21.0782 |
[32m[20221214 14:01:41 @agent_ppo2.py:185][0m |           0.0039 |          16.9843 |          21.0773 |
[32m[20221214 14:01:41 @agent_ppo2.py:185][0m |          -0.0004 |          15.7123 |          21.0743 |
[32m[20221214 14:01:41 @agent_ppo2.py:185][0m |          -0.0067 |          15.1052 |          21.0717 |
[32m[20221214 14:01:41 @agent_ppo2.py:185][0m |           0.0028 |          16.1908 |          21.0720 |
[32m[20221214 14:01:41 @agent_ppo2.py:185][0m |          -0.0005 |          14.4840 |          21.0731 |
[32m[20221214 14:01:41 @agent_ppo2.py:185][0m |          -0.0012 |          13.9329 |          21.0725 |
[32m[20221214 14:01:41 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.71
[32m[20221214 14:01:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.41
[32m[20221214 14:01:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 249.85
[32m[20221214 14:01:41 @agent_ppo2.py:143][0m Total time:       3.66 min
[32m[20221214 14:01:41 @agent_ppo2.py:145][0m 325632 total steps have happened
[32m[20221214 14:01:41 @agent_ppo2.py:121][0m #------------------------ Iteration 159 --------------------------#
[32m[20221214 14:01:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:42 @agent_ppo2.py:185][0m |          -0.0006 |          30.3186 |          21.0706 |
[32m[20221214 14:01:42 @agent_ppo2.py:185][0m |          -0.0041 |          24.0397 |          21.0633 |
[32m[20221214 14:01:42 @agent_ppo2.py:185][0m |           0.0132 |          23.0823 |          21.0593 |
[32m[20221214 14:01:42 @agent_ppo2.py:185][0m |          -0.0011 |          19.8454 |          21.0485 |
[32m[20221214 14:01:42 @agent_ppo2.py:185][0m |          -0.0007 |          18.7469 |          21.0580 |
[32m[20221214 14:01:42 @agent_ppo2.py:185][0m |          -0.0069 |          17.9469 |          21.0508 |
[32m[20221214 14:01:42 @agent_ppo2.py:185][0m |          -0.0020 |          17.2265 |          21.0575 |
[32m[20221214 14:01:42 @agent_ppo2.py:185][0m |           0.0016 |          17.0471 |          21.0575 |
[32m[20221214 14:01:43 @agent_ppo2.py:185][0m |          -0.0022 |          16.2730 |          21.0536 |
[32m[20221214 14:01:43 @agent_ppo2.py:185][0m |          -0.0080 |          15.8482 |          21.0551 |
[32m[20221214 14:01:43 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.90
[32m[20221214 14:01:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.77
[32m[20221214 14:01:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.47
[32m[20221214 14:01:43 @agent_ppo2.py:143][0m Total time:       3.68 min
[32m[20221214 14:01:43 @agent_ppo2.py:145][0m 327680 total steps have happened
[32m[20221214 14:01:43 @agent_ppo2.py:121][0m #------------------------ Iteration 160 --------------------------#
[32m[20221214 14:01:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:01:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:43 @agent_ppo2.py:185][0m |          -0.0014 |          28.0293 |          21.0897 |
[32m[20221214 14:01:43 @agent_ppo2.py:185][0m |           0.0027 |          22.1499 |          21.0865 |
[32m[20221214 14:01:43 @agent_ppo2.py:185][0m |          -0.0064 |          19.6787 |          21.0839 |
[32m[20221214 14:01:43 @agent_ppo2.py:185][0m |          -0.0028 |          17.9038 |          21.0782 |
[32m[20221214 14:01:44 @agent_ppo2.py:185][0m |          -0.0048 |          16.7910 |          21.0716 |
[32m[20221214 14:01:44 @agent_ppo2.py:185][0m |          -0.0018 |          15.8682 |          21.0672 |
[32m[20221214 14:01:44 @agent_ppo2.py:185][0m |          -0.0004 |          15.1390 |          21.0662 |
[32m[20221214 14:01:44 @agent_ppo2.py:185][0m |          -0.0002 |          14.5090 |          21.0589 |
[32m[20221214 14:01:44 @agent_ppo2.py:185][0m |          -0.0086 |          13.9460 |          21.0565 |
[32m[20221214 14:01:44 @agent_ppo2.py:185][0m |           0.0013 |          13.5042 |          21.0559 |
[32m[20221214 14:01:44 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.83
[32m[20221214 14:01:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 326.49
[32m[20221214 14:01:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.77
[32m[20221214 14:01:44 @agent_ppo2.py:143][0m Total time:       3.70 min
[32m[20221214 14:01:44 @agent_ppo2.py:145][0m 329728 total steps have happened
[32m[20221214 14:01:44 @agent_ppo2.py:121][0m #------------------------ Iteration 161 --------------------------#
[32m[20221214 14:01:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:45 @agent_ppo2.py:185][0m |           0.0017 |          20.8588 |          21.0275 |
[32m[20221214 14:01:45 @agent_ppo2.py:185][0m |           0.0023 |          16.2843 |          21.0321 |
[32m[20221214 14:01:45 @agent_ppo2.py:185][0m |          -0.0067 |          14.5286 |          21.0338 |
[32m[20221214 14:01:45 @agent_ppo2.py:185][0m |          -0.0003 |          13.3972 |          21.0357 |
[32m[20221214 14:01:45 @agent_ppo2.py:185][0m |          -0.0016 |          12.6463 |          21.0329 |
[32m[20221214 14:01:45 @agent_ppo2.py:185][0m |           0.0005 |          12.0898 |          21.0298 |
[32m[20221214 14:01:45 @agent_ppo2.py:185][0m |          -0.0030 |          11.5381 |          21.0286 |
[32m[20221214 14:01:45 @agent_ppo2.py:185][0m |          -0.0012 |          11.1804 |          21.0268 |
[32m[20221214 14:01:45 @agent_ppo2.py:185][0m |          -0.0001 |          10.7997 |          21.0295 |
[32m[20221214 14:01:45 @agent_ppo2.py:185][0m |          -0.0023 |          10.4962 |          21.0302 |
[32m[20221214 14:01:45 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.07
[32m[20221214 14:01:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 326.81
[32m[20221214 14:01:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 233.98
[32m[20221214 14:01:46 @agent_ppo2.py:143][0m Total time:       3.72 min
[32m[20221214 14:01:46 @agent_ppo2.py:145][0m 331776 total steps have happened
[32m[20221214 14:01:46 @agent_ppo2.py:121][0m #------------------------ Iteration 162 --------------------------#
[32m[20221214 14:01:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:46 @agent_ppo2.py:185][0m |          -0.0029 |          20.0753 |          21.0147 |
[32m[20221214 14:01:46 @agent_ppo2.py:185][0m |          -0.0072 |          15.3720 |          21.0009 |
[32m[20221214 14:01:46 @agent_ppo2.py:185][0m |          -0.0020 |          13.6546 |          20.9976 |
[32m[20221214 14:01:46 @agent_ppo2.py:185][0m |          -0.0063 |          12.4886 |          21.0030 |
[32m[20221214 14:01:46 @agent_ppo2.py:185][0m |          -0.0041 |          11.6232 |          20.9957 |
[32m[20221214 14:01:46 @agent_ppo2.py:185][0m |          -0.0056 |          10.9753 |          20.9987 |
[32m[20221214 14:01:46 @agent_ppo2.py:185][0m |          -0.0019 |          10.6322 |          20.9936 |
[32m[20221214 14:01:47 @agent_ppo2.py:185][0m |          -0.0042 |          10.0782 |          20.9974 |
[32m[20221214 14:01:47 @agent_ppo2.py:185][0m |          -0.0035 |           9.7013 |          21.0024 |
[32m[20221214 14:01:47 @agent_ppo2.py:185][0m |          -0.0067 |           9.3838 |          21.0008 |
[32m[20221214 14:01:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.32
[32m[20221214 14:01:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 324.71
[32m[20221214 14:01:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 232.37
[32m[20221214 14:01:47 @agent_ppo2.py:143][0m Total time:       3.75 min
[32m[20221214 14:01:47 @agent_ppo2.py:145][0m 333824 total steps have happened
[32m[20221214 14:01:47 @agent_ppo2.py:121][0m #------------------------ Iteration 163 --------------------------#
[32m[20221214 14:01:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:47 @agent_ppo2.py:185][0m |          -0.0014 |          24.2604 |          21.0836 |
[32m[20221214 14:01:47 @agent_ppo2.py:185][0m |          -0.0013 |          18.7206 |          21.0783 |
[32m[20221214 14:01:47 @agent_ppo2.py:185][0m |          -0.0016 |          16.5229 |          21.0740 |
[32m[20221214 14:01:48 @agent_ppo2.py:185][0m |          -0.0041 |          15.1551 |          21.0766 |
[32m[20221214 14:01:48 @agent_ppo2.py:185][0m |          -0.0039 |          14.1732 |          21.0721 |
[32m[20221214 14:01:48 @agent_ppo2.py:185][0m |          -0.0025 |          13.5407 |          21.0722 |
[32m[20221214 14:01:48 @agent_ppo2.py:185][0m |          -0.0066 |          13.0246 |          21.0717 |
[32m[20221214 14:01:48 @agent_ppo2.py:185][0m |          -0.0016 |          12.6197 |          21.0717 |
[32m[20221214 14:01:48 @agent_ppo2.py:185][0m |           0.0216 |          14.5119 |          21.0679 |
[32m[20221214 14:01:48 @agent_ppo2.py:185][0m |          -0.0006 |          12.0724 |          21.0704 |
[32m[20221214 14:01:48 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.18
[32m[20221214 14:01:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.81
[32m[20221214 14:01:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 224.55
[32m[20221214 14:01:48 @agent_ppo2.py:143][0m Total time:       3.77 min
[32m[20221214 14:01:48 @agent_ppo2.py:145][0m 335872 total steps have happened
[32m[20221214 14:01:48 @agent_ppo2.py:121][0m #------------------------ Iteration 164 --------------------------#
[32m[20221214 14:01:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:49 @agent_ppo2.py:185][0m |           0.0059 |          31.8462 |          21.0363 |
[32m[20221214 14:01:49 @agent_ppo2.py:185][0m |          -0.0002 |          24.6377 |          21.0317 |
[32m[20221214 14:01:49 @agent_ppo2.py:185][0m |          -0.0036 |          21.9128 |          21.0196 |
[32m[20221214 14:01:49 @agent_ppo2.py:185][0m |          -0.0051 |          20.1789 |          21.0146 |
[32m[20221214 14:01:49 @agent_ppo2.py:185][0m |           0.0099 |          20.5447 |          21.0074 |
[32m[20221214 14:01:49 @agent_ppo2.py:185][0m |          -0.0030 |          18.3290 |          20.9981 |
[32m[20221214 14:01:49 @agent_ppo2.py:185][0m |           0.0014 |          17.7369 |          20.9951 |
[32m[20221214 14:01:49 @agent_ppo2.py:185][0m |          -0.0039 |          17.1485 |          20.9871 |
[32m[20221214 14:01:49 @agent_ppo2.py:185][0m |          -0.0032 |          16.8232 |          20.9816 |
[32m[20221214 14:01:50 @agent_ppo2.py:185][0m |          -0.0041 |          16.2235 |          20.9765 |
[32m[20221214 14:01:50 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:01:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.33
[32m[20221214 14:01:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.62
[32m[20221214 14:01:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.66
[32m[20221214 14:01:50 @agent_ppo2.py:143][0m Total time:       3.79 min
[32m[20221214 14:01:50 @agent_ppo2.py:145][0m 337920 total steps have happened
[32m[20221214 14:01:50 @agent_ppo2.py:121][0m #------------------------ Iteration 165 --------------------------#
[32m[20221214 14:01:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:50 @agent_ppo2.py:185][0m |           0.0083 |          22.6146 |          20.8306 |
[32m[20221214 14:01:50 @agent_ppo2.py:185][0m |          -0.0026 |          17.5581 |          20.8261 |
[32m[20221214 14:01:50 @agent_ppo2.py:185][0m |          -0.0062 |          15.6084 |          20.8160 |
[32m[20221214 14:01:50 @agent_ppo2.py:185][0m |           0.0029 |          14.3073 |          20.8110 |
[32m[20221214 14:01:50 @agent_ppo2.py:185][0m |          -0.0001 |          13.3293 |          20.8062 |
[32m[20221214 14:01:50 @agent_ppo2.py:185][0m |          -0.0021 |          12.5929 |          20.8093 |
[32m[20221214 14:01:51 @agent_ppo2.py:185][0m |          -0.0023 |          11.9630 |          20.7977 |
[32m[20221214 14:01:51 @agent_ppo2.py:185][0m |          -0.0011 |          11.4601 |          20.8000 |
[32m[20221214 14:01:51 @agent_ppo2.py:185][0m |          -0.0049 |          11.0343 |          20.7945 |
[32m[20221214 14:01:51 @agent_ppo2.py:185][0m |          -0.0007 |          10.6937 |          20.7947 |
[32m[20221214 14:01:51 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.24
[32m[20221214 14:01:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 325.67
[32m[20221214 14:01:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 219.67
[32m[20221214 14:01:51 @agent_ppo2.py:143][0m Total time:       3.82 min
[32m[20221214 14:01:51 @agent_ppo2.py:145][0m 339968 total steps have happened
[32m[20221214 14:01:51 @agent_ppo2.py:121][0m #------------------------ Iteration 166 --------------------------#
[32m[20221214 14:01:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:51 @agent_ppo2.py:185][0m |           0.0157 |          21.5444 |          20.9688 |
[32m[20221214 14:01:51 @agent_ppo2.py:185][0m |          -0.0002 |          15.1386 |          20.9597 |
[32m[20221214 14:01:52 @agent_ppo2.py:185][0m |          -0.0045 |          12.8962 |          20.9479 |
[32m[20221214 14:01:52 @agent_ppo2.py:185][0m |          -0.0021 |          11.8386 |          20.9391 |
[32m[20221214 14:01:52 @agent_ppo2.py:185][0m |          -0.0058 |          10.8274 |          20.9290 |
[32m[20221214 14:01:52 @agent_ppo2.py:185][0m |          -0.0060 |          10.1418 |          20.9221 |
[32m[20221214 14:01:52 @agent_ppo2.py:185][0m |          -0.0045 |           9.6080 |          20.9207 |
[32m[20221214 14:01:52 @agent_ppo2.py:185][0m |          -0.0050 |           8.9856 |          20.9155 |
[32m[20221214 14:01:52 @agent_ppo2.py:185][0m |          -0.0022 |           8.6203 |          20.9133 |
[32m[20221214 14:01:52 @agent_ppo2.py:185][0m |          -0.0041 |           8.2297 |          20.9069 |
[32m[20221214 14:01:52 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:01:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 233.52
[32m[20221214 14:01:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.95
[32m[20221214 14:01:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 214.47
[32m[20221214 14:01:52 @agent_ppo2.py:143][0m Total time:       3.84 min
[32m[20221214 14:01:52 @agent_ppo2.py:145][0m 342016 total steps have happened
[32m[20221214 14:01:52 @agent_ppo2.py:121][0m #------------------------ Iteration 167 --------------------------#
[32m[20221214 14:01:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:53 @agent_ppo2.py:185][0m |          -0.0064 |          25.5383 |          20.9463 |
[32m[20221214 14:01:53 @agent_ppo2.py:185][0m |          -0.0003 |          20.8754 |          20.9401 |
[32m[20221214 14:01:53 @agent_ppo2.py:185][0m |          -0.0042 |          19.0172 |          20.9335 |
[32m[20221214 14:01:53 @agent_ppo2.py:185][0m |          -0.0055 |          18.0500 |          20.9231 |
[32m[20221214 14:01:53 @agent_ppo2.py:185][0m |          -0.0023 |          17.1004 |          20.9142 |
[32m[20221214 14:01:53 @agent_ppo2.py:185][0m |          -0.0044 |          16.4117 |          20.9087 |
[32m[20221214 14:01:53 @agent_ppo2.py:185][0m |          -0.0032 |          15.7762 |          20.9076 |
[32m[20221214 14:01:53 @agent_ppo2.py:185][0m |          -0.0073 |          15.6946 |          20.9038 |
[32m[20221214 14:01:54 @agent_ppo2.py:185][0m |          -0.0082 |          15.1571 |          20.8981 |
[32m[20221214 14:01:54 @agent_ppo2.py:185][0m |           0.0024 |          16.0478 |          20.8964 |
[32m[20221214 14:01:54 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.13
[32m[20221214 14:01:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.36
[32m[20221214 14:01:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.16
[32m[20221214 14:01:54 @agent_ppo2.py:143][0m Total time:       3.86 min
[32m[20221214 14:01:54 @agent_ppo2.py:145][0m 344064 total steps have happened
[32m[20221214 14:01:54 @agent_ppo2.py:121][0m #------------------------ Iteration 168 --------------------------#
[32m[20221214 14:01:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:54 @agent_ppo2.py:185][0m |           0.0006 |          29.4580 |          20.9328 |
[32m[20221214 14:01:54 @agent_ppo2.py:185][0m |          -0.0025 |          23.7573 |          20.9270 |
[32m[20221214 14:01:54 @agent_ppo2.py:185][0m |           0.0014 |          21.1501 |          20.9267 |
[32m[20221214 14:01:54 @agent_ppo2.py:185][0m |          -0.0034 |          19.0995 |          20.9255 |
[32m[20221214 14:01:55 @agent_ppo2.py:185][0m |          -0.0024 |          17.6347 |          20.9253 |
[32m[20221214 14:01:55 @agent_ppo2.py:185][0m |          -0.0022 |          16.6518 |          20.9247 |
[32m[20221214 14:01:55 @agent_ppo2.py:185][0m |          -0.0010 |          15.7842 |          20.9255 |
[32m[20221214 14:01:55 @agent_ppo2.py:185][0m |          -0.0033 |          15.2301 |          20.9260 |
[32m[20221214 14:01:55 @agent_ppo2.py:185][0m |          -0.0037 |          14.5653 |          20.9250 |
[32m[20221214 14:01:55 @agent_ppo2.py:185][0m |          -0.0012 |          14.0311 |          20.9233 |
[32m[20221214 14:01:55 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:01:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.27
[32m[20221214 14:01:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.43
[32m[20221214 14:01:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 222.60
[32m[20221214 14:01:55 @agent_ppo2.py:143][0m Total time:       3.89 min
[32m[20221214 14:01:55 @agent_ppo2.py:145][0m 346112 total steps have happened
[32m[20221214 14:01:55 @agent_ppo2.py:121][0m #------------------------ Iteration 169 --------------------------#
[32m[20221214 14:01:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:56 @agent_ppo2.py:185][0m |          -0.0018 |          31.1275 |          21.0102 |
[32m[20221214 14:01:56 @agent_ppo2.py:185][0m |          -0.0077 |          24.0753 |          21.0121 |
[32m[20221214 14:01:56 @agent_ppo2.py:185][0m |          -0.0035 |          21.1526 |          21.0031 |
[32m[20221214 14:01:56 @agent_ppo2.py:185][0m |           0.0003 |          19.2732 |          21.0014 |
[32m[20221214 14:01:56 @agent_ppo2.py:185][0m |          -0.0041 |          18.1572 |          20.9993 |
[32m[20221214 14:01:56 @agent_ppo2.py:185][0m |           0.0004 |          17.3794 |          20.9953 |
[32m[20221214 14:01:56 @agent_ppo2.py:185][0m |          -0.0048 |          16.4942 |          20.9997 |
[32m[20221214 14:01:56 @agent_ppo2.py:185][0m |          -0.0047 |          15.8552 |          20.9929 |
[32m[20221214 14:01:56 @agent_ppo2.py:185][0m |          -0.0034 |          15.2686 |          20.9960 |
[32m[20221214 14:01:56 @agent_ppo2.py:185][0m |          -0.0047 |          14.8377 |          20.9963 |
[32m[20221214 14:01:56 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.32
[32m[20221214 14:01:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 352.66
[32m[20221214 14:01:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 195.59
[32m[20221214 14:01:57 @agent_ppo2.py:143][0m Total time:       3.91 min
[32m[20221214 14:01:57 @agent_ppo2.py:145][0m 348160 total steps have happened
[32m[20221214 14:01:57 @agent_ppo2.py:121][0m #------------------------ Iteration 170 --------------------------#
[32m[20221214 14:01:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:01:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:57 @agent_ppo2.py:185][0m |          -0.0013 |          21.2931 |          20.9227 |
[32m[20221214 14:01:57 @agent_ppo2.py:185][0m |          -0.0058 |          17.8516 |          20.9252 |
[32m[20221214 14:01:57 @agent_ppo2.py:185][0m |          -0.0027 |          16.3818 |          20.9206 |
[32m[20221214 14:01:57 @agent_ppo2.py:185][0m |          -0.0011 |          15.1271 |          20.9183 |
[32m[20221214 14:01:57 @agent_ppo2.py:185][0m |          -0.0016 |          14.3294 |          20.9211 |
[32m[20221214 14:01:57 @agent_ppo2.py:185][0m |          -0.0031 |          13.7125 |          20.9202 |
[32m[20221214 14:01:57 @agent_ppo2.py:185][0m |           0.0012 |          13.2332 |          20.9214 |
[32m[20221214 14:01:58 @agent_ppo2.py:185][0m |          -0.0040 |          12.7226 |          20.9255 |
[32m[20221214 14:01:58 @agent_ppo2.py:185][0m |          -0.0040 |          12.4214 |          20.9224 |
[32m[20221214 14:01:58 @agent_ppo2.py:185][0m |          -0.0040 |          12.1109 |          20.9264 |
[32m[20221214 14:01:58 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.43
[32m[20221214 14:01:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 361.78
[32m[20221214 14:01:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.44
[32m[20221214 14:01:58 @agent_ppo2.py:143][0m Total time:       3.93 min
[32m[20221214 14:01:58 @agent_ppo2.py:145][0m 350208 total steps have happened
[32m[20221214 14:01:58 @agent_ppo2.py:121][0m #------------------------ Iteration 171 --------------------------#
[32m[20221214 14:01:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:01:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:01:58 @agent_ppo2.py:185][0m |          -0.0034 |          33.5839 |          20.9329 |
[32m[20221214 14:01:58 @agent_ppo2.py:185][0m |          -0.0016 |          27.8733 |          20.9311 |
[32m[20221214 14:01:58 @agent_ppo2.py:185][0m |          -0.0081 |          25.2329 |          20.9170 |
[32m[20221214 14:01:59 @agent_ppo2.py:185][0m |          -0.0004 |          23.2141 |          20.9069 |
[32m[20221214 14:01:59 @agent_ppo2.py:185][0m |          -0.0016 |          21.7013 |          20.8920 |
[32m[20221214 14:01:59 @agent_ppo2.py:185][0m |          -0.0037 |          20.7275 |          20.8894 |
[32m[20221214 14:01:59 @agent_ppo2.py:185][0m |          -0.0059 |          19.8599 |          20.8800 |
[32m[20221214 14:01:59 @agent_ppo2.py:185][0m |          -0.0041 |          19.1129 |          20.8734 |
[32m[20221214 14:01:59 @agent_ppo2.py:185][0m |          -0.0018 |          18.5065 |          20.8711 |
[32m[20221214 14:01:59 @agent_ppo2.py:185][0m |          -0.0050 |          18.0084 |          20.8667 |
[32m[20221214 14:01:59 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:01:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 319.94
[32m[20221214 14:01:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.12
[32m[20221214 14:01:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.62
[32m[20221214 14:01:59 @agent_ppo2.py:143][0m Total time:       3.95 min
[32m[20221214 14:01:59 @agent_ppo2.py:145][0m 352256 total steps have happened
[32m[20221214 14:01:59 @agent_ppo2.py:121][0m #------------------------ Iteration 172 --------------------------#
[32m[20221214 14:01:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:00 @agent_ppo2.py:185][0m |          -0.0040 |          23.1423 |          20.8690 |
[32m[20221214 14:02:00 @agent_ppo2.py:185][0m |          -0.0035 |          18.4014 |          20.8603 |
[32m[20221214 14:02:00 @agent_ppo2.py:185][0m |           0.0026 |          16.4846 |          20.8585 |
[32m[20221214 14:02:00 @agent_ppo2.py:185][0m |          -0.0028 |          14.9537 |          20.8527 |
[32m[20221214 14:02:00 @agent_ppo2.py:185][0m |          -0.0006 |          14.0271 |          20.8480 |
[32m[20221214 14:02:00 @agent_ppo2.py:185][0m |          -0.0028 |          13.1920 |          20.8484 |
[32m[20221214 14:02:00 @agent_ppo2.py:185][0m |          -0.0044 |          12.6815 |          20.8411 |
[32m[20221214 14:02:00 @agent_ppo2.py:185][0m |          -0.0062 |          12.3060 |          20.8454 |
[32m[20221214 14:02:00 @agent_ppo2.py:185][0m |          -0.0040 |          11.9678 |          20.8364 |
[32m[20221214 14:02:00 @agent_ppo2.py:185][0m |          -0.0047 |          11.6921 |          20.8394 |
[32m[20221214 14:02:00 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:02:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.01
[32m[20221214 14:02:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.62
[32m[20221214 14:02:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.42
[32m[20221214 14:02:01 @agent_ppo2.py:143][0m Total time:       3.98 min
[32m[20221214 14:02:01 @agent_ppo2.py:145][0m 354304 total steps have happened
[32m[20221214 14:02:01 @agent_ppo2.py:121][0m #------------------------ Iteration 173 --------------------------#
[32m[20221214 14:02:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:01 @agent_ppo2.py:185][0m |           0.0021 |          27.3537 |          20.9354 |
[32m[20221214 14:02:01 @agent_ppo2.py:185][0m |          -0.0020 |          22.6455 |          20.9259 |
[32m[20221214 14:02:01 @agent_ppo2.py:185][0m |          -0.0053 |          21.0469 |          20.9153 |
[32m[20221214 14:02:01 @agent_ppo2.py:185][0m |          -0.0011 |          19.8821 |          20.9043 |
[32m[20221214 14:02:01 @agent_ppo2.py:185][0m |           0.0012 |          19.1675 |          20.8955 |
[32m[20221214 14:02:01 @agent_ppo2.py:185][0m |          -0.0021 |          18.4625 |          20.8890 |
[32m[20221214 14:02:02 @agent_ppo2.py:185][0m |          -0.0048 |          17.9121 |          20.8853 |
[32m[20221214 14:02:02 @agent_ppo2.py:185][0m |          -0.0006 |          17.4903 |          20.8755 |
[32m[20221214 14:02:02 @agent_ppo2.py:185][0m |          -0.0042 |          17.0862 |          20.8721 |
[32m[20221214 14:02:02 @agent_ppo2.py:185][0m |           0.0021 |          16.9524 |          20.8667 |
[32m[20221214 14:02:02 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:02:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.34
[32m[20221214 14:02:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 325.39
[32m[20221214 14:02:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 249.71
[32m[20221214 14:02:02 @agent_ppo2.py:143][0m Total time:       4.00 min
[32m[20221214 14:02:02 @agent_ppo2.py:145][0m 356352 total steps have happened
[32m[20221214 14:02:02 @agent_ppo2.py:121][0m #------------------------ Iteration 174 --------------------------#
[32m[20221214 14:02:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:02 @agent_ppo2.py:185][0m |           0.0025 |          22.9373 |          20.8864 |
[32m[20221214 14:02:02 @agent_ppo2.py:185][0m |           0.0083 |          19.0515 |          20.8739 |
[32m[20221214 14:02:03 @agent_ppo2.py:185][0m |          -0.0052 |          15.6200 |          20.8601 |
[32m[20221214 14:02:03 @agent_ppo2.py:185][0m |          -0.0052 |          14.1262 |          20.8525 |
[32m[20221214 14:02:03 @agent_ppo2.py:185][0m |          -0.0021 |          12.8288 |          20.8453 |
[32m[20221214 14:02:03 @agent_ppo2.py:185][0m |          -0.0032 |          11.8776 |          20.8397 |
[32m[20221214 14:02:03 @agent_ppo2.py:185][0m |          -0.0084 |          11.0979 |          20.8355 |
[32m[20221214 14:02:03 @agent_ppo2.py:185][0m |          -0.0074 |          10.5545 |          20.8331 |
[32m[20221214 14:02:03 @agent_ppo2.py:185][0m |          -0.0008 |          10.0044 |          20.8261 |
[32m[20221214 14:02:03 @agent_ppo2.py:185][0m |          -0.0063 |           9.5823 |          20.8216 |
[32m[20221214 14:02:03 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.76
[32m[20221214 14:02:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.53
[32m[20221214 14:02:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.51
[32m[20221214 14:02:03 @agent_ppo2.py:143][0m Total time:       4.02 min
[32m[20221214 14:02:03 @agent_ppo2.py:145][0m 358400 total steps have happened
[32m[20221214 14:02:03 @agent_ppo2.py:121][0m #------------------------ Iteration 175 --------------------------#
[32m[20221214 14:02:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:04 @agent_ppo2.py:185][0m |          -0.0020 |          21.2534 |          20.8218 |
[32m[20221214 14:02:04 @agent_ppo2.py:185][0m |           0.0095 |          19.5977 |          20.8111 |
[32m[20221214 14:02:04 @agent_ppo2.py:185][0m |           0.0000 |          15.5945 |          20.8097 |
[32m[20221214 14:02:04 @agent_ppo2.py:185][0m |          -0.0043 |          14.2697 |          20.8094 |
[32m[20221214 14:02:04 @agent_ppo2.py:185][0m |          -0.0075 |          13.4591 |          20.8087 |
[32m[20221214 14:02:04 @agent_ppo2.py:185][0m |          -0.0013 |          12.8861 |          20.8054 |
[32m[20221214 14:02:04 @agent_ppo2.py:185][0m |          -0.0043 |          12.3405 |          20.8033 |
[32m[20221214 14:02:04 @agent_ppo2.py:185][0m |          -0.0079 |          11.9118 |          20.7968 |
[32m[20221214 14:02:05 @agent_ppo2.py:185][0m |          -0.0030 |          11.5925 |          20.8005 |
[32m[20221214 14:02:05 @agent_ppo2.py:185][0m |          -0.0098 |          11.2557 |          20.7944 |
[32m[20221214 14:02:05 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.04
[32m[20221214 14:02:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.06
[32m[20221214 14:02:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 292.95
[32m[20221214 14:02:05 @agent_ppo2.py:143][0m Total time:       4.04 min
[32m[20221214 14:02:05 @agent_ppo2.py:145][0m 360448 total steps have happened
[32m[20221214 14:02:05 @agent_ppo2.py:121][0m #------------------------ Iteration 176 --------------------------#
[32m[20221214 14:02:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:05 @agent_ppo2.py:185][0m |          -0.0021 |          22.8632 |          20.8376 |
[32m[20221214 14:02:05 @agent_ppo2.py:185][0m |           0.0018 |          18.1102 |          20.8323 |
[32m[20221214 14:02:05 @agent_ppo2.py:185][0m |          -0.0023 |          16.2618 |          20.8298 |
[32m[20221214 14:02:05 @agent_ppo2.py:185][0m |           0.0054 |          15.6612 |          20.8186 |
[32m[20221214 14:02:05 @agent_ppo2.py:185][0m |          -0.0069 |          14.1665 |          20.8132 |
[32m[20221214 14:02:06 @agent_ppo2.py:185][0m |          -0.0017 |          13.6215 |          20.8080 |
[32m[20221214 14:02:06 @agent_ppo2.py:185][0m |          -0.0006 |          13.0260 |          20.8012 |
[32m[20221214 14:02:06 @agent_ppo2.py:185][0m |           0.0072 |          12.8778 |          20.7961 |
[32m[20221214 14:02:06 @agent_ppo2.py:185][0m |          -0.0026 |          12.2509 |          20.7898 |
[32m[20221214 14:02:06 @agent_ppo2.py:185][0m |           0.0061 |          14.1048 |          20.7879 |
[32m[20221214 14:02:06 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.25
[32m[20221214 14:02:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.34
[32m[20221214 14:02:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 322.12
[32m[20221214 14:02:06 @agent_ppo2.py:143][0m Total time:       4.07 min
[32m[20221214 14:02:06 @agent_ppo2.py:145][0m 362496 total steps have happened
[32m[20221214 14:02:06 @agent_ppo2.py:121][0m #------------------------ Iteration 177 --------------------------#
[32m[20221214 14:02:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:07 @agent_ppo2.py:185][0m |          -0.0005 |          18.7439 |          20.8207 |
[32m[20221214 14:02:07 @agent_ppo2.py:185][0m |          -0.0001 |          15.8674 |          20.8175 |
[32m[20221214 14:02:07 @agent_ppo2.py:185][0m |          -0.0024 |          14.4182 |          20.8111 |
[32m[20221214 14:02:07 @agent_ppo2.py:185][0m |           0.0033 |          13.6772 |          20.8067 |
[32m[20221214 14:02:07 @agent_ppo2.py:185][0m |           0.0008 |          12.9385 |          20.7995 |
[32m[20221214 14:02:07 @agent_ppo2.py:185][0m |          -0.0024 |          12.0282 |          20.7967 |
[32m[20221214 14:02:07 @agent_ppo2.py:185][0m |          -0.0035 |          11.5424 |          20.7900 |
[32m[20221214 14:02:07 @agent_ppo2.py:185][0m |          -0.0053 |          11.2679 |          20.7882 |
[32m[20221214 14:02:07 @agent_ppo2.py:185][0m |          -0.0050 |          10.8557 |          20.7841 |
[32m[20221214 14:02:07 @agent_ppo2.py:185][0m |          -0.0028 |          10.5572 |          20.7791 |
[32m[20221214 14:02:07 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.40
[32m[20221214 14:02:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.08
[32m[20221214 14:02:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.38
[32m[20221214 14:02:08 @agent_ppo2.py:143][0m Total time:       4.09 min
[32m[20221214 14:02:08 @agent_ppo2.py:145][0m 364544 total steps have happened
[32m[20221214 14:02:08 @agent_ppo2.py:121][0m #------------------------ Iteration 178 --------------------------#
[32m[20221214 14:02:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:08 @agent_ppo2.py:185][0m |          -0.0004 |          19.5752 |          20.8905 |
[32m[20221214 14:02:08 @agent_ppo2.py:185][0m |          -0.0035 |          16.2821 |          20.8790 |
[32m[20221214 14:02:08 @agent_ppo2.py:185][0m |           0.0022 |          15.2842 |          20.8661 |
[32m[20221214 14:02:08 @agent_ppo2.py:185][0m |           0.0007 |          14.5489 |          20.8551 |
[32m[20221214 14:02:08 @agent_ppo2.py:185][0m |          -0.0026 |          14.0178 |          20.8429 |
[32m[20221214 14:02:08 @agent_ppo2.py:185][0m |          -0.0029 |          13.7411 |          20.8433 |
[32m[20221214 14:02:08 @agent_ppo2.py:185][0m |          -0.0079 |          13.3863 |          20.8340 |
[32m[20221214 14:02:09 @agent_ppo2.py:185][0m |          -0.0052 |          13.0210 |          20.8310 |
[32m[20221214 14:02:09 @agent_ppo2.py:185][0m |          -0.0025 |          12.7149 |          20.8332 |
[32m[20221214 14:02:09 @agent_ppo2.py:185][0m |          -0.0036 |          12.4549 |          20.8321 |
[32m[20221214 14:02:09 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 297.76
[32m[20221214 14:02:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 324.28
[32m[20221214 14:02:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.96
[32m[20221214 14:02:09 @agent_ppo2.py:143][0m Total time:       4.11 min
[32m[20221214 14:02:09 @agent_ppo2.py:145][0m 366592 total steps have happened
[32m[20221214 14:02:09 @agent_ppo2.py:121][0m #------------------------ Iteration 179 --------------------------#
[32m[20221214 14:02:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:09 @agent_ppo2.py:185][0m |           0.0016 |          17.1236 |          20.8811 |
[32m[20221214 14:02:09 @agent_ppo2.py:185][0m |           0.0003 |          14.7107 |          20.8821 |
[32m[20221214 14:02:09 @agent_ppo2.py:185][0m |          -0.0031 |          13.1557 |          20.8821 |
[32m[20221214 14:02:10 @agent_ppo2.py:185][0m |          -0.0030 |          12.1462 |          20.8796 |
[32m[20221214 14:02:10 @agent_ppo2.py:185][0m |           0.0038 |          11.6090 |          20.8780 |
[32m[20221214 14:02:10 @agent_ppo2.py:185][0m |          -0.0001 |          10.8644 |          20.8807 |
[32m[20221214 14:02:10 @agent_ppo2.py:185][0m |          -0.0084 |          10.3888 |          20.8850 |
[32m[20221214 14:02:10 @agent_ppo2.py:185][0m |          -0.0037 |          10.0158 |          20.8825 |
[32m[20221214 14:02:10 @agent_ppo2.py:185][0m |           0.0020 |           9.7648 |          20.8831 |
[32m[20221214 14:02:10 @agent_ppo2.py:185][0m |          -0.0031 |           9.4944 |          20.8831 |
[32m[20221214 14:02:10 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.91
[32m[20221214 14:02:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 312.97
[32m[20221214 14:02:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.04
[32m[20221214 14:02:10 @agent_ppo2.py:143][0m Total time:       4.14 min
[32m[20221214 14:02:10 @agent_ppo2.py:145][0m 368640 total steps have happened
[32m[20221214 14:02:10 @agent_ppo2.py:121][0m #------------------------ Iteration 180 --------------------------#
[32m[20221214 14:02:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:02:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:11 @agent_ppo2.py:185][0m |          -0.0005 |          23.1492 |          20.8737 |
[32m[20221214 14:02:11 @agent_ppo2.py:185][0m |          -0.0055 |          17.2366 |          20.8752 |
[32m[20221214 14:02:11 @agent_ppo2.py:185][0m |           0.0004 |          14.5822 |          20.8735 |
[32m[20221214 14:02:11 @agent_ppo2.py:185][0m |          -0.0052 |          12.7635 |          20.8755 |
[32m[20221214 14:02:11 @agent_ppo2.py:185][0m |          -0.0065 |          11.4305 |          20.8745 |
[32m[20221214 14:02:11 @agent_ppo2.py:185][0m |          -0.0038 |          10.5583 |          20.8751 |
[32m[20221214 14:02:11 @agent_ppo2.py:185][0m |          -0.0031 |           9.9079 |          20.8785 |
[32m[20221214 14:02:11 @agent_ppo2.py:185][0m |          -0.0022 |           9.3963 |          20.8799 |
[32m[20221214 14:02:11 @agent_ppo2.py:185][0m |          -0.0039 |           8.9030 |          20.8827 |
[32m[20221214 14:02:11 @agent_ppo2.py:185][0m |          -0.0052 |           8.5152 |          20.8815 |
[32m[20221214 14:02:11 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:02:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.10
[32m[20221214 14:02:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 312.66
[32m[20221214 14:02:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.77
[32m[20221214 14:02:12 @agent_ppo2.py:143][0m Total time:       4.16 min
[32m[20221214 14:02:12 @agent_ppo2.py:145][0m 370688 total steps have happened
[32m[20221214 14:02:12 @agent_ppo2.py:121][0m #------------------------ Iteration 181 --------------------------#
[32m[20221214 14:02:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:12 @agent_ppo2.py:185][0m |          -0.0010 |          21.3767 |          20.8997 |
[32m[20221214 14:02:12 @agent_ppo2.py:185][0m |          -0.0044 |          18.0853 |          20.8962 |
[32m[20221214 14:02:12 @agent_ppo2.py:185][0m |          -0.0044 |          16.6605 |          20.8944 |
[32m[20221214 14:02:12 @agent_ppo2.py:185][0m |          -0.0018 |          15.7772 |          20.8919 |
[32m[20221214 14:02:12 @agent_ppo2.py:185][0m |          -0.0061 |          15.0748 |          20.8869 |
[32m[20221214 14:02:12 @agent_ppo2.py:185][0m |          -0.0038 |          14.5897 |          20.8898 |
[32m[20221214 14:02:13 @agent_ppo2.py:185][0m |          -0.0059 |          14.2093 |          20.8870 |
[32m[20221214 14:02:13 @agent_ppo2.py:185][0m |          -0.0051 |          13.8296 |          20.8903 |
[32m[20221214 14:02:13 @agent_ppo2.py:185][0m |          -0.0036 |          13.4664 |          20.8876 |
[32m[20221214 14:02:13 @agent_ppo2.py:185][0m |          -0.0054 |          13.0866 |          20.8857 |
[32m[20221214 14:02:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:02:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 299.04
[32m[20221214 14:02:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 351.38
[32m[20221214 14:02:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.93
[32m[20221214 14:02:13 @agent_ppo2.py:143][0m Total time:       4.18 min
[32m[20221214 14:02:13 @agent_ppo2.py:145][0m 372736 total steps have happened
[32m[20221214 14:02:13 @agent_ppo2.py:121][0m #------------------------ Iteration 182 --------------------------#
[32m[20221214 14:02:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:13 @agent_ppo2.py:185][0m |          -0.0028 |          23.9783 |          20.7842 |
[32m[20221214 14:02:13 @agent_ppo2.py:185][0m |          -0.0004 |          19.9250 |          20.7737 |
[32m[20221214 14:02:14 @agent_ppo2.py:185][0m |           0.0062 |          18.3637 |          20.7596 |
[32m[20221214 14:02:14 @agent_ppo2.py:185][0m |          -0.0015 |          16.3473 |          20.7546 |
[32m[20221214 14:02:14 @agent_ppo2.py:185][0m |          -0.0022 |          15.3325 |          20.7482 |
[32m[20221214 14:02:14 @agent_ppo2.py:185][0m |          -0.0024 |          14.6519 |          20.7383 |
[32m[20221214 14:02:14 @agent_ppo2.py:185][0m |           0.0006 |          13.9271 |          20.7326 |
[32m[20221214 14:02:14 @agent_ppo2.py:185][0m |          -0.0026 |          13.1666 |          20.7329 |
[32m[20221214 14:02:14 @agent_ppo2.py:185][0m |          -0.0049 |          12.6799 |          20.7231 |
[32m[20221214 14:02:14 @agent_ppo2.py:185][0m |          -0.0076 |          12.1504 |          20.7203 |
[32m[20221214 14:02:14 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.62
[32m[20221214 14:02:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 337.20
[32m[20221214 14:02:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.02
[32m[20221214 14:02:14 @agent_ppo2.py:143][0m Total time:       4.21 min
[32m[20221214 14:02:14 @agent_ppo2.py:145][0m 374784 total steps have happened
[32m[20221214 14:02:14 @agent_ppo2.py:121][0m #------------------------ Iteration 183 --------------------------#
[32m[20221214 14:02:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:15 @agent_ppo2.py:185][0m |          -0.0042 |          21.2130 |          20.8209 |
[32m[20221214 14:02:15 @agent_ppo2.py:185][0m |          -0.0015 |          16.5700 |          20.8088 |
[32m[20221214 14:02:15 @agent_ppo2.py:185][0m |          -0.0035 |          15.0043 |          20.7993 |
[32m[20221214 14:02:15 @agent_ppo2.py:185][0m |          -0.0049 |          14.0531 |          20.7920 |
[32m[20221214 14:02:15 @agent_ppo2.py:185][0m |          -0.0017 |          13.4770 |          20.7818 |
[32m[20221214 14:02:15 @agent_ppo2.py:185][0m |           0.0018 |          13.0111 |          20.7790 |
[32m[20221214 14:02:15 @agent_ppo2.py:185][0m |           0.0092 |          13.5983 |          20.7723 |
[32m[20221214 14:02:15 @agent_ppo2.py:185][0m |          -0.0056 |          12.3323 |          20.7727 |
[32m[20221214 14:02:15 @agent_ppo2.py:185][0m |           0.0021 |          12.0340 |          20.7628 |
[32m[20221214 14:02:16 @agent_ppo2.py:185][0m |          -0.0066 |          11.7715 |          20.7555 |
[32m[20221214 14:02:16 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 319.80
[32m[20221214 14:02:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 376.80
[32m[20221214 14:02:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 195.07
[32m[20221214 14:02:16 @agent_ppo2.py:143][0m Total time:       4.23 min
[32m[20221214 14:02:16 @agent_ppo2.py:145][0m 376832 total steps have happened
[32m[20221214 14:02:16 @agent_ppo2.py:121][0m #------------------------ Iteration 184 --------------------------#
[32m[20221214 14:02:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:16 @agent_ppo2.py:185][0m |           0.0026 |          21.5653 |          20.8357 |
[32m[20221214 14:02:16 @agent_ppo2.py:185][0m |          -0.0051 |          15.1688 |          20.8331 |
[32m[20221214 14:02:16 @agent_ppo2.py:185][0m |           0.0032 |          12.9395 |          20.8305 |
[32m[20221214 14:02:16 @agent_ppo2.py:185][0m |           0.0043 |          11.7782 |          20.8251 |
[32m[20221214 14:02:16 @agent_ppo2.py:185][0m |          -0.0004 |          10.9985 |          20.8189 |
[32m[20221214 14:02:17 @agent_ppo2.py:185][0m |          -0.0033 |          10.4761 |          20.8133 |
[32m[20221214 14:02:17 @agent_ppo2.py:185][0m |          -0.0031 |          10.0353 |          20.8125 |
[32m[20221214 14:02:17 @agent_ppo2.py:185][0m |          -0.0041 |           9.6733 |          20.8032 |
[32m[20221214 14:02:17 @agent_ppo2.py:185][0m |          -0.0062 |           9.3296 |          20.8008 |
[32m[20221214 14:02:17 @agent_ppo2.py:185][0m |           0.0132 |          11.1444 |          20.7948 |
[32m[20221214 14:02:17 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 315.38
[32m[20221214 14:02:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.35
[32m[20221214 14:02:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 211.44
[32m[20221214 14:02:17 @agent_ppo2.py:143][0m Total time:       4.25 min
[32m[20221214 14:02:17 @agent_ppo2.py:145][0m 378880 total steps have happened
[32m[20221214 14:02:17 @agent_ppo2.py:121][0m #------------------------ Iteration 185 --------------------------#
[32m[20221214 14:02:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:17 @agent_ppo2.py:185][0m |          -0.0005 |          22.7655 |          20.8555 |
[32m[20221214 14:02:18 @agent_ppo2.py:185][0m |           0.0066 |          18.7313 |          20.8510 |
[32m[20221214 14:02:18 @agent_ppo2.py:185][0m |          -0.0052 |          16.2330 |          20.8386 |
[32m[20221214 14:02:18 @agent_ppo2.py:185][0m |          -0.0065 |          15.1010 |          20.8378 |
[32m[20221214 14:02:18 @agent_ppo2.py:185][0m |           0.0024 |          14.2063 |          20.8331 |
[32m[20221214 14:02:18 @agent_ppo2.py:185][0m |          -0.0044 |          13.6587 |          20.8246 |
[32m[20221214 14:02:18 @agent_ppo2.py:185][0m |           0.0054 |          13.5588 |          20.8216 |
[32m[20221214 14:02:18 @agent_ppo2.py:185][0m |          -0.0106 |          12.5319 |          20.8154 |
[32m[20221214 14:02:18 @agent_ppo2.py:185][0m |          -0.0006 |          12.1155 |          20.8114 |
[32m[20221214 14:02:18 @agent_ppo2.py:185][0m |          -0.0050 |          11.7953 |          20.8089 |
[32m[20221214 14:02:18 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.02
[32m[20221214 14:02:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 330.89
[32m[20221214 14:02:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.91
[32m[20221214 14:02:19 @agent_ppo2.py:143][0m Total time:       4.27 min
[32m[20221214 14:02:19 @agent_ppo2.py:145][0m 380928 total steps have happened
[32m[20221214 14:02:19 @agent_ppo2.py:121][0m #------------------------ Iteration 186 --------------------------#
[32m[20221214 14:02:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:19 @agent_ppo2.py:185][0m |          -0.0016 |          26.2168 |          20.7536 |
[32m[20221214 14:02:19 @agent_ppo2.py:185][0m |          -0.0022 |          21.4881 |          20.7504 |
[32m[20221214 14:02:19 @agent_ppo2.py:185][0m |          -0.0010 |          19.5772 |          20.7464 |
[32m[20221214 14:02:19 @agent_ppo2.py:185][0m |          -0.0036 |          18.1511 |          20.7433 |
[32m[20221214 14:02:19 @agent_ppo2.py:185][0m |           0.0069 |          17.8670 |          20.7431 |
[32m[20221214 14:02:19 @agent_ppo2.py:185][0m |          -0.0034 |          16.7004 |          20.7382 |
[32m[20221214 14:02:19 @agent_ppo2.py:185][0m |          -0.0041 |          16.1451 |          20.7359 |
[32m[20221214 14:02:20 @agent_ppo2.py:185][0m |          -0.0035 |          15.7083 |          20.7363 |
[32m[20221214 14:02:20 @agent_ppo2.py:185][0m |          -0.0048 |          15.3010 |          20.7293 |
[32m[20221214 14:02:20 @agent_ppo2.py:185][0m |          -0.0023 |          14.7617 |          20.7282 |
[32m[20221214 14:02:20 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 294.51
[32m[20221214 14:02:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.27
[32m[20221214 14:02:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.86
[32m[20221214 14:02:20 @agent_ppo2.py:143][0m Total time:       4.30 min
[32m[20221214 14:02:20 @agent_ppo2.py:145][0m 382976 total steps have happened
[32m[20221214 14:02:20 @agent_ppo2.py:121][0m #------------------------ Iteration 187 --------------------------#
[32m[20221214 14:02:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:20 @agent_ppo2.py:185][0m |          -0.0009 |          26.1800 |          20.7357 |
[32m[20221214 14:02:20 @agent_ppo2.py:185][0m |          -0.0080 |          21.1185 |          20.7307 |
[32m[20221214 14:02:20 @agent_ppo2.py:185][0m |          -0.0017 |          19.0724 |          20.7314 |
[32m[20221214 14:02:21 @agent_ppo2.py:185][0m |          -0.0089 |          17.6393 |          20.7339 |
[32m[20221214 14:02:21 @agent_ppo2.py:185][0m |           0.0008 |          17.1968 |          20.7368 |
[32m[20221214 14:02:21 @agent_ppo2.py:185][0m |           0.0043 |          16.3296 |          20.7329 |
[32m[20221214 14:02:21 @agent_ppo2.py:185][0m |           0.0060 |          17.3151 |          20.7345 |
[32m[20221214 14:02:21 @agent_ppo2.py:185][0m |          -0.0043 |          14.6219 |          20.7334 |
[32m[20221214 14:02:21 @agent_ppo2.py:185][0m |          -0.0007 |          14.0685 |          20.7385 |
[32m[20221214 14:02:21 @agent_ppo2.py:185][0m |          -0.0061 |          13.9413 |          20.7416 |
[32m[20221214 14:02:21 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:02:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 327.15
[32m[20221214 14:02:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 346.92
[32m[20221214 14:02:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 232.57
[32m[20221214 14:02:21 @agent_ppo2.py:143][0m Total time:       4.32 min
[32m[20221214 14:02:21 @agent_ppo2.py:145][0m 385024 total steps have happened
[32m[20221214 14:02:21 @agent_ppo2.py:121][0m #------------------------ Iteration 188 --------------------------#
[32m[20221214 14:02:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:22 @agent_ppo2.py:185][0m |           0.0014 |          26.8653 |          20.6764 |
[32m[20221214 14:02:22 @agent_ppo2.py:185][0m |           0.0046 |          20.4175 |          20.6834 |
[32m[20221214 14:02:22 @agent_ppo2.py:185][0m |          -0.0071 |          17.5559 |          20.6805 |
[32m[20221214 14:02:22 @agent_ppo2.py:185][0m |          -0.0063 |          15.7292 |          20.6861 |
[32m[20221214 14:02:22 @agent_ppo2.py:185][0m |          -0.0035 |          14.5525 |          20.6871 |
[32m[20221214 14:02:22 @agent_ppo2.py:185][0m |           0.0034 |          14.4714 |          20.6966 |
[32m[20221214 14:02:22 @agent_ppo2.py:185][0m |           0.0083 |          13.3821 |          20.7048 |
[32m[20221214 14:02:22 @agent_ppo2.py:185][0m |          -0.0068 |          12.2131 |          20.7050 |
[32m[20221214 14:02:22 @agent_ppo2.py:185][0m |          -0.0076 |          11.6574 |          20.7154 |
[32m[20221214 14:02:22 @agent_ppo2.py:185][0m |          -0.0063 |          11.2337 |          20.7234 |
[32m[20221214 14:02:22 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.42
[32m[20221214 14:02:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 312.74
[32m[20221214 14:02:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.28
[32m[20221214 14:02:23 @agent_ppo2.py:143][0m Total time:       4.34 min
[32m[20221214 14:02:23 @agent_ppo2.py:145][0m 387072 total steps have happened
[32m[20221214 14:02:23 @agent_ppo2.py:121][0m #------------------------ Iteration 189 --------------------------#
[32m[20221214 14:02:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:23 @agent_ppo2.py:185][0m |           0.0019 |          29.5391 |          20.8078 |
[32m[20221214 14:02:23 @agent_ppo2.py:185][0m |          -0.0058 |          22.0419 |          20.8064 |
[32m[20221214 14:02:23 @agent_ppo2.py:185][0m |          -0.0050 |          19.1718 |          20.8051 |
[32m[20221214 14:02:23 @agent_ppo2.py:185][0m |           0.0045 |          17.8774 |          20.7995 |
[32m[20221214 14:02:23 @agent_ppo2.py:185][0m |          -0.0028 |          16.2822 |          20.8006 |
[32m[20221214 14:02:23 @agent_ppo2.py:185][0m |          -0.0068 |          15.3324 |          20.7997 |
[32m[20221214 14:02:24 @agent_ppo2.py:185][0m |          -0.0020 |          14.6342 |          20.7984 |
[32m[20221214 14:02:24 @agent_ppo2.py:185][0m |          -0.0006 |          14.0538 |          20.7954 |
[32m[20221214 14:02:24 @agent_ppo2.py:185][0m |          -0.0036 |          13.5319 |          20.7997 |
[32m[20221214 14:02:24 @agent_ppo2.py:185][0m |          -0.0035 |          13.2024 |          20.7974 |
[32m[20221214 14:02:24 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:02:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.14
[32m[20221214 14:02:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 328.97
[32m[20221214 14:02:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.47
[32m[20221214 14:02:24 @agent_ppo2.py:143][0m Total time:       4.37 min
[32m[20221214 14:02:24 @agent_ppo2.py:145][0m 389120 total steps have happened
[32m[20221214 14:02:24 @agent_ppo2.py:121][0m #------------------------ Iteration 190 --------------------------#
[32m[20221214 14:02:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:02:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:24 @agent_ppo2.py:185][0m |           0.0003 |          33.5688 |          20.8089 |
[32m[20221214 14:02:24 @agent_ppo2.py:185][0m |          -0.0016 |          24.5410 |          20.8029 |
[32m[20221214 14:02:25 @agent_ppo2.py:185][0m |          -0.0056 |          20.9111 |          20.7994 |
[32m[20221214 14:02:25 @agent_ppo2.py:185][0m |          -0.0037 |          18.9765 |          20.7910 |
[32m[20221214 14:02:25 @agent_ppo2.py:185][0m |           0.0014 |          17.8552 |          20.7854 |
[32m[20221214 14:02:25 @agent_ppo2.py:185][0m |          -0.0028 |          16.6306 |          20.7842 |
[32m[20221214 14:02:25 @agent_ppo2.py:185][0m |          -0.0003 |          15.8615 |          20.7826 |
[32m[20221214 14:02:25 @agent_ppo2.py:185][0m |           0.0012 |          15.1447 |          20.7823 |
[32m[20221214 14:02:25 @agent_ppo2.py:185][0m |          -0.0040 |          14.7601 |          20.7827 |
[32m[20221214 14:02:25 @agent_ppo2.py:185][0m |          -0.0038 |          14.0121 |          20.7776 |
[32m[20221214 14:02:25 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:02:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.53
[32m[20221214 14:02:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 333.38
[32m[20221214 14:02:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.50
[32m[20221214 14:02:25 @agent_ppo2.py:143][0m Total time:       4.39 min
[32m[20221214 14:02:25 @agent_ppo2.py:145][0m 391168 total steps have happened
[32m[20221214 14:02:25 @agent_ppo2.py:121][0m #------------------------ Iteration 191 --------------------------#
[32m[20221214 14:02:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:26 @agent_ppo2.py:185][0m |          -0.0047 |          24.4608 |          20.7699 |
[32m[20221214 14:02:26 @agent_ppo2.py:185][0m |          -0.0014 |          18.9136 |          20.7688 |
[32m[20221214 14:02:26 @agent_ppo2.py:185][0m |          -0.0017 |          16.7394 |          20.7693 |
[32m[20221214 14:02:26 @agent_ppo2.py:185][0m |          -0.0059 |          15.5082 |          20.7651 |
[32m[20221214 14:02:26 @agent_ppo2.py:185][0m |           0.0025 |          14.6124 |          20.7624 |
[32m[20221214 14:02:26 @agent_ppo2.py:185][0m |          -0.0002 |          13.9946 |          20.7580 |
[32m[20221214 14:02:26 @agent_ppo2.py:185][0m |          -0.0030 |          13.3491 |          20.7562 |
[32m[20221214 14:02:26 @agent_ppo2.py:185][0m |          -0.0030 |          12.9415 |          20.7564 |
[32m[20221214 14:02:27 @agent_ppo2.py:185][0m |          -0.0036 |          12.5947 |          20.7570 |
[32m[20221214 14:02:27 @agent_ppo2.py:185][0m |           0.0135 |          13.5251 |          20.7535 |
[32m[20221214 14:02:27 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.08
[32m[20221214 14:02:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.72
[32m[20221214 14:02:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.86
[32m[20221214 14:02:27 @agent_ppo2.py:143][0m Total time:       4.41 min
[32m[20221214 14:02:27 @agent_ppo2.py:145][0m 393216 total steps have happened
[32m[20221214 14:02:27 @agent_ppo2.py:121][0m #------------------------ Iteration 192 --------------------------#
[32m[20221214 14:02:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:27 @agent_ppo2.py:185][0m |          -0.0056 |          24.0645 |          20.8837 |
[32m[20221214 14:02:27 @agent_ppo2.py:185][0m |          -0.0032 |          17.3663 |          20.8657 |
[32m[20221214 14:02:27 @agent_ppo2.py:185][0m |          -0.0020 |          14.9352 |          20.8484 |
[32m[20221214 14:02:27 @agent_ppo2.py:185][0m |          -0.0053 |          13.6488 |          20.8411 |
[32m[20221214 14:02:27 @agent_ppo2.py:185][0m |          -0.0040 |          12.7171 |          20.8363 |
[32m[20221214 14:02:28 @agent_ppo2.py:185][0m |           0.0014 |          12.4611 |          20.8291 |
[32m[20221214 14:02:28 @agent_ppo2.py:185][0m |          -0.0003 |          11.5548 |          20.8199 |
[32m[20221214 14:02:28 @agent_ppo2.py:185][0m |          -0.0077 |          11.1246 |          20.8132 |
[32m[20221214 14:02:28 @agent_ppo2.py:185][0m |           0.0061 |          11.3554 |          20.8071 |
[32m[20221214 14:02:28 @agent_ppo2.py:185][0m |          -0.0089 |          10.6533 |          20.7963 |
[32m[20221214 14:02:28 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.08
[32m[20221214 14:02:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 368.73
[32m[20221214 14:02:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 337.59
[32m[20221214 14:02:28 @agent_ppo2.py:143][0m Total time:       4.43 min
[32m[20221214 14:02:28 @agent_ppo2.py:145][0m 395264 total steps have happened
[32m[20221214 14:02:28 @agent_ppo2.py:121][0m #------------------------ Iteration 193 --------------------------#
[32m[20221214 14:02:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:28 @agent_ppo2.py:185][0m |          -0.0015 |          28.6141 |          20.7780 |
[32m[20221214 14:02:29 @agent_ppo2.py:185][0m |          -0.0011 |          23.0346 |          20.7655 |
[32m[20221214 14:02:29 @agent_ppo2.py:185][0m |          -0.0034 |          20.8273 |          20.7577 |
[32m[20221214 14:02:29 @agent_ppo2.py:185][0m |          -0.0055 |          19.3151 |          20.7513 |
[32m[20221214 14:02:29 @agent_ppo2.py:185][0m |          -0.0034 |          18.2901 |          20.7480 |
[32m[20221214 14:02:29 @agent_ppo2.py:185][0m |          -0.0037 |          17.5688 |          20.7406 |
[32m[20221214 14:02:29 @agent_ppo2.py:185][0m |           0.0025 |          17.0541 |          20.7359 |
[32m[20221214 14:02:29 @agent_ppo2.py:185][0m |          -0.0024 |          16.3514 |          20.7314 |
[32m[20221214 14:02:29 @agent_ppo2.py:185][0m |          -0.0084 |          15.8413 |          20.7265 |
[32m[20221214 14:02:29 @agent_ppo2.py:185][0m |          -0.0024 |          15.2580 |          20.7195 |
[32m[20221214 14:02:29 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:02:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.53
[32m[20221214 14:02:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 359.32
[32m[20221214 14:02:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 344.77
[32m[20221214 14:02:30 @agent_ppo2.py:143][0m Total time:       4.46 min
[32m[20221214 14:02:30 @agent_ppo2.py:145][0m 397312 total steps have happened
[32m[20221214 14:02:30 @agent_ppo2.py:121][0m #------------------------ Iteration 194 --------------------------#
[32m[20221214 14:02:30 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:02:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:30 @agent_ppo2.py:185][0m |          -0.0042 |          23.6579 |          20.7188 |
[32m[20221214 14:02:30 @agent_ppo2.py:185][0m |           0.0083 |          18.6510 |          20.7227 |
[32m[20221214 14:02:30 @agent_ppo2.py:185][0m |           0.0063 |          15.5820 |          20.7192 |
[32m[20221214 14:02:30 @agent_ppo2.py:185][0m |           0.0000 |          13.5983 |          20.7202 |
[32m[20221214 14:02:30 @agent_ppo2.py:185][0m |          -0.0052 |          12.6397 |          20.7209 |
[32m[20221214 14:02:30 @agent_ppo2.py:185][0m |           0.0001 |          11.7850 |          20.7204 |
[32m[20221214 14:02:30 @agent_ppo2.py:185][0m |          -0.0003 |          11.2346 |          20.7214 |
[32m[20221214 14:02:31 @agent_ppo2.py:185][0m |          -0.0048 |          10.6500 |          20.7220 |
[32m[20221214 14:02:31 @agent_ppo2.py:185][0m |          -0.0029 |          10.1587 |          20.7204 |
[32m[20221214 14:02:31 @agent_ppo2.py:185][0m |          -0.0057 |           9.7683 |          20.7211 |
[32m[20221214 14:02:31 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 321.39
[32m[20221214 14:02:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.84
[32m[20221214 14:02:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.98
[32m[20221214 14:02:31 @agent_ppo2.py:143][0m Total time:       4.48 min
[32m[20221214 14:02:31 @agent_ppo2.py:145][0m 399360 total steps have happened
[32m[20221214 14:02:31 @agent_ppo2.py:121][0m #------------------------ Iteration 195 --------------------------#
[32m[20221214 14:02:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:31 @agent_ppo2.py:185][0m |           0.0076 |          25.9132 |          20.7665 |
[32m[20221214 14:02:31 @agent_ppo2.py:185][0m |          -0.0017 |          21.3486 |          20.7559 |
[32m[20221214 14:02:31 @agent_ppo2.py:185][0m |          -0.0001 |          19.3747 |          20.7448 |
[32m[20221214 14:02:32 @agent_ppo2.py:185][0m |          -0.0029 |          18.1949 |          20.7368 |
[32m[20221214 14:02:32 @agent_ppo2.py:185][0m |          -0.0028 |          17.0649 |          20.7357 |
[32m[20221214 14:02:32 @agent_ppo2.py:185][0m |          -0.0022 |          16.1701 |          20.7266 |
[32m[20221214 14:02:32 @agent_ppo2.py:185][0m |          -0.0013 |          15.3989 |          20.7220 |
[32m[20221214 14:02:32 @agent_ppo2.py:185][0m |          -0.0047 |          15.1408 |          20.7092 |
[32m[20221214 14:02:32 @agent_ppo2.py:185][0m |           0.0010 |          14.6726 |          20.7054 |
[32m[20221214 14:02:32 @agent_ppo2.py:185][0m |          -0.0047 |          14.1237 |          20.6952 |
[32m[20221214 14:02:32 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.41
[32m[20221214 14:02:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.14
[32m[20221214 14:02:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 339.45
[32m[20221214 14:02:32 @agent_ppo2.py:143][0m Total time:       4.50 min
[32m[20221214 14:02:32 @agent_ppo2.py:145][0m 401408 total steps have happened
[32m[20221214 14:02:32 @agent_ppo2.py:121][0m #------------------------ Iteration 196 --------------------------#
[32m[20221214 14:02:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:33 @agent_ppo2.py:185][0m |          -0.0014 |          24.9283 |          20.7269 |
[32m[20221214 14:02:33 @agent_ppo2.py:185][0m |          -0.0023 |          18.8824 |          20.7124 |
[32m[20221214 14:02:33 @agent_ppo2.py:185][0m |          -0.0045 |          16.3268 |          20.7114 |
[32m[20221214 14:02:33 @agent_ppo2.py:185][0m |          -0.0043 |          15.5523 |          20.7057 |
[32m[20221214 14:02:33 @agent_ppo2.py:185][0m |           0.0029 |          14.6367 |          20.7040 |
[32m[20221214 14:02:33 @agent_ppo2.py:185][0m |           0.0053 |          14.9428 |          20.7011 |
[32m[20221214 14:02:33 @agent_ppo2.py:185][0m |          -0.0029 |          13.6109 |          20.7018 |
[32m[20221214 14:02:33 @agent_ppo2.py:185][0m |          -0.0041 |          12.9901 |          20.6977 |
[32m[20221214 14:02:33 @agent_ppo2.py:185][0m |          -0.0091 |          12.7273 |          20.6998 |
[32m[20221214 14:02:33 @agent_ppo2.py:185][0m |          -0.0057 |          12.3438 |          20.6942 |
[32m[20221214 14:02:33 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.09
[32m[20221214 14:02:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.94
[32m[20221214 14:02:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 407.19
[32m[20221214 14:02:34 @agent_ppo2.py:143][0m Total time:       4.53 min
[32m[20221214 14:02:34 @agent_ppo2.py:145][0m 403456 total steps have happened
[32m[20221214 14:02:34 @agent_ppo2.py:121][0m #------------------------ Iteration 197 --------------------------#
[32m[20221214 14:02:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:34 @agent_ppo2.py:185][0m |           0.0051 |          33.4757 |          20.7091 |
[32m[20221214 14:02:34 @agent_ppo2.py:185][0m |          -0.0012 |          27.6619 |          20.6959 |
[32m[20221214 14:02:34 @agent_ppo2.py:185][0m |          -0.0090 |          24.0361 |          20.6764 |
[32m[20221214 14:02:34 @agent_ppo2.py:185][0m |          -0.0017 |          21.8936 |          20.6696 |
[32m[20221214 14:02:34 @agent_ppo2.py:185][0m |          -0.0045 |          20.2463 |          20.6575 |
[32m[20221214 14:02:34 @agent_ppo2.py:185][0m |          -0.0072 |          19.0208 |          20.6473 |
[32m[20221214 14:02:35 @agent_ppo2.py:185][0m |          -0.0089 |          18.1571 |          20.6368 |
[32m[20221214 14:02:35 @agent_ppo2.py:185][0m |          -0.0059 |          17.3918 |          20.6274 |
[32m[20221214 14:02:35 @agent_ppo2.py:185][0m |          -0.0031 |          16.6998 |          20.6228 |
[32m[20221214 14:02:35 @agent_ppo2.py:185][0m |          -0.0034 |          16.3527 |          20.6151 |
[32m[20221214 14:02:35 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 320.72
[32m[20221214 14:02:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.83
[32m[20221214 14:02:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 322.70
[32m[20221214 14:02:35 @agent_ppo2.py:143][0m Total time:       4.55 min
[32m[20221214 14:02:35 @agent_ppo2.py:145][0m 405504 total steps have happened
[32m[20221214 14:02:35 @agent_ppo2.py:121][0m #------------------------ Iteration 198 --------------------------#
[32m[20221214 14:02:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:35 @agent_ppo2.py:185][0m |          -0.0048 |          33.4078 |          20.5639 |
[32m[20221214 14:02:35 @agent_ppo2.py:185][0m |          -0.0016 |          25.9814 |          20.5612 |
[32m[20221214 14:02:36 @agent_ppo2.py:185][0m |           0.0037 |          23.5003 |          20.5643 |
[32m[20221214 14:02:36 @agent_ppo2.py:185][0m |          -0.0010 |          21.2098 |          20.5624 |
[32m[20221214 14:02:36 @agent_ppo2.py:185][0m |          -0.0017 |          19.9489 |          20.5598 |
[32m[20221214 14:02:36 @agent_ppo2.py:185][0m |          -0.0045 |          19.0699 |          20.5634 |
[32m[20221214 14:02:36 @agent_ppo2.py:185][0m |          -0.0051 |          18.3944 |          20.5661 |
[32m[20221214 14:02:36 @agent_ppo2.py:185][0m |          -0.0034 |          17.9477 |          20.5635 |
[32m[20221214 14:02:36 @agent_ppo2.py:185][0m |          -0.0001 |          17.5460 |          20.5613 |
[32m[20221214 14:02:36 @agent_ppo2.py:185][0m |          -0.0082 |          17.0299 |          20.5640 |
[32m[20221214 14:02:36 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 338.88
[32m[20221214 14:02:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 395.00
[32m[20221214 14:02:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 310.99
[32m[20221214 14:02:36 @agent_ppo2.py:143][0m Total time:       4.57 min
[32m[20221214 14:02:36 @agent_ppo2.py:145][0m 407552 total steps have happened
[32m[20221214 14:02:36 @agent_ppo2.py:121][0m #------------------------ Iteration 199 --------------------------#
[32m[20221214 14:02:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:37 @agent_ppo2.py:185][0m |           0.0010 |          39.0184 |          20.6682 |
[32m[20221214 14:02:37 @agent_ppo2.py:185][0m |          -0.0040 |          31.0261 |          20.6604 |
[32m[20221214 14:02:37 @agent_ppo2.py:185][0m |          -0.0024 |          26.8552 |          20.6513 |
[32m[20221214 14:02:37 @agent_ppo2.py:185][0m |          -0.0044 |          24.4121 |          20.6507 |
[32m[20221214 14:02:37 @agent_ppo2.py:185][0m |          -0.0009 |          22.7689 |          20.6495 |
[32m[20221214 14:02:37 @agent_ppo2.py:185][0m |          -0.0071 |          21.2175 |          20.6533 |
[32m[20221214 14:02:37 @agent_ppo2.py:185][0m |          -0.0065 |          20.2118 |          20.6485 |
[32m[20221214 14:02:37 @agent_ppo2.py:185][0m |          -0.0011 |          19.5262 |          20.6450 |
[32m[20221214 14:02:37 @agent_ppo2.py:185][0m |          -0.0094 |          18.6722 |          20.6492 |
[32m[20221214 14:02:38 @agent_ppo2.py:185][0m |          -0.0068 |          18.1571 |          20.6509 |
[32m[20221214 14:02:38 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:02:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 320.15
[32m[20221214 14:02:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 379.67
[32m[20221214 14:02:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.57
[32m[20221214 14:02:38 @agent_ppo2.py:143][0m Total time:       4.59 min
[32m[20221214 14:02:38 @agent_ppo2.py:145][0m 409600 total steps have happened
[32m[20221214 14:02:38 @agent_ppo2.py:121][0m #------------------------ Iteration 200 --------------------------#
[32m[20221214 14:02:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:02:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:38 @agent_ppo2.py:185][0m |           0.0129 |          39.8425 |          20.5746 |
[32m[20221214 14:02:38 @agent_ppo2.py:185][0m |          -0.0010 |          28.1236 |          20.5681 |
[32m[20221214 14:02:38 @agent_ppo2.py:185][0m |           0.0039 |          24.8530 |          20.5719 |
[32m[20221214 14:02:38 @agent_ppo2.py:185][0m |          -0.0003 |          23.0141 |          20.5730 |
[32m[20221214 14:02:38 @agent_ppo2.py:185][0m |          -0.0050 |          21.9350 |          20.5811 |
[32m[20221214 14:02:39 @agent_ppo2.py:185][0m |          -0.0016 |          21.0116 |          20.5827 |
[32m[20221214 14:02:39 @agent_ppo2.py:185][0m |          -0.0061 |          20.2362 |          20.5868 |
[32m[20221214 14:02:39 @agent_ppo2.py:185][0m |          -0.0042 |          19.6087 |          20.5882 |
[32m[20221214 14:02:39 @agent_ppo2.py:185][0m |          -0.0058 |          19.0524 |          20.5868 |
[32m[20221214 14:02:39 @agent_ppo2.py:185][0m |          -0.0071 |          18.6573 |          20.5892 |
[32m[20221214 14:02:39 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:02:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.42
[32m[20221214 14:02:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.73
[32m[20221214 14:02:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.93
[32m[20221214 14:02:39 @agent_ppo2.py:143][0m Total time:       4.62 min
[32m[20221214 14:02:39 @agent_ppo2.py:145][0m 411648 total steps have happened
[32m[20221214 14:02:39 @agent_ppo2.py:121][0m #------------------------ Iteration 201 --------------------------#
[32m[20221214 14:02:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:39 @agent_ppo2.py:185][0m |           0.0011 |          40.1542 |          20.5662 |
[32m[20221214 14:02:40 @agent_ppo2.py:185][0m |          -0.0036 |          32.3737 |          20.5664 |
[32m[20221214 14:02:40 @agent_ppo2.py:185][0m |           0.0004 |          29.2924 |          20.5641 |
[32m[20221214 14:02:40 @agent_ppo2.py:185][0m |          -0.0052 |          27.7294 |          20.5609 |
[32m[20221214 14:02:40 @agent_ppo2.py:185][0m |          -0.0035 |          26.4303 |          20.5605 |
[32m[20221214 14:02:40 @agent_ppo2.py:185][0m |          -0.0022 |          25.4113 |          20.5621 |
[32m[20221214 14:02:40 @agent_ppo2.py:185][0m |          -0.0022 |          24.5066 |          20.5589 |
[32m[20221214 14:02:40 @agent_ppo2.py:185][0m |          -0.0002 |          23.8867 |          20.5583 |
[32m[20221214 14:02:40 @agent_ppo2.py:185][0m |          -0.0026 |          23.0494 |          20.5575 |
[32m[20221214 14:02:40 @agent_ppo2.py:185][0m |          -0.0018 |          22.5816 |          20.5565 |
[32m[20221214 14:02:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:02:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.71
[32m[20221214 14:02:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.92
[32m[20221214 14:02:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 351.03
[32m[20221214 14:02:40 @agent_ppo2.py:143][0m Total time:       4.64 min
[32m[20221214 14:02:40 @agent_ppo2.py:145][0m 413696 total steps have happened
[32m[20221214 14:02:40 @agent_ppo2.py:121][0m #------------------------ Iteration 202 --------------------------#
[32m[20221214 14:02:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:41 @agent_ppo2.py:185][0m |           0.0007 |          33.3758 |          20.6067 |
[32m[20221214 14:02:41 @agent_ppo2.py:185][0m |          -0.0045 |          28.5582 |          20.6116 |
[32m[20221214 14:02:41 @agent_ppo2.py:185][0m |           0.0051 |          26.7899 |          20.6095 |
[32m[20221214 14:02:41 @agent_ppo2.py:185][0m |          -0.0005 |          25.2953 |          20.6133 |
[32m[20221214 14:02:41 @agent_ppo2.py:185][0m |          -0.0010 |          24.0391 |          20.6161 |
[32m[20221214 14:02:41 @agent_ppo2.py:185][0m |          -0.0004 |          23.1924 |          20.6182 |
[32m[20221214 14:02:41 @agent_ppo2.py:185][0m |          -0.0028 |          22.5969 |          20.6222 |
[32m[20221214 14:02:41 @agent_ppo2.py:185][0m |          -0.0012 |          21.9451 |          20.6209 |
[32m[20221214 14:02:42 @agent_ppo2.py:185][0m |          -0.0024 |          21.4248 |          20.6273 |
[32m[20221214 14:02:42 @agent_ppo2.py:185][0m |          -0.0065 |          21.0280 |          20.6269 |
[32m[20221214 14:02:42 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:02:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.89
[32m[20221214 14:02:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.27
[32m[20221214 14:02:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.05
[32m[20221214 14:02:42 @agent_ppo2.py:143][0m Total time:       4.66 min
[32m[20221214 14:02:42 @agent_ppo2.py:145][0m 415744 total steps have happened
[32m[20221214 14:02:42 @agent_ppo2.py:121][0m #------------------------ Iteration 203 --------------------------#
[32m[20221214 14:02:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:42 @agent_ppo2.py:185][0m |          -0.0005 |          35.4599 |          20.7178 |
[32m[20221214 14:02:42 @agent_ppo2.py:185][0m |          -0.0046 |          28.4118 |          20.7158 |
[32m[20221214 14:02:42 @agent_ppo2.py:185][0m |          -0.0038 |          25.3226 |          20.7071 |
[32m[20221214 14:02:42 @agent_ppo2.py:185][0m |          -0.0040 |          23.4213 |          20.7055 |
[32m[20221214 14:02:43 @agent_ppo2.py:185][0m |          -0.0071 |          22.1923 |          20.6989 |
[32m[20221214 14:02:43 @agent_ppo2.py:185][0m |          -0.0062 |          21.0505 |          20.6950 |
[32m[20221214 14:02:43 @agent_ppo2.py:185][0m |          -0.0052 |          20.2483 |          20.6895 |
[32m[20221214 14:02:43 @agent_ppo2.py:185][0m |           0.0017 |          19.5613 |          20.6840 |
[32m[20221214 14:02:43 @agent_ppo2.py:185][0m |          -0.0052 |          18.7343 |          20.6823 |
[32m[20221214 14:02:43 @agent_ppo2.py:185][0m |          -0.0044 |          18.0845 |          20.6768 |
[32m[20221214 14:02:43 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 334.71
[32m[20221214 14:02:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.56
[32m[20221214 14:02:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.82
[32m[20221214 14:02:43 @agent_ppo2.py:143][0m Total time:       4.69 min
[32m[20221214 14:02:43 @agent_ppo2.py:145][0m 417792 total steps have happened
[32m[20221214 14:02:43 @agent_ppo2.py:121][0m #------------------------ Iteration 204 --------------------------#
[32m[20221214 14:02:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:02:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:44 @agent_ppo2.py:185][0m |          -0.0002 |          33.6966 |          20.7241 |
[32m[20221214 14:02:44 @agent_ppo2.py:185][0m |           0.0002 |          24.8066 |          20.7052 |
[32m[20221214 14:02:44 @agent_ppo2.py:185][0m |          -0.0007 |          21.0497 |          20.6847 |
[32m[20221214 14:02:44 @agent_ppo2.py:185][0m |           0.0058 |          19.4016 |          20.6700 |
[32m[20221214 14:02:44 @agent_ppo2.py:185][0m |          -0.0050 |          17.3723 |          20.6677 |
[32m[20221214 14:02:44 @agent_ppo2.py:185][0m |          -0.0065 |          16.2142 |          20.6544 |
[32m[20221214 14:02:44 @agent_ppo2.py:185][0m |          -0.0061 |          15.4382 |          20.6479 |
[32m[20221214 14:02:44 @agent_ppo2.py:185][0m |          -0.0059 |          14.5899 |          20.6380 |
[32m[20221214 14:02:44 @agent_ppo2.py:185][0m |           0.0050 |          14.0546 |          20.6298 |
[32m[20221214 14:02:44 @agent_ppo2.py:185][0m |          -0.0018 |          13.3463 |          20.6318 |
[32m[20221214 14:02:44 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:02:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.77
[32m[20221214 14:02:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 332.32
[32m[20221214 14:02:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.76
[32m[20221214 14:02:45 @agent_ppo2.py:143][0m Total time:       4.71 min
[32m[20221214 14:02:45 @agent_ppo2.py:145][0m 419840 total steps have happened
[32m[20221214 14:02:45 @agent_ppo2.py:121][0m #------------------------ Iteration 205 --------------------------#
[32m[20221214 14:02:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:45 @agent_ppo2.py:185][0m |          -0.0080 |          34.1477 |          20.5715 |
[32m[20221214 14:02:45 @agent_ppo2.py:185][0m |          -0.0011 |          24.8159 |          20.5627 |
[32m[20221214 14:02:45 @agent_ppo2.py:185][0m |          -0.0018 |          21.6837 |          20.5560 |
[32m[20221214 14:02:45 @agent_ppo2.py:185][0m |          -0.0008 |          19.8761 |          20.5535 |
[32m[20221214 14:02:45 @agent_ppo2.py:185][0m |          -0.0067 |          18.7471 |          20.5498 |
[32m[20221214 14:02:45 @agent_ppo2.py:185][0m |          -0.0064 |          17.7993 |          20.5452 |
[32m[20221214 14:02:45 @agent_ppo2.py:185][0m |          -0.0030 |          17.1464 |          20.5395 |
[32m[20221214 14:02:46 @agent_ppo2.py:185][0m |          -0.0028 |          16.4368 |          20.5404 |
[32m[20221214 14:02:46 @agent_ppo2.py:185][0m |          -0.0039 |          15.8695 |          20.5342 |
[32m[20221214 14:02:46 @agent_ppo2.py:185][0m |          -0.0043 |          15.4118 |          20.5317 |
[32m[20221214 14:02:46 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.01
[32m[20221214 14:02:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.09
[32m[20221214 14:02:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.73
[32m[20221214 14:02:46 @agent_ppo2.py:143][0m Total time:       4.73 min
[32m[20221214 14:02:46 @agent_ppo2.py:145][0m 421888 total steps have happened
[32m[20221214 14:02:46 @agent_ppo2.py:121][0m #------------------------ Iteration 206 --------------------------#
[32m[20221214 14:02:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:46 @agent_ppo2.py:185][0m |           0.0024 |          37.4758 |          20.5300 |
[32m[20221214 14:02:46 @agent_ppo2.py:185][0m |           0.0001 |          30.2011 |          20.5337 |
[32m[20221214 14:02:46 @agent_ppo2.py:185][0m |          -0.0027 |          27.8528 |          20.5339 |
[32m[20221214 14:02:47 @agent_ppo2.py:185][0m |           0.0006 |          26.1327 |          20.5364 |
[32m[20221214 14:02:47 @agent_ppo2.py:185][0m |          -0.0068 |          24.8936 |          20.5433 |
[32m[20221214 14:02:47 @agent_ppo2.py:185][0m |          -0.0020 |          23.8573 |          20.5449 |
[32m[20221214 14:02:47 @agent_ppo2.py:185][0m |          -0.0095 |          23.1155 |          20.5494 |
[32m[20221214 14:02:47 @agent_ppo2.py:185][0m |          -0.0055 |          22.3737 |          20.5548 |
[32m[20221214 14:02:47 @agent_ppo2.py:185][0m |           0.0013 |          22.1433 |          20.5595 |
[32m[20221214 14:02:47 @agent_ppo2.py:185][0m |           0.0066 |          21.8218 |          20.5618 |
[32m[20221214 14:02:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 334.43
[32m[20221214 14:02:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.32
[32m[20221214 14:02:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.42
[32m[20221214 14:02:47 @agent_ppo2.py:143][0m Total time:       4.75 min
[32m[20221214 14:02:47 @agent_ppo2.py:145][0m 423936 total steps have happened
[32m[20221214 14:02:47 @agent_ppo2.py:121][0m #------------------------ Iteration 207 --------------------------#
[32m[20221214 14:02:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:48 @agent_ppo2.py:185][0m |          -0.0039 |          31.3005 |          20.6977 |
[32m[20221214 14:02:48 @agent_ppo2.py:185][0m |          -0.0018 |          24.0771 |          20.6960 |
[32m[20221214 14:02:48 @agent_ppo2.py:185][0m |          -0.0029 |          21.2625 |          20.6891 |
[32m[20221214 14:02:48 @agent_ppo2.py:185][0m |          -0.0049 |          19.5360 |          20.6795 |
[32m[20221214 14:02:48 @agent_ppo2.py:185][0m |           0.0007 |          18.7110 |          20.6756 |
[32m[20221214 14:02:48 @agent_ppo2.py:185][0m |          -0.0020 |          17.6802 |          20.6720 |
[32m[20221214 14:02:48 @agent_ppo2.py:185][0m |          -0.0033 |          16.9315 |          20.6633 |
[32m[20221214 14:02:48 @agent_ppo2.py:185][0m |          -0.0016 |          16.3394 |          20.6596 |
[32m[20221214 14:02:48 @agent_ppo2.py:185][0m |          -0.0014 |          15.8010 |          20.6536 |
[32m[20221214 14:02:49 @agent_ppo2.py:185][0m |           0.0016 |          16.7182 |          20.6512 |
[32m[20221214 14:02:49 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.15
[32m[20221214 14:02:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.99
[32m[20221214 14:02:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.19
[32m[20221214 14:02:49 @agent_ppo2.py:143][0m Total time:       4.78 min
[32m[20221214 14:02:49 @agent_ppo2.py:145][0m 425984 total steps have happened
[32m[20221214 14:02:49 @agent_ppo2.py:121][0m #------------------------ Iteration 208 --------------------------#
[32m[20221214 14:02:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:49 @agent_ppo2.py:185][0m |          -0.0034 |          31.8142 |          20.4840 |
[32m[20221214 14:02:49 @agent_ppo2.py:185][0m |           0.0052 |          25.5472 |          20.4876 |
[32m[20221214 14:02:49 @agent_ppo2.py:185][0m |          -0.0014 |          22.7046 |          20.4864 |
[32m[20221214 14:02:49 @agent_ppo2.py:185][0m |           0.0013 |          21.1871 |          20.4865 |
[32m[20221214 14:02:49 @agent_ppo2.py:185][0m |          -0.0024 |          20.0190 |          20.4835 |
[32m[20221214 14:02:50 @agent_ppo2.py:185][0m |          -0.0056 |          19.2872 |          20.4873 |
[32m[20221214 14:02:50 @agent_ppo2.py:185][0m |           0.0019 |          18.5314 |          20.4838 |
[32m[20221214 14:02:50 @agent_ppo2.py:185][0m |          -0.0024 |          17.9477 |          20.4799 |
[32m[20221214 14:02:50 @agent_ppo2.py:185][0m |          -0.0040 |          17.3366 |          20.4835 |
[32m[20221214 14:02:50 @agent_ppo2.py:185][0m |          -0.0007 |          16.9332 |          20.4815 |
[32m[20221214 14:02:50 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.67
[32m[20221214 14:02:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 383.76
[32m[20221214 14:02:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.31
[32m[20221214 14:02:50 @agent_ppo2.py:143][0m Total time:       4.80 min
[32m[20221214 14:02:50 @agent_ppo2.py:145][0m 428032 total steps have happened
[32m[20221214 14:02:50 @agent_ppo2.py:121][0m #------------------------ Iteration 209 --------------------------#
[32m[20221214 14:02:50 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:02:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:50 @agent_ppo2.py:185][0m |          -0.0005 |          31.0069 |          20.5935 |
[32m[20221214 14:02:50 @agent_ppo2.py:185][0m |           0.0079 |          26.4655 |          20.5982 |
[32m[20221214 14:02:51 @agent_ppo2.py:185][0m |          -0.0028 |          23.9196 |          20.5983 |
[32m[20221214 14:02:51 @agent_ppo2.py:185][0m |          -0.0029 |          22.1577 |          20.6039 |
[32m[20221214 14:02:51 @agent_ppo2.py:185][0m |           0.0119 |          23.7171 |          20.6040 |
[32m[20221214 14:02:51 @agent_ppo2.py:185][0m |          -0.0041 |          20.6315 |          20.6129 |
[32m[20221214 14:02:51 @agent_ppo2.py:185][0m |          -0.0081 |          19.8229 |          20.6157 |
[32m[20221214 14:02:51 @agent_ppo2.py:185][0m |          -0.0091 |          19.4524 |          20.6140 |
[32m[20221214 14:02:51 @agent_ppo2.py:185][0m |          -0.0033 |          18.8783 |          20.6195 |
[32m[20221214 14:02:51 @agent_ppo2.py:185][0m |          -0.0059 |          18.4686 |          20.6260 |
[32m[20221214 14:02:51 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:02:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 316.79
[32m[20221214 14:02:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 358.47
[32m[20221214 14:02:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 339.63
[32m[20221214 14:02:51 @agent_ppo2.py:143][0m Total time:       4.82 min
[32m[20221214 14:02:51 @agent_ppo2.py:145][0m 430080 total steps have happened
[32m[20221214 14:02:51 @agent_ppo2.py:121][0m #------------------------ Iteration 210 --------------------------#
[32m[20221214 14:02:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:02:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:52 @agent_ppo2.py:185][0m |          -0.0016 |          29.1179 |          20.5140 |
[32m[20221214 14:02:52 @agent_ppo2.py:185][0m |          -0.0064 |          23.5668 |          20.5036 |
[32m[20221214 14:02:52 @agent_ppo2.py:185][0m |          -0.0009 |          21.4927 |          20.4920 |
[32m[20221214 14:02:52 @agent_ppo2.py:185][0m |          -0.0036 |          20.1500 |          20.4964 |
[32m[20221214 14:02:52 @agent_ppo2.py:185][0m |          -0.0021 |          19.2352 |          20.4936 |
[32m[20221214 14:02:52 @agent_ppo2.py:185][0m |          -0.0030 |          18.5335 |          20.4932 |
[32m[20221214 14:02:52 @agent_ppo2.py:185][0m |          -0.0045 |          17.8783 |          20.4979 |
[32m[20221214 14:02:52 @agent_ppo2.py:185][0m |           0.0113 |          19.7078 |          20.4964 |
[32m[20221214 14:02:53 @agent_ppo2.py:185][0m |          -0.0066 |          16.8416 |          20.4960 |
[32m[20221214 14:02:53 @agent_ppo2.py:185][0m |          -0.0029 |          16.3023 |          20.4977 |
[32m[20221214 14:02:53 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.51
[32m[20221214 14:02:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 338.01
[32m[20221214 14:02:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 354.99
[32m[20221214 14:02:53 @agent_ppo2.py:143][0m Total time:       4.85 min
[32m[20221214 14:02:53 @agent_ppo2.py:145][0m 432128 total steps have happened
[32m[20221214 14:02:53 @agent_ppo2.py:121][0m #------------------------ Iteration 211 --------------------------#
[32m[20221214 14:02:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:53 @agent_ppo2.py:185][0m |           0.0017 |          47.5793 |          20.6156 |
[32m[20221214 14:02:53 @agent_ppo2.py:185][0m |           0.0075 |          42.5176 |          20.6107 |
[32m[20221214 14:02:53 @agent_ppo2.py:185][0m |          -0.0017 |          35.7172 |          20.6009 |
[32m[20221214 14:02:53 @agent_ppo2.py:185][0m |          -0.0015 |          33.5203 |          20.6060 |
[32m[20221214 14:02:54 @agent_ppo2.py:185][0m |          -0.0043 |          32.2617 |          20.6021 |
[32m[20221214 14:02:54 @agent_ppo2.py:185][0m |          -0.0034 |          31.0512 |          20.6081 |
[32m[20221214 14:02:54 @agent_ppo2.py:185][0m |          -0.0033 |          30.4413 |          20.6135 |
[32m[20221214 14:02:54 @agent_ppo2.py:185][0m |           0.0000 |          29.6324 |          20.6141 |
[32m[20221214 14:02:54 @agent_ppo2.py:185][0m |          -0.0002 |          29.1640 |          20.6177 |
[32m[20221214 14:02:54 @agent_ppo2.py:185][0m |          -0.0030 |          28.4121 |          20.6201 |
[32m[20221214 14:02:54 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:02:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.27
[32m[20221214 14:02:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.54
[32m[20221214 14:02:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.27
[32m[20221214 14:02:54 @agent_ppo2.py:143][0m Total time:       4.87 min
[32m[20221214 14:02:54 @agent_ppo2.py:145][0m 434176 total steps have happened
[32m[20221214 14:02:54 @agent_ppo2.py:121][0m #------------------------ Iteration 212 --------------------------#
[32m[20221214 14:02:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:55 @agent_ppo2.py:185][0m |           0.0082 |          39.7820 |          20.6835 |
[32m[20221214 14:02:55 @agent_ppo2.py:185][0m |          -0.0033 |          29.7916 |          20.6714 |
[32m[20221214 14:02:55 @agent_ppo2.py:185][0m |          -0.0046 |          26.2791 |          20.6633 |
[32m[20221214 14:02:55 @agent_ppo2.py:185][0m |          -0.0004 |          23.9390 |          20.6543 |
[32m[20221214 14:02:55 @agent_ppo2.py:185][0m |          -0.0028 |          22.4634 |          20.6449 |
[32m[20221214 14:02:55 @agent_ppo2.py:185][0m |          -0.0016 |          21.3767 |          20.6476 |
[32m[20221214 14:02:55 @agent_ppo2.py:185][0m |          -0.0042 |          20.5329 |          20.6396 |
[32m[20221214 14:02:55 @agent_ppo2.py:185][0m |          -0.0009 |          20.2172 |          20.6414 |
[32m[20221214 14:02:55 @agent_ppo2.py:185][0m |          -0.0045 |          19.0524 |          20.6392 |
[32m[20221214 14:02:55 @agent_ppo2.py:185][0m |          -0.0024 |          18.5136 |          20.6408 |
[32m[20221214 14:02:55 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.60
[32m[20221214 14:02:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.70
[32m[20221214 14:02:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 394.97
[32m[20221214 14:02:56 @agent_ppo2.py:143][0m Total time:       4.89 min
[32m[20221214 14:02:56 @agent_ppo2.py:145][0m 436224 total steps have happened
[32m[20221214 14:02:56 @agent_ppo2.py:121][0m #------------------------ Iteration 213 --------------------------#
[32m[20221214 14:02:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:56 @agent_ppo2.py:185][0m |          -0.0017 |          44.4238 |          20.5979 |
[32m[20221214 14:02:56 @agent_ppo2.py:185][0m |          -0.0039 |          35.1949 |          20.5838 |
[32m[20221214 14:02:56 @agent_ppo2.py:185][0m |          -0.0016 |          31.6903 |          20.5699 |
[32m[20221214 14:02:56 @agent_ppo2.py:185][0m |          -0.0060 |          29.6191 |          20.5743 |
[32m[20221214 14:02:56 @agent_ppo2.py:185][0m |          -0.0034 |          28.2442 |          20.5725 |
[32m[20221214 14:02:56 @agent_ppo2.py:185][0m |          -0.0036 |          27.1615 |          20.5717 |
[32m[20221214 14:02:56 @agent_ppo2.py:185][0m |          -0.0057 |          26.2699 |          20.5759 |
[32m[20221214 14:02:57 @agent_ppo2.py:185][0m |          -0.0032 |          25.3912 |          20.5788 |
[32m[20221214 14:02:57 @agent_ppo2.py:185][0m |          -0.0066 |          24.7741 |          20.5782 |
[32m[20221214 14:02:57 @agent_ppo2.py:185][0m |          -0.0029 |          24.1413 |          20.5833 |
[32m[20221214 14:02:57 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:02:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.40
[32m[20221214 14:02:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.69
[32m[20221214 14:02:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.50
[32m[20221214 14:02:57 @agent_ppo2.py:143][0m Total time:       4.91 min
[32m[20221214 14:02:57 @agent_ppo2.py:145][0m 438272 total steps have happened
[32m[20221214 14:02:57 @agent_ppo2.py:121][0m #------------------------ Iteration 214 --------------------------#
[32m[20221214 14:02:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:57 @agent_ppo2.py:185][0m |          -0.0003 |          40.6481 |          20.5493 |
[32m[20221214 14:02:57 @agent_ppo2.py:185][0m |          -0.0009 |          32.1629 |          20.5257 |
[32m[20221214 14:02:57 @agent_ppo2.py:185][0m |          -0.0027 |          29.2038 |          20.5259 |
[32m[20221214 14:02:58 @agent_ppo2.py:185][0m |          -0.0064 |          27.3957 |          20.5261 |
[32m[20221214 14:02:58 @agent_ppo2.py:185][0m |          -0.0061 |          26.1544 |          20.5240 |
[32m[20221214 14:02:58 @agent_ppo2.py:185][0m |          -0.0061 |          25.1581 |          20.5243 |
[32m[20221214 14:02:58 @agent_ppo2.py:185][0m |          -0.0038 |          24.0515 |          20.5210 |
[32m[20221214 14:02:58 @agent_ppo2.py:185][0m |          -0.0023 |          23.2708 |          20.5152 |
[32m[20221214 14:02:58 @agent_ppo2.py:185][0m |          -0.0067 |          22.2974 |          20.5146 |
[32m[20221214 14:02:58 @agent_ppo2.py:185][0m |          -0.0057 |          21.8362 |          20.5151 |
[32m[20221214 14:02:58 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:02:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.04
[32m[20221214 14:02:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 409.32
[32m[20221214 14:02:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.17
[32m[20221214 14:02:58 @agent_ppo2.py:143][0m Total time:       4.94 min
[32m[20221214 14:02:58 @agent_ppo2.py:145][0m 440320 total steps have happened
[32m[20221214 14:02:58 @agent_ppo2.py:121][0m #------------------------ Iteration 215 --------------------------#
[32m[20221214 14:02:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:02:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:02:59 @agent_ppo2.py:185][0m |           0.0033 |          44.7109 |          20.5471 |
[32m[20221214 14:02:59 @agent_ppo2.py:185][0m |           0.0010 |          35.3875 |          20.5544 |
[32m[20221214 14:02:59 @agent_ppo2.py:185][0m |          -0.0050 |          31.8083 |          20.5544 |
[32m[20221214 14:02:59 @agent_ppo2.py:185][0m |           0.0070 |          29.6392 |          20.5584 |
[32m[20221214 14:02:59 @agent_ppo2.py:185][0m |           0.0005 |          27.4635 |          20.5673 |
[32m[20221214 14:02:59 @agent_ppo2.py:185][0m |           0.0106 |          31.4929 |          20.5665 |
[32m[20221214 14:02:59 @agent_ppo2.py:185][0m |          -0.0002 |          24.7925 |          20.5610 |
[32m[20221214 14:02:59 @agent_ppo2.py:185][0m |          -0.0018 |          23.5716 |          20.5680 |
[32m[20221214 14:02:59 @agent_ppo2.py:185][0m |          -0.0024 |          22.6917 |          20.5761 |
[32m[20221214 14:03:00 @agent_ppo2.py:185][0m |          -0.0039 |          22.0331 |          20.5833 |
[32m[20221214 14:03:00 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.09
[32m[20221214 14:03:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.29
[32m[20221214 14:03:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.66
[32m[20221214 14:03:00 @agent_ppo2.py:143][0m Total time:       4.96 min
[32m[20221214 14:03:00 @agent_ppo2.py:145][0m 442368 total steps have happened
[32m[20221214 14:03:00 @agent_ppo2.py:121][0m #------------------------ Iteration 216 --------------------------#
[32m[20221214 14:03:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:00 @agent_ppo2.py:185][0m |          -0.0033 |          41.9756 |          20.6833 |
[32m[20221214 14:03:00 @agent_ppo2.py:185][0m |          -0.0062 |          31.4612 |          20.6696 |
[32m[20221214 14:03:00 @agent_ppo2.py:185][0m |          -0.0042 |          27.7489 |          20.6477 |
[32m[20221214 14:03:00 @agent_ppo2.py:185][0m |          -0.0063 |          25.5610 |          20.6423 |
[32m[20221214 14:03:00 @agent_ppo2.py:185][0m |          -0.0061 |          24.1546 |          20.6332 |
[32m[20221214 14:03:00 @agent_ppo2.py:185][0m |          -0.0020 |          22.8331 |          20.6282 |
[32m[20221214 14:03:01 @agent_ppo2.py:185][0m |          -0.0049 |          21.8315 |          20.6169 |
[32m[20221214 14:03:01 @agent_ppo2.py:185][0m |           0.0001 |          20.9763 |          20.6153 |
[32m[20221214 14:03:01 @agent_ppo2.py:185][0m |           0.0059 |          21.6416 |          20.6050 |
[32m[20221214 14:03:01 @agent_ppo2.py:185][0m |          -0.0039 |          19.6487 |          20.6067 |
[32m[20221214 14:03:01 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:03:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.84
[32m[20221214 14:03:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.05
[32m[20221214 14:03:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.47
[32m[20221214 14:03:01 @agent_ppo2.py:143][0m Total time:       4.98 min
[32m[20221214 14:03:01 @agent_ppo2.py:145][0m 444416 total steps have happened
[32m[20221214 14:03:01 @agent_ppo2.py:121][0m #------------------------ Iteration 217 --------------------------#
[32m[20221214 14:03:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:01 @agent_ppo2.py:185][0m |           0.0006 |          47.2802 |          20.6189 |
[32m[20221214 14:03:01 @agent_ppo2.py:185][0m |          -0.0030 |          40.9311 |          20.6064 |
[32m[20221214 14:03:02 @agent_ppo2.py:185][0m |           0.0070 |          38.0988 |          20.5978 |
[32m[20221214 14:03:02 @agent_ppo2.py:185][0m |          -0.0008 |          36.0160 |          20.5934 |
[32m[20221214 14:03:02 @agent_ppo2.py:185][0m |          -0.0059 |          34.3901 |          20.5835 |
[32m[20221214 14:03:02 @agent_ppo2.py:185][0m |           0.0003 |          33.3194 |          20.5789 |
[32m[20221214 14:03:02 @agent_ppo2.py:185][0m |          -0.0060 |          32.2524 |          20.5713 |
[32m[20221214 14:03:02 @agent_ppo2.py:185][0m |          -0.0042 |          31.8987 |          20.5634 |
[32m[20221214 14:03:02 @agent_ppo2.py:185][0m |          -0.0033 |          31.1196 |          20.5590 |
[32m[20221214 14:03:02 @agent_ppo2.py:185][0m |          -0.0009 |          30.7210 |          20.5518 |
[32m[20221214 14:03:02 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.30
[32m[20221214 14:03:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.90
[32m[20221214 14:03:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.56
[32m[20221214 14:03:02 @agent_ppo2.py:143][0m Total time:       5.01 min
[32m[20221214 14:03:02 @agent_ppo2.py:145][0m 446464 total steps have happened
[32m[20221214 14:03:02 @agent_ppo2.py:121][0m #------------------------ Iteration 218 --------------------------#
[32m[20221214 14:03:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:03 @agent_ppo2.py:185][0m |          -0.0043 |          33.0570 |          20.5373 |
[32m[20221214 14:03:03 @agent_ppo2.py:185][0m |          -0.0049 |          24.7093 |          20.5205 |
[32m[20221214 14:03:03 @agent_ppo2.py:185][0m |          -0.0026 |          21.7610 |          20.5202 |
[32m[20221214 14:03:03 @agent_ppo2.py:185][0m |          -0.0015 |          20.1650 |          20.5213 |
[32m[20221214 14:03:03 @agent_ppo2.py:185][0m |          -0.0069 |          18.9220 |          20.5332 |
[32m[20221214 14:03:03 @agent_ppo2.py:185][0m |           0.0105 |          22.2048 |          20.5378 |
[32m[20221214 14:03:03 @agent_ppo2.py:185][0m |          -0.0018 |          17.3638 |          20.5498 |
[32m[20221214 14:03:03 @agent_ppo2.py:185][0m |          -0.0031 |          16.6720 |          20.5545 |
[32m[20221214 14:03:04 @agent_ppo2.py:185][0m |          -0.0052 |          16.0744 |          20.5614 |
[32m[20221214 14:03:04 @agent_ppo2.py:185][0m |          -0.0068 |          15.6263 |          20.5684 |
[32m[20221214 14:03:04 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.31
[32m[20221214 14:03:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.35
[32m[20221214 14:03:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.76
[32m[20221214 14:03:04 @agent_ppo2.py:143][0m Total time:       5.03 min
[32m[20221214 14:03:04 @agent_ppo2.py:145][0m 448512 total steps have happened
[32m[20221214 14:03:04 @agent_ppo2.py:121][0m #------------------------ Iteration 219 --------------------------#
[32m[20221214 14:03:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:04 @agent_ppo2.py:185][0m |          -0.0025 |          45.3909 |          20.5774 |
[32m[20221214 14:03:04 @agent_ppo2.py:185][0m |          -0.0013 |          41.0471 |          20.5697 |
[32m[20221214 14:03:04 @agent_ppo2.py:185][0m |          -0.0051 |          38.6313 |          20.5632 |
[32m[20221214 14:03:04 @agent_ppo2.py:185][0m |           0.0083 |          39.7743 |          20.5647 |
[32m[20221214 14:03:04 @agent_ppo2.py:185][0m |          -0.0028 |          35.7200 |          20.5625 |
[32m[20221214 14:03:05 @agent_ppo2.py:185][0m |          -0.0059 |          34.7442 |          20.5680 |
[32m[20221214 14:03:05 @agent_ppo2.py:185][0m |          -0.0037 |          33.9210 |          20.5693 |
[32m[20221214 14:03:05 @agent_ppo2.py:185][0m |          -0.0037 |          33.2670 |          20.5652 |
[32m[20221214 14:03:05 @agent_ppo2.py:185][0m |          -0.0039 |          32.6861 |          20.5656 |
[32m[20221214 14:03:05 @agent_ppo2.py:185][0m |          -0.0017 |          32.3097 |          20.5676 |
[32m[20221214 14:03:05 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.35
[32m[20221214 14:03:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.47
[32m[20221214 14:03:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.16
[32m[20221214 14:03:05 @agent_ppo2.py:143][0m Total time:       5.05 min
[32m[20221214 14:03:05 @agent_ppo2.py:145][0m 450560 total steps have happened
[32m[20221214 14:03:05 @agent_ppo2.py:121][0m #------------------------ Iteration 220 --------------------------#
[32m[20221214 14:03:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:03:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:06 @agent_ppo2.py:185][0m |           0.0062 |          41.6430 |          20.6413 |
[32m[20221214 14:03:06 @agent_ppo2.py:185][0m |          -0.0033 |          34.4681 |          20.6327 |
[32m[20221214 14:03:06 @agent_ppo2.py:185][0m |           0.0014 |          32.6906 |          20.6304 |
[32m[20221214 14:03:06 @agent_ppo2.py:185][0m |          -0.0027 |          30.4042 |          20.6289 |
[32m[20221214 14:03:06 @agent_ppo2.py:185][0m |           0.0057 |          30.3144 |          20.6256 |
[32m[20221214 14:03:06 @agent_ppo2.py:185][0m |          -0.0032 |          27.9338 |          20.6210 |
[32m[20221214 14:03:06 @agent_ppo2.py:185][0m |           0.0210 |          35.2337 |          20.6229 |
[32m[20221214 14:03:06 @agent_ppo2.py:185][0m |          -0.0061 |          26.6939 |          20.6148 |
[32m[20221214 14:03:06 @agent_ppo2.py:185][0m |           0.0109 |          27.9538 |          20.6163 |
[32m[20221214 14:03:06 @agent_ppo2.py:185][0m |          -0.0041 |          25.4525 |          20.6137 |
[32m[20221214 14:03:06 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.90
[32m[20221214 14:03:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.69
[32m[20221214 14:03:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.81
[32m[20221214 14:03:07 @agent_ppo2.py:143][0m Total time:       5.07 min
[32m[20221214 14:03:07 @agent_ppo2.py:145][0m 452608 total steps have happened
[32m[20221214 14:03:07 @agent_ppo2.py:121][0m #------------------------ Iteration 221 --------------------------#
[32m[20221214 14:03:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:07 @agent_ppo2.py:185][0m |           0.0030 |          34.8419 |          20.5940 |
[32m[20221214 14:03:07 @agent_ppo2.py:185][0m |          -0.0029 |          28.0753 |          20.5897 |
[32m[20221214 14:03:07 @agent_ppo2.py:185][0m |          -0.0020 |          25.1143 |          20.5944 |
[32m[20221214 14:03:07 @agent_ppo2.py:185][0m |          -0.0009 |          23.1765 |          20.5977 |
[32m[20221214 14:03:07 @agent_ppo2.py:185][0m |          -0.0031 |          21.7685 |          20.5971 |
[32m[20221214 14:03:07 @agent_ppo2.py:185][0m |          -0.0041 |          20.5431 |          20.6023 |
[32m[20221214 14:03:07 @agent_ppo2.py:185][0m |          -0.0035 |          19.6063 |          20.6024 |
[32m[20221214 14:03:08 @agent_ppo2.py:185][0m |          -0.0036 |          18.6186 |          20.6067 |
[32m[20221214 14:03:08 @agent_ppo2.py:185][0m |          -0.0031 |          17.9376 |          20.6069 |
[32m[20221214 14:03:08 @agent_ppo2.py:185][0m |          -0.0070 |          17.5924 |          20.6098 |
[32m[20221214 14:03:08 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.97
[32m[20221214 14:03:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.57
[32m[20221214 14:03:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.21
[32m[20221214 14:03:08 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 483.21
[32m[20221214 14:03:08 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 483.21
[32m[20221214 14:03:08 @agent_ppo2.py:143][0m Total time:       5.10 min
[32m[20221214 14:03:08 @agent_ppo2.py:145][0m 454656 total steps have happened
[32m[20221214 14:03:08 @agent_ppo2.py:121][0m #------------------------ Iteration 222 --------------------------#
[32m[20221214 14:03:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:08 @agent_ppo2.py:185][0m |          -0.0011 |          32.5168 |          20.6432 |
[32m[20221214 14:03:08 @agent_ppo2.py:185][0m |          -0.0028 |          27.1943 |          20.6386 |
[32m[20221214 14:03:08 @agent_ppo2.py:185][0m |          -0.0023 |          25.1415 |          20.6309 |
[32m[20221214 14:03:09 @agent_ppo2.py:185][0m |          -0.0026 |          23.5498 |          20.6273 |
[32m[20221214 14:03:09 @agent_ppo2.py:185][0m |          -0.0050 |          22.4510 |          20.6187 |
[32m[20221214 14:03:09 @agent_ppo2.py:185][0m |          -0.0052 |          21.5482 |          20.6135 |
[32m[20221214 14:03:09 @agent_ppo2.py:185][0m |          -0.0050 |          20.7296 |          20.6107 |
[32m[20221214 14:03:09 @agent_ppo2.py:185][0m |          -0.0030 |          20.1525 |          20.6061 |
[32m[20221214 14:03:09 @agent_ppo2.py:185][0m |          -0.0048 |          19.5517 |          20.5997 |
[32m[20221214 14:03:09 @agent_ppo2.py:185][0m |          -0.0049 |          19.0150 |          20.5905 |
[32m[20221214 14:03:09 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.28
[32m[20221214 14:03:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.54
[32m[20221214 14:03:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.80
[32m[20221214 14:03:09 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 523.80
[32m[20221214 14:03:09 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 523.80
[32m[20221214 14:03:09 @agent_ppo2.py:143][0m Total time:       5.12 min
[32m[20221214 14:03:09 @agent_ppo2.py:145][0m 456704 total steps have happened
[32m[20221214 14:03:09 @agent_ppo2.py:121][0m #------------------------ Iteration 223 --------------------------#
[32m[20221214 14:03:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:10 @agent_ppo2.py:185][0m |          -0.0046 |          35.0842 |          20.5731 |
[32m[20221214 14:03:10 @agent_ppo2.py:185][0m |          -0.0017 |          28.5295 |          20.5585 |
[32m[20221214 14:03:10 @agent_ppo2.py:185][0m |          -0.0030 |          26.1615 |          20.5512 |
[32m[20221214 14:03:10 @agent_ppo2.py:185][0m |          -0.0040 |          24.9512 |          20.5419 |
[32m[20221214 14:03:10 @agent_ppo2.py:185][0m |          -0.0027 |          24.0864 |          20.5335 |
[32m[20221214 14:03:10 @agent_ppo2.py:185][0m |          -0.0035 |          23.1787 |          20.5250 |
[32m[20221214 14:03:10 @agent_ppo2.py:185][0m |           0.0114 |          27.0399 |          20.5184 |
[32m[20221214 14:03:10 @agent_ppo2.py:185][0m |          -0.0017 |          22.2523 |          20.5166 |
[32m[20221214 14:03:10 @agent_ppo2.py:185][0m |          -0.0026 |          21.7952 |          20.5050 |
[32m[20221214 14:03:10 @agent_ppo2.py:185][0m |          -0.0028 |          21.2258 |          20.5032 |
[32m[20221214 14:03:10 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:03:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.45
[32m[20221214 14:03:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.36
[32m[20221214 14:03:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.40
[32m[20221214 14:03:11 @agent_ppo2.py:143][0m Total time:       5.14 min
[32m[20221214 14:03:11 @agent_ppo2.py:145][0m 458752 total steps have happened
[32m[20221214 14:03:11 @agent_ppo2.py:121][0m #------------------------ Iteration 224 --------------------------#
[32m[20221214 14:03:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:11 @agent_ppo2.py:185][0m |           0.0001 |          28.2698 |          20.5314 |
[32m[20221214 14:03:11 @agent_ppo2.py:185][0m |          -0.0074 |          23.6681 |          20.5220 |
[32m[20221214 14:03:11 @agent_ppo2.py:185][0m |          -0.0068 |          21.6389 |          20.5122 |
[32m[20221214 14:03:11 @agent_ppo2.py:185][0m |          -0.0057 |          20.1982 |          20.5071 |
[32m[20221214 14:03:11 @agent_ppo2.py:185][0m |          -0.0052 |          19.1400 |          20.5011 |
[32m[20221214 14:03:11 @agent_ppo2.py:185][0m |           0.0067 |          18.4764 |          20.5026 |
[32m[20221214 14:03:12 @agent_ppo2.py:185][0m |           0.0048 |          17.6360 |          20.5014 |
[32m[20221214 14:03:12 @agent_ppo2.py:185][0m |          -0.0058 |          16.9230 |          20.4976 |
[32m[20221214 14:03:12 @agent_ppo2.py:185][0m |          -0.0028 |          16.4929 |          20.4950 |
[32m[20221214 14:03:12 @agent_ppo2.py:185][0m |          -0.0043 |          16.0333 |          20.4869 |
[32m[20221214 14:03:12 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.15
[32m[20221214 14:03:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.61
[32m[20221214 14:03:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.18
[32m[20221214 14:03:12 @agent_ppo2.py:143][0m Total time:       5.17 min
[32m[20221214 14:03:12 @agent_ppo2.py:145][0m 460800 total steps have happened
[32m[20221214 14:03:12 @agent_ppo2.py:121][0m #------------------------ Iteration 225 --------------------------#
[32m[20221214 14:03:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:12 @agent_ppo2.py:185][0m |          -0.0037 |          38.7471 |          20.5180 |
[32m[20221214 14:03:12 @agent_ppo2.py:185][0m |          -0.0018 |          32.5338 |          20.5076 |
[32m[20221214 14:03:13 @agent_ppo2.py:185][0m |          -0.0025 |          29.4254 |          20.4924 |
[32m[20221214 14:03:13 @agent_ppo2.py:185][0m |          -0.0036 |          27.2749 |          20.4921 |
[32m[20221214 14:03:13 @agent_ppo2.py:185][0m |           0.0042 |          25.8378 |          20.4860 |
[32m[20221214 14:03:13 @agent_ppo2.py:185][0m |          -0.0003 |          24.0180 |          20.4719 |
[32m[20221214 14:03:13 @agent_ppo2.py:185][0m |          -0.0006 |          22.9146 |          20.4642 |
[32m[20221214 14:03:13 @agent_ppo2.py:185][0m |          -0.0036 |          22.0850 |          20.4562 |
[32m[20221214 14:03:13 @agent_ppo2.py:185][0m |          -0.0059 |          21.4086 |          20.4544 |
[32m[20221214 14:03:13 @agent_ppo2.py:185][0m |          -0.0050 |          21.0352 |          20.4460 |
[32m[20221214 14:03:13 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 350.41
[32m[20221214 14:03:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 401.55
[32m[20221214 14:03:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.92
[32m[20221214 14:03:13 @agent_ppo2.py:143][0m Total time:       5.19 min
[32m[20221214 14:03:13 @agent_ppo2.py:145][0m 462848 total steps have happened
[32m[20221214 14:03:13 @agent_ppo2.py:121][0m #------------------------ Iteration 226 --------------------------#
[32m[20221214 14:03:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:14 @agent_ppo2.py:185][0m |          -0.0008 |          49.7272 |          20.4101 |
[32m[20221214 14:03:14 @agent_ppo2.py:185][0m |          -0.0044 |          39.2658 |          20.3956 |
[32m[20221214 14:03:14 @agent_ppo2.py:185][0m |          -0.0003 |          34.3272 |          20.3972 |
[32m[20221214 14:03:14 @agent_ppo2.py:185][0m |          -0.0036 |          31.0415 |          20.3865 |
[32m[20221214 14:03:14 @agent_ppo2.py:185][0m |          -0.0019 |          29.1820 |          20.3904 |
[32m[20221214 14:03:14 @agent_ppo2.py:185][0m |           0.0043 |          28.3718 |          20.3846 |
[32m[20221214 14:03:14 @agent_ppo2.py:185][0m |          -0.0048 |          26.4786 |          20.3878 |
[32m[20221214 14:03:14 @agent_ppo2.py:185][0m |          -0.0037 |          25.6710 |          20.3812 |
[32m[20221214 14:03:14 @agent_ppo2.py:185][0m |           0.0115 |          26.6647 |          20.3790 |
[32m[20221214 14:03:15 @agent_ppo2.py:185][0m |          -0.0011 |          24.1374 |          20.3718 |
[32m[20221214 14:03:15 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:03:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.08
[32m[20221214 14:03:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.35
[32m[20221214 14:03:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.58
[32m[20221214 14:03:15 @agent_ppo2.py:143][0m Total time:       5.21 min
[32m[20221214 14:03:15 @agent_ppo2.py:145][0m 464896 total steps have happened
[32m[20221214 14:03:15 @agent_ppo2.py:121][0m #------------------------ Iteration 227 --------------------------#
[32m[20221214 14:03:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:15 @agent_ppo2.py:185][0m |           0.0021 |          49.4245 |          20.4663 |
[32m[20221214 14:03:15 @agent_ppo2.py:185][0m |          -0.0015 |          41.9723 |          20.4596 |
[32m[20221214 14:03:15 @agent_ppo2.py:185][0m |           0.0092 |          41.4656 |          20.4547 |
[32m[20221214 14:03:15 @agent_ppo2.py:185][0m |           0.0012 |          37.1536 |          20.4597 |
[32m[20221214 14:03:15 @agent_ppo2.py:185][0m |          -0.0040 |          35.6855 |          20.4583 |
[32m[20221214 14:03:16 @agent_ppo2.py:185][0m |          -0.0019 |          34.6322 |          20.4561 |
[32m[20221214 14:03:16 @agent_ppo2.py:185][0m |          -0.0031 |          33.7653 |          20.4528 |
[32m[20221214 14:03:16 @agent_ppo2.py:185][0m |          -0.0028 |          33.0254 |          20.4534 |
[32m[20221214 14:03:16 @agent_ppo2.py:185][0m |          -0.0041 |          32.3419 |          20.4506 |
[32m[20221214 14:03:16 @agent_ppo2.py:185][0m |          -0.0023 |          31.5877 |          20.4471 |
[32m[20221214 14:03:16 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:03:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.41
[32m[20221214 14:03:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.78
[32m[20221214 14:03:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.49
[32m[20221214 14:03:16 @agent_ppo2.py:143][0m Total time:       5.23 min
[32m[20221214 14:03:16 @agent_ppo2.py:145][0m 466944 total steps have happened
[32m[20221214 14:03:16 @agent_ppo2.py:121][0m #------------------------ Iteration 228 --------------------------#
[32m[20221214 14:03:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:16 @agent_ppo2.py:185][0m |          -0.0007 |          39.8865 |          20.4473 |
[32m[20221214 14:03:17 @agent_ppo2.py:185][0m |           0.0027 |          33.2548 |          20.4455 |
[32m[20221214 14:03:17 @agent_ppo2.py:185][0m |          -0.0038 |          29.9905 |          20.4384 |
[32m[20221214 14:03:17 @agent_ppo2.py:185][0m |          -0.0018 |          28.2306 |          20.4360 |
[32m[20221214 14:03:17 @agent_ppo2.py:185][0m |          -0.0040 |          27.0788 |          20.4337 |
[32m[20221214 14:03:17 @agent_ppo2.py:185][0m |          -0.0007 |          26.1558 |          20.4324 |
[32m[20221214 14:03:17 @agent_ppo2.py:185][0m |          -0.0043 |          25.4832 |          20.4292 |
[32m[20221214 14:03:17 @agent_ppo2.py:185][0m |          -0.0025 |          24.9801 |          20.4279 |
[32m[20221214 14:03:17 @agent_ppo2.py:185][0m |          -0.0007 |          24.4468 |          20.4326 |
[32m[20221214 14:03:17 @agent_ppo2.py:185][0m |          -0.0038 |          24.0882 |          20.4275 |
[32m[20221214 14:03:17 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.97
[32m[20221214 14:03:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.63
[32m[20221214 14:03:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.60
[32m[20221214 14:03:17 @agent_ppo2.py:143][0m Total time:       5.26 min
[32m[20221214 14:03:17 @agent_ppo2.py:145][0m 468992 total steps have happened
[32m[20221214 14:03:17 @agent_ppo2.py:121][0m #------------------------ Iteration 229 --------------------------#
[32m[20221214 14:03:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:18 @agent_ppo2.py:185][0m |           0.0010 |          45.1765 |          20.4734 |
[32m[20221214 14:03:18 @agent_ppo2.py:185][0m |          -0.0049 |          38.6488 |          20.4788 |
[32m[20221214 14:03:18 @agent_ppo2.py:185][0m |          -0.0030 |          36.1661 |          20.4774 |
[32m[20221214 14:03:18 @agent_ppo2.py:185][0m |          -0.0042 |          34.1966 |          20.4750 |
[32m[20221214 14:03:18 @agent_ppo2.py:185][0m |          -0.0039 |          32.8595 |          20.4804 |
[32m[20221214 14:03:18 @agent_ppo2.py:185][0m |          -0.0039 |          32.0605 |          20.4786 |
[32m[20221214 14:03:18 @agent_ppo2.py:185][0m |           0.0023 |          32.2880 |          20.4779 |
[32m[20221214 14:03:19 @agent_ppo2.py:185][0m |          -0.0057 |          30.5268 |          20.4790 |
[32m[20221214 14:03:19 @agent_ppo2.py:185][0m |          -0.0013 |          29.9088 |          20.4846 |
[32m[20221214 14:03:19 @agent_ppo2.py:185][0m |           0.0001 |          29.6188 |          20.4846 |
[32m[20221214 14:03:19 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.53
[32m[20221214 14:03:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.53
[32m[20221214 14:03:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.65
[32m[20221214 14:03:19 @agent_ppo2.py:143][0m Total time:       5.28 min
[32m[20221214 14:03:19 @agent_ppo2.py:145][0m 471040 total steps have happened
[32m[20221214 14:03:19 @agent_ppo2.py:121][0m #------------------------ Iteration 230 --------------------------#
[32m[20221214 14:03:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:03:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:19 @agent_ppo2.py:185][0m |          -0.0018 |          43.2286 |          20.4528 |
[32m[20221214 14:03:19 @agent_ppo2.py:185][0m |          -0.0077 |          36.5321 |          20.4489 |
[32m[20221214 14:03:19 @agent_ppo2.py:185][0m |          -0.0025 |          33.8274 |          20.4403 |
[32m[20221214 14:03:20 @agent_ppo2.py:185][0m |           0.0088 |          35.3474 |          20.4336 |
[32m[20221214 14:03:20 @agent_ppo2.py:185][0m |          -0.0048 |          31.2207 |          20.4340 |
[32m[20221214 14:03:20 @agent_ppo2.py:185][0m |          -0.0064 |          30.4073 |          20.4307 |
[32m[20221214 14:03:20 @agent_ppo2.py:185][0m |          -0.0052 |          29.9792 |          20.4270 |
[32m[20221214 14:03:20 @agent_ppo2.py:185][0m |          -0.0031 |          29.6396 |          20.4214 |
[32m[20221214 14:03:20 @agent_ppo2.py:185][0m |          -0.0050 |          29.2908 |          20.4167 |
[32m[20221214 14:03:20 @agent_ppo2.py:185][0m |          -0.0011 |          28.9307 |          20.4111 |
[32m[20221214 14:03:20 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:03:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.90
[32m[20221214 14:03:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.70
[32m[20221214 14:03:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.84
[32m[20221214 14:03:20 @agent_ppo2.py:143][0m Total time:       5.30 min
[32m[20221214 14:03:20 @agent_ppo2.py:145][0m 473088 total steps have happened
[32m[20221214 14:03:20 @agent_ppo2.py:121][0m #------------------------ Iteration 231 --------------------------#
[32m[20221214 14:03:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:21 @agent_ppo2.py:185][0m |           0.0009 |          57.9411 |          20.4264 |
[32m[20221214 14:03:21 @agent_ppo2.py:185][0m |          -0.0017 |          48.4256 |          20.4222 |
[32m[20221214 14:03:21 @agent_ppo2.py:185][0m |          -0.0021 |          44.1855 |          20.4175 |
[32m[20221214 14:03:21 @agent_ppo2.py:185][0m |           0.0031 |          41.8520 |          20.4162 |
[32m[20221214 14:03:21 @agent_ppo2.py:185][0m |          -0.0020 |          39.9599 |          20.4108 |
[32m[20221214 14:03:21 @agent_ppo2.py:185][0m |          -0.0040 |          38.5737 |          20.4161 |
[32m[20221214 14:03:21 @agent_ppo2.py:185][0m |          -0.0039 |          37.7715 |          20.4139 |
[32m[20221214 14:03:21 @agent_ppo2.py:185][0m |          -0.0027 |          36.6578 |          20.4119 |
[32m[20221214 14:03:21 @agent_ppo2.py:185][0m |           0.0071 |          37.1352 |          20.4069 |
[32m[20221214 14:03:21 @agent_ppo2.py:185][0m |           0.0009 |          34.9969 |          20.4131 |
[32m[20221214 14:03:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:03:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.56
[32m[20221214 14:03:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.03
[32m[20221214 14:03:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.72
[32m[20221214 14:03:22 @agent_ppo2.py:143][0m Total time:       5.33 min
[32m[20221214 14:03:22 @agent_ppo2.py:145][0m 475136 total steps have happened
[32m[20221214 14:03:22 @agent_ppo2.py:121][0m #------------------------ Iteration 232 --------------------------#
[32m[20221214 14:03:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:22 @agent_ppo2.py:185][0m |          -0.0020 |          51.3884 |          20.5027 |
[32m[20221214 14:03:22 @agent_ppo2.py:185][0m |          -0.0038 |          46.4567 |          20.4957 |
[32m[20221214 14:03:22 @agent_ppo2.py:185][0m |          -0.0001 |          44.3104 |          20.4916 |
[32m[20221214 14:03:22 @agent_ppo2.py:185][0m |          -0.0017 |          42.8815 |          20.4900 |
[32m[20221214 14:03:22 @agent_ppo2.py:185][0m |          -0.0047 |          41.1594 |          20.4856 |
[32m[20221214 14:03:22 @agent_ppo2.py:185][0m |          -0.0024 |          40.1340 |          20.4784 |
[32m[20221214 14:03:23 @agent_ppo2.py:185][0m |          -0.0070 |          39.3442 |          20.4788 |
[32m[20221214 14:03:23 @agent_ppo2.py:185][0m |          -0.0052 |          38.6048 |          20.4814 |
[32m[20221214 14:03:23 @agent_ppo2.py:185][0m |          -0.0035 |          37.9593 |          20.4772 |
[32m[20221214 14:03:23 @agent_ppo2.py:185][0m |          -0.0049 |          37.3993 |          20.4787 |
[32m[20221214 14:03:23 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.81
[32m[20221214 14:03:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.15
[32m[20221214 14:03:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.74
[32m[20221214 14:03:23 @agent_ppo2.py:143][0m Total time:       5.35 min
[32m[20221214 14:03:23 @agent_ppo2.py:145][0m 477184 total steps have happened
[32m[20221214 14:03:23 @agent_ppo2.py:121][0m #------------------------ Iteration 233 --------------------------#
[32m[20221214 14:03:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:23 @agent_ppo2.py:185][0m |           0.0012 |          41.5623 |          20.4242 |
[32m[20221214 14:03:23 @agent_ppo2.py:185][0m |          -0.0015 |          32.9974 |          20.4210 |
[32m[20221214 14:03:24 @agent_ppo2.py:185][0m |          -0.0042 |          29.6836 |          20.4131 |
[32m[20221214 14:03:24 @agent_ppo2.py:185][0m |          -0.0060 |          27.7297 |          20.4042 |
[32m[20221214 14:03:24 @agent_ppo2.py:185][0m |          -0.0044 |          26.3095 |          20.3980 |
[32m[20221214 14:03:24 @agent_ppo2.py:185][0m |          -0.0083 |          25.2036 |          20.3946 |
[32m[20221214 14:03:24 @agent_ppo2.py:185][0m |          -0.0016 |          24.3927 |          20.3910 |
[32m[20221214 14:03:24 @agent_ppo2.py:185][0m |          -0.0049 |          23.4060 |          20.3872 |
[32m[20221214 14:03:24 @agent_ppo2.py:185][0m |          -0.0050 |          22.5846 |          20.3871 |
[32m[20221214 14:03:24 @agent_ppo2.py:185][0m |          -0.0024 |          22.0913 |          20.3832 |
[32m[20221214 14:03:24 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:03:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.86
[32m[20221214 14:03:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.13
[32m[20221214 14:03:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.70
[32m[20221214 14:03:24 @agent_ppo2.py:143][0m Total time:       5.37 min
[32m[20221214 14:03:24 @agent_ppo2.py:145][0m 479232 total steps have happened
[32m[20221214 14:03:24 @agent_ppo2.py:121][0m #------------------------ Iteration 234 --------------------------#
[32m[20221214 14:03:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:25 @agent_ppo2.py:185][0m |           0.0057 |          44.9829 |          20.4121 |
[32m[20221214 14:03:25 @agent_ppo2.py:185][0m |          -0.0005 |          38.4074 |          20.4001 |
[32m[20221214 14:03:25 @agent_ppo2.py:185][0m |           0.0014 |          36.3256 |          20.3932 |
[32m[20221214 14:03:25 @agent_ppo2.py:185][0m |          -0.0055 |          34.1804 |          20.3920 |
[32m[20221214 14:03:25 @agent_ppo2.py:185][0m |          -0.0062 |          33.1151 |          20.3918 |
[32m[20221214 14:03:25 @agent_ppo2.py:185][0m |          -0.0014 |          32.1588 |          20.3910 |
[32m[20221214 14:03:25 @agent_ppo2.py:185][0m |           0.0029 |          32.3915 |          20.3872 |
[32m[20221214 14:03:25 @agent_ppo2.py:185][0m |          -0.0077 |          30.6990 |          20.3842 |
[32m[20221214 14:03:25 @agent_ppo2.py:185][0m |          -0.0031 |          30.4356 |          20.3889 |
[32m[20221214 14:03:26 @agent_ppo2.py:185][0m |          -0.0020 |          29.8445 |          20.3905 |
[32m[20221214 14:03:26 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.75
[32m[20221214 14:03:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.26
[32m[20221214 14:03:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.72
[32m[20221214 14:03:26 @agent_ppo2.py:143][0m Total time:       5.39 min
[32m[20221214 14:03:26 @agent_ppo2.py:145][0m 481280 total steps have happened
[32m[20221214 14:03:26 @agent_ppo2.py:121][0m #------------------------ Iteration 235 --------------------------#
[32m[20221214 14:03:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:26 @agent_ppo2.py:185][0m |          -0.0001 |          68.0694 |          20.3306 |
[32m[20221214 14:03:26 @agent_ppo2.py:185][0m |          -0.0034 |          62.3513 |          20.3192 |
[32m[20221214 14:03:26 @agent_ppo2.py:185][0m |           0.0051 |          63.0297 |          20.3165 |
[32m[20221214 14:03:26 @agent_ppo2.py:185][0m |          -0.0038 |          58.5684 |          20.3119 |
[32m[20221214 14:03:26 @agent_ppo2.py:185][0m |          -0.0056 |          57.4399 |          20.3175 |
[32m[20221214 14:03:27 @agent_ppo2.py:185][0m |          -0.0072 |          56.4910 |          20.3176 |
[32m[20221214 14:03:27 @agent_ppo2.py:185][0m |          -0.0038 |          55.9127 |          20.3145 |
[32m[20221214 14:03:27 @agent_ppo2.py:185][0m |          -0.0035 |          55.5561 |          20.3183 |
[32m[20221214 14:03:27 @agent_ppo2.py:185][0m |          -0.0033 |          54.7930 |          20.3197 |
[32m[20221214 14:03:27 @agent_ppo2.py:185][0m |          -0.0055 |          54.1236 |          20.3180 |
[32m[20221214 14:03:27 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:03:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.77
[32m[20221214 14:03:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.13
[32m[20221214 14:03:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.87
[32m[20221214 14:03:27 @agent_ppo2.py:143][0m Total time:       5.42 min
[32m[20221214 14:03:27 @agent_ppo2.py:145][0m 483328 total steps have happened
[32m[20221214 14:03:27 @agent_ppo2.py:121][0m #------------------------ Iteration 236 --------------------------#
[32m[20221214 14:03:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:27 @agent_ppo2.py:185][0m |          -0.0006 |          56.2741 |          20.4060 |
[32m[20221214 14:03:28 @agent_ppo2.py:185][0m |           0.0078 |          52.5234 |          20.3985 |
[32m[20221214 14:03:28 @agent_ppo2.py:185][0m |           0.0116 |          55.9300 |          20.3899 |
[32m[20221214 14:03:28 @agent_ppo2.py:185][0m |          -0.0020 |          44.9467 |          20.3901 |
[32m[20221214 14:03:28 @agent_ppo2.py:185][0m |          -0.0028 |          43.2062 |          20.3844 |
[32m[20221214 14:03:28 @agent_ppo2.py:185][0m |          -0.0014 |          42.0386 |          20.3796 |
[32m[20221214 14:03:28 @agent_ppo2.py:185][0m |          -0.0030 |          40.8880 |          20.3743 |
[32m[20221214 14:03:28 @agent_ppo2.py:185][0m |          -0.0009 |          39.9877 |          20.3694 |
[32m[20221214 14:03:28 @agent_ppo2.py:185][0m |          -0.0050 |          39.3335 |          20.3696 |
[32m[20221214 14:03:28 @agent_ppo2.py:185][0m |          -0.0016 |          38.9635 |          20.3646 |
[32m[20221214 14:03:28 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:03:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.01
[32m[20221214 14:03:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.68
[32m[20221214 14:03:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.46
[32m[20221214 14:03:28 @agent_ppo2.py:143][0m Total time:       5.44 min
[32m[20221214 14:03:28 @agent_ppo2.py:145][0m 485376 total steps have happened
[32m[20221214 14:03:28 @agent_ppo2.py:121][0m #------------------------ Iteration 237 --------------------------#
[32m[20221214 14:03:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:29 @agent_ppo2.py:185][0m |          -0.0005 |          55.7088 |          20.4200 |
[32m[20221214 14:03:29 @agent_ppo2.py:185][0m |          -0.0036 |          48.5348 |          20.4111 |
[32m[20221214 14:03:29 @agent_ppo2.py:185][0m |           0.0056 |          46.4874 |          20.4065 |
[32m[20221214 14:03:29 @agent_ppo2.py:185][0m |          -0.0051 |          41.3531 |          20.3984 |
[32m[20221214 14:03:29 @agent_ppo2.py:185][0m |          -0.0023 |          39.1685 |          20.3934 |
[32m[20221214 14:03:29 @agent_ppo2.py:185][0m |          -0.0064 |          37.7318 |          20.3911 |
[32m[20221214 14:03:29 @agent_ppo2.py:185][0m |          -0.0008 |          37.2486 |          20.3861 |
[32m[20221214 14:03:29 @agent_ppo2.py:185][0m |          -0.0068 |          35.6207 |          20.3800 |
[32m[20221214 14:03:30 @agent_ppo2.py:185][0m |          -0.0066 |          34.7025 |          20.3767 |
[32m[20221214 14:03:30 @agent_ppo2.py:185][0m |          -0.0080 |          34.1197 |          20.3670 |
[32m[20221214 14:03:30 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.45
[32m[20221214 14:03:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.73
[32m[20221214 14:03:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.50
[32m[20221214 14:03:30 @agent_ppo2.py:143][0m Total time:       5.46 min
[32m[20221214 14:03:30 @agent_ppo2.py:145][0m 487424 total steps have happened
[32m[20221214 14:03:30 @agent_ppo2.py:121][0m #------------------------ Iteration 238 --------------------------#
[32m[20221214 14:03:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:30 @agent_ppo2.py:185][0m |          -0.0011 |          59.9885 |          20.3495 |
[32m[20221214 14:03:30 @agent_ppo2.py:185][0m |          -0.0053 |          54.6812 |          20.3377 |
[32m[20221214 14:03:30 @agent_ppo2.py:185][0m |          -0.0037 |          52.6028 |          20.3316 |
[32m[20221214 14:03:30 @agent_ppo2.py:185][0m |          -0.0039 |          51.4811 |          20.3273 |
[32m[20221214 14:03:31 @agent_ppo2.py:185][0m |           0.0048 |          55.5649 |          20.3261 |
[32m[20221214 14:03:31 @agent_ppo2.py:185][0m |          -0.0089 |          50.1489 |          20.3173 |
[32m[20221214 14:03:31 @agent_ppo2.py:185][0m |          -0.0005 |          50.0590 |          20.3138 |
[32m[20221214 14:03:31 @agent_ppo2.py:185][0m |          -0.0076 |          48.7010 |          20.3125 |
[32m[20221214 14:03:31 @agent_ppo2.py:185][0m |          -0.0056 |          48.0499 |          20.3055 |
[32m[20221214 14:03:31 @agent_ppo2.py:185][0m |          -0.0052 |          47.5881 |          20.3065 |
[32m[20221214 14:03:31 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:03:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.78
[32m[20221214 14:03:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.73
[32m[20221214 14:03:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.72
[32m[20221214 14:03:31 @agent_ppo2.py:143][0m Total time:       5.49 min
[32m[20221214 14:03:31 @agent_ppo2.py:145][0m 489472 total steps have happened
[32m[20221214 14:03:31 @agent_ppo2.py:121][0m #------------------------ Iteration 239 --------------------------#
[32m[20221214 14:03:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:32 @agent_ppo2.py:185][0m |          -0.0020 |          46.2010 |          20.3233 |
[32m[20221214 14:03:32 @agent_ppo2.py:185][0m |          -0.0009 |          40.3096 |          20.3168 |
[32m[20221214 14:03:32 @agent_ppo2.py:185][0m |          -0.0026 |          37.3865 |          20.3104 |
[32m[20221214 14:03:32 @agent_ppo2.py:185][0m |          -0.0034 |          35.8851 |          20.3094 |
[32m[20221214 14:03:32 @agent_ppo2.py:185][0m |           0.0002 |          35.2606 |          20.3101 |
[32m[20221214 14:03:32 @agent_ppo2.py:185][0m |          -0.0066 |          33.7864 |          20.3080 |
[32m[20221214 14:03:32 @agent_ppo2.py:185][0m |          -0.0008 |          33.1675 |          20.3057 |
[32m[20221214 14:03:32 @agent_ppo2.py:185][0m |          -0.0049 |          32.4737 |          20.3007 |
[32m[20221214 14:03:32 @agent_ppo2.py:185][0m |          -0.0047 |          31.9832 |          20.3067 |
[32m[20221214 14:03:32 @agent_ppo2.py:185][0m |          -0.0061 |          31.5756 |          20.3058 |
[32m[20221214 14:03:32 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:03:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.96
[32m[20221214 14:03:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.62
[32m[20221214 14:03:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.51
[32m[20221214 14:03:33 @agent_ppo2.py:143][0m Total time:       5.51 min
[32m[20221214 14:03:33 @agent_ppo2.py:145][0m 491520 total steps have happened
[32m[20221214 14:03:33 @agent_ppo2.py:121][0m #------------------------ Iteration 240 --------------------------#
[32m[20221214 14:03:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:03:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:33 @agent_ppo2.py:185][0m |          -0.0002 |          39.6727 |          20.2878 |
[32m[20221214 14:03:33 @agent_ppo2.py:185][0m |           0.0054 |          34.5713 |          20.2783 |
[32m[20221214 14:03:33 @agent_ppo2.py:185][0m |          -0.0025 |          31.8172 |          20.2625 |
[32m[20221214 14:03:33 @agent_ppo2.py:185][0m |          -0.0069 |          30.2946 |          20.2602 |
[32m[20221214 14:03:33 @agent_ppo2.py:185][0m |          -0.0014 |          29.2352 |          20.2451 |
[32m[20221214 14:03:33 @agent_ppo2.py:185][0m |           0.0012 |          28.6402 |          20.2370 |
[32m[20221214 14:03:34 @agent_ppo2.py:185][0m |          -0.0056 |          28.0304 |          20.2326 |
[32m[20221214 14:03:34 @agent_ppo2.py:185][0m |          -0.0024 |          27.4429 |          20.2229 |
[32m[20221214 14:03:34 @agent_ppo2.py:185][0m |          -0.0050 |          27.1171 |          20.2144 |
[32m[20221214 14:03:34 @agent_ppo2.py:185][0m |          -0.0066 |          26.8354 |          20.2031 |
[32m[20221214 14:03:34 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:03:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.27
[32m[20221214 14:03:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.72
[32m[20221214 14:03:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.44
[32m[20221214 14:03:34 @agent_ppo2.py:143][0m Total time:       5.53 min
[32m[20221214 14:03:34 @agent_ppo2.py:145][0m 493568 total steps have happened
[32m[20221214 14:03:34 @agent_ppo2.py:121][0m #------------------------ Iteration 241 --------------------------#
[32m[20221214 14:03:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:34 @agent_ppo2.py:185][0m |          -0.0003 |          49.9879 |          20.3077 |
[32m[20221214 14:03:34 @agent_ppo2.py:185][0m |          -0.0026 |          40.8891 |          20.3060 |
[32m[20221214 14:03:34 @agent_ppo2.py:185][0m |          -0.0031 |          37.0295 |          20.3022 |
[32m[20221214 14:03:35 @agent_ppo2.py:185][0m |           0.0012 |          34.8961 |          20.2992 |
[32m[20221214 14:03:35 @agent_ppo2.py:185][0m |           0.0194 |          36.2397 |          20.3042 |
[32m[20221214 14:03:35 @agent_ppo2.py:185][0m |           0.0117 |          38.5750 |          20.2956 |
[32m[20221214 14:03:35 @agent_ppo2.py:185][0m |          -0.0029 |          30.8109 |          20.2994 |
[32m[20221214 14:03:35 @agent_ppo2.py:185][0m |          -0.0028 |          29.7317 |          20.3013 |
[32m[20221214 14:03:35 @agent_ppo2.py:185][0m |           0.0006 |          29.0652 |          20.3009 |
[32m[20221214 14:03:35 @agent_ppo2.py:185][0m |          -0.0009 |          28.2629 |          20.3033 |
[32m[20221214 14:03:35 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 441.07
[32m[20221214 14:03:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.58
[32m[20221214 14:03:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 551.92
[32m[20221214 14:03:35 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 551.92
[32m[20221214 14:03:35 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 551.92
[32m[20221214 14:03:35 @agent_ppo2.py:143][0m Total time:       5.55 min
[32m[20221214 14:03:35 @agent_ppo2.py:145][0m 495616 total steps have happened
[32m[20221214 14:03:35 @agent_ppo2.py:121][0m #------------------------ Iteration 242 --------------------------#
[32m[20221214 14:03:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:36 @agent_ppo2.py:185][0m |          -0.0002 |          50.7184 |          20.2225 |
[32m[20221214 14:03:36 @agent_ppo2.py:185][0m |          -0.0046 |          42.2411 |          20.2230 |
[32m[20221214 14:03:36 @agent_ppo2.py:185][0m |          -0.0021 |          39.3271 |          20.2238 |
[32m[20221214 14:03:36 @agent_ppo2.py:185][0m |          -0.0073 |          37.4711 |          20.2156 |
[32m[20221214 14:03:36 @agent_ppo2.py:185][0m |           0.0065 |          38.3151 |          20.2138 |
[32m[20221214 14:03:36 @agent_ppo2.py:185][0m |          -0.0037 |          34.8112 |          20.2080 |
[32m[20221214 14:03:36 @agent_ppo2.py:185][0m |          -0.0032 |          34.0513 |          20.2015 |
[32m[20221214 14:03:36 @agent_ppo2.py:185][0m |          -0.0038 |          33.2525 |          20.1984 |
[32m[20221214 14:03:36 @agent_ppo2.py:185][0m |          -0.0062 |          32.6894 |          20.1960 |
[32m[20221214 14:03:37 @agent_ppo2.py:185][0m |          -0.0011 |          32.0758 |          20.1898 |
[32m[20221214 14:03:37 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.32
[32m[20221214 14:03:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.63
[32m[20221214 14:03:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.17
[32m[20221214 14:03:37 @agent_ppo2.py:143][0m Total time:       5.58 min
[32m[20221214 14:03:37 @agent_ppo2.py:145][0m 497664 total steps have happened
[32m[20221214 14:03:37 @agent_ppo2.py:121][0m #------------------------ Iteration 243 --------------------------#
[32m[20221214 14:03:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:37 @agent_ppo2.py:185][0m |          -0.0040 |          55.3787 |          20.1957 |
[32m[20221214 14:03:37 @agent_ppo2.py:185][0m |          -0.0052 |          49.3454 |          20.1760 |
[32m[20221214 14:03:37 @agent_ppo2.py:185][0m |          -0.0020 |          47.1902 |          20.1716 |
[32m[20221214 14:03:37 @agent_ppo2.py:185][0m |          -0.0034 |          45.5457 |          20.1675 |
[32m[20221214 14:03:37 @agent_ppo2.py:185][0m |          -0.0044 |          44.3861 |          20.1582 |
[32m[20221214 14:03:38 @agent_ppo2.py:185][0m |          -0.0072 |          43.4685 |          20.1581 |
[32m[20221214 14:03:38 @agent_ppo2.py:185][0m |           0.0023 |          45.1425 |          20.1537 |
[32m[20221214 14:03:38 @agent_ppo2.py:185][0m |          -0.0062 |          42.3246 |          20.1562 |
[32m[20221214 14:03:38 @agent_ppo2.py:185][0m |          -0.0024 |          41.8084 |          20.1500 |
[32m[20221214 14:03:38 @agent_ppo2.py:185][0m |          -0.0074 |          41.3130 |          20.1467 |
[32m[20221214 14:03:38 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:03:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.10
[32m[20221214 14:03:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.06
[32m[20221214 14:03:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.48
[32m[20221214 14:03:38 @agent_ppo2.py:143][0m Total time:       5.60 min
[32m[20221214 14:03:38 @agent_ppo2.py:145][0m 499712 total steps have happened
[32m[20221214 14:03:38 @agent_ppo2.py:121][0m #------------------------ Iteration 244 --------------------------#
[32m[20221214 14:03:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:38 @agent_ppo2.py:185][0m |          -0.0027 |          43.3008 |          20.2437 |
[32m[20221214 14:03:39 @agent_ppo2.py:185][0m |           0.0012 |          36.4876 |          20.2210 |
[32m[20221214 14:03:39 @agent_ppo2.py:185][0m |          -0.0038 |          33.6927 |          20.2098 |
[32m[20221214 14:03:39 @agent_ppo2.py:185][0m |           0.0103 |          34.5941 |          20.1955 |
[32m[20221214 14:03:39 @agent_ppo2.py:185][0m |          -0.0043 |          30.7155 |          20.1834 |
[32m[20221214 14:03:39 @agent_ppo2.py:185][0m |          -0.0025 |          29.5120 |          20.1713 |
[32m[20221214 14:03:39 @agent_ppo2.py:185][0m |          -0.0031 |          28.8310 |          20.1611 |
[32m[20221214 14:03:39 @agent_ppo2.py:185][0m |          -0.0036 |          28.3379 |          20.1575 |
[32m[20221214 14:03:39 @agent_ppo2.py:185][0m |          -0.0068 |          27.7360 |          20.1482 |
[32m[20221214 14:03:39 @agent_ppo2.py:185][0m |          -0.0004 |          27.2225 |          20.1385 |
[32m[20221214 14:03:39 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:03:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.22
[32m[20221214 14:03:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.04
[32m[20221214 14:03:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.24
[32m[20221214 14:03:39 @agent_ppo2.py:143][0m Total time:       5.62 min
[32m[20221214 14:03:39 @agent_ppo2.py:145][0m 501760 total steps have happened
[32m[20221214 14:03:39 @agent_ppo2.py:121][0m #------------------------ Iteration 245 --------------------------#
[32m[20221214 14:03:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:40 @agent_ppo2.py:185][0m |          -0.0024 |          49.2555 |          20.1431 |
[32m[20221214 14:03:40 @agent_ppo2.py:185][0m |           0.0018 |          42.6702 |          20.1261 |
[32m[20221214 14:03:40 @agent_ppo2.py:185][0m |          -0.0051 |          38.6775 |          20.1216 |
[32m[20221214 14:03:40 @agent_ppo2.py:185][0m |          -0.0052 |          36.5512 |          20.1179 |
[32m[20221214 14:03:40 @agent_ppo2.py:185][0m |          -0.0030 |          35.3453 |          20.1146 |
[32m[20221214 14:03:40 @agent_ppo2.py:185][0m |          -0.0069 |          34.2460 |          20.1125 |
[32m[20221214 14:03:40 @agent_ppo2.py:185][0m |          -0.0047 |          33.3270 |          20.1121 |
[32m[20221214 14:03:40 @agent_ppo2.py:185][0m |          -0.0081 |          32.6799 |          20.1087 |
[32m[20221214 14:03:41 @agent_ppo2.py:185][0m |          -0.0035 |          32.2545 |          20.1066 |
[32m[20221214 14:03:41 @agent_ppo2.py:185][0m |          -0.0063 |          31.7551 |          20.1061 |
[32m[20221214 14:03:41 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.55
[32m[20221214 14:03:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.20
[32m[20221214 14:03:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.17
[32m[20221214 14:03:41 @agent_ppo2.py:143][0m Total time:       5.65 min
[32m[20221214 14:03:41 @agent_ppo2.py:145][0m 503808 total steps have happened
[32m[20221214 14:03:41 @agent_ppo2.py:121][0m #------------------------ Iteration 246 --------------------------#
[32m[20221214 14:03:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:41 @agent_ppo2.py:185][0m |           0.0012 |          62.7504 |          20.0945 |
[32m[20221214 14:03:41 @agent_ppo2.py:185][0m |          -0.0030 |          53.3711 |          20.0911 |
[32m[20221214 14:03:41 @agent_ppo2.py:185][0m |          -0.0030 |          49.5590 |          20.0943 |
[32m[20221214 14:03:41 @agent_ppo2.py:185][0m |          -0.0052 |          47.0689 |          20.0923 |
[32m[20221214 14:03:42 @agent_ppo2.py:185][0m |           0.0035 |          46.2725 |          20.0948 |
[32m[20221214 14:03:42 @agent_ppo2.py:185][0m |          -0.0026 |          43.9585 |          20.0973 |
[32m[20221214 14:03:42 @agent_ppo2.py:185][0m |          -0.0033 |          42.9797 |          20.1009 |
[32m[20221214 14:03:42 @agent_ppo2.py:185][0m |          -0.0032 |          42.1748 |          20.1018 |
[32m[20221214 14:03:42 @agent_ppo2.py:185][0m |          -0.0043 |          41.4207 |          20.1085 |
[32m[20221214 14:03:42 @agent_ppo2.py:185][0m |          -0.0039 |          40.8375 |          20.1069 |
[32m[20221214 14:03:42 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.72
[32m[20221214 14:03:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.73
[32m[20221214 14:03:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.81
[32m[20221214 14:03:42 @agent_ppo2.py:143][0m Total time:       5.67 min
[32m[20221214 14:03:42 @agent_ppo2.py:145][0m 505856 total steps have happened
[32m[20221214 14:03:42 @agent_ppo2.py:121][0m #------------------------ Iteration 247 --------------------------#
[32m[20221214 14:03:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:43 @agent_ppo2.py:185][0m |           0.0011 |          49.4936 |          20.1711 |
[32m[20221214 14:03:43 @agent_ppo2.py:185][0m |          -0.0038 |          42.5941 |          20.1696 |
[32m[20221214 14:03:43 @agent_ppo2.py:185][0m |          -0.0038 |          40.2001 |          20.1706 |
[32m[20221214 14:03:43 @agent_ppo2.py:185][0m |          -0.0048 |          38.7094 |          20.1753 |
[32m[20221214 14:03:43 @agent_ppo2.py:185][0m |          -0.0088 |          37.6707 |          20.1760 |
[32m[20221214 14:03:43 @agent_ppo2.py:185][0m |          -0.0050 |          36.6449 |          20.1794 |
[32m[20221214 14:03:43 @agent_ppo2.py:185][0m |          -0.0057 |          35.8537 |          20.1857 |
[32m[20221214 14:03:43 @agent_ppo2.py:185][0m |          -0.0054 |          35.5354 |          20.1859 |
[32m[20221214 14:03:43 @agent_ppo2.py:185][0m |          -0.0003 |          35.0336 |          20.1880 |
[32m[20221214 14:03:43 @agent_ppo2.py:185][0m |          -0.0066 |          34.1340 |          20.1884 |
[32m[20221214 14:03:43 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:03:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.97
[32m[20221214 14:03:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.58
[32m[20221214 14:03:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.98
[32m[20221214 14:03:44 @agent_ppo2.py:143][0m Total time:       5.69 min
[32m[20221214 14:03:44 @agent_ppo2.py:145][0m 507904 total steps have happened
[32m[20221214 14:03:44 @agent_ppo2.py:121][0m #------------------------ Iteration 248 --------------------------#
[32m[20221214 14:03:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:44 @agent_ppo2.py:185][0m |           0.0017 |          41.8728 |          20.1423 |
[32m[20221214 14:03:44 @agent_ppo2.py:185][0m |          -0.0016 |          35.1613 |          20.1342 |
[32m[20221214 14:03:44 @agent_ppo2.py:185][0m |          -0.0023 |          32.1864 |          20.1244 |
[32m[20221214 14:03:44 @agent_ppo2.py:185][0m |           0.0061 |          30.2366 |          20.1163 |
[32m[20221214 14:03:44 @agent_ppo2.py:185][0m |          -0.0018 |          28.6444 |          20.1115 |
[32m[20221214 14:03:44 @agent_ppo2.py:185][0m |           0.0026 |          27.5267 |          20.1027 |
[32m[20221214 14:03:44 @agent_ppo2.py:185][0m |          -0.0023 |          26.4273 |          20.0988 |
[32m[20221214 14:03:45 @agent_ppo2.py:185][0m |          -0.0017 |          25.5021 |          20.0924 |
[32m[20221214 14:03:45 @agent_ppo2.py:185][0m |           0.0025 |          25.0643 |          20.0849 |
[32m[20221214 14:03:45 @agent_ppo2.py:185][0m |          -0.0034 |          23.9966 |          20.0826 |
[32m[20221214 14:03:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:03:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.82
[32m[20221214 14:03:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.45
[32m[20221214 14:03:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.63
[32m[20221214 14:03:45 @agent_ppo2.py:143][0m Total time:       5.71 min
[32m[20221214 14:03:45 @agent_ppo2.py:145][0m 509952 total steps have happened
[32m[20221214 14:03:45 @agent_ppo2.py:121][0m #------------------------ Iteration 249 --------------------------#
[32m[20221214 14:03:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:45 @agent_ppo2.py:185][0m |          -0.0015 |          52.6045 |          20.1522 |
[32m[20221214 14:03:45 @agent_ppo2.py:185][0m |           0.0061 |          49.8745 |          20.1445 |
[32m[20221214 14:03:45 @agent_ppo2.py:185][0m |          -0.0066 |          41.7318 |          20.1298 |
[32m[20221214 14:03:46 @agent_ppo2.py:185][0m |          -0.0003 |          39.4932 |          20.1119 |
[32m[20221214 14:03:46 @agent_ppo2.py:185][0m |          -0.0079 |          37.6252 |          20.1057 |
[32m[20221214 14:03:46 @agent_ppo2.py:185][0m |          -0.0063 |          36.4879 |          20.0975 |
[32m[20221214 14:03:46 @agent_ppo2.py:185][0m |          -0.0040 |          35.5023 |          20.0925 |
[32m[20221214 14:03:46 @agent_ppo2.py:185][0m |          -0.0072 |          34.2869 |          20.0834 |
[32m[20221214 14:03:46 @agent_ppo2.py:185][0m |           0.0020 |          35.2019 |          20.0829 |
[32m[20221214 14:03:46 @agent_ppo2.py:185][0m |          -0.0010 |          33.4893 |          20.0749 |
[32m[20221214 14:03:46 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.12
[32m[20221214 14:03:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.55
[32m[20221214 14:03:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.84
[32m[20221214 14:03:46 @agent_ppo2.py:143][0m Total time:       5.74 min
[32m[20221214 14:03:46 @agent_ppo2.py:145][0m 512000 total steps have happened
[32m[20221214 14:03:46 @agent_ppo2.py:121][0m #------------------------ Iteration 250 --------------------------#
[32m[20221214 14:03:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:03:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:47 @agent_ppo2.py:185][0m |          -0.0007 |          52.8685 |          20.1033 |
[32m[20221214 14:03:47 @agent_ppo2.py:185][0m |          -0.0018 |          46.2823 |          20.0868 |
[32m[20221214 14:03:47 @agent_ppo2.py:185][0m |          -0.0024 |          43.7068 |          20.0930 |
[32m[20221214 14:03:47 @agent_ppo2.py:185][0m |          -0.0034 |          41.8275 |          20.0826 |
[32m[20221214 14:03:47 @agent_ppo2.py:185][0m |          -0.0042 |          39.9856 |          20.0906 |
[32m[20221214 14:03:47 @agent_ppo2.py:185][0m |          -0.0047 |          38.4789 |          20.0958 |
[32m[20221214 14:03:47 @agent_ppo2.py:185][0m |          -0.0023 |          36.9645 |          20.0887 |
[32m[20221214 14:03:47 @agent_ppo2.py:185][0m |          -0.0048 |          36.3315 |          20.0927 |
[32m[20221214 14:03:47 @agent_ppo2.py:185][0m |           0.0006 |          35.2566 |          20.0937 |
[32m[20221214 14:03:47 @agent_ppo2.py:185][0m |          -0.0045 |          34.5933 |          20.0987 |
[32m[20221214 14:03:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.49
[32m[20221214 14:03:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.85
[32m[20221214 14:03:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.01
[32m[20221214 14:03:48 @agent_ppo2.py:143][0m Total time:       5.76 min
[32m[20221214 14:03:48 @agent_ppo2.py:145][0m 514048 total steps have happened
[32m[20221214 14:03:48 @agent_ppo2.py:121][0m #------------------------ Iteration 251 --------------------------#
[32m[20221214 14:03:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:48 @agent_ppo2.py:185][0m |           0.0011 |          44.6251 |          20.0494 |
[32m[20221214 14:03:48 @agent_ppo2.py:185][0m |          -0.0027 |          38.2954 |          20.0402 |
[32m[20221214 14:03:48 @agent_ppo2.py:185][0m |          -0.0039 |          35.4494 |          20.0263 |
[32m[20221214 14:03:48 @agent_ppo2.py:185][0m |          -0.0023 |          33.6948 |          20.0252 |
[32m[20221214 14:03:48 @agent_ppo2.py:185][0m |           0.0054 |          32.4493 |          20.0061 |
[32m[20221214 14:03:48 @agent_ppo2.py:185][0m |           0.0085 |          34.1851 |          20.0010 |
[32m[20221214 14:03:49 @agent_ppo2.py:185][0m |          -0.0079 |          29.8972 |          19.9891 |
[32m[20221214 14:03:49 @agent_ppo2.py:185][0m |          -0.0047 |          29.2493 |          19.9889 |
[32m[20221214 14:03:49 @agent_ppo2.py:185][0m |          -0.0004 |          28.0917 |          19.9768 |
[32m[20221214 14:03:49 @agent_ppo2.py:185][0m |          -0.0088 |          27.4204 |          19.9688 |
[32m[20221214 14:03:49 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:03:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.23
[32m[20221214 14:03:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.73
[32m[20221214 14:03:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.68
[32m[20221214 14:03:49 @agent_ppo2.py:143][0m Total time:       5.78 min
[32m[20221214 14:03:49 @agent_ppo2.py:145][0m 516096 total steps have happened
[32m[20221214 14:03:49 @agent_ppo2.py:121][0m #------------------------ Iteration 252 --------------------------#
[32m[20221214 14:03:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:49 @agent_ppo2.py:185][0m |          -0.0019 |          51.1210 |          20.0214 |
[32m[20221214 14:03:49 @agent_ppo2.py:185][0m |          -0.0003 |          43.2241 |          20.0143 |
[32m[20221214 14:03:50 @agent_ppo2.py:185][0m |          -0.0054 |          39.3689 |          20.0030 |
[32m[20221214 14:03:50 @agent_ppo2.py:185][0m |          -0.0055 |          37.2229 |          20.0045 |
[32m[20221214 14:03:50 @agent_ppo2.py:185][0m |          -0.0075 |          35.7689 |          20.0107 |
[32m[20221214 14:03:50 @agent_ppo2.py:185][0m |          -0.0062 |          34.1234 |          19.9984 |
[32m[20221214 14:03:50 @agent_ppo2.py:185][0m |          -0.0090 |          33.1850 |          19.9970 |
[32m[20221214 14:03:50 @agent_ppo2.py:185][0m |          -0.0055 |          32.3399 |          19.9998 |
[32m[20221214 14:03:50 @agent_ppo2.py:185][0m |          -0.0056 |          31.5848 |          19.9981 |
[32m[20221214 14:03:50 @agent_ppo2.py:185][0m |          -0.0060 |          30.8556 |          19.9959 |
[32m[20221214 14:03:50 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.73
[32m[20221214 14:03:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.46
[32m[20221214 14:03:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.57
[32m[20221214 14:03:50 @agent_ppo2.py:143][0m Total time:       5.81 min
[32m[20221214 14:03:50 @agent_ppo2.py:145][0m 518144 total steps have happened
[32m[20221214 14:03:50 @agent_ppo2.py:121][0m #------------------------ Iteration 253 --------------------------#
[32m[20221214 14:03:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:51 @agent_ppo2.py:185][0m |          -0.0004 |          44.7395 |          20.0053 |
[32m[20221214 14:03:51 @agent_ppo2.py:185][0m |           0.0009 |          37.3485 |          20.0052 |
[32m[20221214 14:03:51 @agent_ppo2.py:185][0m |           0.0093 |          36.0747 |          20.0101 |
[32m[20221214 14:03:51 @agent_ppo2.py:185][0m |          -0.0044 |          31.7746 |          20.0113 |
[32m[20221214 14:03:51 @agent_ppo2.py:185][0m |          -0.0033 |          30.0049 |          20.0107 |
[32m[20221214 14:03:51 @agent_ppo2.py:185][0m |          -0.0027 |          29.0671 |          20.0127 |
[32m[20221214 14:03:51 @agent_ppo2.py:185][0m |          -0.0043 |          27.5957 |          20.0220 |
[32m[20221214 14:03:51 @agent_ppo2.py:185][0m |          -0.0072 |          26.7383 |          20.0208 |
[32m[20221214 14:03:51 @agent_ppo2.py:185][0m |          -0.0024 |          25.9679 |          20.0176 |
[32m[20221214 14:03:52 @agent_ppo2.py:185][0m |          -0.0065 |          25.4429 |          20.0268 |
[32m[20221214 14:03:52 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:03:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.61
[32m[20221214 14:03:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.52
[32m[20221214 14:03:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.13
[32m[20221214 14:03:52 @agent_ppo2.py:143][0m Total time:       5.83 min
[32m[20221214 14:03:52 @agent_ppo2.py:145][0m 520192 total steps have happened
[32m[20221214 14:03:52 @agent_ppo2.py:121][0m #------------------------ Iteration 254 --------------------------#
[32m[20221214 14:03:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:52 @agent_ppo2.py:185][0m |          -0.0008 |          50.2655 |          19.9833 |
[32m[20221214 14:03:52 @agent_ppo2.py:185][0m |          -0.0049 |          41.2234 |          19.9802 |
[32m[20221214 14:03:52 @agent_ppo2.py:185][0m |           0.0041 |          38.6086 |          19.9744 |
[32m[20221214 14:03:52 @agent_ppo2.py:185][0m |          -0.0037 |          35.5716 |          19.9663 |
[32m[20221214 14:03:53 @agent_ppo2.py:185][0m |          -0.0019 |          33.8991 |          19.9617 |
[32m[20221214 14:03:53 @agent_ppo2.py:185][0m |           0.0010 |          32.9920 |          19.9605 |
[32m[20221214 14:03:53 @agent_ppo2.py:185][0m |          -0.0034 |          31.7529 |          19.9595 |
[32m[20221214 14:03:53 @agent_ppo2.py:185][0m |          -0.0023 |          30.8920 |          19.9517 |
[32m[20221214 14:03:53 @agent_ppo2.py:185][0m |           0.0038 |          30.2974 |          19.9542 |
[32m[20221214 14:03:53 @agent_ppo2.py:185][0m |          -0.0038 |          29.6342 |          19.9419 |
[32m[20221214 14:03:53 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 14:03:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.38
[32m[20221214 14:03:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.40
[32m[20221214 14:03:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.41
[32m[20221214 14:03:53 @agent_ppo2.py:143][0m Total time:       5.85 min
[32m[20221214 14:03:53 @agent_ppo2.py:145][0m 522240 total steps have happened
[32m[20221214 14:03:53 @agent_ppo2.py:121][0m #------------------------ Iteration 255 --------------------------#
[32m[20221214 14:03:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:54 @agent_ppo2.py:185][0m |          -0.0036 |          47.4122 |          19.9960 |
[32m[20221214 14:03:54 @agent_ppo2.py:185][0m |          -0.0023 |          39.3009 |          19.9870 |
[32m[20221214 14:03:54 @agent_ppo2.py:185][0m |          -0.0037 |          35.1033 |          19.9835 |
[32m[20221214 14:03:54 @agent_ppo2.py:185][0m |           0.0074 |          34.7330 |          19.9754 |
[32m[20221214 14:03:54 @agent_ppo2.py:185][0m |          -0.0060 |          31.0418 |          19.9638 |
[32m[20221214 14:03:54 @agent_ppo2.py:185][0m |          -0.0051 |          29.9509 |          19.9624 |
[32m[20221214 14:03:54 @agent_ppo2.py:185][0m |          -0.0036 |          29.0127 |          19.9581 |
[32m[20221214 14:03:54 @agent_ppo2.py:185][0m |          -0.0086 |          28.1487 |          19.9614 |
[32m[20221214 14:03:54 @agent_ppo2.py:185][0m |          -0.0064 |          27.4395 |          19.9563 |
[32m[20221214 14:03:54 @agent_ppo2.py:185][0m |          -0.0041 |          26.8016 |          19.9585 |
[32m[20221214 14:03:54 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:03:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.07
[32m[20221214 14:03:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.49
[32m[20221214 14:03:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.87
[32m[20221214 14:03:55 @agent_ppo2.py:143][0m Total time:       5.88 min
[32m[20221214 14:03:55 @agent_ppo2.py:145][0m 524288 total steps have happened
[32m[20221214 14:03:55 @agent_ppo2.py:121][0m #------------------------ Iteration 256 --------------------------#
[32m[20221214 14:03:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:55 @agent_ppo2.py:185][0m |          -0.0024 |          41.7572 |          19.9114 |
[32m[20221214 14:03:55 @agent_ppo2.py:185][0m |          -0.0018 |          36.8243 |          19.9001 |
[32m[20221214 14:03:55 @agent_ppo2.py:185][0m |          -0.0004 |          34.7415 |          19.9007 |
[32m[20221214 14:03:55 @agent_ppo2.py:185][0m |          -0.0018 |          33.3318 |          19.8937 |
[32m[20221214 14:03:55 @agent_ppo2.py:185][0m |          -0.0021 |          32.2908 |          19.8885 |
[32m[20221214 14:03:55 @agent_ppo2.py:185][0m |           0.0088 |          32.2996 |          19.8880 |
[32m[20221214 14:03:55 @agent_ppo2.py:185][0m |          -0.0023 |          30.7688 |          19.8850 |
[32m[20221214 14:03:56 @agent_ppo2.py:185][0m |           0.0197 |          35.7680 |          19.8840 |
[32m[20221214 14:03:56 @agent_ppo2.py:185][0m |          -0.0058 |          29.5248 |          19.8894 |
[32m[20221214 14:03:56 @agent_ppo2.py:185][0m |           0.0053 |          29.5980 |          19.8858 |
[32m[20221214 14:03:56 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:03:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.41
[32m[20221214 14:03:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.87
[32m[20221214 14:03:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.93
[32m[20221214 14:03:56 @agent_ppo2.py:143][0m Total time:       5.90 min
[32m[20221214 14:03:56 @agent_ppo2.py:145][0m 526336 total steps have happened
[32m[20221214 14:03:56 @agent_ppo2.py:121][0m #------------------------ Iteration 257 --------------------------#
[32m[20221214 14:03:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:56 @agent_ppo2.py:185][0m |          -0.0031 |          42.7474 |          19.9130 |
[32m[20221214 14:03:56 @agent_ppo2.py:185][0m |          -0.0023 |          35.6358 |          19.9135 |
[32m[20221214 14:03:57 @agent_ppo2.py:185][0m |          -0.0045 |          32.3579 |          19.9040 |
[32m[20221214 14:03:57 @agent_ppo2.py:185][0m |           0.0110 |          33.3157 |          19.9023 |
[32m[20221214 14:03:57 @agent_ppo2.py:185][0m |          -0.0003 |          28.8520 |          19.9020 |
[32m[20221214 14:03:57 @agent_ppo2.py:185][0m |          -0.0022 |          27.5066 |          19.8963 |
[32m[20221214 14:03:57 @agent_ppo2.py:185][0m |          -0.0029 |          26.6062 |          19.8856 |
[32m[20221214 14:03:57 @agent_ppo2.py:185][0m |          -0.0047 |          25.7791 |          19.8916 |
[32m[20221214 14:03:57 @agent_ppo2.py:185][0m |          -0.0078 |          25.2429 |          19.8824 |
[32m[20221214 14:03:57 @agent_ppo2.py:185][0m |          -0.0003 |          24.6196 |          19.8730 |
[32m[20221214 14:03:57 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:03:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.82
[32m[20221214 14:03:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.31
[32m[20221214 14:03:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.22
[32m[20221214 14:03:57 @agent_ppo2.py:143][0m Total time:       5.92 min
[32m[20221214 14:03:57 @agent_ppo2.py:145][0m 528384 total steps have happened
[32m[20221214 14:03:57 @agent_ppo2.py:121][0m #------------------------ Iteration 258 --------------------------#
[32m[20221214 14:03:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:58 @agent_ppo2.py:185][0m |           0.0012 |          37.2284 |          19.8056 |
[32m[20221214 14:03:58 @agent_ppo2.py:185][0m |          -0.0007 |          32.0945 |          19.7916 |
[32m[20221214 14:03:58 @agent_ppo2.py:185][0m |           0.0049 |          32.6235 |          19.7888 |
[32m[20221214 14:03:58 @agent_ppo2.py:185][0m |          -0.0054 |          28.6082 |          19.7848 |
[32m[20221214 14:03:58 @agent_ppo2.py:185][0m |          -0.0096 |          27.5539 |          19.7857 |
[32m[20221214 14:03:58 @agent_ppo2.py:185][0m |          -0.0043 |          26.7121 |          19.7813 |
[32m[20221214 14:03:58 @agent_ppo2.py:185][0m |          -0.0064 |          26.1077 |          19.7717 |
[32m[20221214 14:03:58 @agent_ppo2.py:185][0m |          -0.0065 |          25.4528 |          19.7703 |
[32m[20221214 14:03:58 @agent_ppo2.py:185][0m |          -0.0056 |          24.8923 |          19.7634 |
[32m[20221214 14:03:58 @agent_ppo2.py:185][0m |           0.0026 |          25.2153 |          19.7638 |
[32m[20221214 14:03:58 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:03:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.96
[32m[20221214 14:03:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.36
[32m[20221214 14:03:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.21
[32m[20221214 14:03:59 @agent_ppo2.py:143][0m Total time:       5.94 min
[32m[20221214 14:03:59 @agent_ppo2.py:145][0m 530432 total steps have happened
[32m[20221214 14:03:59 @agent_ppo2.py:121][0m #------------------------ Iteration 259 --------------------------#
[32m[20221214 14:03:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:03:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:03:59 @agent_ppo2.py:185][0m |          -0.0025 |          40.6277 |          19.9040 |
[32m[20221214 14:03:59 @agent_ppo2.py:185][0m |           0.0085 |          36.6876 |          19.8938 |
[32m[20221214 14:03:59 @agent_ppo2.py:185][0m |          -0.0043 |          32.6288 |          19.8841 |
[32m[20221214 14:03:59 @agent_ppo2.py:185][0m |          -0.0026 |          31.3651 |          19.8808 |
[32m[20221214 14:03:59 @agent_ppo2.py:185][0m |          -0.0041 |          30.3778 |          19.8845 |
[32m[20221214 14:03:59 @agent_ppo2.py:185][0m |          -0.0020 |          29.6384 |          19.8831 |
[32m[20221214 14:04:00 @agent_ppo2.py:185][0m |          -0.0052 |          28.9163 |          19.8852 |
[32m[20221214 14:04:00 @agent_ppo2.py:185][0m |           0.0034 |          29.3042 |          19.8904 |
[32m[20221214 14:04:00 @agent_ppo2.py:185][0m |          -0.0054 |          27.7971 |          19.8900 |
[32m[20221214 14:04:00 @agent_ppo2.py:185][0m |          -0.0102 |          27.4659 |          19.8904 |
[32m[20221214 14:04:00 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:04:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.10
[32m[20221214 14:04:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.25
[32m[20221214 14:04:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.85
[32m[20221214 14:04:00 @agent_ppo2.py:143][0m Total time:       5.97 min
[32m[20221214 14:04:00 @agent_ppo2.py:145][0m 532480 total steps have happened
[32m[20221214 14:04:00 @agent_ppo2.py:121][0m #------------------------ Iteration 260 --------------------------#
[32m[20221214 14:04:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:04:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:00 @agent_ppo2.py:185][0m |          -0.0008 |          66.1817 |          19.8218 |
[32m[20221214 14:04:00 @agent_ppo2.py:185][0m |          -0.0000 |          55.5812 |          19.8224 |
[32m[20221214 14:04:01 @agent_ppo2.py:185][0m |          -0.0013 |          51.5339 |          19.8271 |
[32m[20221214 14:04:01 @agent_ppo2.py:185][0m |          -0.0066 |          48.9753 |          19.8338 |
[32m[20221214 14:04:01 @agent_ppo2.py:185][0m |          -0.0014 |          46.5377 |          19.8340 |
[32m[20221214 14:04:01 @agent_ppo2.py:185][0m |           0.0034 |          47.5615 |          19.8377 |
[32m[20221214 14:04:01 @agent_ppo2.py:185][0m |          -0.0066 |          44.1937 |          19.8462 |
[32m[20221214 14:04:01 @agent_ppo2.py:185][0m |          -0.0004 |          43.1463 |          19.8515 |
[32m[20221214 14:04:01 @agent_ppo2.py:185][0m |           0.0002 |          42.2697 |          19.8545 |
[32m[20221214 14:04:01 @agent_ppo2.py:185][0m |          -0.0075 |          41.7345 |          19.8587 |
[32m[20221214 14:04:01 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:04:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.53
[32m[20221214 14:04:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.02
[32m[20221214 14:04:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.78
[32m[20221214 14:04:01 @agent_ppo2.py:143][0m Total time:       5.99 min
[32m[20221214 14:04:01 @agent_ppo2.py:145][0m 534528 total steps have happened
[32m[20221214 14:04:01 @agent_ppo2.py:121][0m #------------------------ Iteration 261 --------------------------#
[32m[20221214 14:04:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:02 @agent_ppo2.py:185][0m |          -0.0023 |          74.6799 |          19.8746 |
[32m[20221214 14:04:02 @agent_ppo2.py:185][0m |          -0.0043 |          68.0393 |          19.8733 |
[32m[20221214 14:04:02 @agent_ppo2.py:185][0m |          -0.0045 |          64.9134 |          19.8699 |
[32m[20221214 14:04:02 @agent_ppo2.py:185][0m |          -0.0026 |          62.3869 |          19.8654 |
[32m[20221214 14:04:02 @agent_ppo2.py:185][0m |          -0.0045 |          60.6588 |          19.8587 |
[32m[20221214 14:04:02 @agent_ppo2.py:185][0m |          -0.0034 |          58.9343 |          19.8550 |
[32m[20221214 14:04:02 @agent_ppo2.py:185][0m |           0.0016 |          59.3514 |          19.8522 |
[32m[20221214 14:04:02 @agent_ppo2.py:185][0m |          -0.0036 |          56.2665 |          19.8501 |
[32m[20221214 14:04:02 @agent_ppo2.py:185][0m |          -0.0015 |          55.2763 |          19.8479 |
[32m[20221214 14:04:03 @agent_ppo2.py:185][0m |           0.0018 |          54.3336 |          19.8451 |
[32m[20221214 14:04:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:04:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.47
[32m[20221214 14:04:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.45
[32m[20221214 14:04:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 324.76
[32m[20221214 14:04:03 @agent_ppo2.py:143][0m Total time:       6.01 min
[32m[20221214 14:04:03 @agent_ppo2.py:145][0m 536576 total steps have happened
[32m[20221214 14:04:03 @agent_ppo2.py:121][0m #------------------------ Iteration 262 --------------------------#
[32m[20221214 14:04:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:03 @agent_ppo2.py:185][0m |          -0.0025 |          68.7182 |          19.8951 |
[32m[20221214 14:04:03 @agent_ppo2.py:185][0m |          -0.0049 |          61.6434 |          19.8888 |
[32m[20221214 14:04:03 @agent_ppo2.py:185][0m |          -0.0039 |          59.2850 |          19.8929 |
[32m[20221214 14:04:03 @agent_ppo2.py:185][0m |          -0.0056 |          57.1514 |          19.8931 |
[32m[20221214 14:04:03 @agent_ppo2.py:185][0m |          -0.0047 |          55.7683 |          19.8996 |
[32m[20221214 14:04:03 @agent_ppo2.py:185][0m |          -0.0052 |          54.8800 |          19.9033 |
[32m[20221214 14:04:04 @agent_ppo2.py:185][0m |          -0.0031 |          54.0873 |          19.9116 |
[32m[20221214 14:04:04 @agent_ppo2.py:185][0m |          -0.0047 |          53.6729 |          19.9165 |
[32m[20221214 14:04:04 @agent_ppo2.py:185][0m |          -0.0057 |          53.2909 |          19.9149 |
[32m[20221214 14:04:04 @agent_ppo2.py:185][0m |          -0.0054 |          53.0425 |          19.9207 |
[32m[20221214 14:04:04 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:04:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.79
[32m[20221214 14:04:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.57
[32m[20221214 14:04:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.86
[32m[20221214 14:04:04 @agent_ppo2.py:143][0m Total time:       6.03 min
[32m[20221214 14:04:04 @agent_ppo2.py:145][0m 538624 total steps have happened
[32m[20221214 14:04:04 @agent_ppo2.py:121][0m #------------------------ Iteration 263 --------------------------#
[32m[20221214 14:04:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:04 @agent_ppo2.py:185][0m |          -0.0010 |          83.6560 |          19.8961 |
[32m[20221214 14:04:04 @agent_ppo2.py:185][0m |           0.0018 |          73.2130 |          19.8943 |
[32m[20221214 14:04:05 @agent_ppo2.py:185][0m |           0.0018 |          67.7553 |          19.8932 |
[32m[20221214 14:04:05 @agent_ppo2.py:185][0m |          -0.0046 |          63.3928 |          19.8931 |
[32m[20221214 14:04:05 @agent_ppo2.py:185][0m |          -0.0034 |          60.6498 |          19.8951 |
[32m[20221214 14:04:05 @agent_ppo2.py:185][0m |          -0.0057 |          58.8242 |          19.8926 |
[32m[20221214 14:04:05 @agent_ppo2.py:185][0m |          -0.0052 |          56.6821 |          19.8947 |
[32m[20221214 14:04:05 @agent_ppo2.py:185][0m |          -0.0065 |          55.1806 |          19.8936 |
[32m[20221214 14:04:05 @agent_ppo2.py:185][0m |          -0.0083 |          53.7460 |          19.8999 |
[32m[20221214 14:04:05 @agent_ppo2.py:185][0m |          -0.0077 |          52.9975 |          19.8980 |
[32m[20221214 14:04:05 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:04:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.60
[32m[20221214 14:04:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 624.83
[32m[20221214 14:04:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.18
[32m[20221214 14:04:05 @agent_ppo2.py:143][0m Total time:       6.05 min
[32m[20221214 14:04:05 @agent_ppo2.py:145][0m 540672 total steps have happened
[32m[20221214 14:04:05 @agent_ppo2.py:121][0m #------------------------ Iteration 264 --------------------------#
[32m[20221214 14:04:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:06 @agent_ppo2.py:185][0m |           0.0054 |          59.0899 |          19.9787 |
[32m[20221214 14:04:06 @agent_ppo2.py:185][0m |           0.0081 |          53.7254 |          19.9642 |
[32m[20221214 14:04:06 @agent_ppo2.py:185][0m |           0.0005 |          51.0790 |          19.9535 |
[32m[20221214 14:04:06 @agent_ppo2.py:185][0m |          -0.0076 |          49.3653 |          19.9414 |
[32m[20221214 14:04:06 @agent_ppo2.py:185][0m |           0.0028 |          48.6384 |          19.9434 |
[32m[20221214 14:04:06 @agent_ppo2.py:185][0m |           0.0061 |          54.6090 |          19.9424 |
[32m[20221214 14:04:06 @agent_ppo2.py:185][0m |          -0.0030 |          46.9355 |          19.9314 |
[32m[20221214 14:04:06 @agent_ppo2.py:185][0m |          -0.0087 |          46.1565 |          19.9298 |
[32m[20221214 14:04:06 @agent_ppo2.py:185][0m |          -0.0059 |          45.3412 |          19.9198 |
[32m[20221214 14:04:07 @agent_ppo2.py:185][0m |           0.0095 |          51.1666 |          19.9142 |
[32m[20221214 14:04:07 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:04:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.77
[32m[20221214 14:04:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.43
[32m[20221214 14:04:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.69
[32m[20221214 14:04:07 @agent_ppo2.py:143][0m Total time:       6.08 min
[32m[20221214 14:04:07 @agent_ppo2.py:145][0m 542720 total steps have happened
[32m[20221214 14:04:07 @agent_ppo2.py:121][0m #------------------------ Iteration 265 --------------------------#
[32m[20221214 14:04:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:07 @agent_ppo2.py:185][0m |           0.0012 |          47.3892 |          19.8011 |
[32m[20221214 14:04:07 @agent_ppo2.py:185][0m |           0.0046 |          36.1806 |          19.7888 |
[32m[20221214 14:04:07 @agent_ppo2.py:185][0m |          -0.0028 |          30.5939 |          19.7815 |
[32m[20221214 14:04:07 @agent_ppo2.py:185][0m |          -0.0021 |          28.2159 |          19.7793 |
[32m[20221214 14:04:07 @agent_ppo2.py:185][0m |          -0.0030 |          26.6311 |          19.7666 |
[32m[20221214 14:04:07 @agent_ppo2.py:185][0m |          -0.0005 |          25.5336 |          19.7547 |
[32m[20221214 14:04:08 @agent_ppo2.py:185][0m |          -0.0038 |          24.3595 |          19.7471 |
[32m[20221214 14:04:08 @agent_ppo2.py:185][0m |          -0.0014 |          23.5727 |          19.7386 |
[32m[20221214 14:04:08 @agent_ppo2.py:185][0m |          -0.0015 |          22.8946 |          19.7316 |
[32m[20221214 14:04:08 @agent_ppo2.py:185][0m |          -0.0056 |          22.2606 |          19.7157 |
[32m[20221214 14:04:08 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:04:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.58
[32m[20221214 14:04:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.74
[32m[20221214 14:04:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.31
[32m[20221214 14:04:08 @agent_ppo2.py:143][0m Total time:       6.10 min
[32m[20221214 14:04:08 @agent_ppo2.py:145][0m 544768 total steps have happened
[32m[20221214 14:04:08 @agent_ppo2.py:121][0m #------------------------ Iteration 266 --------------------------#
[32m[20221214 14:04:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:08 @agent_ppo2.py:185][0m |          -0.0018 |          71.4531 |          19.8647 |
[32m[20221214 14:04:08 @agent_ppo2.py:185][0m |          -0.0034 |          58.2689 |          19.8543 |
[32m[20221214 14:04:09 @agent_ppo2.py:185][0m |          -0.0038 |          53.1671 |          19.8460 |
[32m[20221214 14:04:09 @agent_ppo2.py:185][0m |          -0.0069 |          50.5443 |          19.8451 |
[32m[20221214 14:04:09 @agent_ppo2.py:185][0m |          -0.0053 |          48.6414 |          19.8488 |
[32m[20221214 14:04:09 @agent_ppo2.py:185][0m |           0.0046 |          51.7011 |          19.8412 |
[32m[20221214 14:04:09 @agent_ppo2.py:185][0m |          -0.0034 |          46.8200 |          19.8413 |
[32m[20221214 14:04:09 @agent_ppo2.py:185][0m |          -0.0055 |          46.1979 |          19.8527 |
[32m[20221214 14:04:09 @agent_ppo2.py:185][0m |          -0.0053 |          45.5081 |          19.8508 |
[32m[20221214 14:04:09 @agent_ppo2.py:185][0m |          -0.0063 |          44.9170 |          19.8577 |
[32m[20221214 14:04:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:04:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.22
[32m[20221214 14:04:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.22
[32m[20221214 14:04:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.04
[32m[20221214 14:04:09 @agent_ppo2.py:143][0m Total time:       6.12 min
[32m[20221214 14:04:09 @agent_ppo2.py:145][0m 546816 total steps have happened
[32m[20221214 14:04:09 @agent_ppo2.py:121][0m #------------------------ Iteration 267 --------------------------#
[32m[20221214 14:04:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:10 @agent_ppo2.py:185][0m |           0.0013 |          52.3097 |          19.8810 |
[32m[20221214 14:04:10 @agent_ppo2.py:185][0m |          -0.0025 |          43.9779 |          19.8691 |
[32m[20221214 14:04:10 @agent_ppo2.py:185][0m |          -0.0026 |          40.5197 |          19.8648 |
[32m[20221214 14:04:10 @agent_ppo2.py:185][0m |          -0.0012 |          37.9261 |          19.8550 |
[32m[20221214 14:04:10 @agent_ppo2.py:185][0m |          -0.0032 |          36.1451 |          19.8502 |
[32m[20221214 14:04:10 @agent_ppo2.py:185][0m |           0.0003 |          34.6685 |          19.8430 |
[32m[20221214 14:04:10 @agent_ppo2.py:185][0m |          -0.0065 |          33.7445 |          19.8345 |
[32m[20221214 14:04:10 @agent_ppo2.py:185][0m |          -0.0033 |          32.8835 |          19.8313 |
[32m[20221214 14:04:11 @agent_ppo2.py:185][0m |          -0.0027 |          32.2049 |          19.8222 |
[32m[20221214 14:04:11 @agent_ppo2.py:185][0m |          -0.0033 |          31.8471 |          19.8246 |
[32m[20221214 14:04:11 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:04:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 441.30
[32m[20221214 14:04:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.36
[32m[20221214 14:04:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.52
[32m[20221214 14:04:11 @agent_ppo2.py:143][0m Total time:       6.15 min
[32m[20221214 14:04:11 @agent_ppo2.py:145][0m 548864 total steps have happened
[32m[20221214 14:04:11 @agent_ppo2.py:121][0m #------------------------ Iteration 268 --------------------------#
[32m[20221214 14:04:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:11 @agent_ppo2.py:185][0m |           0.0004 |          41.1315 |          19.7649 |
[32m[20221214 14:04:11 @agent_ppo2.py:185][0m |           0.0008 |          35.5978 |          19.7688 |
[32m[20221214 14:04:11 @agent_ppo2.py:185][0m |           0.0108 |          36.3545 |          19.7630 |
[32m[20221214 14:04:11 @agent_ppo2.py:185][0m |          -0.0082 |          31.2968 |          19.7566 |
[32m[20221214 14:04:12 @agent_ppo2.py:185][0m |           0.0010 |          30.1684 |          19.7515 |
[32m[20221214 14:04:12 @agent_ppo2.py:185][0m |          -0.0075 |          29.0394 |          19.7532 |
[32m[20221214 14:04:12 @agent_ppo2.py:185][0m |          -0.0025 |          28.3413 |          19.7447 |
[32m[20221214 14:04:12 @agent_ppo2.py:185][0m |          -0.0013 |          27.7476 |          19.7378 |
[32m[20221214 14:04:12 @agent_ppo2.py:185][0m |          -0.0062 |          27.0887 |          19.7357 |
[32m[20221214 14:04:12 @agent_ppo2.py:185][0m |           0.0031 |          27.4470 |          19.7325 |
[32m[20221214 14:04:12 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:04:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.56
[32m[20221214 14:04:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.61
[32m[20221214 14:04:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.11
[32m[20221214 14:04:12 @agent_ppo2.py:143][0m Total time:       6.17 min
[32m[20221214 14:04:12 @agent_ppo2.py:145][0m 550912 total steps have happened
[32m[20221214 14:04:12 @agent_ppo2.py:121][0m #------------------------ Iteration 269 --------------------------#
[32m[20221214 14:04:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:12 @agent_ppo2.py:185][0m |          -0.0014 |          36.3936 |          19.7865 |
[32m[20221214 14:04:13 @agent_ppo2.py:185][0m |          -0.0050 |          29.5863 |          19.7743 |
[32m[20221214 14:04:13 @agent_ppo2.py:185][0m |          -0.0052 |          26.9238 |          19.7698 |
[32m[20221214 14:04:13 @agent_ppo2.py:185][0m |          -0.0108 |          25.0369 |          19.7705 |
[32m[20221214 14:04:13 @agent_ppo2.py:185][0m |          -0.0077 |          23.7551 |          19.7722 |
[32m[20221214 14:04:13 @agent_ppo2.py:185][0m |          -0.0022 |          22.9865 |          19.7749 |
[32m[20221214 14:04:13 @agent_ppo2.py:185][0m |          -0.0070 |          22.1941 |          19.7762 |
[32m[20221214 14:04:13 @agent_ppo2.py:185][0m |          -0.0018 |          21.5422 |          19.7797 |
[32m[20221214 14:04:13 @agent_ppo2.py:185][0m |          -0.0077 |          20.9467 |          19.7788 |
[32m[20221214 14:04:13 @agent_ppo2.py:185][0m |          -0.0018 |          20.5878 |          19.7802 |
[32m[20221214 14:04:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:04:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.85
[32m[20221214 14:04:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.63
[32m[20221214 14:04:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.12
[32m[20221214 14:04:14 @agent_ppo2.py:143][0m Total time:       6.19 min
[32m[20221214 14:04:14 @agent_ppo2.py:145][0m 552960 total steps have happened
[32m[20221214 14:04:14 @agent_ppo2.py:121][0m #------------------------ Iteration 270 --------------------------#
[32m[20221214 14:04:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:04:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:14 @agent_ppo2.py:185][0m |           0.0082 |          75.4812 |          19.8075 |
[32m[20221214 14:04:14 @agent_ppo2.py:185][0m |          -0.0024 |          62.7046 |          19.8032 |
[32m[20221214 14:04:14 @agent_ppo2.py:185][0m |          -0.0014 |          58.9414 |          19.7980 |
[32m[20221214 14:04:14 @agent_ppo2.py:185][0m |          -0.0038 |          56.7552 |          19.8043 |
[32m[20221214 14:04:14 @agent_ppo2.py:185][0m |          -0.0031 |          54.8579 |          19.8043 |
[32m[20221214 14:04:14 @agent_ppo2.py:185][0m |          -0.0009 |          53.2874 |          19.8000 |
[32m[20221214 14:04:14 @agent_ppo2.py:185][0m |          -0.0035 |          52.4016 |          19.7988 |
[32m[20221214 14:04:15 @agent_ppo2.py:185][0m |          -0.0021 |          51.5734 |          19.7943 |
[32m[20221214 14:04:15 @agent_ppo2.py:185][0m |          -0.0049 |          50.7644 |          19.8016 |
[32m[20221214 14:04:15 @agent_ppo2.py:185][0m |          -0.0016 |          50.2252 |          19.7919 |
[32m[20221214 14:04:15 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:04:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.97
[32m[20221214 14:04:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 633.95
[32m[20221214 14:04:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.75
[32m[20221214 14:04:15 @agent_ppo2.py:143][0m Total time:       6.21 min
[32m[20221214 14:04:15 @agent_ppo2.py:145][0m 555008 total steps have happened
[32m[20221214 14:04:15 @agent_ppo2.py:121][0m #------------------------ Iteration 271 --------------------------#
[32m[20221214 14:04:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:15 @agent_ppo2.py:185][0m |          -0.0013 |          66.6047 |          19.8067 |
[32m[20221214 14:04:15 @agent_ppo2.py:185][0m |          -0.0044 |          57.7796 |          19.7988 |
[32m[20221214 14:04:15 @agent_ppo2.py:185][0m |          -0.0061 |          53.8861 |          19.7894 |
[32m[20221214 14:04:15 @agent_ppo2.py:185][0m |          -0.0047 |          50.9367 |          19.7832 |
[32m[20221214 14:04:16 @agent_ppo2.py:185][0m |           0.0122 |          62.8788 |          19.7736 |
[32m[20221214 14:04:16 @agent_ppo2.py:185][0m |          -0.0045 |          47.9724 |          19.7717 |
[32m[20221214 14:04:16 @agent_ppo2.py:185][0m |           0.0240 |          61.0173 |          19.7721 |
[32m[20221214 14:04:16 @agent_ppo2.py:185][0m |          -0.0057 |          45.8142 |          19.7648 |
[32m[20221214 14:04:16 @agent_ppo2.py:185][0m |          -0.0044 |          44.0538 |          19.7560 |
[32m[20221214 14:04:16 @agent_ppo2.py:185][0m |          -0.0060 |          43.3625 |          19.7607 |
[32m[20221214 14:04:16 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:04:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.14
[32m[20221214 14:04:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.15
[32m[20221214 14:04:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 350.62
[32m[20221214 14:04:16 @agent_ppo2.py:143][0m Total time:       6.24 min
[32m[20221214 14:04:16 @agent_ppo2.py:145][0m 557056 total steps have happened
[32m[20221214 14:04:16 @agent_ppo2.py:121][0m #------------------------ Iteration 272 --------------------------#
[32m[20221214 14:04:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:17 @agent_ppo2.py:185][0m |          -0.0005 |          77.3682 |          19.7907 |
[32m[20221214 14:04:17 @agent_ppo2.py:185][0m |          -0.0025 |          69.2455 |          19.7817 |
[32m[20221214 14:04:17 @agent_ppo2.py:185][0m |          -0.0034 |          66.6270 |          19.7832 |
[32m[20221214 14:04:17 @agent_ppo2.py:185][0m |          -0.0040 |          64.8113 |          19.7835 |
[32m[20221214 14:04:17 @agent_ppo2.py:185][0m |          -0.0048 |          63.7351 |          19.7797 |
[32m[20221214 14:04:17 @agent_ppo2.py:185][0m |          -0.0032 |          62.5974 |          19.7833 |
[32m[20221214 14:04:17 @agent_ppo2.py:185][0m |           0.0091 |          65.5180 |          19.7797 |
[32m[20221214 14:04:17 @agent_ppo2.py:185][0m |           0.0031 |          62.3842 |          19.7784 |
[32m[20221214 14:04:17 @agent_ppo2.py:185][0m |           0.0051 |          62.4860 |          19.7811 |
[32m[20221214 14:04:17 @agent_ppo2.py:185][0m |          -0.0045 |          60.2400 |          19.7862 |
[32m[20221214 14:04:17 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:04:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.38
[32m[20221214 14:04:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.52
[32m[20221214 14:04:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.85
[32m[20221214 14:04:18 @agent_ppo2.py:143][0m Total time:       6.26 min
[32m[20221214 14:04:18 @agent_ppo2.py:145][0m 559104 total steps have happened
[32m[20221214 14:04:18 @agent_ppo2.py:121][0m #------------------------ Iteration 273 --------------------------#
[32m[20221214 14:04:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:18 @agent_ppo2.py:185][0m |          -0.0003 |          62.8227 |          19.8073 |
[32m[20221214 14:04:18 @agent_ppo2.py:185][0m |          -0.0038 |          55.6693 |          19.8037 |
[32m[20221214 14:04:18 @agent_ppo2.py:185][0m |          -0.0062 |          52.8060 |          19.7949 |
[32m[20221214 14:04:18 @agent_ppo2.py:185][0m |          -0.0064 |          50.7452 |          19.7836 |
[32m[20221214 14:04:18 @agent_ppo2.py:185][0m |          -0.0037 |          49.3961 |          19.7702 |
[32m[20221214 14:04:18 @agent_ppo2.py:185][0m |          -0.0049 |          48.5642 |          19.7627 |
[32m[20221214 14:04:19 @agent_ppo2.py:185][0m |          -0.0039 |          47.5814 |          19.7622 |
[32m[20221214 14:04:19 @agent_ppo2.py:185][0m |          -0.0011 |          46.8399 |          19.7444 |
[32m[20221214 14:04:19 @agent_ppo2.py:185][0m |          -0.0023 |          46.3309 |          19.7406 |
[32m[20221214 14:04:19 @agent_ppo2.py:185][0m |          -0.0064 |          45.7819 |          19.7375 |
[32m[20221214 14:04:19 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 14:04:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 441.75
[32m[20221214 14:04:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.32
[32m[20221214 14:04:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 555.48
[32m[20221214 14:04:19 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 555.48
[32m[20221214 14:04:19 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 555.48
[32m[20221214 14:04:19 @agent_ppo2.py:143][0m Total time:       6.28 min
[32m[20221214 14:04:19 @agent_ppo2.py:145][0m 561152 total steps have happened
[32m[20221214 14:04:19 @agent_ppo2.py:121][0m #------------------------ Iteration 274 --------------------------#
[32m[20221214 14:04:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:19 @agent_ppo2.py:185][0m |          -0.0028 |          61.9340 |          19.6763 |
[32m[20221214 14:04:20 @agent_ppo2.py:185][0m |          -0.0053 |          47.7658 |          19.6769 |
[32m[20221214 14:04:20 @agent_ppo2.py:185][0m |          -0.0004 |          43.9790 |          19.6763 |
[32m[20221214 14:04:20 @agent_ppo2.py:185][0m |          -0.0047 |          41.8789 |          19.6783 |
[32m[20221214 14:04:20 @agent_ppo2.py:185][0m |          -0.0004 |          39.8850 |          19.6788 |
[32m[20221214 14:04:20 @agent_ppo2.py:185][0m |          -0.0044 |          38.5857 |          19.6737 |
[32m[20221214 14:04:20 @agent_ppo2.py:185][0m |          -0.0014 |          37.6297 |          19.6732 |
[32m[20221214 14:04:20 @agent_ppo2.py:185][0m |          -0.0024 |          36.9212 |          19.6792 |
[32m[20221214 14:04:20 @agent_ppo2.py:185][0m |          -0.0078 |          36.6460 |          19.6762 |
[32m[20221214 14:04:20 @agent_ppo2.py:185][0m |          -0.0055 |          35.5025 |          19.6831 |
[32m[20221214 14:04:20 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:04:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.23
[32m[20221214 14:04:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.64
[32m[20221214 14:04:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 409.58
[32m[20221214 14:04:20 @agent_ppo2.py:143][0m Total time:       6.31 min
[32m[20221214 14:04:20 @agent_ppo2.py:145][0m 563200 total steps have happened
[32m[20221214 14:04:20 @agent_ppo2.py:121][0m #------------------------ Iteration 275 --------------------------#
[32m[20221214 14:04:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:21 @agent_ppo2.py:185][0m |           0.0029 |          53.3262 |          19.7507 |
[32m[20221214 14:04:21 @agent_ppo2.py:185][0m |          -0.0063 |          43.4163 |          19.7303 |
[32m[20221214 14:04:21 @agent_ppo2.py:185][0m |          -0.0061 |          39.6351 |          19.7351 |
[32m[20221214 14:04:21 @agent_ppo2.py:185][0m |          -0.0085 |          36.7098 |          19.7372 |
[32m[20221214 14:04:21 @agent_ppo2.py:185][0m |          -0.0040 |          34.6330 |          19.7278 |
[32m[20221214 14:04:21 @agent_ppo2.py:185][0m |          -0.0053 |          33.1355 |          19.7298 |
[32m[20221214 14:04:21 @agent_ppo2.py:185][0m |          -0.0027 |          32.0878 |          19.7296 |
[32m[20221214 14:04:22 @agent_ppo2.py:185][0m |          -0.0067 |          31.2215 |          19.7254 |
[32m[20221214 14:04:22 @agent_ppo2.py:185][0m |          -0.0089 |          30.4619 |          19.7226 |
[32m[20221214 14:04:22 @agent_ppo2.py:185][0m |          -0.0056 |          29.6539 |          19.7251 |
[32m[20221214 14:04:22 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:04:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.15
[32m[20221214 14:04:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.44
[32m[20221214 14:04:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.82
[32m[20221214 14:04:22 @agent_ppo2.py:143][0m Total time:       6.33 min
[32m[20221214 14:04:22 @agent_ppo2.py:145][0m 565248 total steps have happened
[32m[20221214 14:04:22 @agent_ppo2.py:121][0m #------------------------ Iteration 276 --------------------------#
[32m[20221214 14:04:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:22 @agent_ppo2.py:185][0m |           0.0030 |          63.6831 |          19.7477 |
[32m[20221214 14:04:22 @agent_ppo2.py:185][0m |          -0.0030 |          49.8869 |          19.7343 |
[32m[20221214 14:04:22 @agent_ppo2.py:185][0m |           0.0017 |          47.5324 |          19.7363 |
[32m[20221214 14:04:22 @agent_ppo2.py:185][0m |          -0.0049 |          42.6023 |          19.7354 |
[32m[20221214 14:04:23 @agent_ppo2.py:185][0m |          -0.0021 |          40.3827 |          19.7363 |
[32m[20221214 14:04:23 @agent_ppo2.py:185][0m |          -0.0063 |          39.0468 |          19.7408 |
[32m[20221214 14:04:23 @agent_ppo2.py:185][0m |          -0.0022 |          37.8650 |          19.7408 |
[32m[20221214 14:04:23 @agent_ppo2.py:185][0m |          -0.0059 |          36.6181 |          19.7392 |
[32m[20221214 14:04:23 @agent_ppo2.py:185][0m |          -0.0044 |          35.4695 |          19.7438 |
[32m[20221214 14:04:23 @agent_ppo2.py:185][0m |          -0.0041 |          34.6777 |          19.7511 |
[32m[20221214 14:04:23 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:04:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.80
[32m[20221214 14:04:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.12
[32m[20221214 14:04:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.12
[32m[20221214 14:04:23 @agent_ppo2.py:143][0m Total time:       6.35 min
[32m[20221214 14:04:23 @agent_ppo2.py:145][0m 567296 total steps have happened
[32m[20221214 14:04:23 @agent_ppo2.py:121][0m #------------------------ Iteration 277 --------------------------#
[32m[20221214 14:04:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:24 @agent_ppo2.py:185][0m |          -0.0012 |          61.8451 |          19.7280 |
[32m[20221214 14:04:24 @agent_ppo2.py:185][0m |          -0.0047 |          50.0506 |          19.7338 |
[32m[20221214 14:04:24 @agent_ppo2.py:185][0m |          -0.0043 |          44.7644 |          19.7343 |
[32m[20221214 14:04:24 @agent_ppo2.py:185][0m |          -0.0082 |          41.4781 |          19.7341 |
[32m[20221214 14:04:24 @agent_ppo2.py:185][0m |           0.0013 |          40.3181 |          19.7372 |
[32m[20221214 14:04:24 @agent_ppo2.py:185][0m |          -0.0058 |          37.7695 |          19.7442 |
[32m[20221214 14:04:24 @agent_ppo2.py:185][0m |          -0.0061 |          36.2880 |          19.7429 |
[32m[20221214 14:04:24 @agent_ppo2.py:185][0m |          -0.0059 |          35.1321 |          19.7471 |
[32m[20221214 14:04:24 @agent_ppo2.py:185][0m |          -0.0043 |          33.9269 |          19.7543 |
[32m[20221214 14:04:24 @agent_ppo2.py:185][0m |          -0.0063 |          32.9133 |          19.7572 |
[32m[20221214 14:04:24 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:04:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.47
[32m[20221214 14:04:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.88
[32m[20221214 14:04:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.57
[32m[20221214 14:04:25 @agent_ppo2.py:143][0m Total time:       6.38 min
[32m[20221214 14:04:25 @agent_ppo2.py:145][0m 569344 total steps have happened
[32m[20221214 14:04:25 @agent_ppo2.py:121][0m #------------------------ Iteration 278 --------------------------#
[32m[20221214 14:04:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:25 @agent_ppo2.py:185][0m |           0.0024 |          72.8045 |          19.6774 |
[32m[20221214 14:04:25 @agent_ppo2.py:185][0m |          -0.0006 |          60.7003 |          19.6742 |
[32m[20221214 14:04:25 @agent_ppo2.py:185][0m |          -0.0069 |          54.8205 |          19.6689 |
[32m[20221214 14:04:25 @agent_ppo2.py:185][0m |          -0.0034 |          51.5243 |          19.6665 |
[32m[20221214 14:04:25 @agent_ppo2.py:185][0m |          -0.0090 |          48.9712 |          19.6663 |
[32m[20221214 14:04:25 @agent_ppo2.py:185][0m |          -0.0088 |          47.2312 |          19.6633 |
[32m[20221214 14:04:26 @agent_ppo2.py:185][0m |          -0.0081 |          45.7367 |          19.6621 |
[32m[20221214 14:04:26 @agent_ppo2.py:185][0m |          -0.0049 |          44.5685 |          19.6595 |
[32m[20221214 14:04:26 @agent_ppo2.py:185][0m |          -0.0044 |          43.3105 |          19.6587 |
[32m[20221214 14:04:26 @agent_ppo2.py:185][0m |          -0.0062 |          42.3302 |          19.6621 |
[32m[20221214 14:04:26 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:04:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.13
[32m[20221214 14:04:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.51
[32m[20221214 14:04:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.15
[32m[20221214 14:04:26 @agent_ppo2.py:143][0m Total time:       6.40 min
[32m[20221214 14:04:26 @agent_ppo2.py:145][0m 571392 total steps have happened
[32m[20221214 14:04:26 @agent_ppo2.py:121][0m #------------------------ Iteration 279 --------------------------#
[32m[20221214 14:04:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:26 @agent_ppo2.py:185][0m |          -0.0033 |          64.7407 |          19.7773 |
[32m[20221214 14:04:26 @agent_ppo2.py:185][0m |          -0.0053 |          53.3660 |          19.7750 |
[32m[20221214 14:04:27 @agent_ppo2.py:185][0m |          -0.0050 |          48.2205 |          19.7718 |
[32m[20221214 14:04:27 @agent_ppo2.py:185][0m |          -0.0037 |          45.4821 |          19.7648 |
[32m[20221214 14:04:27 @agent_ppo2.py:185][0m |          -0.0036 |          43.8644 |          19.7615 |
[32m[20221214 14:04:27 @agent_ppo2.py:185][0m |          -0.0041 |          42.6333 |          19.7616 |
[32m[20221214 14:04:27 @agent_ppo2.py:185][0m |          -0.0042 |          41.9463 |          19.7609 |
[32m[20221214 14:04:27 @agent_ppo2.py:185][0m |          -0.0025 |          40.7196 |          19.7616 |
[32m[20221214 14:04:27 @agent_ppo2.py:185][0m |          -0.0046 |          39.5515 |          19.7638 |
[32m[20221214 14:04:27 @agent_ppo2.py:185][0m |          -0.0050 |          38.7479 |          19.7631 |
[32m[20221214 14:04:27 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:04:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.77
[32m[20221214 14:04:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.07
[32m[20221214 14:04:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.21
[32m[20221214 14:04:27 @agent_ppo2.py:143][0m Total time:       6.42 min
[32m[20221214 14:04:27 @agent_ppo2.py:145][0m 573440 total steps have happened
[32m[20221214 14:04:27 @agent_ppo2.py:121][0m #------------------------ Iteration 280 --------------------------#
[32m[20221214 14:04:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:04:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:28 @agent_ppo2.py:185][0m |          -0.0011 |          56.2691 |          19.7273 |
[32m[20221214 14:04:28 @agent_ppo2.py:185][0m |          -0.0028 |          48.8933 |          19.7218 |
[32m[20221214 14:04:28 @agent_ppo2.py:185][0m |          -0.0064 |          45.4999 |          19.7145 |
[32m[20221214 14:04:28 @agent_ppo2.py:185][0m |          -0.0032 |          43.5408 |          19.7071 |
[32m[20221214 14:04:28 @agent_ppo2.py:185][0m |          -0.0010 |          41.9366 |          19.7039 |
[32m[20221214 14:04:28 @agent_ppo2.py:185][0m |          -0.0020 |          40.9344 |          19.6965 |
[32m[20221214 14:04:28 @agent_ppo2.py:185][0m |          -0.0017 |          39.8922 |          19.6973 |
[32m[20221214 14:04:28 @agent_ppo2.py:185][0m |          -0.0014 |          39.1826 |          19.6974 |
[32m[20221214 14:04:29 @agent_ppo2.py:185][0m |          -0.0039 |          38.5184 |          19.6999 |
[32m[20221214 14:04:29 @agent_ppo2.py:185][0m |          -0.0029 |          37.8285 |          19.6942 |
[32m[20221214 14:04:29 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:04:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.36
[32m[20221214 14:04:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 562.22
[32m[20221214 14:04:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.86
[32m[20221214 14:04:29 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 613.86
[32m[20221214 14:04:29 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 613.86
[32m[20221214 14:04:29 @agent_ppo2.py:143][0m Total time:       6.45 min
[32m[20221214 14:04:29 @agent_ppo2.py:145][0m 575488 total steps have happened
[32m[20221214 14:04:29 @agent_ppo2.py:121][0m #------------------------ Iteration 281 --------------------------#
[32m[20221214 14:04:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:04:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:29 @agent_ppo2.py:185][0m |           0.0008 |          84.7678 |          19.5890 |
[32m[20221214 14:04:29 @agent_ppo2.py:185][0m |          -0.0022 |          64.7857 |          19.5763 |
[32m[20221214 14:04:29 @agent_ppo2.py:185][0m |          -0.0035 |          58.3185 |          19.5729 |
[32m[20221214 14:04:29 @agent_ppo2.py:185][0m |          -0.0033 |          54.6831 |          19.5611 |
[32m[20221214 14:04:30 @agent_ppo2.py:185][0m |           0.0027 |          52.4640 |          19.5490 |
[32m[20221214 14:04:30 @agent_ppo2.py:185][0m |          -0.0045 |          50.3078 |          19.5465 |
[32m[20221214 14:04:30 @agent_ppo2.py:185][0m |           0.0014 |          48.4293 |          19.5342 |
[32m[20221214 14:04:30 @agent_ppo2.py:185][0m |          -0.0060 |          46.8604 |          19.5294 |
[32m[20221214 14:04:30 @agent_ppo2.py:185][0m |          -0.0036 |          45.4101 |          19.5270 |
[32m[20221214 14:04:30 @agent_ppo2.py:185][0m |          -0.0030 |          44.3948 |          19.5205 |
[32m[20221214 14:04:30 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:04:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.83
[32m[20221214 14:04:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.11
[32m[20221214 14:04:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.18
[32m[20221214 14:04:30 @agent_ppo2.py:143][0m Total time:       6.47 min
[32m[20221214 14:04:30 @agent_ppo2.py:145][0m 577536 total steps have happened
[32m[20221214 14:04:30 @agent_ppo2.py:121][0m #------------------------ Iteration 282 --------------------------#
[32m[20221214 14:04:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:31 @agent_ppo2.py:185][0m |          -0.0061 |          38.0971 |          19.5949 |
[32m[20221214 14:04:31 @agent_ppo2.py:185][0m |          -0.0066 |          27.1068 |          19.5845 |
[32m[20221214 14:04:31 @agent_ppo2.py:185][0m |          -0.0071 |          23.4879 |          19.5961 |
[32m[20221214 14:04:31 @agent_ppo2.py:185][0m |          -0.0079 |          21.3766 |          19.6059 |
[32m[20221214 14:04:31 @agent_ppo2.py:185][0m |          -0.0102 |          19.9767 |          19.6163 |
[32m[20221214 14:04:31 @agent_ppo2.py:185][0m |          -0.0074 |          18.9100 |          19.6251 |
[32m[20221214 14:04:31 @agent_ppo2.py:185][0m |           0.0001 |          20.1129 |          19.6302 |
[32m[20221214 14:04:31 @agent_ppo2.py:185][0m |          -0.0062 |          17.4661 |          19.6348 |
[32m[20221214 14:04:31 @agent_ppo2.py:185][0m |          -0.0091 |          16.8958 |          19.6470 |
[32m[20221214 14:04:31 @agent_ppo2.py:185][0m |          -0.0080 |          16.4089 |          19.6524 |
[32m[20221214 14:04:31 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:04:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 532.65
[32m[20221214 14:04:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 654.88
[32m[20221214 14:04:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.17
[32m[20221214 14:04:32 @agent_ppo2.py:143][0m Total time:       6.49 min
[32m[20221214 14:04:32 @agent_ppo2.py:145][0m 579584 total steps have happened
[32m[20221214 14:04:32 @agent_ppo2.py:121][0m #------------------------ Iteration 283 --------------------------#
[32m[20221214 14:04:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:32 @agent_ppo2.py:185][0m |          -0.0026 |          39.7413 |          19.6775 |
[32m[20221214 14:04:32 @agent_ppo2.py:185][0m |           0.0058 |          33.2549 |          19.6877 |
[32m[20221214 14:04:32 @agent_ppo2.py:185][0m |          -0.0012 |          29.1750 |          19.6912 |
[32m[20221214 14:04:32 @agent_ppo2.py:185][0m |          -0.0040 |          26.2500 |          19.6989 |
[32m[20221214 14:04:32 @agent_ppo2.py:185][0m |          -0.0061 |          24.7166 |          19.6978 |
[32m[20221214 14:04:32 @agent_ppo2.py:185][0m |          -0.0012 |          23.8874 |          19.7003 |
[32m[20221214 14:04:32 @agent_ppo2.py:185][0m |           0.0013 |          23.5559 |          19.7028 |
[32m[20221214 14:04:33 @agent_ppo2.py:185][0m |          -0.0063 |          22.1232 |          19.7100 |
[32m[20221214 14:04:33 @agent_ppo2.py:185][0m |          -0.0062 |          21.5260 |          19.7084 |
[32m[20221214 14:04:33 @agent_ppo2.py:185][0m |          -0.0079 |          20.9040 |          19.7116 |
[32m[20221214 14:04:33 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:04:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 507.95
[32m[20221214 14:04:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 619.02
[32m[20221214 14:04:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.46
[32m[20221214 14:04:33 @agent_ppo2.py:143][0m Total time:       6.52 min
[32m[20221214 14:04:33 @agent_ppo2.py:145][0m 581632 total steps have happened
[32m[20221214 14:04:33 @agent_ppo2.py:121][0m #------------------------ Iteration 284 --------------------------#
[32m[20221214 14:04:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:33 @agent_ppo2.py:185][0m |          -0.0039 |          58.6123 |          19.6852 |
[32m[20221214 14:04:33 @agent_ppo2.py:185][0m |          -0.0041 |          50.1155 |          19.6862 |
[32m[20221214 14:04:34 @agent_ppo2.py:185][0m |          -0.0046 |          46.3028 |          19.6862 |
[32m[20221214 14:04:34 @agent_ppo2.py:185][0m |          -0.0005 |          43.8123 |          19.6902 |
[32m[20221214 14:04:34 @agent_ppo2.py:185][0m |          -0.0022 |          42.0921 |          19.6922 |
[32m[20221214 14:04:34 @agent_ppo2.py:185][0m |           0.0005 |          40.9797 |          19.6892 |
[32m[20221214 14:04:34 @agent_ppo2.py:185][0m |          -0.0024 |          39.4529 |          19.6916 |
[32m[20221214 14:04:34 @agent_ppo2.py:185][0m |          -0.0049 |          38.6467 |          19.6961 |
[32m[20221214 14:04:34 @agent_ppo2.py:185][0m |          -0.0066 |          37.7956 |          19.6965 |
[32m[20221214 14:04:34 @agent_ppo2.py:185][0m |           0.0169 |          47.2850 |          19.7014 |
[32m[20221214 14:04:34 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:04:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.55
[32m[20221214 14:04:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 620.43
[32m[20221214 14:04:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 563.90
[32m[20221214 14:04:34 @agent_ppo2.py:143][0m Total time:       6.54 min
[32m[20221214 14:04:34 @agent_ppo2.py:145][0m 583680 total steps have happened
[32m[20221214 14:04:34 @agent_ppo2.py:121][0m #------------------------ Iteration 285 --------------------------#
[32m[20221214 14:04:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:35 @agent_ppo2.py:185][0m |           0.0003 |          75.0366 |          19.6819 |
[32m[20221214 14:04:35 @agent_ppo2.py:185][0m |           0.0118 |          65.9365 |          19.6714 |
[32m[20221214 14:04:35 @agent_ppo2.py:185][0m |          -0.0045 |          51.8794 |          19.6611 |
[32m[20221214 14:04:35 @agent_ppo2.py:185][0m |          -0.0035 |          49.0805 |          19.6548 |
[32m[20221214 14:04:35 @agent_ppo2.py:185][0m |          -0.0037 |          46.6078 |          19.6517 |
[32m[20221214 14:04:35 @agent_ppo2.py:185][0m |          -0.0043 |          45.0623 |          19.6531 |
[32m[20221214 14:04:35 @agent_ppo2.py:185][0m |          -0.0026 |          43.9724 |          19.6517 |
[32m[20221214 14:04:35 @agent_ppo2.py:185][0m |          -0.0041 |          42.9183 |          19.6532 |
[32m[20221214 14:04:35 @agent_ppo2.py:185][0m |          -0.0074 |          41.8576 |          19.6482 |
[32m[20221214 14:04:36 @agent_ppo2.py:185][0m |          -0.0051 |          41.0880 |          19.6488 |
[32m[20221214 14:04:36 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:04:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.47
[32m[20221214 14:04:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.78
[32m[20221214 14:04:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.50
[32m[20221214 14:04:36 @agent_ppo2.py:143][0m Total time:       6.56 min
[32m[20221214 14:04:36 @agent_ppo2.py:145][0m 585728 total steps have happened
[32m[20221214 14:04:36 @agent_ppo2.py:121][0m #------------------------ Iteration 286 --------------------------#
[32m[20221214 14:04:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:36 @agent_ppo2.py:185][0m |          -0.0037 |          85.9335 |          19.7857 |
[32m[20221214 14:04:36 @agent_ppo2.py:185][0m |          -0.0012 |          75.0633 |          19.7784 |
[32m[20221214 14:04:36 @agent_ppo2.py:185][0m |           0.0043 |          71.2802 |          19.7773 |
[32m[20221214 14:04:36 @agent_ppo2.py:185][0m |           0.0056 |          70.9743 |          19.7708 |
[32m[20221214 14:04:36 @agent_ppo2.py:185][0m |          -0.0064 |          61.8951 |          19.7658 |
[32m[20221214 14:04:37 @agent_ppo2.py:185][0m |          -0.0024 |          61.4255 |          19.7620 |
[32m[20221214 14:04:37 @agent_ppo2.py:185][0m |          -0.0091 |          57.3048 |          19.7540 |
[32m[20221214 14:04:37 @agent_ppo2.py:185][0m |          -0.0071 |          56.1164 |          19.7516 |
[32m[20221214 14:04:37 @agent_ppo2.py:185][0m |          -0.0090 |          54.7658 |          19.7470 |
[32m[20221214 14:04:37 @agent_ppo2.py:185][0m |          -0.0100 |          54.0805 |          19.7474 |
[32m[20221214 14:04:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:04:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.23
[32m[20221214 14:04:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 650.57
[32m[20221214 14:04:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.92
[32m[20221214 14:04:37 @agent_ppo2.py:143][0m Total time:       6.58 min
[32m[20221214 14:04:37 @agent_ppo2.py:145][0m 587776 total steps have happened
[32m[20221214 14:04:37 @agent_ppo2.py:121][0m #------------------------ Iteration 287 --------------------------#
[32m[20221214 14:04:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:37 @agent_ppo2.py:185][0m |          -0.0038 |          96.5999 |          19.6204 |
[32m[20221214 14:04:38 @agent_ppo2.py:185][0m |          -0.0008 |          82.9235 |          19.6068 |
[32m[20221214 14:04:38 @agent_ppo2.py:185][0m |          -0.0048 |          77.2341 |          19.5990 |
[32m[20221214 14:04:38 @agent_ppo2.py:185][0m |           0.0008 |          74.0703 |          19.5909 |
[32m[20221214 14:04:38 @agent_ppo2.py:185][0m |          -0.0036 |          71.0822 |          19.5831 |
[32m[20221214 14:04:38 @agent_ppo2.py:185][0m |          -0.0042 |          69.4774 |          19.5798 |
[32m[20221214 14:04:38 @agent_ppo2.py:185][0m |          -0.0024 |          68.1500 |          19.5684 |
[32m[20221214 14:04:38 @agent_ppo2.py:185][0m |          -0.0041 |          67.3686 |          19.5599 |
[32m[20221214 14:04:38 @agent_ppo2.py:185][0m |          -0.0013 |          66.4588 |          19.5510 |
[32m[20221214 14:04:38 @agent_ppo2.py:185][0m |          -0.0039 |          65.2318 |          19.5414 |
[32m[20221214 14:04:38 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:04:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 589.26
[32m[20221214 14:04:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.02
[32m[20221214 14:04:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.42
[32m[20221214 14:04:38 @agent_ppo2.py:143][0m Total time:       6.61 min
[32m[20221214 14:04:38 @agent_ppo2.py:145][0m 589824 total steps have happened
[32m[20221214 14:04:38 @agent_ppo2.py:121][0m #------------------------ Iteration 288 --------------------------#
[32m[20221214 14:04:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:39 @agent_ppo2.py:185][0m |          -0.0009 |          83.8734 |          19.5826 |
[32m[20221214 14:04:39 @agent_ppo2.py:185][0m |          -0.0045 |          69.7655 |          19.5821 |
[32m[20221214 14:04:39 @agent_ppo2.py:185][0m |           0.0160 |          75.5425 |          19.5860 |
[32m[20221214 14:04:39 @agent_ppo2.py:185][0m |          -0.0056 |          62.3659 |          19.5913 |
[32m[20221214 14:04:39 @agent_ppo2.py:185][0m |          -0.0053 |          59.9039 |          19.5981 |
[32m[20221214 14:04:39 @agent_ppo2.py:185][0m |          -0.0074 |          58.0413 |          19.5982 |
[32m[20221214 14:04:39 @agent_ppo2.py:185][0m |          -0.0051 |          56.8823 |          19.6031 |
[32m[20221214 14:04:39 @agent_ppo2.py:185][0m |          -0.0066 |          55.7154 |          19.6079 |
[32m[20221214 14:04:40 @agent_ppo2.py:185][0m |           0.0011 |          57.2603 |          19.6150 |
[32m[20221214 14:04:40 @agent_ppo2.py:185][0m |          -0.0067 |          54.3412 |          19.6187 |
[32m[20221214 14:04:40 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:04:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 508.25
[32m[20221214 14:04:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 559.05
[32m[20221214 14:04:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.53
[32m[20221214 14:04:40 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 813.53
[32m[20221214 14:04:40 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 813.53
[32m[20221214 14:04:40 @agent_ppo2.py:143][0m Total time:       6.63 min
[32m[20221214 14:04:40 @agent_ppo2.py:145][0m 591872 total steps have happened
[32m[20221214 14:04:40 @agent_ppo2.py:121][0m #------------------------ Iteration 289 --------------------------#
[32m[20221214 14:04:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:40 @agent_ppo2.py:185][0m |          -0.0020 |         105.5884 |          19.6799 |
[32m[20221214 14:04:40 @agent_ppo2.py:185][0m |          -0.0024 |          94.4674 |          19.6665 |
[32m[20221214 14:04:40 @agent_ppo2.py:185][0m |          -0.0007 |          91.5119 |          19.6408 |
[32m[20221214 14:04:40 @agent_ppo2.py:185][0m |          -0.0041 |          89.9499 |          19.6315 |
[32m[20221214 14:04:41 @agent_ppo2.py:185][0m |           0.0036 |          91.3011 |          19.6210 |
[32m[20221214 14:04:41 @agent_ppo2.py:185][0m |          -0.0020 |          87.3970 |          19.6154 |
[32m[20221214 14:04:41 @agent_ppo2.py:185][0m |          -0.0015 |          86.7365 |          19.6056 |
[32m[20221214 14:04:41 @agent_ppo2.py:185][0m |          -0.0059 |          85.4702 |          19.5986 |
[32m[20221214 14:04:41 @agent_ppo2.py:185][0m |          -0.0022 |          84.8347 |          19.5959 |
[32m[20221214 14:04:41 @agent_ppo2.py:185][0m |          -0.0043 |          84.1662 |          19.5802 |
[32m[20221214 14:04:41 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:04:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 595.74
[32m[20221214 14:04:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 646.06
[32m[20221214 14:04:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.11
[32m[20221214 14:04:41 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 818.11
[32m[20221214 14:04:41 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 818.11
[32m[20221214 14:04:41 @agent_ppo2.py:143][0m Total time:       6.65 min
[32m[20221214 14:04:41 @agent_ppo2.py:145][0m 593920 total steps have happened
[32m[20221214 14:04:41 @agent_ppo2.py:121][0m #------------------------ Iteration 290 --------------------------#
[32m[20221214 14:04:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:04:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:42 @agent_ppo2.py:185][0m |           0.0013 |         125.4900 |          19.5784 |
[32m[20221214 14:04:42 @agent_ppo2.py:185][0m |          -0.0044 |         113.9301 |          19.5784 |
[32m[20221214 14:04:42 @agent_ppo2.py:185][0m |          -0.0025 |         109.4622 |          19.5673 |
[32m[20221214 14:04:42 @agent_ppo2.py:185][0m |          -0.0033 |         106.3951 |          19.5571 |
[32m[20221214 14:04:42 @agent_ppo2.py:185][0m |          -0.0025 |         104.4866 |          19.5511 |
[32m[20221214 14:04:42 @agent_ppo2.py:185][0m |          -0.0030 |         103.3464 |          19.5492 |
[32m[20221214 14:04:42 @agent_ppo2.py:185][0m |          -0.0037 |         102.1257 |          19.5339 |
[32m[20221214 14:04:42 @agent_ppo2.py:185][0m |          -0.0019 |         101.3123 |          19.5270 |
[32m[20221214 14:04:42 @agent_ppo2.py:185][0m |          -0.0034 |         100.5753 |          19.5220 |
[32m[20221214 14:04:42 @agent_ppo2.py:185][0m |          -0.0048 |         100.2444 |          19.5202 |
[32m[20221214 14:04:42 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:04:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 717.02
[32m[20221214 14:04:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.51
[32m[20221214 14:04:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.28
[32m[20221214 14:04:43 @agent_ppo2.py:143][0m Total time:       6.68 min
[32m[20221214 14:04:43 @agent_ppo2.py:145][0m 595968 total steps have happened
[32m[20221214 14:04:43 @agent_ppo2.py:121][0m #------------------------ Iteration 291 --------------------------#
[32m[20221214 14:04:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:04:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:43 @agent_ppo2.py:185][0m |          -0.0003 |          96.9493 |          19.5338 |
[32m[20221214 14:04:43 @agent_ppo2.py:185][0m |          -0.0024 |          76.2354 |          19.5357 |
[32m[20221214 14:04:43 @agent_ppo2.py:185][0m |          -0.0009 |          69.5402 |          19.5324 |
[32m[20221214 14:04:43 @agent_ppo2.py:185][0m |          -0.0040 |          65.9474 |          19.5296 |
[32m[20221214 14:04:43 @agent_ppo2.py:185][0m |          -0.0043 |          63.6502 |          19.5247 |
[32m[20221214 14:04:44 @agent_ppo2.py:185][0m |          -0.0045 |          61.6526 |          19.5253 |
[32m[20221214 14:04:44 @agent_ppo2.py:185][0m |          -0.0007 |          59.9546 |          19.5271 |
[32m[20221214 14:04:44 @agent_ppo2.py:185][0m |          -0.0054 |          58.3988 |          19.5211 |
[32m[20221214 14:04:44 @agent_ppo2.py:185][0m |          -0.0026 |          56.9019 |          19.5225 |
[32m[20221214 14:04:44 @agent_ppo2.py:185][0m |          -0.0040 |          55.9273 |          19.5266 |
[32m[20221214 14:04:44 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:04:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.11
[32m[20221214 14:04:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 601.95
[32m[20221214 14:04:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.47
[32m[20221214 14:04:44 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 824.47
[32m[20221214 14:04:44 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 824.47
[32m[20221214 14:04:44 @agent_ppo2.py:143][0m Total time:       6.70 min
[32m[20221214 14:04:44 @agent_ppo2.py:145][0m 598016 total steps have happened
[32m[20221214 14:04:44 @agent_ppo2.py:121][0m #------------------------ Iteration 292 --------------------------#
[32m[20221214 14:04:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:44 @agent_ppo2.py:185][0m |          -0.0018 |          88.7864 |          19.5802 |
[32m[20221214 14:04:45 @agent_ppo2.py:185][0m |          -0.0005 |          75.8295 |          19.5560 |
[32m[20221214 14:04:45 @agent_ppo2.py:185][0m |          -0.0021 |          70.8257 |          19.5585 |
[32m[20221214 14:04:45 @agent_ppo2.py:185][0m |           0.0046 |          69.7123 |          19.5530 |
[32m[20221214 14:04:45 @agent_ppo2.py:185][0m |          -0.0027 |          65.2007 |          19.5487 |
[32m[20221214 14:04:45 @agent_ppo2.py:185][0m |          -0.0023 |          63.3591 |          19.5486 |
[32m[20221214 14:04:45 @agent_ppo2.py:185][0m |          -0.0018 |          61.8676 |          19.5418 |
[32m[20221214 14:04:45 @agent_ppo2.py:185][0m |           0.0022 |          61.5984 |          19.5368 |
[32m[20221214 14:04:45 @agent_ppo2.py:185][0m |          -0.0068 |          59.5947 |          19.5383 |
[32m[20221214 14:04:45 @agent_ppo2.py:185][0m |          -0.0035 |          58.4006 |          19.5355 |
[32m[20221214 14:04:45 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:04:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 560.63
[32m[20221214 14:04:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 665.49
[32m[20221214 14:04:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 544.12
[32m[20221214 14:04:45 @agent_ppo2.py:143][0m Total time:       6.72 min
[32m[20221214 14:04:45 @agent_ppo2.py:145][0m 600064 total steps have happened
[32m[20221214 14:04:45 @agent_ppo2.py:121][0m #------------------------ Iteration 293 --------------------------#
[32m[20221214 14:04:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:46 @agent_ppo2.py:185][0m |           0.0023 |          87.8992 |          19.4088 |
[32m[20221214 14:04:46 @agent_ppo2.py:185][0m |           0.0000 |          76.6777 |          19.3923 |
[32m[20221214 14:04:46 @agent_ppo2.py:185][0m |          -0.0049 |          72.5633 |          19.3819 |
[32m[20221214 14:04:46 @agent_ppo2.py:185][0m |          -0.0045 |          69.9299 |          19.3941 |
[32m[20221214 14:04:46 @agent_ppo2.py:185][0m |          -0.0054 |          67.5393 |          19.3915 |
[32m[20221214 14:04:46 @agent_ppo2.py:185][0m |          -0.0044 |          66.0911 |          19.3938 |
[32m[20221214 14:04:46 @agent_ppo2.py:185][0m |          -0.0068 |          65.0869 |          19.3962 |
[32m[20221214 14:04:46 @agent_ppo2.py:185][0m |          -0.0034 |          64.8918 |          19.3987 |
[32m[20221214 14:04:47 @agent_ppo2.py:185][0m |          -0.0022 |          63.6057 |          19.4012 |
[32m[20221214 14:04:47 @agent_ppo2.py:185][0m |          -0.0002 |          63.3592 |          19.4052 |
[32m[20221214 14:04:47 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:04:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 579.82
[32m[20221214 14:04:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 725.16
[32m[20221214 14:04:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.06
[32m[20221214 14:04:47 @agent_ppo2.py:143][0m Total time:       6.75 min
[32m[20221214 14:04:47 @agent_ppo2.py:145][0m 602112 total steps have happened
[32m[20221214 14:04:47 @agent_ppo2.py:121][0m #------------------------ Iteration 294 --------------------------#
[32m[20221214 14:04:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:47 @agent_ppo2.py:185][0m |          -0.0002 |         118.2199 |          19.4983 |
[32m[20221214 14:04:47 @agent_ppo2.py:185][0m |           0.0063 |         116.9800 |          19.5035 |
[32m[20221214 14:04:47 @agent_ppo2.py:185][0m |          -0.0055 |         109.9060 |          19.5150 |
[32m[20221214 14:04:47 @agent_ppo2.py:185][0m |          -0.0045 |         108.6175 |          19.5159 |
[32m[20221214 14:04:47 @agent_ppo2.py:185][0m |          -0.0018 |         107.2716 |          19.5168 |
[32m[20221214 14:04:48 @agent_ppo2.py:185][0m |          -0.0010 |         105.8729 |          19.5185 |
[32m[20221214 14:04:48 @agent_ppo2.py:185][0m |          -0.0010 |         104.6352 |          19.5231 |
[32m[20221214 14:04:48 @agent_ppo2.py:185][0m |          -0.0039 |         104.0615 |          19.5161 |
[32m[20221214 14:04:48 @agent_ppo2.py:185][0m |           0.0065 |         106.1368 |          19.5180 |
[32m[20221214 14:04:48 @agent_ppo2.py:185][0m |           0.0012 |         103.2254 |          19.5175 |
[32m[20221214 14:04:48 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:04:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 634.97
[32m[20221214 14:04:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.37
[32m[20221214 14:04:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.72
[32m[20221214 14:04:48 @agent_ppo2.py:143][0m Total time:       6.77 min
[32m[20221214 14:04:48 @agent_ppo2.py:145][0m 604160 total steps have happened
[32m[20221214 14:04:48 @agent_ppo2.py:121][0m #------------------------ Iteration 295 --------------------------#
[32m[20221214 14:04:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:48 @agent_ppo2.py:185][0m |          -0.0018 |         122.3675 |          19.4920 |
[32m[20221214 14:04:49 @agent_ppo2.py:185][0m |          -0.0044 |         107.9802 |          19.4816 |
[32m[20221214 14:04:49 @agent_ppo2.py:185][0m |          -0.0048 |         100.3867 |          19.4814 |
[32m[20221214 14:04:49 @agent_ppo2.py:185][0m |          -0.0045 |          96.3955 |          19.4707 |
[32m[20221214 14:04:49 @agent_ppo2.py:185][0m |          -0.0029 |          94.2077 |          19.4662 |
[32m[20221214 14:04:49 @agent_ppo2.py:185][0m |          -0.0072 |          91.8284 |          19.4570 |
[32m[20221214 14:04:49 @agent_ppo2.py:185][0m |          -0.0007 |          91.7336 |          19.4523 |
[32m[20221214 14:04:49 @agent_ppo2.py:185][0m |          -0.0045 |          89.2424 |          19.4436 |
[32m[20221214 14:04:49 @agent_ppo2.py:185][0m |           0.0061 |          98.4061 |          19.4399 |
[32m[20221214 14:04:49 @agent_ppo2.py:185][0m |          -0.0005 |          88.0376 |          19.4258 |
[32m[20221214 14:04:49 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:04:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.97
[32m[20221214 14:04:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 746.96
[32m[20221214 14:04:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.71
[32m[20221214 14:04:50 @agent_ppo2.py:143][0m Total time:       6.79 min
[32m[20221214 14:04:50 @agent_ppo2.py:145][0m 606208 total steps have happened
[32m[20221214 14:04:50 @agent_ppo2.py:121][0m #------------------------ Iteration 296 --------------------------#
[32m[20221214 14:04:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:04:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:50 @agent_ppo2.py:185][0m |          -0.0008 |         136.1026 |          19.4249 |
[32m[20221214 14:04:50 @agent_ppo2.py:185][0m |          -0.0019 |         118.6385 |          19.4131 |
[32m[20221214 14:04:50 @agent_ppo2.py:185][0m |           0.0072 |         115.5317 |          19.4071 |
[32m[20221214 14:04:50 @agent_ppo2.py:185][0m |          -0.0044 |         107.1921 |          19.4003 |
[32m[20221214 14:04:50 @agent_ppo2.py:185][0m |          -0.0041 |         103.8648 |          19.3988 |
[32m[20221214 14:04:50 @agent_ppo2.py:185][0m |          -0.0019 |         101.1823 |          19.3935 |
[32m[20221214 14:04:50 @agent_ppo2.py:185][0m |           0.0030 |         101.1944 |          19.3910 |
[32m[20221214 14:04:50 @agent_ppo2.py:185][0m |           0.0031 |         109.3451 |          19.3864 |
[32m[20221214 14:04:51 @agent_ppo2.py:185][0m |          -0.0016 |          96.8315 |          19.3827 |
[32m[20221214 14:04:51 @agent_ppo2.py:185][0m |          -0.0046 |          95.5380 |          19.3786 |
[32m[20221214 14:04:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:04:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 561.03
[32m[20221214 14:04:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.87
[32m[20221214 14:04:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 759.00
[32m[20221214 14:04:51 @agent_ppo2.py:143][0m Total time:       6.81 min
[32m[20221214 14:04:51 @agent_ppo2.py:145][0m 608256 total steps have happened
[32m[20221214 14:04:51 @agent_ppo2.py:121][0m #------------------------ Iteration 297 --------------------------#
[32m[20221214 14:04:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:51 @agent_ppo2.py:185][0m |           0.0004 |          96.4323 |          19.4369 |
[32m[20221214 14:04:51 @agent_ppo2.py:185][0m |          -0.0013 |          83.2911 |          19.4398 |
[32m[20221214 14:04:51 @agent_ppo2.py:185][0m |          -0.0063 |          75.4091 |          19.4451 |
[32m[20221214 14:04:51 @agent_ppo2.py:185][0m |          -0.0052 |          70.3957 |          19.4482 |
[32m[20221214 14:04:51 @agent_ppo2.py:185][0m |           0.0072 |          72.2713 |          19.4566 |
[32m[20221214 14:04:52 @agent_ppo2.py:185][0m |          -0.0039 |          64.8195 |          19.4640 |
[32m[20221214 14:04:52 @agent_ppo2.py:185][0m |          -0.0023 |          63.0081 |          19.4694 |
[32m[20221214 14:04:52 @agent_ppo2.py:185][0m |          -0.0007 |          61.2670 |          19.4751 |
[32m[20221214 14:04:52 @agent_ppo2.py:185][0m |          -0.0047 |          59.8241 |          19.4798 |
[32m[20221214 14:04:52 @agent_ppo2.py:185][0m |          -0.0078 |          58.6218 |          19.4876 |
[32m[20221214 14:04:52 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:04:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.87
[32m[20221214 14:04:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 729.43
[32m[20221214 14:04:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 724.03
[32m[20221214 14:04:52 @agent_ppo2.py:143][0m Total time:       6.83 min
[32m[20221214 14:04:52 @agent_ppo2.py:145][0m 610304 total steps have happened
[32m[20221214 14:04:52 @agent_ppo2.py:121][0m #------------------------ Iteration 298 --------------------------#
[32m[20221214 14:04:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:04:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:52 @agent_ppo2.py:185][0m |           0.0013 |         133.8489 |          19.4331 |
[32m[20221214 14:04:53 @agent_ppo2.py:185][0m |          -0.0002 |         127.9920 |          19.4411 |
[32m[20221214 14:04:53 @agent_ppo2.py:185][0m |          -0.0008 |         126.0396 |          19.4422 |
[32m[20221214 14:04:53 @agent_ppo2.py:185][0m |          -0.0036 |         123.6248 |          19.4402 |
[32m[20221214 14:04:53 @agent_ppo2.py:185][0m |          -0.0020 |         122.1533 |          19.4444 |
[32m[20221214 14:04:53 @agent_ppo2.py:185][0m |          -0.0029 |         121.1602 |          19.4521 |
[32m[20221214 14:04:53 @agent_ppo2.py:185][0m |          -0.0008 |         119.8776 |          19.4508 |
[32m[20221214 14:04:53 @agent_ppo2.py:185][0m |          -0.0022 |         119.0985 |          19.4520 |
[32m[20221214 14:04:53 @agent_ppo2.py:185][0m |          -0.0020 |         118.1982 |          19.4537 |
[32m[20221214 14:04:53 @agent_ppo2.py:185][0m |          -0.0039 |         117.8634 |          19.4541 |
[32m[20221214 14:04:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:04:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 693.25
[32m[20221214 14:04:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.89
[32m[20221214 14:04:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 687.83
[32m[20221214 14:04:53 @agent_ppo2.py:143][0m Total time:       6.85 min
[32m[20221214 14:04:53 @agent_ppo2.py:145][0m 612352 total steps have happened
[32m[20221214 14:04:53 @agent_ppo2.py:121][0m #------------------------ Iteration 299 --------------------------#
[32m[20221214 14:04:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:04:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:54 @agent_ppo2.py:185][0m |          -0.0011 |         138.1086 |          19.5008 |
[32m[20221214 14:04:54 @agent_ppo2.py:185][0m |          -0.0037 |         125.9516 |          19.4929 |
[32m[20221214 14:04:54 @agent_ppo2.py:185][0m |          -0.0025 |         121.8070 |          19.4941 |
[32m[20221214 14:04:54 @agent_ppo2.py:185][0m |          -0.0041 |         119.7062 |          19.4893 |
[32m[20221214 14:04:54 @agent_ppo2.py:185][0m |          -0.0016 |         117.8384 |          19.4880 |
[32m[20221214 14:04:54 @agent_ppo2.py:185][0m |           0.0005 |         117.5430 |          19.4793 |
[32m[20221214 14:04:54 @agent_ppo2.py:185][0m |           0.0031 |         117.5757 |          19.4822 |
[32m[20221214 14:04:54 @agent_ppo2.py:185][0m |          -0.0056 |         114.0379 |          19.4804 |
[32m[20221214 14:04:54 @agent_ppo2.py:185][0m |          -0.0015 |         112.9980 |          19.4846 |
[32m[20221214 14:04:54 @agent_ppo2.py:185][0m |          -0.0028 |         112.4082 |          19.4824 |
[32m[20221214 14:04:54 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:04:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 625.65
[32m[20221214 14:04:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.78
[32m[20221214 14:04:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 718.97
[32m[20221214 14:04:55 @agent_ppo2.py:143][0m Total time:       6.88 min
[32m[20221214 14:04:55 @agent_ppo2.py:145][0m 614400 total steps have happened
[32m[20221214 14:04:55 @agent_ppo2.py:121][0m #------------------------ Iteration 300 --------------------------#
[32m[20221214 14:04:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:04:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:55 @agent_ppo2.py:185][0m |          -0.0026 |         134.3666 |          19.4365 |
[32m[20221214 14:04:55 @agent_ppo2.py:185][0m |          -0.0013 |         126.2510 |          19.4346 |
[32m[20221214 14:04:55 @agent_ppo2.py:185][0m |          -0.0016 |         123.7314 |          19.4231 |
[32m[20221214 14:04:55 @agent_ppo2.py:185][0m |          -0.0005 |         122.0548 |          19.4234 |
[32m[20221214 14:04:55 @agent_ppo2.py:185][0m |          -0.0021 |         120.6258 |          19.4238 |
[32m[20221214 14:04:55 @agent_ppo2.py:185][0m |          -0.0003 |         119.6104 |          19.4238 |
[32m[20221214 14:04:55 @agent_ppo2.py:185][0m |          -0.0031 |         118.5058 |          19.4196 |
[32m[20221214 14:04:56 @agent_ppo2.py:185][0m |          -0.0035 |         117.8301 |          19.4159 |
[32m[20221214 14:04:56 @agent_ppo2.py:185][0m |           0.0024 |         118.7148 |          19.4109 |
[32m[20221214 14:04:56 @agent_ppo2.py:185][0m |           0.0002 |         117.0605 |          19.4170 |
[32m[20221214 14:04:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:04:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 668.91
[32m[20221214 14:04:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 758.68
[32m[20221214 14:04:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 718.93
[32m[20221214 14:04:56 @agent_ppo2.py:143][0m Total time:       6.90 min
[32m[20221214 14:04:56 @agent_ppo2.py:145][0m 616448 total steps have happened
[32m[20221214 14:04:56 @agent_ppo2.py:121][0m #------------------------ Iteration 301 --------------------------#
[32m[20221214 14:04:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:04:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:56 @agent_ppo2.py:185][0m |          -0.0029 |         119.5052 |          19.3921 |
[32m[20221214 14:04:56 @agent_ppo2.py:185][0m |          -0.0015 |         113.7422 |          19.3844 |
[32m[20221214 14:04:56 @agent_ppo2.py:185][0m |          -0.0019 |         110.4059 |          19.3776 |
[32m[20221214 14:04:56 @agent_ppo2.py:185][0m |          -0.0048 |         108.2100 |          19.3707 |
[32m[20221214 14:04:57 @agent_ppo2.py:185][0m |          -0.0027 |         106.0138 |          19.3667 |
[32m[20221214 14:04:57 @agent_ppo2.py:185][0m |          -0.0032 |         104.5386 |          19.3575 |
[32m[20221214 14:04:57 @agent_ppo2.py:185][0m |          -0.0037 |         102.8121 |          19.3553 |
[32m[20221214 14:04:57 @agent_ppo2.py:185][0m |           0.0003 |         102.1017 |          19.3467 |
[32m[20221214 14:04:57 @agent_ppo2.py:185][0m |          -0.0034 |         100.3037 |          19.3424 |
[32m[20221214 14:04:57 @agent_ppo2.py:185][0m |          -0.0034 |         100.0402 |          19.3376 |
[32m[20221214 14:04:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:04:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 630.97
[32m[20221214 14:04:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 779.30
[32m[20221214 14:04:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.89
[32m[20221214 14:04:57 @agent_ppo2.py:143][0m Total time:       6.92 min
[32m[20221214 14:04:57 @agent_ppo2.py:145][0m 618496 total steps have happened
[32m[20221214 14:04:57 @agent_ppo2.py:121][0m #------------------------ Iteration 302 --------------------------#
[32m[20221214 14:04:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:04:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:57 @agent_ppo2.py:185][0m |          -0.0026 |         130.1601 |          19.4577 |
[32m[20221214 14:04:58 @agent_ppo2.py:185][0m |          -0.0014 |         121.1337 |          19.4586 |
[32m[20221214 14:04:58 @agent_ppo2.py:185][0m |          -0.0018 |         117.9324 |          19.4563 |
[32m[20221214 14:04:58 @agent_ppo2.py:185][0m |          -0.0047 |         115.3768 |          19.4553 |
[32m[20221214 14:04:58 @agent_ppo2.py:185][0m |           0.0101 |         124.7513 |          19.4588 |
[32m[20221214 14:04:58 @agent_ppo2.py:185][0m |           0.0060 |         113.8535 |          19.4663 |
[32m[20221214 14:04:58 @agent_ppo2.py:185][0m |           0.0123 |         128.6825 |          19.4666 |
[32m[20221214 14:04:58 @agent_ppo2.py:185][0m |          -0.0008 |         107.2057 |          19.4624 |
[32m[20221214 14:04:58 @agent_ppo2.py:185][0m |          -0.0021 |         105.5717 |          19.4662 |
[32m[20221214 14:04:58 @agent_ppo2.py:185][0m |          -0.0018 |         104.5552 |          19.4698 |
[32m[20221214 14:04:58 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:04:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.60
[32m[20221214 14:04:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.24
[32m[20221214 14:04:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.73
[32m[20221214 14:04:58 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 841.73
[32m[20221214 14:04:58 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 841.73
[32m[20221214 14:04:58 @agent_ppo2.py:143][0m Total time:       6.94 min
[32m[20221214 14:04:58 @agent_ppo2.py:145][0m 620544 total steps have happened
[32m[20221214 14:04:58 @agent_ppo2.py:121][0m #------------------------ Iteration 303 --------------------------#
[32m[20221214 14:04:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:04:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:04:59 @agent_ppo2.py:185][0m |          -0.0012 |         143.1573 |          19.3833 |
[32m[20221214 14:04:59 @agent_ppo2.py:185][0m |           0.0025 |         140.2468 |          19.3800 |
[32m[20221214 14:04:59 @agent_ppo2.py:185][0m |          -0.0029 |         135.7572 |          19.3803 |
[32m[20221214 14:04:59 @agent_ppo2.py:185][0m |          -0.0031 |         134.2312 |          19.3788 |
[32m[20221214 14:04:59 @agent_ppo2.py:185][0m |          -0.0026 |         133.0459 |          19.3725 |
[32m[20221214 14:04:59 @agent_ppo2.py:185][0m |          -0.0028 |         131.8937 |          19.3738 |
[32m[20221214 14:04:59 @agent_ppo2.py:185][0m |          -0.0019 |         130.8997 |          19.3725 |
[32m[20221214 14:04:59 @agent_ppo2.py:185][0m |          -0.0046 |         130.2991 |          19.3675 |
[32m[20221214 14:04:59 @agent_ppo2.py:185][0m |          -0.0041 |         129.6973 |          19.3655 |
[32m[20221214 14:04:59 @agent_ppo2.py:185][0m |          -0.0039 |         128.8907 |          19.3655 |
[32m[20221214 14:04:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.72
[32m[20221214 14:05:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.31
[32m[20221214 14:05:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.75
[32m[20221214 14:05:00 @agent_ppo2.py:143][0m Total time:       6.96 min
[32m[20221214 14:05:00 @agent_ppo2.py:145][0m 622592 total steps have happened
[32m[20221214 14:05:00 @agent_ppo2.py:121][0m #------------------------ Iteration 304 --------------------------#
[32m[20221214 14:05:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:00 @agent_ppo2.py:185][0m |          -0.0020 |         135.4085 |          19.3784 |
[32m[20221214 14:05:00 @agent_ppo2.py:185][0m |          -0.0015 |         117.7332 |          19.3760 |
[32m[20221214 14:05:00 @agent_ppo2.py:185][0m |          -0.0015 |         112.0804 |          19.3863 |
[32m[20221214 14:05:00 @agent_ppo2.py:185][0m |           0.0101 |         124.0415 |          19.3911 |
[32m[20221214 14:05:00 @agent_ppo2.py:185][0m |           0.0035 |         107.4242 |          19.3833 |
[32m[20221214 14:05:00 @agent_ppo2.py:185][0m |          -0.0041 |         105.8650 |          19.3881 |
[32m[20221214 14:05:00 @agent_ppo2.py:185][0m |           0.0007 |         106.2123 |          19.3904 |
[32m[20221214 14:05:00 @agent_ppo2.py:185][0m |          -0.0024 |         103.8761 |          19.3967 |
[32m[20221214 14:05:01 @agent_ppo2.py:185][0m |          -0.0050 |         102.9143 |          19.3986 |
[32m[20221214 14:05:01 @agent_ppo2.py:185][0m |           0.0040 |         108.5920 |          19.4007 |
[32m[20221214 14:05:01 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:05:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 615.50
[32m[20221214 14:05:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.77
[32m[20221214 14:05:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.36
[32m[20221214 14:05:01 @agent_ppo2.py:143][0m Total time:       6.98 min
[32m[20221214 14:05:01 @agent_ppo2.py:145][0m 624640 total steps have happened
[32m[20221214 14:05:01 @agent_ppo2.py:121][0m #------------------------ Iteration 305 --------------------------#
[32m[20221214 14:05:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:01 @agent_ppo2.py:185][0m |           0.0051 |         155.1502 |          19.5162 |
[32m[20221214 14:05:01 @agent_ppo2.py:185][0m |          -0.0022 |         140.9639 |          19.5066 |
[32m[20221214 14:05:01 @agent_ppo2.py:185][0m |          -0.0047 |         139.1851 |          19.5100 |
[32m[20221214 14:05:01 @agent_ppo2.py:185][0m |          -0.0003 |         139.3342 |          19.5033 |
[32m[20221214 14:05:01 @agent_ppo2.py:185][0m |          -0.0047 |         136.3724 |          19.5021 |
[32m[20221214 14:05:02 @agent_ppo2.py:185][0m |           0.0087 |         145.0705 |          19.4996 |
[32m[20221214 14:05:02 @agent_ppo2.py:185][0m |          -0.0033 |         134.9328 |          19.4915 |
[32m[20221214 14:05:02 @agent_ppo2.py:185][0m |          -0.0055 |         134.0358 |          19.4936 |
[32m[20221214 14:05:02 @agent_ppo2.py:185][0m |           0.0006 |         134.7281 |          19.4909 |
[32m[20221214 14:05:02 @agent_ppo2.py:185][0m |           0.0032 |         137.5372 |          19.4880 |
[32m[20221214 14:05:02 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:05:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 701.71
[32m[20221214 14:05:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.76
[32m[20221214 14:05:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 771.52
[32m[20221214 14:05:02 @agent_ppo2.py:143][0m Total time:       7.00 min
[32m[20221214 14:05:02 @agent_ppo2.py:145][0m 626688 total steps have happened
[32m[20221214 14:05:02 @agent_ppo2.py:121][0m #------------------------ Iteration 306 --------------------------#
[32m[20221214 14:05:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:02 @agent_ppo2.py:185][0m |          -0.0014 |         140.6225 |          19.4324 |
[32m[20221214 14:05:02 @agent_ppo2.py:185][0m |          -0.0026 |         136.6272 |          19.4341 |
[32m[20221214 14:05:03 @agent_ppo2.py:185][0m |           0.0014 |         134.7526 |          19.4323 |
[32m[20221214 14:05:03 @agent_ppo2.py:185][0m |          -0.0016 |         132.2165 |          19.4341 |
[32m[20221214 14:05:03 @agent_ppo2.py:185][0m |          -0.0026 |         130.6432 |          19.4374 |
[32m[20221214 14:05:03 @agent_ppo2.py:185][0m |          -0.0001 |         129.2536 |          19.4356 |
[32m[20221214 14:05:03 @agent_ppo2.py:185][0m |          -0.0038 |         128.2846 |          19.4385 |
[32m[20221214 14:05:03 @agent_ppo2.py:185][0m |           0.0011 |         127.1757 |          19.4385 |
[32m[20221214 14:05:03 @agent_ppo2.py:185][0m |           0.0015 |         127.4819 |          19.4329 |
[32m[20221214 14:05:03 @agent_ppo2.py:185][0m |          -0.0018 |         124.9007 |          19.4355 |
[32m[20221214 14:05:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 717.02
[32m[20221214 14:05:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 757.91
[32m[20221214 14:05:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.18
[32m[20221214 14:05:03 @agent_ppo2.py:143][0m Total time:       7.02 min
[32m[20221214 14:05:03 @agent_ppo2.py:145][0m 628736 total steps have happened
[32m[20221214 14:05:03 @agent_ppo2.py:121][0m #------------------------ Iteration 307 --------------------------#
[32m[20221214 14:05:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:04 @agent_ppo2.py:185][0m |           0.0058 |         155.6557 |          19.5279 |
[32m[20221214 14:05:04 @agent_ppo2.py:185][0m |          -0.0038 |         140.3025 |          19.5194 |
[32m[20221214 14:05:04 @agent_ppo2.py:185][0m |          -0.0045 |         137.6394 |          19.5126 |
[32m[20221214 14:05:04 @agent_ppo2.py:185][0m |          -0.0058 |         135.3950 |          19.5043 |
[32m[20221214 14:05:04 @agent_ppo2.py:185][0m |           0.0070 |         148.8157 |          19.5017 |
[32m[20221214 14:05:04 @agent_ppo2.py:185][0m |          -0.0046 |         132.0851 |          19.4858 |
[32m[20221214 14:05:04 @agent_ppo2.py:185][0m |          -0.0047 |         130.7462 |          19.4859 |
[32m[20221214 14:05:04 @agent_ppo2.py:185][0m |          -0.0047 |         129.8018 |          19.4793 |
[32m[20221214 14:05:04 @agent_ppo2.py:185][0m |          -0.0036 |         129.1592 |          19.4801 |
[32m[20221214 14:05:04 @agent_ppo2.py:185][0m |           0.0035 |         141.6919 |          19.4659 |
[32m[20221214 14:05:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 753.21
[32m[20221214 14:05:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.85
[32m[20221214 14:05:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 696.20
[32m[20221214 14:05:05 @agent_ppo2.py:143][0m Total time:       7.04 min
[32m[20221214 14:05:05 @agent_ppo2.py:145][0m 630784 total steps have happened
[32m[20221214 14:05:05 @agent_ppo2.py:121][0m #------------------------ Iteration 308 --------------------------#
[32m[20221214 14:05:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:05 @agent_ppo2.py:185][0m |          -0.0012 |         150.3269 |          19.3936 |
[32m[20221214 14:05:05 @agent_ppo2.py:185][0m |           0.0006 |         144.2141 |          19.3845 |
[32m[20221214 14:05:05 @agent_ppo2.py:185][0m |          -0.0031 |         141.9713 |          19.3718 |
[32m[20221214 14:05:05 @agent_ppo2.py:185][0m |           0.0107 |         149.6559 |          19.3648 |
[32m[20221214 14:05:05 @agent_ppo2.py:185][0m |           0.0002 |         139.3948 |          19.3506 |
[32m[20221214 14:05:05 @agent_ppo2.py:185][0m |           0.0014 |         139.0340 |          19.3317 |
[32m[20221214 14:05:05 @agent_ppo2.py:185][0m |          -0.0014 |         137.6992 |          19.3211 |
[32m[20221214 14:05:05 @agent_ppo2.py:185][0m |          -0.0039 |         137.3147 |          19.3172 |
[32m[20221214 14:05:06 @agent_ppo2.py:185][0m |          -0.0042 |         136.6878 |          19.3037 |
[32m[20221214 14:05:06 @agent_ppo2.py:185][0m |          -0.0014 |         136.0132 |          19.2994 |
[32m[20221214 14:05:06 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 653.39
[32m[20221214 14:05:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 709.39
[32m[20221214 14:05:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 788.19
[32m[20221214 14:05:06 @agent_ppo2.py:143][0m Total time:       7.06 min
[32m[20221214 14:05:06 @agent_ppo2.py:145][0m 632832 total steps have happened
[32m[20221214 14:05:06 @agent_ppo2.py:121][0m #------------------------ Iteration 309 --------------------------#
[32m[20221214 14:05:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:06 @agent_ppo2.py:185][0m |          -0.0009 |         151.9751 |          19.2397 |
[32m[20221214 14:05:06 @agent_ppo2.py:185][0m |          -0.0042 |         146.2422 |          19.2126 |
[32m[20221214 14:05:06 @agent_ppo2.py:185][0m |          -0.0038 |         143.7054 |          19.2099 |
[32m[20221214 14:05:06 @agent_ppo2.py:185][0m |          -0.0040 |         142.3630 |          19.1975 |
[32m[20221214 14:05:06 @agent_ppo2.py:185][0m |          -0.0049 |         141.2191 |          19.1915 |
[32m[20221214 14:05:06 @agent_ppo2.py:185][0m |          -0.0044 |         140.3703 |          19.1737 |
[32m[20221214 14:05:07 @agent_ppo2.py:185][0m |          -0.0012 |         140.1697 |          19.1709 |
[32m[20221214 14:05:07 @agent_ppo2.py:185][0m |          -0.0038 |         139.4378 |          19.1549 |
[32m[20221214 14:05:07 @agent_ppo2.py:185][0m |          -0.0056 |         139.0653 |          19.1392 |
[32m[20221214 14:05:07 @agent_ppo2.py:185][0m |          -0.0029 |         138.6002 |          19.1425 |
[32m[20221214 14:05:07 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 729.87
[32m[20221214 14:05:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.21
[32m[20221214 14:05:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.64
[32m[20221214 14:05:07 @agent_ppo2.py:143][0m Total time:       7.08 min
[32m[20221214 14:05:07 @agent_ppo2.py:145][0m 634880 total steps have happened
[32m[20221214 14:05:07 @agent_ppo2.py:121][0m #------------------------ Iteration 310 --------------------------#
[32m[20221214 14:05:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:05:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:07 @agent_ppo2.py:185][0m |          -0.0005 |         151.9536 |          19.2339 |
[32m[20221214 14:05:07 @agent_ppo2.py:185][0m |          -0.0033 |         147.2779 |          19.2474 |
[32m[20221214 14:05:07 @agent_ppo2.py:185][0m |          -0.0007 |         144.9155 |          19.2370 |
[32m[20221214 14:05:08 @agent_ppo2.py:185][0m |          -0.0021 |         143.4158 |          19.2363 |
[32m[20221214 14:05:08 @agent_ppo2.py:185][0m |          -0.0035 |         142.3410 |          19.2302 |
[32m[20221214 14:05:08 @agent_ppo2.py:185][0m |          -0.0015 |         141.5181 |          19.2281 |
[32m[20221214 14:05:08 @agent_ppo2.py:185][0m |          -0.0013 |         140.7092 |          19.2276 |
[32m[20221214 14:05:08 @agent_ppo2.py:185][0m |          -0.0013 |         140.3115 |          19.2113 |
[32m[20221214 14:05:08 @agent_ppo2.py:185][0m |          -0.0024 |         139.7185 |          19.2187 |
[32m[20221214 14:05:08 @agent_ppo2.py:185][0m |          -0.0029 |         139.5194 |          19.2130 |
[32m[20221214 14:05:08 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 763.28
[32m[20221214 14:05:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 779.52
[32m[20221214 14:05:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.70
[32m[20221214 14:05:08 @agent_ppo2.py:143][0m Total time:       7.10 min
[32m[20221214 14:05:08 @agent_ppo2.py:145][0m 636928 total steps have happened
[32m[20221214 14:05:08 @agent_ppo2.py:121][0m #------------------------ Iteration 311 --------------------------#
[32m[20221214 14:05:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:09 @agent_ppo2.py:185][0m |          -0.0037 |         153.5011 |          19.1627 |
[32m[20221214 14:05:09 @agent_ppo2.py:185][0m |          -0.0022 |         145.6026 |          19.1594 |
[32m[20221214 14:05:09 @agent_ppo2.py:185][0m |           0.0075 |         161.4610 |          19.1574 |
[32m[20221214 14:05:09 @agent_ppo2.py:185][0m |          -0.0020 |         142.9082 |          19.1594 |
[32m[20221214 14:05:09 @agent_ppo2.py:185][0m |          -0.0052 |         141.5021 |          19.1619 |
[32m[20221214 14:05:09 @agent_ppo2.py:185][0m |          -0.0029 |         141.0309 |          19.1689 |
[32m[20221214 14:05:09 @agent_ppo2.py:185][0m |          -0.0047 |         139.5655 |          19.1678 |
[32m[20221214 14:05:09 @agent_ppo2.py:185][0m |          -0.0029 |         139.2087 |          19.1721 |
[32m[20221214 14:05:09 @agent_ppo2.py:185][0m |          -0.0001 |         141.8286 |          19.1775 |
[32m[20221214 14:05:09 @agent_ppo2.py:185][0m |          -0.0060 |         138.2974 |          19.1841 |
[32m[20221214 14:05:09 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 737.70
[32m[20221214 14:05:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.76
[32m[20221214 14:05:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 772.28
[32m[20221214 14:05:09 @agent_ppo2.py:143][0m Total time:       7.12 min
[32m[20221214 14:05:09 @agent_ppo2.py:145][0m 638976 total steps have happened
[32m[20221214 14:05:09 @agent_ppo2.py:121][0m #------------------------ Iteration 312 --------------------------#
[32m[20221214 14:05:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:10 @agent_ppo2.py:185][0m |          -0.0002 |         154.5789 |          19.2439 |
[32m[20221214 14:05:10 @agent_ppo2.py:185][0m |          -0.0021 |         150.2069 |          19.2439 |
[32m[20221214 14:05:10 @agent_ppo2.py:185][0m |          -0.0007 |         147.4161 |          19.2464 |
[32m[20221214 14:05:10 @agent_ppo2.py:185][0m |          -0.0025 |         145.5315 |          19.2348 |
[32m[20221214 14:05:10 @agent_ppo2.py:185][0m |          -0.0021 |         144.3076 |          19.2407 |
[32m[20221214 14:05:10 @agent_ppo2.py:185][0m |          -0.0004 |         143.0830 |          19.2385 |
[32m[20221214 14:05:10 @agent_ppo2.py:185][0m |          -0.0025 |         142.1628 |          19.2322 |
[32m[20221214 14:05:10 @agent_ppo2.py:185][0m |          -0.0036 |         141.4007 |          19.2272 |
[32m[20221214 14:05:10 @agent_ppo2.py:185][0m |          -0.0025 |         140.5979 |          19.2296 |
[32m[20221214 14:05:11 @agent_ppo2.py:185][0m |          -0.0038 |         139.9664 |          19.2224 |
[32m[20221214 14:05:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:05:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 709.83
[32m[20221214 14:05:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 772.79
[32m[20221214 14:05:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.99
[32m[20221214 14:05:11 @agent_ppo2.py:143][0m Total time:       7.14 min
[32m[20221214 14:05:11 @agent_ppo2.py:145][0m 641024 total steps have happened
[32m[20221214 14:05:11 @agent_ppo2.py:121][0m #------------------------ Iteration 313 --------------------------#
[32m[20221214 14:05:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:05:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:11 @agent_ppo2.py:185][0m |          -0.0008 |         153.1116 |          19.2799 |
[32m[20221214 14:05:11 @agent_ppo2.py:185][0m |          -0.0033 |         146.9747 |          19.2613 |
[32m[20221214 14:05:11 @agent_ppo2.py:185][0m |          -0.0036 |         144.8688 |          19.2462 |
[32m[20221214 14:05:11 @agent_ppo2.py:185][0m |          -0.0030 |         143.9366 |          19.2334 |
[32m[20221214 14:05:11 @agent_ppo2.py:185][0m |          -0.0022 |         143.0129 |          19.2254 |
[32m[20221214 14:05:11 @agent_ppo2.py:185][0m |          -0.0031 |         142.0990 |          19.2167 |
[32m[20221214 14:05:12 @agent_ppo2.py:185][0m |           0.0155 |         162.1155 |          19.2173 |
[32m[20221214 14:05:12 @agent_ppo2.py:185][0m |          -0.0036 |         140.8674 |          19.2076 |
[32m[20221214 14:05:12 @agent_ppo2.py:185][0m |          -0.0028 |         140.1346 |          19.2049 |
[32m[20221214 14:05:12 @agent_ppo2.py:185][0m |          -0.0036 |         139.6045 |          19.1993 |
[32m[20221214 14:05:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:05:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.99
[32m[20221214 14:05:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.69
[32m[20221214 14:05:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 747.67
[32m[20221214 14:05:12 @agent_ppo2.py:143][0m Total time:       7.17 min
[32m[20221214 14:05:12 @agent_ppo2.py:145][0m 643072 total steps have happened
[32m[20221214 14:05:12 @agent_ppo2.py:121][0m #------------------------ Iteration 314 --------------------------#
[32m[20221214 14:05:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:05:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:12 @agent_ppo2.py:185][0m |          -0.0013 |         151.8672 |          19.1676 |
[32m[20221214 14:05:12 @agent_ppo2.py:185][0m |          -0.0036 |         147.7580 |          19.1658 |
[32m[20221214 14:05:13 @agent_ppo2.py:185][0m |          -0.0005 |         145.1516 |          19.1773 |
[32m[20221214 14:05:13 @agent_ppo2.py:185][0m |          -0.0031 |         142.6390 |          19.1788 |
[32m[20221214 14:05:13 @agent_ppo2.py:185][0m |          -0.0013 |         141.0090 |          19.1771 |
[32m[20221214 14:05:13 @agent_ppo2.py:185][0m |          -0.0023 |         139.8774 |          19.1812 |
[32m[20221214 14:05:13 @agent_ppo2.py:185][0m |          -0.0026 |         138.5722 |          19.1823 |
[32m[20221214 14:05:13 @agent_ppo2.py:185][0m |          -0.0033 |         138.1172 |          19.1865 |
[32m[20221214 14:05:13 @agent_ppo2.py:185][0m |          -0.0033 |         137.9345 |          19.1873 |
[32m[20221214 14:05:13 @agent_ppo2.py:185][0m |          -0.0022 |         136.9104 |          19.1872 |
[32m[20221214 14:05:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:05:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.47
[32m[20221214 14:05:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.84
[32m[20221214 14:05:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 602.16
[32m[20221214 14:05:13 @agent_ppo2.py:143][0m Total time:       7.19 min
[32m[20221214 14:05:13 @agent_ppo2.py:145][0m 645120 total steps have happened
[32m[20221214 14:05:13 @agent_ppo2.py:121][0m #------------------------ Iteration 315 --------------------------#
[32m[20221214 14:05:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:14 @agent_ppo2.py:185][0m |          -0.0030 |         154.7718 |          19.1786 |
[32m[20221214 14:05:14 @agent_ppo2.py:185][0m |           0.0011 |         147.9006 |          19.1642 |
[32m[20221214 14:05:14 @agent_ppo2.py:185][0m |          -0.0027 |         144.0131 |          19.1738 |
[32m[20221214 14:05:14 @agent_ppo2.py:185][0m |          -0.0044 |         140.8328 |          19.1588 |
[32m[20221214 14:05:14 @agent_ppo2.py:185][0m |          -0.0039 |         138.9070 |          19.1690 |
[32m[20221214 14:05:14 @agent_ppo2.py:185][0m |          -0.0050 |         137.2442 |          19.1654 |
[32m[20221214 14:05:14 @agent_ppo2.py:185][0m |          -0.0014 |         136.1719 |          19.1581 |
[32m[20221214 14:05:14 @agent_ppo2.py:185][0m |          -0.0029 |         135.0485 |          19.1708 |
[32m[20221214 14:05:14 @agent_ppo2.py:185][0m |          -0.0037 |         134.1698 |          19.1520 |
[32m[20221214 14:05:14 @agent_ppo2.py:185][0m |          -0.0040 |         133.6209 |          19.1582 |
[32m[20221214 14:05:14 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 704.71
[32m[20221214 14:05:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.31
[32m[20221214 14:05:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 712.49
[32m[20221214 14:05:15 @agent_ppo2.py:143][0m Total time:       7.21 min
[32m[20221214 14:05:15 @agent_ppo2.py:145][0m 647168 total steps have happened
[32m[20221214 14:05:15 @agent_ppo2.py:121][0m #------------------------ Iteration 316 --------------------------#
[32m[20221214 14:05:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:05:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:15 @agent_ppo2.py:185][0m |          -0.0017 |         153.8733 |          19.1115 |
[32m[20221214 14:05:15 @agent_ppo2.py:185][0m |          -0.0035 |         148.7746 |          19.1019 |
[32m[20221214 14:05:15 @agent_ppo2.py:185][0m |          -0.0005 |         146.3822 |          19.0944 |
[32m[20221214 14:05:15 @agent_ppo2.py:185][0m |           0.0141 |         165.7487 |          19.0786 |
[32m[20221214 14:05:15 @agent_ppo2.py:185][0m |          -0.0014 |         142.9443 |          19.0743 |
[32m[20221214 14:05:15 @agent_ppo2.py:185][0m |          -0.0035 |         140.8276 |          19.0654 |
[32m[20221214 14:05:15 @agent_ppo2.py:185][0m |          -0.0062 |         139.7472 |          19.0575 |
[32m[20221214 14:05:15 @agent_ppo2.py:185][0m |          -0.0037 |         138.8284 |          19.0497 |
[32m[20221214 14:05:16 @agent_ppo2.py:185][0m |          -0.0040 |         137.8532 |          19.0481 |
[32m[20221214 14:05:16 @agent_ppo2.py:185][0m |          -0.0045 |         136.9998 |          19.0377 |
[32m[20221214 14:05:16 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:05:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 695.51
[32m[20221214 14:05:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 755.36
[32m[20221214 14:05:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 694.47
[32m[20221214 14:05:16 @agent_ppo2.py:143][0m Total time:       7.23 min
[32m[20221214 14:05:16 @agent_ppo2.py:145][0m 649216 total steps have happened
[32m[20221214 14:05:16 @agent_ppo2.py:121][0m #------------------------ Iteration 317 --------------------------#
[32m[20221214 14:05:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:16 @agent_ppo2.py:185][0m |          -0.0025 |         160.3190 |          19.0659 |
[32m[20221214 14:05:16 @agent_ppo2.py:185][0m |          -0.0040 |         142.7492 |          19.0459 |
[32m[20221214 14:05:16 @agent_ppo2.py:185][0m |          -0.0054 |         135.4625 |          19.0541 |
[32m[20221214 14:05:16 @agent_ppo2.py:185][0m |          -0.0048 |         129.9324 |          19.0413 |
[32m[20221214 14:05:16 @agent_ppo2.py:185][0m |           0.0070 |         141.0818 |          19.0374 |
[32m[20221214 14:05:17 @agent_ppo2.py:185][0m |          -0.0008 |         123.0382 |          19.0426 |
[32m[20221214 14:05:17 @agent_ppo2.py:185][0m |          -0.0032 |         119.6561 |          19.0365 |
[32m[20221214 14:05:17 @agent_ppo2.py:185][0m |          -0.0016 |         116.9598 |          19.0324 |
[32m[20221214 14:05:17 @agent_ppo2.py:185][0m |          -0.0032 |         114.3291 |          19.0325 |
[32m[20221214 14:05:17 @agent_ppo2.py:185][0m |           0.0054 |         116.6790 |          19.0368 |
[32m[20221214 14:05:17 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 604.77
[32m[20221214 14:05:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.84
[32m[20221214 14:05:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 643.51
[32m[20221214 14:05:17 @agent_ppo2.py:143][0m Total time:       7.25 min
[32m[20221214 14:05:17 @agent_ppo2.py:145][0m 651264 total steps have happened
[32m[20221214 14:05:17 @agent_ppo2.py:121][0m #------------------------ Iteration 318 --------------------------#
[32m[20221214 14:05:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:17 @agent_ppo2.py:185][0m |          -0.0032 |         144.9930 |          19.0600 |
[32m[20221214 14:05:17 @agent_ppo2.py:185][0m |          -0.0027 |         136.1240 |          19.0405 |
[32m[20221214 14:05:18 @agent_ppo2.py:185][0m |           0.0010 |         134.5549 |          19.0329 |
[32m[20221214 14:05:18 @agent_ppo2.py:185][0m |          -0.0023 |         130.7866 |          19.0111 |
[32m[20221214 14:05:18 @agent_ppo2.py:185][0m |           0.0063 |         136.9369 |          19.0108 |
[32m[20221214 14:05:18 @agent_ppo2.py:185][0m |          -0.0012 |         128.3726 |          19.0015 |
[32m[20221214 14:05:18 @agent_ppo2.py:185][0m |          -0.0004 |         127.1812 |          18.9952 |
[32m[20221214 14:05:18 @agent_ppo2.py:185][0m |           0.0000 |         126.4868 |          18.9908 |
[32m[20221214 14:05:18 @agent_ppo2.py:185][0m |          -0.0019 |         125.7997 |          18.9932 |
[32m[20221214 14:05:18 @agent_ppo2.py:185][0m |          -0.0034 |         124.7472 |          18.9807 |
[32m[20221214 14:05:18 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 644.34
[32m[20221214 14:05:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 764.65
[32m[20221214 14:05:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 748.83
[32m[20221214 14:05:18 @agent_ppo2.py:143][0m Total time:       7.27 min
[32m[20221214 14:05:18 @agent_ppo2.py:145][0m 653312 total steps have happened
[32m[20221214 14:05:18 @agent_ppo2.py:121][0m #------------------------ Iteration 319 --------------------------#
[32m[20221214 14:05:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:19 @agent_ppo2.py:185][0m |          -0.0027 |         128.6727 |          18.9161 |
[32m[20221214 14:05:19 @agent_ppo2.py:185][0m |          -0.0040 |         120.2053 |          18.9229 |
[32m[20221214 14:05:19 @agent_ppo2.py:185][0m |          -0.0032 |         116.9229 |          18.9300 |
[32m[20221214 14:05:19 @agent_ppo2.py:185][0m |          -0.0039 |         114.6052 |          18.9354 |
[32m[20221214 14:05:19 @agent_ppo2.py:185][0m |          -0.0052 |         112.6019 |          18.9490 |
[32m[20221214 14:05:19 @agent_ppo2.py:185][0m |          -0.0046 |         110.8827 |          18.9499 |
[32m[20221214 14:05:19 @agent_ppo2.py:185][0m |           0.0025 |         110.3066 |          18.9596 |
[32m[20221214 14:05:19 @agent_ppo2.py:185][0m |          -0.0033 |         108.8391 |          18.9787 |
[32m[20221214 14:05:19 @agent_ppo2.py:185][0m |          -0.0018 |         109.0228 |          18.9805 |
[32m[20221214 14:05:19 @agent_ppo2.py:185][0m |           0.0039 |         111.9282 |          18.9930 |
[32m[20221214 14:05:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 751.36
[32m[20221214 14:05:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 784.89
[32m[20221214 14:05:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 736.50
[32m[20221214 14:05:19 @agent_ppo2.py:143][0m Total time:       7.29 min
[32m[20221214 14:05:19 @agent_ppo2.py:145][0m 655360 total steps have happened
[32m[20221214 14:05:19 @agent_ppo2.py:121][0m #------------------------ Iteration 320 --------------------------#
[32m[20221214 14:05:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:05:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:20 @agent_ppo2.py:185][0m |           0.0037 |         172.3087 |          19.0893 |
[32m[20221214 14:05:20 @agent_ppo2.py:185][0m |          -0.0034 |         164.2650 |          19.0755 |
[32m[20221214 14:05:20 @agent_ppo2.py:185][0m |           0.0019 |         159.6142 |          19.0663 |
[32m[20221214 14:05:20 @agent_ppo2.py:185][0m |          -0.0023 |         154.4704 |          19.0533 |
[32m[20221214 14:05:20 @agent_ppo2.py:185][0m |          -0.0041 |         151.4761 |          19.0320 |
[32m[20221214 14:05:20 @agent_ppo2.py:185][0m |          -0.0038 |         149.0047 |          19.0288 |
[32m[20221214 14:05:20 @agent_ppo2.py:185][0m |          -0.0006 |         147.4270 |          19.0170 |
[32m[20221214 14:05:20 @agent_ppo2.py:185][0m |          -0.0004 |         146.2522 |          19.0138 |
[32m[20221214 14:05:21 @agent_ppo2.py:185][0m |           0.0034 |         147.2204 |          19.0112 |
[32m[20221214 14:05:21 @agent_ppo2.py:185][0m |           0.0027 |         144.6487 |          18.9999 |
[32m[20221214 14:05:21 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 676.47
[32m[20221214 14:05:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 704.78
[32m[20221214 14:05:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 784.51
[32m[20221214 14:05:21 @agent_ppo2.py:143][0m Total time:       7.31 min
[32m[20221214 14:05:21 @agent_ppo2.py:145][0m 657408 total steps have happened
[32m[20221214 14:05:21 @agent_ppo2.py:121][0m #------------------------ Iteration 321 --------------------------#
[32m[20221214 14:05:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:21 @agent_ppo2.py:185][0m |           0.0016 |         154.3747 |          18.9500 |
[32m[20221214 14:05:21 @agent_ppo2.py:185][0m |          -0.0011 |         140.8833 |          18.9390 |
[32m[20221214 14:05:21 @agent_ppo2.py:185][0m |           0.0051 |         140.1420 |          18.9277 |
[32m[20221214 14:05:21 @agent_ppo2.py:185][0m |          -0.0030 |         133.9011 |          18.9213 |
[32m[20221214 14:05:21 @agent_ppo2.py:185][0m |          -0.0042 |         132.1445 |          18.9088 |
[32m[20221214 14:05:21 @agent_ppo2.py:185][0m |           0.0013 |         131.6666 |          18.9027 |
[32m[20221214 14:05:22 @agent_ppo2.py:185][0m |          -0.0027 |         129.7563 |          18.8936 |
[32m[20221214 14:05:22 @agent_ppo2.py:185][0m |          -0.0027 |         128.9100 |          18.8868 |
[32m[20221214 14:05:22 @agent_ppo2.py:185][0m |          -0.0033 |         127.9545 |          18.8737 |
[32m[20221214 14:05:22 @agent_ppo2.py:185][0m |          -0.0046 |         127.1661 |          18.8695 |
[32m[20221214 14:05:22 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 696.49
[32m[20221214 14:05:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 754.52
[32m[20221214 14:05:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 775.39
[32m[20221214 14:05:22 @agent_ppo2.py:143][0m Total time:       7.33 min
[32m[20221214 14:05:22 @agent_ppo2.py:145][0m 659456 total steps have happened
[32m[20221214 14:05:22 @agent_ppo2.py:121][0m #------------------------ Iteration 322 --------------------------#
[32m[20221214 14:05:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:22 @agent_ppo2.py:185][0m |          -0.0032 |         155.9235 |          18.8842 |
[32m[20221214 14:05:22 @agent_ppo2.py:185][0m |           0.0073 |         166.2862 |          18.8801 |
[32m[20221214 14:05:22 @agent_ppo2.py:185][0m |          -0.0034 |         145.5249 |          18.8804 |
[32m[20221214 14:05:23 @agent_ppo2.py:185][0m |          -0.0050 |         144.2940 |          18.8801 |
[32m[20221214 14:05:23 @agent_ppo2.py:185][0m |           0.0059 |         150.3752 |          18.8762 |
[32m[20221214 14:05:23 @agent_ppo2.py:185][0m |          -0.0047 |         142.1843 |          18.8769 |
[32m[20221214 14:05:23 @agent_ppo2.py:185][0m |           0.0103 |         152.6400 |          18.8777 |
[32m[20221214 14:05:23 @agent_ppo2.py:185][0m |          -0.0023 |         140.5469 |          18.8712 |
[32m[20221214 14:05:23 @agent_ppo2.py:185][0m |           0.0040 |         146.5539 |          18.8652 |
[32m[20221214 14:05:23 @agent_ppo2.py:185][0m |          -0.0037 |         139.3244 |          18.8693 |
[32m[20221214 14:05:23 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 695.98
[32m[20221214 14:05:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.86
[32m[20221214 14:05:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 692.83
[32m[20221214 14:05:23 @agent_ppo2.py:143][0m Total time:       7.35 min
[32m[20221214 14:05:23 @agent_ppo2.py:145][0m 661504 total steps have happened
[32m[20221214 14:05:23 @agent_ppo2.py:121][0m #------------------------ Iteration 323 --------------------------#
[32m[20221214 14:05:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:24 @agent_ppo2.py:185][0m |          -0.0018 |         159.3738 |          18.9189 |
[32m[20221214 14:05:24 @agent_ppo2.py:185][0m |           0.0001 |         149.7637 |          18.8987 |
[32m[20221214 14:05:24 @agent_ppo2.py:185][0m |          -0.0013 |         144.1401 |          18.8984 |
[32m[20221214 14:05:24 @agent_ppo2.py:185][0m |          -0.0022 |         139.1235 |          18.8853 |
[32m[20221214 14:05:24 @agent_ppo2.py:185][0m |           0.0114 |         152.1921 |          18.8748 |
[32m[20221214 14:05:24 @agent_ppo2.py:185][0m |           0.0017 |         133.3908 |          18.8629 |
[32m[20221214 14:05:24 @agent_ppo2.py:185][0m |           0.0069 |         138.8777 |          18.8611 |
[32m[20221214 14:05:24 @agent_ppo2.py:185][0m |          -0.0027 |         127.1349 |          18.8558 |
[32m[20221214 14:05:24 @agent_ppo2.py:185][0m |          -0.0018 |         125.2575 |          18.8555 |
[32m[20221214 14:05:24 @agent_ppo2.py:185][0m |           0.0097 |         134.4508 |          18.8481 |
[32m[20221214 14:05:24 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 633.48
[32m[20221214 14:05:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 740.19
[32m[20221214 14:05:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 719.13
[32m[20221214 14:05:24 @agent_ppo2.py:143][0m Total time:       7.37 min
[32m[20221214 14:05:24 @agent_ppo2.py:145][0m 663552 total steps have happened
[32m[20221214 14:05:24 @agent_ppo2.py:121][0m #------------------------ Iteration 324 --------------------------#
[32m[20221214 14:05:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:25 @agent_ppo2.py:185][0m |           0.0002 |         162.3427 |          18.8405 |
[32m[20221214 14:05:25 @agent_ppo2.py:185][0m |          -0.0016 |         145.6774 |          18.8320 |
[32m[20221214 14:05:25 @agent_ppo2.py:185][0m |          -0.0022 |         139.0483 |          18.8324 |
[32m[20221214 14:05:25 @agent_ppo2.py:185][0m |          -0.0013 |         135.3832 |          18.8161 |
[32m[20221214 14:05:25 @agent_ppo2.py:185][0m |          -0.0026 |         132.4434 |          18.8086 |
[32m[20221214 14:05:25 @agent_ppo2.py:185][0m |          -0.0022 |         129.8497 |          18.7972 |
[32m[20221214 14:05:25 @agent_ppo2.py:185][0m |          -0.0027 |         128.0442 |          18.7867 |
[32m[20221214 14:05:25 @agent_ppo2.py:185][0m |          -0.0022 |         126.2039 |          18.7783 |
[32m[20221214 14:05:25 @agent_ppo2.py:185][0m |          -0.0011 |         124.5349 |          18.7571 |
[32m[20221214 14:05:26 @agent_ppo2.py:185][0m |          -0.0016 |         122.9627 |          18.7597 |
[32m[20221214 14:05:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 588.42
[32m[20221214 14:05:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 687.29
[32m[20221214 14:05:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 670.69
[32m[20221214 14:05:26 @agent_ppo2.py:143][0m Total time:       7.39 min
[32m[20221214 14:05:26 @agent_ppo2.py:145][0m 665600 total steps have happened
[32m[20221214 14:05:26 @agent_ppo2.py:121][0m #------------------------ Iteration 325 --------------------------#
[32m[20221214 14:05:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:26 @agent_ppo2.py:185][0m |          -0.0013 |         157.6431 |          18.7500 |
[32m[20221214 14:05:26 @agent_ppo2.py:185][0m |          -0.0034 |         150.9210 |          18.7459 |
[32m[20221214 14:05:26 @agent_ppo2.py:185][0m |           0.0012 |         149.8297 |          18.7390 |
[32m[20221214 14:05:26 @agent_ppo2.py:185][0m |          -0.0031 |         145.3019 |          18.7214 |
[32m[20221214 14:05:26 @agent_ppo2.py:185][0m |          -0.0034 |         143.7508 |          18.7162 |
[32m[20221214 14:05:26 @agent_ppo2.py:185][0m |          -0.0016 |         143.1828 |          18.7165 |
[32m[20221214 14:05:26 @agent_ppo2.py:185][0m |          -0.0067 |         141.7467 |          18.7038 |
[32m[20221214 14:05:27 @agent_ppo2.py:185][0m |          -0.0048 |         141.1617 |          18.6948 |
[32m[20221214 14:05:27 @agent_ppo2.py:185][0m |          -0.0033 |         140.1894 |          18.6957 |
[32m[20221214 14:05:27 @agent_ppo2.py:185][0m |          -0.0048 |         139.5050 |          18.6916 |
[32m[20221214 14:05:27 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 628.72
[32m[20221214 14:05:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 681.21
[32m[20221214 14:05:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 750.89
[32m[20221214 14:05:27 @agent_ppo2.py:143][0m Total time:       7.41 min
[32m[20221214 14:05:27 @agent_ppo2.py:145][0m 667648 total steps have happened
[32m[20221214 14:05:27 @agent_ppo2.py:121][0m #------------------------ Iteration 326 --------------------------#
[32m[20221214 14:05:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:27 @agent_ppo2.py:185][0m |          -0.0021 |         136.6786 |          18.7481 |
[32m[20221214 14:05:27 @agent_ppo2.py:185][0m |          -0.0021 |         128.7073 |          18.7676 |
[32m[20221214 14:05:27 @agent_ppo2.py:185][0m |          -0.0030 |         125.9523 |          18.7762 |
[32m[20221214 14:05:27 @agent_ppo2.py:185][0m |          -0.0030 |         124.3375 |          18.7813 |
[32m[20221214 14:05:28 @agent_ppo2.py:185][0m |          -0.0024 |         122.8429 |          18.7899 |
[32m[20221214 14:05:28 @agent_ppo2.py:185][0m |          -0.0015 |         121.4975 |          18.7962 |
[32m[20221214 14:05:28 @agent_ppo2.py:185][0m |           0.0003 |         121.7809 |          18.7996 |
[32m[20221214 14:05:28 @agent_ppo2.py:185][0m |          -0.0009 |         119.6099 |          18.8006 |
[32m[20221214 14:05:28 @agent_ppo2.py:185][0m |          -0.0035 |         119.7011 |          18.8023 |
[32m[20221214 14:05:28 @agent_ppo2.py:185][0m |          -0.0053 |         119.0266 |          18.8149 |
[32m[20221214 14:05:28 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:05:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 710.25
[32m[20221214 14:05:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.99
[32m[20221214 14:05:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.21
[32m[20221214 14:05:28 @agent_ppo2.py:143][0m Total time:       7.43 min
[32m[20221214 14:05:28 @agent_ppo2.py:145][0m 669696 total steps have happened
[32m[20221214 14:05:28 @agent_ppo2.py:121][0m #------------------------ Iteration 327 --------------------------#
[32m[20221214 14:05:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:28 @agent_ppo2.py:185][0m |          -0.0000 |         138.3838 |          18.8290 |
[32m[20221214 14:05:29 @agent_ppo2.py:185][0m |          -0.0012 |         131.2941 |          18.8233 |
[32m[20221214 14:05:29 @agent_ppo2.py:185][0m |           0.0063 |         133.1200 |          18.8116 |
[32m[20221214 14:05:29 @agent_ppo2.py:185][0m |          -0.0025 |         127.6950 |          18.8150 |
[32m[20221214 14:05:29 @agent_ppo2.py:185][0m |          -0.0027 |         127.0582 |          18.8100 |
[32m[20221214 14:05:29 @agent_ppo2.py:185][0m |           0.0048 |         129.8117 |          18.8027 |
[32m[20221214 14:05:29 @agent_ppo2.py:185][0m |          -0.0018 |         125.8766 |          18.8001 |
[32m[20221214 14:05:29 @agent_ppo2.py:185][0m |          -0.0012 |         125.3668 |          18.7945 |
[32m[20221214 14:05:29 @agent_ppo2.py:185][0m |          -0.0036 |         125.0870 |          18.7925 |
[32m[20221214 14:05:29 @agent_ppo2.py:185][0m |          -0.0013 |         124.5460 |          18.7879 |
[32m[20221214 14:05:29 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:05:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 763.58
[32m[20221214 14:05:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 784.50
[32m[20221214 14:05:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.46
[32m[20221214 14:05:29 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 848.46
[32m[20221214 14:05:29 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 848.46
[32m[20221214 14:05:29 @agent_ppo2.py:143][0m Total time:       7.46 min
[32m[20221214 14:05:29 @agent_ppo2.py:145][0m 671744 total steps have happened
[32m[20221214 14:05:29 @agent_ppo2.py:121][0m #------------------------ Iteration 328 --------------------------#
[32m[20221214 14:05:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:30 @agent_ppo2.py:185][0m |          -0.0031 |         146.8057 |          18.7884 |
[32m[20221214 14:05:30 @agent_ppo2.py:185][0m |          -0.0011 |         144.9148 |          18.7685 |
[32m[20221214 14:05:30 @agent_ppo2.py:185][0m |          -0.0019 |         142.3006 |          18.7321 |
[32m[20221214 14:05:30 @agent_ppo2.py:185][0m |          -0.0030 |         141.6657 |          18.7263 |
[32m[20221214 14:05:30 @agent_ppo2.py:185][0m |           0.0078 |         148.6322 |          18.7084 |
[32m[20221214 14:05:30 @agent_ppo2.py:185][0m |          -0.0050 |         140.5494 |          18.6949 |
[32m[20221214 14:05:30 @agent_ppo2.py:185][0m |          -0.0042 |         140.6499 |          18.6835 |
[32m[20221214 14:05:30 @agent_ppo2.py:185][0m |          -0.0046 |         139.9224 |          18.6760 |
[32m[20221214 14:05:30 @agent_ppo2.py:185][0m |          -0.0047 |         139.7836 |          18.6701 |
[32m[20221214 14:05:31 @agent_ppo2.py:185][0m |          -0.0041 |         139.1841 |          18.6587 |
[32m[20221214 14:05:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:05:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.22
[32m[20221214 14:05:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.15
[32m[20221214 14:05:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.63
[32m[20221214 14:05:31 @agent_ppo2.py:143][0m Total time:       7.48 min
[32m[20221214 14:05:31 @agent_ppo2.py:145][0m 673792 total steps have happened
[32m[20221214 14:05:31 @agent_ppo2.py:121][0m #------------------------ Iteration 329 --------------------------#
[32m[20221214 14:05:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:05:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:31 @agent_ppo2.py:185][0m |          -0.0024 |         158.7997 |          18.6169 |
[32m[20221214 14:05:31 @agent_ppo2.py:185][0m |          -0.0045 |         154.2556 |          18.6230 |
[32m[20221214 14:05:31 @agent_ppo2.py:185][0m |          -0.0039 |         152.7443 |          18.6253 |
[32m[20221214 14:05:31 @agent_ppo2.py:185][0m |           0.0007 |         153.3315 |          18.6257 |
[32m[20221214 14:05:31 @agent_ppo2.py:185][0m |          -0.0028 |         150.7180 |          18.6309 |
[32m[20221214 14:05:31 @agent_ppo2.py:185][0m |           0.0011 |         151.5477 |          18.6372 |
[32m[20221214 14:05:32 @agent_ppo2.py:185][0m |          -0.0033 |         149.3609 |          18.6319 |
[32m[20221214 14:05:32 @agent_ppo2.py:185][0m |          -0.0046 |         148.8538 |          18.6389 |
[32m[20221214 14:05:32 @agent_ppo2.py:185][0m |           0.0029 |         150.2637 |          18.6445 |
[32m[20221214 14:05:32 @agent_ppo2.py:185][0m |          -0.0019 |         148.1838 |          18.6478 |
[32m[20221214 14:05:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:05:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.43
[32m[20221214 14:05:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.36
[32m[20221214 14:05:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 860.44
[32m[20221214 14:05:32 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 860.44
[32m[20221214 14:05:32 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 860.44
[32m[20221214 14:05:32 @agent_ppo2.py:143][0m Total time:       7.50 min
[32m[20221214 14:05:32 @agent_ppo2.py:145][0m 675840 total steps have happened
[32m[20221214 14:05:32 @agent_ppo2.py:121][0m #------------------------ Iteration 330 --------------------------#
[32m[20221214 14:05:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:05:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:32 @agent_ppo2.py:185][0m |          -0.0021 |         166.5769 |          18.7133 |
[32m[20221214 14:05:32 @agent_ppo2.py:185][0m |          -0.0054 |         159.7373 |          18.7035 |
[32m[20221214 14:05:32 @agent_ppo2.py:185][0m |          -0.0058 |         158.6094 |          18.6993 |
[32m[20221214 14:05:33 @agent_ppo2.py:185][0m |          -0.0063 |         157.7686 |          18.6837 |
[32m[20221214 14:05:33 @agent_ppo2.py:185][0m |          -0.0031 |         157.7109 |          18.6819 |
[32m[20221214 14:05:33 @agent_ppo2.py:185][0m |          -0.0053 |         157.0153 |          18.6777 |
[32m[20221214 14:05:33 @agent_ppo2.py:185][0m |          -0.0038 |         156.7367 |          18.6740 |
[32m[20221214 14:05:33 @agent_ppo2.py:185][0m |          -0.0049 |         156.5307 |          18.6744 |
[32m[20221214 14:05:33 @agent_ppo2.py:185][0m |          -0.0061 |         156.1263 |          18.6670 |
[32m[20221214 14:05:33 @agent_ppo2.py:185][0m |           0.0075 |         177.8847 |          18.6645 |
[32m[20221214 14:05:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:05:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.87
[32m[20221214 14:05:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.54
[32m[20221214 14:05:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.48
[32m[20221214 14:05:33 @agent_ppo2.py:143][0m Total time:       7.52 min
[32m[20221214 14:05:33 @agent_ppo2.py:145][0m 677888 total steps have happened
[32m[20221214 14:05:33 @agent_ppo2.py:121][0m #------------------------ Iteration 331 --------------------------#
[32m[20221214 14:05:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:34 @agent_ppo2.py:185][0m |           0.0044 |         174.9043 |          18.6244 |
[32m[20221214 14:05:34 @agent_ppo2.py:185][0m |          -0.0024 |         166.1024 |          18.6237 |
[32m[20221214 14:05:34 @agent_ppo2.py:185][0m |          -0.0006 |         162.7298 |          18.6145 |
[32m[20221214 14:05:34 @agent_ppo2.py:185][0m |          -0.0015 |         160.1455 |          18.6142 |
[32m[20221214 14:05:34 @agent_ppo2.py:185][0m |           0.0031 |         161.5911 |          18.6125 |
[32m[20221214 14:05:34 @agent_ppo2.py:185][0m |          -0.0018 |         158.0881 |          18.6082 |
[32m[20221214 14:05:34 @agent_ppo2.py:185][0m |           0.0017 |         158.1912 |          18.6065 |
[32m[20221214 14:05:34 @agent_ppo2.py:185][0m |          -0.0015 |         157.2358 |          18.6021 |
[32m[20221214 14:05:34 @agent_ppo2.py:185][0m |           0.0005 |         156.9173 |          18.6008 |
[32m[20221214 14:05:34 @agent_ppo2.py:185][0m |          -0.0038 |         156.0291 |          18.5984 |
[32m[20221214 14:05:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:05:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.35
[32m[20221214 14:05:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.73
[32m[20221214 14:05:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.07
[32m[20221214 14:05:35 @agent_ppo2.py:143][0m Total time:       7.54 min
[32m[20221214 14:05:35 @agent_ppo2.py:145][0m 679936 total steps have happened
[32m[20221214 14:05:35 @agent_ppo2.py:121][0m #------------------------ Iteration 332 --------------------------#
[32m[20221214 14:05:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:35 @agent_ppo2.py:185][0m |          -0.0003 |         176.5178 |          18.5019 |
[32m[20221214 14:05:35 @agent_ppo2.py:185][0m |          -0.0029 |         170.2095 |          18.5124 |
[32m[20221214 14:05:35 @agent_ppo2.py:185][0m |          -0.0016 |         168.0095 |          18.5266 |
[32m[20221214 14:05:35 @agent_ppo2.py:185][0m |          -0.0014 |         167.2455 |          18.5359 |
[32m[20221214 14:05:35 @agent_ppo2.py:185][0m |           0.0009 |         168.9094 |          18.5436 |
[32m[20221214 14:05:35 @agent_ppo2.py:185][0m |          -0.0026 |         166.2019 |          18.5534 |
[32m[20221214 14:05:35 @agent_ppo2.py:185][0m |           0.0062 |         174.1013 |          18.5540 |
[32m[20221214 14:05:35 @agent_ppo2.py:185][0m |          -0.0010 |         165.7408 |          18.5699 |
[32m[20221214 14:05:36 @agent_ppo2.py:185][0m |          -0.0022 |         165.0032 |          18.5858 |
[32m[20221214 14:05:36 @agent_ppo2.py:185][0m |          -0.0038 |         164.6292 |          18.5979 |
[32m[20221214 14:05:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:05:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.71
[32m[20221214 14:05:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.42
[32m[20221214 14:05:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.32
[32m[20221214 14:05:36 @agent_ppo2.py:143][0m Total time:       7.56 min
[32m[20221214 14:05:36 @agent_ppo2.py:145][0m 681984 total steps have happened
[32m[20221214 14:05:36 @agent_ppo2.py:121][0m #------------------------ Iteration 333 --------------------------#
[32m[20221214 14:05:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:36 @agent_ppo2.py:185][0m |          -0.0006 |         162.7511 |          18.6448 |
[32m[20221214 14:05:36 @agent_ppo2.py:185][0m |          -0.0025 |         146.4579 |          18.6506 |
[32m[20221214 14:05:36 @agent_ppo2.py:185][0m |          -0.0027 |         141.7430 |          18.6531 |
[32m[20221214 14:05:36 @agent_ppo2.py:185][0m |          -0.0031 |         138.1298 |          18.6656 |
[32m[20221214 14:05:36 @agent_ppo2.py:185][0m |          -0.0024 |         136.0817 |          18.6755 |
[32m[20221214 14:05:37 @agent_ppo2.py:185][0m |          -0.0065 |         133.4183 |          18.6756 |
[32m[20221214 14:05:37 @agent_ppo2.py:185][0m |          -0.0030 |         130.7553 |          18.6864 |
[32m[20221214 14:05:37 @agent_ppo2.py:185][0m |          -0.0035 |         129.0558 |          18.6937 |
[32m[20221214 14:05:37 @agent_ppo2.py:185][0m |           0.0002 |         127.8368 |          18.6973 |
[32m[20221214 14:05:37 @agent_ppo2.py:185][0m |          -0.0013 |         126.9803 |          18.6971 |
[32m[20221214 14:05:37 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:05:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 623.74
[32m[20221214 14:05:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.97
[32m[20221214 14:05:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.66
[32m[20221214 14:05:37 @agent_ppo2.py:143][0m Total time:       7.58 min
[32m[20221214 14:05:37 @agent_ppo2.py:145][0m 684032 total steps have happened
[32m[20221214 14:05:37 @agent_ppo2.py:121][0m #------------------------ Iteration 334 --------------------------#
[32m[20221214 14:05:37 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:05:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:38 @agent_ppo2.py:185][0m |           0.0012 |         165.9023 |          18.8069 |
[32m[20221214 14:05:38 @agent_ppo2.py:185][0m |          -0.0030 |         148.0614 |          18.8068 |
[32m[20221214 14:05:38 @agent_ppo2.py:185][0m |          -0.0043 |         142.9485 |          18.8132 |
[32m[20221214 14:05:38 @agent_ppo2.py:185][0m |          -0.0009 |         140.1076 |          18.8237 |
[32m[20221214 14:05:38 @agent_ppo2.py:185][0m |           0.0062 |         141.1270 |          18.8258 |
[32m[20221214 14:05:38 @agent_ppo2.py:185][0m |          -0.0032 |         133.2092 |          18.8320 |
[32m[20221214 14:05:38 @agent_ppo2.py:185][0m |          -0.0007 |         132.3599 |          18.8347 |
[32m[20221214 14:05:38 @agent_ppo2.py:185][0m |           0.0088 |         141.4457 |          18.8424 |
[32m[20221214 14:05:38 @agent_ppo2.py:185][0m |           0.0016 |         130.9143 |          18.8474 |
[32m[20221214 14:05:39 @agent_ppo2.py:185][0m |          -0.0039 |         127.7161 |          18.8540 |
[32m[20221214 14:05:39 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 14:05:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.91
[32m[20221214 14:05:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 619.00
[32m[20221214 14:05:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.92
[32m[20221214 14:05:39 @agent_ppo2.py:143][0m Total time:       7.61 min
[32m[20221214 14:05:39 @agent_ppo2.py:145][0m 686080 total steps have happened
[32m[20221214 14:05:39 @agent_ppo2.py:121][0m #------------------------ Iteration 335 --------------------------#
[32m[20221214 14:05:39 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221214 14:05:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:39 @agent_ppo2.py:185][0m |           0.0092 |         166.3309 |          18.8610 |
[32m[20221214 14:05:39 @agent_ppo2.py:185][0m |          -0.0059 |         149.7841 |          18.8454 |
[32m[20221214 14:05:39 @agent_ppo2.py:185][0m |          -0.0071 |         147.8129 |          18.8309 |
[32m[20221214 14:05:39 @agent_ppo2.py:185][0m |          -0.0035 |         146.5291 |          18.8243 |
[32m[20221214 14:05:39 @agent_ppo2.py:185][0m |          -0.0071 |         145.6990 |          18.8088 |
[32m[20221214 14:05:39 @agent_ppo2.py:185][0m |          -0.0054 |         145.0868 |          18.7996 |
[32m[20221214 14:05:40 @agent_ppo2.py:185][0m |           0.0062 |         163.0560 |          18.7930 |
[32m[20221214 14:05:40 @agent_ppo2.py:185][0m |          -0.0056 |         144.2500 |          18.7669 |
[32m[20221214 14:05:40 @agent_ppo2.py:185][0m |          -0.0083 |         143.9962 |          18.7753 |
[32m[20221214 14:05:40 @agent_ppo2.py:185][0m |          -0.0067 |         143.6368 |          18.7629 |
[32m[20221214 14:05:40 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:05:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.31
[32m[20221214 14:05:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.84
[32m[20221214 14:05:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.47
[32m[20221214 14:05:40 @agent_ppo2.py:143][0m Total time:       7.63 min
[32m[20221214 14:05:40 @agent_ppo2.py:145][0m 688128 total steps have happened
[32m[20221214 14:05:40 @agent_ppo2.py:121][0m #------------------------ Iteration 336 --------------------------#
[32m[20221214 14:05:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:40 @agent_ppo2.py:185][0m |          -0.0015 |         158.2830 |          18.7427 |
[32m[20221214 14:05:40 @agent_ppo2.py:185][0m |          -0.0041 |         152.4237 |          18.7366 |
[32m[20221214 14:05:40 @agent_ppo2.py:185][0m |           0.0086 |         160.7983 |          18.7324 |
[32m[20221214 14:05:41 @agent_ppo2.py:185][0m |          -0.0039 |         149.6920 |          18.7401 |
[32m[20221214 14:05:41 @agent_ppo2.py:185][0m |          -0.0007 |         148.2010 |          18.7409 |
[32m[20221214 14:05:41 @agent_ppo2.py:185][0m |           0.0041 |         153.9424 |          18.7419 |
[32m[20221214 14:05:41 @agent_ppo2.py:185][0m |           0.0038 |         148.9685 |          18.7311 |
[32m[20221214 14:05:41 @agent_ppo2.py:185][0m |          -0.0050 |         146.2134 |          18.7414 |
[32m[20221214 14:05:41 @agent_ppo2.py:185][0m |           0.0109 |         156.0463 |          18.7355 |
[32m[20221214 14:05:41 @agent_ppo2.py:185][0m |          -0.0050 |         145.2816 |          18.7462 |
[32m[20221214 14:05:41 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:05:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.36
[32m[20221214 14:05:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.22
[32m[20221214 14:05:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.35
[32m[20221214 14:05:41 @agent_ppo2.py:143][0m Total time:       7.65 min
[32m[20221214 14:05:41 @agent_ppo2.py:145][0m 690176 total steps have happened
[32m[20221214 14:05:41 @agent_ppo2.py:121][0m #------------------------ Iteration 337 --------------------------#
[32m[20221214 14:05:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:42 @agent_ppo2.py:185][0m |          -0.0013 |         169.0066 |          18.6593 |
[32m[20221214 14:05:42 @agent_ppo2.py:185][0m |          -0.0033 |         156.9530 |          18.6531 |
[32m[20221214 14:05:42 @agent_ppo2.py:185][0m |          -0.0069 |         151.5204 |          18.6479 |
[32m[20221214 14:05:42 @agent_ppo2.py:185][0m |          -0.0022 |         147.4344 |          18.6380 |
[32m[20221214 14:05:42 @agent_ppo2.py:185][0m |          -0.0033 |         145.4191 |          18.6324 |
[32m[20221214 14:05:42 @agent_ppo2.py:185][0m |           0.0046 |         145.3556 |          18.6254 |
[32m[20221214 14:05:42 @agent_ppo2.py:185][0m |          -0.0065 |         141.7327 |          18.6198 |
[32m[20221214 14:05:42 @agent_ppo2.py:185][0m |          -0.0046 |         140.2377 |          18.6104 |
[32m[20221214 14:05:42 @agent_ppo2.py:185][0m |          -0.0033 |         138.7887 |          18.6108 |
[32m[20221214 14:05:42 @agent_ppo2.py:185][0m |          -0.0047 |         138.2620 |          18.6065 |
[32m[20221214 14:05:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:05:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 654.69
[32m[20221214 14:05:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 753.79
[32m[20221214 14:05:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 801.15
[32m[20221214 14:05:43 @agent_ppo2.py:143][0m Total time:       7.67 min
[32m[20221214 14:05:43 @agent_ppo2.py:145][0m 692224 total steps have happened
[32m[20221214 14:05:43 @agent_ppo2.py:121][0m #------------------------ Iteration 338 --------------------------#
[32m[20221214 14:05:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:43 @agent_ppo2.py:185][0m |           0.0010 |         165.2874 |          18.6476 |
[32m[20221214 14:05:43 @agent_ppo2.py:185][0m |           0.0019 |         153.0374 |          18.6336 |
[32m[20221214 14:05:43 @agent_ppo2.py:185][0m |          -0.0025 |         147.4048 |          18.6258 |
[32m[20221214 14:05:43 @agent_ppo2.py:185][0m |          -0.0037 |         144.6160 |          18.6274 |
[32m[20221214 14:05:43 @agent_ppo2.py:185][0m |          -0.0031 |         143.0545 |          18.6176 |
[32m[20221214 14:05:43 @agent_ppo2.py:185][0m |          -0.0005 |         142.1221 |          18.6106 |
[32m[20221214 14:05:43 @agent_ppo2.py:185][0m |           0.0009 |         142.3756 |          18.6007 |
[32m[20221214 14:05:43 @agent_ppo2.py:185][0m |          -0.0003 |         140.1978 |          18.5768 |
[32m[20221214 14:05:44 @agent_ppo2.py:185][0m |          -0.0061 |         140.0439 |          18.5773 |
[32m[20221214 14:05:44 @agent_ppo2.py:185][0m |          -0.0033 |         139.1946 |          18.5673 |
[32m[20221214 14:05:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:05:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 682.88
[32m[20221214 14:05:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.13
[32m[20221214 14:05:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.34
[32m[20221214 14:05:44 @agent_ppo2.py:143][0m Total time:       7.70 min
[32m[20221214 14:05:44 @agent_ppo2.py:145][0m 694272 total steps have happened
[32m[20221214 14:05:44 @agent_ppo2.py:121][0m #------------------------ Iteration 339 --------------------------#
[32m[20221214 14:05:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:44 @agent_ppo2.py:185][0m |          -0.0013 |         131.9430 |          18.5874 |
[32m[20221214 14:05:44 @agent_ppo2.py:185][0m |          -0.0015 |         120.7900 |          18.5956 |
[32m[20221214 14:05:44 @agent_ppo2.py:185][0m |          -0.0038 |         115.9423 |          18.5985 |
[32m[20221214 14:05:44 @agent_ppo2.py:185][0m |           0.0111 |         124.6715 |          18.6022 |
[32m[20221214 14:05:44 @agent_ppo2.py:185][0m |           0.0001 |         111.5474 |          18.5860 |
[32m[20221214 14:05:45 @agent_ppo2.py:185][0m |           0.0074 |         116.7367 |          18.5990 |
[32m[20221214 14:05:45 @agent_ppo2.py:185][0m |          -0.0026 |         107.3811 |          18.6163 |
[32m[20221214 14:05:45 @agent_ppo2.py:185][0m |           0.0009 |         106.1580 |          18.6255 |
[32m[20221214 14:05:45 @agent_ppo2.py:185][0m |          -0.0032 |         105.4171 |          18.6233 |
[32m[20221214 14:05:45 @agent_ppo2.py:185][0m |          -0.0032 |         104.5267 |          18.6344 |
[32m[20221214 14:05:45 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:05:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 614.80
[32m[20221214 14:05:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 740.21
[32m[20221214 14:05:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 786.12
[32m[20221214 14:05:45 @agent_ppo2.py:143][0m Total time:       7.72 min
[32m[20221214 14:05:45 @agent_ppo2.py:145][0m 696320 total steps have happened
[32m[20221214 14:05:45 @agent_ppo2.py:121][0m #------------------------ Iteration 340 --------------------------#
[32m[20221214 14:05:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:05:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:45 @agent_ppo2.py:185][0m |          -0.0042 |         139.6314 |          18.6487 |
[32m[20221214 14:05:46 @agent_ppo2.py:185][0m |          -0.0012 |         129.1594 |          18.6621 |
[32m[20221214 14:05:46 @agent_ppo2.py:185][0m |           0.0011 |         128.3000 |          18.6710 |
[32m[20221214 14:05:46 @agent_ppo2.py:185][0m |           0.0130 |         136.0156 |          18.6807 |
[32m[20221214 14:05:46 @agent_ppo2.py:185][0m |          -0.0018 |         121.5353 |          18.6998 |
[32m[20221214 14:05:46 @agent_ppo2.py:185][0m |          -0.0037 |         120.3067 |          18.7051 |
[32m[20221214 14:05:46 @agent_ppo2.py:185][0m |          -0.0063 |         119.1026 |          18.7269 |
[32m[20221214 14:05:46 @agent_ppo2.py:185][0m |          -0.0037 |         118.2536 |          18.7408 |
[32m[20221214 14:05:46 @agent_ppo2.py:185][0m |           0.0037 |         120.8193 |          18.7538 |
[32m[20221214 14:05:46 @agent_ppo2.py:185][0m |          -0.0047 |         117.3301 |          18.7664 |
[32m[20221214 14:05:46 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:05:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 573.89
[32m[20221214 14:05:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 743.59
[32m[20221214 14:05:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 755.87
[32m[20221214 14:05:46 @agent_ppo2.py:143][0m Total time:       7.74 min
[32m[20221214 14:05:46 @agent_ppo2.py:145][0m 698368 total steps have happened
[32m[20221214 14:05:46 @agent_ppo2.py:121][0m #------------------------ Iteration 341 --------------------------#
[32m[20221214 14:05:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:05:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:47 @agent_ppo2.py:185][0m |          -0.0031 |         141.1993 |          18.6981 |
[32m[20221214 14:05:47 @agent_ppo2.py:185][0m |          -0.0008 |         126.2867 |          18.6724 |
[32m[20221214 14:05:47 @agent_ppo2.py:185][0m |          -0.0010 |         117.7107 |          18.6695 |
[32m[20221214 14:05:47 @agent_ppo2.py:185][0m |          -0.0025 |         111.6309 |          18.6476 |
[32m[20221214 14:05:47 @agent_ppo2.py:185][0m |          -0.0018 |         108.9115 |          18.6497 |
[32m[20221214 14:05:47 @agent_ppo2.py:185][0m |           0.0003 |         107.6049 |          18.6368 |
[32m[20221214 14:05:47 @agent_ppo2.py:185][0m |          -0.0050 |         105.1161 |          18.6343 |
[32m[20221214 14:05:47 @agent_ppo2.py:185][0m |           0.0048 |         105.5159 |          18.6289 |
[32m[20221214 14:05:48 @agent_ppo2.py:185][0m |          -0.0017 |         102.2397 |          18.6098 |
[32m[20221214 14:05:48 @agent_ppo2.py:185][0m |          -0.0013 |         100.6644 |          18.6084 |
[32m[20221214 14:05:48 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:05:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 590.45
[32m[20221214 14:05:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.19
[32m[20221214 14:05:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.57
[32m[20221214 14:05:48 @agent_ppo2.py:143][0m Total time:       7.76 min
[32m[20221214 14:05:48 @agent_ppo2.py:145][0m 700416 total steps have happened
[32m[20221214 14:05:48 @agent_ppo2.py:121][0m #------------------------ Iteration 342 --------------------------#
[32m[20221214 14:05:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:05:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:48 @agent_ppo2.py:185][0m |           0.0006 |         159.3079 |          18.6349 |
[32m[20221214 14:05:48 @agent_ppo2.py:185][0m |          -0.0029 |         149.3468 |          18.6516 |
[32m[20221214 14:05:48 @agent_ppo2.py:185][0m |          -0.0032 |         144.9816 |          18.6519 |
[32m[20221214 14:05:48 @agent_ppo2.py:185][0m |          -0.0045 |         142.9579 |          18.6476 |
[32m[20221214 14:05:48 @agent_ppo2.py:185][0m |          -0.0047 |         141.3112 |          18.6505 |
[32m[20221214 14:05:49 @agent_ppo2.py:185][0m |          -0.0054 |         140.1043 |          18.6557 |
[32m[20221214 14:05:49 @agent_ppo2.py:185][0m |           0.0051 |         142.9886 |          18.6601 |
[32m[20221214 14:05:49 @agent_ppo2.py:185][0m |          -0.0025 |         138.5925 |          18.6559 |
[32m[20221214 14:05:49 @agent_ppo2.py:185][0m |          -0.0055 |         137.7005 |          18.6572 |
[32m[20221214 14:05:49 @agent_ppo2.py:185][0m |           0.0005 |         137.5026 |          18.6566 |
[32m[20221214 14:05:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:05:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 722.99
[32m[20221214 14:05:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 765.57
[32m[20221214 14:05:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.97
[32m[20221214 14:05:49 @agent_ppo2.py:143][0m Total time:       7.78 min
[32m[20221214 14:05:49 @agent_ppo2.py:145][0m 702464 total steps have happened
[32m[20221214 14:05:49 @agent_ppo2.py:121][0m #------------------------ Iteration 343 --------------------------#
[32m[20221214 14:05:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:49 @agent_ppo2.py:185][0m |          -0.0027 |         150.6428 |          18.7403 |
[32m[20221214 14:05:50 @agent_ppo2.py:185][0m |           0.0005 |         147.2968 |          18.7406 |
[32m[20221214 14:05:50 @agent_ppo2.py:185][0m |           0.0030 |         149.8118 |          18.7431 |
[32m[20221214 14:05:50 @agent_ppo2.py:185][0m |          -0.0021 |         144.4516 |          18.7435 |
[32m[20221214 14:05:50 @agent_ppo2.py:185][0m |          -0.0025 |         143.6746 |          18.7392 |
[32m[20221214 14:05:50 @agent_ppo2.py:185][0m |          -0.0059 |         142.2049 |          18.7343 |
[32m[20221214 14:05:50 @agent_ppo2.py:185][0m |          -0.0024 |         141.8270 |          18.7308 |
[32m[20221214 14:05:50 @agent_ppo2.py:185][0m |           0.0182 |         161.0118 |          18.7235 |
[32m[20221214 14:05:50 @agent_ppo2.py:185][0m |          -0.0043 |         141.5931 |          18.7305 |
[32m[20221214 14:05:50 @agent_ppo2.py:185][0m |          -0.0029 |         140.6132 |          18.7331 |
[32m[20221214 14:05:50 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:05:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 704.66
[32m[20221214 14:05:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.55
[32m[20221214 14:05:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.07
[32m[20221214 14:05:51 @agent_ppo2.py:143][0m Total time:       7.81 min
[32m[20221214 14:05:51 @agent_ppo2.py:145][0m 704512 total steps have happened
[32m[20221214 14:05:51 @agent_ppo2.py:121][0m #------------------------ Iteration 344 --------------------------#
[32m[20221214 14:05:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:05:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:51 @agent_ppo2.py:185][0m |          -0.0019 |         163.1700 |          18.6741 |
[32m[20221214 14:05:51 @agent_ppo2.py:185][0m |           0.0054 |         163.4417 |          18.6781 |
[32m[20221214 14:05:51 @agent_ppo2.py:185][0m |           0.0000 |         154.6330 |          18.6873 |
[32m[20221214 14:05:51 @agent_ppo2.py:185][0m |          -0.0034 |         151.4880 |          18.6900 |
[32m[20221214 14:05:51 @agent_ppo2.py:185][0m |          -0.0030 |         149.2780 |          18.6915 |
[32m[20221214 14:05:51 @agent_ppo2.py:185][0m |          -0.0034 |         147.7626 |          18.6897 |
[32m[20221214 14:05:51 @agent_ppo2.py:185][0m |           0.0060 |         156.0650 |          18.6914 |
[32m[20221214 14:05:52 @agent_ppo2.py:185][0m |          -0.0024 |         145.8749 |          18.6983 |
[32m[20221214 14:05:52 @agent_ppo2.py:185][0m |           0.0036 |         150.3278 |          18.6998 |
[32m[20221214 14:05:52 @agent_ppo2.py:185][0m |           0.0008 |         144.5568 |          18.6997 |
[32m[20221214 14:05:52 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:05:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 744.77
[32m[20221214 14:05:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 781.51
[32m[20221214 14:05:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.38
[32m[20221214 14:05:52 @agent_ppo2.py:143][0m Total time:       7.83 min
[32m[20221214 14:05:52 @agent_ppo2.py:145][0m 706560 total steps have happened
[32m[20221214 14:05:52 @agent_ppo2.py:121][0m #------------------------ Iteration 345 --------------------------#
[32m[20221214 14:05:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:05:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:52 @agent_ppo2.py:185][0m |          -0.0035 |         139.0487 |          18.7860 |
[32m[20221214 14:05:52 @agent_ppo2.py:185][0m |           0.0013 |         121.9921 |          18.7689 |
[32m[20221214 14:05:52 @agent_ppo2.py:185][0m |          -0.0011 |         113.2802 |          18.7561 |
[32m[20221214 14:05:53 @agent_ppo2.py:185][0m |          -0.0028 |         109.4039 |          18.7411 |
[32m[20221214 14:05:53 @agent_ppo2.py:185][0m |          -0.0017 |         107.1941 |          18.7308 |
[32m[20221214 14:05:53 @agent_ppo2.py:185][0m |          -0.0028 |         104.0344 |          18.7230 |
[32m[20221214 14:05:53 @agent_ppo2.py:185][0m |           0.0054 |         103.0553 |          18.7171 |
[32m[20221214 14:05:53 @agent_ppo2.py:185][0m |          -0.0026 |         100.4929 |          18.7074 |
[32m[20221214 14:05:53 @agent_ppo2.py:185][0m |          -0.0051 |          99.5848 |          18.6996 |
[32m[20221214 14:05:53 @agent_ppo2.py:185][0m |           0.0001 |          98.4666 |          18.6889 |
[32m[20221214 14:05:53 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:05:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 596.66
[32m[20221214 14:05:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.18
[32m[20221214 14:05:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.14
[32m[20221214 14:05:53 @agent_ppo2.py:143][0m Total time:       7.85 min
[32m[20221214 14:05:53 @agent_ppo2.py:145][0m 708608 total steps have happened
[32m[20221214 14:05:53 @agent_ppo2.py:121][0m #------------------------ Iteration 346 --------------------------#
[32m[20221214 14:05:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:05:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:54 @agent_ppo2.py:185][0m |          -0.0007 |         155.1857 |          18.5742 |
[32m[20221214 14:05:54 @agent_ppo2.py:185][0m |          -0.0040 |         142.3155 |          18.5743 |
[32m[20221214 14:05:54 @agent_ppo2.py:185][0m |          -0.0032 |         137.7831 |          18.5610 |
[32m[20221214 14:05:54 @agent_ppo2.py:185][0m |           0.0014 |         136.5708 |          18.5491 |
[32m[20221214 14:05:54 @agent_ppo2.py:185][0m |          -0.0043 |         131.6235 |          18.5434 |
[32m[20221214 14:05:54 @agent_ppo2.py:185][0m |          -0.0049 |         129.7610 |          18.5334 |
[32m[20221214 14:05:54 @agent_ppo2.py:185][0m |           0.0052 |         133.7187 |          18.5248 |
[32m[20221214 14:05:54 @agent_ppo2.py:185][0m |          -0.0011 |         126.3901 |          18.5255 |
[32m[20221214 14:05:54 @agent_ppo2.py:185][0m |          -0.0065 |         125.3030 |          18.5256 |
[32m[20221214 14:05:54 @agent_ppo2.py:185][0m |          -0.0044 |         124.2937 |          18.5076 |
[32m[20221214 14:05:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:05:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 693.75
[32m[20221214 14:05:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.91
[32m[20221214 14:05:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.84
[32m[20221214 14:05:55 @agent_ppo2.py:143][0m Total time:       7.88 min
[32m[20221214 14:05:55 @agent_ppo2.py:145][0m 710656 total steps have happened
[32m[20221214 14:05:55 @agent_ppo2.py:121][0m #------------------------ Iteration 347 --------------------------#
[32m[20221214 14:05:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:05:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:55 @agent_ppo2.py:185][0m |           0.0018 |         138.6368 |          18.5753 |
[32m[20221214 14:05:55 @agent_ppo2.py:185][0m |          -0.0001 |         123.4185 |          18.5697 |
[32m[20221214 14:05:55 @agent_ppo2.py:185][0m |          -0.0018 |         117.6127 |          18.5657 |
[32m[20221214 14:05:55 @agent_ppo2.py:185][0m |          -0.0042 |         113.9728 |          18.5646 |
[32m[20221214 14:05:55 @agent_ppo2.py:185][0m |           0.0052 |         116.8238 |          18.5702 |
[32m[20221214 14:05:55 @agent_ppo2.py:185][0m |          -0.0041 |         110.5256 |          18.5637 |
[32m[20221214 14:05:55 @agent_ppo2.py:185][0m |          -0.0034 |         108.9596 |          18.5592 |
[32m[20221214 14:05:56 @agent_ppo2.py:185][0m |          -0.0041 |         107.8904 |          18.5580 |
[32m[20221214 14:05:56 @agent_ppo2.py:185][0m |          -0.0036 |         106.8717 |          18.5590 |
[32m[20221214 14:05:56 @agent_ppo2.py:185][0m |          -0.0042 |         105.9026 |          18.5555 |
[32m[20221214 14:05:56 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:05:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 671.05
[32m[20221214 14:05:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.58
[32m[20221214 14:05:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.13
[32m[20221214 14:05:56 @agent_ppo2.py:143][0m Total time:       7.90 min
[32m[20221214 14:05:56 @agent_ppo2.py:145][0m 712704 total steps have happened
[32m[20221214 14:05:56 @agent_ppo2.py:121][0m #------------------------ Iteration 348 --------------------------#
[32m[20221214 14:05:56 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:05:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:56 @agent_ppo2.py:185][0m |          -0.0001 |         174.3402 |          18.5563 |
[32m[20221214 14:05:56 @agent_ppo2.py:185][0m |          -0.0046 |         164.5595 |          18.5419 |
[32m[20221214 14:05:57 @agent_ppo2.py:185][0m |           0.0025 |         160.9423 |          18.5321 |
[32m[20221214 14:05:57 @agent_ppo2.py:185][0m |          -0.0035 |         158.2117 |          18.5252 |
[32m[20221214 14:05:57 @agent_ppo2.py:185][0m |          -0.0015 |         156.6940 |          18.5146 |
[32m[20221214 14:05:57 @agent_ppo2.py:185][0m |          -0.0046 |         155.9166 |          18.5125 |
[32m[20221214 14:05:57 @agent_ppo2.py:185][0m |          -0.0033 |         155.1868 |          18.5098 |
[32m[20221214 14:05:57 @agent_ppo2.py:185][0m |          -0.0036 |         154.6574 |          18.5073 |
[32m[20221214 14:05:57 @agent_ppo2.py:185][0m |          -0.0028 |         153.9235 |          18.5004 |
[32m[20221214 14:05:57 @agent_ppo2.py:185][0m |          -0.0013 |         153.4625 |          18.4945 |
[32m[20221214 14:05:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:05:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 716.35
[32m[20221214 14:05:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.83
[32m[20221214 14:05:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.15
[32m[20221214 14:05:57 @agent_ppo2.py:143][0m Total time:       7.92 min
[32m[20221214 14:05:57 @agent_ppo2.py:145][0m 714752 total steps have happened
[32m[20221214 14:05:57 @agent_ppo2.py:121][0m #------------------------ Iteration 349 --------------------------#
[32m[20221214 14:05:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:05:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:58 @agent_ppo2.py:185][0m |           0.0131 |         191.6332 |          18.4783 |
[32m[20221214 14:05:58 @agent_ppo2.py:185][0m |          -0.0011 |         162.4529 |          18.4935 |
[32m[20221214 14:05:58 @agent_ppo2.py:185][0m |          -0.0016 |         160.4984 |          18.4910 |
[32m[20221214 14:05:58 @agent_ppo2.py:185][0m |          -0.0006 |         158.7426 |          18.4875 |
[32m[20221214 14:05:58 @agent_ppo2.py:185][0m |           0.0012 |         158.6994 |          18.4870 |
[32m[20221214 14:05:58 @agent_ppo2.py:185][0m |          -0.0015 |         157.2924 |          18.4786 |
[32m[20221214 14:05:58 @agent_ppo2.py:185][0m |          -0.0021 |         156.6701 |          18.4829 |
[32m[20221214 14:05:58 @agent_ppo2.py:185][0m |          -0.0014 |         156.0190 |          18.4758 |
[32m[20221214 14:05:58 @agent_ppo2.py:185][0m |          -0.0024 |         155.3528 |          18.4769 |
[32m[20221214 14:05:59 @agent_ppo2.py:185][0m |           0.0050 |         162.0830 |          18.4824 |
[32m[20221214 14:05:59 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:05:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.57
[32m[20221214 14:05:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.46
[32m[20221214 14:05:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.47
[32m[20221214 14:05:59 @agent_ppo2.py:143][0m Total time:       7.94 min
[32m[20221214 14:05:59 @agent_ppo2.py:145][0m 716800 total steps have happened
[32m[20221214 14:05:59 @agent_ppo2.py:121][0m #------------------------ Iteration 350 --------------------------#
[32m[20221214 14:05:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:05:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:05:59 @agent_ppo2.py:185][0m |          -0.0015 |         161.7510 |          18.5866 |
[32m[20221214 14:05:59 @agent_ppo2.py:185][0m |          -0.0008 |         146.4534 |          18.5682 |
[32m[20221214 14:05:59 @agent_ppo2.py:185][0m |          -0.0009 |         139.2118 |          18.5570 |
[32m[20221214 14:05:59 @agent_ppo2.py:185][0m |          -0.0019 |         134.0794 |          18.5509 |
[32m[20221214 14:05:59 @agent_ppo2.py:185][0m |          -0.0065 |         130.3495 |          18.5458 |
[32m[20221214 14:06:00 @agent_ppo2.py:185][0m |          -0.0035 |         127.7690 |          18.5377 |
[32m[20221214 14:06:00 @agent_ppo2.py:185][0m |          -0.0056 |         125.9688 |          18.5267 |
[32m[20221214 14:06:00 @agent_ppo2.py:185][0m |          -0.0057 |         124.7546 |          18.5306 |
[32m[20221214 14:06:00 @agent_ppo2.py:185][0m |           0.0045 |         130.9288 |          18.5257 |
[32m[20221214 14:06:00 @agent_ppo2.py:185][0m |          -0.0015 |         123.3828 |          18.5223 |
[32m[20221214 14:06:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:06:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 683.18
[32m[20221214 14:06:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.87
[32m[20221214 14:06:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.63
[32m[20221214 14:06:00 @agent_ppo2.py:143][0m Total time:       7.97 min
[32m[20221214 14:06:00 @agent_ppo2.py:145][0m 718848 total steps have happened
[32m[20221214 14:06:00 @agent_ppo2.py:121][0m #------------------------ Iteration 351 --------------------------#
[32m[20221214 14:06:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:06:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:00 @agent_ppo2.py:185][0m |          -0.0035 |         176.5009 |          18.4614 |
[32m[20221214 14:06:00 @agent_ppo2.py:185][0m |           0.0034 |         176.6418 |          18.4417 |
[32m[20221214 14:06:01 @agent_ppo2.py:185][0m |          -0.0047 |         172.7313 |          18.4230 |
[32m[20221214 14:06:01 @agent_ppo2.py:185][0m |          -0.0039 |         170.9530 |          18.3956 |
[32m[20221214 14:06:01 @agent_ppo2.py:185][0m |          -0.0039 |         170.0897 |          18.3801 |
[32m[20221214 14:06:01 @agent_ppo2.py:185][0m |          -0.0044 |         169.2263 |          18.3628 |
[32m[20221214 14:06:01 @agent_ppo2.py:185][0m |          -0.0041 |         168.2014 |          18.3456 |
[32m[20221214 14:06:01 @agent_ppo2.py:185][0m |          -0.0044 |         167.7966 |          18.3375 |
[32m[20221214 14:06:01 @agent_ppo2.py:185][0m |          -0.0005 |         168.3619 |          18.3181 |
[32m[20221214 14:06:01 @agent_ppo2.py:185][0m |          -0.0037 |         166.5828 |          18.3074 |
[32m[20221214 14:06:01 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:06:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.31
[32m[20221214 14:06:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.79
[32m[20221214 14:06:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.73
[32m[20221214 14:06:01 @agent_ppo2.py:143][0m Total time:       7.99 min
[32m[20221214 14:06:01 @agent_ppo2.py:145][0m 720896 total steps have happened
[32m[20221214 14:06:01 @agent_ppo2.py:121][0m #------------------------ Iteration 352 --------------------------#
[32m[20221214 14:06:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:02 @agent_ppo2.py:185][0m |          -0.0015 |         154.1695 |          18.2663 |
[32m[20221214 14:06:02 @agent_ppo2.py:185][0m |          -0.0036 |         141.3756 |          18.2760 |
[32m[20221214 14:06:02 @agent_ppo2.py:185][0m |          -0.0028 |         136.8291 |          18.2775 |
[32m[20221214 14:06:02 @agent_ppo2.py:185][0m |          -0.0005 |         134.1213 |          18.2789 |
[32m[20221214 14:06:02 @agent_ppo2.py:185][0m |          -0.0019 |         131.3791 |          18.2830 |
[32m[20221214 14:06:02 @agent_ppo2.py:185][0m |          -0.0035 |         129.7551 |          18.2872 |
[32m[20221214 14:06:02 @agent_ppo2.py:185][0m |          -0.0029 |         128.0787 |          18.3101 |
[32m[20221214 14:06:02 @agent_ppo2.py:185][0m |          -0.0009 |         127.0993 |          18.2982 |
[32m[20221214 14:06:02 @agent_ppo2.py:185][0m |           0.0079 |         134.2659 |          18.3078 |
[32m[20221214 14:06:02 @agent_ppo2.py:185][0m |          -0.0006 |         125.1990 |          18.3181 |
[32m[20221214 14:06:02 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:06:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.02
[32m[20221214 14:06:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.90
[32m[20221214 14:06:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.98
[32m[20221214 14:06:03 @agent_ppo2.py:143][0m Total time:       8.01 min
[32m[20221214 14:06:03 @agent_ppo2.py:145][0m 722944 total steps have happened
[32m[20221214 14:06:03 @agent_ppo2.py:121][0m #------------------------ Iteration 353 --------------------------#
[32m[20221214 14:06:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:03 @agent_ppo2.py:185][0m |          -0.0042 |         176.8159 |          18.3775 |
[32m[20221214 14:06:03 @agent_ppo2.py:185][0m |          -0.0037 |         170.0681 |          18.3770 |
[32m[20221214 14:06:03 @agent_ppo2.py:185][0m |          -0.0032 |         167.0686 |          18.3632 |
[32m[20221214 14:06:03 @agent_ppo2.py:185][0m |          -0.0035 |         165.3972 |          18.3531 |
[32m[20221214 14:06:03 @agent_ppo2.py:185][0m |          -0.0048 |         164.1134 |          18.3554 |
[32m[20221214 14:06:03 @agent_ppo2.py:185][0m |          -0.0041 |         163.5813 |          18.3408 |
[32m[20221214 14:06:03 @agent_ppo2.py:185][0m |          -0.0035 |         162.9403 |          18.3374 |
[32m[20221214 14:06:03 @agent_ppo2.py:185][0m |          -0.0023 |         162.3548 |          18.3247 |
[32m[20221214 14:06:04 @agent_ppo2.py:185][0m |          -0.0043 |         162.0581 |          18.3151 |
[32m[20221214 14:06:04 @agent_ppo2.py:185][0m |          -0.0034 |         162.0170 |          18.3051 |
[32m[20221214 14:06:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:06:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.74
[32m[20221214 14:06:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.48
[32m[20221214 14:06:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.26
[32m[20221214 14:06:04 @agent_ppo2.py:143][0m Total time:       8.03 min
[32m[20221214 14:06:04 @agent_ppo2.py:145][0m 724992 total steps have happened
[32m[20221214 14:06:04 @agent_ppo2.py:121][0m #------------------------ Iteration 354 --------------------------#
[32m[20221214 14:06:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:06:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:04 @agent_ppo2.py:185][0m |          -0.0016 |         181.9115 |          18.3247 |
[32m[20221214 14:06:04 @agent_ppo2.py:185][0m |           0.0039 |         178.8925 |          18.3041 |
[32m[20221214 14:06:04 @agent_ppo2.py:185][0m |          -0.0020 |         171.8258 |          18.2827 |
[32m[20221214 14:06:04 @agent_ppo2.py:185][0m |          -0.0037 |         170.3114 |          18.2646 |
[32m[20221214 14:06:05 @agent_ppo2.py:185][0m |           0.0011 |         169.0005 |          18.2566 |
[32m[20221214 14:06:05 @agent_ppo2.py:185][0m |           0.0093 |         190.9047 |          18.2473 |
[32m[20221214 14:06:05 @agent_ppo2.py:185][0m |          -0.0020 |         167.7085 |          18.2345 |
[32m[20221214 14:06:05 @agent_ppo2.py:185][0m |          -0.0030 |         166.2896 |          18.2223 |
[32m[20221214 14:06:05 @agent_ppo2.py:185][0m |          -0.0017 |         165.7695 |          18.2095 |
[32m[20221214 14:06:05 @agent_ppo2.py:185][0m |          -0.0040 |         165.1483 |          18.2022 |
[32m[20221214 14:06:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:06:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 716.42
[32m[20221214 14:06:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.41
[32m[20221214 14:06:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.72
[32m[20221214 14:06:05 @agent_ppo2.py:143][0m Total time:       8.05 min
[32m[20221214 14:06:05 @agent_ppo2.py:145][0m 727040 total steps have happened
[32m[20221214 14:06:05 @agent_ppo2.py:121][0m #------------------------ Iteration 355 --------------------------#
[32m[20221214 14:06:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:06:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:05 @agent_ppo2.py:185][0m |          -0.0024 |         177.4509 |          18.1629 |
[32m[20221214 14:06:06 @agent_ppo2.py:185][0m |          -0.0020 |         170.4778 |          18.1579 |
[32m[20221214 14:06:06 @agent_ppo2.py:185][0m |          -0.0006 |         167.3836 |          18.1471 |
[32m[20221214 14:06:06 @agent_ppo2.py:185][0m |          -0.0011 |         165.4795 |          18.1412 |
[32m[20221214 14:06:06 @agent_ppo2.py:185][0m |          -0.0029 |         163.4991 |          18.1261 |
[32m[20221214 14:06:06 @agent_ppo2.py:185][0m |          -0.0004 |         162.1982 |          18.1203 |
[32m[20221214 14:06:06 @agent_ppo2.py:185][0m |          -0.0018 |         160.9975 |          18.1149 |
[32m[20221214 14:06:06 @agent_ppo2.py:185][0m |          -0.0003 |         160.2594 |          18.1082 |
[32m[20221214 14:06:06 @agent_ppo2.py:185][0m |          -0.0023 |         160.2631 |          18.0964 |
[32m[20221214 14:06:06 @agent_ppo2.py:185][0m |          -0.0012 |         159.6136 |          18.0910 |
[32m[20221214 14:06:06 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:06:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.49
[32m[20221214 14:06:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.80
[32m[20221214 14:06:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.28
[32m[20221214 14:06:07 @agent_ppo2.py:143][0m Total time:       8.07 min
[32m[20221214 14:06:07 @agent_ppo2.py:145][0m 729088 total steps have happened
[32m[20221214 14:06:07 @agent_ppo2.py:121][0m #------------------------ Iteration 356 --------------------------#
[32m[20221214 14:06:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:06:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:07 @agent_ppo2.py:185][0m |          -0.0003 |         183.8012 |          18.1040 |
[32m[20221214 14:06:07 @agent_ppo2.py:185][0m |          -0.0027 |         176.6679 |          18.1130 |
[32m[20221214 14:06:07 @agent_ppo2.py:185][0m |          -0.0027 |         171.9504 |          18.1176 |
[32m[20221214 14:06:07 @agent_ppo2.py:185][0m |          -0.0012 |         168.5758 |          18.1156 |
[32m[20221214 14:06:07 @agent_ppo2.py:185][0m |          -0.0038 |         166.0495 |          18.1116 |
[32m[20221214 14:06:07 @agent_ppo2.py:185][0m |          -0.0019 |         162.9915 |          18.1034 |
[32m[20221214 14:06:07 @agent_ppo2.py:185][0m |          -0.0031 |         161.1242 |          18.1020 |
[32m[20221214 14:06:08 @agent_ppo2.py:185][0m |          -0.0022 |         160.2459 |          18.0892 |
[32m[20221214 14:06:08 @agent_ppo2.py:185][0m |          -0.0043 |         159.7103 |          18.0863 |
[32m[20221214 14:06:08 @agent_ppo2.py:185][0m |          -0.0022 |         158.7498 |          18.0865 |
[32m[20221214 14:06:08 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:06:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.25
[32m[20221214 14:06:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.20
[32m[20221214 14:06:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.14
[32m[20221214 14:06:08 @agent_ppo2.py:143][0m Total time:       8.10 min
[32m[20221214 14:06:08 @agent_ppo2.py:145][0m 731136 total steps have happened
[32m[20221214 14:06:08 @agent_ppo2.py:121][0m #------------------------ Iteration 357 --------------------------#
[32m[20221214 14:06:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:06:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:08 @agent_ppo2.py:185][0m |          -0.0028 |         175.6286 |          17.9911 |
[32m[20221214 14:06:08 @agent_ppo2.py:185][0m |          -0.0025 |         168.9413 |          17.9974 |
[32m[20221214 14:06:08 @agent_ppo2.py:185][0m |          -0.0008 |         165.5254 |          17.9964 |
[32m[20221214 14:06:08 @agent_ppo2.py:185][0m |          -0.0036 |         161.3580 |          17.9959 |
[32m[20221214 14:06:09 @agent_ppo2.py:185][0m |          -0.0028 |         158.3322 |          17.9933 |
[32m[20221214 14:06:09 @agent_ppo2.py:185][0m |          -0.0023 |         156.1886 |          18.0024 |
[32m[20221214 14:06:09 @agent_ppo2.py:185][0m |          -0.0015 |         154.4784 |          18.0013 |
[32m[20221214 14:06:09 @agent_ppo2.py:185][0m |           0.0018 |         157.6258 |          18.0068 |
[32m[20221214 14:06:09 @agent_ppo2.py:185][0m |          -0.0046 |         152.5377 |          17.9977 |
[32m[20221214 14:06:09 @agent_ppo2.py:185][0m |           0.0029 |         159.4700 |          17.9946 |
[32m[20221214 14:06:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:06:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.83
[32m[20221214 14:06:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.23
[32m[20221214 14:06:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.71
[32m[20221214 14:06:09 @agent_ppo2.py:143][0m Total time:       8.12 min
[32m[20221214 14:06:09 @agent_ppo2.py:145][0m 733184 total steps have happened
[32m[20221214 14:06:09 @agent_ppo2.py:121][0m #------------------------ Iteration 358 --------------------------#
[32m[20221214 14:06:09 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:06:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:10 @agent_ppo2.py:185][0m |           0.0038 |         162.6507 |          18.0840 |
[32m[20221214 14:06:10 @agent_ppo2.py:185][0m |          -0.0018 |         155.8392 |          18.0993 |
[32m[20221214 14:06:10 @agent_ppo2.py:185][0m |          -0.0030 |         154.9907 |          18.1062 |
[32m[20221214 14:06:10 @agent_ppo2.py:185][0m |          -0.0030 |         154.5230 |          18.1221 |
[32m[20221214 14:06:10 @agent_ppo2.py:185][0m |          -0.0010 |         154.0715 |          18.1379 |
[32m[20221214 14:06:10 @agent_ppo2.py:185][0m |          -0.0010 |         153.6420 |          18.1515 |
[32m[20221214 14:06:10 @agent_ppo2.py:185][0m |           0.0082 |         163.3720 |          18.1442 |
[32m[20221214 14:06:10 @agent_ppo2.py:185][0m |           0.0010 |         153.6052 |          18.1068 |
[32m[20221214 14:06:10 @agent_ppo2.py:185][0m |          -0.0023 |         152.8079 |          18.1102 |
[32m[20221214 14:06:10 @agent_ppo2.py:185][0m |          -0.0024 |         152.7620 |          18.1407 |
[32m[20221214 14:06:10 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:06:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 769.72
[32m[20221214 14:06:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 779.45
[32m[20221214 14:06:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.82
[32m[20221214 14:06:10 @agent_ppo2.py:143][0m Total time:       8.14 min
[32m[20221214 14:06:10 @agent_ppo2.py:145][0m 735232 total steps have happened
[32m[20221214 14:06:10 @agent_ppo2.py:121][0m #------------------------ Iteration 359 --------------------------#
[32m[20221214 14:06:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:06:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:11 @agent_ppo2.py:185][0m |          -0.0032 |         157.2965 |          18.0502 |
[32m[20221214 14:06:11 @agent_ppo2.py:185][0m |          -0.0005 |         151.7646 |          18.0630 |
[32m[20221214 14:06:11 @agent_ppo2.py:185][0m |           0.0098 |         160.9739 |          18.0413 |
[32m[20221214 14:06:11 @agent_ppo2.py:185][0m |          -0.0010 |         149.0731 |          18.0515 |
[32m[20221214 14:06:11 @agent_ppo2.py:185][0m |          -0.0025 |         148.3516 |          18.0551 |
[32m[20221214 14:06:11 @agent_ppo2.py:185][0m |           0.0106 |         166.0173 |          18.0462 |
[32m[20221214 14:06:11 @agent_ppo2.py:185][0m |          -0.0032 |         148.2124 |          18.0433 |
[32m[20221214 14:06:11 @agent_ppo2.py:185][0m |          -0.0018 |         147.0917 |          18.0489 |
[32m[20221214 14:06:11 @agent_ppo2.py:185][0m |          -0.0012 |         146.4796 |          18.0461 |
[32m[20221214 14:06:12 @agent_ppo2.py:185][0m |           0.0008 |         146.3876 |          18.0378 |
[32m[20221214 14:06:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:06:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.96
[32m[20221214 14:06:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.82
[32m[20221214 14:06:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.62
[32m[20221214 14:06:12 @agent_ppo2.py:143][0m Total time:       8.16 min
[32m[20221214 14:06:12 @agent_ppo2.py:145][0m 737280 total steps have happened
[32m[20221214 14:06:12 @agent_ppo2.py:121][0m #------------------------ Iteration 360 --------------------------#
[32m[20221214 14:06:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:06:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:12 @agent_ppo2.py:185][0m |           0.0005 |         179.8385 |          18.2897 |
[32m[20221214 14:06:12 @agent_ppo2.py:185][0m |           0.0045 |         174.6061 |          18.2609 |
[32m[20221214 14:06:12 @agent_ppo2.py:185][0m |          -0.0035 |         168.9994 |          18.2528 |
[32m[20221214 14:06:12 @agent_ppo2.py:185][0m |          -0.0004 |         168.5860 |          18.2452 |
[32m[20221214 14:06:12 @agent_ppo2.py:185][0m |           0.0018 |         169.4104 |          18.2447 |
[32m[20221214 14:06:12 @agent_ppo2.py:185][0m |          -0.0043 |         164.8780 |          18.2426 |
[32m[20221214 14:06:13 @agent_ppo2.py:185][0m |          -0.0050 |         163.8303 |          18.2453 |
[32m[20221214 14:06:13 @agent_ppo2.py:185][0m |          -0.0037 |         163.2064 |          18.2389 |
[32m[20221214 14:06:13 @agent_ppo2.py:185][0m |          -0.0052 |         162.4084 |          18.2344 |
[32m[20221214 14:06:13 @agent_ppo2.py:185][0m |          -0.0051 |         162.0706 |          18.2414 |
[32m[20221214 14:06:13 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:06:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.91
[32m[20221214 14:06:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.90
[32m[20221214 14:06:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.59
[32m[20221214 14:06:13 @agent_ppo2.py:143][0m Total time:       8.18 min
[32m[20221214 14:06:13 @agent_ppo2.py:145][0m 739328 total steps have happened
[32m[20221214 14:06:13 @agent_ppo2.py:121][0m #------------------------ Iteration 361 --------------------------#
[32m[20221214 14:06:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:13 @agent_ppo2.py:185][0m |           0.0030 |         135.1838 |          18.1758 |
[32m[20221214 14:06:13 @agent_ppo2.py:185][0m |          -0.0021 |         119.7377 |          18.1422 |
[32m[20221214 14:06:13 @agent_ppo2.py:185][0m |           0.0032 |         117.8544 |          18.1276 |
[32m[20221214 14:06:14 @agent_ppo2.py:185][0m |          -0.0032 |         111.2877 |          18.1225 |
[32m[20221214 14:06:14 @agent_ppo2.py:185][0m |           0.0030 |         111.7599 |          18.1178 |
[32m[20221214 14:06:14 @agent_ppo2.py:185][0m |          -0.0039 |         106.2322 |          18.1128 |
[32m[20221214 14:06:14 @agent_ppo2.py:185][0m |          -0.0034 |         104.6063 |          18.1114 |
[32m[20221214 14:06:14 @agent_ppo2.py:185][0m |          -0.0039 |         103.2767 |          18.1041 |
[32m[20221214 14:06:14 @agent_ppo2.py:185][0m |          -0.0064 |         102.3536 |          18.1001 |
[32m[20221214 14:06:14 @agent_ppo2.py:185][0m |          -0.0018 |         101.4931 |          18.0855 |
[32m[20221214 14:06:14 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:06:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.68
[32m[20221214 14:06:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.11
[32m[20221214 14:06:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.84
[32m[20221214 14:06:14 @agent_ppo2.py:143][0m Total time:       8.20 min
[32m[20221214 14:06:14 @agent_ppo2.py:145][0m 741376 total steps have happened
[32m[20221214 14:06:14 @agent_ppo2.py:121][0m #------------------------ Iteration 362 --------------------------#
[32m[20221214 14:06:15 @agent_ppo2.py:127][0m Sampling time: 0.30 s by 5 slaves
[32m[20221214 14:06:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:15 @agent_ppo2.py:185][0m |          -0.0016 |         157.0966 |          18.0254 |
[32m[20221214 14:06:15 @agent_ppo2.py:185][0m |          -0.0024 |         143.7207 |          18.0254 |
[32m[20221214 14:06:15 @agent_ppo2.py:185][0m |          -0.0017 |         135.8076 |          18.0210 |
[32m[20221214 14:06:15 @agent_ppo2.py:185][0m |          -0.0039 |         131.4473 |          18.0200 |
[32m[20221214 14:06:15 @agent_ppo2.py:185][0m |          -0.0031 |         128.4553 |          18.0249 |
[32m[20221214 14:06:15 @agent_ppo2.py:185][0m |          -0.0018 |         126.2799 |          18.0387 |
[32m[20221214 14:06:15 @agent_ppo2.py:185][0m |           0.0010 |         125.2154 |          18.0461 |
[32m[20221214 14:06:15 @agent_ppo2.py:185][0m |          -0.0004 |         123.9151 |          18.0454 |
[32m[20221214 14:06:16 @agent_ppo2.py:185][0m |          -0.0025 |         123.2107 |          18.0478 |
[32m[20221214 14:06:16 @agent_ppo2.py:185][0m |          -0.0004 |         122.1238 |          18.0504 |
[32m[20221214 14:06:16 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:06:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.96
[32m[20221214 14:06:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.47
[32m[20221214 14:06:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.94
[32m[20221214 14:06:16 @agent_ppo2.py:143][0m Total time:       8.23 min
[32m[20221214 14:06:16 @agent_ppo2.py:145][0m 743424 total steps have happened
[32m[20221214 14:06:16 @agent_ppo2.py:121][0m #------------------------ Iteration 363 --------------------------#
[32m[20221214 14:06:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:16 @agent_ppo2.py:185][0m |          -0.0006 |         153.9014 |          18.0930 |
[32m[20221214 14:06:16 @agent_ppo2.py:185][0m |          -0.0045 |         149.1821 |          18.0836 |
[32m[20221214 14:06:16 @agent_ppo2.py:185][0m |           0.0017 |         153.1587 |          18.0738 |
[32m[20221214 14:06:16 @agent_ppo2.py:185][0m |          -0.0007 |         146.3426 |          18.0528 |
[32m[20221214 14:06:17 @agent_ppo2.py:185][0m |          -0.0035 |         143.4482 |          18.0517 |
[32m[20221214 14:06:17 @agent_ppo2.py:185][0m |          -0.0021 |         142.3514 |          18.0493 |
[32m[20221214 14:06:17 @agent_ppo2.py:185][0m |           0.0233 |         179.0683 |          18.0312 |
[32m[20221214 14:06:17 @agent_ppo2.py:185][0m |           0.0001 |         140.4114 |          18.0134 |
[32m[20221214 14:06:17 @agent_ppo2.py:185][0m |          -0.0050 |         139.4302 |          18.0219 |
[32m[20221214 14:06:17 @agent_ppo2.py:185][0m |           0.0076 |         153.0583 |          18.0183 |
[32m[20221214 14:06:17 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:06:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.24
[32m[20221214 14:06:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.82
[32m[20221214 14:06:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.78
[32m[20221214 14:06:17 @agent_ppo2.py:143][0m Total time:       8.25 min
[32m[20221214 14:06:17 @agent_ppo2.py:145][0m 745472 total steps have happened
[32m[20221214 14:06:17 @agent_ppo2.py:121][0m #------------------------ Iteration 364 --------------------------#
[32m[20221214 14:06:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:18 @agent_ppo2.py:185][0m |          -0.0010 |         191.1381 |          17.9218 |
[32m[20221214 14:06:18 @agent_ppo2.py:185][0m |          -0.0052 |         185.7199 |          17.9070 |
[32m[20221214 14:06:18 @agent_ppo2.py:185][0m |          -0.0006 |         181.5160 |          17.9009 |
[32m[20221214 14:06:18 @agent_ppo2.py:185][0m |          -0.0018 |         178.9498 |          17.8915 |
[32m[20221214 14:06:18 @agent_ppo2.py:185][0m |          -0.0004 |         177.7566 |          17.8765 |
[32m[20221214 14:06:18 @agent_ppo2.py:185][0m |           0.0038 |         178.7826 |          17.8699 |
[32m[20221214 14:06:18 @agent_ppo2.py:185][0m |          -0.0026 |         175.7530 |          17.8658 |
[32m[20221214 14:06:18 @agent_ppo2.py:185][0m |          -0.0009 |         174.7718 |          17.8540 |
[32m[20221214 14:06:18 @agent_ppo2.py:185][0m |          -0.0015 |         174.5359 |          17.8442 |
[32m[20221214 14:06:18 @agent_ppo2.py:185][0m |           0.0005 |         173.8798 |          17.8370 |
[32m[20221214 14:06:18 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:06:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 750.36
[32m[20221214 14:06:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.43
[32m[20221214 14:06:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.08
[32m[20221214 14:06:19 @agent_ppo2.py:143][0m Total time:       8.27 min
[32m[20221214 14:06:19 @agent_ppo2.py:145][0m 747520 total steps have happened
[32m[20221214 14:06:19 @agent_ppo2.py:121][0m #------------------------ Iteration 365 --------------------------#
[32m[20221214 14:06:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:19 @agent_ppo2.py:185][0m |          -0.0018 |         186.3399 |          17.8845 |
[32m[20221214 14:06:19 @agent_ppo2.py:185][0m |          -0.0037 |         182.3943 |          17.8608 |
[32m[20221214 14:06:19 @agent_ppo2.py:185][0m |          -0.0019 |         179.7931 |          17.8636 |
[32m[20221214 14:06:19 @agent_ppo2.py:185][0m |           0.0011 |         179.0527 |          17.8555 |
[32m[20221214 14:06:19 @agent_ppo2.py:185][0m |          -0.0020 |         176.9342 |          17.8563 |
[32m[20221214 14:06:19 @agent_ppo2.py:185][0m |          -0.0035 |         175.8139 |          17.8527 |
[32m[20221214 14:06:19 @agent_ppo2.py:185][0m |          -0.0023 |         174.9283 |          17.8559 |
[32m[20221214 14:06:19 @agent_ppo2.py:185][0m |          -0.0039 |         174.5902 |          17.8549 |
[32m[20221214 14:06:20 @agent_ppo2.py:185][0m |           0.0025 |         176.0123 |          17.8541 |
[32m[20221214 14:06:20 @agent_ppo2.py:185][0m |          -0.0037 |         173.5651 |          17.8628 |
[32m[20221214 14:06:20 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:06:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.09
[32m[20221214 14:06:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.52
[32m[20221214 14:06:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.06
[32m[20221214 14:06:20 @agent_ppo2.py:143][0m Total time:       8.30 min
[32m[20221214 14:06:20 @agent_ppo2.py:145][0m 749568 total steps have happened
[32m[20221214 14:06:20 @agent_ppo2.py:121][0m #------------------------ Iteration 366 --------------------------#
[32m[20221214 14:06:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:20 @agent_ppo2.py:185][0m |          -0.0015 |         166.1938 |          17.8506 |
[32m[20221214 14:06:20 @agent_ppo2.py:185][0m |           0.0027 |         163.4263 |          17.8488 |
[32m[20221214 14:06:20 @agent_ppo2.py:185][0m |           0.0008 |         157.8299 |          17.8460 |
[32m[20221214 14:06:20 @agent_ppo2.py:185][0m |           0.0009 |         156.4031 |          17.8442 |
[32m[20221214 14:06:21 @agent_ppo2.py:185][0m |          -0.0036 |         153.8899 |          17.8567 |
[32m[20221214 14:06:21 @agent_ppo2.py:185][0m |          -0.0017 |         152.7040 |          17.8525 |
[32m[20221214 14:06:21 @agent_ppo2.py:185][0m |           0.0075 |         165.6382 |          17.8545 |
[32m[20221214 14:06:21 @agent_ppo2.py:185][0m |          -0.0007 |         149.5301 |          17.8660 |
[32m[20221214 14:06:21 @agent_ppo2.py:185][0m |          -0.0033 |         147.9548 |          17.8639 |
[32m[20221214 14:06:21 @agent_ppo2.py:185][0m |           0.0048 |         152.5846 |          17.8621 |
[32m[20221214 14:06:21 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:06:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 662.52
[32m[20221214 14:06:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.65
[32m[20221214 14:06:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.57
[32m[20221214 14:06:21 @agent_ppo2.py:143][0m Total time:       8.32 min
[32m[20221214 14:06:21 @agent_ppo2.py:145][0m 751616 total steps have happened
[32m[20221214 14:06:21 @agent_ppo2.py:121][0m #------------------------ Iteration 367 --------------------------#
[32m[20221214 14:06:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:22 @agent_ppo2.py:185][0m |          -0.0016 |         183.9481 |          17.8810 |
[32m[20221214 14:06:22 @agent_ppo2.py:185][0m |           0.0062 |         190.7990 |          17.8838 |
[32m[20221214 14:06:22 @agent_ppo2.py:185][0m |          -0.0025 |         180.4887 |          17.8989 |
[32m[20221214 14:06:22 @agent_ppo2.py:185][0m |          -0.0029 |         179.3482 |          17.8896 |
[32m[20221214 14:06:22 @agent_ppo2.py:185][0m |          -0.0021 |         177.8893 |          17.8985 |
[32m[20221214 14:06:22 @agent_ppo2.py:185][0m |          -0.0016 |         176.9297 |          17.8903 |
[32m[20221214 14:06:22 @agent_ppo2.py:185][0m |          -0.0018 |         176.1149 |          17.8967 |
[32m[20221214 14:06:22 @agent_ppo2.py:185][0m |          -0.0020 |         175.5478 |          17.8994 |
[32m[20221214 14:06:22 @agent_ppo2.py:185][0m |          -0.0021 |         175.6513 |          17.9134 |
[32m[20221214 14:06:22 @agent_ppo2.py:185][0m |          -0.0019 |         174.9178 |          17.9072 |
[32m[20221214 14:06:22 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:06:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 784.71
[32m[20221214 14:06:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.36
[32m[20221214 14:06:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.56
[32m[20221214 14:06:23 @agent_ppo2.py:143][0m Total time:       8.34 min
[32m[20221214 14:06:23 @agent_ppo2.py:145][0m 753664 total steps have happened
[32m[20221214 14:06:23 @agent_ppo2.py:121][0m #------------------------ Iteration 368 --------------------------#
[32m[20221214 14:06:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:06:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:23 @agent_ppo2.py:185][0m |          -0.0039 |         183.9031 |          17.9370 |
[32m[20221214 14:06:23 @agent_ppo2.py:185][0m |          -0.0057 |         177.2476 |          17.9197 |
[32m[20221214 14:06:23 @agent_ppo2.py:185][0m |          -0.0050 |         173.0723 |          17.9196 |
[32m[20221214 14:06:23 @agent_ppo2.py:185][0m |          -0.0046 |         171.2851 |          17.9168 |
[32m[20221214 14:06:23 @agent_ppo2.py:185][0m |          -0.0034 |         169.7470 |          17.9200 |
[32m[20221214 14:06:23 @agent_ppo2.py:185][0m |          -0.0046 |         168.4325 |          17.9205 |
[32m[20221214 14:06:23 @agent_ppo2.py:185][0m |           0.0061 |         182.3649 |          17.9180 |
[32m[20221214 14:06:24 @agent_ppo2.py:185][0m |          -0.0033 |         166.0299 |          17.9118 |
[32m[20221214 14:06:24 @agent_ppo2.py:185][0m |          -0.0048 |         165.2158 |          17.9207 |
[32m[20221214 14:06:24 @agent_ppo2.py:185][0m |           0.0083 |         180.4119 |          17.9235 |
[32m[20221214 14:06:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:06:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.71
[32m[20221214 14:06:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.95
[32m[20221214 14:06:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.49
[32m[20221214 14:06:24 @agent_ppo2.py:143][0m Total time:       8.36 min
[32m[20221214 14:06:24 @agent_ppo2.py:145][0m 755712 total steps have happened
[32m[20221214 14:06:24 @agent_ppo2.py:121][0m #------------------------ Iteration 369 --------------------------#
[32m[20221214 14:06:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:06:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:24 @agent_ppo2.py:185][0m |          -0.0015 |         208.5312 |          17.9165 |
[32m[20221214 14:06:24 @agent_ppo2.py:185][0m |          -0.0016 |         206.1578 |          17.9283 |
[32m[20221214 14:06:24 @agent_ppo2.py:185][0m |          -0.0024 |         204.4796 |          17.9179 |
[32m[20221214 14:06:24 @agent_ppo2.py:185][0m |          -0.0003 |         203.9655 |          17.9161 |
[32m[20221214 14:06:25 @agent_ppo2.py:185][0m |           0.0005 |         203.1426 |          17.9267 |
[32m[20221214 14:06:25 @agent_ppo2.py:185][0m |          -0.0019 |         201.5621 |          17.9323 |
[32m[20221214 14:06:25 @agent_ppo2.py:185][0m |           0.0004 |         200.8514 |          17.9323 |
[32m[20221214 14:06:25 @agent_ppo2.py:185][0m |          -0.0021 |         200.7848 |          17.9336 |
[32m[20221214 14:06:25 @agent_ppo2.py:185][0m |          -0.0023 |         199.7554 |          17.9373 |
[32m[20221214 14:06:25 @agent_ppo2.py:185][0m |          -0.0015 |         199.6897 |          17.9413 |
[32m[20221214 14:06:25 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:06:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.39
[32m[20221214 14:06:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.94
[32m[20221214 14:06:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.62
[32m[20221214 14:06:25 @agent_ppo2.py:143][0m Total time:       8.38 min
[32m[20221214 14:06:25 @agent_ppo2.py:145][0m 757760 total steps have happened
[32m[20221214 14:06:25 @agent_ppo2.py:121][0m #------------------------ Iteration 370 --------------------------#
[32m[20221214 14:06:25 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221214 14:06:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:26 @agent_ppo2.py:185][0m |          -0.0022 |         216.3006 |          17.9297 |
[32m[20221214 14:06:26 @agent_ppo2.py:185][0m |          -0.0022 |         211.5672 |          17.9051 |
[32m[20221214 14:06:26 @agent_ppo2.py:185][0m |          -0.0023 |         209.1233 |          17.8950 |
[32m[20221214 14:06:26 @agent_ppo2.py:185][0m |          -0.0021 |         207.7084 |          17.8849 |
[32m[20221214 14:06:26 @agent_ppo2.py:185][0m |          -0.0001 |         207.5137 |          17.8579 |
[32m[20221214 14:06:26 @agent_ppo2.py:185][0m |          -0.0036 |         206.2613 |          17.8644 |
[32m[20221214 14:06:26 @agent_ppo2.py:185][0m |          -0.0023 |         205.1635 |          17.8470 |
[32m[20221214 14:06:26 @agent_ppo2.py:185][0m |          -0.0016 |         204.8574 |          17.8346 |
[32m[20221214 14:06:26 @agent_ppo2.py:185][0m |           0.0017 |         204.6034 |          17.8112 |
[32m[20221214 14:06:26 @agent_ppo2.py:185][0m |          -0.0036 |         203.9471 |          17.8043 |
[32m[20221214 14:06:26 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:06:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.66
[32m[20221214 14:06:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.92
[32m[20221214 14:06:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.47
[32m[20221214 14:06:27 @agent_ppo2.py:143][0m Total time:       8.41 min
[32m[20221214 14:06:27 @agent_ppo2.py:145][0m 759808 total steps have happened
[32m[20221214 14:06:27 @agent_ppo2.py:121][0m #------------------------ Iteration 371 --------------------------#
[32m[20221214 14:06:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:27 @agent_ppo2.py:185][0m |          -0.0019 |         213.2053 |          17.8695 |
[32m[20221214 14:06:27 @agent_ppo2.py:185][0m |          -0.0022 |         208.8102 |          17.8614 |
[32m[20221214 14:06:27 @agent_ppo2.py:185][0m |           0.0099 |         223.0302 |          17.8643 |
[32m[20221214 14:06:27 @agent_ppo2.py:185][0m |          -0.0014 |         201.3635 |          17.8689 |
[32m[20221214 14:06:27 @agent_ppo2.py:185][0m |           0.0106 |         215.4787 |          17.8703 |
[32m[20221214 14:06:27 @agent_ppo2.py:185][0m |           0.0029 |         200.0925 |          17.8689 |
[32m[20221214 14:06:27 @agent_ppo2.py:185][0m |          -0.0022 |         194.3432 |          17.8650 |
[32m[20221214 14:06:28 @agent_ppo2.py:185][0m |          -0.0015 |         192.0258 |          17.8680 |
[32m[20221214 14:06:28 @agent_ppo2.py:185][0m |          -0.0030 |         190.5494 |          17.8653 |
[32m[20221214 14:06:28 @agent_ppo2.py:185][0m |          -0.0026 |         189.3471 |          17.8662 |
[32m[20221214 14:06:28 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:06:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.96
[32m[20221214 14:06:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.91
[32m[20221214 14:06:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.26
[32m[20221214 14:06:28 @agent_ppo2.py:143][0m Total time:       8.43 min
[32m[20221214 14:06:28 @agent_ppo2.py:145][0m 761856 total steps have happened
[32m[20221214 14:06:28 @agent_ppo2.py:121][0m #------------------------ Iteration 372 --------------------------#
[32m[20221214 14:06:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:28 @agent_ppo2.py:185][0m |          -0.0021 |         214.9704 |          17.8586 |
[32m[20221214 14:06:28 @agent_ppo2.py:185][0m |           0.0066 |         221.5256 |          17.8626 |
[32m[20221214 14:06:28 @agent_ppo2.py:185][0m |           0.0122 |         230.9608 |          17.8592 |
[32m[20221214 14:06:29 @agent_ppo2.py:185][0m |           0.0003 |         208.4297 |          17.8484 |
[32m[20221214 14:06:29 @agent_ppo2.py:185][0m |           0.0082 |         221.6271 |          17.8424 |
[32m[20221214 14:06:29 @agent_ppo2.py:185][0m |          -0.0027 |         207.5333 |          17.8489 |
[32m[20221214 14:06:29 @agent_ppo2.py:185][0m |          -0.0014 |         207.1425 |          17.8508 |
[32m[20221214 14:06:29 @agent_ppo2.py:185][0m |          -0.0013 |         206.8030 |          17.8384 |
[32m[20221214 14:06:29 @agent_ppo2.py:185][0m |          -0.0027 |         206.7051 |          17.8382 |
[32m[20221214 14:06:29 @agent_ppo2.py:185][0m |          -0.0019 |         206.0258 |          17.8394 |
[32m[20221214 14:06:29 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:06:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.64
[32m[20221214 14:06:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.93
[32m[20221214 14:06:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.40
[32m[20221214 14:06:29 @agent_ppo2.py:143][0m Total time:       8.45 min
[32m[20221214 14:06:29 @agent_ppo2.py:145][0m 763904 total steps have happened
[32m[20221214 14:06:29 @agent_ppo2.py:121][0m #------------------------ Iteration 373 --------------------------#
[32m[20221214 14:06:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:06:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:30 @agent_ppo2.py:185][0m |           0.0140 |         235.9677 |          17.8385 |
[32m[20221214 14:06:30 @agent_ppo2.py:185][0m |           0.0039 |         208.5513 |          17.8515 |
[32m[20221214 14:06:30 @agent_ppo2.py:185][0m |          -0.0004 |         203.2258 |          17.8367 |
[32m[20221214 14:06:30 @agent_ppo2.py:185][0m |          -0.0018 |         202.9178 |          17.8292 |
[32m[20221214 14:06:30 @agent_ppo2.py:185][0m |          -0.0020 |         202.5521 |          17.8219 |
[32m[20221214 14:06:30 @agent_ppo2.py:185][0m |           0.0062 |         212.0117 |          17.8116 |
[32m[20221214 14:06:30 @agent_ppo2.py:185][0m |           0.0131 |         231.9436 |          17.7920 |
[32m[20221214 14:06:30 @agent_ppo2.py:185][0m |          -0.0019 |         201.5546 |          17.7826 |
[32m[20221214 14:06:30 @agent_ppo2.py:185][0m |          -0.0026 |         201.3042 |          17.7691 |
[32m[20221214 14:06:30 @agent_ppo2.py:185][0m |           0.0061 |         210.4828 |          17.7695 |
[32m[20221214 14:06:30 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:06:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.51
[32m[20221214 14:06:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.72
[32m[20221214 14:06:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.47
[32m[20221214 14:06:31 @agent_ppo2.py:143][0m Total time:       8.48 min
[32m[20221214 14:06:31 @agent_ppo2.py:145][0m 765952 total steps have happened
[32m[20221214 14:06:31 @agent_ppo2.py:121][0m #------------------------ Iteration 374 --------------------------#
[32m[20221214 14:06:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:31 @agent_ppo2.py:185][0m |          -0.0021 |         206.7743 |          17.8710 |
[32m[20221214 14:06:31 @agent_ppo2.py:185][0m |          -0.0022 |         203.6425 |          17.8680 |
[32m[20221214 14:06:31 @agent_ppo2.py:185][0m |          -0.0030 |         202.2198 |          17.8697 |
[32m[20221214 14:06:31 @agent_ppo2.py:185][0m |          -0.0030 |         200.7942 |          17.8583 |
[32m[20221214 14:06:31 @agent_ppo2.py:185][0m |          -0.0025 |         199.6275 |          17.8657 |
[32m[20221214 14:06:32 @agent_ppo2.py:185][0m |          -0.0025 |         198.9649 |          17.8571 |
[32m[20221214 14:06:32 @agent_ppo2.py:185][0m |          -0.0026 |         198.3162 |          17.8563 |
[32m[20221214 14:06:32 @agent_ppo2.py:185][0m |           0.0185 |         235.8198 |          17.8510 |
[32m[20221214 14:06:32 @agent_ppo2.py:185][0m |          -0.0016 |         198.9062 |          17.8558 |
[32m[20221214 14:06:32 @agent_ppo2.py:185][0m |          -0.0000 |         198.4752 |          17.8471 |
[32m[20221214 14:06:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:06:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.46
[32m[20221214 14:06:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.80
[32m[20221214 14:06:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.16
[32m[20221214 14:06:32 @agent_ppo2.py:143][0m Total time:       8.50 min
[32m[20221214 14:06:32 @agent_ppo2.py:145][0m 768000 total steps have happened
[32m[20221214 14:06:32 @agent_ppo2.py:121][0m #------------------------ Iteration 375 --------------------------#
[32m[20221214 14:06:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:32 @agent_ppo2.py:185][0m |          -0.0008 |         197.5691 |          17.8124 |
[32m[20221214 14:06:32 @agent_ppo2.py:185][0m |          -0.0037 |         195.8540 |          17.7975 |
[32m[20221214 14:06:33 @agent_ppo2.py:185][0m |          -0.0027 |         193.8574 |          17.8038 |
[32m[20221214 14:06:33 @agent_ppo2.py:185][0m |          -0.0020 |         193.1259 |          17.8047 |
[32m[20221214 14:06:33 @agent_ppo2.py:185][0m |          -0.0037 |         191.7887 |          17.7994 |
[32m[20221214 14:06:33 @agent_ppo2.py:185][0m |          -0.0026 |         191.7124 |          17.7966 |
[32m[20221214 14:06:33 @agent_ppo2.py:185][0m |          -0.0043 |         190.6456 |          17.7947 |
[32m[20221214 14:06:33 @agent_ppo2.py:185][0m |          -0.0030 |         190.3878 |          17.7867 |
[32m[20221214 14:06:33 @agent_ppo2.py:185][0m |          -0.0041 |         190.1740 |          17.7907 |
[32m[20221214 14:06:33 @agent_ppo2.py:185][0m |          -0.0048 |         189.6853 |          17.7926 |
[32m[20221214 14:06:33 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:06:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.17
[32m[20221214 14:06:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.29
[32m[20221214 14:06:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.33
[32m[20221214 14:06:33 @agent_ppo2.py:143][0m Total time:       8.52 min
[32m[20221214 14:06:33 @agent_ppo2.py:145][0m 770048 total steps have happened
[32m[20221214 14:06:33 @agent_ppo2.py:121][0m #------------------------ Iteration 376 --------------------------#
[32m[20221214 14:06:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:34 @agent_ppo2.py:185][0m |          -0.0017 |         196.7714 |          17.7759 |
[32m[20221214 14:06:34 @agent_ppo2.py:185][0m |          -0.0036 |         194.6009 |          17.7758 |
[32m[20221214 14:06:34 @agent_ppo2.py:185][0m |          -0.0036 |         193.5220 |          17.7623 |
[32m[20221214 14:06:34 @agent_ppo2.py:185][0m |          -0.0035 |         192.7262 |          17.7482 |
[32m[20221214 14:06:34 @agent_ppo2.py:185][0m |          -0.0019 |         191.7631 |          17.7449 |
[32m[20221214 14:06:34 @agent_ppo2.py:185][0m |          -0.0041 |         191.3295 |          17.7324 |
[32m[20221214 14:06:34 @agent_ppo2.py:185][0m |          -0.0020 |         190.6445 |          17.7273 |
[32m[20221214 14:06:34 @agent_ppo2.py:185][0m |          -0.0026 |         190.8174 |          17.7099 |
[32m[20221214 14:06:34 @agent_ppo2.py:185][0m |          -0.0031 |         189.7295 |          17.7030 |
[32m[20221214 14:06:35 @agent_ppo2.py:185][0m |          -0.0038 |         189.8188 |          17.6996 |
[32m[20221214 14:06:35 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:06:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.60
[32m[20221214 14:06:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.37
[32m[20221214 14:06:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 749.55
[32m[20221214 14:06:35 @agent_ppo2.py:143][0m Total time:       8.54 min
[32m[20221214 14:06:35 @agent_ppo2.py:145][0m 772096 total steps have happened
[32m[20221214 14:06:35 @agent_ppo2.py:121][0m #------------------------ Iteration 377 --------------------------#
[32m[20221214 14:06:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:35 @agent_ppo2.py:185][0m |           0.0046 |         224.7569 |          17.7270 |
[32m[20221214 14:06:35 @agent_ppo2.py:185][0m |           0.0011 |         217.5947 |          17.7213 |
[32m[20221214 14:06:35 @agent_ppo2.py:185][0m |           0.0070 |         228.7655 |          17.7103 |
[32m[20221214 14:06:35 @agent_ppo2.py:185][0m |           0.0009 |         216.3701 |          17.7103 |
[32m[20221214 14:06:35 @agent_ppo2.py:185][0m |          -0.0022 |         215.2663 |          17.7031 |
[32m[20221214 14:06:36 @agent_ppo2.py:185][0m |           0.0110 |         246.9270 |          17.6942 |
[32m[20221214 14:06:36 @agent_ppo2.py:185][0m |           0.0072 |         219.2647 |          17.6872 |
[32m[20221214 14:06:36 @agent_ppo2.py:185][0m |          -0.0028 |         214.5883 |          17.6652 |
[32m[20221214 14:06:36 @agent_ppo2.py:185][0m |           0.0033 |         218.0249 |          17.6629 |
[32m[20221214 14:06:36 @agent_ppo2.py:185][0m |           0.0022 |         217.0006 |          17.6544 |
[32m[20221214 14:06:36 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:06:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.16
[32m[20221214 14:06:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.38
[32m[20221214 14:06:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.41
[32m[20221214 14:06:36 @agent_ppo2.py:143][0m Total time:       8.57 min
[32m[20221214 14:06:36 @agent_ppo2.py:145][0m 774144 total steps have happened
[32m[20221214 14:06:36 @agent_ppo2.py:121][0m #------------------------ Iteration 378 --------------------------#
[32m[20221214 14:06:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:36 @agent_ppo2.py:185][0m |           0.0014 |         136.7603 |          17.5256 |
[32m[20221214 14:06:37 @agent_ppo2.py:185][0m |          -0.0042 |         122.1480 |          17.5336 |
[32m[20221214 14:06:37 @agent_ppo2.py:185][0m |          -0.0021 |         118.9959 |          17.5283 |
[32m[20221214 14:06:37 @agent_ppo2.py:185][0m |          -0.0016 |         117.2140 |          17.5282 |
[32m[20221214 14:06:37 @agent_ppo2.py:185][0m |          -0.0053 |         115.5448 |          17.5220 |
[32m[20221214 14:06:37 @agent_ppo2.py:185][0m |          -0.0056 |         113.8748 |          17.5250 |
[32m[20221214 14:06:37 @agent_ppo2.py:185][0m |          -0.0032 |         112.6062 |          17.5163 |
[32m[20221214 14:06:37 @agent_ppo2.py:185][0m |          -0.0028 |         111.6811 |          17.5264 |
[32m[20221214 14:06:37 @agent_ppo2.py:185][0m |          -0.0045 |         110.4888 |          17.5121 |
[32m[20221214 14:06:37 @agent_ppo2.py:185][0m |          -0.0016 |         110.0285 |          17.5121 |
[32m[20221214 14:06:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:06:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 566.50
[32m[20221214 14:06:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.71
[32m[20221214 14:06:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.66
[32m[20221214 14:06:37 @agent_ppo2.py:143][0m Total time:       8.59 min
[32m[20221214 14:06:37 @agent_ppo2.py:145][0m 776192 total steps have happened
[32m[20221214 14:06:37 @agent_ppo2.py:121][0m #------------------------ Iteration 379 --------------------------#
[32m[20221214 14:06:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:38 @agent_ppo2.py:185][0m |          -0.0046 |         149.9328 |          17.4929 |
[32m[20221214 14:06:38 @agent_ppo2.py:185][0m |          -0.0050 |         128.6614 |          17.4846 |
[32m[20221214 14:06:38 @agent_ppo2.py:185][0m |          -0.0056 |         121.5258 |          17.4812 |
[32m[20221214 14:06:38 @agent_ppo2.py:185][0m |          -0.0051 |         117.3179 |          17.4811 |
[32m[20221214 14:06:38 @agent_ppo2.py:185][0m |           0.0006 |         120.0683 |          17.4741 |
[32m[20221214 14:06:38 @agent_ppo2.py:185][0m |           0.0090 |         133.6395 |          17.4796 |
[32m[20221214 14:06:38 @agent_ppo2.py:185][0m |          -0.0025 |         112.2440 |          17.4771 |
[32m[20221214 14:06:38 @agent_ppo2.py:185][0m |          -0.0053 |         110.7326 |          17.4745 |
[32m[20221214 14:06:38 @agent_ppo2.py:185][0m |          -0.0013 |         111.1569 |          17.4700 |
[32m[20221214 14:06:39 @agent_ppo2.py:185][0m |          -0.0043 |         108.2745 |          17.4635 |
[32m[20221214 14:06:39 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:06:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.87
[32m[20221214 14:06:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.04
[32m[20221214 14:06:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.55
[32m[20221214 14:06:39 @agent_ppo2.py:143][0m Total time:       8.61 min
[32m[20221214 14:06:39 @agent_ppo2.py:145][0m 778240 total steps have happened
[32m[20221214 14:06:39 @agent_ppo2.py:121][0m #------------------------ Iteration 380 --------------------------#
[32m[20221214 14:06:39 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:06:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:39 @agent_ppo2.py:185][0m |          -0.0023 |         193.7296 |          17.5604 |
[32m[20221214 14:06:39 @agent_ppo2.py:185][0m |          -0.0042 |         189.6661 |          17.5658 |
[32m[20221214 14:06:39 @agent_ppo2.py:185][0m |          -0.0028 |         188.4023 |          17.5503 |
[32m[20221214 14:06:39 @agent_ppo2.py:185][0m |          -0.0015 |         187.1420 |          17.5510 |
[32m[20221214 14:06:39 @agent_ppo2.py:185][0m |          -0.0031 |         185.6795 |          17.5437 |
[32m[20221214 14:06:39 @agent_ppo2.py:185][0m |          -0.0042 |         185.5382 |          17.5423 |
[32m[20221214 14:06:40 @agent_ppo2.py:185][0m |          -0.0019 |         184.5247 |          17.5447 |
[32m[20221214 14:06:40 @agent_ppo2.py:185][0m |           0.0075 |         203.2244 |          17.5279 |
[32m[20221214 14:06:40 @agent_ppo2.py:185][0m |          -0.0031 |         182.9159 |          17.5302 |
[32m[20221214 14:06:40 @agent_ppo2.py:185][0m |          -0.0047 |         182.2739 |          17.5276 |
[32m[20221214 14:06:40 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:06:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 708.09
[32m[20221214 14:06:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.52
[32m[20221214 14:06:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.64
[32m[20221214 14:06:40 @agent_ppo2.py:143][0m Total time:       8.63 min
[32m[20221214 14:06:40 @agent_ppo2.py:145][0m 780288 total steps have happened
[32m[20221214 14:06:40 @agent_ppo2.py:121][0m #------------------------ Iteration 381 --------------------------#
[32m[20221214 14:06:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:06:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:40 @agent_ppo2.py:185][0m |          -0.0028 |         183.2997 |          17.4035 |
[32m[20221214 14:06:40 @agent_ppo2.py:185][0m |           0.0011 |         181.0538 |          17.4108 |
[32m[20221214 14:06:40 @agent_ppo2.py:185][0m |          -0.0022 |         177.8900 |          17.4011 |
[32m[20221214 14:06:41 @agent_ppo2.py:185][0m |          -0.0021 |         174.8676 |          17.3955 |
[32m[20221214 14:06:41 @agent_ppo2.py:185][0m |           0.0066 |         178.4107 |          17.3867 |
[32m[20221214 14:06:41 @agent_ppo2.py:185][0m |          -0.0012 |         172.6887 |          17.3826 |
[32m[20221214 14:06:41 @agent_ppo2.py:185][0m |          -0.0028 |         172.1761 |          17.3856 |
[32m[20221214 14:06:41 @agent_ppo2.py:185][0m |          -0.0028 |         171.6892 |          17.3725 |
[32m[20221214 14:06:41 @agent_ppo2.py:185][0m |          -0.0007 |         171.1720 |          17.3710 |
[32m[20221214 14:06:41 @agent_ppo2.py:185][0m |          -0.0016 |         170.6432 |          17.3679 |
[32m[20221214 14:06:41 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:06:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.84
[32m[20221214 14:06:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.02
[32m[20221214 14:06:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 750.39
[32m[20221214 14:06:41 @agent_ppo2.py:143][0m Total time:       8.65 min
[32m[20221214 14:06:41 @agent_ppo2.py:145][0m 782336 total steps have happened
[32m[20221214 14:06:41 @agent_ppo2.py:121][0m #------------------------ Iteration 382 --------------------------#
[32m[20221214 14:06:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:06:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:42 @agent_ppo2.py:185][0m |          -0.0009 |         185.0416 |          17.3876 |
[32m[20221214 14:06:42 @agent_ppo2.py:185][0m |          -0.0030 |         177.6300 |          17.4113 |
[32m[20221214 14:06:42 @agent_ppo2.py:185][0m |          -0.0046 |         175.1010 |          17.4107 |
[32m[20221214 14:06:42 @agent_ppo2.py:185][0m |          -0.0010 |         173.4909 |          17.4062 |
[32m[20221214 14:06:42 @agent_ppo2.py:185][0m |          -0.0037 |         171.7517 |          17.4068 |
[32m[20221214 14:06:42 @agent_ppo2.py:185][0m |           0.0040 |         181.4948 |          17.4126 |
[32m[20221214 14:06:42 @agent_ppo2.py:185][0m |          -0.0033 |         169.9949 |          17.4191 |
[32m[20221214 14:06:42 @agent_ppo2.py:185][0m |          -0.0032 |         168.3260 |          17.4194 |
[32m[20221214 14:06:42 @agent_ppo2.py:185][0m |          -0.0031 |         167.7181 |          17.4160 |
[32m[20221214 14:06:42 @agent_ppo2.py:185][0m |          -0.0031 |         166.7841 |          17.4185 |
[32m[20221214 14:06:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:06:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 706.60
[32m[20221214 14:06:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.36
[32m[20221214 14:06:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.78
[32m[20221214 14:06:42 @agent_ppo2.py:143][0m Total time:       8.67 min
[32m[20221214 14:06:42 @agent_ppo2.py:145][0m 784384 total steps have happened
[32m[20221214 14:06:42 @agent_ppo2.py:121][0m #------------------------ Iteration 383 --------------------------#
[32m[20221214 14:06:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:06:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:43 @agent_ppo2.py:185][0m |           0.0080 |         191.2061 |          17.4588 |
[32m[20221214 14:06:43 @agent_ppo2.py:185][0m |           0.0003 |         178.7746 |          17.4773 |
[32m[20221214 14:06:43 @agent_ppo2.py:185][0m |          -0.0028 |         176.9460 |          17.4812 |
[32m[20221214 14:06:43 @agent_ppo2.py:185][0m |          -0.0040 |         175.8600 |          17.4873 |
[32m[20221214 14:06:43 @agent_ppo2.py:185][0m |           0.0047 |         178.6506 |          17.4989 |
[32m[20221214 14:06:43 @agent_ppo2.py:185][0m |           0.0011 |         174.9582 |          17.5069 |
[32m[20221214 14:06:43 @agent_ppo2.py:185][0m |          -0.0014 |         172.3041 |          17.5140 |
[32m[20221214 14:06:43 @agent_ppo2.py:185][0m |           0.0060 |         178.3925 |          17.5278 |
[32m[20221214 14:06:43 @agent_ppo2.py:185][0m |          -0.0047 |         171.4060 |          17.5292 |
[32m[20221214 14:06:44 @agent_ppo2.py:185][0m |          -0.0005 |         171.8037 |          17.5352 |
[32m[20221214 14:06:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:06:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.67
[32m[20221214 14:06:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.33
[32m[20221214 14:06:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.36
[32m[20221214 14:06:44 @agent_ppo2.py:143][0m Total time:       8.69 min
[32m[20221214 14:06:44 @agent_ppo2.py:145][0m 786432 total steps have happened
[32m[20221214 14:06:44 @agent_ppo2.py:121][0m #------------------------ Iteration 384 --------------------------#
[32m[20221214 14:06:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:44 @agent_ppo2.py:185][0m |          -0.0030 |         181.8098 |          17.5256 |
[32m[20221214 14:06:44 @agent_ppo2.py:185][0m |          -0.0042 |         175.5243 |          17.5206 |
[32m[20221214 14:06:44 @agent_ppo2.py:185][0m |          -0.0038 |         173.2936 |          17.5184 |
[32m[20221214 14:06:44 @agent_ppo2.py:185][0m |           0.0019 |         175.7317 |          17.5135 |
[32m[20221214 14:06:44 @agent_ppo2.py:185][0m |          -0.0011 |         170.4912 |          17.5177 |
[32m[20221214 14:06:45 @agent_ppo2.py:185][0m |          -0.0053 |         169.8755 |          17.5058 |
[32m[20221214 14:06:45 @agent_ppo2.py:185][0m |           0.0035 |         174.6179 |          17.5113 |
[32m[20221214 14:06:45 @agent_ppo2.py:185][0m |          -0.0049 |         168.7263 |          17.5044 |
[32m[20221214 14:06:45 @agent_ppo2.py:185][0m |           0.0005 |         169.9991 |          17.4992 |
[32m[20221214 14:06:45 @agent_ppo2.py:185][0m |          -0.0035 |         168.3444 |          17.4947 |
[32m[20221214 14:06:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:06:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.32
[32m[20221214 14:06:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.03
[32m[20221214 14:06:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 741.78
[32m[20221214 14:06:45 @agent_ppo2.py:143][0m Total time:       8.72 min
[32m[20221214 14:06:45 @agent_ppo2.py:145][0m 788480 total steps have happened
[32m[20221214 14:06:45 @agent_ppo2.py:121][0m #------------------------ Iteration 385 --------------------------#
[32m[20221214 14:06:45 @agent_ppo2.py:127][0m Sampling time: 0.27 s by 5 slaves
[32m[20221214 14:06:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:46 @agent_ppo2.py:185][0m |           0.0123 |         199.8385 |          17.4462 |
[32m[20221214 14:06:46 @agent_ppo2.py:185][0m |          -0.0020 |         180.2386 |          17.4164 |
[32m[20221214 14:06:46 @agent_ppo2.py:185][0m |           0.0011 |         179.2474 |          17.4136 |
[32m[20221214 14:06:46 @agent_ppo2.py:185][0m |           0.0029 |         182.0576 |          17.4147 |
[32m[20221214 14:06:46 @agent_ppo2.py:185][0m |          -0.0026 |         176.6139 |          17.3964 |
[32m[20221214 14:06:46 @agent_ppo2.py:185][0m |           0.0013 |         176.4526 |          17.3753 |
[32m[20221214 14:06:46 @agent_ppo2.py:185][0m |           0.0081 |         183.1241 |          17.3651 |
[32m[20221214 14:06:46 @agent_ppo2.py:185][0m |          -0.0027 |         174.4357 |          17.3594 |
[32m[20221214 14:06:46 @agent_ppo2.py:185][0m |          -0.0014 |         173.9249 |          17.3345 |
[32m[20221214 14:06:46 @agent_ppo2.py:185][0m |          -0.0009 |         173.5035 |          17.3401 |
[32m[20221214 14:06:46 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:06:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 724.55
[32m[20221214 14:06:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 742.88
[32m[20221214 14:06:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 749.40
[32m[20221214 14:06:47 @agent_ppo2.py:143][0m Total time:       8.74 min
[32m[20221214 14:06:47 @agent_ppo2.py:145][0m 790528 total steps have happened
[32m[20221214 14:06:47 @agent_ppo2.py:121][0m #------------------------ Iteration 386 --------------------------#
[32m[20221214 14:06:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:47 @agent_ppo2.py:185][0m |          -0.0026 |         180.7090 |          17.3668 |
[32m[20221214 14:06:47 @agent_ppo2.py:185][0m |          -0.0034 |         176.0984 |          17.3579 |
[32m[20221214 14:06:47 @agent_ppo2.py:185][0m |          -0.0015 |         174.0915 |          17.3586 |
[32m[20221214 14:06:47 @agent_ppo2.py:185][0m |          -0.0009 |         172.6308 |          17.3575 |
[32m[20221214 14:06:47 @agent_ppo2.py:185][0m |           0.0018 |         175.2990 |          17.3578 |
[32m[20221214 14:06:47 @agent_ppo2.py:185][0m |          -0.0032 |         170.1469 |          17.3605 |
[32m[20221214 14:06:48 @agent_ppo2.py:185][0m |          -0.0014 |         169.6278 |          17.3515 |
[32m[20221214 14:06:48 @agent_ppo2.py:185][0m |          -0.0025 |         168.1817 |          17.3632 |
[32m[20221214 14:06:48 @agent_ppo2.py:185][0m |          -0.0016 |         167.8383 |          17.3598 |
[32m[20221214 14:06:48 @agent_ppo2.py:185][0m |           0.0068 |         179.0189 |          17.3666 |
[32m[20221214 14:06:48 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:06:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 688.45
[32m[20221214 14:06:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 738.38
[32m[20221214 14:06:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 771.98
[32m[20221214 14:06:48 @agent_ppo2.py:143][0m Total time:       8.77 min
[32m[20221214 14:06:48 @agent_ppo2.py:145][0m 792576 total steps have happened
[32m[20221214 14:06:48 @agent_ppo2.py:121][0m #------------------------ Iteration 387 --------------------------#
[32m[20221214 14:06:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:48 @agent_ppo2.py:185][0m |          -0.0022 |         175.8754 |          17.4690 |
[32m[20221214 14:06:48 @agent_ppo2.py:185][0m |          -0.0033 |         172.8107 |          17.4578 |
[32m[20221214 14:06:49 @agent_ppo2.py:185][0m |          -0.0014 |         171.3554 |          17.4539 |
[32m[20221214 14:06:49 @agent_ppo2.py:185][0m |          -0.0026 |         170.4964 |          17.4523 |
[32m[20221214 14:06:49 @agent_ppo2.py:185][0m |          -0.0037 |         169.4205 |          17.4598 |
[32m[20221214 14:06:49 @agent_ppo2.py:185][0m |          -0.0044 |         168.9916 |          17.4580 |
[32m[20221214 14:06:49 @agent_ppo2.py:185][0m |          -0.0032 |         168.7199 |          17.4548 |
[32m[20221214 14:06:49 @agent_ppo2.py:185][0m |          -0.0028 |         167.5242 |          17.4582 |
[32m[20221214 14:06:49 @agent_ppo2.py:185][0m |          -0.0029 |         167.5549 |          17.4641 |
[32m[20221214 14:06:49 @agent_ppo2.py:185][0m |          -0.0026 |         166.9352 |          17.4631 |
[32m[20221214 14:06:49 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:06:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 731.50
[32m[20221214 14:06:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 741.15
[32m[20221214 14:06:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 766.02
[32m[20221214 14:06:49 @agent_ppo2.py:143][0m Total time:       8.79 min
[32m[20221214 14:06:49 @agent_ppo2.py:145][0m 794624 total steps have happened
[32m[20221214 14:06:49 @agent_ppo2.py:121][0m #------------------------ Iteration 388 --------------------------#
[32m[20221214 14:06:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:06:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:50 @agent_ppo2.py:185][0m |          -0.0015 |         131.4411 |          17.5223 |
[32m[20221214 14:06:50 @agent_ppo2.py:185][0m |          -0.0023 |         112.8130 |          17.5059 |
[32m[20221214 14:06:50 @agent_ppo2.py:185][0m |          -0.0053 |         107.1195 |          17.4999 |
[32m[20221214 14:06:50 @agent_ppo2.py:185][0m |          -0.0054 |         104.3221 |          17.4892 |
[32m[20221214 14:06:50 @agent_ppo2.py:185][0m |          -0.0037 |         101.8730 |          17.4793 |
[32m[20221214 14:06:50 @agent_ppo2.py:185][0m |          -0.0022 |         101.1082 |          17.4695 |
[32m[20221214 14:06:50 @agent_ppo2.py:185][0m |          -0.0043 |          99.3137 |          17.4647 |
[32m[20221214 14:06:50 @agent_ppo2.py:185][0m |          -0.0050 |          98.8638 |          17.4650 |
[32m[20221214 14:06:50 @agent_ppo2.py:185][0m |          -0.0062 |          98.0811 |          17.4444 |
[32m[20221214 14:06:51 @agent_ppo2.py:185][0m |          -0.0042 |          96.7017 |          17.4399 |
[32m[20221214 14:06:51 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:06:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 645.29
[32m[20221214 14:06:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 756.08
[32m[20221214 14:06:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.83
[32m[20221214 14:06:51 @agent_ppo2.py:143][0m Total time:       8.81 min
[32m[20221214 14:06:51 @agent_ppo2.py:145][0m 796672 total steps have happened
[32m[20221214 14:06:51 @agent_ppo2.py:121][0m #------------------------ Iteration 389 --------------------------#
[32m[20221214 14:06:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:06:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:51 @agent_ppo2.py:185][0m |          -0.0050 |         194.6318 |          17.3045 |
[32m[20221214 14:06:51 @agent_ppo2.py:185][0m |          -0.0060 |         179.7171 |          17.3202 |
[32m[20221214 14:06:51 @agent_ppo2.py:185][0m |          -0.0048 |         171.2837 |          17.3090 |
[32m[20221214 14:06:51 @agent_ppo2.py:185][0m |          -0.0010 |         166.0241 |          17.3287 |
[32m[20221214 14:06:51 @agent_ppo2.py:185][0m |           0.0004 |         160.6331 |          17.3216 |
[32m[20221214 14:06:51 @agent_ppo2.py:185][0m |          -0.0057 |         157.4086 |          17.3239 |
[32m[20221214 14:06:52 @agent_ppo2.py:185][0m |          -0.0048 |         156.0793 |          17.3272 |
[32m[20221214 14:06:52 @agent_ppo2.py:185][0m |          -0.0070 |         155.2865 |          17.3263 |
[32m[20221214 14:06:52 @agent_ppo2.py:185][0m |          -0.0061 |         154.2804 |          17.3225 |
[32m[20221214 14:06:52 @agent_ppo2.py:185][0m |          -0.0066 |         153.7469 |          17.3227 |
[32m[20221214 14:06:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:06:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 721.65
[32m[20221214 14:06:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.27
[32m[20221214 14:06:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 786.83
[32m[20221214 14:06:52 @agent_ppo2.py:143][0m Total time:       8.83 min
[32m[20221214 14:06:52 @agent_ppo2.py:145][0m 798720 total steps have happened
[32m[20221214 14:06:52 @agent_ppo2.py:121][0m #------------------------ Iteration 390 --------------------------#
[32m[20221214 14:06:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:06:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:52 @agent_ppo2.py:185][0m |          -0.0033 |         168.0574 |          17.2333 |
[32m[20221214 14:06:52 @agent_ppo2.py:185][0m |          -0.0019 |         142.8251 |          17.2346 |
[32m[20221214 14:06:52 @agent_ppo2.py:185][0m |           0.0005 |         136.7913 |          17.2174 |
[32m[20221214 14:06:53 @agent_ppo2.py:185][0m |          -0.0006 |         132.8508 |          17.2084 |
[32m[20221214 14:06:53 @agent_ppo2.py:185][0m |          -0.0021 |         130.6963 |          17.2038 |
[32m[20221214 14:06:53 @agent_ppo2.py:185][0m |          -0.0051 |         128.2999 |          17.1839 |
[32m[20221214 14:06:53 @agent_ppo2.py:185][0m |          -0.0005 |         126.8037 |          17.1886 |
[32m[20221214 14:06:53 @agent_ppo2.py:185][0m |          -0.0021 |         124.9105 |          17.1731 |
[32m[20221214 14:06:53 @agent_ppo2.py:185][0m |          -0.0034 |         123.2473 |          17.1605 |
[32m[20221214 14:06:53 @agent_ppo2.py:185][0m |          -0.0038 |         121.9689 |          17.1538 |
[32m[20221214 14:06:53 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:06:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 661.34
[32m[20221214 14:06:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 744.84
[32m[20221214 14:06:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 783.58
[32m[20221214 14:06:53 @agent_ppo2.py:143][0m Total time:       8.85 min
[32m[20221214 14:06:53 @agent_ppo2.py:145][0m 800768 total steps have happened
[32m[20221214 14:06:53 @agent_ppo2.py:121][0m #------------------------ Iteration 391 --------------------------#
[32m[20221214 14:06:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:54 @agent_ppo2.py:185][0m |          -0.0031 |         151.9900 |          17.3024 |
[32m[20221214 14:06:54 @agent_ppo2.py:185][0m |          -0.0011 |         144.2343 |          17.3026 |
[32m[20221214 14:06:54 @agent_ppo2.py:185][0m |          -0.0040 |         141.9597 |          17.3121 |
[32m[20221214 14:06:54 @agent_ppo2.py:185][0m |           0.0026 |         141.1308 |          17.3088 |
[32m[20221214 14:06:54 @agent_ppo2.py:185][0m |          -0.0020 |         139.5326 |          17.2983 |
[32m[20221214 14:06:54 @agent_ppo2.py:185][0m |          -0.0018 |         138.8838 |          17.2946 |
[32m[20221214 14:06:54 @agent_ppo2.py:185][0m |           0.0107 |         146.2316 |          17.3013 |
[32m[20221214 14:06:54 @agent_ppo2.py:185][0m |          -0.0006 |         140.0922 |          17.2987 |
[32m[20221214 14:06:54 @agent_ppo2.py:185][0m |          -0.0033 |         136.8168 |          17.2850 |
[32m[20221214 14:06:54 @agent_ppo2.py:185][0m |          -0.0006 |         136.9848 |          17.2897 |
[32m[20221214 14:06:54 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:06:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 750.68
[32m[20221214 14:06:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 772.40
[32m[20221214 14:06:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 707.66
[32m[20221214 14:06:54 @agent_ppo2.py:143][0m Total time:       8.87 min
[32m[20221214 14:06:54 @agent_ppo2.py:145][0m 802816 total steps have happened
[32m[20221214 14:06:54 @agent_ppo2.py:121][0m #------------------------ Iteration 392 --------------------------#
[32m[20221214 14:06:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:06:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:55 @agent_ppo2.py:185][0m |          -0.0007 |         129.4620 |          17.2672 |
[32m[20221214 14:06:55 @agent_ppo2.py:185][0m |          -0.0025 |         113.3477 |          17.2763 |
[32m[20221214 14:06:55 @agent_ppo2.py:185][0m |          -0.0033 |         108.2143 |          17.2874 |
[32m[20221214 14:06:55 @agent_ppo2.py:185][0m |          -0.0013 |         105.1215 |          17.2932 |
[32m[20221214 14:06:55 @agent_ppo2.py:185][0m |          -0.0021 |         102.6939 |          17.2934 |
[32m[20221214 14:06:55 @agent_ppo2.py:185][0m |          -0.0031 |         100.3991 |          17.3065 |
[32m[20221214 14:06:55 @agent_ppo2.py:185][0m |          -0.0046 |          98.3650 |          17.3249 |
[32m[20221214 14:06:55 @agent_ppo2.py:185][0m |          -0.0023 |          97.0551 |          17.3296 |
[32m[20221214 14:06:55 @agent_ppo2.py:185][0m |          -0.0012 |          96.1982 |          17.3369 |
[32m[20221214 14:06:56 @agent_ppo2.py:185][0m |          -0.0049 |          95.6518 |          17.3411 |
[32m[20221214 14:06:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:06:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 710.42
[32m[20221214 14:06:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.34
[32m[20221214 14:06:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.88
[32m[20221214 14:06:56 @agent_ppo2.py:143][0m Total time:       8.89 min
[32m[20221214 14:06:56 @agent_ppo2.py:145][0m 804864 total steps have happened
[32m[20221214 14:06:56 @agent_ppo2.py:121][0m #------------------------ Iteration 393 --------------------------#
[32m[20221214 14:06:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:56 @agent_ppo2.py:185][0m |          -0.0023 |         151.3032 |          17.2212 |
[32m[20221214 14:06:56 @agent_ppo2.py:185][0m |          -0.0031 |         136.0374 |          17.2411 |
[32m[20221214 14:06:56 @agent_ppo2.py:185][0m |          -0.0041 |         128.5090 |          17.2364 |
[32m[20221214 14:06:56 @agent_ppo2.py:185][0m |          -0.0037 |         124.0536 |          17.2315 |
[32m[20221214 14:06:56 @agent_ppo2.py:185][0m |          -0.0021 |         121.8971 |          17.2290 |
[32m[20221214 14:06:57 @agent_ppo2.py:185][0m |          -0.0022 |         119.2155 |          17.2341 |
[32m[20221214 14:06:57 @agent_ppo2.py:185][0m |          -0.0035 |         117.0092 |          17.2319 |
[32m[20221214 14:06:57 @agent_ppo2.py:185][0m |          -0.0059 |         115.0796 |          17.2247 |
[32m[20221214 14:06:57 @agent_ppo2.py:185][0m |          -0.0071 |         113.5035 |          17.2271 |
[32m[20221214 14:06:57 @agent_ppo2.py:185][0m |          -0.0046 |         111.0673 |          17.2272 |
[32m[20221214 14:06:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:06:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 648.85
[32m[20221214 14:06:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 743.14
[32m[20221214 14:06:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 578.89
[32m[20221214 14:06:57 @agent_ppo2.py:143][0m Total time:       8.92 min
[32m[20221214 14:06:57 @agent_ppo2.py:145][0m 806912 total steps have happened
[32m[20221214 14:06:57 @agent_ppo2.py:121][0m #------------------------ Iteration 394 --------------------------#
[32m[20221214 14:06:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:57 @agent_ppo2.py:185][0m |           0.0016 |         146.1632 |          17.3204 |
[32m[20221214 14:06:58 @agent_ppo2.py:185][0m |          -0.0014 |         124.3317 |          17.3089 |
[32m[20221214 14:06:58 @agent_ppo2.py:185][0m |          -0.0024 |         114.2492 |          17.3060 |
[32m[20221214 14:06:58 @agent_ppo2.py:185][0m |           0.0126 |         127.1385 |          17.3062 |
[32m[20221214 14:06:58 @agent_ppo2.py:185][0m |          -0.0014 |         105.4142 |          17.3107 |
[32m[20221214 14:06:58 @agent_ppo2.py:185][0m |          -0.0003 |         103.2046 |          17.3027 |
[32m[20221214 14:06:58 @agent_ppo2.py:185][0m |          -0.0015 |         101.6773 |          17.3090 |
[32m[20221214 14:06:58 @agent_ppo2.py:185][0m |          -0.0015 |         100.6084 |          17.3083 |
[32m[20221214 14:06:58 @agent_ppo2.py:185][0m |           0.0052 |         103.2567 |          17.3037 |
[32m[20221214 14:06:58 @agent_ppo2.py:185][0m |           0.0067 |         101.3674 |          17.3169 |
[32m[20221214 14:06:58 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:06:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.27
[32m[20221214 14:06:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.62
[32m[20221214 14:06:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 791.33
[32m[20221214 14:06:58 @agent_ppo2.py:143][0m Total time:       8.94 min
[32m[20221214 14:06:58 @agent_ppo2.py:145][0m 808960 total steps have happened
[32m[20221214 14:06:58 @agent_ppo2.py:121][0m #------------------------ Iteration 395 --------------------------#
[32m[20221214 14:06:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:06:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:06:59 @agent_ppo2.py:185][0m |          -0.0029 |         175.1285 |          17.2440 |
[32m[20221214 14:06:59 @agent_ppo2.py:185][0m |          -0.0007 |         156.3060 |          17.2338 |
[32m[20221214 14:06:59 @agent_ppo2.py:185][0m |          -0.0025 |         148.3666 |          17.2388 |
[32m[20221214 14:06:59 @agent_ppo2.py:185][0m |          -0.0020 |         143.4245 |          17.2289 |
[32m[20221214 14:06:59 @agent_ppo2.py:185][0m |          -0.0063 |         140.2846 |          17.2271 |
[32m[20221214 14:06:59 @agent_ppo2.py:185][0m |          -0.0013 |         138.7882 |          17.2232 |
[32m[20221214 14:06:59 @agent_ppo2.py:185][0m |           0.0015 |         136.6463 |          17.2246 |
[32m[20221214 14:06:59 @agent_ppo2.py:185][0m |           0.0022 |         136.7770 |          17.2298 |
[32m[20221214 14:07:00 @agent_ppo2.py:185][0m |          -0.0025 |         133.3334 |          17.2398 |
[32m[20221214 14:07:00 @agent_ppo2.py:185][0m |          -0.0037 |         131.8962 |          17.2540 |
[32m[20221214 14:07:00 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:07:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 677.93
[32m[20221214 14:07:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.46
[32m[20221214 14:07:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.95
[32m[20221214 14:07:00 @agent_ppo2.py:143][0m Total time:       8.96 min
[32m[20221214 14:07:00 @agent_ppo2.py:145][0m 811008 total steps have happened
[32m[20221214 14:07:00 @agent_ppo2.py:121][0m #------------------------ Iteration 396 --------------------------#
[32m[20221214 14:07:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:00 @agent_ppo2.py:185][0m |           0.0058 |         189.2378 |          17.1691 |
[32m[20221214 14:07:00 @agent_ppo2.py:185][0m |           0.0099 |         191.6041 |          17.1739 |
[32m[20221214 14:07:00 @agent_ppo2.py:185][0m |          -0.0018 |         167.6702 |          17.1795 |
[32m[20221214 14:07:00 @agent_ppo2.py:185][0m |          -0.0021 |         164.3856 |          17.1747 |
[32m[20221214 14:07:01 @agent_ppo2.py:185][0m |          -0.0043 |         162.0805 |          17.1694 |
[32m[20221214 14:07:01 @agent_ppo2.py:185][0m |          -0.0028 |         160.1569 |          17.1766 |
[32m[20221214 14:07:01 @agent_ppo2.py:185][0m |           0.0035 |         165.0750 |          17.1665 |
[32m[20221214 14:07:01 @agent_ppo2.py:185][0m |          -0.0036 |         158.2135 |          17.1678 |
[32m[20221214 14:07:01 @agent_ppo2.py:185][0m |           0.0102 |         180.7510 |          17.1784 |
[32m[20221214 14:07:01 @agent_ppo2.py:185][0m |          -0.0015 |         157.5318 |          17.1807 |
[32m[20221214 14:07:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:07:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.12
[32m[20221214 14:07:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.72
[32m[20221214 14:07:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.43
[32m[20221214 14:07:01 @agent_ppo2.py:143][0m Total time:       8.98 min
[32m[20221214 14:07:01 @agent_ppo2.py:145][0m 813056 total steps have happened
[32m[20221214 14:07:01 @agent_ppo2.py:121][0m #------------------------ Iteration 397 --------------------------#
[32m[20221214 14:07:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:01 @agent_ppo2.py:185][0m |          -0.0019 |         188.4745 |          17.2524 |
[32m[20221214 14:07:02 @agent_ppo2.py:185][0m |          -0.0035 |         178.1608 |          17.2435 |
[32m[20221214 14:07:02 @agent_ppo2.py:185][0m |          -0.0039 |         174.3977 |          17.2358 |
[32m[20221214 14:07:02 @agent_ppo2.py:185][0m |          -0.0019 |         173.5218 |          17.2420 |
[32m[20221214 14:07:02 @agent_ppo2.py:185][0m |           0.0005 |         172.4405 |          17.2225 |
[32m[20221214 14:07:02 @agent_ppo2.py:185][0m |          -0.0038 |         171.7349 |          17.2199 |
[32m[20221214 14:07:02 @agent_ppo2.py:185][0m |          -0.0029 |         170.9146 |          17.2222 |
[32m[20221214 14:07:02 @agent_ppo2.py:185][0m |          -0.0036 |         170.4524 |          17.2190 |
[32m[20221214 14:07:02 @agent_ppo2.py:185][0m |          -0.0053 |         170.0990 |          17.2011 |
[32m[20221214 14:07:02 @agent_ppo2.py:185][0m |          -0.0017 |         169.4188 |          17.2089 |
[32m[20221214 14:07:02 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:07:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.27
[32m[20221214 14:07:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.27
[32m[20221214 14:07:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.22
[32m[20221214 14:07:02 @agent_ppo2.py:143][0m Total time:       9.01 min
[32m[20221214 14:07:02 @agent_ppo2.py:145][0m 815104 total steps have happened
[32m[20221214 14:07:02 @agent_ppo2.py:121][0m #------------------------ Iteration 398 --------------------------#
[32m[20221214 14:07:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:03 @agent_ppo2.py:185][0m |          -0.0030 |         159.1892 |          17.3246 |
[32m[20221214 14:07:03 @agent_ppo2.py:185][0m |           0.0000 |         151.6192 |          17.3076 |
[32m[20221214 14:07:03 @agent_ppo2.py:185][0m |          -0.0039 |         146.7808 |          17.3145 |
[32m[20221214 14:07:03 @agent_ppo2.py:185][0m |          -0.0052 |         144.3659 |          17.3144 |
[32m[20221214 14:07:03 @agent_ppo2.py:185][0m |          -0.0061 |         142.2537 |          17.3114 |
[32m[20221214 14:07:03 @agent_ppo2.py:185][0m |          -0.0051 |         141.0599 |          17.3180 |
[32m[20221214 14:07:03 @agent_ppo2.py:185][0m |           0.0033 |         145.5460 |          17.3080 |
[32m[20221214 14:07:03 @agent_ppo2.py:185][0m |          -0.0035 |         138.2137 |          17.3149 |
[32m[20221214 14:07:03 @agent_ppo2.py:185][0m |          -0.0053 |         137.4903 |          17.3097 |
[32m[20221214 14:07:03 @agent_ppo2.py:185][0m |          -0.0061 |         136.6705 |          17.3041 |
[32m[20221214 14:07:03 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:07:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 706.97
[32m[20221214 14:07:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.61
[32m[20221214 14:07:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.62
[32m[20221214 14:07:04 @agent_ppo2.py:143][0m Total time:       9.03 min
[32m[20221214 14:07:04 @agent_ppo2.py:145][0m 817152 total steps have happened
[32m[20221214 14:07:04 @agent_ppo2.py:121][0m #------------------------ Iteration 399 --------------------------#
[32m[20221214 14:07:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:04 @agent_ppo2.py:185][0m |          -0.0047 |         176.8633 |          17.2470 |
[32m[20221214 14:07:04 @agent_ppo2.py:185][0m |          -0.0023 |         162.5761 |          17.2459 |
[32m[20221214 14:07:04 @agent_ppo2.py:185][0m |          -0.0039 |         157.3871 |          17.2320 |
[32m[20221214 14:07:04 @agent_ppo2.py:185][0m |          -0.0021 |         154.3662 |          17.2101 |
[32m[20221214 14:07:04 @agent_ppo2.py:185][0m |           0.0000 |         152.9390 |          17.2159 |
[32m[20221214 14:07:04 @agent_ppo2.py:185][0m |          -0.0036 |         151.1603 |          17.2127 |
[32m[20221214 14:07:04 @agent_ppo2.py:185][0m |          -0.0045 |         149.2751 |          17.1998 |
[32m[20221214 14:07:05 @agent_ppo2.py:185][0m |          -0.0013 |         148.2590 |          17.1899 |
[32m[20221214 14:07:05 @agent_ppo2.py:185][0m |           0.0080 |         158.1760 |          17.1825 |
[32m[20221214 14:07:05 @agent_ppo2.py:185][0m |          -0.0039 |         147.2219 |          17.1651 |
[32m[20221214 14:07:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:07:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.08
[32m[20221214 14:07:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.17
[32m[20221214 14:07:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.65
[32m[20221214 14:07:05 @agent_ppo2.py:143][0m Total time:       9.05 min
[32m[20221214 14:07:05 @agent_ppo2.py:145][0m 819200 total steps have happened
[32m[20221214 14:07:05 @agent_ppo2.py:121][0m #------------------------ Iteration 400 --------------------------#
[32m[20221214 14:07:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:07:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:05 @agent_ppo2.py:185][0m |          -0.0021 |         176.3777 |          17.0163 |
[32m[20221214 14:07:05 @agent_ppo2.py:185][0m |          -0.0036 |         169.1447 |          17.0132 |
[32m[20221214 14:07:05 @agent_ppo2.py:185][0m |           0.0110 |         188.5889 |          16.9970 |
[32m[20221214 14:07:06 @agent_ppo2.py:185][0m |          -0.0042 |         165.2760 |          16.9940 |
[32m[20221214 14:07:06 @agent_ppo2.py:185][0m |          -0.0024 |         163.2589 |          16.9975 |
[32m[20221214 14:07:06 @agent_ppo2.py:185][0m |          -0.0026 |         161.8533 |          16.9998 |
[32m[20221214 14:07:06 @agent_ppo2.py:185][0m |           0.0015 |         162.7578 |          16.9838 |
[32m[20221214 14:07:06 @agent_ppo2.py:185][0m |          -0.0030 |         160.1141 |          16.9739 |
[32m[20221214 14:07:06 @agent_ppo2.py:185][0m |          -0.0031 |         159.7719 |          16.9784 |
[32m[20221214 14:07:06 @agent_ppo2.py:185][0m |          -0.0036 |         158.7423 |          16.9719 |
[32m[20221214 14:07:06 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:07:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.67
[32m[20221214 14:07:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.14
[32m[20221214 14:07:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.61
[32m[20221214 14:07:06 @agent_ppo2.py:143][0m Total time:       9.07 min
[32m[20221214 14:07:06 @agent_ppo2.py:145][0m 821248 total steps have happened
[32m[20221214 14:07:06 @agent_ppo2.py:121][0m #------------------------ Iteration 401 --------------------------#
[32m[20221214 14:07:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:07:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:07 @agent_ppo2.py:185][0m |           0.0074 |         164.9888 |          16.9778 |
[32m[20221214 14:07:07 @agent_ppo2.py:185][0m |          -0.0036 |         141.0068 |          16.9871 |
[32m[20221214 14:07:07 @agent_ppo2.py:185][0m |          -0.0032 |         133.4332 |          16.9938 |
[32m[20221214 14:07:07 @agent_ppo2.py:185][0m |          -0.0046 |         129.1059 |          16.9935 |
[32m[20221214 14:07:07 @agent_ppo2.py:185][0m |          -0.0039 |         126.4451 |          17.0058 |
[32m[20221214 14:07:07 @agent_ppo2.py:185][0m |          -0.0033 |         123.8591 |          17.0101 |
[32m[20221214 14:07:07 @agent_ppo2.py:185][0m |          -0.0044 |         122.3606 |          17.0175 |
[32m[20221214 14:07:07 @agent_ppo2.py:185][0m |          -0.0057 |         121.5127 |          17.0166 |
[32m[20221214 14:07:07 @agent_ppo2.py:185][0m |          -0.0047 |         121.1623 |          17.0291 |
[32m[20221214 14:07:08 @agent_ppo2.py:185][0m |          -0.0044 |         119.6245 |          17.0350 |
[32m[20221214 14:07:08 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:07:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.88
[32m[20221214 14:07:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.69
[32m[20221214 14:07:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.69
[32m[20221214 14:07:08 @agent_ppo2.py:143][0m Total time:       9.09 min
[32m[20221214 14:07:08 @agent_ppo2.py:145][0m 823296 total steps have happened
[32m[20221214 14:07:08 @agent_ppo2.py:121][0m #------------------------ Iteration 402 --------------------------#
[32m[20221214 14:07:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:08 @agent_ppo2.py:185][0m |           0.0001 |         187.5846 |          17.0866 |
[32m[20221214 14:07:08 @agent_ppo2.py:185][0m |          -0.0023 |         175.0164 |          17.0749 |
[32m[20221214 14:07:08 @agent_ppo2.py:185][0m |          -0.0021 |         170.6474 |          17.0289 |
[32m[20221214 14:07:08 @agent_ppo2.py:185][0m |          -0.0020 |         167.6598 |          17.0075 |
[32m[20221214 14:07:08 @agent_ppo2.py:185][0m |          -0.0031 |         165.8111 |          17.0117 |
[32m[20221214 14:07:08 @agent_ppo2.py:185][0m |           0.0006 |         164.3921 |          16.9960 |
[32m[20221214 14:07:09 @agent_ppo2.py:185][0m |          -0.0040 |         162.9629 |          16.9681 |
[32m[20221214 14:07:09 @agent_ppo2.py:185][0m |          -0.0033 |         162.1829 |          16.9614 |
[32m[20221214 14:07:09 @agent_ppo2.py:185][0m |           0.0016 |         162.3492 |          16.9571 |
[32m[20221214 14:07:09 @agent_ppo2.py:185][0m |          -0.0035 |         160.0663 |          16.9312 |
[32m[20221214 14:07:09 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:07:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.73
[32m[20221214 14:07:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.83
[32m[20221214 14:07:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.80
[32m[20221214 14:07:09 @agent_ppo2.py:143][0m Total time:       9.12 min
[32m[20221214 14:07:09 @agent_ppo2.py:145][0m 825344 total steps have happened
[32m[20221214 14:07:09 @agent_ppo2.py:121][0m #------------------------ Iteration 403 --------------------------#
[32m[20221214 14:07:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:09 @agent_ppo2.py:185][0m |          -0.0015 |         164.8838 |          16.9588 |
[32m[20221214 14:07:09 @agent_ppo2.py:185][0m |           0.0015 |         162.5577 |          16.9391 |
[32m[20221214 14:07:10 @agent_ppo2.py:185][0m |          -0.0032 |         157.8296 |          16.9284 |
[32m[20221214 14:07:10 @agent_ppo2.py:185][0m |          -0.0006 |         156.7563 |          16.9048 |
[32m[20221214 14:07:10 @agent_ppo2.py:185][0m |          -0.0034 |         155.3563 |          16.8961 |
[32m[20221214 14:07:10 @agent_ppo2.py:185][0m |          -0.0028 |         154.3342 |          16.8812 |
[32m[20221214 14:07:10 @agent_ppo2.py:185][0m |          -0.0024 |         153.6101 |          16.8642 |
[32m[20221214 14:07:10 @agent_ppo2.py:185][0m |          -0.0025 |         152.9034 |          16.8521 |
[32m[20221214 14:07:10 @agent_ppo2.py:185][0m |          -0.0004 |         152.7529 |          16.8182 |
[32m[20221214 14:07:10 @agent_ppo2.py:185][0m |           0.0012 |         153.9416 |          16.8155 |
[32m[20221214 14:07:10 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:07:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.57
[32m[20221214 14:07:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.46
[32m[20221214 14:07:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.66
[32m[20221214 14:07:10 @agent_ppo2.py:143][0m Total time:       9.14 min
[32m[20221214 14:07:10 @agent_ppo2.py:145][0m 827392 total steps have happened
[32m[20221214 14:07:10 @agent_ppo2.py:121][0m #------------------------ Iteration 404 --------------------------#
[32m[20221214 14:07:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:11 @agent_ppo2.py:185][0m |          -0.0020 |         181.8782 |          16.8008 |
[32m[20221214 14:07:11 @agent_ppo2.py:185][0m |          -0.0018 |         174.8464 |          16.7980 |
[32m[20221214 14:07:11 @agent_ppo2.py:185][0m |          -0.0021 |         172.7418 |          16.7988 |
[32m[20221214 14:07:11 @agent_ppo2.py:185][0m |          -0.0033 |         171.4873 |          16.7993 |
[32m[20221214 14:07:11 @agent_ppo2.py:185][0m |           0.0002 |         171.0248 |          16.8021 |
[32m[20221214 14:07:11 @agent_ppo2.py:185][0m |           0.0053 |         176.6073 |          16.7977 |
[32m[20221214 14:07:11 @agent_ppo2.py:185][0m |          -0.0040 |         168.4059 |          16.7916 |
[32m[20221214 14:07:11 @agent_ppo2.py:185][0m |           0.0021 |         170.7680 |          16.7947 |
[32m[20221214 14:07:11 @agent_ppo2.py:185][0m |          -0.0032 |         165.6444 |          16.7902 |
[32m[20221214 14:07:12 @agent_ppo2.py:185][0m |           0.0126 |         176.2752 |          16.7922 |
[32m[20221214 14:07:12 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:07:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.94
[32m[20221214 14:07:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.40
[32m[20221214 14:07:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.96
[32m[20221214 14:07:12 @agent_ppo2.py:143][0m Total time:       9.16 min
[32m[20221214 14:07:12 @agent_ppo2.py:145][0m 829440 total steps have happened
[32m[20221214 14:07:12 @agent_ppo2.py:121][0m #------------------------ Iteration 405 --------------------------#
[32m[20221214 14:07:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:12 @agent_ppo2.py:185][0m |          -0.0015 |         175.4592 |          16.7339 |
[32m[20221214 14:07:12 @agent_ppo2.py:185][0m |           0.0005 |         164.8726 |          16.7197 |
[32m[20221214 14:07:12 @agent_ppo2.py:185][0m |          -0.0021 |         160.7876 |          16.7286 |
[32m[20221214 14:07:12 @agent_ppo2.py:185][0m |           0.0054 |         165.4378 |          16.7110 |
[32m[20221214 14:07:12 @agent_ppo2.py:185][0m |           0.0061 |         168.9215 |          16.7068 |
[32m[20221214 14:07:13 @agent_ppo2.py:185][0m |           0.0012 |         158.5729 |          16.6964 |
[32m[20221214 14:07:13 @agent_ppo2.py:185][0m |           0.0022 |         157.6459 |          16.6814 |
[32m[20221214 14:07:13 @agent_ppo2.py:185][0m |           0.0020 |         157.1322 |          16.6981 |
[32m[20221214 14:07:13 @agent_ppo2.py:185][0m |          -0.0022 |         155.6541 |          16.6815 |
[32m[20221214 14:07:13 @agent_ppo2.py:185][0m |           0.0036 |         156.7257 |          16.6842 |
[32m[20221214 14:07:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:07:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.03
[32m[20221214 14:07:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.64
[32m[20221214 14:07:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.67
[32m[20221214 14:07:13 @agent_ppo2.py:143][0m Total time:       9.18 min
[32m[20221214 14:07:13 @agent_ppo2.py:145][0m 831488 total steps have happened
[32m[20221214 14:07:13 @agent_ppo2.py:121][0m #------------------------ Iteration 406 --------------------------#
[32m[20221214 14:07:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:13 @agent_ppo2.py:185][0m |          -0.0027 |         188.8422 |          16.6535 |
[32m[20221214 14:07:14 @agent_ppo2.py:185][0m |          -0.0006 |         183.3263 |          16.6396 |
[32m[20221214 14:07:14 @agent_ppo2.py:185][0m |          -0.0016 |         181.0695 |          16.6534 |
[32m[20221214 14:07:14 @agent_ppo2.py:185][0m |           0.0105 |         192.4497 |          16.6376 |
[32m[20221214 14:07:14 @agent_ppo2.py:185][0m |          -0.0007 |         177.0095 |          16.6378 |
[32m[20221214 14:07:14 @agent_ppo2.py:185][0m |           0.0128 |         207.0970 |          16.6278 |
[32m[20221214 14:07:14 @agent_ppo2.py:185][0m |          -0.0025 |         174.9775 |          16.6293 |
[32m[20221214 14:07:14 @agent_ppo2.py:185][0m |          -0.0031 |         174.1921 |          16.6317 |
[32m[20221214 14:07:14 @agent_ppo2.py:185][0m |          -0.0020 |         173.5283 |          16.6334 |
[32m[20221214 14:07:14 @agent_ppo2.py:185][0m |          -0.0023 |         173.0259 |          16.6348 |
[32m[20221214 14:07:14 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:07:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.31
[32m[20221214 14:07:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.82
[32m[20221214 14:07:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.28
[32m[20221214 14:07:14 @agent_ppo2.py:143][0m Total time:       9.21 min
[32m[20221214 14:07:14 @agent_ppo2.py:145][0m 833536 total steps have happened
[32m[20221214 14:07:14 @agent_ppo2.py:121][0m #------------------------ Iteration 407 --------------------------#
[32m[20221214 14:07:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:15 @agent_ppo2.py:185][0m |           0.0035 |         180.2771 |          16.7685 |
[32m[20221214 14:07:15 @agent_ppo2.py:185][0m |          -0.0008 |         171.1971 |          16.7445 |
[32m[20221214 14:07:15 @agent_ppo2.py:185][0m |          -0.0020 |         168.3673 |          16.7483 |
[32m[20221214 14:07:15 @agent_ppo2.py:185][0m |          -0.0007 |         166.7080 |          16.7353 |
[32m[20221214 14:07:15 @agent_ppo2.py:185][0m |           0.0078 |         175.2497 |          16.7299 |
[32m[20221214 14:07:15 @agent_ppo2.py:185][0m |          -0.0037 |         163.9827 |          16.7224 |
[32m[20221214 14:07:15 @agent_ppo2.py:185][0m |          -0.0002 |         162.9953 |          16.7112 |
[32m[20221214 14:07:15 @agent_ppo2.py:185][0m |          -0.0017 |         162.1272 |          16.7048 |
[32m[20221214 14:07:15 @agent_ppo2.py:185][0m |          -0.0003 |         161.6468 |          16.6976 |
[32m[20221214 14:07:15 @agent_ppo2.py:185][0m |           0.0001 |         161.9638 |          16.6976 |
[32m[20221214 14:07:15 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.05
[32m[20221214 14:07:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.10
[32m[20221214 14:07:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.01
[32m[20221214 14:07:16 @agent_ppo2.py:143][0m Total time:       9.23 min
[32m[20221214 14:07:16 @agent_ppo2.py:145][0m 835584 total steps have happened
[32m[20221214 14:07:16 @agent_ppo2.py:121][0m #------------------------ Iteration 408 --------------------------#
[32m[20221214 14:07:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:16 @agent_ppo2.py:185][0m |          -0.0003 |         201.4498 |          16.7225 |
[32m[20221214 14:07:16 @agent_ppo2.py:185][0m |           0.0009 |         191.8002 |          16.6983 |
[32m[20221214 14:07:16 @agent_ppo2.py:185][0m |          -0.0037 |         185.9551 |          16.6953 |
[32m[20221214 14:07:16 @agent_ppo2.py:185][0m |          -0.0021 |         182.9356 |          16.6896 |
[32m[20221214 14:07:16 @agent_ppo2.py:185][0m |          -0.0019 |         180.8474 |          16.6781 |
[32m[20221214 14:07:16 @agent_ppo2.py:185][0m |           0.0055 |         194.1209 |          16.6736 |
[32m[20221214 14:07:16 @agent_ppo2.py:185][0m |          -0.0031 |         177.5325 |          16.6717 |
[32m[20221214 14:07:17 @agent_ppo2.py:185][0m |          -0.0011 |         177.3244 |          16.6652 |
[32m[20221214 14:07:17 @agent_ppo2.py:185][0m |           0.0050 |         178.3860 |          16.6701 |
[32m[20221214 14:07:17 @agent_ppo2.py:185][0m |          -0.0030 |         175.8149 |          16.6528 |
[32m[20221214 14:07:17 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.17
[32m[20221214 14:07:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.12
[32m[20221214 14:07:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.39
[32m[20221214 14:07:17 @agent_ppo2.py:143][0m Total time:       9.25 min
[32m[20221214 14:07:17 @agent_ppo2.py:145][0m 837632 total steps have happened
[32m[20221214 14:07:17 @agent_ppo2.py:121][0m #------------------------ Iteration 409 --------------------------#
[32m[20221214 14:07:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:17 @agent_ppo2.py:185][0m |          -0.0033 |         186.3518 |          16.4794 |
[32m[20221214 14:07:17 @agent_ppo2.py:185][0m |          -0.0015 |         173.1543 |          16.4788 |
[32m[20221214 14:07:17 @agent_ppo2.py:185][0m |           0.0037 |         170.5479 |          16.4817 |
[32m[20221214 14:07:17 @agent_ppo2.py:185][0m |          -0.0021 |         166.7644 |          16.4985 |
[32m[20221214 14:07:18 @agent_ppo2.py:185][0m |          -0.0014 |         163.5670 |          16.4947 |
[32m[20221214 14:07:18 @agent_ppo2.py:185][0m |           0.0125 |         181.7697 |          16.4922 |
[32m[20221214 14:07:18 @agent_ppo2.py:185][0m |          -0.0017 |         160.3835 |          16.4897 |
[32m[20221214 14:07:18 @agent_ppo2.py:185][0m |          -0.0018 |         159.2230 |          16.4998 |
[32m[20221214 14:07:18 @agent_ppo2.py:185][0m |          -0.0024 |         159.4370 |          16.5085 |
[32m[20221214 14:07:18 @agent_ppo2.py:185][0m |           0.0005 |         159.9263 |          16.5114 |
[32m[20221214 14:07:18 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:07:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.37
[32m[20221214 14:07:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.44
[32m[20221214 14:07:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.49
[32m[20221214 14:07:18 @agent_ppo2.py:143][0m Total time:       9.27 min
[32m[20221214 14:07:18 @agent_ppo2.py:145][0m 839680 total steps have happened
[32m[20221214 14:07:18 @agent_ppo2.py:121][0m #------------------------ Iteration 410 --------------------------#
[32m[20221214 14:07:18 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221214 14:07:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:19 @agent_ppo2.py:185][0m |          -0.0025 |         167.3178 |          16.6159 |
[32m[20221214 14:07:19 @agent_ppo2.py:185][0m |          -0.0038 |         162.6300 |          16.6013 |
[32m[20221214 14:07:19 @agent_ppo2.py:185][0m |          -0.0024 |         159.0799 |          16.6063 |
[32m[20221214 14:07:19 @agent_ppo2.py:185][0m |          -0.0033 |         156.9463 |          16.6023 |
[32m[20221214 14:07:19 @agent_ppo2.py:185][0m |          -0.0028 |         155.4710 |          16.5916 |
[32m[20221214 14:07:19 @agent_ppo2.py:185][0m |          -0.0030 |         153.5146 |          16.5879 |
[32m[20221214 14:07:19 @agent_ppo2.py:185][0m |           0.0043 |         157.5782 |          16.5932 |
[32m[20221214 14:07:19 @agent_ppo2.py:185][0m |           0.0035 |         154.8099 |          16.5861 |
[32m[20221214 14:07:19 @agent_ppo2.py:185][0m |          -0.0016 |         151.1566 |          16.5893 |
[32m[20221214 14:07:19 @agent_ppo2.py:185][0m |          -0.0012 |         150.3467 |          16.5850 |
[32m[20221214 14:07:19 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:07:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.83
[32m[20221214 14:07:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.04
[32m[20221214 14:07:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.74
[32m[20221214 14:07:20 @agent_ppo2.py:143][0m Total time:       9.29 min
[32m[20221214 14:07:20 @agent_ppo2.py:145][0m 841728 total steps have happened
[32m[20221214 14:07:20 @agent_ppo2.py:121][0m #------------------------ Iteration 411 --------------------------#
[32m[20221214 14:07:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:20 @agent_ppo2.py:185][0m |          -0.0047 |         184.5758 |          16.5828 |
[32m[20221214 14:07:20 @agent_ppo2.py:185][0m |          -0.0048 |         170.4962 |          16.5760 |
[32m[20221214 14:07:20 @agent_ppo2.py:185][0m |          -0.0058 |         162.9043 |          16.5792 |
[32m[20221214 14:07:20 @agent_ppo2.py:185][0m |          -0.0061 |         157.8065 |          16.5727 |
[32m[20221214 14:07:20 @agent_ppo2.py:185][0m |          -0.0041 |         154.0134 |          16.5642 |
[32m[20221214 14:07:20 @agent_ppo2.py:185][0m |          -0.0036 |         151.0098 |          16.5592 |
[32m[20221214 14:07:20 @agent_ppo2.py:185][0m |          -0.0021 |         148.5827 |          16.5568 |
[32m[20221214 14:07:21 @agent_ppo2.py:185][0m |          -0.0049 |         146.8802 |          16.5550 |
[32m[20221214 14:07:21 @agent_ppo2.py:185][0m |          -0.0048 |         145.0250 |          16.5472 |
[32m[20221214 14:07:21 @agent_ppo2.py:185][0m |          -0.0033 |         143.4565 |          16.5416 |
[32m[20221214 14:07:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:07:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.43
[32m[20221214 14:07:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.68
[32m[20221214 14:07:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.97
[32m[20221214 14:07:21 @agent_ppo2.py:143][0m Total time:       9.31 min
[32m[20221214 14:07:21 @agent_ppo2.py:145][0m 843776 total steps have happened
[32m[20221214 14:07:21 @agent_ppo2.py:121][0m #------------------------ Iteration 412 --------------------------#
[32m[20221214 14:07:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:21 @agent_ppo2.py:185][0m |          -0.0071 |         178.7133 |          16.4426 |
[32m[20221214 14:07:21 @agent_ppo2.py:185][0m |          -0.0034 |         160.3351 |          16.4185 |
[32m[20221214 14:07:21 @agent_ppo2.py:185][0m |          -0.0031 |         151.6478 |          16.3966 |
[32m[20221214 14:07:22 @agent_ppo2.py:185][0m |          -0.0029 |         146.2321 |          16.3830 |
[32m[20221214 14:07:22 @agent_ppo2.py:185][0m |          -0.0008 |         142.6333 |          16.3700 |
[32m[20221214 14:07:22 @agent_ppo2.py:185][0m |          -0.0009 |         139.5980 |          16.3520 |
[32m[20221214 14:07:22 @agent_ppo2.py:185][0m |          -0.0046 |         137.5556 |          16.3391 |
[32m[20221214 14:07:22 @agent_ppo2.py:185][0m |          -0.0046 |         136.0436 |          16.3226 |
[32m[20221214 14:07:22 @agent_ppo2.py:185][0m |          -0.0036 |         135.1473 |          16.3216 |
[32m[20221214 14:07:22 @agent_ppo2.py:185][0m |          -0.0015 |         134.0299 |          16.2905 |
[32m[20221214 14:07:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:07:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 698.46
[32m[20221214 14:07:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.47
[32m[20221214 14:07:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.84
[32m[20221214 14:07:22 @agent_ppo2.py:143][0m Total time:       9.34 min
[32m[20221214 14:07:22 @agent_ppo2.py:145][0m 845824 total steps have happened
[32m[20221214 14:07:22 @agent_ppo2.py:121][0m #------------------------ Iteration 413 --------------------------#
[32m[20221214 14:07:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:23 @agent_ppo2.py:185][0m |           0.0139 |         191.7040 |          16.3115 |
[32m[20221214 14:07:23 @agent_ppo2.py:185][0m |          -0.0039 |         152.3415 |          16.3472 |
[32m[20221214 14:07:23 @agent_ppo2.py:185][0m |          -0.0017 |         144.9540 |          16.3454 |
[32m[20221214 14:07:23 @agent_ppo2.py:185][0m |          -0.0040 |         140.0536 |          16.3543 |
[32m[20221214 14:07:23 @agent_ppo2.py:185][0m |           0.0094 |         157.8868 |          16.3678 |
[32m[20221214 14:07:23 @agent_ppo2.py:185][0m |          -0.0026 |         132.4597 |          16.3906 |
[32m[20221214 14:07:23 @agent_ppo2.py:185][0m |          -0.0073 |         129.9815 |          16.3959 |
[32m[20221214 14:07:23 @agent_ppo2.py:185][0m |          -0.0050 |         128.0252 |          16.4084 |
[32m[20221214 14:07:23 @agent_ppo2.py:185][0m |          -0.0039 |         126.1688 |          16.4183 |
[32m[20221214 14:07:23 @agent_ppo2.py:185][0m |          -0.0031 |         124.3974 |          16.4258 |
[32m[20221214 14:07:23 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:07:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 598.28
[32m[20221214 14:07:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 761.79
[32m[20221214 14:07:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 774.80
[32m[20221214 14:07:24 @agent_ppo2.py:143][0m Total time:       9.36 min
[32m[20221214 14:07:24 @agent_ppo2.py:145][0m 847872 total steps have happened
[32m[20221214 14:07:24 @agent_ppo2.py:121][0m #------------------------ Iteration 414 --------------------------#
[32m[20221214 14:07:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:24 @agent_ppo2.py:185][0m |           0.0004 |         142.0786 |          16.4431 |
[32m[20221214 14:07:24 @agent_ppo2.py:185][0m |          -0.0030 |         126.6152 |          16.4417 |
[32m[20221214 14:07:24 @agent_ppo2.py:185][0m |          -0.0002 |         121.9310 |          16.4272 |
[32m[20221214 14:07:24 @agent_ppo2.py:185][0m |          -0.0034 |         117.6067 |          16.4239 |
[32m[20221214 14:07:24 @agent_ppo2.py:185][0m |          -0.0042 |         116.6525 |          16.4108 |
[32m[20221214 14:07:24 @agent_ppo2.py:185][0m |           0.0008 |         116.4653 |          16.4044 |
[32m[20221214 14:07:24 @agent_ppo2.py:185][0m |          -0.0026 |         113.3445 |          16.3957 |
[32m[20221214 14:07:25 @agent_ppo2.py:185][0m |          -0.0041 |         112.8325 |          16.3897 |
[32m[20221214 14:07:25 @agent_ppo2.py:185][0m |          -0.0059 |         112.3665 |          16.3930 |
[32m[20221214 14:07:25 @agent_ppo2.py:185][0m |          -0.0022 |         112.1982 |          16.3712 |
[32m[20221214 14:07:25 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:07:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 724.95
[32m[20221214 14:07:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.16
[32m[20221214 14:07:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 721.29
[32m[20221214 14:07:25 @agent_ppo2.py:143][0m Total time:       9.38 min
[32m[20221214 14:07:25 @agent_ppo2.py:145][0m 849920 total steps have happened
[32m[20221214 14:07:25 @agent_ppo2.py:121][0m #------------------------ Iteration 415 --------------------------#
[32m[20221214 14:07:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:25 @agent_ppo2.py:185][0m |          -0.0010 |         147.5772 |          16.4127 |
[32m[20221214 14:07:25 @agent_ppo2.py:185][0m |          -0.0035 |         130.4104 |          16.4227 |
[32m[20221214 14:07:25 @agent_ppo2.py:185][0m |          -0.0071 |         123.2199 |          16.4151 |
[32m[20221214 14:07:26 @agent_ppo2.py:185][0m |           0.0077 |         129.4201 |          16.4164 |
[32m[20221214 14:07:26 @agent_ppo2.py:185][0m |          -0.0028 |         114.7462 |          16.4175 |
[32m[20221214 14:07:26 @agent_ppo2.py:185][0m |          -0.0025 |         111.8131 |          16.4160 |
[32m[20221214 14:07:26 @agent_ppo2.py:185][0m |          -0.0024 |         109.4900 |          16.4188 |
[32m[20221214 14:07:26 @agent_ppo2.py:185][0m |          -0.0026 |         107.6149 |          16.4142 |
[32m[20221214 14:07:26 @agent_ppo2.py:185][0m |          -0.0019 |         105.8280 |          16.4391 |
[32m[20221214 14:07:26 @agent_ppo2.py:185][0m |          -0.0057 |         104.4604 |          16.4243 |
[32m[20221214 14:07:26 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:07:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 663.77
[32m[20221214 14:07:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.73
[32m[20221214 14:07:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.25
[32m[20221214 14:07:26 @agent_ppo2.py:143][0m Total time:       9.40 min
[32m[20221214 14:07:26 @agent_ppo2.py:145][0m 851968 total steps have happened
[32m[20221214 14:07:26 @agent_ppo2.py:121][0m #------------------------ Iteration 416 --------------------------#
[32m[20221214 14:07:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:07:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:27 @agent_ppo2.py:185][0m |          -0.0007 |         152.6985 |          16.3470 |
[32m[20221214 14:07:27 @agent_ppo2.py:185][0m |          -0.0038 |         144.8159 |          16.3389 |
[32m[20221214 14:07:27 @agent_ppo2.py:185][0m |          -0.0042 |         142.5025 |          16.3270 |
[32m[20221214 14:07:27 @agent_ppo2.py:185][0m |           0.0105 |         159.2756 |          16.3233 |
[32m[20221214 14:07:27 @agent_ppo2.py:185][0m |          -0.0033 |         138.7704 |          16.3142 |
[32m[20221214 14:07:27 @agent_ppo2.py:185][0m |          -0.0028 |         137.3551 |          16.3147 |
[32m[20221214 14:07:27 @agent_ppo2.py:185][0m |          -0.0041 |         136.6312 |          16.3020 |
[32m[20221214 14:07:27 @agent_ppo2.py:185][0m |          -0.0021 |         135.6650 |          16.2875 |
[32m[20221214 14:07:27 @agent_ppo2.py:185][0m |          -0.0039 |         135.6277 |          16.2881 |
[32m[20221214 14:07:27 @agent_ppo2.py:185][0m |          -0.0049 |         135.1959 |          16.2861 |
[32m[20221214 14:07:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:07:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 765.83
[32m[20221214 14:07:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.52
[32m[20221214 14:07:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 746.45
[32m[20221214 14:07:28 @agent_ppo2.py:143][0m Total time:       9.43 min
[32m[20221214 14:07:28 @agent_ppo2.py:145][0m 854016 total steps have happened
[32m[20221214 14:07:28 @agent_ppo2.py:121][0m #------------------------ Iteration 417 --------------------------#
[32m[20221214 14:07:28 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:07:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:28 @agent_ppo2.py:185][0m |           0.0008 |         138.7407 |          16.3308 |
[32m[20221214 14:07:28 @agent_ppo2.py:185][0m |           0.0029 |         126.5931 |          16.3348 |
[32m[20221214 14:07:28 @agent_ppo2.py:185][0m |          -0.0028 |         119.1271 |          16.3383 |
[32m[20221214 14:07:28 @agent_ppo2.py:185][0m |          -0.0012 |         116.1843 |          16.3401 |
[32m[20221214 14:07:28 @agent_ppo2.py:185][0m |          -0.0050 |         113.1099 |          16.3429 |
[32m[20221214 14:07:29 @agent_ppo2.py:185][0m |           0.0059 |         116.5759 |          16.3339 |
[32m[20221214 14:07:29 @agent_ppo2.py:185][0m |          -0.0031 |         109.8519 |          16.3376 |
[32m[20221214 14:07:29 @agent_ppo2.py:185][0m |          -0.0028 |         108.3706 |          16.3375 |
[32m[20221214 14:07:29 @agent_ppo2.py:185][0m |          -0.0016 |         107.4410 |          16.3297 |
[32m[20221214 14:07:29 @agent_ppo2.py:185][0m |           0.0007 |         106.3177 |          16.3374 |
[32m[20221214 14:07:29 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 14:07:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 673.05
[32m[20221214 14:07:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 753.84
[32m[20221214 14:07:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.99
[32m[20221214 14:07:29 @agent_ppo2.py:143][0m Total time:       9.45 min
[32m[20221214 14:07:29 @agent_ppo2.py:145][0m 856064 total steps have happened
[32m[20221214 14:07:29 @agent_ppo2.py:121][0m #------------------------ Iteration 418 --------------------------#
[32m[20221214 14:07:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:29 @agent_ppo2.py:185][0m |          -0.0017 |         149.5167 |          16.1867 |
[32m[20221214 14:07:30 @agent_ppo2.py:185][0m |          -0.0009 |         143.4523 |          16.1584 |
[32m[20221214 14:07:30 @agent_ppo2.py:185][0m |          -0.0024 |         139.1623 |          16.1443 |
[32m[20221214 14:07:30 @agent_ppo2.py:185][0m |          -0.0030 |         135.9502 |          16.1178 |
[32m[20221214 14:07:30 @agent_ppo2.py:185][0m |          -0.0044 |         134.1142 |          16.0998 |
[32m[20221214 14:07:30 @agent_ppo2.py:185][0m |          -0.0028 |         132.2229 |          16.0878 |
[32m[20221214 14:07:30 @agent_ppo2.py:185][0m |          -0.0020 |         131.3975 |          16.0648 |
[32m[20221214 14:07:30 @agent_ppo2.py:185][0m |          -0.0044 |         130.7973 |          16.0523 |
[32m[20221214 14:07:30 @agent_ppo2.py:185][0m |           0.0048 |         133.0036 |          16.0357 |
[32m[20221214 14:07:30 @agent_ppo2.py:185][0m |           0.0095 |         143.2143 |          16.0170 |
[32m[20221214 14:07:30 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:07:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 716.67
[32m[20221214 14:07:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.43
[32m[20221214 14:07:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.94
[32m[20221214 14:07:30 @agent_ppo2.py:143][0m Total time:       9.47 min
[32m[20221214 14:07:30 @agent_ppo2.py:145][0m 858112 total steps have happened
[32m[20221214 14:07:30 @agent_ppo2.py:121][0m #------------------------ Iteration 419 --------------------------#
[32m[20221214 14:07:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:31 @agent_ppo2.py:185][0m |          -0.0009 |         151.7432 |          15.9606 |
[32m[20221214 14:07:31 @agent_ppo2.py:185][0m |          -0.0035 |         136.3821 |          15.9513 |
[32m[20221214 14:07:31 @agent_ppo2.py:185][0m |           0.0070 |         146.7673 |          15.9406 |
[32m[20221214 14:07:31 @agent_ppo2.py:185][0m |          -0.0048 |         126.4612 |          15.9319 |
[32m[20221214 14:07:31 @agent_ppo2.py:185][0m |          -0.0019 |         125.4379 |          15.9086 |
[32m[20221214 14:07:31 @agent_ppo2.py:185][0m |           0.0019 |         125.1124 |          15.8914 |
[32m[20221214 14:07:31 @agent_ppo2.py:185][0m |          -0.0054 |         119.6684 |          15.8775 |
[32m[20221214 14:07:31 @agent_ppo2.py:185][0m |          -0.0026 |         117.9554 |          15.8634 |
[32m[20221214 14:07:32 @agent_ppo2.py:185][0m |          -0.0070 |         116.1615 |          15.8580 |
[32m[20221214 14:07:32 @agent_ppo2.py:185][0m |          -0.0051 |         114.1571 |          15.8273 |
[32m[20221214 14:07:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:07:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 686.76
[32m[20221214 14:07:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.04
[32m[20221214 14:07:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 750.63
[32m[20221214 14:07:32 @agent_ppo2.py:143][0m Total time:       9.50 min
[32m[20221214 14:07:32 @agent_ppo2.py:145][0m 860160 total steps have happened
[32m[20221214 14:07:32 @agent_ppo2.py:121][0m #------------------------ Iteration 420 --------------------------#
[32m[20221214 14:07:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:07:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:32 @agent_ppo2.py:185][0m |          -0.0010 |         147.6211 |          15.8890 |
[32m[20221214 14:07:32 @agent_ppo2.py:185][0m |          -0.0019 |         137.7635 |          15.8860 |
[32m[20221214 14:07:32 @agent_ppo2.py:185][0m |          -0.0010 |         133.7371 |          15.8835 |
[32m[20221214 14:07:32 @agent_ppo2.py:185][0m |          -0.0032 |         131.2210 |          15.8867 |
[32m[20221214 14:07:33 @agent_ppo2.py:185][0m |          -0.0016 |         129.5811 |          15.8749 |
[32m[20221214 14:07:33 @agent_ppo2.py:185][0m |          -0.0025 |         127.7711 |          15.8642 |
[32m[20221214 14:07:33 @agent_ppo2.py:185][0m |           0.0005 |         126.2726 |          15.8712 |
[32m[20221214 14:07:33 @agent_ppo2.py:185][0m |          -0.0031 |         125.0283 |          15.8540 |
[32m[20221214 14:07:33 @agent_ppo2.py:185][0m |           0.0043 |         124.7348 |          15.8510 |
[32m[20221214 14:07:33 @agent_ppo2.py:185][0m |           0.0011 |         122.9189 |          15.8470 |
[32m[20221214 14:07:33 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:07:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 718.26
[32m[20221214 14:07:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.60
[32m[20221214 14:07:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.98
[32m[20221214 14:07:33 @agent_ppo2.py:143][0m Total time:       9.52 min
[32m[20221214 14:07:33 @agent_ppo2.py:145][0m 862208 total steps have happened
[32m[20221214 14:07:33 @agent_ppo2.py:121][0m #------------------------ Iteration 421 --------------------------#
[32m[20221214 14:07:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:34 @agent_ppo2.py:185][0m |          -0.0008 |         127.4060 |          15.8561 |
[32m[20221214 14:07:34 @agent_ppo2.py:185][0m |          -0.0019 |         118.9125 |          15.8629 |
[32m[20221214 14:07:34 @agent_ppo2.py:185][0m |           0.0017 |         116.5824 |          15.8671 |
[32m[20221214 14:07:34 @agent_ppo2.py:185][0m |          -0.0012 |         115.3002 |          15.8791 |
[32m[20221214 14:07:34 @agent_ppo2.py:185][0m |          -0.0020 |         114.4292 |          15.8863 |
[32m[20221214 14:07:34 @agent_ppo2.py:185][0m |          -0.0045 |         113.6382 |          15.8940 |
[32m[20221214 14:07:34 @agent_ppo2.py:185][0m |           0.0083 |         126.9026 |          15.8930 |
[32m[20221214 14:07:34 @agent_ppo2.py:185][0m |          -0.0009 |         112.5272 |          15.9109 |
[32m[20221214 14:07:34 @agent_ppo2.py:185][0m |           0.0007 |         112.7879 |          15.9088 |
[32m[20221214 14:07:34 @agent_ppo2.py:185][0m |           0.0021 |         112.0207 |          15.9005 |
[32m[20221214 14:07:34 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:07:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 755.55
[32m[20221214 14:07:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.87
[32m[20221214 14:07:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.35
[32m[20221214 14:07:35 @agent_ppo2.py:143][0m Total time:       9.54 min
[32m[20221214 14:07:35 @agent_ppo2.py:145][0m 864256 total steps have happened
[32m[20221214 14:07:35 @agent_ppo2.py:121][0m #------------------------ Iteration 422 --------------------------#
[32m[20221214 14:07:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:35 @agent_ppo2.py:185][0m |          -0.0027 |         146.0403 |          15.8558 |
[32m[20221214 14:07:35 @agent_ppo2.py:185][0m |           0.0061 |         138.6545 |          15.8301 |
[32m[20221214 14:07:35 @agent_ppo2.py:185][0m |          -0.0048 |         129.7418 |          15.8191 |
[32m[20221214 14:07:35 @agent_ppo2.py:185][0m |           0.0028 |         130.6009 |          15.8021 |
[32m[20221214 14:07:35 @agent_ppo2.py:185][0m |          -0.0025 |         125.1498 |          15.7828 |
[32m[20221214 14:07:35 @agent_ppo2.py:185][0m |          -0.0052 |         124.0192 |          15.7684 |
[32m[20221214 14:07:35 @agent_ppo2.py:185][0m |           0.0168 |         147.2608 |          15.7509 |
[32m[20221214 14:07:36 @agent_ppo2.py:185][0m |           0.0090 |         138.0951 |          15.7412 |
[32m[20221214 14:07:36 @agent_ppo2.py:185][0m |          -0.0046 |         121.7726 |          15.7300 |
[32m[20221214 14:07:36 @agent_ppo2.py:185][0m |          -0.0029 |         121.7765 |          15.7085 |
[32m[20221214 14:07:36 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:07:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.59
[32m[20221214 14:07:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.26
[32m[20221214 14:07:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.71
[32m[20221214 14:07:36 @agent_ppo2.py:143][0m Total time:       9.56 min
[32m[20221214 14:07:36 @agent_ppo2.py:145][0m 866304 total steps have happened
[32m[20221214 14:07:36 @agent_ppo2.py:121][0m #------------------------ Iteration 423 --------------------------#
[32m[20221214 14:07:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:36 @agent_ppo2.py:185][0m |          -0.0001 |         127.7354 |          15.6576 |
[32m[20221214 14:07:36 @agent_ppo2.py:185][0m |          -0.0003 |         118.1951 |          15.6746 |
[32m[20221214 14:07:36 @agent_ppo2.py:185][0m |          -0.0011 |         116.1417 |          15.6731 |
[32m[20221214 14:07:37 @agent_ppo2.py:185][0m |           0.0020 |         114.5298 |          15.6589 |
[32m[20221214 14:07:37 @agent_ppo2.py:185][0m |           0.0009 |         113.9574 |          15.6889 |
[32m[20221214 14:07:37 @agent_ppo2.py:185][0m |          -0.0012 |         112.4363 |          15.6970 |
[32m[20221214 14:07:37 @agent_ppo2.py:185][0m |          -0.0023 |         111.4160 |          15.6959 |
[32m[20221214 14:07:37 @agent_ppo2.py:185][0m |           0.0003 |         111.0223 |          15.7064 |
[32m[20221214 14:07:37 @agent_ppo2.py:185][0m |          -0.0017 |         110.5304 |          15.7138 |
[32m[20221214 14:07:37 @agent_ppo2.py:185][0m |           0.0046 |         112.5861 |          15.7154 |
[32m[20221214 14:07:37 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:07:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.99
[32m[20221214 14:07:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.08
[32m[20221214 14:07:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.62
[32m[20221214 14:07:37 @agent_ppo2.py:143][0m Total time:       9.59 min
[32m[20221214 14:07:37 @agent_ppo2.py:145][0m 868352 total steps have happened
[32m[20221214 14:07:37 @agent_ppo2.py:121][0m #------------------------ Iteration 424 --------------------------#
[32m[20221214 14:07:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:38 @agent_ppo2.py:185][0m |          -0.0031 |         158.2929 |          15.6546 |
[32m[20221214 14:07:38 @agent_ppo2.py:185][0m |          -0.0039 |         147.5131 |          15.7102 |
[32m[20221214 14:07:38 @agent_ppo2.py:185][0m |           0.0049 |         149.8738 |          15.7033 |
[32m[20221214 14:07:38 @agent_ppo2.py:185][0m |          -0.0020 |         142.4527 |          15.7011 |
[32m[20221214 14:07:38 @agent_ppo2.py:185][0m |          -0.0048 |         140.6451 |          15.7027 |
[32m[20221214 14:07:38 @agent_ppo2.py:185][0m |           0.0103 |         156.4101 |          15.6963 |
[32m[20221214 14:07:38 @agent_ppo2.py:185][0m |          -0.0039 |         139.5499 |          15.7069 |
[32m[20221214 14:07:38 @agent_ppo2.py:185][0m |           0.0022 |         139.4927 |          15.6938 |
[32m[20221214 14:07:38 @agent_ppo2.py:185][0m |          -0.0010 |         138.6184 |          15.6857 |
[32m[20221214 14:07:38 @agent_ppo2.py:185][0m |           0.0100 |         157.6144 |          15.6919 |
[32m[20221214 14:07:38 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.77
[32m[20221214 14:07:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.38
[32m[20221214 14:07:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.74
[32m[20221214 14:07:39 @agent_ppo2.py:143][0m Total time:       9.61 min
[32m[20221214 14:07:39 @agent_ppo2.py:145][0m 870400 total steps have happened
[32m[20221214 14:07:39 @agent_ppo2.py:121][0m #------------------------ Iteration 425 --------------------------#
[32m[20221214 14:07:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:39 @agent_ppo2.py:185][0m |           0.0073 |         168.8220 |          15.7742 |
[32m[20221214 14:07:39 @agent_ppo2.py:185][0m |           0.0044 |         161.5492 |          15.7782 |
[32m[20221214 14:07:39 @agent_ppo2.py:185][0m |           0.0032 |         158.3482 |          15.7945 |
[32m[20221214 14:07:39 @agent_ppo2.py:185][0m |           0.0009 |         157.3291 |          15.8042 |
[32m[20221214 14:07:39 @agent_ppo2.py:185][0m |          -0.0012 |         156.5172 |          15.8089 |
[32m[20221214 14:07:39 @agent_ppo2.py:185][0m |          -0.0009 |         156.0438 |          15.8108 |
[32m[20221214 14:07:39 @agent_ppo2.py:185][0m |          -0.0015 |         155.7826 |          15.8045 |
[32m[20221214 14:07:39 @agent_ppo2.py:185][0m |           0.0043 |         156.9503 |          15.8169 |
[32m[20221214 14:07:39 @agent_ppo2.py:185][0m |          -0.0032 |         155.2506 |          15.8163 |
[32m[20221214 14:07:40 @agent_ppo2.py:185][0m |          -0.0028 |         155.1743 |          15.8387 |
[32m[20221214 14:07:40 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.20
[32m[20221214 14:07:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.43
[32m[20221214 14:07:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.92
[32m[20221214 14:07:40 @agent_ppo2.py:143][0m Total time:       9.63 min
[32m[20221214 14:07:40 @agent_ppo2.py:145][0m 872448 total steps have happened
[32m[20221214 14:07:40 @agent_ppo2.py:121][0m #------------------------ Iteration 426 --------------------------#
[32m[20221214 14:07:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:40 @agent_ppo2.py:185][0m |           0.0042 |         148.5878 |          15.7698 |
[32m[20221214 14:07:40 @agent_ppo2.py:185][0m |          -0.0004 |         142.0306 |          15.7660 |
[32m[20221214 14:07:40 @agent_ppo2.py:185][0m |           0.0128 |         154.6954 |          15.7522 |
[32m[20221214 14:07:40 @agent_ppo2.py:185][0m |          -0.0033 |         138.4905 |          15.7328 |
[32m[20221214 14:07:40 @agent_ppo2.py:185][0m |          -0.0035 |         137.7619 |          15.7234 |
[32m[20221214 14:07:40 @agent_ppo2.py:185][0m |          -0.0009 |         137.0176 |          15.7100 |
[32m[20221214 14:07:41 @agent_ppo2.py:185][0m |           0.0095 |         148.6649 |          15.7068 |
[32m[20221214 14:07:41 @agent_ppo2.py:185][0m |          -0.0033 |         136.1441 |          15.7079 |
[32m[20221214 14:07:41 @agent_ppo2.py:185][0m |          -0.0034 |         135.4071 |          15.7141 |
[32m[20221214 14:07:41 @agent_ppo2.py:185][0m |          -0.0030 |         135.0921 |          15.6998 |
[32m[20221214 14:07:41 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.09
[32m[20221214 14:07:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.99
[32m[20221214 14:07:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.93
[32m[20221214 14:07:41 @agent_ppo2.py:143][0m Total time:       9.65 min
[32m[20221214 14:07:41 @agent_ppo2.py:145][0m 874496 total steps have happened
[32m[20221214 14:07:41 @agent_ppo2.py:121][0m #------------------------ Iteration 427 --------------------------#
[32m[20221214 14:07:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:41 @agent_ppo2.py:185][0m |          -0.0037 |         166.6024 |          15.7278 |
[32m[20221214 14:07:41 @agent_ppo2.py:185][0m |           0.0019 |         159.5350 |          15.7339 |
[32m[20221214 14:07:41 @agent_ppo2.py:185][0m |           0.0017 |         157.8808 |          15.7298 |
[32m[20221214 14:07:42 @agent_ppo2.py:185][0m |          -0.0027 |         154.8473 |          15.7265 |
[32m[20221214 14:07:42 @agent_ppo2.py:185][0m |          -0.0047 |         154.2113 |          15.7248 |
[32m[20221214 14:07:42 @agent_ppo2.py:185][0m |           0.0025 |         155.9553 |          15.7249 |
[32m[20221214 14:07:42 @agent_ppo2.py:185][0m |          -0.0026 |         153.6953 |          15.7262 |
[32m[20221214 14:07:42 @agent_ppo2.py:185][0m |          -0.0015 |         153.0497 |          15.7306 |
[32m[20221214 14:07:42 @agent_ppo2.py:185][0m |          -0.0025 |         152.9961 |          15.7261 |
[32m[20221214 14:07:42 @agent_ppo2.py:185][0m |          -0.0031 |         151.9780 |          15.7134 |
[32m[20221214 14:07:42 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:07:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.26
[32m[20221214 14:07:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.94
[32m[20221214 14:07:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.45
[32m[20221214 14:07:42 @agent_ppo2.py:143][0m Total time:       9.67 min
[32m[20221214 14:07:42 @agent_ppo2.py:145][0m 876544 total steps have happened
[32m[20221214 14:07:42 @agent_ppo2.py:121][0m #------------------------ Iteration 428 --------------------------#
[32m[20221214 14:07:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:43 @agent_ppo2.py:185][0m |          -0.0023 |         166.3976 |          15.6854 |
[32m[20221214 14:07:43 @agent_ppo2.py:185][0m |          -0.0041 |         154.5287 |          15.6875 |
[32m[20221214 14:07:43 @agent_ppo2.py:185][0m |          -0.0037 |         150.9271 |          15.6906 |
[32m[20221214 14:07:43 @agent_ppo2.py:185][0m |          -0.0062 |         147.7372 |          15.6883 |
[32m[20221214 14:07:43 @agent_ppo2.py:185][0m |          -0.0031 |         145.5561 |          15.6770 |
[32m[20221214 14:07:43 @agent_ppo2.py:185][0m |          -0.0041 |         143.9020 |          15.6986 |
[32m[20221214 14:07:43 @agent_ppo2.py:185][0m |          -0.0047 |         142.0871 |          15.6832 |
[32m[20221214 14:07:43 @agent_ppo2.py:185][0m |          -0.0043 |         141.0913 |          15.6860 |
[32m[20221214 14:07:43 @agent_ppo2.py:185][0m |          -0.0052 |         139.9010 |          15.6866 |
[32m[20221214 14:07:43 @agent_ppo2.py:185][0m |          -0.0063 |         139.0674 |          15.6943 |
[32m[20221214 14:07:43 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 754.24
[32m[20221214 14:07:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.15
[32m[20221214 14:07:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.44
[32m[20221214 14:07:43 @agent_ppo2.py:143][0m Total time:       9.69 min
[32m[20221214 14:07:43 @agent_ppo2.py:145][0m 878592 total steps have happened
[32m[20221214 14:07:43 @agent_ppo2.py:121][0m #------------------------ Iteration 429 --------------------------#
[32m[20221214 14:07:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:44 @agent_ppo2.py:185][0m |          -0.0028 |         144.3069 |          15.7265 |
[32m[20221214 14:07:44 @agent_ppo2.py:185][0m |          -0.0024 |         113.5098 |          15.7046 |
[32m[20221214 14:07:44 @agent_ppo2.py:185][0m |          -0.0017 |         103.4219 |          15.6971 |
[32m[20221214 14:07:44 @agent_ppo2.py:185][0m |          -0.0003 |          97.5056 |          15.6802 |
[32m[20221214 14:07:44 @agent_ppo2.py:185][0m |          -0.0023 |          93.7177 |          15.6675 |
[32m[20221214 14:07:44 @agent_ppo2.py:185][0m |          -0.0012 |          91.3292 |          15.6715 |
[32m[20221214 14:07:44 @agent_ppo2.py:185][0m |           0.0011 |          89.4240 |          15.6644 |
[32m[20221214 14:07:44 @agent_ppo2.py:185][0m |          -0.0028 |          86.5477 |          15.6446 |
[32m[20221214 14:07:44 @agent_ppo2.py:185][0m |           0.0025 |          84.8081 |          15.6379 |
[32m[20221214 14:07:45 @agent_ppo2.py:185][0m |          -0.0020 |          83.1914 |          15.6314 |
[32m[20221214 14:07:45 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 595.50
[32m[20221214 14:07:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 722.60
[32m[20221214 14:07:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 754.19
[32m[20221214 14:07:45 @agent_ppo2.py:143][0m Total time:       9.71 min
[32m[20221214 14:07:45 @agent_ppo2.py:145][0m 880640 total steps have happened
[32m[20221214 14:07:45 @agent_ppo2.py:121][0m #------------------------ Iteration 430 --------------------------#
[32m[20221214 14:07:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:07:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:45 @agent_ppo2.py:185][0m |          -0.0004 |         139.0564 |          15.5800 |
[32m[20221214 14:07:45 @agent_ppo2.py:185][0m |           0.0072 |         129.0793 |          15.5677 |
[32m[20221214 14:07:45 @agent_ppo2.py:185][0m |           0.0094 |         125.9538 |          15.5481 |
[32m[20221214 14:07:45 @agent_ppo2.py:185][0m |           0.0036 |         115.0517 |          15.5441 |
[32m[20221214 14:07:45 @agent_ppo2.py:185][0m |           0.0025 |         109.1376 |          15.5257 |
[32m[20221214 14:07:45 @agent_ppo2.py:185][0m |          -0.0049 |         106.7493 |          15.5150 |
[32m[20221214 14:07:46 @agent_ppo2.py:185][0m |           0.0000 |         105.4913 |          15.5113 |
[32m[20221214 14:07:46 @agent_ppo2.py:185][0m |           0.0003 |         104.1351 |          15.4818 |
[32m[20221214 14:07:46 @agent_ppo2.py:185][0m |          -0.0020 |         103.2772 |          15.4656 |
[32m[20221214 14:07:46 @agent_ppo2.py:185][0m |          -0.0019 |         103.0778 |          15.4597 |
[32m[20221214 14:07:46 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:07:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 739.86
[32m[20221214 14:07:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 781.04
[32m[20221214 14:07:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 772.75
[32m[20221214 14:07:46 @agent_ppo2.py:143][0m Total time:       9.73 min
[32m[20221214 14:07:46 @agent_ppo2.py:145][0m 882688 total steps have happened
[32m[20221214 14:07:46 @agent_ppo2.py:121][0m #------------------------ Iteration 431 --------------------------#
[32m[20221214 14:07:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:07:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:46 @agent_ppo2.py:185][0m |          -0.0011 |         160.4545 |          15.2605 |
[32m[20221214 14:07:46 @agent_ppo2.py:185][0m |           0.0019 |         148.1416 |          15.2602 |
[32m[20221214 14:07:46 @agent_ppo2.py:185][0m |          -0.0007 |         143.9349 |          15.2494 |
[32m[20221214 14:07:46 @agent_ppo2.py:185][0m |           0.0002 |         140.9797 |          15.2479 |
[32m[20221214 14:07:47 @agent_ppo2.py:185][0m |           0.0072 |         143.4593 |          15.2389 |
[32m[20221214 14:07:47 @agent_ppo2.py:185][0m |           0.0019 |         137.0027 |          15.2274 |
[32m[20221214 14:07:47 @agent_ppo2.py:185][0m |           0.0000 |         135.4236 |          15.2312 |
[32m[20221214 14:07:47 @agent_ppo2.py:185][0m |          -0.0032 |         134.2891 |          15.2217 |
[32m[20221214 14:07:47 @agent_ppo2.py:185][0m |          -0.0002 |         132.5968 |          15.2136 |
[32m[20221214 14:07:47 @agent_ppo2.py:185][0m |          -0.0033 |         132.0696 |          15.2004 |
[32m[20221214 14:07:47 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.81
[32m[20221214 14:07:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 773.12
[32m[20221214 14:07:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 768.25
[32m[20221214 14:07:47 @agent_ppo2.py:143][0m Total time:       9.75 min
[32m[20221214 14:07:47 @agent_ppo2.py:145][0m 884736 total steps have happened
[32m[20221214 14:07:47 @agent_ppo2.py:121][0m #------------------------ Iteration 432 --------------------------#
[32m[20221214 14:07:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:47 @agent_ppo2.py:185][0m |          -0.0003 |         133.8327 |          15.2715 |
[32m[20221214 14:07:48 @agent_ppo2.py:185][0m |           0.0011 |         118.9476 |          15.2576 |
[32m[20221214 14:07:48 @agent_ppo2.py:185][0m |          -0.0007 |         113.5335 |          15.2330 |
[32m[20221214 14:07:48 @agent_ppo2.py:185][0m |           0.0007 |         110.4988 |          15.2204 |
[32m[20221214 14:07:48 @agent_ppo2.py:185][0m |          -0.0032 |         108.1734 |          15.2068 |
[32m[20221214 14:07:48 @agent_ppo2.py:185][0m |          -0.0038 |         106.5707 |          15.2003 |
[32m[20221214 14:07:48 @agent_ppo2.py:185][0m |           0.0008 |         107.6723 |          15.1710 |
[32m[20221214 14:07:48 @agent_ppo2.py:185][0m |          -0.0034 |         104.3317 |          15.1520 |
[32m[20221214 14:07:48 @agent_ppo2.py:185][0m |           0.0086 |         115.5001 |          15.1534 |
[32m[20221214 14:07:48 @agent_ppo2.py:185][0m |          -0.0019 |         103.1799 |          15.1404 |
[32m[20221214 14:07:48 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 674.57
[32m[20221214 14:07:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.58
[32m[20221214 14:07:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 786.55
[32m[20221214 14:07:48 @agent_ppo2.py:143][0m Total time:       9.77 min
[32m[20221214 14:07:48 @agent_ppo2.py:145][0m 886784 total steps have happened
[32m[20221214 14:07:48 @agent_ppo2.py:121][0m #------------------------ Iteration 433 --------------------------#
[32m[20221214 14:07:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:49 @agent_ppo2.py:185][0m |           0.0071 |         152.8636 |          15.2100 |
[32m[20221214 14:07:49 @agent_ppo2.py:185][0m |           0.0002 |         134.5441 |          15.2173 |
[32m[20221214 14:07:49 @agent_ppo2.py:185][0m |          -0.0035 |         131.9100 |          15.2243 |
[32m[20221214 14:07:49 @agent_ppo2.py:185][0m |          -0.0042 |         130.3887 |          15.2330 |
[32m[20221214 14:07:49 @agent_ppo2.py:185][0m |           0.0000 |         129.6218 |          15.2333 |
[32m[20221214 14:07:49 @agent_ppo2.py:185][0m |           0.0022 |         129.8060 |          15.2471 |
[32m[20221214 14:07:49 @agent_ppo2.py:185][0m |          -0.0031 |         128.0496 |          15.2470 |
[32m[20221214 14:07:49 @agent_ppo2.py:185][0m |           0.0054 |         131.3133 |          15.2724 |
[32m[20221214 14:07:49 @agent_ppo2.py:185][0m |          -0.0017 |         127.0416 |          15.2706 |
[32m[20221214 14:07:49 @agent_ppo2.py:185][0m |          -0.0023 |         126.4541 |          15.2735 |
[32m[20221214 14:07:49 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 744.98
[32m[20221214 14:07:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.14
[32m[20221214 14:07:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 765.99
[32m[20221214 14:07:50 @agent_ppo2.py:143][0m Total time:       9.79 min
[32m[20221214 14:07:50 @agent_ppo2.py:145][0m 888832 total steps have happened
[32m[20221214 14:07:50 @agent_ppo2.py:121][0m #------------------------ Iteration 434 --------------------------#
[32m[20221214 14:07:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:50 @agent_ppo2.py:185][0m |          -0.0013 |         142.1263 |          15.2451 |
[32m[20221214 14:07:50 @agent_ppo2.py:185][0m |          -0.0008 |         123.2400 |          15.2591 |
[32m[20221214 14:07:50 @agent_ppo2.py:185][0m |          -0.0040 |         114.4676 |          15.2459 |
[32m[20221214 14:07:50 @agent_ppo2.py:185][0m |          -0.0018 |         110.3932 |          15.2618 |
[32m[20221214 14:07:50 @agent_ppo2.py:185][0m |          -0.0030 |         107.7662 |          15.2584 |
[32m[20221214 14:07:50 @agent_ppo2.py:185][0m |          -0.0043 |         105.7294 |          15.2634 |
[32m[20221214 14:07:50 @agent_ppo2.py:185][0m |          -0.0030 |         104.3128 |          15.2525 |
[32m[20221214 14:07:51 @agent_ppo2.py:185][0m |          -0.0012 |         103.1384 |          15.2439 |
[32m[20221214 14:07:51 @agent_ppo2.py:185][0m |           0.0099 |         116.7186 |          15.2340 |
[32m[20221214 14:07:51 @agent_ppo2.py:185][0m |          -0.0013 |         101.7268 |          15.2599 |
[32m[20221214 14:07:51 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 686.40
[32m[20221214 14:07:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 775.55
[32m[20221214 14:07:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.64
[32m[20221214 14:07:51 @agent_ppo2.py:143][0m Total time:       9.81 min
[32m[20221214 14:07:51 @agent_ppo2.py:145][0m 890880 total steps have happened
[32m[20221214 14:07:51 @agent_ppo2.py:121][0m #------------------------ Iteration 435 --------------------------#
[32m[20221214 14:07:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:51 @agent_ppo2.py:185][0m |          -0.0014 |         144.3187 |          15.2999 |
[32m[20221214 14:07:51 @agent_ppo2.py:185][0m |          -0.0029 |         134.0365 |          15.2866 |
[32m[20221214 14:07:51 @agent_ppo2.py:185][0m |          -0.0018 |         129.6969 |          15.2829 |
[32m[20221214 14:07:51 @agent_ppo2.py:185][0m |          -0.0029 |         127.9226 |          15.2676 |
[32m[20221214 14:07:52 @agent_ppo2.py:185][0m |           0.0000 |         126.6032 |          15.2722 |
[32m[20221214 14:07:52 @agent_ppo2.py:185][0m |           0.0006 |         127.0537 |          15.2634 |
[32m[20221214 14:07:52 @agent_ppo2.py:185][0m |          -0.0027 |         125.0511 |          15.2440 |
[32m[20221214 14:07:52 @agent_ppo2.py:185][0m |          -0.0038 |         124.6862 |          15.2463 |
[32m[20221214 14:07:52 @agent_ppo2.py:185][0m |           0.0005 |         123.8261 |          15.2436 |
[32m[20221214 14:07:52 @agent_ppo2.py:185][0m |           0.0014 |         124.2949 |          15.2426 |
[32m[20221214 14:07:52 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:07:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 718.87
[32m[20221214 14:07:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.04
[32m[20221214 14:07:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 708.11
[32m[20221214 14:07:52 @agent_ppo2.py:143][0m Total time:       9.83 min
[32m[20221214 14:07:52 @agent_ppo2.py:145][0m 892928 total steps have happened
[32m[20221214 14:07:52 @agent_ppo2.py:121][0m #------------------------ Iteration 436 --------------------------#
[32m[20221214 14:07:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:52 @agent_ppo2.py:185][0m |          -0.0011 |         145.3013 |          15.2147 |
[32m[20221214 14:07:52 @agent_ppo2.py:185][0m |          -0.0019 |         135.5977 |          15.2298 |
[32m[20221214 14:07:53 @agent_ppo2.py:185][0m |          -0.0021 |         131.3832 |          15.2324 |
[32m[20221214 14:07:53 @agent_ppo2.py:185][0m |           0.0058 |         139.7191 |          15.2443 |
[32m[20221214 14:07:53 @agent_ppo2.py:185][0m |          -0.0046 |         127.5864 |          15.2546 |
[32m[20221214 14:07:53 @agent_ppo2.py:185][0m |           0.0017 |         128.5537 |          15.2615 |
[32m[20221214 14:07:53 @agent_ppo2.py:185][0m |          -0.0042 |         124.3994 |          15.2478 |
[32m[20221214 14:07:53 @agent_ppo2.py:185][0m |          -0.0002 |         123.4924 |          15.2540 |
[32m[20221214 14:07:53 @agent_ppo2.py:185][0m |          -0.0051 |         122.3748 |          15.2607 |
[32m[20221214 14:07:53 @agent_ppo2.py:185][0m |          -0.0004 |         122.7587 |          15.2565 |
[32m[20221214 14:07:53 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 724.82
[32m[20221214 14:07:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.30
[32m[20221214 14:07:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 704.77
[32m[20221214 14:07:53 @agent_ppo2.py:143][0m Total time:       9.85 min
[32m[20221214 14:07:53 @agent_ppo2.py:145][0m 894976 total steps have happened
[32m[20221214 14:07:53 @agent_ppo2.py:121][0m #------------------------ Iteration 437 --------------------------#
[32m[20221214 14:07:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:54 @agent_ppo2.py:185][0m |          -0.0040 |         145.9918 |          15.1543 |
[32m[20221214 14:07:54 @agent_ppo2.py:185][0m |          -0.0021 |         138.7799 |          15.1454 |
[32m[20221214 14:07:54 @agent_ppo2.py:185][0m |          -0.0034 |         135.9216 |          15.1289 |
[32m[20221214 14:07:54 @agent_ppo2.py:185][0m |          -0.0011 |         133.1688 |          15.1014 |
[32m[20221214 14:07:54 @agent_ppo2.py:185][0m |          -0.0024 |         131.7411 |          15.1004 |
[32m[20221214 14:07:54 @agent_ppo2.py:185][0m |          -0.0039 |         130.8066 |          15.0910 |
[32m[20221214 14:07:54 @agent_ppo2.py:185][0m |          -0.0023 |         129.6717 |          15.0778 |
[32m[20221214 14:07:54 @agent_ppo2.py:185][0m |          -0.0020 |         128.9294 |          15.0691 |
[32m[20221214 14:07:54 @agent_ppo2.py:185][0m |           0.0004 |         130.0814 |          15.0521 |
[32m[20221214 14:07:54 @agent_ppo2.py:185][0m |          -0.0027 |         127.8772 |          15.0365 |
[32m[20221214 14:07:54 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 697.55
[32m[20221214 14:07:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.44
[32m[20221214 14:07:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 710.01
[32m[20221214 14:07:55 @agent_ppo2.py:143][0m Total time:       9.87 min
[32m[20221214 14:07:55 @agent_ppo2.py:145][0m 897024 total steps have happened
[32m[20221214 14:07:55 @agent_ppo2.py:121][0m #------------------------ Iteration 438 --------------------------#
[32m[20221214 14:07:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:55 @agent_ppo2.py:185][0m |          -0.0035 |         132.6202 |          15.0361 |
[32m[20221214 14:07:55 @agent_ppo2.py:185][0m |          -0.0046 |         127.7257 |          15.0251 |
[32m[20221214 14:07:55 @agent_ppo2.py:185][0m |          -0.0055 |         125.1147 |          15.0344 |
[32m[20221214 14:07:55 @agent_ppo2.py:185][0m |          -0.0055 |         123.1210 |          15.0378 |
[32m[20221214 14:07:55 @agent_ppo2.py:185][0m |          -0.0050 |         120.6188 |          15.0314 |
[32m[20221214 14:07:55 @agent_ppo2.py:185][0m |          -0.0063 |         119.2023 |          15.0307 |
[32m[20221214 14:07:55 @agent_ppo2.py:185][0m |          -0.0032 |         117.9821 |          15.0256 |
[32m[20221214 14:07:55 @agent_ppo2.py:185][0m |          -0.0070 |         116.9524 |          15.0353 |
[32m[20221214 14:07:56 @agent_ppo2.py:185][0m |           0.0020 |         118.0550 |          15.0370 |
[32m[20221214 14:07:56 @agent_ppo2.py:185][0m |          -0.0044 |         115.7387 |          15.0231 |
[32m[20221214 14:07:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 716.49
[32m[20221214 14:07:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 720.54
[32m[20221214 14:07:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 705.54
[32m[20221214 14:07:56 @agent_ppo2.py:143][0m Total time:       9.90 min
[32m[20221214 14:07:56 @agent_ppo2.py:145][0m 899072 total steps have happened
[32m[20221214 14:07:56 @agent_ppo2.py:121][0m #------------------------ Iteration 439 --------------------------#
[32m[20221214 14:07:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:56 @agent_ppo2.py:185][0m |          -0.0026 |         142.0240 |          15.0659 |
[32m[20221214 14:07:56 @agent_ppo2.py:185][0m |          -0.0000 |         137.3652 |          15.0587 |
[32m[20221214 14:07:56 @agent_ppo2.py:185][0m |          -0.0006 |         133.8769 |          15.0469 |
[32m[20221214 14:07:56 @agent_ppo2.py:185][0m |          -0.0029 |         132.0255 |          15.0434 |
[32m[20221214 14:07:56 @agent_ppo2.py:185][0m |          -0.0010 |         131.1455 |          15.0182 |
[32m[20221214 14:07:57 @agent_ppo2.py:185][0m |          -0.0028 |         129.4653 |          15.0241 |
[32m[20221214 14:07:57 @agent_ppo2.py:185][0m |          -0.0017 |         129.2448 |          15.0069 |
[32m[20221214 14:07:57 @agent_ppo2.py:185][0m |          -0.0020 |         127.7476 |          15.0087 |
[32m[20221214 14:07:57 @agent_ppo2.py:185][0m |          -0.0039 |         126.9497 |          14.9923 |
[32m[20221214 14:07:57 @agent_ppo2.py:185][0m |          -0.0028 |         126.5822 |          14.9729 |
[32m[20221214 14:07:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:07:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 671.99
[32m[20221214 14:07:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.60
[32m[20221214 14:07:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 708.67
[32m[20221214 14:07:57 @agent_ppo2.py:143][0m Total time:       9.92 min
[32m[20221214 14:07:57 @agent_ppo2.py:145][0m 901120 total steps have happened
[32m[20221214 14:07:57 @agent_ppo2.py:121][0m #------------------------ Iteration 440 --------------------------#
[32m[20221214 14:07:57 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:07:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:57 @agent_ppo2.py:185][0m |          -0.0014 |         142.3020 |          14.9113 |
[32m[20221214 14:07:57 @agent_ppo2.py:185][0m |          -0.0009 |         131.0689 |          14.9116 |
[32m[20221214 14:07:58 @agent_ppo2.py:185][0m |          -0.0012 |         123.0521 |          14.9100 |
[32m[20221214 14:07:58 @agent_ppo2.py:185][0m |          -0.0019 |         119.3575 |          14.9093 |
[32m[20221214 14:07:58 @agent_ppo2.py:185][0m |          -0.0038 |         117.5072 |          14.9093 |
[32m[20221214 14:07:58 @agent_ppo2.py:185][0m |           0.0003 |         115.8978 |          14.8927 |
[32m[20221214 14:07:58 @agent_ppo2.py:185][0m |          -0.0024 |         115.1159 |          14.8906 |
[32m[20221214 14:07:58 @agent_ppo2.py:185][0m |          -0.0019 |         114.3028 |          14.8739 |
[32m[20221214 14:07:58 @agent_ppo2.py:185][0m |           0.0082 |         127.9134 |          14.8909 |
[32m[20221214 14:07:58 @agent_ppo2.py:185][0m |          -0.0034 |         112.5578 |          14.8922 |
[32m[20221214 14:07:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:07:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 681.04
[32m[20221214 14:07:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 707.07
[32m[20221214 14:07:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 742.78
[32m[20221214 14:07:58 @agent_ppo2.py:143][0m Total time:       9.94 min
[32m[20221214 14:07:58 @agent_ppo2.py:145][0m 903168 total steps have happened
[32m[20221214 14:07:58 @agent_ppo2.py:121][0m #------------------------ Iteration 441 --------------------------#
[32m[20221214 14:07:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:07:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:07:59 @agent_ppo2.py:185][0m |           0.0127 |         158.4938 |          14.8816 |
[32m[20221214 14:07:59 @agent_ppo2.py:185][0m |          -0.0014 |         134.7308 |          14.9035 |
[32m[20221214 14:07:59 @agent_ppo2.py:185][0m |           0.0047 |         137.1725 |          14.9017 |
[32m[20221214 14:07:59 @agent_ppo2.py:185][0m |           0.0100 |         142.8477 |          14.8825 |
[32m[20221214 14:07:59 @agent_ppo2.py:185][0m |          -0.0035 |         130.5218 |          14.8957 |
[32m[20221214 14:07:59 @agent_ppo2.py:185][0m |           0.0003 |         128.8984 |          14.8812 |
[32m[20221214 14:07:59 @agent_ppo2.py:185][0m |          -0.0030 |         128.1181 |          14.8863 |
[32m[20221214 14:07:59 @agent_ppo2.py:185][0m |          -0.0031 |         126.4862 |          14.8916 |
[32m[20221214 14:07:59 @agent_ppo2.py:185][0m |          -0.0044 |         126.1595 |          14.8928 |
[32m[20221214 14:07:59 @agent_ppo2.py:185][0m |           0.0016 |         126.2414 |          14.8822 |
[32m[20221214 14:07:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:08:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 718.82
[32m[20221214 14:08:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.23
[32m[20221214 14:08:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 746.30
[32m[20221214 14:08:00 @agent_ppo2.py:143][0m Total time:       9.96 min
[32m[20221214 14:08:00 @agent_ppo2.py:145][0m 905216 total steps have happened
[32m[20221214 14:08:00 @agent_ppo2.py:121][0m #------------------------ Iteration 442 --------------------------#
[32m[20221214 14:08:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:00 @agent_ppo2.py:185][0m |          -0.0008 |         145.4111 |          14.8512 |
[32m[20221214 14:08:00 @agent_ppo2.py:185][0m |          -0.0008 |         138.6395 |          14.8581 |
[32m[20221214 14:08:00 @agent_ppo2.py:185][0m |          -0.0011 |         132.0214 |          14.8664 |
[32m[20221214 14:08:00 @agent_ppo2.py:185][0m |           0.0063 |         132.3574 |          14.8432 |
[32m[20221214 14:08:00 @agent_ppo2.py:185][0m |          -0.0025 |         126.4665 |          14.8438 |
[32m[20221214 14:08:00 @agent_ppo2.py:185][0m |          -0.0015 |         124.8135 |          14.8361 |
[32m[20221214 14:08:00 @agent_ppo2.py:185][0m |          -0.0011 |         123.6090 |          14.8100 |
[32m[20221214 14:08:01 @agent_ppo2.py:185][0m |          -0.0020 |         122.6901 |          14.8202 |
[32m[20221214 14:08:01 @agent_ppo2.py:185][0m |          -0.0020 |         122.0406 |          14.8111 |
[32m[20221214 14:08:01 @agent_ppo2.py:185][0m |          -0.0008 |         121.3917 |          14.8046 |
[32m[20221214 14:08:01 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:08:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 704.34
[32m[20221214 14:08:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.61
[32m[20221214 14:08:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 755.42
[32m[20221214 14:08:01 @agent_ppo2.py:143][0m Total time:       9.98 min
[32m[20221214 14:08:01 @agent_ppo2.py:145][0m 907264 total steps have happened
[32m[20221214 14:08:01 @agent_ppo2.py:121][0m #------------------------ Iteration 443 --------------------------#
[32m[20221214 14:08:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:01 @agent_ppo2.py:185][0m |          -0.0007 |         135.3786 |          14.6552 |
[32m[20221214 14:08:01 @agent_ppo2.py:185][0m |          -0.0017 |         130.9026 |          14.6702 |
[32m[20221214 14:08:01 @agent_ppo2.py:185][0m |          -0.0009 |         128.8181 |          14.6861 |
[32m[20221214 14:08:02 @agent_ppo2.py:185][0m |          -0.0024 |         127.4431 |          14.6968 |
[32m[20221214 14:08:02 @agent_ppo2.py:185][0m |           0.0090 |         132.7949 |          14.7025 |
[32m[20221214 14:08:02 @agent_ppo2.py:185][0m |           0.0006 |         126.5516 |          14.7049 |
[32m[20221214 14:08:02 @agent_ppo2.py:185][0m |          -0.0021 |         124.4682 |          14.7225 |
[32m[20221214 14:08:02 @agent_ppo2.py:185][0m |           0.0066 |         133.8942 |          14.7299 |
[32m[20221214 14:08:02 @agent_ppo2.py:185][0m |          -0.0035 |         124.2507 |          14.7616 |
[32m[20221214 14:08:02 @agent_ppo2.py:185][0m |          -0.0025 |         122.9110 |          14.7414 |
[32m[20221214 14:08:02 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:08:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 725.78
[32m[20221214 14:08:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 742.97
[32m[20221214 14:08:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 767.67
[32m[20221214 14:08:02 @agent_ppo2.py:143][0m Total time:      10.01 min
[32m[20221214 14:08:02 @agent_ppo2.py:145][0m 909312 total steps have happened
[32m[20221214 14:08:02 @agent_ppo2.py:121][0m #------------------------ Iteration 444 --------------------------#
[32m[20221214 14:08:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:03 @agent_ppo2.py:185][0m |          -0.0026 |         135.8255 |          14.7879 |
[32m[20221214 14:08:03 @agent_ppo2.py:185][0m |           0.0018 |         131.6258 |          14.7672 |
[32m[20221214 14:08:03 @agent_ppo2.py:185][0m |          -0.0033 |         127.3167 |          14.7442 |
[32m[20221214 14:08:03 @agent_ppo2.py:185][0m |          -0.0027 |         125.5301 |          14.7390 |
[32m[20221214 14:08:03 @agent_ppo2.py:185][0m |          -0.0042 |         123.7510 |          14.7224 |
[32m[20221214 14:08:03 @agent_ppo2.py:185][0m |          -0.0051 |         122.3721 |          14.7113 |
[32m[20221214 14:08:03 @agent_ppo2.py:185][0m |          -0.0047 |         121.3514 |          14.7169 |
[32m[20221214 14:08:03 @agent_ppo2.py:185][0m |           0.0075 |         139.6191 |          14.6864 |
[32m[20221214 14:08:03 @agent_ppo2.py:185][0m |          -0.0071 |         119.7820 |          14.6748 |
[32m[20221214 14:08:03 @agent_ppo2.py:185][0m |          -0.0041 |         119.3921 |          14.6605 |
[32m[20221214 14:08:03 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:08:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 729.81
[32m[20221214 14:08:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 757.92
[32m[20221214 14:08:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 736.21
[32m[20221214 14:08:04 @agent_ppo2.py:143][0m Total time:      10.03 min
[32m[20221214 14:08:04 @agent_ppo2.py:145][0m 911360 total steps have happened
[32m[20221214 14:08:04 @agent_ppo2.py:121][0m #------------------------ Iteration 445 --------------------------#
[32m[20221214 14:08:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:08:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:04 @agent_ppo2.py:185][0m |           0.0012 |         131.8316 |          14.5641 |
[32m[20221214 14:08:04 @agent_ppo2.py:185][0m |          -0.0011 |         126.8331 |          14.5745 |
[32m[20221214 14:08:04 @agent_ppo2.py:185][0m |          -0.0023 |         125.2655 |          14.5845 |
[32m[20221214 14:08:04 @agent_ppo2.py:185][0m |           0.0002 |         123.8954 |          14.5838 |
[32m[20221214 14:08:04 @agent_ppo2.py:185][0m |          -0.0028 |         122.5665 |          14.5867 |
[32m[20221214 14:08:04 @agent_ppo2.py:185][0m |          -0.0003 |         121.6543 |          14.5719 |
[32m[20221214 14:08:04 @agent_ppo2.py:185][0m |           0.0138 |         129.7128 |          14.5868 |
[32m[20221214 14:08:05 @agent_ppo2.py:185][0m |          -0.0037 |         120.1827 |          14.5847 |
[32m[20221214 14:08:05 @agent_ppo2.py:185][0m |          -0.0029 |         118.9982 |          14.5738 |
[32m[20221214 14:08:05 @agent_ppo2.py:185][0m |          -0.0009 |         118.3663 |          14.5834 |
[32m[20221214 14:08:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:08:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 704.95
[32m[20221214 14:08:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 731.39
[32m[20221214 14:08:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 735.53
[32m[20221214 14:08:05 @agent_ppo2.py:143][0m Total time:      10.05 min
[32m[20221214 14:08:05 @agent_ppo2.py:145][0m 913408 total steps have happened
[32m[20221214 14:08:05 @agent_ppo2.py:121][0m #------------------------ Iteration 446 --------------------------#
[32m[20221214 14:08:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:08:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:05 @agent_ppo2.py:185][0m |           0.0087 |         108.9717 |          14.7593 |
[32m[20221214 14:08:05 @agent_ppo2.py:185][0m |          -0.0024 |          90.9573 |          14.7678 |
[32m[20221214 14:08:05 @agent_ppo2.py:185][0m |          -0.0020 |          85.5370 |          14.7488 |
[32m[20221214 14:08:05 @agent_ppo2.py:185][0m |          -0.0005 |          82.0501 |          14.7567 |
[32m[20221214 14:08:06 @agent_ppo2.py:185][0m |           0.0085 |          89.9168 |          14.7813 |
[32m[20221214 14:08:06 @agent_ppo2.py:185][0m |          -0.0013 |          77.9424 |          14.7665 |
[32m[20221214 14:08:06 @agent_ppo2.py:185][0m |           0.0001 |          75.9435 |          14.7910 |
[32m[20221214 14:08:06 @agent_ppo2.py:185][0m |          -0.0023 |          74.3624 |          14.7604 |
[32m[20221214 14:08:06 @agent_ppo2.py:185][0m |           0.0029 |          73.5906 |          14.7611 |
[32m[20221214 14:08:06 @agent_ppo2.py:185][0m |          -0.0008 |          72.8599 |          14.7714 |
[32m[20221214 14:08:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:08:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 690.65
[32m[20221214 14:08:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 714.49
[32m[20221214 14:08:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 745.27
[32m[20221214 14:08:06 @agent_ppo2.py:143][0m Total time:      10.07 min
[32m[20221214 14:08:06 @agent_ppo2.py:145][0m 915456 total steps have happened
[32m[20221214 14:08:06 @agent_ppo2.py:121][0m #------------------------ Iteration 447 --------------------------#
[32m[20221214 14:08:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:08:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:06 @agent_ppo2.py:185][0m |           0.0116 |         160.3388 |          14.6601 |
[32m[20221214 14:08:07 @agent_ppo2.py:185][0m |          -0.0020 |         132.6645 |          14.6587 |
[32m[20221214 14:08:07 @agent_ppo2.py:185][0m |          -0.0001 |         129.0376 |          14.6680 |
[32m[20221214 14:08:07 @agent_ppo2.py:185][0m |          -0.0041 |         125.3552 |          14.6334 |
[32m[20221214 14:08:07 @agent_ppo2.py:185][0m |           0.0042 |         126.2721 |          14.6201 |
[32m[20221214 14:08:07 @agent_ppo2.py:185][0m |          -0.0025 |         119.0156 |          14.6003 |
[32m[20221214 14:08:07 @agent_ppo2.py:185][0m |          -0.0030 |         116.4181 |          14.5937 |
[32m[20221214 14:08:07 @agent_ppo2.py:185][0m |          -0.0034 |         114.4638 |          14.5675 |
[32m[20221214 14:08:07 @agent_ppo2.py:185][0m |          -0.0015 |         113.1883 |          14.5528 |
[32m[20221214 14:08:07 @agent_ppo2.py:185][0m |          -0.0047 |         112.6115 |          14.5453 |
[32m[20221214 14:08:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:08:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 682.41
[32m[20221214 14:08:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 712.45
[32m[20221214 14:08:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 714.18
[32m[20221214 14:08:07 @agent_ppo2.py:143][0m Total time:      10.09 min
[32m[20221214 14:08:07 @agent_ppo2.py:145][0m 917504 total steps have happened
[32m[20221214 14:08:07 @agent_ppo2.py:121][0m #------------------------ Iteration 448 --------------------------#
[32m[20221214 14:08:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:08:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:08 @agent_ppo2.py:185][0m |          -0.0021 |         106.3243 |          14.5104 |
[32m[20221214 14:08:08 @agent_ppo2.py:185][0m |          -0.0033 |          90.6251 |          14.4953 |
[32m[20221214 14:08:08 @agent_ppo2.py:185][0m |          -0.0010 |          84.8790 |          14.4867 |
[32m[20221214 14:08:08 @agent_ppo2.py:185][0m |          -0.0018 |          81.4132 |          14.4515 |
[32m[20221214 14:08:08 @agent_ppo2.py:185][0m |           0.0082 |          81.3055 |          14.4250 |
[32m[20221214 14:08:08 @agent_ppo2.py:185][0m |          -0.0011 |          77.1988 |          14.4160 |
[32m[20221214 14:08:08 @agent_ppo2.py:185][0m |           0.0126 |          83.0196 |          14.3976 |
[32m[20221214 14:08:08 @agent_ppo2.py:185][0m |           0.0012 |          74.7783 |          14.3557 |
[32m[20221214 14:08:08 @agent_ppo2.py:185][0m |           0.0062 |          76.7447 |          14.3346 |
[32m[20221214 14:08:08 @agent_ppo2.py:185][0m |          -0.0029 |          73.5351 |          14.3190 |
[32m[20221214 14:08:08 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:08:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 669.97
[32m[20221214 14:08:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 689.77
[32m[20221214 14:08:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 707.97
[32m[20221214 14:08:09 @agent_ppo2.py:143][0m Total time:      10.11 min
[32m[20221214 14:08:09 @agent_ppo2.py:145][0m 919552 total steps have happened
[32m[20221214 14:08:09 @agent_ppo2.py:121][0m #------------------------ Iteration 449 --------------------------#
[32m[20221214 14:08:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:08:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:09 @agent_ppo2.py:185][0m |          -0.0008 |          99.2244 |          14.2114 |
[32m[20221214 14:08:09 @agent_ppo2.py:185][0m |          -0.0032 |          79.3831 |          14.2175 |
[32m[20221214 14:08:09 @agent_ppo2.py:185][0m |           0.0012 |          72.9251 |          14.2304 |
[32m[20221214 14:08:09 @agent_ppo2.py:185][0m |          -0.0006 |          69.8653 |          14.2254 |
[32m[20221214 14:08:09 @agent_ppo2.py:185][0m |           0.0126 |          87.7324 |          14.2188 |
[32m[20221214 14:08:09 @agent_ppo2.py:185][0m |          -0.0048 |          65.0878 |          14.2073 |
[32m[20221214 14:08:09 @agent_ppo2.py:185][0m |          -0.0063 |          63.6180 |          14.2159 |
[32m[20221214 14:08:10 @agent_ppo2.py:185][0m |          -0.0056 |          62.5456 |          14.2140 |
[32m[20221214 14:08:10 @agent_ppo2.py:185][0m |           0.0016 |          61.2651 |          14.2124 |
[32m[20221214 14:08:10 @agent_ppo2.py:185][0m |          -0.0047 |          60.3679 |          14.2193 |
[32m[20221214 14:08:10 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:08:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 612.91
[32m[20221214 14:08:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 654.65
[32m[20221214 14:08:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 719.26
[32m[20221214 14:08:10 @agent_ppo2.py:143][0m Total time:      10.13 min
[32m[20221214 14:08:10 @agent_ppo2.py:145][0m 921600 total steps have happened
[32m[20221214 14:08:10 @agent_ppo2.py:121][0m #------------------------ Iteration 450 --------------------------#
[32m[20221214 14:08:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:08:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:10 @agent_ppo2.py:185][0m |          -0.0030 |         101.8369 |          14.3468 |
[32m[20221214 14:08:10 @agent_ppo2.py:185][0m |          -0.0013 |          90.1918 |          14.3468 |
[32m[20221214 14:08:10 @agent_ppo2.py:185][0m |          -0.0048 |          88.5587 |          14.3344 |
[32m[20221214 14:08:11 @agent_ppo2.py:185][0m |          -0.0033 |          86.5653 |          14.3429 |
[32m[20221214 14:08:11 @agent_ppo2.py:185][0m |          -0.0037 |          85.9984 |          14.3457 |
[32m[20221214 14:08:11 @agent_ppo2.py:185][0m |           0.0027 |          86.4001 |          14.3506 |
[32m[20221214 14:08:11 @agent_ppo2.py:185][0m |          -0.0037 |          84.1326 |          14.3585 |
[32m[20221214 14:08:11 @agent_ppo2.py:185][0m |          -0.0018 |          83.1207 |          14.3581 |
[32m[20221214 14:08:11 @agent_ppo2.py:185][0m |          -0.0011 |          82.6513 |          14.3623 |
[32m[20221214 14:08:11 @agent_ppo2.py:185][0m |          -0.0022 |          81.9865 |          14.3525 |
[32m[20221214 14:08:11 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 14:08:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 698.48
[32m[20221214 14:08:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 716.62
[32m[20221214 14:08:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 703.34
[32m[20221214 14:08:11 @agent_ppo2.py:143][0m Total time:      10.15 min
[32m[20221214 14:08:11 @agent_ppo2.py:145][0m 923648 total steps have happened
[32m[20221214 14:08:11 @agent_ppo2.py:121][0m #------------------------ Iteration 451 --------------------------#
[32m[20221214 14:08:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:12 @agent_ppo2.py:185][0m |           0.0055 |          62.1358 |          14.2218 |
[32m[20221214 14:08:12 @agent_ppo2.py:185][0m |          -0.0041 |          47.7883 |          14.2659 |
[32m[20221214 14:08:12 @agent_ppo2.py:185][0m |          -0.0042 |          42.0714 |          14.2624 |
[32m[20221214 14:08:12 @agent_ppo2.py:185][0m |          -0.0011 |          38.6617 |          14.2516 |
[32m[20221214 14:08:12 @agent_ppo2.py:185][0m |          -0.0048 |          36.5204 |          14.2583 |
[32m[20221214 14:08:12 @agent_ppo2.py:185][0m |          -0.0025 |          34.8689 |          14.2516 |
[32m[20221214 14:08:12 @agent_ppo2.py:185][0m |          -0.0043 |          33.8709 |          14.2619 |
[32m[20221214 14:08:12 @agent_ppo2.py:185][0m |          -0.0028 |          32.8970 |          14.2592 |
[32m[20221214 14:08:12 @agent_ppo2.py:185][0m |          -0.0091 |          32.3140 |          14.2474 |
[32m[20221214 14:08:13 @agent_ppo2.py:185][0m |          -0.0059 |          31.2824 |          14.2467 |
[32m[20221214 14:08:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:08:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 641.54
[32m[20221214 14:08:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 655.43
[32m[20221214 14:08:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 705.90
[32m[20221214 14:08:13 @agent_ppo2.py:143][0m Total time:      10.18 min
[32m[20221214 14:08:13 @agent_ppo2.py:145][0m 925696 total steps have happened
[32m[20221214 14:08:13 @agent_ppo2.py:121][0m #------------------------ Iteration 452 --------------------------#
[32m[20221214 14:08:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:13 @agent_ppo2.py:185][0m |          -0.0005 |          77.3467 |          14.4161 |
[32m[20221214 14:08:13 @agent_ppo2.py:185][0m |          -0.0010 |          69.8730 |          14.4053 |
[32m[20221214 14:08:13 @agent_ppo2.py:185][0m |          -0.0067 |          67.2918 |          14.4091 |
[32m[20221214 14:08:13 @agent_ppo2.py:185][0m |          -0.0023 |          65.6660 |          14.3953 |
[32m[20221214 14:08:13 @agent_ppo2.py:185][0m |          -0.0038 |          64.3792 |          14.3933 |
[32m[20221214 14:08:14 @agent_ppo2.py:185][0m |          -0.0026 |          63.5932 |          14.3936 |
[32m[20221214 14:08:14 @agent_ppo2.py:185][0m |          -0.0027 |          62.7923 |          14.3906 |
[32m[20221214 14:08:14 @agent_ppo2.py:185][0m |          -0.0083 |          61.9466 |          14.3814 |
[32m[20221214 14:08:14 @agent_ppo2.py:185][0m |          -0.0051 |          61.5004 |          14.3834 |
[32m[20221214 14:08:14 @agent_ppo2.py:185][0m |          -0.0039 |          61.1209 |          14.3772 |
[32m[20221214 14:08:14 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:08:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 648.55
[32m[20221214 14:08:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.81
[32m[20221214 14:08:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 715.66
[32m[20221214 14:08:14 @agent_ppo2.py:143][0m Total time:      10.20 min
[32m[20221214 14:08:14 @agent_ppo2.py:145][0m 927744 total steps have happened
[32m[20221214 14:08:14 @agent_ppo2.py:121][0m #------------------------ Iteration 453 --------------------------#
[32m[20221214 14:08:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:14 @agent_ppo2.py:185][0m |          -0.0022 |          69.4081 |          14.2206 |
[32m[20221214 14:08:14 @agent_ppo2.py:185][0m |          -0.0034 |          62.8776 |          14.2384 |
[32m[20221214 14:08:15 @agent_ppo2.py:185][0m |          -0.0044 |          61.9050 |          14.2321 |
[32m[20221214 14:08:15 @agent_ppo2.py:185][0m |          -0.0050 |          60.3434 |          14.2204 |
[32m[20221214 14:08:15 @agent_ppo2.py:185][0m |          -0.0055 |          59.8740 |          14.2385 |
[32m[20221214 14:08:15 @agent_ppo2.py:185][0m |          -0.0052 |          59.7516 |          14.2300 |
[32m[20221214 14:08:15 @agent_ppo2.py:185][0m |          -0.0070 |          59.4712 |          14.2276 |
[32m[20221214 14:08:15 @agent_ppo2.py:185][0m |          -0.0026 |          58.4753 |          14.2233 |
[32m[20221214 14:08:15 @agent_ppo2.py:185][0m |          -0.0053 |          57.9817 |          14.2226 |
[32m[20221214 14:08:15 @agent_ppo2.py:185][0m |          -0.0073 |          58.1294 |          14.2096 |
[32m[20221214 14:08:15 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:08:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 687.98
[32m[20221214 14:08:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.52
[32m[20221214 14:08:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 761.69
[32m[20221214 14:08:15 @agent_ppo2.py:143][0m Total time:      10.22 min
[32m[20221214 14:08:15 @agent_ppo2.py:145][0m 929792 total steps have happened
[32m[20221214 14:08:15 @agent_ppo2.py:121][0m #------------------------ Iteration 454 --------------------------#
[32m[20221214 14:08:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:08:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:16 @agent_ppo2.py:185][0m |           0.0079 |         168.1790 |          14.0946 |
[32m[20221214 14:08:16 @agent_ppo2.py:185][0m |          -0.0050 |         159.4191 |          14.0818 |
[32m[20221214 14:08:16 @agent_ppo2.py:185][0m |          -0.0034 |         158.1953 |          14.0950 |
[32m[20221214 14:08:16 @agent_ppo2.py:185][0m |          -0.0028 |         157.7224 |          14.0934 |
[32m[20221214 14:08:16 @agent_ppo2.py:185][0m |          -0.0028 |         157.4468 |          14.1042 |
[32m[20221214 14:08:16 @agent_ppo2.py:185][0m |           0.0001 |         159.5227 |          14.0933 |
[32m[20221214 14:08:16 @agent_ppo2.py:185][0m |          -0.0037 |         156.2133 |          14.0877 |
[32m[20221214 14:08:16 @agent_ppo2.py:185][0m |          -0.0034 |         156.0124 |          14.1082 |
[32m[20221214 14:08:17 @agent_ppo2.py:185][0m |          -0.0035 |         155.8932 |          14.1053 |
[32m[20221214 14:08:17 @agent_ppo2.py:185][0m |          -0.0044 |         155.6917 |          14.1062 |
[32m[20221214 14:08:17 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:08:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 723.63
[32m[20221214 14:08:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 740.94
[32m[20221214 14:08:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.93
[32m[20221214 14:08:17 @agent_ppo2.py:143][0m Total time:      10.25 min
[32m[20221214 14:08:17 @agent_ppo2.py:145][0m 931840 total steps have happened
[32m[20221214 14:08:17 @agent_ppo2.py:121][0m #------------------------ Iteration 455 --------------------------#
[32m[20221214 14:08:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:17 @agent_ppo2.py:185][0m |           0.0023 |         163.6276 |          14.1568 |
[32m[20221214 14:08:17 @agent_ppo2.py:185][0m |          -0.0004 |         152.6571 |          14.1187 |
[32m[20221214 14:08:17 @agent_ppo2.py:185][0m |          -0.0012 |         147.6770 |          14.1118 |
[32m[20221214 14:08:17 @agent_ppo2.py:185][0m |          -0.0025 |         145.0455 |          14.1176 |
[32m[20221214 14:08:18 @agent_ppo2.py:185][0m |          -0.0034 |         143.4228 |          14.1086 |
[32m[20221214 14:08:18 @agent_ppo2.py:185][0m |          -0.0008 |         142.5961 |          14.0973 |
[32m[20221214 14:08:18 @agent_ppo2.py:185][0m |          -0.0032 |         141.1750 |          14.1042 |
[32m[20221214 14:08:18 @agent_ppo2.py:185][0m |          -0.0021 |         141.1903 |          14.0912 |
[32m[20221214 14:08:18 @agent_ppo2.py:185][0m |          -0.0027 |         139.6457 |          14.0895 |
[32m[20221214 14:08:18 @agent_ppo2.py:185][0m |          -0.0031 |         138.5243 |          14.0886 |
[32m[20221214 14:08:18 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:08:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 758.25
[32m[20221214 14:08:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 763.49
[32m[20221214 14:08:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.48
[32m[20221214 14:08:18 @agent_ppo2.py:143][0m Total time:      10.27 min
[32m[20221214 14:08:18 @agent_ppo2.py:145][0m 933888 total steps have happened
[32m[20221214 14:08:18 @agent_ppo2.py:121][0m #------------------------ Iteration 456 --------------------------#
[32m[20221214 14:08:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:19 @agent_ppo2.py:185][0m |          -0.0007 |         169.0535 |          14.1818 |
[32m[20221214 14:08:19 @agent_ppo2.py:185][0m |           0.0024 |         161.5889 |          14.1331 |
[32m[20221214 14:08:19 @agent_ppo2.py:185][0m |          -0.0013 |         157.1478 |          14.1161 |
[32m[20221214 14:08:19 @agent_ppo2.py:185][0m |          -0.0035 |         155.3965 |          14.1221 |
[32m[20221214 14:08:19 @agent_ppo2.py:185][0m |          -0.0037 |         154.4218 |          14.0682 |
[32m[20221214 14:08:19 @agent_ppo2.py:185][0m |          -0.0018 |         153.1975 |          14.0557 |
[32m[20221214 14:08:19 @agent_ppo2.py:185][0m |          -0.0016 |         152.7897 |          14.0455 |
[32m[20221214 14:08:19 @agent_ppo2.py:185][0m |          -0.0007 |         151.7527 |          14.0394 |
[32m[20221214 14:08:19 @agent_ppo2.py:185][0m |          -0.0019 |         150.3302 |          14.0206 |
[32m[20221214 14:08:19 @agent_ppo2.py:185][0m |          -0.0039 |         148.4318 |          14.0048 |
[32m[20221214 14:08:19 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:08:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 769.51
[32m[20221214 14:08:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.36
[32m[20221214 14:08:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.06
[32m[20221214 14:08:20 @agent_ppo2.py:143][0m Total time:      10.29 min
[32m[20221214 14:08:20 @agent_ppo2.py:145][0m 935936 total steps have happened
[32m[20221214 14:08:20 @agent_ppo2.py:121][0m #------------------------ Iteration 457 --------------------------#
[32m[20221214 14:08:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:20 @agent_ppo2.py:185][0m |           0.0043 |         179.8394 |          13.9194 |
[32m[20221214 14:08:20 @agent_ppo2.py:185][0m |           0.0004 |         170.6813 |          13.8825 |
[32m[20221214 14:08:20 @agent_ppo2.py:185][0m |           0.0079 |         176.2484 |          13.8801 |
[32m[20221214 14:08:20 @agent_ppo2.py:185][0m |          -0.0025 |         165.9914 |          13.8402 |
[32m[20221214 14:08:20 @agent_ppo2.py:185][0m |          -0.0027 |         163.8993 |          13.8213 |
[32m[20221214 14:08:20 @agent_ppo2.py:185][0m |          -0.0036 |         162.0627 |          13.7906 |
[32m[20221214 14:08:20 @agent_ppo2.py:185][0m |          -0.0026 |         161.5472 |          13.7756 |
[32m[20221214 14:08:21 @agent_ppo2.py:185][0m |          -0.0032 |         160.7109 |          13.7483 |
[32m[20221214 14:08:21 @agent_ppo2.py:185][0m |          -0.0026 |         160.2664 |          13.7281 |
[32m[20221214 14:08:21 @agent_ppo2.py:185][0m |          -0.0034 |         159.5532 |          13.7091 |
[32m[20221214 14:08:21 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:08:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.93
[32m[20221214 14:08:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.10
[32m[20221214 14:08:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.25
[32m[20221214 14:08:21 @agent_ppo2.py:143][0m Total time:      10.31 min
[32m[20221214 14:08:21 @agent_ppo2.py:145][0m 937984 total steps have happened
[32m[20221214 14:08:21 @agent_ppo2.py:121][0m #------------------------ Iteration 458 --------------------------#
[32m[20221214 14:08:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:21 @agent_ppo2.py:185][0m |          -0.0008 |         164.9588 |          13.7193 |
[32m[20221214 14:08:21 @agent_ppo2.py:185][0m |           0.0014 |         160.9560 |          13.6990 |
[32m[20221214 14:08:21 @agent_ppo2.py:185][0m |          -0.0027 |         156.8087 |          13.7133 |
[32m[20221214 14:08:22 @agent_ppo2.py:185][0m |          -0.0030 |         155.8370 |          13.7070 |
[32m[20221214 14:08:22 @agent_ppo2.py:185][0m |          -0.0004 |         155.3084 |          13.7144 |
[32m[20221214 14:08:22 @agent_ppo2.py:185][0m |          -0.0008 |         154.9523 |          13.7152 |
[32m[20221214 14:08:22 @agent_ppo2.py:185][0m |          -0.0003 |         154.5531 |          13.7127 |
[32m[20221214 14:08:22 @agent_ppo2.py:185][0m |           0.0089 |         158.3255 |          13.7139 |
[32m[20221214 14:08:22 @agent_ppo2.py:185][0m |          -0.0028 |         153.4782 |          13.7144 |
[32m[20221214 14:08:22 @agent_ppo2.py:185][0m |          -0.0036 |         153.0382 |          13.7257 |
[32m[20221214 14:08:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:08:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.22
[32m[20221214 14:08:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.26
[32m[20221214 14:08:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.67
[32m[20221214 14:08:22 @agent_ppo2.py:143][0m Total time:      10.34 min
[32m[20221214 14:08:22 @agent_ppo2.py:145][0m 940032 total steps have happened
[32m[20221214 14:08:22 @agent_ppo2.py:121][0m #------------------------ Iteration 459 --------------------------#
[32m[20221214 14:08:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:23 @agent_ppo2.py:185][0m |           0.0077 |         196.6298 |          13.6038 |
[32m[20221214 14:08:23 @agent_ppo2.py:185][0m |          -0.0024 |         165.4423 |          13.5999 |
[32m[20221214 14:08:23 @agent_ppo2.py:185][0m |           0.0038 |         166.2946 |          13.5742 |
[32m[20221214 14:08:23 @agent_ppo2.py:185][0m |          -0.0016 |         157.3630 |          13.5506 |
[32m[20221214 14:08:23 @agent_ppo2.py:185][0m |           0.0065 |         167.6394 |          13.5374 |
[32m[20221214 14:08:23 @agent_ppo2.py:185][0m |          -0.0043 |         154.5579 |          13.5082 |
[32m[20221214 14:08:23 @agent_ppo2.py:185][0m |          -0.0023 |         153.1364 |          13.5101 |
[32m[20221214 14:08:23 @agent_ppo2.py:185][0m |           0.0004 |         151.1757 |          13.5115 |
[32m[20221214 14:08:23 @agent_ppo2.py:185][0m |          -0.0036 |         149.4798 |          13.4948 |
[32m[20221214 14:08:23 @agent_ppo2.py:185][0m |          -0.0027 |         148.5008 |          13.4833 |
[32m[20221214 14:08:23 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:08:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.90
[32m[20221214 14:08:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.51
[32m[20221214 14:08:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.42
[32m[20221214 14:08:24 @agent_ppo2.py:143][0m Total time:      10.36 min
[32m[20221214 14:08:24 @agent_ppo2.py:145][0m 942080 total steps have happened
[32m[20221214 14:08:24 @agent_ppo2.py:121][0m #------------------------ Iteration 460 --------------------------#
[32m[20221214 14:08:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:08:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:24 @agent_ppo2.py:185][0m |           0.0006 |         138.5845 |          13.5834 |
[32m[20221214 14:08:24 @agent_ppo2.py:185][0m |          -0.0033 |         130.9002 |          13.5744 |
[32m[20221214 14:08:24 @agent_ppo2.py:185][0m |           0.0118 |         148.4876 |          13.5702 |
[32m[20221214 14:08:24 @agent_ppo2.py:185][0m |          -0.0026 |         128.6860 |          13.5772 |
[32m[20221214 14:08:24 @agent_ppo2.py:185][0m |          -0.0053 |         126.5861 |          13.5820 |
[32m[20221214 14:08:24 @agent_ppo2.py:185][0m |          -0.0053 |         126.5413 |          13.5816 |
[32m[20221214 14:08:24 @agent_ppo2.py:185][0m |          -0.0017 |         125.6772 |          13.5814 |
[32m[20221214 14:08:25 @agent_ppo2.py:185][0m |          -0.0027 |         125.3616 |          13.5776 |
[32m[20221214 14:08:25 @agent_ppo2.py:185][0m |          -0.0087 |         124.5306 |          13.5774 |
[32m[20221214 14:08:25 @agent_ppo2.py:185][0m |           0.0169 |         143.5011 |          13.5913 |
[32m[20221214 14:08:25 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:08:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 770.95
[32m[20221214 14:08:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 784.83
[32m[20221214 14:08:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.25
[32m[20221214 14:08:25 @agent_ppo2.py:143][0m Total time:      10.38 min
[32m[20221214 14:08:25 @agent_ppo2.py:145][0m 944128 total steps have happened
[32m[20221214 14:08:25 @agent_ppo2.py:121][0m #------------------------ Iteration 461 --------------------------#
[32m[20221214 14:08:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:25 @agent_ppo2.py:185][0m |          -0.0002 |         143.1146 |          13.5392 |
[32m[20221214 14:08:25 @agent_ppo2.py:185][0m |          -0.0013 |         138.1046 |          13.5226 |
[32m[20221214 14:08:25 @agent_ppo2.py:185][0m |          -0.0021 |         136.6826 |          13.5400 |
[32m[20221214 14:08:26 @agent_ppo2.py:185][0m |           0.0025 |         134.9830 |          13.5098 |
[32m[20221214 14:08:26 @agent_ppo2.py:185][0m |          -0.0028 |         133.7780 |          13.5051 |
[32m[20221214 14:08:26 @agent_ppo2.py:185][0m |          -0.0004 |         132.1961 |          13.4985 |
[32m[20221214 14:08:26 @agent_ppo2.py:185][0m |          -0.0012 |         131.8534 |          13.4613 |
[32m[20221214 14:08:26 @agent_ppo2.py:185][0m |          -0.0023 |         131.2713 |          13.4221 |
[32m[20221214 14:08:26 @agent_ppo2.py:185][0m |          -0.0020 |         130.4751 |          13.4202 |
[32m[20221214 14:08:26 @agent_ppo2.py:185][0m |          -0.0024 |         130.3301 |          13.4073 |
[32m[20221214 14:08:26 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:08:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.42
[32m[20221214 14:08:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.98
[32m[20221214 14:08:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.38
[32m[20221214 14:08:26 @agent_ppo2.py:143][0m Total time:      10.40 min
[32m[20221214 14:08:26 @agent_ppo2.py:145][0m 946176 total steps have happened
[32m[20221214 14:08:26 @agent_ppo2.py:121][0m #------------------------ Iteration 462 --------------------------#
[32m[20221214 14:08:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:27 @agent_ppo2.py:185][0m |          -0.0013 |         118.7257 |          13.3151 |
[32m[20221214 14:08:27 @agent_ppo2.py:185][0m |          -0.0018 |         111.9885 |          13.3038 |
[32m[20221214 14:08:27 @agent_ppo2.py:185][0m |          -0.0029 |         109.6507 |          13.3036 |
[32m[20221214 14:08:27 @agent_ppo2.py:185][0m |          -0.0028 |         108.1573 |          13.3060 |
[32m[20221214 14:08:27 @agent_ppo2.py:185][0m |          -0.0030 |         106.9787 |          13.3060 |
[32m[20221214 14:08:27 @agent_ppo2.py:185][0m |          -0.0021 |         105.5387 |          13.2949 |
[32m[20221214 14:08:27 @agent_ppo2.py:185][0m |           0.0010 |         104.2199 |          13.2814 |
[32m[20221214 14:08:27 @agent_ppo2.py:185][0m |          -0.0039 |         102.9110 |          13.2935 |
[32m[20221214 14:08:27 @agent_ppo2.py:185][0m |          -0.0041 |         102.3560 |          13.2810 |
[32m[20221214 14:08:27 @agent_ppo2.py:185][0m |           0.0010 |         101.8061 |          13.2770 |
[32m[20221214 14:08:27 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:08:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.58
[32m[20221214 14:08:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.00
[32m[20221214 14:08:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.65
[32m[20221214 14:08:28 @agent_ppo2.py:143][0m Total time:      10.42 min
[32m[20221214 14:08:28 @agent_ppo2.py:145][0m 948224 total steps have happened
[32m[20221214 14:08:28 @agent_ppo2.py:121][0m #------------------------ Iteration 463 --------------------------#
[32m[20221214 14:08:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:28 @agent_ppo2.py:185][0m |           0.0031 |         141.1729 |          13.1933 |
[32m[20221214 14:08:28 @agent_ppo2.py:185][0m |          -0.0008 |         133.3125 |          13.1602 |
[32m[20221214 14:08:28 @agent_ppo2.py:185][0m |          -0.0009 |         130.9707 |          13.1354 |
[32m[20221214 14:08:28 @agent_ppo2.py:185][0m |           0.0003 |         129.3333 |          13.1075 |
[32m[20221214 14:08:28 @agent_ppo2.py:185][0m |          -0.0025 |         128.1451 |          13.0822 |
[32m[20221214 14:08:28 @agent_ppo2.py:185][0m |           0.0027 |         127.8129 |          13.0442 |
[32m[20221214 14:08:28 @agent_ppo2.py:185][0m |          -0.0013 |         126.6409 |          13.0289 |
[32m[20221214 14:08:29 @agent_ppo2.py:185][0m |          -0.0021 |         125.7720 |          12.9943 |
[32m[20221214 14:08:29 @agent_ppo2.py:185][0m |          -0.0019 |         124.5853 |          12.9643 |
[32m[20221214 14:08:29 @agent_ppo2.py:185][0m |          -0.0033 |         124.5506 |          12.9485 |
[32m[20221214 14:08:29 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:08:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.37
[32m[20221214 14:08:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.49
[32m[20221214 14:08:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.82
[32m[20221214 14:08:29 @agent_ppo2.py:143][0m Total time:      10.45 min
[32m[20221214 14:08:29 @agent_ppo2.py:145][0m 950272 total steps have happened
[32m[20221214 14:08:29 @agent_ppo2.py:121][0m #------------------------ Iteration 464 --------------------------#
[32m[20221214 14:08:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:29 @agent_ppo2.py:185][0m |           0.0002 |         143.4206 |          12.7606 |
[32m[20221214 14:08:29 @agent_ppo2.py:185][0m |           0.0099 |         155.2828 |          12.7717 |
[32m[20221214 14:08:29 @agent_ppo2.py:185][0m |          -0.0019 |         136.4702 |          12.7706 |
[32m[20221214 14:08:30 @agent_ppo2.py:185][0m |           0.0040 |         135.1093 |          12.7353 |
[32m[20221214 14:08:30 @agent_ppo2.py:185][0m |          -0.0020 |         132.7055 |          12.7446 |
[32m[20221214 14:08:30 @agent_ppo2.py:185][0m |           0.0086 |         137.6852 |          12.7387 |
[32m[20221214 14:08:30 @agent_ppo2.py:185][0m |          -0.0020 |         131.1845 |          12.7189 |
[32m[20221214 14:08:30 @agent_ppo2.py:185][0m |           0.0078 |         136.2484 |          12.7156 |
[32m[20221214 14:08:30 @agent_ppo2.py:185][0m |          -0.0021 |         129.8436 |          12.7154 |
[32m[20221214 14:08:30 @agent_ppo2.py:185][0m |          -0.0004 |         129.2907 |          12.7001 |
[32m[20221214 14:08:30 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:08:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.18
[32m[20221214 14:08:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.65
[32m[20221214 14:08:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.54
[32m[20221214 14:08:30 @agent_ppo2.py:143][0m Total time:      10.47 min
[32m[20221214 14:08:30 @agent_ppo2.py:145][0m 952320 total steps have happened
[32m[20221214 14:08:30 @agent_ppo2.py:121][0m #------------------------ Iteration 465 --------------------------#
[32m[20221214 14:08:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:31 @agent_ppo2.py:185][0m |          -0.0027 |         177.6362 |          12.9432 |
[32m[20221214 14:08:31 @agent_ppo2.py:185][0m |          -0.0029 |         166.9867 |          12.9613 |
[32m[20221214 14:08:31 @agent_ppo2.py:185][0m |          -0.0024 |         164.6297 |          12.9605 |
[32m[20221214 14:08:31 @agent_ppo2.py:185][0m |          -0.0023 |         162.5681 |          12.9572 |
[32m[20221214 14:08:31 @agent_ppo2.py:185][0m |          -0.0051 |         161.8712 |          12.9408 |
[32m[20221214 14:08:31 @agent_ppo2.py:185][0m |          -0.0047 |         160.8056 |          12.9333 |
[32m[20221214 14:08:31 @agent_ppo2.py:185][0m |          -0.0038 |         159.9481 |          12.9316 |
[32m[20221214 14:08:31 @agent_ppo2.py:185][0m |           0.0047 |         170.3213 |          12.9251 |
[32m[20221214 14:08:31 @agent_ppo2.py:185][0m |          -0.0046 |         159.5180 |          12.9225 |
[32m[20221214 14:08:31 @agent_ppo2.py:185][0m |          -0.0043 |         157.8507 |          12.9102 |
[32m[20221214 14:08:31 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:08:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.10
[32m[20221214 14:08:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.71
[32m[20221214 14:08:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.07
[32m[20221214 14:08:32 @agent_ppo2.py:143][0m Total time:      10.49 min
[32m[20221214 14:08:32 @agent_ppo2.py:145][0m 954368 total steps have happened
[32m[20221214 14:08:32 @agent_ppo2.py:121][0m #------------------------ Iteration 466 --------------------------#
[32m[20221214 14:08:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:32 @agent_ppo2.py:185][0m |          -0.0035 |         211.6018 |          12.9486 |
[32m[20221214 14:08:32 @agent_ppo2.py:185][0m |           0.0034 |         199.6721 |          12.9645 |
[32m[20221214 14:08:32 @agent_ppo2.py:185][0m |          -0.0040 |         187.9843 |          12.9611 |
[32m[20221214 14:08:32 @agent_ppo2.py:185][0m |          -0.0015 |         185.4572 |          12.9457 |
[32m[20221214 14:08:32 @agent_ppo2.py:185][0m |          -0.0041 |         183.5050 |          12.9447 |
[32m[20221214 14:08:32 @agent_ppo2.py:185][0m |          -0.0019 |         182.0481 |          12.9434 |
[32m[20221214 14:08:32 @agent_ppo2.py:185][0m |          -0.0035 |         181.1424 |          12.9383 |
[32m[20221214 14:08:33 @agent_ppo2.py:185][0m |          -0.0010 |         181.2101 |          12.9399 |
[32m[20221214 14:08:33 @agent_ppo2.py:185][0m |          -0.0044 |         179.3238 |          12.9292 |
[32m[20221214 14:08:33 @agent_ppo2.py:185][0m |          -0.0013 |         178.9730 |          12.9394 |
[32m[20221214 14:08:33 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:08:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.66
[32m[20221214 14:08:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.86
[32m[20221214 14:08:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.59
[32m[20221214 14:08:33 @agent_ppo2.py:143][0m Total time:      10.51 min
[32m[20221214 14:08:33 @agent_ppo2.py:145][0m 956416 total steps have happened
[32m[20221214 14:08:33 @agent_ppo2.py:121][0m #------------------------ Iteration 467 --------------------------#
[32m[20221214 14:08:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:33 @agent_ppo2.py:185][0m |           0.0032 |         240.1651 |          12.7903 |
[32m[20221214 14:08:33 @agent_ppo2.py:185][0m |           0.0048 |         229.4591 |          12.7681 |
[32m[20221214 14:08:33 @agent_ppo2.py:185][0m |          -0.0007 |         218.5563 |          12.7358 |
[32m[20221214 14:08:34 @agent_ppo2.py:185][0m |          -0.0018 |         216.5582 |          12.7310 |
[32m[20221214 14:08:34 @agent_ppo2.py:185][0m |          -0.0014 |         214.8594 |          12.7585 |
[32m[20221214 14:08:34 @agent_ppo2.py:185][0m |           0.0097 |         242.6034 |          12.7314 |
[32m[20221214 14:08:34 @agent_ppo2.py:185][0m |          -0.0013 |         212.8869 |          12.7685 |
[32m[20221214 14:08:34 @agent_ppo2.py:185][0m |          -0.0010 |         211.7085 |          12.7310 |
[32m[20221214 14:08:34 @agent_ppo2.py:185][0m |          -0.0036 |         211.6253 |          12.7384 |
[32m[20221214 14:08:34 @agent_ppo2.py:185][0m |           0.0020 |         213.1275 |          12.7490 |
[32m[20221214 14:08:34 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:08:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.98
[32m[20221214 14:08:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.27
[32m[20221214 14:08:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.01
[32m[20221214 14:08:34 @agent_ppo2.py:143][0m Total time:      10.54 min
[32m[20221214 14:08:34 @agent_ppo2.py:145][0m 958464 total steps have happened
[32m[20221214 14:08:34 @agent_ppo2.py:121][0m #------------------------ Iteration 468 --------------------------#
[32m[20221214 14:08:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:35 @agent_ppo2.py:185][0m |          -0.0019 |         195.0135 |          12.8336 |
[32m[20221214 14:08:35 @agent_ppo2.py:185][0m |          -0.0066 |         187.6749 |          12.7986 |
[32m[20221214 14:08:35 @agent_ppo2.py:185][0m |          -0.0069 |         185.6063 |          12.8018 |
[32m[20221214 14:08:35 @agent_ppo2.py:185][0m |          -0.0054 |         184.0440 |          12.8093 |
[32m[20221214 14:08:35 @agent_ppo2.py:185][0m |          -0.0042 |         184.2319 |          12.7974 |
[32m[20221214 14:08:35 @agent_ppo2.py:185][0m |          -0.0056 |         183.1624 |          12.7964 |
[32m[20221214 14:08:35 @agent_ppo2.py:185][0m |          -0.0074 |         182.2841 |          12.7949 |
[32m[20221214 14:08:35 @agent_ppo2.py:185][0m |          -0.0034 |         181.6216 |          12.7955 |
[32m[20221214 14:08:35 @agent_ppo2.py:185][0m |          -0.0057 |         181.5889 |          12.7802 |
[32m[20221214 14:08:35 @agent_ppo2.py:185][0m |           0.0071 |         201.8261 |          12.7975 |
[32m[20221214 14:08:35 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:08:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.86
[32m[20221214 14:08:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.51
[32m[20221214 14:08:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.44
[32m[20221214 14:08:36 @agent_ppo2.py:143][0m Total time:      10.56 min
[32m[20221214 14:08:36 @agent_ppo2.py:145][0m 960512 total steps have happened
[32m[20221214 14:08:36 @agent_ppo2.py:121][0m #------------------------ Iteration 469 --------------------------#
[32m[20221214 14:08:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:36 @agent_ppo2.py:185][0m |          -0.0016 |         267.6512 |          12.6632 |
[32m[20221214 14:08:36 @agent_ppo2.py:185][0m |          -0.0018 |         256.7321 |          12.6648 |
[32m[20221214 14:08:36 @agent_ppo2.py:185][0m |          -0.0014 |         250.5768 |          12.6739 |
[32m[20221214 14:08:36 @agent_ppo2.py:185][0m |          -0.0016 |         247.6694 |          12.6884 |
[32m[20221214 14:08:36 @agent_ppo2.py:185][0m |          -0.0021 |         245.5857 |          12.7140 |
[32m[20221214 14:08:36 @agent_ppo2.py:185][0m |          -0.0015 |         243.0832 |          12.7196 |
[32m[20221214 14:08:36 @agent_ppo2.py:185][0m |           0.0093 |         264.0949 |          12.7017 |
[32m[20221214 14:08:37 @agent_ppo2.py:185][0m |          -0.0009 |         239.9542 |          12.7432 |
[32m[20221214 14:08:37 @agent_ppo2.py:185][0m |          -0.0011 |         238.5369 |          12.7564 |
[32m[20221214 14:08:37 @agent_ppo2.py:185][0m |          -0.0023 |         237.4129 |          12.7648 |
[32m[20221214 14:08:37 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:08:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.43
[32m[20221214 14:08:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.82
[32m[20221214 14:08:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.55
[32m[20221214 14:08:37 @agent_ppo2.py:143][0m Total time:      10.58 min
[32m[20221214 14:08:37 @agent_ppo2.py:145][0m 962560 total steps have happened
[32m[20221214 14:08:37 @agent_ppo2.py:121][0m #------------------------ Iteration 470 --------------------------#
[32m[20221214 14:08:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:08:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:37 @agent_ppo2.py:185][0m |          -0.0017 |         197.9721 |          12.9816 |
[32m[20221214 14:08:37 @agent_ppo2.py:185][0m |          -0.0011 |         188.1982 |          12.9878 |
[32m[20221214 14:08:37 @agent_ppo2.py:185][0m |          -0.0005 |         184.4692 |          12.9868 |
[32m[20221214 14:08:38 @agent_ppo2.py:185][0m |           0.0005 |         183.5992 |          12.9851 |
[32m[20221214 14:08:38 @agent_ppo2.py:185][0m |          -0.0013 |         182.1940 |          12.9694 |
[32m[20221214 14:08:38 @agent_ppo2.py:185][0m |          -0.0023 |         182.5683 |          12.9797 |
[32m[20221214 14:08:38 @agent_ppo2.py:185][0m |          -0.0024 |         180.6633 |          12.9665 |
[32m[20221214 14:08:38 @agent_ppo2.py:185][0m |           0.0075 |         184.0157 |          12.9685 |
[32m[20221214 14:08:38 @agent_ppo2.py:185][0m |          -0.0028 |         178.7441 |          12.9710 |
[32m[20221214 14:08:38 @agent_ppo2.py:185][0m |           0.0051 |         182.0180 |          12.9700 |
[32m[20221214 14:08:38 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:08:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.07
[32m[20221214 14:08:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.24
[32m[20221214 14:08:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.21
[32m[20221214 14:08:38 @agent_ppo2.py:143][0m Total time:      10.60 min
[32m[20221214 14:08:38 @agent_ppo2.py:145][0m 964608 total steps have happened
[32m[20221214 14:08:38 @agent_ppo2.py:121][0m #------------------------ Iteration 471 --------------------------#
[32m[20221214 14:08:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:39 @agent_ppo2.py:185][0m |           0.0003 |         251.7333 |          12.9331 |
[32m[20221214 14:08:39 @agent_ppo2.py:185][0m |          -0.0007 |         243.6099 |          12.9180 |
[32m[20221214 14:08:39 @agent_ppo2.py:185][0m |           0.0054 |         250.1949 |          12.9262 |
[32m[20221214 14:08:39 @agent_ppo2.py:185][0m |          -0.0027 |         238.4852 |          12.9130 |
[32m[20221214 14:08:39 @agent_ppo2.py:185][0m |          -0.0024 |         236.2428 |          12.8928 |
[32m[20221214 14:08:39 @agent_ppo2.py:185][0m |          -0.0008 |         233.8389 |          12.8929 |
[32m[20221214 14:08:39 @agent_ppo2.py:185][0m |          -0.0020 |         232.3343 |          12.8741 |
[32m[20221214 14:08:39 @agent_ppo2.py:185][0m |          -0.0029 |         230.4856 |          12.8570 |
[32m[20221214 14:08:39 @agent_ppo2.py:185][0m |          -0.0029 |         228.1839 |          12.8414 |
[32m[20221214 14:08:39 @agent_ppo2.py:185][0m |          -0.0007 |         226.9228 |          12.8534 |
[32m[20221214 14:08:39 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:08:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.54
[32m[20221214 14:08:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.03
[32m[20221214 14:08:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.86
[32m[20221214 14:08:40 @agent_ppo2.py:143][0m Total time:      10.63 min
[32m[20221214 14:08:40 @agent_ppo2.py:145][0m 966656 total steps have happened
[32m[20221214 14:08:40 @agent_ppo2.py:121][0m #------------------------ Iteration 472 --------------------------#
[32m[20221214 14:08:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:40 @agent_ppo2.py:185][0m |          -0.0024 |         187.1663 |          12.6265 |
[32m[20221214 14:08:40 @agent_ppo2.py:185][0m |          -0.0027 |         158.7378 |          12.6468 |
[32m[20221214 14:08:40 @agent_ppo2.py:185][0m |           0.0123 |         169.6155 |          12.6489 |
[32m[20221214 14:08:40 @agent_ppo2.py:185][0m |          -0.0030 |         145.2593 |          12.6599 |
[32m[20221214 14:08:40 @agent_ppo2.py:185][0m |          -0.0036 |         139.9373 |          12.6753 |
[32m[20221214 14:08:40 @agent_ppo2.py:185][0m |          -0.0039 |         134.3266 |          12.6663 |
[32m[20221214 14:08:41 @agent_ppo2.py:185][0m |          -0.0028 |         129.0930 |          12.6937 |
[32m[20221214 14:08:41 @agent_ppo2.py:185][0m |          -0.0035 |         126.1749 |          12.7102 |
[32m[20221214 14:08:41 @agent_ppo2.py:185][0m |          -0.0035 |         124.2908 |          12.7260 |
[32m[20221214 14:08:41 @agent_ppo2.py:185][0m |           0.0074 |         133.5782 |          12.7272 |
[32m[20221214 14:08:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:08:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.37
[32m[20221214 14:08:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.89
[32m[20221214 14:08:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.59
[32m[20221214 14:08:41 @agent_ppo2.py:143][0m Total time:      10.65 min
[32m[20221214 14:08:41 @agent_ppo2.py:145][0m 968704 total steps have happened
[32m[20221214 14:08:41 @agent_ppo2.py:121][0m #------------------------ Iteration 473 --------------------------#
[32m[20221214 14:08:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:41 @agent_ppo2.py:185][0m |           0.0039 |         119.6590 |          13.0303 |
[32m[20221214 14:08:41 @agent_ppo2.py:185][0m |          -0.0038 |          89.6146 |          13.0187 |
[32m[20221214 14:08:41 @agent_ppo2.py:185][0m |          -0.0037 |          78.4599 |          13.0003 |
[32m[20221214 14:08:42 @agent_ppo2.py:185][0m |          -0.0015 |          72.5263 |          12.9878 |
[32m[20221214 14:08:42 @agent_ppo2.py:185][0m |          -0.0051 |          69.4980 |          12.9629 |
[32m[20221214 14:08:42 @agent_ppo2.py:185][0m |          -0.0028 |          67.8639 |          12.9517 |
[32m[20221214 14:08:42 @agent_ppo2.py:185][0m |          -0.0039 |          65.4236 |          12.9485 |
[32m[20221214 14:08:42 @agent_ppo2.py:185][0m |          -0.0064 |          63.4483 |          12.9226 |
[32m[20221214 14:08:42 @agent_ppo2.py:185][0m |          -0.0005 |          62.5619 |          12.9064 |
[32m[20221214 14:08:42 @agent_ppo2.py:185][0m |          -0.0028 |          61.8256 |          12.8925 |
[32m[20221214 14:08:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:08:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.29
[32m[20221214 14:08:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.09
[32m[20221214 14:08:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 738.82
[32m[20221214 14:08:42 @agent_ppo2.py:143][0m Total time:      10.67 min
[32m[20221214 14:08:42 @agent_ppo2.py:145][0m 970752 total steps have happened
[32m[20221214 14:08:42 @agent_ppo2.py:121][0m #------------------------ Iteration 474 --------------------------#
[32m[20221214 14:08:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:08:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:43 @agent_ppo2.py:185][0m |          -0.0008 |         181.3330 |          12.7590 |
[32m[20221214 14:08:43 @agent_ppo2.py:185][0m |           0.0095 |         172.4009 |          12.7811 |
[32m[20221214 14:08:43 @agent_ppo2.py:185][0m |          -0.0021 |         164.1627 |          12.7660 |
[32m[20221214 14:08:43 @agent_ppo2.py:185][0m |          -0.0018 |         161.6343 |          12.8037 |
[32m[20221214 14:08:43 @agent_ppo2.py:185][0m |          -0.0030 |         160.6392 |          12.8279 |
[32m[20221214 14:08:43 @agent_ppo2.py:185][0m |           0.0002 |         159.4413 |          12.8476 |
[32m[20221214 14:08:43 @agent_ppo2.py:185][0m |          -0.0027 |         157.5444 |          12.8653 |
[32m[20221214 14:08:43 @agent_ppo2.py:185][0m |          -0.0014 |         157.6344 |          12.8897 |
[32m[20221214 14:08:43 @agent_ppo2.py:185][0m |          -0.0026 |         156.0567 |          12.9122 |
[32m[20221214 14:08:43 @agent_ppo2.py:185][0m |          -0.0014 |         155.6586 |          12.9181 |
[32m[20221214 14:08:43 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:08:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 770.14
[32m[20221214 14:08:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.41
[32m[20221214 14:08:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 735.60
[32m[20221214 14:08:44 @agent_ppo2.py:143][0m Total time:      10.69 min
[32m[20221214 14:08:44 @agent_ppo2.py:145][0m 972800 total steps have happened
[32m[20221214 14:08:44 @agent_ppo2.py:121][0m #------------------------ Iteration 475 --------------------------#
[32m[20221214 14:08:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:08:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:44 @agent_ppo2.py:185][0m |          -0.0040 |         164.8225 |          12.9703 |
[32m[20221214 14:08:44 @agent_ppo2.py:185][0m |          -0.0029 |         160.0652 |          13.0092 |
[32m[20221214 14:08:44 @agent_ppo2.py:185][0m |          -0.0044 |         157.7625 |          13.0161 |
[32m[20221214 14:08:44 @agent_ppo2.py:185][0m |          -0.0009 |         155.6251 |          13.0597 |
[32m[20221214 14:08:44 @agent_ppo2.py:185][0m |          -0.0046 |         154.4762 |          13.0656 |
[32m[20221214 14:08:44 @agent_ppo2.py:185][0m |           0.0128 |         185.5470 |          13.0815 |
[32m[20221214 14:08:44 @agent_ppo2.py:185][0m |           0.0062 |         156.8147 |          13.1090 |
[32m[20221214 14:08:44 @agent_ppo2.py:185][0m |          -0.0038 |         151.9711 |          13.1141 |
[32m[20221214 14:08:45 @agent_ppo2.py:185][0m |          -0.0008 |         150.3859 |          13.1205 |
[32m[20221214 14:08:45 @agent_ppo2.py:185][0m |          -0.0027 |         149.3747 |          13.1358 |
[32m[20221214 14:08:45 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:08:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.07
[32m[20221214 14:08:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.98
[32m[20221214 14:08:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.21
[32m[20221214 14:08:45 @agent_ppo2.py:143][0m Total time:      10.71 min
[32m[20221214 14:08:45 @agent_ppo2.py:145][0m 974848 total steps have happened
[32m[20221214 14:08:45 @agent_ppo2.py:121][0m #------------------------ Iteration 476 --------------------------#
[32m[20221214 14:08:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:08:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:45 @agent_ppo2.py:185][0m |          -0.0021 |         228.8611 |          13.0006 |
[32m[20221214 14:08:45 @agent_ppo2.py:185][0m |           0.0015 |         215.4794 |          13.0067 |
[32m[20221214 14:08:45 @agent_ppo2.py:185][0m |           0.0035 |         212.5121 |          12.9781 |
[32m[20221214 14:08:45 @agent_ppo2.py:185][0m |          -0.0027 |         204.2157 |          12.9803 |
[32m[20221214 14:08:45 @agent_ppo2.py:185][0m |          -0.0019 |         201.9314 |          12.9825 |
[32m[20221214 14:08:46 @agent_ppo2.py:185][0m |          -0.0013 |         200.9694 |          12.9620 |
[32m[20221214 14:08:46 @agent_ppo2.py:185][0m |           0.0084 |         209.7365 |          12.9574 |
[32m[20221214 14:08:46 @agent_ppo2.py:185][0m |          -0.0052 |         197.6880 |          12.9641 |
[32m[20221214 14:08:46 @agent_ppo2.py:185][0m |          -0.0026 |         196.6762 |          12.9502 |
[32m[20221214 14:08:46 @agent_ppo2.py:185][0m |          -0.0034 |         195.3636 |          12.9437 |
[32m[20221214 14:08:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:08:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.67
[32m[20221214 14:08:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.59
[32m[20221214 14:08:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.56
[32m[20221214 14:08:46 @agent_ppo2.py:143][0m Total time:      10.73 min
[32m[20221214 14:08:46 @agent_ppo2.py:145][0m 976896 total steps have happened
[32m[20221214 14:08:46 @agent_ppo2.py:121][0m #------------------------ Iteration 477 --------------------------#
[32m[20221214 14:08:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:08:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:46 @agent_ppo2.py:185][0m |           0.0065 |         277.2711 |          13.0116 |
[32m[20221214 14:08:46 @agent_ppo2.py:185][0m |          -0.0037 |         260.1434 |          12.9946 |
[32m[20221214 14:08:47 @agent_ppo2.py:185][0m |          -0.0042 |         258.6320 |          12.9965 |
[32m[20221214 14:08:47 @agent_ppo2.py:185][0m |          -0.0046 |         257.7354 |          12.9813 |
[32m[20221214 14:08:47 @agent_ppo2.py:185][0m |          -0.0009 |         258.4084 |          12.9867 |
[32m[20221214 14:08:47 @agent_ppo2.py:185][0m |          -0.0053 |         256.3254 |          12.9794 |
[32m[20221214 14:08:47 @agent_ppo2.py:185][0m |          -0.0046 |         255.8496 |          12.9608 |
[32m[20221214 14:08:47 @agent_ppo2.py:185][0m |           0.0065 |         279.4765 |          12.9553 |
[32m[20221214 14:08:47 @agent_ppo2.py:185][0m |          -0.0032 |         254.2571 |          12.9741 |
[32m[20221214 14:08:47 @agent_ppo2.py:185][0m |          -0.0019 |         254.7157 |          12.9509 |
[32m[20221214 14:08:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:08:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.23
[32m[20221214 14:08:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.00
[32m[20221214 14:08:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 767.04
[32m[20221214 14:08:47 @agent_ppo2.py:143][0m Total time:      10.75 min
[32m[20221214 14:08:47 @agent_ppo2.py:145][0m 978944 total steps have happened
[32m[20221214 14:08:47 @agent_ppo2.py:121][0m #------------------------ Iteration 478 --------------------------#
[32m[20221214 14:08:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:08:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:48 @agent_ppo2.py:185][0m |           0.0009 |         224.5849 |          12.8789 |
[32m[20221214 14:08:48 @agent_ppo2.py:185][0m |           0.0037 |         217.4807 |          12.8626 |
[32m[20221214 14:08:48 @agent_ppo2.py:185][0m |          -0.0016 |         213.2758 |          12.8302 |
[32m[20221214 14:08:48 @agent_ppo2.py:185][0m |          -0.0021 |         211.0759 |          12.8146 |
[32m[20221214 14:08:48 @agent_ppo2.py:185][0m |          -0.0027 |         208.9222 |          12.7925 |
[32m[20221214 14:08:48 @agent_ppo2.py:185][0m |           0.0003 |         207.8318 |          12.7888 |
[32m[20221214 14:08:48 @agent_ppo2.py:185][0m |          -0.0038 |         206.6827 |          12.8104 |
[32m[20221214 14:08:48 @agent_ppo2.py:185][0m |          -0.0024 |         206.2172 |          12.7667 |
[32m[20221214 14:08:48 @agent_ppo2.py:185][0m |          -0.0042 |         205.3886 |          12.7675 |
[32m[20221214 14:08:48 @agent_ppo2.py:185][0m |          -0.0010 |         205.1109 |          12.7625 |
[32m[20221214 14:08:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:08:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 728.91
[32m[20221214 14:08:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.51
[32m[20221214 14:08:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 739.13
[32m[20221214 14:08:49 @agent_ppo2.py:143][0m Total time:      10.77 min
[32m[20221214 14:08:49 @agent_ppo2.py:145][0m 980992 total steps have happened
[32m[20221214 14:08:49 @agent_ppo2.py:121][0m #------------------------ Iteration 479 --------------------------#
[32m[20221214 14:08:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:49 @agent_ppo2.py:185][0m |          -0.0013 |         248.5690 |          12.7848 |
[32m[20221214 14:08:49 @agent_ppo2.py:185][0m |          -0.0032 |         245.6272 |          12.7721 |
[32m[20221214 14:08:49 @agent_ppo2.py:185][0m |          -0.0038 |         244.7692 |          12.7413 |
[32m[20221214 14:08:49 @agent_ppo2.py:185][0m |          -0.0023 |         244.7623 |          12.7280 |
[32m[20221214 14:08:49 @agent_ppo2.py:185][0m |           0.0030 |         251.7193 |          12.7183 |
[32m[20221214 14:08:49 @agent_ppo2.py:185][0m |          -0.0040 |         243.7409 |          12.7167 |
[32m[20221214 14:08:49 @agent_ppo2.py:185][0m |          -0.0036 |         243.0913 |          12.7194 |
[32m[20221214 14:08:49 @agent_ppo2.py:185][0m |          -0.0010 |         244.4690 |          12.6935 |
[32m[20221214 14:08:50 @agent_ppo2.py:185][0m |           0.0033 |         248.5055 |          12.7090 |
[32m[20221214 14:08:50 @agent_ppo2.py:185][0m |          -0.0039 |         242.3334 |          12.7058 |
[32m[20221214 14:08:50 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:08:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 747.51
[32m[20221214 14:08:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.74
[32m[20221214 14:08:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 645.08
[32m[20221214 14:08:50 @agent_ppo2.py:143][0m Total time:      10.80 min
[32m[20221214 14:08:50 @agent_ppo2.py:145][0m 983040 total steps have happened
[32m[20221214 14:08:50 @agent_ppo2.py:121][0m #------------------------ Iteration 480 --------------------------#
[32m[20221214 14:08:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:08:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:50 @agent_ppo2.py:185][0m |          -0.0036 |         220.0568 |          12.7748 |
[32m[20221214 14:08:50 @agent_ppo2.py:185][0m |           0.0097 |         231.0681 |          12.7872 |
[32m[20221214 14:08:50 @agent_ppo2.py:185][0m |          -0.0038 |         209.9993 |          12.8017 |
[32m[20221214 14:08:50 @agent_ppo2.py:185][0m |          -0.0033 |         207.5056 |          12.8062 |
[32m[20221214 14:08:51 @agent_ppo2.py:185][0m |           0.0108 |         230.9178 |          12.7848 |
[32m[20221214 14:08:51 @agent_ppo2.py:185][0m |          -0.0009 |         205.3763 |          12.8008 |
[32m[20221214 14:08:51 @agent_ppo2.py:185][0m |          -0.0021 |         204.7379 |          12.8198 |
[32m[20221214 14:08:51 @agent_ppo2.py:185][0m |           0.0017 |         205.9332 |          12.8201 |
[32m[20221214 14:08:51 @agent_ppo2.py:185][0m |           0.0008 |         203.8083 |          12.8284 |
[32m[20221214 14:08:51 @agent_ppo2.py:185][0m |          -0.0035 |         202.0103 |          12.8383 |
[32m[20221214 14:08:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:08:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 715.81
[32m[20221214 14:08:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.26
[32m[20221214 14:08:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 693.97
[32m[20221214 14:08:51 @agent_ppo2.py:143][0m Total time:      10.82 min
[32m[20221214 14:08:51 @agent_ppo2.py:145][0m 985088 total steps have happened
[32m[20221214 14:08:51 @agent_ppo2.py:121][0m #------------------------ Iteration 481 --------------------------#
[32m[20221214 14:08:51 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:08:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:52 @agent_ppo2.py:185][0m |          -0.0020 |         240.1965 |          12.8492 |
[32m[20221214 14:08:52 @agent_ppo2.py:185][0m |          -0.0031 |         231.6594 |          12.8308 |
[32m[20221214 14:08:52 @agent_ppo2.py:185][0m |          -0.0044 |         228.0617 |          12.8275 |
[32m[20221214 14:08:52 @agent_ppo2.py:185][0m |          -0.0035 |         225.9622 |          12.8290 |
[32m[20221214 14:08:52 @agent_ppo2.py:185][0m |          -0.0024 |         224.1893 |          12.8017 |
[32m[20221214 14:08:52 @agent_ppo2.py:185][0m |          -0.0079 |         222.6598 |          12.8006 |
[32m[20221214 14:08:52 @agent_ppo2.py:185][0m |          -0.0019 |         221.8574 |          12.7949 |
[32m[20221214 14:08:52 @agent_ppo2.py:185][0m |          -0.0028 |         220.5923 |          12.7732 |
[32m[20221214 14:08:52 @agent_ppo2.py:185][0m |          -0.0029 |         219.9414 |          12.7567 |
[32m[20221214 14:08:52 @agent_ppo2.py:185][0m |          -0.0037 |         219.2637 |          12.7451 |
[32m[20221214 14:08:52 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 14:08:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 683.31
[32m[20221214 14:08:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 736.17
[32m[20221214 14:08:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 751.07
[32m[20221214 14:08:53 @agent_ppo2.py:143][0m Total time:      10.84 min
[32m[20221214 14:08:53 @agent_ppo2.py:145][0m 987136 total steps have happened
[32m[20221214 14:08:53 @agent_ppo2.py:121][0m #------------------------ Iteration 482 --------------------------#
[32m[20221214 14:08:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:53 @agent_ppo2.py:185][0m |          -0.0017 |         224.1719 |          12.5971 |
[32m[20221214 14:08:53 @agent_ppo2.py:185][0m |           0.0085 |         231.3137 |          12.6289 |
[32m[20221214 14:08:53 @agent_ppo2.py:185][0m |          -0.0039 |         217.8710 |          12.6176 |
[32m[20221214 14:08:53 @agent_ppo2.py:185][0m |           0.0015 |         218.1983 |          12.6185 |
[32m[20221214 14:08:53 @agent_ppo2.py:185][0m |           0.0131 |         247.3642 |          12.5884 |
[32m[20221214 14:08:53 @agent_ppo2.py:185][0m |          -0.0014 |         215.8524 |          12.5186 |
[32m[20221214 14:08:54 @agent_ppo2.py:185][0m |          -0.0030 |         214.9316 |          12.5247 |
[32m[20221214 14:08:54 @agent_ppo2.py:185][0m |          -0.0030 |         214.6782 |          12.5434 |
[32m[20221214 14:08:54 @agent_ppo2.py:185][0m |          -0.0032 |         214.7704 |          12.5330 |
[32m[20221214 14:08:54 @agent_ppo2.py:185][0m |           0.0031 |         216.2292 |          12.5057 |
[32m[20221214 14:08:54 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:08:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 736.13
[32m[20221214 14:08:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.56
[32m[20221214 14:08:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 715.91
[32m[20221214 14:08:54 @agent_ppo2.py:143][0m Total time:      10.86 min
[32m[20221214 14:08:54 @agent_ppo2.py:145][0m 989184 total steps have happened
[32m[20221214 14:08:54 @agent_ppo2.py:121][0m #------------------------ Iteration 483 --------------------------#
[32m[20221214 14:08:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:54 @agent_ppo2.py:185][0m |           0.0013 |         213.8761 |          12.7696 |
[32m[20221214 14:08:54 @agent_ppo2.py:185][0m |          -0.0027 |         209.0443 |          12.7644 |
[32m[20221214 14:08:54 @agent_ppo2.py:185][0m |          -0.0024 |         207.5670 |          12.7447 |
[32m[20221214 14:08:55 @agent_ppo2.py:185][0m |          -0.0008 |         206.7611 |          12.7311 |
[32m[20221214 14:08:55 @agent_ppo2.py:185][0m |           0.0025 |         208.9080 |          12.7229 |
[32m[20221214 14:08:55 @agent_ppo2.py:185][0m |          -0.0019 |         205.5994 |          12.7079 |
[32m[20221214 14:08:55 @agent_ppo2.py:185][0m |          -0.0028 |         205.3878 |          12.7109 |
[32m[20221214 14:08:55 @agent_ppo2.py:185][0m |          -0.0017 |         204.7000 |          12.7039 |
[32m[20221214 14:08:55 @agent_ppo2.py:185][0m |          -0.0034 |         204.6761 |          12.7017 |
[32m[20221214 14:08:55 @agent_ppo2.py:185][0m |          -0.0023 |         204.3893 |          12.6824 |
[32m[20221214 14:08:55 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:08:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 712.72
[32m[20221214 14:08:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.39
[32m[20221214 14:08:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 724.26
[32m[20221214 14:08:55 @agent_ppo2.py:143][0m Total time:      10.89 min
[32m[20221214 14:08:55 @agent_ppo2.py:145][0m 991232 total steps have happened
[32m[20221214 14:08:55 @agent_ppo2.py:121][0m #------------------------ Iteration 484 --------------------------#
[32m[20221214 14:08:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:56 @agent_ppo2.py:185][0m |           0.0213 |         238.5513 |          12.1690 |
[32m[20221214 14:08:56 @agent_ppo2.py:185][0m |          -0.0017 |         198.3821 |          12.1708 |
[32m[20221214 14:08:56 @agent_ppo2.py:185][0m |          -0.0019 |         196.9711 |          12.1697 |
[32m[20221214 14:08:56 @agent_ppo2.py:185][0m |          -0.0021 |         196.2562 |          12.1723 |
[32m[20221214 14:08:56 @agent_ppo2.py:185][0m |          -0.0001 |         195.3995 |          12.1648 |
[32m[20221214 14:08:56 @agent_ppo2.py:185][0m |          -0.0026 |         195.0134 |          12.1471 |
[32m[20221214 14:08:56 @agent_ppo2.py:185][0m |           0.0147 |         222.0054 |          12.1539 |
[32m[20221214 14:08:56 @agent_ppo2.py:185][0m |          -0.0016 |         194.1071 |          12.1414 |
[32m[20221214 14:08:56 @agent_ppo2.py:185][0m |          -0.0022 |         193.2797 |          12.1645 |
[32m[20221214 14:08:57 @agent_ppo2.py:185][0m |          -0.0017 |         193.0224 |          12.1361 |
[32m[20221214 14:08:57 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:08:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 707.32
[32m[20221214 14:08:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 728.83
[32m[20221214 14:08:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 734.29
[32m[20221214 14:08:57 @agent_ppo2.py:143][0m Total time:      10.91 min
[32m[20221214 14:08:57 @agent_ppo2.py:145][0m 993280 total steps have happened
[32m[20221214 14:08:57 @agent_ppo2.py:121][0m #------------------------ Iteration 485 --------------------------#
[32m[20221214 14:08:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:57 @agent_ppo2.py:185][0m |          -0.0032 |         159.5747 |          12.4685 |
[32m[20221214 14:08:57 @agent_ppo2.py:185][0m |           0.0036 |         158.6906 |          12.4533 |
[32m[20221214 14:08:57 @agent_ppo2.py:185][0m |          -0.0036 |         153.9606 |          12.4499 |
[32m[20221214 14:08:57 @agent_ppo2.py:185][0m |           0.0015 |         153.8465 |          12.4709 |
[32m[20221214 14:08:57 @agent_ppo2.py:185][0m |          -0.0042 |         152.3510 |          12.4666 |
[32m[20221214 14:08:57 @agent_ppo2.py:185][0m |           0.0049 |         160.2027 |          12.4726 |
[32m[20221214 14:08:58 @agent_ppo2.py:185][0m |          -0.0031 |         151.3367 |          12.5056 |
[32m[20221214 14:08:58 @agent_ppo2.py:185][0m |          -0.0041 |         151.1032 |          12.5025 |
[32m[20221214 14:08:58 @agent_ppo2.py:185][0m |          -0.0034 |         150.7284 |          12.4995 |
[32m[20221214 14:08:58 @agent_ppo2.py:185][0m |          -0.0048 |         150.3399 |          12.4831 |
[32m[20221214 14:08:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:08:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 688.37
[32m[20221214 14:08:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 758.96
[32m[20221214 14:08:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 763.58
[32m[20221214 14:08:58 @agent_ppo2.py:143][0m Total time:      10.93 min
[32m[20221214 14:08:58 @agent_ppo2.py:145][0m 995328 total steps have happened
[32m[20221214 14:08:58 @agent_ppo2.py:121][0m #------------------------ Iteration 486 --------------------------#
[32m[20221214 14:08:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:08:58 @agent_ppo2.py:185][0m |          -0.0003 |         174.5736 |          12.4520 |
[32m[20221214 14:08:58 @agent_ppo2.py:185][0m |          -0.0019 |         163.1643 |          12.3994 |
[32m[20221214 14:08:58 @agent_ppo2.py:185][0m |          -0.0015 |         158.6235 |          12.3996 |
[32m[20221214 14:08:59 @agent_ppo2.py:185][0m |          -0.0023 |         154.5173 |          12.3393 |
[32m[20221214 14:08:59 @agent_ppo2.py:185][0m |          -0.0021 |         151.7334 |          12.3462 |
[32m[20221214 14:08:59 @agent_ppo2.py:185][0m |          -0.0016 |         150.7998 |          12.3123 |
[32m[20221214 14:08:59 @agent_ppo2.py:185][0m |           0.0067 |         157.4722 |          12.2775 |
[32m[20221214 14:08:59 @agent_ppo2.py:185][0m |          -0.0021 |         149.0216 |          12.2449 |
[32m[20221214 14:08:59 @agent_ppo2.py:185][0m |          -0.0024 |         148.6848 |          12.2242 |
[32m[20221214 14:08:59 @agent_ppo2.py:185][0m |           0.0054 |         153.9103 |          12.2312 |
[32m[20221214 14:08:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:08:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.71
[32m[20221214 14:08:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.59
[32m[20221214 14:08:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.92
[32m[20221214 14:08:59 @agent_ppo2.py:143][0m Total time:      10.95 min
[32m[20221214 14:08:59 @agent_ppo2.py:145][0m 997376 total steps have happened
[32m[20221214 14:08:59 @agent_ppo2.py:121][0m #------------------------ Iteration 487 --------------------------#
[32m[20221214 14:08:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:08:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:00 @agent_ppo2.py:185][0m |          -0.0031 |         164.8943 |          12.0369 |
[32m[20221214 14:09:00 @agent_ppo2.py:185][0m |          -0.0001 |         153.1232 |          12.0531 |
[32m[20221214 14:09:00 @agent_ppo2.py:185][0m |           0.0046 |         156.2104 |          12.0458 |
[32m[20221214 14:09:00 @agent_ppo2.py:185][0m |          -0.0063 |         149.1424 |          12.0393 |
[32m[20221214 14:09:00 @agent_ppo2.py:185][0m |          -0.0003 |         146.9952 |          12.0566 |
[32m[20221214 14:09:00 @agent_ppo2.py:185][0m |          -0.0041 |         145.4774 |          12.0567 |
[32m[20221214 14:09:00 @agent_ppo2.py:185][0m |           0.0015 |         148.2121 |          12.0758 |
[32m[20221214 14:09:00 @agent_ppo2.py:185][0m |          -0.0023 |         143.4561 |          12.0641 |
[32m[20221214 14:09:00 @agent_ppo2.py:185][0m |           0.0124 |         164.3329 |          12.0738 |
[32m[20221214 14:09:00 @agent_ppo2.py:185][0m |           0.0045 |         144.9379 |          12.0664 |
[32m[20221214 14:09:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:09:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 758.55
[32m[20221214 14:09:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.59
[32m[20221214 14:09:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 738.46
[32m[20221214 14:09:01 @agent_ppo2.py:143][0m Total time:      10.97 min
[32m[20221214 14:09:01 @agent_ppo2.py:145][0m 999424 total steps have happened
[32m[20221214 14:09:01 @agent_ppo2.py:121][0m #------------------------ Iteration 488 --------------------------#
[32m[20221214 14:09:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:01 @agent_ppo2.py:185][0m |          -0.0050 |         111.6131 |          12.2013 |
[32m[20221214 14:09:01 @agent_ppo2.py:185][0m |           0.0005 |         110.5078 |          12.1819 |
[32m[20221214 14:09:01 @agent_ppo2.py:185][0m |          -0.0063 |         107.1149 |          12.1966 |
[32m[20221214 14:09:01 @agent_ppo2.py:185][0m |          -0.0090 |         106.2798 |          12.1874 |
[32m[20221214 14:09:01 @agent_ppo2.py:185][0m |          -0.0088 |         106.2697 |          12.1942 |
[32m[20221214 14:09:01 @agent_ppo2.py:185][0m |          -0.0051 |         105.9616 |          12.2057 |
[32m[20221214 14:09:01 @agent_ppo2.py:185][0m |           0.0000 |         108.9788 |          12.2195 |
[32m[20221214 14:09:02 @agent_ppo2.py:185][0m |          -0.0068 |         105.3767 |          12.2201 |
[32m[20221214 14:09:02 @agent_ppo2.py:185][0m |          -0.0081 |         106.1862 |          12.2245 |
[32m[20221214 14:09:02 @agent_ppo2.py:185][0m |          -0.0045 |         106.3858 |          12.2347 |
[32m[20221214 14:09:02 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:09:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 712.77
[32m[20221214 14:09:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 746.67
[32m[20221214 14:09:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 759.70
[32m[20221214 14:09:02 @agent_ppo2.py:143][0m Total time:      11.00 min
[32m[20221214 14:09:02 @agent_ppo2.py:145][0m 1001472 total steps have happened
[32m[20221214 14:09:02 @agent_ppo2.py:121][0m #------------------------ Iteration 489 --------------------------#
[32m[20221214 14:09:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:02 @agent_ppo2.py:185][0m |          -0.0023 |         174.9250 |          12.1066 |
[32m[20221214 14:09:02 @agent_ppo2.py:185][0m |           0.0112 |         196.5763 |          12.0536 |
[32m[20221214 14:09:02 @agent_ppo2.py:185][0m |          -0.0044 |         171.3528 |          12.0252 |
[32m[20221214 14:09:03 @agent_ppo2.py:185][0m |          -0.0042 |         170.2957 |          12.0020 |
[32m[20221214 14:09:03 @agent_ppo2.py:185][0m |          -0.0043 |         170.0334 |          11.9857 |
[32m[20221214 14:09:03 @agent_ppo2.py:185][0m |          -0.0044 |         169.9892 |          11.9776 |
[32m[20221214 14:09:03 @agent_ppo2.py:185][0m |          -0.0037 |         169.2157 |          11.9519 |
[32m[20221214 14:09:03 @agent_ppo2.py:185][0m |          -0.0050 |         169.0803 |          11.9141 |
[32m[20221214 14:09:03 @agent_ppo2.py:185][0m |          -0.0042 |         168.2129 |          11.9073 |
[32m[20221214 14:09:03 @agent_ppo2.py:185][0m |          -0.0036 |         169.5760 |          11.8900 |
[32m[20221214 14:09:03 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:09:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 734.83
[32m[20221214 14:09:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 748.66
[32m[20221214 14:09:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 791.35
[32m[20221214 14:09:03 @agent_ppo2.py:143][0m Total time:      11.02 min
[32m[20221214 14:09:03 @agent_ppo2.py:145][0m 1003520 total steps have happened
[32m[20221214 14:09:03 @agent_ppo2.py:121][0m #------------------------ Iteration 490 --------------------------#
[32m[20221214 14:09:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:09:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:04 @agent_ppo2.py:185][0m |          -0.0022 |         212.0283 |          11.9851 |
[32m[20221214 14:09:04 @agent_ppo2.py:185][0m |          -0.0020 |         209.2851 |          11.9646 |
[32m[20221214 14:09:04 @agent_ppo2.py:185][0m |          -0.0024 |         207.8690 |          11.9942 |
[32m[20221214 14:09:04 @agent_ppo2.py:185][0m |           0.0012 |         208.1397 |          11.9727 |
[32m[20221214 14:09:04 @agent_ppo2.py:185][0m |           0.0001 |         206.2837 |          11.9539 |
[32m[20221214 14:09:04 @agent_ppo2.py:185][0m |          -0.0024 |         205.2670 |          11.9637 |
[32m[20221214 14:09:04 @agent_ppo2.py:185][0m |          -0.0035 |         204.3083 |          11.9344 |
[32m[20221214 14:09:04 @agent_ppo2.py:185][0m |          -0.0028 |         204.3928 |          11.9423 |
[32m[20221214 14:09:04 @agent_ppo2.py:185][0m |          -0.0021 |         204.3653 |          11.9192 |
[32m[20221214 14:09:04 @agent_ppo2.py:185][0m |          -0.0025 |         203.5186 |          11.9262 |
[32m[20221214 14:09:04 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:09:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.07
[32m[20221214 14:09:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.91
[32m[20221214 14:09:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.48
[32m[20221214 14:09:05 @agent_ppo2.py:143][0m Total time:      11.04 min
[32m[20221214 14:09:05 @agent_ppo2.py:145][0m 1005568 total steps have happened
[32m[20221214 14:09:05 @agent_ppo2.py:121][0m #------------------------ Iteration 491 --------------------------#
[32m[20221214 14:09:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:05 @agent_ppo2.py:185][0m |          -0.0034 |         205.9092 |          12.0462 |
[32m[20221214 14:09:05 @agent_ppo2.py:185][0m |           0.0017 |         205.0706 |          12.0734 |
[32m[20221214 14:09:05 @agent_ppo2.py:185][0m |           0.0083 |         213.3516 |          12.0346 |
[32m[20221214 14:09:05 @agent_ppo2.py:185][0m |          -0.0040 |         200.8239 |          12.0624 |
[32m[20221214 14:09:05 @agent_ppo2.py:185][0m |           0.0069 |         220.6861 |          12.0398 |
[32m[20221214 14:09:05 @agent_ppo2.py:185][0m |          -0.0028 |         199.7096 |          12.0309 |
[32m[20221214 14:09:05 @agent_ppo2.py:185][0m |          -0.0068 |         199.7779 |          12.0337 |
[32m[20221214 14:09:06 @agent_ppo2.py:185][0m |          -0.0034 |         199.0244 |          12.0335 |
[32m[20221214 14:09:06 @agent_ppo2.py:185][0m |          -0.0054 |         198.4812 |          12.0122 |
[32m[20221214 14:09:06 @agent_ppo2.py:185][0m |          -0.0013 |         198.6401 |          12.0171 |
[32m[20221214 14:09:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:09:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.14
[32m[20221214 14:09:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.15
[32m[20221214 14:09:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.79
[32m[20221214 14:09:06 @agent_ppo2.py:143][0m Total time:      11.06 min
[32m[20221214 14:09:06 @agent_ppo2.py:145][0m 1007616 total steps have happened
[32m[20221214 14:09:06 @agent_ppo2.py:121][0m #------------------------ Iteration 492 --------------------------#
[32m[20221214 14:09:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:06 @agent_ppo2.py:185][0m |          -0.0027 |         193.1773 |          11.5459 |
[32m[20221214 14:09:06 @agent_ppo2.py:185][0m |          -0.0034 |         190.8825 |          11.5761 |
[32m[20221214 14:09:06 @agent_ppo2.py:185][0m |          -0.0017 |         189.5191 |          11.5665 |
[32m[20221214 14:09:06 @agent_ppo2.py:185][0m |          -0.0004 |         189.8060 |          11.5583 |
[32m[20221214 14:09:07 @agent_ppo2.py:185][0m |          -0.0023 |         189.2376 |          11.5735 |
[32m[20221214 14:09:07 @agent_ppo2.py:185][0m |          -0.0022 |         188.3119 |          11.5726 |
[32m[20221214 14:09:07 @agent_ppo2.py:185][0m |          -0.0010 |         188.2909 |          11.5724 |
[32m[20221214 14:09:07 @agent_ppo2.py:185][0m |          -0.0033 |         188.7016 |          11.5794 |
[32m[20221214 14:09:07 @agent_ppo2.py:185][0m |          -0.0022 |         188.2515 |          11.5891 |
[32m[20221214 14:09:07 @agent_ppo2.py:185][0m |          -0.0025 |         187.7218 |          11.5818 |
[32m[20221214 14:09:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:09:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.60
[32m[20221214 14:09:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.76
[32m[20221214 14:09:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.48
[32m[20221214 14:09:07 @agent_ppo2.py:143][0m Total time:      11.08 min
[32m[20221214 14:09:07 @agent_ppo2.py:145][0m 1009664 total steps have happened
[32m[20221214 14:09:07 @agent_ppo2.py:121][0m #------------------------ Iteration 493 --------------------------#
[32m[20221214 14:09:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:07 @agent_ppo2.py:185][0m |          -0.0022 |         195.4929 |          11.6523 |
[32m[20221214 14:09:08 @agent_ppo2.py:185][0m |           0.0049 |         196.0538 |          11.6602 |
[32m[20221214 14:09:08 @agent_ppo2.py:185][0m |          -0.0020 |         190.8093 |          11.6491 |
[32m[20221214 14:09:08 @agent_ppo2.py:185][0m |          -0.0018 |         189.2590 |          11.6506 |
[32m[20221214 14:09:08 @agent_ppo2.py:185][0m |           0.0054 |         192.7727 |          11.6426 |
[32m[20221214 14:09:08 @agent_ppo2.py:185][0m |          -0.0005 |         188.2409 |          11.6190 |
[32m[20221214 14:09:08 @agent_ppo2.py:185][0m |          -0.0006 |         187.6840 |          11.5960 |
[32m[20221214 14:09:08 @agent_ppo2.py:185][0m |          -0.0014 |         186.4486 |          11.5905 |
[32m[20221214 14:09:08 @agent_ppo2.py:185][0m |          -0.0014 |         186.0830 |          11.6033 |
[32m[20221214 14:09:08 @agent_ppo2.py:185][0m |          -0.0024 |         186.0866 |          11.5775 |
[32m[20221214 14:09:08 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:09:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.61
[32m[20221214 14:09:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.62
[32m[20221214 14:09:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.81
[32m[20221214 14:09:08 @agent_ppo2.py:143][0m Total time:      11.11 min
[32m[20221214 14:09:08 @agent_ppo2.py:145][0m 1011712 total steps have happened
[32m[20221214 14:09:08 @agent_ppo2.py:121][0m #------------------------ Iteration 494 --------------------------#
[32m[20221214 14:09:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:09 @agent_ppo2.py:185][0m |           0.0005 |         190.0806 |          11.7202 |
[32m[20221214 14:09:09 @agent_ppo2.py:185][0m |          -0.0007 |         186.9848 |          11.7127 |
[32m[20221214 14:09:09 @agent_ppo2.py:185][0m |          -0.0016 |         184.9770 |          11.6790 |
[32m[20221214 14:09:09 @agent_ppo2.py:185][0m |          -0.0010 |         183.9226 |          11.6832 |
[32m[20221214 14:09:09 @agent_ppo2.py:185][0m |          -0.0012 |         182.9842 |          11.6503 |
[32m[20221214 14:09:09 @agent_ppo2.py:185][0m |          -0.0020 |         182.7147 |          11.6119 |
[32m[20221214 14:09:09 @agent_ppo2.py:185][0m |           0.0069 |         188.9889 |          11.5863 |
[32m[20221214 14:09:09 @agent_ppo2.py:185][0m |          -0.0036 |         180.7820 |          11.5774 |
[32m[20221214 14:09:09 @agent_ppo2.py:185][0m |           0.0053 |         183.8543 |          11.5549 |
[32m[20221214 14:09:09 @agent_ppo2.py:185][0m |          -0.0035 |         179.7859 |          11.5510 |
[32m[20221214 14:09:09 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:09:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.77
[32m[20221214 14:09:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.32
[32m[20221214 14:09:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.43
[32m[20221214 14:09:10 @agent_ppo2.py:143][0m Total time:      11.13 min
[32m[20221214 14:09:10 @agent_ppo2.py:145][0m 1013760 total steps have happened
[32m[20221214 14:09:10 @agent_ppo2.py:121][0m #------------------------ Iteration 495 --------------------------#
[32m[20221214 14:09:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:10 @agent_ppo2.py:185][0m |          -0.0059 |         189.3780 |          11.5526 |
[32m[20221214 14:09:10 @agent_ppo2.py:185][0m |          -0.0051 |         180.4281 |          11.5327 |
[32m[20221214 14:09:10 @agent_ppo2.py:185][0m |          -0.0048 |         177.9073 |          11.5622 |
[32m[20221214 14:09:10 @agent_ppo2.py:185][0m |          -0.0027 |         176.6157 |          11.5404 |
[32m[20221214 14:09:10 @agent_ppo2.py:185][0m |          -0.0040 |         176.0895 |          11.5174 |
[32m[20221214 14:09:11 @agent_ppo2.py:185][0m |          -0.0079 |         175.5106 |          11.5071 |
[32m[20221214 14:09:11 @agent_ppo2.py:185][0m |          -0.0036 |         175.7762 |          11.5296 |
[32m[20221214 14:09:11 @agent_ppo2.py:185][0m |          -0.0043 |         175.3538 |          11.4954 |
[32m[20221214 14:09:11 @agent_ppo2.py:185][0m |          -0.0064 |         174.6264 |          11.5004 |
[32m[20221214 14:09:11 @agent_ppo2.py:185][0m |          -0.0060 |         174.7678 |          11.4873 |
[32m[20221214 14:09:11 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:09:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.07
[32m[20221214 14:09:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.61
[32m[20221214 14:09:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.87
[32m[20221214 14:09:11 @agent_ppo2.py:143][0m Total time:      11.15 min
[32m[20221214 14:09:11 @agent_ppo2.py:145][0m 1015808 total steps have happened
[32m[20221214 14:09:11 @agent_ppo2.py:121][0m #------------------------ Iteration 496 --------------------------#
[32m[20221214 14:09:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:11 @agent_ppo2.py:185][0m |           0.0007 |         192.8973 |          11.2888 |
[32m[20221214 14:09:12 @agent_ppo2.py:185][0m |           0.0010 |         186.9023 |          11.2695 |
[32m[20221214 14:09:12 @agent_ppo2.py:185][0m |          -0.0030 |         186.1521 |          11.2504 |
[32m[20221214 14:09:12 @agent_ppo2.py:185][0m |          -0.0013 |         186.3997 |          11.2311 |
[32m[20221214 14:09:12 @agent_ppo2.py:185][0m |          -0.0019 |         184.5808 |          11.1874 |
[32m[20221214 14:09:12 @agent_ppo2.py:185][0m |          -0.0024 |         184.1537 |          11.1492 |
[32m[20221214 14:09:12 @agent_ppo2.py:185][0m |          -0.0036 |         183.9721 |          11.1594 |
[32m[20221214 14:09:12 @agent_ppo2.py:185][0m |          -0.0022 |         183.0318 |          11.0936 |
[32m[20221214 14:09:12 @agent_ppo2.py:185][0m |          -0.0013 |         183.3921 |          11.0714 |
[32m[20221214 14:09:12 @agent_ppo2.py:185][0m |           0.0003 |         184.2458 |          11.0480 |
[32m[20221214 14:09:12 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:09:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.21
[32m[20221214 14:09:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.58
[32m[20221214 14:09:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.96
[32m[20221214 14:09:12 @agent_ppo2.py:143][0m Total time:      11.17 min
[32m[20221214 14:09:12 @agent_ppo2.py:145][0m 1017856 total steps have happened
[32m[20221214 14:09:12 @agent_ppo2.py:121][0m #------------------------ Iteration 497 --------------------------#
[32m[20221214 14:09:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:13 @agent_ppo2.py:185][0m |          -0.0019 |         174.7760 |          10.9409 |
[32m[20221214 14:09:13 @agent_ppo2.py:185][0m |          -0.0006 |         166.2700 |          10.9597 |
[32m[20221214 14:09:13 @agent_ppo2.py:185][0m |           0.0109 |         190.5552 |          10.9136 |
[32m[20221214 14:09:13 @agent_ppo2.py:185][0m |          -0.0035 |         163.2648 |          10.9437 |
[32m[20221214 14:09:13 @agent_ppo2.py:185][0m |           0.0027 |         163.7166 |          10.9144 |
[32m[20221214 14:09:13 @agent_ppo2.py:185][0m |           0.0094 |         177.9728 |          10.9183 |
[32m[20221214 14:09:13 @agent_ppo2.py:185][0m |          -0.0016 |         161.5448 |          10.9254 |
[32m[20221214 14:09:13 @agent_ppo2.py:185][0m |          -0.0027 |         161.1645 |          10.8852 |
[32m[20221214 14:09:14 @agent_ppo2.py:185][0m |          -0.0014 |         160.9294 |          10.9072 |
[32m[20221214 14:09:14 @agent_ppo2.py:185][0m |           0.0062 |         172.9593 |          10.8599 |
[32m[20221214 14:09:14 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:09:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.62
[32m[20221214 14:09:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.16
[32m[20221214 14:09:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.78
[32m[20221214 14:09:14 @agent_ppo2.py:143][0m Total time:      11.19 min
[32m[20221214 14:09:14 @agent_ppo2.py:145][0m 1019904 total steps have happened
[32m[20221214 14:09:14 @agent_ppo2.py:121][0m #------------------------ Iteration 498 --------------------------#
[32m[20221214 14:09:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:14 @agent_ppo2.py:185][0m |          -0.0029 |         196.7313 |          11.1259 |
[32m[20221214 14:09:14 @agent_ppo2.py:185][0m |           0.0018 |         192.0729 |          11.0740 |
[32m[20221214 14:09:14 @agent_ppo2.py:185][0m |          -0.0033 |         189.0948 |          11.0703 |
[32m[20221214 14:09:14 @agent_ppo2.py:185][0m |          -0.0027 |         188.2186 |          11.0807 |
[32m[20221214 14:09:14 @agent_ppo2.py:185][0m |          -0.0049 |         187.4728 |          11.0761 |
[32m[20221214 14:09:15 @agent_ppo2.py:185][0m |           0.0078 |         204.1162 |          11.0719 |
[32m[20221214 14:09:15 @agent_ppo2.py:185][0m |          -0.0025 |         187.3003 |          11.0905 |
[32m[20221214 14:09:15 @agent_ppo2.py:185][0m |           0.0042 |         191.8313 |          11.0561 |
[32m[20221214 14:09:15 @agent_ppo2.py:185][0m |           0.0040 |         191.0510 |          11.0557 |
[32m[20221214 14:09:15 @agent_ppo2.py:185][0m |          -0.0041 |         186.1747 |          11.0402 |
[32m[20221214 14:09:15 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:09:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.35
[32m[20221214 14:09:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.47
[32m[20221214 14:09:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.44
[32m[20221214 14:09:15 @agent_ppo2.py:143][0m Total time:      11.22 min
[32m[20221214 14:09:15 @agent_ppo2.py:145][0m 1021952 total steps have happened
[32m[20221214 14:09:15 @agent_ppo2.py:121][0m #------------------------ Iteration 499 --------------------------#
[32m[20221214 14:09:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:15 @agent_ppo2.py:185][0m |          -0.0020 |         208.4533 |          10.8592 |
[32m[20221214 14:09:16 @agent_ppo2.py:185][0m |          -0.0015 |         203.3647 |          10.8533 |
[32m[20221214 14:09:16 @agent_ppo2.py:185][0m |          -0.0012 |         201.6129 |          10.8681 |
[32m[20221214 14:09:16 @agent_ppo2.py:185][0m |           0.0011 |         201.2547 |          10.8210 |
[32m[20221214 14:09:16 @agent_ppo2.py:185][0m |          -0.0042 |         200.1912 |          10.7948 |
[32m[20221214 14:09:16 @agent_ppo2.py:185][0m |          -0.0029 |         199.3613 |          10.8171 |
[32m[20221214 14:09:16 @agent_ppo2.py:185][0m |           0.0041 |         209.1047 |          10.7737 |
[32m[20221214 14:09:16 @agent_ppo2.py:185][0m |          -0.0036 |         199.0734 |          10.7941 |
[32m[20221214 14:09:16 @agent_ppo2.py:185][0m |           0.0034 |         203.7287 |          10.7716 |
[32m[20221214 14:09:16 @agent_ppo2.py:185][0m |           0.0092 |         215.3795 |          10.7389 |
[32m[20221214 14:09:16 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:09:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.90
[32m[20221214 14:09:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.51
[32m[20221214 14:09:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.80
[32m[20221214 14:09:16 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 860.44
[32m[20221214 14:09:16 @agent_ppo2.py:143][0m Total time:      11.24 min
[32m[20221214 14:09:16 @agent_ppo2.py:145][0m 1024000 total steps have happened
[32m[20221214 14:09:16 @agent_ppo2.py:121][0m #------------------------ Iteration 500 --------------------------#
[32m[20221214 14:09:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:09:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:17 @agent_ppo2.py:185][0m |          -0.0025 |         207.5501 |          10.7105 |
[32m[20221214 14:09:17 @agent_ppo2.py:185][0m |          -0.0035 |         198.2114 |          10.7616 |
[32m[20221214 14:09:17 @agent_ppo2.py:185][0m |          -0.0065 |         195.5445 |          10.7484 |
[32m[20221214 14:09:17 @agent_ppo2.py:185][0m |          -0.0072 |         193.9742 |          10.7303 |
[32m[20221214 14:09:17 @agent_ppo2.py:185][0m |          -0.0070 |         193.3260 |          10.7251 |
[32m[20221214 14:09:17 @agent_ppo2.py:185][0m |          -0.0024 |         195.2273 |          10.7152 |
[32m[20221214 14:09:17 @agent_ppo2.py:185][0m |          -0.0006 |         195.7866 |          10.6939 |
[32m[20221214 14:09:17 @agent_ppo2.py:185][0m |          -0.0078 |         190.9960 |          10.6781 |
[32m[20221214 14:09:18 @agent_ppo2.py:185][0m |          -0.0063 |         190.9553 |          10.6653 |
[32m[20221214 14:09:18 @agent_ppo2.py:185][0m |           0.0008 |         197.9037 |          10.6562 |
[32m[20221214 14:09:18 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:09:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.67
[32m[20221214 14:09:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.79
[32m[20221214 14:09:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.96
[32m[20221214 14:09:18 @agent_ppo2.py:143][0m Total time:      11.26 min
[32m[20221214 14:09:18 @agent_ppo2.py:145][0m 1026048 total steps have happened
[32m[20221214 14:09:18 @agent_ppo2.py:121][0m #------------------------ Iteration 501 --------------------------#
[32m[20221214 14:09:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:18 @agent_ppo2.py:185][0m |          -0.0035 |         218.7091 |          10.5059 |
[32m[20221214 14:09:18 @agent_ppo2.py:185][0m |          -0.0040 |         212.7240 |          10.4559 |
[32m[20221214 14:09:18 @agent_ppo2.py:185][0m |          -0.0028 |         211.4540 |          10.4517 |
[32m[20221214 14:09:18 @agent_ppo2.py:185][0m |          -0.0042 |         210.1893 |          10.4531 |
[32m[20221214 14:09:19 @agent_ppo2.py:185][0m |          -0.0050 |         209.6899 |          10.4387 |
[32m[20221214 14:09:19 @agent_ppo2.py:185][0m |          -0.0047 |         208.9016 |          10.4331 |
[32m[20221214 14:09:19 @agent_ppo2.py:185][0m |          -0.0025 |         208.6337 |          10.4274 |
[32m[20221214 14:09:19 @agent_ppo2.py:185][0m |          -0.0011 |         208.3083 |          10.3996 |
[32m[20221214 14:09:19 @agent_ppo2.py:185][0m |          -0.0046 |         208.1147 |          10.4070 |
[32m[20221214 14:09:19 @agent_ppo2.py:185][0m |          -0.0046 |         207.8160 |          10.4004 |
[32m[20221214 14:09:19 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:09:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.54
[32m[20221214 14:09:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.50
[32m[20221214 14:09:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.00
[32m[20221214 14:09:19 @agent_ppo2.py:143][0m Total time:      11.28 min
[32m[20221214 14:09:19 @agent_ppo2.py:145][0m 1028096 total steps have happened
[32m[20221214 14:09:19 @agent_ppo2.py:121][0m #------------------------ Iteration 502 --------------------------#
[32m[20221214 14:09:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:19 @agent_ppo2.py:185][0m |          -0.0018 |         214.8058 |          10.5817 |
[32m[20221214 14:09:20 @agent_ppo2.py:185][0m |           0.0056 |         221.2466 |          10.5917 |
[32m[20221214 14:09:20 @agent_ppo2.py:185][0m |          -0.0027 |         203.8652 |          10.6153 |
[32m[20221214 14:09:20 @agent_ppo2.py:185][0m |          -0.0027 |         202.7393 |          10.5908 |
[32m[20221214 14:09:20 @agent_ppo2.py:185][0m |          -0.0040 |         201.3916 |          10.5950 |
[32m[20221214 14:09:20 @agent_ppo2.py:185][0m |          -0.0046 |         200.6321 |          10.5980 |
[32m[20221214 14:09:20 @agent_ppo2.py:185][0m |          -0.0035 |         199.9388 |          10.6095 |
[32m[20221214 14:09:20 @agent_ppo2.py:185][0m |          -0.0035 |         198.9978 |          10.5846 |
[32m[20221214 14:09:20 @agent_ppo2.py:185][0m |          -0.0033 |         198.7130 |          10.5960 |
[32m[20221214 14:09:20 @agent_ppo2.py:185][0m |          -0.0029 |         197.9871 |          10.6209 |
[32m[20221214 14:09:20 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:09:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.53
[32m[20221214 14:09:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.36
[32m[20221214 14:09:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.89
[32m[20221214 14:09:20 @agent_ppo2.py:143][0m Total time:      11.31 min
[32m[20221214 14:09:20 @agent_ppo2.py:145][0m 1030144 total steps have happened
[32m[20221214 14:09:20 @agent_ppo2.py:121][0m #------------------------ Iteration 503 --------------------------#
[32m[20221214 14:09:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:21 @agent_ppo2.py:185][0m |           0.0087 |         233.7754 |          10.4279 |
[32m[20221214 14:09:21 @agent_ppo2.py:185][0m |          -0.0039 |         212.2928 |          10.4389 |
[32m[20221214 14:09:21 @agent_ppo2.py:185][0m |           0.0024 |         211.5158 |          10.4511 |
[32m[20221214 14:09:21 @agent_ppo2.py:185][0m |          -0.0009 |         209.8358 |          10.3967 |
[32m[20221214 14:09:21 @agent_ppo2.py:185][0m |          -0.0018 |         209.2445 |          10.3999 |
[32m[20221214 14:09:21 @agent_ppo2.py:185][0m |           0.0005 |         209.7651 |          10.4026 |
[32m[20221214 14:09:21 @agent_ppo2.py:185][0m |          -0.0028 |         208.2436 |          10.3658 |
[32m[20221214 14:09:21 @agent_ppo2.py:185][0m |          -0.0029 |         207.9189 |          10.3607 |
[32m[20221214 14:09:22 @agent_ppo2.py:185][0m |          -0.0026 |         207.0570 |          10.3711 |
[32m[20221214 14:09:22 @agent_ppo2.py:185][0m |          -0.0029 |         206.8870 |          10.3506 |
[32m[20221214 14:09:22 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:09:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.16
[32m[20221214 14:09:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.96
[32m[20221214 14:09:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.21
[32m[20221214 14:09:22 @agent_ppo2.py:143][0m Total time:      11.33 min
[32m[20221214 14:09:22 @agent_ppo2.py:145][0m 1032192 total steps have happened
[32m[20221214 14:09:22 @agent_ppo2.py:121][0m #------------------------ Iteration 504 --------------------------#
[32m[20221214 14:09:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:22 @agent_ppo2.py:185][0m |          -0.0027 |         205.2048 |          10.2614 |
[32m[20221214 14:09:22 @agent_ppo2.py:185][0m |          -0.0028 |         197.0683 |          10.2740 |
[32m[20221214 14:09:22 @agent_ppo2.py:185][0m |          -0.0026 |         194.2360 |          10.2390 |
[32m[20221214 14:09:22 @agent_ppo2.py:185][0m |          -0.0030 |         192.9161 |          10.2276 |
[32m[20221214 14:09:23 @agent_ppo2.py:185][0m |          -0.0014 |         191.5973 |          10.2256 |
[32m[20221214 14:09:23 @agent_ppo2.py:185][0m |          -0.0032 |         190.7492 |          10.1777 |
[32m[20221214 14:09:23 @agent_ppo2.py:185][0m |           0.0008 |         191.8029 |          10.1658 |
[32m[20221214 14:09:23 @agent_ppo2.py:185][0m |           0.0010 |         190.4931 |          10.1364 |
[32m[20221214 14:09:23 @agent_ppo2.py:185][0m |          -0.0034 |         188.5720 |          10.1140 |
[32m[20221214 14:09:23 @agent_ppo2.py:185][0m |          -0.0036 |         187.6301 |          10.0930 |
[32m[20221214 14:09:23 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:09:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.05
[32m[20221214 14:09:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.62
[32m[20221214 14:09:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.77
[32m[20221214 14:09:23 @agent_ppo2.py:143][0m Total time:      11.35 min
[32m[20221214 14:09:23 @agent_ppo2.py:145][0m 1034240 total steps have happened
[32m[20221214 14:09:23 @agent_ppo2.py:121][0m #------------------------ Iteration 505 --------------------------#
[32m[20221214 14:09:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:24 @agent_ppo2.py:185][0m |          -0.0019 |         213.6348 |           9.8870 |
[32m[20221214 14:09:24 @agent_ppo2.py:185][0m |          -0.0017 |         206.4053 |           9.9113 |
[32m[20221214 14:09:24 @agent_ppo2.py:185][0m |          -0.0013 |         204.3680 |           9.9385 |
[32m[20221214 14:09:24 @agent_ppo2.py:185][0m |          -0.0011 |         202.1876 |           9.9476 |
[32m[20221214 14:09:24 @agent_ppo2.py:185][0m |           0.0062 |         209.1513 |           9.9668 |
[32m[20221214 14:09:24 @agent_ppo2.py:185][0m |          -0.0001 |         200.9610 |          10.0063 |
[32m[20221214 14:09:24 @agent_ppo2.py:185][0m |           0.0001 |         200.4885 |          10.0261 |
[32m[20221214 14:09:24 @agent_ppo2.py:185][0m |          -0.0017 |         200.5536 |          10.0313 |
[32m[20221214 14:09:24 @agent_ppo2.py:185][0m |          -0.0000 |         200.4927 |          10.0794 |
[32m[20221214 14:09:24 @agent_ppo2.py:185][0m |          -0.0009 |         200.1529 |          10.0957 |
[32m[20221214 14:09:24 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:09:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.90
[32m[20221214 14:09:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.33
[32m[20221214 14:09:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.26
[32m[20221214 14:09:25 @agent_ppo2.py:143][0m Total time:      11.37 min
[32m[20221214 14:09:25 @agent_ppo2.py:145][0m 1036288 total steps have happened
[32m[20221214 14:09:25 @agent_ppo2.py:121][0m #------------------------ Iteration 506 --------------------------#
[32m[20221214 14:09:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:25 @agent_ppo2.py:185][0m |          -0.0034 |         209.9427 |          10.4904 |
[32m[20221214 14:09:25 @agent_ppo2.py:185][0m |          -0.0024 |         206.9569 |          10.4936 |
[32m[20221214 14:09:25 @agent_ppo2.py:185][0m |          -0.0028 |         205.8222 |          10.4934 |
[32m[20221214 14:09:25 @agent_ppo2.py:185][0m |          -0.0008 |         205.2334 |          10.4996 |
[32m[20221214 14:09:25 @agent_ppo2.py:185][0m |          -0.0021 |         204.2911 |          10.5037 |
[32m[20221214 14:09:25 @agent_ppo2.py:185][0m |           0.0030 |         206.4945 |          10.4963 |
[32m[20221214 14:09:25 @agent_ppo2.py:185][0m |           0.0001 |         204.9170 |          10.4976 |
[32m[20221214 14:09:25 @agent_ppo2.py:185][0m |          -0.0028 |         203.0475 |          10.4842 |
[32m[20221214 14:09:26 @agent_ppo2.py:185][0m |          -0.0018 |         202.4182 |          10.4925 |
[32m[20221214 14:09:26 @agent_ppo2.py:185][0m |           0.0068 |         217.2629 |          10.5219 |
[32m[20221214 14:09:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:09:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.89
[32m[20221214 14:09:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.91
[32m[20221214 14:09:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.51
[32m[20221214 14:09:26 @agent_ppo2.py:143][0m Total time:      11.40 min
[32m[20221214 14:09:26 @agent_ppo2.py:145][0m 1038336 total steps have happened
[32m[20221214 14:09:26 @agent_ppo2.py:121][0m #------------------------ Iteration 507 --------------------------#
[32m[20221214 14:09:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:26 @agent_ppo2.py:185][0m |          -0.0013 |         203.6662 |          10.2384 |
[32m[20221214 14:09:26 @agent_ppo2.py:185][0m |           0.0005 |         201.9611 |          10.2124 |
[32m[20221214 14:09:26 @agent_ppo2.py:185][0m |          -0.0018 |         198.3026 |          10.1926 |
[32m[20221214 14:09:26 @agent_ppo2.py:185][0m |           0.0076 |         206.5627 |          10.1651 |
[32m[20221214 14:09:26 @agent_ppo2.py:185][0m |          -0.0025 |         195.1211 |          10.1415 |
[32m[20221214 14:09:27 @agent_ppo2.py:185][0m |          -0.0026 |         194.5557 |          10.1331 |
[32m[20221214 14:09:27 @agent_ppo2.py:185][0m |          -0.0022 |         193.9859 |          10.1012 |
[32m[20221214 14:09:27 @agent_ppo2.py:185][0m |          -0.0028 |         193.8610 |          10.0999 |
[32m[20221214 14:09:27 @agent_ppo2.py:185][0m |          -0.0015 |         193.1205 |          10.0490 |
[32m[20221214 14:09:27 @agent_ppo2.py:185][0m |          -0.0018 |         192.6786 |          10.0681 |
[32m[20221214 14:09:27 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:09:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.26
[32m[20221214 14:09:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.79
[32m[20221214 14:09:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.92
[32m[20221214 14:09:27 @agent_ppo2.py:143][0m Total time:      11.42 min
[32m[20221214 14:09:27 @agent_ppo2.py:145][0m 1040384 total steps have happened
[32m[20221214 14:09:27 @agent_ppo2.py:121][0m #------------------------ Iteration 508 --------------------------#
[32m[20221214 14:09:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:27 @agent_ppo2.py:185][0m |          -0.0025 |         207.3241 |           9.9730 |
[32m[20221214 14:09:27 @agent_ppo2.py:185][0m |           0.0125 |         218.1478 |           9.9695 |
[32m[20221214 14:09:28 @agent_ppo2.py:185][0m |          -0.0027 |         198.4638 |          10.0113 |
[32m[20221214 14:09:28 @agent_ppo2.py:185][0m |          -0.0029 |         196.6578 |          10.0135 |
[32m[20221214 14:09:28 @agent_ppo2.py:185][0m |          -0.0019 |         196.1033 |          10.0380 |
[32m[20221214 14:09:28 @agent_ppo2.py:185][0m |          -0.0014 |         195.3466 |          10.0334 |
[32m[20221214 14:09:28 @agent_ppo2.py:185][0m |           0.0020 |         197.4033 |          10.0336 |
[32m[20221214 14:09:28 @agent_ppo2.py:185][0m |          -0.0022 |         194.6220 |          10.0460 |
[32m[20221214 14:09:28 @agent_ppo2.py:185][0m |          -0.0024 |         194.3869 |          10.0609 |
[32m[20221214 14:09:28 @agent_ppo2.py:185][0m |           0.0027 |         194.5597 |          10.0865 |
[32m[20221214 14:09:28 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:09:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.60
[32m[20221214 14:09:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.59
[32m[20221214 14:09:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.42
[32m[20221214 14:09:28 @agent_ppo2.py:143][0m Total time:      11.44 min
[32m[20221214 14:09:28 @agent_ppo2.py:145][0m 1042432 total steps have happened
[32m[20221214 14:09:28 @agent_ppo2.py:121][0m #------------------------ Iteration 509 --------------------------#
[32m[20221214 14:09:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:29 @agent_ppo2.py:185][0m |          -0.0021 |         207.2968 |          10.0222 |
[32m[20221214 14:09:29 @agent_ppo2.py:185][0m |          -0.0021 |         205.2219 |          10.0137 |
[32m[20221214 14:09:29 @agent_ppo2.py:185][0m |          -0.0029 |         204.2024 |          10.0081 |
[32m[20221214 14:09:29 @agent_ppo2.py:185][0m |          -0.0021 |         203.4594 |          10.0285 |
[32m[20221214 14:09:29 @agent_ppo2.py:185][0m |          -0.0026 |         202.8952 |          10.0299 |
[32m[20221214 14:09:29 @agent_ppo2.py:185][0m |           0.0076 |         223.8985 |          10.0107 |
[32m[20221214 14:09:29 @agent_ppo2.py:185][0m |          -0.0033 |         203.0215 |          10.0194 |
[32m[20221214 14:09:29 @agent_ppo2.py:185][0m |          -0.0016 |         201.8252 |          10.0089 |
[32m[20221214 14:09:29 @agent_ppo2.py:185][0m |          -0.0016 |         201.3578 |          10.0476 |
[32m[20221214 14:09:29 @agent_ppo2.py:185][0m |          -0.0035 |         201.8561 |          10.0276 |
[32m[20221214 14:09:29 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:09:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.63
[32m[20221214 14:09:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.35
[32m[20221214 14:09:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.79
[32m[20221214 14:09:29 @agent_ppo2.py:143][0m Total time:      11.46 min
[32m[20221214 14:09:29 @agent_ppo2.py:145][0m 1044480 total steps have happened
[32m[20221214 14:09:29 @agent_ppo2.py:121][0m #------------------------ Iteration 510 --------------------------#
[32m[20221214 14:09:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:09:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:30 @agent_ppo2.py:185][0m |          -0.0015 |         209.7468 |          10.1395 |
[32m[20221214 14:09:30 @agent_ppo2.py:185][0m |          -0.0048 |         207.9598 |          10.1149 |
[32m[20221214 14:09:30 @agent_ppo2.py:185][0m |          -0.0007 |         207.3342 |          10.1199 |
[32m[20221214 14:09:30 @agent_ppo2.py:185][0m |          -0.0005 |         207.7879 |          10.0974 |
[32m[20221214 14:09:30 @agent_ppo2.py:185][0m |           0.0093 |         225.5723 |          10.1074 |
[32m[20221214 14:09:30 @agent_ppo2.py:185][0m |           0.0060 |         218.8754 |          10.1046 |
[32m[20221214 14:09:30 @agent_ppo2.py:185][0m |          -0.0025 |         205.3229 |          10.1075 |
[32m[20221214 14:09:30 @agent_ppo2.py:185][0m |          -0.0042 |         205.1173 |          10.1045 |
[32m[20221214 14:09:31 @agent_ppo2.py:185][0m |          -0.0021 |         204.8011 |          10.0996 |
[32m[20221214 14:09:31 @agent_ppo2.py:185][0m |          -0.0028 |         204.4923 |          10.0974 |
[32m[20221214 14:09:31 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:09:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.15
[32m[20221214 14:09:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.51
[32m[20221214 14:09:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.53
[32m[20221214 14:09:31 @agent_ppo2.py:143][0m Total time:      11.48 min
[32m[20221214 14:09:31 @agent_ppo2.py:145][0m 1046528 total steps have happened
[32m[20221214 14:09:31 @agent_ppo2.py:121][0m #------------------------ Iteration 511 --------------------------#
[32m[20221214 14:09:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:31 @agent_ppo2.py:185][0m |          -0.0013 |         195.3183 |          10.0454 |
[32m[20221214 14:09:31 @agent_ppo2.py:185][0m |          -0.0019 |         188.8238 |          10.0268 |
[32m[20221214 14:09:31 @agent_ppo2.py:185][0m |          -0.0026 |         186.2283 |          10.0270 |
[32m[20221214 14:09:31 @agent_ppo2.py:185][0m |           0.0102 |         194.4111 |          10.0150 |
[32m[20221214 14:09:31 @agent_ppo2.py:185][0m |          -0.0026 |         183.2253 |          10.0172 |
[32m[20221214 14:09:31 @agent_ppo2.py:185][0m |          -0.0010 |         182.2104 |          10.0204 |
[32m[20221214 14:09:32 @agent_ppo2.py:185][0m |          -0.0023 |         181.2390 |          10.0038 |
[32m[20221214 14:09:32 @agent_ppo2.py:185][0m |           0.0041 |         182.5873 |           9.9950 |
[32m[20221214 14:09:32 @agent_ppo2.py:185][0m |          -0.0034 |         179.7744 |           9.9975 |
[32m[20221214 14:09:32 @agent_ppo2.py:185][0m |           0.0011 |         178.0210 |           9.9876 |
[32m[20221214 14:09:32 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:09:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.43
[32m[20221214 14:09:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.35
[32m[20221214 14:09:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.15
[32m[20221214 14:09:32 @agent_ppo2.py:143][0m Total time:      11.50 min
[32m[20221214 14:09:32 @agent_ppo2.py:145][0m 1048576 total steps have happened
[32m[20221214 14:09:32 @agent_ppo2.py:121][0m #------------------------ Iteration 512 --------------------------#
[32m[20221214 14:09:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:32 @agent_ppo2.py:185][0m |          -0.0028 |         208.4958 |          10.0757 |
[32m[20221214 14:09:32 @agent_ppo2.py:185][0m |          -0.0037 |         203.3019 |          10.1082 |
[32m[20221214 14:09:32 @agent_ppo2.py:185][0m |          -0.0028 |         201.4213 |          10.0765 |
[32m[20221214 14:09:33 @agent_ppo2.py:185][0m |          -0.0033 |         199.8510 |          10.0901 |
[32m[20221214 14:09:33 @agent_ppo2.py:185][0m |          -0.0034 |         198.7354 |          10.0608 |
[32m[20221214 14:09:33 @agent_ppo2.py:185][0m |          -0.0045 |         198.2475 |          10.0845 |
[32m[20221214 14:09:33 @agent_ppo2.py:185][0m |          -0.0033 |         197.7440 |          10.0656 |
[32m[20221214 14:09:33 @agent_ppo2.py:185][0m |          -0.0014 |         197.5147 |          10.0677 |
[32m[20221214 14:09:33 @agent_ppo2.py:185][0m |          -0.0031 |         197.9948 |          10.0612 |
[32m[20221214 14:09:33 @agent_ppo2.py:185][0m |          -0.0033 |         196.3980 |          10.0532 |
[32m[20221214 14:09:33 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:09:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.28
[32m[20221214 14:09:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.45
[32m[20221214 14:09:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.30
[32m[20221214 14:09:33 @agent_ppo2.py:143][0m Total time:      11.52 min
[32m[20221214 14:09:33 @agent_ppo2.py:145][0m 1050624 total steps have happened
[32m[20221214 14:09:33 @agent_ppo2.py:121][0m #------------------------ Iteration 513 --------------------------#
[32m[20221214 14:09:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:34 @agent_ppo2.py:185][0m |          -0.0008 |         192.0255 |           9.9055 |
[32m[20221214 14:09:34 @agent_ppo2.py:185][0m |          -0.0025 |         188.1000 |           9.8748 |
[32m[20221214 14:09:34 @agent_ppo2.py:185][0m |          -0.0027 |         186.1319 |           9.8897 |
[32m[20221214 14:09:34 @agent_ppo2.py:185][0m |          -0.0023 |         184.7517 |           9.8943 |
[32m[20221214 14:09:34 @agent_ppo2.py:185][0m |          -0.0008 |         185.0879 |           9.8890 |
[32m[20221214 14:09:34 @agent_ppo2.py:185][0m |          -0.0001 |         184.2002 |           9.8791 |
[32m[20221214 14:09:34 @agent_ppo2.py:185][0m |          -0.0009 |         183.8371 |           9.8757 |
[32m[20221214 14:09:34 @agent_ppo2.py:185][0m |           0.0103 |         203.4373 |           9.8747 |
[32m[20221214 14:09:34 @agent_ppo2.py:185][0m |           0.0053 |         188.2824 |           9.8890 |
[32m[20221214 14:09:34 @agent_ppo2.py:185][0m |          -0.0020 |         182.5438 |           9.8920 |
[32m[20221214 14:09:34 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:09:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.44
[32m[20221214 14:09:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.71
[32m[20221214 14:09:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.67
[32m[20221214 14:09:34 @agent_ppo2.py:143][0m Total time:      11.54 min
[32m[20221214 14:09:34 @agent_ppo2.py:145][0m 1052672 total steps have happened
[32m[20221214 14:09:34 @agent_ppo2.py:121][0m #------------------------ Iteration 514 --------------------------#
[32m[20221214 14:09:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:35 @agent_ppo2.py:185][0m |          -0.0022 |         209.1942 |           9.9598 |
[32m[20221214 14:09:35 @agent_ppo2.py:185][0m |          -0.0014 |         203.2230 |           9.9896 |
[32m[20221214 14:09:35 @agent_ppo2.py:185][0m |          -0.0033 |         200.9895 |           9.9670 |
[32m[20221214 14:09:35 @agent_ppo2.py:185][0m |          -0.0021 |         199.5013 |           9.9565 |
[32m[20221214 14:09:35 @agent_ppo2.py:185][0m |          -0.0029 |         198.0819 |           9.9766 |
[32m[20221214 14:09:35 @agent_ppo2.py:185][0m |          -0.0013 |         196.5882 |           9.9609 |
[32m[20221214 14:09:35 @agent_ppo2.py:185][0m |          -0.0024 |         196.4596 |           9.9561 |
[32m[20221214 14:09:35 @agent_ppo2.py:185][0m |           0.0009 |         195.8196 |           9.9425 |
[32m[20221214 14:09:35 @agent_ppo2.py:185][0m |          -0.0028 |         195.0604 |           9.9446 |
[32m[20221214 14:09:36 @agent_ppo2.py:185][0m |          -0.0036 |         195.0587 |           9.9446 |
[32m[20221214 14:09:36 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:09:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.47
[32m[20221214 14:09:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.52
[32m[20221214 14:09:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.24
[32m[20221214 14:09:36 @agent_ppo2.py:143][0m Total time:      11.56 min
[32m[20221214 14:09:36 @agent_ppo2.py:145][0m 1054720 total steps have happened
[32m[20221214 14:09:36 @agent_ppo2.py:121][0m #------------------------ Iteration 515 --------------------------#
[32m[20221214 14:09:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:36 @agent_ppo2.py:185][0m |           0.0012 |         197.2215 |           9.7412 |
[32m[20221214 14:09:36 @agent_ppo2.py:185][0m |          -0.0028 |         192.4133 |           9.7416 |
[32m[20221214 14:09:36 @agent_ppo2.py:185][0m |           0.0048 |         193.9195 |           9.7333 |
[32m[20221214 14:09:36 @agent_ppo2.py:185][0m |          -0.0003 |         189.8277 |           9.7180 |
[32m[20221214 14:09:36 @agent_ppo2.py:185][0m |          -0.0020 |         189.4159 |           9.7288 |
[32m[20221214 14:09:36 @agent_ppo2.py:185][0m |          -0.0019 |         188.9547 |           9.7311 |
[32m[20221214 14:09:37 @agent_ppo2.py:185][0m |          -0.0026 |         188.3183 |           9.7420 |
[32m[20221214 14:09:37 @agent_ppo2.py:185][0m |           0.0017 |         189.0983 |           9.7325 |
[32m[20221214 14:09:37 @agent_ppo2.py:185][0m |          -0.0018 |         187.4599 |           9.7449 |
[32m[20221214 14:09:37 @agent_ppo2.py:185][0m |           0.0011 |         188.0358 |           9.7525 |
[32m[20221214 14:09:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:09:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.57
[32m[20221214 14:09:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.08
[32m[20221214 14:09:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.12
[32m[20221214 14:09:37 @agent_ppo2.py:143][0m Total time:      11.58 min
[32m[20221214 14:09:37 @agent_ppo2.py:145][0m 1056768 total steps have happened
[32m[20221214 14:09:37 @agent_ppo2.py:121][0m #------------------------ Iteration 516 --------------------------#
[32m[20221214 14:09:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:37 @agent_ppo2.py:185][0m |           0.0041 |         204.7796 |           9.6678 |
[32m[20221214 14:09:37 @agent_ppo2.py:185][0m |          -0.0017 |         198.9087 |           9.6170 |
[32m[20221214 14:09:37 @agent_ppo2.py:185][0m |          -0.0039 |         197.2031 |           9.6006 |
[32m[20221214 14:09:38 @agent_ppo2.py:185][0m |          -0.0019 |         195.9559 |           9.5946 |
[32m[20221214 14:09:38 @agent_ppo2.py:185][0m |          -0.0018 |         195.0990 |           9.5645 |
[32m[20221214 14:09:38 @agent_ppo2.py:185][0m |          -0.0018 |         194.7852 |           9.5381 |
[32m[20221214 14:09:38 @agent_ppo2.py:185][0m |          -0.0024 |         194.5832 |           9.5211 |
[32m[20221214 14:09:38 @agent_ppo2.py:185][0m |          -0.0026 |         194.1563 |           9.4888 |
[32m[20221214 14:09:38 @agent_ppo2.py:185][0m |          -0.0015 |         194.0131 |           9.4867 |
[32m[20221214 14:09:38 @agent_ppo2.py:185][0m |          -0.0020 |         193.4519 |           9.4357 |
[32m[20221214 14:09:38 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:09:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.40
[32m[20221214 14:09:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.56
[32m[20221214 14:09:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.21
[32m[20221214 14:09:38 @agent_ppo2.py:143][0m Total time:      11.60 min
[32m[20221214 14:09:38 @agent_ppo2.py:145][0m 1058816 total steps have happened
[32m[20221214 14:09:38 @agent_ppo2.py:121][0m #------------------------ Iteration 517 --------------------------#
[32m[20221214 14:09:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:39 @agent_ppo2.py:185][0m |          -0.0013 |         200.4581 |           9.4514 |
[32m[20221214 14:09:39 @agent_ppo2.py:185][0m |          -0.0031 |         197.6450 |           9.4505 |
[32m[20221214 14:09:39 @agent_ppo2.py:185][0m |          -0.0024 |         195.4560 |           9.4445 |
[32m[20221214 14:09:39 @agent_ppo2.py:185][0m |          -0.0018 |         194.3939 |           9.4344 |
[32m[20221214 14:09:39 @agent_ppo2.py:185][0m |          -0.0017 |         193.6731 |           9.4385 |
[32m[20221214 14:09:39 @agent_ppo2.py:185][0m |          -0.0022 |         192.8821 |           9.4433 |
[32m[20221214 14:09:39 @agent_ppo2.py:185][0m |          -0.0031 |         192.0663 |           9.4248 |
[32m[20221214 14:09:39 @agent_ppo2.py:185][0m |          -0.0028 |         191.9221 |           9.4268 |
[32m[20221214 14:09:39 @agent_ppo2.py:185][0m |          -0.0021 |         191.1060 |           9.4360 |
[32m[20221214 14:09:39 @agent_ppo2.py:185][0m |           0.0059 |         197.4110 |           9.4281 |
[32m[20221214 14:09:39 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:09:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.42
[32m[20221214 14:09:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.14
[32m[20221214 14:09:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.87
[32m[20221214 14:09:39 @agent_ppo2.py:143][0m Total time:      11.62 min
[32m[20221214 14:09:39 @agent_ppo2.py:145][0m 1060864 total steps have happened
[32m[20221214 14:09:39 @agent_ppo2.py:121][0m #------------------------ Iteration 518 --------------------------#
[32m[20221214 14:09:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:40 @agent_ppo2.py:185][0m |          -0.0010 |         211.0123 |           9.3755 |
[32m[20221214 14:09:40 @agent_ppo2.py:185][0m |          -0.0024 |         200.9468 |           9.3983 |
[32m[20221214 14:09:40 @agent_ppo2.py:185][0m |          -0.0017 |         198.4897 |           9.4204 |
[32m[20221214 14:09:40 @agent_ppo2.py:185][0m |          -0.0020 |         196.4375 |           9.4267 |
[32m[20221214 14:09:40 @agent_ppo2.py:185][0m |          -0.0010 |         194.0797 |           9.4344 |
[32m[20221214 14:09:40 @agent_ppo2.py:185][0m |           0.0077 |         212.9864 |           9.4486 |
[32m[20221214 14:09:40 @agent_ppo2.py:185][0m |          -0.0016 |         191.6038 |           9.4514 |
[32m[20221214 14:09:40 @agent_ppo2.py:185][0m |          -0.0018 |         190.8837 |           9.4847 |
[32m[20221214 14:09:40 @agent_ppo2.py:185][0m |          -0.0045 |         190.5541 |           9.4789 |
[32m[20221214 14:09:41 @agent_ppo2.py:185][0m |          -0.0031 |         190.3510 |           9.4927 |
[32m[20221214 14:09:41 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:09:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.38
[32m[20221214 14:09:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.14
[32m[20221214 14:09:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.25
[32m[20221214 14:09:41 @agent_ppo2.py:143][0m Total time:      11.64 min
[32m[20221214 14:09:41 @agent_ppo2.py:145][0m 1062912 total steps have happened
[32m[20221214 14:09:41 @agent_ppo2.py:121][0m #------------------------ Iteration 519 --------------------------#
[32m[20221214 14:09:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:41 @agent_ppo2.py:185][0m |          -0.0006 |         213.0520 |           9.5677 |
[32m[20221214 14:09:41 @agent_ppo2.py:185][0m |          -0.0026 |         208.8919 |           9.5924 |
[32m[20221214 14:09:41 @agent_ppo2.py:185][0m |          -0.0012 |         207.3861 |           9.5650 |
[32m[20221214 14:09:41 @agent_ppo2.py:185][0m |          -0.0021 |         206.1108 |           9.5997 |
[32m[20221214 14:09:41 @agent_ppo2.py:185][0m |          -0.0006 |         205.2504 |           9.5923 |
[32m[20221214 14:09:41 @agent_ppo2.py:185][0m |          -0.0018 |         204.7218 |           9.6119 |
[32m[20221214 14:09:42 @agent_ppo2.py:185][0m |           0.0008 |         204.4231 |           9.6128 |
[32m[20221214 14:09:42 @agent_ppo2.py:185][0m |          -0.0032 |         202.9728 |           9.5903 |
[32m[20221214 14:09:42 @agent_ppo2.py:185][0m |          -0.0015 |         201.8826 |           9.6325 |
[32m[20221214 14:09:42 @agent_ppo2.py:185][0m |          -0.0033 |         201.1895 |           9.5887 |
[32m[20221214 14:09:42 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:09:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.46
[32m[20221214 14:09:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.57
[32m[20221214 14:09:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.60
[32m[20221214 14:09:42 @agent_ppo2.py:143][0m Total time:      11.66 min
[32m[20221214 14:09:42 @agent_ppo2.py:145][0m 1064960 total steps have happened
[32m[20221214 14:09:42 @agent_ppo2.py:121][0m #------------------------ Iteration 520 --------------------------#
[32m[20221214 14:09:42 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:09:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:42 @agent_ppo2.py:185][0m |          -0.0023 |         207.0261 |           9.6531 |
[32m[20221214 14:09:42 @agent_ppo2.py:185][0m |          -0.0017 |         204.9091 |           9.6566 |
[32m[20221214 14:09:42 @agent_ppo2.py:185][0m |           0.0079 |         217.0137 |           9.6563 |
[32m[20221214 14:09:43 @agent_ppo2.py:185][0m |          -0.0018 |         203.2891 |           9.6838 |
[32m[20221214 14:09:43 @agent_ppo2.py:185][0m |           0.0047 |         207.5519 |           9.6860 |
[32m[20221214 14:09:43 @agent_ppo2.py:185][0m |          -0.0019 |         202.1684 |           9.6925 |
[32m[20221214 14:09:43 @agent_ppo2.py:185][0m |          -0.0028 |         201.6243 |           9.6857 |
[32m[20221214 14:09:43 @agent_ppo2.py:185][0m |           0.0036 |         206.9899 |           9.7090 |
[32m[20221214 14:09:43 @agent_ppo2.py:185][0m |          -0.0025 |         201.3216 |           9.6936 |
[32m[20221214 14:09:43 @agent_ppo2.py:185][0m |          -0.0034 |         200.8040 |           9.7283 |
[32m[20221214 14:09:43 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:09:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.55
[32m[20221214 14:09:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.47
[32m[20221214 14:09:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.84
[32m[20221214 14:09:43 @agent_ppo2.py:143][0m Total time:      11.69 min
[32m[20221214 14:09:43 @agent_ppo2.py:145][0m 1067008 total steps have happened
[32m[20221214 14:09:43 @agent_ppo2.py:121][0m #------------------------ Iteration 521 --------------------------#
[32m[20221214 14:09:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:44 @agent_ppo2.py:185][0m |           0.0004 |         209.4971 |           9.6633 |
[32m[20221214 14:09:44 @agent_ppo2.py:185][0m |          -0.0014 |         207.6112 |           9.6535 |
[32m[20221214 14:09:44 @agent_ppo2.py:185][0m |           0.0102 |         224.0760 |           9.6831 |
[32m[20221214 14:09:44 @agent_ppo2.py:185][0m |          -0.0004 |         205.6217 |           9.7206 |
[32m[20221214 14:09:44 @agent_ppo2.py:185][0m |          -0.0017 |         205.5645 |           9.7367 |
[32m[20221214 14:09:44 @agent_ppo2.py:185][0m |          -0.0003 |         206.1762 |           9.6953 |
[32m[20221214 14:09:44 @agent_ppo2.py:185][0m |          -0.0020 |         204.5542 |           9.6991 |
[32m[20221214 14:09:44 @agent_ppo2.py:185][0m |          -0.0032 |         204.0825 |           9.7189 |
[32m[20221214 14:09:44 @agent_ppo2.py:185][0m |           0.0022 |         205.5029 |           9.7242 |
[32m[20221214 14:09:44 @agent_ppo2.py:185][0m |          -0.0022 |         203.9625 |           9.7146 |
[32m[20221214 14:09:44 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:09:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.68
[32m[20221214 14:09:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.98
[32m[20221214 14:09:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.65
[32m[20221214 14:09:44 @agent_ppo2.py:143][0m Total time:      11.71 min
[32m[20221214 14:09:44 @agent_ppo2.py:145][0m 1069056 total steps have happened
[32m[20221214 14:09:44 @agent_ppo2.py:121][0m #------------------------ Iteration 522 --------------------------#
[32m[20221214 14:09:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:45 @agent_ppo2.py:185][0m |          -0.0013 |         221.1230 |           9.7132 |
[32m[20221214 14:09:45 @agent_ppo2.py:185][0m |           0.0022 |         214.8367 |           9.7097 |
[32m[20221214 14:09:45 @agent_ppo2.py:185][0m |          -0.0016 |         211.7146 |           9.7053 |
[32m[20221214 14:09:45 @agent_ppo2.py:185][0m |          -0.0011 |         210.2776 |           9.7035 |
[32m[20221214 14:09:45 @agent_ppo2.py:185][0m |           0.0197 |         247.2836 |           9.7227 |
[32m[20221214 14:09:45 @agent_ppo2.py:185][0m |           0.0097 |         229.7277 |           9.7445 |
[32m[20221214 14:09:45 @agent_ppo2.py:185][0m |           0.0010 |         209.3861 |           9.7396 |
[32m[20221214 14:09:45 @agent_ppo2.py:185][0m |           0.0038 |         213.5369 |           9.7474 |
[32m[20221214 14:09:45 @agent_ppo2.py:185][0m |           0.0019 |         207.6921 |           9.7369 |
[32m[20221214 14:09:46 @agent_ppo2.py:185][0m |          -0.0007 |         207.2362 |           9.7579 |
[32m[20221214 14:09:46 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:09:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.69
[32m[20221214 14:09:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.87
[32m[20221214 14:09:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.62
[32m[20221214 14:09:46 @agent_ppo2.py:143][0m Total time:      11.73 min
[32m[20221214 14:09:46 @agent_ppo2.py:145][0m 1071104 total steps have happened
[32m[20221214 14:09:46 @agent_ppo2.py:121][0m #------------------------ Iteration 523 --------------------------#
[32m[20221214 14:09:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:46 @agent_ppo2.py:185][0m |          -0.0012 |         206.6116 |           9.6643 |
[32m[20221214 14:09:46 @agent_ppo2.py:185][0m |          -0.0039 |         201.7754 |           9.6157 |
[32m[20221214 14:09:46 @agent_ppo2.py:185][0m |          -0.0033 |         199.6663 |           9.5821 |
[32m[20221214 14:09:46 @agent_ppo2.py:185][0m |          -0.0027 |         198.3945 |           9.5694 |
[32m[20221214 14:09:46 @agent_ppo2.py:185][0m |          -0.0048 |         197.8666 |           9.5519 |
[32m[20221214 14:09:46 @agent_ppo2.py:185][0m |          -0.0056 |         197.3393 |           9.5247 |
[32m[20221214 14:09:46 @agent_ppo2.py:185][0m |          -0.0037 |         197.2224 |           9.5034 |
[32m[20221214 14:09:47 @agent_ppo2.py:185][0m |           0.0100 |         223.1837 |           9.4900 |
[32m[20221214 14:09:47 @agent_ppo2.py:185][0m |           0.0126 |         225.5012 |           9.4793 |
[32m[20221214 14:09:47 @agent_ppo2.py:185][0m |          -0.0046 |         197.0107 |           9.4402 |
[32m[20221214 14:09:47 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:09:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.68
[32m[20221214 14:09:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.33
[32m[20221214 14:09:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.06
[32m[20221214 14:09:47 @agent_ppo2.py:143][0m Total time:      11.75 min
[32m[20221214 14:09:47 @agent_ppo2.py:145][0m 1073152 total steps have happened
[32m[20221214 14:09:47 @agent_ppo2.py:121][0m #------------------------ Iteration 524 --------------------------#
[32m[20221214 14:09:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:47 @agent_ppo2.py:185][0m |          -0.0008 |         217.4183 |           9.9010 |
[32m[20221214 14:09:47 @agent_ppo2.py:185][0m |          -0.0036 |         208.2475 |           9.8711 |
[32m[20221214 14:09:47 @agent_ppo2.py:185][0m |          -0.0049 |         205.3304 |           9.8665 |
[32m[20221214 14:09:48 @agent_ppo2.py:185][0m |          -0.0008 |         204.2019 |           9.8339 |
[32m[20221214 14:09:48 @agent_ppo2.py:185][0m |          -0.0029 |         202.3852 |           9.8325 |
[32m[20221214 14:09:48 @agent_ppo2.py:185][0m |          -0.0040 |         201.6189 |           9.8353 |
[32m[20221214 14:09:48 @agent_ppo2.py:185][0m |          -0.0044 |         200.9793 |           9.8196 |
[32m[20221214 14:09:48 @agent_ppo2.py:185][0m |          -0.0026 |         200.2665 |           9.8093 |
[32m[20221214 14:09:48 @agent_ppo2.py:185][0m |          -0.0013 |         200.3933 |           9.7851 |
[32m[20221214 14:09:48 @agent_ppo2.py:185][0m |          -0.0022 |         199.9254 |           9.7919 |
[32m[20221214 14:09:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:09:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.65
[32m[20221214 14:09:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.82
[32m[20221214 14:09:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.80
[32m[20221214 14:09:48 @agent_ppo2.py:143][0m Total time:      11.77 min
[32m[20221214 14:09:48 @agent_ppo2.py:145][0m 1075200 total steps have happened
[32m[20221214 14:09:48 @agent_ppo2.py:121][0m #------------------------ Iteration 525 --------------------------#
[32m[20221214 14:09:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:49 @agent_ppo2.py:185][0m |          -0.0022 |         215.0590 |           9.4253 |
[32m[20221214 14:09:49 @agent_ppo2.py:185][0m |          -0.0013 |         210.6046 |           9.4444 |
[32m[20221214 14:09:49 @agent_ppo2.py:185][0m |           0.0004 |         209.2695 |           9.4529 |
[32m[20221214 14:09:49 @agent_ppo2.py:185][0m |          -0.0027 |         208.3501 |           9.4642 |
[32m[20221214 14:09:49 @agent_ppo2.py:185][0m |          -0.0010 |         207.3210 |           9.4735 |
[32m[20221214 14:09:49 @agent_ppo2.py:185][0m |          -0.0020 |         206.7573 |           9.4739 |
[32m[20221214 14:09:49 @agent_ppo2.py:185][0m |          -0.0013 |         206.3531 |           9.4605 |
[32m[20221214 14:09:49 @agent_ppo2.py:185][0m |          -0.0017 |         206.1759 |           9.4669 |
[32m[20221214 14:09:49 @agent_ppo2.py:185][0m |          -0.0016 |         205.7643 |           9.4877 |
[32m[20221214 14:09:49 @agent_ppo2.py:185][0m |          -0.0007 |         205.7238 |           9.4826 |
[32m[20221214 14:09:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:09:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.38
[32m[20221214 14:09:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.54
[32m[20221214 14:09:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.57
[32m[20221214 14:09:49 @agent_ppo2.py:143][0m Total time:      11.79 min
[32m[20221214 14:09:49 @agent_ppo2.py:145][0m 1077248 total steps have happened
[32m[20221214 14:09:49 @agent_ppo2.py:121][0m #------------------------ Iteration 526 --------------------------#
[32m[20221214 14:09:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:50 @agent_ppo2.py:185][0m |          -0.0030 |         204.1169 |           9.4897 |
[32m[20221214 14:09:50 @agent_ppo2.py:185][0m |          -0.0043 |         201.0426 |           9.4553 |
[32m[20221214 14:09:50 @agent_ppo2.py:185][0m |          -0.0033 |         200.5745 |           9.4161 |
[32m[20221214 14:09:50 @agent_ppo2.py:185][0m |          -0.0059 |         200.1176 |           9.4158 |
[32m[20221214 14:09:50 @agent_ppo2.py:185][0m |          -0.0026 |         198.8113 |           9.4228 |
[32m[20221214 14:09:50 @agent_ppo2.py:185][0m |          -0.0031 |         198.1683 |           9.3734 |
[32m[20221214 14:09:50 @agent_ppo2.py:185][0m |          -0.0018 |         197.2391 |           9.3878 |
[32m[20221214 14:09:50 @agent_ppo2.py:185][0m |          -0.0032 |         196.6014 |           9.3789 |
[32m[20221214 14:09:51 @agent_ppo2.py:185][0m |           0.0036 |         199.3632 |           9.3450 |
[32m[20221214 14:09:51 @agent_ppo2.py:185][0m |          -0.0037 |         195.9509 |           9.3453 |
[32m[20221214 14:09:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:09:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.31
[32m[20221214 14:09:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.80
[32m[20221214 14:09:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.46
[32m[20221214 14:09:51 @agent_ppo2.py:143][0m Total time:      11.81 min
[32m[20221214 14:09:51 @agent_ppo2.py:145][0m 1079296 total steps have happened
[32m[20221214 14:09:51 @agent_ppo2.py:121][0m #------------------------ Iteration 527 --------------------------#
[32m[20221214 14:09:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:51 @agent_ppo2.py:185][0m |           0.0081 |         207.3786 |           9.4072 |
[32m[20221214 14:09:51 @agent_ppo2.py:185][0m |          -0.0032 |         190.3006 |           9.3566 |
[32m[20221214 14:09:51 @agent_ppo2.py:185][0m |           0.0025 |         191.7948 |           9.3463 |
[32m[20221214 14:09:51 @agent_ppo2.py:185][0m |           0.0009 |         184.7758 |           9.3276 |
[32m[20221214 14:09:51 @agent_ppo2.py:185][0m |          -0.0036 |         182.5640 |           9.3503 |
[32m[20221214 14:09:52 @agent_ppo2.py:185][0m |          -0.0052 |         181.2172 |           9.3196 |
[32m[20221214 14:09:52 @agent_ppo2.py:185][0m |          -0.0039 |         180.5076 |           9.3021 |
[32m[20221214 14:09:52 @agent_ppo2.py:185][0m |          -0.0052 |         180.0839 |           9.2913 |
[32m[20221214 14:09:52 @agent_ppo2.py:185][0m |          -0.0051 |         180.2323 |           9.2763 |
[32m[20221214 14:09:52 @agent_ppo2.py:185][0m |          -0.0042 |         178.9706 |           9.2647 |
[32m[20221214 14:09:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:09:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.41
[32m[20221214 14:09:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.92
[32m[20221214 14:09:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.04
[32m[20221214 14:09:52 @agent_ppo2.py:143][0m Total time:      11.83 min
[32m[20221214 14:09:52 @agent_ppo2.py:145][0m 1081344 total steps have happened
[32m[20221214 14:09:52 @agent_ppo2.py:121][0m #------------------------ Iteration 528 --------------------------#
[32m[20221214 14:09:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:52 @agent_ppo2.py:185][0m |          -0.0003 |         219.3250 |           9.1380 |
[32m[20221214 14:09:52 @agent_ppo2.py:185][0m |          -0.0021 |         213.0424 |           9.1405 |
[32m[20221214 14:09:53 @agent_ppo2.py:185][0m |           0.0048 |         214.7714 |           9.1413 |
[32m[20221214 14:09:53 @agent_ppo2.py:185][0m |          -0.0009 |         208.8688 |           9.1609 |
[32m[20221214 14:09:53 @agent_ppo2.py:185][0m |          -0.0021 |         208.0633 |           9.1093 |
[32m[20221214 14:09:53 @agent_ppo2.py:185][0m |          -0.0042 |         207.1432 |           9.1082 |
[32m[20221214 14:09:53 @agent_ppo2.py:185][0m |          -0.0026 |         206.9560 |           9.1593 |
[32m[20221214 14:09:53 @agent_ppo2.py:185][0m |          -0.0034 |         206.6862 |           9.1065 |
[32m[20221214 14:09:53 @agent_ppo2.py:185][0m |          -0.0034 |         206.2691 |           9.0979 |
[32m[20221214 14:09:53 @agent_ppo2.py:185][0m |          -0.0002 |         206.2603 |           9.1123 |
[32m[20221214 14:09:53 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:09:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.28
[32m[20221214 14:09:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.46
[32m[20221214 14:09:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.83
[32m[20221214 14:09:53 @agent_ppo2.py:143][0m Total time:      11.85 min
[32m[20221214 14:09:53 @agent_ppo2.py:145][0m 1083392 total steps have happened
[32m[20221214 14:09:53 @agent_ppo2.py:121][0m #------------------------ Iteration 529 --------------------------#
[32m[20221214 14:09:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:54 @agent_ppo2.py:185][0m |           0.0002 |         218.8019 |           8.9989 |
[32m[20221214 14:09:54 @agent_ppo2.py:185][0m |          -0.0054 |         196.5035 |           8.9983 |
[32m[20221214 14:09:54 @agent_ppo2.py:185][0m |          -0.0006 |         195.0135 |           8.9945 |
[32m[20221214 14:09:54 @agent_ppo2.py:185][0m |          -0.0041 |         184.9524 |           8.9738 |
[32m[20221214 14:09:54 @agent_ppo2.py:185][0m |          -0.0045 |         181.7717 |           8.9523 |
[32m[20221214 14:09:54 @agent_ppo2.py:185][0m |          -0.0045 |         178.6820 |           8.9662 |
[32m[20221214 14:09:54 @agent_ppo2.py:185][0m |          -0.0065 |         176.5944 |           8.9502 |
[32m[20221214 14:09:54 @agent_ppo2.py:185][0m |          -0.0052 |         174.3272 |           8.9650 |
[32m[20221214 14:09:54 @agent_ppo2.py:185][0m |          -0.0046 |         171.9842 |           8.9317 |
[32m[20221214 14:09:54 @agent_ppo2.py:185][0m |          -0.0037 |         169.4842 |           8.9255 |
[32m[20221214 14:09:54 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:09:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 711.33
[32m[20221214 14:09:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.65
[32m[20221214 14:09:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.87
[32m[20221214 14:09:55 @agent_ppo2.py:143][0m Total time:      11.87 min
[32m[20221214 14:09:55 @agent_ppo2.py:145][0m 1085440 total steps have happened
[32m[20221214 14:09:55 @agent_ppo2.py:121][0m #------------------------ Iteration 530 --------------------------#
[32m[20221214 14:09:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:09:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:55 @agent_ppo2.py:185][0m |          -0.0021 |         190.2863 |           9.2295 |
[32m[20221214 14:09:55 @agent_ppo2.py:185][0m |          -0.0018 |         167.9562 |           9.2360 |
[32m[20221214 14:09:55 @agent_ppo2.py:185][0m |          -0.0031 |         161.8630 |           9.2356 |
[32m[20221214 14:09:55 @agent_ppo2.py:185][0m |          -0.0028 |         159.0161 |           9.2342 |
[32m[20221214 14:09:55 @agent_ppo2.py:185][0m |          -0.0042 |         155.6965 |           9.2383 |
[32m[20221214 14:09:55 @agent_ppo2.py:185][0m |          -0.0008 |         153.1427 |           9.2379 |
[32m[20221214 14:09:55 @agent_ppo2.py:185][0m |          -0.0025 |         150.6994 |           9.2468 |
[32m[20221214 14:09:56 @agent_ppo2.py:185][0m |           0.0143 |         172.0342 |           9.2588 |
[32m[20221214 14:09:56 @agent_ppo2.py:185][0m |          -0.0027 |         146.8509 |           9.2279 |
[32m[20221214 14:09:56 @agent_ppo2.py:185][0m |          -0.0033 |         145.2717 |           9.2511 |
[32m[20221214 14:09:56 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:09:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 687.31
[32m[20221214 14:09:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 779.69
[32m[20221214 14:09:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.47
[32m[20221214 14:09:56 @agent_ppo2.py:143][0m Total time:      11.90 min
[32m[20221214 14:09:56 @agent_ppo2.py:145][0m 1087488 total steps have happened
[32m[20221214 14:09:56 @agent_ppo2.py:121][0m #------------------------ Iteration 531 --------------------------#
[32m[20221214 14:09:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:56 @agent_ppo2.py:185][0m |          -0.0021 |         184.9908 |           9.2093 |
[32m[20221214 14:09:56 @agent_ppo2.py:185][0m |           0.0048 |         169.8843 |           9.1984 |
[32m[20221214 14:09:56 @agent_ppo2.py:185][0m |          -0.0020 |         163.8557 |           9.2000 |
[32m[20221214 14:09:56 @agent_ppo2.py:185][0m |          -0.0020 |         160.9837 |           9.1714 |
[32m[20221214 14:09:56 @agent_ppo2.py:185][0m |          -0.0023 |         158.7690 |           9.1693 |
[32m[20221214 14:09:57 @agent_ppo2.py:185][0m |          -0.0011 |         157.6664 |           9.1322 |
[32m[20221214 14:09:57 @agent_ppo2.py:185][0m |          -0.0034 |         156.3547 |           9.1213 |
[32m[20221214 14:09:57 @agent_ppo2.py:185][0m |           0.0026 |         159.8056 |           9.1150 |
[32m[20221214 14:09:57 @agent_ppo2.py:185][0m |          -0.0029 |         154.3514 |           9.0824 |
[32m[20221214 14:09:57 @agent_ppo2.py:185][0m |           0.0045 |         153.5907 |           9.0867 |
[32m[20221214 14:09:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:09:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 770.01
[32m[20221214 14:09:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.84
[32m[20221214 14:09:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.48
[32m[20221214 14:09:57 @agent_ppo2.py:143][0m Total time:      11.92 min
[32m[20221214 14:09:57 @agent_ppo2.py:145][0m 1089536 total steps have happened
[32m[20221214 14:09:57 @agent_ppo2.py:121][0m #------------------------ Iteration 532 --------------------------#
[32m[20221214 14:09:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:09:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:57 @agent_ppo2.py:185][0m |           0.0100 |         221.5663 |           8.8910 |
[32m[20221214 14:09:57 @agent_ppo2.py:185][0m |          -0.0023 |         184.3438 |           8.8986 |
[32m[20221214 14:09:58 @agent_ppo2.py:185][0m |           0.0007 |         172.6784 |           8.9236 |
[32m[20221214 14:09:58 @agent_ppo2.py:185][0m |           0.0011 |         165.8536 |           8.9199 |
[32m[20221214 14:09:58 @agent_ppo2.py:185][0m |           0.0055 |         167.3815 |           8.9184 |
[32m[20221214 14:09:58 @agent_ppo2.py:185][0m |          -0.0030 |         158.2311 |           8.9430 |
[32m[20221214 14:09:58 @agent_ppo2.py:185][0m |          -0.0015 |         156.7443 |           8.9565 |
[32m[20221214 14:09:58 @agent_ppo2.py:185][0m |           0.0012 |         155.7849 |           8.9545 |
[32m[20221214 14:09:58 @agent_ppo2.py:185][0m |           0.0020 |         154.8893 |           8.9520 |
[32m[20221214 14:09:58 @agent_ppo2.py:185][0m |          -0.0042 |         153.5593 |           8.9471 |
[32m[20221214 14:09:58 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:09:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 736.02
[32m[20221214 14:09:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.49
[32m[20221214 14:09:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.43
[32m[20221214 14:09:58 @agent_ppo2.py:143][0m Total time:      11.94 min
[32m[20221214 14:09:58 @agent_ppo2.py:145][0m 1091584 total steps have happened
[32m[20221214 14:09:58 @agent_ppo2.py:121][0m #------------------------ Iteration 533 --------------------------#
[32m[20221214 14:09:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:09:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:09:59 @agent_ppo2.py:185][0m |          -0.0015 |         185.2222 |           9.0563 |
[32m[20221214 14:09:59 @agent_ppo2.py:185][0m |          -0.0036 |         166.2746 |           9.0511 |
[32m[20221214 14:09:59 @agent_ppo2.py:185][0m |          -0.0014 |         160.3705 |           9.0660 |
[32m[20221214 14:09:59 @agent_ppo2.py:185][0m |           0.0082 |         164.3924 |           9.0803 |
[32m[20221214 14:09:59 @agent_ppo2.py:185][0m |          -0.0042 |         155.2012 |           9.0980 |
[32m[20221214 14:09:59 @agent_ppo2.py:185][0m |           0.0020 |         153.3148 |           9.0998 |
[32m[20221214 14:09:59 @agent_ppo2.py:185][0m |           0.0014 |         151.8372 |           9.1109 |
[32m[20221214 14:09:59 @agent_ppo2.py:185][0m |          -0.0013 |         151.0062 |           9.1071 |
[32m[20221214 14:09:59 @agent_ppo2.py:185][0m |          -0.0018 |         149.8372 |           9.1082 |
[32m[20221214 14:09:59 @agent_ppo2.py:185][0m |          -0.0004 |         149.0337 |           9.1049 |
[32m[20221214 14:09:59 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:10:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 761.57
[32m[20221214 14:10:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.33
[32m[20221214 14:10:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 748.73
[32m[20221214 14:10:00 @agent_ppo2.py:143][0m Total time:      11.96 min
[32m[20221214 14:10:00 @agent_ppo2.py:145][0m 1093632 total steps have happened
[32m[20221214 14:10:00 @agent_ppo2.py:121][0m #------------------------ Iteration 534 --------------------------#
[32m[20221214 14:10:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:10:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:00 @agent_ppo2.py:185][0m |          -0.0079 |         170.1441 |           8.6052 |
[32m[20221214 14:10:00 @agent_ppo2.py:185][0m |          -0.0028 |         136.4961 |           8.6114 |
[32m[20221214 14:10:00 @agent_ppo2.py:185][0m |          -0.0009 |         129.4702 |           8.6338 |
[32m[20221214 14:10:00 @agent_ppo2.py:185][0m |           0.0010 |         126.9706 |           8.6562 |
[32m[20221214 14:10:00 @agent_ppo2.py:185][0m |          -0.0039 |         118.7818 |           8.6755 |
[32m[20221214 14:10:00 @agent_ppo2.py:185][0m |          -0.0063 |         115.1211 |           8.6968 |
[32m[20221214 14:10:00 @agent_ppo2.py:185][0m |          -0.0103 |         112.4138 |           8.7353 |
[32m[20221214 14:10:00 @agent_ppo2.py:185][0m |          -0.0066 |         110.3576 |           8.7395 |
[32m[20221214 14:10:01 @agent_ppo2.py:185][0m |          -0.0094 |         108.7285 |           8.7624 |
[32m[20221214 14:10:01 @agent_ppo2.py:185][0m |          -0.0118 |         107.5326 |           8.7756 |
[32m[20221214 14:10:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:10:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 669.28
[32m[20221214 14:10:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 761.74
[32m[20221214 14:10:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.83
[32m[20221214 14:10:01 @agent_ppo2.py:143][0m Total time:      11.98 min
[32m[20221214 14:10:01 @agent_ppo2.py:145][0m 1095680 total steps have happened
[32m[20221214 14:10:01 @agent_ppo2.py:121][0m #------------------------ Iteration 535 --------------------------#
[32m[20221214 14:10:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:10:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:01 @agent_ppo2.py:185][0m |           0.0016 |         137.7488 |           8.9668 |
[32m[20221214 14:10:01 @agent_ppo2.py:185][0m |          -0.0055 |         119.8702 |           8.9554 |
[32m[20221214 14:10:01 @agent_ppo2.py:185][0m |           0.0117 |         135.8601 |           8.9618 |
[32m[20221214 14:10:01 @agent_ppo2.py:185][0m |          -0.0042 |         110.6537 |           8.9462 |
[32m[20221214 14:10:01 @agent_ppo2.py:185][0m |          -0.0049 |         107.7967 |           8.9177 |
[32m[20221214 14:10:02 @agent_ppo2.py:185][0m |          -0.0043 |         106.5871 |           8.9093 |
[32m[20221214 14:10:02 @agent_ppo2.py:185][0m |          -0.0020 |         105.2888 |           8.9019 |
[32m[20221214 14:10:02 @agent_ppo2.py:185][0m |          -0.0072 |         103.1401 |           8.8772 |
[32m[20221214 14:10:02 @agent_ppo2.py:185][0m |          -0.0048 |         102.2363 |           8.9031 |
[32m[20221214 14:10:02 @agent_ppo2.py:185][0m |          -0.0003 |         102.7928 |           8.8788 |
[32m[20221214 14:10:02 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:10:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 706.01
[32m[20221214 14:10:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.17
[32m[20221214 14:10:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.48
[32m[20221214 14:10:02 @agent_ppo2.py:143][0m Total time:      12.00 min
[32m[20221214 14:10:02 @agent_ppo2.py:145][0m 1097728 total steps have happened
[32m[20221214 14:10:02 @agent_ppo2.py:121][0m #------------------------ Iteration 536 --------------------------#
[32m[20221214 14:10:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:10:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:02 @agent_ppo2.py:185][0m |          -0.0003 |         170.7851 |           9.3690 |
[32m[20221214 14:10:02 @agent_ppo2.py:185][0m |           0.0019 |         159.2532 |           9.3749 |
[32m[20221214 14:10:03 @agent_ppo2.py:185][0m |          -0.0023 |         151.4703 |           9.3774 |
[32m[20221214 14:10:03 @agent_ppo2.py:185][0m |           0.0002 |         149.8244 |           9.3823 |
[32m[20221214 14:10:03 @agent_ppo2.py:185][0m |          -0.0042 |         145.8916 |           9.4044 |
[32m[20221214 14:10:03 @agent_ppo2.py:185][0m |           0.0008 |         146.6967 |           9.4092 |
[32m[20221214 14:10:03 @agent_ppo2.py:185][0m |          -0.0042 |         142.9849 |           9.4230 |
[32m[20221214 14:10:03 @agent_ppo2.py:185][0m |          -0.0032 |         142.4899 |           9.4306 |
[32m[20221214 14:10:03 @agent_ppo2.py:185][0m |          -0.0042 |         141.0921 |           9.4252 |
[32m[20221214 14:10:03 @agent_ppo2.py:185][0m |          -0.0025 |         139.7837 |           9.4449 |
[32m[20221214 14:10:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:10:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 763.86
[32m[20221214 14:10:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.73
[32m[20221214 14:10:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.89
[32m[20221214 14:10:03 @agent_ppo2.py:143][0m Total time:      12.02 min
[32m[20221214 14:10:03 @agent_ppo2.py:145][0m 1099776 total steps have happened
[32m[20221214 14:10:03 @agent_ppo2.py:121][0m #------------------------ Iteration 537 --------------------------#
[32m[20221214 14:10:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:10:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:04 @agent_ppo2.py:185][0m |          -0.0011 |         212.5978 |           9.3246 |
[32m[20221214 14:10:04 @agent_ppo2.py:185][0m |          -0.0021 |         199.5288 |           9.3379 |
[32m[20221214 14:10:04 @agent_ppo2.py:185][0m |          -0.0026 |         193.8629 |           9.3340 |
[32m[20221214 14:10:04 @agent_ppo2.py:185][0m |          -0.0017 |         191.0085 |           9.3492 |
[32m[20221214 14:10:04 @agent_ppo2.py:185][0m |          -0.0023 |         189.2613 |           9.3389 |
[32m[20221214 14:10:04 @agent_ppo2.py:185][0m |          -0.0005 |         187.8657 |           9.3253 |
[32m[20221214 14:10:04 @agent_ppo2.py:185][0m |          -0.0030 |         186.8210 |           9.3243 |
[32m[20221214 14:10:04 @agent_ppo2.py:185][0m |           0.0062 |         191.8445 |           9.3276 |
[32m[20221214 14:10:04 @agent_ppo2.py:185][0m |          -0.0011 |         184.7880 |           9.3176 |
[32m[20221214 14:10:04 @agent_ppo2.py:185][0m |          -0.0038 |         183.9503 |           9.3002 |
[32m[20221214 14:10:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:10:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.19
[32m[20221214 14:10:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.28
[32m[20221214 14:10:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.90
[32m[20221214 14:10:05 @agent_ppo2.py:143][0m Total time:      12.04 min
[32m[20221214 14:10:05 @agent_ppo2.py:145][0m 1101824 total steps have happened
[32m[20221214 14:10:05 @agent_ppo2.py:121][0m #------------------------ Iteration 538 --------------------------#
[32m[20221214 14:10:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:10:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:05 @agent_ppo2.py:185][0m |          -0.0017 |         210.5277 |           9.1893 |
[32m[20221214 14:10:05 @agent_ppo2.py:185][0m |          -0.0013 |         201.1768 |           9.1982 |
[32m[20221214 14:10:05 @agent_ppo2.py:185][0m |          -0.0020 |         197.9165 |           9.2124 |
[32m[20221214 14:10:05 @agent_ppo2.py:185][0m |          -0.0022 |         195.4075 |           9.1957 |
[32m[20221214 14:10:05 @agent_ppo2.py:185][0m |          -0.0001 |         193.7538 |           9.2081 |
[32m[20221214 14:10:05 @agent_ppo2.py:185][0m |          -0.0039 |         192.3572 |           9.2096 |
[32m[20221214 14:10:05 @agent_ppo2.py:185][0m |           0.0099 |         213.2025 |           9.2273 |
[32m[20221214 14:10:05 @agent_ppo2.py:185][0m |          -0.0034 |         190.8581 |           9.2595 |
[32m[20221214 14:10:06 @agent_ppo2.py:185][0m |          -0.0034 |         189.8497 |           9.2286 |
[32m[20221214 14:10:06 @agent_ppo2.py:185][0m |          -0.0032 |         189.7819 |           9.2407 |
[32m[20221214 14:10:06 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:10:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.20
[32m[20221214 14:10:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.62
[32m[20221214 14:10:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.75
[32m[20221214 14:10:06 @agent_ppo2.py:143][0m Total time:      12.06 min
[32m[20221214 14:10:06 @agent_ppo2.py:145][0m 1103872 total steps have happened
[32m[20221214 14:10:06 @agent_ppo2.py:121][0m #------------------------ Iteration 539 --------------------------#
[32m[20221214 14:10:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:10:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:06 @agent_ppo2.py:185][0m |          -0.0016 |         219.2950 |           9.3821 |
[32m[20221214 14:10:06 @agent_ppo2.py:185][0m |          -0.0011 |         208.2471 |           9.3767 |
[32m[20221214 14:10:06 @agent_ppo2.py:185][0m |          -0.0041 |         203.0932 |           9.3730 |
[32m[20221214 14:10:06 @agent_ppo2.py:185][0m |           0.0034 |         207.0916 |           9.3530 |
[32m[20221214 14:10:06 @agent_ppo2.py:185][0m |          -0.0003 |         200.7402 |           9.3659 |
[32m[20221214 14:10:07 @agent_ppo2.py:185][0m |          -0.0015 |         196.4310 |           9.3654 |
[32m[20221214 14:10:07 @agent_ppo2.py:185][0m |          -0.0010 |         195.7323 |           9.3184 |
[32m[20221214 14:10:07 @agent_ppo2.py:185][0m |          -0.0023 |         195.2196 |           9.3257 |
[32m[20221214 14:10:07 @agent_ppo2.py:185][0m |          -0.0026 |         194.2833 |           9.3030 |
[32m[20221214 14:10:07 @agent_ppo2.py:185][0m |          -0.0039 |         193.6369 |           9.2697 |
[32m[20221214 14:10:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:10:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.30
[32m[20221214 14:10:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.09
[32m[20221214 14:10:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.29
[32m[20221214 14:10:07 @agent_ppo2.py:143][0m Total time:      12.08 min
[32m[20221214 14:10:07 @agent_ppo2.py:145][0m 1105920 total steps have happened
[32m[20221214 14:10:07 @agent_ppo2.py:121][0m #------------------------ Iteration 540 --------------------------#
[32m[20221214 14:10:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:10:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:07 @agent_ppo2.py:185][0m |           0.0014 |         230.7015 |           8.6394 |
[32m[20221214 14:10:07 @agent_ppo2.py:185][0m |          -0.0014 |         213.9672 |           8.6543 |
[32m[20221214 14:10:08 @agent_ppo2.py:185][0m |          -0.0019 |         208.6418 |           8.6557 |
[32m[20221214 14:10:08 @agent_ppo2.py:185][0m |          -0.0015 |         206.3159 |           8.6476 |
[32m[20221214 14:10:08 @agent_ppo2.py:185][0m |          -0.0029 |         205.8184 |           8.6764 |
[32m[20221214 14:10:08 @agent_ppo2.py:185][0m |           0.0002 |         204.5119 |           8.6478 |
[32m[20221214 14:10:08 @agent_ppo2.py:185][0m |          -0.0005 |         203.7624 |           8.6620 |
[32m[20221214 14:10:08 @agent_ppo2.py:185][0m |           0.0197 |         232.7352 |           8.6536 |
[32m[20221214 14:10:08 @agent_ppo2.py:185][0m |           0.0035 |         206.8227 |           8.7091 |
[32m[20221214 14:10:08 @agent_ppo2.py:185][0m |          -0.0013 |         202.5743 |           8.6567 |
[32m[20221214 14:10:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:10:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.14
[32m[20221214 14:10:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.32
[32m[20221214 14:10:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.13
[32m[20221214 14:10:08 @agent_ppo2.py:143][0m Total time:      12.10 min
[32m[20221214 14:10:08 @agent_ppo2.py:145][0m 1107968 total steps have happened
[32m[20221214 14:10:08 @agent_ppo2.py:121][0m #------------------------ Iteration 541 --------------------------#
[32m[20221214 14:10:09 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:10:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:09 @agent_ppo2.py:185][0m |          -0.0030 |         232.6250 |           8.8894 |
[32m[20221214 14:10:09 @agent_ppo2.py:185][0m |          -0.0030 |         222.6766 |           8.8623 |
[32m[20221214 14:10:09 @agent_ppo2.py:185][0m |          -0.0038 |         218.5431 |           8.8732 |
[32m[20221214 14:10:09 @agent_ppo2.py:185][0m |           0.0050 |         223.6404 |           8.8442 |
[32m[20221214 14:10:09 @agent_ppo2.py:185][0m |          -0.0018 |         215.0830 |           8.8621 |
[32m[20221214 14:10:09 @agent_ppo2.py:185][0m |          -0.0040 |         214.5087 |           8.8250 |
[32m[20221214 14:10:09 @agent_ppo2.py:185][0m |          -0.0032 |         213.7513 |           8.8156 |
[32m[20221214 14:10:10 @agent_ppo2.py:185][0m |          -0.0037 |         213.3593 |           8.8295 |
[32m[20221214 14:10:10 @agent_ppo2.py:185][0m |           0.0014 |         213.5271 |           8.7966 |
[32m[20221214 14:10:10 @agent_ppo2.py:185][0m |          -0.0017 |         212.4619 |           8.7802 |
[32m[20221214 14:10:10 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 14:10:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.80
[32m[20221214 14:10:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.12
[32m[20221214 14:10:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.87
[32m[20221214 14:10:10 @agent_ppo2.py:143][0m Total time:      12.13 min
[32m[20221214 14:10:10 @agent_ppo2.py:145][0m 1110016 total steps have happened
[32m[20221214 14:10:10 @agent_ppo2.py:121][0m #------------------------ Iteration 542 --------------------------#
[32m[20221214 14:10:10 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:10:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:10 @agent_ppo2.py:185][0m |           0.0007 |         229.2630 |           8.8131 |
[32m[20221214 14:10:10 @agent_ppo2.py:185][0m |          -0.0029 |         224.1329 |           8.8389 |
[32m[20221214 14:10:10 @agent_ppo2.py:185][0m |          -0.0029 |         222.7095 |           8.8297 |
[32m[20221214 14:10:11 @agent_ppo2.py:185][0m |          -0.0029 |         221.8403 |           8.8488 |
[32m[20221214 14:10:11 @agent_ppo2.py:185][0m |          -0.0030 |         220.2713 |           8.8580 |
[32m[20221214 14:10:11 @agent_ppo2.py:185][0m |           0.0048 |         230.0716 |           8.8422 |
[32m[20221214 14:10:11 @agent_ppo2.py:185][0m |          -0.0024 |         218.8343 |           8.8722 |
[32m[20221214 14:10:11 @agent_ppo2.py:185][0m |          -0.0027 |         218.6919 |           8.8774 |
[32m[20221214 14:10:11 @agent_ppo2.py:185][0m |          -0.0032 |         218.0545 |           8.8709 |
[32m[20221214 14:10:11 @agent_ppo2.py:185][0m |           0.0005 |         217.8146 |           8.8852 |
[32m[20221214 14:10:11 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:10:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.53
[32m[20221214 14:10:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.64
[32m[20221214 14:10:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.57
[32m[20221214 14:10:11 @agent_ppo2.py:143][0m Total time:      12.15 min
[32m[20221214 14:10:11 @agent_ppo2.py:145][0m 1112064 total steps have happened
[32m[20221214 14:10:11 @agent_ppo2.py:121][0m #------------------------ Iteration 543 --------------------------#
[32m[20221214 14:10:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:12 @agent_ppo2.py:185][0m |          -0.0015 |         241.2928 |           9.0187 |
[32m[20221214 14:10:12 @agent_ppo2.py:185][0m |          -0.0009 |         235.4960 |           9.0195 |
[32m[20221214 14:10:12 @agent_ppo2.py:185][0m |          -0.0015 |         233.0904 |           8.9994 |
[32m[20221214 14:10:12 @agent_ppo2.py:185][0m |          -0.0037 |         230.8659 |           9.0046 |
[32m[20221214 14:10:12 @agent_ppo2.py:185][0m |          -0.0030 |         229.3048 |           8.9944 |
[32m[20221214 14:10:12 @agent_ppo2.py:185][0m |           0.0045 |         237.9358 |           8.9823 |
[32m[20221214 14:10:12 @agent_ppo2.py:185][0m |           0.0145 |         247.6583 |           8.9987 |
[32m[20221214 14:10:12 @agent_ppo2.py:185][0m |          -0.0027 |         226.7941 |           9.0212 |
[32m[20221214 14:10:12 @agent_ppo2.py:185][0m |          -0.0045 |         225.8596 |           8.9714 |
[32m[20221214 14:10:12 @agent_ppo2.py:185][0m |          -0.0042 |         225.0733 |           8.9725 |
[32m[20221214 14:10:12 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:10:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.43
[32m[20221214 14:10:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.32
[32m[20221214 14:10:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.36
[32m[20221214 14:10:13 @agent_ppo2.py:143][0m Total time:      12.18 min
[32m[20221214 14:10:13 @agent_ppo2.py:145][0m 1114112 total steps have happened
[32m[20221214 14:10:13 @agent_ppo2.py:121][0m #------------------------ Iteration 544 --------------------------#
[32m[20221214 14:10:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:13 @agent_ppo2.py:185][0m |           0.0048 |         245.6751 |           8.8318 |
[32m[20221214 14:10:13 @agent_ppo2.py:185][0m |          -0.0046 |         227.8635 |           8.8136 |
[32m[20221214 14:10:13 @agent_ppo2.py:185][0m |          -0.0035 |         224.9935 |           8.7959 |
[32m[20221214 14:10:13 @agent_ppo2.py:185][0m |          -0.0040 |         223.4078 |           8.8035 |
[32m[20221214 14:10:13 @agent_ppo2.py:185][0m |           0.0045 |         230.3926 |           8.7761 |
[32m[20221214 14:10:13 @agent_ppo2.py:185][0m |          -0.0040 |         221.3753 |           8.7429 |
[32m[20221214 14:10:14 @agent_ppo2.py:185][0m |          -0.0048 |         220.5172 |           8.7531 |
[32m[20221214 14:10:14 @agent_ppo2.py:185][0m |           0.0091 |         242.4099 |           8.7512 |
[32m[20221214 14:10:14 @agent_ppo2.py:185][0m |           0.0007 |         219.8729 |           8.7281 |
[32m[20221214 14:10:14 @agent_ppo2.py:185][0m |           0.0045 |         224.8086 |           8.7105 |
[32m[20221214 14:10:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:10:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.28
[32m[20221214 14:10:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.83
[32m[20221214 14:10:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.65
[32m[20221214 14:10:14 @agent_ppo2.py:143][0m Total time:      12.20 min
[32m[20221214 14:10:14 @agent_ppo2.py:145][0m 1116160 total steps have happened
[32m[20221214 14:10:14 @agent_ppo2.py:121][0m #------------------------ Iteration 545 --------------------------#
[32m[20221214 14:10:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:10:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:14 @agent_ppo2.py:185][0m |          -0.0030 |         243.7929 |           8.6301 |
[32m[20221214 14:10:14 @agent_ppo2.py:185][0m |          -0.0017 |         231.0872 |           8.6372 |
[32m[20221214 14:10:15 @agent_ppo2.py:185][0m |           0.0010 |         226.9037 |           8.6307 |
[32m[20221214 14:10:15 @agent_ppo2.py:185][0m |          -0.0018 |         223.8077 |           8.6435 |
[32m[20221214 14:10:15 @agent_ppo2.py:185][0m |           0.0005 |         224.8784 |           8.6643 |
[32m[20221214 14:10:15 @agent_ppo2.py:185][0m |          -0.0034 |         221.7232 |           8.6695 |
[32m[20221214 14:10:15 @agent_ppo2.py:185][0m |          -0.0007 |         221.2240 |           8.6588 |
[32m[20221214 14:10:15 @agent_ppo2.py:185][0m |           0.0105 |         241.0240 |           8.6676 |
[32m[20221214 14:10:15 @agent_ppo2.py:185][0m |          -0.0026 |         219.9950 |           8.6539 |
[32m[20221214 14:10:15 @agent_ppo2.py:185][0m |          -0.0018 |         219.6326 |           8.6589 |
[32m[20221214 14:10:15 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:10:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.41
[32m[20221214 14:10:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.37
[32m[20221214 14:10:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.67
[32m[20221214 14:10:15 @agent_ppo2.py:143][0m Total time:      12.22 min
[32m[20221214 14:10:15 @agent_ppo2.py:145][0m 1118208 total steps have happened
[32m[20221214 14:10:15 @agent_ppo2.py:121][0m #------------------------ Iteration 546 --------------------------#
[32m[20221214 14:10:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:16 @agent_ppo2.py:185][0m |          -0.0010 |         224.1888 |           8.8210 |
[32m[20221214 14:10:16 @agent_ppo2.py:185][0m |          -0.0025 |         214.7780 |           8.7777 |
[32m[20221214 14:10:16 @agent_ppo2.py:185][0m |          -0.0001 |         210.4162 |           8.7502 |
[32m[20221214 14:10:16 @agent_ppo2.py:185][0m |          -0.0032 |         208.6974 |           8.7389 |
[32m[20221214 14:10:16 @agent_ppo2.py:185][0m |          -0.0027 |         206.6432 |           8.7292 |
[32m[20221214 14:10:16 @agent_ppo2.py:185][0m |          -0.0032 |         205.6588 |           8.6781 |
[32m[20221214 14:10:16 @agent_ppo2.py:185][0m |          -0.0039 |         205.6185 |           8.7025 |
[32m[20221214 14:10:16 @agent_ppo2.py:185][0m |          -0.0024 |         203.9209 |           8.6463 |
[32m[20221214 14:10:16 @agent_ppo2.py:185][0m |          -0.0018 |         203.5129 |           8.6430 |
[32m[20221214 14:10:17 @agent_ppo2.py:185][0m |          -0.0028 |         203.0110 |           8.6143 |
[32m[20221214 14:10:17 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:10:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.81
[32m[20221214 14:10:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.23
[32m[20221214 14:10:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.13
[32m[20221214 14:10:17 @agent_ppo2.py:143][0m Total time:      12.24 min
[32m[20221214 14:10:17 @agent_ppo2.py:145][0m 1120256 total steps have happened
[32m[20221214 14:10:17 @agent_ppo2.py:121][0m #------------------------ Iteration 547 --------------------------#
[32m[20221214 14:10:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:17 @agent_ppo2.py:185][0m |          -0.0026 |         230.1858 |           8.5275 |
[32m[20221214 14:10:17 @agent_ppo2.py:185][0m |          -0.0038 |         221.1785 |           8.5359 |
[32m[20221214 14:10:17 @agent_ppo2.py:185][0m |           0.0103 |         232.1722 |           8.5470 |
[32m[20221214 14:10:17 @agent_ppo2.py:185][0m |          -0.0032 |         215.3927 |           8.5292 |
[32m[20221214 14:10:17 @agent_ppo2.py:185][0m |          -0.0027 |         213.9757 |           8.5558 |
[32m[20221214 14:10:18 @agent_ppo2.py:185][0m |          -0.0030 |         213.2061 |           8.5528 |
[32m[20221214 14:10:18 @agent_ppo2.py:185][0m |          -0.0033 |         212.5398 |           8.5437 |
[32m[20221214 14:10:18 @agent_ppo2.py:185][0m |           0.0026 |         214.3874 |           8.5511 |
[32m[20221214 14:10:18 @agent_ppo2.py:185][0m |          -0.0029 |         211.0722 |           8.5603 |
[32m[20221214 14:10:18 @agent_ppo2.py:185][0m |          -0.0008 |         210.4980 |           8.5507 |
[32m[20221214 14:10:18 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:10:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.47
[32m[20221214 14:10:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.64
[32m[20221214 14:10:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.75
[32m[20221214 14:10:18 @agent_ppo2.py:143][0m Total time:      12.27 min
[32m[20221214 14:10:18 @agent_ppo2.py:145][0m 1122304 total steps have happened
[32m[20221214 14:10:18 @agent_ppo2.py:121][0m #------------------------ Iteration 548 --------------------------#
[32m[20221214 14:10:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:18 @agent_ppo2.py:185][0m |           0.0002 |         234.4159 |           8.6710 |
[32m[20221214 14:10:19 @agent_ppo2.py:185][0m |          -0.0024 |         220.5760 |           8.6249 |
[32m[20221214 14:10:19 @agent_ppo2.py:185][0m |          -0.0015 |         216.1780 |           8.6565 |
[32m[20221214 14:10:19 @agent_ppo2.py:185][0m |          -0.0022 |         213.8976 |           8.5831 |
[32m[20221214 14:10:19 @agent_ppo2.py:185][0m |          -0.0022 |         212.8623 |           8.5907 |
[32m[20221214 14:10:19 @agent_ppo2.py:185][0m |          -0.0021 |         210.8269 |           8.5486 |
[32m[20221214 14:10:19 @agent_ppo2.py:185][0m |          -0.0001 |         210.0713 |           8.5176 |
[32m[20221214 14:10:19 @agent_ppo2.py:185][0m |          -0.0025 |         209.1772 |           8.5222 |
[32m[20221214 14:10:19 @agent_ppo2.py:185][0m |           0.0004 |         208.0138 |           8.4923 |
[32m[20221214 14:10:19 @agent_ppo2.py:185][0m |          -0.0027 |         206.8231 |           8.4551 |
[32m[20221214 14:10:19 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:10:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.02
[32m[20221214 14:10:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.80
[32m[20221214 14:10:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.02
[32m[20221214 14:10:19 @agent_ppo2.py:143][0m Total time:      12.29 min
[32m[20221214 14:10:19 @agent_ppo2.py:145][0m 1124352 total steps have happened
[32m[20221214 14:10:19 @agent_ppo2.py:121][0m #------------------------ Iteration 549 --------------------------#
[32m[20221214 14:10:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:20 @agent_ppo2.py:185][0m |          -0.0010 |         228.1047 |           8.3187 |
[32m[20221214 14:10:20 @agent_ppo2.py:185][0m |           0.0053 |         225.8606 |           8.3555 |
[32m[20221214 14:10:20 @agent_ppo2.py:185][0m |          -0.0014 |         214.7933 |           8.3754 |
[32m[20221214 14:10:20 @agent_ppo2.py:185][0m |          -0.0008 |         212.1089 |           8.3736 |
[32m[20221214 14:10:20 @agent_ppo2.py:185][0m |          -0.0010 |         210.4053 |           8.3472 |
[32m[20221214 14:10:20 @agent_ppo2.py:185][0m |           0.0078 |         222.6777 |           8.3833 |
[32m[20221214 14:10:20 @agent_ppo2.py:185][0m |           0.0086 |         220.6372 |           8.3764 |
[32m[20221214 14:10:21 @agent_ppo2.py:185][0m |          -0.0013 |         208.9265 |           8.3528 |
[32m[20221214 14:10:21 @agent_ppo2.py:185][0m |          -0.0023 |         207.7006 |           8.3542 |
[32m[20221214 14:10:21 @agent_ppo2.py:185][0m |           0.0077 |         218.1079 |           8.3615 |
[32m[20221214 14:10:21 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:10:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.12
[32m[20221214 14:10:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.62
[32m[20221214 14:10:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.21
[32m[20221214 14:10:21 @agent_ppo2.py:143][0m Total time:      12.31 min
[32m[20221214 14:10:21 @agent_ppo2.py:145][0m 1126400 total steps have happened
[32m[20221214 14:10:21 @agent_ppo2.py:121][0m #------------------------ Iteration 550 --------------------------#
[32m[20221214 14:10:21 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:10:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:21 @agent_ppo2.py:185][0m |           0.0049 |         256.6830 |           8.4160 |
[32m[20221214 14:10:21 @agent_ppo2.py:185][0m |          -0.0060 |         237.2488 |           8.4195 |
[32m[20221214 14:10:21 @agent_ppo2.py:185][0m |          -0.0055 |         234.3411 |           8.4191 |
[32m[20221214 14:10:22 @agent_ppo2.py:185][0m |          -0.0026 |         233.7811 |           8.4061 |
[32m[20221214 14:10:22 @agent_ppo2.py:185][0m |          -0.0057 |         232.1056 |           8.4086 |
[32m[20221214 14:10:22 @agent_ppo2.py:185][0m |          -0.0045 |         230.9058 |           8.3961 |
[32m[20221214 14:10:22 @agent_ppo2.py:185][0m |          -0.0057 |         230.2305 |           8.4027 |
[32m[20221214 14:10:22 @agent_ppo2.py:185][0m |          -0.0050 |         229.1262 |           8.4075 |
[32m[20221214 14:10:22 @agent_ppo2.py:185][0m |          -0.0058 |         228.5685 |           8.4036 |
[32m[20221214 14:10:22 @agent_ppo2.py:185][0m |          -0.0050 |         228.8080 |           8.3868 |
[32m[20221214 14:10:22 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:10:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.05
[32m[20221214 14:10:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.67
[32m[20221214 14:10:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.29
[32m[20221214 14:10:22 @agent_ppo2.py:143][0m Total time:      12.34 min
[32m[20221214 14:10:22 @agent_ppo2.py:145][0m 1128448 total steps have happened
[32m[20221214 14:10:22 @agent_ppo2.py:121][0m #------------------------ Iteration 551 --------------------------#
[32m[20221214 14:10:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:23 @agent_ppo2.py:185][0m |          -0.0014 |         264.0483 |           8.1708 |
[32m[20221214 14:10:23 @agent_ppo2.py:185][0m |           0.0023 |         261.5539 |           8.1979 |
[32m[20221214 14:10:23 @agent_ppo2.py:185][0m |          -0.0019 |         256.9000 |           8.1790 |
[32m[20221214 14:10:23 @agent_ppo2.py:185][0m |           0.0047 |         265.7184 |           8.1881 |
[32m[20221214 14:10:23 @agent_ppo2.py:185][0m |          -0.0011 |         254.7336 |           8.2055 |
[32m[20221214 14:10:23 @agent_ppo2.py:185][0m |          -0.0021 |         253.2202 |           8.1933 |
[32m[20221214 14:10:23 @agent_ppo2.py:185][0m |          -0.0010 |         252.4297 |           8.1649 |
[32m[20221214 14:10:23 @agent_ppo2.py:185][0m |          -0.0009 |         251.7808 |           8.1586 |
[32m[20221214 14:10:23 @agent_ppo2.py:185][0m |          -0.0011 |         251.1421 |           8.1745 |
[32m[20221214 14:10:23 @agent_ppo2.py:185][0m |          -0.0012 |         250.6323 |           8.1981 |
[32m[20221214 14:10:23 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:10:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.34
[32m[20221214 14:10:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.79
[32m[20221214 14:10:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.50
[32m[20221214 14:10:24 @agent_ppo2.py:143][0m Total time:      12.36 min
[32m[20221214 14:10:24 @agent_ppo2.py:145][0m 1130496 total steps have happened
[32m[20221214 14:10:24 @agent_ppo2.py:121][0m #------------------------ Iteration 552 --------------------------#
[32m[20221214 14:10:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:24 @agent_ppo2.py:185][0m |          -0.0005 |         235.0652 |           8.2798 |
[32m[20221214 14:10:24 @agent_ppo2.py:185][0m |          -0.0029 |         230.6471 |           8.2785 |
[32m[20221214 14:10:24 @agent_ppo2.py:185][0m |           0.0050 |         231.8794 |           8.2632 |
[32m[20221214 14:10:24 @agent_ppo2.py:185][0m |           0.0000 |         229.4720 |           8.2646 |
[32m[20221214 14:10:24 @agent_ppo2.py:185][0m |          -0.0031 |         226.7503 |           8.2632 |
[32m[20221214 14:10:24 @agent_ppo2.py:185][0m |          -0.0021 |         226.0227 |           8.2328 |
[32m[20221214 14:10:24 @agent_ppo2.py:185][0m |          -0.0032 |         225.6840 |           8.2315 |
[32m[20221214 14:10:25 @agent_ppo2.py:185][0m |          -0.0023 |         225.3255 |           8.2316 |
[32m[20221214 14:10:25 @agent_ppo2.py:185][0m |          -0.0033 |         224.1907 |           8.2220 |
[32m[20221214 14:10:25 @agent_ppo2.py:185][0m |           0.0045 |         233.2104 |           8.2173 |
[32m[20221214 14:10:25 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:10:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.22
[32m[20221214 14:10:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.70
[32m[20221214 14:10:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.98
[32m[20221214 14:10:25 @agent_ppo2.py:143][0m Total time:      12.38 min
[32m[20221214 14:10:25 @agent_ppo2.py:145][0m 1132544 total steps have happened
[32m[20221214 14:10:25 @agent_ppo2.py:121][0m #------------------------ Iteration 553 --------------------------#
[32m[20221214 14:10:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:25 @agent_ppo2.py:185][0m |           0.0058 |         267.3566 |           7.8794 |
[32m[20221214 14:10:25 @agent_ppo2.py:185][0m |          -0.0014 |         257.5401 |           7.8431 |
[32m[20221214 14:10:25 @agent_ppo2.py:185][0m |          -0.0037 |         257.2096 |           7.8589 |
[32m[20221214 14:10:26 @agent_ppo2.py:185][0m |          -0.0015 |         257.0681 |           7.8229 |
[32m[20221214 14:10:26 @agent_ppo2.py:185][0m |          -0.0026 |         254.7070 |           7.8242 |
[32m[20221214 14:10:26 @agent_ppo2.py:185][0m |          -0.0027 |         253.2783 |           7.7857 |
[32m[20221214 14:10:26 @agent_ppo2.py:185][0m |          -0.0020 |         252.4905 |           7.7497 |
[32m[20221214 14:10:26 @agent_ppo2.py:185][0m |          -0.0016 |         252.1887 |           7.7508 |
[32m[20221214 14:10:26 @agent_ppo2.py:185][0m |          -0.0029 |         251.9282 |           7.7202 |
[32m[20221214 14:10:26 @agent_ppo2.py:185][0m |          -0.0047 |         251.4658 |           7.7285 |
[32m[20221214 14:10:26 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:10:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.96
[32m[20221214 14:10:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.83
[32m[20221214 14:10:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.71
[32m[20221214 14:10:26 @agent_ppo2.py:143][0m Total time:      12.40 min
[32m[20221214 14:10:26 @agent_ppo2.py:145][0m 1134592 total steps have happened
[32m[20221214 14:10:26 @agent_ppo2.py:121][0m #------------------------ Iteration 554 --------------------------#
[32m[20221214 14:10:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:27 @agent_ppo2.py:185][0m |          -0.0023 |         256.4359 |           7.7659 |
[32m[20221214 14:10:27 @agent_ppo2.py:185][0m |          -0.0034 |         247.2581 |           7.7795 |
[32m[20221214 14:10:27 @agent_ppo2.py:185][0m |          -0.0021 |         244.0328 |           7.7883 |
[32m[20221214 14:10:27 @agent_ppo2.py:185][0m |          -0.0034 |         242.3633 |           7.8312 |
[32m[20221214 14:10:27 @agent_ppo2.py:185][0m |           0.0010 |         242.1821 |           7.7878 |
[32m[20221214 14:10:27 @agent_ppo2.py:185][0m |           0.0006 |         243.2313 |           7.8514 |
[32m[20221214 14:10:27 @agent_ppo2.py:185][0m |           0.0087 |         258.9659 |           7.8605 |
[32m[20221214 14:10:27 @agent_ppo2.py:185][0m |          -0.0037 |         237.5768 |           7.8610 |
[32m[20221214 14:10:27 @agent_ppo2.py:185][0m |          -0.0028 |         236.9793 |           7.8790 |
[32m[20221214 14:10:27 @agent_ppo2.py:185][0m |           0.0012 |         239.4969 |           7.9066 |
[32m[20221214 14:10:27 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:10:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.62
[32m[20221214 14:10:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.54
[32m[20221214 14:10:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.59
[32m[20221214 14:10:28 @agent_ppo2.py:143][0m Total time:      12.43 min
[32m[20221214 14:10:28 @agent_ppo2.py:145][0m 1136640 total steps have happened
[32m[20221214 14:10:28 @agent_ppo2.py:121][0m #------------------------ Iteration 555 --------------------------#
[32m[20221214 14:10:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:28 @agent_ppo2.py:185][0m |           0.0010 |         258.7527 |           8.0640 |
[32m[20221214 14:10:28 @agent_ppo2.py:185][0m |          -0.0031 |         239.7152 |           8.0770 |
[32m[20221214 14:10:28 @agent_ppo2.py:185][0m |          -0.0013 |         236.8155 |           8.1082 |
[32m[20221214 14:10:28 @agent_ppo2.py:185][0m |           0.0008 |         236.1472 |           8.1392 |
[32m[20221214 14:10:28 @agent_ppo2.py:185][0m |           0.0014 |         235.6597 |           8.1338 |
[32m[20221214 14:10:28 @agent_ppo2.py:185][0m |          -0.0021 |         234.7584 |           8.1698 |
[32m[20221214 14:10:29 @agent_ppo2.py:185][0m |          -0.0015 |         231.9454 |           8.1687 |
[32m[20221214 14:10:29 @agent_ppo2.py:185][0m |          -0.0016 |         231.3134 |           8.1885 |
[32m[20221214 14:10:29 @agent_ppo2.py:185][0m |          -0.0016 |         230.2187 |           8.2194 |
[32m[20221214 14:10:29 @agent_ppo2.py:185][0m |          -0.0032 |         230.4254 |           8.2233 |
[32m[20221214 14:10:29 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:10:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.84
[32m[20221214 14:10:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.95
[32m[20221214 14:10:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.09
[32m[20221214 14:10:29 @agent_ppo2.py:143][0m Total time:      12.45 min
[32m[20221214 14:10:29 @agent_ppo2.py:145][0m 1138688 total steps have happened
[32m[20221214 14:10:29 @agent_ppo2.py:121][0m #------------------------ Iteration 556 --------------------------#
[32m[20221214 14:10:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:29 @agent_ppo2.py:185][0m |           0.0103 |         278.7532 |           8.3687 |
[32m[20221214 14:10:29 @agent_ppo2.py:185][0m |          -0.0032 |         243.6375 |           8.4402 |
[32m[20221214 14:10:29 @agent_ppo2.py:185][0m |          -0.0039 |         240.5403 |           8.4300 |
[32m[20221214 14:10:30 @agent_ppo2.py:185][0m |          -0.0035 |         238.9947 |           8.4303 |
[32m[20221214 14:10:30 @agent_ppo2.py:185][0m |          -0.0020 |         238.0615 |           8.4566 |
[32m[20221214 14:10:30 @agent_ppo2.py:185][0m |          -0.0028 |         238.2622 |           8.4220 |
[32m[20221214 14:10:30 @agent_ppo2.py:185][0m |          -0.0026 |         237.1283 |           8.4307 |
[32m[20221214 14:10:30 @agent_ppo2.py:185][0m |          -0.0025 |         235.8744 |           8.4294 |
[32m[20221214 14:10:30 @agent_ppo2.py:185][0m |          -0.0032 |         236.3300 |           8.4356 |
[32m[20221214 14:10:30 @agent_ppo2.py:185][0m |          -0.0028 |         235.1124 |           8.4510 |
[32m[20221214 14:10:30 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:10:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.11
[32m[20221214 14:10:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.81
[32m[20221214 14:10:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.30
[32m[20221214 14:10:30 @agent_ppo2.py:143][0m Total time:      12.47 min
[32m[20221214 14:10:30 @agent_ppo2.py:145][0m 1140736 total steps have happened
[32m[20221214 14:10:30 @agent_ppo2.py:121][0m #------------------------ Iteration 557 --------------------------#
[32m[20221214 14:10:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:31 @agent_ppo2.py:185][0m |          -0.0031 |         261.2289 |           8.0590 |
[32m[20221214 14:10:31 @agent_ppo2.py:185][0m |          -0.0023 |         247.8490 |           8.0586 |
[32m[20221214 14:10:31 @agent_ppo2.py:185][0m |           0.0076 |         250.5234 |           8.0076 |
[32m[20221214 14:10:31 @agent_ppo2.py:185][0m |           0.0003 |         240.4461 |           7.9829 |
[32m[20221214 14:10:31 @agent_ppo2.py:185][0m |          -0.0046 |         232.3160 |           7.9540 |
[32m[20221214 14:10:31 @agent_ppo2.py:185][0m |          -0.0043 |         230.4585 |           7.9420 |
[32m[20221214 14:10:31 @agent_ppo2.py:185][0m |          -0.0041 |         228.0895 |           7.8733 |
[32m[20221214 14:10:31 @agent_ppo2.py:185][0m |           0.0041 |         231.8450 |           7.8682 |
[32m[20221214 14:10:31 @agent_ppo2.py:185][0m |          -0.0047 |         223.2602 |           7.8421 |
[32m[20221214 14:10:32 @agent_ppo2.py:185][0m |          -0.0039 |         221.1315 |           7.8155 |
[32m[20221214 14:10:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:10:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.24
[32m[20221214 14:10:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.95
[32m[20221214 14:10:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.92
[32m[20221214 14:10:32 @agent_ppo2.py:143][0m Total time:      12.49 min
[32m[20221214 14:10:32 @agent_ppo2.py:145][0m 1142784 total steps have happened
[32m[20221214 14:10:32 @agent_ppo2.py:121][0m #------------------------ Iteration 558 --------------------------#
[32m[20221214 14:10:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:32 @agent_ppo2.py:185][0m |          -0.0019 |         260.2685 |           7.8745 |
[32m[20221214 14:10:32 @agent_ppo2.py:185][0m |           0.0059 |         259.9708 |           7.8949 |
[32m[20221214 14:10:32 @agent_ppo2.py:185][0m |           0.0059 |         251.4948 |           7.8674 |
[32m[20221214 14:10:32 @agent_ppo2.py:185][0m |           0.0129 |         275.3372 |           7.8898 |
[32m[20221214 14:10:32 @agent_ppo2.py:185][0m |          -0.0001 |         237.7327 |           7.8715 |
[32m[20221214 14:10:33 @agent_ppo2.py:185][0m |          -0.0008 |         236.1528 |           7.8882 |
[32m[20221214 14:10:33 @agent_ppo2.py:185][0m |          -0.0030 |         234.8607 |           7.8728 |
[32m[20221214 14:10:33 @agent_ppo2.py:185][0m |          -0.0030 |         233.6778 |           7.8675 |
[32m[20221214 14:10:33 @agent_ppo2.py:185][0m |          -0.0005 |         232.9480 |           7.8438 |
[32m[20221214 14:10:33 @agent_ppo2.py:185][0m |          -0.0017 |         231.3257 |           7.8485 |
[32m[20221214 14:10:33 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:10:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.72
[32m[20221214 14:10:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.35
[32m[20221214 14:10:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.54
[32m[20221214 14:10:33 @agent_ppo2.py:143][0m Total time:      12.52 min
[32m[20221214 14:10:33 @agent_ppo2.py:145][0m 1144832 total steps have happened
[32m[20221214 14:10:33 @agent_ppo2.py:121][0m #------------------------ Iteration 559 --------------------------#
[32m[20221214 14:10:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:10:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:33 @agent_ppo2.py:185][0m |           0.0003 |         262.3828 |           7.9667 |
[32m[20221214 14:10:33 @agent_ppo2.py:185][0m |          -0.0025 |         251.5943 |           7.9673 |
[32m[20221214 14:10:34 @agent_ppo2.py:185][0m |          -0.0004 |         246.8592 |           8.0024 |
[32m[20221214 14:10:34 @agent_ppo2.py:185][0m |          -0.0016 |         244.2186 |           7.9899 |
[32m[20221214 14:10:34 @agent_ppo2.py:185][0m |          -0.0027 |         242.6996 |           7.9980 |
[32m[20221214 14:10:34 @agent_ppo2.py:185][0m |           0.0061 |         247.0680 |           7.9687 |
[32m[20221214 14:10:34 @agent_ppo2.py:185][0m |          -0.0020 |         240.4872 |           8.0023 |
[32m[20221214 14:10:34 @agent_ppo2.py:185][0m |          -0.0019 |         239.4788 |           7.9897 |
[32m[20221214 14:10:34 @agent_ppo2.py:185][0m |          -0.0014 |         238.9417 |           8.0165 |
[32m[20221214 14:10:34 @agent_ppo2.py:185][0m |          -0.0011 |         237.6733 |           8.0088 |
[32m[20221214 14:10:34 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:10:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.36
[32m[20221214 14:10:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.36
[32m[20221214 14:10:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.19
[32m[20221214 14:10:34 @agent_ppo2.py:143][0m Total time:      12.54 min
[32m[20221214 14:10:34 @agent_ppo2.py:145][0m 1146880 total steps have happened
[32m[20221214 14:10:34 @agent_ppo2.py:121][0m #------------------------ Iteration 560 --------------------------#
[32m[20221214 14:10:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:10:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:35 @agent_ppo2.py:185][0m |          -0.0008 |         245.3894 |           8.0389 |
[32m[20221214 14:10:35 @agent_ppo2.py:185][0m |           0.0077 |         248.2458 |           8.0575 |
[32m[20221214 14:10:35 @agent_ppo2.py:185][0m |          -0.0016 |         233.7043 |           8.1078 |
[32m[20221214 14:10:35 @agent_ppo2.py:185][0m |          -0.0035 |         230.9231 |           8.0684 |
[32m[20221214 14:10:35 @agent_ppo2.py:185][0m |           0.0050 |         231.1512 |           8.0340 |
[32m[20221214 14:10:35 @agent_ppo2.py:185][0m |          -0.0028 |         227.4436 |           8.0528 |
[32m[20221214 14:10:35 @agent_ppo2.py:185][0m |           0.0116 |         249.7087 |           8.0379 |
[32m[20221214 14:10:35 @agent_ppo2.py:185][0m |          -0.0013 |         225.3409 |           8.0259 |
[32m[20221214 14:10:36 @agent_ppo2.py:185][0m |          -0.0042 |         224.6587 |           8.0271 |
[32m[20221214 14:10:36 @agent_ppo2.py:185][0m |          -0.0032 |         223.8242 |           8.1111 |
[32m[20221214 14:10:36 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:10:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.17
[32m[20221214 14:10:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.76
[32m[20221214 14:10:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.90
[32m[20221214 14:10:36 @agent_ppo2.py:143][0m Total time:      12.56 min
[32m[20221214 14:10:36 @agent_ppo2.py:145][0m 1148928 total steps have happened
[32m[20221214 14:10:36 @agent_ppo2.py:121][0m #------------------------ Iteration 561 --------------------------#
[32m[20221214 14:10:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:36 @agent_ppo2.py:185][0m |          -0.0003 |         220.0872 |           7.7436 |
[32m[20221214 14:10:36 @agent_ppo2.py:185][0m |           0.0065 |         202.9641 |           7.7486 |
[32m[20221214 14:10:36 @agent_ppo2.py:185][0m |          -0.0021 |         185.6102 |           7.7820 |
[32m[20221214 14:10:36 @agent_ppo2.py:185][0m |          -0.0029 |         181.5931 |           7.7819 |
[32m[20221214 14:10:36 @agent_ppo2.py:185][0m |          -0.0015 |         177.9580 |           7.7756 |
[32m[20221214 14:10:37 @agent_ppo2.py:185][0m |          -0.0015 |         174.7415 |           7.7753 |
[32m[20221214 14:10:37 @agent_ppo2.py:185][0m |          -0.0024 |         173.8355 |           7.7666 |
[32m[20221214 14:10:37 @agent_ppo2.py:185][0m |          -0.0020 |         171.1268 |           7.7975 |
[32m[20221214 14:10:37 @agent_ppo2.py:185][0m |          -0.0031 |         169.8184 |           7.7900 |
[32m[20221214 14:10:37 @agent_ppo2.py:185][0m |          -0.0034 |         168.9943 |           7.7968 |
[32m[20221214 14:10:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:10:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.37
[32m[20221214 14:10:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.48
[32m[20221214 14:10:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.99
[32m[20221214 14:10:37 @agent_ppo2.py:143][0m Total time:      12.58 min
[32m[20221214 14:10:37 @agent_ppo2.py:145][0m 1150976 total steps have happened
[32m[20221214 14:10:37 @agent_ppo2.py:121][0m #------------------------ Iteration 562 --------------------------#
[32m[20221214 14:10:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:37 @agent_ppo2.py:185][0m |          -0.0025 |         239.0424 |           7.9771 |
[32m[20221214 14:10:38 @agent_ppo2.py:185][0m |          -0.0031 |         220.4657 |           7.9803 |
[32m[20221214 14:10:38 @agent_ppo2.py:185][0m |          -0.0036 |         214.9460 |           7.9256 |
[32m[20221214 14:10:38 @agent_ppo2.py:185][0m |          -0.0031 |         209.0236 |           7.9051 |
[32m[20221214 14:10:38 @agent_ppo2.py:185][0m |          -0.0013 |         205.8687 |           7.9029 |
[32m[20221214 14:10:38 @agent_ppo2.py:185][0m |          -0.0041 |         204.5598 |           7.8638 |
[32m[20221214 14:10:38 @agent_ppo2.py:185][0m |          -0.0044 |         201.8870 |           7.8370 |
[32m[20221214 14:10:38 @agent_ppo2.py:185][0m |           0.0009 |         201.0996 |           7.7905 |
[32m[20221214 14:10:38 @agent_ppo2.py:185][0m |          -0.0030 |         199.2827 |           7.7365 |
[32m[20221214 14:10:38 @agent_ppo2.py:185][0m |          -0.0032 |         198.5772 |           7.7423 |
[32m[20221214 14:10:38 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:10:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.91
[32m[20221214 14:10:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.06
[32m[20221214 14:10:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.39
[32m[20221214 14:10:38 @agent_ppo2.py:143][0m Total time:      12.61 min
[32m[20221214 14:10:38 @agent_ppo2.py:145][0m 1153024 total steps have happened
[32m[20221214 14:10:38 @agent_ppo2.py:121][0m #------------------------ Iteration 563 --------------------------#
[32m[20221214 14:10:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:39 @agent_ppo2.py:185][0m |          -0.0024 |         254.6196 |           7.8395 |
[32m[20221214 14:10:39 @agent_ppo2.py:185][0m |          -0.0013 |         246.7327 |           7.8621 |
[32m[20221214 14:10:39 @agent_ppo2.py:185][0m |          -0.0021 |         244.4675 |           7.8620 |
[32m[20221214 14:10:39 @agent_ppo2.py:185][0m |          -0.0025 |         242.9124 |           7.8548 |
[32m[20221214 14:10:39 @agent_ppo2.py:185][0m |          -0.0018 |         242.0404 |           7.8661 |
[32m[20221214 14:10:39 @agent_ppo2.py:185][0m |          -0.0027 |         240.9256 |           7.8675 |
[32m[20221214 14:10:39 @agent_ppo2.py:185][0m |           0.0032 |         248.2899 |           7.8584 |
[32m[20221214 14:10:39 @agent_ppo2.py:185][0m |          -0.0021 |         239.8993 |           7.8731 |
[32m[20221214 14:10:40 @agent_ppo2.py:185][0m |          -0.0010 |         238.6866 |           7.8573 |
[32m[20221214 14:10:40 @agent_ppo2.py:185][0m |          -0.0028 |         238.4171 |           7.8575 |
[32m[20221214 14:10:40 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:10:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.54
[32m[20221214 14:10:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.21
[32m[20221214 14:10:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.36
[32m[20221214 14:10:40 @agent_ppo2.py:143][0m Total time:      12.63 min
[32m[20221214 14:10:40 @agent_ppo2.py:145][0m 1155072 total steps have happened
[32m[20221214 14:10:40 @agent_ppo2.py:121][0m #------------------------ Iteration 564 --------------------------#
[32m[20221214 14:10:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:40 @agent_ppo2.py:185][0m |          -0.0054 |         246.8282 |           7.5359 |
[32m[20221214 14:10:40 @agent_ppo2.py:185][0m |          -0.0034 |         239.1856 |           7.5821 |
[32m[20221214 14:10:40 @agent_ppo2.py:185][0m |           0.0022 |         236.8389 |           7.5741 |
[32m[20221214 14:10:40 @agent_ppo2.py:185][0m |          -0.0038 |         233.4582 |           7.5727 |
[32m[20221214 14:10:41 @agent_ppo2.py:185][0m |          -0.0033 |         230.9107 |           7.5863 |
[32m[20221214 14:10:41 @agent_ppo2.py:185][0m |          -0.0004 |         230.9142 |           7.6039 |
[32m[20221214 14:10:41 @agent_ppo2.py:185][0m |          -0.0042 |         229.5896 |           7.5869 |
[32m[20221214 14:10:41 @agent_ppo2.py:185][0m |           0.0021 |         233.9233 |           7.6047 |
[32m[20221214 14:10:41 @agent_ppo2.py:185][0m |          -0.0039 |         228.8405 |           7.6028 |
[32m[20221214 14:10:41 @agent_ppo2.py:185][0m |          -0.0040 |         228.0894 |           7.6356 |
[32m[20221214 14:10:41 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:10:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.67
[32m[20221214 14:10:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.42
[32m[20221214 14:10:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.02
[32m[20221214 14:10:41 @agent_ppo2.py:143][0m Total time:      12.65 min
[32m[20221214 14:10:41 @agent_ppo2.py:145][0m 1157120 total steps have happened
[32m[20221214 14:10:41 @agent_ppo2.py:121][0m #------------------------ Iteration 565 --------------------------#
[32m[20221214 14:10:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:10:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:42 @agent_ppo2.py:185][0m |          -0.0017 |         255.6563 |           7.5793 |
[32m[20221214 14:10:42 @agent_ppo2.py:185][0m |          -0.0047 |         250.4135 |           7.5884 |
[32m[20221214 14:10:42 @agent_ppo2.py:185][0m |          -0.0022 |         248.5654 |           7.5448 |
[32m[20221214 14:10:42 @agent_ppo2.py:185][0m |          -0.0050 |         247.2453 |           7.5452 |
[32m[20221214 14:10:42 @agent_ppo2.py:185][0m |           0.0069 |         281.1975 |           7.5418 |
[32m[20221214 14:10:42 @agent_ppo2.py:185][0m |           0.0024 |         258.3812 |           7.5496 |
[32m[20221214 14:10:42 @agent_ppo2.py:185][0m |          -0.0052 |         243.7222 |           7.5476 |
[32m[20221214 14:10:42 @agent_ppo2.py:185][0m |           0.0071 |         250.0108 |           7.5435 |
[32m[20221214 14:10:42 @agent_ppo2.py:185][0m |          -0.0037 |         242.9409 |           7.5065 |
[32m[20221214 14:10:42 @agent_ppo2.py:185][0m |          -0.0065 |         242.3185 |           7.5122 |
[32m[20221214 14:10:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:10:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.82
[32m[20221214 14:10:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.26
[32m[20221214 14:10:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.29
[32m[20221214 14:10:42 @agent_ppo2.py:143][0m Total time:      12.67 min
[32m[20221214 14:10:42 @agent_ppo2.py:145][0m 1159168 total steps have happened
[32m[20221214 14:10:42 @agent_ppo2.py:121][0m #------------------------ Iteration 566 --------------------------#
[32m[20221214 14:10:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:10:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:43 @agent_ppo2.py:185][0m |           0.0085 |         295.1463 |           7.5531 |
[32m[20221214 14:10:43 @agent_ppo2.py:185][0m |           0.0172 |         299.0133 |           7.5559 |
[32m[20221214 14:10:43 @agent_ppo2.py:185][0m |          -0.0025 |         260.7598 |           7.5272 |
[32m[20221214 14:10:43 @agent_ppo2.py:185][0m |          -0.0039 |         257.5527 |           7.4853 |
[32m[20221214 14:10:43 @agent_ppo2.py:185][0m |          -0.0038 |         255.5098 |           7.4930 |
[32m[20221214 14:10:43 @agent_ppo2.py:185][0m |          -0.0053 |         253.6365 |           7.4523 |
[32m[20221214 14:10:43 @agent_ppo2.py:185][0m |          -0.0033 |         252.1036 |           7.4578 |
[32m[20221214 14:10:44 @agent_ppo2.py:185][0m |          -0.0036 |         251.2746 |           7.4246 |
[32m[20221214 14:10:44 @agent_ppo2.py:185][0m |          -0.0022 |         249.9495 |           7.3868 |
[32m[20221214 14:10:44 @agent_ppo2.py:185][0m |           0.0003 |         250.3534 |           7.3770 |
[32m[20221214 14:10:44 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:10:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.30
[32m[20221214 14:10:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.67
[32m[20221214 14:10:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.57
[32m[20221214 14:10:44 @agent_ppo2.py:143][0m Total time:      12.70 min
[32m[20221214 14:10:44 @agent_ppo2.py:145][0m 1161216 total steps have happened
[32m[20221214 14:10:44 @agent_ppo2.py:121][0m #------------------------ Iteration 567 --------------------------#
[32m[20221214 14:10:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:44 @agent_ppo2.py:185][0m |           0.0002 |         273.7218 |           7.2376 |
[32m[20221214 14:10:44 @agent_ppo2.py:185][0m |           0.0021 |         263.5892 |           7.2356 |
[32m[20221214 14:10:44 @agent_ppo2.py:185][0m |          -0.0026 |         253.4862 |           7.2227 |
[32m[20221214 14:10:45 @agent_ppo2.py:185][0m |          -0.0037 |         249.3098 |           7.2137 |
[32m[20221214 14:10:45 @agent_ppo2.py:185][0m |          -0.0031 |         246.0388 |           7.2073 |
[32m[20221214 14:10:45 @agent_ppo2.py:185][0m |          -0.0023 |         244.9783 |           7.2427 |
[32m[20221214 14:10:45 @agent_ppo2.py:185][0m |           0.0017 |         246.5534 |           7.1719 |
[32m[20221214 14:10:45 @agent_ppo2.py:185][0m |           0.0047 |         259.5317 |           7.1762 |
[32m[20221214 14:10:45 @agent_ppo2.py:185][0m |           0.0021 |         241.9740 |           7.2294 |
[32m[20221214 14:10:45 @agent_ppo2.py:185][0m |          -0.0035 |         238.8878 |           7.1591 |
[32m[20221214 14:10:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:10:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.32
[32m[20221214 14:10:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.71
[32m[20221214 14:10:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.47
[32m[20221214 14:10:45 @agent_ppo2.py:143][0m Total time:      12.72 min
[32m[20221214 14:10:45 @agent_ppo2.py:145][0m 1163264 total steps have happened
[32m[20221214 14:10:45 @agent_ppo2.py:121][0m #------------------------ Iteration 568 --------------------------#
[32m[20221214 14:10:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:46 @agent_ppo2.py:185][0m |          -0.0007 |         259.7610 |           7.2377 |
[32m[20221214 14:10:46 @agent_ppo2.py:185][0m |          -0.0013 |         251.2124 |           7.2273 |
[32m[20221214 14:10:46 @agent_ppo2.py:185][0m |          -0.0019 |         249.7555 |           7.2114 |
[32m[20221214 14:10:46 @agent_ppo2.py:185][0m |          -0.0020 |         248.7174 |           7.2013 |
[32m[20221214 14:10:46 @agent_ppo2.py:185][0m |           0.0195 |         284.7729 |           7.1836 |
[32m[20221214 14:10:46 @agent_ppo2.py:185][0m |           0.0107 |         273.2921 |           7.1670 |
[32m[20221214 14:10:46 @agent_ppo2.py:185][0m |          -0.0030 |         248.6228 |           7.1250 |
[32m[20221214 14:10:46 @agent_ppo2.py:185][0m |          -0.0027 |         246.9598 |           7.1437 |
[32m[20221214 14:10:46 @agent_ppo2.py:185][0m |          -0.0031 |         246.9288 |           7.1351 |
[32m[20221214 14:10:46 @agent_ppo2.py:185][0m |          -0.0009 |         246.4335 |           7.1061 |
[32m[20221214 14:10:46 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:10:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.90
[32m[20221214 14:10:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.85
[32m[20221214 14:10:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.11
[32m[20221214 14:10:47 @agent_ppo2.py:143][0m Total time:      12.74 min
[32m[20221214 14:10:47 @agent_ppo2.py:145][0m 1165312 total steps have happened
[32m[20221214 14:10:47 @agent_ppo2.py:121][0m #------------------------ Iteration 569 --------------------------#
[32m[20221214 14:10:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:47 @agent_ppo2.py:185][0m |           0.0131 |         285.2673 |           7.2134 |
[32m[20221214 14:10:47 @agent_ppo2.py:185][0m |           0.0115 |         259.1446 |           7.2610 |
[32m[20221214 14:10:47 @agent_ppo2.py:185][0m |          -0.0025 |         240.0023 |           7.2846 |
[32m[20221214 14:10:47 @agent_ppo2.py:185][0m |          -0.0015 |         240.2226 |           7.2775 |
[32m[20221214 14:10:47 @agent_ppo2.py:185][0m |           0.0071 |         253.9302 |           7.3020 |
[32m[20221214 14:10:47 @agent_ppo2.py:185][0m |          -0.0020 |         238.5010 |           7.2463 |
[32m[20221214 14:10:47 @agent_ppo2.py:185][0m |          -0.0034 |         238.0271 |           7.3018 |
[32m[20221214 14:10:48 @agent_ppo2.py:185][0m |          -0.0039 |         237.6779 |           7.2792 |
[32m[20221214 14:10:48 @agent_ppo2.py:185][0m |          -0.0041 |         237.4812 |           7.2750 |
[32m[20221214 14:10:48 @agent_ppo2.py:185][0m |          -0.0044 |         237.5149 |           7.2753 |
[32m[20221214 14:10:48 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:10:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.75
[32m[20221214 14:10:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.42
[32m[20221214 14:10:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.87
[32m[20221214 14:10:48 @agent_ppo2.py:143][0m Total time:      12.76 min
[32m[20221214 14:10:48 @agent_ppo2.py:145][0m 1167360 total steps have happened
[32m[20221214 14:10:48 @agent_ppo2.py:121][0m #------------------------ Iteration 570 --------------------------#
[32m[20221214 14:10:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:10:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:48 @agent_ppo2.py:185][0m |          -0.0032 |         226.7236 |           7.0956 |
[32m[20221214 14:10:48 @agent_ppo2.py:185][0m |           0.0009 |         224.3458 |           7.1048 |
[32m[20221214 14:10:48 @agent_ppo2.py:185][0m |           0.0064 |         236.4541 |           7.1028 |
[32m[20221214 14:10:49 @agent_ppo2.py:185][0m |           0.0041 |         225.7479 |           7.1264 |
[32m[20221214 14:10:49 @agent_ppo2.py:185][0m |          -0.0030 |         221.2439 |           7.1502 |
[32m[20221214 14:10:49 @agent_ppo2.py:185][0m |          -0.0024 |         220.5673 |           7.1304 |
[32m[20221214 14:10:49 @agent_ppo2.py:185][0m |          -0.0026 |         220.3068 |           7.1314 |
[32m[20221214 14:10:49 @agent_ppo2.py:185][0m |          -0.0036 |         219.9932 |           7.1375 |
[32m[20221214 14:10:49 @agent_ppo2.py:185][0m |           0.0055 |         227.2751 |           7.1467 |
[32m[20221214 14:10:49 @agent_ppo2.py:185][0m |          -0.0030 |         219.5834 |           7.1365 |
[32m[20221214 14:10:49 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:10:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.27
[32m[20221214 14:10:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.10
[32m[20221214 14:10:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.12
[32m[20221214 14:10:49 @agent_ppo2.py:143][0m Total time:      12.79 min
[32m[20221214 14:10:49 @agent_ppo2.py:145][0m 1169408 total steps have happened
[32m[20221214 14:10:49 @agent_ppo2.py:121][0m #------------------------ Iteration 571 --------------------------#
[32m[20221214 14:10:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:50 @agent_ppo2.py:185][0m |          -0.0023 |         228.2294 |           6.9659 |
[32m[20221214 14:10:50 @agent_ppo2.py:185][0m |          -0.0014 |         221.0605 |           7.0065 |
[32m[20221214 14:10:50 @agent_ppo2.py:185][0m |          -0.0025 |         216.4700 |           6.9064 |
[32m[20221214 14:10:50 @agent_ppo2.py:185][0m |          -0.0036 |         215.0216 |           6.9268 |
[32m[20221214 14:10:50 @agent_ppo2.py:185][0m |          -0.0031 |         213.8938 |           6.8695 |
[32m[20221214 14:10:50 @agent_ppo2.py:185][0m |          -0.0039 |         213.7846 |           6.8493 |
[32m[20221214 14:10:50 @agent_ppo2.py:185][0m |          -0.0029 |         213.1216 |           6.7991 |
[32m[20221214 14:10:50 @agent_ppo2.py:185][0m |          -0.0028 |         212.6242 |           6.7751 |
[32m[20221214 14:10:50 @agent_ppo2.py:185][0m |          -0.0019 |         212.2565 |           6.7833 |
[32m[20221214 14:10:50 @agent_ppo2.py:185][0m |          -0.0031 |         212.5626 |           6.7275 |
[32m[20221214 14:10:50 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:10:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.85
[32m[20221214 14:10:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.40
[32m[20221214 14:10:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.71
[32m[20221214 14:10:51 @agent_ppo2.py:143][0m Total time:      12.81 min
[32m[20221214 14:10:51 @agent_ppo2.py:145][0m 1171456 total steps have happened
[32m[20221214 14:10:51 @agent_ppo2.py:121][0m #------------------------ Iteration 572 --------------------------#
[32m[20221214 14:10:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:51 @agent_ppo2.py:185][0m |          -0.0017 |         228.1406 |           6.8698 |
[32m[20221214 14:10:51 @agent_ppo2.py:185][0m |          -0.0043 |         223.3107 |           6.8397 |
[32m[20221214 14:10:51 @agent_ppo2.py:185][0m |          -0.0027 |         221.8806 |           6.7937 |
[32m[20221214 14:10:51 @agent_ppo2.py:185][0m |          -0.0017 |         224.0193 |           6.7879 |
[32m[20221214 14:10:51 @agent_ppo2.py:185][0m |          -0.0040 |         221.0125 |           6.7887 |
[32m[20221214 14:10:51 @agent_ppo2.py:185][0m |          -0.0052 |         220.7526 |           6.7745 |
[32m[20221214 14:10:52 @agent_ppo2.py:185][0m |          -0.0049 |         220.3996 |           6.7365 |
[32m[20221214 14:10:52 @agent_ppo2.py:185][0m |          -0.0037 |         220.6025 |           6.7576 |
[32m[20221214 14:10:52 @agent_ppo2.py:185][0m |           0.0027 |         225.2703 |           6.7381 |
[32m[20221214 14:10:52 @agent_ppo2.py:185][0m |          -0.0059 |         220.3257 |           6.6913 |
[32m[20221214 14:10:52 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:10:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.93
[32m[20221214 14:10:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.89
[32m[20221214 14:10:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.26
[32m[20221214 14:10:52 @agent_ppo2.py:143][0m Total time:      12.83 min
[32m[20221214 14:10:52 @agent_ppo2.py:145][0m 1173504 total steps have happened
[32m[20221214 14:10:52 @agent_ppo2.py:121][0m #------------------------ Iteration 573 --------------------------#
[32m[20221214 14:10:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:10:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:52 @agent_ppo2.py:185][0m |           0.0016 |         234.0235 |           6.7924 |
[32m[20221214 14:10:52 @agent_ppo2.py:185][0m |          -0.0036 |         226.9418 |           6.7829 |
[32m[20221214 14:10:53 @agent_ppo2.py:185][0m |          -0.0041 |         225.6083 |           6.7580 |
[32m[20221214 14:10:53 @agent_ppo2.py:185][0m |          -0.0032 |         225.0198 |           6.7610 |
[32m[20221214 14:10:53 @agent_ppo2.py:185][0m |          -0.0002 |         226.4475 |           6.7436 |
[32m[20221214 14:10:53 @agent_ppo2.py:185][0m |          -0.0047 |         224.0567 |           6.7321 |
[32m[20221214 14:10:53 @agent_ppo2.py:185][0m |          -0.0023 |         223.4552 |           6.6848 |
[32m[20221214 14:10:53 @agent_ppo2.py:185][0m |          -0.0041 |         223.1492 |           6.6739 |
[32m[20221214 14:10:53 @agent_ppo2.py:185][0m |          -0.0037 |         222.7081 |           6.6554 |
[32m[20221214 14:10:53 @agent_ppo2.py:185][0m |           0.0005 |         226.6363 |           6.6528 |
[32m[20221214 14:10:53 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:10:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.40
[32m[20221214 14:10:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 784.55
[32m[20221214 14:10:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.45
[32m[20221214 14:10:53 @agent_ppo2.py:143][0m Total time:      12.86 min
[32m[20221214 14:10:53 @agent_ppo2.py:145][0m 1175552 total steps have happened
[32m[20221214 14:10:53 @agent_ppo2.py:121][0m #------------------------ Iteration 574 --------------------------#
[32m[20221214 14:10:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:54 @agent_ppo2.py:185][0m |          -0.0029 |         238.1391 |           6.1798 |
[32m[20221214 14:10:54 @agent_ppo2.py:185][0m |          -0.0027 |         231.5222 |           6.2067 |
[32m[20221214 14:10:54 @agent_ppo2.py:185][0m |          -0.0021 |         228.4776 |           6.2305 |
[32m[20221214 14:10:54 @agent_ppo2.py:185][0m |          -0.0031 |         225.9204 |           6.2194 |
[32m[20221214 14:10:54 @agent_ppo2.py:185][0m |          -0.0033 |         222.9247 |           6.2700 |
[32m[20221214 14:10:54 @agent_ppo2.py:185][0m |          -0.0030 |         221.0029 |           6.2327 |
[32m[20221214 14:10:54 @agent_ppo2.py:185][0m |          -0.0048 |         218.9096 |           6.2679 |
[32m[20221214 14:10:54 @agent_ppo2.py:185][0m |          -0.0032 |         217.5731 |           6.2723 |
[32m[20221214 14:10:54 @agent_ppo2.py:185][0m |          -0.0032 |         216.3694 |           6.2831 |
[32m[20221214 14:10:55 @agent_ppo2.py:185][0m |          -0.0032 |         215.2150 |           6.2889 |
[32m[20221214 14:10:55 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:10:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 753.51
[32m[20221214 14:10:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 764.62
[32m[20221214 14:10:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.57
[32m[20221214 14:10:55 @agent_ppo2.py:143][0m Total time:      12.88 min
[32m[20221214 14:10:55 @agent_ppo2.py:145][0m 1177600 total steps have happened
[32m[20221214 14:10:55 @agent_ppo2.py:121][0m #------------------------ Iteration 575 --------------------------#
[32m[20221214 14:10:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:55 @agent_ppo2.py:185][0m |          -0.0028 |         225.4158 |           6.4180 |
[32m[20221214 14:10:55 @agent_ppo2.py:185][0m |          -0.0011 |         223.1030 |           6.4303 |
[32m[20221214 14:10:55 @agent_ppo2.py:185][0m |           0.0114 |         244.1121 |           6.3982 |
[32m[20221214 14:10:55 @agent_ppo2.py:185][0m |           0.0000 |         221.8728 |           6.3550 |
[32m[20221214 14:10:55 @agent_ppo2.py:185][0m |           0.0017 |         222.4243 |           6.3794 |
[32m[20221214 14:10:56 @agent_ppo2.py:185][0m |          -0.0011 |         221.2434 |           6.3181 |
[32m[20221214 14:10:56 @agent_ppo2.py:185][0m |          -0.0015 |         221.1280 |           6.3118 |
[32m[20221214 14:10:56 @agent_ppo2.py:185][0m |           0.0001 |         221.1560 |           6.2967 |
[32m[20221214 14:10:56 @agent_ppo2.py:185][0m |          -0.0008 |         220.7606 |           6.2906 |
[32m[20221214 14:10:56 @agent_ppo2.py:185][0m |          -0.0019 |         220.6892 |           6.2325 |
[32m[20221214 14:10:56 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:10:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.75
[32m[20221214 14:10:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 772.65
[32m[20221214 14:10:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.13
[32m[20221214 14:10:56 @agent_ppo2.py:143][0m Total time:      12.90 min
[32m[20221214 14:10:56 @agent_ppo2.py:145][0m 1179648 total steps have happened
[32m[20221214 14:10:56 @agent_ppo2.py:121][0m #------------------------ Iteration 576 --------------------------#
[32m[20221214 14:10:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:10:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:56 @agent_ppo2.py:185][0m |           0.0009 |         223.8647 |           6.1904 |
[32m[20221214 14:10:56 @agent_ppo2.py:185][0m |           0.0122 |         233.0929 |           6.1874 |
[32m[20221214 14:10:57 @agent_ppo2.py:185][0m |          -0.0018 |         205.4574 |           6.1839 |
[32m[20221214 14:10:57 @agent_ppo2.py:185][0m |          -0.0010 |         203.0188 |           6.1836 |
[32m[20221214 14:10:57 @agent_ppo2.py:185][0m |          -0.0024 |         203.0683 |           6.1940 |
[32m[20221214 14:10:57 @agent_ppo2.py:185][0m |          -0.0018 |         200.9389 |           6.1741 |
[32m[20221214 14:10:57 @agent_ppo2.py:185][0m |          -0.0028 |         200.1458 |           6.1663 |
[32m[20221214 14:10:57 @agent_ppo2.py:185][0m |           0.0057 |         207.2156 |           6.1650 |
[32m[20221214 14:10:57 @agent_ppo2.py:185][0m |           0.0051 |         203.6763 |           6.1023 |
[32m[20221214 14:10:57 @agent_ppo2.py:185][0m |           0.0002 |         199.6543 |           6.1357 |
[32m[20221214 14:10:57 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:10:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.46
[32m[20221214 14:10:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.20
[32m[20221214 14:10:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.81
[32m[20221214 14:10:57 @agent_ppo2.py:143][0m Total time:      12.92 min
[32m[20221214 14:10:57 @agent_ppo2.py:145][0m 1181696 total steps have happened
[32m[20221214 14:10:57 @agent_ppo2.py:121][0m #------------------------ Iteration 577 --------------------------#
[32m[20221214 14:10:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:10:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:58 @agent_ppo2.py:185][0m |          -0.0019 |         225.6427 |           5.9999 |
[32m[20221214 14:10:58 @agent_ppo2.py:185][0m |          -0.0020 |         218.0469 |           6.0111 |
[32m[20221214 14:10:58 @agent_ppo2.py:185][0m |          -0.0027 |         216.0881 |           6.0084 |
[32m[20221214 14:10:58 @agent_ppo2.py:185][0m |          -0.0027 |         215.2730 |           6.0207 |
[32m[20221214 14:10:58 @agent_ppo2.py:185][0m |          -0.0033 |         213.6475 |           6.0244 |
[32m[20221214 14:10:58 @agent_ppo2.py:185][0m |           0.0092 |         225.9505 |           6.0508 |
[32m[20221214 14:10:58 @agent_ppo2.py:185][0m |           0.0038 |         219.4617 |           6.0574 |
[32m[20221214 14:10:58 @agent_ppo2.py:185][0m |          -0.0030 |         211.7292 |           6.0624 |
[32m[20221214 14:10:58 @agent_ppo2.py:185][0m |           0.0061 |         223.0445 |           6.0768 |
[32m[20221214 14:10:59 @agent_ppo2.py:185][0m |          -0.0028 |         210.5889 |           6.0838 |
[32m[20221214 14:10:59 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:10:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 744.21
[32m[20221214 14:10:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 760.05
[32m[20221214 14:10:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 763.54
[32m[20221214 14:10:59 @agent_ppo2.py:143][0m Total time:      12.94 min
[32m[20221214 14:10:59 @agent_ppo2.py:145][0m 1183744 total steps have happened
[32m[20221214 14:10:59 @agent_ppo2.py:121][0m #------------------------ Iteration 578 --------------------------#
[32m[20221214 14:10:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:10:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:10:59 @agent_ppo2.py:185][0m |           0.0143 |         261.4280 |           6.3357 |
[32m[20221214 14:10:59 @agent_ppo2.py:185][0m |          -0.0018 |         215.1323 |           6.3422 |
[32m[20221214 14:10:59 @agent_ppo2.py:185][0m |           0.0036 |         209.7524 |           6.3423 |
[32m[20221214 14:10:59 @agent_ppo2.py:185][0m |          -0.0024 |         205.9392 |           6.3428 |
[32m[20221214 14:10:59 @agent_ppo2.py:185][0m |          -0.0046 |         202.6834 |           6.3313 |
[32m[20221214 14:10:59 @agent_ppo2.py:185][0m |          -0.0021 |         200.9833 |           6.3281 |
[32m[20221214 14:10:59 @agent_ppo2.py:185][0m |           0.0049 |         202.8528 |           6.3189 |
[32m[20221214 14:11:00 @agent_ppo2.py:185][0m |          -0.0048 |         197.7520 |           6.3437 |
[32m[20221214 14:11:00 @agent_ppo2.py:185][0m |          -0.0013 |         196.5619 |           6.3723 |
[32m[20221214 14:11:00 @agent_ppo2.py:185][0m |           0.0043 |         204.5331 |           6.3885 |
[32m[20221214 14:11:00 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:11:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 639.13
[32m[20221214 14:11:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 754.26
[32m[20221214 14:11:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.89
[32m[20221214 14:11:00 @agent_ppo2.py:143][0m Total time:      12.96 min
[32m[20221214 14:11:00 @agent_ppo2.py:145][0m 1185792 total steps have happened
[32m[20221214 14:11:00 @agent_ppo2.py:121][0m #------------------------ Iteration 579 --------------------------#
[32m[20221214 14:11:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:00 @agent_ppo2.py:185][0m |          -0.0015 |         226.7247 |           6.3663 |
[32m[20221214 14:11:00 @agent_ppo2.py:185][0m |           0.0027 |         216.8876 |           6.3735 |
[32m[20221214 14:11:00 @agent_ppo2.py:185][0m |           0.0046 |         217.8392 |           6.4171 |
[32m[20221214 14:11:00 @agent_ppo2.py:185][0m |          -0.0001 |         209.5212 |           6.3694 |
[32m[20221214 14:11:01 @agent_ppo2.py:185][0m |          -0.0009 |         208.0992 |           6.3699 |
[32m[20221214 14:11:01 @agent_ppo2.py:185][0m |          -0.0012 |         207.5146 |           6.3717 |
[32m[20221214 14:11:01 @agent_ppo2.py:185][0m |          -0.0018 |         206.4999 |           6.3437 |
[32m[20221214 14:11:01 @agent_ppo2.py:185][0m |          -0.0023 |         205.9006 |           6.3805 |
[32m[20221214 14:11:01 @agent_ppo2.py:185][0m |          -0.0014 |         205.0818 |           6.3804 |
[32m[20221214 14:11:01 @agent_ppo2.py:185][0m |          -0.0046 |         204.7435 |           6.3867 |
[32m[20221214 14:11:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:11:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 661.83
[32m[20221214 14:11:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.24
[32m[20221214 14:11:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.76
[32m[20221214 14:11:01 @agent_ppo2.py:143][0m Total time:      12.98 min
[32m[20221214 14:11:01 @agent_ppo2.py:145][0m 1187840 total steps have happened
[32m[20221214 14:11:01 @agent_ppo2.py:121][0m #------------------------ Iteration 580 --------------------------#
[32m[20221214 14:11:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:11:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:01 @agent_ppo2.py:185][0m |           0.0064 |         200.8922 |           6.3958 |
[32m[20221214 14:11:02 @agent_ppo2.py:185][0m |           0.0015 |         193.2501 |           6.4072 |
[32m[20221214 14:11:02 @agent_ppo2.py:185][0m |          -0.0027 |         188.1638 |           6.4055 |
[32m[20221214 14:11:02 @agent_ppo2.py:185][0m |          -0.0021 |         187.7451 |           6.4219 |
[32m[20221214 14:11:02 @agent_ppo2.py:185][0m |          -0.0023 |         186.9924 |           6.4256 |
[32m[20221214 14:11:02 @agent_ppo2.py:185][0m |           0.0025 |         189.4342 |           6.4235 |
[32m[20221214 14:11:02 @agent_ppo2.py:185][0m |          -0.0025 |         186.3327 |           6.4405 |
[32m[20221214 14:11:02 @agent_ppo2.py:185][0m |           0.0025 |         188.9212 |           6.4383 |
[32m[20221214 14:11:02 @agent_ppo2.py:185][0m |          -0.0030 |         185.8917 |           6.4189 |
[32m[20221214 14:11:02 @agent_ppo2.py:185][0m |          -0.0013 |         185.8400 |           6.4427 |
[32m[20221214 14:11:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:11:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 750.08
[32m[20221214 14:11:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 756.32
[32m[20221214 14:11:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.81
[32m[20221214 14:11:02 @agent_ppo2.py:143][0m Total time:      13.01 min
[32m[20221214 14:11:02 @agent_ppo2.py:145][0m 1189888 total steps have happened
[32m[20221214 14:11:02 @agent_ppo2.py:121][0m #------------------------ Iteration 581 --------------------------#
[32m[20221214 14:11:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:03 @agent_ppo2.py:185][0m |          -0.0012 |         201.6105 |           6.6049 |
[32m[20221214 14:11:03 @agent_ppo2.py:185][0m |          -0.0031 |         200.1384 |           6.5933 |
[32m[20221214 14:11:03 @agent_ppo2.py:185][0m |          -0.0019 |         198.2271 |           6.6077 |
[32m[20221214 14:11:03 @agent_ppo2.py:185][0m |          -0.0030 |         197.3687 |           6.5894 |
[32m[20221214 14:11:03 @agent_ppo2.py:185][0m |          -0.0026 |         196.6880 |           6.5634 |
[32m[20221214 14:11:03 @agent_ppo2.py:185][0m |          -0.0003 |         197.3071 |           6.5435 |
[32m[20221214 14:11:03 @agent_ppo2.py:185][0m |          -0.0033 |         195.6909 |           6.5273 |
[32m[20221214 14:11:03 @agent_ppo2.py:185][0m |           0.0094 |         209.6795 |           6.5088 |
[32m[20221214 14:11:03 @agent_ppo2.py:185][0m |           0.0007 |         196.0475 |           6.4635 |
[32m[20221214 14:11:03 @agent_ppo2.py:185][0m |          -0.0033 |         195.1348 |           6.4532 |
[32m[20221214 14:11:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:11:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 757.56
[32m[20221214 14:11:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 773.14
[32m[20221214 14:11:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 777.99
[32m[20221214 14:11:04 @agent_ppo2.py:143][0m Total time:      13.03 min
[32m[20221214 14:11:04 @agent_ppo2.py:145][0m 1191936 total steps have happened
[32m[20221214 14:11:04 @agent_ppo2.py:121][0m #------------------------ Iteration 582 --------------------------#
[32m[20221214 14:11:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:04 @agent_ppo2.py:185][0m |           0.0032 |         174.2460 |           6.1001 |
[32m[20221214 14:11:04 @agent_ppo2.py:185][0m |          -0.0029 |         165.7329 |           6.1059 |
[32m[20221214 14:11:04 @agent_ppo2.py:185][0m |          -0.0031 |         163.3593 |           6.1005 |
[32m[20221214 14:11:04 @agent_ppo2.py:185][0m |          -0.0020 |         162.2579 |           6.0600 |
[32m[20221214 14:11:04 @agent_ppo2.py:185][0m |          -0.0020 |         161.3741 |           6.0432 |
[32m[20221214 14:11:04 @agent_ppo2.py:185][0m |          -0.0020 |         161.1211 |           6.0294 |
[32m[20221214 14:11:04 @agent_ppo2.py:185][0m |           0.0070 |         170.1725 |           6.0100 |
[32m[20221214 14:11:05 @agent_ppo2.py:185][0m |           0.0006 |         159.5263 |           5.9542 |
[32m[20221214 14:11:05 @agent_ppo2.py:185][0m |          -0.0025 |         159.9109 |           5.9620 |
[32m[20221214 14:11:05 @agent_ppo2.py:185][0m |          -0.0011 |         158.7137 |           5.9365 |
[32m[20221214 14:11:05 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:11:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 746.15
[32m[20221214 14:11:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 751.62
[32m[20221214 14:11:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 758.88
[32m[20221214 14:11:05 @agent_ppo2.py:143][0m Total time:      13.05 min
[32m[20221214 14:11:05 @agent_ppo2.py:145][0m 1193984 total steps have happened
[32m[20221214 14:11:05 @agent_ppo2.py:121][0m #------------------------ Iteration 583 --------------------------#
[32m[20221214 14:11:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:05 @agent_ppo2.py:185][0m |          -0.0014 |         158.0212 |           5.7664 |
[32m[20221214 14:11:05 @agent_ppo2.py:185][0m |          -0.0013 |         156.2427 |           5.8017 |
[32m[20221214 14:11:05 @agent_ppo2.py:185][0m |          -0.0019 |         154.9418 |           5.7529 |
[32m[20221214 14:11:05 @agent_ppo2.py:185][0m |          -0.0018 |         154.9661 |           5.7329 |
[32m[20221214 14:11:06 @agent_ppo2.py:185][0m |          -0.0017 |         153.8743 |           5.7461 |
[32m[20221214 14:11:06 @agent_ppo2.py:185][0m |          -0.0023 |         154.3118 |           5.7179 |
[32m[20221214 14:11:06 @agent_ppo2.py:185][0m |          -0.0023 |         153.3144 |           5.6569 |
[32m[20221214 14:11:06 @agent_ppo2.py:185][0m |          -0.0030 |         153.2626 |           5.6840 |
[32m[20221214 14:11:06 @agent_ppo2.py:185][0m |          -0.0020 |         153.2578 |           5.6579 |
[32m[20221214 14:11:06 @agent_ppo2.py:185][0m |          -0.0023 |         152.8734 |           5.6387 |
[32m[20221214 14:11:06 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:11:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 729.42
[32m[20221214 14:11:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 737.85
[32m[20221214 14:11:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 730.98
[32m[20221214 14:11:06 @agent_ppo2.py:143][0m Total time:      13.07 min
[32m[20221214 14:11:06 @agent_ppo2.py:145][0m 1196032 total steps have happened
[32m[20221214 14:11:06 @agent_ppo2.py:121][0m #------------------------ Iteration 584 --------------------------#
[32m[20221214 14:11:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:06 @agent_ppo2.py:185][0m |           0.0053 |         140.7636 |           5.7879 |
[32m[20221214 14:11:07 @agent_ppo2.py:185][0m |          -0.0018 |         131.5808 |           5.8200 |
[32m[20221214 14:11:07 @agent_ppo2.py:185][0m |          -0.0017 |         126.8299 |           5.7966 |
[32m[20221214 14:11:07 @agent_ppo2.py:185][0m |          -0.0013 |         125.1715 |           5.7535 |
[32m[20221214 14:11:07 @agent_ppo2.py:185][0m |          -0.0010 |         123.4230 |           5.8222 |
[32m[20221214 14:11:07 @agent_ppo2.py:185][0m |           0.0123 |         135.4852 |           5.8243 |
[32m[20221214 14:11:07 @agent_ppo2.py:185][0m |          -0.0024 |         121.3656 |           5.8788 |
[32m[20221214 14:11:07 @agent_ppo2.py:185][0m |           0.0001 |         120.1899 |           5.7377 |
[32m[20221214 14:11:07 @agent_ppo2.py:185][0m |          -0.0023 |         120.3174 |           5.7757 |
[32m[20221214 14:11:07 @agent_ppo2.py:185][0m |           0.0043 |         120.2697 |           5.8224 |
[32m[20221214 14:11:07 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:11:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 704.07
[32m[20221214 14:11:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 714.56
[32m[20221214 14:11:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.46
[32m[20221214 14:11:07 @agent_ppo2.py:143][0m Total time:      13.09 min
[32m[20221214 14:11:07 @agent_ppo2.py:145][0m 1198080 total steps have happened
[32m[20221214 14:11:07 @agent_ppo2.py:121][0m #------------------------ Iteration 585 --------------------------#
[32m[20221214 14:11:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:08 @agent_ppo2.py:185][0m |           0.0050 |         138.6607 |           5.5719 |
[32m[20221214 14:11:08 @agent_ppo2.py:185][0m |          -0.0010 |         132.8003 |           5.5927 |
[32m[20221214 14:11:08 @agent_ppo2.py:185][0m |          -0.0016 |         131.1156 |           5.5512 |
[32m[20221214 14:11:08 @agent_ppo2.py:185][0m |          -0.0035 |         130.4719 |           5.5599 |
[32m[20221214 14:11:08 @agent_ppo2.py:185][0m |          -0.0027 |         130.1755 |           5.5213 |
[32m[20221214 14:11:08 @agent_ppo2.py:185][0m |          -0.0028 |         129.4549 |           5.5008 |
[32m[20221214 14:11:08 @agent_ppo2.py:185][0m |          -0.0018 |         129.1467 |           5.4801 |
[32m[20221214 14:11:08 @agent_ppo2.py:185][0m |           0.0003 |         129.4317 |           5.4645 |
[32m[20221214 14:11:08 @agent_ppo2.py:185][0m |           0.0004 |         128.5658 |           5.4417 |
[32m[20221214 14:11:08 @agent_ppo2.py:185][0m |          -0.0026 |         128.7725 |           5.4031 |
[32m[20221214 14:11:08 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:11:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.44
[32m[20221214 14:11:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 724.81
[32m[20221214 14:11:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 768.47
[32m[20221214 14:11:09 @agent_ppo2.py:143][0m Total time:      13.11 min
[32m[20221214 14:11:09 @agent_ppo2.py:145][0m 1200128 total steps have happened
[32m[20221214 14:11:09 @agent_ppo2.py:121][0m #------------------------ Iteration 586 --------------------------#
[32m[20221214 14:11:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:09 @agent_ppo2.py:185][0m |           0.0026 |         166.7479 |           5.4249 |
[32m[20221214 14:11:09 @agent_ppo2.py:185][0m |          -0.0026 |         160.9619 |           5.3971 |
[32m[20221214 14:11:09 @agent_ppo2.py:185][0m |          -0.0014 |         160.0527 |           5.4421 |
[32m[20221214 14:11:09 @agent_ppo2.py:185][0m |           0.0042 |         161.7528 |           5.4711 |
[32m[20221214 14:11:09 @agent_ppo2.py:185][0m |          -0.0016 |         158.2031 |           5.4105 |
[32m[20221214 14:11:09 @agent_ppo2.py:185][0m |           0.0011 |         158.8937 |           5.4378 |
[32m[20221214 14:11:09 @agent_ppo2.py:185][0m |          -0.0018 |         156.9790 |           5.4568 |
[32m[20221214 14:11:09 @agent_ppo2.py:185][0m |          -0.0017 |         156.8709 |           5.4910 |
[32m[20221214 14:11:10 @agent_ppo2.py:185][0m |           0.0005 |         157.5918 |           5.4945 |
[32m[20221214 14:11:10 @agent_ppo2.py:185][0m |           0.0005 |         157.0710 |           5.4448 |
[32m[20221214 14:11:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:11:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 724.36
[32m[20221214 14:11:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 760.73
[32m[20221214 14:11:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 781.81
[32m[20221214 14:11:10 @agent_ppo2.py:143][0m Total time:      13.13 min
[32m[20221214 14:11:10 @agent_ppo2.py:145][0m 1202176 total steps have happened
[32m[20221214 14:11:10 @agent_ppo2.py:121][0m #------------------------ Iteration 587 --------------------------#
[32m[20221214 14:11:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:10 @agent_ppo2.py:185][0m |          -0.0021 |         194.5852 |           5.5582 |
[32m[20221214 14:11:10 @agent_ppo2.py:185][0m |          -0.0032 |         189.2694 |           5.5559 |
[32m[20221214 14:11:10 @agent_ppo2.py:185][0m |          -0.0040 |         187.7278 |           5.5156 |
[32m[20221214 14:11:10 @agent_ppo2.py:185][0m |           0.0012 |         189.1996 |           5.5084 |
[32m[20221214 14:11:11 @agent_ppo2.py:185][0m |          -0.0019 |         186.2072 |           5.4694 |
[32m[20221214 14:11:11 @agent_ppo2.py:185][0m |          -0.0026 |         185.5512 |           5.4548 |
[32m[20221214 14:11:11 @agent_ppo2.py:185][0m |          -0.0033 |         185.2163 |           5.4522 |
[32m[20221214 14:11:11 @agent_ppo2.py:185][0m |          -0.0028 |         185.0280 |           5.4374 |
[32m[20221214 14:11:11 @agent_ppo2.py:185][0m |          -0.0037 |         184.5728 |           5.4224 |
[32m[20221214 14:11:11 @agent_ppo2.py:185][0m |           0.0108 |         204.2493 |           5.3824 |
[32m[20221214 14:11:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:11:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 754.31
[32m[20221214 14:11:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 758.24
[32m[20221214 14:11:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.34
[32m[20221214 14:11:11 @agent_ppo2.py:143][0m Total time:      13.15 min
[32m[20221214 14:11:11 @agent_ppo2.py:145][0m 1204224 total steps have happened
[32m[20221214 14:11:11 @agent_ppo2.py:121][0m #------------------------ Iteration 588 --------------------------#
[32m[20221214 14:11:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:11 @agent_ppo2.py:185][0m |           0.0096 |         214.1803 |           5.1835 |
[32m[20221214 14:11:12 @agent_ppo2.py:185][0m |           0.0060 |         208.7448 |           5.1684 |
[32m[20221214 14:11:12 @agent_ppo2.py:185][0m |          -0.0033 |         198.3644 |           5.2248 |
[32m[20221214 14:11:12 @agent_ppo2.py:185][0m |          -0.0017 |         197.0285 |           5.2304 |
[32m[20221214 14:11:12 @agent_ppo2.py:185][0m |          -0.0024 |         196.0916 |           5.2345 |
[32m[20221214 14:11:12 @agent_ppo2.py:185][0m |          -0.0025 |         195.1663 |           5.2289 |
[32m[20221214 14:11:12 @agent_ppo2.py:185][0m |          -0.0006 |         194.0919 |           5.2423 |
[32m[20221214 14:11:12 @agent_ppo2.py:185][0m |           0.0034 |         194.9786 |           5.2405 |
[32m[20221214 14:11:12 @agent_ppo2.py:185][0m |          -0.0021 |         192.5422 |           5.2276 |
[32m[20221214 14:11:12 @agent_ppo2.py:185][0m |           0.0099 |         215.6589 |           5.2461 |
[32m[20221214 14:11:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:11:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.73
[32m[20221214 14:11:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.32
[32m[20221214 14:11:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.66
[32m[20221214 14:11:12 @agent_ppo2.py:143][0m Total time:      13.17 min
[32m[20221214 14:11:12 @agent_ppo2.py:145][0m 1206272 total steps have happened
[32m[20221214 14:11:12 @agent_ppo2.py:121][0m #------------------------ Iteration 589 --------------------------#
[32m[20221214 14:11:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:13 @agent_ppo2.py:185][0m |          -0.0007 |         210.2626 |           5.2577 |
[32m[20221214 14:11:13 @agent_ppo2.py:185][0m |          -0.0020 |         207.0605 |           5.2583 |
[32m[20221214 14:11:13 @agent_ppo2.py:185][0m |          -0.0021 |         205.7925 |           5.2171 |
[32m[20221214 14:11:13 @agent_ppo2.py:185][0m |           0.0190 |         235.6716 |           5.2261 |
[32m[20221214 14:11:13 @agent_ppo2.py:185][0m |          -0.0022 |         204.9823 |           5.2764 |
[32m[20221214 14:11:13 @agent_ppo2.py:185][0m |          -0.0012 |         204.4151 |           5.2595 |
[32m[20221214 14:11:13 @agent_ppo2.py:185][0m |           0.0008 |         204.4013 |           5.2840 |
[32m[20221214 14:11:13 @agent_ppo2.py:185][0m |          -0.0017 |         203.9921 |           5.2787 |
[32m[20221214 14:11:13 @agent_ppo2.py:185][0m |          -0.0007 |         203.6387 |           5.2910 |
[32m[20221214 14:11:14 @agent_ppo2.py:185][0m |          -0.0025 |         203.5499 |           5.3023 |
[32m[20221214 14:11:14 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:11:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.46
[32m[20221214 14:11:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.84
[32m[20221214 14:11:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.25
[32m[20221214 14:11:14 @agent_ppo2.py:143][0m Total time:      13.19 min
[32m[20221214 14:11:14 @agent_ppo2.py:145][0m 1208320 total steps have happened
[32m[20221214 14:11:14 @agent_ppo2.py:121][0m #------------------------ Iteration 590 --------------------------#
[32m[20221214 14:11:14 @agent_ppo2.py:127][0m Sampling time: 0.26 s by 5 slaves
[32m[20221214 14:11:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:14 @agent_ppo2.py:185][0m |          -0.0015 |         177.1049 |           5.6108 |
[32m[20221214 14:11:14 @agent_ppo2.py:185][0m |          -0.0026 |         168.0106 |           5.6191 |
[32m[20221214 14:11:14 @agent_ppo2.py:185][0m |           0.0094 |         179.6705 |           5.6024 |
[32m[20221214 14:11:14 @agent_ppo2.py:185][0m |           0.0040 |         166.9422 |           5.6290 |
[32m[20221214 14:11:15 @agent_ppo2.py:185][0m |           0.0004 |         162.6041 |           5.6039 |
[32m[20221214 14:11:15 @agent_ppo2.py:185][0m |          -0.0024 |         162.0126 |           5.5882 |
[32m[20221214 14:11:15 @agent_ppo2.py:185][0m |          -0.0022 |         161.0330 |           5.6042 |
[32m[20221214 14:11:15 @agent_ppo2.py:185][0m |          -0.0029 |         161.3544 |           5.5865 |
[32m[20221214 14:11:15 @agent_ppo2.py:185][0m |          -0.0028 |         160.4776 |           5.5678 |
[32m[20221214 14:11:15 @agent_ppo2.py:185][0m |          -0.0033 |         160.0866 |           5.5895 |
[32m[20221214 14:11:15 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:11:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.14
[32m[20221214 14:11:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.05
[32m[20221214 14:11:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.51
[32m[20221214 14:11:15 @agent_ppo2.py:143][0m Total time:      13.22 min
[32m[20221214 14:11:15 @agent_ppo2.py:145][0m 1210368 total steps have happened
[32m[20221214 14:11:15 @agent_ppo2.py:121][0m #------------------------ Iteration 591 --------------------------#
[32m[20221214 14:11:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:16 @agent_ppo2.py:185][0m |          -0.0022 |         200.6289 |           5.5624 |
[32m[20221214 14:11:16 @agent_ppo2.py:185][0m |          -0.0020 |         192.0220 |           5.5650 |
[32m[20221214 14:11:16 @agent_ppo2.py:185][0m |          -0.0030 |         187.1737 |           5.5610 |
[32m[20221214 14:11:16 @agent_ppo2.py:185][0m |          -0.0032 |         184.8833 |           5.5842 |
[32m[20221214 14:11:16 @agent_ppo2.py:185][0m |          -0.0023 |         183.3827 |           5.5637 |
[32m[20221214 14:11:16 @agent_ppo2.py:185][0m |           0.0092 |         193.1671 |           5.5619 |
[32m[20221214 14:11:16 @agent_ppo2.py:185][0m |           0.0094 |         189.3548 |           5.5767 |
[32m[20221214 14:11:16 @agent_ppo2.py:185][0m |          -0.0021 |         181.3101 |           5.5910 |
[32m[20221214 14:11:16 @agent_ppo2.py:185][0m |          -0.0026 |         180.9785 |           5.5487 |
[32m[20221214 14:11:16 @agent_ppo2.py:185][0m |           0.0093 |         192.1787 |           5.5641 |
[32m[20221214 14:11:16 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:11:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 769.67
[32m[20221214 14:11:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.82
[32m[20221214 14:11:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.82
[32m[20221214 14:11:17 @agent_ppo2.py:143][0m Total time:      13.24 min
[32m[20221214 14:11:17 @agent_ppo2.py:145][0m 1212416 total steps have happened
[32m[20221214 14:11:17 @agent_ppo2.py:121][0m #------------------------ Iteration 592 --------------------------#
[32m[20221214 14:11:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:11:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:17 @agent_ppo2.py:185][0m |          -0.0000 |         148.3559 |           5.3965 |
[32m[20221214 14:11:17 @agent_ppo2.py:185][0m |          -0.0015 |         146.2411 |           5.3747 |
[32m[20221214 14:11:17 @agent_ppo2.py:185][0m |           0.0089 |         158.7083 |           5.3691 |
[32m[20221214 14:11:17 @agent_ppo2.py:185][0m |          -0.0000 |         144.4027 |           5.3484 |
[32m[20221214 14:11:17 @agent_ppo2.py:185][0m |           0.0014 |         143.6528 |           5.2799 |
[32m[20221214 14:11:17 @agent_ppo2.py:185][0m |          -0.0019 |         143.0064 |           5.2663 |
[32m[20221214 14:11:17 @agent_ppo2.py:185][0m |          -0.0013 |         141.7444 |           5.2439 |
[32m[20221214 14:11:18 @agent_ppo2.py:185][0m |          -0.0033 |         141.7853 |           5.2219 |
[32m[20221214 14:11:18 @agent_ppo2.py:185][0m |           0.0004 |         140.8204 |           5.1943 |
[32m[20221214 14:11:18 @agent_ppo2.py:185][0m |          -0.0012 |         140.7129 |           5.1983 |
[32m[20221214 14:11:18 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:11:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.93
[32m[20221214 14:11:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.30
[32m[20221214 14:11:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.96
[32m[20221214 14:11:18 @agent_ppo2.py:143][0m Total time:      13.26 min
[32m[20221214 14:11:18 @agent_ppo2.py:145][0m 1214464 total steps have happened
[32m[20221214 14:11:18 @agent_ppo2.py:121][0m #------------------------ Iteration 593 --------------------------#
[32m[20221214 14:11:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:18 @agent_ppo2.py:185][0m |          -0.0019 |         147.4112 |           4.9004 |
[32m[20221214 14:11:18 @agent_ppo2.py:185][0m |           0.0043 |         149.8747 |           4.9442 |
[32m[20221214 14:11:18 @agent_ppo2.py:185][0m |          -0.0029 |         140.2629 |           4.9813 |
[32m[20221214 14:11:18 @agent_ppo2.py:185][0m |          -0.0025 |         137.9206 |           4.9384 |
[32m[20221214 14:11:19 @agent_ppo2.py:185][0m |          -0.0032 |         136.1151 |           4.9584 |
[32m[20221214 14:11:19 @agent_ppo2.py:185][0m |          -0.0022 |         135.3130 |           4.8975 |
[32m[20221214 14:11:19 @agent_ppo2.py:185][0m |          -0.0039 |         134.8737 |           4.9612 |
[32m[20221214 14:11:19 @agent_ppo2.py:185][0m |          -0.0028 |         134.5408 |           4.8897 |
[32m[20221214 14:11:19 @agent_ppo2.py:185][0m |           0.0052 |         139.2639 |           4.9039 |
[32m[20221214 14:11:19 @agent_ppo2.py:185][0m |          -0.0040 |         134.8191 |           4.9168 |
[32m[20221214 14:11:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:11:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.55
[32m[20221214 14:11:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.26
[32m[20221214 14:11:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.22
[32m[20221214 14:11:19 @agent_ppo2.py:143][0m Total time:      13.28 min
[32m[20221214 14:11:19 @agent_ppo2.py:145][0m 1216512 total steps have happened
[32m[20221214 14:11:19 @agent_ppo2.py:121][0m #------------------------ Iteration 594 --------------------------#
[32m[20221214 14:11:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:19 @agent_ppo2.py:185][0m |          -0.0039 |         161.1906 |           5.4470 |
[32m[20221214 14:11:20 @agent_ppo2.py:185][0m |           0.0018 |         158.6319 |           5.4283 |
[32m[20221214 14:11:20 @agent_ppo2.py:185][0m |          -0.0035 |         153.1081 |           5.4375 |
[32m[20221214 14:11:20 @agent_ppo2.py:185][0m |           0.0065 |         161.4054 |           5.4634 |
[32m[20221214 14:11:20 @agent_ppo2.py:185][0m |          -0.0032 |         149.4169 |           5.4164 |
[32m[20221214 14:11:20 @agent_ppo2.py:185][0m |          -0.0034 |         147.9678 |           5.4142 |
[32m[20221214 14:11:20 @agent_ppo2.py:185][0m |           0.0052 |         153.6954 |           5.4210 |
[32m[20221214 14:11:20 @agent_ppo2.py:185][0m |          -0.0035 |         146.5209 |           5.4262 |
[32m[20221214 14:11:20 @agent_ppo2.py:185][0m |          -0.0041 |         145.7394 |           5.4227 |
[32m[20221214 14:11:20 @agent_ppo2.py:185][0m |          -0.0027 |         145.1301 |           5.4180 |
[32m[20221214 14:11:20 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:11:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.23
[32m[20221214 14:11:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.47
[32m[20221214 14:11:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.77
[32m[20221214 14:11:20 @agent_ppo2.py:143][0m Total time:      13.31 min
[32m[20221214 14:11:20 @agent_ppo2.py:145][0m 1218560 total steps have happened
[32m[20221214 14:11:20 @agent_ppo2.py:121][0m #------------------------ Iteration 595 --------------------------#
[32m[20221214 14:11:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:21 @agent_ppo2.py:185][0m |          -0.0020 |         190.1255 |           5.2160 |
[32m[20221214 14:11:21 @agent_ppo2.py:185][0m |          -0.0011 |         185.2295 |           5.2607 |
[32m[20221214 14:11:21 @agent_ppo2.py:185][0m |          -0.0027 |         184.3907 |           5.2258 |
[32m[20221214 14:11:21 @agent_ppo2.py:185][0m |           0.0002 |         184.3360 |           5.1618 |
[32m[20221214 14:11:21 @agent_ppo2.py:185][0m |          -0.0027 |         183.1471 |           5.2059 |
[32m[20221214 14:11:21 @agent_ppo2.py:185][0m |           0.0058 |         189.5664 |           5.1990 |
[32m[20221214 14:11:21 @agent_ppo2.py:185][0m |          -0.0032 |         182.6951 |           5.1778 |
[32m[20221214 14:11:21 @agent_ppo2.py:185][0m |          -0.0028 |         182.0092 |           5.1605 |
[32m[20221214 14:11:21 @agent_ppo2.py:185][0m |          -0.0031 |         182.4074 |           5.1328 |
[32m[20221214 14:11:21 @agent_ppo2.py:185][0m |          -0.0027 |         181.4491 |           5.1756 |
[32m[20221214 14:11:21 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:11:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.92
[32m[20221214 14:11:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.70
[32m[20221214 14:11:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.25
[32m[20221214 14:11:22 @agent_ppo2.py:143][0m Total time:      13.33 min
[32m[20221214 14:11:22 @agent_ppo2.py:145][0m 1220608 total steps have happened
[32m[20221214 14:11:22 @agent_ppo2.py:121][0m #------------------------ Iteration 596 --------------------------#
[32m[20221214 14:11:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:22 @agent_ppo2.py:185][0m |          -0.0029 |         243.9374 |           4.8004 |
[32m[20221214 14:11:22 @agent_ppo2.py:185][0m |          -0.0045 |         238.4806 |           4.7731 |
[32m[20221214 14:11:22 @agent_ppo2.py:185][0m |          -0.0030 |         238.1006 |           4.7526 |
[32m[20221214 14:11:22 @agent_ppo2.py:185][0m |          -0.0029 |         237.3198 |           4.7390 |
[32m[20221214 14:11:22 @agent_ppo2.py:185][0m |          -0.0053 |         236.3329 |           4.7394 |
[32m[20221214 14:11:22 @agent_ppo2.py:185][0m |          -0.0043 |         236.0457 |           4.6793 |
[32m[20221214 14:11:22 @agent_ppo2.py:185][0m |          -0.0050 |         235.8681 |           4.6873 |
[32m[20221214 14:11:23 @agent_ppo2.py:185][0m |           0.0028 |         240.2891 |           4.6256 |
[32m[20221214 14:11:23 @agent_ppo2.py:185][0m |          -0.0047 |         235.1472 |           4.6660 |
[32m[20221214 14:11:23 @agent_ppo2.py:185][0m |          -0.0039 |         235.2675 |           4.6315 |
[32m[20221214 14:11:23 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:11:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.71
[32m[20221214 14:11:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.15
[32m[20221214 14:11:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.44
[32m[20221214 14:11:23 @agent_ppo2.py:143][0m Total time:      13.35 min
[32m[20221214 14:11:23 @agent_ppo2.py:145][0m 1222656 total steps have happened
[32m[20221214 14:11:23 @agent_ppo2.py:121][0m #------------------------ Iteration 597 --------------------------#
[32m[20221214 14:11:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:23 @agent_ppo2.py:185][0m |           0.0000 |         250.2144 |           4.7432 |
[32m[20221214 14:11:23 @agent_ppo2.py:185][0m |          -0.0022 |         248.1311 |           4.7485 |
[32m[20221214 14:11:23 @agent_ppo2.py:185][0m |          -0.0024 |         246.2340 |           4.7819 |
[32m[20221214 14:11:23 @agent_ppo2.py:185][0m |          -0.0022 |         245.0426 |           4.7915 |
[32m[20221214 14:11:24 @agent_ppo2.py:185][0m |          -0.0001 |         244.3710 |           4.8248 |
[32m[20221214 14:11:24 @agent_ppo2.py:185][0m |           0.0101 |         273.8342 |           4.8401 |
[32m[20221214 14:11:24 @agent_ppo2.py:185][0m |          -0.0000 |         242.6038 |           4.8345 |
[32m[20221214 14:11:24 @agent_ppo2.py:185][0m |          -0.0035 |         242.2900 |           4.8832 |
[32m[20221214 14:11:24 @agent_ppo2.py:185][0m |          -0.0025 |         241.2506 |           4.8838 |
[32m[20221214 14:11:24 @agent_ppo2.py:185][0m |          -0.0019 |         240.2843 |           4.9242 |
[32m[20221214 14:11:24 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:11:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.48
[32m[20221214 14:11:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.54
[32m[20221214 14:11:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.60
[32m[20221214 14:11:24 @agent_ppo2.py:143][0m Total time:      13.37 min
[32m[20221214 14:11:24 @agent_ppo2.py:145][0m 1224704 total steps have happened
[32m[20221214 14:11:24 @agent_ppo2.py:121][0m #------------------------ Iteration 598 --------------------------#
[32m[20221214 14:11:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:24 @agent_ppo2.py:185][0m |           0.0004 |         245.7678 |           5.1168 |
[32m[20221214 14:11:25 @agent_ppo2.py:185][0m |          -0.0037 |         241.4720 |           5.1488 |
[32m[20221214 14:11:25 @agent_ppo2.py:185][0m |          -0.0033 |         238.7676 |           5.1893 |
[32m[20221214 14:11:25 @agent_ppo2.py:185][0m |           0.0083 |         262.9705 |           5.2044 |
[32m[20221214 14:11:25 @agent_ppo2.py:185][0m |           0.0202 |         273.1676 |           5.2224 |
[32m[20221214 14:11:25 @agent_ppo2.py:185][0m |          -0.0003 |         239.2331 |           5.1828 |
[32m[20221214 14:11:25 @agent_ppo2.py:185][0m |          -0.0045 |         237.0419 |           5.2599 |
[32m[20221214 14:11:25 @agent_ppo2.py:185][0m |          -0.0041 |         237.0536 |           5.2652 |
[32m[20221214 14:11:25 @agent_ppo2.py:185][0m |          -0.0046 |         236.0830 |           5.2820 |
[32m[20221214 14:11:25 @agent_ppo2.py:185][0m |          -0.0051 |         236.5061 |           5.2941 |
[32m[20221214 14:11:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:11:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.02
[32m[20221214 14:11:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.60
[32m[20221214 14:11:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.09
[32m[20221214 14:11:25 @agent_ppo2.py:143][0m Total time:      13.39 min
[32m[20221214 14:11:25 @agent_ppo2.py:145][0m 1226752 total steps have happened
[32m[20221214 14:11:25 @agent_ppo2.py:121][0m #------------------------ Iteration 599 --------------------------#
[32m[20221214 14:11:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:26 @agent_ppo2.py:185][0m |           0.0000 |         215.0263 |           5.4556 |
[32m[20221214 14:11:26 @agent_ppo2.py:185][0m |          -0.0046 |         210.0816 |           5.5421 |
[32m[20221214 14:11:26 @agent_ppo2.py:185][0m |          -0.0025 |         208.4318 |           5.5130 |
[32m[20221214 14:11:26 @agent_ppo2.py:185][0m |          -0.0031 |         207.0450 |           5.5215 |
[32m[20221214 14:11:26 @agent_ppo2.py:185][0m |          -0.0042 |         206.3545 |           5.5482 |
[32m[20221214 14:11:26 @agent_ppo2.py:185][0m |           0.0019 |         206.9803 |           5.5516 |
[32m[20221214 14:11:26 @agent_ppo2.py:185][0m |          -0.0022 |         204.2400 |           5.5399 |
[32m[20221214 14:11:26 @agent_ppo2.py:185][0m |          -0.0037 |         204.6965 |           5.5915 |
[32m[20221214 14:11:26 @agent_ppo2.py:185][0m |          -0.0026 |         204.1251 |           5.5521 |
[32m[20221214 14:11:26 @agent_ppo2.py:185][0m |          -0.0045 |         203.9008 |           5.5186 |
[32m[20221214 14:11:26 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:11:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.65
[32m[20221214 14:11:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.17
[32m[20221214 14:11:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.77
[32m[20221214 14:11:27 @agent_ppo2.py:143][0m Total time:      13.41 min
[32m[20221214 14:11:27 @agent_ppo2.py:145][0m 1228800 total steps have happened
[32m[20221214 14:11:27 @agent_ppo2.py:121][0m #------------------------ Iteration 600 --------------------------#
[32m[20221214 14:11:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:11:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:27 @agent_ppo2.py:185][0m |          -0.0000 |         206.6919 |           4.9952 |
[32m[20221214 14:11:27 @agent_ppo2.py:185][0m |           0.0061 |         207.9685 |           4.9446 |
[32m[20221214 14:11:27 @agent_ppo2.py:185][0m |           0.0022 |         202.7058 |           4.9530 |
[32m[20221214 14:11:27 @agent_ppo2.py:185][0m |          -0.0001 |         199.3413 |           4.9310 |
[32m[20221214 14:11:27 @agent_ppo2.py:185][0m |          -0.0026 |         199.0282 |           4.8968 |
[32m[20221214 14:11:27 @agent_ppo2.py:185][0m |          -0.0022 |         198.4385 |           4.8567 |
[32m[20221214 14:11:28 @agent_ppo2.py:185][0m |          -0.0012 |         197.8174 |           4.8444 |
[32m[20221214 14:11:28 @agent_ppo2.py:185][0m |          -0.0021 |         197.4527 |           4.8564 |
[32m[20221214 14:11:28 @agent_ppo2.py:185][0m |          -0.0010 |         197.3211 |           4.7886 |
[32m[20221214 14:11:28 @agent_ppo2.py:185][0m |          -0.0020 |         197.3351 |           4.7846 |
[32m[20221214 14:11:28 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:11:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.96
[32m[20221214 14:11:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.61
[32m[20221214 14:11:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.60
[32m[20221214 14:11:28 @agent_ppo2.py:143][0m Total time:      13.43 min
[32m[20221214 14:11:28 @agent_ppo2.py:145][0m 1230848 total steps have happened
[32m[20221214 14:11:28 @agent_ppo2.py:121][0m #------------------------ Iteration 601 --------------------------#
[32m[20221214 14:11:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:28 @agent_ppo2.py:185][0m |          -0.0009 |         205.5283 |           5.0414 |
[32m[20221214 14:11:28 @agent_ppo2.py:185][0m |          -0.0011 |         201.1736 |           5.0251 |
[32m[20221214 14:11:28 @agent_ppo2.py:185][0m |          -0.0022 |         197.4909 |           5.0891 |
[32m[20221214 14:11:29 @agent_ppo2.py:185][0m |          -0.0013 |         194.9627 |           5.0286 |
[32m[20221214 14:11:29 @agent_ppo2.py:185][0m |          -0.0021 |         194.4993 |           5.0265 |
[32m[20221214 14:11:29 @agent_ppo2.py:185][0m |           0.0005 |         192.6748 |           5.0624 |
[32m[20221214 14:11:29 @agent_ppo2.py:185][0m |          -0.0029 |         192.0440 |           5.0061 |
[32m[20221214 14:11:29 @agent_ppo2.py:185][0m |           0.0088 |         205.4927 |           5.0181 |
[32m[20221214 14:11:29 @agent_ppo2.py:185][0m |          -0.0016 |         191.0026 |           5.0918 |
[32m[20221214 14:11:29 @agent_ppo2.py:185][0m |           0.0188 |         222.1682 |           5.0140 |
[32m[20221214 14:11:29 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:11:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.38
[32m[20221214 14:11:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.98
[32m[20221214 14:11:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.03
[32m[20221214 14:11:29 @agent_ppo2.py:143][0m Total time:      13.45 min
[32m[20221214 14:11:29 @agent_ppo2.py:145][0m 1232896 total steps have happened
[32m[20221214 14:11:29 @agent_ppo2.py:121][0m #------------------------ Iteration 602 --------------------------#
[32m[20221214 14:11:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:30 @agent_ppo2.py:185][0m |          -0.0009 |         189.4165 |           5.0576 |
[32m[20221214 14:11:30 @agent_ppo2.py:185][0m |          -0.0021 |         186.7043 |           5.0730 |
[32m[20221214 14:11:30 @agent_ppo2.py:185][0m |          -0.0020 |         185.1093 |           5.0778 |
[32m[20221214 14:11:30 @agent_ppo2.py:185][0m |           0.0080 |         193.1900 |           5.0739 |
[32m[20221214 14:11:30 @agent_ppo2.py:185][0m |          -0.0034 |         183.5610 |           5.0708 |
[32m[20221214 14:11:30 @agent_ppo2.py:185][0m |           0.0152 |         206.9692 |           5.0518 |
[32m[20221214 14:11:30 @agent_ppo2.py:185][0m |          -0.0003 |         182.7243 |           5.0814 |
[32m[20221214 14:11:30 @agent_ppo2.py:185][0m |          -0.0012 |         182.4260 |           5.0294 |
[32m[20221214 14:11:30 @agent_ppo2.py:185][0m |          -0.0027 |         181.6325 |           5.0110 |
[32m[20221214 14:11:30 @agent_ppo2.py:185][0m |          -0.0017 |         181.3454 |           5.0105 |
[32m[20221214 14:11:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:11:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.07
[32m[20221214 14:11:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.53
[32m[20221214 14:11:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.24
[32m[20221214 14:11:30 @agent_ppo2.py:143][0m Total time:      13.47 min
[32m[20221214 14:11:30 @agent_ppo2.py:145][0m 1234944 total steps have happened
[32m[20221214 14:11:30 @agent_ppo2.py:121][0m #------------------------ Iteration 603 --------------------------#
[32m[20221214 14:11:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:31 @agent_ppo2.py:185][0m |          -0.0017 |         182.9612 |           4.8828 |
[32m[20221214 14:11:31 @agent_ppo2.py:185][0m |           0.0021 |         176.1798 |           4.9291 |
[32m[20221214 14:11:31 @agent_ppo2.py:185][0m |          -0.0013 |         173.5000 |           4.9553 |
[32m[20221214 14:11:31 @agent_ppo2.py:185][0m |          -0.0032 |         171.2376 |           4.9766 |
[32m[20221214 14:11:31 @agent_ppo2.py:185][0m |           0.0087 |         184.5571 |           5.0175 |
[32m[20221214 14:11:31 @agent_ppo2.py:185][0m |          -0.0013 |         169.4608 |           5.0064 |
[32m[20221214 14:11:31 @agent_ppo2.py:185][0m |          -0.0026 |         169.5974 |           5.0752 |
[32m[20221214 14:11:31 @agent_ppo2.py:185][0m |          -0.0028 |         168.3850 |           5.0907 |
[32m[20221214 14:11:31 @agent_ppo2.py:185][0m |           0.0153 |         187.1397 |           5.1141 |
[32m[20221214 14:11:32 @agent_ppo2.py:185][0m |          -0.0038 |         168.1402 |           5.1640 |
[32m[20221214 14:11:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:11:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.41
[32m[20221214 14:11:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.00
[32m[20221214 14:11:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.56
[32m[20221214 14:11:32 @agent_ppo2.py:143][0m Total time:      13.49 min
[32m[20221214 14:11:32 @agent_ppo2.py:145][0m 1236992 total steps have happened
[32m[20221214 14:11:32 @agent_ppo2.py:121][0m #------------------------ Iteration 604 --------------------------#
[32m[20221214 14:11:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:32 @agent_ppo2.py:185][0m |           0.0015 |         203.7643 |           5.1833 |
[32m[20221214 14:11:32 @agent_ppo2.py:185][0m |          -0.0010 |         198.3334 |           5.1505 |
[32m[20221214 14:11:32 @agent_ppo2.py:185][0m |          -0.0022 |         196.3455 |           5.1781 |
[32m[20221214 14:11:32 @agent_ppo2.py:185][0m |          -0.0027 |         194.5843 |           5.1728 |
[32m[20221214 14:11:32 @agent_ppo2.py:185][0m |          -0.0008 |         193.6074 |           5.1527 |
[32m[20221214 14:11:32 @agent_ppo2.py:185][0m |          -0.0016 |         192.5810 |           5.1646 |
[32m[20221214 14:11:33 @agent_ppo2.py:185][0m |          -0.0033 |         192.0890 |           5.1749 |
[32m[20221214 14:11:33 @agent_ppo2.py:185][0m |          -0.0017 |         190.9716 |           5.1282 |
[32m[20221214 14:11:33 @agent_ppo2.py:185][0m |           0.0075 |         202.0116 |           5.1295 |
[32m[20221214 14:11:33 @agent_ppo2.py:185][0m |          -0.0016 |         190.1151 |           5.1439 |
[32m[20221214 14:11:33 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:11:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.36
[32m[20221214 14:11:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.84
[32m[20221214 14:11:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.51
[32m[20221214 14:11:33 @agent_ppo2.py:143][0m Total time:      13.51 min
[32m[20221214 14:11:33 @agent_ppo2.py:145][0m 1239040 total steps have happened
[32m[20221214 14:11:33 @agent_ppo2.py:121][0m #------------------------ Iteration 605 --------------------------#
[32m[20221214 14:11:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:11:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:33 @agent_ppo2.py:185][0m |          -0.0016 |         188.4721 |           5.2585 |
[32m[20221214 14:11:33 @agent_ppo2.py:185][0m |          -0.0030 |         186.8105 |           5.2803 |
[32m[20221214 14:11:34 @agent_ppo2.py:185][0m |           0.0114 |         205.9288 |           5.2980 |
[32m[20221214 14:11:34 @agent_ppo2.py:185][0m |          -0.0040 |         186.1898 |           5.2619 |
[32m[20221214 14:11:34 @agent_ppo2.py:185][0m |          -0.0023 |         185.1667 |           5.2564 |
[32m[20221214 14:11:34 @agent_ppo2.py:185][0m |          -0.0016 |         184.7671 |           5.2264 |
[32m[20221214 14:11:34 @agent_ppo2.py:185][0m |          -0.0025 |         184.9733 |           5.2429 |
[32m[20221214 14:11:34 @agent_ppo2.py:185][0m |          -0.0025 |         184.0728 |           5.2106 |
[32m[20221214 14:11:34 @agent_ppo2.py:185][0m |          -0.0011 |         183.9721 |           5.2053 |
[32m[20221214 14:11:34 @agent_ppo2.py:185][0m |          -0.0038 |         183.6652 |           5.2287 |
[32m[20221214 14:11:34 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:11:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.79
[32m[20221214 14:11:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.37
[32m[20221214 14:11:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.02
[32m[20221214 14:11:34 @agent_ppo2.py:143][0m Total time:      13.54 min
[32m[20221214 14:11:34 @agent_ppo2.py:145][0m 1241088 total steps have happened
[32m[20221214 14:11:34 @agent_ppo2.py:121][0m #------------------------ Iteration 606 --------------------------#
[32m[20221214 14:11:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:35 @agent_ppo2.py:185][0m |          -0.0008 |         182.5203 |           4.7734 |
[32m[20221214 14:11:35 @agent_ppo2.py:185][0m |           0.0112 |         196.4130 |           4.7916 |
[32m[20221214 14:11:35 @agent_ppo2.py:185][0m |          -0.0042 |         175.1489 |           4.7986 |
[32m[20221214 14:11:35 @agent_ppo2.py:185][0m |          -0.0036 |         172.3715 |           4.7657 |
[32m[20221214 14:11:35 @agent_ppo2.py:185][0m |          -0.0038 |         171.6043 |           4.7603 |
[32m[20221214 14:11:35 @agent_ppo2.py:185][0m |          -0.0024 |         170.9379 |           4.7328 |
[32m[20221214 14:11:35 @agent_ppo2.py:185][0m |          -0.0043 |         170.6362 |           4.7343 |
[32m[20221214 14:11:35 @agent_ppo2.py:185][0m |          -0.0035 |         169.7588 |           4.6843 |
[32m[20221214 14:11:35 @agent_ppo2.py:185][0m |          -0.0053 |         169.6622 |           4.6902 |
[32m[20221214 14:11:36 @agent_ppo2.py:185][0m |          -0.0046 |         168.7862 |           4.6849 |
[32m[20221214 14:11:36 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:11:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.96
[32m[20221214 14:11:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.08
[32m[20221214 14:11:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.02
[32m[20221214 14:11:36 @agent_ppo2.py:143][0m Total time:      13.56 min
[32m[20221214 14:11:36 @agent_ppo2.py:145][0m 1243136 total steps have happened
[32m[20221214 14:11:36 @agent_ppo2.py:121][0m #------------------------ Iteration 607 --------------------------#
[32m[20221214 14:11:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:36 @agent_ppo2.py:185][0m |          -0.0010 |         208.1425 |           4.7503 |
[32m[20221214 14:11:36 @agent_ppo2.py:185][0m |          -0.0014 |         204.9127 |           4.7517 |
[32m[20221214 14:11:36 @agent_ppo2.py:185][0m |          -0.0021 |         202.8975 |           4.7662 |
[32m[20221214 14:11:36 @agent_ppo2.py:185][0m |          -0.0018 |         202.0191 |           4.7451 |
[32m[20221214 14:11:36 @agent_ppo2.py:185][0m |           0.0105 |         225.7631 |           4.7152 |
[32m[20221214 14:11:37 @agent_ppo2.py:185][0m |           0.0068 |         210.1583 |           4.7363 |
[32m[20221214 14:11:37 @agent_ppo2.py:185][0m |          -0.0012 |         198.7046 |           4.7017 |
[32m[20221214 14:11:37 @agent_ppo2.py:185][0m |          -0.0014 |         198.2640 |           4.7090 |
[32m[20221214 14:11:37 @agent_ppo2.py:185][0m |          -0.0012 |         197.2621 |           4.6844 |
[32m[20221214 14:11:37 @agent_ppo2.py:185][0m |          -0.0023 |         197.1983 |           4.6825 |
[32m[20221214 14:11:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:11:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.61
[32m[20221214 14:11:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.20
[32m[20221214 14:11:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.15
[32m[20221214 14:11:37 @agent_ppo2.py:143][0m Total time:      13.58 min
[32m[20221214 14:11:37 @agent_ppo2.py:145][0m 1245184 total steps have happened
[32m[20221214 14:11:37 @agent_ppo2.py:121][0m #------------------------ Iteration 608 --------------------------#
[32m[20221214 14:11:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:37 @agent_ppo2.py:185][0m |          -0.0048 |         207.6860 |           4.8187 |
[32m[20221214 14:11:38 @agent_ppo2.py:185][0m |          -0.0043 |         201.4103 |           4.8211 |
[32m[20221214 14:11:38 @agent_ppo2.py:185][0m |          -0.0041 |         197.8370 |           4.8142 |
[32m[20221214 14:11:38 @agent_ppo2.py:185][0m |          -0.0033 |         195.9629 |           4.7913 |
[32m[20221214 14:11:38 @agent_ppo2.py:185][0m |          -0.0047 |         194.6909 |           4.8063 |
[32m[20221214 14:11:38 @agent_ppo2.py:185][0m |           0.0028 |         199.0737 |           4.7710 |
[32m[20221214 14:11:38 @agent_ppo2.py:185][0m |          -0.0048 |         193.0545 |           4.7627 |
[32m[20221214 14:11:38 @agent_ppo2.py:185][0m |          -0.0040 |         192.9462 |           4.7483 |
[32m[20221214 14:11:38 @agent_ppo2.py:185][0m |          -0.0041 |         192.6010 |           4.7272 |
[32m[20221214 14:11:38 @agent_ppo2.py:185][0m |          -0.0037 |         191.5830 |           4.7036 |
[32m[20221214 14:11:38 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:11:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.32
[32m[20221214 14:11:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.22
[32m[20221214 14:11:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.09
[32m[20221214 14:11:38 @agent_ppo2.py:143][0m Total time:      13.61 min
[32m[20221214 14:11:38 @agent_ppo2.py:145][0m 1247232 total steps have happened
[32m[20221214 14:11:38 @agent_ppo2.py:121][0m #------------------------ Iteration 609 --------------------------#
[32m[20221214 14:11:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:39 @agent_ppo2.py:185][0m |          -0.0013 |         177.9963 |           4.7262 |
[32m[20221214 14:11:39 @agent_ppo2.py:185][0m |          -0.0006 |         174.3298 |           4.7604 |
[32m[20221214 14:11:39 @agent_ppo2.py:185][0m |          -0.0024 |         174.2106 |           4.7488 |
[32m[20221214 14:11:39 @agent_ppo2.py:185][0m |          -0.0021 |         173.4407 |           4.7345 |
[32m[20221214 14:11:39 @agent_ppo2.py:185][0m |          -0.0018 |         171.8145 |           4.7376 |
[32m[20221214 14:11:39 @agent_ppo2.py:185][0m |          -0.0023 |         171.3376 |           4.7043 |
[32m[20221214 14:11:39 @agent_ppo2.py:185][0m |          -0.0042 |         171.4028 |           4.6988 |
[32m[20221214 14:11:39 @agent_ppo2.py:185][0m |          -0.0016 |         170.6916 |           4.6828 |
[32m[20221214 14:11:40 @agent_ppo2.py:185][0m |          -0.0009 |         170.8214 |           4.6344 |
[32m[20221214 14:11:40 @agent_ppo2.py:185][0m |           0.0018 |         171.5675 |           4.6541 |
[32m[20221214 14:11:40 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:11:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.00
[32m[20221214 14:11:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.21
[32m[20221214 14:11:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.16
[32m[20221214 14:11:40 @agent_ppo2.py:143][0m Total time:      13.63 min
[32m[20221214 14:11:40 @agent_ppo2.py:145][0m 1249280 total steps have happened
[32m[20221214 14:11:40 @agent_ppo2.py:121][0m #------------------------ Iteration 610 --------------------------#
[32m[20221214 14:11:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:11:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:40 @agent_ppo2.py:185][0m |           0.0060 |         200.8653 |           4.3424 |
[32m[20221214 14:11:40 @agent_ppo2.py:185][0m |          -0.0024 |         188.7617 |           4.3650 |
[32m[20221214 14:11:40 @agent_ppo2.py:185][0m |           0.0029 |         188.6124 |           4.3663 |
[32m[20221214 14:11:40 @agent_ppo2.py:185][0m |          -0.0028 |         183.7261 |           4.3965 |
[32m[20221214 14:11:41 @agent_ppo2.py:185][0m |           0.0058 |         188.9396 |           4.3873 |
[32m[20221214 14:11:41 @agent_ppo2.py:185][0m |          -0.0020 |         182.7476 |           4.3878 |
[32m[20221214 14:11:41 @agent_ppo2.py:185][0m |          -0.0030 |         182.1667 |           4.4051 |
[32m[20221214 14:11:41 @agent_ppo2.py:185][0m |          -0.0024 |         181.2826 |           4.3955 |
[32m[20221214 14:11:41 @agent_ppo2.py:185][0m |           0.0030 |         181.8764 |           4.4282 |
[32m[20221214 14:11:41 @agent_ppo2.py:185][0m |          -0.0023 |         180.5376 |           4.4291 |
[32m[20221214 14:11:41 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:11:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.46
[32m[20221214 14:11:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.47
[32m[20221214 14:11:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.49
[32m[20221214 14:11:41 @agent_ppo2.py:143][0m Total time:      13.65 min
[32m[20221214 14:11:41 @agent_ppo2.py:145][0m 1251328 total steps have happened
[32m[20221214 14:11:41 @agent_ppo2.py:121][0m #------------------------ Iteration 611 --------------------------#
[32m[20221214 14:11:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:42 @agent_ppo2.py:185][0m |          -0.0043 |         169.9939 |           4.3897 |
[32m[20221214 14:11:42 @agent_ppo2.py:185][0m |          -0.0060 |         162.6102 |           4.4407 |
[32m[20221214 14:11:42 @agent_ppo2.py:185][0m |          -0.0056 |         158.6679 |           4.4014 |
[32m[20221214 14:11:42 @agent_ppo2.py:185][0m |          -0.0056 |         156.4437 |           4.3961 |
[32m[20221214 14:11:42 @agent_ppo2.py:185][0m |          -0.0073 |         154.7646 |           4.3759 |
[32m[20221214 14:11:42 @agent_ppo2.py:185][0m |          -0.0055 |         153.1103 |           4.3533 |
[32m[20221214 14:11:42 @agent_ppo2.py:185][0m |          -0.0045 |         151.6768 |           4.3923 |
[32m[20221214 14:11:42 @agent_ppo2.py:185][0m |          -0.0066 |         150.5777 |           4.3084 |
[32m[20221214 14:11:42 @agent_ppo2.py:185][0m |          -0.0064 |         150.1622 |           4.3352 |
[32m[20221214 14:11:42 @agent_ppo2.py:185][0m |          -0.0059 |         149.7613 |           4.3348 |
[32m[20221214 14:11:42 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:11:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.21
[32m[20221214 14:11:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.19
[32m[20221214 14:11:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.56
[32m[20221214 14:11:43 @agent_ppo2.py:143][0m Total time:      13.67 min
[32m[20221214 14:11:43 @agent_ppo2.py:145][0m 1253376 total steps have happened
[32m[20221214 14:11:43 @agent_ppo2.py:121][0m #------------------------ Iteration 612 --------------------------#
[32m[20221214 14:11:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:43 @agent_ppo2.py:185][0m |           0.0044 |         177.7070 |           4.3237 |
[32m[20221214 14:11:43 @agent_ppo2.py:185][0m |           0.0006 |         169.8096 |           4.3339 |
[32m[20221214 14:11:43 @agent_ppo2.py:185][0m |          -0.0034 |         166.4361 |           4.3540 |
[32m[20221214 14:11:43 @agent_ppo2.py:185][0m |          -0.0005 |         164.7843 |           4.3598 |
[32m[20221214 14:11:43 @agent_ppo2.py:185][0m |          -0.0047 |         163.6791 |           4.3845 |
[32m[20221214 14:11:43 @agent_ppo2.py:185][0m |          -0.0002 |         162.3913 |           4.3708 |
[32m[20221214 14:11:43 @agent_ppo2.py:185][0m |          -0.0022 |         161.2840 |           4.3587 |
[32m[20221214 14:11:44 @agent_ppo2.py:185][0m |          -0.0040 |         160.1209 |           4.3813 |
[32m[20221214 14:11:44 @agent_ppo2.py:185][0m |           0.0060 |         171.6650 |           4.3916 |
[32m[20221214 14:11:44 @agent_ppo2.py:185][0m |          -0.0056 |         159.5789 |           4.4072 |
[32m[20221214 14:11:44 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:11:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.78
[32m[20221214 14:11:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.00
[32m[20221214 14:11:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.64
[32m[20221214 14:11:44 @agent_ppo2.py:143][0m Total time:      13.70 min
[32m[20221214 14:11:44 @agent_ppo2.py:145][0m 1255424 total steps have happened
[32m[20221214 14:11:44 @agent_ppo2.py:121][0m #------------------------ Iteration 613 --------------------------#
[32m[20221214 14:11:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:44 @agent_ppo2.py:185][0m |           0.0023 |         196.1849 |           4.3056 |
[32m[20221214 14:11:44 @agent_ppo2.py:185][0m |          -0.0015 |         190.7853 |           4.3400 |
[32m[20221214 14:11:44 @agent_ppo2.py:185][0m |          -0.0016 |         189.3622 |           4.3144 |
[32m[20221214 14:11:45 @agent_ppo2.py:185][0m |           0.0038 |         191.5081 |           4.3408 |
[32m[20221214 14:11:45 @agent_ppo2.py:185][0m |          -0.0017 |         187.3552 |           4.3718 |
[32m[20221214 14:11:45 @agent_ppo2.py:185][0m |           0.0082 |         195.7408 |           4.3382 |
[32m[20221214 14:11:45 @agent_ppo2.py:185][0m |           0.0001 |         186.1906 |           4.3711 |
[32m[20221214 14:11:45 @agent_ppo2.py:185][0m |          -0.0007 |         185.7308 |           4.3623 |
[32m[20221214 14:11:45 @agent_ppo2.py:185][0m |          -0.0020 |         185.8708 |           4.3756 |
[32m[20221214 14:11:45 @agent_ppo2.py:185][0m |          -0.0022 |         184.9037 |           4.3809 |
[32m[20221214 14:11:45 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:11:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.37
[32m[20221214 14:11:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.10
[32m[20221214 14:11:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.30
[32m[20221214 14:11:45 @agent_ppo2.py:143][0m Total time:      13.72 min
[32m[20221214 14:11:45 @agent_ppo2.py:145][0m 1257472 total steps have happened
[32m[20221214 14:11:45 @agent_ppo2.py:121][0m #------------------------ Iteration 614 --------------------------#
[32m[20221214 14:11:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:46 @agent_ppo2.py:185][0m |          -0.0023 |         193.9060 |           4.6206 |
[32m[20221214 14:11:46 @agent_ppo2.py:185][0m |          -0.0032 |         187.8282 |           4.6756 |
[32m[20221214 14:11:46 @agent_ppo2.py:185][0m |           0.0028 |         188.1663 |           4.7322 |
[32m[20221214 14:11:46 @agent_ppo2.py:185][0m |           0.0141 |         211.5757 |           4.7612 |
[32m[20221214 14:11:46 @agent_ppo2.py:185][0m |          -0.0038 |         184.9181 |           4.8297 |
[32m[20221214 14:11:46 @agent_ppo2.py:185][0m |          -0.0033 |         183.0622 |           4.8619 |
[32m[20221214 14:11:46 @agent_ppo2.py:185][0m |          -0.0024 |         182.9927 |           4.9056 |
[32m[20221214 14:11:46 @agent_ppo2.py:185][0m |          -0.0027 |         182.4975 |           4.9072 |
[32m[20221214 14:11:46 @agent_ppo2.py:185][0m |          -0.0024 |         182.4024 |           4.9519 |
[32m[20221214 14:11:46 @agent_ppo2.py:185][0m |          -0.0039 |         181.7575 |           4.9815 |
[32m[20221214 14:11:46 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:11:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.07
[32m[20221214 14:11:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.57
[32m[20221214 14:11:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.80
[32m[20221214 14:11:47 @agent_ppo2.py:143][0m Total time:      13.74 min
[32m[20221214 14:11:47 @agent_ppo2.py:145][0m 1259520 total steps have happened
[32m[20221214 14:11:47 @agent_ppo2.py:121][0m #------------------------ Iteration 615 --------------------------#
[32m[20221214 14:11:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:47 @agent_ppo2.py:185][0m |          -0.0011 |         198.7470 |           5.0230 |
[32m[20221214 14:11:47 @agent_ppo2.py:185][0m |           0.0010 |         189.2175 |           5.0492 |
[32m[20221214 14:11:47 @agent_ppo2.py:185][0m |          -0.0028 |         180.8054 |           5.0451 |
[32m[20221214 14:11:47 @agent_ppo2.py:185][0m |          -0.0019 |         178.1466 |           5.0957 |
[32m[20221214 14:11:47 @agent_ppo2.py:185][0m |          -0.0019 |         176.6585 |           5.0970 |
[32m[20221214 14:11:47 @agent_ppo2.py:185][0m |          -0.0026 |         175.2862 |           5.0952 |
[32m[20221214 14:11:48 @agent_ppo2.py:185][0m |          -0.0018 |         173.5837 |           5.1109 |
[32m[20221214 14:11:48 @agent_ppo2.py:185][0m |          -0.0008 |         173.2454 |           5.1334 |
[32m[20221214 14:11:48 @agent_ppo2.py:185][0m |          -0.0016 |         171.6879 |           5.0740 |
[32m[20221214 14:11:48 @agent_ppo2.py:185][0m |           0.0107 |         178.6733 |           5.1372 |
[32m[20221214 14:11:48 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:11:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.25
[32m[20221214 14:11:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.40
[32m[20221214 14:11:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.30
[32m[20221214 14:11:48 @agent_ppo2.py:143][0m Total time:      13.76 min
[32m[20221214 14:11:48 @agent_ppo2.py:145][0m 1261568 total steps have happened
[32m[20221214 14:11:48 @agent_ppo2.py:121][0m #------------------------ Iteration 616 --------------------------#
[32m[20221214 14:11:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:48 @agent_ppo2.py:185][0m |           0.0071 |         237.3101 |           5.1561 |
[32m[20221214 14:11:48 @agent_ppo2.py:185][0m |          -0.0029 |         222.7384 |           5.2035 |
[32m[20221214 14:11:49 @agent_ppo2.py:185][0m |           0.0027 |         222.9962 |           5.1926 |
[32m[20221214 14:11:49 @agent_ppo2.py:185][0m |          -0.0021 |         219.9936 |           5.1632 |
[32m[20221214 14:11:49 @agent_ppo2.py:185][0m |          -0.0025 |         219.8640 |           5.1528 |
[32m[20221214 14:11:49 @agent_ppo2.py:185][0m |          -0.0048 |         218.8551 |           5.1174 |
[32m[20221214 14:11:49 @agent_ppo2.py:185][0m |           0.0001 |         219.2765 |           5.1320 |
[32m[20221214 14:11:49 @agent_ppo2.py:185][0m |           0.0081 |         234.4982 |           5.0846 |
[32m[20221214 14:11:49 @agent_ppo2.py:185][0m |          -0.0009 |         218.5404 |           5.1060 |
[32m[20221214 14:11:49 @agent_ppo2.py:185][0m |          -0.0025 |         216.8849 |           5.0831 |
[32m[20221214 14:11:49 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:11:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.45
[32m[20221214 14:11:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.13
[32m[20221214 14:11:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.20
[32m[20221214 14:11:49 @agent_ppo2.py:143][0m Total time:      13.79 min
[32m[20221214 14:11:49 @agent_ppo2.py:145][0m 1263616 total steps have happened
[32m[20221214 14:11:49 @agent_ppo2.py:121][0m #------------------------ Iteration 617 --------------------------#
[32m[20221214 14:11:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:50 @agent_ppo2.py:185][0m |           0.0000 |         241.4575 |           4.8270 |
[32m[20221214 14:11:50 @agent_ppo2.py:185][0m |          -0.0025 |         237.6984 |           4.7783 |
[32m[20221214 14:11:50 @agent_ppo2.py:185][0m |          -0.0034 |         236.0985 |           4.7593 |
[32m[20221214 14:11:50 @agent_ppo2.py:185][0m |          -0.0028 |         235.2285 |           4.7184 |
[32m[20221214 14:11:50 @agent_ppo2.py:185][0m |          -0.0024 |         234.8126 |           4.6818 |
[32m[20221214 14:11:50 @agent_ppo2.py:185][0m |           0.0174 |         260.5995 |           4.7167 |
[32m[20221214 14:11:50 @agent_ppo2.py:185][0m |          -0.0011 |         234.8122 |           4.6812 |
[32m[20221214 14:11:50 @agent_ppo2.py:185][0m |          -0.0023 |         233.7155 |           4.6253 |
[32m[20221214 14:11:50 @agent_ppo2.py:185][0m |           0.0005 |         235.3193 |           4.5939 |
[32m[20221214 14:11:51 @agent_ppo2.py:185][0m |          -0.0032 |         233.3360 |           4.5924 |
[32m[20221214 14:11:51 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:11:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.59
[32m[20221214 14:11:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.35
[32m[20221214 14:11:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.10
[32m[20221214 14:11:51 @agent_ppo2.py:143][0m Total time:      13.81 min
[32m[20221214 14:11:51 @agent_ppo2.py:145][0m 1265664 total steps have happened
[32m[20221214 14:11:51 @agent_ppo2.py:121][0m #------------------------ Iteration 618 --------------------------#
[32m[20221214 14:11:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:51 @agent_ppo2.py:185][0m |          -0.0020 |         235.7444 |           4.7784 |
[32m[20221214 14:11:51 @agent_ppo2.py:185][0m |           0.0115 |         252.6987 |           4.7578 |
[32m[20221214 14:11:51 @agent_ppo2.py:185][0m |          -0.0023 |         230.2568 |           4.7393 |
[32m[20221214 14:11:51 @agent_ppo2.py:185][0m |          -0.0021 |         228.8501 |           4.7020 |
[32m[20221214 14:11:51 @agent_ppo2.py:185][0m |          -0.0020 |         227.9382 |           4.6774 |
[32m[20221214 14:11:51 @agent_ppo2.py:185][0m |          -0.0016 |         227.0153 |           4.6439 |
[32m[20221214 14:11:52 @agent_ppo2.py:185][0m |          -0.0020 |         226.3607 |           4.5972 |
[32m[20221214 14:11:52 @agent_ppo2.py:185][0m |          -0.0021 |         225.9876 |           4.5985 |
[32m[20221214 14:11:52 @agent_ppo2.py:185][0m |           0.0018 |         227.7039 |           4.5502 |
[32m[20221214 14:11:52 @agent_ppo2.py:185][0m |          -0.0043 |         225.2051 |           4.4962 |
[32m[20221214 14:11:52 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:11:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.71
[32m[20221214 14:11:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.80
[32m[20221214 14:11:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.53
[32m[20221214 14:11:52 @agent_ppo2.py:143][0m Total time:      13.83 min
[32m[20221214 14:11:52 @agent_ppo2.py:145][0m 1267712 total steps have happened
[32m[20221214 14:11:52 @agent_ppo2.py:121][0m #------------------------ Iteration 619 --------------------------#
[32m[20221214 14:11:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:52 @agent_ppo2.py:185][0m |          -0.0027 |         236.1780 |           3.8101 |
[32m[20221214 14:11:52 @agent_ppo2.py:185][0m |          -0.0026 |         232.2405 |           3.8560 |
[32m[20221214 14:11:53 @agent_ppo2.py:185][0m |          -0.0046 |         229.3071 |           3.8371 |
[32m[20221214 14:11:53 @agent_ppo2.py:185][0m |          -0.0026 |         227.7761 |           3.8240 |
[32m[20221214 14:11:53 @agent_ppo2.py:185][0m |          -0.0022 |         227.3137 |           3.8581 |
[32m[20221214 14:11:53 @agent_ppo2.py:185][0m |          -0.0052 |         226.2907 |           3.8335 |
[32m[20221214 14:11:53 @agent_ppo2.py:185][0m |          -0.0053 |         226.2317 |           3.8556 |
[32m[20221214 14:11:53 @agent_ppo2.py:185][0m |          -0.0049 |         225.8866 |           3.8376 |
[32m[20221214 14:11:53 @agent_ppo2.py:185][0m |          -0.0026 |         226.2346 |           3.8481 |
[32m[20221214 14:11:53 @agent_ppo2.py:185][0m |           0.0023 |         232.1150 |           3.8358 |
[32m[20221214 14:11:53 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:11:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.38
[32m[20221214 14:11:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.96
[32m[20221214 14:11:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.52
[32m[20221214 14:11:53 @agent_ppo2.py:143][0m Total time:      13.85 min
[32m[20221214 14:11:53 @agent_ppo2.py:145][0m 1269760 total steps have happened
[32m[20221214 14:11:53 @agent_ppo2.py:121][0m #------------------------ Iteration 620 --------------------------#
[32m[20221214 14:11:54 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:11:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:54 @agent_ppo2.py:185][0m |          -0.0029 |         206.0036 |           4.5422 |
[32m[20221214 14:11:54 @agent_ppo2.py:185][0m |           0.0015 |         203.2031 |           4.5190 |
[32m[20221214 14:11:54 @agent_ppo2.py:185][0m |          -0.0007 |         199.8956 |           4.3718 |
[32m[20221214 14:11:54 @agent_ppo2.py:185][0m |          -0.0023 |         198.5022 |           4.4201 |
[32m[20221214 14:11:54 @agent_ppo2.py:185][0m |          -0.0028 |         198.2018 |           4.3469 |
[32m[20221214 14:11:54 @agent_ppo2.py:185][0m |          -0.0029 |         197.8155 |           4.3566 |
[32m[20221214 14:11:54 @agent_ppo2.py:185][0m |           0.0029 |         201.5300 |           4.3416 |
[32m[20221214 14:11:54 @agent_ppo2.py:185][0m |          -0.0019 |         196.7147 |           4.3484 |
[32m[20221214 14:11:54 @agent_ppo2.py:185][0m |          -0.0025 |         196.7045 |           4.2948 |
[32m[20221214 14:11:55 @agent_ppo2.py:185][0m |          -0.0042 |         196.6648 |           4.2724 |
[32m[20221214 14:11:55 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:11:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.13
[32m[20221214 14:11:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.10
[32m[20221214 14:11:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.46
[32m[20221214 14:11:55 @agent_ppo2.py:143][0m Total time:      13.88 min
[32m[20221214 14:11:55 @agent_ppo2.py:145][0m 1271808 total steps have happened
[32m[20221214 14:11:55 @agent_ppo2.py:121][0m #------------------------ Iteration 621 --------------------------#
[32m[20221214 14:11:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:55 @agent_ppo2.py:185][0m |          -0.0037 |         204.3121 |           3.7129 |
[32m[20221214 14:11:55 @agent_ppo2.py:185][0m |          -0.0047 |         197.9511 |           3.7218 |
[32m[20221214 14:11:55 @agent_ppo2.py:185][0m |          -0.0015 |         193.9724 |           3.7375 |
[32m[20221214 14:11:55 @agent_ppo2.py:185][0m |          -0.0012 |         191.4047 |           3.7378 |
[32m[20221214 14:11:55 @agent_ppo2.py:185][0m |          -0.0033 |         190.2881 |           3.7217 |
[32m[20221214 14:11:56 @agent_ppo2.py:185][0m |          -0.0028 |         189.5752 |           3.6860 |
[32m[20221214 14:11:56 @agent_ppo2.py:185][0m |          -0.0049 |         188.7802 |           3.6478 |
[32m[20221214 14:11:56 @agent_ppo2.py:185][0m |          -0.0030 |         188.3997 |           3.6805 |
[32m[20221214 14:11:56 @agent_ppo2.py:185][0m |          -0.0018 |         187.7069 |           3.6231 |
[32m[20221214 14:11:56 @agent_ppo2.py:185][0m |           0.0020 |         190.5348 |           3.6552 |
[32m[20221214 14:11:56 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:11:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.30
[32m[20221214 14:11:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.58
[32m[20221214 14:11:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 860.05
[32m[20221214 14:11:56 @agent_ppo2.py:143][0m Total time:      13.90 min
[32m[20221214 14:11:56 @agent_ppo2.py:145][0m 1273856 total steps have happened
[32m[20221214 14:11:56 @agent_ppo2.py:121][0m #------------------------ Iteration 622 --------------------------#
[32m[20221214 14:11:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:56 @agent_ppo2.py:185][0m |          -0.0022 |         208.7950 |           3.4267 |
[32m[20221214 14:11:57 @agent_ppo2.py:185][0m |          -0.0047 |         204.1610 |           3.4372 |
[32m[20221214 14:11:57 @agent_ppo2.py:185][0m |          -0.0048 |         201.6472 |           3.4419 |
[32m[20221214 14:11:57 @agent_ppo2.py:185][0m |          -0.0048 |         199.7233 |           3.4140 |
[32m[20221214 14:11:57 @agent_ppo2.py:185][0m |          -0.0033 |         198.3367 |           3.3916 |
[32m[20221214 14:11:57 @agent_ppo2.py:185][0m |          -0.0034 |         197.3110 |           3.3677 |
[32m[20221214 14:11:57 @agent_ppo2.py:185][0m |          -0.0038 |         196.3814 |           3.3677 |
[32m[20221214 14:11:57 @agent_ppo2.py:185][0m |          -0.0050 |         196.4757 |           3.3441 |
[32m[20221214 14:11:57 @agent_ppo2.py:185][0m |          -0.0058 |         195.1387 |           3.3457 |
[32m[20221214 14:11:57 @agent_ppo2.py:185][0m |          -0.0004 |         195.5495 |           3.3191 |
[32m[20221214 14:11:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:11:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.52
[32m[20221214 14:11:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.90
[32m[20221214 14:11:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.94
[32m[20221214 14:11:57 @agent_ppo2.py:143][0m Total time:      13.92 min
[32m[20221214 14:11:57 @agent_ppo2.py:145][0m 1275904 total steps have happened
[32m[20221214 14:11:57 @agent_ppo2.py:121][0m #------------------------ Iteration 623 --------------------------#
[32m[20221214 14:11:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:58 @agent_ppo2.py:185][0m |           0.0015 |         205.2278 |           3.2407 |
[32m[20221214 14:11:58 @agent_ppo2.py:185][0m |          -0.0018 |         199.7017 |           3.2840 |
[32m[20221214 14:11:58 @agent_ppo2.py:185][0m |          -0.0022 |         196.2324 |           3.2971 |
[32m[20221214 14:11:58 @agent_ppo2.py:185][0m |          -0.0031 |         193.5524 |           3.3042 |
[32m[20221214 14:11:58 @agent_ppo2.py:185][0m |          -0.0044 |         192.1138 |           3.2875 |
[32m[20221214 14:11:58 @agent_ppo2.py:185][0m |          -0.0038 |         190.9370 |           3.3370 |
[32m[20221214 14:11:58 @agent_ppo2.py:185][0m |          -0.0016 |         189.3336 |           3.3135 |
[32m[20221214 14:11:58 @agent_ppo2.py:185][0m |          -0.0037 |         188.3861 |           3.3185 |
[32m[20221214 14:11:59 @agent_ppo2.py:185][0m |          -0.0025 |         187.5375 |           3.3423 |
[32m[20221214 14:11:59 @agent_ppo2.py:185][0m |           0.0019 |         187.7165 |           3.3378 |
[32m[20221214 14:11:59 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:11:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.55
[32m[20221214 14:11:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.62
[32m[20221214 14:11:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.54
[32m[20221214 14:11:59 @agent_ppo2.py:143][0m Total time:      13.95 min
[32m[20221214 14:11:59 @agent_ppo2.py:145][0m 1277952 total steps have happened
[32m[20221214 14:11:59 @agent_ppo2.py:121][0m #------------------------ Iteration 624 --------------------------#
[32m[20221214 14:11:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:11:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:11:59 @agent_ppo2.py:185][0m |          -0.0004 |         204.4263 |           3.5510 |
[32m[20221214 14:11:59 @agent_ppo2.py:185][0m |          -0.0036 |         200.5573 |           3.5397 |
[32m[20221214 14:11:59 @agent_ppo2.py:185][0m |          -0.0027 |         196.5583 |           3.5145 |
[32m[20221214 14:11:59 @agent_ppo2.py:185][0m |          -0.0014 |         195.7557 |           3.5301 |
[32m[20221214 14:12:00 @agent_ppo2.py:185][0m |          -0.0032 |         195.7621 |           3.5664 |
[32m[20221214 14:12:00 @agent_ppo2.py:185][0m |          -0.0025 |         194.9695 |           3.5433 |
[32m[20221214 14:12:00 @agent_ppo2.py:185][0m |          -0.0023 |         194.9077 |           3.5969 |
[32m[20221214 14:12:00 @agent_ppo2.py:185][0m |          -0.0031 |         194.9632 |           3.5730 |
[32m[20221214 14:12:00 @agent_ppo2.py:185][0m |          -0.0032 |         194.7435 |           3.5802 |
[32m[20221214 14:12:00 @agent_ppo2.py:185][0m |          -0.0009 |         194.0417 |           3.5979 |
[32m[20221214 14:12:00 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:12:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.96
[32m[20221214 14:12:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.21
[32m[20221214 14:12:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.21
[32m[20221214 14:12:00 @agent_ppo2.py:143][0m Total time:      13.97 min
[32m[20221214 14:12:00 @agent_ppo2.py:145][0m 1280000 total steps have happened
[32m[20221214 14:12:00 @agent_ppo2.py:121][0m #------------------------ Iteration 625 --------------------------#
[32m[20221214 14:12:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:00 @agent_ppo2.py:185][0m |          -0.0043 |         223.8532 |           3.5565 |
[32m[20221214 14:12:01 @agent_ppo2.py:185][0m |          -0.0039 |         211.8726 |           3.6358 |
[32m[20221214 14:12:01 @agent_ppo2.py:185][0m |          -0.0030 |         207.3902 |           3.5428 |
[32m[20221214 14:12:01 @agent_ppo2.py:185][0m |           0.0008 |         205.3810 |           3.5920 |
[32m[20221214 14:12:01 @agent_ppo2.py:185][0m |           0.0102 |         210.6733 |           3.5439 |
[32m[20221214 14:12:01 @agent_ppo2.py:185][0m |           0.0014 |         205.5425 |           3.5330 |
[32m[20221214 14:12:01 @agent_ppo2.py:185][0m |          -0.0037 |         198.8138 |           3.5916 |
[32m[20221214 14:12:01 @agent_ppo2.py:185][0m |          -0.0023 |         198.6872 |           3.5538 |
[32m[20221214 14:12:01 @agent_ppo2.py:185][0m |          -0.0033 |         197.8216 |           3.4820 |
[32m[20221214 14:12:01 @agent_ppo2.py:185][0m |          -0.0040 |         197.2457 |           3.5544 |
[32m[20221214 14:12:01 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:12:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.36
[32m[20221214 14:12:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.21
[32m[20221214 14:12:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.34
[32m[20221214 14:12:01 @agent_ppo2.py:143][0m Total time:      13.99 min
[32m[20221214 14:12:01 @agent_ppo2.py:145][0m 1282048 total steps have happened
[32m[20221214 14:12:01 @agent_ppo2.py:121][0m #------------------------ Iteration 626 --------------------------#
[32m[20221214 14:12:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:02 @agent_ppo2.py:185][0m |          -0.0002 |         248.6018 |           3.3877 |
[32m[20221214 14:12:02 @agent_ppo2.py:185][0m |          -0.0027 |         245.4925 |           3.4093 |
[32m[20221214 14:12:02 @agent_ppo2.py:185][0m |          -0.0029 |         242.4337 |           3.3568 |
[32m[20221214 14:12:02 @agent_ppo2.py:185][0m |          -0.0031 |         240.8259 |           3.3546 |
[32m[20221214 14:12:02 @agent_ppo2.py:185][0m |          -0.0028 |         239.6758 |           3.3341 |
[32m[20221214 14:12:02 @agent_ppo2.py:185][0m |          -0.0030 |         238.6520 |           3.3143 |
[32m[20221214 14:12:02 @agent_ppo2.py:185][0m |          -0.0035 |         237.8789 |           3.2734 |
[32m[20221214 14:12:02 @agent_ppo2.py:185][0m |          -0.0028 |         237.1291 |           3.3067 |
[32m[20221214 14:12:03 @agent_ppo2.py:185][0m |          -0.0041 |         236.7548 |           3.2678 |
[32m[20221214 14:12:03 @agent_ppo2.py:185][0m |          -0.0025 |         236.7925 |           3.2531 |
[32m[20221214 14:12:03 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:12:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.14
[32m[20221214 14:12:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.67
[32m[20221214 14:12:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.52
[32m[20221214 14:12:03 @agent_ppo2.py:143][0m Total time:      14.01 min
[32m[20221214 14:12:03 @agent_ppo2.py:145][0m 1284096 total steps have happened
[32m[20221214 14:12:03 @agent_ppo2.py:121][0m #------------------------ Iteration 627 --------------------------#
[32m[20221214 14:12:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:03 @agent_ppo2.py:185][0m |           0.0046 |         242.8695 |           3.0226 |
[32m[20221214 14:12:03 @agent_ppo2.py:185][0m |          -0.0030 |         232.2401 |           3.0380 |
[32m[20221214 14:12:03 @agent_ppo2.py:185][0m |          -0.0026 |         229.6714 |           3.0126 |
[32m[20221214 14:12:03 @agent_ppo2.py:185][0m |          -0.0031 |         228.2250 |           2.9646 |
[32m[20221214 14:12:04 @agent_ppo2.py:185][0m |          -0.0013 |         226.3729 |           2.9881 |
[32m[20221214 14:12:04 @agent_ppo2.py:185][0m |          -0.0028 |         225.8035 |           3.0094 |
[32m[20221214 14:12:04 @agent_ppo2.py:185][0m |          -0.0034 |         224.9345 |           2.9823 |
[32m[20221214 14:12:04 @agent_ppo2.py:185][0m |          -0.0013 |         224.6144 |           2.9778 |
[32m[20221214 14:12:04 @agent_ppo2.py:185][0m |          -0.0030 |         223.6776 |           2.9362 |
[32m[20221214 14:12:04 @agent_ppo2.py:185][0m |           0.0077 |         247.0404 |           2.9789 |
[32m[20221214 14:12:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:12:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.48
[32m[20221214 14:12:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.47
[32m[20221214 14:12:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.78
[32m[20221214 14:12:04 @agent_ppo2.py:143][0m Total time:      14.04 min
[32m[20221214 14:12:04 @agent_ppo2.py:145][0m 1286144 total steps have happened
[32m[20221214 14:12:04 @agent_ppo2.py:121][0m #------------------------ Iteration 628 --------------------------#
[32m[20221214 14:12:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:05 @agent_ppo2.py:185][0m |          -0.0007 |         250.3774 |           2.7510 |
[32m[20221214 14:12:05 @agent_ppo2.py:185][0m |          -0.0020 |         247.5999 |           2.7212 |
[32m[20221214 14:12:05 @agent_ppo2.py:185][0m |          -0.0020 |         244.5171 |           2.7282 |
[32m[20221214 14:12:05 @agent_ppo2.py:185][0m |           0.0002 |         243.6138 |           2.7540 |
[32m[20221214 14:12:05 @agent_ppo2.py:185][0m |          -0.0008 |         243.0360 |           2.7744 |
[32m[20221214 14:12:05 @agent_ppo2.py:185][0m |          -0.0015 |         241.8736 |           2.7945 |
[32m[20221214 14:12:05 @agent_ppo2.py:185][0m |           0.0003 |         242.1382 |           2.8361 |
[32m[20221214 14:12:05 @agent_ppo2.py:185][0m |           0.0132 |         279.1166 |           2.8159 |
[32m[20221214 14:12:05 @agent_ppo2.py:185][0m |          -0.0005 |         241.7406 |           2.8396 |
[32m[20221214 14:12:05 @agent_ppo2.py:185][0m |          -0.0003 |         240.5831 |           2.8559 |
[32m[20221214 14:12:05 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:12:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.37
[32m[20221214 14:12:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.60
[32m[20221214 14:12:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.35
[32m[20221214 14:12:06 @agent_ppo2.py:143][0m Total time:      14.06 min
[32m[20221214 14:12:06 @agent_ppo2.py:145][0m 1288192 total steps have happened
[32m[20221214 14:12:06 @agent_ppo2.py:121][0m #------------------------ Iteration 629 --------------------------#
[32m[20221214 14:12:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:06 @agent_ppo2.py:185][0m |          -0.0039 |         242.6857 |           3.2151 |
[32m[20221214 14:12:06 @agent_ppo2.py:185][0m |           0.0039 |         256.8158 |           3.2099 |
[32m[20221214 14:12:06 @agent_ppo2.py:185][0m |          -0.0040 |         238.8426 |           3.2140 |
[32m[20221214 14:12:06 @agent_ppo2.py:185][0m |          -0.0030 |         238.3375 |           3.1641 |
[32m[20221214 14:12:06 @agent_ppo2.py:185][0m |           0.0175 |         274.1467 |           3.1413 |
[32m[20221214 14:12:06 @agent_ppo2.py:185][0m |          -0.0027 |         237.4609 |           3.1966 |
[32m[20221214 14:12:06 @agent_ppo2.py:185][0m |          -0.0034 |         236.7138 |           3.1334 |
[32m[20221214 14:12:07 @agent_ppo2.py:185][0m |          -0.0030 |         236.2846 |           3.1421 |
[32m[20221214 14:12:07 @agent_ppo2.py:185][0m |          -0.0046 |         236.3218 |           3.1039 |
[32m[20221214 14:12:07 @agent_ppo2.py:185][0m |          -0.0037 |         235.6199 |           3.0950 |
[32m[20221214 14:12:07 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:12:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.98
[32m[20221214 14:12:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.15
[32m[20221214 14:12:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.89
[32m[20221214 14:12:07 @agent_ppo2.py:143][0m Total time:      14.08 min
[32m[20221214 14:12:07 @agent_ppo2.py:145][0m 1290240 total steps have happened
[32m[20221214 14:12:07 @agent_ppo2.py:121][0m #------------------------ Iteration 630 --------------------------#
[32m[20221214 14:12:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:12:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:07 @agent_ppo2.py:185][0m |           0.0086 |         216.6336 |           2.7899 |
[32m[20221214 14:12:07 @agent_ppo2.py:185][0m |          -0.0016 |         201.8594 |           2.8189 |
[32m[20221214 14:12:07 @agent_ppo2.py:185][0m |           0.0010 |         197.4792 |           2.7217 |
[32m[20221214 14:12:08 @agent_ppo2.py:185][0m |          -0.0008 |         195.5155 |           2.6923 |
[32m[20221214 14:12:08 @agent_ppo2.py:185][0m |          -0.0022 |         195.8712 |           2.6799 |
[32m[20221214 14:12:08 @agent_ppo2.py:185][0m |          -0.0016 |         194.3585 |           2.6818 |
[32m[20221214 14:12:08 @agent_ppo2.py:185][0m |          -0.0026 |         193.5349 |           2.6595 |
[32m[20221214 14:12:08 @agent_ppo2.py:185][0m |          -0.0017 |         192.3151 |           2.6969 |
[32m[20221214 14:12:08 @agent_ppo2.py:185][0m |          -0.0017 |         192.0942 |           2.7152 |
[32m[20221214 14:12:08 @agent_ppo2.py:185][0m |          -0.0013 |         192.6266 |           2.7060 |
[32m[20221214 14:12:08 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:12:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.46
[32m[20221214 14:12:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.49
[32m[20221214 14:12:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.44
[32m[20221214 14:12:08 @agent_ppo2.py:143][0m Total time:      14.10 min
[32m[20221214 14:12:08 @agent_ppo2.py:145][0m 1292288 total steps have happened
[32m[20221214 14:12:08 @agent_ppo2.py:121][0m #------------------------ Iteration 631 --------------------------#
[32m[20221214 14:12:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:09 @agent_ppo2.py:185][0m |           0.0064 |         191.9042 |           2.9642 |
[32m[20221214 14:12:09 @agent_ppo2.py:185][0m |          -0.0030 |         176.9507 |           3.0223 |
[32m[20221214 14:12:09 @agent_ppo2.py:185][0m |           0.0044 |         181.9320 |           2.9834 |
[32m[20221214 14:12:09 @agent_ppo2.py:185][0m |          -0.0038 |         173.0948 |           2.9974 |
[32m[20221214 14:12:09 @agent_ppo2.py:185][0m |          -0.0049 |         171.6746 |           2.9822 |
[32m[20221214 14:12:09 @agent_ppo2.py:185][0m |          -0.0021 |         170.2347 |           2.9856 |
[32m[20221214 14:12:09 @agent_ppo2.py:185][0m |          -0.0017 |         169.3529 |           2.9526 |
[32m[20221214 14:12:09 @agent_ppo2.py:185][0m |           0.0034 |         175.7923 |           2.9510 |
[32m[20221214 14:12:09 @agent_ppo2.py:185][0m |          -0.0040 |         168.1500 |           2.9793 |
[32m[20221214 14:12:09 @agent_ppo2.py:185][0m |          -0.0026 |         167.6713 |           2.9584 |
[32m[20221214 14:12:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:12:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.78
[32m[20221214 14:12:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.74
[32m[20221214 14:12:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.00
[32m[20221214 14:12:10 @agent_ppo2.py:143][0m Total time:      14.13 min
[32m[20221214 14:12:10 @agent_ppo2.py:145][0m 1294336 total steps have happened
[32m[20221214 14:12:10 @agent_ppo2.py:121][0m #------------------------ Iteration 632 --------------------------#
[32m[20221214 14:12:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:10 @agent_ppo2.py:185][0m |           0.0009 |         251.4112 |           2.9708 |
[32m[20221214 14:12:10 @agent_ppo2.py:185][0m |          -0.0028 |         238.4931 |           2.9617 |
[32m[20221214 14:12:10 @agent_ppo2.py:185][0m |          -0.0040 |         236.0979 |           2.9006 |
[32m[20221214 14:12:10 @agent_ppo2.py:185][0m |          -0.0024 |         234.5064 |           2.8554 |
[32m[20221214 14:12:10 @agent_ppo2.py:185][0m |          -0.0045 |         231.2741 |           2.8549 |
[32m[20221214 14:12:10 @agent_ppo2.py:185][0m |          -0.0039 |         229.3268 |           2.8211 |
[32m[20221214 14:12:10 @agent_ppo2.py:185][0m |          -0.0041 |         228.1851 |           2.7794 |
[32m[20221214 14:12:11 @agent_ppo2.py:185][0m |          -0.0038 |         227.4842 |           2.7628 |
[32m[20221214 14:12:11 @agent_ppo2.py:185][0m |          -0.0040 |         226.2440 |           2.7188 |
[32m[20221214 14:12:11 @agent_ppo2.py:185][0m |           0.0081 |         237.3411 |           2.7152 |
[32m[20221214 14:12:11 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:12:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.73
[32m[20221214 14:12:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.41
[32m[20221214 14:12:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.72
[32m[20221214 14:12:11 @agent_ppo2.py:143][0m Total time:      14.15 min
[32m[20221214 14:12:11 @agent_ppo2.py:145][0m 1296384 total steps have happened
[32m[20221214 14:12:11 @agent_ppo2.py:121][0m #------------------------ Iteration 633 --------------------------#
[32m[20221214 14:12:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:11 @agent_ppo2.py:185][0m |          -0.0014 |         279.4963 |           2.8535 |
[32m[20221214 14:12:11 @agent_ppo2.py:185][0m |          -0.0015 |         272.7768 |           2.7317 |
[32m[20221214 14:12:11 @agent_ppo2.py:185][0m |          -0.0041 |         267.5124 |           2.7246 |
[32m[20221214 14:12:12 @agent_ppo2.py:185][0m |          -0.0042 |         265.3504 |           2.6832 |
[32m[20221214 14:12:12 @agent_ppo2.py:185][0m |          -0.0066 |         264.7455 |           2.6971 |
[32m[20221214 14:12:12 @agent_ppo2.py:185][0m |          -0.0065 |         263.6150 |           2.6683 |
[32m[20221214 14:12:12 @agent_ppo2.py:185][0m |          -0.0031 |         263.2111 |           2.6532 |
[32m[20221214 14:12:12 @agent_ppo2.py:185][0m |          -0.0048 |         262.6306 |           2.6473 |
[32m[20221214 14:12:12 @agent_ppo2.py:185][0m |          -0.0057 |         261.5001 |           2.6234 |
[32m[20221214 14:12:12 @agent_ppo2.py:185][0m |          -0.0054 |         262.2547 |           2.6043 |
[32m[20221214 14:12:12 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:12:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.67
[32m[20221214 14:12:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.64
[32m[20221214 14:12:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.66
[32m[20221214 14:12:12 @agent_ppo2.py:143][0m Total time:      14.17 min
[32m[20221214 14:12:12 @agent_ppo2.py:145][0m 1298432 total steps have happened
[32m[20221214 14:12:12 @agent_ppo2.py:121][0m #------------------------ Iteration 634 --------------------------#
[32m[20221214 14:12:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:13 @agent_ppo2.py:185][0m |           0.0015 |         287.2381 |           2.3807 |
[32m[20221214 14:12:13 @agent_ppo2.py:185][0m |           0.0047 |         283.9651 |           2.3441 |
[32m[20221214 14:12:13 @agent_ppo2.py:185][0m |          -0.0006 |         274.4336 |           2.3645 |
[32m[20221214 14:12:13 @agent_ppo2.py:185][0m |          -0.0009 |         272.1968 |           2.3349 |
[32m[20221214 14:12:13 @agent_ppo2.py:185][0m |          -0.0011 |         270.7009 |           2.3288 |
[32m[20221214 14:12:13 @agent_ppo2.py:185][0m |          -0.0013 |         269.9758 |           2.3398 |
[32m[20221214 14:12:13 @agent_ppo2.py:185][0m |          -0.0012 |         269.3689 |           2.2851 |
[32m[20221214 14:12:13 @agent_ppo2.py:185][0m |          -0.0029 |         269.4121 |           2.2836 |
[32m[20221214 14:12:13 @agent_ppo2.py:185][0m |          -0.0021 |         268.4095 |           2.2921 |
[32m[20221214 14:12:14 @agent_ppo2.py:185][0m |          -0.0021 |         268.3941 |           2.2753 |
[32m[20221214 14:12:14 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:12:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.42
[32m[20221214 14:12:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.81
[32m[20221214 14:12:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.25
[32m[20221214 14:12:14 @agent_ppo2.py:143][0m Total time:      14.19 min
[32m[20221214 14:12:14 @agent_ppo2.py:145][0m 1300480 total steps have happened
[32m[20221214 14:12:14 @agent_ppo2.py:121][0m #------------------------ Iteration 635 --------------------------#
[32m[20221214 14:12:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:12:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:14 @agent_ppo2.py:185][0m |          -0.0025 |         269.9599 |           2.2090 |
[32m[20221214 14:12:14 @agent_ppo2.py:185][0m |           0.0083 |         280.8534 |           2.1332 |
[32m[20221214 14:12:14 @agent_ppo2.py:185][0m |           0.0084 |         278.4326 |           2.1724 |
[32m[20221214 14:12:14 @agent_ppo2.py:185][0m |          -0.0010 |         265.4478 |           2.0876 |
[32m[20221214 14:12:14 @agent_ppo2.py:185][0m |          -0.0023 |         265.0171 |           2.0760 |
[32m[20221214 14:12:14 @agent_ppo2.py:185][0m |           0.0002 |         264.8529 |           2.1220 |
[32m[20221214 14:12:15 @agent_ppo2.py:185][0m |          -0.0019 |         264.5920 |           2.0410 |
[32m[20221214 14:12:15 @agent_ppo2.py:185][0m |          -0.0003 |         264.0399 |           2.0469 |
[32m[20221214 14:12:15 @agent_ppo2.py:185][0m |          -0.0021 |         263.4250 |           2.0861 |
[32m[20221214 14:12:15 @agent_ppo2.py:185][0m |           0.0014 |         264.7309 |           2.0257 |
[32m[20221214 14:12:15 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:12:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.39
[32m[20221214 14:12:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.53
[32m[20221214 14:12:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.68
[32m[20221214 14:12:15 @agent_ppo2.py:143][0m Total time:      14.22 min
[32m[20221214 14:12:15 @agent_ppo2.py:145][0m 1302528 total steps have happened
[32m[20221214 14:12:15 @agent_ppo2.py:121][0m #------------------------ Iteration 636 --------------------------#
[32m[20221214 14:12:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:12:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:15 @agent_ppo2.py:185][0m |           0.0233 |         291.4300 |           2.1527 |
[32m[20221214 14:12:15 @agent_ppo2.py:185][0m |          -0.0005 |         244.3652 |           2.1093 |
[32m[20221214 14:12:16 @agent_ppo2.py:185][0m |          -0.0010 |         239.8603 |           2.1006 |
[32m[20221214 14:12:16 @agent_ppo2.py:185][0m |          -0.0021 |         238.7655 |           2.0557 |
[32m[20221214 14:12:16 @agent_ppo2.py:185][0m |           0.0004 |         238.1504 |           2.0500 |
[32m[20221214 14:12:16 @agent_ppo2.py:185][0m |          -0.0024 |         236.7510 |           2.0442 |
[32m[20221214 14:12:16 @agent_ppo2.py:185][0m |          -0.0020 |         234.8985 |           2.0168 |
[32m[20221214 14:12:16 @agent_ppo2.py:185][0m |          -0.0025 |         233.5583 |           2.0118 |
[32m[20221214 14:12:16 @agent_ppo2.py:185][0m |          -0.0024 |         232.7731 |           1.9504 |
[32m[20221214 14:12:16 @agent_ppo2.py:185][0m |          -0.0017 |         231.3662 |           1.9642 |
[32m[20221214 14:12:16 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:12:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.00
[32m[20221214 14:12:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.48
[32m[20221214 14:12:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.49
[32m[20221214 14:12:16 @agent_ppo2.py:143][0m Total time:      14.24 min
[32m[20221214 14:12:16 @agent_ppo2.py:145][0m 1304576 total steps have happened
[32m[20221214 14:12:16 @agent_ppo2.py:121][0m #------------------------ Iteration 637 --------------------------#
[32m[20221214 14:12:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:12:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:17 @agent_ppo2.py:185][0m |           0.0071 |         262.6910 |           1.3664 |
[32m[20221214 14:12:17 @agent_ppo2.py:185][0m |          -0.0038 |         242.6109 |           1.4058 |
[32m[20221214 14:12:17 @agent_ppo2.py:185][0m |          -0.0046 |         240.6463 |           1.3516 |
[32m[20221214 14:12:17 @agent_ppo2.py:185][0m |          -0.0035 |         240.8180 |           1.3443 |
[32m[20221214 14:12:17 @agent_ppo2.py:185][0m |          -0.0046 |         240.2736 |           1.3272 |
[32m[20221214 14:12:17 @agent_ppo2.py:185][0m |          -0.0026 |         240.0142 |           1.2935 |
[32m[20221214 14:12:17 @agent_ppo2.py:185][0m |          -0.0048 |         239.7984 |           1.3229 |
[32m[20221214 14:12:17 @agent_ppo2.py:185][0m |           0.0053 |         253.0696 |           1.2499 |
[32m[20221214 14:12:17 @agent_ppo2.py:185][0m |          -0.0039 |         238.4796 |           1.2599 |
[32m[20221214 14:12:17 @agent_ppo2.py:185][0m |           0.0082 |         260.8908 |           1.2504 |
[32m[20221214 14:12:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:12:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.43
[32m[20221214 14:12:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.28
[32m[20221214 14:12:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.60
[32m[20221214 14:12:18 @agent_ppo2.py:143][0m Total time:      14.26 min
[32m[20221214 14:12:18 @agent_ppo2.py:145][0m 1306624 total steps have happened
[32m[20221214 14:12:18 @agent_ppo2.py:121][0m #------------------------ Iteration 638 --------------------------#
[32m[20221214 14:12:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:12:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:18 @agent_ppo2.py:185][0m |           0.0032 |         238.5997 |           1.3589 |
[32m[20221214 14:12:18 @agent_ppo2.py:185][0m |           0.0027 |         232.0540 |           1.3437 |
[32m[20221214 14:12:18 @agent_ppo2.py:185][0m |           0.0127 |         252.1990 |           1.3596 |
[32m[20221214 14:12:18 @agent_ppo2.py:185][0m |          -0.0025 |         225.8227 |           1.3586 |
[32m[20221214 14:12:18 @agent_ppo2.py:185][0m |          -0.0023 |         225.4210 |           1.3222 |
[32m[20221214 14:12:18 @agent_ppo2.py:185][0m |          -0.0029 |         225.0831 |           1.3512 |
[32m[20221214 14:12:18 @agent_ppo2.py:185][0m |          -0.0024 |         223.9798 |           1.3336 |
[32m[20221214 14:12:19 @agent_ppo2.py:185][0m |          -0.0005 |         223.8209 |           1.3498 |
[32m[20221214 14:12:19 @agent_ppo2.py:185][0m |           0.0051 |         242.4895 |           1.3250 |
[32m[20221214 14:12:19 @agent_ppo2.py:185][0m |          -0.0023 |         222.6512 |           1.3183 |
[32m[20221214 14:12:19 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:12:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.52
[32m[20221214 14:12:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.83
[32m[20221214 14:12:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.88
[32m[20221214 14:12:19 @agent_ppo2.py:143][0m Total time:      14.28 min
[32m[20221214 14:12:19 @agent_ppo2.py:145][0m 1308672 total steps have happened
[32m[20221214 14:12:19 @agent_ppo2.py:121][0m #------------------------ Iteration 639 --------------------------#
[32m[20221214 14:12:19 @agent_ppo2.py:127][0m Sampling time: 0.27 s by 5 slaves
[32m[20221214 14:12:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:19 @agent_ppo2.py:185][0m |          -0.0030 |         200.4073 |           1.3101 |
[32m[20221214 14:12:20 @agent_ppo2.py:185][0m |          -0.0027 |         196.8387 |           1.2782 |
[32m[20221214 14:12:20 @agent_ppo2.py:185][0m |          -0.0051 |         194.9995 |           1.2918 |
[32m[20221214 14:12:20 @agent_ppo2.py:185][0m |           0.0047 |         203.7919 |           1.2655 |
[32m[20221214 14:12:20 @agent_ppo2.py:185][0m |          -0.0039 |         193.8295 |           1.2643 |
[32m[20221214 14:12:20 @agent_ppo2.py:185][0m |          -0.0033 |         192.7259 |           1.2604 |
[32m[20221214 14:12:20 @agent_ppo2.py:185][0m |          -0.0041 |         192.5333 |           1.2342 |
[32m[20221214 14:12:20 @agent_ppo2.py:185][0m |          -0.0033 |         191.9672 |           1.2340 |
[32m[20221214 14:12:20 @agent_ppo2.py:185][0m |          -0.0046 |         191.3045 |           1.2331 |
[32m[20221214 14:12:20 @agent_ppo2.py:185][0m |          -0.0054 |         190.9723 |           1.2155 |
[32m[20221214 14:12:20 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:12:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.89
[32m[20221214 14:12:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.63
[32m[20221214 14:12:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.83
[32m[20221214 14:12:20 @agent_ppo2.py:143][0m Total time:      14.31 min
[32m[20221214 14:12:20 @agent_ppo2.py:145][0m 1310720 total steps have happened
[32m[20221214 14:12:20 @agent_ppo2.py:121][0m #------------------------ Iteration 640 --------------------------#
[32m[20221214 14:12:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:12:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:21 @agent_ppo2.py:185][0m |          -0.0019 |         249.4400 |           1.1006 |
[32m[20221214 14:12:21 @agent_ppo2.py:185][0m |          -0.0008 |         247.5417 |           1.0111 |
[32m[20221214 14:12:21 @agent_ppo2.py:185][0m |          -0.0014 |         246.9708 |           0.9568 |
[32m[20221214 14:12:21 @agent_ppo2.py:185][0m |          -0.0005 |         246.1338 |           0.9229 |
[32m[20221214 14:12:21 @agent_ppo2.py:185][0m |          -0.0016 |         245.9687 |           0.8617 |
[32m[20221214 14:12:21 @agent_ppo2.py:185][0m |          -0.0016 |         245.1482 |           0.8188 |
[32m[20221214 14:12:21 @agent_ppo2.py:185][0m |          -0.0025 |         245.2012 |           0.7799 |
[32m[20221214 14:12:21 @agent_ppo2.py:185][0m |          -0.0015 |         244.4902 |           0.7582 |
[32m[20221214 14:12:22 @agent_ppo2.py:185][0m |           0.0036 |         247.0327 |           0.7143 |
[32m[20221214 14:12:22 @agent_ppo2.py:185][0m |           0.0013 |         244.1930 |           0.6722 |
[32m[20221214 14:12:22 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:12:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.49
[32m[20221214 14:12:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.97
[32m[20221214 14:12:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.84
[32m[20221214 14:12:22 @agent_ppo2.py:143][0m Total time:      14.33 min
[32m[20221214 14:12:22 @agent_ppo2.py:145][0m 1312768 total steps have happened
[32m[20221214 14:12:22 @agent_ppo2.py:121][0m #------------------------ Iteration 641 --------------------------#
[32m[20221214 14:12:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:12:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:22 @agent_ppo2.py:185][0m |          -0.0007 |         230.6667 |           0.6756 |
[32m[20221214 14:12:22 @agent_ppo2.py:185][0m |           0.0019 |         225.6538 |           0.7050 |
[32m[20221214 14:12:22 @agent_ppo2.py:185][0m |          -0.0027 |         221.1869 |           0.7211 |
[32m[20221214 14:12:22 @agent_ppo2.py:185][0m |          -0.0024 |         220.4394 |           0.7359 |
[32m[20221214 14:12:23 @agent_ppo2.py:185][0m |          -0.0040 |         219.0360 |           0.7231 |
[32m[20221214 14:12:23 @agent_ppo2.py:185][0m |           0.0028 |         223.0209 |           0.7528 |
[32m[20221214 14:12:23 @agent_ppo2.py:185][0m |           0.0084 |         234.1267 |           0.7549 |
[32m[20221214 14:12:23 @agent_ppo2.py:185][0m |          -0.0026 |         216.6361 |           0.7475 |
[32m[20221214 14:12:23 @agent_ppo2.py:185][0m |          -0.0031 |         216.4862 |           0.7905 |
[32m[20221214 14:12:23 @agent_ppo2.py:185][0m |          -0.0030 |         215.7868 |           0.7907 |
[32m[20221214 14:12:23 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:12:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.56
[32m[20221214 14:12:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.60
[32m[20221214 14:12:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.58
[32m[20221214 14:12:23 @agent_ppo2.py:143][0m Total time:      14.35 min
[32m[20221214 14:12:23 @agent_ppo2.py:145][0m 1314816 total steps have happened
[32m[20221214 14:12:23 @agent_ppo2.py:121][0m #------------------------ Iteration 642 --------------------------#
[32m[20221214 14:12:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:23 @agent_ppo2.py:185][0m |          -0.0022 |         228.0036 |           0.7847 |
[32m[20221214 14:12:24 @agent_ppo2.py:185][0m |          -0.0029 |         223.0026 |           0.8467 |
[32m[20221214 14:12:24 @agent_ppo2.py:185][0m |          -0.0010 |         219.9918 |           0.8130 |
[32m[20221214 14:12:24 @agent_ppo2.py:185][0m |          -0.0033 |         219.5766 |           0.7987 |
[32m[20221214 14:12:24 @agent_ppo2.py:185][0m |          -0.0023 |         218.1144 |           0.7511 |
[32m[20221214 14:12:24 @agent_ppo2.py:185][0m |          -0.0027 |         217.5427 |           0.8063 |
[32m[20221214 14:12:24 @agent_ppo2.py:185][0m |          -0.0006 |         217.3824 |           0.7670 |
[32m[20221214 14:12:24 @agent_ppo2.py:185][0m |          -0.0026 |         217.0836 |           0.7375 |
[32m[20221214 14:12:24 @agent_ppo2.py:185][0m |           0.0099 |         248.5220 |           0.7386 |
[32m[20221214 14:12:24 @agent_ppo2.py:185][0m |          -0.0012 |         217.1727 |           0.7574 |
[32m[20221214 14:12:24 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:12:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.26
[32m[20221214 14:12:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.43
[32m[20221214 14:12:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.26
[32m[20221214 14:12:24 @agent_ppo2.py:143][0m Total time:      14.37 min
[32m[20221214 14:12:24 @agent_ppo2.py:145][0m 1316864 total steps have happened
[32m[20221214 14:12:24 @agent_ppo2.py:121][0m #------------------------ Iteration 643 --------------------------#
[32m[20221214 14:12:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:25 @agent_ppo2.py:185][0m |          -0.0018 |         197.5783 |           0.4180 |
[32m[20221214 14:12:25 @agent_ppo2.py:185][0m |          -0.0042 |         193.0947 |           0.4600 |
[32m[20221214 14:12:25 @agent_ppo2.py:185][0m |          -0.0042 |         192.0600 |           0.4142 |
[32m[20221214 14:12:25 @agent_ppo2.py:185][0m |          -0.0048 |         191.4465 |           0.4409 |
[32m[20221214 14:12:25 @agent_ppo2.py:185][0m |          -0.0030 |         190.6215 |           0.3861 |
[32m[20221214 14:12:25 @agent_ppo2.py:185][0m |          -0.0065 |         190.9091 |           0.4083 |
[32m[20221214 14:12:25 @agent_ppo2.py:185][0m |          -0.0017 |         191.2017 |           0.3525 |
[32m[20221214 14:12:25 @agent_ppo2.py:185][0m |          -0.0056 |         189.6390 |           0.3428 |
[32m[20221214 14:12:25 @agent_ppo2.py:185][0m |          -0.0052 |         189.2716 |           0.3294 |
[32m[20221214 14:12:26 @agent_ppo2.py:185][0m |          -0.0046 |         189.0020 |           0.2966 |
[32m[20221214 14:12:26 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:12:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.75
[32m[20221214 14:12:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.40
[32m[20221214 14:12:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.66
[32m[20221214 14:12:26 @agent_ppo2.py:143][0m Total time:      14.39 min
[32m[20221214 14:12:26 @agent_ppo2.py:145][0m 1318912 total steps have happened
[32m[20221214 14:12:26 @agent_ppo2.py:121][0m #------------------------ Iteration 644 --------------------------#
[32m[20221214 14:12:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:12:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:26 @agent_ppo2.py:185][0m |           0.0037 |         231.0479 |           0.3555 |
[32m[20221214 14:12:26 @agent_ppo2.py:185][0m |          -0.0052 |         222.0135 |           0.3691 |
[32m[20221214 14:12:26 @agent_ppo2.py:185][0m |           0.0070 |         237.1647 |           0.4021 |
[32m[20221214 14:12:26 @agent_ppo2.py:185][0m |          -0.0057 |         219.4442 |           0.3615 |
[32m[20221214 14:12:26 @agent_ppo2.py:185][0m |          -0.0034 |         218.5495 |           0.3949 |
[32m[20221214 14:12:26 @agent_ppo2.py:185][0m |          -0.0050 |         218.1542 |           0.3433 |
[32m[20221214 14:12:27 @agent_ppo2.py:185][0m |          -0.0041 |         218.4508 |           0.3021 |
[32m[20221214 14:12:27 @agent_ppo2.py:185][0m |          -0.0034 |         217.7457 |           0.2883 |
[32m[20221214 14:12:27 @agent_ppo2.py:185][0m |          -0.0053 |         217.4905 |           0.2596 |
[32m[20221214 14:12:27 @agent_ppo2.py:185][0m |          -0.0051 |         216.9288 |           0.2769 |
[32m[20221214 14:12:27 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:12:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.97
[32m[20221214 14:12:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.68
[32m[20221214 14:12:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.10
[32m[20221214 14:12:27 @agent_ppo2.py:143][0m Total time:      14.41 min
[32m[20221214 14:12:27 @agent_ppo2.py:145][0m 1320960 total steps have happened
[32m[20221214 14:12:27 @agent_ppo2.py:121][0m #------------------------ Iteration 645 --------------------------#
[32m[20221214 14:12:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:27 @agent_ppo2.py:185][0m |          -0.0016 |         236.6890 |           0.2796 |
[32m[20221214 14:12:27 @agent_ppo2.py:185][0m |          -0.0023 |         233.2100 |           0.2835 |
[32m[20221214 14:12:27 @agent_ppo2.py:185][0m |          -0.0024 |         231.3205 |           0.2672 |
[32m[20221214 14:12:28 @agent_ppo2.py:185][0m |          -0.0032 |         229.9714 |           0.2219 |
[32m[20221214 14:12:28 @agent_ppo2.py:185][0m |          -0.0028 |         229.3765 |           0.2316 |
[32m[20221214 14:12:28 @agent_ppo2.py:185][0m |          -0.0040 |         228.7343 |           0.1952 |
[32m[20221214 14:12:28 @agent_ppo2.py:185][0m |          -0.0045 |         228.2190 |           0.2011 |
[32m[20221214 14:12:28 @agent_ppo2.py:185][0m |          -0.0031 |         226.7933 |           0.1868 |
[32m[20221214 14:12:28 @agent_ppo2.py:185][0m |          -0.0037 |         227.0622 |           0.1663 |
[32m[20221214 14:12:28 @agent_ppo2.py:185][0m |           0.0075 |         246.3572 |           0.1292 |
[32m[20221214 14:12:28 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:12:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.92
[32m[20221214 14:12:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.96
[32m[20221214 14:12:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.14
[32m[20221214 14:12:28 @agent_ppo2.py:143][0m Total time:      14.44 min
[32m[20221214 14:12:28 @agent_ppo2.py:145][0m 1323008 total steps have happened
[32m[20221214 14:12:28 @agent_ppo2.py:121][0m #------------------------ Iteration 646 --------------------------#
[32m[20221214 14:12:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:29 @agent_ppo2.py:185][0m |          -0.0020 |         216.7092 |          -0.2291 |
[32m[20221214 14:12:29 @agent_ppo2.py:185][0m |           0.0030 |         213.6716 |          -0.2145 |
[32m[20221214 14:12:29 @agent_ppo2.py:185][0m |          -0.0026 |         210.8310 |          -0.2212 |
[32m[20221214 14:12:29 @agent_ppo2.py:185][0m |          -0.0013 |         210.1087 |          -0.2432 |
[32m[20221214 14:12:29 @agent_ppo2.py:185][0m |           0.0053 |         215.9970 |          -0.2458 |
[32m[20221214 14:12:29 @agent_ppo2.py:185][0m |          -0.0004 |         208.4003 |          -0.2282 |
[32m[20221214 14:12:29 @agent_ppo2.py:185][0m |           0.0003 |         208.0110 |          -0.2312 |
[32m[20221214 14:12:29 @agent_ppo2.py:185][0m |           0.0064 |         213.9427 |          -0.2803 |
[32m[20221214 14:12:29 @agent_ppo2.py:185][0m |          -0.0025 |         207.9762 |          -0.2810 |
[32m[20221214 14:12:29 @agent_ppo2.py:185][0m |          -0.0017 |         207.5487 |          -0.2780 |
[32m[20221214 14:12:29 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:12:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.05
[32m[20221214 14:12:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.02
[32m[20221214 14:12:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.12
[32m[20221214 14:12:30 @agent_ppo2.py:143][0m Total time:      14.46 min
[32m[20221214 14:12:30 @agent_ppo2.py:145][0m 1325056 total steps have happened
[32m[20221214 14:12:30 @agent_ppo2.py:121][0m #------------------------ Iteration 647 --------------------------#
[32m[20221214 14:12:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:30 @agent_ppo2.py:185][0m |          -0.0013 |         214.5842 |          -0.5178 |
[32m[20221214 14:12:30 @agent_ppo2.py:185][0m |           0.0077 |         224.3893 |          -0.4902 |
[32m[20221214 14:12:30 @agent_ppo2.py:185][0m |          -0.0016 |         210.6388 |          -0.4604 |
[32m[20221214 14:12:30 @agent_ppo2.py:185][0m |          -0.0022 |         209.5930 |          -0.5565 |
[32m[20221214 14:12:30 @agent_ppo2.py:185][0m |          -0.0024 |         209.6785 |          -0.5121 |
[32m[20221214 14:12:30 @agent_ppo2.py:185][0m |           0.0114 |         237.5827 |          -0.5763 |
[32m[20221214 14:12:31 @agent_ppo2.py:185][0m |           0.0129 |         225.5835 |          -0.5609 |
[32m[20221214 14:12:31 @agent_ppo2.py:185][0m |          -0.0009 |         209.3935 |          -0.5315 |
[32m[20221214 14:12:31 @agent_ppo2.py:185][0m |           0.0001 |         208.8186 |          -0.6072 |
[32m[20221214 14:12:31 @agent_ppo2.py:185][0m |          -0.0020 |         208.4591 |          -0.6019 |
[32m[20221214 14:12:31 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:12:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.79
[32m[20221214 14:12:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.74
[32m[20221214 14:12:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.89
[32m[20221214 14:12:31 @agent_ppo2.py:143][0m Total time:      14.48 min
[32m[20221214 14:12:31 @agent_ppo2.py:145][0m 1327104 total steps have happened
[32m[20221214 14:12:31 @agent_ppo2.py:121][0m #------------------------ Iteration 648 --------------------------#
[32m[20221214 14:12:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:31 @agent_ppo2.py:185][0m |           0.0071 |         222.9719 |          -0.2726 |
[32m[20221214 14:12:31 @agent_ppo2.py:185][0m |          -0.0010 |         209.3433 |          -0.3263 |
[32m[20221214 14:12:32 @agent_ppo2.py:185][0m |          -0.0031 |         207.1734 |          -0.3479 |
[32m[20221214 14:12:32 @agent_ppo2.py:185][0m |          -0.0050 |         206.3737 |          -0.3530 |
[32m[20221214 14:12:32 @agent_ppo2.py:185][0m |          -0.0035 |         205.7557 |          -0.4654 |
[32m[20221214 14:12:32 @agent_ppo2.py:185][0m |          -0.0014 |         205.3570 |          -0.3951 |
[32m[20221214 14:12:32 @agent_ppo2.py:185][0m |          -0.0046 |         204.8268 |          -0.4840 |
[32m[20221214 14:12:32 @agent_ppo2.py:185][0m |          -0.0033 |         204.0449 |          -0.4499 |
[32m[20221214 14:12:32 @agent_ppo2.py:185][0m |           0.0064 |         209.5872 |          -0.5469 |
[32m[20221214 14:12:32 @agent_ppo2.py:185][0m |           0.0009 |         205.7719 |          -0.5368 |
[32m[20221214 14:12:32 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:12:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.27
[32m[20221214 14:12:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.03
[32m[20221214 14:12:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.77
[32m[20221214 14:12:32 @agent_ppo2.py:143][0m Total time:      14.50 min
[32m[20221214 14:12:32 @agent_ppo2.py:145][0m 1329152 total steps have happened
[32m[20221214 14:12:32 @agent_ppo2.py:121][0m #------------------------ Iteration 649 --------------------------#
[32m[20221214 14:12:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:33 @agent_ppo2.py:185][0m |           0.0079 |         198.8751 |          -0.1884 |
[32m[20221214 14:12:33 @agent_ppo2.py:185][0m |          -0.0008 |         184.3851 |          -0.1762 |
[32m[20221214 14:12:33 @agent_ppo2.py:185][0m |           0.0003 |         182.2683 |          -0.1636 |
[32m[20221214 14:12:33 @agent_ppo2.py:185][0m |          -0.0031 |         180.9520 |          -0.1822 |
[32m[20221214 14:12:33 @agent_ppo2.py:185][0m |           0.0012 |         179.6438 |          -0.1653 |
[32m[20221214 14:12:33 @agent_ppo2.py:185][0m |          -0.0005 |         177.6670 |          -0.1835 |
[32m[20221214 14:12:33 @agent_ppo2.py:185][0m |           0.0134 |         202.0578 |          -0.1720 |
[32m[20221214 14:12:33 @agent_ppo2.py:185][0m |          -0.0019 |         176.3333 |          -0.1900 |
[32m[20221214 14:12:33 @agent_ppo2.py:185][0m |          -0.0021 |         175.6355 |          -0.1819 |
[32m[20221214 14:12:34 @agent_ppo2.py:185][0m |          -0.0025 |         175.8187 |          -0.1867 |
[32m[20221214 14:12:34 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:12:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.87
[32m[20221214 14:12:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.56
[32m[20221214 14:12:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.15
[32m[20221214 14:12:34 @agent_ppo2.py:143][0m Total time:      14.53 min
[32m[20221214 14:12:34 @agent_ppo2.py:145][0m 1331200 total steps have happened
[32m[20221214 14:12:34 @agent_ppo2.py:121][0m #------------------------ Iteration 650 --------------------------#
[32m[20221214 14:12:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:12:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:34 @agent_ppo2.py:185][0m |           0.0079 |         193.4600 |          -0.5663 |
[32m[20221214 14:12:34 @agent_ppo2.py:185][0m |          -0.0028 |         181.4348 |          -0.5105 |
[32m[20221214 14:12:34 @agent_ppo2.py:185][0m |           0.0035 |         180.7665 |          -0.5145 |
[32m[20221214 14:12:34 @agent_ppo2.py:185][0m |          -0.0035 |         176.6544 |          -0.4912 |
[32m[20221214 14:12:34 @agent_ppo2.py:185][0m |          -0.0033 |         174.7580 |          -0.5263 |
[32m[20221214 14:12:35 @agent_ppo2.py:185][0m |           0.0036 |         177.1235 |          -0.5629 |
[32m[20221214 14:12:35 @agent_ppo2.py:185][0m |          -0.0022 |         172.7477 |          -0.5883 |
[32m[20221214 14:12:35 @agent_ppo2.py:185][0m |          -0.0044 |         172.9616 |          -0.5925 |
[32m[20221214 14:12:35 @agent_ppo2.py:185][0m |          -0.0032 |         172.6042 |          -0.6109 |
[32m[20221214 14:12:35 @agent_ppo2.py:185][0m |          -0.0059 |         171.7514 |          -0.6379 |
[32m[20221214 14:12:35 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:12:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.18
[32m[20221214 14:12:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.53
[32m[20221214 14:12:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.09
[32m[20221214 14:12:35 @agent_ppo2.py:143][0m Total time:      14.55 min
[32m[20221214 14:12:35 @agent_ppo2.py:145][0m 1333248 total steps have happened
[32m[20221214 14:12:35 @agent_ppo2.py:121][0m #------------------------ Iteration 651 --------------------------#
[32m[20221214 14:12:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:35 @agent_ppo2.py:185][0m |          -0.0026 |         163.1433 |          -1.4067 |
[32m[20221214 14:12:36 @agent_ppo2.py:185][0m |          -0.0006 |         160.1031 |          -1.4625 |
[32m[20221214 14:12:36 @agent_ppo2.py:185][0m |           0.0093 |         168.5173 |          -1.4929 |
[32m[20221214 14:12:36 @agent_ppo2.py:185][0m |           0.0028 |         162.9530 |          -1.5135 |
[32m[20221214 14:12:36 @agent_ppo2.py:185][0m |          -0.0019 |         157.8827 |          -1.5749 |
[32m[20221214 14:12:36 @agent_ppo2.py:185][0m |          -0.0020 |         157.2516 |          -1.5715 |
[32m[20221214 14:12:36 @agent_ppo2.py:185][0m |          -0.0033 |         157.0266 |          -1.6181 |
[32m[20221214 14:12:36 @agent_ppo2.py:185][0m |           0.0005 |         156.8875 |          -1.6499 |
[32m[20221214 14:12:36 @agent_ppo2.py:185][0m |          -0.0018 |         156.5852 |          -1.6958 |
[32m[20221214 14:12:36 @agent_ppo2.py:185][0m |           0.0124 |         174.3242 |          -1.6810 |
[32m[20221214 14:12:36 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:12:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.05
[32m[20221214 14:12:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.22
[32m[20221214 14:12:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.78
[32m[20221214 14:12:36 @agent_ppo2.py:143][0m Total time:      14.57 min
[32m[20221214 14:12:36 @agent_ppo2.py:145][0m 1335296 total steps have happened
[32m[20221214 14:12:36 @agent_ppo2.py:121][0m #------------------------ Iteration 652 --------------------------#
[32m[20221214 14:12:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:37 @agent_ppo2.py:185][0m |           0.0019 |         150.6070 |          -1.2741 |
[32m[20221214 14:12:37 @agent_ppo2.py:185][0m |          -0.0028 |         142.8724 |          -1.2378 |
[32m[20221214 14:12:37 @agent_ppo2.py:185][0m |          -0.0026 |         140.9633 |          -1.2594 |
[32m[20221214 14:12:37 @agent_ppo2.py:185][0m |           0.0093 |         159.6154 |          -1.2530 |
[32m[20221214 14:12:37 @agent_ppo2.py:185][0m |           0.0034 |         140.5876 |          -1.2345 |
[32m[20221214 14:12:37 @agent_ppo2.py:185][0m |          -0.0006 |         137.6210 |          -1.2749 |
[32m[20221214 14:12:37 @agent_ppo2.py:185][0m |           0.0063 |         141.4450 |          -1.2834 |
[32m[20221214 14:12:37 @agent_ppo2.py:185][0m |           0.0074 |         144.2634 |          -1.2727 |
[32m[20221214 14:12:38 @agent_ppo2.py:185][0m |          -0.0017 |         137.1273 |          -1.2848 |
[32m[20221214 14:12:38 @agent_ppo2.py:185][0m |          -0.0028 |         136.0993 |          -1.2593 |
[32m[20221214 14:12:38 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:12:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.88
[32m[20221214 14:12:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.06
[32m[20221214 14:12:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.99
[32m[20221214 14:12:38 @agent_ppo2.py:143][0m Total time:      14.60 min
[32m[20221214 14:12:38 @agent_ppo2.py:145][0m 1337344 total steps have happened
[32m[20221214 14:12:38 @agent_ppo2.py:121][0m #------------------------ Iteration 653 --------------------------#
[32m[20221214 14:12:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:38 @agent_ppo2.py:185][0m |           0.0060 |         151.1320 |          -1.3772 |
[32m[20221214 14:12:38 @agent_ppo2.py:185][0m |           0.0005 |         141.0412 |          -1.3277 |
[32m[20221214 14:12:38 @agent_ppo2.py:185][0m |           0.0053 |         143.7992 |          -1.3405 |
[32m[20221214 14:12:38 @agent_ppo2.py:185][0m |          -0.0021 |         137.3407 |          -1.3453 |
[32m[20221214 14:12:39 @agent_ppo2.py:185][0m |          -0.0038 |         136.2325 |          -1.2708 |
[32m[20221214 14:12:39 @agent_ppo2.py:185][0m |          -0.0002 |         135.5260 |          -1.3012 |
[32m[20221214 14:12:39 @agent_ppo2.py:185][0m |          -0.0006 |         135.2069 |          -1.2597 |
[32m[20221214 14:12:39 @agent_ppo2.py:185][0m |          -0.0014 |         134.7838 |          -1.2384 |
[32m[20221214 14:12:39 @agent_ppo2.py:185][0m |          -0.0010 |         134.2822 |          -1.2572 |
[32m[20221214 14:12:39 @agent_ppo2.py:185][0m |          -0.0022 |         133.7041 |          -1.1927 |
[32m[20221214 14:12:39 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:12:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.80
[32m[20221214 14:12:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.08
[32m[20221214 14:12:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.12
[32m[20221214 14:12:39 @agent_ppo2.py:143][0m Total time:      14.62 min
[32m[20221214 14:12:39 @agent_ppo2.py:145][0m 1339392 total steps have happened
[32m[20221214 14:12:39 @agent_ppo2.py:121][0m #------------------------ Iteration 654 --------------------------#
[32m[20221214 14:12:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:40 @agent_ppo2.py:185][0m |          -0.0013 |         177.6814 |          -1.2669 |
[32m[20221214 14:12:40 @agent_ppo2.py:185][0m |          -0.0012 |         172.6251 |          -1.3195 |
[32m[20221214 14:12:40 @agent_ppo2.py:185][0m |          -0.0041 |         172.0517 |          -1.3693 |
[32m[20221214 14:12:40 @agent_ppo2.py:185][0m |          -0.0016 |         172.2394 |          -1.3406 |
[32m[20221214 14:12:40 @agent_ppo2.py:185][0m |          -0.0023 |         171.3207 |          -1.4599 |
[32m[20221214 14:12:40 @agent_ppo2.py:185][0m |           0.0007 |         172.7657 |          -1.4925 |
[32m[20221214 14:12:40 @agent_ppo2.py:185][0m |          -0.0017 |         170.4755 |          -1.5410 |
[32m[20221214 14:12:40 @agent_ppo2.py:185][0m |          -0.0031 |         170.1285 |          -1.5532 |
[32m[20221214 14:12:40 @agent_ppo2.py:185][0m |           0.0071 |         180.7229 |          -1.6054 |
[32m[20221214 14:12:40 @agent_ppo2.py:185][0m |          -0.0039 |         169.5765 |          -1.6083 |
[32m[20221214 14:12:40 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:12:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.11
[32m[20221214 14:12:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.27
[32m[20221214 14:12:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.41
[32m[20221214 14:12:41 @agent_ppo2.py:143][0m Total time:      14.64 min
[32m[20221214 14:12:41 @agent_ppo2.py:145][0m 1341440 total steps have happened
[32m[20221214 14:12:41 @agent_ppo2.py:121][0m #------------------------ Iteration 655 --------------------------#
[32m[20221214 14:12:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:41 @agent_ppo2.py:185][0m |           0.0128 |         182.3168 |          -2.1874 |
[32m[20221214 14:12:41 @agent_ppo2.py:185][0m |          -0.0011 |         163.8047 |          -2.1777 |
[32m[20221214 14:12:41 @agent_ppo2.py:185][0m |          -0.0019 |         162.3527 |          -2.2497 |
[32m[20221214 14:12:41 @agent_ppo2.py:185][0m |          -0.0024 |         161.6621 |          -2.2678 |
[32m[20221214 14:12:41 @agent_ppo2.py:185][0m |          -0.0008 |         160.1417 |          -2.2992 |
[32m[20221214 14:12:41 @agent_ppo2.py:185][0m |          -0.0019 |         159.4556 |          -2.3115 |
[32m[20221214 14:12:41 @agent_ppo2.py:185][0m |          -0.0014 |         159.0087 |          -2.3373 |
[32m[20221214 14:12:41 @agent_ppo2.py:185][0m |          -0.0011 |         158.4671 |          -2.4003 |
[32m[20221214 14:12:42 @agent_ppo2.py:185][0m |          -0.0028 |         158.0138 |          -2.3790 |
[32m[20221214 14:12:42 @agent_ppo2.py:185][0m |           0.0041 |         161.4894 |          -2.4262 |
[32m[20221214 14:12:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:12:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.38
[32m[20221214 14:12:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.51
[32m[20221214 14:12:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.70
[32m[20221214 14:12:42 @agent_ppo2.py:143][0m Total time:      14.66 min
[32m[20221214 14:12:42 @agent_ppo2.py:145][0m 1343488 total steps have happened
[32m[20221214 14:12:42 @agent_ppo2.py:121][0m #------------------------ Iteration 656 --------------------------#
[32m[20221214 14:12:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:42 @agent_ppo2.py:185][0m |          -0.0049 |         155.5577 |          -2.0700 |
[32m[20221214 14:12:42 @agent_ppo2.py:185][0m |          -0.0052 |         152.2066 |          -2.0420 |
[32m[20221214 14:12:42 @agent_ppo2.py:185][0m |          -0.0076 |         151.2352 |          -2.0722 |
[32m[20221214 14:12:42 @agent_ppo2.py:185][0m |          -0.0039 |         150.9793 |          -2.0697 |
[32m[20221214 14:12:43 @agent_ppo2.py:185][0m |          -0.0003 |         157.4718 |          -2.0899 |
[32m[20221214 14:12:43 @agent_ppo2.py:185][0m |           0.0054 |         157.2932 |          -2.1011 |
[32m[20221214 14:12:43 @agent_ppo2.py:185][0m |          -0.0090 |         150.1042 |          -2.1381 |
[32m[20221214 14:12:43 @agent_ppo2.py:185][0m |          -0.0061 |         150.6720 |          -2.1568 |
[32m[20221214 14:12:43 @agent_ppo2.py:185][0m |          -0.0031 |         149.8885 |          -2.1781 |
[32m[20221214 14:12:43 @agent_ppo2.py:185][0m |          -0.0071 |         149.1449 |          -2.1468 |
[32m[20221214 14:12:43 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:12:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.16
[32m[20221214 14:12:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.08
[32m[20221214 14:12:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.45
[32m[20221214 14:12:43 @agent_ppo2.py:143][0m Total time:      14.69 min
[32m[20221214 14:12:43 @agent_ppo2.py:145][0m 1345536 total steps have happened
[32m[20221214 14:12:43 @agent_ppo2.py:121][0m #------------------------ Iteration 657 --------------------------#
[32m[20221214 14:12:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:44 @agent_ppo2.py:185][0m |          -0.0038 |         181.0279 |          -2.3162 |
[32m[20221214 14:12:44 @agent_ppo2.py:185][0m |          -0.0030 |         177.6351 |          -2.3650 |
[32m[20221214 14:12:44 @agent_ppo2.py:185][0m |          -0.0027 |         175.4170 |          -2.3658 |
[32m[20221214 14:12:44 @agent_ppo2.py:185][0m |          -0.0046 |         174.6271 |          -2.4299 |
[32m[20221214 14:12:44 @agent_ppo2.py:185][0m |          -0.0051 |         174.3373 |          -2.4473 |
[32m[20221214 14:12:44 @agent_ppo2.py:185][0m |          -0.0047 |         174.1299 |          -2.4146 |
[32m[20221214 14:12:44 @agent_ppo2.py:185][0m |          -0.0048 |         173.6112 |          -2.4533 |
[32m[20221214 14:12:44 @agent_ppo2.py:185][0m |          -0.0034 |         172.6662 |          -2.4593 |
[32m[20221214 14:12:44 @agent_ppo2.py:185][0m |          -0.0038 |         172.7764 |          -2.4762 |
[32m[20221214 14:12:44 @agent_ppo2.py:185][0m |          -0.0029 |         172.2857 |          -2.5380 |
[32m[20221214 14:12:44 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:12:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.44
[32m[20221214 14:12:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.39
[32m[20221214 14:12:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.40
[32m[20221214 14:12:45 @agent_ppo2.py:143][0m Total time:      14.71 min
[32m[20221214 14:12:45 @agent_ppo2.py:145][0m 1347584 total steps have happened
[32m[20221214 14:12:45 @agent_ppo2.py:121][0m #------------------------ Iteration 658 --------------------------#
[32m[20221214 14:12:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:45 @agent_ppo2.py:185][0m |          -0.0011 |         186.3512 |          -2.5993 |
[32m[20221214 14:12:45 @agent_ppo2.py:185][0m |          -0.0040 |         181.7651 |          -2.6431 |
[32m[20221214 14:12:45 @agent_ppo2.py:185][0m |          -0.0006 |         179.6419 |          -2.6325 |
[32m[20221214 14:12:45 @agent_ppo2.py:185][0m |           0.0051 |         187.5622 |          -2.6538 |
[32m[20221214 14:12:45 @agent_ppo2.py:185][0m |           0.0050 |         186.0522 |          -2.6418 |
[32m[20221214 14:12:45 @agent_ppo2.py:185][0m |           0.0022 |         177.5443 |          -2.5945 |
[32m[20221214 14:12:45 @agent_ppo2.py:185][0m |           0.0123 |         190.3031 |          -2.6692 |
[32m[20221214 14:12:46 @agent_ppo2.py:185][0m |           0.0138 |         198.3591 |          -2.6382 |
[32m[20221214 14:12:46 @agent_ppo2.py:185][0m |           0.0056 |         183.6840 |          -2.6855 |
[32m[20221214 14:12:46 @agent_ppo2.py:185][0m |          -0.0037 |         174.9703 |          -2.7017 |
[32m[20221214 14:12:46 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:12:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.25
[32m[20221214 14:12:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.30
[32m[20221214 14:12:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.08
[32m[20221214 14:12:46 @agent_ppo2.py:143][0m Total time:      14.73 min
[32m[20221214 14:12:46 @agent_ppo2.py:145][0m 1349632 total steps have happened
[32m[20221214 14:12:46 @agent_ppo2.py:121][0m #------------------------ Iteration 659 --------------------------#
[32m[20221214 14:12:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:12:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:46 @agent_ppo2.py:185][0m |          -0.0048 |         170.2771 |          -2.7090 |
[32m[20221214 14:12:46 @agent_ppo2.py:185][0m |          -0.0053 |         167.1768 |          -2.6925 |
[32m[20221214 14:12:46 @agent_ppo2.py:185][0m |          -0.0043 |         165.8683 |          -2.6878 |
[32m[20221214 14:12:47 @agent_ppo2.py:185][0m |          -0.0049 |         166.1222 |          -2.6901 |
[32m[20221214 14:12:47 @agent_ppo2.py:185][0m |           0.0023 |         168.1873 |          -2.6755 |
[32m[20221214 14:12:47 @agent_ppo2.py:185][0m |          -0.0053 |         165.6462 |          -2.7000 |
[32m[20221214 14:12:47 @agent_ppo2.py:185][0m |          -0.0070 |         164.7067 |          -2.6894 |
[32m[20221214 14:12:47 @agent_ppo2.py:185][0m |           0.0064 |         177.5578 |          -2.7082 |
[32m[20221214 14:12:47 @agent_ppo2.py:185][0m |          -0.0059 |         165.3059 |          -2.6559 |
[32m[20221214 14:12:47 @agent_ppo2.py:185][0m |           0.0029 |         170.5409 |          -2.6508 |
[32m[20221214 14:12:47 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:12:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.73
[32m[20221214 14:12:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.72
[32m[20221214 14:12:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.40
[32m[20221214 14:12:47 @agent_ppo2.py:143][0m Total time:      14.75 min
[32m[20221214 14:12:47 @agent_ppo2.py:145][0m 1351680 total steps have happened
[32m[20221214 14:12:47 @agent_ppo2.py:121][0m #------------------------ Iteration 660 --------------------------#
[32m[20221214 14:12:47 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:12:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:48 @agent_ppo2.py:185][0m |          -0.0014 |         179.0808 |          -2.6366 |
[32m[20221214 14:12:48 @agent_ppo2.py:185][0m |          -0.0030 |         174.6079 |          -2.6031 |
[32m[20221214 14:12:48 @agent_ppo2.py:185][0m |          -0.0055 |         171.7656 |          -2.5937 |
[32m[20221214 14:12:48 @agent_ppo2.py:185][0m |          -0.0008 |         172.1825 |          -2.6117 |
[32m[20221214 14:12:48 @agent_ppo2.py:185][0m |          -0.0038 |         166.7293 |          -2.6110 |
[32m[20221214 14:12:48 @agent_ppo2.py:185][0m |          -0.0053 |         165.5519 |          -2.5829 |
[32m[20221214 14:12:48 @agent_ppo2.py:185][0m |          -0.0050 |         164.3979 |          -2.6090 |
[32m[20221214 14:12:48 @agent_ppo2.py:185][0m |          -0.0035 |         163.4430 |          -2.6267 |
[32m[20221214 14:12:48 @agent_ppo2.py:185][0m |          -0.0009 |         163.7184 |          -2.6596 |
[32m[20221214 14:12:48 @agent_ppo2.py:185][0m |          -0.0028 |         163.0172 |          -2.6876 |
[32m[20221214 14:12:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:12:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.99
[32m[20221214 14:12:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.92
[32m[20221214 14:12:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.79
[32m[20221214 14:12:49 @agent_ppo2.py:143][0m Total time:      14.77 min
[32m[20221214 14:12:49 @agent_ppo2.py:145][0m 1353728 total steps have happened
[32m[20221214 14:12:49 @agent_ppo2.py:121][0m #------------------------ Iteration 661 --------------------------#
[32m[20221214 14:12:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:12:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:49 @agent_ppo2.py:185][0m |           0.0047 |         180.6952 |          -3.3946 |
[32m[20221214 14:12:49 @agent_ppo2.py:185][0m |          -0.0021 |         172.6781 |          -3.4081 |
[32m[20221214 14:12:49 @agent_ppo2.py:185][0m |          -0.0012 |         169.4698 |          -3.3916 |
[32m[20221214 14:12:49 @agent_ppo2.py:185][0m |           0.0008 |         168.3597 |          -3.3865 |
[32m[20221214 14:12:49 @agent_ppo2.py:185][0m |          -0.0040 |         166.6847 |          -3.4422 |
[32m[20221214 14:12:49 @agent_ppo2.py:185][0m |          -0.0035 |         166.0413 |          -3.4066 |
[32m[20221214 14:12:49 @agent_ppo2.py:185][0m |          -0.0037 |         164.9845 |          -3.4302 |
[32m[20221214 14:12:49 @agent_ppo2.py:185][0m |          -0.0042 |         164.5113 |          -3.4476 |
[32m[20221214 14:12:50 @agent_ppo2.py:185][0m |          -0.0040 |         164.5513 |          -3.4570 |
[32m[20221214 14:12:50 @agent_ppo2.py:185][0m |          -0.0046 |         164.7329 |          -3.4918 |
[32m[20221214 14:12:50 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:12:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.53
[32m[20221214 14:12:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.24
[32m[20221214 14:12:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.08
[32m[20221214 14:12:50 @agent_ppo2.py:143][0m Total time:      14.80 min
[32m[20221214 14:12:50 @agent_ppo2.py:145][0m 1355776 total steps have happened
[32m[20221214 14:12:50 @agent_ppo2.py:121][0m #------------------------ Iteration 662 --------------------------#
[32m[20221214 14:12:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:12:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:50 @agent_ppo2.py:185][0m |          -0.0013 |         174.8197 |          -3.2816 |
[32m[20221214 14:12:50 @agent_ppo2.py:185][0m |          -0.0028 |         168.8326 |          -3.2598 |
[32m[20221214 14:12:50 @agent_ppo2.py:185][0m |          -0.0030 |         167.0811 |          -3.2726 |
[32m[20221214 14:12:50 @agent_ppo2.py:185][0m |          -0.0037 |         165.9767 |          -3.3125 |
[32m[20221214 14:12:50 @agent_ppo2.py:185][0m |          -0.0020 |         165.2734 |          -3.3074 |
[32m[20221214 14:12:51 @agent_ppo2.py:185][0m |          -0.0001 |         166.5287 |          -3.3601 |
[32m[20221214 14:12:51 @agent_ppo2.py:185][0m |          -0.0023 |         163.9998 |          -3.3795 |
[32m[20221214 14:12:51 @agent_ppo2.py:185][0m |          -0.0020 |         164.2707 |          -3.3991 |
[32m[20221214 14:12:51 @agent_ppo2.py:185][0m |          -0.0030 |         163.7257 |          -3.3977 |
[32m[20221214 14:12:51 @agent_ppo2.py:185][0m |          -0.0033 |         163.8226 |          -3.4390 |
[32m[20221214 14:12:51 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:12:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.40
[32m[20221214 14:12:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.29
[32m[20221214 14:12:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.77
[32m[20221214 14:12:51 @agent_ppo2.py:143][0m Total time:      14.82 min
[32m[20221214 14:12:51 @agent_ppo2.py:145][0m 1357824 total steps have happened
[32m[20221214 14:12:51 @agent_ppo2.py:121][0m #------------------------ Iteration 663 --------------------------#
[32m[20221214 14:12:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:12:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:51 @agent_ppo2.py:185][0m |           0.0114 |         167.0734 |          -3.2268 |
[32m[20221214 14:12:51 @agent_ppo2.py:185][0m |          -0.0029 |         153.6241 |          -3.1901 |
[32m[20221214 14:12:52 @agent_ppo2.py:185][0m |          -0.0024 |         152.8556 |          -3.1536 |
[32m[20221214 14:12:52 @agent_ppo2.py:185][0m |          -0.0016 |         151.8852 |          -3.1535 |
[32m[20221214 14:12:52 @agent_ppo2.py:185][0m |          -0.0013 |         151.4873 |          -3.1988 |
[32m[20221214 14:12:52 @agent_ppo2.py:185][0m |          -0.0014 |         151.1165 |          -3.1317 |
[32m[20221214 14:12:52 @agent_ppo2.py:185][0m |           0.0146 |         161.9771 |          -3.1884 |
[32m[20221214 14:12:52 @agent_ppo2.py:185][0m |          -0.0013 |         150.7139 |          -3.1107 |
[32m[20221214 14:12:52 @agent_ppo2.py:185][0m |          -0.0006 |         150.0062 |          -3.1287 |
[32m[20221214 14:12:52 @agent_ppo2.py:185][0m |          -0.0014 |         149.9586 |          -3.1433 |
[32m[20221214 14:12:52 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:12:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.75
[32m[20221214 14:12:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.98
[32m[20221214 14:12:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.91
[32m[20221214 14:12:52 @agent_ppo2.py:143][0m Total time:      14.84 min
[32m[20221214 14:12:52 @agent_ppo2.py:145][0m 1359872 total steps have happened
[32m[20221214 14:12:52 @agent_ppo2.py:121][0m #------------------------ Iteration 664 --------------------------#
[32m[20221214 14:12:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:12:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:53 @agent_ppo2.py:185][0m |           0.0055 |         182.0531 |          -3.6046 |
[32m[20221214 14:12:53 @agent_ppo2.py:185][0m |          -0.0027 |         174.4164 |          -3.5561 |
[32m[20221214 14:12:53 @agent_ppo2.py:185][0m |          -0.0020 |         173.8572 |          -3.5228 |
[32m[20221214 14:12:53 @agent_ppo2.py:185][0m |          -0.0017 |         173.7756 |          -3.4973 |
[32m[20221214 14:12:53 @agent_ppo2.py:185][0m |          -0.0032 |         173.4385 |          -3.4160 |
[32m[20221214 14:12:53 @agent_ppo2.py:185][0m |           0.0088 |         182.1883 |          -3.4134 |
[32m[20221214 14:12:53 @agent_ppo2.py:185][0m |          -0.0022 |         172.9450 |          -3.2696 |
[32m[20221214 14:12:53 @agent_ppo2.py:185][0m |           0.0128 |         189.9910 |          -3.2147 |
[32m[20221214 14:12:53 @agent_ppo2.py:185][0m |           0.0113 |         186.2712 |          -3.2689 |
[32m[20221214 14:12:53 @agent_ppo2.py:185][0m |           0.0005 |         173.3228 |          -3.1666 |
[32m[20221214 14:12:53 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:12:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.40
[32m[20221214 14:12:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.65
[32m[20221214 14:12:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.45
[32m[20221214 14:12:53 @agent_ppo2.py:143][0m Total time:      14.86 min
[32m[20221214 14:12:53 @agent_ppo2.py:145][0m 1361920 total steps have happened
[32m[20221214 14:12:53 @agent_ppo2.py:121][0m #------------------------ Iteration 665 --------------------------#
[32m[20221214 14:12:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:12:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:54 @agent_ppo2.py:185][0m |          -0.0012 |         175.4289 |          -3.2240 |
[32m[20221214 14:12:54 @agent_ppo2.py:185][0m |          -0.0028 |         172.5527 |          -3.1927 |
[32m[20221214 14:12:54 @agent_ppo2.py:185][0m |           0.0087 |         179.3927 |          -3.1850 |
[32m[20221214 14:12:54 @agent_ppo2.py:185][0m |           0.0099 |         185.9362 |          -3.1931 |
[32m[20221214 14:12:54 @agent_ppo2.py:185][0m |          -0.0018 |         169.5998 |          -3.1700 |
[32m[20221214 14:12:54 @agent_ppo2.py:185][0m |           0.0072 |         179.2625 |          -3.1426 |
[32m[20221214 14:12:54 @agent_ppo2.py:185][0m |          -0.0028 |         168.6071 |          -3.1153 |
[32m[20221214 14:12:54 @agent_ppo2.py:185][0m |          -0.0014 |         167.3930 |          -3.1290 |
[32m[20221214 14:12:54 @agent_ppo2.py:185][0m |          -0.0046 |         167.8343 |          -3.1042 |
[32m[20221214 14:12:55 @agent_ppo2.py:185][0m |           0.0031 |         168.6673 |          -3.0889 |
[32m[20221214 14:12:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:12:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.73
[32m[20221214 14:12:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.96
[32m[20221214 14:12:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.11
[32m[20221214 14:12:55 @agent_ppo2.py:143][0m Total time:      14.88 min
[32m[20221214 14:12:55 @agent_ppo2.py:145][0m 1363968 total steps have happened
[32m[20221214 14:12:55 @agent_ppo2.py:121][0m #------------------------ Iteration 666 --------------------------#
[32m[20221214 14:12:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:12:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:55 @agent_ppo2.py:185][0m |           0.0075 |         190.5997 |          -2.8582 |
[32m[20221214 14:12:55 @agent_ppo2.py:185][0m |           0.0076 |         190.8113 |          -2.8508 |
[32m[20221214 14:12:55 @agent_ppo2.py:185][0m |           0.0043 |         178.5390 |          -2.8294 |
[32m[20221214 14:12:55 @agent_ppo2.py:185][0m |          -0.0043 |         172.0991 |          -2.8682 |
[32m[20221214 14:12:55 @agent_ppo2.py:185][0m |          -0.0032 |         171.6263 |          -2.8973 |
[32m[20221214 14:12:55 @agent_ppo2.py:185][0m |          -0.0033 |         170.5285 |          -2.9262 |
[32m[20221214 14:12:56 @agent_ppo2.py:185][0m |          -0.0051 |         170.8703 |          -2.9743 |
[32m[20221214 14:12:56 @agent_ppo2.py:185][0m |          -0.0060 |         170.3182 |          -2.9947 |
[32m[20221214 14:12:56 @agent_ppo2.py:185][0m |          -0.0023 |         170.1100 |          -3.0017 |
[32m[20221214 14:12:56 @agent_ppo2.py:185][0m |           0.0006 |         171.0474 |          -3.0011 |
[32m[20221214 14:12:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:12:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.82
[32m[20221214 14:12:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.88
[32m[20221214 14:12:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.64
[32m[20221214 14:12:56 @agent_ppo2.py:143][0m Total time:      14.90 min
[32m[20221214 14:12:56 @agent_ppo2.py:145][0m 1366016 total steps have happened
[32m[20221214 14:12:56 @agent_ppo2.py:121][0m #------------------------ Iteration 667 --------------------------#
[32m[20221214 14:12:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:12:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:56 @agent_ppo2.py:185][0m |          -0.0002 |         182.6851 |          -2.7413 |
[32m[20221214 14:12:56 @agent_ppo2.py:185][0m |          -0.0027 |         176.5121 |          -2.7335 |
[32m[20221214 14:12:56 @agent_ppo2.py:185][0m |          -0.0032 |         173.3220 |          -2.7096 |
[32m[20221214 14:12:57 @agent_ppo2.py:185][0m |           0.0002 |         172.0473 |          -2.7735 |
[32m[20221214 14:12:57 @agent_ppo2.py:185][0m |           0.0121 |         186.2998 |          -2.7674 |
[32m[20221214 14:12:57 @agent_ppo2.py:185][0m |          -0.0034 |         169.5120 |          -2.8088 |
[32m[20221214 14:12:57 @agent_ppo2.py:185][0m |          -0.0026 |         167.6013 |          -2.7716 |
[32m[20221214 14:12:57 @agent_ppo2.py:185][0m |           0.0189 |         198.1218 |          -2.7615 |
[32m[20221214 14:12:57 @agent_ppo2.py:185][0m |          -0.0024 |         167.7471 |          -2.8012 |
[32m[20221214 14:12:57 @agent_ppo2.py:185][0m |          -0.0012 |         166.3271 |          -2.8271 |
[32m[20221214 14:12:57 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:12:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.47
[32m[20221214 14:12:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.15
[32m[20221214 14:12:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.07
[32m[20221214 14:12:57 @agent_ppo2.py:143][0m Total time:      14.92 min
[32m[20221214 14:12:57 @agent_ppo2.py:145][0m 1368064 total steps have happened
[32m[20221214 14:12:57 @agent_ppo2.py:121][0m #------------------------ Iteration 668 --------------------------#
[32m[20221214 14:12:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:12:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:58 @agent_ppo2.py:185][0m |          -0.0015 |         182.8657 |          -4.0027 |
[32m[20221214 14:12:58 @agent_ppo2.py:185][0m |          -0.0014 |         176.4991 |          -4.0291 |
[32m[20221214 14:12:58 @agent_ppo2.py:185][0m |          -0.0020 |         173.5622 |          -4.0518 |
[32m[20221214 14:12:58 @agent_ppo2.py:185][0m |           0.0001 |         170.7256 |          -4.1507 |
[32m[20221214 14:12:58 @agent_ppo2.py:185][0m |           0.0018 |         169.9245 |          -4.1873 |
[32m[20221214 14:12:58 @agent_ppo2.py:185][0m |           0.0003 |         167.7607 |          -4.2538 |
[32m[20221214 14:12:58 @agent_ppo2.py:185][0m |          -0.0010 |         167.0223 |          -4.2895 |
[32m[20221214 14:12:58 @agent_ppo2.py:185][0m |           0.0015 |         166.9556 |          -4.3240 |
[32m[20221214 14:12:58 @agent_ppo2.py:185][0m |          -0.0035 |         165.7018 |          -4.3467 |
[32m[20221214 14:12:58 @agent_ppo2.py:185][0m |          -0.0014 |         164.8938 |          -4.3749 |
[32m[20221214 14:12:58 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:12:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.34
[32m[20221214 14:12:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.17
[32m[20221214 14:12:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.86
[32m[20221214 14:12:58 @agent_ppo2.py:143][0m Total time:      14.94 min
[32m[20221214 14:12:58 @agent_ppo2.py:145][0m 1370112 total steps have happened
[32m[20221214 14:12:58 @agent_ppo2.py:121][0m #------------------------ Iteration 669 --------------------------#
[32m[20221214 14:12:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:12:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:12:59 @agent_ppo2.py:185][0m |          -0.0008 |         151.6440 |          -4.0046 |
[32m[20221214 14:12:59 @agent_ppo2.py:185][0m |           0.0014 |         146.9729 |          -3.9892 |
[32m[20221214 14:12:59 @agent_ppo2.py:185][0m |          -0.0003 |         146.4793 |          -4.0487 |
[32m[20221214 14:12:59 @agent_ppo2.py:185][0m |           0.0101 |         155.1237 |          -4.0268 |
[32m[20221214 14:12:59 @agent_ppo2.py:185][0m |           0.0122 |         169.1426 |          -3.9422 |
[32m[20221214 14:12:59 @agent_ppo2.py:185][0m |          -0.0014 |         144.3032 |          -4.1105 |
[32m[20221214 14:12:59 @agent_ppo2.py:185][0m |          -0.0021 |         142.4468 |          -4.0330 |
[32m[20221214 14:12:59 @agent_ppo2.py:185][0m |          -0.0038 |         141.9014 |          -4.0317 |
[32m[20221214 14:12:59 @agent_ppo2.py:185][0m |           0.0010 |         142.4941 |          -4.0030 |
[32m[20221214 14:12:59 @agent_ppo2.py:185][0m |           0.0003 |         141.1188 |          -4.0623 |
[32m[20221214 14:12:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:13:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.05
[32m[20221214 14:13:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.04
[32m[20221214 14:13:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.70
[32m[20221214 14:13:00 @agent_ppo2.py:143][0m Total time:      14.96 min
[32m[20221214 14:13:00 @agent_ppo2.py:145][0m 1372160 total steps have happened
[32m[20221214 14:13:00 @agent_ppo2.py:121][0m #------------------------ Iteration 670 --------------------------#
[32m[20221214 14:13:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:13:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:00 @agent_ppo2.py:185][0m |          -0.0008 |         197.9876 |          -3.9246 |
[32m[20221214 14:13:00 @agent_ppo2.py:185][0m |           0.0032 |         194.4908 |          -3.9012 |
[32m[20221214 14:13:00 @agent_ppo2.py:185][0m |          -0.0034 |         187.2422 |          -3.9488 |
[32m[20221214 14:13:00 @agent_ppo2.py:185][0m |          -0.0012 |         185.6533 |          -3.9763 |
[32m[20221214 14:13:00 @agent_ppo2.py:185][0m |          -0.0008 |         185.4139 |          -4.0045 |
[32m[20221214 14:13:00 @agent_ppo2.py:185][0m |          -0.0022 |         185.0435 |          -4.0171 |
[32m[20221214 14:13:00 @agent_ppo2.py:185][0m |           0.0038 |         186.2114 |          -4.0554 |
[32m[20221214 14:13:01 @agent_ppo2.py:185][0m |          -0.0053 |         183.9886 |          -4.0655 |
[32m[20221214 14:13:01 @agent_ppo2.py:185][0m |          -0.0010 |         182.9896 |          -4.1114 |
[32m[20221214 14:13:01 @agent_ppo2.py:185][0m |           0.0049 |         187.1095 |          -4.1071 |
[32m[20221214 14:13:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:13:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.04
[32m[20221214 14:13:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.80
[32m[20221214 14:13:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.24
[32m[20221214 14:13:01 @agent_ppo2.py:143][0m Total time:      14.98 min
[32m[20221214 14:13:01 @agent_ppo2.py:145][0m 1374208 total steps have happened
[32m[20221214 14:13:01 @agent_ppo2.py:121][0m #------------------------ Iteration 671 --------------------------#
[32m[20221214 14:13:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:01 @agent_ppo2.py:185][0m |          -0.0007 |         189.9345 |          -4.1430 |
[32m[20221214 14:13:01 @agent_ppo2.py:185][0m |           0.0006 |         187.5157 |          -4.1505 |
[32m[20221214 14:13:01 @agent_ppo2.py:185][0m |          -0.0037 |         185.8401 |          -4.1630 |
[32m[20221214 14:13:01 @agent_ppo2.py:185][0m |          -0.0021 |         184.9305 |          -4.0936 |
[32m[20221214 14:13:02 @agent_ppo2.py:185][0m |           0.0040 |         192.6212 |          -4.1437 |
[32m[20221214 14:13:02 @agent_ppo2.py:185][0m |           0.0031 |         186.1426 |          -4.1768 |
[32m[20221214 14:13:02 @agent_ppo2.py:185][0m |          -0.0036 |         183.2622 |          -4.1639 |
[32m[20221214 14:13:02 @agent_ppo2.py:185][0m |          -0.0036 |         182.7212 |          -4.2333 |
[32m[20221214 14:13:02 @agent_ppo2.py:185][0m |           0.0072 |         195.2604 |          -4.2794 |
[32m[20221214 14:13:02 @agent_ppo2.py:185][0m |          -0.0009 |         182.1355 |          -4.1809 |
[32m[20221214 14:13:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:13:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.96
[32m[20221214 14:13:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.41
[32m[20221214 14:13:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.22
[32m[20221214 14:13:02 @agent_ppo2.py:143][0m Total time:      15.00 min
[32m[20221214 14:13:02 @agent_ppo2.py:145][0m 1376256 total steps have happened
[32m[20221214 14:13:02 @agent_ppo2.py:121][0m #------------------------ Iteration 672 --------------------------#
[32m[20221214 14:13:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:13:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:02 @agent_ppo2.py:185][0m |          -0.0044 |         210.3316 |          -4.8687 |
[32m[20221214 14:13:03 @agent_ppo2.py:185][0m |          -0.0024 |         208.0276 |          -4.9331 |
[32m[20221214 14:13:03 @agent_ppo2.py:185][0m |          -0.0039 |         207.8256 |          -4.9196 |
[32m[20221214 14:13:03 @agent_ppo2.py:185][0m |          -0.0025 |         206.8302 |          -4.9514 |
[32m[20221214 14:13:03 @agent_ppo2.py:185][0m |          -0.0047 |         206.8333 |          -4.9167 |
[32m[20221214 14:13:03 @agent_ppo2.py:185][0m |           0.0040 |         218.7991 |          -5.0010 |
[32m[20221214 14:13:03 @agent_ppo2.py:185][0m |          -0.0042 |         206.0115 |          -4.9950 |
[32m[20221214 14:13:03 @agent_ppo2.py:185][0m |           0.0030 |         211.9575 |          -5.0311 |
[32m[20221214 14:13:03 @agent_ppo2.py:185][0m |           0.0015 |         211.1360 |          -5.0690 |
[32m[20221214 14:13:03 @agent_ppo2.py:185][0m |          -0.0033 |         205.4795 |          -5.0716 |
[32m[20221214 14:13:03 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:13:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.35
[32m[20221214 14:13:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.37
[32m[20221214 14:13:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.27
[32m[20221214 14:13:03 @agent_ppo2.py:143][0m Total time:      15.02 min
[32m[20221214 14:13:03 @agent_ppo2.py:145][0m 1378304 total steps have happened
[32m[20221214 14:13:03 @agent_ppo2.py:121][0m #------------------------ Iteration 673 --------------------------#
[32m[20221214 14:13:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:13:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:04 @agent_ppo2.py:185][0m |          -0.0000 |         214.8339 |          -5.3533 |
[32m[20221214 14:13:04 @agent_ppo2.py:185][0m |          -0.0018 |         211.7494 |          -5.2989 |
[32m[20221214 14:13:04 @agent_ppo2.py:185][0m |          -0.0015 |         211.0096 |          -5.3734 |
[32m[20221214 14:13:04 @agent_ppo2.py:185][0m |           0.0048 |         214.7730 |          -5.3828 |
[32m[20221214 14:13:04 @agent_ppo2.py:185][0m |           0.0051 |         215.0304 |          -5.3637 |
[32m[20221214 14:13:04 @agent_ppo2.py:185][0m |           0.0112 |         230.2640 |          -5.3585 |
[32m[20221214 14:13:04 @agent_ppo2.py:185][0m |           0.0029 |         210.1714 |          -5.3725 |
[32m[20221214 14:13:04 @agent_ppo2.py:185][0m |          -0.0010 |         207.6163 |          -5.4079 |
[32m[20221214 14:13:04 @agent_ppo2.py:185][0m |           0.0060 |         212.9782 |          -5.3914 |
[32m[20221214 14:13:04 @agent_ppo2.py:185][0m |          -0.0024 |         207.3330 |          -5.4223 |
[32m[20221214 14:13:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:13:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.89
[32m[20221214 14:13:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.25
[32m[20221214 14:13:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.14
[32m[20221214 14:13:05 @agent_ppo2.py:143][0m Total time:      15.04 min
[32m[20221214 14:13:05 @agent_ppo2.py:145][0m 1380352 total steps have happened
[32m[20221214 14:13:05 @agent_ppo2.py:121][0m #------------------------ Iteration 674 --------------------------#
[32m[20221214 14:13:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:13:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:05 @agent_ppo2.py:185][0m |           0.0280 |         267.0735 |          -5.5918 |
[32m[20221214 14:13:05 @agent_ppo2.py:185][0m |          -0.0002 |         210.6503 |          -5.5715 |
[32m[20221214 14:13:05 @agent_ppo2.py:185][0m |           0.0079 |         223.0845 |          -5.6089 |
[32m[20221214 14:13:05 @agent_ppo2.py:185][0m |          -0.0005 |         206.3011 |          -5.5407 |
[32m[20221214 14:13:05 @agent_ppo2.py:185][0m |          -0.0025 |         205.7819 |          -5.5600 |
[32m[20221214 14:13:05 @agent_ppo2.py:185][0m |          -0.0014 |         204.9827 |          -5.5226 |
[32m[20221214 14:13:05 @agent_ppo2.py:185][0m |           0.0009 |         205.1426 |          -5.5128 |
[32m[20221214 14:13:06 @agent_ppo2.py:185][0m |          -0.0019 |         205.0615 |          -5.5574 |
[32m[20221214 14:13:06 @agent_ppo2.py:185][0m |           0.0080 |         220.9072 |          -5.5668 |
[32m[20221214 14:13:06 @agent_ppo2.py:185][0m |          -0.0033 |         204.5024 |          -5.5296 |
[32m[20221214 14:13:06 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:13:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.03
[32m[20221214 14:13:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.88
[32m[20221214 14:13:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.26
[32m[20221214 14:13:06 @agent_ppo2.py:143][0m Total time:      15.06 min
[32m[20221214 14:13:06 @agent_ppo2.py:145][0m 1382400 total steps have happened
[32m[20221214 14:13:06 @agent_ppo2.py:121][0m #------------------------ Iteration 675 --------------------------#
[32m[20221214 14:13:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:13:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:06 @agent_ppo2.py:185][0m |           0.0005 |         212.6030 |          -5.7315 |
[32m[20221214 14:13:06 @agent_ppo2.py:185][0m |           0.0077 |         219.0687 |          -5.7072 |
[32m[20221214 14:13:06 @agent_ppo2.py:185][0m |          -0.0018 |         208.9948 |          -5.7848 |
[32m[20221214 14:13:06 @agent_ppo2.py:185][0m |          -0.0031 |         207.5187 |          -5.7546 |
[32m[20221214 14:13:06 @agent_ppo2.py:185][0m |          -0.0005 |         208.5344 |          -5.8499 |
[32m[20221214 14:13:07 @agent_ppo2.py:185][0m |           0.0098 |         226.0983 |          -5.8250 |
[32m[20221214 14:13:07 @agent_ppo2.py:185][0m |          -0.0019 |         206.3763 |          -5.7681 |
[32m[20221214 14:13:07 @agent_ppo2.py:185][0m |          -0.0048 |         205.6701 |          -5.9514 |
[32m[20221214 14:13:07 @agent_ppo2.py:185][0m |          -0.0043 |         205.7633 |          -5.9830 |
[32m[20221214 14:13:07 @agent_ppo2.py:185][0m |          -0.0029 |         205.0772 |          -6.0388 |
[32m[20221214 14:13:07 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:13:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.57
[32m[20221214 14:13:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.94
[32m[20221214 14:13:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.83
[32m[20221214 14:13:07 @agent_ppo2.py:143][0m Total time:      15.08 min
[32m[20221214 14:13:07 @agent_ppo2.py:145][0m 1384448 total steps have happened
[32m[20221214 14:13:07 @agent_ppo2.py:121][0m #------------------------ Iteration 676 --------------------------#
[32m[20221214 14:13:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:13:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:07 @agent_ppo2.py:185][0m |          -0.0024 |         199.5303 |          -5.9251 |
[32m[20221214 14:13:07 @agent_ppo2.py:185][0m |          -0.0023 |         191.8854 |          -5.8706 |
[32m[20221214 14:13:08 @agent_ppo2.py:185][0m |          -0.0041 |         187.3486 |          -5.8727 |
[32m[20221214 14:13:08 @agent_ppo2.py:185][0m |           0.0015 |         185.8862 |          -5.8535 |
[32m[20221214 14:13:08 @agent_ppo2.py:185][0m |           0.0003 |         183.3297 |          -5.8844 |
[32m[20221214 14:13:08 @agent_ppo2.py:185][0m |          -0.0027 |         182.0114 |          -5.8934 |
[32m[20221214 14:13:08 @agent_ppo2.py:185][0m |          -0.0011 |         180.4084 |          -5.8336 |
[32m[20221214 14:13:08 @agent_ppo2.py:185][0m |          -0.0019 |         179.7720 |          -5.8682 |
[32m[20221214 14:13:08 @agent_ppo2.py:185][0m |          -0.0015 |         179.1557 |          -5.8939 |
[32m[20221214 14:13:08 @agent_ppo2.py:185][0m |          -0.0022 |         178.8582 |          -5.9492 |
[32m[20221214 14:13:08 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:13:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.50
[32m[20221214 14:13:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.07
[32m[20221214 14:13:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.50
[32m[20221214 14:13:08 @agent_ppo2.py:143][0m Total time:      15.10 min
[32m[20221214 14:13:08 @agent_ppo2.py:145][0m 1386496 total steps have happened
[32m[20221214 14:13:08 @agent_ppo2.py:121][0m #------------------------ Iteration 677 --------------------------#
[32m[20221214 14:13:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:13:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:09 @agent_ppo2.py:185][0m |          -0.0004 |         210.1164 |          -5.6040 |
[32m[20221214 14:13:09 @agent_ppo2.py:185][0m |           0.0024 |         209.8716 |          -5.6143 |
[32m[20221214 14:13:09 @agent_ppo2.py:185][0m |          -0.0021 |         209.1358 |          -5.5084 |
[32m[20221214 14:13:09 @agent_ppo2.py:185][0m |          -0.0015 |         207.6817 |          -5.5412 |
[32m[20221214 14:13:09 @agent_ppo2.py:185][0m |          -0.0017 |         207.4324 |          -5.5586 |
[32m[20221214 14:13:09 @agent_ppo2.py:185][0m |          -0.0022 |         207.0126 |          -5.5395 |
[32m[20221214 14:13:09 @agent_ppo2.py:185][0m |          -0.0004 |         207.2619 |          -5.5501 |
[32m[20221214 14:13:09 @agent_ppo2.py:185][0m |          -0.0008 |         207.2792 |          -5.5073 |
[32m[20221214 14:13:09 @agent_ppo2.py:185][0m |          -0.0028 |         206.5181 |          -5.5499 |
[32m[20221214 14:13:09 @agent_ppo2.py:185][0m |           0.0023 |         210.9854 |          -5.5316 |
[32m[20221214 14:13:09 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:13:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.49
[32m[20221214 14:13:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.59
[32m[20221214 14:13:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.30
[32m[20221214 14:13:10 @agent_ppo2.py:143][0m Total time:      15.12 min
[32m[20221214 14:13:10 @agent_ppo2.py:145][0m 1388544 total steps have happened
[32m[20221214 14:13:10 @agent_ppo2.py:121][0m #------------------------ Iteration 678 --------------------------#
[32m[20221214 14:13:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:13:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:10 @agent_ppo2.py:185][0m |          -0.0026 |         199.5854 |          -5.9036 |
[32m[20221214 14:13:10 @agent_ppo2.py:185][0m |          -0.0009 |         196.6068 |          -5.8963 |
[32m[20221214 14:13:10 @agent_ppo2.py:185][0m |          -0.0016 |         195.5071 |          -5.9569 |
[32m[20221214 14:13:10 @agent_ppo2.py:185][0m |          -0.0013 |         194.7584 |          -5.9704 |
[32m[20221214 14:13:10 @agent_ppo2.py:185][0m |           0.0011 |         193.8809 |          -6.0217 |
[32m[20221214 14:13:10 @agent_ppo2.py:185][0m |          -0.0022 |         193.0059 |          -6.0375 |
[32m[20221214 14:13:10 @agent_ppo2.py:185][0m |          -0.0001 |         192.5312 |          -6.0891 |
[32m[20221214 14:13:11 @agent_ppo2.py:185][0m |          -0.0017 |         191.8695 |          -6.1278 |
[32m[20221214 14:13:11 @agent_ppo2.py:185][0m |           0.0030 |         193.6314 |          -6.1729 |
[32m[20221214 14:13:11 @agent_ppo2.py:185][0m |          -0.0041 |         191.1151 |          -6.2119 |
[32m[20221214 14:13:11 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:13:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.72
[32m[20221214 14:13:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.30
[32m[20221214 14:13:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.38
[32m[20221214 14:13:11 @agent_ppo2.py:143][0m Total time:      15.15 min
[32m[20221214 14:13:11 @agent_ppo2.py:145][0m 1390592 total steps have happened
[32m[20221214 14:13:11 @agent_ppo2.py:121][0m #------------------------ Iteration 679 --------------------------#
[32m[20221214 14:13:11 @agent_ppo2.py:127][0m Sampling time: 0.26 s by 5 slaves
[32m[20221214 14:13:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:11 @agent_ppo2.py:185][0m |          -0.0032 |         198.8189 |          -5.8328 |
[32m[20221214 14:13:11 @agent_ppo2.py:185][0m |          -0.0017 |         193.3067 |          -5.8474 |
[32m[20221214 14:13:12 @agent_ppo2.py:185][0m |           0.0103 |         196.6813 |          -5.9213 |
[32m[20221214 14:13:12 @agent_ppo2.py:185][0m |          -0.0011 |         188.5413 |          -6.0401 |
[32m[20221214 14:13:12 @agent_ppo2.py:185][0m |          -0.0036 |         187.8519 |          -5.9111 |
[32m[20221214 14:13:12 @agent_ppo2.py:185][0m |          -0.0017 |         186.8434 |          -5.9789 |
[32m[20221214 14:13:12 @agent_ppo2.py:185][0m |          -0.0052 |         186.5740 |          -5.9799 |
[32m[20221214 14:13:12 @agent_ppo2.py:185][0m |           0.0115 |         211.6788 |          -5.9726 |
[32m[20221214 14:13:12 @agent_ppo2.py:185][0m |           0.0037 |         189.3628 |          -6.2859 |
[32m[20221214 14:13:12 @agent_ppo2.py:185][0m |           0.0074 |         198.8141 |          -6.0423 |
[32m[20221214 14:13:12 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:13:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.69
[32m[20221214 14:13:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.80
[32m[20221214 14:13:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.80
[32m[20221214 14:13:12 @agent_ppo2.py:143][0m Total time:      15.17 min
[32m[20221214 14:13:12 @agent_ppo2.py:145][0m 1392640 total steps have happened
[32m[20221214 14:13:12 @agent_ppo2.py:121][0m #------------------------ Iteration 680 --------------------------#
[32m[20221214 14:13:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:13:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:13 @agent_ppo2.py:185][0m |          -0.0028 |         195.7904 |          -6.5945 |
[32m[20221214 14:13:13 @agent_ppo2.py:185][0m |          -0.0054 |         186.7182 |          -6.5568 |
[32m[20221214 14:13:13 @agent_ppo2.py:185][0m |          -0.0072 |         183.1107 |          -6.5463 |
[32m[20221214 14:13:13 @agent_ppo2.py:185][0m |          -0.0040 |         181.2498 |          -6.6199 |
[32m[20221214 14:13:13 @agent_ppo2.py:185][0m |          -0.0052 |         180.3934 |          -6.6252 |
[32m[20221214 14:13:13 @agent_ppo2.py:185][0m |          -0.0056 |         178.7750 |          -6.6249 |
[32m[20221214 14:13:13 @agent_ppo2.py:185][0m |          -0.0054 |         177.8049 |          -6.6011 |
[32m[20221214 14:13:13 @agent_ppo2.py:185][0m |          -0.0038 |         177.2274 |          -6.5554 |
[32m[20221214 14:13:13 @agent_ppo2.py:185][0m |          -0.0059 |         175.9709 |          -6.6014 |
[32m[20221214 14:13:14 @agent_ppo2.py:185][0m |          -0.0060 |         175.6402 |          -6.6301 |
[32m[20221214 14:13:14 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:13:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.45
[32m[20221214 14:13:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.80
[32m[20221214 14:13:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.89
[32m[20221214 14:13:14 @agent_ppo2.py:143][0m Total time:      15.19 min
[32m[20221214 14:13:14 @agent_ppo2.py:145][0m 1394688 total steps have happened
[32m[20221214 14:13:14 @agent_ppo2.py:121][0m #------------------------ Iteration 681 --------------------------#
[32m[20221214 14:13:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:14 @agent_ppo2.py:185][0m |          -0.0020 |         212.6547 |          -6.2842 |
[32m[20221214 14:13:14 @agent_ppo2.py:185][0m |          -0.0012 |         209.2804 |          -6.1535 |
[32m[20221214 14:13:14 @agent_ppo2.py:185][0m |          -0.0041 |         208.2369 |          -6.2590 |
[32m[20221214 14:13:14 @agent_ppo2.py:185][0m |          -0.0031 |         207.4791 |          -6.2572 |
[32m[20221214 14:13:14 @agent_ppo2.py:185][0m |          -0.0028 |         204.8713 |          -6.3072 |
[32m[20221214 14:13:15 @agent_ppo2.py:185][0m |          -0.0029 |         204.4001 |          -6.2865 |
[32m[20221214 14:13:15 @agent_ppo2.py:185][0m |           0.0160 |         233.6521 |          -6.2571 |
[32m[20221214 14:13:15 @agent_ppo2.py:185][0m |          -0.0007 |         201.1880 |          -6.4089 |
[32m[20221214 14:13:15 @agent_ppo2.py:185][0m |          -0.0021 |         200.0373 |          -6.3701 |
[32m[20221214 14:13:15 @agent_ppo2.py:185][0m |           0.0005 |         201.3153 |          -6.3906 |
[32m[20221214 14:13:15 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:13:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.36
[32m[20221214 14:13:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.16
[32m[20221214 14:13:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.15
[32m[20221214 14:13:15 @agent_ppo2.py:143][0m Total time:      15.22 min
[32m[20221214 14:13:15 @agent_ppo2.py:145][0m 1396736 total steps have happened
[32m[20221214 14:13:15 @agent_ppo2.py:121][0m #------------------------ Iteration 682 --------------------------#
[32m[20221214 14:13:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:15 @agent_ppo2.py:185][0m |          -0.0014 |         246.6782 |          -6.4557 |
[32m[20221214 14:13:16 @agent_ppo2.py:185][0m |          -0.0012 |         239.7939 |          -6.4862 |
[32m[20221214 14:13:16 @agent_ppo2.py:185][0m |          -0.0031 |         237.6060 |          -6.5159 |
[32m[20221214 14:13:16 @agent_ppo2.py:185][0m |           0.0007 |         237.0061 |          -6.5320 |
[32m[20221214 14:13:16 @agent_ppo2.py:185][0m |           0.0027 |         238.2485 |          -6.6211 |
[32m[20221214 14:13:16 @agent_ppo2.py:185][0m |           0.0174 |         260.8554 |          -6.6507 |
[32m[20221214 14:13:16 @agent_ppo2.py:185][0m |           0.0054 |         247.1433 |          -6.6601 |
[32m[20221214 14:13:16 @agent_ppo2.py:185][0m |          -0.0023 |         233.4758 |          -6.6613 |
[32m[20221214 14:13:16 @agent_ppo2.py:185][0m |          -0.0024 |         233.1331 |          -6.7096 |
[32m[20221214 14:13:16 @agent_ppo2.py:185][0m |           0.0013 |         235.3796 |          -6.7510 |
[32m[20221214 14:13:16 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:13:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.81
[32m[20221214 14:13:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.53
[32m[20221214 14:13:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.19
[32m[20221214 14:13:16 @agent_ppo2.py:143][0m Total time:      15.24 min
[32m[20221214 14:13:16 @agent_ppo2.py:145][0m 1398784 total steps have happened
[32m[20221214 14:13:16 @agent_ppo2.py:121][0m #------------------------ Iteration 683 --------------------------#
[32m[20221214 14:13:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:17 @agent_ppo2.py:185][0m |          -0.0021 |         236.8593 |          -7.1228 |
[32m[20221214 14:13:17 @agent_ppo2.py:185][0m |           0.0027 |         238.4135 |          -7.0841 |
[32m[20221214 14:13:17 @agent_ppo2.py:185][0m |          -0.0029 |         233.1571 |          -7.0663 |
[32m[20221214 14:13:17 @agent_ppo2.py:185][0m |          -0.0006 |         231.1663 |          -7.1476 |
[32m[20221214 14:13:17 @agent_ppo2.py:185][0m |          -0.0044 |         231.0716 |          -7.1550 |
[32m[20221214 14:13:17 @agent_ppo2.py:185][0m |           0.0054 |         239.7175 |          -7.1673 |
[32m[20221214 14:13:17 @agent_ppo2.py:185][0m |          -0.0036 |         229.6271 |          -7.1811 |
[32m[20221214 14:13:17 @agent_ppo2.py:185][0m |           0.0073 |         248.7327 |          -7.1529 |
[32m[20221214 14:13:17 @agent_ppo2.py:185][0m |          -0.0011 |         229.2798 |          -7.2996 |
[32m[20221214 14:13:17 @agent_ppo2.py:185][0m |          -0.0017 |         228.8821 |          -7.2534 |
[32m[20221214 14:13:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:13:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.94
[32m[20221214 14:13:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.01
[32m[20221214 14:13:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.63
[32m[20221214 14:13:18 @agent_ppo2.py:143][0m Total time:      15.26 min
[32m[20221214 14:13:18 @agent_ppo2.py:145][0m 1400832 total steps have happened
[32m[20221214 14:13:18 @agent_ppo2.py:121][0m #------------------------ Iteration 684 --------------------------#
[32m[20221214 14:13:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:13:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:18 @agent_ppo2.py:185][0m |           0.0001 |         232.1639 |          -7.3634 |
[32m[20221214 14:13:18 @agent_ppo2.py:185][0m |          -0.0023 |         226.3118 |          -7.3800 |
[32m[20221214 14:13:18 @agent_ppo2.py:185][0m |          -0.0015 |         224.3641 |          -7.4211 |
[32m[20221214 14:13:18 @agent_ppo2.py:185][0m |           0.0067 |         234.0659 |          -7.4062 |
[32m[20221214 14:13:18 @agent_ppo2.py:185][0m |          -0.0022 |         221.7118 |          -7.4395 |
[32m[20221214 14:13:18 @agent_ppo2.py:185][0m |          -0.0021 |         220.8596 |          -7.4437 |
[32m[20221214 14:13:19 @agent_ppo2.py:185][0m |          -0.0018 |         220.1808 |          -7.4275 |
[32m[20221214 14:13:19 @agent_ppo2.py:185][0m |           0.0080 |         234.4552 |          -7.4623 |
[32m[20221214 14:13:19 @agent_ppo2.py:185][0m |          -0.0027 |         219.4468 |          -7.4562 |
[32m[20221214 14:13:19 @agent_ppo2.py:185][0m |          -0.0017 |         218.7964 |          -7.4699 |
[32m[20221214 14:13:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:13:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.07
[32m[20221214 14:13:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.25
[32m[20221214 14:13:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.42
[32m[20221214 14:13:19 @agent_ppo2.py:143][0m Total time:      15.28 min
[32m[20221214 14:13:19 @agent_ppo2.py:145][0m 1402880 total steps have happened
[32m[20221214 14:13:19 @agent_ppo2.py:121][0m #------------------------ Iteration 685 --------------------------#
[32m[20221214 14:13:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:19 @agent_ppo2.py:185][0m |          -0.0022 |         248.2198 |          -7.6807 |
[32m[20221214 14:13:19 @agent_ppo2.py:185][0m |          -0.0011 |         245.5695 |          -7.6018 |
[32m[20221214 14:13:19 @agent_ppo2.py:185][0m |          -0.0029 |         242.5685 |          -7.6223 |
[32m[20221214 14:13:20 @agent_ppo2.py:185][0m |          -0.0016 |         240.9165 |          -7.5561 |
[32m[20221214 14:13:20 @agent_ppo2.py:185][0m |          -0.0014 |         240.0314 |          -7.5940 |
[32m[20221214 14:13:20 @agent_ppo2.py:185][0m |           0.0006 |         240.1394 |          -7.5085 |
[32m[20221214 14:13:20 @agent_ppo2.py:185][0m |           0.0102 |         249.4405 |          -7.5075 |
[32m[20221214 14:13:20 @agent_ppo2.py:185][0m |          -0.0001 |         238.7058 |          -7.5917 |
[32m[20221214 14:13:20 @agent_ppo2.py:185][0m |          -0.0023 |         237.6838 |          -7.5212 |
[32m[20221214 14:13:20 @agent_ppo2.py:185][0m |          -0.0017 |         237.5502 |          -7.4344 |
[32m[20221214 14:13:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:13:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.07
[32m[20221214 14:13:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.53
[32m[20221214 14:13:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.61
[32m[20221214 14:13:20 @agent_ppo2.py:143][0m Total time:      15.30 min
[32m[20221214 14:13:20 @agent_ppo2.py:145][0m 1404928 total steps have happened
[32m[20221214 14:13:20 @agent_ppo2.py:121][0m #------------------------ Iteration 686 --------------------------#
[32m[20221214 14:13:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:21 @agent_ppo2.py:185][0m |           0.0051 |         226.7609 |          -7.3050 |
[32m[20221214 14:13:21 @agent_ppo2.py:185][0m |          -0.0028 |         217.8267 |          -7.2395 |
[32m[20221214 14:13:21 @agent_ppo2.py:185][0m |           0.0041 |         221.0096 |          -7.2904 |
[32m[20221214 14:13:21 @agent_ppo2.py:185][0m |          -0.0011 |         215.4376 |          -7.3446 |
[32m[20221214 14:13:21 @agent_ppo2.py:185][0m |           0.0115 |         232.2934 |          -7.3542 |
[32m[20221214 14:13:21 @agent_ppo2.py:185][0m |          -0.0016 |         213.6359 |          -7.3792 |
[32m[20221214 14:13:21 @agent_ppo2.py:185][0m |           0.0047 |         217.3161 |          -7.4024 |
[32m[20221214 14:13:21 @agent_ppo2.py:185][0m |          -0.0036 |         213.5405 |          -7.4162 |
[32m[20221214 14:13:21 @agent_ppo2.py:185][0m |          -0.0021 |         212.1759 |          -7.4503 |
[32m[20221214 14:13:21 @agent_ppo2.py:185][0m |           0.0071 |         224.1798 |          -7.4347 |
[32m[20221214 14:13:21 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:13:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.18
[32m[20221214 14:13:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.45
[32m[20221214 14:13:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.42
[32m[20221214 14:13:22 @agent_ppo2.py:143][0m Total time:      15.33 min
[32m[20221214 14:13:22 @agent_ppo2.py:145][0m 1406976 total steps have happened
[32m[20221214 14:13:22 @agent_ppo2.py:121][0m #------------------------ Iteration 687 --------------------------#
[32m[20221214 14:13:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:22 @agent_ppo2.py:185][0m |           0.0005 |         211.1408 |          -8.1554 |
[32m[20221214 14:13:22 @agent_ppo2.py:185][0m |          -0.0018 |         206.0847 |          -8.1484 |
[32m[20221214 14:13:22 @agent_ppo2.py:185][0m |           0.0006 |         205.0625 |          -8.1797 |
[32m[20221214 14:13:22 @agent_ppo2.py:185][0m |          -0.0003 |         201.6710 |          -8.1368 |
[32m[20221214 14:13:22 @agent_ppo2.py:185][0m |          -0.0017 |         200.9959 |          -8.1358 |
[32m[20221214 14:13:22 @agent_ppo2.py:185][0m |           0.0028 |         201.8852 |          -8.0415 |
[32m[20221214 14:13:23 @agent_ppo2.py:185][0m |          -0.0012 |         198.4172 |          -8.0965 |
[32m[20221214 14:13:23 @agent_ppo2.py:185][0m |           0.0002 |         199.4556 |          -8.0532 |
[32m[20221214 14:13:23 @agent_ppo2.py:185][0m |          -0.0016 |         198.5471 |          -8.0652 |
[32m[20221214 14:13:23 @agent_ppo2.py:185][0m |          -0.0005 |         198.1744 |          -8.0002 |
[32m[20221214 14:13:23 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:13:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.12
[32m[20221214 14:13:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.50
[32m[20221214 14:13:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.93
[32m[20221214 14:13:23 @agent_ppo2.py:143][0m Total time:      15.35 min
[32m[20221214 14:13:23 @agent_ppo2.py:145][0m 1409024 total steps have happened
[32m[20221214 14:13:23 @agent_ppo2.py:121][0m #------------------------ Iteration 688 --------------------------#
[32m[20221214 14:13:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:23 @agent_ppo2.py:185][0m |          -0.0036 |         216.6207 |          -7.0229 |
[32m[20221214 14:13:23 @agent_ppo2.py:185][0m |          -0.0001 |         215.4051 |          -6.9544 |
[32m[20221214 14:13:23 @agent_ppo2.py:185][0m |          -0.0038 |         213.4346 |          -7.0232 |
[32m[20221214 14:13:24 @agent_ppo2.py:185][0m |          -0.0036 |         213.6445 |          -6.9994 |
[32m[20221214 14:13:24 @agent_ppo2.py:185][0m |          -0.0037 |         212.8869 |          -6.9997 |
[32m[20221214 14:13:24 @agent_ppo2.py:185][0m |          -0.0002 |         214.7684 |          -7.0158 |
[32m[20221214 14:13:24 @agent_ppo2.py:185][0m |          -0.0016 |         211.7228 |          -6.9997 |
[32m[20221214 14:13:24 @agent_ppo2.py:185][0m |          -0.0035 |         212.1820 |          -7.0406 |
[32m[20221214 14:13:24 @agent_ppo2.py:185][0m |          -0.0033 |         211.5297 |          -7.0876 |
[32m[20221214 14:13:24 @agent_ppo2.py:185][0m |          -0.0060 |         211.3149 |          -7.0807 |
[32m[20221214 14:13:24 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:13:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.11
[32m[20221214 14:13:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.19
[32m[20221214 14:13:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.21
[32m[20221214 14:13:24 @agent_ppo2.py:143][0m Total time:      15.37 min
[32m[20221214 14:13:24 @agent_ppo2.py:145][0m 1411072 total steps have happened
[32m[20221214 14:13:24 @agent_ppo2.py:121][0m #------------------------ Iteration 689 --------------------------#
[32m[20221214 14:13:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:25 @agent_ppo2.py:185][0m |          -0.0020 |         251.8858 |          -7.8320 |
[32m[20221214 14:13:25 @agent_ppo2.py:185][0m |          -0.0024 |         250.8786 |          -7.9024 |
[32m[20221214 14:13:25 @agent_ppo2.py:185][0m |          -0.0018 |         249.3335 |          -7.9202 |
[32m[20221214 14:13:25 @agent_ppo2.py:185][0m |          -0.0031 |         247.9700 |          -7.9582 |
[32m[20221214 14:13:25 @agent_ppo2.py:185][0m |          -0.0026 |         247.2699 |          -8.0132 |
[32m[20221214 14:13:25 @agent_ppo2.py:185][0m |          -0.0027 |         245.9705 |          -8.0597 |
[32m[20221214 14:13:25 @agent_ppo2.py:185][0m |          -0.0036 |         245.9469 |          -8.0916 |
[32m[20221214 14:13:25 @agent_ppo2.py:185][0m |          -0.0027 |         245.2610 |          -8.0677 |
[32m[20221214 14:13:25 @agent_ppo2.py:185][0m |          -0.0029 |         245.0045 |          -8.1205 |
[32m[20221214 14:13:26 @agent_ppo2.py:185][0m |          -0.0024 |         244.3722 |          -8.1463 |
[32m[20221214 14:13:26 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:13:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.11
[32m[20221214 14:13:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.84
[32m[20221214 14:13:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.95
[32m[20221214 14:13:26 @agent_ppo2.py:143][0m Total time:      15.39 min
[32m[20221214 14:13:26 @agent_ppo2.py:145][0m 1413120 total steps have happened
[32m[20221214 14:13:26 @agent_ppo2.py:121][0m #------------------------ Iteration 690 --------------------------#
[32m[20221214 14:13:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:13:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:26 @agent_ppo2.py:185][0m |          -0.0023 |         239.3717 |          -7.8387 |
[32m[20221214 14:13:26 @agent_ppo2.py:185][0m |           0.0004 |         232.6179 |          -7.8910 |
[32m[20221214 14:13:26 @agent_ppo2.py:185][0m |          -0.0022 |         230.8140 |          -7.7641 |
[32m[20221214 14:13:26 @agent_ppo2.py:185][0m |           0.0059 |         241.9489 |          -7.8403 |
[32m[20221214 14:13:26 @agent_ppo2.py:185][0m |          -0.0031 |         228.0448 |          -7.7921 |
[32m[20221214 14:13:27 @agent_ppo2.py:185][0m |          -0.0012 |         227.2126 |          -7.8151 |
[32m[20221214 14:13:27 @agent_ppo2.py:185][0m |          -0.0020 |         227.3723 |          -7.8489 |
[32m[20221214 14:13:27 @agent_ppo2.py:185][0m |           0.0104 |         245.9986 |          -7.8672 |
[32m[20221214 14:13:27 @agent_ppo2.py:185][0m |          -0.0017 |         226.7197 |          -7.9894 |
[32m[20221214 14:13:27 @agent_ppo2.py:185][0m |           0.0097 |         232.4002 |          -7.9547 |
[32m[20221214 14:13:27 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:13:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.55
[32m[20221214 14:13:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.85
[32m[20221214 14:13:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.86
[32m[20221214 14:13:27 @agent_ppo2.py:143][0m Total time:      15.42 min
[32m[20221214 14:13:27 @agent_ppo2.py:145][0m 1415168 total steps have happened
[32m[20221214 14:13:27 @agent_ppo2.py:121][0m #------------------------ Iteration 691 --------------------------#
[32m[20221214 14:13:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:27 @agent_ppo2.py:185][0m |          -0.0021 |         267.5685 |          -7.9089 |
[32m[20221214 14:13:27 @agent_ppo2.py:185][0m |          -0.0019 |         262.7569 |          -7.8608 |
[32m[20221214 14:13:28 @agent_ppo2.py:185][0m |          -0.0012 |         261.4653 |          -7.9167 |
[32m[20221214 14:13:28 @agent_ppo2.py:185][0m |          -0.0020 |         259.9164 |          -7.8933 |
[32m[20221214 14:13:28 @agent_ppo2.py:185][0m |           0.0007 |         258.8512 |          -7.9697 |
[32m[20221214 14:13:28 @agent_ppo2.py:185][0m |          -0.0027 |         257.7884 |          -7.9935 |
[32m[20221214 14:13:28 @agent_ppo2.py:185][0m |          -0.0002 |         257.5388 |          -7.9864 |
[32m[20221214 14:13:28 @agent_ppo2.py:185][0m |           0.0017 |         260.8506 |          -8.0073 |
[32m[20221214 14:13:28 @agent_ppo2.py:185][0m |          -0.0028 |         256.3411 |          -8.0234 |
[32m[20221214 14:13:28 @agent_ppo2.py:185][0m |           0.0002 |         256.1276 |          -8.0648 |
[32m[20221214 14:13:28 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:13:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.76
[32m[20221214 14:13:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.37
[32m[20221214 14:13:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.83
[32m[20221214 14:13:28 @agent_ppo2.py:143][0m Total time:      15.44 min
[32m[20221214 14:13:28 @agent_ppo2.py:145][0m 1417216 total steps have happened
[32m[20221214 14:13:28 @agent_ppo2.py:121][0m #------------------------ Iteration 692 --------------------------#
[32m[20221214 14:13:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:29 @agent_ppo2.py:185][0m |          -0.0047 |         255.0246 |          -7.5183 |
[32m[20221214 14:13:29 @agent_ppo2.py:185][0m |          -0.0057 |         252.6019 |          -7.5201 |
[32m[20221214 14:13:29 @agent_ppo2.py:185][0m |           0.0053 |         261.7947 |          -7.5371 |
[32m[20221214 14:13:29 @agent_ppo2.py:185][0m |          -0.0061 |         250.9600 |          -7.5432 |
[32m[20221214 14:13:29 @agent_ppo2.py:185][0m |          -0.0061 |         250.0749 |          -7.5571 |
[32m[20221214 14:13:29 @agent_ppo2.py:185][0m |          -0.0059 |         249.2920 |          -7.4900 |
[32m[20221214 14:13:29 @agent_ppo2.py:185][0m |          -0.0047 |         249.4643 |          -7.5745 |
[32m[20221214 14:13:29 @agent_ppo2.py:185][0m |          -0.0063 |         248.8142 |          -7.5614 |
[32m[20221214 14:13:29 @agent_ppo2.py:185][0m |           0.0008 |         253.4865 |          -7.6149 |
[32m[20221214 14:13:30 @agent_ppo2.py:185][0m |          -0.0036 |         249.5839 |          -7.5922 |
[32m[20221214 14:13:30 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:13:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.16
[32m[20221214 14:13:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.20
[32m[20221214 14:13:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.09
[32m[20221214 14:13:30 @agent_ppo2.py:143][0m Total time:      15.46 min
[32m[20221214 14:13:30 @agent_ppo2.py:145][0m 1419264 total steps have happened
[32m[20221214 14:13:30 @agent_ppo2.py:121][0m #------------------------ Iteration 693 --------------------------#
[32m[20221214 14:13:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:13:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:30 @agent_ppo2.py:185][0m |           0.0116 |         277.4917 |          -7.7719 |
[32m[20221214 14:13:30 @agent_ppo2.py:185][0m |          -0.0026 |         238.4172 |          -7.6149 |
[32m[20221214 14:13:30 @agent_ppo2.py:185][0m |           0.0009 |         236.1426 |          -7.6738 |
[32m[20221214 14:13:30 @agent_ppo2.py:185][0m |           0.0203 |         260.2334 |          -7.6997 |
[32m[20221214 14:13:30 @agent_ppo2.py:185][0m |          -0.0031 |         232.0959 |          -7.8239 |
[32m[20221214 14:13:31 @agent_ppo2.py:185][0m |          -0.0034 |         231.2910 |          -7.6794 |
[32m[20221214 14:13:31 @agent_ppo2.py:185][0m |          -0.0032 |         230.2624 |          -7.7357 |
[32m[20221214 14:13:31 @agent_ppo2.py:185][0m |          -0.0028 |         230.0322 |          -7.8044 |
[32m[20221214 14:13:31 @agent_ppo2.py:185][0m |          -0.0032 |         228.9462 |          -7.7671 |
[32m[20221214 14:13:31 @agent_ppo2.py:185][0m |          -0.0051 |         228.5000 |          -7.7620 |
[32m[20221214 14:13:31 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:13:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.54
[32m[20221214 14:13:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.65
[32m[20221214 14:13:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 801.60
[32m[20221214 14:13:31 @agent_ppo2.py:143][0m Total time:      15.48 min
[32m[20221214 14:13:31 @agent_ppo2.py:145][0m 1421312 total steps have happened
[32m[20221214 14:13:31 @agent_ppo2.py:121][0m #------------------------ Iteration 694 --------------------------#
[32m[20221214 14:13:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:31 @agent_ppo2.py:185][0m |          -0.0014 |         273.9522 |          -7.6613 |
[32m[20221214 14:13:32 @agent_ppo2.py:185][0m |          -0.0013 |         270.8459 |          -7.6676 |
[32m[20221214 14:13:32 @agent_ppo2.py:185][0m |          -0.0004 |         269.9838 |          -7.7023 |
[32m[20221214 14:13:32 @agent_ppo2.py:185][0m |          -0.0017 |         269.6318 |          -7.7251 |
[32m[20221214 14:13:32 @agent_ppo2.py:185][0m |           0.0013 |         269.2897 |          -7.7397 |
[32m[20221214 14:13:32 @agent_ppo2.py:185][0m |          -0.0014 |         268.2495 |          -7.7840 |
[32m[20221214 14:13:32 @agent_ppo2.py:185][0m |          -0.0024 |         267.9421 |          -7.7822 |
[32m[20221214 14:13:32 @agent_ppo2.py:185][0m |          -0.0018 |         267.9743 |          -7.7721 |
[32m[20221214 14:13:32 @agent_ppo2.py:185][0m |          -0.0019 |         267.3090 |          -7.8032 |
[32m[20221214 14:13:32 @agent_ppo2.py:185][0m |          -0.0023 |         266.8311 |          -7.8233 |
[32m[20221214 14:13:32 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:13:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.52
[32m[20221214 14:13:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.03
[32m[20221214 14:13:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.68
[32m[20221214 14:13:32 @agent_ppo2.py:143][0m Total time:      15.50 min
[32m[20221214 14:13:32 @agent_ppo2.py:145][0m 1423360 total steps have happened
[32m[20221214 14:13:32 @agent_ppo2.py:121][0m #------------------------ Iteration 695 --------------------------#
[32m[20221214 14:13:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:33 @agent_ppo2.py:185][0m |          -0.0021 |         263.7114 |          -8.9928 |
[32m[20221214 14:13:33 @agent_ppo2.py:185][0m |          -0.0013 |         260.9456 |          -8.9620 |
[32m[20221214 14:13:33 @agent_ppo2.py:185][0m |          -0.0012 |         259.4715 |          -8.9342 |
[32m[20221214 14:13:33 @agent_ppo2.py:185][0m |          -0.0018 |         258.6050 |          -8.9642 |
[32m[20221214 14:13:33 @agent_ppo2.py:185][0m |           0.0118 |         292.0430 |          -8.9453 |
[32m[20221214 14:13:33 @agent_ppo2.py:185][0m |           0.0004 |         257.5704 |          -8.9344 |
[32m[20221214 14:13:33 @agent_ppo2.py:185][0m |          -0.0028 |         256.9951 |          -8.9510 |
[32m[20221214 14:13:33 @agent_ppo2.py:185][0m |          -0.0027 |         257.6246 |          -8.9317 |
[32m[20221214 14:13:33 @agent_ppo2.py:185][0m |          -0.0020 |         256.9366 |          -9.0009 |
[32m[20221214 14:13:33 @agent_ppo2.py:185][0m |          -0.0022 |         257.4081 |          -8.9947 |
[32m[20221214 14:13:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:13:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.57
[32m[20221214 14:13:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.84
[32m[20221214 14:13:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.67
[32m[20221214 14:13:34 @agent_ppo2.py:143][0m Total time:      15.53 min
[32m[20221214 14:13:34 @agent_ppo2.py:145][0m 1425408 total steps have happened
[32m[20221214 14:13:34 @agent_ppo2.py:121][0m #------------------------ Iteration 696 --------------------------#
[32m[20221214 14:13:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:13:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:34 @agent_ppo2.py:185][0m |          -0.0041 |         249.0832 |          -8.8727 |
[32m[20221214 14:13:34 @agent_ppo2.py:185][0m |          -0.0054 |         245.4262 |          -8.7832 |
[32m[20221214 14:13:34 @agent_ppo2.py:185][0m |          -0.0065 |         243.3178 |          -8.8319 |
[32m[20221214 14:13:34 @agent_ppo2.py:185][0m |          -0.0054 |         243.5749 |          -8.9174 |
[32m[20221214 14:13:34 @agent_ppo2.py:185][0m |          -0.0021 |         244.1008 |          -8.8558 |
[32m[20221214 14:13:34 @agent_ppo2.py:185][0m |          -0.0052 |         242.2008 |          -8.9076 |
[32m[20221214 14:13:34 @agent_ppo2.py:185][0m |          -0.0060 |         242.0521 |          -8.9006 |
[32m[20221214 14:13:35 @agent_ppo2.py:185][0m |          -0.0056 |         241.7883 |          -8.8676 |
[32m[20221214 14:13:35 @agent_ppo2.py:185][0m |          -0.0045 |         241.0254 |          -8.8797 |
[32m[20221214 14:13:35 @agent_ppo2.py:185][0m |          -0.0058 |         240.8421 |          -8.8695 |
[32m[20221214 14:13:35 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:13:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.86
[32m[20221214 14:13:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.98
[32m[20221214 14:13:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.24
[32m[20221214 14:13:35 @agent_ppo2.py:143][0m Total time:      15.55 min
[32m[20221214 14:13:35 @agent_ppo2.py:145][0m 1427456 total steps have happened
[32m[20221214 14:13:35 @agent_ppo2.py:121][0m #------------------------ Iteration 697 --------------------------#
[32m[20221214 14:13:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:13:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:35 @agent_ppo2.py:185][0m |          -0.0029 |         245.2957 |          -8.7630 |
[32m[20221214 14:13:35 @agent_ppo2.py:185][0m |           0.0038 |         238.2924 |          -8.7316 |
[32m[20221214 14:13:35 @agent_ppo2.py:185][0m |          -0.0060 |         226.9774 |          -8.7199 |
[32m[20221214 14:13:35 @agent_ppo2.py:185][0m |          -0.0058 |         224.3044 |          -8.7502 |
[32m[20221214 14:13:36 @agent_ppo2.py:185][0m |          -0.0041 |         222.4440 |          -8.7470 |
[32m[20221214 14:13:36 @agent_ppo2.py:185][0m |          -0.0048 |         221.6469 |          -8.7425 |
[32m[20221214 14:13:36 @agent_ppo2.py:185][0m |          -0.0047 |         220.8719 |          -8.7053 |
[32m[20221214 14:13:36 @agent_ppo2.py:185][0m |          -0.0050 |         219.6968 |          -8.7515 |
[32m[20221214 14:13:36 @agent_ppo2.py:185][0m |          -0.0029 |         219.7120 |          -8.7889 |
[32m[20221214 14:13:36 @agent_ppo2.py:185][0m |          -0.0052 |         219.3238 |          -8.7271 |
[32m[20221214 14:13:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:13:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.25
[32m[20221214 14:13:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.35
[32m[20221214 14:13:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 786.99
[32m[20221214 14:13:36 @agent_ppo2.py:143][0m Total time:      15.57 min
[32m[20221214 14:13:36 @agent_ppo2.py:145][0m 1429504 total steps have happened
[32m[20221214 14:13:36 @agent_ppo2.py:121][0m #------------------------ Iteration 698 --------------------------#
[32m[20221214 14:13:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:36 @agent_ppo2.py:185][0m |           0.0031 |         257.3942 |          -8.4984 |
[32m[20221214 14:13:37 @agent_ppo2.py:185][0m |          -0.0004 |         248.8511 |          -8.4447 |
[32m[20221214 14:13:37 @agent_ppo2.py:185][0m |           0.0023 |         248.6080 |          -8.4412 |
[32m[20221214 14:13:37 @agent_ppo2.py:185][0m |           0.0071 |         254.2219 |          -8.2946 |
[32m[20221214 14:13:37 @agent_ppo2.py:185][0m |          -0.0019 |         244.2147 |          -8.3729 |
[32m[20221214 14:13:37 @agent_ppo2.py:185][0m |          -0.0017 |         243.1617 |          -8.3384 |
[32m[20221214 14:13:37 @agent_ppo2.py:185][0m |          -0.0014 |         242.6359 |          -8.3170 |
[32m[20221214 14:13:37 @agent_ppo2.py:185][0m |          -0.0033 |         242.4133 |          -8.3290 |
[32m[20221214 14:13:37 @agent_ppo2.py:185][0m |          -0.0029 |         242.4877 |          -8.2972 |
[32m[20221214 14:13:37 @agent_ppo2.py:185][0m |           0.0012 |         242.6779 |          -8.2870 |
[32m[20221214 14:13:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:13:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 770.44
[32m[20221214 14:13:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.64
[32m[20221214 14:13:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 786.45
[32m[20221214 14:13:37 @agent_ppo2.py:143][0m Total time:      15.59 min
[32m[20221214 14:13:37 @agent_ppo2.py:145][0m 1431552 total steps have happened
[32m[20221214 14:13:37 @agent_ppo2.py:121][0m #------------------------ Iteration 699 --------------------------#
[32m[20221214 14:13:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:38 @agent_ppo2.py:185][0m |          -0.0024 |         235.8884 |          -8.1536 |
[32m[20221214 14:13:38 @agent_ppo2.py:185][0m |          -0.0018 |         231.9063 |          -8.0816 |
[32m[20221214 14:13:38 @agent_ppo2.py:185][0m |          -0.0024 |         229.6074 |          -8.0383 |
[32m[20221214 14:13:38 @agent_ppo2.py:185][0m |          -0.0029 |         228.9170 |          -8.0276 |
[32m[20221214 14:13:38 @agent_ppo2.py:185][0m |          -0.0013 |         229.2221 |          -8.0186 |
[32m[20221214 14:13:38 @agent_ppo2.py:185][0m |          -0.0033 |         227.7951 |          -7.9735 |
[32m[20221214 14:13:38 @agent_ppo2.py:185][0m |          -0.0019 |         227.5244 |          -7.9531 |
[32m[20221214 14:13:39 @agent_ppo2.py:185][0m |          -0.0023 |         227.2165 |          -7.9158 |
[32m[20221214 14:13:39 @agent_ppo2.py:185][0m |          -0.0019 |         226.9946 |          -7.8998 |
[32m[20221214 14:13:39 @agent_ppo2.py:185][0m |          -0.0029 |         226.7213 |          -7.8535 |
[32m[20221214 14:13:39 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 14:13:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.36
[32m[20221214 14:13:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 773.40
[32m[20221214 14:13:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 783.67
[32m[20221214 14:13:39 @agent_ppo2.py:143][0m Total time:      15.61 min
[32m[20221214 14:13:39 @agent_ppo2.py:145][0m 1433600 total steps have happened
[32m[20221214 14:13:39 @agent_ppo2.py:121][0m #------------------------ Iteration 700 --------------------------#
[32m[20221214 14:13:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:13:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:39 @agent_ppo2.py:185][0m |          -0.0015 |         228.6612 |          -7.9382 |
[32m[20221214 14:13:39 @agent_ppo2.py:185][0m |          -0.0009 |         221.7601 |          -8.0295 |
[32m[20221214 14:13:39 @agent_ppo2.py:185][0m |          -0.0015 |         219.6577 |          -7.9447 |
[32m[20221214 14:13:40 @agent_ppo2.py:185][0m |          -0.0002 |         218.8565 |          -8.0555 |
[32m[20221214 14:13:40 @agent_ppo2.py:185][0m |          -0.0016 |         218.7910 |          -8.1116 |
[32m[20221214 14:13:40 @agent_ppo2.py:185][0m |           0.0101 |         242.4031 |          -8.1112 |
[32m[20221214 14:13:40 @agent_ppo2.py:185][0m |          -0.0015 |         217.7464 |          -8.1134 |
[32m[20221214 14:13:40 @agent_ppo2.py:185][0m |          -0.0012 |         218.1423 |          -8.0887 |
[32m[20221214 14:13:40 @agent_ppo2.py:185][0m |           0.0029 |         220.1374 |          -8.1429 |
[32m[20221214 14:13:40 @agent_ppo2.py:185][0m |          -0.0026 |         218.0570 |          -8.1828 |
[32m[20221214 14:13:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:13:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.47
[32m[20221214 14:13:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.83
[32m[20221214 14:13:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 778.39
[32m[20221214 14:13:40 @agent_ppo2.py:143][0m Total time:      15.64 min
[32m[20221214 14:13:40 @agent_ppo2.py:145][0m 1435648 total steps have happened
[32m[20221214 14:13:40 @agent_ppo2.py:121][0m #------------------------ Iteration 701 --------------------------#
[32m[20221214 14:13:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:41 @agent_ppo2.py:185][0m |          -0.0023 |         223.4767 |          -8.4824 |
[32m[20221214 14:13:41 @agent_ppo2.py:185][0m |          -0.0017 |         218.8356 |          -8.4018 |
[32m[20221214 14:13:41 @agent_ppo2.py:185][0m |          -0.0014 |         216.5765 |          -8.3600 |
[32m[20221214 14:13:41 @agent_ppo2.py:185][0m |          -0.0008 |         214.1613 |          -8.4357 |
[32m[20221214 14:13:41 @agent_ppo2.py:185][0m |          -0.0019 |         212.6227 |          -8.3472 |
[32m[20221214 14:13:41 @agent_ppo2.py:185][0m |          -0.0008 |         212.0418 |          -8.3257 |
[32m[20221214 14:13:41 @agent_ppo2.py:185][0m |          -0.0006 |         211.1485 |          -8.4719 |
[32m[20221214 14:13:41 @agent_ppo2.py:185][0m |           0.0009 |         210.9368 |          -8.3378 |
[32m[20221214 14:13:41 @agent_ppo2.py:185][0m |          -0.0028 |         210.5777 |          -8.4102 |
[32m[20221214 14:13:41 @agent_ppo2.py:185][0m |          -0.0027 |         210.4043 |          -8.3868 |
[32m[20221214 14:13:41 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:13:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 757.72
[32m[20221214 14:13:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.79
[32m[20221214 14:13:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.32
[32m[20221214 14:13:42 @agent_ppo2.py:143][0m Total time:      15.66 min
[32m[20221214 14:13:42 @agent_ppo2.py:145][0m 1437696 total steps have happened
[32m[20221214 14:13:42 @agent_ppo2.py:121][0m #------------------------ Iteration 702 --------------------------#
[32m[20221214 14:13:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:42 @agent_ppo2.py:185][0m |          -0.0018 |         204.5289 |          -8.3715 |
[32m[20221214 14:13:42 @agent_ppo2.py:185][0m |          -0.0021 |         199.6018 |          -8.3767 |
[32m[20221214 14:13:42 @agent_ppo2.py:185][0m |           0.0003 |         199.0962 |          -8.4032 |
[32m[20221214 14:13:42 @agent_ppo2.py:185][0m |          -0.0016 |         197.0414 |          -8.2168 |
[32m[20221214 14:13:42 @agent_ppo2.py:185][0m |          -0.0018 |         196.2520 |          -8.4012 |
[32m[20221214 14:13:42 @agent_ppo2.py:185][0m |          -0.0004 |         195.9858 |          -8.3746 |
[32m[20221214 14:13:42 @agent_ppo2.py:185][0m |           0.0151 |         221.6953 |          -8.3999 |
[32m[20221214 14:13:43 @agent_ppo2.py:185][0m |           0.0053 |         209.3108 |          -8.3940 |
[32m[20221214 14:13:43 @agent_ppo2.py:185][0m |           0.0045 |         195.3982 |          -8.0698 |
[32m[20221214 14:13:43 @agent_ppo2.py:185][0m |           0.0022 |         197.2298 |          -8.3020 |
[32m[20221214 14:13:43 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:13:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.62
[32m[20221214 14:13:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 765.77
[32m[20221214 14:13:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.12
[32m[20221214 14:13:43 @agent_ppo2.py:143][0m Total time:      15.68 min
[32m[20221214 14:13:43 @agent_ppo2.py:145][0m 1439744 total steps have happened
[32m[20221214 14:13:43 @agent_ppo2.py:121][0m #------------------------ Iteration 703 --------------------------#
[32m[20221214 14:13:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:43 @agent_ppo2.py:185][0m |           0.0058 |         190.1945 |          -8.5094 |
[32m[20221214 14:13:43 @agent_ppo2.py:185][0m |          -0.0021 |         173.1567 |          -8.5089 |
[32m[20221214 14:13:43 @agent_ppo2.py:185][0m |           0.0001 |         171.4197 |          -8.4843 |
[32m[20221214 14:13:44 @agent_ppo2.py:185][0m |          -0.0013 |         168.8920 |          -8.4740 |
[32m[20221214 14:13:44 @agent_ppo2.py:185][0m |           0.0019 |         170.1870 |          -8.5226 |
[32m[20221214 14:13:44 @agent_ppo2.py:185][0m |          -0.0028 |         166.6882 |          -8.5130 |
[32m[20221214 14:13:44 @agent_ppo2.py:185][0m |          -0.0017 |         166.4448 |          -8.5084 |
[32m[20221214 14:13:44 @agent_ppo2.py:185][0m |          -0.0012 |         165.6672 |          -8.4969 |
[32m[20221214 14:13:44 @agent_ppo2.py:185][0m |          -0.0022 |         165.1122 |          -8.5058 |
[32m[20221214 14:13:44 @agent_ppo2.py:185][0m |          -0.0039 |         164.8897 |          -8.5478 |
[32m[20221214 14:13:44 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:13:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 752.57
[32m[20221214 14:13:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.30
[32m[20221214 14:13:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.93
[32m[20221214 14:13:44 @agent_ppo2.py:143][0m Total time:      15.70 min
[32m[20221214 14:13:44 @agent_ppo2.py:145][0m 1441792 total steps have happened
[32m[20221214 14:13:44 @agent_ppo2.py:121][0m #------------------------ Iteration 704 --------------------------#
[32m[20221214 14:13:45 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:13:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:45 @agent_ppo2.py:185][0m |           0.0053 |         208.4635 |          -8.0521 |
[32m[20221214 14:13:45 @agent_ppo2.py:185][0m |           0.0098 |         211.1818 |          -8.0854 |
[32m[20221214 14:13:45 @agent_ppo2.py:185][0m |           0.0038 |         204.9936 |          -8.1497 |
[32m[20221214 14:13:45 @agent_ppo2.py:185][0m |           0.0095 |         208.2396 |          -8.1446 |
[32m[20221214 14:13:45 @agent_ppo2.py:185][0m |           0.0054 |         212.8630 |          -8.1448 |
[32m[20221214 14:13:45 @agent_ppo2.py:185][0m |          -0.0021 |         199.4848 |          -8.2172 |
[32m[20221214 14:13:45 @agent_ppo2.py:185][0m |          -0.0022 |         199.3226 |          -8.2353 |
[32m[20221214 14:13:45 @agent_ppo2.py:185][0m |          -0.0024 |         199.3752 |          -8.3117 |
[32m[20221214 14:13:45 @agent_ppo2.py:185][0m |           0.0038 |         203.0069 |          -8.2970 |
[32m[20221214 14:13:46 @agent_ppo2.py:185][0m |           0.0094 |         219.8624 |          -8.3182 |
[32m[20221214 14:13:46 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:13:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 755.93
[32m[20221214 14:13:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.14
[32m[20221214 14:13:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.56
[32m[20221214 14:13:46 @agent_ppo2.py:143][0m Total time:      15.73 min
[32m[20221214 14:13:46 @agent_ppo2.py:145][0m 1443840 total steps have happened
[32m[20221214 14:13:46 @agent_ppo2.py:121][0m #------------------------ Iteration 705 --------------------------#
[32m[20221214 14:13:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:13:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:46 @agent_ppo2.py:185][0m |          -0.0011 |         187.8480 |          -9.0232 |
[32m[20221214 14:13:46 @agent_ppo2.py:185][0m |          -0.0036 |         174.6844 |          -8.9933 |
[32m[20221214 14:13:46 @agent_ppo2.py:185][0m |          -0.0043 |         171.9021 |          -9.0355 |
[32m[20221214 14:13:46 @agent_ppo2.py:185][0m |          -0.0038 |         169.6328 |          -9.0587 |
[32m[20221214 14:13:46 @agent_ppo2.py:185][0m |          -0.0034 |         168.2147 |          -9.0649 |
[32m[20221214 14:13:46 @agent_ppo2.py:185][0m |          -0.0034 |         166.3481 |          -9.0664 |
[32m[20221214 14:13:47 @agent_ppo2.py:185][0m |          -0.0010 |         165.8499 |          -9.0432 |
[32m[20221214 14:13:47 @agent_ppo2.py:185][0m |          -0.0001 |         164.2693 |          -9.0636 |
[32m[20221214 14:13:47 @agent_ppo2.py:185][0m |          -0.0034 |         162.7185 |          -9.0933 |
[32m[20221214 14:13:47 @agent_ppo2.py:185][0m |          -0.0048 |         162.3498 |          -9.0568 |
[32m[20221214 14:13:47 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:13:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 744.42
[32m[20221214 14:13:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.40
[32m[20221214 14:13:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 777.44
[32m[20221214 14:13:47 @agent_ppo2.py:143][0m Total time:      15.75 min
[32m[20221214 14:13:47 @agent_ppo2.py:145][0m 1445888 total steps have happened
[32m[20221214 14:13:47 @agent_ppo2.py:121][0m #------------------------ Iteration 706 --------------------------#
[32m[20221214 14:13:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:47 @agent_ppo2.py:185][0m |           0.0024 |         184.7130 |          -8.8277 |
[32m[20221214 14:13:48 @agent_ppo2.py:185][0m |           0.0052 |         182.3177 |          -8.8324 |
[32m[20221214 14:13:48 @agent_ppo2.py:185][0m |          -0.0011 |         175.6081 |          -8.8305 |
[32m[20221214 14:13:48 @agent_ppo2.py:185][0m |          -0.0017 |         173.5053 |          -8.8076 |
[32m[20221214 14:13:48 @agent_ppo2.py:185][0m |          -0.0002 |         172.6566 |          -8.8799 |
[32m[20221214 14:13:48 @agent_ppo2.py:185][0m |          -0.0003 |         172.8029 |          -8.8843 |
[32m[20221214 14:13:48 @agent_ppo2.py:185][0m |          -0.0011 |         171.0335 |          -8.8889 |
[32m[20221214 14:13:48 @agent_ppo2.py:185][0m |          -0.0002 |         170.3134 |          -8.8808 |
[32m[20221214 14:13:48 @agent_ppo2.py:185][0m |           0.0119 |         193.2889 |          -8.9144 |
[32m[20221214 14:13:48 @agent_ppo2.py:185][0m |           0.0088 |         180.2266 |          -8.8338 |
[32m[20221214 14:13:48 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:13:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.94
[32m[20221214 14:13:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 764.19
[32m[20221214 14:13:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.93
[32m[20221214 14:13:48 @agent_ppo2.py:143][0m Total time:      15.77 min
[32m[20221214 14:13:48 @agent_ppo2.py:145][0m 1447936 total steps have happened
[32m[20221214 14:13:48 @agent_ppo2.py:121][0m #------------------------ Iteration 707 --------------------------#
[32m[20221214 14:13:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:49 @agent_ppo2.py:185][0m |          -0.0032 |         154.9723 |          -9.0884 |
[32m[20221214 14:13:49 @agent_ppo2.py:185][0m |          -0.0040 |         146.7633 |          -9.0694 |
[32m[20221214 14:13:49 @agent_ppo2.py:185][0m |          -0.0015 |         143.1589 |          -9.0201 |
[32m[20221214 14:13:49 @agent_ppo2.py:185][0m |          -0.0032 |         141.5804 |          -9.0101 |
[32m[20221214 14:13:49 @agent_ppo2.py:185][0m |          -0.0033 |         140.1130 |          -9.1276 |
[32m[20221214 14:13:49 @agent_ppo2.py:185][0m |          -0.0047 |         140.2102 |          -9.0470 |
[32m[20221214 14:13:49 @agent_ppo2.py:185][0m |          -0.0033 |         138.8412 |          -9.0283 |
[32m[20221214 14:13:49 @agent_ppo2.py:185][0m |          -0.0018 |         139.9506 |          -9.0109 |
[32m[20221214 14:13:50 @agent_ppo2.py:185][0m |          -0.0035 |         137.9494 |          -9.0092 |
[32m[20221214 14:13:50 @agent_ppo2.py:185][0m |          -0.0014 |         137.3662 |          -8.9855 |
[32m[20221214 14:13:50 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:13:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 754.71
[32m[20221214 14:13:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.07
[32m[20221214 14:13:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 764.88
[32m[20221214 14:13:50 @agent_ppo2.py:143][0m Total time:      15.80 min
[32m[20221214 14:13:50 @agent_ppo2.py:145][0m 1449984 total steps have happened
[32m[20221214 14:13:50 @agent_ppo2.py:121][0m #------------------------ Iteration 708 --------------------------#
[32m[20221214 14:13:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:50 @agent_ppo2.py:185][0m |           0.0057 |         120.0126 |          -9.1144 |
[32m[20221214 14:13:50 @agent_ppo2.py:185][0m |           0.0082 |         112.1450 |          -9.1122 |
[32m[20221214 14:13:50 @agent_ppo2.py:185][0m |           0.0004 |         108.6272 |          -9.0324 |
[32m[20221214 14:13:50 @agent_ppo2.py:185][0m |          -0.0022 |         107.3555 |          -8.9903 |
[32m[20221214 14:13:51 @agent_ppo2.py:185][0m |          -0.0019 |         105.7540 |          -8.9723 |
[32m[20221214 14:13:51 @agent_ppo2.py:185][0m |          -0.0013 |         105.0069 |          -8.9120 |
[32m[20221214 14:13:51 @agent_ppo2.py:185][0m |           0.0071 |         113.4874 |          -8.8768 |
[32m[20221214 14:13:51 @agent_ppo2.py:185][0m |          -0.0048 |         104.4991 |          -8.8531 |
[32m[20221214 14:13:51 @agent_ppo2.py:185][0m |           0.0043 |         106.9247 |          -8.8225 |
[32m[20221214 14:13:51 @agent_ppo2.py:185][0m |          -0.0049 |         103.4456 |          -8.7663 |
[32m[20221214 14:13:51 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:13:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.78
[32m[20221214 14:13:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.81
[32m[20221214 14:13:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 754.13
[32m[20221214 14:13:51 @agent_ppo2.py:143][0m Total time:      15.82 min
[32m[20221214 14:13:51 @agent_ppo2.py:145][0m 1452032 total steps have happened
[32m[20221214 14:13:51 @agent_ppo2.py:121][0m #------------------------ Iteration 709 --------------------------#
[32m[20221214 14:13:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:52 @agent_ppo2.py:185][0m |           0.0028 |          81.6469 |          -8.3253 |
[32m[20221214 14:13:52 @agent_ppo2.py:185][0m |          -0.0008 |          65.2613 |          -8.2125 |
[32m[20221214 14:13:52 @agent_ppo2.py:185][0m |           0.0034 |          63.8688 |          -8.1755 |
[32m[20221214 14:13:52 @agent_ppo2.py:185][0m |          -0.0029 |          60.5222 |          -8.0553 |
[32m[20221214 14:13:52 @agent_ppo2.py:185][0m |          -0.0014 |          59.6756 |          -8.0537 |
[32m[20221214 14:13:52 @agent_ppo2.py:185][0m |          -0.0005 |          59.1984 |          -7.9909 |
[32m[20221214 14:13:52 @agent_ppo2.py:185][0m |          -0.0001 |          58.1535 |          -7.9532 |
[32m[20221214 14:13:52 @agent_ppo2.py:185][0m |           0.0115 |          71.1801 |          -7.8630 |
[32m[20221214 14:13:52 @agent_ppo2.py:185][0m |          -0.0017 |          58.4280 |          -7.7946 |
[32m[20221214 14:13:52 @agent_ppo2.py:185][0m |          -0.0016 |          57.2296 |          -7.7648 |
[32m[20221214 14:13:52 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:13:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 705.43
[32m[20221214 14:13:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 738.77
[32m[20221214 14:13:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 758.11
[32m[20221214 14:13:53 @agent_ppo2.py:143][0m Total time:      15.84 min
[32m[20221214 14:13:53 @agent_ppo2.py:145][0m 1454080 total steps have happened
[32m[20221214 14:13:53 @agent_ppo2.py:121][0m #------------------------ Iteration 710 --------------------------#
[32m[20221214 14:13:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:13:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:53 @agent_ppo2.py:185][0m |          -0.0006 |         110.7446 |          -8.1322 |
[32m[20221214 14:13:53 @agent_ppo2.py:185][0m |          -0.0012 |          99.4965 |          -8.0965 |
[32m[20221214 14:13:53 @agent_ppo2.py:185][0m |          -0.0030 |          96.6224 |          -8.1311 |
[32m[20221214 14:13:53 @agent_ppo2.py:185][0m |          -0.0031 |          95.1833 |          -8.0369 |
[32m[20221214 14:13:53 @agent_ppo2.py:185][0m |           0.0036 |          92.5640 |          -8.0824 |
[32m[20221214 14:13:53 @agent_ppo2.py:185][0m |          -0.0022 |          91.7082 |          -8.0952 |
[32m[20221214 14:13:53 @agent_ppo2.py:185][0m |          -0.0029 |          90.6999 |          -8.0970 |
[32m[20221214 14:13:54 @agent_ppo2.py:185][0m |          -0.0038 |          90.6932 |          -8.0634 |
[32m[20221214 14:13:54 @agent_ppo2.py:185][0m |          -0.0010 |          90.6515 |          -8.0841 |
[32m[20221214 14:13:54 @agent_ppo2.py:185][0m |          -0.0019 |          89.2088 |          -8.0280 |
[32m[20221214 14:13:54 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:13:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.66
[32m[20221214 14:13:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 753.62
[32m[20221214 14:13:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 767.85
[32m[20221214 14:13:54 @agent_ppo2.py:143][0m Total time:      15.86 min
[32m[20221214 14:13:54 @agent_ppo2.py:145][0m 1456128 total steps have happened
[32m[20221214 14:13:54 @agent_ppo2.py:121][0m #------------------------ Iteration 711 --------------------------#
[32m[20221214 14:13:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:54 @agent_ppo2.py:185][0m |           0.0007 |         144.1296 |          -7.5571 |
[32m[20221214 14:13:54 @agent_ppo2.py:185][0m |           0.0060 |         142.3765 |          -7.5331 |
[32m[20221214 14:13:54 @agent_ppo2.py:185][0m |          -0.0002 |         134.6939 |          -7.5381 |
[32m[20221214 14:13:55 @agent_ppo2.py:185][0m |           0.0024 |         134.1314 |          -7.5043 |
[32m[20221214 14:13:55 @agent_ppo2.py:185][0m |          -0.0014 |         131.3751 |          -7.4025 |
[32m[20221214 14:13:55 @agent_ppo2.py:185][0m |           0.0300 |         160.5481 |          -7.5218 |
[32m[20221214 14:13:55 @agent_ppo2.py:185][0m |           0.0005 |         132.2553 |          -7.4841 |
[32m[20221214 14:13:55 @agent_ppo2.py:185][0m |          -0.0003 |         128.4128 |          -7.5343 |
[32m[20221214 14:13:55 @agent_ppo2.py:185][0m |          -0.0019 |         128.2405 |          -7.5690 |
[32m[20221214 14:13:55 @agent_ppo2.py:185][0m |          -0.0013 |         128.2615 |          -7.6142 |
[32m[20221214 14:13:55 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:13:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.74
[32m[20221214 14:13:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 758.96
[32m[20221214 14:13:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.94
[32m[20221214 14:13:55 @agent_ppo2.py:143][0m Total time:      15.89 min
[32m[20221214 14:13:55 @agent_ppo2.py:145][0m 1458176 total steps have happened
[32m[20221214 14:13:55 @agent_ppo2.py:121][0m #------------------------ Iteration 712 --------------------------#
[32m[20221214 14:13:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:56 @agent_ppo2.py:185][0m |          -0.0022 |         157.6692 |          -8.3404 |
[32m[20221214 14:13:56 @agent_ppo2.py:185][0m |           0.0020 |         149.0452 |          -8.3962 |
[32m[20221214 14:13:56 @agent_ppo2.py:185][0m |          -0.0008 |         146.2277 |          -8.2830 |
[32m[20221214 14:13:56 @agent_ppo2.py:185][0m |          -0.0014 |         143.9420 |          -8.3882 |
[32m[20221214 14:13:56 @agent_ppo2.py:185][0m |           0.0072 |         150.9275 |          -8.3500 |
[32m[20221214 14:13:56 @agent_ppo2.py:185][0m |          -0.0022 |         144.1235 |          -8.3791 |
[32m[20221214 14:13:56 @agent_ppo2.py:185][0m |          -0.0028 |         142.0290 |          -8.3802 |
[32m[20221214 14:13:56 @agent_ppo2.py:185][0m |          -0.0028 |         141.2836 |          -8.3984 |
[32m[20221214 14:13:56 @agent_ppo2.py:185][0m |           0.0054 |         147.5395 |          -8.3956 |
[32m[20221214 14:13:56 @agent_ppo2.py:185][0m |          -0.0014 |         140.7295 |          -8.3780 |
[32m[20221214 14:13:56 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:13:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 760.22
[32m[20221214 14:13:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.18
[32m[20221214 14:13:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 779.36
[32m[20221214 14:13:57 @agent_ppo2.py:143][0m Total time:      15.91 min
[32m[20221214 14:13:57 @agent_ppo2.py:145][0m 1460224 total steps have happened
[32m[20221214 14:13:57 @agent_ppo2.py:121][0m #------------------------ Iteration 713 --------------------------#
[32m[20221214 14:13:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:13:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:57 @agent_ppo2.py:185][0m |           0.0004 |         165.8331 |          -7.6930 |
[32m[20221214 14:13:57 @agent_ppo2.py:185][0m |           0.0108 |         176.9773 |          -7.6026 |
[32m[20221214 14:13:57 @agent_ppo2.py:185][0m |          -0.0021 |         163.2935 |          -7.6705 |
[32m[20221214 14:13:57 @agent_ppo2.py:185][0m |          -0.0005 |         161.0169 |          -7.5889 |
[32m[20221214 14:13:57 @agent_ppo2.py:185][0m |          -0.0021 |         160.5769 |          -7.6724 |
[32m[20221214 14:13:57 @agent_ppo2.py:185][0m |          -0.0018 |         159.7129 |          -7.6042 |
[32m[20221214 14:13:57 @agent_ppo2.py:185][0m |          -0.0033 |         159.7609 |          -7.5611 |
[32m[20221214 14:13:58 @agent_ppo2.py:185][0m |          -0.0018 |         159.5090 |          -7.6463 |
[32m[20221214 14:13:58 @agent_ppo2.py:185][0m |          -0.0029 |         160.1700 |          -7.6248 |
[32m[20221214 14:13:58 @agent_ppo2.py:185][0m |          -0.0019 |         158.8510 |          -7.6027 |
[32m[20221214 14:13:58 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:13:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 761.16
[32m[20221214 14:13:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 768.51
[32m[20221214 14:13:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 775.27
[32m[20221214 14:13:58 @agent_ppo2.py:143][0m Total time:      15.93 min
[32m[20221214 14:13:58 @agent_ppo2.py:145][0m 1462272 total steps have happened
[32m[20221214 14:13:58 @agent_ppo2.py:121][0m #------------------------ Iteration 714 --------------------------#
[32m[20221214 14:13:58 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221214 14:13:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:13:58 @agent_ppo2.py:185][0m |           0.0001 |          95.2610 |          -7.5485 |
[32m[20221214 14:13:58 @agent_ppo2.py:185][0m |           0.0085 |          98.2567 |          -7.4470 |
[32m[20221214 14:13:59 @agent_ppo2.py:185][0m |          -0.0019 |          92.4871 |          -7.3585 |
[32m[20221214 14:13:59 @agent_ppo2.py:185][0m |          -0.0001 |          92.0198 |          -7.3763 |
[32m[20221214 14:13:59 @agent_ppo2.py:185][0m |          -0.0000 |          91.5073 |          -7.4152 |
[32m[20221214 14:13:59 @agent_ppo2.py:185][0m |           0.0011 |          91.7571 |          -7.3360 |
[32m[20221214 14:13:59 @agent_ppo2.py:185][0m |          -0.0015 |          90.8561 |          -7.3484 |
[32m[20221214 14:13:59 @agent_ppo2.py:185][0m |           0.0003 |          90.5679 |          -7.3529 |
[32m[20221214 14:13:59 @agent_ppo2.py:185][0m |          -0.0013 |          90.5660 |          -7.4254 |
[32m[20221214 14:13:59 @agent_ppo2.py:185][0m |           0.0004 |          90.2355 |          -7.4009 |
[32m[20221214 14:13:59 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 14:14:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.50
[32m[20221214 14:14:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 761.97
[32m[20221214 14:14:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 783.72
[32m[20221214 14:14:00 @agent_ppo2.py:143][0m Total time:      15.96 min
[32m[20221214 14:14:00 @agent_ppo2.py:145][0m 1464320 total steps have happened
[32m[20221214 14:14:00 @agent_ppo2.py:121][0m #------------------------ Iteration 715 --------------------------#
[32m[20221214 14:14:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:00 @agent_ppo2.py:185][0m |           0.0005 |         175.7928 |          -7.4494 |
[32m[20221214 14:14:00 @agent_ppo2.py:185][0m |          -0.0003 |         169.9778 |          -7.4759 |
[32m[20221214 14:14:00 @agent_ppo2.py:185][0m |          -0.0023 |         167.8030 |          -7.4974 |
[32m[20221214 14:14:00 @agent_ppo2.py:185][0m |           0.0087 |         174.9952 |          -7.4722 |
[32m[20221214 14:14:00 @agent_ppo2.py:185][0m |           0.0023 |         167.8426 |          -7.5076 |
[32m[20221214 14:14:00 @agent_ppo2.py:185][0m |          -0.0023 |         164.8532 |          -7.5267 |
[32m[20221214 14:14:00 @agent_ppo2.py:185][0m |          -0.0021 |         163.8219 |          -7.5045 |
[32m[20221214 14:14:00 @agent_ppo2.py:185][0m |           0.0007 |         163.2118 |          -7.5748 |
[32m[20221214 14:14:01 @agent_ppo2.py:185][0m |           0.0015 |         163.9048 |          -7.5940 |
[32m[20221214 14:14:01 @agent_ppo2.py:185][0m |          -0.0001 |         161.8048 |          -7.5970 |
[32m[20221214 14:14:01 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:14:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.42
[32m[20221214 14:14:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.08
[32m[20221214 14:14:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 766.53
[32m[20221214 14:14:01 @agent_ppo2.py:143][0m Total time:      15.98 min
[32m[20221214 14:14:01 @agent_ppo2.py:145][0m 1466368 total steps have happened
[32m[20221214 14:14:01 @agent_ppo2.py:121][0m #------------------------ Iteration 716 --------------------------#
[32m[20221214 14:14:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:01 @agent_ppo2.py:185][0m |          -0.0033 |          63.6622 |          -8.0431 |
[32m[20221214 14:14:01 @agent_ppo2.py:185][0m |          -0.0017 |          55.2063 |          -8.2051 |
[32m[20221214 14:14:01 @agent_ppo2.py:185][0m |          -0.0067 |          51.6141 |          -8.2469 |
[32m[20221214 14:14:01 @agent_ppo2.py:185][0m |          -0.0048 |          49.8592 |          -8.2629 |
[32m[20221214 14:14:02 @agent_ppo2.py:185][0m |          -0.0025 |          48.9375 |          -8.2650 |
[32m[20221214 14:14:02 @agent_ppo2.py:185][0m |           0.0071 |          51.7667 |          -8.3559 |
[32m[20221214 14:14:02 @agent_ppo2.py:185][0m |          -0.0043 |          47.0694 |          -8.4005 |
[32m[20221214 14:14:02 @agent_ppo2.py:185][0m |           0.0007 |          47.8273 |          -8.4668 |
[32m[20221214 14:14:02 @agent_ppo2.py:185][0m |          -0.0011 |          45.0344 |          -8.4996 |
[32m[20221214 14:14:02 @agent_ppo2.py:185][0m |          -0.0051 |          44.4852 |          -8.4280 |
[32m[20221214 14:14:02 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:14:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 732.00
[32m[20221214 14:14:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 758.24
[32m[20221214 14:14:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 763.71
[32m[20221214 14:14:02 @agent_ppo2.py:143][0m Total time:      16.00 min
[32m[20221214 14:14:02 @agent_ppo2.py:145][0m 1468416 total steps have happened
[32m[20221214 14:14:02 @agent_ppo2.py:121][0m #------------------------ Iteration 717 --------------------------#
[32m[20221214 14:14:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:03 @agent_ppo2.py:185][0m |          -0.0026 |          94.6511 |          -8.5328 |
[32m[20221214 14:14:03 @agent_ppo2.py:185][0m |          -0.0041 |          93.1800 |          -8.5564 |
[32m[20221214 14:14:03 @agent_ppo2.py:185][0m |          -0.0062 |          92.3819 |          -8.5904 |
[32m[20221214 14:14:03 @agent_ppo2.py:185][0m |          -0.0022 |          92.5054 |          -8.6178 |
[32m[20221214 14:14:03 @agent_ppo2.py:185][0m |          -0.0038 |          91.5571 |          -8.6282 |
[32m[20221214 14:14:03 @agent_ppo2.py:185][0m |          -0.0057 |          91.1605 |          -8.7043 |
[32m[20221214 14:14:03 @agent_ppo2.py:185][0m |           0.0022 |          95.7730 |          -8.6754 |
[32m[20221214 14:14:03 @agent_ppo2.py:185][0m |          -0.0031 |          91.0861 |          -8.6575 |
[32m[20221214 14:14:03 @agent_ppo2.py:185][0m |          -0.0049 |          90.5242 |          -8.7284 |
[32m[20221214 14:14:03 @agent_ppo2.py:185][0m |          -0.0038 |          89.6681 |          -8.7036 |
[32m[20221214 14:14:03 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:14:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.49
[32m[20221214 14:14:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.24
[32m[20221214 14:14:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 781.30
[32m[20221214 14:14:04 @agent_ppo2.py:143][0m Total time:      16.02 min
[32m[20221214 14:14:04 @agent_ppo2.py:145][0m 1470464 total steps have happened
[32m[20221214 14:14:04 @agent_ppo2.py:121][0m #------------------------ Iteration 718 --------------------------#
[32m[20221214 14:14:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:04 @agent_ppo2.py:185][0m |          -0.0014 |         141.2744 |          -9.1134 |
[32m[20221214 14:14:04 @agent_ppo2.py:185][0m |          -0.0029 |         135.8526 |          -9.0445 |
[32m[20221214 14:14:04 @agent_ppo2.py:185][0m |           0.0029 |         139.0135 |          -9.0263 |
[32m[20221214 14:14:04 @agent_ppo2.py:185][0m |          -0.0040 |         132.8349 |          -9.0082 |
[32m[20221214 14:14:04 @agent_ppo2.py:185][0m |          -0.0056 |         131.8980 |          -9.0706 |
[32m[20221214 14:14:04 @agent_ppo2.py:185][0m |          -0.0069 |         130.8983 |          -9.0698 |
[32m[20221214 14:14:04 @agent_ppo2.py:185][0m |          -0.0038 |         130.0408 |          -9.1390 |
[32m[20221214 14:14:05 @agent_ppo2.py:185][0m |          -0.0054 |         129.7711 |          -9.1305 |
[32m[20221214 14:14:05 @agent_ppo2.py:185][0m |          -0.0070 |         129.9951 |          -9.1928 |
[32m[20221214 14:14:05 @agent_ppo2.py:185][0m |          -0.0046 |         128.6697 |          -9.1788 |
[32m[20221214 14:14:05 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:14:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 757.09
[32m[20221214 14:14:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.77
[32m[20221214 14:14:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 773.72
[32m[20221214 14:14:05 @agent_ppo2.py:143][0m Total time:      16.05 min
[32m[20221214 14:14:05 @agent_ppo2.py:145][0m 1472512 total steps have happened
[32m[20221214 14:14:05 @agent_ppo2.py:121][0m #------------------------ Iteration 719 --------------------------#
[32m[20221214 14:14:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:05 @agent_ppo2.py:185][0m |           0.0083 |         189.9945 |          -9.4275 |
[32m[20221214 14:14:05 @agent_ppo2.py:185][0m |          -0.0020 |         165.3577 |          -9.4270 |
[32m[20221214 14:14:05 @agent_ppo2.py:185][0m |          -0.0046 |         161.9251 |          -9.3701 |
[32m[20221214 14:14:06 @agent_ppo2.py:185][0m |          -0.0020 |         160.2467 |          -9.4215 |
[32m[20221214 14:14:06 @agent_ppo2.py:185][0m |          -0.0023 |         159.3328 |          -9.4208 |
[32m[20221214 14:14:06 @agent_ppo2.py:185][0m |          -0.0030 |         159.7362 |          -9.4861 |
[32m[20221214 14:14:06 @agent_ppo2.py:185][0m |          -0.0015 |         158.9632 |          -9.4897 |
[32m[20221214 14:14:06 @agent_ppo2.py:185][0m |          -0.0017 |         157.5427 |          -9.4974 |
[32m[20221214 14:14:06 @agent_ppo2.py:185][0m |           0.0016 |         157.9574 |          -9.5058 |
[32m[20221214 14:14:06 @agent_ppo2.py:185][0m |          -0.0030 |         157.2743 |          -9.5492 |
[32m[20221214 14:14:06 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:14:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.14
[32m[20221214 14:14:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 781.60
[32m[20221214 14:14:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.64
[32m[20221214 14:14:06 @agent_ppo2.py:143][0m Total time:      16.07 min
[32m[20221214 14:14:06 @agent_ppo2.py:145][0m 1474560 total steps have happened
[32m[20221214 14:14:06 @agent_ppo2.py:121][0m #------------------------ Iteration 720 --------------------------#
[32m[20221214 14:14:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:14:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:07 @agent_ppo2.py:185][0m |          -0.0029 |         211.3274 |          -9.4247 |
[32m[20221214 14:14:07 @agent_ppo2.py:185][0m |          -0.0032 |         201.3509 |          -9.4065 |
[32m[20221214 14:14:07 @agent_ppo2.py:185][0m |          -0.0051 |         197.3856 |          -9.4167 |
[32m[20221214 14:14:07 @agent_ppo2.py:185][0m |          -0.0062 |         195.5670 |          -9.4117 |
[32m[20221214 14:14:07 @agent_ppo2.py:185][0m |          -0.0029 |         193.3683 |          -9.4233 |
[32m[20221214 14:14:07 @agent_ppo2.py:185][0m |          -0.0047 |         191.3008 |          -9.4531 |
[32m[20221214 14:14:07 @agent_ppo2.py:185][0m |          -0.0049 |         190.3729 |          -9.4438 |
[32m[20221214 14:14:07 @agent_ppo2.py:185][0m |          -0.0037 |         188.0867 |          -9.3608 |
[32m[20221214 14:14:07 @agent_ppo2.py:185][0m |          -0.0052 |         185.8372 |          -9.3907 |
[32m[20221214 14:14:07 @agent_ppo2.py:185][0m |          -0.0047 |         184.8527 |          -9.3929 |
[32m[20221214 14:14:07 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:14:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 769.27
[32m[20221214 14:14:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.92
[32m[20221214 14:14:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.88
[32m[20221214 14:14:08 @agent_ppo2.py:143][0m Total time:      16.09 min
[32m[20221214 14:14:08 @agent_ppo2.py:145][0m 1476608 total steps have happened
[32m[20221214 14:14:08 @agent_ppo2.py:121][0m #------------------------ Iteration 721 --------------------------#
[32m[20221214 14:14:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:08 @agent_ppo2.py:185][0m |          -0.0031 |         227.4802 |          -9.2029 |
[32m[20221214 14:14:08 @agent_ppo2.py:185][0m |          -0.0018 |         219.5321 |          -9.2927 |
[32m[20221214 14:14:08 @agent_ppo2.py:185][0m |          -0.0000 |         217.2395 |          -9.2171 |
[32m[20221214 14:14:08 @agent_ppo2.py:185][0m |          -0.0012 |         215.2377 |          -9.2793 |
[32m[20221214 14:14:08 @agent_ppo2.py:185][0m |          -0.0011 |         215.1100 |          -9.1438 |
[32m[20221214 14:14:08 @agent_ppo2.py:185][0m |          -0.0022 |         214.1428 |          -9.2541 |
[32m[20221214 14:14:09 @agent_ppo2.py:185][0m |          -0.0017 |         213.5384 |          -9.1814 |
[32m[20221214 14:14:09 @agent_ppo2.py:185][0m |          -0.0036 |         212.7738 |          -9.2738 |
[32m[20221214 14:14:09 @agent_ppo2.py:185][0m |           0.0012 |         212.9418 |          -9.2043 |
[32m[20221214 14:14:09 @agent_ppo2.py:185][0m |          -0.0034 |         212.0667 |          -9.2741 |
[32m[20221214 14:14:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:14:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.49
[32m[20221214 14:14:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.74
[32m[20221214 14:14:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 784.54
[32m[20221214 14:14:09 @agent_ppo2.py:143][0m Total time:      16.11 min
[32m[20221214 14:14:09 @agent_ppo2.py:145][0m 1478656 total steps have happened
[32m[20221214 14:14:09 @agent_ppo2.py:121][0m #------------------------ Iteration 722 --------------------------#
[32m[20221214 14:14:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:09 @agent_ppo2.py:185][0m |           0.0035 |         246.4752 |          -9.6729 |
[32m[20221214 14:14:09 @agent_ppo2.py:185][0m |          -0.0023 |         228.5115 |          -9.6542 |
[32m[20221214 14:14:09 @agent_ppo2.py:185][0m |           0.0083 |         238.4139 |          -9.5652 |
[32m[20221214 14:14:10 @agent_ppo2.py:185][0m |          -0.0024 |         224.8810 |          -9.5351 |
[32m[20221214 14:14:10 @agent_ppo2.py:185][0m |          -0.0012 |         224.0064 |          -9.5351 |
[32m[20221214 14:14:10 @agent_ppo2.py:185][0m |          -0.0036 |         223.1502 |          -9.4478 |
[32m[20221214 14:14:10 @agent_ppo2.py:185][0m |          -0.0041 |         222.5810 |          -9.3422 |
[32m[20221214 14:14:10 @agent_ppo2.py:185][0m |          -0.0029 |         221.6833 |          -9.3685 |
[32m[20221214 14:14:10 @agent_ppo2.py:185][0m |          -0.0036 |         221.0885 |          -9.2941 |
[32m[20221214 14:14:10 @agent_ppo2.py:185][0m |          -0.0025 |         220.4409 |          -9.2040 |
[32m[20221214 14:14:10 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:14:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.20
[32m[20221214 14:14:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 781.87
[32m[20221214 14:14:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.94
[32m[20221214 14:14:10 @agent_ppo2.py:143][0m Total time:      16.14 min
[32m[20221214 14:14:10 @agent_ppo2.py:145][0m 1480704 total steps have happened
[32m[20221214 14:14:10 @agent_ppo2.py:121][0m #------------------------ Iteration 723 --------------------------#
[32m[20221214 14:14:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:11 @agent_ppo2.py:185][0m |          -0.0033 |         227.4471 |          -8.9725 |
[32m[20221214 14:14:11 @agent_ppo2.py:185][0m |          -0.0012 |         219.0176 |          -8.9059 |
[32m[20221214 14:14:11 @agent_ppo2.py:185][0m |          -0.0022 |         216.9745 |          -8.8880 |
[32m[20221214 14:14:11 @agent_ppo2.py:185][0m |          -0.0044 |         215.4113 |          -8.8911 |
[32m[20221214 14:14:11 @agent_ppo2.py:185][0m |          -0.0016 |         214.7088 |          -9.0015 |
[32m[20221214 14:14:11 @agent_ppo2.py:185][0m |          -0.0040 |         214.9527 |          -8.9023 |
[32m[20221214 14:14:11 @agent_ppo2.py:185][0m |          -0.0022 |         215.1957 |          -8.9263 |
[32m[20221214 14:14:11 @agent_ppo2.py:185][0m |          -0.0016 |         213.4484 |          -8.9757 |
[32m[20221214 14:14:11 @agent_ppo2.py:185][0m |           0.0085 |         232.3578 |          -8.9582 |
[32m[20221214 14:14:11 @agent_ppo2.py:185][0m |          -0.0028 |         213.0904 |          -8.9523 |
[32m[20221214 14:14:11 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:14:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 750.78
[32m[20221214 14:14:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.01
[32m[20221214 14:14:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 785.79
[32m[20221214 14:14:12 @agent_ppo2.py:143][0m Total time:      16.16 min
[32m[20221214 14:14:12 @agent_ppo2.py:145][0m 1482752 total steps have happened
[32m[20221214 14:14:12 @agent_ppo2.py:121][0m #------------------------ Iteration 724 --------------------------#
[32m[20221214 14:14:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:12 @agent_ppo2.py:185][0m |          -0.0011 |         215.9477 |          -9.4462 |
[32m[20221214 14:14:12 @agent_ppo2.py:185][0m |          -0.0022 |         212.1434 |          -9.5432 |
[32m[20221214 14:14:12 @agent_ppo2.py:185][0m |          -0.0029 |         210.8930 |          -9.5434 |
[32m[20221214 14:14:12 @agent_ppo2.py:185][0m |          -0.0009 |         210.7144 |          -9.5630 |
[32m[20221214 14:14:12 @agent_ppo2.py:185][0m |          -0.0018 |         209.6679 |          -9.5487 |
[32m[20221214 14:14:12 @agent_ppo2.py:185][0m |          -0.0011 |         210.1072 |          -9.5014 |
[32m[20221214 14:14:13 @agent_ppo2.py:185][0m |           0.0052 |         213.5358 |          -9.5737 |
[32m[20221214 14:14:13 @agent_ppo2.py:185][0m |          -0.0029 |         209.2365 |          -9.5872 |
[32m[20221214 14:14:13 @agent_ppo2.py:185][0m |          -0.0028 |         208.1889 |          -9.5715 |
[32m[20221214 14:14:13 @agent_ppo2.py:185][0m |          -0.0001 |         208.1659 |          -9.6185 |
[32m[20221214 14:14:13 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:14:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.98
[32m[20221214 14:14:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 781.66
[32m[20221214 14:14:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.67
[32m[20221214 14:14:13 @agent_ppo2.py:143][0m Total time:      16.18 min
[32m[20221214 14:14:13 @agent_ppo2.py:145][0m 1484800 total steps have happened
[32m[20221214 14:14:13 @agent_ppo2.py:121][0m #------------------------ Iteration 725 --------------------------#
[32m[20221214 14:14:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:13 @agent_ppo2.py:185][0m |           0.0122 |         242.3810 |          -9.4734 |
[32m[20221214 14:14:13 @agent_ppo2.py:185][0m |          -0.0020 |         209.0398 |          -9.4305 |
[32m[20221214 14:14:14 @agent_ppo2.py:185][0m |          -0.0015 |         204.7652 |          -9.4568 |
[32m[20221214 14:14:14 @agent_ppo2.py:185][0m |           0.0010 |         203.5099 |          -9.3671 |
[32m[20221214 14:14:14 @agent_ppo2.py:185][0m |          -0.0021 |         200.5249 |          -9.4835 |
[32m[20221214 14:14:14 @agent_ppo2.py:185][0m |          -0.0011 |         200.1686 |          -9.4793 |
[32m[20221214 14:14:14 @agent_ppo2.py:185][0m |          -0.0013 |         199.3905 |          -9.4807 |
[32m[20221214 14:14:14 @agent_ppo2.py:185][0m |          -0.0017 |         198.1132 |          -9.4193 |
[32m[20221214 14:14:14 @agent_ppo2.py:185][0m |          -0.0012 |         197.6124 |          -9.5238 |
[32m[20221214 14:14:14 @agent_ppo2.py:185][0m |          -0.0019 |         197.1416 |          -9.4746 |
[32m[20221214 14:14:14 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:14:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 770.62
[32m[20221214 14:14:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 775.19
[32m[20221214 14:14:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 788.20
[32m[20221214 14:14:14 @agent_ppo2.py:143][0m Total time:      16.20 min
[32m[20221214 14:14:14 @agent_ppo2.py:145][0m 1486848 total steps have happened
[32m[20221214 14:14:14 @agent_ppo2.py:121][0m #------------------------ Iteration 726 --------------------------#
[32m[20221214 14:14:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:15 @agent_ppo2.py:185][0m |          -0.0034 |         206.6328 |          -9.4804 |
[32m[20221214 14:14:15 @agent_ppo2.py:185][0m |          -0.0032 |         196.2743 |          -9.4480 |
[32m[20221214 14:14:15 @agent_ppo2.py:185][0m |          -0.0033 |         192.1179 |          -9.4188 |
[32m[20221214 14:14:15 @agent_ppo2.py:185][0m |          -0.0033 |         189.8427 |          -9.3640 |
[32m[20221214 14:14:15 @agent_ppo2.py:185][0m |          -0.0044 |         189.6973 |          -9.3160 |
[32m[20221214 14:14:15 @agent_ppo2.py:185][0m |          -0.0024 |         187.5142 |          -9.2718 |
[32m[20221214 14:14:15 @agent_ppo2.py:185][0m |          -0.0042 |         186.7301 |          -9.2875 |
[32m[20221214 14:14:15 @agent_ppo2.py:185][0m |          -0.0031 |         185.7375 |          -9.1823 |
[32m[20221214 14:14:15 @agent_ppo2.py:185][0m |          -0.0007 |         187.1054 |          -9.1799 |
[32m[20221214 14:14:15 @agent_ppo2.py:185][0m |          -0.0042 |         184.1473 |          -9.1283 |
[32m[20221214 14:14:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:14:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.90
[32m[20221214 14:14:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 784.91
[32m[20221214 14:14:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.63
[32m[20221214 14:14:16 @agent_ppo2.py:143][0m Total time:      16.23 min
[32m[20221214 14:14:16 @agent_ppo2.py:145][0m 1488896 total steps have happened
[32m[20221214 14:14:16 @agent_ppo2.py:121][0m #------------------------ Iteration 727 --------------------------#
[32m[20221214 14:14:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:14:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:16 @agent_ppo2.py:185][0m |           0.0004 |         194.5589 |          -8.5667 |
[32m[20221214 14:14:16 @agent_ppo2.py:185][0m |          -0.0015 |         191.8457 |          -8.4926 |
[32m[20221214 14:14:16 @agent_ppo2.py:185][0m |          -0.0022 |         190.2025 |          -8.4852 |
[32m[20221214 14:14:16 @agent_ppo2.py:185][0m |          -0.0007 |         189.2296 |          -8.5066 |
[32m[20221214 14:14:16 @agent_ppo2.py:185][0m |           0.0010 |         188.8626 |          -8.4797 |
[32m[20221214 14:14:16 @agent_ppo2.py:185][0m |          -0.0014 |         188.2619 |          -8.5438 |
[32m[20221214 14:14:17 @agent_ppo2.py:185][0m |          -0.0011 |         187.9055 |          -8.5058 |
[32m[20221214 14:14:17 @agent_ppo2.py:185][0m |          -0.0011 |         187.6841 |          -8.5420 |
[32m[20221214 14:14:17 @agent_ppo2.py:185][0m |          -0.0014 |         188.0663 |          -8.5463 |
[32m[20221214 14:14:17 @agent_ppo2.py:185][0m |          -0.0008 |         187.0421 |          -8.5552 |
[32m[20221214 14:14:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:14:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 779.10
[32m[20221214 14:14:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.85
[32m[20221214 14:14:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.24
[32m[20221214 14:14:17 @agent_ppo2.py:143][0m Total time:      16.25 min
[32m[20221214 14:14:17 @agent_ppo2.py:145][0m 1490944 total steps have happened
[32m[20221214 14:14:17 @agent_ppo2.py:121][0m #------------------------ Iteration 728 --------------------------#
[32m[20221214 14:14:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:14:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:17 @agent_ppo2.py:185][0m |          -0.0039 |         188.1033 |          -9.0716 |
[32m[20221214 14:14:17 @agent_ppo2.py:185][0m |           0.0002 |         184.8596 |          -8.9906 |
[32m[20221214 14:14:17 @agent_ppo2.py:185][0m |          -0.0042 |         182.9771 |          -9.1270 |
[32m[20221214 14:14:18 @agent_ppo2.py:185][0m |          -0.0016 |         182.1103 |          -9.1322 |
[32m[20221214 14:14:18 @agent_ppo2.py:185][0m |          -0.0047 |         181.4626 |          -9.0809 |
[32m[20221214 14:14:18 @agent_ppo2.py:185][0m |          -0.0035 |         180.4068 |          -9.1841 |
[32m[20221214 14:14:18 @agent_ppo2.py:185][0m |           0.0020 |         180.9866 |          -9.1733 |
[32m[20221214 14:14:18 @agent_ppo2.py:185][0m |          -0.0039 |         178.7270 |          -9.2629 |
[32m[20221214 14:14:18 @agent_ppo2.py:185][0m |          -0.0038 |         177.7699 |          -9.2170 |
[32m[20221214 14:14:18 @agent_ppo2.py:185][0m |          -0.0029 |         177.2763 |          -9.1909 |
[32m[20221214 14:14:18 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:14:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.26
[32m[20221214 14:14:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.31
[32m[20221214 14:14:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.78
[32m[20221214 14:14:18 @agent_ppo2.py:143][0m Total time:      16.27 min
[32m[20221214 14:14:18 @agent_ppo2.py:145][0m 1492992 total steps have happened
[32m[20221214 14:14:18 @agent_ppo2.py:121][0m #------------------------ Iteration 729 --------------------------#
[32m[20221214 14:14:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:19 @agent_ppo2.py:185][0m |           0.0026 |         159.1748 |          -9.2083 |
[32m[20221214 14:14:19 @agent_ppo2.py:185][0m |          -0.0016 |         153.2390 |          -9.1979 |
[32m[20221214 14:14:19 @agent_ppo2.py:185][0m |          -0.0019 |         150.8536 |          -9.2345 |
[32m[20221214 14:14:19 @agent_ppo2.py:185][0m |          -0.0023 |         151.2925 |          -9.2460 |
[32m[20221214 14:14:19 @agent_ppo2.py:185][0m |          -0.0033 |         150.8679 |          -9.2362 |
[32m[20221214 14:14:19 @agent_ppo2.py:185][0m |          -0.0020 |         150.4283 |          -9.1886 |
[32m[20221214 14:14:19 @agent_ppo2.py:185][0m |          -0.0011 |         149.5060 |          -9.2031 |
[32m[20221214 14:14:19 @agent_ppo2.py:185][0m |          -0.0032 |         149.0258 |          -9.1736 |
[32m[20221214 14:14:19 @agent_ppo2.py:185][0m |          -0.0025 |         148.8311 |          -9.2042 |
[32m[20221214 14:14:19 @agent_ppo2.py:185][0m |          -0.0035 |         148.8271 |          -9.1573 |
[32m[20221214 14:14:19 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:14:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 760.96
[32m[20221214 14:14:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.11
[32m[20221214 14:14:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 775.76
[32m[20221214 14:14:20 @agent_ppo2.py:143][0m Total time:      16.29 min
[32m[20221214 14:14:20 @agent_ppo2.py:145][0m 1495040 total steps have happened
[32m[20221214 14:14:20 @agent_ppo2.py:121][0m #------------------------ Iteration 730 --------------------------#
[32m[20221214 14:14:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:14:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:20 @agent_ppo2.py:185][0m |           0.0028 |         178.5218 |          -8.9905 |
[32m[20221214 14:14:20 @agent_ppo2.py:185][0m |          -0.0026 |         171.8354 |          -8.9035 |
[32m[20221214 14:14:20 @agent_ppo2.py:185][0m |          -0.0035 |         170.6625 |          -8.9196 |
[32m[20221214 14:14:20 @agent_ppo2.py:185][0m |          -0.0020 |         169.9455 |          -8.9512 |
[32m[20221214 14:14:20 @agent_ppo2.py:185][0m |           0.0066 |         177.2115 |          -8.9661 |
[32m[20221214 14:14:20 @agent_ppo2.py:185][0m |          -0.0030 |         170.1909 |          -8.9870 |
[32m[20221214 14:14:21 @agent_ppo2.py:185][0m |           0.0087 |         188.9867 |          -9.0359 |
[32m[20221214 14:14:21 @agent_ppo2.py:185][0m |           0.0007 |         169.5792 |          -8.9950 |
[32m[20221214 14:14:21 @agent_ppo2.py:185][0m |          -0.0026 |         168.9838 |          -9.1180 |
[32m[20221214 14:14:21 @agent_ppo2.py:185][0m |          -0.0018 |         169.2633 |          -9.1367 |
[32m[20221214 14:14:21 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:14:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 765.05
[32m[20221214 14:14:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.13
[32m[20221214 14:14:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 754.73
[32m[20221214 14:14:21 @agent_ppo2.py:143][0m Total time:      16.32 min
[32m[20221214 14:14:21 @agent_ppo2.py:145][0m 1497088 total steps have happened
[32m[20221214 14:14:21 @agent_ppo2.py:121][0m #------------------------ Iteration 731 --------------------------#
[32m[20221214 14:14:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:21 @agent_ppo2.py:185][0m |           0.0004 |          80.0519 |          -9.8242 |
[32m[20221214 14:14:21 @agent_ppo2.py:185][0m |          -0.0049 |          77.2792 |          -9.8854 |
[32m[20221214 14:14:22 @agent_ppo2.py:185][0m |          -0.0000 |          76.0493 |          -9.9152 |
[32m[20221214 14:14:22 @agent_ppo2.py:185][0m |          -0.0007 |          75.3330 |          -9.9543 |
[32m[20221214 14:14:22 @agent_ppo2.py:185][0m |           0.0039 |          76.7831 |          -9.9214 |
[32m[20221214 14:14:22 @agent_ppo2.py:185][0m |           0.0000 |          75.9552 |          -9.9256 |
[32m[20221214 14:14:22 @agent_ppo2.py:185][0m |          -0.0046 |          74.9752 |         -10.0596 |
[32m[20221214 14:14:22 @agent_ppo2.py:185][0m |           0.0007 |          76.1312 |         -10.0992 |
[32m[20221214 14:14:22 @agent_ppo2.py:185][0m |          -0.0063 |          74.5962 |         -10.1614 |
[32m[20221214 14:14:22 @agent_ppo2.py:185][0m |          -0.0013 |          74.1113 |         -10.1433 |
[32m[20221214 14:14:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:14:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 737.43
[32m[20221214 14:14:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.38
[32m[20221214 14:14:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 761.27
[32m[20221214 14:14:22 @agent_ppo2.py:143][0m Total time:      16.34 min
[32m[20221214 14:14:22 @agent_ppo2.py:145][0m 1499136 total steps have happened
[32m[20221214 14:14:22 @agent_ppo2.py:121][0m #------------------------ Iteration 732 --------------------------#
[32m[20221214 14:14:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:23 @agent_ppo2.py:185][0m |          -0.0036 |         124.1592 |         -10.0572 |
[32m[20221214 14:14:23 @agent_ppo2.py:185][0m |          -0.0058 |         119.6725 |         -10.0204 |
[32m[20221214 14:14:23 @agent_ppo2.py:185][0m |          -0.0057 |         117.8195 |          -9.9908 |
[32m[20221214 14:14:23 @agent_ppo2.py:185][0m |          -0.0061 |         116.7826 |         -10.0375 |
[32m[20221214 14:14:23 @agent_ppo2.py:185][0m |          -0.0047 |         116.6707 |         -10.0726 |
[32m[20221214 14:14:23 @agent_ppo2.py:185][0m |          -0.0046 |         116.3765 |         -10.0684 |
[32m[20221214 14:14:23 @agent_ppo2.py:185][0m |          -0.0068 |         116.2911 |         -10.0575 |
[32m[20221214 14:14:23 @agent_ppo2.py:185][0m |          -0.0059 |         115.9137 |         -10.0652 |
[32m[20221214 14:14:23 @agent_ppo2.py:185][0m |          -0.0069 |         115.6732 |         -10.0636 |
[32m[20221214 14:14:24 @agent_ppo2.py:185][0m |          -0.0046 |         115.1756 |         -10.0800 |
[32m[20221214 14:14:24 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:14:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.87
[32m[20221214 14:14:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 760.33
[32m[20221214 14:14:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 772.77
[32m[20221214 14:14:24 @agent_ppo2.py:143][0m Total time:      16.36 min
[32m[20221214 14:14:24 @agent_ppo2.py:145][0m 1501184 total steps have happened
[32m[20221214 14:14:24 @agent_ppo2.py:121][0m #------------------------ Iteration 733 --------------------------#
[32m[20221214 14:14:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:14:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:24 @agent_ppo2.py:185][0m |          -0.0044 |         177.9993 |          -9.8872 |
[32m[20221214 14:14:24 @agent_ppo2.py:185][0m |          -0.0036 |         171.3527 |          -9.9721 |
[32m[20221214 14:14:24 @agent_ppo2.py:185][0m |          -0.0060 |         169.3154 |          -9.9156 |
[32m[20221214 14:14:24 @agent_ppo2.py:185][0m |           0.0005 |         171.8933 |          -9.9983 |
[32m[20221214 14:14:24 @agent_ppo2.py:185][0m |          -0.0046 |         167.1634 |         -10.0060 |
[32m[20221214 14:14:24 @agent_ppo2.py:185][0m |          -0.0058 |         166.2728 |         -10.1139 |
[32m[20221214 14:14:25 @agent_ppo2.py:185][0m |           0.0007 |         171.6314 |         -10.2214 |
[32m[20221214 14:14:25 @agent_ppo2.py:185][0m |          -0.0068 |         165.7244 |         -10.1780 |
[32m[20221214 14:14:25 @agent_ppo2.py:185][0m |          -0.0058 |         164.7460 |         -10.2292 |
[32m[20221214 14:14:25 @agent_ppo2.py:185][0m |          -0.0051 |         164.0843 |         -10.2611 |
[32m[20221214 14:14:25 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:14:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 761.88
[32m[20221214 14:14:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.84
[32m[20221214 14:14:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 783.01
[32m[20221214 14:14:25 @agent_ppo2.py:143][0m Total time:      16.38 min
[32m[20221214 14:14:25 @agent_ppo2.py:145][0m 1503232 total steps have happened
[32m[20221214 14:14:25 @agent_ppo2.py:121][0m #------------------------ Iteration 734 --------------------------#
[32m[20221214 14:14:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:25 @agent_ppo2.py:185][0m |          -0.0036 |         208.0624 |         -10.9555 |
[32m[20221214 14:14:25 @agent_ppo2.py:185][0m |          -0.0020 |         198.5196 |         -10.9608 |
[32m[20221214 14:14:26 @agent_ppo2.py:185][0m |          -0.0041 |         196.7940 |         -10.9302 |
[32m[20221214 14:14:26 @agent_ppo2.py:185][0m |          -0.0019 |         195.0938 |         -10.9647 |
[32m[20221214 14:14:26 @agent_ppo2.py:185][0m |           0.0090 |         210.4986 |         -10.9151 |
[32m[20221214 14:14:26 @agent_ppo2.py:185][0m |          -0.0036 |         193.5410 |         -10.9197 |
[32m[20221214 14:14:26 @agent_ppo2.py:185][0m |          -0.0023 |         194.3524 |         -10.9508 |
[32m[20221214 14:14:26 @agent_ppo2.py:185][0m |          -0.0042 |         192.6193 |         -10.9460 |
[32m[20221214 14:14:26 @agent_ppo2.py:185][0m |          -0.0037 |         192.1089 |         -10.9922 |
[32m[20221214 14:14:26 @agent_ppo2.py:185][0m |           0.0015 |         192.3826 |         -10.9513 |
[32m[20221214 14:14:26 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:14:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.47
[32m[20221214 14:14:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.88
[32m[20221214 14:14:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 787.63
[32m[20221214 14:14:26 @agent_ppo2.py:143][0m Total time:      16.40 min
[32m[20221214 14:14:26 @agent_ppo2.py:145][0m 1505280 total steps have happened
[32m[20221214 14:14:26 @agent_ppo2.py:121][0m #------------------------ Iteration 735 --------------------------#
[32m[20221214 14:14:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:27 @agent_ppo2.py:185][0m |           0.0027 |         205.6982 |         -11.3236 |
[32m[20221214 14:14:27 @agent_ppo2.py:185][0m |           0.0099 |         210.3553 |         -11.3280 |
[32m[20221214 14:14:27 @agent_ppo2.py:185][0m |           0.0007 |         199.6538 |         -11.2057 |
[32m[20221214 14:14:27 @agent_ppo2.py:185][0m |          -0.0001 |         199.3636 |         -11.2428 |
[32m[20221214 14:14:27 @agent_ppo2.py:185][0m |          -0.0034 |         199.1355 |         -11.1707 |
[32m[20221214 14:14:27 @agent_ppo2.py:185][0m |          -0.0022 |         198.3752 |         -11.1889 |
[32m[20221214 14:14:27 @agent_ppo2.py:185][0m |          -0.0030 |         197.7259 |         -11.1671 |
[32m[20221214 14:14:27 @agent_ppo2.py:185][0m |          -0.0024 |         198.4890 |         -11.1817 |
[32m[20221214 14:14:27 @agent_ppo2.py:185][0m |           0.0083 |         212.3729 |         -11.1856 |
[32m[20221214 14:14:28 @agent_ppo2.py:185][0m |          -0.0026 |         199.2962 |         -11.1432 |
[32m[20221214 14:14:28 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:14:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.58
[32m[20221214 14:14:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.77
[32m[20221214 14:14:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.23
[32m[20221214 14:14:28 @agent_ppo2.py:143][0m Total time:      16.43 min
[32m[20221214 14:14:28 @agent_ppo2.py:145][0m 1507328 total steps have happened
[32m[20221214 14:14:28 @agent_ppo2.py:121][0m #------------------------ Iteration 736 --------------------------#
[32m[20221214 14:14:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:28 @agent_ppo2.py:185][0m |          -0.0018 |         194.4541 |         -10.1449 |
[32m[20221214 14:14:28 @agent_ppo2.py:185][0m |           0.0029 |         190.6397 |         -10.0244 |
[32m[20221214 14:14:28 @agent_ppo2.py:185][0m |           0.0034 |         190.0166 |         -10.1845 |
[32m[20221214 14:14:28 @agent_ppo2.py:185][0m |           0.0022 |         189.8986 |         -10.0085 |
[32m[20221214 14:14:28 @agent_ppo2.py:185][0m |          -0.0010 |         183.8832 |          -9.9476 |
[32m[20221214 14:14:29 @agent_ppo2.py:185][0m |          -0.0012 |         183.5476 |         -10.1015 |
[32m[20221214 14:14:29 @agent_ppo2.py:185][0m |           0.0079 |         192.3457 |          -9.9815 |
[32m[20221214 14:14:29 @agent_ppo2.py:185][0m |          -0.0010 |         183.2702 |          -9.9592 |
[32m[20221214 14:14:29 @agent_ppo2.py:185][0m |          -0.0015 |         182.0175 |          -9.9654 |
[32m[20221214 14:14:29 @agent_ppo2.py:185][0m |          -0.0034 |         182.2846 |          -9.9680 |
[32m[20221214 14:14:29 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:14:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.96
[32m[20221214 14:14:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.54
[32m[20221214 14:14:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.70
[32m[20221214 14:14:29 @agent_ppo2.py:143][0m Total time:      16.45 min
[32m[20221214 14:14:29 @agent_ppo2.py:145][0m 1509376 total steps have happened
[32m[20221214 14:14:29 @agent_ppo2.py:121][0m #------------------------ Iteration 737 --------------------------#
[32m[20221214 14:14:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:29 @agent_ppo2.py:185][0m |          -0.0031 |         196.4030 |         -10.8381 |
[32m[20221214 14:14:30 @agent_ppo2.py:185][0m |          -0.0016 |         191.6379 |         -10.8733 |
[32m[20221214 14:14:30 @agent_ppo2.py:185][0m |          -0.0016 |         189.2649 |         -10.9537 |
[32m[20221214 14:14:30 @agent_ppo2.py:185][0m |          -0.0012 |         189.0663 |         -10.9994 |
[32m[20221214 14:14:30 @agent_ppo2.py:185][0m |          -0.0021 |         188.3860 |         -11.0174 |
[32m[20221214 14:14:30 @agent_ppo2.py:185][0m |          -0.0014 |         187.7710 |         -11.0620 |
[32m[20221214 14:14:30 @agent_ppo2.py:185][0m |          -0.0030 |         186.9364 |         -11.0381 |
[32m[20221214 14:14:30 @agent_ppo2.py:185][0m |          -0.0011 |         186.6446 |         -11.0675 |
[32m[20221214 14:14:30 @agent_ppo2.py:185][0m |          -0.0009 |         186.1600 |         -11.0919 |
[32m[20221214 14:14:30 @agent_ppo2.py:185][0m |           0.0009 |         187.2871 |         -11.1170 |
[32m[20221214 14:14:30 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:14:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.42
[32m[20221214 14:14:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.66
[32m[20221214 14:14:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.28
[32m[20221214 14:14:30 @agent_ppo2.py:143][0m Total time:      16.47 min
[32m[20221214 14:14:30 @agent_ppo2.py:145][0m 1511424 total steps have happened
[32m[20221214 14:14:30 @agent_ppo2.py:121][0m #------------------------ Iteration 738 --------------------------#
[32m[20221214 14:14:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:31 @agent_ppo2.py:185][0m |          -0.0021 |         189.6360 |         -10.4855 |
[32m[20221214 14:14:31 @agent_ppo2.py:185][0m |           0.0001 |         187.7685 |         -10.4145 |
[32m[20221214 14:14:31 @agent_ppo2.py:185][0m |           0.0008 |         187.1676 |         -10.3692 |
[32m[20221214 14:14:31 @agent_ppo2.py:185][0m |          -0.0019 |         186.1415 |         -10.4146 |
[32m[20221214 14:14:31 @agent_ppo2.py:185][0m |          -0.0005 |         185.0880 |         -10.2639 |
[32m[20221214 14:14:31 @agent_ppo2.py:185][0m |          -0.0013 |         184.8483 |         -10.3241 |
[32m[20221214 14:14:31 @agent_ppo2.py:185][0m |          -0.0017 |         184.8717 |         -10.3209 |
[32m[20221214 14:14:31 @agent_ppo2.py:185][0m |           0.0019 |         184.9075 |         -10.2551 |
[32m[20221214 14:14:31 @agent_ppo2.py:185][0m |           0.0177 |         210.2121 |         -10.2066 |
[32m[20221214 14:14:31 @agent_ppo2.py:185][0m |           0.0239 |         224.5389 |         -10.1349 |
[32m[20221214 14:14:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:14:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.11
[32m[20221214 14:14:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.21
[32m[20221214 14:14:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.40
[32m[20221214 14:14:32 @agent_ppo2.py:143][0m Total time:      16.49 min
[32m[20221214 14:14:32 @agent_ppo2.py:145][0m 1513472 total steps have happened
[32m[20221214 14:14:32 @agent_ppo2.py:121][0m #------------------------ Iteration 739 --------------------------#
[32m[20221214 14:14:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:32 @agent_ppo2.py:185][0m |          -0.0007 |         188.6322 |         -10.5659 |
[32m[20221214 14:14:32 @agent_ppo2.py:185][0m |           0.0004 |         183.3818 |         -10.6335 |
[32m[20221214 14:14:32 @agent_ppo2.py:185][0m |          -0.0023 |         182.0617 |         -10.7186 |
[32m[20221214 14:14:32 @agent_ppo2.py:185][0m |          -0.0010 |         180.6279 |         -10.6796 |
[32m[20221214 14:14:32 @agent_ppo2.py:185][0m |           0.0011 |         181.2680 |         -10.7512 |
[32m[20221214 14:14:32 @agent_ppo2.py:185][0m |           0.0002 |         179.2197 |         -10.7917 |
[32m[20221214 14:14:32 @agent_ppo2.py:185][0m |          -0.0015 |         179.0278 |         -10.7927 |
[32m[20221214 14:14:33 @agent_ppo2.py:185][0m |          -0.0027 |         178.8083 |         -10.9184 |
[32m[20221214 14:14:33 @agent_ppo2.py:185][0m |          -0.0014 |         178.5505 |         -10.8784 |
[32m[20221214 14:14:33 @agent_ppo2.py:185][0m |          -0.0034 |         178.1116 |         -10.7989 |
[32m[20221214 14:14:33 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:14:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.74
[32m[20221214 14:14:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.73
[32m[20221214 14:14:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.33
[32m[20221214 14:14:33 @agent_ppo2.py:143][0m Total time:      16.51 min
[32m[20221214 14:14:33 @agent_ppo2.py:145][0m 1515520 total steps have happened
[32m[20221214 14:14:33 @agent_ppo2.py:121][0m #------------------------ Iteration 740 --------------------------#
[32m[20221214 14:14:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:14:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:33 @agent_ppo2.py:185][0m |          -0.0030 |         182.1342 |         -11.0060 |
[32m[20221214 14:14:33 @agent_ppo2.py:185][0m |          -0.0023 |         179.0105 |         -11.0181 |
[32m[20221214 14:14:34 @agent_ppo2.py:185][0m |          -0.0016 |         177.2465 |         -11.1074 |
[32m[20221214 14:14:34 @agent_ppo2.py:185][0m |          -0.0023 |         175.9479 |         -11.0164 |
[32m[20221214 14:14:34 @agent_ppo2.py:185][0m |           0.0012 |         175.1343 |         -11.0045 |
[32m[20221214 14:14:34 @agent_ppo2.py:185][0m |          -0.0023 |         173.7921 |         -11.0174 |
[32m[20221214 14:14:34 @agent_ppo2.py:185][0m |          -0.0020 |         174.1082 |         -10.9733 |
[32m[20221214 14:14:34 @agent_ppo2.py:185][0m |           0.0020 |         174.0016 |         -10.9402 |
[32m[20221214 14:14:34 @agent_ppo2.py:185][0m |          -0.0018 |         173.2325 |         -10.9515 |
[32m[20221214 14:14:34 @agent_ppo2.py:185][0m |           0.0004 |         172.7436 |         -10.9435 |
[32m[20221214 14:14:34 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 14:14:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.37
[32m[20221214 14:14:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.83
[32m[20221214 14:14:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.44
[32m[20221214 14:14:34 @agent_ppo2.py:143][0m Total time:      16.54 min
[32m[20221214 14:14:34 @agent_ppo2.py:145][0m 1517568 total steps have happened
[32m[20221214 14:14:34 @agent_ppo2.py:121][0m #------------------------ Iteration 741 --------------------------#
[32m[20221214 14:14:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:35 @agent_ppo2.py:185][0m |          -0.0022 |         173.9142 |         -10.4272 |
[32m[20221214 14:14:35 @agent_ppo2.py:185][0m |          -0.0026 |         173.0800 |         -10.4138 |
[32m[20221214 14:14:35 @agent_ppo2.py:185][0m |           0.0095 |         191.5583 |         -10.3927 |
[32m[20221214 14:14:35 @agent_ppo2.py:185][0m |           0.0105 |         198.2885 |         -10.4137 |
[32m[20221214 14:14:35 @agent_ppo2.py:185][0m |          -0.0028 |         172.4873 |         -10.3637 |
[32m[20221214 14:14:35 @agent_ppo2.py:185][0m |          -0.0032 |         171.5882 |         -10.3727 |
[32m[20221214 14:14:35 @agent_ppo2.py:185][0m |          -0.0039 |         171.6909 |         -10.4259 |
[32m[20221214 14:14:35 @agent_ppo2.py:185][0m |          -0.0039 |         171.4980 |         -10.4528 |
[32m[20221214 14:14:35 @agent_ppo2.py:185][0m |          -0.0014 |         171.9936 |         -10.4423 |
[32m[20221214 14:14:36 @agent_ppo2.py:185][0m |          -0.0036 |         171.4499 |         -10.4408 |
[32m[20221214 14:14:36 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:14:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.72
[32m[20221214 14:14:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.76
[32m[20221214 14:14:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.98
[32m[20221214 14:14:36 @agent_ppo2.py:143][0m Total time:      16.56 min
[32m[20221214 14:14:36 @agent_ppo2.py:145][0m 1519616 total steps have happened
[32m[20221214 14:14:36 @agent_ppo2.py:121][0m #------------------------ Iteration 742 --------------------------#
[32m[20221214 14:14:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:14:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:36 @agent_ppo2.py:185][0m |          -0.0023 |         175.1133 |         -10.0216 |
[32m[20221214 14:14:36 @agent_ppo2.py:185][0m |          -0.0012 |         173.1349 |         -10.0498 |
[32m[20221214 14:14:36 @agent_ppo2.py:185][0m |           0.0092 |         182.6341 |         -10.0746 |
[32m[20221214 14:14:36 @agent_ppo2.py:185][0m |          -0.0005 |         172.2912 |         -10.0588 |
[32m[20221214 14:14:36 @agent_ppo2.py:185][0m |          -0.0005 |         171.3295 |         -10.0189 |
[32m[20221214 14:14:37 @agent_ppo2.py:185][0m |          -0.0016 |         171.0463 |         -10.0651 |
[32m[20221214 14:14:37 @agent_ppo2.py:185][0m |           0.0041 |         175.3115 |         -10.1283 |
[32m[20221214 14:14:37 @agent_ppo2.py:185][0m |          -0.0017 |         170.6813 |         -10.1158 |
[32m[20221214 14:14:37 @agent_ppo2.py:185][0m |          -0.0005 |         170.2276 |         -10.0647 |
[32m[20221214 14:14:37 @agent_ppo2.py:185][0m |           0.0106 |         183.5275 |         -10.0784 |
[32m[20221214 14:14:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:14:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.60
[32m[20221214 14:14:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.01
[32m[20221214 14:14:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.44
[32m[20221214 14:14:37 @agent_ppo2.py:143][0m Total time:      16.58 min
[32m[20221214 14:14:37 @agent_ppo2.py:145][0m 1521664 total steps have happened
[32m[20221214 14:14:37 @agent_ppo2.py:121][0m #------------------------ Iteration 743 --------------------------#
[32m[20221214 14:14:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:14:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:37 @agent_ppo2.py:185][0m |          -0.0008 |         175.4987 |         -10.4998 |
[32m[20221214 14:14:37 @agent_ppo2.py:185][0m |           0.0040 |         175.6050 |         -10.4175 |
[32m[20221214 14:14:38 @agent_ppo2.py:185][0m |          -0.0013 |         170.0768 |         -10.4109 |
[32m[20221214 14:14:38 @agent_ppo2.py:185][0m |          -0.0022 |         169.0400 |         -10.3424 |
[32m[20221214 14:14:38 @agent_ppo2.py:185][0m |          -0.0022 |         168.0474 |         -10.3770 |
[32m[20221214 14:14:38 @agent_ppo2.py:185][0m |          -0.0032 |         167.8275 |         -10.3556 |
[32m[20221214 14:14:38 @agent_ppo2.py:185][0m |           0.0056 |         174.8808 |         -10.3427 |
[32m[20221214 14:14:38 @agent_ppo2.py:185][0m |          -0.0029 |         166.6040 |         -10.3064 |
[32m[20221214 14:14:38 @agent_ppo2.py:185][0m |          -0.0032 |         166.0273 |         -10.2630 |
[32m[20221214 14:14:38 @agent_ppo2.py:185][0m |          -0.0031 |         165.7205 |         -10.2018 |
[32m[20221214 14:14:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:14:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.29
[32m[20221214 14:14:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.74
[32m[20221214 14:14:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.87
[32m[20221214 14:14:38 @agent_ppo2.py:143][0m Total time:      16.60 min
[32m[20221214 14:14:38 @agent_ppo2.py:145][0m 1523712 total steps have happened
[32m[20221214 14:14:38 @agent_ppo2.py:121][0m #------------------------ Iteration 744 --------------------------#
[32m[20221214 14:14:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:39 @agent_ppo2.py:185][0m |          -0.0004 |         182.9449 |          -9.9784 |
[32m[20221214 14:14:39 @agent_ppo2.py:185][0m |          -0.0019 |         181.9546 |          -9.8975 |
[32m[20221214 14:14:39 @agent_ppo2.py:185][0m |          -0.0010 |         181.0716 |          -9.9233 |
[32m[20221214 14:14:39 @agent_ppo2.py:185][0m |          -0.0001 |         180.7675 |          -9.8205 |
[32m[20221214 14:14:39 @agent_ppo2.py:185][0m |          -0.0019 |         180.2713 |          -9.8448 |
[32m[20221214 14:14:39 @agent_ppo2.py:185][0m |          -0.0012 |         180.1532 |          -9.7912 |
[32m[20221214 14:14:39 @agent_ppo2.py:185][0m |          -0.0027 |         180.0496 |          -9.5794 |
[32m[20221214 14:14:39 @agent_ppo2.py:185][0m |          -0.0020 |         179.6495 |          -9.7297 |
[32m[20221214 14:14:39 @agent_ppo2.py:185][0m |          -0.0028 |         179.3257 |          -9.6946 |
[32m[20221214 14:14:39 @agent_ppo2.py:185][0m |           0.0036 |         184.9927 |          -9.6306 |
[32m[20221214 14:14:39 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:14:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.84
[32m[20221214 14:14:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.30
[32m[20221214 14:14:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.77
[32m[20221214 14:14:40 @agent_ppo2.py:143][0m Total time:      16.63 min
[32m[20221214 14:14:40 @agent_ppo2.py:145][0m 1525760 total steps have happened
[32m[20221214 14:14:40 @agent_ppo2.py:121][0m #------------------------ Iteration 745 --------------------------#
[32m[20221214 14:14:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:14:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:40 @agent_ppo2.py:185][0m |          -0.0005 |         185.4018 |          -9.7772 |
[32m[20221214 14:14:40 @agent_ppo2.py:185][0m |          -0.0023 |         182.0880 |          -9.9002 |
[32m[20221214 14:14:40 @agent_ppo2.py:185][0m |          -0.0032 |         180.2522 |          -9.8767 |
[32m[20221214 14:14:40 @agent_ppo2.py:185][0m |          -0.0016 |         180.8796 |         -10.0426 |
[32m[20221214 14:14:40 @agent_ppo2.py:185][0m |          -0.0017 |         178.9361 |         -10.1463 |
[32m[20221214 14:14:40 @agent_ppo2.py:185][0m |          -0.0031 |         178.3736 |         -10.0602 |
[32m[20221214 14:14:40 @agent_ppo2.py:185][0m |           0.0019 |         179.1294 |         -10.2192 |
[32m[20221214 14:14:41 @agent_ppo2.py:185][0m |          -0.0010 |         177.6843 |         -10.2736 |
[32m[20221214 14:14:41 @agent_ppo2.py:185][0m |          -0.0042 |         176.7803 |         -10.2391 |
[32m[20221214 14:14:41 @agent_ppo2.py:185][0m |          -0.0010 |         176.3014 |         -10.3192 |
[32m[20221214 14:14:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:14:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.23
[32m[20221214 14:14:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.00
[32m[20221214 14:14:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.00
[32m[20221214 14:14:41 @agent_ppo2.py:143][0m Total time:      16.65 min
[32m[20221214 14:14:41 @agent_ppo2.py:145][0m 1527808 total steps have happened
[32m[20221214 14:14:41 @agent_ppo2.py:121][0m #------------------------ Iteration 746 --------------------------#
[32m[20221214 14:14:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:41 @agent_ppo2.py:185][0m |          -0.0025 |         181.5523 |         -10.2781 |
[32m[20221214 14:14:41 @agent_ppo2.py:185][0m |          -0.0008 |         178.5203 |         -10.2996 |
[32m[20221214 14:14:41 @agent_ppo2.py:185][0m |           0.0016 |         177.2962 |         -10.2970 |
[32m[20221214 14:14:42 @agent_ppo2.py:185][0m |          -0.0016 |         176.2786 |         -10.3218 |
[32m[20221214 14:14:42 @agent_ppo2.py:185][0m |          -0.0008 |         175.2970 |         -10.2970 |
[32m[20221214 14:14:42 @agent_ppo2.py:185][0m |          -0.0024 |         175.0482 |         -10.3311 |
[32m[20221214 14:14:42 @agent_ppo2.py:185][0m |          -0.0016 |         174.4006 |         -10.3596 |
[32m[20221214 14:14:42 @agent_ppo2.py:185][0m |           0.0025 |         175.4899 |         -10.2833 |
[32m[20221214 14:14:42 @agent_ppo2.py:185][0m |           0.0107 |         189.5018 |         -10.3143 |
[32m[20221214 14:14:42 @agent_ppo2.py:185][0m |          -0.0023 |         173.3977 |         -10.3328 |
[32m[20221214 14:14:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:14:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.97
[32m[20221214 14:14:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.44
[32m[20221214 14:14:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 801.35
[32m[20221214 14:14:42 @agent_ppo2.py:143][0m Total time:      16.67 min
[32m[20221214 14:14:42 @agent_ppo2.py:145][0m 1529856 total steps have happened
[32m[20221214 14:14:42 @agent_ppo2.py:121][0m #------------------------ Iteration 747 --------------------------#
[32m[20221214 14:14:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:14:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:43 @agent_ppo2.py:185][0m |          -0.0030 |         174.2697 |         -10.0722 |
[32m[20221214 14:14:43 @agent_ppo2.py:185][0m |          -0.0008 |         173.1508 |         -10.2103 |
[32m[20221214 14:14:43 @agent_ppo2.py:185][0m |          -0.0023 |         172.7708 |         -10.1352 |
[32m[20221214 14:14:43 @agent_ppo2.py:185][0m |           0.0011 |         173.3830 |         -10.1172 |
[32m[20221214 14:14:43 @agent_ppo2.py:185][0m |          -0.0024 |         172.7445 |         -10.1413 |
[32m[20221214 14:14:43 @agent_ppo2.py:185][0m |          -0.0009 |         171.9790 |         -10.1312 |
[32m[20221214 14:14:43 @agent_ppo2.py:185][0m |           0.0141 |         193.3542 |         -10.1567 |
[32m[20221214 14:14:43 @agent_ppo2.py:185][0m |          -0.0031 |         171.9364 |         -10.2987 |
[32m[20221214 14:14:43 @agent_ppo2.py:185][0m |          -0.0021 |         171.6325 |         -10.2377 |
[32m[20221214 14:14:43 @agent_ppo2.py:185][0m |          -0.0010 |         171.7388 |         -10.2022 |
[32m[20221214 14:14:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:14:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.94
[32m[20221214 14:14:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.07
[32m[20221214 14:14:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.14
[32m[20221214 14:14:43 @agent_ppo2.py:143][0m Total time:      16.69 min
[32m[20221214 14:14:43 @agent_ppo2.py:145][0m 1531904 total steps have happened
[32m[20221214 14:14:43 @agent_ppo2.py:121][0m #------------------------ Iteration 748 --------------------------#
[32m[20221214 14:14:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:14:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:44 @agent_ppo2.py:185][0m |          -0.0016 |         174.0739 |         -10.3318 |
[32m[20221214 14:14:44 @agent_ppo2.py:185][0m |          -0.0029 |         173.1648 |         -10.3282 |
[32m[20221214 14:14:44 @agent_ppo2.py:185][0m |          -0.0036 |         171.2516 |         -10.3393 |
[32m[20221214 14:14:44 @agent_ppo2.py:185][0m |          -0.0004 |         171.3280 |         -10.3151 |
[32m[20221214 14:14:44 @agent_ppo2.py:185][0m |           0.0118 |         196.2846 |         -10.3899 |
[32m[20221214 14:14:44 @agent_ppo2.py:185][0m |          -0.0010 |         170.6949 |         -10.4456 |
[32m[20221214 14:14:44 @agent_ppo2.py:185][0m |          -0.0028 |         169.5294 |         -10.3940 |
[32m[20221214 14:14:44 @agent_ppo2.py:185][0m |          -0.0019 |         169.4208 |         -10.4535 |
[32m[20221214 14:14:44 @agent_ppo2.py:185][0m |          -0.0031 |         169.2563 |         -10.4542 |
[32m[20221214 14:14:45 @agent_ppo2.py:185][0m |          -0.0015 |         168.9322 |         -10.4727 |
[32m[20221214 14:14:45 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:14:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.22
[32m[20221214 14:14:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.39
[32m[20221214 14:14:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.61
[32m[20221214 14:14:45 @agent_ppo2.py:143][0m Total time:      16.71 min
[32m[20221214 14:14:45 @agent_ppo2.py:145][0m 1533952 total steps have happened
[32m[20221214 14:14:45 @agent_ppo2.py:121][0m #------------------------ Iteration 749 --------------------------#
[32m[20221214 14:14:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:45 @agent_ppo2.py:185][0m |           0.0139 |         195.7226 |         -10.8908 |
[32m[20221214 14:14:45 @agent_ppo2.py:185][0m |          -0.0025 |         169.1403 |         -10.8424 |
[32m[20221214 14:14:45 @agent_ppo2.py:185][0m |          -0.0027 |         167.8905 |         -10.8236 |
[32m[20221214 14:14:45 @agent_ppo2.py:185][0m |           0.0017 |         168.5144 |         -10.8331 |
[32m[20221214 14:14:45 @agent_ppo2.py:185][0m |          -0.0013 |         167.2652 |         -10.7869 |
[32m[20221214 14:14:45 @agent_ppo2.py:185][0m |          -0.0017 |         166.3892 |         -10.7871 |
[32m[20221214 14:14:46 @agent_ppo2.py:185][0m |          -0.0010 |         166.5180 |         -10.8030 |
[32m[20221214 14:14:46 @agent_ppo2.py:185][0m |          -0.0029 |         166.6698 |         -10.7504 |
[32m[20221214 14:14:46 @agent_ppo2.py:185][0m |           0.0029 |         170.0771 |         -10.6803 |
[32m[20221214 14:14:46 @agent_ppo2.py:185][0m |          -0.0019 |         166.2105 |         -10.7446 |
[32m[20221214 14:14:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:14:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.50
[32m[20221214 14:14:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.62
[32m[20221214 14:14:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.18
[32m[20221214 14:14:46 @agent_ppo2.py:143][0m Total time:      16.73 min
[32m[20221214 14:14:46 @agent_ppo2.py:145][0m 1536000 total steps have happened
[32m[20221214 14:14:46 @agent_ppo2.py:121][0m #------------------------ Iteration 750 --------------------------#
[32m[20221214 14:14:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:14:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:46 @agent_ppo2.py:185][0m |          -0.0024 |         167.0650 |         -11.1176 |
[32m[20221214 14:14:46 @agent_ppo2.py:185][0m |          -0.0071 |         164.2214 |         -11.0757 |
[32m[20221214 14:14:46 @agent_ppo2.py:185][0m |          -0.0053 |         163.1936 |         -11.0492 |
[32m[20221214 14:14:47 @agent_ppo2.py:185][0m |          -0.0024 |         163.6486 |         -11.1362 |
[32m[20221214 14:14:47 @agent_ppo2.py:185][0m |          -0.0046 |         161.3487 |         -11.1120 |
[32m[20221214 14:14:47 @agent_ppo2.py:185][0m |          -0.0040 |         160.5765 |         -11.1250 |
[32m[20221214 14:14:47 @agent_ppo2.py:185][0m |          -0.0032 |         159.9408 |         -11.0784 |
[32m[20221214 14:14:47 @agent_ppo2.py:185][0m |          -0.0044 |         159.6377 |         -11.1367 |
[32m[20221214 14:14:47 @agent_ppo2.py:185][0m |          -0.0056 |         159.2837 |         -11.1362 |
[32m[20221214 14:14:47 @agent_ppo2.py:185][0m |          -0.0040 |         159.0335 |         -11.1355 |
[32m[20221214 14:14:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:14:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 779.80
[32m[20221214 14:14:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.98
[32m[20221214 14:14:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.58
[32m[20221214 14:14:47 @agent_ppo2.py:143][0m Total time:      16.75 min
[32m[20221214 14:14:47 @agent_ppo2.py:145][0m 1538048 total steps have happened
[32m[20221214 14:14:47 @agent_ppo2.py:121][0m #------------------------ Iteration 751 --------------------------#
[32m[20221214 14:14:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:14:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:48 @agent_ppo2.py:185][0m |           0.0043 |         175.3826 |         -11.0705 |
[32m[20221214 14:14:48 @agent_ppo2.py:185][0m |          -0.0068 |         164.9340 |         -11.0955 |
[32m[20221214 14:14:48 @agent_ppo2.py:185][0m |          -0.0066 |         164.7628 |         -11.1149 |
[32m[20221214 14:14:48 @agent_ppo2.py:185][0m |          -0.0073 |         164.3754 |         -11.1088 |
[32m[20221214 14:14:48 @agent_ppo2.py:185][0m |          -0.0059 |         163.9282 |         -11.0698 |
[32m[20221214 14:14:48 @agent_ppo2.py:185][0m |          -0.0068 |         163.9699 |         -11.1327 |
[32m[20221214 14:14:48 @agent_ppo2.py:185][0m |          -0.0073 |         163.7169 |         -11.1475 |
[32m[20221214 14:14:48 @agent_ppo2.py:185][0m |          -0.0044 |         163.0972 |         -11.1170 |
[32m[20221214 14:14:48 @agent_ppo2.py:185][0m |          -0.0038 |         164.0433 |         -11.2140 |
[32m[20221214 14:14:48 @agent_ppo2.py:185][0m |          -0.0079 |         163.3188 |         -11.1467 |
[32m[20221214 14:14:48 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:14:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.88
[32m[20221214 14:14:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.23
[32m[20221214 14:14:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.19
[32m[20221214 14:14:49 @agent_ppo2.py:143][0m Total time:      16.78 min
[32m[20221214 14:14:49 @agent_ppo2.py:145][0m 1540096 total steps have happened
[32m[20221214 14:14:49 @agent_ppo2.py:121][0m #------------------------ Iteration 752 --------------------------#
[32m[20221214 14:14:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:49 @agent_ppo2.py:185][0m |          -0.0032 |         186.6373 |         -11.1346 |
[32m[20221214 14:14:49 @agent_ppo2.py:185][0m |          -0.0032 |         179.9905 |         -11.1613 |
[32m[20221214 14:14:49 @agent_ppo2.py:185][0m |          -0.0014 |         177.1446 |         -11.1519 |
[32m[20221214 14:14:49 @agent_ppo2.py:185][0m |           0.0049 |         181.3207 |         -11.1751 |
[32m[20221214 14:14:49 @agent_ppo2.py:185][0m |          -0.0045 |         175.4031 |         -11.2232 |
[32m[20221214 14:14:49 @agent_ppo2.py:185][0m |           0.0004 |         175.2408 |         -11.2334 |
[32m[20221214 14:14:50 @agent_ppo2.py:185][0m |          -0.0026 |         173.0848 |         -11.2327 |
[32m[20221214 14:14:50 @agent_ppo2.py:185][0m |          -0.0021 |         172.5401 |         -11.2730 |
[32m[20221214 14:14:50 @agent_ppo2.py:185][0m |          -0.0030 |         172.0272 |         -11.3037 |
[32m[20221214 14:14:50 @agent_ppo2.py:185][0m |           0.0030 |         177.2069 |         -11.3365 |
[32m[20221214 14:14:50 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:14:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.63
[32m[20221214 14:14:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.04
[32m[20221214 14:14:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.01
[32m[20221214 14:14:50 @agent_ppo2.py:143][0m Total time:      16.80 min
[32m[20221214 14:14:50 @agent_ppo2.py:145][0m 1542144 total steps have happened
[32m[20221214 14:14:50 @agent_ppo2.py:121][0m #------------------------ Iteration 753 --------------------------#
[32m[20221214 14:14:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:50 @agent_ppo2.py:185][0m |           0.0039 |         192.5009 |         -11.9531 |
[32m[20221214 14:14:50 @agent_ppo2.py:185][0m |          -0.0020 |         186.8608 |         -11.9364 |
[32m[20221214 14:14:51 @agent_ppo2.py:185][0m |          -0.0014 |         184.2347 |         -11.9080 |
[32m[20221214 14:14:51 @agent_ppo2.py:185][0m |          -0.0017 |         183.0890 |         -11.9376 |
[32m[20221214 14:14:51 @agent_ppo2.py:185][0m |          -0.0015 |         181.9318 |         -11.8521 |
[32m[20221214 14:14:51 @agent_ppo2.py:185][0m |          -0.0013 |         180.5162 |         -11.9017 |
[32m[20221214 14:14:51 @agent_ppo2.py:185][0m |           0.0045 |         182.6874 |         -11.9744 |
[32m[20221214 14:14:51 @agent_ppo2.py:185][0m |          -0.0017 |         179.1919 |         -11.9202 |
[32m[20221214 14:14:51 @agent_ppo2.py:185][0m |          -0.0010 |         179.4442 |         -11.9643 |
[32m[20221214 14:14:51 @agent_ppo2.py:185][0m |          -0.0018 |         178.8194 |         -11.9655 |
[32m[20221214 14:14:51 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:14:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.79
[32m[20221214 14:14:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.34
[32m[20221214 14:14:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.33
[32m[20221214 14:14:51 @agent_ppo2.py:143][0m Total time:      16.82 min
[32m[20221214 14:14:51 @agent_ppo2.py:145][0m 1544192 total steps have happened
[32m[20221214 14:14:51 @agent_ppo2.py:121][0m #------------------------ Iteration 754 --------------------------#
[32m[20221214 14:14:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:52 @agent_ppo2.py:185][0m |           0.0064 |         184.1466 |         -12.1784 |
[32m[20221214 14:14:52 @agent_ppo2.py:185][0m |           0.0033 |         176.7910 |         -12.2110 |
[32m[20221214 14:14:52 @agent_ppo2.py:185][0m |          -0.0025 |         171.8585 |         -12.1666 |
[32m[20221214 14:14:52 @agent_ppo2.py:185][0m |          -0.0019 |         169.0986 |         -12.1139 |
[32m[20221214 14:14:52 @agent_ppo2.py:185][0m |          -0.0025 |         167.8798 |         -12.1493 |
[32m[20221214 14:14:52 @agent_ppo2.py:185][0m |          -0.0009 |         167.3107 |         -12.1456 |
[32m[20221214 14:14:52 @agent_ppo2.py:185][0m |          -0.0038 |         166.1264 |         -12.1130 |
[32m[20221214 14:14:52 @agent_ppo2.py:185][0m |          -0.0031 |         165.5907 |         -12.0964 |
[32m[20221214 14:14:52 @agent_ppo2.py:185][0m |          -0.0026 |         165.6381 |         -12.1309 |
[32m[20221214 14:14:53 @agent_ppo2.py:185][0m |          -0.0017 |         164.1907 |         -12.1419 |
[32m[20221214 14:14:53 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:14:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.35
[32m[20221214 14:14:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.96
[32m[20221214 14:14:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.37
[32m[20221214 14:14:53 @agent_ppo2.py:143][0m Total time:      16.84 min
[32m[20221214 14:14:53 @agent_ppo2.py:145][0m 1546240 total steps have happened
[32m[20221214 14:14:53 @agent_ppo2.py:121][0m #------------------------ Iteration 755 --------------------------#
[32m[20221214 14:14:53 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:14:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:53 @agent_ppo2.py:185][0m |          -0.0021 |         165.8529 |         -12.5623 |
[32m[20221214 14:14:53 @agent_ppo2.py:185][0m |          -0.0038 |         163.9533 |         -12.5137 |
[32m[20221214 14:14:53 @agent_ppo2.py:185][0m |          -0.0011 |         162.5928 |         -12.5048 |
[32m[20221214 14:14:53 @agent_ppo2.py:185][0m |          -0.0027 |         162.3581 |         -12.5186 |
[32m[20221214 14:14:53 @agent_ppo2.py:185][0m |          -0.0018 |         161.8564 |         -12.5113 |
[32m[20221214 14:14:54 @agent_ppo2.py:185][0m |          -0.0016 |         161.5621 |         -12.4931 |
[32m[20221214 14:14:54 @agent_ppo2.py:185][0m |          -0.0024 |         161.0615 |         -12.5102 |
[32m[20221214 14:14:54 @agent_ppo2.py:185][0m |           0.0001 |         161.0487 |         -12.5262 |
[32m[20221214 14:14:54 @agent_ppo2.py:185][0m |          -0.0022 |         161.0579 |         -12.4789 |
[32m[20221214 14:14:54 @agent_ppo2.py:185][0m |          -0.0016 |         160.5811 |         -12.3800 |
[32m[20221214 14:14:54 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:14:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.31
[32m[20221214 14:14:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.88
[32m[20221214 14:14:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.35
[32m[20221214 14:14:54 @agent_ppo2.py:143][0m Total time:      16.87 min
[32m[20221214 14:14:54 @agent_ppo2.py:145][0m 1548288 total steps have happened
[32m[20221214 14:14:54 @agent_ppo2.py:121][0m #------------------------ Iteration 756 --------------------------#
[32m[20221214 14:14:54 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:14:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:55 @agent_ppo2.py:185][0m |          -0.0019 |         181.4829 |         -11.6511 |
[32m[20221214 14:14:55 @agent_ppo2.py:185][0m |          -0.0015 |         179.6794 |         -11.6170 |
[32m[20221214 14:14:55 @agent_ppo2.py:185][0m |          -0.0024 |         178.9360 |         -11.5510 |
[32m[20221214 14:14:55 @agent_ppo2.py:185][0m |          -0.0025 |         178.2474 |         -11.5705 |
[32m[20221214 14:14:55 @agent_ppo2.py:185][0m |          -0.0023 |         178.1546 |         -11.5462 |
[32m[20221214 14:14:55 @agent_ppo2.py:185][0m |          -0.0023 |         176.9318 |         -11.5485 |
[32m[20221214 14:14:55 @agent_ppo2.py:185][0m |          -0.0025 |         176.6325 |         -11.5468 |
[32m[20221214 14:14:55 @agent_ppo2.py:185][0m |          -0.0020 |         176.3842 |         -11.5079 |
[32m[20221214 14:14:55 @agent_ppo2.py:185][0m |          -0.0028 |         176.6492 |         -11.5126 |
[32m[20221214 14:14:55 @agent_ppo2.py:185][0m |           0.0085 |         189.4414 |         -11.4722 |
[32m[20221214 14:14:55 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:14:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.57
[32m[20221214 14:14:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.68
[32m[20221214 14:14:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.43
[32m[20221214 14:14:56 @agent_ppo2.py:143][0m Total time:      16.89 min
[32m[20221214 14:14:56 @agent_ppo2.py:145][0m 1550336 total steps have happened
[32m[20221214 14:14:56 @agent_ppo2.py:121][0m #------------------------ Iteration 757 --------------------------#
[32m[20221214 14:14:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:56 @agent_ppo2.py:185][0m |          -0.0025 |         182.3342 |         -11.7470 |
[32m[20221214 14:14:56 @agent_ppo2.py:185][0m |          -0.0026 |         179.5431 |         -11.6761 |
[32m[20221214 14:14:56 @agent_ppo2.py:185][0m |          -0.0038 |         178.3158 |         -11.7050 |
[32m[20221214 14:14:56 @agent_ppo2.py:185][0m |           0.0018 |         178.7229 |         -11.7113 |
[32m[20221214 14:14:56 @agent_ppo2.py:185][0m |          -0.0031 |         176.4024 |         -11.8062 |
[32m[20221214 14:14:56 @agent_ppo2.py:185][0m |          -0.0033 |         176.2001 |         -11.7445 |
[32m[20221214 14:14:57 @agent_ppo2.py:185][0m |          -0.0029 |         175.3738 |         -11.7884 |
[32m[20221214 14:14:57 @agent_ppo2.py:185][0m |          -0.0020 |         175.0176 |         -11.7387 |
[32m[20221214 14:14:57 @agent_ppo2.py:185][0m |          -0.0025 |         174.9511 |         -11.8014 |
[32m[20221214 14:14:57 @agent_ppo2.py:185][0m |          -0.0035 |         174.8098 |         -11.7995 |
[32m[20221214 14:14:57 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:14:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.35
[32m[20221214 14:14:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.45
[32m[20221214 14:14:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.05
[32m[20221214 14:14:57 @agent_ppo2.py:143][0m Total time:      16.92 min
[32m[20221214 14:14:57 @agent_ppo2.py:145][0m 1552384 total steps have happened
[32m[20221214 14:14:57 @agent_ppo2.py:121][0m #------------------------ Iteration 758 --------------------------#
[32m[20221214 14:14:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:57 @agent_ppo2.py:185][0m |          -0.0012 |         184.3070 |         -12.2524 |
[32m[20221214 14:14:57 @agent_ppo2.py:185][0m |           0.0024 |         181.3858 |         -12.3271 |
[32m[20221214 14:14:58 @agent_ppo2.py:185][0m |           0.0000 |         177.3823 |         -12.3445 |
[32m[20221214 14:14:58 @agent_ppo2.py:185][0m |          -0.0026 |         176.5004 |         -12.3603 |
[32m[20221214 14:14:58 @agent_ppo2.py:185][0m |          -0.0011 |         175.2026 |         -12.2630 |
[32m[20221214 14:14:58 @agent_ppo2.py:185][0m |           0.0123 |         199.6519 |         -12.3734 |
[32m[20221214 14:14:58 @agent_ppo2.py:185][0m |          -0.0002 |         175.0657 |         -12.3343 |
[32m[20221214 14:14:58 @agent_ppo2.py:185][0m |           0.0005 |         174.5405 |         -12.3616 |
[32m[20221214 14:14:58 @agent_ppo2.py:185][0m |          -0.0017 |         173.9941 |         -12.3136 |
[32m[20221214 14:14:58 @agent_ppo2.py:185][0m |          -0.0019 |         174.1115 |         -12.4611 |
[32m[20221214 14:14:58 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:14:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.05
[32m[20221214 14:14:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.95
[32m[20221214 14:14:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.27
[32m[20221214 14:14:58 @agent_ppo2.py:143][0m Total time:      16.94 min
[32m[20221214 14:14:58 @agent_ppo2.py:145][0m 1554432 total steps have happened
[32m[20221214 14:14:58 @agent_ppo2.py:121][0m #------------------------ Iteration 759 --------------------------#
[32m[20221214 14:14:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:14:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:14:59 @agent_ppo2.py:185][0m |          -0.0005 |         188.0512 |         -11.9783 |
[32m[20221214 14:14:59 @agent_ppo2.py:185][0m |           0.0009 |         184.8615 |         -11.9943 |
[32m[20221214 14:14:59 @agent_ppo2.py:185][0m |          -0.0015 |         181.9933 |         -12.0329 |
[32m[20221214 14:14:59 @agent_ppo2.py:185][0m |          -0.0020 |         180.8328 |         -12.0595 |
[32m[20221214 14:14:59 @agent_ppo2.py:185][0m |          -0.0019 |         180.1378 |         -12.1661 |
[32m[20221214 14:14:59 @agent_ppo2.py:185][0m |           0.0011 |         180.0504 |         -12.1446 |
[32m[20221214 14:14:59 @agent_ppo2.py:185][0m |           0.0060 |         192.9336 |         -12.2176 |
[32m[20221214 14:14:59 @agent_ppo2.py:185][0m |          -0.0021 |         180.3060 |         -12.1577 |
[32m[20221214 14:14:59 @agent_ppo2.py:185][0m |          -0.0003 |         179.2949 |         -12.2635 |
[32m[20221214 14:15:00 @agent_ppo2.py:185][0m |          -0.0020 |         179.2600 |         -12.2950 |
[32m[20221214 14:15:00 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:15:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.86
[32m[20221214 14:15:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.90
[32m[20221214 14:15:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.44
[32m[20221214 14:15:00 @agent_ppo2.py:143][0m Total time:      16.96 min
[32m[20221214 14:15:00 @agent_ppo2.py:145][0m 1556480 total steps have happened
[32m[20221214 14:15:00 @agent_ppo2.py:121][0m #------------------------ Iteration 760 --------------------------#
[32m[20221214 14:15:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:15:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:00 @agent_ppo2.py:185][0m |          -0.0009 |         202.1386 |         -12.9145 |
[32m[20221214 14:15:00 @agent_ppo2.py:185][0m |           0.0000 |         198.7022 |         -12.9064 |
[32m[20221214 14:15:00 @agent_ppo2.py:185][0m |          -0.0007 |         196.3194 |         -12.8992 |
[32m[20221214 14:15:00 @agent_ppo2.py:185][0m |           0.0121 |         218.5532 |         -12.8619 |
[32m[20221214 14:15:00 @agent_ppo2.py:185][0m |          -0.0023 |         193.7842 |         -12.8847 |
[32m[20221214 14:15:01 @agent_ppo2.py:185][0m |           0.0007 |         192.9106 |         -12.8715 |
[32m[20221214 14:15:01 @agent_ppo2.py:185][0m |          -0.0005 |         192.1435 |         -12.8627 |
[32m[20221214 14:15:01 @agent_ppo2.py:185][0m |          -0.0022 |         191.1941 |         -12.8520 |
[32m[20221214 14:15:01 @agent_ppo2.py:185][0m |           0.0012 |         191.3067 |         -12.8162 |
[32m[20221214 14:15:01 @agent_ppo2.py:185][0m |          -0.0018 |         190.5071 |         -12.7962 |
[32m[20221214 14:15:01 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:15:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.72
[32m[20221214 14:15:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.49
[32m[20221214 14:15:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.16
[32m[20221214 14:15:01 @agent_ppo2.py:143][0m Total time:      16.98 min
[32m[20221214 14:15:01 @agent_ppo2.py:145][0m 1558528 total steps have happened
[32m[20221214 14:15:01 @agent_ppo2.py:121][0m #------------------------ Iteration 761 --------------------------#
[32m[20221214 14:15:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:01 @agent_ppo2.py:185][0m |           0.0060 |         196.0756 |         -12.9553 |
[32m[20221214 14:15:01 @agent_ppo2.py:185][0m |           0.0083 |         199.8590 |         -12.8924 |
[32m[20221214 14:15:02 @agent_ppo2.py:185][0m |           0.0059 |         193.3179 |         -12.8178 |
[32m[20221214 14:15:02 @agent_ppo2.py:185][0m |          -0.0025 |         184.3968 |         -12.7767 |
[32m[20221214 14:15:02 @agent_ppo2.py:185][0m |          -0.0017 |         183.4829 |         -12.9957 |
[32m[20221214 14:15:02 @agent_ppo2.py:185][0m |          -0.0017 |         181.8623 |         -12.8664 |
[32m[20221214 14:15:02 @agent_ppo2.py:185][0m |          -0.0014 |         182.0202 |         -12.8841 |
[32m[20221214 14:15:02 @agent_ppo2.py:185][0m |          -0.0006 |         181.0379 |         -12.9634 |
[32m[20221214 14:15:02 @agent_ppo2.py:185][0m |           0.0049 |         183.6410 |         -12.8924 |
[32m[20221214 14:15:02 @agent_ppo2.py:185][0m |          -0.0021 |         179.9626 |         -12.9853 |
[32m[20221214 14:15:02 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:15:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.20
[32m[20221214 14:15:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.50
[32m[20221214 14:15:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.54
[32m[20221214 14:15:02 @agent_ppo2.py:143][0m Total time:      17.01 min
[32m[20221214 14:15:02 @agent_ppo2.py:145][0m 1560576 total steps have happened
[32m[20221214 14:15:02 @agent_ppo2.py:121][0m #------------------------ Iteration 762 --------------------------#
[32m[20221214 14:15:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:15:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:03 @agent_ppo2.py:185][0m |          -0.0018 |         184.9457 |         -12.4671 |
[32m[20221214 14:15:03 @agent_ppo2.py:185][0m |          -0.0016 |         178.1023 |         -12.4983 |
[32m[20221214 14:15:03 @agent_ppo2.py:185][0m |          -0.0005 |         174.8201 |         -12.6084 |
[32m[20221214 14:15:03 @agent_ppo2.py:185][0m |           0.0115 |         196.1352 |         -12.6275 |
[32m[20221214 14:15:03 @agent_ppo2.py:185][0m |          -0.0041 |         170.4675 |         -12.6822 |
[32m[20221214 14:15:03 @agent_ppo2.py:185][0m |          -0.0037 |         167.8914 |         -12.6759 |
[32m[20221214 14:15:03 @agent_ppo2.py:185][0m |          -0.0035 |         166.2132 |         -12.7376 |
[32m[20221214 14:15:03 @agent_ppo2.py:185][0m |          -0.0038 |         164.8278 |         -12.6708 |
[32m[20221214 14:15:03 @agent_ppo2.py:185][0m |          -0.0032 |         163.6913 |         -12.7026 |
[32m[20221214 14:15:04 @agent_ppo2.py:185][0m |          -0.0051 |         163.3730 |         -12.7110 |
[32m[20221214 14:15:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:15:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.64
[32m[20221214 14:15:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.62
[32m[20221214 14:15:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.73
[32m[20221214 14:15:04 @agent_ppo2.py:143][0m Total time:      17.03 min
[32m[20221214 14:15:04 @agent_ppo2.py:145][0m 1562624 total steps have happened
[32m[20221214 14:15:04 @agent_ppo2.py:121][0m #------------------------ Iteration 763 --------------------------#
[32m[20221214 14:15:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:04 @agent_ppo2.py:185][0m |           0.0080 |         227.8297 |         -13.3928 |
[32m[20221214 14:15:04 @agent_ppo2.py:185][0m |          -0.0016 |         209.5065 |         -13.4501 |
[32m[20221214 14:15:04 @agent_ppo2.py:185][0m |           0.0057 |         212.7337 |         -13.2979 |
[32m[20221214 14:15:04 @agent_ppo2.py:185][0m |          -0.0020 |         209.0327 |         -13.3969 |
[32m[20221214 14:15:04 @agent_ppo2.py:185][0m |          -0.0017 |         208.2301 |         -13.3913 |
[32m[20221214 14:15:04 @agent_ppo2.py:185][0m |          -0.0011 |         208.1563 |         -13.3754 |
[32m[20221214 14:15:05 @agent_ppo2.py:185][0m |           0.0125 |         229.2461 |         -13.3885 |
[32m[20221214 14:15:05 @agent_ppo2.py:185][0m |          -0.0006 |         208.4531 |         -13.3539 |
[32m[20221214 14:15:05 @agent_ppo2.py:185][0m |          -0.0019 |         207.7010 |         -13.3901 |
[32m[20221214 14:15:05 @agent_ppo2.py:185][0m |           0.0083 |         219.6887 |         -13.3600 |
[32m[20221214 14:15:05 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:15:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.70
[32m[20221214 14:15:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.15
[32m[20221214 14:15:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.16
[32m[20221214 14:15:05 @agent_ppo2.py:143][0m Total time:      17.05 min
[32m[20221214 14:15:05 @agent_ppo2.py:145][0m 1564672 total steps have happened
[32m[20221214 14:15:05 @agent_ppo2.py:121][0m #------------------------ Iteration 764 --------------------------#
[32m[20221214 14:15:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:15:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:05 @agent_ppo2.py:185][0m |          -0.0014 |         199.5419 |         -13.1825 |
[32m[20221214 14:15:05 @agent_ppo2.py:185][0m |          -0.0034 |         198.5511 |         -13.2355 |
[32m[20221214 14:15:05 @agent_ppo2.py:185][0m |           0.0087 |         214.8262 |         -13.1184 |
[32m[20221214 14:15:06 @agent_ppo2.py:185][0m |          -0.0017 |         197.3047 |         -13.1631 |
[32m[20221214 14:15:06 @agent_ppo2.py:185][0m |          -0.0027 |         195.5428 |         -13.2150 |
[32m[20221214 14:15:06 @agent_ppo2.py:185][0m |          -0.0011 |         195.7701 |         -13.2467 |
[32m[20221214 14:15:06 @agent_ppo2.py:185][0m |          -0.0052 |         195.3567 |         -13.1628 |
[32m[20221214 14:15:06 @agent_ppo2.py:185][0m |          -0.0005 |         194.9582 |         -13.2216 |
[32m[20221214 14:15:06 @agent_ppo2.py:185][0m |          -0.0019 |         194.9217 |         -13.1778 |
[32m[20221214 14:15:06 @agent_ppo2.py:185][0m |          -0.0018 |         194.2501 |         -13.1892 |
[32m[20221214 14:15:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:15:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.65
[32m[20221214 14:15:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.26
[32m[20221214 14:15:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.66
[32m[20221214 14:15:06 @agent_ppo2.py:143][0m Total time:      17.07 min
[32m[20221214 14:15:06 @agent_ppo2.py:145][0m 1566720 total steps have happened
[32m[20221214 14:15:06 @agent_ppo2.py:121][0m #------------------------ Iteration 765 --------------------------#
[32m[20221214 14:15:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:15:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:07 @agent_ppo2.py:185][0m |          -0.0018 |         190.4732 |         -12.9918 |
[32m[20221214 14:15:07 @agent_ppo2.py:185][0m |          -0.0014 |         185.9715 |         -12.9513 |
[32m[20221214 14:15:07 @agent_ppo2.py:185][0m |          -0.0014 |         184.7981 |         -12.9110 |
[32m[20221214 14:15:07 @agent_ppo2.py:185][0m |          -0.0021 |         182.4323 |         -12.8099 |
[32m[20221214 14:15:07 @agent_ppo2.py:185][0m |          -0.0031 |         181.3659 |         -12.8958 |
[32m[20221214 14:15:07 @agent_ppo2.py:185][0m |          -0.0044 |         180.8253 |         -12.8576 |
[32m[20221214 14:15:07 @agent_ppo2.py:185][0m |          -0.0022 |         180.6513 |         -12.8207 |
[32m[20221214 14:15:07 @agent_ppo2.py:185][0m |          -0.0038 |         178.5412 |         -12.7840 |
[32m[20221214 14:15:07 @agent_ppo2.py:185][0m |          -0.0002 |         184.9024 |         -12.7722 |
[32m[20221214 14:15:07 @agent_ppo2.py:185][0m |          -0.0064 |         177.7137 |         -12.7846 |
[32m[20221214 14:15:07 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:15:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.15
[32m[20221214 14:15:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.66
[32m[20221214 14:15:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.89
[32m[20221214 14:15:07 @agent_ppo2.py:143][0m Total time:      17.09 min
[32m[20221214 14:15:07 @agent_ppo2.py:145][0m 1568768 total steps have happened
[32m[20221214 14:15:07 @agent_ppo2.py:121][0m #------------------------ Iteration 766 --------------------------#
[32m[20221214 14:15:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:15:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:08 @agent_ppo2.py:185][0m |          -0.0032 |         195.6313 |         -13.4760 |
[32m[20221214 14:15:08 @agent_ppo2.py:185][0m |          -0.0038 |         194.6095 |         -13.4828 |
[32m[20221214 14:15:08 @agent_ppo2.py:185][0m |          -0.0015 |         192.4382 |         -13.5795 |
[32m[20221214 14:15:08 @agent_ppo2.py:185][0m |          -0.0025 |         192.0708 |         -13.5599 |
[32m[20221214 14:15:08 @agent_ppo2.py:185][0m |          -0.0024 |         191.9971 |         -13.6029 |
[32m[20221214 14:15:08 @agent_ppo2.py:185][0m |          -0.0022 |         191.3463 |         -13.6232 |
[32m[20221214 14:15:08 @agent_ppo2.py:185][0m |          -0.0042 |         191.2440 |         -13.6141 |
[32m[20221214 14:15:08 @agent_ppo2.py:185][0m |          -0.0041 |         190.9293 |         -13.6287 |
[32m[20221214 14:15:09 @agent_ppo2.py:185][0m |          -0.0019 |         190.1823 |         -13.7649 |
[32m[20221214 14:15:09 @agent_ppo2.py:185][0m |          -0.0039 |         190.4105 |         -13.7140 |
[32m[20221214 14:15:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:15:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.52
[32m[20221214 14:15:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.72
[32m[20221214 14:15:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.26
[32m[20221214 14:15:09 @agent_ppo2.py:143][0m Total time:      17.11 min
[32m[20221214 14:15:09 @agent_ppo2.py:145][0m 1570816 total steps have happened
[32m[20221214 14:15:09 @agent_ppo2.py:121][0m #------------------------ Iteration 767 --------------------------#
[32m[20221214 14:15:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:15:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:09 @agent_ppo2.py:185][0m |          -0.0018 |         180.3024 |         -13.4095 |
[32m[20221214 14:15:09 @agent_ppo2.py:185][0m |          -0.0030 |         176.6600 |         -13.3442 |
[32m[20221214 14:15:09 @agent_ppo2.py:185][0m |          -0.0016 |         176.4182 |         -13.3522 |
[32m[20221214 14:15:09 @agent_ppo2.py:185][0m |           0.0016 |         175.5358 |         -13.3661 |
[32m[20221214 14:15:09 @agent_ppo2.py:185][0m |           0.0109 |         192.1673 |         -13.4237 |
[32m[20221214 14:15:10 @agent_ppo2.py:185][0m |          -0.0027 |         174.3329 |         -13.4135 |
[32m[20221214 14:15:10 @agent_ppo2.py:185][0m |          -0.0038 |         173.8542 |         -13.4213 |
[32m[20221214 14:15:10 @agent_ppo2.py:185][0m |           0.0007 |         174.8136 |         -13.3973 |
[32m[20221214 14:15:10 @agent_ppo2.py:185][0m |           0.0063 |         183.8783 |         -13.4156 |
[32m[20221214 14:15:10 @agent_ppo2.py:185][0m |          -0.0010 |         172.5816 |         -13.3449 |
[32m[20221214 14:15:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:15:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.69
[32m[20221214 14:15:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.66
[32m[20221214 14:15:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.93
[32m[20221214 14:15:10 @agent_ppo2.py:143][0m Total time:      17.13 min
[32m[20221214 14:15:10 @agent_ppo2.py:145][0m 1572864 total steps have happened
[32m[20221214 14:15:10 @agent_ppo2.py:121][0m #------------------------ Iteration 768 --------------------------#
[32m[20221214 14:15:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:15:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:10 @agent_ppo2.py:185][0m |           0.0002 |         195.2029 |         -14.4796 |
[32m[20221214 14:15:10 @agent_ppo2.py:185][0m |          -0.0009 |         190.2000 |         -14.3494 |
[32m[20221214 14:15:11 @agent_ppo2.py:185][0m |          -0.0011 |         188.5028 |         -14.3927 |
[32m[20221214 14:15:11 @agent_ppo2.py:185][0m |          -0.0046 |         187.2840 |         -14.3021 |
[32m[20221214 14:15:11 @agent_ppo2.py:185][0m |           0.0046 |         191.5321 |         -14.4053 |
[32m[20221214 14:15:11 @agent_ppo2.py:185][0m |          -0.0025 |         185.9560 |         -14.3096 |
[32m[20221214 14:15:11 @agent_ppo2.py:185][0m |           0.0009 |         187.1898 |         -14.2731 |
[32m[20221214 14:15:11 @agent_ppo2.py:185][0m |          -0.0017 |         185.3424 |         -14.3047 |
[32m[20221214 14:15:11 @agent_ppo2.py:185][0m |          -0.0017 |         185.1250 |         -14.2020 |
[32m[20221214 14:15:11 @agent_ppo2.py:185][0m |          -0.0027 |         185.0055 |         -14.3070 |
[32m[20221214 14:15:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:15:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.14
[32m[20221214 14:15:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.82
[32m[20221214 14:15:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.41
[32m[20221214 14:15:11 @agent_ppo2.py:143][0m Total time:      17.15 min
[32m[20221214 14:15:11 @agent_ppo2.py:145][0m 1574912 total steps have happened
[32m[20221214 14:15:11 @agent_ppo2.py:121][0m #------------------------ Iteration 769 --------------------------#
[32m[20221214 14:15:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:15:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:12 @agent_ppo2.py:185][0m |          -0.0021 |         186.4167 |         -13.3169 |
[32m[20221214 14:15:12 @agent_ppo2.py:185][0m |          -0.0026 |         184.3092 |         -13.3241 |
[32m[20221214 14:15:12 @agent_ppo2.py:185][0m |           0.0049 |         191.7828 |         -13.2491 |
[32m[20221214 14:15:12 @agent_ppo2.py:185][0m |           0.0019 |         185.7697 |         -13.2890 |
[32m[20221214 14:15:12 @agent_ppo2.py:185][0m |          -0.0023 |         182.2618 |         -13.2795 |
[32m[20221214 14:15:12 @agent_ppo2.py:185][0m |           0.0103 |         200.0242 |         -13.2734 |
[32m[20221214 14:15:12 @agent_ppo2.py:185][0m |          -0.0027 |         179.6178 |         -13.3465 |
[32m[20221214 14:15:12 @agent_ppo2.py:185][0m |          -0.0031 |         179.5691 |         -13.2871 |
[32m[20221214 14:15:12 @agent_ppo2.py:185][0m |          -0.0033 |         179.2588 |         -13.2503 |
[32m[20221214 14:15:13 @agent_ppo2.py:185][0m |           0.0107 |         206.5113 |         -13.3014 |
[32m[20221214 14:15:13 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:15:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.11
[32m[20221214 14:15:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.52
[32m[20221214 14:15:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.43
[32m[20221214 14:15:13 @agent_ppo2.py:143][0m Total time:      17.18 min
[32m[20221214 14:15:13 @agent_ppo2.py:145][0m 1576960 total steps have happened
[32m[20221214 14:15:13 @agent_ppo2.py:121][0m #------------------------ Iteration 770 --------------------------#
[32m[20221214 14:15:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:15:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:13 @agent_ppo2.py:185][0m |          -0.0018 |         186.8964 |         -13.6764 |
[32m[20221214 14:15:13 @agent_ppo2.py:185][0m |          -0.0007 |         182.7518 |         -13.7737 |
[32m[20221214 14:15:13 @agent_ppo2.py:185][0m |          -0.0003 |         181.9822 |         -13.6916 |
[32m[20221214 14:15:13 @agent_ppo2.py:185][0m |          -0.0011 |         182.0695 |         -13.6867 |
[32m[20221214 14:15:14 @agent_ppo2.py:185][0m |          -0.0002 |         182.8094 |         -13.6963 |
[32m[20221214 14:15:14 @agent_ppo2.py:185][0m |          -0.0010 |         181.0279 |         -13.7861 |
[32m[20221214 14:15:14 @agent_ppo2.py:185][0m |           0.0080 |         186.2518 |         -13.8291 |
[32m[20221214 14:15:14 @agent_ppo2.py:185][0m |          -0.0035 |         181.5023 |         -13.8117 |
[32m[20221214 14:15:14 @agent_ppo2.py:185][0m |           0.0008 |         180.9056 |         -13.7223 |
[32m[20221214 14:15:14 @agent_ppo2.py:185][0m |          -0.0026 |         180.4520 |         -13.8133 |
[32m[20221214 14:15:14 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:15:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.35
[32m[20221214 14:15:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.80
[32m[20221214 14:15:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.62
[32m[20221214 14:15:14 @agent_ppo2.py:143][0m Total time:      17.20 min
[32m[20221214 14:15:14 @agent_ppo2.py:145][0m 1579008 total steps have happened
[32m[20221214 14:15:14 @agent_ppo2.py:121][0m #------------------------ Iteration 771 --------------------------#
[32m[20221214 14:15:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:14 @agent_ppo2.py:185][0m |          -0.0007 |         186.9743 |         -13.8423 |
[32m[20221214 14:15:15 @agent_ppo2.py:185][0m |          -0.0005 |         184.7968 |         -13.8734 |
[32m[20221214 14:15:15 @agent_ppo2.py:185][0m |          -0.0011 |         183.2068 |         -13.8773 |
[32m[20221214 14:15:15 @agent_ppo2.py:185][0m |          -0.0035 |         182.3995 |         -13.8504 |
[32m[20221214 14:15:15 @agent_ppo2.py:185][0m |          -0.0029 |         181.5656 |         -13.8668 |
[32m[20221214 14:15:15 @agent_ppo2.py:185][0m |           0.0001 |         181.1179 |         -13.7643 |
[32m[20221214 14:15:15 @agent_ppo2.py:185][0m |          -0.0024 |         180.7844 |         -13.9565 |
[32m[20221214 14:15:15 @agent_ppo2.py:185][0m |          -0.0028 |         180.3633 |         -13.9148 |
[32m[20221214 14:15:15 @agent_ppo2.py:185][0m |          -0.0023 |         180.0793 |         -13.9621 |
[32m[20221214 14:15:15 @agent_ppo2.py:185][0m |          -0.0025 |         180.1493 |         -13.8755 |
[32m[20221214 14:15:15 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:15:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.11
[32m[20221214 14:15:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.54
[32m[20221214 14:15:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.83
[32m[20221214 14:15:16 @agent_ppo2.py:143][0m Total time:      17.22 min
[32m[20221214 14:15:16 @agent_ppo2.py:145][0m 1581056 total steps have happened
[32m[20221214 14:15:16 @agent_ppo2.py:121][0m #------------------------ Iteration 772 --------------------------#
[32m[20221214 14:15:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:16 @agent_ppo2.py:185][0m |           0.0093 |         199.4664 |         -14.1219 |
[32m[20221214 14:15:16 @agent_ppo2.py:185][0m |          -0.0022 |         176.0113 |         -14.0543 |
[32m[20221214 14:15:16 @agent_ppo2.py:185][0m |          -0.0036 |         171.7026 |         -14.0499 |
[32m[20221214 14:15:16 @agent_ppo2.py:185][0m |          -0.0013 |         169.1791 |         -14.1288 |
[32m[20221214 14:15:16 @agent_ppo2.py:185][0m |          -0.0021 |         168.3370 |         -14.1049 |
[32m[20221214 14:15:16 @agent_ppo2.py:185][0m |          -0.0004 |         169.9573 |         -14.0136 |
[32m[20221214 14:15:16 @agent_ppo2.py:185][0m |          -0.0012 |         167.5214 |         -14.0494 |
[32m[20221214 14:15:17 @agent_ppo2.py:185][0m |          -0.0012 |         168.1242 |         -14.0689 |
[32m[20221214 14:15:17 @agent_ppo2.py:185][0m |          -0.0023 |         167.0633 |         -14.0208 |
[32m[20221214 14:15:17 @agent_ppo2.py:185][0m |          -0.0027 |         166.9358 |         -14.0343 |
[32m[20221214 14:15:17 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:15:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.44
[32m[20221214 14:15:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.05
[32m[20221214 14:15:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.80
[32m[20221214 14:15:17 @agent_ppo2.py:143][0m Total time:      17.25 min
[32m[20221214 14:15:17 @agent_ppo2.py:145][0m 1583104 total steps have happened
[32m[20221214 14:15:17 @agent_ppo2.py:121][0m #------------------------ Iteration 773 --------------------------#
[32m[20221214 14:15:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:17 @agent_ppo2.py:185][0m |           0.0101 |         165.2134 |         -13.8857 |
[32m[20221214 14:15:17 @agent_ppo2.py:185][0m |          -0.0001 |         149.6715 |         -13.8473 |
[32m[20221214 14:15:17 @agent_ppo2.py:185][0m |          -0.0057 |         147.4122 |         -13.8056 |
[32m[20221214 14:15:18 @agent_ppo2.py:185][0m |          -0.0058 |         146.6725 |         -13.8022 |
[32m[20221214 14:15:18 @agent_ppo2.py:185][0m |           0.0079 |         161.1043 |         -13.7217 |
[32m[20221214 14:15:18 @agent_ppo2.py:185][0m |           0.0030 |         154.8680 |         -13.7443 |
[32m[20221214 14:15:18 @agent_ppo2.py:185][0m |          -0.0041 |         143.9016 |         -13.6440 |
[32m[20221214 14:15:18 @agent_ppo2.py:185][0m |          -0.0048 |         143.7574 |         -13.7271 |
[32m[20221214 14:15:18 @agent_ppo2.py:185][0m |          -0.0029 |         143.8465 |         -13.6918 |
[32m[20221214 14:15:18 @agent_ppo2.py:185][0m |           0.0060 |         160.4362 |         -13.6462 |
[32m[20221214 14:15:18 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:15:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.53
[32m[20221214 14:15:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.45
[32m[20221214 14:15:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.18
[32m[20221214 14:15:18 @agent_ppo2.py:143][0m Total time:      17.27 min
[32m[20221214 14:15:18 @agent_ppo2.py:145][0m 1585152 total steps have happened
[32m[20221214 14:15:18 @agent_ppo2.py:121][0m #------------------------ Iteration 774 --------------------------#
[32m[20221214 14:15:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:19 @agent_ppo2.py:185][0m |           0.0000 |         176.5589 |         -13.6724 |
[32m[20221214 14:15:19 @agent_ppo2.py:185][0m |          -0.0004 |         172.3691 |         -13.7732 |
[32m[20221214 14:15:19 @agent_ppo2.py:185][0m |          -0.0013 |         170.2213 |         -13.6940 |
[32m[20221214 14:15:19 @agent_ppo2.py:185][0m |          -0.0006 |         168.8187 |         -13.8503 |
[32m[20221214 14:15:19 @agent_ppo2.py:185][0m |           0.0070 |         176.4177 |         -13.9138 |
[32m[20221214 14:15:19 @agent_ppo2.py:185][0m |          -0.0011 |         167.3638 |         -13.9135 |
[32m[20221214 14:15:19 @agent_ppo2.py:185][0m |           0.0046 |         172.6628 |         -14.0299 |
[32m[20221214 14:15:19 @agent_ppo2.py:185][0m |           0.0025 |         167.0951 |         -13.9987 |
[32m[20221214 14:15:19 @agent_ppo2.py:185][0m |          -0.0012 |         165.2338 |         -14.0403 |
[32m[20221214 14:15:19 @agent_ppo2.py:185][0m |          -0.0014 |         164.4863 |         -14.0454 |
[32m[20221214 14:15:19 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:15:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.65
[32m[20221214 14:15:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.91
[32m[20221214 14:15:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.16
[32m[20221214 14:15:20 @agent_ppo2.py:143][0m Total time:      17.29 min
[32m[20221214 14:15:20 @agent_ppo2.py:145][0m 1587200 total steps have happened
[32m[20221214 14:15:20 @agent_ppo2.py:121][0m #------------------------ Iteration 775 --------------------------#
[32m[20221214 14:15:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:20 @agent_ppo2.py:185][0m |          -0.0013 |         171.4043 |         -14.7468 |
[32m[20221214 14:15:20 @agent_ppo2.py:185][0m |           0.0136 |         177.7257 |         -14.7538 |
[32m[20221214 14:15:20 @agent_ppo2.py:185][0m |           0.0093 |         181.2248 |         -14.6945 |
[32m[20221214 14:15:20 @agent_ppo2.py:185][0m |           0.0086 |         178.6333 |         -14.7987 |
[32m[20221214 14:15:20 @agent_ppo2.py:185][0m |           0.0057 |         176.8805 |         -14.7459 |
[32m[20221214 14:15:21 @agent_ppo2.py:185][0m |          -0.0002 |         166.3411 |         -14.7678 |
[32m[20221214 14:15:21 @agent_ppo2.py:185][0m |          -0.0026 |         165.8429 |         -14.8974 |
[32m[20221214 14:15:21 @agent_ppo2.py:185][0m |          -0.0031 |         166.0013 |         -14.8904 |
[32m[20221214 14:15:21 @agent_ppo2.py:185][0m |          -0.0014 |         164.7021 |         -14.9264 |
[32m[20221214 14:15:21 @agent_ppo2.py:185][0m |          -0.0015 |         164.9403 |         -14.9364 |
[32m[20221214 14:15:21 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:15:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.76
[32m[20221214 14:15:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.36
[32m[20221214 14:15:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.16
[32m[20221214 14:15:21 @agent_ppo2.py:143][0m Total time:      17.32 min
[32m[20221214 14:15:21 @agent_ppo2.py:145][0m 1589248 total steps have happened
[32m[20221214 14:15:21 @agent_ppo2.py:121][0m #------------------------ Iteration 776 --------------------------#
[32m[20221214 14:15:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:21 @agent_ppo2.py:185][0m |           0.0179 |         185.6917 |         -14.6887 |
[32m[20221214 14:15:21 @agent_ppo2.py:185][0m |           0.0020 |         162.7890 |         -14.4726 |
[32m[20221214 14:15:22 @agent_ppo2.py:185][0m |          -0.0029 |         159.9960 |         -14.7308 |
[32m[20221214 14:15:22 @agent_ppo2.py:185][0m |          -0.0007 |         158.8691 |         -14.7156 |
[32m[20221214 14:15:22 @agent_ppo2.py:185][0m |          -0.0036 |         158.0845 |         -14.7574 |
[32m[20221214 14:15:22 @agent_ppo2.py:185][0m |           0.0102 |         172.5494 |         -14.7537 |
[32m[20221214 14:15:22 @agent_ppo2.py:185][0m |           0.0178 |         175.2224 |         -14.7867 |
[32m[20221214 14:15:22 @agent_ppo2.py:185][0m |          -0.0025 |         157.4782 |         -14.7612 |
[32m[20221214 14:15:22 @agent_ppo2.py:185][0m |          -0.0029 |         156.0664 |         -14.7654 |
[32m[20221214 14:15:22 @agent_ppo2.py:185][0m |          -0.0029 |         155.8856 |         -14.8592 |
[32m[20221214 14:15:22 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:15:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.31
[32m[20221214 14:15:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.05
[32m[20221214 14:15:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.74
[32m[20221214 14:15:22 @agent_ppo2.py:143][0m Total time:      17.34 min
[32m[20221214 14:15:22 @agent_ppo2.py:145][0m 1591296 total steps have happened
[32m[20221214 14:15:22 @agent_ppo2.py:121][0m #------------------------ Iteration 777 --------------------------#
[32m[20221214 14:15:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:15:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:23 @agent_ppo2.py:185][0m |          -0.0023 |         168.4284 |         -14.8024 |
[32m[20221214 14:15:23 @agent_ppo2.py:185][0m |          -0.0021 |         165.8866 |         -14.8252 |
[32m[20221214 14:15:23 @agent_ppo2.py:185][0m |          -0.0017 |         165.7196 |         -14.7828 |
[32m[20221214 14:15:23 @agent_ppo2.py:185][0m |          -0.0009 |         164.5263 |         -14.7133 |
[32m[20221214 14:15:23 @agent_ppo2.py:185][0m |          -0.0005 |         163.7798 |         -14.7181 |
[32m[20221214 14:15:23 @agent_ppo2.py:185][0m |          -0.0022 |         163.9004 |         -14.7104 |
[32m[20221214 14:15:23 @agent_ppo2.py:185][0m |           0.0115 |         178.6215 |         -14.7104 |
[32m[20221214 14:15:23 @agent_ppo2.py:185][0m |          -0.0010 |         163.8242 |         -14.6643 |
[32m[20221214 14:15:24 @agent_ppo2.py:185][0m |          -0.0012 |         162.9492 |         -14.6533 |
[32m[20221214 14:15:24 @agent_ppo2.py:185][0m |           0.0021 |         164.6324 |         -14.6239 |
[32m[20221214 14:15:24 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 14:15:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.97
[32m[20221214 14:15:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.84
[32m[20221214 14:15:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.36
[32m[20221214 14:15:24 @agent_ppo2.py:143][0m Total time:      17.36 min
[32m[20221214 14:15:24 @agent_ppo2.py:145][0m 1593344 total steps have happened
[32m[20221214 14:15:24 @agent_ppo2.py:121][0m #------------------------ Iteration 778 --------------------------#
[32m[20221214 14:15:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:24 @agent_ppo2.py:185][0m |          -0.0022 |         173.2745 |         -14.3223 |
[32m[20221214 14:15:24 @agent_ppo2.py:185][0m |          -0.0006 |         169.9873 |         -14.2666 |
[32m[20221214 14:15:24 @agent_ppo2.py:185][0m |          -0.0045 |         169.5057 |         -14.2519 |
[32m[20221214 14:15:24 @agent_ppo2.py:185][0m |          -0.0022 |         168.5343 |         -14.3157 |
[32m[20221214 14:15:25 @agent_ppo2.py:185][0m |           0.0009 |         170.1977 |         -14.2748 |
[32m[20221214 14:15:25 @agent_ppo2.py:185][0m |          -0.0008 |         167.2220 |         -14.3162 |
[32m[20221214 14:15:25 @agent_ppo2.py:185][0m |          -0.0034 |         166.7030 |         -14.2997 |
[32m[20221214 14:15:25 @agent_ppo2.py:185][0m |          -0.0016 |         166.3908 |         -14.3842 |
[32m[20221214 14:15:25 @agent_ppo2.py:185][0m |          -0.0024 |         166.0525 |         -14.4217 |
[32m[20221214 14:15:25 @agent_ppo2.py:185][0m |          -0.0022 |         165.8008 |         -14.3198 |
[32m[20221214 14:15:25 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:15:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.14
[32m[20221214 14:15:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.10
[32m[20221214 14:15:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.62
[32m[20221214 14:15:25 @agent_ppo2.py:143][0m Total time:      17.39 min
[32m[20221214 14:15:25 @agent_ppo2.py:145][0m 1595392 total steps have happened
[32m[20221214 14:15:25 @agent_ppo2.py:121][0m #------------------------ Iteration 779 --------------------------#
[32m[20221214 14:15:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:26 @agent_ppo2.py:185][0m |          -0.0037 |         172.7568 |         -15.0773 |
[32m[20221214 14:15:26 @agent_ppo2.py:185][0m |          -0.0055 |         169.8257 |         -15.0270 |
[32m[20221214 14:15:26 @agent_ppo2.py:185][0m |          -0.0032 |         168.5949 |         -15.0285 |
[32m[20221214 14:15:26 @agent_ppo2.py:185][0m |           0.0083 |         192.0589 |         -15.0339 |
[32m[20221214 14:15:26 @agent_ppo2.py:185][0m |          -0.0035 |         168.4363 |         -15.0104 |
[32m[20221214 14:15:26 @agent_ppo2.py:185][0m |          -0.0047 |         166.8612 |         -14.9359 |
[32m[20221214 14:15:26 @agent_ppo2.py:185][0m |           0.0098 |         189.5165 |         -15.0451 |
[32m[20221214 14:15:26 @agent_ppo2.py:185][0m |          -0.0044 |         167.0094 |         -14.9154 |
[32m[20221214 14:15:26 @agent_ppo2.py:185][0m |          -0.0017 |         166.8324 |         -15.0200 |
[32m[20221214 14:15:26 @agent_ppo2.py:185][0m |          -0.0043 |         166.2094 |         -14.9216 |
[32m[20221214 14:15:26 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:15:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.85
[32m[20221214 14:15:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.22
[32m[20221214 14:15:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.82
[32m[20221214 14:15:27 @agent_ppo2.py:143][0m Total time:      17.41 min
[32m[20221214 14:15:27 @agent_ppo2.py:145][0m 1597440 total steps have happened
[32m[20221214 14:15:27 @agent_ppo2.py:121][0m #------------------------ Iteration 780 --------------------------#
[32m[20221214 14:15:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:15:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:27 @agent_ppo2.py:185][0m |           0.0097 |         207.0797 |         -15.2505 |
[32m[20221214 14:15:27 @agent_ppo2.py:185][0m |          -0.0034 |         179.6402 |         -15.3214 |
[32m[20221214 14:15:27 @agent_ppo2.py:185][0m |          -0.0046 |         176.8081 |         -15.2971 |
[32m[20221214 14:15:27 @agent_ppo2.py:185][0m |          -0.0045 |         175.7199 |         -15.2652 |
[32m[20221214 14:15:27 @agent_ppo2.py:185][0m |          -0.0046 |         175.0944 |         -15.3472 |
[32m[20221214 14:15:27 @agent_ppo2.py:185][0m |           0.0020 |         177.1515 |         -15.3621 |
[32m[20221214 14:15:27 @agent_ppo2.py:185][0m |          -0.0036 |         173.6728 |         -15.2930 |
[32m[20221214 14:15:28 @agent_ppo2.py:185][0m |          -0.0035 |         172.8416 |         -15.3594 |
[32m[20221214 14:15:28 @agent_ppo2.py:185][0m |          -0.0031 |         172.3344 |         -15.3444 |
[32m[20221214 14:15:28 @agent_ppo2.py:185][0m |          -0.0054 |         171.8913 |         -15.2493 |
[32m[20221214 14:15:28 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:15:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.50
[32m[20221214 14:15:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.01
[32m[20221214 14:15:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.67
[32m[20221214 14:15:28 @agent_ppo2.py:143][0m Total time:      17.43 min
[32m[20221214 14:15:28 @agent_ppo2.py:145][0m 1599488 total steps have happened
[32m[20221214 14:15:28 @agent_ppo2.py:121][0m #------------------------ Iteration 781 --------------------------#
[32m[20221214 14:15:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:28 @agent_ppo2.py:185][0m |          -0.0016 |         171.2313 |         -14.5217 |
[32m[20221214 14:15:28 @agent_ppo2.py:185][0m |          -0.0006 |         159.6140 |         -14.5858 |
[32m[20221214 14:15:28 @agent_ppo2.py:185][0m |          -0.0042 |         156.9410 |         -14.5751 |
[32m[20221214 14:15:29 @agent_ppo2.py:185][0m |          -0.0043 |         155.7348 |         -14.6212 |
[32m[20221214 14:15:29 @agent_ppo2.py:185][0m |          -0.0019 |         154.8760 |         -14.6213 |
[32m[20221214 14:15:29 @agent_ppo2.py:185][0m |          -0.0031 |         153.9860 |         -14.7235 |
[32m[20221214 14:15:29 @agent_ppo2.py:185][0m |          -0.0033 |         153.5538 |         -14.7657 |
[32m[20221214 14:15:29 @agent_ppo2.py:185][0m |          -0.0033 |         153.4308 |         -14.7562 |
[32m[20221214 14:15:29 @agent_ppo2.py:185][0m |           0.0104 |         179.9532 |         -14.7616 |
[32m[20221214 14:15:29 @agent_ppo2.py:185][0m |           0.0131 |         165.8998 |         -14.7285 |
[32m[20221214 14:15:29 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:15:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.89
[32m[20221214 14:15:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.73
[32m[20221214 14:15:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.17
[32m[20221214 14:15:29 @agent_ppo2.py:143][0m Total time:      17.45 min
[32m[20221214 14:15:29 @agent_ppo2.py:145][0m 1601536 total steps have happened
[32m[20221214 14:15:29 @agent_ppo2.py:121][0m #------------------------ Iteration 782 --------------------------#
[32m[20221214 14:15:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:30 @agent_ppo2.py:185][0m |          -0.0001 |         186.6333 |         -14.3863 |
[32m[20221214 14:15:30 @agent_ppo2.py:185][0m |          -0.0010 |         175.9678 |         -14.4844 |
[32m[20221214 14:15:30 @agent_ppo2.py:185][0m |          -0.0007 |         171.7938 |         -14.5896 |
[32m[20221214 14:15:30 @agent_ppo2.py:185][0m |          -0.0024 |         169.4286 |         -14.5518 |
[32m[20221214 14:15:30 @agent_ppo2.py:185][0m |          -0.0017 |         167.2057 |         -14.6134 |
[32m[20221214 14:15:30 @agent_ppo2.py:185][0m |          -0.0010 |         166.6330 |         -14.5893 |
[32m[20221214 14:15:30 @agent_ppo2.py:185][0m |           0.0043 |         169.7448 |         -14.7335 |
[32m[20221214 14:15:30 @agent_ppo2.py:185][0m |          -0.0014 |         165.0378 |         -14.6866 |
[32m[20221214 14:15:30 @agent_ppo2.py:185][0m |          -0.0011 |         162.9932 |         -14.7582 |
[32m[20221214 14:15:31 @agent_ppo2.py:185][0m |          -0.0020 |         162.8534 |         -14.8772 |
[32m[20221214 14:15:31 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:15:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 778.97
[32m[20221214 14:15:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.27
[32m[20221214 14:15:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.02
[32m[20221214 14:15:31 @agent_ppo2.py:143][0m Total time:      17.48 min
[32m[20221214 14:15:31 @agent_ppo2.py:145][0m 1603584 total steps have happened
[32m[20221214 14:15:31 @agent_ppo2.py:121][0m #------------------------ Iteration 783 --------------------------#
[32m[20221214 14:15:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:31 @agent_ppo2.py:185][0m |          -0.0018 |         179.9919 |         -14.9641 |
[32m[20221214 14:15:31 @agent_ppo2.py:185][0m |          -0.0000 |         174.6103 |         -15.0145 |
[32m[20221214 14:15:31 @agent_ppo2.py:185][0m |           0.0207 |         202.9536 |         -14.9780 |
[32m[20221214 14:15:31 @agent_ppo2.py:185][0m |           0.0011 |         172.0057 |         -15.0952 |
[32m[20221214 14:15:31 @agent_ppo2.py:185][0m |          -0.0020 |         169.0641 |         -15.1155 |
[32m[20221214 14:15:31 @agent_ppo2.py:185][0m |          -0.0022 |         167.6808 |         -15.1137 |
[32m[20221214 14:15:32 @agent_ppo2.py:185][0m |          -0.0020 |         166.9299 |         -15.1292 |
[32m[20221214 14:15:32 @agent_ppo2.py:185][0m |          -0.0026 |         166.7656 |         -15.1803 |
[32m[20221214 14:15:32 @agent_ppo2.py:185][0m |          -0.0014 |         165.6476 |         -15.0968 |
[32m[20221214 14:15:32 @agent_ppo2.py:185][0m |          -0.0015 |         166.0136 |         -15.2147 |
[32m[20221214 14:15:32 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:15:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.10
[32m[20221214 14:15:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.41
[32m[20221214 14:15:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.70
[32m[20221214 14:15:32 @agent_ppo2.py:143][0m Total time:      17.50 min
[32m[20221214 14:15:32 @agent_ppo2.py:145][0m 1605632 total steps have happened
[32m[20221214 14:15:32 @agent_ppo2.py:121][0m #------------------------ Iteration 784 --------------------------#
[32m[20221214 14:15:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:32 @agent_ppo2.py:185][0m |          -0.0028 |         182.8446 |         -16.3010 |
[32m[20221214 14:15:33 @agent_ppo2.py:185][0m |           0.0136 |         199.0089 |         -16.2163 |
[32m[20221214 14:15:33 @agent_ppo2.py:185][0m |           0.0077 |         181.8729 |         -16.2719 |
[32m[20221214 14:15:33 @agent_ppo2.py:185][0m |          -0.0045 |         175.1335 |         -16.2672 |
[32m[20221214 14:15:33 @agent_ppo2.py:185][0m |           0.0084 |         181.7813 |         -16.2770 |
[32m[20221214 14:15:33 @agent_ppo2.py:185][0m |          -0.0015 |         173.6229 |         -16.2532 |
[32m[20221214 14:15:33 @agent_ppo2.py:185][0m |          -0.0049 |         173.3690 |         -16.2828 |
[32m[20221214 14:15:33 @agent_ppo2.py:185][0m |           0.0118 |         183.4565 |         -16.3431 |
[32m[20221214 14:15:33 @agent_ppo2.py:185][0m |           0.0037 |         175.3851 |         -16.3222 |
[32m[20221214 14:15:33 @agent_ppo2.py:185][0m |          -0.0018 |         171.6289 |         -16.3929 |
[32m[20221214 14:15:33 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:15:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.13
[32m[20221214 14:15:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.44
[32m[20221214 14:15:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.84
[32m[20221214 14:15:33 @agent_ppo2.py:143][0m Total time:      17.52 min
[32m[20221214 14:15:33 @agent_ppo2.py:145][0m 1607680 total steps have happened
[32m[20221214 14:15:33 @agent_ppo2.py:121][0m #------------------------ Iteration 785 --------------------------#
[32m[20221214 14:15:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:34 @agent_ppo2.py:185][0m |          -0.0022 |         162.9328 |         -15.9705 |
[32m[20221214 14:15:34 @agent_ppo2.py:185][0m |          -0.0016 |         156.5988 |         -16.0222 |
[32m[20221214 14:15:34 @agent_ppo2.py:185][0m |          -0.0033 |         154.8218 |         -16.0459 |
[32m[20221214 14:15:34 @agent_ppo2.py:185][0m |          -0.0033 |         153.3894 |         -16.0592 |
[32m[20221214 14:15:34 @agent_ppo2.py:185][0m |           0.0040 |         157.2484 |         -16.0838 |
[32m[20221214 14:15:34 @agent_ppo2.py:185][0m |          -0.0030 |         150.5228 |         -16.0676 |
[32m[20221214 14:15:34 @agent_ppo2.py:185][0m |          -0.0020 |         149.4756 |         -16.0065 |
[32m[20221214 14:15:34 @agent_ppo2.py:185][0m |          -0.0032 |         148.7842 |         -16.0529 |
[32m[20221214 14:15:35 @agent_ppo2.py:185][0m |           0.0037 |         152.0954 |         -16.0290 |
[32m[20221214 14:15:35 @agent_ppo2.py:185][0m |          -0.0033 |         147.3530 |         -16.1192 |
[32m[20221214 14:15:35 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:15:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 784.57
[32m[20221214 14:15:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.77
[32m[20221214 14:15:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.74
[32m[20221214 14:15:35 @agent_ppo2.py:143][0m Total time:      17.55 min
[32m[20221214 14:15:35 @agent_ppo2.py:145][0m 1609728 total steps have happened
[32m[20221214 14:15:35 @agent_ppo2.py:121][0m #------------------------ Iteration 786 --------------------------#
[32m[20221214 14:15:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:15:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:35 @agent_ppo2.py:185][0m |          -0.0007 |         168.5874 |         -16.5690 |
[32m[20221214 14:15:35 @agent_ppo2.py:185][0m |           0.0010 |         166.4888 |         -16.5251 |
[32m[20221214 14:15:35 @agent_ppo2.py:185][0m |           0.0054 |         170.3697 |         -16.4916 |
[32m[20221214 14:15:35 @agent_ppo2.py:185][0m |           0.0013 |         163.5030 |         -16.5069 |
[32m[20221214 14:15:36 @agent_ppo2.py:185][0m |          -0.0006 |         162.4963 |         -16.4055 |
[32m[20221214 14:15:36 @agent_ppo2.py:185][0m |           0.0072 |         166.0875 |         -16.5169 |
[32m[20221214 14:15:36 @agent_ppo2.py:185][0m |          -0.0029 |         161.6455 |         -16.5739 |
[32m[20221214 14:15:36 @agent_ppo2.py:185][0m |          -0.0006 |         159.8991 |         -16.5444 |
[32m[20221214 14:15:36 @agent_ppo2.py:185][0m |           0.0091 |         174.4657 |         -16.5598 |
[32m[20221214 14:15:36 @agent_ppo2.py:185][0m |           0.0019 |         160.1277 |         -16.5824 |
[32m[20221214 14:15:36 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:15:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.17
[32m[20221214 14:15:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.44
[32m[20221214 14:15:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.78
[32m[20221214 14:15:36 @agent_ppo2.py:143][0m Total time:      17.57 min
[32m[20221214 14:15:36 @agent_ppo2.py:145][0m 1611776 total steps have happened
[32m[20221214 14:15:36 @agent_ppo2.py:121][0m #------------------------ Iteration 787 --------------------------#
[32m[20221214 14:15:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:37 @agent_ppo2.py:185][0m |           0.0041 |         216.7227 |         -16.3088 |
[32m[20221214 14:15:37 @agent_ppo2.py:185][0m |           0.0025 |         213.6949 |         -16.4104 |
[32m[20221214 14:15:37 @agent_ppo2.py:185][0m |           0.0008 |         209.4576 |         -16.3825 |
[32m[20221214 14:15:37 @agent_ppo2.py:185][0m |          -0.0038 |         208.5720 |         -16.3832 |
[32m[20221214 14:15:37 @agent_ppo2.py:185][0m |           0.0027 |         209.7116 |         -16.3905 |
[32m[20221214 14:15:37 @agent_ppo2.py:185][0m |          -0.0021 |         206.5182 |         -16.3629 |
[32m[20221214 14:15:37 @agent_ppo2.py:185][0m |          -0.0008 |         207.0554 |         -16.3693 |
[32m[20221214 14:15:37 @agent_ppo2.py:185][0m |          -0.0041 |         205.8883 |         -16.3510 |
[32m[20221214 14:15:37 @agent_ppo2.py:185][0m |           0.0007 |         207.3871 |         -16.2867 |
[32m[20221214 14:15:37 @agent_ppo2.py:185][0m |          -0.0025 |         205.2563 |         -16.2606 |
[32m[20221214 14:15:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:15:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.76
[32m[20221214 14:15:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.42
[32m[20221214 14:15:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.75
[32m[20221214 14:15:37 @agent_ppo2.py:143][0m Total time:      17.59 min
[32m[20221214 14:15:37 @agent_ppo2.py:145][0m 1613824 total steps have happened
[32m[20221214 14:15:37 @agent_ppo2.py:121][0m #------------------------ Iteration 788 --------------------------#
[32m[20221214 14:15:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:38 @agent_ppo2.py:185][0m |           0.0019 |         211.6903 |         -16.0451 |
[32m[20221214 14:15:38 @agent_ppo2.py:185][0m |          -0.0026 |         204.1299 |         -15.9740 |
[32m[20221214 14:15:38 @agent_ppo2.py:185][0m |          -0.0039 |         202.3895 |         -15.9532 |
[32m[20221214 14:15:38 @agent_ppo2.py:185][0m |          -0.0040 |         200.7772 |         -15.9654 |
[32m[20221214 14:15:38 @agent_ppo2.py:185][0m |          -0.0046 |         200.4408 |         -15.9364 |
[32m[20221214 14:15:38 @agent_ppo2.py:185][0m |          -0.0051 |         199.7620 |         -15.9315 |
[32m[20221214 14:15:38 @agent_ppo2.py:185][0m |          -0.0038 |         199.6782 |         -15.8858 |
[32m[20221214 14:15:38 @agent_ppo2.py:185][0m |           0.0066 |         209.8103 |         -15.8515 |
[32m[20221214 14:15:38 @agent_ppo2.py:185][0m |          -0.0059 |         199.2637 |         -15.8544 |
[32m[20221214 14:15:39 @agent_ppo2.py:185][0m |          -0.0042 |         199.0311 |         -15.8330 |
[32m[20221214 14:15:39 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:15:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.81
[32m[20221214 14:15:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.01
[32m[20221214 14:15:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.21
[32m[20221214 14:15:39 @agent_ppo2.py:143][0m Total time:      17.61 min
[32m[20221214 14:15:39 @agent_ppo2.py:145][0m 1615872 total steps have happened
[32m[20221214 14:15:39 @agent_ppo2.py:121][0m #------------------------ Iteration 789 --------------------------#
[32m[20221214 14:15:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:15:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:39 @agent_ppo2.py:185][0m |          -0.0034 |         225.0520 |         -14.9568 |
[32m[20221214 14:15:39 @agent_ppo2.py:185][0m |          -0.0020 |         223.4352 |         -15.0487 |
[32m[20221214 14:15:39 @agent_ppo2.py:185][0m |          -0.0013 |         222.7441 |         -15.1834 |
[32m[20221214 14:15:39 @agent_ppo2.py:185][0m |          -0.0012 |         222.0783 |         -15.1550 |
[32m[20221214 14:15:39 @agent_ppo2.py:185][0m |          -0.0008 |         222.3786 |         -15.1337 |
[32m[20221214 14:15:39 @agent_ppo2.py:185][0m |          -0.0002 |         222.5639 |         -15.2302 |
[32m[20221214 14:15:40 @agent_ppo2.py:185][0m |          -0.0010 |         222.0106 |         -15.3969 |
[32m[20221214 14:15:40 @agent_ppo2.py:185][0m |          -0.0032 |         221.6148 |         -15.3581 |
[32m[20221214 14:15:40 @agent_ppo2.py:185][0m |          -0.0023 |         221.7253 |         -15.4338 |
[32m[20221214 14:15:40 @agent_ppo2.py:185][0m |          -0.0002 |         222.7395 |         -15.4309 |
[32m[20221214 14:15:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:15:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.95
[32m[20221214 14:15:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.21
[32m[20221214 14:15:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.18
[32m[20221214 14:15:40 @agent_ppo2.py:143][0m Total time:      17.63 min
[32m[20221214 14:15:40 @agent_ppo2.py:145][0m 1617920 total steps have happened
[32m[20221214 14:15:40 @agent_ppo2.py:121][0m #------------------------ Iteration 790 --------------------------#
[32m[20221214 14:15:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:15:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:40 @agent_ppo2.py:185][0m |           0.0024 |         246.4750 |         -16.4398 |
[32m[20221214 14:15:40 @agent_ppo2.py:185][0m |           0.0067 |         245.3617 |         -16.3939 |
[32m[20221214 14:15:41 @agent_ppo2.py:185][0m |          -0.0023 |         236.1749 |         -16.3520 |
[32m[20221214 14:15:41 @agent_ppo2.py:185][0m |          -0.0021 |         234.4046 |         -16.4351 |
[32m[20221214 14:15:41 @agent_ppo2.py:185][0m |          -0.0029 |         232.9755 |         -16.3995 |
[32m[20221214 14:15:41 @agent_ppo2.py:185][0m |           0.0032 |         237.6079 |         -16.4172 |
[32m[20221214 14:15:41 @agent_ppo2.py:185][0m |          -0.0032 |         230.3051 |         -16.4130 |
[32m[20221214 14:15:41 @agent_ppo2.py:185][0m |          -0.0032 |         230.1948 |         -16.4090 |
[32m[20221214 14:15:41 @agent_ppo2.py:185][0m |          -0.0031 |         228.6489 |         -16.4018 |
[32m[20221214 14:15:41 @agent_ppo2.py:185][0m |          -0.0034 |         227.6209 |         -16.4386 |
[32m[20221214 14:15:41 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:15:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.07
[32m[20221214 14:15:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.86
[32m[20221214 14:15:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.90
[32m[20221214 14:15:41 @agent_ppo2.py:143][0m Total time:      17.65 min
[32m[20221214 14:15:41 @agent_ppo2.py:145][0m 1619968 total steps have happened
[32m[20221214 14:15:41 @agent_ppo2.py:121][0m #------------------------ Iteration 791 --------------------------#
[32m[20221214 14:15:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:42 @agent_ppo2.py:185][0m |          -0.0015 |         244.7253 |         -16.6476 |
[32m[20221214 14:15:42 @agent_ppo2.py:185][0m |          -0.0017 |         238.7824 |         -16.5576 |
[32m[20221214 14:15:42 @agent_ppo2.py:185][0m |          -0.0009 |         237.7171 |         -16.5405 |
[32m[20221214 14:15:42 @agent_ppo2.py:185][0m |          -0.0006 |         237.9229 |         -16.5748 |
[32m[20221214 14:15:42 @agent_ppo2.py:185][0m |          -0.0029 |         236.5172 |         -16.5426 |
[32m[20221214 14:15:42 @agent_ppo2.py:185][0m |           0.0101 |         258.4287 |         -16.4084 |
[32m[20221214 14:15:42 @agent_ppo2.py:185][0m |           0.0112 |         263.3400 |         -16.5382 |
[32m[20221214 14:15:42 @agent_ppo2.py:185][0m |          -0.0010 |         236.6763 |         -16.5680 |
[32m[20221214 14:15:42 @agent_ppo2.py:185][0m |          -0.0019 |         235.7989 |         -16.3119 |
[32m[20221214 14:15:42 @agent_ppo2.py:185][0m |          -0.0020 |         235.9267 |         -16.4379 |
[32m[20221214 14:15:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:15:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.74
[32m[20221214 14:15:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.87
[32m[20221214 14:15:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.58
[32m[20221214 14:15:43 @agent_ppo2.py:143][0m Total time:      17.67 min
[32m[20221214 14:15:43 @agent_ppo2.py:145][0m 1622016 total steps have happened
[32m[20221214 14:15:43 @agent_ppo2.py:121][0m #------------------------ Iteration 792 --------------------------#
[32m[20221214 14:15:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:43 @agent_ppo2.py:185][0m |          -0.0023 |         245.3741 |         -16.0465 |
[32m[20221214 14:15:43 @agent_ppo2.py:185][0m |           0.0001 |         243.3279 |         -16.0432 |
[32m[20221214 14:15:43 @agent_ppo2.py:185][0m |           0.0074 |         262.0906 |         -16.0931 |
[32m[20221214 14:15:43 @agent_ppo2.py:185][0m |          -0.0022 |         236.9999 |         -15.9755 |
[32m[20221214 14:15:43 @agent_ppo2.py:185][0m |          -0.0046 |         235.1421 |         -16.1086 |
[32m[20221214 14:15:43 @agent_ppo2.py:185][0m |           0.0037 |         239.6998 |         -16.1457 |
[32m[20221214 14:15:44 @agent_ppo2.py:185][0m |           0.0065 |         247.4626 |         -16.2060 |
[32m[20221214 14:15:44 @agent_ppo2.py:185][0m |          -0.0030 |         232.9905 |         -16.2268 |
[32m[20221214 14:15:44 @agent_ppo2.py:185][0m |           0.0009 |         232.3943 |         -16.2457 |
[32m[20221214 14:15:44 @agent_ppo2.py:185][0m |          -0.0043 |         231.3908 |         -16.2572 |
[32m[20221214 14:15:44 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:15:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.96
[32m[20221214 14:15:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.77
[32m[20221214 14:15:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.67
[32m[20221214 14:15:44 @agent_ppo2.py:143][0m Total time:      17.70 min
[32m[20221214 14:15:44 @agent_ppo2.py:145][0m 1624064 total steps have happened
[32m[20221214 14:15:44 @agent_ppo2.py:121][0m #------------------------ Iteration 793 --------------------------#
[32m[20221214 14:15:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:44 @agent_ppo2.py:185][0m |          -0.0008 |         243.3646 |         -16.1703 |
[32m[20221214 14:15:44 @agent_ppo2.py:185][0m |          -0.0025 |         234.9055 |         -16.1349 |
[32m[20221214 14:15:45 @agent_ppo2.py:185][0m |           0.0063 |         242.4375 |         -16.0860 |
[32m[20221214 14:15:45 @agent_ppo2.py:185][0m |           0.0013 |         232.1580 |         -16.0174 |
[32m[20221214 14:15:45 @agent_ppo2.py:185][0m |          -0.0016 |         230.3547 |         -15.9711 |
[32m[20221214 14:15:45 @agent_ppo2.py:185][0m |          -0.0008 |         230.3145 |         -15.9706 |
[32m[20221214 14:15:45 @agent_ppo2.py:185][0m |          -0.0022 |         230.4098 |         -15.9445 |
[32m[20221214 14:15:45 @agent_ppo2.py:185][0m |           0.0020 |         232.3063 |         -15.9146 |
[32m[20221214 14:15:45 @agent_ppo2.py:185][0m |          -0.0007 |         229.2819 |         -15.8330 |
[32m[20221214 14:15:45 @agent_ppo2.py:185][0m |          -0.0012 |         229.7959 |         -15.8112 |
[32m[20221214 14:15:45 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:15:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.98
[32m[20221214 14:15:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.37
[32m[20221214 14:15:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.26
[32m[20221214 14:15:45 @agent_ppo2.py:143][0m Total time:      17.72 min
[32m[20221214 14:15:45 @agent_ppo2.py:145][0m 1626112 total steps have happened
[32m[20221214 14:15:45 @agent_ppo2.py:121][0m #------------------------ Iteration 794 --------------------------#
[32m[20221214 14:15:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:46 @agent_ppo2.py:185][0m |          -0.0011 |         225.6542 |         -15.8308 |
[32m[20221214 14:15:46 @agent_ppo2.py:185][0m |           0.0125 |         242.9357 |         -15.7484 |
[32m[20221214 14:15:46 @agent_ppo2.py:185][0m |          -0.0018 |         222.7895 |         -15.8216 |
[32m[20221214 14:15:46 @agent_ppo2.py:185][0m |          -0.0016 |         222.2308 |         -15.8311 |
[32m[20221214 14:15:46 @agent_ppo2.py:185][0m |          -0.0018 |         222.3340 |         -15.8226 |
[32m[20221214 14:15:46 @agent_ppo2.py:185][0m |          -0.0023 |         221.6922 |         -15.9365 |
[32m[20221214 14:15:46 @agent_ppo2.py:185][0m |           0.0027 |         223.1601 |         -15.8232 |
[32m[20221214 14:15:46 @agent_ppo2.py:185][0m |          -0.0030 |         222.1325 |         -15.8437 |
[32m[20221214 14:15:47 @agent_ppo2.py:185][0m |           0.0032 |         225.2575 |         -15.9884 |
[32m[20221214 14:15:47 @agent_ppo2.py:185][0m |           0.0035 |         223.7312 |         -15.9161 |
[32m[20221214 14:15:47 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:15:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.78
[32m[20221214 14:15:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.94
[32m[20221214 14:15:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.48
[32m[20221214 14:15:47 @agent_ppo2.py:143][0m Total time:      17.75 min
[32m[20221214 14:15:47 @agent_ppo2.py:145][0m 1628160 total steps have happened
[32m[20221214 14:15:47 @agent_ppo2.py:121][0m #------------------------ Iteration 795 --------------------------#
[32m[20221214 14:15:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:47 @agent_ppo2.py:185][0m |          -0.0016 |         236.2462 |         -16.3458 |
[32m[20221214 14:15:47 @agent_ppo2.py:185][0m |          -0.0017 |         227.6137 |         -16.4767 |
[32m[20221214 14:15:47 @agent_ppo2.py:185][0m |          -0.0018 |         225.7538 |         -16.3436 |
[32m[20221214 14:15:47 @agent_ppo2.py:185][0m |           0.0003 |         222.8186 |         -16.4334 |
[32m[20221214 14:15:48 @agent_ppo2.py:185][0m |          -0.0028 |         221.7746 |         -16.4325 |
[32m[20221214 14:15:48 @agent_ppo2.py:185][0m |           0.0020 |         222.4233 |         -16.3868 |
[32m[20221214 14:15:48 @agent_ppo2.py:185][0m |          -0.0040 |         219.0487 |         -16.3846 |
[32m[20221214 14:15:48 @agent_ppo2.py:185][0m |          -0.0041 |         218.6520 |         -16.4489 |
[32m[20221214 14:15:48 @agent_ppo2.py:185][0m |          -0.0042 |         217.6833 |         -16.4362 |
[32m[20221214 14:15:48 @agent_ppo2.py:185][0m |          -0.0048 |         217.1692 |         -16.4366 |
[32m[20221214 14:15:48 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:15:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.43
[32m[20221214 14:15:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.18
[32m[20221214 14:15:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.97
[32m[20221214 14:15:48 @agent_ppo2.py:143][0m Total time:      17.77 min
[32m[20221214 14:15:48 @agent_ppo2.py:145][0m 1630208 total steps have happened
[32m[20221214 14:15:48 @agent_ppo2.py:121][0m #------------------------ Iteration 796 --------------------------#
[32m[20221214 14:15:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:48 @agent_ppo2.py:185][0m |          -0.0016 |         222.2259 |         -16.7225 |
[32m[20221214 14:15:49 @agent_ppo2.py:185][0m |          -0.0018 |         219.2921 |         -16.7550 |
[32m[20221214 14:15:49 @agent_ppo2.py:185][0m |          -0.0015 |         217.4426 |         -16.7570 |
[32m[20221214 14:15:49 @agent_ppo2.py:185][0m |          -0.0018 |         216.8503 |         -16.7286 |
[32m[20221214 14:15:49 @agent_ppo2.py:185][0m |          -0.0032 |         216.1978 |         -16.8465 |
[32m[20221214 14:15:49 @agent_ppo2.py:185][0m |           0.0021 |         215.1344 |         -16.8741 |
[32m[20221214 14:15:49 @agent_ppo2.py:185][0m |          -0.0034 |         214.0250 |         -16.6199 |
[32m[20221214 14:15:49 @agent_ppo2.py:185][0m |          -0.0011 |         213.4545 |         -16.9156 |
[32m[20221214 14:15:49 @agent_ppo2.py:185][0m |           0.0003 |         213.3295 |         -16.8778 |
[32m[20221214 14:15:49 @agent_ppo2.py:185][0m |          -0.0018 |         212.9745 |         -16.8823 |
[32m[20221214 14:15:49 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:15:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.94
[32m[20221214 14:15:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.46
[32m[20221214 14:15:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.91
[32m[20221214 14:15:50 @agent_ppo2.py:143][0m Total time:      17.79 min
[32m[20221214 14:15:50 @agent_ppo2.py:145][0m 1632256 total steps have happened
[32m[20221214 14:15:50 @agent_ppo2.py:121][0m #------------------------ Iteration 797 --------------------------#
[32m[20221214 14:15:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:50 @agent_ppo2.py:185][0m |           0.0011 |         218.8137 |         -16.1140 |
[32m[20221214 14:15:50 @agent_ppo2.py:185][0m |          -0.0011 |         214.8161 |         -16.1236 |
[32m[20221214 14:15:50 @agent_ppo2.py:185][0m |           0.0045 |         220.0788 |         -16.1986 |
[32m[20221214 14:15:50 @agent_ppo2.py:185][0m |          -0.0008 |         211.5695 |         -16.0892 |
[32m[20221214 14:15:50 @agent_ppo2.py:185][0m |           0.0095 |         232.9139 |         -16.2009 |
[32m[20221214 14:15:50 @agent_ppo2.py:185][0m |           0.0004 |         211.4958 |         -16.2065 |
[32m[20221214 14:15:50 @agent_ppo2.py:185][0m |           0.0064 |         219.7751 |         -16.2271 |
[32m[20221214 14:15:51 @agent_ppo2.py:185][0m |          -0.0023 |         210.7009 |         -16.2162 |
[32m[20221214 14:15:51 @agent_ppo2.py:185][0m |           0.0050 |         219.4514 |         -16.2393 |
[32m[20221214 14:15:51 @agent_ppo2.py:185][0m |          -0.0004 |         210.8248 |         -16.1952 |
[32m[20221214 14:15:51 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:15:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.47
[32m[20221214 14:15:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.80
[32m[20221214 14:15:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.26
[32m[20221214 14:15:51 @agent_ppo2.py:143][0m Total time:      17.81 min
[32m[20221214 14:15:51 @agent_ppo2.py:145][0m 1634304 total steps have happened
[32m[20221214 14:15:51 @agent_ppo2.py:121][0m #------------------------ Iteration 798 --------------------------#
[32m[20221214 14:15:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:51 @agent_ppo2.py:185][0m |          -0.0010 |         212.1730 |         -16.9456 |
[32m[20221214 14:15:51 @agent_ppo2.py:185][0m |           0.0021 |         213.9222 |         -16.9762 |
[32m[20221214 14:15:51 @agent_ppo2.py:185][0m |          -0.0027 |         210.1126 |         -16.9384 |
[32m[20221214 14:15:52 @agent_ppo2.py:185][0m |          -0.0039 |         210.2561 |         -16.9761 |
[32m[20221214 14:15:52 @agent_ppo2.py:185][0m |          -0.0019 |         209.8569 |         -16.9711 |
[32m[20221214 14:15:52 @agent_ppo2.py:185][0m |           0.0110 |         235.5145 |         -16.9159 |
[32m[20221214 14:15:52 @agent_ppo2.py:185][0m |          -0.0002 |         208.2823 |         -16.9081 |
[32m[20221214 14:15:52 @agent_ppo2.py:185][0m |          -0.0028 |         208.2672 |         -16.9419 |
[32m[20221214 14:15:52 @agent_ppo2.py:185][0m |          -0.0020 |         207.3202 |         -16.8744 |
[32m[20221214 14:15:52 @agent_ppo2.py:185][0m |          -0.0029 |         207.7749 |         -16.8124 |
[32m[20221214 14:15:52 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:15:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.55
[32m[20221214 14:15:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.22
[32m[20221214 14:15:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.07
[32m[20221214 14:15:52 @agent_ppo2.py:143][0m Total time:      17.84 min
[32m[20221214 14:15:52 @agent_ppo2.py:145][0m 1636352 total steps have happened
[32m[20221214 14:15:52 @agent_ppo2.py:121][0m #------------------------ Iteration 799 --------------------------#
[32m[20221214 14:15:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:53 @agent_ppo2.py:185][0m |           0.0029 |         215.0977 |         -17.4312 |
[32m[20221214 14:15:53 @agent_ppo2.py:185][0m |           0.0101 |         225.3013 |         -17.4435 |
[32m[20221214 14:15:53 @agent_ppo2.py:185][0m |          -0.0001 |         212.6673 |         -17.4914 |
[32m[20221214 14:15:53 @agent_ppo2.py:185][0m |           0.0060 |         220.5929 |         -17.5021 |
[32m[20221214 14:15:53 @agent_ppo2.py:185][0m |          -0.0002 |         211.4958 |         -17.5467 |
[32m[20221214 14:15:53 @agent_ppo2.py:185][0m |           0.0013 |         211.6133 |         -17.6232 |
[32m[20221214 14:15:53 @agent_ppo2.py:185][0m |           0.0073 |         223.0058 |         -17.5455 |
[32m[20221214 14:15:53 @agent_ppo2.py:185][0m |          -0.0016 |         210.5340 |         -17.7533 |
[32m[20221214 14:15:53 @agent_ppo2.py:185][0m |           0.0018 |         210.6083 |         -17.7177 |
[32m[20221214 14:15:53 @agent_ppo2.py:185][0m |          -0.0016 |         209.6971 |         -17.6258 |
[32m[20221214 14:15:53 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:15:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.87
[32m[20221214 14:15:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.90
[32m[20221214 14:15:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.85
[32m[20221214 14:15:54 @agent_ppo2.py:143][0m Total time:      17.86 min
[32m[20221214 14:15:54 @agent_ppo2.py:145][0m 1638400 total steps have happened
[32m[20221214 14:15:54 @agent_ppo2.py:121][0m #------------------------ Iteration 800 --------------------------#
[32m[20221214 14:15:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:15:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:54 @agent_ppo2.py:185][0m |          -0.0017 |         209.2174 |         -17.6172 |
[32m[20221214 14:15:54 @agent_ppo2.py:185][0m |          -0.0039 |         206.5597 |         -17.6459 |
[32m[20221214 14:15:54 @agent_ppo2.py:185][0m |          -0.0043 |         205.8080 |         -17.6781 |
[32m[20221214 14:15:54 @agent_ppo2.py:185][0m |          -0.0039 |         204.2981 |         -17.6801 |
[32m[20221214 14:15:54 @agent_ppo2.py:185][0m |          -0.0019 |         204.0209 |         -17.7321 |
[32m[20221214 14:15:54 @agent_ppo2.py:185][0m |           0.0001 |         204.6393 |         -17.7040 |
[32m[20221214 14:15:55 @agent_ppo2.py:185][0m |          -0.0014 |         204.0124 |         -17.7985 |
[32m[20221214 14:15:55 @agent_ppo2.py:185][0m |          -0.0046 |         204.1442 |         -17.7745 |
[32m[20221214 14:15:55 @agent_ppo2.py:185][0m |          -0.0038 |         203.0917 |         -17.8091 |
[32m[20221214 14:15:55 @agent_ppo2.py:185][0m |          -0.0031 |         202.8655 |         -17.8257 |
[32m[20221214 14:15:55 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:15:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.59
[32m[20221214 14:15:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.69
[32m[20221214 14:15:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.50
[32m[20221214 14:15:55 @agent_ppo2.py:143][0m Total time:      17.88 min
[32m[20221214 14:15:55 @agent_ppo2.py:145][0m 1640448 total steps have happened
[32m[20221214 14:15:55 @agent_ppo2.py:121][0m #------------------------ Iteration 801 --------------------------#
[32m[20221214 14:15:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:55 @agent_ppo2.py:185][0m |           0.0002 |         229.1002 |         -17.6453 |
[32m[20221214 14:15:55 @agent_ppo2.py:185][0m |          -0.0001 |         221.2203 |         -17.5724 |
[32m[20221214 14:15:56 @agent_ppo2.py:185][0m |          -0.0011 |         216.1052 |         -17.6466 |
[32m[20221214 14:15:56 @agent_ppo2.py:185][0m |          -0.0033 |         213.2165 |         -17.5501 |
[32m[20221214 14:15:56 @agent_ppo2.py:185][0m |           0.0081 |         215.3268 |         -17.6491 |
[32m[20221214 14:15:56 @agent_ppo2.py:185][0m |           0.0017 |         213.8029 |         -17.6796 |
[32m[20221214 14:15:56 @agent_ppo2.py:185][0m |          -0.0019 |         209.2129 |         -17.6268 |
[32m[20221214 14:15:56 @agent_ppo2.py:185][0m |          -0.0015 |         208.3386 |         -17.6288 |
[32m[20221214 14:15:56 @agent_ppo2.py:185][0m |          -0.0016 |         207.9148 |         -17.6738 |
[32m[20221214 14:15:56 @agent_ppo2.py:185][0m |          -0.0014 |         208.7216 |         -17.7129 |
[32m[20221214 14:15:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:15:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.71
[32m[20221214 14:15:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.28
[32m[20221214 14:15:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.10
[32m[20221214 14:15:56 @agent_ppo2.py:143][0m Total time:      17.91 min
[32m[20221214 14:15:56 @agent_ppo2.py:145][0m 1642496 total steps have happened
[32m[20221214 14:15:56 @agent_ppo2.py:121][0m #------------------------ Iteration 802 --------------------------#
[32m[20221214 14:15:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:57 @agent_ppo2.py:185][0m |          -0.0028 |         215.0361 |         -18.2920 |
[32m[20221214 14:15:57 @agent_ppo2.py:185][0m |          -0.0018 |         213.5351 |         -18.1967 |
[32m[20221214 14:15:57 @agent_ppo2.py:185][0m |          -0.0017 |         212.6946 |         -18.1798 |
[32m[20221214 14:15:57 @agent_ppo2.py:185][0m |          -0.0031 |         212.7908 |         -18.1500 |
[32m[20221214 14:15:57 @agent_ppo2.py:185][0m |          -0.0024 |         212.2393 |         -18.1442 |
[32m[20221214 14:15:57 @agent_ppo2.py:185][0m |          -0.0030 |         211.9286 |         -18.1581 |
[32m[20221214 14:15:57 @agent_ppo2.py:185][0m |          -0.0022 |         211.6409 |         -18.1276 |
[32m[20221214 14:15:57 @agent_ppo2.py:185][0m |           0.0003 |         212.2495 |         -17.9988 |
[32m[20221214 14:15:57 @agent_ppo2.py:185][0m |          -0.0018 |         211.2700 |         -18.1111 |
[32m[20221214 14:15:58 @agent_ppo2.py:185][0m |          -0.0032 |         210.8286 |         -18.0232 |
[32m[20221214 14:15:58 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:15:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.41
[32m[20221214 14:15:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.98
[32m[20221214 14:15:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.17
[32m[20221214 14:15:58 @agent_ppo2.py:143][0m Total time:      17.93 min
[32m[20221214 14:15:58 @agent_ppo2.py:145][0m 1644544 total steps have happened
[32m[20221214 14:15:58 @agent_ppo2.py:121][0m #------------------------ Iteration 803 --------------------------#
[32m[20221214 14:15:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:58 @agent_ppo2.py:185][0m |           0.0090 |         226.3351 |         -18.2702 |
[32m[20221214 14:15:58 @agent_ppo2.py:185][0m |          -0.0011 |         206.7902 |         -18.3301 |
[32m[20221214 14:15:58 @agent_ppo2.py:185][0m |          -0.0025 |         203.8664 |         -18.3870 |
[32m[20221214 14:15:58 @agent_ppo2.py:185][0m |          -0.0026 |         201.5452 |         -18.4021 |
[32m[20221214 14:15:58 @agent_ppo2.py:185][0m |          -0.0020 |         200.0015 |         -18.4207 |
[32m[20221214 14:15:59 @agent_ppo2.py:185][0m |          -0.0017 |         199.2724 |         -18.3835 |
[32m[20221214 14:15:59 @agent_ppo2.py:185][0m |          -0.0032 |         198.2490 |         -18.4557 |
[32m[20221214 14:15:59 @agent_ppo2.py:185][0m |          -0.0039 |         197.2448 |         -18.4655 |
[32m[20221214 14:15:59 @agent_ppo2.py:185][0m |           0.0105 |         220.4707 |         -18.4645 |
[32m[20221214 14:15:59 @agent_ppo2.py:185][0m |          -0.0030 |         196.9945 |         -18.3958 |
[32m[20221214 14:15:59 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:15:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.87
[32m[20221214 14:15:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.25
[32m[20221214 14:15:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.86
[32m[20221214 14:15:59 @agent_ppo2.py:143][0m Total time:      17.95 min
[32m[20221214 14:15:59 @agent_ppo2.py:145][0m 1646592 total steps have happened
[32m[20221214 14:15:59 @agent_ppo2.py:121][0m #------------------------ Iteration 804 --------------------------#
[32m[20221214 14:15:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:15:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:15:59 @agent_ppo2.py:185][0m |          -0.0015 |         212.5140 |         -18.5744 |
[32m[20221214 14:16:00 @agent_ppo2.py:185][0m |          -0.0029 |         210.9799 |         -18.6705 |
[32m[20221214 14:16:00 @agent_ppo2.py:185][0m |          -0.0035 |         210.5241 |         -18.7023 |
[32m[20221214 14:16:00 @agent_ppo2.py:185][0m |          -0.0007 |         210.0632 |         -18.6959 |
[32m[20221214 14:16:00 @agent_ppo2.py:185][0m |          -0.0024 |         209.4217 |         -18.7691 |
[32m[20221214 14:16:00 @agent_ppo2.py:185][0m |           0.0152 |         231.5039 |         -18.8167 |
[32m[20221214 14:16:00 @agent_ppo2.py:185][0m |          -0.0012 |         210.5818 |         -18.7461 |
[32m[20221214 14:16:00 @agent_ppo2.py:185][0m |          -0.0033 |         208.7117 |         -18.7934 |
[32m[20221214 14:16:00 @agent_ppo2.py:185][0m |          -0.0020 |         208.6142 |         -18.8168 |
[32m[20221214 14:16:00 @agent_ppo2.py:185][0m |          -0.0024 |         208.4135 |         -18.9172 |
[32m[20221214 14:16:00 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:16:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.77
[32m[20221214 14:16:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.22
[32m[20221214 14:16:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.85
[32m[20221214 14:16:00 @agent_ppo2.py:143][0m Total time:      17.97 min
[32m[20221214 14:16:00 @agent_ppo2.py:145][0m 1648640 total steps have happened
[32m[20221214 14:16:00 @agent_ppo2.py:121][0m #------------------------ Iteration 805 --------------------------#
[32m[20221214 14:16:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:01 @agent_ppo2.py:185][0m |           0.0036 |         206.9814 |         -17.1042 |
[32m[20221214 14:16:01 @agent_ppo2.py:185][0m |          -0.0018 |         203.3721 |         -17.1096 |
[32m[20221214 14:16:01 @agent_ppo2.py:185][0m |          -0.0023 |         201.7564 |         -17.1669 |
[32m[20221214 14:16:01 @agent_ppo2.py:185][0m |           0.0119 |         228.0166 |         -17.2060 |
[32m[20221214 14:16:01 @agent_ppo2.py:185][0m |          -0.0025 |         201.3090 |         -17.1934 |
[32m[20221214 14:16:01 @agent_ppo2.py:185][0m |          -0.0017 |         200.6188 |         -17.1926 |
[32m[20221214 14:16:01 @agent_ppo2.py:185][0m |           0.0075 |         216.5137 |         -17.1693 |
[32m[20221214 14:16:02 @agent_ppo2.py:185][0m |          -0.0031 |         200.1390 |         -17.2026 |
[32m[20221214 14:16:02 @agent_ppo2.py:185][0m |          -0.0016 |         200.1148 |         -17.0836 |
[32m[20221214 14:16:02 @agent_ppo2.py:185][0m |           0.0120 |         226.5193 |         -17.2457 |
[32m[20221214 14:16:02 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:16:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.16
[32m[20221214 14:16:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.14
[32m[20221214 14:16:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.61
[32m[20221214 14:16:02 @agent_ppo2.py:143][0m Total time:      18.00 min
[32m[20221214 14:16:02 @agent_ppo2.py:145][0m 1650688 total steps have happened
[32m[20221214 14:16:02 @agent_ppo2.py:121][0m #------------------------ Iteration 806 --------------------------#
[32m[20221214 14:16:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:02 @agent_ppo2.py:185][0m |          -0.0004 |         193.8779 |         -17.9904 |
[32m[20221214 14:16:02 @agent_ppo2.py:185][0m |          -0.0021 |         191.4504 |         -18.0894 |
[32m[20221214 14:16:02 @agent_ppo2.py:185][0m |          -0.0030 |         190.4199 |         -18.1020 |
[32m[20221214 14:16:02 @agent_ppo2.py:185][0m |          -0.0018 |         189.2546 |         -18.1767 |
[32m[20221214 14:16:03 @agent_ppo2.py:185][0m |           0.0016 |         191.2895 |         -18.1558 |
[32m[20221214 14:16:03 @agent_ppo2.py:185][0m |          -0.0015 |         188.2048 |         -18.1648 |
[32m[20221214 14:16:03 @agent_ppo2.py:185][0m |          -0.0008 |         188.2847 |         -18.2014 |
[32m[20221214 14:16:03 @agent_ppo2.py:185][0m |          -0.0011 |         187.8544 |         -18.2255 |
[32m[20221214 14:16:03 @agent_ppo2.py:185][0m |          -0.0030 |         187.3344 |         -18.2376 |
[32m[20221214 14:16:03 @agent_ppo2.py:185][0m |          -0.0020 |         187.2959 |         -18.3024 |
[32m[20221214 14:16:03 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.08
[32m[20221214 14:16:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.73
[32m[20221214 14:16:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.96
[32m[20221214 14:16:03 @agent_ppo2.py:143][0m Total time:      18.02 min
[32m[20221214 14:16:03 @agent_ppo2.py:145][0m 1652736 total steps have happened
[32m[20221214 14:16:03 @agent_ppo2.py:121][0m #------------------------ Iteration 807 --------------------------#
[32m[20221214 14:16:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:04 @agent_ppo2.py:185][0m |          -0.0021 |         197.9573 |         -19.3387 |
[32m[20221214 14:16:04 @agent_ppo2.py:185][0m |          -0.0032 |         196.3125 |         -19.2828 |
[32m[20221214 14:16:04 @agent_ppo2.py:185][0m |          -0.0032 |         195.4606 |         -19.2544 |
[32m[20221214 14:16:04 @agent_ppo2.py:185][0m |          -0.0020 |         195.1104 |         -19.3680 |
[32m[20221214 14:16:04 @agent_ppo2.py:185][0m |          -0.0037 |         194.5433 |         -19.2392 |
[32m[20221214 14:16:04 @agent_ppo2.py:185][0m |           0.0012 |         194.6653 |         -19.3669 |
[32m[20221214 14:16:04 @agent_ppo2.py:185][0m |          -0.0016 |         194.7246 |         -19.2491 |
[32m[20221214 14:16:04 @agent_ppo2.py:185][0m |          -0.0017 |         193.9885 |         -19.2653 |
[32m[20221214 14:16:04 @agent_ppo2.py:185][0m |          -0.0037 |         193.9747 |         -19.2673 |
[32m[20221214 14:16:04 @agent_ppo2.py:185][0m |          -0.0033 |         193.8435 |         -19.2787 |
[32m[20221214 14:16:04 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:16:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.95
[32m[20221214 14:16:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.07
[32m[20221214 14:16:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.62
[32m[20221214 14:16:05 @agent_ppo2.py:143][0m Total time:      18.04 min
[32m[20221214 14:16:05 @agent_ppo2.py:145][0m 1654784 total steps have happened
[32m[20221214 14:16:05 @agent_ppo2.py:121][0m #------------------------ Iteration 808 --------------------------#
[32m[20221214 14:16:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:05 @agent_ppo2.py:185][0m |          -0.0015 |         207.1843 |         -18.9383 |
[32m[20221214 14:16:05 @agent_ppo2.py:185][0m |           0.0131 |         223.5395 |         -18.8939 |
[32m[20221214 14:16:05 @agent_ppo2.py:185][0m |          -0.0016 |         198.7344 |         -18.8515 |
[32m[20221214 14:16:05 @agent_ppo2.py:185][0m |          -0.0016 |         196.6683 |         -18.7575 |
[32m[20221214 14:16:05 @agent_ppo2.py:185][0m |          -0.0016 |         195.5782 |         -18.8200 |
[32m[20221214 14:16:05 @agent_ppo2.py:185][0m |          -0.0022 |         194.6832 |         -18.8192 |
[32m[20221214 14:16:06 @agent_ppo2.py:185][0m |          -0.0028 |         193.3019 |         -18.7776 |
[32m[20221214 14:16:06 @agent_ppo2.py:185][0m |           0.0012 |         193.2166 |         -18.7940 |
[32m[20221214 14:16:06 @agent_ppo2.py:185][0m |          -0.0007 |         191.1918 |         -18.5995 |
[32m[20221214 14:16:06 @agent_ppo2.py:185][0m |           0.0025 |         191.5185 |         -18.7716 |
[32m[20221214 14:16:06 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.61
[32m[20221214 14:16:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.38
[32m[20221214 14:16:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.64
[32m[20221214 14:16:06 @agent_ppo2.py:143][0m Total time:      18.06 min
[32m[20221214 14:16:06 @agent_ppo2.py:145][0m 1656832 total steps have happened
[32m[20221214 14:16:06 @agent_ppo2.py:121][0m #------------------------ Iteration 809 --------------------------#
[32m[20221214 14:16:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:06 @agent_ppo2.py:185][0m |           0.0009 |         200.4501 |         -18.7722 |
[32m[20221214 14:16:06 @agent_ppo2.py:185][0m |          -0.0006 |         197.0830 |         -18.7217 |
[32m[20221214 14:16:07 @agent_ppo2.py:185][0m |          -0.0026 |         195.7644 |         -18.6759 |
[32m[20221214 14:16:07 @agent_ppo2.py:185][0m |          -0.0007 |         194.5936 |         -18.6640 |
[32m[20221214 14:16:07 @agent_ppo2.py:185][0m |           0.0008 |         194.4046 |         -18.6341 |
[32m[20221214 14:16:07 @agent_ppo2.py:185][0m |          -0.0026 |         193.8658 |         -18.6231 |
[32m[20221214 14:16:07 @agent_ppo2.py:185][0m |          -0.0015 |         193.2681 |         -18.6238 |
[32m[20221214 14:16:07 @agent_ppo2.py:185][0m |           0.0014 |         195.4322 |         -18.6316 |
[32m[20221214 14:16:07 @agent_ppo2.py:185][0m |          -0.0008 |         192.7133 |         -18.6733 |
[32m[20221214 14:16:07 @agent_ppo2.py:185][0m |          -0.0022 |         192.4094 |         -18.5859 |
[32m[20221214 14:16:07 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.23
[32m[20221214 14:16:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.38
[32m[20221214 14:16:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.72
[32m[20221214 14:16:07 @agent_ppo2.py:143][0m Total time:      18.09 min
[32m[20221214 14:16:07 @agent_ppo2.py:145][0m 1658880 total steps have happened
[32m[20221214 14:16:07 @agent_ppo2.py:121][0m #------------------------ Iteration 810 --------------------------#
[32m[20221214 14:16:08 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:16:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:08 @agent_ppo2.py:185][0m |          -0.0023 |         187.2676 |         -18.2958 |
[32m[20221214 14:16:08 @agent_ppo2.py:185][0m |          -0.0016 |         187.3336 |         -18.4098 |
[32m[20221214 14:16:08 @agent_ppo2.py:185][0m |           0.0013 |         187.1255 |         -18.3104 |
[32m[20221214 14:16:08 @agent_ppo2.py:185][0m |           0.0055 |         193.6048 |         -18.4008 |
[32m[20221214 14:16:08 @agent_ppo2.py:185][0m |           0.0013 |         185.7295 |         -18.3946 |
[32m[20221214 14:16:08 @agent_ppo2.py:185][0m |          -0.0014 |         183.9619 |         -18.4326 |
[32m[20221214 14:16:08 @agent_ppo2.py:185][0m |          -0.0024 |         182.4926 |         -18.2888 |
[32m[20221214 14:16:08 @agent_ppo2.py:185][0m |          -0.0012 |         181.8269 |         -18.3484 |
[32m[20221214 14:16:08 @agent_ppo2.py:185][0m |          -0.0006 |         181.4898 |         -18.3784 |
[32m[20221214 14:16:09 @agent_ppo2.py:185][0m |          -0.0031 |         182.3387 |         -18.3550 |
[32m[20221214 14:16:09 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.04
[32m[20221214 14:16:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.31
[32m[20221214 14:16:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.65
[32m[20221214 14:16:09 @agent_ppo2.py:143][0m Total time:      18.11 min
[32m[20221214 14:16:09 @agent_ppo2.py:145][0m 1660928 total steps have happened
[32m[20221214 14:16:09 @agent_ppo2.py:121][0m #------------------------ Iteration 811 --------------------------#
[32m[20221214 14:16:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:09 @agent_ppo2.py:185][0m |          -0.0017 |         195.0855 |         -19.1358 |
[32m[20221214 14:16:09 @agent_ppo2.py:185][0m |          -0.0014 |         189.9929 |         -19.1393 |
[32m[20221214 14:16:09 @agent_ppo2.py:185][0m |          -0.0043 |         189.3466 |         -19.1049 |
[32m[20221214 14:16:09 @agent_ppo2.py:185][0m |           0.0112 |         210.6021 |         -18.8826 |
[32m[20221214 14:16:09 @agent_ppo2.py:185][0m |          -0.0029 |         187.9353 |         -19.1283 |
[32m[20221214 14:16:10 @agent_ppo2.py:185][0m |          -0.0027 |         187.2139 |         -19.1277 |
[32m[20221214 14:16:10 @agent_ppo2.py:185][0m |           0.0132 |         210.2532 |         -19.0118 |
[32m[20221214 14:16:10 @agent_ppo2.py:185][0m |          -0.0016 |         187.3872 |         -19.1729 |
[32m[20221214 14:16:10 @agent_ppo2.py:185][0m |          -0.0031 |         186.5114 |         -19.1024 |
[32m[20221214 14:16:10 @agent_ppo2.py:185][0m |          -0.0018 |         186.0010 |         -19.0904 |
[32m[20221214 14:16:10 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.72
[32m[20221214 14:16:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.31
[32m[20221214 14:16:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.91
[32m[20221214 14:16:10 @agent_ppo2.py:143][0m Total time:      18.13 min
[32m[20221214 14:16:10 @agent_ppo2.py:145][0m 1662976 total steps have happened
[32m[20221214 14:16:10 @agent_ppo2.py:121][0m #------------------------ Iteration 812 --------------------------#
[32m[20221214 14:16:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:10 @agent_ppo2.py:185][0m |          -0.0006 |         201.1807 |         -18.5510 |
[32m[20221214 14:16:11 @agent_ppo2.py:185][0m |          -0.0012 |         197.3148 |         -18.5343 |
[32m[20221214 14:16:11 @agent_ppo2.py:185][0m |          -0.0008 |         195.4454 |         -18.5313 |
[32m[20221214 14:16:11 @agent_ppo2.py:185][0m |          -0.0019 |         194.5130 |         -18.5622 |
[32m[20221214 14:16:11 @agent_ppo2.py:185][0m |          -0.0006 |         193.3678 |         -18.5955 |
[32m[20221214 14:16:11 @agent_ppo2.py:185][0m |          -0.0019 |         192.8987 |         -18.5461 |
[32m[20221214 14:16:11 @agent_ppo2.py:185][0m |           0.0083 |         199.5328 |         -18.5328 |
[32m[20221214 14:16:11 @agent_ppo2.py:185][0m |           0.0104 |         211.5364 |         -18.4163 |
[32m[20221214 14:16:11 @agent_ppo2.py:185][0m |           0.0036 |         194.9226 |         -18.5425 |
[32m[20221214 14:16:11 @agent_ppo2.py:185][0m |          -0.0005 |         191.1673 |         -18.5108 |
[32m[20221214 14:16:11 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:16:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.93
[32m[20221214 14:16:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.11
[32m[20221214 14:16:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.29
[32m[20221214 14:16:11 @agent_ppo2.py:143][0m Total time:      18.16 min
[32m[20221214 14:16:11 @agent_ppo2.py:145][0m 1665024 total steps have happened
[32m[20221214 14:16:11 @agent_ppo2.py:121][0m #------------------------ Iteration 813 --------------------------#
[32m[20221214 14:16:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:12 @agent_ppo2.py:185][0m |          -0.0021 |         196.5308 |         -18.8314 |
[32m[20221214 14:16:12 @agent_ppo2.py:185][0m |           0.0008 |         194.1898 |         -18.8274 |
[32m[20221214 14:16:12 @agent_ppo2.py:185][0m |           0.0052 |         199.4112 |         -18.7138 |
[32m[20221214 14:16:12 @agent_ppo2.py:185][0m |           0.0037 |         194.9725 |         -18.6483 |
[32m[20221214 14:16:12 @agent_ppo2.py:185][0m |          -0.0029 |         187.7006 |         -18.7034 |
[32m[20221214 14:16:12 @agent_ppo2.py:185][0m |          -0.0044 |         187.2213 |         -18.7047 |
[32m[20221214 14:16:12 @agent_ppo2.py:185][0m |          -0.0018 |         186.4140 |         -18.6247 |
[32m[20221214 14:16:12 @agent_ppo2.py:185][0m |          -0.0016 |         186.2267 |         -18.7225 |
[32m[20221214 14:16:13 @agent_ppo2.py:185][0m |          -0.0010 |         185.2085 |         -18.6826 |
[32m[20221214 14:16:13 @agent_ppo2.py:185][0m |          -0.0038 |         184.6088 |         -18.7496 |
[32m[20221214 14:16:13 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.65
[32m[20221214 14:16:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.81
[32m[20221214 14:16:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 791.15
[32m[20221214 14:16:13 @agent_ppo2.py:143][0m Total time:      18.18 min
[32m[20221214 14:16:13 @agent_ppo2.py:145][0m 1667072 total steps have happened
[32m[20221214 14:16:13 @agent_ppo2.py:121][0m #------------------------ Iteration 814 --------------------------#
[32m[20221214 14:16:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:13 @agent_ppo2.py:185][0m |           0.0003 |         141.9082 |         -18.9275 |
[32m[20221214 14:16:13 @agent_ppo2.py:185][0m |          -0.0046 |         116.9553 |         -18.8491 |
[32m[20221214 14:16:13 @agent_ppo2.py:185][0m |           0.0064 |         112.7923 |         -18.8526 |
[32m[20221214 14:16:13 @agent_ppo2.py:185][0m |          -0.0062 |         105.7138 |         -18.8175 |
[32m[20221214 14:16:14 @agent_ppo2.py:185][0m |          -0.0037 |         103.1795 |         -18.8846 |
[32m[20221214 14:16:14 @agent_ppo2.py:185][0m |           0.0002 |         102.2283 |         -18.7864 |
[32m[20221214 14:16:14 @agent_ppo2.py:185][0m |           0.0024 |          99.7626 |         -18.8752 |
[32m[20221214 14:16:14 @agent_ppo2.py:185][0m |          -0.0063 |          99.0540 |         -18.8963 |
[32m[20221214 14:16:14 @agent_ppo2.py:185][0m |          -0.0032 |          97.5756 |         -18.9150 |
[32m[20221214 14:16:14 @agent_ppo2.py:185][0m |           0.0010 |          97.6401 |         -18.8851 |
[32m[20221214 14:16:14 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 728.24
[32m[20221214 14:16:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.02
[32m[20221214 14:16:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 773.12
[32m[20221214 14:16:14 @agent_ppo2.py:143][0m Total time:      18.20 min
[32m[20221214 14:16:14 @agent_ppo2.py:145][0m 1669120 total steps have happened
[32m[20221214 14:16:14 @agent_ppo2.py:121][0m #------------------------ Iteration 815 --------------------------#
[32m[20221214 14:16:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:15 @agent_ppo2.py:185][0m |          -0.0001 |         212.1597 |         -18.7941 |
[32m[20221214 14:16:15 @agent_ppo2.py:185][0m |          -0.0039 |         171.3020 |         -18.7503 |
[32m[20221214 14:16:15 @agent_ppo2.py:185][0m |          -0.0108 |         159.8939 |         -18.6716 |
[32m[20221214 14:16:15 @agent_ppo2.py:185][0m |           0.0027 |         153.4180 |         -18.6874 |
[32m[20221214 14:16:15 @agent_ppo2.py:185][0m |          -0.0002 |         152.3807 |         -18.6723 |
[32m[20221214 14:16:15 @agent_ppo2.py:185][0m |           0.0074 |         155.1917 |         -18.6262 |
[32m[20221214 14:16:15 @agent_ppo2.py:185][0m |           0.0025 |         141.1668 |         -18.5485 |
[32m[20221214 14:16:15 @agent_ppo2.py:185][0m |           0.0020 |         138.2583 |         -18.5270 |
[32m[20221214 14:16:15 @agent_ppo2.py:185][0m |          -0.0073 |         134.9265 |         -18.5484 |
[32m[20221214 14:16:15 @agent_ppo2.py:185][0m |          -0.0021 |         132.9542 |         -18.5586 |
[32m[20221214 14:16:15 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:16:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 675.43
[32m[20221214 14:16:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.72
[32m[20221214 14:16:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 787.61
[32m[20221214 14:16:16 @agent_ppo2.py:143][0m Total time:      18.23 min
[32m[20221214 14:16:16 @agent_ppo2.py:145][0m 1671168 total steps have happened
[32m[20221214 14:16:16 @agent_ppo2.py:121][0m #------------------------ Iteration 816 --------------------------#
[32m[20221214 14:16:16 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:16:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:16 @agent_ppo2.py:185][0m |          -0.0016 |         160.5038 |         -19.4841 |
[32m[20221214 14:16:16 @agent_ppo2.py:185][0m |          -0.0040 |         150.1109 |         -19.4035 |
[32m[20221214 14:16:16 @agent_ppo2.py:185][0m |          -0.0033 |         147.6126 |         -19.3147 |
[32m[20221214 14:16:16 @agent_ppo2.py:185][0m |          -0.0033 |         144.6605 |         -19.3855 |
[32m[20221214 14:16:16 @agent_ppo2.py:185][0m |          -0.0051 |         142.9947 |         -19.3124 |
[32m[20221214 14:16:16 @agent_ppo2.py:185][0m |          -0.0027 |         141.3954 |         -19.4449 |
[32m[20221214 14:16:17 @agent_ppo2.py:185][0m |          -0.0035 |         139.9451 |         -19.3716 |
[32m[20221214 14:16:17 @agent_ppo2.py:185][0m |          -0.0036 |         139.0791 |         -19.3787 |
[32m[20221214 14:16:17 @agent_ppo2.py:185][0m |           0.0035 |         140.7592 |         -19.3304 |
[32m[20221214 14:16:17 @agent_ppo2.py:185][0m |          -0.0046 |         136.4771 |         -19.3841 |
[32m[20221214 14:16:17 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:16:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.77
[32m[20221214 14:16:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.38
[32m[20221214 14:16:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.76
[32m[20221214 14:16:17 @agent_ppo2.py:143][0m Total time:      18.25 min
[32m[20221214 14:16:17 @agent_ppo2.py:145][0m 1673216 total steps have happened
[32m[20221214 14:16:17 @agent_ppo2.py:121][0m #------------------------ Iteration 817 --------------------------#
[32m[20221214 14:16:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:17 @agent_ppo2.py:185][0m |          -0.0027 |         187.0019 |         -18.5758 |
[32m[20221214 14:16:17 @agent_ppo2.py:185][0m |           0.0107 |         199.1325 |         -18.5162 |
[32m[20221214 14:16:18 @agent_ppo2.py:185][0m |          -0.0046 |         182.8451 |         -18.5150 |
[32m[20221214 14:16:18 @agent_ppo2.py:185][0m |          -0.0036 |         181.7666 |         -18.5025 |
[32m[20221214 14:16:18 @agent_ppo2.py:185][0m |          -0.0047 |         181.1431 |         -18.4910 |
[32m[20221214 14:16:18 @agent_ppo2.py:185][0m |          -0.0052 |         180.5663 |         -18.5851 |
[32m[20221214 14:16:18 @agent_ppo2.py:185][0m |          -0.0017 |         182.0481 |         -18.5885 |
[32m[20221214 14:16:18 @agent_ppo2.py:185][0m |          -0.0012 |         182.2299 |         -18.6563 |
[32m[20221214 14:16:18 @agent_ppo2.py:185][0m |          -0.0045 |         179.9603 |         -18.7075 |
[32m[20221214 14:16:18 @agent_ppo2.py:185][0m |          -0.0074 |         179.0126 |         -18.7646 |
[32m[20221214 14:16:18 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:16:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.54
[32m[20221214 14:16:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.87
[32m[20221214 14:16:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.51
[32m[20221214 14:16:18 @agent_ppo2.py:143][0m Total time:      18.27 min
[32m[20221214 14:16:18 @agent_ppo2.py:145][0m 1675264 total steps have happened
[32m[20221214 14:16:18 @agent_ppo2.py:121][0m #------------------------ Iteration 818 --------------------------#
[32m[20221214 14:16:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:19 @agent_ppo2.py:185][0m |          -0.0016 |         198.0837 |         -19.1785 |
[32m[20221214 14:16:19 @agent_ppo2.py:185][0m |          -0.0017 |         191.4493 |         -19.1759 |
[32m[20221214 14:16:19 @agent_ppo2.py:185][0m |          -0.0042 |         189.1247 |         -19.0459 |
[32m[20221214 14:16:19 @agent_ppo2.py:185][0m |           0.0002 |         187.6733 |         -19.0870 |
[32m[20221214 14:16:19 @agent_ppo2.py:185][0m |          -0.0029 |         186.8016 |         -19.0280 |
[32m[20221214 14:16:19 @agent_ppo2.py:185][0m |          -0.0029 |         185.2418 |         -18.9703 |
[32m[20221214 14:16:19 @agent_ppo2.py:185][0m |          -0.0026 |         184.0763 |         -18.9597 |
[32m[20221214 14:16:19 @agent_ppo2.py:185][0m |           0.0021 |         186.6802 |         -18.9143 |
[32m[20221214 14:16:19 @agent_ppo2.py:185][0m |           0.0097 |         195.4907 |         -19.0469 |
[32m[20221214 14:16:20 @agent_ppo2.py:185][0m |          -0.0038 |         181.6096 |         -18.8927 |
[32m[20221214 14:16:20 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:16:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.94
[32m[20221214 14:16:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.57
[32m[20221214 14:16:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.49
[32m[20221214 14:16:20 @agent_ppo2.py:143][0m Total time:      18.29 min
[32m[20221214 14:16:20 @agent_ppo2.py:145][0m 1677312 total steps have happened
[32m[20221214 14:16:20 @agent_ppo2.py:121][0m #------------------------ Iteration 819 --------------------------#
[32m[20221214 14:16:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:20 @agent_ppo2.py:185][0m |          -0.0016 |         185.8299 |         -19.2652 |
[32m[20221214 14:16:20 @agent_ppo2.py:185][0m |          -0.0037 |         185.0151 |         -19.2493 |
[32m[20221214 14:16:20 @agent_ppo2.py:185][0m |          -0.0032 |         184.1150 |         -19.2365 |
[32m[20221214 14:16:20 @agent_ppo2.py:185][0m |          -0.0023 |         184.2995 |         -19.3833 |
[32m[20221214 14:16:20 @agent_ppo2.py:185][0m |          -0.0039 |         184.7876 |         -19.4351 |
[32m[20221214 14:16:21 @agent_ppo2.py:185][0m |          -0.0014 |         183.7159 |         -19.4780 |
[32m[20221214 14:16:21 @agent_ppo2.py:185][0m |           0.0054 |         190.6279 |         -19.5339 |
[32m[20221214 14:16:21 @agent_ppo2.py:185][0m |          -0.0048 |         182.8291 |         -19.5548 |
[32m[20221214 14:16:21 @agent_ppo2.py:185][0m |          -0.0040 |         183.1395 |         -19.6351 |
[32m[20221214 14:16:21 @agent_ppo2.py:185][0m |          -0.0011 |         182.5890 |         -19.7603 |
[32m[20221214 14:16:21 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:16:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.95
[32m[20221214 14:16:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.41
[32m[20221214 14:16:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.22
[32m[20221214 14:16:21 @agent_ppo2.py:143][0m Total time:      18.32 min
[32m[20221214 14:16:21 @agent_ppo2.py:145][0m 1679360 total steps have happened
[32m[20221214 14:16:21 @agent_ppo2.py:121][0m #------------------------ Iteration 820 --------------------------#
[32m[20221214 14:16:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:16:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:21 @agent_ppo2.py:185][0m |          -0.0022 |         182.7035 |         -19.6872 |
[32m[20221214 14:16:22 @agent_ppo2.py:185][0m |           0.0058 |         185.4414 |         -19.6590 |
[32m[20221214 14:16:22 @agent_ppo2.py:185][0m |          -0.0005 |         179.7838 |         -19.6401 |
[32m[20221214 14:16:22 @agent_ppo2.py:185][0m |          -0.0001 |         180.2498 |         -19.7398 |
[32m[20221214 14:16:22 @agent_ppo2.py:185][0m |          -0.0008 |         178.5495 |         -19.7467 |
[32m[20221214 14:16:22 @agent_ppo2.py:185][0m |          -0.0028 |         177.8339 |         -19.7743 |
[32m[20221214 14:16:22 @agent_ppo2.py:185][0m |          -0.0021 |         177.4604 |         -19.7377 |
[32m[20221214 14:16:22 @agent_ppo2.py:185][0m |          -0.0020 |         176.8530 |         -19.8448 |
[32m[20221214 14:16:22 @agent_ppo2.py:185][0m |          -0.0017 |         176.3625 |         -19.9386 |
[32m[20221214 14:16:22 @agent_ppo2.py:185][0m |           0.0070 |         186.5457 |         -19.7933 |
[32m[20221214 14:16:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:16:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.57
[32m[20221214 14:16:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.78
[32m[20221214 14:16:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.51
[32m[20221214 14:16:22 @agent_ppo2.py:143][0m Total time:      18.34 min
[32m[20221214 14:16:22 @agent_ppo2.py:145][0m 1681408 total steps have happened
[32m[20221214 14:16:22 @agent_ppo2.py:121][0m #------------------------ Iteration 821 --------------------------#
[32m[20221214 14:16:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:23 @agent_ppo2.py:185][0m |           0.0003 |         175.9727 |         -19.7200 |
[32m[20221214 14:16:23 @agent_ppo2.py:185][0m |           0.0042 |         177.7004 |         -19.7641 |
[32m[20221214 14:16:23 @agent_ppo2.py:185][0m |           0.0001 |         174.3549 |         -19.8917 |
[32m[20221214 14:16:23 @agent_ppo2.py:185][0m |           0.0074 |         182.4000 |         -19.8750 |
[32m[20221214 14:16:23 @agent_ppo2.py:185][0m |           0.0022 |         174.7188 |         -19.8115 |
[32m[20221214 14:16:23 @agent_ppo2.py:185][0m |          -0.0033 |         173.4094 |         -19.9436 |
[32m[20221214 14:16:23 @agent_ppo2.py:185][0m |          -0.0012 |         172.2570 |         -20.0065 |
[32m[20221214 14:16:23 @agent_ppo2.py:185][0m |          -0.0030 |         172.7389 |         -19.9285 |
[32m[20221214 14:16:24 @agent_ppo2.py:185][0m |          -0.0035 |         171.0296 |         -20.0103 |
[32m[20221214 14:16:24 @agent_ppo2.py:185][0m |          -0.0023 |         172.1741 |         -20.0192 |
[32m[20221214 14:16:24 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.73
[32m[20221214 14:16:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.98
[32m[20221214 14:16:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.18
[32m[20221214 14:16:24 @agent_ppo2.py:143][0m Total time:      18.36 min
[32m[20221214 14:16:24 @agent_ppo2.py:145][0m 1683456 total steps have happened
[32m[20221214 14:16:24 @agent_ppo2.py:121][0m #------------------------ Iteration 822 --------------------------#
[32m[20221214 14:16:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:24 @agent_ppo2.py:185][0m |           0.0048 |         208.6580 |         -20.7290 |
[32m[20221214 14:16:24 @agent_ppo2.py:185][0m |          -0.0025 |         198.3853 |         -20.6668 |
[32m[20221214 14:16:24 @agent_ppo2.py:185][0m |          -0.0026 |         197.7570 |         -20.7006 |
[32m[20221214 14:16:24 @agent_ppo2.py:185][0m |           0.0231 |         232.7069 |         -20.6345 |
[32m[20221214 14:16:25 @agent_ppo2.py:185][0m |          -0.0035 |         197.4922 |         -20.6402 |
[32m[20221214 14:16:25 @agent_ppo2.py:185][0m |          -0.0054 |         195.5393 |         -20.6602 |
[32m[20221214 14:16:25 @agent_ppo2.py:185][0m |           0.0128 |         225.9581 |         -20.6811 |
[32m[20221214 14:16:25 @agent_ppo2.py:185][0m |          -0.0047 |         196.1834 |         -20.7203 |
[32m[20221214 14:16:25 @agent_ppo2.py:185][0m |           0.0010 |         197.3970 |         -20.7435 |
[32m[20221214 14:16:25 @agent_ppo2.py:185][0m |          -0.0051 |         194.7456 |         -20.7064 |
[32m[20221214 14:16:25 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:16:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.97
[32m[20221214 14:16:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.91
[32m[20221214 14:16:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.23
[32m[20221214 14:16:25 @agent_ppo2.py:143][0m Total time:      18.38 min
[32m[20221214 14:16:25 @agent_ppo2.py:145][0m 1685504 total steps have happened
[32m[20221214 14:16:25 @agent_ppo2.py:121][0m #------------------------ Iteration 823 --------------------------#
[32m[20221214 14:16:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:25 @agent_ppo2.py:185][0m |          -0.0025 |         210.6874 |         -20.5303 |
[32m[20221214 14:16:26 @agent_ppo2.py:185][0m |          -0.0025 |         205.3093 |         -20.5017 |
[32m[20221214 14:16:26 @agent_ppo2.py:185][0m |           0.0156 |         232.1442 |         -20.4717 |
[32m[20221214 14:16:26 @agent_ppo2.py:185][0m |          -0.0018 |         202.1994 |         -20.4650 |
[32m[20221214 14:16:26 @agent_ppo2.py:185][0m |           0.0001 |         200.0864 |         -20.5489 |
[32m[20221214 14:16:26 @agent_ppo2.py:185][0m |           0.0127 |         230.3049 |         -20.5482 |
[32m[20221214 14:16:26 @agent_ppo2.py:185][0m |          -0.0001 |         200.0795 |         -20.5992 |
[32m[20221214 14:16:26 @agent_ppo2.py:185][0m |          -0.0010 |         199.3986 |         -20.5932 |
[32m[20221214 14:16:26 @agent_ppo2.py:185][0m |          -0.0026 |         198.3129 |         -20.6852 |
[32m[20221214 14:16:26 @agent_ppo2.py:185][0m |          -0.0009 |         197.8173 |         -20.6321 |
[32m[20221214 14:16:26 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.14
[32m[20221214 14:16:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.14
[32m[20221214 14:16:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.64
[32m[20221214 14:16:27 @agent_ppo2.py:143][0m Total time:      18.41 min
[32m[20221214 14:16:27 @agent_ppo2.py:145][0m 1687552 total steps have happened
[32m[20221214 14:16:27 @agent_ppo2.py:121][0m #------------------------ Iteration 824 --------------------------#
[32m[20221214 14:16:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:27 @agent_ppo2.py:185][0m |           0.0080 |         201.0909 |         -20.5146 |
[32m[20221214 14:16:27 @agent_ppo2.py:185][0m |          -0.0025 |         187.5559 |         -20.5459 |
[32m[20221214 14:16:27 @agent_ppo2.py:185][0m |          -0.0025 |         185.7011 |         -20.6124 |
[32m[20221214 14:16:27 @agent_ppo2.py:185][0m |          -0.0018 |         185.9477 |         -20.4807 |
[32m[20221214 14:16:27 @agent_ppo2.py:185][0m |          -0.0026 |         184.6727 |         -20.5762 |
[32m[20221214 14:16:27 @agent_ppo2.py:185][0m |           0.0098 |         214.3994 |         -20.5116 |
[32m[20221214 14:16:27 @agent_ppo2.py:185][0m |          -0.0026 |         184.2789 |         -20.3039 |
[32m[20221214 14:16:28 @agent_ppo2.py:185][0m |          -0.0025 |         183.9482 |         -20.4866 |
[32m[20221214 14:16:28 @agent_ppo2.py:185][0m |          -0.0025 |         182.6131 |         -20.4251 |
[32m[20221214 14:16:28 @agent_ppo2.py:185][0m |          -0.0035 |         183.2854 |         -20.4373 |
[32m[20221214 14:16:28 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:16:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.03
[32m[20221214 14:16:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.70
[32m[20221214 14:16:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.34
[32m[20221214 14:16:28 @agent_ppo2.py:143][0m Total time:      18.43 min
[32m[20221214 14:16:28 @agent_ppo2.py:145][0m 1689600 total steps have happened
[32m[20221214 14:16:28 @agent_ppo2.py:121][0m #------------------------ Iteration 825 --------------------------#
[32m[20221214 14:16:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:28 @agent_ppo2.py:185][0m |          -0.0014 |         215.4056 |         -20.7426 |
[32m[20221214 14:16:28 @agent_ppo2.py:185][0m |          -0.0010 |         212.9405 |         -20.6537 |
[32m[20221214 14:16:28 @agent_ppo2.py:185][0m |           0.0063 |         217.6881 |         -20.7085 |
[32m[20221214 14:16:29 @agent_ppo2.py:185][0m |          -0.0010 |         211.5557 |         -20.6629 |
[32m[20221214 14:16:29 @agent_ppo2.py:185][0m |          -0.0022 |         210.3930 |         -20.6603 |
[32m[20221214 14:16:29 @agent_ppo2.py:185][0m |           0.0007 |         211.0264 |         -20.6987 |
[32m[20221214 14:16:29 @agent_ppo2.py:185][0m |          -0.0013 |         209.8538 |         -20.7416 |
[32m[20221214 14:16:29 @agent_ppo2.py:185][0m |          -0.0012 |         209.2788 |         -20.7454 |
[32m[20221214 14:16:29 @agent_ppo2.py:185][0m |          -0.0013 |         209.8034 |         -20.6923 |
[32m[20221214 14:16:29 @agent_ppo2.py:185][0m |          -0.0024 |         208.9169 |         -20.6565 |
[32m[20221214 14:16:29 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:16:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.03
[32m[20221214 14:16:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.42
[32m[20221214 14:16:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.50
[32m[20221214 14:16:29 @agent_ppo2.py:143][0m Total time:      18.45 min
[32m[20221214 14:16:29 @agent_ppo2.py:145][0m 1691648 total steps have happened
[32m[20221214 14:16:29 @agent_ppo2.py:121][0m #------------------------ Iteration 826 --------------------------#
[32m[20221214 14:16:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:30 @agent_ppo2.py:185][0m |          -0.0022 |         192.6696 |         -21.4943 |
[32m[20221214 14:16:30 @agent_ppo2.py:185][0m |          -0.0031 |         188.1751 |         -21.4035 |
[32m[20221214 14:16:30 @agent_ppo2.py:185][0m |          -0.0030 |         185.3444 |         -21.5321 |
[32m[20221214 14:16:30 @agent_ppo2.py:185][0m |          -0.0035 |         184.4443 |         -21.5638 |
[32m[20221214 14:16:30 @agent_ppo2.py:185][0m |          -0.0040 |         183.0932 |         -21.4920 |
[32m[20221214 14:16:30 @agent_ppo2.py:185][0m |          -0.0040 |         182.1690 |         -21.4700 |
[32m[20221214 14:16:30 @agent_ppo2.py:185][0m |           0.0012 |         184.4366 |         -21.4743 |
[32m[20221214 14:16:30 @agent_ppo2.py:185][0m |          -0.0033 |         180.7403 |         -21.6607 |
[32m[20221214 14:16:30 @agent_ppo2.py:185][0m |          -0.0038 |         181.7078 |         -21.5404 |
[32m[20221214 14:16:30 @agent_ppo2.py:185][0m |           0.0076 |         196.3177 |         -21.5968 |
[32m[20221214 14:16:30 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:16:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.37
[32m[20221214 14:16:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.75
[32m[20221214 14:16:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.02
[32m[20221214 14:16:31 @agent_ppo2.py:143][0m Total time:      18.48 min
[32m[20221214 14:16:31 @agent_ppo2.py:145][0m 1693696 total steps have happened
[32m[20221214 14:16:31 @agent_ppo2.py:121][0m #------------------------ Iteration 827 --------------------------#
[32m[20221214 14:16:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:31 @agent_ppo2.py:185][0m |          -0.0025 |         193.2231 |         -21.2013 |
[32m[20221214 14:16:31 @agent_ppo2.py:185][0m |          -0.0001 |         188.9894 |         -21.0596 |
[32m[20221214 14:16:31 @agent_ppo2.py:185][0m |           0.0001 |         187.4522 |         -21.1957 |
[32m[20221214 14:16:31 @agent_ppo2.py:185][0m |           0.0042 |         190.7891 |         -21.1787 |
[32m[20221214 14:16:31 @agent_ppo2.py:185][0m |          -0.0026 |         186.0881 |         -21.1825 |
[32m[20221214 14:16:31 @agent_ppo2.py:185][0m |          -0.0020 |         185.0166 |         -20.9878 |
[32m[20221214 14:16:32 @agent_ppo2.py:185][0m |           0.0127 |         195.0898 |         -21.1283 |
[32m[20221214 14:16:32 @agent_ppo2.py:185][0m |          -0.0002 |         184.7724 |         -21.1501 |
[32m[20221214 14:16:32 @agent_ppo2.py:185][0m |          -0.0035 |         184.7328 |         -20.9226 |
[32m[20221214 14:16:32 @agent_ppo2.py:185][0m |          -0.0003 |         183.1980 |         -20.9699 |
[32m[20221214 14:16:32 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.62
[32m[20221214 14:16:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.76
[32m[20221214 14:16:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.08
[32m[20221214 14:16:32 @agent_ppo2.py:143][0m Total time:      18.50 min
[32m[20221214 14:16:32 @agent_ppo2.py:145][0m 1695744 total steps have happened
[32m[20221214 14:16:32 @agent_ppo2.py:121][0m #------------------------ Iteration 828 --------------------------#
[32m[20221214 14:16:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:32 @agent_ppo2.py:185][0m |          -0.0011 |         185.7808 |         -20.0485 |
[32m[20221214 14:16:32 @agent_ppo2.py:185][0m |          -0.0009 |         183.8064 |         -20.1184 |
[32m[20221214 14:16:33 @agent_ppo2.py:185][0m |          -0.0018 |         183.1979 |         -20.2033 |
[32m[20221214 14:16:33 @agent_ppo2.py:185][0m |           0.0039 |         187.7492 |         -20.1369 |
[32m[20221214 14:16:33 @agent_ppo2.py:185][0m |          -0.0012 |         183.7858 |         -20.2267 |
[32m[20221214 14:16:33 @agent_ppo2.py:185][0m |          -0.0029 |         182.9357 |         -20.2198 |
[32m[20221214 14:16:33 @agent_ppo2.py:185][0m |          -0.0004 |         182.5193 |         -20.2346 |
[32m[20221214 14:16:33 @agent_ppo2.py:185][0m |           0.0023 |         184.8029 |         -20.3487 |
[32m[20221214 14:16:33 @agent_ppo2.py:185][0m |          -0.0021 |         182.3718 |         -20.3682 |
[32m[20221214 14:16:33 @agent_ppo2.py:185][0m |           0.0046 |         186.2496 |         -20.4149 |
[32m[20221214 14:16:33 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:16:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.54
[32m[20221214 14:16:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.04
[32m[20221214 14:16:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.35
[32m[20221214 14:16:33 @agent_ppo2.py:143][0m Total time:      18.52 min
[32m[20221214 14:16:33 @agent_ppo2.py:145][0m 1697792 total steps have happened
[32m[20221214 14:16:33 @agent_ppo2.py:121][0m #------------------------ Iteration 829 --------------------------#
[32m[20221214 14:16:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:34 @agent_ppo2.py:185][0m |           0.0121 |         182.6445 |         -21.7233 |
[32m[20221214 14:16:34 @agent_ppo2.py:185][0m |          -0.0032 |         154.2564 |         -21.5495 |
[32m[20221214 14:16:34 @agent_ppo2.py:185][0m |          -0.0025 |         151.1209 |         -21.6354 |
[32m[20221214 14:16:34 @agent_ppo2.py:185][0m |          -0.0020 |         149.6036 |         -21.8898 |
[32m[20221214 14:16:34 @agent_ppo2.py:185][0m |           0.0075 |         157.3783 |         -21.6985 |
[32m[20221214 14:16:34 @agent_ppo2.py:185][0m |          -0.0031 |         148.3575 |         -21.7880 |
[32m[20221214 14:16:34 @agent_ppo2.py:185][0m |          -0.0010 |         148.9293 |         -21.8118 |
[32m[20221214 14:16:34 @agent_ppo2.py:185][0m |           0.0030 |         149.4872 |         -21.8360 |
[32m[20221214 14:16:34 @agent_ppo2.py:185][0m |          -0.0018 |         147.7727 |         -21.8528 |
[32m[20221214 14:16:35 @agent_ppo2.py:185][0m |           0.0020 |         148.0763 |         -21.7314 |
[32m[20221214 14:16:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:16:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.33
[32m[20221214 14:16:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.34
[32m[20221214 14:16:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.54
[32m[20221214 14:16:35 @agent_ppo2.py:143][0m Total time:      18.54 min
[32m[20221214 14:16:35 @agent_ppo2.py:145][0m 1699840 total steps have happened
[32m[20221214 14:16:35 @agent_ppo2.py:121][0m #------------------------ Iteration 830 --------------------------#
[32m[20221214 14:16:35 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:16:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:35 @agent_ppo2.py:185][0m |           0.0108 |         230.5162 |         -22.2187 |
[32m[20221214 14:16:35 @agent_ppo2.py:185][0m |          -0.0014 |         218.1295 |         -22.1951 |
[32m[20221214 14:16:35 @agent_ppo2.py:185][0m |          -0.0020 |         217.2239 |         -22.1403 |
[32m[20221214 14:16:35 @agent_ppo2.py:185][0m |          -0.0002 |         217.6916 |         -22.1735 |
[32m[20221214 14:16:36 @agent_ppo2.py:185][0m |          -0.0035 |         217.0717 |         -22.1308 |
[32m[20221214 14:16:36 @agent_ppo2.py:185][0m |          -0.0020 |         216.5238 |         -22.1894 |
[32m[20221214 14:16:36 @agent_ppo2.py:185][0m |          -0.0013 |         215.6339 |         -22.1105 |
[32m[20221214 14:16:36 @agent_ppo2.py:185][0m |          -0.0023 |         216.0174 |         -22.1058 |
[32m[20221214 14:16:36 @agent_ppo2.py:185][0m |          -0.0034 |         216.4261 |         -22.1105 |
[32m[20221214 14:16:36 @agent_ppo2.py:185][0m |          -0.0010 |         215.1189 |         -21.9779 |
[32m[20221214 14:16:36 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 14:16:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.08
[32m[20221214 14:16:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.84
[32m[20221214 14:16:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.04
[32m[20221214 14:16:36 @agent_ppo2.py:143][0m Total time:      18.57 min
[32m[20221214 14:16:36 @agent_ppo2.py:145][0m 1701888 total steps have happened
[32m[20221214 14:16:36 @agent_ppo2.py:121][0m #------------------------ Iteration 831 --------------------------#
[32m[20221214 14:16:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:16:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:37 @agent_ppo2.py:185][0m |           0.0164 |         194.2406 |         -21.6375 |
[32m[20221214 14:16:37 @agent_ppo2.py:185][0m |          -0.0047 |         168.8269 |         -21.4238 |
[32m[20221214 14:16:37 @agent_ppo2.py:185][0m |           0.0005 |         168.0810 |         -21.4617 |
[32m[20221214 14:16:37 @agent_ppo2.py:185][0m |          -0.0034 |         166.5131 |         -21.5370 |
[32m[20221214 14:16:37 @agent_ppo2.py:185][0m |          -0.0060 |         166.8062 |         -21.4889 |
[32m[20221214 14:16:37 @agent_ppo2.py:185][0m |          -0.0020 |         165.6573 |         -21.4746 |
[32m[20221214 14:16:37 @agent_ppo2.py:185][0m |          -0.0046 |         165.2683 |         -21.4535 |
[32m[20221214 14:16:37 @agent_ppo2.py:185][0m |          -0.0055 |         164.6676 |         -21.4092 |
[32m[20221214 14:16:37 @agent_ppo2.py:185][0m |          -0.0024 |         164.1597 |         -21.4117 |
[32m[20221214 14:16:37 @agent_ppo2.py:185][0m |          -0.0030 |         164.2818 |         -21.3442 |
[32m[20221214 14:16:37 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:16:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.28
[32m[20221214 14:16:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.23
[32m[20221214 14:16:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.12
[32m[20221214 14:16:38 @agent_ppo2.py:143][0m Total time:      18.59 min
[32m[20221214 14:16:38 @agent_ppo2.py:145][0m 1703936 total steps have happened
[32m[20221214 14:16:38 @agent_ppo2.py:121][0m #------------------------ Iteration 832 --------------------------#
[32m[20221214 14:16:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:38 @agent_ppo2.py:185][0m |          -0.0006 |         235.7576 |         -21.0589 |
[32m[20221214 14:16:38 @agent_ppo2.py:185][0m |          -0.0021 |         232.1377 |         -21.0225 |
[32m[20221214 14:16:38 @agent_ppo2.py:185][0m |           0.0055 |         234.9111 |         -21.0756 |
[32m[20221214 14:16:38 @agent_ppo2.py:185][0m |          -0.0022 |         228.0226 |         -21.1280 |
[32m[20221214 14:16:38 @agent_ppo2.py:185][0m |          -0.0022 |         226.6259 |         -21.1368 |
[32m[20221214 14:16:38 @agent_ppo2.py:185][0m |          -0.0025 |         226.2342 |         -21.1019 |
[32m[20221214 14:16:38 @agent_ppo2.py:185][0m |          -0.0030 |         224.8881 |         -21.1744 |
[32m[20221214 14:16:39 @agent_ppo2.py:185][0m |          -0.0019 |         224.5114 |         -21.1674 |
[32m[20221214 14:16:39 @agent_ppo2.py:185][0m |           0.0017 |         225.2922 |         -21.1991 |
[32m[20221214 14:16:39 @agent_ppo2.py:185][0m |          -0.0032 |         224.5131 |         -21.2230 |
[32m[20221214 14:16:39 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:16:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.51
[32m[20221214 14:16:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.74
[32m[20221214 14:16:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.89
[32m[20221214 14:16:39 @agent_ppo2.py:143][0m Total time:      18.61 min
[32m[20221214 14:16:39 @agent_ppo2.py:145][0m 1705984 total steps have happened
[32m[20221214 14:16:39 @agent_ppo2.py:121][0m #------------------------ Iteration 833 --------------------------#
[32m[20221214 14:16:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:39 @agent_ppo2.py:185][0m |           0.0185 |         260.1484 |         -21.4927 |
[32m[20221214 14:16:39 @agent_ppo2.py:185][0m |          -0.0033 |         229.7857 |         -21.4690 |
[32m[20221214 14:16:39 @agent_ppo2.py:185][0m |           0.0041 |         228.3934 |         -21.5970 |
[32m[20221214 14:16:40 @agent_ppo2.py:185][0m |          -0.0025 |         225.1337 |         -21.5948 |
[32m[20221214 14:16:40 @agent_ppo2.py:185][0m |           0.0009 |         226.9127 |         -21.6313 |
[32m[20221214 14:16:40 @agent_ppo2.py:185][0m |          -0.0007 |         224.5041 |         -21.7854 |
[32m[20221214 14:16:40 @agent_ppo2.py:185][0m |          -0.0018 |         224.5589 |         -21.6823 |
[32m[20221214 14:16:40 @agent_ppo2.py:185][0m |          -0.0034 |         223.1418 |         -21.7984 |
[32m[20221214 14:16:40 @agent_ppo2.py:185][0m |          -0.0021 |         222.8658 |         -21.8317 |
[32m[20221214 14:16:40 @agent_ppo2.py:185][0m |          -0.0024 |         224.0123 |         -21.9280 |
[32m[20221214 14:16:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:16:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.41
[32m[20221214 14:16:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.76
[32m[20221214 14:16:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.62
[32m[20221214 14:16:40 @agent_ppo2.py:143][0m Total time:      18.64 min
[32m[20221214 14:16:40 @agent_ppo2.py:145][0m 1708032 total steps have happened
[32m[20221214 14:16:40 @agent_ppo2.py:121][0m #------------------------ Iteration 834 --------------------------#
[32m[20221214 14:16:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:41 @agent_ppo2.py:185][0m |          -0.0024 |         211.0148 |         -21.6542 |
[32m[20221214 14:16:41 @agent_ppo2.py:185][0m |           0.0015 |         203.5415 |         -21.7721 |
[32m[20221214 14:16:41 @agent_ppo2.py:185][0m |           0.0004 |         199.2242 |         -21.7274 |
[32m[20221214 14:16:41 @agent_ppo2.py:185][0m |          -0.0032 |         195.8737 |         -21.8654 |
[32m[20221214 14:16:41 @agent_ppo2.py:185][0m |          -0.0032 |         194.0763 |         -21.8551 |
[32m[20221214 14:16:41 @agent_ppo2.py:185][0m |          -0.0010 |         194.0127 |         -21.8402 |
[32m[20221214 14:16:41 @agent_ppo2.py:185][0m |           0.0031 |         197.9252 |         -21.8431 |
[32m[20221214 14:16:41 @agent_ppo2.py:185][0m |          -0.0010 |         192.8525 |         -21.8780 |
[32m[20221214 14:16:41 @agent_ppo2.py:185][0m |           0.0101 |         209.3800 |         -21.9193 |
[32m[20221214 14:16:42 @agent_ppo2.py:185][0m |           0.0044 |         199.2647 |         -21.8529 |
[32m[20221214 14:16:42 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:16:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.79
[32m[20221214 14:16:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.97
[32m[20221214 14:16:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.01
[32m[20221214 14:16:42 @agent_ppo2.py:143][0m Total time:      18.66 min
[32m[20221214 14:16:42 @agent_ppo2.py:145][0m 1710080 total steps have happened
[32m[20221214 14:16:42 @agent_ppo2.py:121][0m #------------------------ Iteration 835 --------------------------#
[32m[20221214 14:16:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:42 @agent_ppo2.py:185][0m |           0.0003 |         184.7727 |         -21.7723 |
[32m[20221214 14:16:42 @agent_ppo2.py:185][0m |          -0.0014 |         176.0659 |         -21.8669 |
[32m[20221214 14:16:42 @agent_ppo2.py:185][0m |          -0.0001 |         172.4290 |         -21.8335 |
[32m[20221214 14:16:42 @agent_ppo2.py:185][0m |          -0.0004 |         169.6856 |         -21.9318 |
[32m[20221214 14:16:42 @agent_ppo2.py:185][0m |           0.0012 |         168.1825 |         -21.9145 |
[32m[20221214 14:16:42 @agent_ppo2.py:185][0m |          -0.0018 |         165.5779 |         -21.9997 |
[32m[20221214 14:16:43 @agent_ppo2.py:185][0m |          -0.0024 |         165.1741 |         -21.8659 |
[32m[20221214 14:16:43 @agent_ppo2.py:185][0m |          -0.0030 |         163.6457 |         -22.0480 |
[32m[20221214 14:16:43 @agent_ppo2.py:185][0m |           0.0109 |         177.0634 |         -22.0932 |
[32m[20221214 14:16:43 @agent_ppo2.py:185][0m |          -0.0002 |         163.2767 |         -22.0959 |
[32m[20221214 14:16:43 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.19
[32m[20221214 14:16:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.75
[32m[20221214 14:16:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.88
[32m[20221214 14:16:43 @agent_ppo2.py:143][0m Total time:      18.68 min
[32m[20221214 14:16:43 @agent_ppo2.py:145][0m 1712128 total steps have happened
[32m[20221214 14:16:43 @agent_ppo2.py:121][0m #------------------------ Iteration 836 --------------------------#
[32m[20221214 14:16:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:43 @agent_ppo2.py:185][0m |           0.0001 |         153.7348 |         -22.6397 |
[32m[20221214 14:16:43 @agent_ppo2.py:185][0m |          -0.0052 |         150.3106 |         -22.6253 |
[32m[20221214 14:16:44 @agent_ppo2.py:185][0m |           0.0027 |         149.5604 |         -22.6766 |
[32m[20221214 14:16:44 @agent_ppo2.py:185][0m |          -0.0009 |         148.6246 |         -22.7222 |
[32m[20221214 14:16:44 @agent_ppo2.py:185][0m |          -0.0016 |         148.1830 |         -22.7457 |
[32m[20221214 14:16:44 @agent_ppo2.py:185][0m |          -0.0023 |         147.5972 |         -22.8292 |
[32m[20221214 14:16:44 @agent_ppo2.py:185][0m |          -0.0009 |         147.2499 |         -22.8488 |
[32m[20221214 14:16:44 @agent_ppo2.py:185][0m |          -0.0008 |         146.8887 |         -22.8192 |
[32m[20221214 14:16:44 @agent_ppo2.py:185][0m |          -0.0004 |         147.0472 |         -22.9258 |
[32m[20221214 14:16:44 @agent_ppo2.py:185][0m |           0.0097 |         157.1073 |         -22.9605 |
[32m[20221214 14:16:44 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.27
[32m[20221214 14:16:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.49
[32m[20221214 14:16:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.21
[32m[20221214 14:16:44 @agent_ppo2.py:143][0m Total time:      18.71 min
[32m[20221214 14:16:44 @agent_ppo2.py:145][0m 1714176 total steps have happened
[32m[20221214 14:16:44 @agent_ppo2.py:121][0m #------------------------ Iteration 837 --------------------------#
[32m[20221214 14:16:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:45 @agent_ppo2.py:185][0m |           0.0033 |         163.3587 |         -23.3069 |
[32m[20221214 14:16:45 @agent_ppo2.py:185][0m |          -0.0020 |         158.9071 |         -23.3885 |
[32m[20221214 14:16:45 @agent_ppo2.py:185][0m |          -0.0004 |         157.7897 |         -23.3277 |
[32m[20221214 14:16:45 @agent_ppo2.py:185][0m |          -0.0002 |         156.8981 |         -23.4720 |
[32m[20221214 14:16:45 @agent_ppo2.py:185][0m |           0.0114 |         175.0403 |         -23.4088 |
[32m[20221214 14:16:45 @agent_ppo2.py:185][0m |          -0.0010 |         156.9988 |         -23.2550 |
[32m[20221214 14:16:45 @agent_ppo2.py:185][0m |          -0.0014 |         155.9328 |         -23.4111 |
[32m[20221214 14:16:45 @agent_ppo2.py:185][0m |          -0.0013 |         156.3722 |         -23.3471 |
[32m[20221214 14:16:46 @agent_ppo2.py:185][0m |          -0.0002 |         155.2694 |         -23.4104 |
[32m[20221214 14:16:46 @agent_ppo2.py:185][0m |           0.0082 |         167.0601 |         -23.4640 |
[32m[20221214 14:16:46 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.76
[32m[20221214 14:16:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.95
[32m[20221214 14:16:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.78
[32m[20221214 14:16:46 @agent_ppo2.py:143][0m Total time:      18.73 min
[32m[20221214 14:16:46 @agent_ppo2.py:145][0m 1716224 total steps have happened
[32m[20221214 14:16:46 @agent_ppo2.py:121][0m #------------------------ Iteration 838 --------------------------#
[32m[20221214 14:16:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:46 @agent_ppo2.py:185][0m |          -0.0031 |         172.1398 |         -22.7523 |
[32m[20221214 14:16:46 @agent_ppo2.py:185][0m |          -0.0031 |         164.4089 |         -22.7638 |
[32m[20221214 14:16:46 @agent_ppo2.py:185][0m |          -0.0028 |         160.0761 |         -22.8972 |
[32m[20221214 14:16:46 @agent_ppo2.py:185][0m |           0.0069 |         168.6211 |         -22.8830 |
[32m[20221214 14:16:47 @agent_ppo2.py:185][0m |          -0.0022 |         156.3147 |         -22.9475 |
[32m[20221214 14:16:47 @agent_ppo2.py:185][0m |          -0.0042 |         156.0012 |         -23.0112 |
[32m[20221214 14:16:47 @agent_ppo2.py:185][0m |           0.0073 |         166.1093 |         -23.0416 |
[32m[20221214 14:16:47 @agent_ppo2.py:185][0m |          -0.0020 |         155.3988 |         -23.0287 |
[32m[20221214 14:16:47 @agent_ppo2.py:185][0m |          -0.0028 |         154.5216 |         -23.1261 |
[32m[20221214 14:16:47 @agent_ppo2.py:185][0m |          -0.0012 |         153.6240 |         -23.1918 |
[32m[20221214 14:16:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.99
[32m[20221214 14:16:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.10
[32m[20221214 14:16:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.55
[32m[20221214 14:16:47 @agent_ppo2.py:143][0m Total time:      18.75 min
[32m[20221214 14:16:47 @agent_ppo2.py:145][0m 1718272 total steps have happened
[32m[20221214 14:16:47 @agent_ppo2.py:121][0m #------------------------ Iteration 839 --------------------------#
[32m[20221214 14:16:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:48 @agent_ppo2.py:185][0m |           0.0038 |         208.0012 |         -23.5429 |
[32m[20221214 14:16:48 @agent_ppo2.py:185][0m |           0.0107 |         212.4765 |         -23.5790 |
[32m[20221214 14:16:48 @agent_ppo2.py:185][0m |          -0.0017 |         199.5833 |         -23.5969 |
[32m[20221214 14:16:48 @agent_ppo2.py:185][0m |          -0.0023 |         197.3570 |         -23.6511 |
[32m[20221214 14:16:48 @agent_ppo2.py:185][0m |          -0.0000 |         195.5888 |         -23.6288 |
[32m[20221214 14:16:48 @agent_ppo2.py:185][0m |          -0.0021 |         195.1595 |         -23.6340 |
[32m[20221214 14:16:48 @agent_ppo2.py:185][0m |           0.0192 |         220.4294 |         -23.6227 |
[32m[20221214 14:16:48 @agent_ppo2.py:185][0m |          -0.0020 |         195.2185 |         -23.4711 |
[32m[20221214 14:16:48 @agent_ppo2.py:185][0m |          -0.0018 |         192.3708 |         -23.5888 |
[32m[20221214 14:16:48 @agent_ppo2.py:185][0m |          -0.0020 |         192.8617 |         -23.5834 |
[32m[20221214 14:16:48 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:16:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.24
[32m[20221214 14:16:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.56
[32m[20221214 14:16:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.70
[32m[20221214 14:16:49 @agent_ppo2.py:143][0m Total time:      18.77 min
[32m[20221214 14:16:49 @agent_ppo2.py:145][0m 1720320 total steps have happened
[32m[20221214 14:16:49 @agent_ppo2.py:121][0m #------------------------ Iteration 840 --------------------------#
[32m[20221214 14:16:49 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:16:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:49 @agent_ppo2.py:185][0m |          -0.0012 |         184.5082 |         -23.1634 |
[32m[20221214 14:16:49 @agent_ppo2.py:185][0m |           0.0006 |         181.1640 |         -23.2759 |
[32m[20221214 14:16:49 @agent_ppo2.py:185][0m |          -0.0018 |         180.6137 |         -23.2700 |
[32m[20221214 14:16:49 @agent_ppo2.py:185][0m |          -0.0024 |         179.1267 |         -23.2345 |
[32m[20221214 14:16:49 @agent_ppo2.py:185][0m |          -0.0017 |         179.4802 |         -23.2468 |
[32m[20221214 14:16:49 @agent_ppo2.py:185][0m |           0.0105 |         183.1985 |         -23.2719 |
[32m[20221214 14:16:50 @agent_ppo2.py:185][0m |          -0.0016 |         178.7469 |         -23.2932 |
[32m[20221214 14:16:50 @agent_ppo2.py:185][0m |          -0.0016 |         177.3288 |         -23.2670 |
[32m[20221214 14:16:50 @agent_ppo2.py:185][0m |          -0.0020 |         177.1334 |         -23.1475 |
[32m[20221214 14:16:50 @agent_ppo2.py:185][0m |           0.0111 |         192.5429 |         -23.1945 |
[32m[20221214 14:16:50 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:16:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.11
[32m[20221214 14:16:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.42
[32m[20221214 14:16:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.56
[32m[20221214 14:16:50 @agent_ppo2.py:143][0m Total time:      18.80 min
[32m[20221214 14:16:50 @agent_ppo2.py:145][0m 1722368 total steps have happened
[32m[20221214 14:16:50 @agent_ppo2.py:121][0m #------------------------ Iteration 841 --------------------------#
[32m[20221214 14:16:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:50 @agent_ppo2.py:185][0m |           0.0033 |         208.0458 |         -23.9201 |
[32m[20221214 14:16:50 @agent_ppo2.py:185][0m |           0.0018 |         205.4837 |         -23.8123 |
[32m[20221214 14:16:51 @agent_ppo2.py:185][0m |          -0.0025 |         204.1211 |         -23.7650 |
[32m[20221214 14:16:51 @agent_ppo2.py:185][0m |          -0.0020 |         203.5422 |         -23.6728 |
[32m[20221214 14:16:51 @agent_ppo2.py:185][0m |          -0.0018 |         203.5417 |         -23.8180 |
[32m[20221214 14:16:51 @agent_ppo2.py:185][0m |          -0.0030 |         203.3011 |         -23.6668 |
[32m[20221214 14:16:51 @agent_ppo2.py:185][0m |          -0.0015 |         202.9955 |         -23.7869 |
[32m[20221214 14:16:51 @agent_ppo2.py:185][0m |          -0.0015 |         202.5454 |         -23.7712 |
[32m[20221214 14:16:51 @agent_ppo2.py:185][0m |          -0.0023 |         202.5536 |         -23.7420 |
[32m[20221214 14:16:51 @agent_ppo2.py:185][0m |          -0.0023 |         201.8644 |         -23.8060 |
[32m[20221214 14:16:51 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:16:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.32
[32m[20221214 14:16:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.69
[32m[20221214 14:16:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.92
[32m[20221214 14:16:51 @agent_ppo2.py:143][0m Total time:      18.82 min
[32m[20221214 14:16:51 @agent_ppo2.py:145][0m 1724416 total steps have happened
[32m[20221214 14:16:51 @agent_ppo2.py:121][0m #------------------------ Iteration 842 --------------------------#
[32m[20221214 14:16:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:52 @agent_ppo2.py:185][0m |          -0.0022 |         234.3863 |         -24.2384 |
[32m[20221214 14:16:52 @agent_ppo2.py:185][0m |          -0.0013 |         226.8297 |         -24.2856 |
[32m[20221214 14:16:52 @agent_ppo2.py:185][0m |          -0.0015 |         220.5800 |         -24.3628 |
[32m[20221214 14:16:52 @agent_ppo2.py:185][0m |          -0.0022 |         216.4446 |         -24.4761 |
[32m[20221214 14:16:52 @agent_ppo2.py:185][0m |          -0.0012 |         213.1646 |         -24.4530 |
[32m[20221214 14:16:52 @agent_ppo2.py:185][0m |          -0.0021 |         213.3613 |         -24.5754 |
[32m[20221214 14:16:52 @agent_ppo2.py:185][0m |          -0.0015 |         212.9699 |         -24.6687 |
[32m[20221214 14:16:52 @agent_ppo2.py:185][0m |          -0.0022 |         212.1373 |         -24.5276 |
[32m[20221214 14:16:52 @agent_ppo2.py:185][0m |           0.0055 |         212.9521 |         -24.7725 |
[32m[20221214 14:16:53 @agent_ppo2.py:185][0m |          -0.0019 |         212.1343 |         -24.8067 |
[32m[20221214 14:16:53 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:16:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.97
[32m[20221214 14:16:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.23
[32m[20221214 14:16:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.36
[32m[20221214 14:16:53 @agent_ppo2.py:143][0m Total time:      18.84 min
[32m[20221214 14:16:53 @agent_ppo2.py:145][0m 1726464 total steps have happened
[32m[20221214 14:16:53 @agent_ppo2.py:121][0m #------------------------ Iteration 843 --------------------------#
[32m[20221214 14:16:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:53 @agent_ppo2.py:185][0m |          -0.0013 |         238.5726 |         -25.1432 |
[32m[20221214 14:16:53 @agent_ppo2.py:185][0m |          -0.0015 |         233.4941 |         -25.1599 |
[32m[20221214 14:16:53 @agent_ppo2.py:185][0m |          -0.0010 |         233.7464 |         -25.1523 |
[32m[20221214 14:16:53 @agent_ppo2.py:185][0m |          -0.0021 |         232.1281 |         -25.0704 |
[32m[20221214 14:16:53 @agent_ppo2.py:185][0m |          -0.0021 |         233.0605 |         -25.0504 |
[32m[20221214 14:16:54 @agent_ppo2.py:185][0m |           0.0005 |         231.8118 |         -25.1090 |
[32m[20221214 14:16:54 @agent_ppo2.py:185][0m |          -0.0012 |         232.4192 |         -24.9708 |
[32m[20221214 14:16:54 @agent_ppo2.py:185][0m |          -0.0013 |         231.8225 |         -25.0421 |
[32m[20221214 14:16:54 @agent_ppo2.py:185][0m |          -0.0020 |         231.4770 |         -25.0411 |
[32m[20221214 14:16:54 @agent_ppo2.py:185][0m |          -0.0008 |         231.2260 |         -24.9094 |
[32m[20221214 14:16:54 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:16:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.24
[32m[20221214 14:16:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.54
[32m[20221214 14:16:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.68
[32m[20221214 14:16:54 @agent_ppo2.py:143][0m Total time:      18.87 min
[32m[20221214 14:16:54 @agent_ppo2.py:145][0m 1728512 total steps have happened
[32m[20221214 14:16:54 @agent_ppo2.py:121][0m #------------------------ Iteration 844 --------------------------#
[32m[20221214 14:16:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:54 @agent_ppo2.py:185][0m |           0.0074 |         162.6301 |         -25.1002 |
[32m[20221214 14:16:55 @agent_ppo2.py:185][0m |           0.0048 |         156.5016 |         -25.1223 |
[32m[20221214 14:16:55 @agent_ppo2.py:185][0m |          -0.0028 |         149.1175 |         -25.0915 |
[32m[20221214 14:16:55 @agent_ppo2.py:185][0m |          -0.0013 |         146.4934 |         -25.1041 |
[32m[20221214 14:16:55 @agent_ppo2.py:185][0m |          -0.0026 |         147.5154 |         -25.1801 |
[32m[20221214 14:16:55 @agent_ppo2.py:185][0m |          -0.0026 |         145.2459 |         -25.1390 |
[32m[20221214 14:16:55 @agent_ppo2.py:185][0m |          -0.0016 |         144.2393 |         -25.1092 |
[32m[20221214 14:16:55 @agent_ppo2.py:185][0m |          -0.0021 |         143.1793 |         -25.1813 |
[32m[20221214 14:16:55 @agent_ppo2.py:185][0m |          -0.0035 |         144.1301 |         -25.2176 |
[32m[20221214 14:16:55 @agent_ppo2.py:185][0m |          -0.0018 |         143.8119 |         -25.1567 |
[32m[20221214 14:16:55 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:16:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.24
[32m[20221214 14:16:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.31
[32m[20221214 14:16:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.34
[32m[20221214 14:16:55 @agent_ppo2.py:143][0m Total time:      18.89 min
[32m[20221214 14:16:55 @agent_ppo2.py:145][0m 1730560 total steps have happened
[32m[20221214 14:16:55 @agent_ppo2.py:121][0m #------------------------ Iteration 845 --------------------------#
[32m[20221214 14:16:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:56 @agent_ppo2.py:185][0m |           0.0131 |         174.1684 |         -24.5515 |
[32m[20221214 14:16:56 @agent_ppo2.py:185][0m |          -0.0012 |         150.9275 |         -24.5126 |
[32m[20221214 14:16:56 @agent_ppo2.py:185][0m |          -0.0036 |         148.4775 |         -24.2996 |
[32m[20221214 14:16:56 @agent_ppo2.py:185][0m |          -0.0021 |         148.2358 |         -24.3764 |
[32m[20221214 14:16:56 @agent_ppo2.py:185][0m |          -0.0027 |         146.9613 |         -24.3935 |
[32m[20221214 14:16:56 @agent_ppo2.py:185][0m |          -0.0019 |         147.9122 |         -24.4616 |
[32m[20221214 14:16:56 @agent_ppo2.py:185][0m |          -0.0031 |         147.1214 |         -24.4031 |
[32m[20221214 14:16:56 @agent_ppo2.py:185][0m |          -0.0013 |         147.4850 |         -24.4461 |
[32m[20221214 14:16:57 @agent_ppo2.py:185][0m |          -0.0031 |         147.4328 |         -24.3989 |
[32m[20221214 14:16:57 @agent_ppo2.py:185][0m |          -0.0024 |         146.2673 |         -24.3712 |
[32m[20221214 14:16:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:16:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.07
[32m[20221214 14:16:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.00
[32m[20221214 14:16:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.51
[32m[20221214 14:16:57 @agent_ppo2.py:143][0m Total time:      18.91 min
[32m[20221214 14:16:57 @agent_ppo2.py:145][0m 1732608 total steps have happened
[32m[20221214 14:16:57 @agent_ppo2.py:121][0m #------------------------ Iteration 846 --------------------------#
[32m[20221214 14:16:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:57 @agent_ppo2.py:185][0m |           0.0002 |         111.4221 |         -24.9737 |
[32m[20221214 14:16:57 @agent_ppo2.py:185][0m |          -0.0015 |         110.3859 |         -24.9415 |
[32m[20221214 14:16:57 @agent_ppo2.py:185][0m |           0.0050 |         111.8535 |         -24.9502 |
[32m[20221214 14:16:57 @agent_ppo2.py:185][0m |           0.0000 |         109.8320 |         -24.9133 |
[32m[20221214 14:16:58 @agent_ppo2.py:185][0m |          -0.0037 |         108.6130 |         -24.9983 |
[32m[20221214 14:16:58 @agent_ppo2.py:185][0m |          -0.0018 |         109.4412 |         -25.0015 |
[32m[20221214 14:16:58 @agent_ppo2.py:185][0m |          -0.0005 |         107.9715 |         -24.9134 |
[32m[20221214 14:16:58 @agent_ppo2.py:185][0m |          -0.0049 |         107.3828 |         -24.9721 |
[32m[20221214 14:16:58 @agent_ppo2.py:185][0m |          -0.0056 |         107.2999 |         -24.9460 |
[32m[20221214 14:16:58 @agent_ppo2.py:185][0m |           0.0009 |         108.0921 |         -24.9449 |
[32m[20221214 14:16:58 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:16:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.89
[32m[20221214 14:16:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.38
[32m[20221214 14:16:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.68
[32m[20221214 14:16:58 @agent_ppo2.py:143][0m Total time:      18.94 min
[32m[20221214 14:16:58 @agent_ppo2.py:145][0m 1734656 total steps have happened
[32m[20221214 14:16:58 @agent_ppo2.py:121][0m #------------------------ Iteration 847 --------------------------#
[32m[20221214 14:16:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:16:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:16:59 @agent_ppo2.py:185][0m |           0.0056 |          86.0993 |         -24.7738 |
[32m[20221214 14:16:59 @agent_ppo2.py:185][0m |          -0.0017 |          80.7993 |         -24.7003 |
[32m[20221214 14:16:59 @agent_ppo2.py:185][0m |          -0.0030 |          79.4931 |         -24.7032 |
[32m[20221214 14:16:59 @agent_ppo2.py:185][0m |          -0.0038 |          79.8433 |         -24.6586 |
[32m[20221214 14:16:59 @agent_ppo2.py:185][0m |          -0.0042 |          77.8890 |         -24.5939 |
[32m[20221214 14:16:59 @agent_ppo2.py:185][0m |          -0.0059 |          77.3857 |         -24.6730 |
[32m[20221214 14:16:59 @agent_ppo2.py:185][0m |           0.0089 |          81.9051 |         -24.6195 |
[32m[20221214 14:16:59 @agent_ppo2.py:185][0m |          -0.0008 |          76.6826 |         -24.5746 |
[32m[20221214 14:16:59 @agent_ppo2.py:185][0m |           0.0010 |          76.9918 |         -24.5373 |
[32m[20221214 14:16:59 @agent_ppo2.py:185][0m |           0.0041 |          76.3812 |         -24.6304 |
[32m[20221214 14:16:59 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.04
[32m[20221214 14:17:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.59
[32m[20221214 14:17:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.19
[32m[20221214 14:17:00 @agent_ppo2.py:143][0m Total time:      18.96 min
[32m[20221214 14:17:00 @agent_ppo2.py:145][0m 1736704 total steps have happened
[32m[20221214 14:17:00 @agent_ppo2.py:121][0m #------------------------ Iteration 848 --------------------------#
[32m[20221214 14:17:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:00 @agent_ppo2.py:185][0m |          -0.0013 |         180.0185 |         -24.1183 |
[32m[20221214 14:17:00 @agent_ppo2.py:185][0m |           0.0131 |         196.5111 |         -24.2018 |
[32m[20221214 14:17:00 @agent_ppo2.py:185][0m |          -0.0004 |         177.9399 |         -24.2359 |
[32m[20221214 14:17:00 @agent_ppo2.py:185][0m |          -0.0018 |         173.6583 |         -24.4049 |
[32m[20221214 14:17:00 @agent_ppo2.py:185][0m |          -0.0030 |         172.3868 |         -24.4668 |
[32m[20221214 14:17:00 @agent_ppo2.py:185][0m |          -0.0025 |         171.2959 |         -24.5196 |
[32m[20221214 14:17:00 @agent_ppo2.py:185][0m |          -0.0028 |         171.5734 |         -24.5429 |
[32m[20221214 14:17:01 @agent_ppo2.py:185][0m |          -0.0026 |         171.1157 |         -24.5714 |
[32m[20221214 14:17:01 @agent_ppo2.py:185][0m |          -0.0023 |         169.6912 |         -24.6616 |
[32m[20221214 14:17:01 @agent_ppo2.py:185][0m |          -0.0019 |         169.3732 |         -24.6907 |
[32m[20221214 14:17:01 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.52
[32m[20221214 14:17:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.45
[32m[20221214 14:17:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.47
[32m[20221214 14:17:01 @agent_ppo2.py:143][0m Total time:      18.98 min
[32m[20221214 14:17:01 @agent_ppo2.py:145][0m 1738752 total steps have happened
[32m[20221214 14:17:01 @agent_ppo2.py:121][0m #------------------------ Iteration 849 --------------------------#
[32m[20221214 14:17:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:01 @agent_ppo2.py:185][0m |          -0.0049 |         147.9076 |         -24.7587 |
[32m[20221214 14:17:01 @agent_ppo2.py:185][0m |          -0.0072 |         145.8333 |         -24.7081 |
[32m[20221214 14:17:01 @agent_ppo2.py:185][0m |          -0.0038 |         144.5315 |         -24.5996 |
[32m[20221214 14:17:02 @agent_ppo2.py:185][0m |          -0.0040 |         144.0237 |         -24.6843 |
[32m[20221214 14:17:02 @agent_ppo2.py:185][0m |          -0.0061 |         143.6758 |         -24.7232 |
[32m[20221214 14:17:02 @agent_ppo2.py:185][0m |          -0.0053 |         143.3224 |         -24.7212 |
[32m[20221214 14:17:02 @agent_ppo2.py:185][0m |          -0.0050 |         143.1453 |         -24.7367 |
[32m[20221214 14:17:02 @agent_ppo2.py:185][0m |          -0.0006 |         145.6592 |         -24.7293 |
[32m[20221214 14:17:02 @agent_ppo2.py:185][0m |          -0.0057 |         143.7047 |         -24.6296 |
[32m[20221214 14:17:02 @agent_ppo2.py:185][0m |          -0.0048 |         142.1586 |         -24.7181 |
[32m[20221214 14:17:02 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.07
[32m[20221214 14:17:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.15
[32m[20221214 14:17:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.22
[32m[20221214 14:17:02 @agent_ppo2.py:143][0m Total time:      19.00 min
[32m[20221214 14:17:02 @agent_ppo2.py:145][0m 1740800 total steps have happened
[32m[20221214 14:17:02 @agent_ppo2.py:121][0m #------------------------ Iteration 850 --------------------------#
[32m[20221214 14:17:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:17:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:03 @agent_ppo2.py:185][0m |          -0.0013 |         176.7948 |         -25.7813 |
[32m[20221214 14:17:03 @agent_ppo2.py:185][0m |          -0.0021 |         176.6581 |         -25.8316 |
[32m[20221214 14:17:03 @agent_ppo2.py:185][0m |          -0.0015 |         176.8349 |         -25.7979 |
[32m[20221214 14:17:03 @agent_ppo2.py:185][0m |          -0.0004 |         175.9461 |         -25.7807 |
[32m[20221214 14:17:03 @agent_ppo2.py:185][0m |          -0.0006 |         175.1383 |         -25.9488 |
[32m[20221214 14:17:03 @agent_ppo2.py:185][0m |          -0.0036 |         174.9276 |         -25.8119 |
[32m[20221214 14:17:03 @agent_ppo2.py:185][0m |          -0.0013 |         174.9018 |         -25.9011 |
[32m[20221214 14:17:03 @agent_ppo2.py:185][0m |           0.0018 |         173.7373 |         -25.8536 |
[32m[20221214 14:17:03 @agent_ppo2.py:185][0m |           0.0000 |         173.1658 |         -25.8343 |
[32m[20221214 14:17:03 @agent_ppo2.py:185][0m |          -0.0001 |         174.0570 |         -25.8220 |
[32m[20221214 14:17:03 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.77
[32m[20221214 14:17:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.85
[32m[20221214 14:17:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.51
[32m[20221214 14:17:04 @agent_ppo2.py:143][0m Total time:      19.03 min
[32m[20221214 14:17:04 @agent_ppo2.py:145][0m 1742848 total steps have happened
[32m[20221214 14:17:04 @agent_ppo2.py:121][0m #------------------------ Iteration 851 --------------------------#
[32m[20221214 14:17:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:04 @agent_ppo2.py:185][0m |          -0.0026 |         214.5138 |         -24.6063 |
[32m[20221214 14:17:04 @agent_ppo2.py:185][0m |          -0.0038 |         212.6933 |         -24.5687 |
[32m[20221214 14:17:04 @agent_ppo2.py:185][0m |          -0.0028 |         211.0737 |         -24.6977 |
[32m[20221214 14:17:04 @agent_ppo2.py:185][0m |           0.0062 |         221.4782 |         -24.8501 |
[32m[20221214 14:17:04 @agent_ppo2.py:185][0m |           0.0013 |         214.1668 |         -24.8799 |
[32m[20221214 14:17:04 @agent_ppo2.py:185][0m |           0.0087 |         219.7889 |         -24.8562 |
[32m[20221214 14:17:05 @agent_ppo2.py:185][0m |          -0.0031 |         211.7647 |         -24.9348 |
[32m[20221214 14:17:05 @agent_ppo2.py:185][0m |          -0.0036 |         209.8416 |         -24.9419 |
[32m[20221214 14:17:05 @agent_ppo2.py:185][0m |           0.0099 |         238.4036 |         -25.0849 |
[32m[20221214 14:17:05 @agent_ppo2.py:185][0m |           0.0060 |         217.4435 |         -24.9823 |
[32m[20221214 14:17:05 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.98
[32m[20221214 14:17:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.20
[32m[20221214 14:17:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.27
[32m[20221214 14:17:05 @agent_ppo2.py:143][0m Total time:      19.05 min
[32m[20221214 14:17:05 @agent_ppo2.py:145][0m 1744896 total steps have happened
[32m[20221214 14:17:05 @agent_ppo2.py:121][0m #------------------------ Iteration 852 --------------------------#
[32m[20221214 14:17:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:05 @agent_ppo2.py:185][0m |          -0.0016 |         261.7820 |         -25.6524 |
[32m[20221214 14:17:05 @agent_ppo2.py:185][0m |           0.0136 |         273.6275 |         -25.6581 |
[32m[20221214 14:17:06 @agent_ppo2.py:185][0m |          -0.0016 |         250.6560 |         -25.5690 |
[32m[20221214 14:17:06 @agent_ppo2.py:185][0m |          -0.0021 |         246.6818 |         -25.5546 |
[32m[20221214 14:17:06 @agent_ppo2.py:185][0m |           0.0080 |         261.1495 |         -25.4682 |
[32m[20221214 14:17:06 @agent_ppo2.py:185][0m |           0.0016 |         247.9499 |         -25.2401 |
[32m[20221214 14:17:06 @agent_ppo2.py:185][0m |          -0.0034 |         244.9348 |         -25.4157 |
[32m[20221214 14:17:06 @agent_ppo2.py:185][0m |          -0.0013 |         242.8588 |         -25.3336 |
[32m[20221214 14:17:06 @agent_ppo2.py:185][0m |           0.0010 |         245.1769 |         -25.3192 |
[32m[20221214 14:17:06 @agent_ppo2.py:185][0m |           0.0020 |         245.5496 |         -25.3504 |
[32m[20221214 14:17:06 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.46
[32m[20221214 14:17:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.00
[32m[20221214 14:17:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.82
[32m[20221214 14:17:06 @agent_ppo2.py:143][0m Total time:      19.07 min
[32m[20221214 14:17:06 @agent_ppo2.py:145][0m 1746944 total steps have happened
[32m[20221214 14:17:06 @agent_ppo2.py:121][0m #------------------------ Iteration 853 --------------------------#
[32m[20221214 14:17:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:07 @agent_ppo2.py:185][0m |          -0.0015 |         230.0154 |         -24.8992 |
[32m[20221214 14:17:07 @agent_ppo2.py:185][0m |          -0.0077 |         226.9946 |         -24.8190 |
[32m[20221214 14:17:07 @agent_ppo2.py:185][0m |          -0.0030 |         226.3740 |         -24.8691 |
[32m[20221214 14:17:07 @agent_ppo2.py:185][0m |          -0.0057 |         225.3004 |         -24.8006 |
[32m[20221214 14:17:07 @agent_ppo2.py:185][0m |          -0.0027 |         227.3121 |         -24.8235 |
[32m[20221214 14:17:07 @agent_ppo2.py:185][0m |           0.0000 |         226.6035 |         -24.7734 |
[32m[20221214 14:17:07 @agent_ppo2.py:185][0m |          -0.0054 |         225.5782 |         -24.8306 |
[32m[20221214 14:17:07 @agent_ppo2.py:185][0m |          -0.0015 |         226.0594 |         -24.8045 |
[32m[20221214 14:17:07 @agent_ppo2.py:185][0m |          -0.0046 |         224.6489 |         -24.7091 |
[32m[20221214 14:17:08 @agent_ppo2.py:185][0m |          -0.0058 |         225.1860 |         -24.6698 |
[32m[20221214 14:17:08 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.74
[32m[20221214 14:17:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.53
[32m[20221214 14:17:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.96
[32m[20221214 14:17:08 @agent_ppo2.py:143][0m Total time:      19.09 min
[32m[20221214 14:17:08 @agent_ppo2.py:145][0m 1748992 total steps have happened
[32m[20221214 14:17:08 @agent_ppo2.py:121][0m #------------------------ Iteration 854 --------------------------#
[32m[20221214 14:17:08 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:17:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:08 @agent_ppo2.py:185][0m |          -0.0024 |         188.1194 |         -24.9278 |
[32m[20221214 14:17:08 @agent_ppo2.py:185][0m |          -0.0005 |         183.0702 |         -24.8673 |
[32m[20221214 14:17:08 @agent_ppo2.py:185][0m |           0.0119 |         207.1339 |         -24.8651 |
[32m[20221214 14:17:08 @agent_ppo2.py:185][0m |           0.0026 |         183.9584 |         -24.8556 |
[32m[20221214 14:17:08 @agent_ppo2.py:185][0m |           0.0012 |         182.9423 |         -24.9966 |
[32m[20221214 14:17:09 @agent_ppo2.py:185][0m |          -0.0004 |         181.4822 |         -25.0070 |
[32m[20221214 14:17:09 @agent_ppo2.py:185][0m |          -0.0018 |         179.5310 |         -25.0729 |
[32m[20221214 14:17:09 @agent_ppo2.py:185][0m |          -0.0014 |         180.1185 |         -24.9695 |
[32m[20221214 14:17:09 @agent_ppo2.py:185][0m |          -0.0024 |         179.6218 |         -24.9740 |
[32m[20221214 14:17:09 @agent_ppo2.py:185][0m |           0.0012 |         179.0549 |         -24.9912 |
[32m[20221214 14:17:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:17:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.71
[32m[20221214 14:17:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.82
[32m[20221214 14:17:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.10
[32m[20221214 14:17:09 @agent_ppo2.py:143][0m Total time:      19.12 min
[32m[20221214 14:17:09 @agent_ppo2.py:145][0m 1751040 total steps have happened
[32m[20221214 14:17:09 @agent_ppo2.py:121][0m #------------------------ Iteration 855 --------------------------#
[32m[20221214 14:17:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:17:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:09 @agent_ppo2.py:185][0m |           0.0005 |         189.8418 |         -25.4381 |
[32m[20221214 14:17:09 @agent_ppo2.py:185][0m |          -0.0016 |         187.8285 |         -25.3881 |
[32m[20221214 14:17:10 @agent_ppo2.py:185][0m |           0.0000 |         186.7605 |         -25.4833 |
[32m[20221214 14:17:10 @agent_ppo2.py:185][0m |          -0.0013 |         186.1166 |         -25.4319 |
[32m[20221214 14:17:10 @agent_ppo2.py:185][0m |          -0.0001 |         186.6073 |         -25.4859 |
[32m[20221214 14:17:10 @agent_ppo2.py:185][0m |           0.0004 |         184.8330 |         -25.4358 |
[32m[20221214 14:17:10 @agent_ppo2.py:185][0m |          -0.0003 |         185.2652 |         -25.4726 |
[32m[20221214 14:17:10 @agent_ppo2.py:185][0m |          -0.0017 |         185.3011 |         -25.4929 |
[32m[20221214 14:17:10 @agent_ppo2.py:185][0m |           0.0012 |         185.5214 |         -25.5060 |
[32m[20221214 14:17:10 @agent_ppo2.py:185][0m |          -0.0010 |         184.4602 |         -25.5145 |
[32m[20221214 14:17:10 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 14:17:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.91
[32m[20221214 14:17:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.03
[32m[20221214 14:17:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.30
[32m[20221214 14:17:11 @agent_ppo2.py:143][0m Total time:      19.14 min
[32m[20221214 14:17:11 @agent_ppo2.py:145][0m 1753088 total steps have happened
[32m[20221214 14:17:11 @agent_ppo2.py:121][0m #------------------------ Iteration 856 --------------------------#
[32m[20221214 14:17:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:11 @agent_ppo2.py:185][0m |          -0.0040 |         170.6510 |         -25.2252 |
[32m[20221214 14:17:11 @agent_ppo2.py:185][0m |          -0.0041 |         169.3610 |         -25.1514 |
[32m[20221214 14:17:11 @agent_ppo2.py:185][0m |           0.0011 |         170.7275 |         -25.2192 |
[32m[20221214 14:17:11 @agent_ppo2.py:185][0m |           0.0001 |         170.0470 |         -25.3701 |
[32m[20221214 14:17:11 @agent_ppo2.py:185][0m |          -0.0034 |         167.4436 |         -25.3806 |
[32m[20221214 14:17:11 @agent_ppo2.py:185][0m |          -0.0017 |         167.4816 |         -25.4696 |
[32m[20221214 14:17:11 @agent_ppo2.py:185][0m |          -0.0039 |         166.9992 |         -25.5763 |
[32m[20221214 14:17:12 @agent_ppo2.py:185][0m |          -0.0052 |         167.0248 |         -25.5457 |
[32m[20221214 14:17:12 @agent_ppo2.py:185][0m |          -0.0028 |         166.1501 |         -25.6654 |
[32m[20221214 14:17:12 @agent_ppo2.py:185][0m |          -0.0042 |         166.8808 |         -25.6469 |
[32m[20221214 14:17:12 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.90
[32m[20221214 14:17:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.69
[32m[20221214 14:17:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.31
[32m[20221214 14:17:12 @agent_ppo2.py:143][0m Total time:      19.16 min
[32m[20221214 14:17:12 @agent_ppo2.py:145][0m 1755136 total steps have happened
[32m[20221214 14:17:12 @agent_ppo2.py:121][0m #------------------------ Iteration 857 --------------------------#
[32m[20221214 14:17:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:12 @agent_ppo2.py:185][0m |           0.0001 |         219.7672 |         -26.4690 |
[32m[20221214 14:17:12 @agent_ppo2.py:185][0m |           0.0114 |         240.6968 |         -26.4511 |
[32m[20221214 14:17:12 @agent_ppo2.py:185][0m |          -0.0003 |         216.0630 |         -26.4468 |
[32m[20221214 14:17:13 @agent_ppo2.py:185][0m |          -0.0027 |         214.8548 |         -26.4035 |
[32m[20221214 14:17:13 @agent_ppo2.py:185][0m |          -0.0011 |         214.9919 |         -26.4122 |
[32m[20221214 14:17:13 @agent_ppo2.py:185][0m |           0.0064 |         231.0842 |         -26.4360 |
[32m[20221214 14:17:13 @agent_ppo2.py:185][0m |           0.0029 |         214.5198 |         -26.3826 |
[32m[20221214 14:17:13 @agent_ppo2.py:185][0m |          -0.0022 |         214.3119 |         -26.4427 |
[32m[20221214 14:17:13 @agent_ppo2.py:185][0m |           0.0003 |         213.5964 |         -26.4508 |
[32m[20221214 14:17:13 @agent_ppo2.py:185][0m |          -0.0004 |         213.4252 |         -26.4694 |
[32m[20221214 14:17:13 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:17:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.37
[32m[20221214 14:17:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.44
[32m[20221214 14:17:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.49
[32m[20221214 14:17:13 @agent_ppo2.py:143][0m Total time:      19.19 min
[32m[20221214 14:17:13 @agent_ppo2.py:145][0m 1757184 total steps have happened
[32m[20221214 14:17:13 @agent_ppo2.py:121][0m #------------------------ Iteration 858 --------------------------#
[32m[20221214 14:17:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:14 @agent_ppo2.py:185][0m |           0.0114 |         275.9232 |         -26.4493 |
[32m[20221214 14:17:14 @agent_ppo2.py:185][0m |           0.0014 |         248.3245 |         -26.4889 |
[32m[20221214 14:17:14 @agent_ppo2.py:185][0m |          -0.0009 |         246.7822 |         -26.5946 |
[32m[20221214 14:17:14 @agent_ppo2.py:185][0m |          -0.0030 |         246.1915 |         -26.6472 |
[32m[20221214 14:17:14 @agent_ppo2.py:185][0m |          -0.0021 |         245.7215 |         -26.6680 |
[32m[20221214 14:17:14 @agent_ppo2.py:185][0m |          -0.0030 |         245.9652 |         -26.8083 |
[32m[20221214 14:17:14 @agent_ppo2.py:185][0m |          -0.0014 |         245.6607 |         -26.8184 |
[32m[20221214 14:17:14 @agent_ppo2.py:185][0m |          -0.0026 |         245.3461 |         -26.7993 |
[32m[20221214 14:17:14 @agent_ppo2.py:185][0m |          -0.0024 |         244.9036 |         -26.8919 |
[32m[20221214 14:17:14 @agent_ppo2.py:185][0m |          -0.0028 |         244.8257 |         -26.9148 |
[32m[20221214 14:17:14 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:17:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.63
[32m[20221214 14:17:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.86
[32m[20221214 14:17:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.37
[32m[20221214 14:17:15 @agent_ppo2.py:143][0m Total time:      19.21 min
[32m[20221214 14:17:15 @agent_ppo2.py:145][0m 1759232 total steps have happened
[32m[20221214 14:17:15 @agent_ppo2.py:121][0m #------------------------ Iteration 859 --------------------------#
[32m[20221214 14:17:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:15 @agent_ppo2.py:185][0m |          -0.0008 |         255.5887 |         -27.2635 |
[32m[20221214 14:17:15 @agent_ppo2.py:185][0m |           0.0031 |         253.6369 |         -27.4793 |
[32m[20221214 14:17:15 @agent_ppo2.py:185][0m |          -0.0019 |         250.6312 |         -27.4683 |
[32m[20221214 14:17:15 @agent_ppo2.py:185][0m |          -0.0019 |         248.2812 |         -27.4661 |
[32m[20221214 14:17:15 @agent_ppo2.py:185][0m |          -0.0007 |         247.3964 |         -27.3970 |
[32m[20221214 14:17:15 @agent_ppo2.py:185][0m |          -0.0021 |         246.6011 |         -27.4393 |
[32m[20221214 14:17:16 @agent_ppo2.py:185][0m |           0.0092 |         255.8091 |         -27.3196 |
[32m[20221214 14:17:16 @agent_ppo2.py:185][0m |          -0.0026 |         245.3196 |         -27.5852 |
[32m[20221214 14:17:16 @agent_ppo2.py:185][0m |          -0.0014 |         245.5688 |         -27.5017 |
[32m[20221214 14:17:16 @agent_ppo2.py:185][0m |           0.0000 |         245.1400 |         -27.6359 |
[32m[20221214 14:17:16 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:17:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.05
[32m[20221214 14:17:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.82
[32m[20221214 14:17:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.66
[32m[20221214 14:17:16 @agent_ppo2.py:143][0m Total time:      19.23 min
[32m[20221214 14:17:16 @agent_ppo2.py:145][0m 1761280 total steps have happened
[32m[20221214 14:17:16 @agent_ppo2.py:121][0m #------------------------ Iteration 860 --------------------------#
[32m[20221214 14:17:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:17:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:16 @agent_ppo2.py:185][0m |          -0.0015 |         200.3697 |         -27.9653 |
[32m[20221214 14:17:16 @agent_ppo2.py:185][0m |          -0.0038 |         196.7339 |         -27.9402 |
[32m[20221214 14:17:17 @agent_ppo2.py:185][0m |           0.0008 |         195.6816 |         -28.0548 |
[32m[20221214 14:17:17 @agent_ppo2.py:185][0m |          -0.0037 |         194.5945 |         -28.1117 |
[32m[20221214 14:17:17 @agent_ppo2.py:185][0m |          -0.0040 |         194.0585 |         -28.1365 |
[32m[20221214 14:17:17 @agent_ppo2.py:185][0m |          -0.0022 |         194.1088 |         -28.1781 |
[32m[20221214 14:17:17 @agent_ppo2.py:185][0m |          -0.0024 |         194.0734 |         -28.3414 |
[32m[20221214 14:17:17 @agent_ppo2.py:185][0m |          -0.0018 |         193.2070 |         -28.3630 |
[32m[20221214 14:17:17 @agent_ppo2.py:185][0m |           0.0002 |         192.9276 |         -28.4061 |
[32m[20221214 14:17:17 @agent_ppo2.py:185][0m |          -0.0041 |         193.0485 |         -28.4183 |
[32m[20221214 14:17:17 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:17:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.11
[32m[20221214 14:17:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.47
[32m[20221214 14:17:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.36
[32m[20221214 14:17:17 @agent_ppo2.py:143][0m Total time:      19.26 min
[32m[20221214 14:17:17 @agent_ppo2.py:145][0m 1763328 total steps have happened
[32m[20221214 14:17:17 @agent_ppo2.py:121][0m #------------------------ Iteration 861 --------------------------#
[32m[20221214 14:17:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:18 @agent_ppo2.py:185][0m |          -0.0018 |         173.0759 |         -28.7256 |
[32m[20221214 14:17:18 @agent_ppo2.py:185][0m |           0.0056 |         172.2201 |         -28.8099 |
[32m[20221214 14:17:18 @agent_ppo2.py:185][0m |          -0.0005 |         168.8980 |         -28.6991 |
[32m[20221214 14:17:18 @agent_ppo2.py:185][0m |          -0.0035 |         168.5979 |         -28.6194 |
[32m[20221214 14:17:18 @agent_ppo2.py:185][0m |          -0.0028 |         168.0951 |         -28.7748 |
[32m[20221214 14:17:18 @agent_ppo2.py:185][0m |           0.0075 |         177.7951 |         -28.6605 |
[32m[20221214 14:17:18 @agent_ppo2.py:185][0m |          -0.0018 |         169.8733 |         -28.6801 |
[32m[20221214 14:17:18 @agent_ppo2.py:185][0m |           0.0003 |         166.5999 |         -28.6498 |
[32m[20221214 14:17:18 @agent_ppo2.py:185][0m |          -0.0019 |         166.5129 |         -28.4898 |
[32m[20221214 14:17:19 @agent_ppo2.py:185][0m |           0.0089 |         171.2805 |         -28.6189 |
[32m[20221214 14:17:19 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.19
[32m[20221214 14:17:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.07
[32m[20221214 14:17:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.32
[32m[20221214 14:17:19 @agent_ppo2.py:143][0m Total time:      19.28 min
[32m[20221214 14:17:19 @agent_ppo2.py:145][0m 1765376 total steps have happened
[32m[20221214 14:17:19 @agent_ppo2.py:121][0m #------------------------ Iteration 862 --------------------------#
[32m[20221214 14:17:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:19 @agent_ppo2.py:185][0m |           0.0086 |         183.3473 |         -27.5911 |
[32m[20221214 14:17:19 @agent_ppo2.py:185][0m |           0.0082 |         177.9844 |         -27.6871 |
[32m[20221214 14:17:19 @agent_ppo2.py:185][0m |          -0.0051 |         158.3731 |         -27.3693 |
[32m[20221214 14:17:19 @agent_ppo2.py:185][0m |          -0.0040 |         156.5577 |         -27.6225 |
[32m[20221214 14:17:19 @agent_ppo2.py:185][0m |          -0.0048 |         155.4147 |         -27.6437 |
[32m[20221214 14:17:20 @agent_ppo2.py:185][0m |          -0.0034 |         154.1845 |         -27.6786 |
[32m[20221214 14:17:20 @agent_ppo2.py:185][0m |          -0.0016 |         155.3158 |         -27.7539 |
[32m[20221214 14:17:20 @agent_ppo2.py:185][0m |          -0.0001 |         153.2159 |         -27.7582 |
[32m[20221214 14:17:20 @agent_ppo2.py:185][0m |          -0.0033 |         152.1828 |         -27.8170 |
[32m[20221214 14:17:20 @agent_ppo2.py:185][0m |          -0.0047 |         152.1125 |         -27.9329 |
[32m[20221214 14:17:20 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.83
[32m[20221214 14:17:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.64
[32m[20221214 14:17:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.04
[32m[20221214 14:17:20 @agent_ppo2.py:143][0m Total time:      19.30 min
[32m[20221214 14:17:20 @agent_ppo2.py:145][0m 1767424 total steps have happened
[32m[20221214 14:17:20 @agent_ppo2.py:121][0m #------------------------ Iteration 863 --------------------------#
[32m[20221214 14:17:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:20 @agent_ppo2.py:185][0m |          -0.0031 |         194.0777 |         -28.4374 |
[32m[20221214 14:17:21 @agent_ppo2.py:185][0m |          -0.0029 |         190.9129 |         -28.4650 |
[32m[20221214 14:17:21 @agent_ppo2.py:185][0m |           0.0120 |         209.4886 |         -28.3800 |
[32m[20221214 14:17:21 @agent_ppo2.py:185][0m |          -0.0038 |         188.9350 |         -28.2983 |
[32m[20221214 14:17:21 @agent_ppo2.py:185][0m |          -0.0045 |         190.2605 |         -28.3159 |
[32m[20221214 14:17:21 @agent_ppo2.py:185][0m |          -0.0052 |         186.7207 |         -28.4291 |
[32m[20221214 14:17:21 @agent_ppo2.py:185][0m |          -0.0032 |         186.3492 |         -28.2846 |
[32m[20221214 14:17:21 @agent_ppo2.py:185][0m |          -0.0047 |         185.6662 |         -28.4251 |
[32m[20221214 14:17:21 @agent_ppo2.py:185][0m |          -0.0023 |         186.9156 |         -28.2986 |
[32m[20221214 14:17:21 @agent_ppo2.py:185][0m |          -0.0042 |         186.7969 |         -28.3936 |
[32m[20221214 14:17:21 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:17:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.82
[32m[20221214 14:17:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.79
[32m[20221214 14:17:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.50
[32m[20221214 14:17:21 @agent_ppo2.py:143][0m Total time:      19.32 min
[32m[20221214 14:17:21 @agent_ppo2.py:145][0m 1769472 total steps have happened
[32m[20221214 14:17:21 @agent_ppo2.py:121][0m #------------------------ Iteration 864 --------------------------#
[32m[20221214 14:17:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:22 @agent_ppo2.py:185][0m |          -0.0030 |         266.7564 |         -29.1457 |
[32m[20221214 14:17:22 @agent_ppo2.py:185][0m |          -0.0036 |         262.8033 |         -29.0278 |
[32m[20221214 14:17:22 @agent_ppo2.py:185][0m |          -0.0040 |         261.7284 |         -29.0037 |
[32m[20221214 14:17:22 @agent_ppo2.py:185][0m |          -0.0038 |         261.1598 |         -28.9830 |
[32m[20221214 14:17:22 @agent_ppo2.py:185][0m |          -0.0010 |         260.9949 |         -28.9844 |
[32m[20221214 14:17:22 @agent_ppo2.py:185][0m |           0.0003 |         260.7948 |         -28.9717 |
[32m[20221214 14:17:22 @agent_ppo2.py:185][0m |          -0.0012 |         260.1393 |         -28.9283 |
[32m[20221214 14:17:22 @agent_ppo2.py:185][0m |          -0.0033 |         259.1268 |         -28.8904 |
[32m[20221214 14:17:23 @agent_ppo2.py:185][0m |          -0.0035 |         259.6168 |         -28.8581 |
[32m[20221214 14:17:23 @agent_ppo2.py:185][0m |          -0.0038 |         258.9238 |         -28.8080 |
[32m[20221214 14:17:23 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.11
[32m[20221214 14:17:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.17
[32m[20221214 14:17:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.89
[32m[20221214 14:17:23 @agent_ppo2.py:143][0m Total time:      19.35 min
[32m[20221214 14:17:23 @agent_ppo2.py:145][0m 1771520 total steps have happened
[32m[20221214 14:17:23 @agent_ppo2.py:121][0m #------------------------ Iteration 865 --------------------------#
[32m[20221214 14:17:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:23 @agent_ppo2.py:185][0m |          -0.0026 |         295.4260 |         -28.7071 |
[32m[20221214 14:17:23 @agent_ppo2.py:185][0m |          -0.0034 |         291.4374 |         -28.6834 |
[32m[20221214 14:17:23 @agent_ppo2.py:185][0m |           0.0026 |         300.4147 |         -28.6068 |
[32m[20221214 14:17:23 @agent_ppo2.py:185][0m |          -0.0038 |         290.2666 |         -28.6046 |
[32m[20221214 14:17:24 @agent_ppo2.py:185][0m |          -0.0026 |         288.4750 |         -28.5335 |
[32m[20221214 14:17:24 @agent_ppo2.py:185][0m |          -0.0031 |         288.3548 |         -28.4954 |
[32m[20221214 14:17:24 @agent_ppo2.py:185][0m |          -0.0037 |         288.6251 |         -28.5265 |
[32m[20221214 14:17:24 @agent_ppo2.py:185][0m |           0.0101 |         319.1911 |         -28.4862 |
[32m[20221214 14:17:24 @agent_ppo2.py:185][0m |          -0.0024 |         288.2561 |         -28.4063 |
[32m[20221214 14:17:24 @agent_ppo2.py:185][0m |          -0.0037 |         287.5913 |         -28.4415 |
[32m[20221214 14:17:24 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 844.72
[32m[20221214 14:17:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.36
[32m[20221214 14:17:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.82
[32m[20221214 14:17:24 @agent_ppo2.py:143][0m Total time:      19.37 min
[32m[20221214 14:17:24 @agent_ppo2.py:145][0m 1773568 total steps have happened
[32m[20221214 14:17:24 @agent_ppo2.py:121][0m #------------------------ Iteration 866 --------------------------#
[32m[20221214 14:17:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:25 @agent_ppo2.py:185][0m |           0.0006 |         295.3947 |         -28.4427 |
[32m[20221214 14:17:25 @agent_ppo2.py:185][0m |          -0.0027 |         287.5401 |         -28.4823 |
[32m[20221214 14:17:25 @agent_ppo2.py:185][0m |          -0.0029 |         285.6791 |         -28.4985 |
[32m[20221214 14:17:25 @agent_ppo2.py:185][0m |          -0.0021 |         284.2479 |         -28.5283 |
[32m[20221214 14:17:25 @agent_ppo2.py:185][0m |          -0.0035 |         283.4430 |         -28.5369 |
[32m[20221214 14:17:25 @agent_ppo2.py:185][0m |          -0.0011 |         282.0877 |         -28.6379 |
[32m[20221214 14:17:25 @agent_ppo2.py:185][0m |          -0.0024 |         281.6185 |         -28.5567 |
[32m[20221214 14:17:25 @agent_ppo2.py:185][0m |          -0.0010 |         281.6249 |         -28.6279 |
[32m[20221214 14:17:25 @agent_ppo2.py:185][0m |          -0.0020 |         280.3713 |         -28.5930 |
[32m[20221214 14:17:25 @agent_ppo2.py:185][0m |           0.0039 |         286.3893 |         -28.5397 |
[32m[20221214 14:17:25 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 845.68
[32m[20221214 14:17:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.74
[32m[20221214 14:17:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.78
[32m[20221214 14:17:26 @agent_ppo2.py:143][0m Total time:      19.39 min
[32m[20221214 14:17:26 @agent_ppo2.py:145][0m 1775616 total steps have happened
[32m[20221214 14:17:26 @agent_ppo2.py:121][0m #------------------------ Iteration 867 --------------------------#
[32m[20221214 14:17:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:17:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:26 @agent_ppo2.py:185][0m |          -0.0030 |         280.4235 |         -27.8770 |
[32m[20221214 14:17:26 @agent_ppo2.py:185][0m |           0.0002 |         278.3589 |         -27.8759 |
[32m[20221214 14:17:26 @agent_ppo2.py:185][0m |          -0.0023 |         277.2631 |         -27.8714 |
[32m[20221214 14:17:26 @agent_ppo2.py:185][0m |          -0.0028 |         277.5167 |         -27.9223 |
[32m[20221214 14:17:26 @agent_ppo2.py:185][0m |          -0.0038 |         277.8531 |         -27.9920 |
[32m[20221214 14:17:26 @agent_ppo2.py:185][0m |          -0.0034 |         277.0483 |         -28.0870 |
[32m[20221214 14:17:26 @agent_ppo2.py:185][0m |          -0.0041 |         276.9051 |         -28.0565 |
[32m[20221214 14:17:27 @agent_ppo2.py:185][0m |          -0.0045 |         276.9095 |         -28.1290 |
[32m[20221214 14:17:27 @agent_ppo2.py:185][0m |           0.0061 |         283.9395 |         -28.1237 |
[32m[20221214 14:17:27 @agent_ppo2.py:185][0m |          -0.0013 |         276.2680 |         -28.1384 |
[32m[20221214 14:17:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:17:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 845.49
[32m[20221214 14:17:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.97
[32m[20221214 14:17:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.04
[32m[20221214 14:17:27 @agent_ppo2.py:143][0m Total time:      19.41 min
[32m[20221214 14:17:27 @agent_ppo2.py:145][0m 1777664 total steps have happened
[32m[20221214 14:17:27 @agent_ppo2.py:121][0m #------------------------ Iteration 868 --------------------------#
[32m[20221214 14:17:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:17:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:27 @agent_ppo2.py:185][0m |          -0.0019 |         314.3418 |         -28.4488 |
[32m[20221214 14:17:27 @agent_ppo2.py:185][0m |          -0.0033 |         311.0713 |         -28.4819 |
[32m[20221214 14:17:27 @agent_ppo2.py:185][0m |          -0.0032 |         309.7393 |         -28.4399 |
[32m[20221214 14:17:27 @agent_ppo2.py:185][0m |           0.0072 |         335.3463 |         -28.4788 |
[32m[20221214 14:17:28 @agent_ppo2.py:185][0m |          -0.0035 |         307.7831 |         -28.4753 |
[32m[20221214 14:17:28 @agent_ppo2.py:185][0m |          -0.0031 |         306.8889 |         -28.6705 |
[32m[20221214 14:17:28 @agent_ppo2.py:185][0m |           0.0076 |         329.6451 |         -28.5610 |
[32m[20221214 14:17:28 @agent_ppo2.py:185][0m |           0.0013 |         306.8013 |         -28.5794 |
[32m[20221214 14:17:28 @agent_ppo2.py:185][0m |          -0.0005 |         307.3643 |         -28.6563 |
[32m[20221214 14:17:28 @agent_ppo2.py:185][0m |          -0.0028 |         305.0965 |         -28.5880 |
[32m[20221214 14:17:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:17:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 845.44
[32m[20221214 14:17:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.83
[32m[20221214 14:17:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.39
[32m[20221214 14:17:28 @agent_ppo2.py:143][0m Total time:      19.43 min
[32m[20221214 14:17:28 @agent_ppo2.py:145][0m 1779712 total steps have happened
[32m[20221214 14:17:28 @agent_ppo2.py:121][0m #------------------------ Iteration 869 --------------------------#
[32m[20221214 14:17:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:29 @agent_ppo2.py:185][0m |          -0.0012 |         303.0788 |         -29.1523 |
[32m[20221214 14:17:29 @agent_ppo2.py:185][0m |          -0.0020 |         301.0288 |         -29.1831 |
[32m[20221214 14:17:29 @agent_ppo2.py:185][0m |          -0.0022 |         300.0923 |         -29.2701 |
[32m[20221214 14:17:29 @agent_ppo2.py:185][0m |          -0.0023 |         298.9307 |         -29.3064 |
[32m[20221214 14:17:29 @agent_ppo2.py:185][0m |          -0.0014 |         298.5102 |         -29.1440 |
[32m[20221214 14:17:29 @agent_ppo2.py:185][0m |          -0.0009 |         299.0616 |         -29.2700 |
[32m[20221214 14:17:29 @agent_ppo2.py:185][0m |           0.0079 |         308.4989 |         -29.2985 |
[32m[20221214 14:17:29 @agent_ppo2.py:185][0m |          -0.0014 |         297.2234 |         -29.3610 |
[32m[20221214 14:17:29 @agent_ppo2.py:185][0m |          -0.0009 |         296.3799 |         -29.3585 |
[32m[20221214 14:17:29 @agent_ppo2.py:185][0m |          -0.0018 |         297.4802 |         -29.2793 |
[32m[20221214 14:17:29 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:17:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 847.39
[32m[20221214 14:17:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 848.52
[32m[20221214 14:17:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.89
[32m[20221214 14:17:30 @agent_ppo2.py:143][0m Total time:      19.46 min
[32m[20221214 14:17:30 @agent_ppo2.py:145][0m 1781760 total steps have happened
[32m[20221214 14:17:30 @agent_ppo2.py:121][0m #------------------------ Iteration 870 --------------------------#
[32m[20221214 14:17:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:17:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:30 @agent_ppo2.py:185][0m |          -0.0010 |         285.4869 |         -29.2355 |
[32m[20221214 14:17:30 @agent_ppo2.py:185][0m |          -0.0010 |         282.5808 |         -29.2354 |
[32m[20221214 14:17:30 @agent_ppo2.py:185][0m |          -0.0013 |         280.9651 |         -29.3657 |
[32m[20221214 14:17:30 @agent_ppo2.py:185][0m |          -0.0021 |         280.4308 |         -29.4171 |
[32m[20221214 14:17:30 @agent_ppo2.py:185][0m |           0.0045 |         281.2663 |         -29.4078 |
[32m[20221214 14:17:30 @agent_ppo2.py:185][0m |           0.0052 |         287.6823 |         -29.4974 |
[32m[20221214 14:17:31 @agent_ppo2.py:185][0m |          -0.0013 |         277.6284 |         -29.4954 |
[32m[20221214 14:17:31 @agent_ppo2.py:185][0m |          -0.0010 |         277.1216 |         -29.5732 |
[32m[20221214 14:17:31 @agent_ppo2.py:185][0m |           0.0008 |         277.5426 |         -29.5173 |
[32m[20221214 14:17:31 @agent_ppo2.py:185][0m |          -0.0021 |         277.9562 |         -29.6068 |
[32m[20221214 14:17:31 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:17:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 845.00
[32m[20221214 14:17:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.74
[32m[20221214 14:17:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.57
[32m[20221214 14:17:31 @agent_ppo2.py:143][0m Total time:      19.48 min
[32m[20221214 14:17:31 @agent_ppo2.py:145][0m 1783808 total steps have happened
[32m[20221214 14:17:31 @agent_ppo2.py:121][0m #------------------------ Iteration 871 --------------------------#
[32m[20221214 14:17:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:31 @agent_ppo2.py:185][0m |          -0.0003 |         275.7047 |         -29.0059 |
[32m[20221214 14:17:31 @agent_ppo2.py:185][0m |          -0.0009 |         270.9043 |         -28.9861 |
[32m[20221214 14:17:31 @agent_ppo2.py:185][0m |          -0.0006 |         268.8393 |         -29.0317 |
[32m[20221214 14:17:32 @agent_ppo2.py:185][0m |           0.0001 |         268.1843 |         -28.9470 |
[32m[20221214 14:17:32 @agent_ppo2.py:185][0m |          -0.0021 |         268.9770 |         -29.0174 |
[32m[20221214 14:17:32 @agent_ppo2.py:185][0m |          -0.0011 |         267.8776 |         -28.9362 |
[32m[20221214 14:17:32 @agent_ppo2.py:185][0m |          -0.0013 |         267.7245 |         -28.9899 |
[32m[20221214 14:17:32 @agent_ppo2.py:185][0m |          -0.0010 |         268.1312 |         -28.9279 |
[32m[20221214 14:17:32 @agent_ppo2.py:185][0m |          -0.0008 |         267.1436 |         -28.7949 |
[32m[20221214 14:17:32 @agent_ppo2.py:185][0m |           0.0069 |         273.7573 |         -28.8755 |
[32m[20221214 14:17:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:17:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 846.58
[32m[20221214 14:17:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.19
[32m[20221214 14:17:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.14
[32m[20221214 14:17:32 @agent_ppo2.py:143][0m Total time:      19.50 min
[32m[20221214 14:17:32 @agent_ppo2.py:145][0m 1785856 total steps have happened
[32m[20221214 14:17:32 @agent_ppo2.py:121][0m #------------------------ Iteration 872 --------------------------#
[32m[20221214 14:17:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:17:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:33 @agent_ppo2.py:185][0m |           0.0108 |         277.3683 |         -29.6205 |
[32m[20221214 14:17:33 @agent_ppo2.py:185][0m |          -0.0020 |         256.6267 |         -29.5652 |
[32m[20221214 14:17:33 @agent_ppo2.py:185][0m |          -0.0007 |         253.3802 |         -29.7239 |
[32m[20221214 14:17:33 @agent_ppo2.py:185][0m |           0.0021 |         252.5914 |         -29.7724 |
[32m[20221214 14:17:33 @agent_ppo2.py:185][0m |          -0.0001 |         248.5213 |         -29.7050 |
[32m[20221214 14:17:33 @agent_ppo2.py:185][0m |          -0.0012 |         247.8560 |         -29.8830 |
[32m[20221214 14:17:33 @agent_ppo2.py:185][0m |          -0.0019 |         245.7402 |         -29.9538 |
[32m[20221214 14:17:33 @agent_ppo2.py:185][0m |          -0.0022 |         245.8687 |         -29.9738 |
[32m[20221214 14:17:33 @agent_ppo2.py:185][0m |           0.0011 |         245.5829 |         -30.0721 |
[32m[20221214 14:17:34 @agent_ppo2.py:185][0m |           0.0084 |         261.4358 |         -30.0628 |
[32m[20221214 14:17:34 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:17:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 846.38
[32m[20221214 14:17:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.36
[32m[20221214 14:17:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.06
[32m[20221214 14:17:34 @agent_ppo2.py:143][0m Total time:      19.53 min
[32m[20221214 14:17:34 @agent_ppo2.py:145][0m 1787904 total steps have happened
[32m[20221214 14:17:34 @agent_ppo2.py:121][0m #------------------------ Iteration 873 --------------------------#
[32m[20221214 14:17:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:17:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:34 @agent_ppo2.py:185][0m |          -0.0009 |         220.8134 |         -30.8062 |
[32m[20221214 14:17:34 @agent_ppo2.py:185][0m |           0.0118 |         225.8421 |         -30.8403 |
[32m[20221214 14:17:34 @agent_ppo2.py:185][0m |          -0.0011 |         212.3438 |         -30.8856 |
[32m[20221214 14:17:34 @agent_ppo2.py:185][0m |          -0.0023 |         209.3968 |         -30.8970 |
[32m[20221214 14:17:34 @agent_ppo2.py:185][0m |          -0.0025 |         205.1197 |         -30.8301 |
[32m[20221214 14:17:34 @agent_ppo2.py:185][0m |          -0.0019 |         203.5909 |         -30.9717 |
[32m[20221214 14:17:34 @agent_ppo2.py:185][0m |          -0.0027 |         203.4258 |         -31.0401 |
[32m[20221214 14:17:35 @agent_ppo2.py:185][0m |          -0.0031 |         202.4475 |         -30.9832 |
[32m[20221214 14:17:35 @agent_ppo2.py:185][0m |          -0.0028 |         201.8544 |         -31.0588 |
[32m[20221214 14:17:35 @agent_ppo2.py:185][0m |          -0.0021 |         201.3175 |         -31.0854 |
[32m[20221214 14:17:35 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:17:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 847.11
[32m[20221214 14:17:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 848.32
[32m[20221214 14:17:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.29
[32m[20221214 14:17:35 @agent_ppo2.py:143][0m Total time:      19.55 min
[32m[20221214 14:17:35 @agent_ppo2.py:145][0m 1789952 total steps have happened
[32m[20221214 14:17:35 @agent_ppo2.py:121][0m #------------------------ Iteration 874 --------------------------#
[32m[20221214 14:17:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:35 @agent_ppo2.py:185][0m |          -0.0019 |         268.9364 |         -30.2590 |
[32m[20221214 14:17:35 @agent_ppo2.py:185][0m |          -0.0041 |         267.1402 |         -30.2301 |
[32m[20221214 14:17:35 @agent_ppo2.py:185][0m |          -0.0013 |         265.3039 |         -30.3616 |
[32m[20221214 14:17:36 @agent_ppo2.py:185][0m |           0.0007 |         265.0308 |         -30.2261 |
[32m[20221214 14:17:36 @agent_ppo2.py:185][0m |          -0.0030 |         264.9286 |         -30.2362 |
[32m[20221214 14:17:36 @agent_ppo2.py:185][0m |           0.0019 |         270.5847 |         -30.2138 |
[32m[20221214 14:17:36 @agent_ppo2.py:185][0m |           0.0025 |         273.5435 |         -30.2169 |
[32m[20221214 14:17:36 @agent_ppo2.py:185][0m |          -0.0018 |         265.6560 |         -30.0860 |
[32m[20221214 14:17:36 @agent_ppo2.py:185][0m |          -0.0020 |         264.3489 |         -30.2951 |
[32m[20221214 14:17:36 @agent_ppo2.py:185][0m |          -0.0024 |         264.3542 |         -30.2788 |
[32m[20221214 14:17:36 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:17:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.75
[32m[20221214 14:17:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.38
[32m[20221214 14:17:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.35
[32m[20221214 14:17:36 @agent_ppo2.py:143][0m Total time:      19.57 min
[32m[20221214 14:17:36 @agent_ppo2.py:145][0m 1792000 total steps have happened
[32m[20221214 14:17:36 @agent_ppo2.py:121][0m #------------------------ Iteration 875 --------------------------#
[32m[20221214 14:17:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:17:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:36 @agent_ppo2.py:185][0m |           0.0002 |         280.6750 |         -29.6403 |
[32m[20221214 14:17:37 @agent_ppo2.py:185][0m |           0.0034 |         274.3776 |         -29.7381 |
[32m[20221214 14:17:37 @agent_ppo2.py:185][0m |          -0.0020 |         267.9975 |         -29.7424 |
[32m[20221214 14:17:37 @agent_ppo2.py:185][0m |           0.0000 |         266.7847 |         -29.7642 |
[32m[20221214 14:17:37 @agent_ppo2.py:185][0m |           0.0099 |         294.0382 |         -29.7799 |
[32m[20221214 14:17:37 @agent_ppo2.py:185][0m |          -0.0017 |         264.9967 |         -29.7093 |
[32m[20221214 14:17:37 @agent_ppo2.py:185][0m |          -0.0005 |         264.6167 |         -29.7220 |
[32m[20221214 14:17:37 @agent_ppo2.py:185][0m |          -0.0021 |         264.5076 |         -29.7449 |
[32m[20221214 14:17:37 @agent_ppo2.py:185][0m |          -0.0035 |         263.7909 |         -29.7607 |
[32m[20221214 14:17:37 @agent_ppo2.py:185][0m |          -0.0018 |         263.5372 |         -29.7616 |
[32m[20221214 14:17:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:17:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.47
[32m[20221214 14:17:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.60
[32m[20221214 14:17:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.13
[32m[20221214 14:17:37 @agent_ppo2.py:143][0m Total time:      19.59 min
[32m[20221214 14:17:37 @agent_ppo2.py:145][0m 1794048 total steps have happened
[32m[20221214 14:17:37 @agent_ppo2.py:121][0m #------------------------ Iteration 876 --------------------------#
[32m[20221214 14:17:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:38 @agent_ppo2.py:185][0m |          -0.0003 |         272.4663 |         -31.2063 |
[32m[20221214 14:17:38 @agent_ppo2.py:185][0m |          -0.0010 |         264.0879 |         -31.0475 |
[32m[20221214 14:17:38 @agent_ppo2.py:185][0m |           0.0097 |         286.7152 |         -30.9694 |
[32m[20221214 14:17:38 @agent_ppo2.py:185][0m |          -0.0028 |         261.4005 |         -31.0194 |
[32m[20221214 14:17:38 @agent_ppo2.py:185][0m |          -0.0045 |         260.6574 |         -30.9168 |
[32m[20221214 14:17:38 @agent_ppo2.py:185][0m |           0.0007 |         263.1928 |         -30.9923 |
[32m[20221214 14:17:38 @agent_ppo2.py:185][0m |          -0.0024 |         259.3791 |         -30.9897 |
[32m[20221214 14:17:39 @agent_ppo2.py:185][0m |           0.0069 |         273.6508 |         -30.9488 |
[32m[20221214 14:17:39 @agent_ppo2.py:185][0m |          -0.0026 |         258.3715 |         -30.9611 |
[32m[20221214 14:17:39 @agent_ppo2.py:185][0m |          -0.0037 |         257.7122 |         -30.9588 |
[32m[20221214 14:17:39 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 14:17:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.78
[32m[20221214 14:17:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.52
[32m[20221214 14:17:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.35
[32m[20221214 14:17:39 @agent_ppo2.py:143][0m Total time:      19.61 min
[32m[20221214 14:17:39 @agent_ppo2.py:145][0m 1796096 total steps have happened
[32m[20221214 14:17:39 @agent_ppo2.py:121][0m #------------------------ Iteration 877 --------------------------#
[32m[20221214 14:17:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:39 @agent_ppo2.py:185][0m |           0.0002 |         232.8382 |         -31.2121 |
[32m[20221214 14:17:39 @agent_ppo2.py:185][0m |          -0.0043 |         226.9449 |         -31.2629 |
[32m[20221214 14:17:39 @agent_ppo2.py:185][0m |          -0.0014 |         226.8020 |         -31.1552 |
[32m[20221214 14:17:40 @agent_ppo2.py:185][0m |           0.0008 |         225.9987 |         -31.2191 |
[32m[20221214 14:17:40 @agent_ppo2.py:185][0m |          -0.0007 |         225.9258 |         -31.2670 |
[32m[20221214 14:17:40 @agent_ppo2.py:185][0m |           0.0006 |         226.1882 |         -31.1820 |
[32m[20221214 14:17:40 @agent_ppo2.py:185][0m |          -0.0032 |         225.4417 |         -31.2177 |
[32m[20221214 14:17:40 @agent_ppo2.py:185][0m |           0.0000 |         227.0578 |         -31.1850 |
[32m[20221214 14:17:40 @agent_ppo2.py:185][0m |          -0.0019 |         225.2330 |         -31.1837 |
[32m[20221214 14:17:40 @agent_ppo2.py:185][0m |           0.0021 |         226.7185 |         -31.1744 |
[32m[20221214 14:17:40 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:17:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.56
[32m[20221214 14:17:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 848.90
[32m[20221214 14:17:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.94
[32m[20221214 14:17:40 @agent_ppo2.py:143][0m Total time:      19.64 min
[32m[20221214 14:17:40 @agent_ppo2.py:145][0m 1798144 total steps have happened
[32m[20221214 14:17:40 @agent_ppo2.py:121][0m #------------------------ Iteration 878 --------------------------#
[32m[20221214 14:17:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:41 @agent_ppo2.py:185][0m |          -0.0002 |         208.1500 |         -30.0488 |
[32m[20221214 14:17:41 @agent_ppo2.py:185][0m |          -0.0017 |         204.4279 |         -29.8653 |
[32m[20221214 14:17:41 @agent_ppo2.py:185][0m |          -0.0032 |         202.4393 |         -29.9990 |
[32m[20221214 14:17:41 @agent_ppo2.py:185][0m |           0.0072 |         210.2244 |         -30.1090 |
[32m[20221214 14:17:41 @agent_ppo2.py:185][0m |           0.0104 |         223.5192 |         -29.9364 |
[32m[20221214 14:17:41 @agent_ppo2.py:185][0m |          -0.0023 |         202.6262 |         -29.8288 |
[32m[20221214 14:17:41 @agent_ppo2.py:185][0m |          -0.0030 |         201.3120 |         -30.0635 |
[32m[20221214 14:17:41 @agent_ppo2.py:185][0m |          -0.0037 |         201.5496 |         -29.9167 |
[32m[20221214 14:17:41 @agent_ppo2.py:185][0m |           0.0012 |         203.7970 |         -30.1254 |
[32m[20221214 14:17:42 @agent_ppo2.py:185][0m |          -0.0000 |         202.1326 |         -30.0118 |
[32m[20221214 14:17:42 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 847.59
[32m[20221214 14:17:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 848.83
[32m[20221214 14:17:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.08
[32m[20221214 14:17:42 @agent_ppo2.py:143][0m Total time:      19.66 min
[32m[20221214 14:17:42 @agent_ppo2.py:145][0m 1800192 total steps have happened
[32m[20221214 14:17:42 @agent_ppo2.py:121][0m #------------------------ Iteration 879 --------------------------#
[32m[20221214 14:17:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:42 @agent_ppo2.py:185][0m |          -0.0026 |         241.8322 |         -29.5969 |
[32m[20221214 14:17:42 @agent_ppo2.py:185][0m |          -0.0027 |         240.0508 |         -29.6397 |
[32m[20221214 14:17:42 @agent_ppo2.py:185][0m |          -0.0040 |         239.6957 |         -29.6001 |
[32m[20221214 14:17:42 @agent_ppo2.py:185][0m |           0.0014 |         250.4250 |         -29.6132 |
[32m[20221214 14:17:42 @agent_ppo2.py:185][0m |          -0.0033 |         238.7385 |         -29.5764 |
[32m[20221214 14:17:42 @agent_ppo2.py:185][0m |           0.0088 |         258.0699 |         -29.5374 |
[32m[20221214 14:17:43 @agent_ppo2.py:185][0m |          -0.0042 |         238.9714 |         -29.4930 |
[32m[20221214 14:17:43 @agent_ppo2.py:185][0m |           0.0027 |         247.1906 |         -29.5532 |
[32m[20221214 14:17:43 @agent_ppo2.py:185][0m |          -0.0045 |         238.5711 |         -29.5255 |
[32m[20221214 14:17:43 @agent_ppo2.py:185][0m |          -0.0050 |         236.9514 |         -29.6143 |
[32m[20221214 14:17:43 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 847.92
[32m[20221214 14:17:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.89
[32m[20221214 14:17:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.24
[32m[20221214 14:17:43 @agent_ppo2.py:143][0m Total time:      19.68 min
[32m[20221214 14:17:43 @agent_ppo2.py:145][0m 1802240 total steps have happened
[32m[20221214 14:17:43 @agent_ppo2.py:121][0m #------------------------ Iteration 880 --------------------------#
[32m[20221214 14:17:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:17:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:43 @agent_ppo2.py:185][0m |          -0.0009 |         240.7032 |         -30.0874 |
[32m[20221214 14:17:43 @agent_ppo2.py:185][0m |          -0.0005 |         238.8696 |         -29.9740 |
[32m[20221214 14:17:44 @agent_ppo2.py:185][0m |          -0.0026 |         235.3157 |         -30.1024 |
[32m[20221214 14:17:44 @agent_ppo2.py:185][0m |          -0.0020 |         234.1190 |         -30.0105 |
[32m[20221214 14:17:44 @agent_ppo2.py:185][0m |          -0.0024 |         233.7148 |         -29.9856 |
[32m[20221214 14:17:44 @agent_ppo2.py:185][0m |           0.0079 |         249.9060 |         -29.9838 |
[32m[20221214 14:17:44 @agent_ppo2.py:185][0m |           0.0055 |         238.4735 |         -30.0981 |
[32m[20221214 14:17:44 @agent_ppo2.py:185][0m |          -0.0037 |         232.3446 |         -29.8834 |
[32m[20221214 14:17:44 @agent_ppo2.py:185][0m |          -0.0018 |         233.4182 |         -29.9344 |
[32m[20221214 14:17:44 @agent_ppo2.py:185][0m |          -0.0021 |         232.7833 |         -29.9320 |
[32m[20221214 14:17:44 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.28
[32m[20221214 14:17:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 848.76
[32m[20221214 14:17:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 859.07
[32m[20221214 14:17:44 @agent_ppo2.py:143][0m Total time:      19.71 min
[32m[20221214 14:17:44 @agent_ppo2.py:145][0m 1804288 total steps have happened
[32m[20221214 14:17:44 @agent_ppo2.py:121][0m #------------------------ Iteration 881 --------------------------#
[32m[20221214 14:17:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:45 @agent_ppo2.py:185][0m |          -0.0016 |         264.9449 |         -29.9719 |
[32m[20221214 14:17:45 @agent_ppo2.py:185][0m |          -0.0039 |         264.4211 |         -30.0917 |
[32m[20221214 14:17:45 @agent_ppo2.py:185][0m |          -0.0027 |         262.6953 |         -30.1063 |
[32m[20221214 14:17:45 @agent_ppo2.py:185][0m |          -0.0022 |         262.2293 |         -30.1891 |
[32m[20221214 14:17:45 @agent_ppo2.py:185][0m |          -0.0020 |         262.0349 |         -30.2800 |
[32m[20221214 14:17:45 @agent_ppo2.py:185][0m |          -0.0045 |         261.1273 |         -30.2220 |
[32m[20221214 14:17:45 @agent_ppo2.py:185][0m |          -0.0038 |         261.6609 |         -30.2475 |
[32m[20221214 14:17:45 @agent_ppo2.py:185][0m |          -0.0012 |         260.9550 |         -30.3167 |
[32m[20221214 14:17:46 @agent_ppo2.py:185][0m |           0.0107 |         282.2290 |         -30.4350 |
[32m[20221214 14:17:46 @agent_ppo2.py:185][0m |          -0.0035 |         263.8175 |         -30.3421 |
[32m[20221214 14:17:46 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:17:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 849.42
[32m[20221214 14:17:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 850.21
[32m[20221214 14:17:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.78
[32m[20221214 14:17:46 @agent_ppo2.py:143][0m Total time:      19.73 min
[32m[20221214 14:17:46 @agent_ppo2.py:145][0m 1806336 total steps have happened
[32m[20221214 14:17:46 @agent_ppo2.py:121][0m #------------------------ Iteration 882 --------------------------#
[32m[20221214 14:17:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:46 @agent_ppo2.py:185][0m |           0.0006 |         280.6135 |         -30.8832 |
[32m[20221214 14:17:46 @agent_ppo2.py:185][0m |          -0.0014 |         278.1518 |         -30.9129 |
[32m[20221214 14:17:46 @agent_ppo2.py:185][0m |          -0.0011 |         276.6571 |         -30.9672 |
[32m[20221214 14:17:46 @agent_ppo2.py:185][0m |           0.0027 |         280.6453 |         -31.0240 |
[32m[20221214 14:17:46 @agent_ppo2.py:185][0m |          -0.0003 |         276.1495 |         -31.0690 |
[32m[20221214 14:17:47 @agent_ppo2.py:185][0m |          -0.0014 |         275.3720 |         -31.0641 |
[32m[20221214 14:17:47 @agent_ppo2.py:185][0m |          -0.0024 |         275.2095 |         -31.2066 |
[32m[20221214 14:17:47 @agent_ppo2.py:185][0m |           0.0123 |         315.7754 |         -31.2501 |
[32m[20221214 14:17:47 @agent_ppo2.py:185][0m |          -0.0018 |         274.3321 |         -31.2493 |
[32m[20221214 14:17:47 @agent_ppo2.py:185][0m |          -0.0008 |         274.7156 |         -31.3248 |
[32m[20221214 14:17:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:17:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 849.37
[32m[20221214 14:17:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 850.41
[32m[20221214 14:17:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.92
[32m[20221214 14:17:47 @agent_ppo2.py:143][0m Total time:      19.75 min
[32m[20221214 14:17:47 @agent_ppo2.py:145][0m 1808384 total steps have happened
[32m[20221214 14:17:47 @agent_ppo2.py:121][0m #------------------------ Iteration 883 --------------------------#
[32m[20221214 14:17:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:47 @agent_ppo2.py:185][0m |          -0.0011 |         259.8116 |         -31.5514 |
[32m[20221214 14:17:48 @agent_ppo2.py:185][0m |           0.0109 |         275.4872 |         -31.4949 |
[32m[20221214 14:17:48 @agent_ppo2.py:185][0m |           0.0038 |         259.6853 |         -31.4754 |
[32m[20221214 14:17:48 @agent_ppo2.py:185][0m |           0.0002 |         257.2715 |         -31.4261 |
[32m[20221214 14:17:48 @agent_ppo2.py:185][0m |          -0.0016 |         257.6887 |         -31.5732 |
[32m[20221214 14:17:48 @agent_ppo2.py:185][0m |          -0.0022 |         257.3558 |         -31.5697 |
[32m[20221214 14:17:48 @agent_ppo2.py:185][0m |           0.0018 |         260.0440 |         -31.5650 |
[32m[20221214 14:17:48 @agent_ppo2.py:185][0m |           0.0088 |         273.5600 |         -31.5387 |
[32m[20221214 14:17:48 @agent_ppo2.py:185][0m |          -0.0004 |         257.0482 |         -31.6411 |
[32m[20221214 14:17:48 @agent_ppo2.py:185][0m |          -0.0017 |         256.4634 |         -31.7248 |
[32m[20221214 14:17:48 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:17:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 847.83
[32m[20221214 14:17:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.09
[32m[20221214 14:17:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.29
[32m[20221214 14:17:49 @agent_ppo2.py:143][0m Total time:      19.77 min
[32m[20221214 14:17:49 @agent_ppo2.py:145][0m 1810432 total steps have happened
[32m[20221214 14:17:49 @agent_ppo2.py:121][0m #------------------------ Iteration 884 --------------------------#
[32m[20221214 14:17:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:49 @agent_ppo2.py:185][0m |          -0.0012 |         243.0502 |         -31.5204 |
[32m[20221214 14:17:49 @agent_ppo2.py:185][0m |           0.0087 |         272.2060 |         -31.5399 |
[32m[20221214 14:17:49 @agent_ppo2.py:185][0m |           0.0072 |         256.0318 |         -31.5453 |
[32m[20221214 14:17:49 @agent_ppo2.py:185][0m |          -0.0035 |         236.4017 |         -31.4858 |
[32m[20221214 14:17:49 @agent_ppo2.py:185][0m |          -0.0032 |         234.5437 |         -31.6207 |
[32m[20221214 14:17:49 @agent_ppo2.py:185][0m |          -0.0035 |         233.7532 |         -31.8785 |
[32m[20221214 14:17:49 @agent_ppo2.py:185][0m |          -0.0035 |         233.9046 |         -31.7298 |
[32m[20221214 14:17:50 @agent_ppo2.py:185][0m |          -0.0034 |         233.7243 |         -31.7518 |
[32m[20221214 14:17:50 @agent_ppo2.py:185][0m |          -0.0036 |         232.3038 |         -31.8592 |
[32m[20221214 14:17:50 @agent_ppo2.py:185][0m |           0.0104 |         256.2748 |         -31.9346 |
[32m[20221214 14:17:50 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:17:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.49
[32m[20221214 14:17:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.22
[32m[20221214 14:17:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.18
[32m[20221214 14:17:50 @agent_ppo2.py:143][0m Total time:      19.80 min
[32m[20221214 14:17:50 @agent_ppo2.py:145][0m 1812480 total steps have happened
[32m[20221214 14:17:50 @agent_ppo2.py:121][0m #------------------------ Iteration 885 --------------------------#
[32m[20221214 14:17:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:50 @agent_ppo2.py:185][0m |          -0.0010 |         263.6825 |         -32.2149 |
[32m[20221214 14:17:50 @agent_ppo2.py:185][0m |          -0.0040 |         262.1347 |         -32.0991 |
[32m[20221214 14:17:50 @agent_ppo2.py:185][0m |          -0.0035 |         260.5858 |         -32.0881 |
[32m[20221214 14:17:51 @agent_ppo2.py:185][0m |          -0.0039 |         260.3749 |         -32.0258 |
[32m[20221214 14:17:51 @agent_ppo2.py:185][0m |          -0.0032 |         259.4918 |         -32.1162 |
[32m[20221214 14:17:51 @agent_ppo2.py:185][0m |          -0.0036 |         258.8906 |         -32.0102 |
[32m[20221214 14:17:51 @agent_ppo2.py:185][0m |          -0.0045 |         258.8785 |         -32.0813 |
[32m[20221214 14:17:51 @agent_ppo2.py:185][0m |          -0.0038 |         258.6670 |         -31.9581 |
[32m[20221214 14:17:51 @agent_ppo2.py:185][0m |          -0.0050 |         258.5531 |         -32.0077 |
[32m[20221214 14:17:51 @agent_ppo2.py:185][0m |          -0.0036 |         259.0733 |         -32.0626 |
[32m[20221214 14:17:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:17:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 847.75
[32m[20221214 14:17:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.18
[32m[20221214 14:17:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.02
[32m[20221214 14:17:51 @agent_ppo2.py:143][0m Total time:      19.82 min
[32m[20221214 14:17:51 @agent_ppo2.py:145][0m 1814528 total steps have happened
[32m[20221214 14:17:51 @agent_ppo2.py:121][0m #------------------------ Iteration 886 --------------------------#
[32m[20221214 14:17:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:17:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:52 @agent_ppo2.py:185][0m |          -0.0010 |         300.6303 |         -32.6409 |
[32m[20221214 14:17:52 @agent_ppo2.py:185][0m |          -0.0021 |         297.9061 |         -32.7385 |
[32m[20221214 14:17:52 @agent_ppo2.py:185][0m |          -0.0013 |         296.6254 |         -32.7086 |
[32m[20221214 14:17:52 @agent_ppo2.py:185][0m |           0.0019 |         299.6086 |         -32.8143 |
[32m[20221214 14:17:52 @agent_ppo2.py:185][0m |          -0.0023 |         295.8791 |         -32.8115 |
[32m[20221214 14:17:52 @agent_ppo2.py:185][0m |          -0.0027 |         295.0998 |         -32.8000 |
[32m[20221214 14:17:52 @agent_ppo2.py:185][0m |          -0.0021 |         294.8572 |         -32.9002 |
[32m[20221214 14:17:52 @agent_ppo2.py:185][0m |          -0.0027 |         293.9684 |         -32.8898 |
[32m[20221214 14:17:52 @agent_ppo2.py:185][0m |          -0.0022 |         294.4458 |         -32.9954 |
[32m[20221214 14:17:52 @agent_ppo2.py:185][0m |          -0.0003 |         294.6960 |         -32.8560 |
[32m[20221214 14:17:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:17:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 847.27
[32m[20221214 14:17:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.35
[32m[20221214 14:17:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.69
[32m[20221214 14:17:52 @agent_ppo2.py:143][0m Total time:      19.84 min
[32m[20221214 14:17:52 @agent_ppo2.py:145][0m 1816576 total steps have happened
[32m[20221214 14:17:52 @agent_ppo2.py:121][0m #------------------------ Iteration 887 --------------------------#
[32m[20221214 14:17:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:17:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:53 @agent_ppo2.py:185][0m |           0.0093 |         304.7483 |         -32.0499 |
[32m[20221214 14:17:53 @agent_ppo2.py:185][0m |          -0.0011 |         285.6236 |         -32.1811 |
[32m[20221214 14:17:53 @agent_ppo2.py:185][0m |          -0.0019 |         282.1366 |         -32.1223 |
[32m[20221214 14:17:53 @agent_ppo2.py:185][0m |          -0.0020 |         280.5959 |         -32.1143 |
[32m[20221214 14:17:53 @agent_ppo2.py:185][0m |          -0.0011 |         280.0242 |         -32.0845 |
[32m[20221214 14:17:53 @agent_ppo2.py:185][0m |          -0.0021 |         278.2822 |         -32.0680 |
[32m[20221214 14:17:53 @agent_ppo2.py:185][0m |          -0.0015 |         277.4582 |         -32.0162 |
[32m[20221214 14:17:53 @agent_ppo2.py:185][0m |           0.0003 |         277.5810 |         -31.9785 |
[32m[20221214 14:17:54 @agent_ppo2.py:185][0m |          -0.0013 |         275.6867 |         -32.0257 |
[32m[20221214 14:17:54 @agent_ppo2.py:185][0m |          -0.0013 |         275.4498 |         -32.0305 |
[32m[20221214 14:17:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:17:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 847.53
[32m[20221214 14:17:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.87
[32m[20221214 14:17:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.87
[32m[20221214 14:17:54 @agent_ppo2.py:143][0m Total time:      19.86 min
[32m[20221214 14:17:54 @agent_ppo2.py:145][0m 1818624 total steps have happened
[32m[20221214 14:17:54 @agent_ppo2.py:121][0m #------------------------ Iteration 888 --------------------------#
[32m[20221214 14:17:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:17:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:54 @agent_ppo2.py:185][0m |           0.0030 |         282.6744 |         -32.7185 |
[32m[20221214 14:17:54 @agent_ppo2.py:185][0m |          -0.0014 |         276.8338 |         -32.6358 |
[32m[20221214 14:17:54 @agent_ppo2.py:185][0m |           0.0019 |         279.4217 |         -32.7019 |
[32m[20221214 14:17:54 @agent_ppo2.py:185][0m |          -0.0004 |         274.8500 |         -32.7017 |
[32m[20221214 14:17:54 @agent_ppo2.py:185][0m |           0.0034 |         279.2315 |         -32.7126 |
[32m[20221214 14:17:55 @agent_ppo2.py:185][0m |          -0.0018 |         274.1947 |         -32.8908 |
[32m[20221214 14:17:55 @agent_ppo2.py:185][0m |          -0.0024 |         273.6480 |         -32.7562 |
[32m[20221214 14:17:55 @agent_ppo2.py:185][0m |          -0.0025 |         274.0241 |         -32.7828 |
[32m[20221214 14:17:55 @agent_ppo2.py:185][0m |          -0.0029 |         272.9469 |         -32.7498 |
[32m[20221214 14:17:55 @agent_ppo2.py:185][0m |          -0.0003 |         272.8573 |         -32.8006 |
[32m[20221214 14:17:55 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:17:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.18
[32m[20221214 14:17:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 850.00
[32m[20221214 14:17:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 858.62
[32m[20221214 14:17:55 @agent_ppo2.py:143][0m Total time:      19.88 min
[32m[20221214 14:17:55 @agent_ppo2.py:145][0m 1820672 total steps have happened
[32m[20221214 14:17:55 @agent_ppo2.py:121][0m #------------------------ Iteration 889 --------------------------#
[32m[20221214 14:17:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:17:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:55 @agent_ppo2.py:185][0m |           0.0008 |         292.9091 |         -32.0947 |
[32m[20221214 14:17:55 @agent_ppo2.py:185][0m |          -0.0009 |         284.8223 |         -32.1534 |
[32m[20221214 14:17:56 @agent_ppo2.py:185][0m |          -0.0025 |         285.1188 |         -32.0583 |
[32m[20221214 14:17:56 @agent_ppo2.py:185][0m |          -0.0020 |         280.8790 |         -32.0730 |
[32m[20221214 14:17:56 @agent_ppo2.py:185][0m |          -0.0018 |         280.5609 |         -31.9796 |
[32m[20221214 14:17:56 @agent_ppo2.py:185][0m |           0.0002 |         278.8257 |         -31.8549 |
[32m[20221214 14:17:56 @agent_ppo2.py:185][0m |          -0.0044 |         277.8907 |         -31.8845 |
[32m[20221214 14:17:56 @agent_ppo2.py:185][0m |           0.0021 |         281.6018 |         -31.8409 |
[32m[20221214 14:17:56 @agent_ppo2.py:185][0m |          -0.0008 |         278.9550 |         -31.9015 |
[32m[20221214 14:17:56 @agent_ppo2.py:185][0m |          -0.0025 |         276.9108 |         -31.7615 |
[32m[20221214 14:17:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:17:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 847.86
[32m[20221214 14:17:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 850.24
[32m[20221214 14:17:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.52
[32m[20221214 14:17:56 @agent_ppo2.py:143][0m Total time:      19.90 min
[32m[20221214 14:17:56 @agent_ppo2.py:145][0m 1822720 total steps have happened
[32m[20221214 14:17:56 @agent_ppo2.py:121][0m #------------------------ Iteration 890 --------------------------#
[32m[20221214 14:17:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:17:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:57 @agent_ppo2.py:185][0m |          -0.0007 |         277.7832 |         -32.2556 |
[32m[20221214 14:17:57 @agent_ppo2.py:185][0m |           0.0100 |         298.2740 |         -32.2596 |
[32m[20221214 14:17:57 @agent_ppo2.py:185][0m |          -0.0002 |         271.4053 |         -32.1167 |
[32m[20221214 14:17:57 @agent_ppo2.py:185][0m |          -0.0021 |         269.7811 |         -32.1508 |
[32m[20221214 14:17:57 @agent_ppo2.py:185][0m |           0.0001 |         269.4754 |         -32.1119 |
[32m[20221214 14:17:57 @agent_ppo2.py:185][0m |           0.0056 |         271.7594 |         -31.9907 |
[32m[20221214 14:17:57 @agent_ppo2.py:185][0m |           0.0016 |         270.8614 |         -32.0332 |
[32m[20221214 14:17:57 @agent_ppo2.py:185][0m |           0.0032 |         271.4094 |         -31.9887 |
[32m[20221214 14:17:57 @agent_ppo2.py:185][0m |          -0.0015 |         267.7626 |         -31.9722 |
[32m[20221214 14:17:57 @agent_ppo2.py:185][0m |           0.0007 |         267.8232 |         -31.8983 |
[32m[20221214 14:17:57 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:17:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 847.33
[32m[20221214 14:17:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.01
[32m[20221214 14:17:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.20
[32m[20221214 14:17:58 @agent_ppo2.py:143][0m Total time:      19.92 min
[32m[20221214 14:17:58 @agent_ppo2.py:145][0m 1824768 total steps have happened
[32m[20221214 14:17:58 @agent_ppo2.py:121][0m #------------------------ Iteration 891 --------------------------#
[32m[20221214 14:17:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:17:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:58 @agent_ppo2.py:185][0m |          -0.0007 |         283.5748 |         -31.4428 |
[32m[20221214 14:17:58 @agent_ppo2.py:185][0m |          -0.0004 |         278.9349 |         -31.4368 |
[32m[20221214 14:17:58 @agent_ppo2.py:185][0m |          -0.0007 |         276.7644 |         -31.6644 |
[32m[20221214 14:17:58 @agent_ppo2.py:185][0m |          -0.0025 |         276.1724 |         -31.6203 |
[32m[20221214 14:17:58 @agent_ppo2.py:185][0m |          -0.0004 |         275.9603 |         -31.6333 |
[32m[20221214 14:17:58 @agent_ppo2.py:185][0m |           0.0016 |         276.9755 |         -31.8411 |
[32m[20221214 14:17:59 @agent_ppo2.py:185][0m |           0.0101 |         290.1147 |         -31.8950 |
[32m[20221214 14:17:59 @agent_ppo2.py:185][0m |          -0.0017 |         275.1591 |         -31.9240 |
[32m[20221214 14:17:59 @agent_ppo2.py:185][0m |          -0.0018 |         274.6953 |         -31.9254 |
[32m[20221214 14:17:59 @agent_ppo2.py:185][0m |          -0.0018 |         274.8676 |         -31.9740 |
[32m[20221214 14:17:59 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:17:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 847.58
[32m[20221214 14:17:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.84
[32m[20221214 14:17:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.90
[32m[20221214 14:17:59 @agent_ppo2.py:143][0m Total time:      19.95 min
[32m[20221214 14:17:59 @agent_ppo2.py:145][0m 1826816 total steps have happened
[32m[20221214 14:17:59 @agent_ppo2.py:121][0m #------------------------ Iteration 892 --------------------------#
[32m[20221214 14:17:59 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:17:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:17:59 @agent_ppo2.py:185][0m |          -0.0001 |         265.8536 |         -32.2140 |
[32m[20221214 14:17:59 @agent_ppo2.py:185][0m |          -0.0010 |         262.0283 |         -32.2137 |
[32m[20221214 14:18:00 @agent_ppo2.py:185][0m |           0.0012 |         261.2662 |         -32.1154 |
[32m[20221214 14:18:00 @agent_ppo2.py:185][0m |          -0.0029 |         261.2385 |         -32.2140 |
[32m[20221214 14:18:00 @agent_ppo2.py:185][0m |           0.0019 |         260.2808 |         -32.2358 |
[32m[20221214 14:18:00 @agent_ppo2.py:185][0m |          -0.0016 |         260.1625 |         -32.2203 |
[32m[20221214 14:18:00 @agent_ppo2.py:185][0m |          -0.0019 |         259.9954 |         -32.1793 |
[32m[20221214 14:18:00 @agent_ppo2.py:185][0m |          -0.0006 |         259.6576 |         -32.2013 |
[32m[20221214 14:18:00 @agent_ppo2.py:185][0m |          -0.0018 |         259.4630 |         -32.2618 |
[32m[20221214 14:18:00 @agent_ppo2.py:185][0m |          -0.0027 |         259.4163 |         -32.1917 |
[32m[20221214 14:18:00 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:18:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 848.29
[32m[20221214 14:18:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 850.07
[32m[20221214 14:18:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.20
[32m[20221214 14:18:00 @agent_ppo2.py:143][0m Total time:      19.97 min
[32m[20221214 14:18:00 @agent_ppo2.py:145][0m 1828864 total steps have happened
[32m[20221214 14:18:00 @agent_ppo2.py:121][0m #------------------------ Iteration 893 --------------------------#
[32m[20221214 14:18:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:18:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:01 @agent_ppo2.py:185][0m |           0.0015 |         249.3655 |         -31.9707 |
[32m[20221214 14:18:01 @agent_ppo2.py:185][0m |          -0.0023 |         246.5481 |         -31.8772 |
[32m[20221214 14:18:01 @agent_ppo2.py:185][0m |          -0.0020 |         245.5899 |         -31.9704 |
[32m[20221214 14:18:01 @agent_ppo2.py:185][0m |           0.0006 |         244.9915 |         -31.9728 |
[32m[20221214 14:18:01 @agent_ppo2.py:185][0m |          -0.0011 |         243.3678 |         -32.0085 |
[32m[20221214 14:18:01 @agent_ppo2.py:185][0m |          -0.0020 |         242.7019 |         -32.0934 |
[32m[20221214 14:18:01 @agent_ppo2.py:185][0m |           0.0011 |         244.3269 |         -32.1370 |
[32m[20221214 14:18:01 @agent_ppo2.py:185][0m |           0.0055 |         249.2283 |         -32.1163 |
[32m[20221214 14:18:01 @agent_ppo2.py:185][0m |          -0.0020 |         242.7620 |         -32.0040 |
[32m[20221214 14:18:02 @agent_ppo2.py:185][0m |           0.0006 |         242.2846 |         -32.1691 |
[32m[20221214 14:18:02 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:18:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 847.48
[32m[20221214 14:18:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.17
[32m[20221214 14:18:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.08
[32m[20221214 14:18:02 @agent_ppo2.py:143][0m Total time:      19.99 min
[32m[20221214 14:18:02 @agent_ppo2.py:145][0m 1830912 total steps have happened
[32m[20221214 14:18:02 @agent_ppo2.py:121][0m #------------------------ Iteration 894 --------------------------#
[32m[20221214 14:18:02 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221214 14:18:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:02 @agent_ppo2.py:185][0m |          -0.0027 |         232.1375 |         -33.2829 |
[32m[20221214 14:18:02 @agent_ppo2.py:185][0m |          -0.0029 |         230.0435 |         -33.2853 |
[32m[20221214 14:18:02 @agent_ppo2.py:185][0m |          -0.0024 |         229.4551 |         -33.2881 |
[32m[20221214 14:18:03 @agent_ppo2.py:185][0m |          -0.0030 |         228.3442 |         -33.3121 |
[32m[20221214 14:18:03 @agent_ppo2.py:185][0m |          -0.0028 |         228.5134 |         -33.2228 |
[32m[20221214 14:18:03 @agent_ppo2.py:185][0m |          -0.0022 |         228.0567 |         -33.2453 |
[32m[20221214 14:18:03 @agent_ppo2.py:185][0m |          -0.0028 |         228.3833 |         -33.2325 |
[32m[20221214 14:18:03 @agent_ppo2.py:185][0m |           0.0042 |         233.6339 |         -33.1304 |
[32m[20221214 14:18:03 @agent_ppo2.py:185][0m |          -0.0001 |         229.0884 |         -33.1194 |
[32m[20221214 14:18:03 @agent_ppo2.py:185][0m |          -0.0021 |         227.8500 |         -33.2270 |
[32m[20221214 14:18:03 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 14:18:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 847.30
[32m[20221214 14:18:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 848.20
[32m[20221214 14:18:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.62
[32m[20221214 14:18:03 @agent_ppo2.py:143][0m Total time:      20.02 min
[32m[20221214 14:18:03 @agent_ppo2.py:145][0m 1832960 total steps have happened
[32m[20221214 14:18:03 @agent_ppo2.py:121][0m #------------------------ Iteration 895 --------------------------#
[32m[20221214 14:18:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:04 @agent_ppo2.py:185][0m |          -0.0023 |         256.5381 |         -33.1887 |
[32m[20221214 14:18:04 @agent_ppo2.py:185][0m |          -0.0045 |         251.7348 |         -33.1614 |
[32m[20221214 14:18:04 @agent_ppo2.py:185][0m |          -0.0019 |         253.3823 |         -33.2235 |
[32m[20221214 14:18:04 @agent_ppo2.py:185][0m |          -0.0020 |         250.3350 |         -33.2333 |
[32m[20221214 14:18:04 @agent_ppo2.py:185][0m |          -0.0026 |         249.4446 |         -33.2640 |
[32m[20221214 14:18:04 @agent_ppo2.py:185][0m |          -0.0009 |         249.1795 |         -33.1168 |
[32m[20221214 14:18:04 @agent_ppo2.py:185][0m |          -0.0033 |         249.0942 |         -33.1536 |
[32m[20221214 14:18:04 @agent_ppo2.py:185][0m |          -0.0031 |         249.2561 |         -33.3058 |
[32m[20221214 14:18:04 @agent_ppo2.py:185][0m |           0.0085 |         276.7268 |         -33.2883 |
[32m[20221214 14:18:05 @agent_ppo2.py:185][0m |          -0.0024 |         249.3907 |         -33.3690 |
[32m[20221214 14:18:05 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:18:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 846.04
[32m[20221214 14:18:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 848.59
[32m[20221214 14:18:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.30
[32m[20221214 14:18:05 @agent_ppo2.py:143][0m Total time:      20.04 min
[32m[20221214 14:18:05 @agent_ppo2.py:145][0m 1835008 total steps have happened
[32m[20221214 14:18:05 @agent_ppo2.py:121][0m #------------------------ Iteration 896 --------------------------#
[32m[20221214 14:18:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:05 @agent_ppo2.py:185][0m |          -0.0013 |         262.2338 |         -32.1327 |
[32m[20221214 14:18:05 @agent_ppo2.py:185][0m |           0.0140 |         301.0362 |         -32.2175 |
[32m[20221214 14:18:05 @agent_ppo2.py:185][0m |          -0.0022 |         261.1800 |         -32.2494 |
[32m[20221214 14:18:05 @agent_ppo2.py:185][0m |           0.0030 |         263.8578 |         -32.2425 |
[32m[20221214 14:18:05 @agent_ppo2.py:185][0m |           0.0114 |         293.8482 |         -32.4288 |
[32m[20221214 14:18:06 @agent_ppo2.py:185][0m |          -0.0029 |         260.4358 |         -32.3802 |
[32m[20221214 14:18:06 @agent_ppo2.py:185][0m |          -0.0013 |         260.2318 |         -32.4887 |
[32m[20221214 14:18:06 @agent_ppo2.py:185][0m |          -0.0020 |         260.4502 |         -32.5916 |
[32m[20221214 14:18:06 @agent_ppo2.py:185][0m |          -0.0022 |         259.6171 |         -32.5377 |
[32m[20221214 14:18:06 @agent_ppo2.py:185][0m |          -0.0028 |         259.5890 |         -32.5704 |
[32m[20221214 14:18:06 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:18:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 847.50
[32m[20221214 14:18:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 849.67
[32m[20221214 14:18:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 857.61
[32m[20221214 14:18:06 @agent_ppo2.py:143][0m Total time:      20.07 min
[32m[20221214 14:18:06 @agent_ppo2.py:145][0m 1837056 total steps have happened
[32m[20221214 14:18:06 @agent_ppo2.py:121][0m #------------------------ Iteration 897 --------------------------#
[32m[20221214 14:18:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:06 @agent_ppo2.py:185][0m |          -0.0012 |         245.2934 |         -33.2987 |
[32m[20221214 14:18:07 @agent_ppo2.py:185][0m |          -0.0015 |         243.3776 |         -33.2866 |
[32m[20221214 14:18:07 @agent_ppo2.py:185][0m |          -0.0022 |         243.2291 |         -33.3239 |
[32m[20221214 14:18:07 @agent_ppo2.py:185][0m |           0.0122 |         272.8391 |         -33.2688 |
[32m[20221214 14:18:07 @agent_ppo2.py:185][0m |          -0.0033 |         243.9672 |         -33.3176 |
[32m[20221214 14:18:07 @agent_ppo2.py:185][0m |           0.0021 |         247.2806 |         -33.2864 |
[32m[20221214 14:18:07 @agent_ppo2.py:185][0m |          -0.0041 |         242.0430 |         -33.2916 |
[32m[20221214 14:18:07 @agent_ppo2.py:185][0m |          -0.0029 |         241.4605 |         -33.3298 |
[32m[20221214 14:18:07 @agent_ppo2.py:185][0m |           0.0027 |         243.6378 |         -33.2308 |
[32m[20221214 14:18:07 @agent_ppo2.py:185][0m |           0.0002 |         241.9034 |         -33.2429 |
[32m[20221214 14:18:07 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:18:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 846.88
[32m[20221214 14:18:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.65
[32m[20221214 14:18:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 856.16
[32m[20221214 14:18:07 @agent_ppo2.py:143][0m Total time:      20.09 min
[32m[20221214 14:18:07 @agent_ppo2.py:145][0m 1839104 total steps have happened
[32m[20221214 14:18:07 @agent_ppo2.py:121][0m #------------------------ Iteration 898 --------------------------#
[32m[20221214 14:18:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:08 @agent_ppo2.py:185][0m |          -0.0014 |         247.3755 |         -33.2251 |
[32m[20221214 14:18:08 @agent_ppo2.py:185][0m |          -0.0007 |         243.9811 |         -33.2435 |
[32m[20221214 14:18:08 @agent_ppo2.py:185][0m |          -0.0014 |         242.4083 |         -33.2928 |
[32m[20221214 14:18:08 @agent_ppo2.py:185][0m |          -0.0018 |         241.4370 |         -33.3449 |
[32m[20221214 14:18:08 @agent_ppo2.py:185][0m |          -0.0010 |         240.9896 |         -33.3659 |
[32m[20221214 14:18:08 @agent_ppo2.py:185][0m |           0.0004 |         240.7803 |         -33.4738 |
[32m[20221214 14:18:08 @agent_ppo2.py:185][0m |          -0.0002 |         239.4416 |         -33.4968 |
[32m[20221214 14:18:08 @agent_ppo2.py:185][0m |           0.0047 |         243.5261 |         -33.5758 |
[32m[20221214 14:18:09 @agent_ppo2.py:185][0m |          -0.0023 |         239.0969 |         -33.5504 |
[32m[20221214 14:18:09 @agent_ppo2.py:185][0m |          -0.0014 |         238.6782 |         -33.7001 |
[32m[20221214 14:18:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:18:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.68
[32m[20221214 14:18:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.73
[32m[20221214 14:18:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.04
[32m[20221214 14:18:09 @agent_ppo2.py:143][0m Total time:      20.11 min
[32m[20221214 14:18:09 @agent_ppo2.py:145][0m 1841152 total steps have happened
[32m[20221214 14:18:09 @agent_ppo2.py:121][0m #------------------------ Iteration 899 --------------------------#
[32m[20221214 14:18:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:09 @agent_ppo2.py:185][0m |           0.0110 |         292.3665 |         -34.8000 |
[32m[20221214 14:18:09 @agent_ppo2.py:185][0m |           0.0031 |         264.0605 |         -34.6148 |
[32m[20221214 14:18:09 @agent_ppo2.py:185][0m |          -0.0025 |         257.1931 |         -34.6144 |
[32m[20221214 14:18:09 @agent_ppo2.py:185][0m |           0.0040 |         262.8065 |         -34.7587 |
[32m[20221214 14:18:09 @agent_ppo2.py:185][0m |          -0.0024 |         256.3030 |         -34.7733 |
[32m[20221214 14:18:09 @agent_ppo2.py:185][0m |          -0.0021 |         256.2620 |         -34.7378 |
[32m[20221214 14:18:10 @agent_ppo2.py:185][0m |          -0.0022 |         255.9301 |         -34.6456 |
[32m[20221214 14:18:10 @agent_ppo2.py:185][0m |          -0.0029 |         255.6654 |         -34.6583 |
[32m[20221214 14:18:10 @agent_ppo2.py:185][0m |          -0.0016 |         256.4843 |         -34.7557 |
[32m[20221214 14:18:10 @agent_ppo2.py:185][0m |          -0.0024 |         255.6832 |         -34.6455 |
[32m[20221214 14:18:10 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:18:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 845.94
[32m[20221214 14:18:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.49
[32m[20221214 14:18:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.45
[32m[20221214 14:18:10 @agent_ppo2.py:143][0m Total time:      20.13 min
[32m[20221214 14:18:10 @agent_ppo2.py:145][0m 1843200 total steps have happened
[32m[20221214 14:18:10 @agent_ppo2.py:121][0m #------------------------ Iteration 900 --------------------------#
[32m[20221214 14:18:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:18:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:10 @agent_ppo2.py:185][0m |          -0.0000 |         229.2883 |         -34.2993 |
[32m[20221214 14:18:10 @agent_ppo2.py:185][0m |          -0.0010 |         224.6980 |         -34.2783 |
[32m[20221214 14:18:11 @agent_ppo2.py:185][0m |          -0.0023 |         222.3771 |         -34.2489 |
[32m[20221214 14:18:11 @agent_ppo2.py:185][0m |          -0.0035 |         221.6389 |         -34.2300 |
[32m[20221214 14:18:11 @agent_ppo2.py:185][0m |          -0.0020 |         222.1659 |         -34.0526 |
[32m[20221214 14:18:11 @agent_ppo2.py:185][0m |           0.0001 |         220.9550 |         -34.2309 |
[32m[20221214 14:18:11 @agent_ppo2.py:185][0m |          -0.0024 |         220.1323 |         -34.2155 |
[32m[20221214 14:18:11 @agent_ppo2.py:185][0m |          -0.0038 |         221.0733 |         -34.1953 |
[32m[20221214 14:18:11 @agent_ppo2.py:185][0m |          -0.0023 |         219.7400 |         -34.1708 |
[32m[20221214 14:18:11 @agent_ppo2.py:185][0m |          -0.0013 |         219.6045 |         -34.2218 |
[32m[20221214 14:18:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:18:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.32
[32m[20221214 14:18:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.11
[32m[20221214 14:18:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.96
[32m[20221214 14:18:11 @agent_ppo2.py:143][0m Total time:      20.15 min
[32m[20221214 14:18:11 @agent_ppo2.py:145][0m 1845248 total steps have happened
[32m[20221214 14:18:11 @agent_ppo2.py:121][0m #------------------------ Iteration 901 --------------------------#
[32m[20221214 14:18:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:18:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:12 @agent_ppo2.py:185][0m |          -0.0024 |         243.8118 |         -32.9415 |
[32m[20221214 14:18:12 @agent_ppo2.py:185][0m |          -0.0027 |         238.0434 |         -32.9873 |
[32m[20221214 14:18:12 @agent_ppo2.py:185][0m |          -0.0026 |         236.1212 |         -32.9095 |
[32m[20221214 14:18:12 @agent_ppo2.py:185][0m |          -0.0042 |         233.9244 |         -32.9387 |
[32m[20221214 14:18:12 @agent_ppo2.py:185][0m |           0.0010 |         232.8363 |         -32.8850 |
[32m[20221214 14:18:12 @agent_ppo2.py:185][0m |          -0.0017 |         232.0440 |         -32.8788 |
[32m[20221214 14:18:12 @agent_ppo2.py:185][0m |          -0.0023 |         231.4961 |         -33.0307 |
[32m[20221214 14:18:12 @agent_ppo2.py:185][0m |          -0.0013 |         229.6966 |         -32.9357 |
[32m[20221214 14:18:12 @agent_ppo2.py:185][0m |          -0.0028 |         231.1905 |         -32.9872 |
[32m[20221214 14:18:12 @agent_ppo2.py:185][0m |          -0.0023 |         230.6894 |         -32.8095 |
[32m[20221214 14:18:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:18:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.33
[32m[20221214 14:18:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.33
[32m[20221214 14:18:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 855.65
[32m[20221214 14:18:13 @agent_ppo2.py:143][0m Total time:      20.17 min
[32m[20221214 14:18:13 @agent_ppo2.py:145][0m 1847296 total steps have happened
[32m[20221214 14:18:13 @agent_ppo2.py:121][0m #------------------------ Iteration 902 --------------------------#
[32m[20221214 14:18:13 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:18:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:13 @agent_ppo2.py:185][0m |          -0.0025 |         213.1469 |         -33.6477 |
[32m[20221214 14:18:13 @agent_ppo2.py:185][0m |          -0.0056 |         208.2270 |         -33.6160 |
[32m[20221214 14:18:13 @agent_ppo2.py:185][0m |          -0.0049 |         206.4210 |         -33.6256 |
[32m[20221214 14:18:13 @agent_ppo2.py:185][0m |          -0.0066 |         205.7713 |         -33.5975 |
[32m[20221214 14:18:13 @agent_ppo2.py:185][0m |           0.0062 |         219.4470 |         -33.7447 |
[32m[20221214 14:18:13 @agent_ppo2.py:185][0m |          -0.0046 |         204.8115 |         -33.6244 |
[32m[20221214 14:18:14 @agent_ppo2.py:185][0m |           0.0032 |         210.8469 |         -33.5671 |
[32m[20221214 14:18:14 @agent_ppo2.py:185][0m |          -0.0049 |         204.8822 |         -33.6341 |
[32m[20221214 14:18:14 @agent_ppo2.py:185][0m |          -0.0054 |         203.9087 |         -33.5762 |
[32m[20221214 14:18:14 @agent_ppo2.py:185][0m |          -0.0059 |         203.8193 |         -33.6282 |
[32m[20221214 14:18:14 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:18:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 844.83
[32m[20221214 14:18:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.76
[32m[20221214 14:18:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.67
[32m[20221214 14:18:14 @agent_ppo2.py:143][0m Total time:      20.20 min
[32m[20221214 14:18:14 @agent_ppo2.py:145][0m 1849344 total steps have happened
[32m[20221214 14:18:14 @agent_ppo2.py:121][0m #------------------------ Iteration 903 --------------------------#
[32m[20221214 14:18:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:14 @agent_ppo2.py:185][0m |          -0.0034 |         226.4355 |         -34.3276 |
[32m[20221214 14:18:14 @agent_ppo2.py:185][0m |           0.0033 |         231.5368 |         -34.3326 |
[32m[20221214 14:18:14 @agent_ppo2.py:185][0m |          -0.0019 |         221.1572 |         -34.1976 |
[32m[20221214 14:18:15 @agent_ppo2.py:185][0m |           0.0143 |         245.2917 |         -34.4544 |
[32m[20221214 14:18:15 @agent_ppo2.py:185][0m |          -0.0039 |         220.4925 |         -34.1979 |
[32m[20221214 14:18:15 @agent_ppo2.py:185][0m |           0.0040 |         230.6303 |         -34.5516 |
[32m[20221214 14:18:15 @agent_ppo2.py:185][0m |          -0.0056 |         218.3359 |         -34.3858 |
[32m[20221214 14:18:15 @agent_ppo2.py:185][0m |          -0.0060 |         217.4495 |         -34.4070 |
[32m[20221214 14:18:15 @agent_ppo2.py:185][0m |          -0.0013 |         219.5538 |         -34.5455 |
[32m[20221214 14:18:15 @agent_ppo2.py:185][0m |          -0.0049 |         216.1816 |         -34.4910 |
[32m[20221214 14:18:15 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:18:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 844.23
[32m[20221214 14:18:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.89
[32m[20221214 14:18:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.77
[32m[20221214 14:18:15 @agent_ppo2.py:143][0m Total time:      20.22 min
[32m[20221214 14:18:15 @agent_ppo2.py:145][0m 1851392 total steps have happened
[32m[20221214 14:18:15 @agent_ppo2.py:121][0m #------------------------ Iteration 904 --------------------------#
[32m[20221214 14:18:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:16 @agent_ppo2.py:185][0m |          -0.0022 |         278.0174 |         -35.4224 |
[32m[20221214 14:18:16 @agent_ppo2.py:185][0m |          -0.0017 |         275.9240 |         -35.4993 |
[32m[20221214 14:18:16 @agent_ppo2.py:185][0m |          -0.0041 |         274.3589 |         -35.3612 |
[32m[20221214 14:18:16 @agent_ppo2.py:185][0m |          -0.0013 |         273.4040 |         -35.3867 |
[32m[20221214 14:18:16 @agent_ppo2.py:185][0m |           0.0027 |         278.9893 |         -35.3058 |
[32m[20221214 14:18:16 @agent_ppo2.py:185][0m |           0.0058 |         285.3929 |         -35.2241 |
[32m[20221214 14:18:16 @agent_ppo2.py:185][0m |          -0.0025 |         272.1415 |         -35.2424 |
[32m[20221214 14:18:16 @agent_ppo2.py:185][0m |          -0.0040 |         271.4519 |         -35.2485 |
[32m[20221214 14:18:16 @agent_ppo2.py:185][0m |          -0.0020 |         271.9203 |         -35.2162 |
[32m[20221214 14:18:17 @agent_ppo2.py:185][0m |           0.0062 |         283.2386 |         -35.2834 |
[32m[20221214 14:18:17 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:18:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.27
[32m[20221214 14:18:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.16
[32m[20221214 14:18:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.99
[32m[20221214 14:18:17 @agent_ppo2.py:143][0m Total time:      20.24 min
[32m[20221214 14:18:17 @agent_ppo2.py:145][0m 1853440 total steps have happened
[32m[20221214 14:18:17 @agent_ppo2.py:121][0m #------------------------ Iteration 905 --------------------------#
[32m[20221214 14:18:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:17 @agent_ppo2.py:185][0m |          -0.0015 |         259.5102 |         -34.3829 |
[32m[20221214 14:18:17 @agent_ppo2.py:185][0m |          -0.0018 |         248.9272 |         -34.4235 |
[32m[20221214 14:18:17 @agent_ppo2.py:185][0m |          -0.0025 |         246.4874 |         -34.4508 |
[32m[20221214 14:18:17 @agent_ppo2.py:185][0m |          -0.0014 |         244.2765 |         -34.4944 |
[32m[20221214 14:18:17 @agent_ppo2.py:185][0m |          -0.0031 |         243.4411 |         -34.4608 |
[32m[20221214 14:18:18 @agent_ppo2.py:185][0m |          -0.0034 |         241.8559 |         -34.5348 |
[32m[20221214 14:18:18 @agent_ppo2.py:185][0m |          -0.0020 |         241.8100 |         -34.5598 |
[32m[20221214 14:18:18 @agent_ppo2.py:185][0m |          -0.0020 |         240.3688 |         -34.5343 |
[32m[20221214 14:18:18 @agent_ppo2.py:185][0m |           0.0029 |         242.9953 |         -34.5898 |
[32m[20221214 14:18:18 @agent_ppo2.py:185][0m |          -0.0005 |         239.6058 |         -34.6713 |
[32m[20221214 14:18:18 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:18:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.36
[32m[20221214 14:18:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.16
[32m[20221214 14:18:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.38
[32m[20221214 14:18:18 @agent_ppo2.py:143][0m Total time:      20.27 min
[32m[20221214 14:18:18 @agent_ppo2.py:145][0m 1855488 total steps have happened
[32m[20221214 14:18:18 @agent_ppo2.py:121][0m #------------------------ Iteration 906 --------------------------#
[32m[20221214 14:18:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:18 @agent_ppo2.py:185][0m |          -0.0016 |         227.7816 |         -34.1823 |
[32m[20221214 14:18:19 @agent_ppo2.py:185][0m |           0.0005 |         220.1753 |         -34.1971 |
[32m[20221214 14:18:19 @agent_ppo2.py:185][0m |          -0.0024 |         214.6122 |         -34.2854 |
[32m[20221214 14:18:19 @agent_ppo2.py:185][0m |           0.0022 |         214.9336 |         -34.3117 |
[32m[20221214 14:18:19 @agent_ppo2.py:185][0m |           0.0054 |         225.4050 |         -34.4022 |
[32m[20221214 14:18:19 @agent_ppo2.py:185][0m |          -0.0036 |         213.6389 |         -34.3519 |
[32m[20221214 14:18:19 @agent_ppo2.py:185][0m |           0.0098 |         231.8464 |         -34.2078 |
[32m[20221214 14:18:19 @agent_ppo2.py:185][0m |          -0.0010 |         211.4003 |         -34.2695 |
[32m[20221214 14:18:19 @agent_ppo2.py:185][0m |          -0.0013 |         211.5652 |         -34.2984 |
[32m[20221214 14:18:19 @agent_ppo2.py:185][0m |          -0.0023 |         210.5262 |         -34.2306 |
[32m[20221214 14:18:19 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:18:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.82
[32m[20221214 14:18:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.48
[32m[20221214 14:18:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.95
[32m[20221214 14:18:19 @agent_ppo2.py:143][0m Total time:      20.29 min
[32m[20221214 14:18:19 @agent_ppo2.py:145][0m 1857536 total steps have happened
[32m[20221214 14:18:19 @agent_ppo2.py:121][0m #------------------------ Iteration 907 --------------------------#
[32m[20221214 14:18:20 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:18:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:20 @agent_ppo2.py:185][0m |          -0.0017 |         240.4583 |         -34.5323 |
[32m[20221214 14:18:20 @agent_ppo2.py:185][0m |          -0.0011 |         239.2875 |         -34.5420 |
[32m[20221214 14:18:20 @agent_ppo2.py:185][0m |          -0.0047 |         237.2243 |         -34.5636 |
[32m[20221214 14:18:20 @agent_ppo2.py:185][0m |          -0.0047 |         234.7597 |         -34.6251 |
[32m[20221214 14:18:20 @agent_ppo2.py:185][0m |          -0.0024 |         234.5174 |         -34.6803 |
[32m[20221214 14:18:20 @agent_ppo2.py:185][0m |          -0.0026 |         233.5931 |         -34.7459 |
[32m[20221214 14:18:20 @agent_ppo2.py:185][0m |          -0.0031 |         231.9386 |         -34.7629 |
[32m[20221214 14:18:20 @agent_ppo2.py:185][0m |          -0.0039 |         231.2921 |         -34.7354 |
[32m[20221214 14:18:21 @agent_ppo2.py:185][0m |          -0.0028 |         231.0969 |         -34.8605 |
[32m[20221214 14:18:21 @agent_ppo2.py:185][0m |          -0.0031 |         230.3798 |         -34.8575 |
[32m[20221214 14:18:21 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:18:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.62
[32m[20221214 14:18:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.25
[32m[20221214 14:18:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.31
[32m[20221214 14:18:21 @agent_ppo2.py:143][0m Total time:      20.31 min
[32m[20221214 14:18:21 @agent_ppo2.py:145][0m 1859584 total steps have happened
[32m[20221214 14:18:21 @agent_ppo2.py:121][0m #------------------------ Iteration 908 --------------------------#
[32m[20221214 14:18:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:21 @agent_ppo2.py:185][0m |          -0.0032 |         263.8299 |         -35.6170 |
[32m[20221214 14:18:21 @agent_ppo2.py:185][0m |          -0.0045 |         259.1048 |         -35.4653 |
[32m[20221214 14:18:21 @agent_ppo2.py:185][0m |          -0.0023 |         257.2548 |         -35.4244 |
[32m[20221214 14:18:21 @agent_ppo2.py:185][0m |          -0.0044 |         256.4117 |         -35.3966 |
[32m[20221214 14:18:22 @agent_ppo2.py:185][0m |          -0.0046 |         255.3571 |         -35.4514 |
[32m[20221214 14:18:22 @agent_ppo2.py:185][0m |          -0.0040 |         255.3929 |         -35.2483 |
[32m[20221214 14:18:22 @agent_ppo2.py:185][0m |           0.0005 |         258.2447 |         -35.3302 |
[32m[20221214 14:18:22 @agent_ppo2.py:185][0m |          -0.0015 |         255.7820 |         -35.2471 |
[32m[20221214 14:18:22 @agent_ppo2.py:185][0m |          -0.0040 |         254.2783 |         -35.1988 |
[32m[20221214 14:18:22 @agent_ppo2.py:185][0m |           0.0237 |         324.2344 |         -35.1317 |
[32m[20221214 14:18:22 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:18:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.80
[32m[20221214 14:18:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.25
[32m[20221214 14:18:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.89
[32m[20221214 14:18:22 @agent_ppo2.py:143][0m Total time:      20.33 min
[32m[20221214 14:18:22 @agent_ppo2.py:145][0m 1861632 total steps have happened
[32m[20221214 14:18:22 @agent_ppo2.py:121][0m #------------------------ Iteration 909 --------------------------#
[32m[20221214 14:18:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:18:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:22 @agent_ppo2.py:185][0m |           0.0028 |         246.2540 |         -33.7239 |
[32m[20221214 14:18:22 @agent_ppo2.py:185][0m |          -0.0003 |         233.1675 |         -34.1787 |
[32m[20221214 14:18:23 @agent_ppo2.py:185][0m |          -0.0007 |         227.1559 |         -34.2369 |
[32m[20221214 14:18:23 @agent_ppo2.py:185][0m |           0.0012 |         225.1679 |         -34.2447 |
[32m[20221214 14:18:23 @agent_ppo2.py:185][0m |           0.0012 |         223.7726 |         -34.0420 |
[32m[20221214 14:18:23 @agent_ppo2.py:185][0m |          -0.0009 |         219.9274 |         -34.0916 |
[32m[20221214 14:18:23 @agent_ppo2.py:185][0m |           0.0041 |         221.5629 |         -34.4196 |
[32m[20221214 14:18:23 @agent_ppo2.py:185][0m |          -0.0020 |         218.7089 |         -34.2666 |
[32m[20221214 14:18:23 @agent_ppo2.py:185][0m |           0.0086 |         229.6161 |         -34.4930 |
[32m[20221214 14:18:23 @agent_ppo2.py:185][0m |           0.0041 |         217.2215 |         -34.4934 |
[32m[20221214 14:18:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:18:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.35
[32m[20221214 14:18:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.62
[32m[20221214 14:18:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.98
[32m[20221214 14:18:23 @agent_ppo2.py:143][0m Total time:      20.35 min
[32m[20221214 14:18:23 @agent_ppo2.py:145][0m 1863680 total steps have happened
[32m[20221214 14:18:23 @agent_ppo2.py:121][0m #------------------------ Iteration 910 --------------------------#
[32m[20221214 14:18:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:18:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:24 @agent_ppo2.py:185][0m |          -0.0009 |         240.2477 |         -35.2192 |
[32m[20221214 14:18:24 @agent_ppo2.py:185][0m |          -0.0001 |         234.3559 |         -35.2213 |
[32m[20221214 14:18:24 @agent_ppo2.py:185][0m |          -0.0043 |         230.3229 |         -35.1307 |
[32m[20221214 14:18:24 @agent_ppo2.py:185][0m |          -0.0033 |         228.4562 |         -35.1633 |
[32m[20221214 14:18:24 @agent_ppo2.py:185][0m |          -0.0029 |         225.4203 |         -35.1049 |
[32m[20221214 14:18:24 @agent_ppo2.py:185][0m |          -0.0036 |         225.2613 |         -35.0595 |
[32m[20221214 14:18:24 @agent_ppo2.py:185][0m |          -0.0035 |         223.6104 |         -34.9619 |
[32m[20221214 14:18:24 @agent_ppo2.py:185][0m |          -0.0039 |         223.4081 |         -35.0160 |
[32m[20221214 14:18:24 @agent_ppo2.py:185][0m |           0.0027 |         225.9226 |         -34.9929 |
[32m[20221214 14:18:25 @agent_ppo2.py:185][0m |          -0.0047 |         223.1385 |         -34.9032 |
[32m[20221214 14:18:25 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:18:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.01
[32m[20221214 14:18:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.67
[32m[20221214 14:18:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.68
[32m[20221214 14:18:25 @agent_ppo2.py:143][0m Total time:      20.38 min
[32m[20221214 14:18:25 @agent_ppo2.py:145][0m 1865728 total steps have happened
[32m[20221214 14:18:25 @agent_ppo2.py:121][0m #------------------------ Iteration 911 --------------------------#
[32m[20221214 14:18:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:25 @agent_ppo2.py:185][0m |           0.0073 |         261.1291 |         -34.9697 |
[32m[20221214 14:18:25 @agent_ppo2.py:185][0m |           0.0026 |         248.2305 |         -35.0099 |
[32m[20221214 14:18:25 @agent_ppo2.py:185][0m |           0.0044 |         249.8960 |         -34.9489 |
[32m[20221214 14:18:25 @agent_ppo2.py:185][0m |          -0.0034 |         241.8722 |         -34.9573 |
[32m[20221214 14:18:25 @agent_ppo2.py:185][0m |          -0.0030 |         241.1927 |         -35.0657 |
[32m[20221214 14:18:26 @agent_ppo2.py:185][0m |          -0.0008 |         242.0271 |         -34.9989 |
[32m[20221214 14:18:26 @agent_ppo2.py:185][0m |          -0.0038 |         240.7713 |         -35.0417 |
[32m[20221214 14:18:26 @agent_ppo2.py:185][0m |          -0.0033 |         240.2308 |         -35.1411 |
[32m[20221214 14:18:26 @agent_ppo2.py:185][0m |          -0.0038 |         240.6613 |         -35.0937 |
[32m[20221214 14:18:26 @agent_ppo2.py:185][0m |          -0.0032 |         240.8960 |         -35.2173 |
[32m[20221214 14:18:26 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:18:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.00
[32m[20221214 14:18:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.74
[32m[20221214 14:18:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.36
[32m[20221214 14:18:26 @agent_ppo2.py:143][0m Total time:      20.40 min
[32m[20221214 14:18:26 @agent_ppo2.py:145][0m 1867776 total steps have happened
[32m[20221214 14:18:26 @agent_ppo2.py:121][0m #------------------------ Iteration 912 --------------------------#
[32m[20221214 14:18:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:26 @agent_ppo2.py:185][0m |           0.0034 |         285.4177 |         -34.8397 |
[32m[20221214 14:18:27 @agent_ppo2.py:185][0m |          -0.0028 |         279.2193 |         -34.8211 |
[32m[20221214 14:18:27 @agent_ppo2.py:185][0m |           0.0134 |         300.0796 |         -34.9400 |
[32m[20221214 14:18:27 @agent_ppo2.py:185][0m |          -0.0022 |         279.3073 |         -34.9511 |
[32m[20221214 14:18:27 @agent_ppo2.py:185][0m |          -0.0034 |         277.0862 |         -34.9922 |
[32m[20221214 14:18:27 @agent_ppo2.py:185][0m |           0.0005 |         279.2250 |         -35.0087 |
[32m[20221214 14:18:27 @agent_ppo2.py:185][0m |          -0.0023 |         277.3293 |         -35.0270 |
[32m[20221214 14:18:27 @agent_ppo2.py:185][0m |          -0.0022 |         276.2771 |         -35.0658 |
[32m[20221214 14:18:27 @agent_ppo2.py:185][0m |          -0.0018 |         276.6828 |         -35.1895 |
[32m[20221214 14:18:27 @agent_ppo2.py:185][0m |          -0.0024 |         275.9258 |         -35.0952 |
[32m[20221214 14:18:27 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:18:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.69
[32m[20221214 14:18:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.66
[32m[20221214 14:18:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.02
[32m[20221214 14:18:27 @agent_ppo2.py:143][0m Total time:      20.42 min
[32m[20221214 14:18:27 @agent_ppo2.py:145][0m 1869824 total steps have happened
[32m[20221214 14:18:27 @agent_ppo2.py:121][0m #------------------------ Iteration 913 --------------------------#
[32m[20221214 14:18:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:28 @agent_ppo2.py:185][0m |          -0.0008 |         257.8285 |         -35.4899 |
[32m[20221214 14:18:28 @agent_ppo2.py:185][0m |           0.0123 |         285.1250 |         -35.4052 |
[32m[20221214 14:18:28 @agent_ppo2.py:185][0m |           0.0005 |         255.0689 |         -35.5651 |
[32m[20221214 14:18:28 @agent_ppo2.py:185][0m |          -0.0017 |         253.4392 |         -35.5289 |
[32m[20221214 14:18:28 @agent_ppo2.py:185][0m |          -0.0013 |         253.2589 |         -35.7921 |
[32m[20221214 14:18:28 @agent_ppo2.py:185][0m |           0.0016 |         254.2796 |         -35.8013 |
[32m[20221214 14:18:28 @agent_ppo2.py:185][0m |          -0.0021 |         253.1713 |         -35.7852 |
[32m[20221214 14:18:28 @agent_ppo2.py:185][0m |          -0.0021 |         253.4020 |         -36.0006 |
[32m[20221214 14:18:29 @agent_ppo2.py:185][0m |           0.0037 |         255.7199 |         -35.9392 |
[32m[20221214 14:18:29 @agent_ppo2.py:185][0m |          -0.0021 |         251.8513 |         -36.2372 |
[32m[20221214 14:18:29 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:18:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 844.25
[32m[20221214 14:18:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.17
[32m[20221214 14:18:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.33
[32m[20221214 14:18:29 @agent_ppo2.py:143][0m Total time:      20.45 min
[32m[20221214 14:18:29 @agent_ppo2.py:145][0m 1871872 total steps have happened
[32m[20221214 14:18:29 @agent_ppo2.py:121][0m #------------------------ Iteration 914 --------------------------#
[32m[20221214 14:18:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:29 @agent_ppo2.py:185][0m |          -0.0017 |         266.1038 |         -37.6522 |
[32m[20221214 14:18:29 @agent_ppo2.py:185][0m |          -0.0009 |         265.0939 |         -37.5166 |
[32m[20221214 14:18:29 @agent_ppo2.py:185][0m |          -0.0028 |         264.5120 |         -37.7579 |
[32m[20221214 14:18:30 @agent_ppo2.py:185][0m |          -0.0006 |         263.6576 |         -37.7168 |
[32m[20221214 14:18:30 @agent_ppo2.py:185][0m |          -0.0024 |         262.4463 |         -37.8577 |
[32m[20221214 14:18:30 @agent_ppo2.py:185][0m |          -0.0029 |         263.4365 |         -37.9424 |
[32m[20221214 14:18:30 @agent_ppo2.py:185][0m |           0.0065 |         272.5386 |         -38.0102 |
[32m[20221214 14:18:30 @agent_ppo2.py:185][0m |           0.0126 |         299.5271 |         -38.0949 |
[32m[20221214 14:18:30 @agent_ppo2.py:185][0m |          -0.0021 |         261.9188 |         -38.1211 |
[32m[20221214 14:18:30 @agent_ppo2.py:185][0m |           0.0039 |         265.6489 |         -38.1138 |
[32m[20221214 14:18:30 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:18:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 844.14
[32m[20221214 14:18:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.28
[32m[20221214 14:18:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.97
[32m[20221214 14:18:30 @agent_ppo2.py:143][0m Total time:      20.47 min
[32m[20221214 14:18:30 @agent_ppo2.py:145][0m 1873920 total steps have happened
[32m[20221214 14:18:30 @agent_ppo2.py:121][0m #------------------------ Iteration 915 --------------------------#
[32m[20221214 14:18:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:31 @agent_ppo2.py:185][0m |           0.0137 |         289.4487 |         -37.5921 |
[32m[20221214 14:18:31 @agent_ppo2.py:185][0m |          -0.0022 |         252.9770 |         -37.5634 |
[32m[20221214 14:18:31 @agent_ppo2.py:185][0m |           0.0004 |         252.5312 |         -37.5277 |
[32m[20221214 14:18:31 @agent_ppo2.py:185][0m |          -0.0034 |         252.0874 |         -37.4550 |
[32m[20221214 14:18:31 @agent_ppo2.py:185][0m |          -0.0017 |         251.7916 |         -37.6248 |
[32m[20221214 14:18:31 @agent_ppo2.py:185][0m |          -0.0030 |         250.8405 |         -37.4613 |
[32m[20221214 14:18:31 @agent_ppo2.py:185][0m |           0.0002 |         251.2040 |         -37.6055 |
[32m[20221214 14:18:31 @agent_ppo2.py:185][0m |          -0.0024 |         250.6765 |         -37.5773 |
[32m[20221214 14:18:31 @agent_ppo2.py:185][0m |          -0.0019 |         250.2372 |         -37.6564 |
[32m[20221214 14:18:31 @agent_ppo2.py:185][0m |          -0.0053 |         250.6307 |         -37.5679 |
[32m[20221214 14:18:31 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:18:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 846.50
[32m[20221214 14:18:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.45
[32m[20221214 14:18:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.96
[32m[20221214 14:18:31 @agent_ppo2.py:143][0m Total time:      20.49 min
[32m[20221214 14:18:31 @agent_ppo2.py:145][0m 1875968 total steps have happened
[32m[20221214 14:18:31 @agent_ppo2.py:121][0m #------------------------ Iteration 916 --------------------------#
[32m[20221214 14:18:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:32 @agent_ppo2.py:185][0m |          -0.0024 |         267.0635 |         -38.2353 |
[32m[20221214 14:18:32 @agent_ppo2.py:185][0m |          -0.0023 |         265.1637 |         -38.2149 |
[32m[20221214 14:18:32 @agent_ppo2.py:185][0m |          -0.0034 |         262.9007 |         -38.1488 |
[32m[20221214 14:18:32 @agent_ppo2.py:185][0m |          -0.0031 |         262.1316 |         -38.2952 |
[32m[20221214 14:18:32 @agent_ppo2.py:185][0m |          -0.0034 |         261.9864 |         -38.2931 |
[32m[20221214 14:18:32 @agent_ppo2.py:185][0m |          -0.0001 |         263.1593 |         -38.2485 |
[32m[20221214 14:18:32 @agent_ppo2.py:185][0m |           0.0034 |         268.1548 |         -38.3068 |
[32m[20221214 14:18:32 @agent_ppo2.py:185][0m |          -0.0020 |         261.0785 |         -38.3421 |
[32m[20221214 14:18:32 @agent_ppo2.py:185][0m |          -0.0036 |         261.1064 |         -38.2331 |
[32m[20221214 14:18:33 @agent_ppo2.py:185][0m |          -0.0029 |         261.3864 |         -38.2794 |
[32m[20221214 14:18:33 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:18:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 844.68
[32m[20221214 14:18:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.77
[32m[20221214 14:18:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.84
[32m[20221214 14:18:33 @agent_ppo2.py:143][0m Total time:      20.51 min
[32m[20221214 14:18:33 @agent_ppo2.py:145][0m 1878016 total steps have happened
[32m[20221214 14:18:33 @agent_ppo2.py:121][0m #------------------------ Iteration 917 --------------------------#
[32m[20221214 14:18:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:18:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:33 @agent_ppo2.py:185][0m |          -0.0011 |         277.7402 |         -38.2663 |
[32m[20221214 14:18:33 @agent_ppo2.py:185][0m |          -0.0018 |         275.0032 |         -38.2760 |
[32m[20221214 14:18:33 @agent_ppo2.py:185][0m |          -0.0012 |         273.8035 |         -38.2580 |
[32m[20221214 14:18:33 @agent_ppo2.py:185][0m |           0.0050 |         282.8706 |         -38.2161 |
[32m[20221214 14:18:33 @agent_ppo2.py:185][0m |          -0.0013 |         273.1222 |         -38.2340 |
[32m[20221214 14:18:33 @agent_ppo2.py:185][0m |          -0.0015 |         273.1013 |         -38.2454 |
[32m[20221214 14:18:34 @agent_ppo2.py:185][0m |           0.0002 |         273.7053 |         -38.2768 |
[32m[20221214 14:18:34 @agent_ppo2.py:185][0m |           0.0128 |         294.5972 |         -38.1268 |
[32m[20221214 14:18:34 @agent_ppo2.py:185][0m |          -0.0008 |         273.3056 |         -38.2366 |
[32m[20221214 14:18:34 @agent_ppo2.py:185][0m |           0.0145 |         313.4873 |         -38.2441 |
[32m[20221214 14:18:34 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:18:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 844.67
[32m[20221214 14:18:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.79
[32m[20221214 14:18:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.34
[32m[20221214 14:18:34 @agent_ppo2.py:143][0m Total time:      20.53 min
[32m[20221214 14:18:34 @agent_ppo2.py:145][0m 1880064 total steps have happened
[32m[20221214 14:18:34 @agent_ppo2.py:121][0m #------------------------ Iteration 918 --------------------------#
[32m[20221214 14:18:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:34 @agent_ppo2.py:185][0m |          -0.0004 |         258.8462 |         -37.9523 |
[32m[20221214 14:18:34 @agent_ppo2.py:185][0m |          -0.0013 |         256.5194 |         -37.9564 |
[32m[20221214 14:18:34 @agent_ppo2.py:185][0m |          -0.0016 |         256.5240 |         -37.9192 |
[32m[20221214 14:18:35 @agent_ppo2.py:185][0m |          -0.0031 |         255.4454 |         -37.8749 |
[32m[20221214 14:18:35 @agent_ppo2.py:185][0m |          -0.0026 |         255.5744 |         -37.9759 |
[32m[20221214 14:18:35 @agent_ppo2.py:185][0m |          -0.0019 |         255.4346 |         -38.0039 |
[32m[20221214 14:18:35 @agent_ppo2.py:185][0m |          -0.0021 |         255.3730 |         -37.9500 |
[32m[20221214 14:18:35 @agent_ppo2.py:185][0m |          -0.0017 |         254.9082 |         -37.9772 |
[32m[20221214 14:18:35 @agent_ppo2.py:185][0m |          -0.0019 |         254.8437 |         -38.0272 |
[32m[20221214 14:18:35 @agent_ppo2.py:185][0m |          -0.0024 |         256.2026 |         -37.9784 |
[32m[20221214 14:18:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:18:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.86
[32m[20221214 14:18:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.42
[32m[20221214 14:18:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.17
[32m[20221214 14:18:35 @agent_ppo2.py:143][0m Total time:      20.55 min
[32m[20221214 14:18:35 @agent_ppo2.py:145][0m 1882112 total steps have happened
[32m[20221214 14:18:35 @agent_ppo2.py:121][0m #------------------------ Iteration 919 --------------------------#
[32m[20221214 14:18:35 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:18:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:36 @agent_ppo2.py:185][0m |          -0.0002 |         246.9659 |         -38.2044 |
[32m[20221214 14:18:36 @agent_ppo2.py:185][0m |           0.0056 |         251.8048 |         -38.1961 |
[32m[20221214 14:18:36 @agent_ppo2.py:185][0m |          -0.0017 |         244.6519 |         -38.2523 |
[32m[20221214 14:18:36 @agent_ppo2.py:185][0m |          -0.0013 |         243.6994 |         -38.2898 |
[32m[20221214 14:18:36 @agent_ppo2.py:185][0m |          -0.0015 |         242.2840 |         -38.1996 |
[32m[20221214 14:18:36 @agent_ppo2.py:185][0m |          -0.0011 |         241.3736 |         -38.3465 |
[32m[20221214 14:18:36 @agent_ppo2.py:185][0m |           0.0016 |         243.2196 |         -38.4586 |
[32m[20221214 14:18:36 @agent_ppo2.py:185][0m |           0.0045 |         244.8858 |         -38.5328 |
[32m[20221214 14:18:36 @agent_ppo2.py:185][0m |          -0.0026 |         239.8616 |         -38.5736 |
[32m[20221214 14:18:36 @agent_ppo2.py:185][0m |          -0.0025 |         239.9227 |         -38.5555 |
[32m[20221214 14:18:36 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:18:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.89
[32m[20221214 14:18:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.85
[32m[20221214 14:18:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.23
[32m[20221214 14:18:37 @agent_ppo2.py:143][0m Total time:      20.58 min
[32m[20221214 14:18:37 @agent_ppo2.py:145][0m 1884160 total steps have happened
[32m[20221214 14:18:37 @agent_ppo2.py:121][0m #------------------------ Iteration 920 --------------------------#
[32m[20221214 14:18:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:18:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:37 @agent_ppo2.py:185][0m |          -0.0036 |         215.2714 |         -38.3046 |
[32m[20221214 14:18:37 @agent_ppo2.py:185][0m |          -0.0040 |         214.4711 |         -38.3020 |
[32m[20221214 14:18:37 @agent_ppo2.py:185][0m |          -0.0038 |         213.9537 |         -38.3458 |
[32m[20221214 14:18:37 @agent_ppo2.py:185][0m |          -0.0029 |         213.1288 |         -38.3923 |
[32m[20221214 14:18:37 @agent_ppo2.py:185][0m |           0.0063 |         233.9689 |         -38.3363 |
[32m[20221214 14:18:37 @agent_ppo2.py:185][0m |          -0.0006 |         215.7149 |         -38.4523 |
[32m[20221214 14:18:38 @agent_ppo2.py:185][0m |          -0.0013 |         212.2529 |         -38.4762 |
[32m[20221214 14:18:38 @agent_ppo2.py:185][0m |          -0.0044 |         212.2351 |         -38.4925 |
[32m[20221214 14:18:38 @agent_ppo2.py:185][0m |           0.0051 |         220.8243 |         -38.5770 |
[32m[20221214 14:18:38 @agent_ppo2.py:185][0m |           0.0069 |         222.9565 |         -38.6395 |
[32m[20221214 14:18:38 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:18:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 844.31
[32m[20221214 14:18:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.26
[32m[20221214 14:18:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.33
[32m[20221214 14:18:38 @agent_ppo2.py:143][0m Total time:      20.60 min
[32m[20221214 14:18:38 @agent_ppo2.py:145][0m 1886208 total steps have happened
[32m[20221214 14:18:38 @agent_ppo2.py:121][0m #------------------------ Iteration 921 --------------------------#
[32m[20221214 14:18:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:38 @agent_ppo2.py:185][0m |           0.0189 |         275.1592 |         -38.8425 |
[32m[20221214 14:18:38 @agent_ppo2.py:185][0m |          -0.0015 |         246.5610 |         -38.8370 |
[32m[20221214 14:18:39 @agent_ppo2.py:185][0m |           0.0096 |         256.7150 |         -38.7701 |
[32m[20221214 14:18:39 @agent_ppo2.py:185][0m |          -0.0008 |         244.0247 |         -38.9387 |
[32m[20221214 14:18:39 @agent_ppo2.py:185][0m |           0.0010 |         244.3632 |         -38.9616 |
[32m[20221214 14:18:39 @agent_ppo2.py:185][0m |          -0.0008 |         243.4696 |         -38.8680 |
[32m[20221214 14:18:39 @agent_ppo2.py:185][0m |          -0.0008 |         242.2748 |         -38.9502 |
[32m[20221214 14:18:39 @agent_ppo2.py:185][0m |          -0.0018 |         242.8271 |         -39.0644 |
[32m[20221214 14:18:39 @agent_ppo2.py:185][0m |          -0.0022 |         242.8521 |         -38.9314 |
[32m[20221214 14:18:39 @agent_ppo2.py:185][0m |           0.0002 |         242.5844 |         -39.0030 |
[32m[20221214 14:18:39 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:18:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.79
[32m[20221214 14:18:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.46
[32m[20221214 14:18:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.76
[32m[20221214 14:18:39 @agent_ppo2.py:143][0m Total time:      20.62 min
[32m[20221214 14:18:39 @agent_ppo2.py:145][0m 1888256 total steps have happened
[32m[20221214 14:18:39 @agent_ppo2.py:121][0m #------------------------ Iteration 922 --------------------------#
[32m[20221214 14:18:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:40 @agent_ppo2.py:185][0m |          -0.0008 |         239.3570 |         -38.6057 |
[32m[20221214 14:18:40 @agent_ppo2.py:185][0m |          -0.0012 |         237.8888 |         -38.6658 |
[32m[20221214 14:18:40 @agent_ppo2.py:185][0m |          -0.0017 |         237.5064 |         -38.6104 |
[32m[20221214 14:18:40 @agent_ppo2.py:185][0m |           0.0117 |         264.2288 |         -38.6059 |
[32m[20221214 14:18:40 @agent_ppo2.py:185][0m |           0.0038 |         238.3964 |         -38.5799 |
[32m[20221214 14:18:40 @agent_ppo2.py:185][0m |          -0.0012 |         236.0353 |         -38.4032 |
[32m[20221214 14:18:40 @agent_ppo2.py:185][0m |           0.0164 |         270.6625 |         -38.6039 |
[32m[20221214 14:18:40 @agent_ppo2.py:185][0m |          -0.0011 |         236.4408 |         -38.3271 |
[32m[20221214 14:18:40 @agent_ppo2.py:185][0m |          -0.0020 |         234.3212 |         -38.5123 |
[32m[20221214 14:18:40 @agent_ppo2.py:185][0m |          -0.0021 |         233.7948 |         -38.5507 |
[32m[20221214 14:18:40 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:18:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.92
[32m[20221214 14:18:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.27
[32m[20221214 14:18:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.86
[32m[20221214 14:18:41 @agent_ppo2.py:143][0m Total time:      20.64 min
[32m[20221214 14:18:41 @agent_ppo2.py:145][0m 1890304 total steps have happened
[32m[20221214 14:18:41 @agent_ppo2.py:121][0m #------------------------ Iteration 923 --------------------------#
[32m[20221214 14:18:41 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:18:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:41 @agent_ppo2.py:185][0m |           0.0125 |         284.1100 |         -38.9220 |
[32m[20221214 14:18:41 @agent_ppo2.py:185][0m |          -0.0043 |         249.7798 |         -38.9439 |
[32m[20221214 14:18:41 @agent_ppo2.py:185][0m |          -0.0036 |         249.0483 |         -38.8838 |
[32m[20221214 14:18:41 @agent_ppo2.py:185][0m |          -0.0039 |         249.0776 |         -38.9704 |
[32m[20221214 14:18:41 @agent_ppo2.py:185][0m |           0.0011 |         253.0808 |         -39.1076 |
[32m[20221214 14:18:42 @agent_ppo2.py:185][0m |          -0.0044 |         247.4996 |         -39.0540 |
[32m[20221214 14:18:42 @agent_ppo2.py:185][0m |          -0.0048 |         246.9683 |         -39.2022 |
[32m[20221214 14:18:42 @agent_ppo2.py:185][0m |          -0.0035 |         246.4459 |         -39.1494 |
[32m[20221214 14:18:42 @agent_ppo2.py:185][0m |          -0.0015 |         246.9364 |         -39.3313 |
[32m[20221214 14:18:42 @agent_ppo2.py:185][0m |          -0.0028 |         245.9290 |         -39.2476 |
[32m[20221214 14:18:42 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 14:18:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.52
[32m[20221214 14:18:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.30
[32m[20221214 14:18:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 851.82
[32m[20221214 14:18:42 @agent_ppo2.py:143][0m Total time:      20.67 min
[32m[20221214 14:18:42 @agent_ppo2.py:145][0m 1892352 total steps have happened
[32m[20221214 14:18:42 @agent_ppo2.py:121][0m #------------------------ Iteration 924 --------------------------#
[32m[20221214 14:18:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:43 @agent_ppo2.py:185][0m |          -0.0006 |         208.2745 |         -38.8401 |
[32m[20221214 14:18:43 @agent_ppo2.py:185][0m |          -0.0030 |         208.0472 |         -38.8544 |
[32m[20221214 14:18:43 @agent_ppo2.py:185][0m |           0.0083 |         219.0110 |         -38.8502 |
[32m[20221214 14:18:43 @agent_ppo2.py:185][0m |          -0.0016 |         206.8974 |         -38.9467 |
[32m[20221214 14:18:43 @agent_ppo2.py:185][0m |          -0.0038 |         207.9538 |         -38.9749 |
[32m[20221214 14:18:43 @agent_ppo2.py:185][0m |          -0.0049 |         207.0954 |         -38.8865 |
[32m[20221214 14:18:43 @agent_ppo2.py:185][0m |          -0.0037 |         206.2919 |         -39.0325 |
[32m[20221214 14:18:43 @agent_ppo2.py:185][0m |          -0.0031 |         205.2812 |         -38.9579 |
[32m[20221214 14:18:43 @agent_ppo2.py:185][0m |          -0.0007 |         205.8239 |         -39.0388 |
[32m[20221214 14:18:43 @agent_ppo2.py:185][0m |          -0.0022 |         205.7049 |         -38.9543 |
[32m[20221214 14:18:43 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:18:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.17
[32m[20221214 14:18:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.59
[32m[20221214 14:18:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.97
[32m[20221214 14:18:44 @agent_ppo2.py:143][0m Total time:      20.69 min
[32m[20221214 14:18:44 @agent_ppo2.py:145][0m 1894400 total steps have happened
[32m[20221214 14:18:44 @agent_ppo2.py:121][0m #------------------------ Iteration 925 --------------------------#
[32m[20221214 14:18:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:44 @agent_ppo2.py:185][0m |           0.0020 |         204.6786 |         -39.1564 |
[32m[20221214 14:18:44 @agent_ppo2.py:185][0m |          -0.0029 |         199.0478 |         -38.9771 |
[32m[20221214 14:18:44 @agent_ppo2.py:185][0m |           0.0005 |         199.6771 |         -39.0345 |
[32m[20221214 14:18:44 @agent_ppo2.py:185][0m |          -0.0020 |         197.6064 |         -38.8878 |
[32m[20221214 14:18:44 @agent_ppo2.py:185][0m |          -0.0028 |         196.6073 |         -38.7603 |
[32m[20221214 14:18:44 @agent_ppo2.py:185][0m |          -0.0001 |         197.1384 |         -38.9795 |
[32m[20221214 14:18:44 @agent_ppo2.py:185][0m |          -0.0040 |         196.0199 |         -38.8473 |
[32m[20221214 14:18:45 @agent_ppo2.py:185][0m |           0.0069 |         202.2268 |         -38.8288 |
[32m[20221214 14:18:45 @agent_ppo2.py:185][0m |          -0.0024 |         197.0810 |         -38.7579 |
[32m[20221214 14:18:45 @agent_ppo2.py:185][0m |           0.0008 |         196.7732 |         -38.7741 |
[32m[20221214 14:18:45 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:18:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 844.52
[32m[20221214 14:18:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.38
[32m[20221214 14:18:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.41
[32m[20221214 14:18:45 @agent_ppo2.py:143][0m Total time:      20.71 min
[32m[20221214 14:18:45 @agent_ppo2.py:145][0m 1896448 total steps have happened
[32m[20221214 14:18:45 @agent_ppo2.py:121][0m #------------------------ Iteration 926 --------------------------#
[32m[20221214 14:18:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:45 @agent_ppo2.py:185][0m |          -0.0025 |         227.8447 |         -40.6124 |
[32m[20221214 14:18:45 @agent_ppo2.py:185][0m |          -0.0013 |         223.9514 |         -40.5244 |
[32m[20221214 14:18:45 @agent_ppo2.py:185][0m |           0.0050 |         229.1544 |         -40.6841 |
[32m[20221214 14:18:46 @agent_ppo2.py:185][0m |          -0.0011 |         223.6042 |         -40.6901 |
[32m[20221214 14:18:46 @agent_ppo2.py:185][0m |          -0.0008 |         222.2242 |         -40.6002 |
[32m[20221214 14:18:46 @agent_ppo2.py:185][0m |          -0.0015 |         221.7973 |         -40.6447 |
[32m[20221214 14:18:46 @agent_ppo2.py:185][0m |          -0.0028 |         221.4410 |         -40.7682 |
[32m[20221214 14:18:46 @agent_ppo2.py:185][0m |          -0.0011 |         221.1408 |         -40.7418 |
[32m[20221214 14:18:46 @agent_ppo2.py:185][0m |          -0.0017 |         221.5293 |         -40.9433 |
[32m[20221214 14:18:46 @agent_ppo2.py:185][0m |          -0.0023 |         221.0748 |         -40.7732 |
[32m[20221214 14:18:46 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:18:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 846.22
[32m[20221214 14:18:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.97
[32m[20221214 14:18:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.77
[32m[20221214 14:18:46 @agent_ppo2.py:143][0m Total time:      20.74 min
[32m[20221214 14:18:46 @agent_ppo2.py:145][0m 1898496 total steps have happened
[32m[20221214 14:18:46 @agent_ppo2.py:121][0m #------------------------ Iteration 927 --------------------------#
[32m[20221214 14:18:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:18:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:47 @agent_ppo2.py:185][0m |          -0.0012 |         201.7791 |         -41.2388 |
[32m[20221214 14:18:47 @agent_ppo2.py:185][0m |          -0.0007 |         201.2716 |         -41.1279 |
[32m[20221214 14:18:47 @agent_ppo2.py:185][0m |           0.0068 |         203.0294 |         -41.4048 |
[32m[20221214 14:18:47 @agent_ppo2.py:185][0m |          -0.0018 |         199.9980 |         -41.2811 |
[32m[20221214 14:18:47 @agent_ppo2.py:185][0m |           0.0000 |         199.7462 |         -41.3900 |
[32m[20221214 14:18:47 @agent_ppo2.py:185][0m |           0.0031 |         198.9933 |         -41.3432 |
[32m[20221214 14:18:47 @agent_ppo2.py:185][0m |          -0.0034 |         199.5235 |         -41.3064 |
[32m[20221214 14:18:47 @agent_ppo2.py:185][0m |           0.0051 |         203.7726 |         -41.5002 |
[32m[20221214 14:18:47 @agent_ppo2.py:185][0m |          -0.0021 |         198.0947 |         -41.4063 |
[32m[20221214 14:18:47 @agent_ppo2.py:185][0m |           0.0080 |         207.8672 |         -41.4497 |
[32m[20221214 14:18:47 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:18:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 846.55
[32m[20221214 14:18:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.21
[32m[20221214 14:18:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.96
[32m[20221214 14:18:48 @agent_ppo2.py:143][0m Total time:      20.76 min
[32m[20221214 14:18:48 @agent_ppo2.py:145][0m 1900544 total steps have happened
[32m[20221214 14:18:48 @agent_ppo2.py:121][0m #------------------------ Iteration 928 --------------------------#
[32m[20221214 14:18:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:48 @agent_ppo2.py:185][0m |          -0.0020 |         188.6242 |         -41.0256 |
[32m[20221214 14:18:48 @agent_ppo2.py:185][0m |          -0.0035 |         183.4430 |         -40.9295 |
[32m[20221214 14:18:48 @agent_ppo2.py:185][0m |           0.0011 |         185.7747 |         -40.9291 |
[32m[20221214 14:18:48 @agent_ppo2.py:185][0m |          -0.0027 |         181.2253 |         -41.0257 |
[32m[20221214 14:18:48 @agent_ppo2.py:185][0m |          -0.0036 |         181.1959 |         -41.0796 |
[32m[20221214 14:18:48 @agent_ppo2.py:185][0m |          -0.0050 |         180.5991 |         -41.0273 |
[32m[20221214 14:18:48 @agent_ppo2.py:185][0m |           0.0016 |         181.7762 |         -41.3488 |
[32m[20221214 14:18:48 @agent_ppo2.py:185][0m |          -0.0051 |         180.7356 |         -41.2760 |
[32m[20221214 14:18:49 @agent_ppo2.py:185][0m |           0.0054 |         185.1987 |         -41.2721 |
[32m[20221214 14:18:49 @agent_ppo2.py:185][0m |          -0.0026 |         180.5907 |         -41.1334 |
[32m[20221214 14:18:49 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:18:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 846.09
[32m[20221214 14:18:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.08
[32m[20221214 14:18:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.46
[32m[20221214 14:18:49 @agent_ppo2.py:143][0m Total time:      20.78 min
[32m[20221214 14:18:49 @agent_ppo2.py:145][0m 1902592 total steps have happened
[32m[20221214 14:18:49 @agent_ppo2.py:121][0m #------------------------ Iteration 929 --------------------------#
[32m[20221214 14:18:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:49 @agent_ppo2.py:185][0m |           0.0042 |         211.4376 |         -42.2592 |
[32m[20221214 14:18:49 @agent_ppo2.py:185][0m |          -0.0062 |         205.3207 |         -42.1754 |
[32m[20221214 14:18:49 @agent_ppo2.py:185][0m |          -0.0036 |         202.7426 |         -42.2557 |
[32m[20221214 14:18:49 @agent_ppo2.py:185][0m |          -0.0049 |         202.4990 |         -42.1584 |
[32m[20221214 14:18:50 @agent_ppo2.py:185][0m |          -0.0039 |         201.2887 |         -42.1702 |
[32m[20221214 14:18:50 @agent_ppo2.py:185][0m |          -0.0048 |         201.7200 |         -42.0952 |
[32m[20221214 14:18:50 @agent_ppo2.py:185][0m |          -0.0044 |         201.0428 |         -42.0576 |
[32m[20221214 14:18:50 @agent_ppo2.py:185][0m |          -0.0030 |         200.4053 |         -41.9397 |
[32m[20221214 14:18:50 @agent_ppo2.py:185][0m |          -0.0050 |         200.1699 |         -42.0199 |
[32m[20221214 14:18:50 @agent_ppo2.py:185][0m |          -0.0050 |         200.0567 |         -42.0139 |
[32m[20221214 14:18:50 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:18:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 844.29
[32m[20221214 14:18:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.76
[32m[20221214 14:18:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.09
[32m[20221214 14:18:50 @agent_ppo2.py:143][0m Total time:      20.80 min
[32m[20221214 14:18:50 @agent_ppo2.py:145][0m 1904640 total steps have happened
[32m[20221214 14:18:50 @agent_ppo2.py:121][0m #------------------------ Iteration 930 --------------------------#
[32m[20221214 14:18:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:18:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:50 @agent_ppo2.py:185][0m |           0.0063 |         259.3389 |         -41.6213 |
[32m[20221214 14:18:51 @agent_ppo2.py:185][0m |          -0.0013 |         235.3390 |         -41.8874 |
[32m[20221214 14:18:51 @agent_ppo2.py:185][0m |          -0.0051 |         233.0769 |         -41.7287 |
[32m[20221214 14:18:51 @agent_ppo2.py:185][0m |          -0.0044 |         232.7915 |         -41.7022 |
[32m[20221214 14:18:51 @agent_ppo2.py:185][0m |          -0.0043 |         232.0325 |         -41.6843 |
[32m[20221214 14:18:51 @agent_ppo2.py:185][0m |          -0.0025 |         232.1013 |         -41.7831 |
[32m[20221214 14:18:51 @agent_ppo2.py:185][0m |          -0.0047 |         232.2907 |         -41.6097 |
[32m[20221214 14:18:51 @agent_ppo2.py:185][0m |          -0.0030 |         231.1706 |         -41.8922 |
[32m[20221214 14:18:51 @agent_ppo2.py:185][0m |          -0.0026 |         231.2542 |         -41.5780 |
[32m[20221214 14:18:51 @agent_ppo2.py:185][0m |          -0.0040 |         230.8520 |         -41.7559 |
[32m[20221214 14:18:51 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:18:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.70
[32m[20221214 14:18:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.51
[32m[20221214 14:18:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.51
[32m[20221214 14:18:51 @agent_ppo2.py:143][0m Total time:      20.82 min
[32m[20221214 14:18:51 @agent_ppo2.py:145][0m 1906688 total steps have happened
[32m[20221214 14:18:51 @agent_ppo2.py:121][0m #------------------------ Iteration 931 --------------------------#
[32m[20221214 14:18:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:18:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:52 @agent_ppo2.py:185][0m |          -0.0004 |         254.3071 |         -40.4797 |
[32m[20221214 14:18:52 @agent_ppo2.py:185][0m |          -0.0071 |         248.6171 |         -40.4485 |
[32m[20221214 14:18:52 @agent_ppo2.py:185][0m |          -0.0062 |         246.8661 |         -40.4264 |
[32m[20221214 14:18:52 @agent_ppo2.py:185][0m |          -0.0060 |         246.6290 |         -40.4511 |
[32m[20221214 14:18:52 @agent_ppo2.py:185][0m |          -0.0077 |         246.3842 |         -40.5062 |
[32m[20221214 14:18:52 @agent_ppo2.py:185][0m |          -0.0074 |         245.8224 |         -40.5032 |
[32m[20221214 14:18:52 @agent_ppo2.py:185][0m |          -0.0068 |         245.5287 |         -40.5485 |
[32m[20221214 14:18:52 @agent_ppo2.py:185][0m |          -0.0068 |         244.9861 |         -40.4590 |
[32m[20221214 14:18:52 @agent_ppo2.py:185][0m |          -0.0059 |         245.4303 |         -40.4552 |
[32m[20221214 14:18:53 @agent_ppo2.py:185][0m |          -0.0010 |         249.3540 |         -40.5514 |
[32m[20221214 14:18:53 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:18:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.56
[32m[20221214 14:18:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.58
[32m[20221214 14:18:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.96
[32m[20221214 14:18:53 @agent_ppo2.py:143][0m Total time:      20.84 min
[32m[20221214 14:18:53 @agent_ppo2.py:145][0m 1908736 total steps have happened
[32m[20221214 14:18:53 @agent_ppo2.py:121][0m #------------------------ Iteration 932 --------------------------#
[32m[20221214 14:18:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:53 @agent_ppo2.py:185][0m |          -0.0038 |         267.8338 |         -40.6275 |
[32m[20221214 14:18:53 @agent_ppo2.py:185][0m |          -0.0009 |         262.8260 |         -40.5932 |
[32m[20221214 14:18:53 @agent_ppo2.py:185][0m |          -0.0016 |         261.0498 |         -40.5746 |
[32m[20221214 14:18:53 @agent_ppo2.py:185][0m |          -0.0030 |         259.6631 |         -40.7602 |
[32m[20221214 14:18:53 @agent_ppo2.py:185][0m |          -0.0025 |         258.7848 |         -40.7307 |
[32m[20221214 14:18:54 @agent_ppo2.py:185][0m |          -0.0043 |         258.7833 |         -40.7917 |
[32m[20221214 14:18:54 @agent_ppo2.py:185][0m |          -0.0045 |         257.2316 |         -40.8255 |
[32m[20221214 14:18:54 @agent_ppo2.py:185][0m |           0.0024 |         260.8212 |         -40.9145 |
[32m[20221214 14:18:54 @agent_ppo2.py:185][0m |           0.0073 |         281.8271 |         -40.7496 |
[32m[20221214 14:18:54 @agent_ppo2.py:185][0m |           0.0123 |         288.6988 |         -40.9407 |
[32m[20221214 14:18:54 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:18:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.90
[32m[20221214 14:18:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.67
[32m[20221214 14:18:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.95
[32m[20221214 14:18:54 @agent_ppo2.py:143][0m Total time:      20.87 min
[32m[20221214 14:18:54 @agent_ppo2.py:145][0m 1910784 total steps have happened
[32m[20221214 14:18:54 @agent_ppo2.py:121][0m #------------------------ Iteration 933 --------------------------#
[32m[20221214 14:18:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:54 @agent_ppo2.py:185][0m |          -0.0041 |         268.7106 |         -41.6951 |
[32m[20221214 14:18:55 @agent_ppo2.py:185][0m |          -0.0059 |         264.5778 |         -41.8239 |
[32m[20221214 14:18:55 @agent_ppo2.py:185][0m |           0.0081 |         302.2332 |         -41.9273 |
[32m[20221214 14:18:55 @agent_ppo2.py:185][0m |          -0.0025 |         265.0862 |         -41.9219 |
[32m[20221214 14:18:55 @agent_ppo2.py:185][0m |          -0.0021 |         263.5369 |         -42.1144 |
[32m[20221214 14:18:55 @agent_ppo2.py:185][0m |          -0.0053 |         262.4045 |         -42.2708 |
[32m[20221214 14:18:55 @agent_ppo2.py:185][0m |          -0.0011 |         265.8809 |         -42.3540 |
[32m[20221214 14:18:55 @agent_ppo2.py:185][0m |          -0.0050 |         261.4429 |         -42.3526 |
[32m[20221214 14:18:55 @agent_ppo2.py:185][0m |          -0.0055 |         260.0756 |         -42.5162 |
[32m[20221214 14:18:55 @agent_ppo2.py:185][0m |          -0.0054 |         259.7411 |         -42.6487 |
[32m[20221214 14:18:55 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:18:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.13
[32m[20221214 14:18:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.34
[32m[20221214 14:18:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.76
[32m[20221214 14:18:55 @agent_ppo2.py:143][0m Total time:      20.89 min
[32m[20221214 14:18:55 @agent_ppo2.py:145][0m 1912832 total steps have happened
[32m[20221214 14:18:55 @agent_ppo2.py:121][0m #------------------------ Iteration 934 --------------------------#
[32m[20221214 14:18:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:56 @agent_ppo2.py:185][0m |          -0.0028 |         302.7304 |         -41.8217 |
[32m[20221214 14:18:56 @agent_ppo2.py:185][0m |          -0.0063 |         299.2455 |         -41.7958 |
[32m[20221214 14:18:56 @agent_ppo2.py:185][0m |          -0.0048 |         296.7868 |         -41.8468 |
[32m[20221214 14:18:56 @agent_ppo2.py:185][0m |          -0.0042 |         296.0418 |         -41.8343 |
[32m[20221214 14:18:56 @agent_ppo2.py:185][0m |          -0.0015 |         297.4187 |         -41.7816 |
[32m[20221214 14:18:56 @agent_ppo2.py:185][0m |          -0.0037 |         293.6335 |         -41.6918 |
[32m[20221214 14:18:56 @agent_ppo2.py:185][0m |          -0.0042 |         293.0107 |         -41.7985 |
[32m[20221214 14:18:56 @agent_ppo2.py:185][0m |          -0.0045 |         292.7061 |         -41.8632 |
[32m[20221214 14:18:57 @agent_ppo2.py:185][0m |          -0.0048 |         292.6805 |         -41.7184 |
[32m[20221214 14:18:57 @agent_ppo2.py:185][0m |           0.0011 |         294.7010 |         -41.8243 |
[32m[20221214 14:18:57 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:18:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.04
[32m[20221214 14:18:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.56
[32m[20221214 14:18:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.03
[32m[20221214 14:18:57 @agent_ppo2.py:143][0m Total time:      20.91 min
[32m[20221214 14:18:57 @agent_ppo2.py:145][0m 1914880 total steps have happened
[32m[20221214 14:18:57 @agent_ppo2.py:121][0m #------------------------ Iteration 935 --------------------------#
[32m[20221214 14:18:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:57 @agent_ppo2.py:185][0m |           0.0029 |         297.1479 |         -42.9180 |
[32m[20221214 14:18:57 @agent_ppo2.py:185][0m |           0.0044 |         290.3742 |         -42.8941 |
[32m[20221214 14:18:57 @agent_ppo2.py:185][0m |           0.0014 |         288.3120 |         -42.8651 |
[32m[20221214 14:18:57 @agent_ppo2.py:185][0m |           0.0132 |         308.4467 |         -43.0624 |
[32m[20221214 14:18:58 @agent_ppo2.py:185][0m |           0.0005 |         285.2978 |         -43.1177 |
[32m[20221214 14:18:58 @agent_ppo2.py:185][0m |          -0.0045 |         284.5451 |         -43.1699 |
[32m[20221214 14:18:58 @agent_ppo2.py:185][0m |          -0.0012 |         284.1100 |         -43.1459 |
[32m[20221214 14:18:58 @agent_ppo2.py:185][0m |           0.0028 |         289.9778 |         -43.4376 |
[32m[20221214 14:18:58 @agent_ppo2.py:185][0m |          -0.0018 |         283.7889 |         -43.4832 |
[32m[20221214 14:18:58 @agent_ppo2.py:185][0m |           0.0098 |         307.5085 |         -43.5388 |
[32m[20221214 14:18:58 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:18:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.30
[32m[20221214 14:18:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.01
[32m[20221214 14:18:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.90
[32m[20221214 14:18:58 @agent_ppo2.py:143][0m Total time:      20.94 min
[32m[20221214 14:18:58 @agent_ppo2.py:145][0m 1916928 total steps have happened
[32m[20221214 14:18:58 @agent_ppo2.py:121][0m #------------------------ Iteration 936 --------------------------#
[32m[20221214 14:18:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:18:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:18:59 @agent_ppo2.py:185][0m |          -0.0011 |         272.9787 |         -43.6520 |
[32m[20221214 14:18:59 @agent_ppo2.py:185][0m |          -0.0012 |         271.2776 |         -43.6270 |
[32m[20221214 14:18:59 @agent_ppo2.py:185][0m |          -0.0014 |         270.4601 |         -43.6219 |
[32m[20221214 14:18:59 @agent_ppo2.py:185][0m |           0.0130 |         306.8339 |         -43.7125 |
[32m[20221214 14:18:59 @agent_ppo2.py:185][0m |          -0.0016 |         270.6345 |         -43.5383 |
[32m[20221214 14:18:59 @agent_ppo2.py:185][0m |          -0.0038 |         269.9582 |         -43.6174 |
[32m[20221214 14:18:59 @agent_ppo2.py:185][0m |          -0.0009 |         269.3886 |         -43.6223 |
[32m[20221214 14:18:59 @agent_ppo2.py:185][0m |          -0.0026 |         269.1809 |         -43.6243 |
[32m[20221214 14:18:59 @agent_ppo2.py:185][0m |          -0.0011 |         268.5254 |         -43.6556 |
[32m[20221214 14:18:59 @agent_ppo2.py:185][0m |          -0.0021 |         268.9203 |         -43.5797 |
[32m[20221214 14:18:59 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:19:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.44
[32m[20221214 14:19:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.79
[32m[20221214 14:19:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.64
[32m[20221214 14:19:00 @agent_ppo2.py:143][0m Total time:      20.96 min
[32m[20221214 14:19:00 @agent_ppo2.py:145][0m 1918976 total steps have happened
[32m[20221214 14:19:00 @agent_ppo2.py:121][0m #------------------------ Iteration 937 --------------------------#
[32m[20221214 14:19:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:00 @agent_ppo2.py:185][0m |          -0.0022 |         260.7449 |         -43.6582 |
[32m[20221214 14:19:00 @agent_ppo2.py:185][0m |          -0.0019 |         255.3560 |         -43.7269 |
[32m[20221214 14:19:00 @agent_ppo2.py:185][0m |          -0.0027 |         254.2191 |         -43.7117 |
[32m[20221214 14:19:00 @agent_ppo2.py:185][0m |          -0.0010 |         252.8106 |         -43.7791 |
[32m[20221214 14:19:00 @agent_ppo2.py:185][0m |           0.0085 |         269.9350 |         -43.7536 |
[32m[20221214 14:19:00 @agent_ppo2.py:185][0m |           0.0015 |         252.7034 |         -43.7614 |
[32m[20221214 14:19:00 @agent_ppo2.py:185][0m |          -0.0017 |         252.0710 |         -43.7834 |
[32m[20221214 14:19:01 @agent_ppo2.py:185][0m |          -0.0015 |         251.2946 |         -43.8011 |
[32m[20221214 14:19:01 @agent_ppo2.py:185][0m |          -0.0007 |         251.1539 |         -43.8445 |
[32m[20221214 14:19:01 @agent_ppo2.py:185][0m |          -0.0019 |         252.6976 |         -43.6615 |
[32m[20221214 14:19:01 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:19:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.76
[32m[20221214 14:19:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.39
[32m[20221214 14:19:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.65
[32m[20221214 14:19:01 @agent_ppo2.py:143][0m Total time:      20.98 min
[32m[20221214 14:19:01 @agent_ppo2.py:145][0m 1921024 total steps have happened
[32m[20221214 14:19:01 @agent_ppo2.py:121][0m #------------------------ Iteration 938 --------------------------#
[32m[20221214 14:19:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:01 @agent_ppo2.py:185][0m |          -0.0029 |         245.6187 |         -44.3779 |
[32m[20221214 14:19:01 @agent_ppo2.py:185][0m |          -0.0006 |         242.9119 |         -44.3970 |
[32m[20221214 14:19:01 @agent_ppo2.py:185][0m |          -0.0010 |         241.0776 |         -44.4712 |
[32m[20221214 14:19:02 @agent_ppo2.py:185][0m |           0.0005 |         241.3607 |         -44.5344 |
[32m[20221214 14:19:02 @agent_ppo2.py:185][0m |          -0.0015 |         240.6503 |         -44.5930 |
[32m[20221214 14:19:02 @agent_ppo2.py:185][0m |          -0.0016 |         240.0572 |         -44.5119 |
[32m[20221214 14:19:02 @agent_ppo2.py:185][0m |           0.0002 |         239.9438 |         -44.8108 |
[32m[20221214 14:19:02 @agent_ppo2.py:185][0m |          -0.0045 |         240.1104 |         -44.6309 |
[32m[20221214 14:19:02 @agent_ppo2.py:185][0m |          -0.0022 |         239.7614 |         -44.9347 |
[32m[20221214 14:19:02 @agent_ppo2.py:185][0m |           0.0117 |         273.8592 |         -44.7942 |
[32m[20221214 14:19:02 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:19:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.36
[32m[20221214 14:19:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.00
[32m[20221214 14:19:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.21
[32m[20221214 14:19:02 @agent_ppo2.py:143][0m Total time:      21.00 min
[32m[20221214 14:19:02 @agent_ppo2.py:145][0m 1923072 total steps have happened
[32m[20221214 14:19:02 @agent_ppo2.py:121][0m #------------------------ Iteration 939 --------------------------#
[32m[20221214 14:19:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:03 @agent_ppo2.py:185][0m |          -0.0010 |         249.1992 |         -44.6727 |
[32m[20221214 14:19:03 @agent_ppo2.py:185][0m |          -0.0015 |         243.3391 |         -44.6522 |
[32m[20221214 14:19:03 @agent_ppo2.py:185][0m |           0.0042 |         244.5334 |         -44.5661 |
[32m[20221214 14:19:03 @agent_ppo2.py:185][0m |           0.0069 |         251.3160 |         -44.6633 |
[32m[20221214 14:19:03 @agent_ppo2.py:185][0m |          -0.0016 |         241.6638 |         -44.6521 |
[32m[20221214 14:19:03 @agent_ppo2.py:185][0m |          -0.0028 |         238.6877 |         -44.7884 |
[32m[20221214 14:19:03 @agent_ppo2.py:185][0m |          -0.0002 |         238.4482 |         -44.5973 |
[32m[20221214 14:19:03 @agent_ppo2.py:185][0m |          -0.0006 |         238.3776 |         -44.5564 |
[32m[20221214 14:19:03 @agent_ppo2.py:185][0m |          -0.0009 |         238.8216 |         -44.5822 |
[32m[20221214 14:19:03 @agent_ppo2.py:185][0m |          -0.0025 |         236.9827 |         -44.7443 |
[32m[20221214 14:19:03 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:19:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.63
[32m[20221214 14:19:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.70
[32m[20221214 14:19:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.10
[32m[20221214 14:19:04 @agent_ppo2.py:143][0m Total time:      21.03 min
[32m[20221214 14:19:04 @agent_ppo2.py:145][0m 1925120 total steps have happened
[32m[20221214 14:19:04 @agent_ppo2.py:121][0m #------------------------ Iteration 940 --------------------------#
[32m[20221214 14:19:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:19:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:04 @agent_ppo2.py:185][0m |          -0.0029 |         244.5824 |         -44.5443 |
[32m[20221214 14:19:04 @agent_ppo2.py:185][0m |           0.0108 |         277.1361 |         -44.6278 |
[32m[20221214 14:19:04 @agent_ppo2.py:185][0m |          -0.0025 |         240.7420 |         -44.5762 |
[32m[20221214 14:19:04 @agent_ppo2.py:185][0m |          -0.0026 |         239.1267 |         -44.6417 |
[32m[20221214 14:19:04 @agent_ppo2.py:185][0m |          -0.0023 |         237.5324 |         -44.7042 |
[32m[20221214 14:19:04 @agent_ppo2.py:185][0m |          -0.0016 |         236.8112 |         -44.5901 |
[32m[20221214 14:19:05 @agent_ppo2.py:185][0m |          -0.0050 |         236.4200 |         -44.6714 |
[32m[20221214 14:19:05 @agent_ppo2.py:185][0m |          -0.0025 |         236.3176 |         -44.4942 |
[32m[20221214 14:19:05 @agent_ppo2.py:185][0m |          -0.0042 |         235.9952 |         -44.7164 |
[32m[20221214 14:19:05 @agent_ppo2.py:185][0m |          -0.0039 |         235.2823 |         -44.6758 |
[32m[20221214 14:19:05 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:19:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.27
[32m[20221214 14:19:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.59
[32m[20221214 14:19:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.05
[32m[20221214 14:19:05 @agent_ppo2.py:143][0m Total time:      21.05 min
[32m[20221214 14:19:05 @agent_ppo2.py:145][0m 1927168 total steps have happened
[32m[20221214 14:19:05 @agent_ppo2.py:121][0m #------------------------ Iteration 941 --------------------------#
[32m[20221214 14:19:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:05 @agent_ppo2.py:185][0m |           0.0071 |         258.8134 |         -44.6063 |
[32m[20221214 14:19:05 @agent_ppo2.py:185][0m |          -0.0027 |         249.0912 |         -44.6009 |
[32m[20221214 14:19:06 @agent_ppo2.py:185][0m |           0.0208 |         288.7911 |         -44.3511 |
[32m[20221214 14:19:06 @agent_ppo2.py:185][0m |          -0.0007 |         252.5484 |         -44.3935 |
[32m[20221214 14:19:06 @agent_ppo2.py:185][0m |          -0.0024 |         246.1326 |         -44.5622 |
[32m[20221214 14:19:06 @agent_ppo2.py:185][0m |          -0.0015 |         245.9075 |         -44.5733 |
[32m[20221214 14:19:06 @agent_ppo2.py:185][0m |          -0.0014 |         245.6246 |         -44.5816 |
[32m[20221214 14:19:06 @agent_ppo2.py:185][0m |          -0.0004 |         245.1826 |         -44.4156 |
[32m[20221214 14:19:06 @agent_ppo2.py:185][0m |          -0.0020 |         245.0563 |         -44.5957 |
[32m[20221214 14:19:06 @agent_ppo2.py:185][0m |          -0.0023 |         244.6145 |         -44.5493 |
[32m[20221214 14:19:06 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:19:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.28
[32m[20221214 14:19:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.81
[32m[20221214 14:19:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.48
[32m[20221214 14:19:06 @agent_ppo2.py:143][0m Total time:      21.07 min
[32m[20221214 14:19:06 @agent_ppo2.py:145][0m 1929216 total steps have happened
[32m[20221214 14:19:06 @agent_ppo2.py:121][0m #------------------------ Iteration 942 --------------------------#
[32m[20221214 14:19:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:07 @agent_ppo2.py:185][0m |          -0.0037 |         259.4182 |         -45.4722 |
[32m[20221214 14:19:07 @agent_ppo2.py:185][0m |          -0.0017 |         255.6051 |         -45.6437 |
[32m[20221214 14:19:07 @agent_ppo2.py:185][0m |          -0.0023 |         253.9897 |         -45.7295 |
[32m[20221214 14:19:07 @agent_ppo2.py:185][0m |           0.0033 |         255.8626 |         -45.7703 |
[32m[20221214 14:19:07 @agent_ppo2.py:185][0m |          -0.0006 |         251.6271 |         -46.1210 |
[32m[20221214 14:19:07 @agent_ppo2.py:185][0m |           0.0103 |         277.4805 |         -46.0547 |
[32m[20221214 14:19:07 @agent_ppo2.py:185][0m |          -0.0033 |         250.8740 |         -46.0863 |
[32m[20221214 14:19:07 @agent_ppo2.py:185][0m |          -0.0041 |         250.1626 |         -46.0474 |
[32m[20221214 14:19:07 @agent_ppo2.py:185][0m |          -0.0032 |         249.6095 |         -46.2334 |
[32m[20221214 14:19:08 @agent_ppo2.py:185][0m |          -0.0006 |         250.3019 |         -46.0876 |
[32m[20221214 14:19:08 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:19:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.95
[32m[20221214 14:19:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.14
[32m[20221214 14:19:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.83
[32m[20221214 14:19:08 @agent_ppo2.py:143][0m Total time:      21.09 min
[32m[20221214 14:19:08 @agent_ppo2.py:145][0m 1931264 total steps have happened
[32m[20221214 14:19:08 @agent_ppo2.py:121][0m #------------------------ Iteration 943 --------------------------#
[32m[20221214 14:19:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:08 @agent_ppo2.py:185][0m |          -0.0027 |         249.3756 |         -46.5348 |
[32m[20221214 14:19:08 @agent_ppo2.py:185][0m |          -0.0028 |         238.5609 |         -46.7059 |
[32m[20221214 14:19:08 @agent_ppo2.py:185][0m |          -0.0020 |         236.1484 |         -46.7243 |
[32m[20221214 14:19:08 @agent_ppo2.py:185][0m |          -0.0025 |         234.8275 |         -46.5696 |
[32m[20221214 14:19:08 @agent_ppo2.py:185][0m |          -0.0028 |         235.2705 |         -46.6520 |
[32m[20221214 14:19:09 @agent_ppo2.py:185][0m |          -0.0028 |         234.5779 |         -46.4367 |
[32m[20221214 14:19:09 @agent_ppo2.py:185][0m |          -0.0036 |         234.4874 |         -46.7375 |
[32m[20221214 14:19:09 @agent_ppo2.py:185][0m |          -0.0040 |         234.1144 |         -46.4715 |
[32m[20221214 14:19:09 @agent_ppo2.py:185][0m |           0.0075 |         255.5449 |         -46.4764 |
[32m[20221214 14:19:09 @agent_ppo2.py:185][0m |          -0.0019 |         233.7590 |         -46.7494 |
[32m[20221214 14:19:09 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:19:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.21
[32m[20221214 14:19:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.66
[32m[20221214 14:19:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.75
[32m[20221214 14:19:09 @agent_ppo2.py:143][0m Total time:      21.12 min
[32m[20221214 14:19:09 @agent_ppo2.py:145][0m 1933312 total steps have happened
[32m[20221214 14:19:09 @agent_ppo2.py:121][0m #------------------------ Iteration 944 --------------------------#
[32m[20221214 14:19:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:09 @agent_ppo2.py:185][0m |          -0.0009 |         240.4545 |         -45.9573 |
[32m[20221214 14:19:10 @agent_ppo2.py:185][0m |          -0.0027 |         233.5605 |         -45.8025 |
[32m[20221214 14:19:10 @agent_ppo2.py:185][0m |           0.0028 |         235.6195 |         -45.9726 |
[32m[20221214 14:19:10 @agent_ppo2.py:185][0m |          -0.0031 |         232.8760 |         -45.8558 |
[32m[20221214 14:19:10 @agent_ppo2.py:185][0m |          -0.0031 |         232.3309 |         -45.8287 |
[32m[20221214 14:19:10 @agent_ppo2.py:185][0m |          -0.0006 |         232.1240 |         -45.8830 |
[32m[20221214 14:19:10 @agent_ppo2.py:185][0m |          -0.0028 |         232.2778 |         -45.7936 |
[32m[20221214 14:19:10 @agent_ppo2.py:185][0m |          -0.0011 |         232.2592 |         -45.9225 |
[32m[20221214 14:19:10 @agent_ppo2.py:185][0m |          -0.0010 |         231.5093 |         -45.7811 |
[32m[20221214 14:19:10 @agent_ppo2.py:185][0m |           0.0025 |         234.4328 |         -45.9843 |
[32m[20221214 14:19:10 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:19:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.46
[32m[20221214 14:19:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.32
[32m[20221214 14:19:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.29
[32m[20221214 14:19:10 @agent_ppo2.py:143][0m Total time:      21.14 min
[32m[20221214 14:19:10 @agent_ppo2.py:145][0m 1935360 total steps have happened
[32m[20221214 14:19:10 @agent_ppo2.py:121][0m #------------------------ Iteration 945 --------------------------#
[32m[20221214 14:19:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:11 @agent_ppo2.py:185][0m |          -0.0008 |         245.9188 |         -47.4016 |
[32m[20221214 14:19:11 @agent_ppo2.py:185][0m |          -0.0020 |         243.8260 |         -47.4383 |
[32m[20221214 14:19:11 @agent_ppo2.py:185][0m |          -0.0021 |         242.1107 |         -47.3587 |
[32m[20221214 14:19:11 @agent_ppo2.py:185][0m |          -0.0014 |         241.0377 |         -47.3519 |
[32m[20221214 14:19:11 @agent_ppo2.py:185][0m |          -0.0021 |         241.0890 |         -47.2977 |
[32m[20221214 14:19:11 @agent_ppo2.py:185][0m |          -0.0027 |         240.0736 |         -47.5400 |
[32m[20221214 14:19:11 @agent_ppo2.py:185][0m |           0.0019 |         241.2086 |         -47.3143 |
[32m[20221214 14:19:11 @agent_ppo2.py:185][0m |          -0.0014 |         240.6195 |         -47.3710 |
[32m[20221214 14:19:12 @agent_ppo2.py:185][0m |          -0.0018 |         239.2651 |         -47.5245 |
[32m[20221214 14:19:12 @agent_ppo2.py:185][0m |          -0.0009 |         238.6445 |         -47.4199 |
[32m[20221214 14:19:12 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:19:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.99
[32m[20221214 14:19:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.45
[32m[20221214 14:19:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.76
[32m[20221214 14:19:12 @agent_ppo2.py:143][0m Total time:      21.16 min
[32m[20221214 14:19:12 @agent_ppo2.py:145][0m 1937408 total steps have happened
[32m[20221214 14:19:12 @agent_ppo2.py:121][0m #------------------------ Iteration 946 --------------------------#
[32m[20221214 14:19:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:12 @agent_ppo2.py:185][0m |          -0.0002 |         235.6984 |         -45.7857 |
[32m[20221214 14:19:12 @agent_ppo2.py:185][0m |           0.0103 |         249.5385 |         -45.7302 |
[32m[20221214 14:19:12 @agent_ppo2.py:185][0m |          -0.0005 |         226.3998 |         -45.6602 |
[32m[20221214 14:19:12 @agent_ppo2.py:185][0m |          -0.0005 |         225.4010 |         -45.6482 |
[32m[20221214 14:19:13 @agent_ppo2.py:185][0m |          -0.0008 |         224.5583 |         -45.5745 |
[32m[20221214 14:19:13 @agent_ppo2.py:185][0m |          -0.0014 |         223.7963 |         -45.4784 |
[32m[20221214 14:19:13 @agent_ppo2.py:185][0m |          -0.0022 |         223.5816 |         -45.4956 |
[32m[20221214 14:19:13 @agent_ppo2.py:185][0m |          -0.0017 |         223.0179 |         -45.4701 |
[32m[20221214 14:19:13 @agent_ppo2.py:185][0m |          -0.0019 |         223.0832 |         -45.4193 |
[32m[20221214 14:19:13 @agent_ppo2.py:185][0m |          -0.0020 |         222.8551 |         -45.4622 |
[32m[20221214 14:19:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:19:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.63
[32m[20221214 14:19:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.21
[32m[20221214 14:19:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.70
[32m[20221214 14:19:13 @agent_ppo2.py:143][0m Total time:      21.19 min
[32m[20221214 14:19:13 @agent_ppo2.py:145][0m 1939456 total steps have happened
[32m[20221214 14:19:13 @agent_ppo2.py:121][0m #------------------------ Iteration 947 --------------------------#
[32m[20221214 14:19:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:14 @agent_ppo2.py:185][0m |           0.0063 |         236.2003 |         -45.6062 |
[32m[20221214 14:19:14 @agent_ppo2.py:185][0m |          -0.0011 |         212.6857 |         -45.5695 |
[32m[20221214 14:19:14 @agent_ppo2.py:185][0m |          -0.0039 |         208.0043 |         -45.6770 |
[32m[20221214 14:19:14 @agent_ppo2.py:185][0m |          -0.0014 |         204.0705 |         -45.7554 |
[32m[20221214 14:19:14 @agent_ppo2.py:185][0m |           0.0008 |         202.9281 |         -45.5703 |
[32m[20221214 14:19:14 @agent_ppo2.py:185][0m |          -0.0032 |         200.1518 |         -45.7458 |
[32m[20221214 14:19:14 @agent_ppo2.py:185][0m |           0.0069 |         209.2329 |         -45.7325 |
[32m[20221214 14:19:14 @agent_ppo2.py:185][0m |           0.0001 |         197.6013 |         -45.7355 |
[32m[20221214 14:19:14 @agent_ppo2.py:185][0m |          -0.0026 |         196.4508 |         -45.6472 |
[32m[20221214 14:19:14 @agent_ppo2.py:185][0m |           0.0028 |         197.5450 |         -45.7482 |
[32m[20221214 14:19:14 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:19:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.35
[32m[20221214 14:19:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.13
[32m[20221214 14:19:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.82
[32m[20221214 14:19:15 @agent_ppo2.py:143][0m Total time:      21.21 min
[32m[20221214 14:19:15 @agent_ppo2.py:145][0m 1941504 total steps have happened
[32m[20221214 14:19:15 @agent_ppo2.py:121][0m #------------------------ Iteration 948 --------------------------#
[32m[20221214 14:19:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:15 @agent_ppo2.py:185][0m |           0.0014 |         215.9267 |         -45.6651 |
[32m[20221214 14:19:15 @agent_ppo2.py:185][0m |          -0.0011 |         203.7865 |         -45.7942 |
[32m[20221214 14:19:15 @agent_ppo2.py:185][0m |          -0.0040 |         200.5624 |         -45.9536 |
[32m[20221214 14:19:15 @agent_ppo2.py:185][0m |           0.0052 |         204.8710 |         -45.8850 |
[32m[20221214 14:19:15 @agent_ppo2.py:185][0m |          -0.0026 |         197.0679 |         -45.9716 |
[32m[20221214 14:19:15 @agent_ppo2.py:185][0m |           0.0010 |         194.4474 |         -45.7888 |
[32m[20221214 14:19:16 @agent_ppo2.py:185][0m |          -0.0018 |         192.7355 |         -46.0690 |
[32m[20221214 14:19:16 @agent_ppo2.py:185][0m |           0.0021 |         193.0496 |         -46.0191 |
[32m[20221214 14:19:16 @agent_ppo2.py:185][0m |          -0.0030 |         189.6736 |         -45.9793 |
[32m[20221214 14:19:16 @agent_ppo2.py:185][0m |          -0.0049 |         189.9254 |         -45.9536 |
[32m[20221214 14:19:16 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:19:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.83
[32m[20221214 14:19:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.37
[32m[20221214 14:19:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.99
[32m[20221214 14:19:16 @agent_ppo2.py:143][0m Total time:      21.23 min
[32m[20221214 14:19:16 @agent_ppo2.py:145][0m 1943552 total steps have happened
[32m[20221214 14:19:16 @agent_ppo2.py:121][0m #------------------------ Iteration 949 --------------------------#
[32m[20221214 14:19:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:19:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:16 @agent_ppo2.py:185][0m |           0.0009 |         204.8150 |         -46.3616 |
[32m[20221214 14:19:16 @agent_ppo2.py:185][0m |          -0.0038 |         197.3745 |         -46.3846 |
[32m[20221214 14:19:16 @agent_ppo2.py:185][0m |          -0.0001 |         194.0129 |         -46.3465 |
[32m[20221214 14:19:17 @agent_ppo2.py:185][0m |          -0.0005 |         192.8847 |         -46.4592 |
[32m[20221214 14:19:17 @agent_ppo2.py:185][0m |           0.0000 |         191.9754 |         -46.5314 |
[32m[20221214 14:19:17 @agent_ppo2.py:185][0m |          -0.0017 |         190.6227 |         -46.4405 |
[32m[20221214 14:19:17 @agent_ppo2.py:185][0m |           0.0015 |         190.5982 |         -46.6849 |
[32m[20221214 14:19:17 @agent_ppo2.py:185][0m |           0.0093 |         212.9741 |         -46.7166 |
[32m[20221214 14:19:17 @agent_ppo2.py:185][0m |          -0.0014 |         188.7986 |         -46.7901 |
[32m[20221214 14:19:17 @agent_ppo2.py:185][0m |           0.0063 |         199.6465 |         -46.7350 |
[32m[20221214 14:19:17 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:19:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.17
[32m[20221214 14:19:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.26
[32m[20221214 14:19:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.96
[32m[20221214 14:19:17 @agent_ppo2.py:143][0m Total time:      21.25 min
[32m[20221214 14:19:17 @agent_ppo2.py:145][0m 1945600 total steps have happened
[32m[20221214 14:19:17 @agent_ppo2.py:121][0m #------------------------ Iteration 950 --------------------------#
[32m[20221214 14:19:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:19:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:18 @agent_ppo2.py:185][0m |          -0.0019 |         220.5866 |         -46.8255 |
[32m[20221214 14:19:18 @agent_ppo2.py:185][0m |          -0.0015 |         211.6608 |         -47.0951 |
[32m[20221214 14:19:18 @agent_ppo2.py:185][0m |           0.0010 |         211.4325 |         -46.9188 |
[32m[20221214 14:19:18 @agent_ppo2.py:185][0m |          -0.0030 |         207.6607 |         -46.8060 |
[32m[20221214 14:19:18 @agent_ppo2.py:185][0m |          -0.0003 |         207.3288 |         -47.0686 |
[32m[20221214 14:19:18 @agent_ppo2.py:185][0m |           0.0040 |         211.7733 |         -46.8445 |
[32m[20221214 14:19:18 @agent_ppo2.py:185][0m |          -0.0050 |         204.0819 |         -46.8945 |
[32m[20221214 14:19:18 @agent_ppo2.py:185][0m |          -0.0035 |         203.4467 |         -46.8093 |
[32m[20221214 14:19:18 @agent_ppo2.py:185][0m |           0.0034 |         210.0869 |         -46.8436 |
[32m[20221214 14:19:18 @agent_ppo2.py:185][0m |          -0.0032 |         203.0112 |         -46.9100 |
[32m[20221214 14:19:18 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:19:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.32
[32m[20221214 14:19:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.35
[32m[20221214 14:19:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.58
[32m[20221214 14:19:19 @agent_ppo2.py:143][0m Total time:      21.27 min
[32m[20221214 14:19:19 @agent_ppo2.py:145][0m 1947648 total steps have happened
[32m[20221214 14:19:19 @agent_ppo2.py:121][0m #------------------------ Iteration 951 --------------------------#
[32m[20221214 14:19:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:19 @agent_ppo2.py:185][0m |           0.0146 |         248.8572 |         -46.5050 |
[32m[20221214 14:19:19 @agent_ppo2.py:185][0m |          -0.0015 |         219.5795 |         -46.4826 |
[32m[20221214 14:19:19 @agent_ppo2.py:185][0m |           0.0003 |         216.9677 |         -46.4775 |
[32m[20221214 14:19:19 @agent_ppo2.py:185][0m |          -0.0010 |         214.2244 |         -46.3289 |
[32m[20221214 14:19:19 @agent_ppo2.py:185][0m |          -0.0002 |         212.5045 |         -46.4715 |
[32m[20221214 14:19:19 @agent_ppo2.py:185][0m |           0.0025 |         212.9215 |         -46.4763 |
[32m[20221214 14:19:20 @agent_ppo2.py:185][0m |          -0.0020 |         209.9684 |         -46.4627 |
[32m[20221214 14:19:20 @agent_ppo2.py:185][0m |          -0.0020 |         209.4532 |         -46.4624 |
[32m[20221214 14:19:20 @agent_ppo2.py:185][0m |          -0.0007 |         208.8060 |         -46.4221 |
[32m[20221214 14:19:20 @agent_ppo2.py:185][0m |          -0.0032 |         207.7612 |         -46.4738 |
[32m[20221214 14:19:20 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:19:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.65
[32m[20221214 14:19:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.32
[32m[20221214 14:19:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.25
[32m[20221214 14:19:20 @agent_ppo2.py:143][0m Total time:      21.30 min
[32m[20221214 14:19:20 @agent_ppo2.py:145][0m 1949696 total steps have happened
[32m[20221214 14:19:20 @agent_ppo2.py:121][0m #------------------------ Iteration 952 --------------------------#
[32m[20221214 14:19:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:20 @agent_ppo2.py:185][0m |          -0.0006 |         265.5506 |         -47.9641 |
[32m[20221214 14:19:20 @agent_ppo2.py:185][0m |          -0.0019 |         218.5748 |         -47.8226 |
[32m[20221214 14:19:21 @agent_ppo2.py:185][0m |          -0.0034 |         204.4561 |         -47.6892 |
[32m[20221214 14:19:21 @agent_ppo2.py:185][0m |          -0.0020 |         195.7124 |         -47.7036 |
[32m[20221214 14:19:21 @agent_ppo2.py:185][0m |          -0.0026 |         191.2880 |         -47.7537 |
[32m[20221214 14:19:21 @agent_ppo2.py:185][0m |          -0.0109 |         187.3694 |         -47.6381 |
[32m[20221214 14:19:21 @agent_ppo2.py:185][0m |           0.0047 |         185.3791 |         -47.5629 |
[32m[20221214 14:19:21 @agent_ppo2.py:185][0m |          -0.0013 |         181.2864 |         -47.5516 |
[32m[20221214 14:19:21 @agent_ppo2.py:185][0m |           0.0157 |         189.2043 |         -47.6787 |
[32m[20221214 14:19:21 @agent_ppo2.py:185][0m |           0.0018 |         179.1725 |         -47.7625 |
[32m[20221214 14:19:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:19:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 755.29
[32m[20221214 14:19:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.97
[32m[20221214 14:19:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.71
[32m[20221214 14:19:21 @agent_ppo2.py:143][0m Total time:      21.32 min
[32m[20221214 14:19:21 @agent_ppo2.py:145][0m 1951744 total steps have happened
[32m[20221214 14:19:21 @agent_ppo2.py:121][0m #------------------------ Iteration 953 --------------------------#
[32m[20221214 14:19:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:22 @agent_ppo2.py:185][0m |           0.0003 |         216.0801 |         -45.4208 |
[32m[20221214 14:19:22 @agent_ppo2.py:185][0m |          -0.0035 |         201.4845 |         -45.5023 |
[32m[20221214 14:19:22 @agent_ppo2.py:185][0m |           0.0058 |         205.9226 |         -45.4670 |
[32m[20221214 14:19:22 @agent_ppo2.py:185][0m |           0.0036 |         195.0203 |         -45.4507 |
[32m[20221214 14:19:22 @agent_ppo2.py:185][0m |          -0.0025 |         187.1582 |         -45.3835 |
[32m[20221214 14:19:22 @agent_ppo2.py:185][0m |           0.0010 |         187.2742 |         -45.4168 |
[32m[20221214 14:19:22 @agent_ppo2.py:185][0m |          -0.0016 |         183.7810 |         -45.3099 |
[32m[20221214 14:19:22 @agent_ppo2.py:185][0m |          -0.0032 |         182.4261 |         -45.2738 |
[32m[20221214 14:19:22 @agent_ppo2.py:185][0m |          -0.0032 |         180.9378 |         -45.2824 |
[32m[20221214 14:19:23 @agent_ppo2.py:185][0m |          -0.0026 |         179.6559 |         -45.2314 |
[32m[20221214 14:19:23 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:19:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.58
[32m[20221214 14:19:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.75
[32m[20221214 14:19:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 765.84
[32m[20221214 14:19:23 @agent_ppo2.py:143][0m Total time:      21.34 min
[32m[20221214 14:19:23 @agent_ppo2.py:145][0m 1953792 total steps have happened
[32m[20221214 14:19:23 @agent_ppo2.py:121][0m #------------------------ Iteration 954 --------------------------#
[32m[20221214 14:19:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:19:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:23 @agent_ppo2.py:185][0m |           0.0007 |         187.4872 |         -45.0607 |
[32m[20221214 14:19:23 @agent_ppo2.py:185][0m |          -0.0006 |         174.3260 |         -44.8959 |
[32m[20221214 14:19:23 @agent_ppo2.py:185][0m |          -0.0040 |         166.3436 |         -44.8402 |
[32m[20221214 14:19:23 @agent_ppo2.py:185][0m |          -0.0056 |         161.0744 |         -44.7615 |
[32m[20221214 14:19:23 @agent_ppo2.py:185][0m |          -0.0013 |         156.0327 |         -44.6355 |
[32m[20221214 14:19:24 @agent_ppo2.py:185][0m |          -0.0036 |         153.2199 |         -44.7003 |
[32m[20221214 14:19:24 @agent_ppo2.py:185][0m |           0.0082 |         162.7684 |         -44.5796 |
[32m[20221214 14:19:24 @agent_ppo2.py:185][0m |          -0.0030 |         148.5137 |         -44.5496 |
[32m[20221214 14:19:24 @agent_ppo2.py:185][0m |          -0.0035 |         146.9958 |         -44.5426 |
[32m[20221214 14:19:24 @agent_ppo2.py:185][0m |          -0.0065 |         145.3665 |         -44.3480 |
[32m[20221214 14:19:24 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:19:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.28
[32m[20221214 14:19:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.54
[32m[20221214 14:19:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.70
[32m[20221214 14:19:24 @agent_ppo2.py:143][0m Total time:      21.37 min
[32m[20221214 14:19:24 @agent_ppo2.py:145][0m 1955840 total steps have happened
[32m[20221214 14:19:24 @agent_ppo2.py:121][0m #------------------------ Iteration 955 --------------------------#
[32m[20221214 14:19:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:24 @agent_ppo2.py:185][0m |          -0.0008 |         281.1380 |         -45.0967 |
[32m[20221214 14:19:25 @agent_ppo2.py:185][0m |          -0.0033 |         251.5604 |         -44.9770 |
[32m[20221214 14:19:25 @agent_ppo2.py:185][0m |          -0.0012 |         237.6392 |         -44.9439 |
[32m[20221214 14:19:25 @agent_ppo2.py:185][0m |          -0.0002 |         232.2752 |         -45.0361 |
[32m[20221214 14:19:25 @agent_ppo2.py:185][0m |          -0.0023 |         228.6101 |         -44.9339 |
[32m[20221214 14:19:25 @agent_ppo2.py:185][0m |          -0.0015 |         226.3110 |         -45.1230 |
[32m[20221214 14:19:25 @agent_ppo2.py:185][0m |           0.0021 |         224.2226 |         -45.0508 |
[32m[20221214 14:19:25 @agent_ppo2.py:185][0m |          -0.0025 |         222.3517 |         -45.0460 |
[32m[20221214 14:19:25 @agent_ppo2.py:185][0m |          -0.0005 |         220.0024 |         -45.1686 |
[32m[20221214 14:19:25 @agent_ppo2.py:185][0m |          -0.0020 |         219.0132 |         -45.1291 |
[32m[20221214 14:19:25 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 14:19:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.63
[32m[20221214 14:19:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.07
[32m[20221214 14:19:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.49
[32m[20221214 14:19:26 @agent_ppo2.py:143][0m Total time:      21.39 min
[32m[20221214 14:19:26 @agent_ppo2.py:145][0m 1957888 total steps have happened
[32m[20221214 14:19:26 @agent_ppo2.py:121][0m #------------------------ Iteration 956 --------------------------#
[32m[20221214 14:19:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:26 @agent_ppo2.py:185][0m |          -0.0005 |         198.7677 |         -45.1896 |
[32m[20221214 14:19:26 @agent_ppo2.py:185][0m |          -0.0015 |         187.6583 |         -45.2417 |
[32m[20221214 14:19:26 @agent_ppo2.py:185][0m |           0.0012 |         183.6931 |         -45.2412 |
[32m[20221214 14:19:26 @agent_ppo2.py:185][0m |          -0.0032 |         180.6321 |         -45.3641 |
[32m[20221214 14:19:26 @agent_ppo2.py:185][0m |          -0.0023 |         178.6706 |         -45.3024 |
[32m[20221214 14:19:26 @agent_ppo2.py:185][0m |          -0.0032 |         176.8756 |         -45.4239 |
[32m[20221214 14:19:26 @agent_ppo2.py:185][0m |          -0.0033 |         176.0542 |         -45.4751 |
[32m[20221214 14:19:27 @agent_ppo2.py:185][0m |          -0.0027 |         174.8659 |         -45.4871 |
[32m[20221214 14:19:27 @agent_ppo2.py:185][0m |          -0.0016 |         174.3978 |         -45.5301 |
[32m[20221214 14:19:27 @agent_ppo2.py:185][0m |          -0.0015 |         173.3237 |         -45.7533 |
[32m[20221214 14:19:27 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:19:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.25
[32m[20221214 14:19:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.71
[32m[20221214 14:19:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.16
[32m[20221214 14:19:27 @agent_ppo2.py:143][0m Total time:      21.41 min
[32m[20221214 14:19:27 @agent_ppo2.py:145][0m 1959936 total steps have happened
[32m[20221214 14:19:27 @agent_ppo2.py:121][0m #------------------------ Iteration 957 --------------------------#
[32m[20221214 14:19:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:27 @agent_ppo2.py:185][0m |          -0.0033 |         214.3614 |         -45.6838 |
[32m[20221214 14:19:27 @agent_ppo2.py:185][0m |          -0.0031 |         204.3856 |         -45.2533 |
[32m[20221214 14:19:27 @agent_ppo2.py:185][0m |           0.0018 |         213.4360 |         -45.3563 |
[32m[20221214 14:19:28 @agent_ppo2.py:185][0m |           0.0025 |         205.4209 |         -44.9763 |
[32m[20221214 14:19:28 @agent_ppo2.py:185][0m |           0.0038 |         196.4213 |         -45.1325 |
[32m[20221214 14:19:28 @agent_ppo2.py:185][0m |          -0.0041 |         191.3112 |         -45.1387 |
[32m[20221214 14:19:28 @agent_ppo2.py:185][0m |          -0.0039 |         189.4547 |         -45.1278 |
[32m[20221214 14:19:28 @agent_ppo2.py:185][0m |          -0.0053 |         188.9831 |         -45.0172 |
[32m[20221214 14:19:28 @agent_ppo2.py:185][0m |          -0.0064 |         187.8379 |         -45.0625 |
[32m[20221214 14:19:28 @agent_ppo2.py:185][0m |          -0.0046 |         186.9617 |         -44.8441 |
[32m[20221214 14:19:28 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:19:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.31
[32m[20221214 14:19:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.23
[32m[20221214 14:19:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 731.38
[32m[20221214 14:19:28 @agent_ppo2.py:143][0m Total time:      21.44 min
[32m[20221214 14:19:28 @agent_ppo2.py:145][0m 1961984 total steps have happened
[32m[20221214 14:19:28 @agent_ppo2.py:121][0m #------------------------ Iteration 958 --------------------------#
[32m[20221214 14:19:28 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:19:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:29 @agent_ppo2.py:185][0m |           0.0061 |         281.8029 |         -46.0518 |
[32m[20221214 14:19:29 @agent_ppo2.py:185][0m |          -0.0020 |         195.6789 |         -45.8632 |
[32m[20221214 14:19:29 @agent_ppo2.py:185][0m |          -0.0028 |         172.7825 |         -45.8453 |
[32m[20221214 14:19:29 @agent_ppo2.py:185][0m |          -0.0002 |         162.6159 |         -45.9380 |
[32m[20221214 14:19:29 @agent_ppo2.py:185][0m |          -0.0010 |         155.8148 |         -45.9538 |
[32m[20221214 14:19:29 @agent_ppo2.py:185][0m |           0.0062 |         158.6190 |         -46.0092 |
[32m[20221214 14:19:29 @agent_ppo2.py:185][0m |          -0.0069 |         148.2904 |         -45.9038 |
[32m[20221214 14:19:29 @agent_ppo2.py:185][0m |          -0.0038 |         145.8302 |         -46.0319 |
[32m[20221214 14:19:29 @agent_ppo2.py:185][0m |          -0.0025 |         143.7269 |         -45.9597 |
[32m[20221214 14:19:29 @agent_ppo2.py:185][0m |          -0.0062 |         141.7000 |         -45.9608 |
[32m[20221214 14:19:29 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:19:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 702.03
[32m[20221214 14:19:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 765.43
[32m[20221214 14:19:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.74
[32m[20221214 14:19:30 @agent_ppo2.py:143][0m Total time:      21.46 min
[32m[20221214 14:19:30 @agent_ppo2.py:145][0m 1964032 total steps have happened
[32m[20221214 14:19:30 @agent_ppo2.py:121][0m #------------------------ Iteration 959 --------------------------#
[32m[20221214 14:19:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:19:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:30 @agent_ppo2.py:185][0m |          -0.0017 |         192.4259 |         -45.4207 |
[32m[20221214 14:19:30 @agent_ppo2.py:185][0m |           0.0126 |         208.0592 |         -45.3966 |
[32m[20221214 14:19:30 @agent_ppo2.py:185][0m |          -0.0001 |         173.9982 |         -45.3548 |
[32m[20221214 14:19:30 @agent_ppo2.py:185][0m |          -0.0043 |         169.8520 |         -45.2347 |
[32m[20221214 14:19:30 @agent_ppo2.py:185][0m |           0.0008 |         167.6319 |         -45.3284 |
[32m[20221214 14:19:30 @agent_ppo2.py:185][0m |           0.0120 |         177.8097 |         -45.3467 |
[32m[20221214 14:19:30 @agent_ppo2.py:185][0m |          -0.0006 |         165.1342 |         -45.3873 |
[32m[20221214 14:19:31 @agent_ppo2.py:185][0m |          -0.0014 |         163.7267 |         -45.3670 |
[32m[20221214 14:19:31 @agent_ppo2.py:185][0m |          -0.0009 |         162.6669 |         -45.3815 |
[32m[20221214 14:19:31 @agent_ppo2.py:185][0m |          -0.0000 |         161.6659 |         -45.4371 |
[32m[20221214 14:19:31 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:19:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.88
[32m[20221214 14:19:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.08
[32m[20221214 14:19:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 779.49
[32m[20221214 14:19:31 @agent_ppo2.py:143][0m Total time:      21.48 min
[32m[20221214 14:19:31 @agent_ppo2.py:145][0m 1966080 total steps have happened
[32m[20221214 14:19:31 @agent_ppo2.py:121][0m #------------------------ Iteration 960 --------------------------#
[32m[20221214 14:19:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:19:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:31 @agent_ppo2.py:185][0m |          -0.0013 |         171.1668 |         -45.4538 |
[32m[20221214 14:19:31 @agent_ppo2.py:185][0m |          -0.0007 |         165.4750 |         -45.5367 |
[32m[20221214 14:19:31 @agent_ppo2.py:185][0m |          -0.0015 |         163.6685 |         -45.6479 |
[32m[20221214 14:19:31 @agent_ppo2.py:185][0m |           0.0046 |         165.5945 |         -45.7404 |
[32m[20221214 14:19:32 @agent_ppo2.py:185][0m |          -0.0020 |         162.0166 |         -45.7109 |
[32m[20221214 14:19:32 @agent_ppo2.py:185][0m |          -0.0033 |         161.1289 |         -45.8292 |
[32m[20221214 14:19:32 @agent_ppo2.py:185][0m |          -0.0037 |         160.7539 |         -45.9380 |
[32m[20221214 14:19:32 @agent_ppo2.py:185][0m |          -0.0024 |         160.4167 |         -45.8664 |
[32m[20221214 14:19:32 @agent_ppo2.py:185][0m |          -0.0025 |         159.5447 |         -45.9548 |
[32m[20221214 14:19:32 @agent_ppo2.py:185][0m |          -0.0026 |         159.3403 |         -46.0681 |
[32m[20221214 14:19:32 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:19:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.84
[32m[20221214 14:19:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 781.34
[32m[20221214 14:19:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.30
[32m[20221214 14:19:32 @agent_ppo2.py:143][0m Total time:      21.50 min
[32m[20221214 14:19:32 @agent_ppo2.py:145][0m 1968128 total steps have happened
[32m[20221214 14:19:32 @agent_ppo2.py:121][0m #------------------------ Iteration 961 --------------------------#
[32m[20221214 14:19:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:19:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:33 @agent_ppo2.py:185][0m |          -0.0014 |         195.6054 |         -46.9780 |
[32m[20221214 14:19:33 @agent_ppo2.py:185][0m |          -0.0018 |         187.2610 |         -46.8131 |
[32m[20221214 14:19:33 @agent_ppo2.py:185][0m |          -0.0032 |         184.5726 |         -46.5702 |
[32m[20221214 14:19:33 @agent_ppo2.py:185][0m |          -0.0042 |         181.5426 |         -46.3455 |
[32m[20221214 14:19:33 @agent_ppo2.py:185][0m |          -0.0013 |         180.2539 |         -46.3284 |
[32m[20221214 14:19:33 @agent_ppo2.py:185][0m |           0.0104 |         195.0174 |         -46.3150 |
[32m[20221214 14:19:33 @agent_ppo2.py:185][0m |          -0.0035 |         179.1391 |         -46.1751 |
[32m[20221214 14:19:33 @agent_ppo2.py:185][0m |          -0.0014 |         177.3819 |         -46.0251 |
[32m[20221214 14:19:33 @agent_ppo2.py:185][0m |          -0.0019 |         177.1174 |         -46.1063 |
[32m[20221214 14:19:33 @agent_ppo2.py:185][0m |          -0.0025 |         176.7707 |         -45.9299 |
[32m[20221214 14:19:33 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:19:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.21
[32m[20221214 14:19:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.90
[32m[20221214 14:19:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.87
[32m[20221214 14:19:33 @agent_ppo2.py:143][0m Total time:      21.52 min
[32m[20221214 14:19:33 @agent_ppo2.py:145][0m 1970176 total steps have happened
[32m[20221214 14:19:33 @agent_ppo2.py:121][0m #------------------------ Iteration 962 --------------------------#
[32m[20221214 14:19:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:19:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:34 @agent_ppo2.py:185][0m |          -0.0033 |         184.3372 |         -44.5467 |
[32m[20221214 14:19:34 @agent_ppo2.py:185][0m |          -0.0021 |         178.8560 |         -44.7125 |
[32m[20221214 14:19:34 @agent_ppo2.py:185][0m |          -0.0026 |         177.1646 |         -44.6650 |
[32m[20221214 14:19:34 @agent_ppo2.py:185][0m |          -0.0003 |         176.2921 |         -44.6635 |
[32m[20221214 14:19:34 @agent_ppo2.py:185][0m |          -0.0012 |         176.5441 |         -44.7298 |
[32m[20221214 14:19:34 @agent_ppo2.py:185][0m |          -0.0021 |         175.3882 |         -44.8840 |
[32m[20221214 14:19:34 @agent_ppo2.py:185][0m |          -0.0010 |         174.7666 |         -44.9394 |
[32m[20221214 14:19:34 @agent_ppo2.py:185][0m |          -0.0022 |         174.8101 |         -44.9157 |
[32m[20221214 14:19:34 @agent_ppo2.py:185][0m |           0.0017 |         176.2600 |         -44.9835 |
[32m[20221214 14:19:35 @agent_ppo2.py:185][0m |          -0.0015 |         173.4637 |         -45.0503 |
[32m[20221214 14:19:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:19:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.94
[32m[20221214 14:19:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.61
[32m[20221214 14:19:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.97
[32m[20221214 14:19:35 @agent_ppo2.py:143][0m Total time:      21.54 min
[32m[20221214 14:19:35 @agent_ppo2.py:145][0m 1972224 total steps have happened
[32m[20221214 14:19:35 @agent_ppo2.py:121][0m #------------------------ Iteration 963 --------------------------#
[32m[20221214 14:19:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:35 @agent_ppo2.py:185][0m |           0.0011 |         214.2063 |         -44.4784 |
[32m[20221214 14:19:35 @agent_ppo2.py:185][0m |          -0.0012 |         207.5210 |         -44.2843 |
[32m[20221214 14:19:35 @agent_ppo2.py:185][0m |          -0.0012 |         204.3782 |         -44.3816 |
[32m[20221214 14:19:35 @agent_ppo2.py:185][0m |          -0.0015 |         202.3861 |         -44.5893 |
[32m[20221214 14:19:35 @agent_ppo2.py:185][0m |          -0.0026 |         201.2204 |         -44.4279 |
[32m[20221214 14:19:36 @agent_ppo2.py:185][0m |          -0.0026 |         200.6469 |         -44.5177 |
[32m[20221214 14:19:36 @agent_ppo2.py:185][0m |          -0.0007 |         200.6712 |         -44.6121 |
[32m[20221214 14:19:36 @agent_ppo2.py:185][0m |          -0.0035 |         199.6115 |         -44.5907 |
[32m[20221214 14:19:36 @agent_ppo2.py:185][0m |          -0.0006 |         199.4045 |         -44.5369 |
[32m[20221214 14:19:36 @agent_ppo2.py:185][0m |          -0.0021 |         198.4024 |         -44.5744 |
[32m[20221214 14:19:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:19:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.34
[32m[20221214 14:19:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.93
[32m[20221214 14:19:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.49
[32m[20221214 14:19:36 @agent_ppo2.py:143][0m Total time:      21.57 min
[32m[20221214 14:19:36 @agent_ppo2.py:145][0m 1974272 total steps have happened
[32m[20221214 14:19:36 @agent_ppo2.py:121][0m #------------------------ Iteration 964 --------------------------#
[32m[20221214 14:19:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:19:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:36 @agent_ppo2.py:185][0m |          -0.0020 |         198.2927 |         -45.2372 |
[32m[20221214 14:19:36 @agent_ppo2.py:185][0m |          -0.0002 |         190.1285 |         -45.1900 |
[32m[20221214 14:19:37 @agent_ppo2.py:185][0m |           0.0238 |         233.9283 |         -45.4041 |
[32m[20221214 14:19:37 @agent_ppo2.py:185][0m |          -0.0007 |         186.1545 |         -45.3126 |
[32m[20221214 14:19:37 @agent_ppo2.py:185][0m |           0.0039 |         186.5948 |         -45.4023 |
[32m[20221214 14:19:37 @agent_ppo2.py:185][0m |           0.0122 |         205.6932 |         -45.4118 |
[32m[20221214 14:19:37 @agent_ppo2.py:185][0m |          -0.0030 |         179.5816 |         -45.3452 |
[32m[20221214 14:19:37 @agent_ppo2.py:185][0m |          -0.0030 |         178.1249 |         -45.5109 |
[32m[20221214 14:19:37 @agent_ppo2.py:185][0m |          -0.0030 |         177.1393 |         -45.6106 |
[32m[20221214 14:19:37 @agent_ppo2.py:185][0m |           0.0137 |         199.9609 |         -45.5102 |
[32m[20221214 14:19:37 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:19:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 779.76
[32m[20221214 14:19:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.83
[32m[20221214 14:19:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 772.32
[32m[20221214 14:19:37 @agent_ppo2.py:143][0m Total time:      21.59 min
[32m[20221214 14:19:37 @agent_ppo2.py:145][0m 1976320 total steps have happened
[32m[20221214 14:19:37 @agent_ppo2.py:121][0m #------------------------ Iteration 965 --------------------------#
[32m[20221214 14:19:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:19:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:38 @agent_ppo2.py:185][0m |           0.0013 |         151.3658 |         -47.3366 |
[32m[20221214 14:19:38 @agent_ppo2.py:185][0m |           0.0169 |         168.2058 |         -47.2742 |
[32m[20221214 14:19:38 @agent_ppo2.py:185][0m |          -0.0048 |         141.1150 |         -47.4046 |
[32m[20221214 14:19:38 @agent_ppo2.py:185][0m |           0.0023 |         137.7737 |         -47.4232 |
[32m[20221214 14:19:38 @agent_ppo2.py:185][0m |          -0.0040 |         135.3786 |         -47.2978 |
[32m[20221214 14:19:38 @agent_ppo2.py:185][0m |          -0.0007 |         134.9354 |         -47.2403 |
[32m[20221214 14:19:38 @agent_ppo2.py:185][0m |          -0.0054 |         133.5458 |         -47.1984 |
[32m[20221214 14:19:38 @agent_ppo2.py:185][0m |          -0.0007 |         132.4559 |         -47.1460 |
[32m[20221214 14:19:38 @agent_ppo2.py:185][0m |          -0.0042 |         131.6736 |         -46.9718 |
[32m[20221214 14:19:38 @agent_ppo2.py:185][0m |          -0.0036 |         130.9129 |         -47.1080 |
[32m[20221214 14:19:38 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:19:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.63
[32m[20221214 14:19:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.40
[32m[20221214 14:19:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.49
[32m[20221214 14:19:39 @agent_ppo2.py:143][0m Total time:      21.61 min
[32m[20221214 14:19:39 @agent_ppo2.py:145][0m 1978368 total steps have happened
[32m[20221214 14:19:39 @agent_ppo2.py:121][0m #------------------------ Iteration 966 --------------------------#
[32m[20221214 14:19:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:19:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:39 @agent_ppo2.py:185][0m |          -0.0005 |         156.1545 |         -46.7637 |
[32m[20221214 14:19:39 @agent_ppo2.py:185][0m |          -0.0017 |         150.4431 |         -46.7483 |
[32m[20221214 14:19:39 @agent_ppo2.py:185][0m |          -0.0023 |         147.1675 |         -46.5354 |
[32m[20221214 14:19:39 @agent_ppo2.py:185][0m |          -0.0036 |         145.3349 |         -46.7327 |
[32m[20221214 14:19:39 @agent_ppo2.py:185][0m |          -0.0016 |         143.9934 |         -46.8334 |
[32m[20221214 14:19:39 @agent_ppo2.py:185][0m |          -0.0054 |         141.1554 |         -46.8740 |
[32m[20221214 14:19:39 @agent_ppo2.py:185][0m |           0.0049 |         147.3940 |         -46.9152 |
[32m[20221214 14:19:39 @agent_ppo2.py:185][0m |          -0.0043 |         138.3916 |         -46.8381 |
[32m[20221214 14:19:40 @agent_ppo2.py:185][0m |          -0.0027 |         137.8906 |         -46.8748 |
[32m[20221214 14:19:40 @agent_ppo2.py:185][0m |          -0.0063 |         135.8643 |         -46.9566 |
[32m[20221214 14:19:40 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:19:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.23
[32m[20221214 14:19:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.56
[32m[20221214 14:19:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.07
[32m[20221214 14:19:40 @agent_ppo2.py:143][0m Total time:      21.63 min
[32m[20221214 14:19:40 @agent_ppo2.py:145][0m 1980416 total steps have happened
[32m[20221214 14:19:40 @agent_ppo2.py:121][0m #------------------------ Iteration 967 --------------------------#
[32m[20221214 14:19:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:19:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:40 @agent_ppo2.py:185][0m |          -0.0017 |         219.0734 |         -47.2219 |
[32m[20221214 14:19:40 @agent_ppo2.py:185][0m |           0.0000 |         208.5195 |         -47.1656 |
[32m[20221214 14:19:40 @agent_ppo2.py:185][0m |          -0.0028 |         205.7263 |         -46.9470 |
[32m[20221214 14:19:40 @agent_ppo2.py:185][0m |          -0.0015 |         204.6793 |         -46.9427 |
[32m[20221214 14:19:41 @agent_ppo2.py:185][0m |           0.0019 |         203.7132 |         -46.9803 |
[32m[20221214 14:19:41 @agent_ppo2.py:185][0m |           0.0079 |         212.7096 |         -46.7261 |
[32m[20221214 14:19:41 @agent_ppo2.py:185][0m |          -0.0017 |         200.1474 |         -46.6713 |
[32m[20221214 14:19:41 @agent_ppo2.py:185][0m |          -0.0028 |         199.9970 |         -46.6210 |
[32m[20221214 14:19:41 @agent_ppo2.py:185][0m |          -0.0035 |         198.5649 |         -46.5337 |
[32m[20221214 14:19:41 @agent_ppo2.py:185][0m |          -0.0031 |         198.2105 |         -46.5387 |
[32m[20221214 14:19:41 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:19:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.53
[32m[20221214 14:19:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.83
[32m[20221214 14:19:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.41
[32m[20221214 14:19:41 @agent_ppo2.py:143][0m Total time:      21.65 min
[32m[20221214 14:19:41 @agent_ppo2.py:145][0m 1982464 total steps have happened
[32m[20221214 14:19:41 @agent_ppo2.py:121][0m #------------------------ Iteration 968 --------------------------#
[32m[20221214 14:19:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:19:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:41 @agent_ppo2.py:185][0m |          -0.0025 |         222.9110 |         -46.5922 |
[32m[20221214 14:19:42 @agent_ppo2.py:185][0m |          -0.0058 |         215.3238 |         -46.5183 |
[32m[20221214 14:19:42 @agent_ppo2.py:185][0m |          -0.0001 |         212.5857 |         -46.5460 |
[32m[20221214 14:19:42 @agent_ppo2.py:185][0m |          -0.0018 |         209.1163 |         -46.4243 |
[32m[20221214 14:19:42 @agent_ppo2.py:185][0m |          -0.0036 |         207.8935 |         -46.5389 |
[32m[20221214 14:19:42 @agent_ppo2.py:185][0m |          -0.0037 |         207.7472 |         -46.6096 |
[32m[20221214 14:19:42 @agent_ppo2.py:185][0m |          -0.0031 |         205.4663 |         -46.5234 |
[32m[20221214 14:19:42 @agent_ppo2.py:185][0m |          -0.0042 |         203.9861 |         -46.4131 |
[32m[20221214 14:19:42 @agent_ppo2.py:185][0m |          -0.0036 |         202.5136 |         -46.4206 |
[32m[20221214 14:19:42 @agent_ppo2.py:185][0m |          -0.0037 |         201.9147 |         -46.4066 |
[32m[20221214 14:19:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:19:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.82
[32m[20221214 14:19:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.34
[32m[20221214 14:19:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.62
[32m[20221214 14:19:42 @agent_ppo2.py:143][0m Total time:      21.67 min
[32m[20221214 14:19:42 @agent_ppo2.py:145][0m 1984512 total steps have happened
[32m[20221214 14:19:42 @agent_ppo2.py:121][0m #------------------------ Iteration 969 --------------------------#
[32m[20221214 14:19:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:19:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:43 @agent_ppo2.py:185][0m |           0.0039 |         243.2146 |         -44.8313 |
[32m[20221214 14:19:43 @agent_ppo2.py:185][0m |           0.0009 |         231.0352 |         -44.9058 |
[32m[20221214 14:19:43 @agent_ppo2.py:185][0m |          -0.0038 |         226.5835 |         -44.9437 |
[32m[20221214 14:19:43 @agent_ppo2.py:185][0m |          -0.0022 |         224.3348 |         -44.9115 |
[32m[20221214 14:19:43 @agent_ppo2.py:185][0m |          -0.0026 |         222.9317 |         -45.0720 |
[32m[20221214 14:19:43 @agent_ppo2.py:185][0m |          -0.0020 |         222.0203 |         -45.0043 |
[32m[20221214 14:19:43 @agent_ppo2.py:185][0m |           0.0011 |         221.3651 |         -45.0615 |
[32m[20221214 14:19:43 @agent_ppo2.py:185][0m |          -0.0020 |         220.3642 |         -45.1791 |
[32m[20221214 14:19:43 @agent_ppo2.py:185][0m |          -0.0010 |         219.8913 |         -44.9770 |
[32m[20221214 14:19:43 @agent_ppo2.py:185][0m |          -0.0032 |         219.3702 |         -45.1855 |
[32m[20221214 14:19:43 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:19:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.36
[32m[20221214 14:19:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.56
[32m[20221214 14:19:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.43
[32m[20221214 14:19:44 @agent_ppo2.py:143][0m Total time:      21.69 min
[32m[20221214 14:19:44 @agent_ppo2.py:145][0m 1986560 total steps have happened
[32m[20221214 14:19:44 @agent_ppo2.py:121][0m #------------------------ Iteration 970 --------------------------#
[32m[20221214 14:19:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:19:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:44 @agent_ppo2.py:185][0m |           0.0006 |         224.7457 |         -46.1083 |
[32m[20221214 14:19:44 @agent_ppo2.py:185][0m |          -0.0020 |         221.5774 |         -46.0286 |
[32m[20221214 14:19:44 @agent_ppo2.py:185][0m |           0.0146 |         252.6015 |         -46.0561 |
[32m[20221214 14:19:44 @agent_ppo2.py:185][0m |          -0.0026 |         219.4547 |         -46.0749 |
[32m[20221214 14:19:44 @agent_ppo2.py:185][0m |          -0.0021 |         218.6514 |         -45.9329 |
[32m[20221214 14:19:44 @agent_ppo2.py:185][0m |          -0.0014 |         217.0889 |         -46.1283 |
[32m[20221214 14:19:44 @agent_ppo2.py:185][0m |          -0.0023 |         216.3232 |         -46.2971 |
[32m[20221214 14:19:45 @agent_ppo2.py:185][0m |          -0.0033 |         216.1549 |         -46.3094 |
[32m[20221214 14:19:45 @agent_ppo2.py:185][0m |           0.0024 |         216.4272 |         -46.4574 |
[32m[20221214 14:19:45 @agent_ppo2.py:185][0m |          -0.0024 |         214.4956 |         -46.3427 |
[32m[20221214 14:19:45 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:19:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.13
[32m[20221214 14:19:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.35
[32m[20221214 14:19:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.60
[32m[20221214 14:19:45 @agent_ppo2.py:143][0m Total time:      21.71 min
[32m[20221214 14:19:45 @agent_ppo2.py:145][0m 1988608 total steps have happened
[32m[20221214 14:19:45 @agent_ppo2.py:121][0m #------------------------ Iteration 971 --------------------------#
[32m[20221214 14:19:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:45 @agent_ppo2.py:185][0m |           0.0078 |         220.9568 |         -48.4544 |
[32m[20221214 14:19:45 @agent_ppo2.py:185][0m |          -0.0026 |         203.8431 |         -48.5148 |
[32m[20221214 14:19:45 @agent_ppo2.py:185][0m |          -0.0012 |         202.5138 |         -48.6222 |
[32m[20221214 14:19:46 @agent_ppo2.py:185][0m |           0.0006 |         201.8552 |         -48.5064 |
[32m[20221214 14:19:46 @agent_ppo2.py:185][0m |           0.0046 |         206.4094 |         -48.6329 |
[32m[20221214 14:19:46 @agent_ppo2.py:185][0m |          -0.0025 |         200.7906 |         -48.4505 |
[32m[20221214 14:19:46 @agent_ppo2.py:185][0m |          -0.0026 |         200.7633 |         -48.6086 |
[32m[20221214 14:19:46 @agent_ppo2.py:185][0m |          -0.0025 |         200.3263 |         -48.5356 |
[32m[20221214 14:19:46 @agent_ppo2.py:185][0m |          -0.0029 |         200.0602 |         -48.6205 |
[32m[20221214 14:19:46 @agent_ppo2.py:185][0m |          -0.0013 |         199.7995 |         -48.6933 |
[32m[20221214 14:19:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:19:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.33
[32m[20221214 14:19:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.03
[32m[20221214 14:19:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.05
[32m[20221214 14:19:46 @agent_ppo2.py:143][0m Total time:      21.73 min
[32m[20221214 14:19:46 @agent_ppo2.py:145][0m 1990656 total steps have happened
[32m[20221214 14:19:46 @agent_ppo2.py:121][0m #------------------------ Iteration 972 --------------------------#
[32m[20221214 14:19:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:47 @agent_ppo2.py:185][0m |           0.0046 |         194.9584 |         -46.1843 |
[32m[20221214 14:19:47 @agent_ppo2.py:185][0m |          -0.0015 |         183.7158 |         -46.2103 |
[32m[20221214 14:19:47 @agent_ppo2.py:185][0m |          -0.0037 |         181.0041 |         -46.1758 |
[32m[20221214 14:19:47 @agent_ppo2.py:185][0m |          -0.0005 |         180.1248 |         -46.2943 |
[32m[20221214 14:19:47 @agent_ppo2.py:185][0m |          -0.0019 |         178.8811 |         -45.9219 |
[32m[20221214 14:19:47 @agent_ppo2.py:185][0m |          -0.0008 |         178.7959 |         -46.1935 |
[32m[20221214 14:19:47 @agent_ppo2.py:185][0m |          -0.0022 |         177.5384 |         -45.9793 |
[32m[20221214 14:19:47 @agent_ppo2.py:185][0m |          -0.0023 |         176.3549 |         -46.1289 |
[32m[20221214 14:19:47 @agent_ppo2.py:185][0m |          -0.0024 |         176.8377 |         -46.2473 |
[32m[20221214 14:19:47 @agent_ppo2.py:185][0m |          -0.0017 |         176.5424 |         -45.9955 |
[32m[20221214 14:19:47 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:19:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.47
[32m[20221214 14:19:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.95
[32m[20221214 14:19:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.83
[32m[20221214 14:19:48 @agent_ppo2.py:143][0m Total time:      21.76 min
[32m[20221214 14:19:48 @agent_ppo2.py:145][0m 1992704 total steps have happened
[32m[20221214 14:19:48 @agent_ppo2.py:121][0m #------------------------ Iteration 973 --------------------------#
[32m[20221214 14:19:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:48 @agent_ppo2.py:185][0m |          -0.0008 |         204.7250 |         -46.8233 |
[32m[20221214 14:19:48 @agent_ppo2.py:185][0m |          -0.0002 |         204.8850 |         -46.7182 |
[32m[20221214 14:19:48 @agent_ppo2.py:185][0m |          -0.0014 |         203.3898 |         -46.7726 |
[32m[20221214 14:19:48 @agent_ppo2.py:185][0m |           0.0013 |         204.6751 |         -46.8842 |
[32m[20221214 14:19:48 @agent_ppo2.py:185][0m |           0.0065 |         205.3079 |         -46.7919 |
[32m[20221214 14:19:48 @agent_ppo2.py:185][0m |           0.0011 |         203.3965 |         -46.8474 |
[32m[20221214 14:19:49 @agent_ppo2.py:185][0m |           0.0030 |         202.8449 |         -46.8018 |
[32m[20221214 14:19:49 @agent_ppo2.py:185][0m |           0.0096 |         220.1056 |         -46.7865 |
[32m[20221214 14:19:49 @agent_ppo2.py:185][0m |           0.0119 |         226.8440 |         -46.6605 |
[32m[20221214 14:19:49 @agent_ppo2.py:185][0m |          -0.0004 |         202.5935 |         -46.5713 |
[32m[20221214 14:19:49 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:19:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.44
[32m[20221214 14:19:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.63
[32m[20221214 14:19:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.73
[32m[20221214 14:19:49 @agent_ppo2.py:143][0m Total time:      21.78 min
[32m[20221214 14:19:49 @agent_ppo2.py:145][0m 1994752 total steps have happened
[32m[20221214 14:19:49 @agent_ppo2.py:121][0m #------------------------ Iteration 974 --------------------------#
[32m[20221214 14:19:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:49 @agent_ppo2.py:185][0m |           0.0000 |         199.3745 |         -46.7682 |
[32m[20221214 14:19:49 @agent_ppo2.py:185][0m |          -0.0015 |         193.5330 |         -46.7439 |
[32m[20221214 14:19:50 @agent_ppo2.py:185][0m |          -0.0016 |         192.5707 |         -46.7655 |
[32m[20221214 14:19:50 @agent_ppo2.py:185][0m |          -0.0013 |         191.5375 |         -47.0196 |
[32m[20221214 14:19:50 @agent_ppo2.py:185][0m |          -0.0015 |         192.1922 |         -47.0260 |
[32m[20221214 14:19:50 @agent_ppo2.py:185][0m |          -0.0041 |         191.0744 |         -47.1740 |
[32m[20221214 14:19:50 @agent_ppo2.py:185][0m |           0.0036 |         195.7986 |         -47.3025 |
[32m[20221214 14:19:50 @agent_ppo2.py:185][0m |          -0.0008 |         191.3196 |         -47.3666 |
[32m[20221214 14:19:50 @agent_ppo2.py:185][0m |          -0.0010 |         189.8634 |         -47.4473 |
[32m[20221214 14:19:50 @agent_ppo2.py:185][0m |          -0.0030 |         190.5036 |         -47.5357 |
[32m[20221214 14:19:50 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:19:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.48
[32m[20221214 14:19:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.74
[32m[20221214 14:19:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.08
[32m[20221214 14:19:50 @agent_ppo2.py:143][0m Total time:      21.81 min
[32m[20221214 14:19:50 @agent_ppo2.py:145][0m 1996800 total steps have happened
[32m[20221214 14:19:50 @agent_ppo2.py:121][0m #------------------------ Iteration 975 --------------------------#
[32m[20221214 14:19:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:51 @agent_ppo2.py:185][0m |           0.0113 |         186.7094 |         -48.3446 |
[32m[20221214 14:19:51 @agent_ppo2.py:185][0m |          -0.0035 |         174.9414 |         -48.3619 |
[32m[20221214 14:19:51 @agent_ppo2.py:185][0m |          -0.0029 |         173.1143 |         -48.2701 |
[32m[20221214 14:19:51 @agent_ppo2.py:185][0m |          -0.0039 |         171.6630 |         -48.2290 |
[32m[20221214 14:19:51 @agent_ppo2.py:185][0m |          -0.0019 |         170.4502 |         -48.1875 |
[32m[20221214 14:19:51 @agent_ppo2.py:185][0m |          -0.0040 |         169.6915 |         -48.1915 |
[32m[20221214 14:19:51 @agent_ppo2.py:185][0m |          -0.0036 |         168.3028 |         -48.0984 |
[32m[20221214 14:19:51 @agent_ppo2.py:185][0m |          -0.0055 |         169.0002 |         -48.0888 |
[32m[20221214 14:19:52 @agent_ppo2.py:185][0m |          -0.0038 |         168.2260 |         -48.0412 |
[32m[20221214 14:19:52 @agent_ppo2.py:185][0m |          -0.0047 |         168.0926 |         -48.1387 |
[32m[20221214 14:19:52 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:19:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.75
[32m[20221214 14:19:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.92
[32m[20221214 14:19:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.06
[32m[20221214 14:19:52 @agent_ppo2.py:143][0m Total time:      21.83 min
[32m[20221214 14:19:52 @agent_ppo2.py:145][0m 1998848 total steps have happened
[32m[20221214 14:19:52 @agent_ppo2.py:121][0m #------------------------ Iteration 976 --------------------------#
[32m[20221214 14:19:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:52 @agent_ppo2.py:185][0m |          -0.0048 |         207.8861 |         -48.0625 |
[32m[20221214 14:19:52 @agent_ppo2.py:185][0m |          -0.0036 |         204.0660 |         -47.9668 |
[32m[20221214 14:19:52 @agent_ppo2.py:185][0m |          -0.0019 |         202.6792 |         -48.0526 |
[32m[20221214 14:19:52 @agent_ppo2.py:185][0m |          -0.0024 |         201.7105 |         -47.9478 |
[32m[20221214 14:19:52 @agent_ppo2.py:185][0m |          -0.0045 |         201.6681 |         -47.9538 |
[32m[20221214 14:19:53 @agent_ppo2.py:185][0m |          -0.0043 |         200.6909 |         -47.9521 |
[32m[20221214 14:19:53 @agent_ppo2.py:185][0m |          -0.0047 |         201.1632 |         -47.9087 |
[32m[20221214 14:19:53 @agent_ppo2.py:185][0m |          -0.0059 |         200.3087 |         -47.8747 |
[32m[20221214 14:19:53 @agent_ppo2.py:185][0m |          -0.0076 |         200.8879 |         -47.8927 |
[32m[20221214 14:19:53 @agent_ppo2.py:185][0m |           0.0056 |         216.6607 |         -47.8740 |
[32m[20221214 14:19:53 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:19:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.34
[32m[20221214 14:19:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.30
[32m[20221214 14:19:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.66
[32m[20221214 14:19:53 @agent_ppo2.py:143][0m Total time:      21.85 min
[32m[20221214 14:19:53 @agent_ppo2.py:145][0m 2000896 total steps have happened
[32m[20221214 14:19:53 @agent_ppo2.py:121][0m #------------------------ Iteration 977 --------------------------#
[32m[20221214 14:19:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:53 @agent_ppo2.py:185][0m |           0.0015 |         165.9766 |         -47.2960 |
[32m[20221214 14:19:54 @agent_ppo2.py:185][0m |          -0.0017 |         161.4310 |         -47.4819 |
[32m[20221214 14:19:54 @agent_ppo2.py:185][0m |          -0.0019 |         160.0622 |         -47.4740 |
[32m[20221214 14:19:54 @agent_ppo2.py:185][0m |          -0.0022 |         158.3597 |         -47.4133 |
[32m[20221214 14:19:54 @agent_ppo2.py:185][0m |          -0.0018 |         157.6071 |         -47.3952 |
[32m[20221214 14:19:54 @agent_ppo2.py:185][0m |          -0.0005 |         157.8273 |         -47.4691 |
[32m[20221214 14:19:54 @agent_ppo2.py:185][0m |           0.0025 |         157.8507 |         -47.6177 |
[32m[20221214 14:19:54 @agent_ppo2.py:185][0m |          -0.0015 |         156.4952 |         -47.5937 |
[32m[20221214 14:19:54 @agent_ppo2.py:185][0m |           0.0011 |         156.2017 |         -47.7997 |
[32m[20221214 14:19:54 @agent_ppo2.py:185][0m |           0.0073 |         158.2088 |         -47.7553 |
[32m[20221214 14:19:54 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:19:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.67
[32m[20221214 14:19:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.63
[32m[20221214 14:19:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.61
[32m[20221214 14:19:54 @agent_ppo2.py:143][0m Total time:      21.87 min
[32m[20221214 14:19:54 @agent_ppo2.py:145][0m 2002944 total steps have happened
[32m[20221214 14:19:54 @agent_ppo2.py:121][0m #------------------------ Iteration 978 --------------------------#
[32m[20221214 14:19:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:19:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:55 @agent_ppo2.py:185][0m |          -0.0016 |         163.6927 |         -47.9745 |
[32m[20221214 14:19:55 @agent_ppo2.py:185][0m |          -0.0032 |         160.0850 |         -48.0393 |
[32m[20221214 14:19:55 @agent_ppo2.py:185][0m |          -0.0029 |         158.4710 |         -48.0459 |
[32m[20221214 14:19:55 @agent_ppo2.py:185][0m |           0.0030 |         162.9094 |         -47.9399 |
[32m[20221214 14:19:55 @agent_ppo2.py:185][0m |           0.0033 |         158.0550 |         -47.9737 |
[32m[20221214 14:19:55 @agent_ppo2.py:185][0m |           0.0047 |         162.7838 |         -47.8663 |
[32m[20221214 14:19:55 @agent_ppo2.py:185][0m |           0.0014 |         156.3312 |         -48.0370 |
[32m[20221214 14:19:55 @agent_ppo2.py:185][0m |          -0.0019 |         156.0084 |         -48.1303 |
[32m[20221214 14:19:55 @agent_ppo2.py:185][0m |          -0.0018 |         155.7290 |         -48.0643 |
[32m[20221214 14:19:56 @agent_ppo2.py:185][0m |          -0.0002 |         155.4097 |         -48.0890 |
[32m[20221214 14:19:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:19:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.65
[32m[20221214 14:19:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.24
[32m[20221214 14:19:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.81
[32m[20221214 14:19:56 @agent_ppo2.py:143][0m Total time:      21.89 min
[32m[20221214 14:19:56 @agent_ppo2.py:145][0m 2004992 total steps have happened
[32m[20221214 14:19:56 @agent_ppo2.py:121][0m #------------------------ Iteration 979 --------------------------#
[32m[20221214 14:19:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:19:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:56 @agent_ppo2.py:185][0m |          -0.0011 |         174.2220 |         -49.8434 |
[32m[20221214 14:19:56 @agent_ppo2.py:185][0m |          -0.0024 |         170.8772 |         -49.9264 |
[32m[20221214 14:19:56 @agent_ppo2.py:185][0m |          -0.0026 |         170.5830 |         -49.8853 |
[32m[20221214 14:19:56 @agent_ppo2.py:185][0m |          -0.0027 |         170.2671 |         -49.7066 |
[32m[20221214 14:19:56 @agent_ppo2.py:185][0m |          -0.0004 |         169.4368 |         -49.9598 |
[32m[20221214 14:19:56 @agent_ppo2.py:185][0m |          -0.0009 |         168.8268 |         -49.8430 |
[32m[20221214 14:19:57 @agent_ppo2.py:185][0m |          -0.0009 |         168.8770 |         -50.0249 |
[32m[20221214 14:19:57 @agent_ppo2.py:185][0m |           0.0082 |         177.9094 |         -50.0163 |
[32m[20221214 14:19:57 @agent_ppo2.py:185][0m |          -0.0013 |         169.1171 |         -49.6383 |
[32m[20221214 14:19:57 @agent_ppo2.py:185][0m |          -0.0017 |         168.3596 |         -49.7307 |
[32m[20221214 14:19:57 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:19:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.02
[32m[20221214 14:19:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.05
[32m[20221214 14:19:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.76
[32m[20221214 14:19:57 @agent_ppo2.py:143][0m Total time:      21.91 min
[32m[20221214 14:19:57 @agent_ppo2.py:145][0m 2007040 total steps have happened
[32m[20221214 14:19:57 @agent_ppo2.py:121][0m #------------------------ Iteration 980 --------------------------#
[32m[20221214 14:19:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:19:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:57 @agent_ppo2.py:185][0m |          -0.0022 |         180.2402 |         -49.6030 |
[32m[20221214 14:19:57 @agent_ppo2.py:185][0m |           0.0003 |         178.7894 |         -49.6233 |
[32m[20221214 14:19:57 @agent_ppo2.py:185][0m |          -0.0014 |         177.4480 |         -49.7193 |
[32m[20221214 14:19:58 @agent_ppo2.py:185][0m |          -0.0009 |         177.2843 |         -49.6498 |
[32m[20221214 14:19:58 @agent_ppo2.py:185][0m |          -0.0010 |         175.8141 |         -49.5807 |
[32m[20221214 14:19:58 @agent_ppo2.py:185][0m |          -0.0011 |         176.0202 |         -49.8686 |
[32m[20221214 14:19:58 @agent_ppo2.py:185][0m |          -0.0036 |         175.6974 |         -49.8823 |
[32m[20221214 14:19:58 @agent_ppo2.py:185][0m |          -0.0003 |         174.7736 |         -49.7632 |
[32m[20221214 14:19:58 @agent_ppo2.py:185][0m |          -0.0010 |         174.4420 |         -50.0490 |
[32m[20221214 14:19:58 @agent_ppo2.py:185][0m |          -0.0015 |         174.5307 |         -50.0544 |
[32m[20221214 14:19:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:19:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.06
[32m[20221214 14:19:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.32
[32m[20221214 14:19:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.84
[32m[20221214 14:19:58 @agent_ppo2.py:143][0m Total time:      21.94 min
[32m[20221214 14:19:58 @agent_ppo2.py:145][0m 2009088 total steps have happened
[32m[20221214 14:19:58 @agent_ppo2.py:121][0m #------------------------ Iteration 981 --------------------------#
[32m[20221214 14:19:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:19:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:19:59 @agent_ppo2.py:185][0m |           0.0031 |         180.6618 |         -49.7930 |
[32m[20221214 14:19:59 @agent_ppo2.py:185][0m |          -0.0001 |         177.3762 |         -50.0031 |
[32m[20221214 14:19:59 @agent_ppo2.py:185][0m |          -0.0024 |         176.4981 |         -49.7536 |
[32m[20221214 14:19:59 @agent_ppo2.py:185][0m |          -0.0001 |         176.2335 |         -49.9023 |
[32m[20221214 14:19:59 @agent_ppo2.py:185][0m |          -0.0021 |         175.9631 |         -50.0426 |
[32m[20221214 14:19:59 @agent_ppo2.py:185][0m |           0.0005 |         176.4799 |         -49.9672 |
[32m[20221214 14:19:59 @agent_ppo2.py:185][0m |          -0.0012 |         175.4427 |         -49.9869 |
[32m[20221214 14:19:59 @agent_ppo2.py:185][0m |          -0.0009 |         174.8553 |         -49.9647 |
[32m[20221214 14:19:59 @agent_ppo2.py:185][0m |          -0.0031 |         174.8494 |         -49.9876 |
[32m[20221214 14:19:59 @agent_ppo2.py:185][0m |           0.0035 |         175.9784 |         -50.1620 |
[32m[20221214 14:19:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:19:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.55
[32m[20221214 14:19:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221214 14:19:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.06
[32m[20221214 14:19:59 @agent_ppo2.py:143][0m Total time:      21.96 min
[32m[20221214 14:19:59 @agent_ppo2.py:145][0m 2011136 total steps have happened
[32m[20221214 14:19:59 @agent_ppo2.py:121][0m #------------------------ Iteration 982 --------------------------#
[32m[20221214 14:20:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:20:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:00 @agent_ppo2.py:185][0m |          -0.0007 |         219.1412 |         -49.2601 |
[32m[20221214 14:20:00 @agent_ppo2.py:185][0m |           0.0050 |         220.9143 |         -49.0868 |
[32m[20221214 14:20:00 @agent_ppo2.py:185][0m |          -0.0024 |         214.4498 |         -49.0935 |
[32m[20221214 14:20:00 @agent_ppo2.py:185][0m |          -0.0007 |         214.6149 |         -49.2400 |
[32m[20221214 14:20:00 @agent_ppo2.py:185][0m |          -0.0000 |         213.4789 |         -49.1559 |
[32m[20221214 14:20:00 @agent_ppo2.py:185][0m |          -0.0013 |         213.2473 |         -49.0687 |
[32m[20221214 14:20:00 @agent_ppo2.py:185][0m |           0.0043 |         216.3702 |         -49.1367 |
[32m[20221214 14:20:00 @agent_ppo2.py:185][0m |          -0.0024 |         212.3069 |         -49.1549 |
[32m[20221214 14:20:00 @agent_ppo2.py:185][0m |          -0.0003 |         211.9069 |         -49.2457 |
[32m[20221214 14:20:01 @agent_ppo2.py:185][0m |           0.0033 |         215.4187 |         -49.1809 |
[32m[20221214 14:20:01 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:20:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.71
[32m[20221214 14:20:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.40
[32m[20221214 14:20:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.13
[32m[20221214 14:20:01 @agent_ppo2.py:143][0m Total time:      21.98 min
[32m[20221214 14:20:01 @agent_ppo2.py:145][0m 2013184 total steps have happened
[32m[20221214 14:20:01 @agent_ppo2.py:121][0m #------------------------ Iteration 983 --------------------------#
[32m[20221214 14:20:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:20:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:01 @agent_ppo2.py:185][0m |          -0.0033 |         206.1059 |         -49.3934 |
[32m[20221214 14:20:01 @agent_ppo2.py:185][0m |          -0.0044 |         202.4156 |         -49.5802 |
[32m[20221214 14:20:01 @agent_ppo2.py:185][0m |          -0.0057 |         201.9914 |         -49.4616 |
[32m[20221214 14:20:01 @agent_ppo2.py:185][0m |          -0.0037 |         202.1857 |         -49.4625 |
[32m[20221214 14:20:01 @agent_ppo2.py:185][0m |           0.0004 |         204.1217 |         -49.6281 |
[32m[20221214 14:20:02 @agent_ppo2.py:185][0m |          -0.0058 |         200.5879 |         -49.4894 |
[32m[20221214 14:20:02 @agent_ppo2.py:185][0m |          -0.0054 |         199.3726 |         -49.4017 |
[32m[20221214 14:20:02 @agent_ppo2.py:185][0m |          -0.0059 |         199.1526 |         -49.6240 |
[32m[20221214 14:20:02 @agent_ppo2.py:185][0m |          -0.0055 |         198.7630 |         -49.4835 |
[32m[20221214 14:20:02 @agent_ppo2.py:185][0m |          -0.0064 |         198.7477 |         -49.5606 |
[32m[20221214 14:20:02 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:20:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.75
[32m[20221214 14:20:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.48
[32m[20221214 14:20:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.13
[32m[20221214 14:20:02 @agent_ppo2.py:143][0m Total time:      22.00 min
[32m[20221214 14:20:02 @agent_ppo2.py:145][0m 2015232 total steps have happened
[32m[20221214 14:20:02 @agent_ppo2.py:121][0m #------------------------ Iteration 984 --------------------------#
[32m[20221214 14:20:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:03 @agent_ppo2.py:185][0m |          -0.0007 |         212.3161 |         -49.6960 |
[32m[20221214 14:20:03 @agent_ppo2.py:185][0m |           0.0002 |         206.7320 |         -49.7364 |
[32m[20221214 14:20:03 @agent_ppo2.py:185][0m |          -0.0034 |         205.1849 |         -49.7378 |
[32m[20221214 14:20:03 @agent_ppo2.py:185][0m |          -0.0017 |         205.0153 |         -49.6389 |
[32m[20221214 14:20:03 @agent_ppo2.py:185][0m |          -0.0038 |         202.7164 |         -49.5676 |
[32m[20221214 14:20:03 @agent_ppo2.py:185][0m |           0.0005 |         205.8956 |         -49.5163 |
[32m[20221214 14:20:03 @agent_ppo2.py:185][0m |          -0.0034 |         202.2263 |         -49.4402 |
[32m[20221214 14:20:03 @agent_ppo2.py:185][0m |          -0.0002 |         204.4634 |         -49.3788 |
[32m[20221214 14:20:03 @agent_ppo2.py:185][0m |          -0.0036 |         201.4714 |         -49.2170 |
[32m[20221214 14:20:03 @agent_ppo2.py:185][0m |          -0.0055 |         201.1562 |         -49.1888 |
[32m[20221214 14:20:03 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:20:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.46
[32m[20221214 14:20:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.65
[32m[20221214 14:20:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.65
[32m[20221214 14:20:04 @agent_ppo2.py:143][0m Total time:      22.02 min
[32m[20221214 14:20:04 @agent_ppo2.py:145][0m 2017280 total steps have happened
[32m[20221214 14:20:04 @agent_ppo2.py:121][0m #------------------------ Iteration 985 --------------------------#
[32m[20221214 14:20:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:04 @agent_ppo2.py:185][0m |          -0.0021 |         214.6105 |         -49.7553 |
[32m[20221214 14:20:04 @agent_ppo2.py:185][0m |          -0.0032 |         213.2491 |         -49.5807 |
[32m[20221214 14:20:04 @agent_ppo2.py:185][0m |          -0.0036 |         212.8840 |         -49.5675 |
[32m[20221214 14:20:04 @agent_ppo2.py:185][0m |          -0.0035 |         211.6159 |         -49.6476 |
[32m[20221214 14:20:04 @agent_ppo2.py:185][0m |          -0.0024 |         211.2624 |         -49.5955 |
[32m[20221214 14:20:04 @agent_ppo2.py:185][0m |          -0.0034 |         211.0319 |         -49.5058 |
[32m[20221214 14:20:04 @agent_ppo2.py:185][0m |           0.0001 |         212.6164 |         -49.4820 |
[32m[20221214 14:20:05 @agent_ppo2.py:185][0m |          -0.0045 |         210.6526 |         -49.4695 |
[32m[20221214 14:20:05 @agent_ppo2.py:185][0m |          -0.0043 |         210.1442 |         -49.5111 |
[32m[20221214 14:20:05 @agent_ppo2.py:185][0m |           0.0038 |         216.8764 |         -49.4441 |
[32m[20221214 14:20:05 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:20:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.60
[32m[20221214 14:20:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.83
[32m[20221214 14:20:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.68
[32m[20221214 14:20:05 @agent_ppo2.py:143][0m Total time:      22.05 min
[32m[20221214 14:20:05 @agent_ppo2.py:145][0m 2019328 total steps have happened
[32m[20221214 14:20:05 @agent_ppo2.py:121][0m #------------------------ Iteration 986 --------------------------#
[32m[20221214 14:20:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:05 @agent_ppo2.py:185][0m |           0.0111 |         275.5616 |         -49.6337 |
[32m[20221214 14:20:05 @agent_ppo2.py:185][0m |          -0.0004 |         237.7474 |         -49.3737 |
[32m[20221214 14:20:05 @agent_ppo2.py:185][0m |          -0.0027 |         234.9273 |         -49.4035 |
[32m[20221214 14:20:05 @agent_ppo2.py:185][0m |          -0.0020 |         234.0526 |         -49.5828 |
[32m[20221214 14:20:06 @agent_ppo2.py:185][0m |           0.0062 |         250.2015 |         -49.5288 |
[32m[20221214 14:20:06 @agent_ppo2.py:185][0m |          -0.0016 |         232.6235 |         -49.2470 |
[32m[20221214 14:20:06 @agent_ppo2.py:185][0m |          -0.0015 |         231.7820 |         -49.4744 |
[32m[20221214 14:20:06 @agent_ppo2.py:185][0m |          -0.0021 |         230.9501 |         -49.5388 |
[32m[20221214 14:20:06 @agent_ppo2.py:185][0m |          -0.0019 |         230.3450 |         -49.4912 |
[32m[20221214 14:20:06 @agent_ppo2.py:185][0m |          -0.0035 |         230.0087 |         -49.5196 |
[32m[20221214 14:20:06 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:20:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.94
[32m[20221214 14:20:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.44
[32m[20221214 14:20:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 801.16
[32m[20221214 14:20:06 @agent_ppo2.py:143][0m Total time:      22.07 min
[32m[20221214 14:20:06 @agent_ppo2.py:145][0m 2021376 total steps have happened
[32m[20221214 14:20:06 @agent_ppo2.py:121][0m #------------------------ Iteration 987 --------------------------#
[32m[20221214 14:20:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:07 @agent_ppo2.py:185][0m |          -0.0004 |         231.9182 |         -49.8737 |
[32m[20221214 14:20:07 @agent_ppo2.py:185][0m |           0.0004 |         225.5798 |         -49.9354 |
[32m[20221214 14:20:07 @agent_ppo2.py:185][0m |          -0.0015 |         223.6106 |         -49.9814 |
[32m[20221214 14:20:07 @agent_ppo2.py:185][0m |           0.0047 |         226.1531 |         -49.8519 |
[32m[20221214 14:20:07 @agent_ppo2.py:185][0m |          -0.0027 |         222.6826 |         -49.8842 |
[32m[20221214 14:20:07 @agent_ppo2.py:185][0m |           0.0032 |         222.3691 |         -50.1305 |
[32m[20221214 14:20:07 @agent_ppo2.py:185][0m |          -0.0012 |         220.9389 |         -50.0540 |
[32m[20221214 14:20:07 @agent_ppo2.py:185][0m |          -0.0015 |         220.4250 |         -49.9797 |
[32m[20221214 14:20:07 @agent_ppo2.py:185][0m |           0.0002 |         219.2922 |         -49.9937 |
[32m[20221214 14:20:07 @agent_ppo2.py:185][0m |          -0.0012 |         219.3620 |         -50.0697 |
[32m[20221214 14:20:07 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:20:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.93
[32m[20221214 14:20:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.03
[32m[20221214 14:20:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.25
[32m[20221214 14:20:08 @agent_ppo2.py:143][0m Total time:      22.09 min
[32m[20221214 14:20:08 @agent_ppo2.py:145][0m 2023424 total steps have happened
[32m[20221214 14:20:08 @agent_ppo2.py:121][0m #------------------------ Iteration 988 --------------------------#
[32m[20221214 14:20:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:08 @agent_ppo2.py:185][0m |          -0.0002 |         246.0846 |         -49.6097 |
[32m[20221214 14:20:08 @agent_ppo2.py:185][0m |           0.0079 |         249.0847 |         -49.5375 |
[32m[20221214 14:20:08 @agent_ppo2.py:185][0m |           0.0055 |         252.1336 |         -49.5562 |
[32m[20221214 14:20:08 @agent_ppo2.py:185][0m |          -0.0000 |         242.6115 |         -49.6011 |
[32m[20221214 14:20:08 @agent_ppo2.py:185][0m |          -0.0005 |         241.8831 |         -49.6726 |
[32m[20221214 14:20:08 @agent_ppo2.py:185][0m |           0.0003 |         242.6819 |         -49.6363 |
[32m[20221214 14:20:09 @agent_ppo2.py:185][0m |          -0.0010 |         241.4047 |         -49.7488 |
[32m[20221214 14:20:09 @agent_ppo2.py:185][0m |          -0.0018 |         241.3886 |         -49.8841 |
[32m[20221214 14:20:09 @agent_ppo2.py:185][0m |          -0.0010 |         240.8422 |         -49.8012 |
[32m[20221214 14:20:09 @agent_ppo2.py:185][0m |          -0.0009 |         240.6209 |         -49.8839 |
[32m[20221214 14:20:09 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:20:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.27
[32m[20221214 14:20:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.98
[32m[20221214 14:20:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.83
[32m[20221214 14:20:09 @agent_ppo2.py:143][0m Total time:      22.11 min
[32m[20221214 14:20:09 @agent_ppo2.py:145][0m 2025472 total steps have happened
[32m[20221214 14:20:09 @agent_ppo2.py:121][0m #------------------------ Iteration 989 --------------------------#
[32m[20221214 14:20:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:09 @agent_ppo2.py:185][0m |          -0.0006 |         238.6692 |         -49.8638 |
[32m[20221214 14:20:09 @agent_ppo2.py:185][0m |          -0.0018 |         235.5874 |         -49.5694 |
[32m[20221214 14:20:09 @agent_ppo2.py:185][0m |          -0.0029 |         234.3947 |         -49.6540 |
[32m[20221214 14:20:10 @agent_ppo2.py:185][0m |          -0.0015 |         234.5450 |         -49.7231 |
[32m[20221214 14:20:10 @agent_ppo2.py:185][0m |           0.0017 |         234.3830 |         -49.7353 |
[32m[20221214 14:20:10 @agent_ppo2.py:185][0m |           0.0057 |         238.4064 |         -49.7873 |
[32m[20221214 14:20:10 @agent_ppo2.py:185][0m |          -0.0038 |         231.7619 |         -49.7735 |
[32m[20221214 14:20:10 @agent_ppo2.py:185][0m |          -0.0025 |         231.1959 |         -49.8691 |
[32m[20221214 14:20:10 @agent_ppo2.py:185][0m |          -0.0025 |         231.4490 |         -49.6895 |
[32m[20221214 14:20:10 @agent_ppo2.py:185][0m |          -0.0034 |         230.6894 |         -50.0990 |
[32m[20221214 14:20:10 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:20:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.01
[32m[20221214 14:20:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.70
[32m[20221214 14:20:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.83
[32m[20221214 14:20:10 @agent_ppo2.py:143][0m Total time:      22.14 min
[32m[20221214 14:20:10 @agent_ppo2.py:145][0m 2027520 total steps have happened
[32m[20221214 14:20:10 @agent_ppo2.py:121][0m #------------------------ Iteration 990 --------------------------#
[32m[20221214 14:20:11 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:20:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:11 @agent_ppo2.py:185][0m |          -0.0004 |         198.5194 |         -50.4423 |
[32m[20221214 14:20:11 @agent_ppo2.py:185][0m |          -0.0026 |         193.5496 |         -50.4691 |
[32m[20221214 14:20:11 @agent_ppo2.py:185][0m |          -0.0016 |         192.6358 |         -50.5449 |
[32m[20221214 14:20:11 @agent_ppo2.py:185][0m |          -0.0042 |         191.7768 |         -50.5810 |
[32m[20221214 14:20:11 @agent_ppo2.py:185][0m |          -0.0033 |         189.4592 |         -50.4959 |
[32m[20221214 14:20:11 @agent_ppo2.py:185][0m |          -0.0029 |         189.9992 |         -50.6741 |
[32m[20221214 14:20:11 @agent_ppo2.py:185][0m |           0.0149 |         208.1091 |         -50.6744 |
[32m[20221214 14:20:11 @agent_ppo2.py:185][0m |          -0.0016 |         188.3834 |         -50.7536 |
[32m[20221214 14:20:11 @agent_ppo2.py:185][0m |          -0.0012 |         187.3185 |         -50.8071 |
[32m[20221214 14:20:12 @agent_ppo2.py:185][0m |          -0.0014 |         188.0171 |         -50.8357 |
[32m[20221214 14:20:12 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:20:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.60
[32m[20221214 14:20:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.19
[32m[20221214 14:20:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.62
[32m[20221214 14:20:12 @agent_ppo2.py:143][0m Total time:      22.16 min
[32m[20221214 14:20:12 @agent_ppo2.py:145][0m 2029568 total steps have happened
[32m[20221214 14:20:12 @agent_ppo2.py:121][0m #------------------------ Iteration 991 --------------------------#
[32m[20221214 14:20:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:12 @agent_ppo2.py:185][0m |           0.0107 |         241.1040 |         -51.2717 |
[32m[20221214 14:20:12 @agent_ppo2.py:185][0m |           0.0020 |         211.1653 |         -51.0204 |
[32m[20221214 14:20:12 @agent_ppo2.py:185][0m |           0.0094 |         234.0530 |         -50.9908 |
[32m[20221214 14:20:12 @agent_ppo2.py:185][0m |          -0.0023 |         209.3570 |         -50.9986 |
[32m[20221214 14:20:12 @agent_ppo2.py:185][0m |          -0.0027 |         206.6188 |         -51.0282 |
[32m[20221214 14:20:13 @agent_ppo2.py:185][0m |          -0.0026 |         206.1237 |         -50.9638 |
[32m[20221214 14:20:13 @agent_ppo2.py:185][0m |          -0.0026 |         205.5050 |         -50.8603 |
[32m[20221214 14:20:13 @agent_ppo2.py:185][0m |          -0.0039 |         204.1920 |         -50.9723 |
[32m[20221214 14:20:13 @agent_ppo2.py:185][0m |           0.0041 |         209.0901 |         -50.9435 |
[32m[20221214 14:20:13 @agent_ppo2.py:185][0m |          -0.0030 |         203.7884 |         -50.9114 |
[32m[20221214 14:20:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:20:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.29
[32m[20221214 14:20:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.90
[32m[20221214 14:20:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.18
[32m[20221214 14:20:13 @agent_ppo2.py:143][0m Total time:      22.18 min
[32m[20221214 14:20:13 @agent_ppo2.py:145][0m 2031616 total steps have happened
[32m[20221214 14:20:13 @agent_ppo2.py:121][0m #------------------------ Iteration 992 --------------------------#
[32m[20221214 14:20:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:13 @agent_ppo2.py:185][0m |           0.0005 |         233.1980 |         -50.5767 |
[32m[20221214 14:20:13 @agent_ppo2.py:185][0m |           0.0084 |         245.4645 |         -50.3420 |
[32m[20221214 14:20:14 @agent_ppo2.py:185][0m |          -0.0056 |         222.7966 |         -50.5144 |
[32m[20221214 14:20:14 @agent_ppo2.py:185][0m |          -0.0087 |         220.4887 |         -50.4559 |
[32m[20221214 14:20:14 @agent_ppo2.py:185][0m |          -0.0080 |         219.9030 |         -50.2987 |
[32m[20221214 14:20:14 @agent_ppo2.py:185][0m |          -0.0087 |         220.4557 |         -50.3825 |
[32m[20221214 14:20:14 @agent_ppo2.py:185][0m |          -0.0031 |         222.6573 |         -50.4232 |
[32m[20221214 14:20:14 @agent_ppo2.py:185][0m |          -0.0019 |         227.8271 |         -50.4530 |
[32m[20221214 14:20:14 @agent_ppo2.py:185][0m |           0.0026 |         233.5478 |         -50.2327 |
[32m[20221214 14:20:14 @agent_ppo2.py:185][0m |          -0.0037 |         221.8095 |         -50.2951 |
[32m[20221214 14:20:14 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:20:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.09
[32m[20221214 14:20:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.92
[32m[20221214 14:20:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.39
[32m[20221214 14:20:14 @agent_ppo2.py:143][0m Total time:      22.21 min
[32m[20221214 14:20:14 @agent_ppo2.py:145][0m 2033664 total steps have happened
[32m[20221214 14:20:14 @agent_ppo2.py:121][0m #------------------------ Iteration 993 --------------------------#
[32m[20221214 14:20:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:15 @agent_ppo2.py:185][0m |           0.0025 |         266.6690 |         -51.1841 |
[32m[20221214 14:20:15 @agent_ppo2.py:185][0m |           0.0034 |         259.2110 |         -51.3375 |
[32m[20221214 14:20:15 @agent_ppo2.py:185][0m |          -0.0021 |         250.7596 |         -51.3869 |
[32m[20221214 14:20:15 @agent_ppo2.py:185][0m |           0.0103 |         282.1150 |         -51.4481 |
[32m[20221214 14:20:15 @agent_ppo2.py:185][0m |          -0.0002 |         245.7259 |         -51.3167 |
[32m[20221214 14:20:15 @agent_ppo2.py:185][0m |          -0.0018 |         245.2453 |         -51.5112 |
[32m[20221214 14:20:15 @agent_ppo2.py:185][0m |          -0.0022 |         244.7772 |         -51.6464 |
[32m[20221214 14:20:15 @agent_ppo2.py:185][0m |           0.0003 |         244.9017 |         -51.7105 |
[32m[20221214 14:20:16 @agent_ppo2.py:185][0m |           0.0001 |         244.5736 |         -51.6373 |
[32m[20221214 14:20:16 @agent_ppo2.py:185][0m |           0.0002 |         243.8933 |         -51.8237 |
[32m[20221214 14:20:16 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:20:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.97
[32m[20221214 14:20:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.77
[32m[20221214 14:20:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 791.23
[32m[20221214 14:20:16 @agent_ppo2.py:143][0m Total time:      22.23 min
[32m[20221214 14:20:16 @agent_ppo2.py:145][0m 2035712 total steps have happened
[32m[20221214 14:20:16 @agent_ppo2.py:121][0m #------------------------ Iteration 994 --------------------------#
[32m[20221214 14:20:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:20:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:16 @agent_ppo2.py:185][0m |           0.0007 |         234.2148 |         -51.1069 |
[32m[20221214 14:20:16 @agent_ppo2.py:185][0m |           0.0052 |         227.7534 |         -51.0951 |
[32m[20221214 14:20:16 @agent_ppo2.py:185][0m |          -0.0003 |         221.9471 |         -51.4311 |
[32m[20221214 14:20:16 @agent_ppo2.py:185][0m |           0.0101 |         244.6024 |         -51.5420 |
[32m[20221214 14:20:17 @agent_ppo2.py:185][0m |          -0.0006 |         219.7583 |         -51.4275 |
[32m[20221214 14:20:17 @agent_ppo2.py:185][0m |           0.0094 |         228.8613 |         -51.6887 |
[32m[20221214 14:20:17 @agent_ppo2.py:185][0m |          -0.0025 |         217.5556 |         -51.7197 |
[32m[20221214 14:20:17 @agent_ppo2.py:185][0m |          -0.0018 |         216.6037 |         -51.7660 |
[32m[20221214 14:20:17 @agent_ppo2.py:185][0m |          -0.0039 |         215.8141 |         -52.0304 |
[32m[20221214 14:20:17 @agent_ppo2.py:185][0m |          -0.0037 |         215.4227 |         -52.0766 |
[32m[20221214 14:20:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:20:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 784.31
[32m[20221214 14:20:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.58
[32m[20221214 14:20:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 788.95
[32m[20221214 14:20:17 @agent_ppo2.py:143][0m Total time:      22.25 min
[32m[20221214 14:20:17 @agent_ppo2.py:145][0m 2037760 total steps have happened
[32m[20221214 14:20:17 @agent_ppo2.py:121][0m #------------------------ Iteration 995 --------------------------#
[32m[20221214 14:20:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:17 @agent_ppo2.py:185][0m |          -0.0041 |         233.0409 |         -52.6148 |
[32m[20221214 14:20:18 @agent_ppo2.py:185][0m |           0.0093 |         252.2399 |         -52.5319 |
[32m[20221214 14:20:18 @agent_ppo2.py:185][0m |          -0.0023 |         228.1070 |         -52.4851 |
[32m[20221214 14:20:18 @agent_ppo2.py:185][0m |          -0.0035 |         226.0460 |         -52.3248 |
[32m[20221214 14:20:18 @agent_ppo2.py:185][0m |          -0.0017 |         225.9782 |         -52.3420 |
[32m[20221214 14:20:18 @agent_ppo2.py:185][0m |          -0.0003 |         225.1886 |         -52.4129 |
[32m[20221214 14:20:18 @agent_ppo2.py:185][0m |          -0.0004 |         226.2069 |         -52.3276 |
[32m[20221214 14:20:18 @agent_ppo2.py:185][0m |          -0.0032 |         225.1610 |         -52.3109 |
[32m[20221214 14:20:18 @agent_ppo2.py:185][0m |          -0.0027 |         223.9350 |         -52.4002 |
[32m[20221214 14:20:18 @agent_ppo2.py:185][0m |          -0.0019 |         223.6994 |         -52.2288 |
[32m[20221214 14:20:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:20:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.15
[32m[20221214 14:20:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 779.00
[32m[20221214 14:20:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 791.26
[32m[20221214 14:20:18 @agent_ppo2.py:143][0m Total time:      22.27 min
[32m[20221214 14:20:18 @agent_ppo2.py:145][0m 2039808 total steps have happened
[32m[20221214 14:20:18 @agent_ppo2.py:121][0m #------------------------ Iteration 996 --------------------------#
[32m[20221214 14:20:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:20:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:19 @agent_ppo2.py:185][0m |          -0.0028 |         205.8068 |         -53.0936 |
[32m[20221214 14:20:19 @agent_ppo2.py:185][0m |          -0.0013 |         196.0749 |         -53.0917 |
[32m[20221214 14:20:19 @agent_ppo2.py:185][0m |           0.0013 |         195.6268 |         -53.1994 |
[32m[20221214 14:20:19 @agent_ppo2.py:185][0m |           0.0103 |         211.1959 |         -53.3396 |
[32m[20221214 14:20:19 @agent_ppo2.py:185][0m |          -0.0006 |         196.3639 |         -53.1398 |
[32m[20221214 14:20:19 @agent_ppo2.py:185][0m |           0.0003 |         193.0712 |         -53.2983 |
[32m[20221214 14:20:19 @agent_ppo2.py:185][0m |          -0.0009 |         192.3691 |         -53.4149 |
[32m[20221214 14:20:19 @agent_ppo2.py:185][0m |           0.0007 |         193.9323 |         -53.5040 |
[32m[20221214 14:20:19 @agent_ppo2.py:185][0m |          -0.0019 |         191.5541 |         -53.5599 |
[32m[20221214 14:20:19 @agent_ppo2.py:185][0m |           0.0004 |         191.7035 |         -53.6815 |
[32m[20221214 14:20:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:20:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.58
[32m[20221214 14:20:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.42
[32m[20221214 14:20:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.25
[32m[20221214 14:20:20 @agent_ppo2.py:143][0m Total time:      22.29 min
[32m[20221214 14:20:20 @agent_ppo2.py:145][0m 2041856 total steps have happened
[32m[20221214 14:20:20 @agent_ppo2.py:121][0m #------------------------ Iteration 997 --------------------------#
[32m[20221214 14:20:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:20 @agent_ppo2.py:185][0m |           0.0074 |         210.2700 |         -53.4404 |
[32m[20221214 14:20:20 @agent_ppo2.py:185][0m |          -0.0021 |         203.7777 |         -53.4802 |
[32m[20221214 14:20:20 @agent_ppo2.py:185][0m |          -0.0037 |         200.8481 |         -53.3823 |
[32m[20221214 14:20:20 @agent_ppo2.py:185][0m |          -0.0006 |         200.4638 |         -53.4213 |
[32m[20221214 14:20:20 @agent_ppo2.py:185][0m |          -0.0022 |         200.0456 |         -53.4884 |
[32m[20221214 14:20:20 @agent_ppo2.py:185][0m |          -0.0018 |         199.8000 |         -53.5088 |
[32m[20221214 14:20:21 @agent_ppo2.py:185][0m |          -0.0024 |         199.7464 |         -53.3031 |
[32m[20221214 14:20:21 @agent_ppo2.py:185][0m |          -0.0022 |         198.5340 |         -53.3965 |
[32m[20221214 14:20:21 @agent_ppo2.py:185][0m |          -0.0026 |         198.5021 |         -53.4932 |
[32m[20221214 14:20:21 @agent_ppo2.py:185][0m |          -0.0036 |         198.4372 |         -53.5143 |
[32m[20221214 14:20:21 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 14:20:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.69
[32m[20221214 14:20:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.01
[32m[20221214 14:20:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 787.92
[32m[20221214 14:20:21 @agent_ppo2.py:143][0m Total time:      22.32 min
[32m[20221214 14:20:21 @agent_ppo2.py:145][0m 2043904 total steps have happened
[32m[20221214 14:20:21 @agent_ppo2.py:121][0m #------------------------ Iteration 998 --------------------------#
[32m[20221214 14:20:21 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:20:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:21 @agent_ppo2.py:185][0m |          -0.0003 |         200.4912 |         -54.0406 |
[32m[20221214 14:20:22 @agent_ppo2.py:185][0m |          -0.0007 |         198.7114 |         -53.9863 |
[32m[20221214 14:20:22 @agent_ppo2.py:185][0m |          -0.0020 |         197.5479 |         -53.9735 |
[32m[20221214 14:20:22 @agent_ppo2.py:185][0m |          -0.0021 |         197.0204 |         -53.9536 |
[32m[20221214 14:20:22 @agent_ppo2.py:185][0m |           0.0040 |         199.6317 |         -54.0239 |
[32m[20221214 14:20:22 @agent_ppo2.py:185][0m |          -0.0038 |         196.5970 |         -53.9260 |
[32m[20221214 14:20:22 @agent_ppo2.py:185][0m |          -0.0025 |         196.0283 |         -53.9921 |
[32m[20221214 14:20:22 @agent_ppo2.py:185][0m |          -0.0020 |         197.4392 |         -54.0084 |
[32m[20221214 14:20:22 @agent_ppo2.py:185][0m |           0.0008 |         197.3955 |         -54.0133 |
[32m[20221214 14:20:22 @agent_ppo2.py:185][0m |          -0.0024 |         195.7149 |         -54.1112 |
[32m[20221214 14:20:22 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:20:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.09
[32m[20221214 14:20:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.73
[32m[20221214 14:20:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 787.22
[32m[20221214 14:20:22 @agent_ppo2.py:143][0m Total time:      22.34 min
[32m[20221214 14:20:22 @agent_ppo2.py:145][0m 2045952 total steps have happened
[32m[20221214 14:20:22 @agent_ppo2.py:121][0m #------------------------ Iteration 999 --------------------------#
[32m[20221214 14:20:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:23 @agent_ppo2.py:185][0m |          -0.0004 |         177.9823 |         -54.0751 |
[32m[20221214 14:20:23 @agent_ppo2.py:185][0m |          -0.0012 |         174.7294 |         -53.9767 |
[32m[20221214 14:20:23 @agent_ppo2.py:185][0m |          -0.0023 |         174.0590 |         -53.8727 |
[32m[20221214 14:20:23 @agent_ppo2.py:185][0m |          -0.0003 |         173.0139 |         -53.9293 |
[32m[20221214 14:20:23 @agent_ppo2.py:185][0m |          -0.0016 |         172.7832 |         -53.8743 |
[32m[20221214 14:20:23 @agent_ppo2.py:185][0m |          -0.0028 |         172.2853 |         -53.8843 |
[32m[20221214 14:20:23 @agent_ppo2.py:185][0m |           0.0026 |         173.0933 |         -53.8404 |
[32m[20221214 14:20:23 @agent_ppo2.py:185][0m |          -0.0011 |         171.5220 |         -53.8058 |
[32m[20221214 14:20:24 @agent_ppo2.py:185][0m |          -0.0029 |         172.2086 |         -53.7024 |
[32m[20221214 14:20:24 @agent_ppo2.py:185][0m |          -0.0032 |         171.3000 |         -53.7943 |
[32m[20221214 14:20:24 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:20:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.72
[32m[20221214 14:20:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.87
[32m[20221214 14:20:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 788.15
[32m[20221214 14:20:24 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 860.44
[32m[20221214 14:20:24 @agent_ppo2.py:143][0m Total time:      22.36 min
[32m[20221214 14:20:24 @agent_ppo2.py:145][0m 2048000 total steps have happened
[32m[20221214 14:20:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1000 --------------------------#
[32m[20221214 14:20:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:20:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:24 @agent_ppo2.py:185][0m |          -0.0021 |         167.3169 |         -53.4710 |
[32m[20221214 14:20:24 @agent_ppo2.py:185][0m |           0.0041 |         172.3411 |         -53.3356 |
[32m[20221214 14:20:24 @agent_ppo2.py:185][0m |          -0.0017 |         165.7209 |         -53.4374 |
[32m[20221214 14:20:24 @agent_ppo2.py:185][0m |          -0.0046 |         166.1029 |         -53.3116 |
[32m[20221214 14:20:25 @agent_ppo2.py:185][0m |          -0.0048 |         164.6645 |         -53.3066 |
[32m[20221214 14:20:25 @agent_ppo2.py:185][0m |           0.0032 |         164.9363 |         -53.4969 |
[32m[20221214 14:20:25 @agent_ppo2.py:185][0m |          -0.0041 |         164.9236 |         -53.5324 |
[32m[20221214 14:20:25 @agent_ppo2.py:185][0m |          -0.0032 |         164.8442 |         -53.2538 |
[32m[20221214 14:20:25 @agent_ppo2.py:185][0m |          -0.0029 |         163.9035 |         -53.4738 |
[32m[20221214 14:20:25 @agent_ppo2.py:185][0m |          -0.0033 |         163.7610 |         -53.5369 |
[32m[20221214 14:20:25 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:20:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.89
[32m[20221214 14:20:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.49
[32m[20221214 14:20:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 779.04
[32m[20221214 14:20:25 @agent_ppo2.py:143][0m Total time:      22.39 min
[32m[20221214 14:20:25 @agent_ppo2.py:145][0m 2050048 total steps have happened
[32m[20221214 14:20:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1001 --------------------------#
[32m[20221214 14:20:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:26 @agent_ppo2.py:185][0m |          -0.0032 |         202.3223 |         -54.8138 |
[32m[20221214 14:20:26 @agent_ppo2.py:185][0m |          -0.0022 |         200.7194 |         -54.8120 |
[32m[20221214 14:20:26 @agent_ppo2.py:185][0m |           0.0001 |         200.5701 |         -54.7289 |
[32m[20221214 14:20:26 @agent_ppo2.py:185][0m |           0.0060 |         208.1632 |         -54.5256 |
[32m[20221214 14:20:26 @agent_ppo2.py:185][0m |          -0.0012 |         199.8610 |         -54.5272 |
[32m[20221214 14:20:26 @agent_ppo2.py:185][0m |          -0.0018 |         200.0290 |         -54.4921 |
[32m[20221214 14:20:26 @agent_ppo2.py:185][0m |           0.0075 |         208.5927 |         -54.3591 |
[32m[20221214 14:20:26 @agent_ppo2.py:185][0m |          -0.0033 |         199.6647 |         -54.4793 |
[32m[20221214 14:20:26 @agent_ppo2.py:185][0m |          -0.0034 |         199.3172 |         -54.3374 |
[32m[20221214 14:20:26 @agent_ppo2.py:185][0m |          -0.0021 |         199.0227 |         -54.3213 |
[32m[20221214 14:20:26 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:20:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.34
[32m[20221214 14:20:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.36
[32m[20221214 14:20:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 771.26
[32m[20221214 14:20:27 @agent_ppo2.py:143][0m Total time:      22.41 min
[32m[20221214 14:20:27 @agent_ppo2.py:145][0m 2052096 total steps have happened
[32m[20221214 14:20:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1002 --------------------------#
[32m[20221214 14:20:27 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:20:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:27 @agent_ppo2.py:185][0m |          -0.0021 |         202.2618 |         -53.0676 |
[32m[20221214 14:20:27 @agent_ppo2.py:185][0m |          -0.0026 |         199.4569 |         -53.0215 |
[32m[20221214 14:20:27 @agent_ppo2.py:185][0m |          -0.0008 |         198.5518 |         -52.9554 |
[32m[20221214 14:20:27 @agent_ppo2.py:185][0m |          -0.0015 |         198.2293 |         -53.0254 |
[32m[20221214 14:20:27 @agent_ppo2.py:185][0m |           0.0051 |         201.4648 |         -52.8199 |
[32m[20221214 14:20:27 @agent_ppo2.py:185][0m |          -0.0006 |         197.9933 |         -52.8979 |
[32m[20221214 14:20:28 @agent_ppo2.py:185][0m |           0.0062 |         210.3795 |         -52.9889 |
[32m[20221214 14:20:28 @agent_ppo2.py:185][0m |          -0.0020 |         198.3584 |         -52.9182 |
[32m[20221214 14:20:28 @agent_ppo2.py:185][0m |          -0.0000 |         197.2802 |         -53.0944 |
[32m[20221214 14:20:28 @agent_ppo2.py:185][0m |          -0.0024 |         195.9817 |         -53.1023 |
[32m[20221214 14:20:28 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:20:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.18
[32m[20221214 14:20:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.31
[32m[20221214 14:20:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 764.42
[32m[20221214 14:20:28 @agent_ppo2.py:143][0m Total time:      22.43 min
[32m[20221214 14:20:28 @agent_ppo2.py:145][0m 2054144 total steps have happened
[32m[20221214 14:20:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1003 --------------------------#
[32m[20221214 14:20:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:20:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:28 @agent_ppo2.py:185][0m |          -0.0002 |         217.1809 |         -53.8984 |
[32m[20221214 14:20:28 @agent_ppo2.py:185][0m |          -0.0011 |         213.7847 |         -54.1203 |
[32m[20221214 14:20:28 @agent_ppo2.py:185][0m |          -0.0028 |         213.3224 |         -53.9738 |
[32m[20221214 14:20:29 @agent_ppo2.py:185][0m |          -0.0025 |         212.2124 |         -54.1083 |
[32m[20221214 14:20:29 @agent_ppo2.py:185][0m |          -0.0006 |         212.2607 |         -53.8704 |
[32m[20221214 14:20:29 @agent_ppo2.py:185][0m |          -0.0014 |         210.8495 |         -54.0182 |
[32m[20221214 14:20:29 @agent_ppo2.py:185][0m |          -0.0009 |         211.1252 |         -54.0349 |
[32m[20221214 14:20:29 @agent_ppo2.py:185][0m |          -0.0009 |         211.1673 |         -54.0637 |
[32m[20221214 14:20:29 @agent_ppo2.py:185][0m |          -0.0029 |         210.2105 |         -53.9172 |
[32m[20221214 14:20:29 @agent_ppo2.py:185][0m |          -0.0031 |         211.6682 |         -54.0299 |
[32m[20221214 14:20:29 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:20:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 759.16
[32m[20221214 14:20:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.33
[32m[20221214 14:20:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 764.13
[32m[20221214 14:20:29 @agent_ppo2.py:143][0m Total time:      22.45 min
[32m[20221214 14:20:29 @agent_ppo2.py:145][0m 2056192 total steps have happened
[32m[20221214 14:20:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1004 --------------------------#
[32m[20221214 14:20:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:30 @agent_ppo2.py:185][0m |          -0.0015 |         209.0542 |         -53.0277 |
[32m[20221214 14:20:30 @agent_ppo2.py:185][0m |          -0.0017 |         207.2146 |         -53.1705 |
[32m[20221214 14:20:30 @agent_ppo2.py:185][0m |          -0.0037 |         206.3979 |         -53.0907 |
[32m[20221214 14:20:30 @agent_ppo2.py:185][0m |          -0.0033 |         206.1129 |         -53.0053 |
[32m[20221214 14:20:30 @agent_ppo2.py:185][0m |          -0.0024 |         205.3785 |         -52.9487 |
[32m[20221214 14:20:30 @agent_ppo2.py:185][0m |           0.0031 |         208.6709 |         -53.1754 |
[32m[20221214 14:20:30 @agent_ppo2.py:185][0m |          -0.0020 |         205.0671 |         -52.8850 |
[32m[20221214 14:20:30 @agent_ppo2.py:185][0m |          -0.0016 |         204.7381 |         -52.9812 |
[32m[20221214 14:20:30 @agent_ppo2.py:185][0m |          -0.0031 |         204.2954 |         -52.9253 |
[32m[20221214 14:20:31 @agent_ppo2.py:185][0m |           0.0056 |         210.0784 |         -53.1905 |
[32m[20221214 14:20:31 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:20:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 760.08
[32m[20221214 14:20:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 764.11
[32m[20221214 14:20:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 767.78
[32m[20221214 14:20:31 @agent_ppo2.py:143][0m Total time:      22.48 min
[32m[20221214 14:20:31 @agent_ppo2.py:145][0m 2058240 total steps have happened
[32m[20221214 14:20:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1005 --------------------------#
[32m[20221214 14:20:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:31 @agent_ppo2.py:185][0m |           0.0003 |         181.3249 |         -51.8781 |
[32m[20221214 14:20:31 @agent_ppo2.py:185][0m |          -0.0034 |         176.9764 |         -51.5624 |
[32m[20221214 14:20:31 @agent_ppo2.py:185][0m |          -0.0005 |         174.8460 |         -51.7494 |
[32m[20221214 14:20:31 @agent_ppo2.py:185][0m |          -0.0015 |         173.3139 |         -51.8970 |
[32m[20221214 14:20:31 @agent_ppo2.py:185][0m |          -0.0010 |         171.9590 |         -51.8013 |
[32m[20221214 14:20:31 @agent_ppo2.py:185][0m |          -0.0000 |         170.2067 |         -51.7591 |
[32m[20221214 14:20:32 @agent_ppo2.py:185][0m |          -0.0019 |         168.6077 |         -51.7568 |
[32m[20221214 14:20:32 @agent_ppo2.py:185][0m |          -0.0049 |         167.9359 |         -51.7833 |
[32m[20221214 14:20:32 @agent_ppo2.py:185][0m |          -0.0034 |         167.2905 |         -51.8879 |
[32m[20221214 14:20:32 @agent_ppo2.py:185][0m |          -0.0016 |         166.6728 |         -51.9583 |
[32m[20221214 14:20:32 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:20:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 758.77
[32m[20221214 14:20:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 763.17
[32m[20221214 14:20:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 758.35
[32m[20221214 14:20:32 @agent_ppo2.py:143][0m Total time:      22.50 min
[32m[20221214 14:20:32 @agent_ppo2.py:145][0m 2060288 total steps have happened
[32m[20221214 14:20:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1006 --------------------------#
[32m[20221214 14:20:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:32 @agent_ppo2.py:185][0m |           0.0077 |         172.0731 |         -54.5629 |
[32m[20221214 14:20:32 @agent_ppo2.py:185][0m |          -0.0043 |         160.4882 |         -54.7997 |
[32m[20221214 14:20:33 @agent_ppo2.py:185][0m |          -0.0031 |         157.9255 |         -54.7218 |
[32m[20221214 14:20:33 @agent_ppo2.py:185][0m |          -0.0014 |         156.4882 |         -54.8730 |
[32m[20221214 14:20:33 @agent_ppo2.py:185][0m |          -0.0041 |         155.3598 |         -54.8844 |
[32m[20221214 14:20:33 @agent_ppo2.py:185][0m |          -0.0011 |         154.6020 |         -54.7933 |
[32m[20221214 14:20:33 @agent_ppo2.py:185][0m |          -0.0017 |         154.1007 |         -54.8389 |
[32m[20221214 14:20:33 @agent_ppo2.py:185][0m |          -0.0044 |         154.2513 |         -54.8625 |
[32m[20221214 14:20:33 @agent_ppo2.py:185][0m |          -0.0000 |         154.7648 |         -54.7842 |
[32m[20221214 14:20:33 @agent_ppo2.py:185][0m |           0.0003 |         153.8840 |         -55.0229 |
[32m[20221214 14:20:33 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:20:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 747.59
[32m[20221214 14:20:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 758.90
[32m[20221214 14:20:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.31
[32m[20221214 14:20:33 @agent_ppo2.py:143][0m Total time:      22.52 min
[32m[20221214 14:20:33 @agent_ppo2.py:145][0m 2062336 total steps have happened
[32m[20221214 14:20:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1007 --------------------------#
[32m[20221214 14:20:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:34 @agent_ppo2.py:185][0m |          -0.0011 |         190.4861 |         -54.5219 |
[32m[20221214 14:20:34 @agent_ppo2.py:185][0m |           0.0053 |         193.7201 |         -54.5213 |
[32m[20221214 14:20:34 @agent_ppo2.py:185][0m |          -0.0023 |         187.2850 |         -54.3021 |
[32m[20221214 14:20:34 @agent_ppo2.py:185][0m |          -0.0022 |         186.4454 |         -54.4998 |
[32m[20221214 14:20:34 @agent_ppo2.py:185][0m |           0.0035 |         188.0872 |         -54.2960 |
[32m[20221214 14:20:34 @agent_ppo2.py:185][0m |          -0.0041 |         185.5721 |         -54.3611 |
[32m[20221214 14:20:34 @agent_ppo2.py:185][0m |           0.0049 |         190.1966 |         -54.3711 |
[32m[20221214 14:20:34 @agent_ppo2.py:185][0m |           0.0044 |         192.8271 |         -54.1367 |
[32m[20221214 14:20:35 @agent_ppo2.py:185][0m |          -0.0019 |         184.5045 |         -54.1935 |
[32m[20221214 14:20:35 @agent_ppo2.py:185][0m |          -0.0061 |         184.0616 |         -54.0455 |
[32m[20221214 14:20:35 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:20:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 754.50
[32m[20221214 14:20:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 763.50
[32m[20221214 14:20:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 751.76
[32m[20221214 14:20:35 @agent_ppo2.py:143][0m Total time:      22.55 min
[32m[20221214 14:20:35 @agent_ppo2.py:145][0m 2064384 total steps have happened
[32m[20221214 14:20:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1008 --------------------------#
[32m[20221214 14:20:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:35 @agent_ppo2.py:185][0m |          -0.0002 |         167.3374 |         -54.4219 |
[32m[20221214 14:20:35 @agent_ppo2.py:185][0m |           0.0024 |         160.0854 |         -54.3280 |
[32m[20221214 14:20:35 @agent_ppo2.py:185][0m |           0.0042 |         160.6651 |         -54.3925 |
[32m[20221214 14:20:35 @agent_ppo2.py:185][0m |          -0.0012 |         156.7296 |         -54.4624 |
[32m[20221214 14:20:36 @agent_ppo2.py:185][0m |          -0.0040 |         155.9099 |         -54.5183 |
[32m[20221214 14:20:36 @agent_ppo2.py:185][0m |           0.0020 |         156.2064 |         -54.5919 |
[32m[20221214 14:20:36 @agent_ppo2.py:185][0m |          -0.0019 |         153.3606 |         -54.4963 |
[32m[20221214 14:20:36 @agent_ppo2.py:185][0m |           0.0034 |         153.9993 |         -54.7185 |
[32m[20221214 14:20:36 @agent_ppo2.py:185][0m |          -0.0042 |         152.5571 |         -54.6141 |
[32m[20221214 14:20:36 @agent_ppo2.py:185][0m |           0.0026 |         154.2075 |         -54.7682 |
[32m[20221214 14:20:36 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:20:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 744.25
[32m[20221214 14:20:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 758.27
[32m[20221214 14:20:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 765.92
[32m[20221214 14:20:36 @agent_ppo2.py:143][0m Total time:      22.57 min
[32m[20221214 14:20:36 @agent_ppo2.py:145][0m 2066432 total steps have happened
[32m[20221214 14:20:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1009 --------------------------#
[32m[20221214 14:20:36 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:20:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:37 @agent_ppo2.py:185][0m |           0.0083 |         158.9115 |         -53.2085 |
[32m[20221214 14:20:37 @agent_ppo2.py:185][0m |           0.0001 |         145.9643 |         -53.4771 |
[32m[20221214 14:20:37 @agent_ppo2.py:185][0m |          -0.0014 |         143.1967 |         -53.4383 |
[32m[20221214 14:20:37 @agent_ppo2.py:185][0m |          -0.0024 |         141.3564 |         -53.3881 |
[32m[20221214 14:20:37 @agent_ppo2.py:185][0m |          -0.0049 |         140.6882 |         -53.5462 |
[32m[20221214 14:20:37 @agent_ppo2.py:185][0m |          -0.0012 |         140.2961 |         -53.2289 |
[32m[20221214 14:20:37 @agent_ppo2.py:185][0m |          -0.0002 |         139.7931 |         -53.6219 |
[32m[20221214 14:20:37 @agent_ppo2.py:185][0m |          -0.0020 |         137.0772 |         -53.5467 |
[32m[20221214 14:20:37 @agent_ppo2.py:185][0m |          -0.0019 |         136.3451 |         -53.4109 |
[32m[20221214 14:20:37 @agent_ppo2.py:185][0m |          -0.0020 |         136.7734 |         -53.5911 |
[32m[20221214 14:20:37 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:20:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 751.39
[32m[20221214 14:20:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 757.27
[32m[20221214 14:20:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 775.30
[32m[20221214 14:20:38 @agent_ppo2.py:143][0m Total time:      22.59 min
[32m[20221214 14:20:38 @agent_ppo2.py:145][0m 2068480 total steps have happened
[32m[20221214 14:20:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1010 --------------------------#
[32m[20221214 14:20:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:20:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:38 @agent_ppo2.py:185][0m |          -0.0021 |         146.5887 |         -55.6039 |
[32m[20221214 14:20:38 @agent_ppo2.py:185][0m |          -0.0037 |         142.2470 |         -55.5826 |
[32m[20221214 14:20:38 @agent_ppo2.py:185][0m |           0.0006 |         141.3383 |         -55.5416 |
[32m[20221214 14:20:38 @agent_ppo2.py:185][0m |          -0.0005 |         139.1470 |         -55.6659 |
[32m[20221214 14:20:38 @agent_ppo2.py:185][0m |          -0.0006 |         139.1826 |         -55.7007 |
[32m[20221214 14:20:38 @agent_ppo2.py:185][0m |          -0.0002 |         138.8636 |         -55.8148 |
[32m[20221214 14:20:38 @agent_ppo2.py:185][0m |           0.0073 |         143.3369 |         -55.8516 |
[32m[20221214 14:20:39 @agent_ppo2.py:185][0m |          -0.0020 |         137.4331 |         -55.7164 |
[32m[20221214 14:20:39 @agent_ppo2.py:185][0m |          -0.0042 |         137.0268 |         -55.7901 |
[32m[20221214 14:20:39 @agent_ppo2.py:185][0m |          -0.0027 |         136.7148 |         -55.9687 |
[32m[20221214 14:20:39 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:20:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 761.53
[32m[20221214 14:20:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.34
[32m[20221214 14:20:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 768.49
[32m[20221214 14:20:39 @agent_ppo2.py:143][0m Total time:      22.61 min
[32m[20221214 14:20:39 @agent_ppo2.py:145][0m 2070528 total steps have happened
[32m[20221214 14:20:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1011 --------------------------#
[32m[20221214 14:20:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:39 @agent_ppo2.py:185][0m |          -0.0012 |         137.8218 |         -56.1192 |
[32m[20221214 14:20:39 @agent_ppo2.py:185][0m |          -0.0052 |         134.9259 |         -55.8715 |
[32m[20221214 14:20:39 @agent_ppo2.py:185][0m |          -0.0043 |         133.0691 |         -55.7655 |
[32m[20221214 14:20:40 @agent_ppo2.py:185][0m |          -0.0019 |         131.8017 |         -55.8506 |
[32m[20221214 14:20:40 @agent_ppo2.py:185][0m |          -0.0053 |         131.5606 |         -55.8090 |
[32m[20221214 14:20:40 @agent_ppo2.py:185][0m |          -0.0013 |         130.6899 |         -55.6614 |
[32m[20221214 14:20:40 @agent_ppo2.py:185][0m |          -0.0052 |         130.3900 |         -55.4936 |
[32m[20221214 14:20:40 @agent_ppo2.py:185][0m |          -0.0031 |         129.4960 |         -55.6307 |
[32m[20221214 14:20:40 @agent_ppo2.py:185][0m |           0.0001 |         129.8449 |         -55.7387 |
[32m[20221214 14:20:40 @agent_ppo2.py:185][0m |          -0.0005 |         128.9881 |         -55.7668 |
[32m[20221214 14:20:40 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:20:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 761.56
[32m[20221214 14:20:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.83
[32m[20221214 14:20:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 748.25
[32m[20221214 14:20:40 @agent_ppo2.py:143][0m Total time:      22.64 min
[32m[20221214 14:20:40 @agent_ppo2.py:145][0m 2072576 total steps have happened
[32m[20221214 14:20:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1012 --------------------------#
[32m[20221214 14:20:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:41 @agent_ppo2.py:185][0m |          -0.0038 |         142.9794 |         -55.8496 |
[32m[20221214 14:20:41 @agent_ppo2.py:185][0m |          -0.0034 |         138.4761 |         -55.7821 |
[32m[20221214 14:20:41 @agent_ppo2.py:185][0m |          -0.0020 |         137.3284 |         -55.7942 |
[32m[20221214 14:20:41 @agent_ppo2.py:185][0m |           0.0015 |         138.4087 |         -55.5908 |
[32m[20221214 14:20:41 @agent_ppo2.py:185][0m |           0.0067 |         156.4088 |         -55.7406 |
[32m[20221214 14:20:41 @agent_ppo2.py:185][0m |          -0.0026 |         136.3539 |         -55.1731 |
[32m[20221214 14:20:41 @agent_ppo2.py:185][0m |          -0.0039 |         135.0527 |         -55.6023 |
[32m[20221214 14:20:41 @agent_ppo2.py:185][0m |          -0.0026 |         134.8625 |         -55.6738 |
[32m[20221214 14:20:41 @agent_ppo2.py:185][0m |          -0.0013 |         134.0638 |         -55.3586 |
[32m[20221214 14:20:41 @agent_ppo2.py:185][0m |          -0.0029 |         134.2264 |         -55.4869 |
[32m[20221214 14:20:41 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:20:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 741.87
[32m[20221214 14:20:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.41
[32m[20221214 14:20:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.57
[32m[20221214 14:20:42 @agent_ppo2.py:143][0m Total time:      22.66 min
[32m[20221214 14:20:42 @agent_ppo2.py:145][0m 2074624 total steps have happened
[32m[20221214 14:20:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1013 --------------------------#
[32m[20221214 14:20:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:42 @agent_ppo2.py:185][0m |          -0.0011 |         154.8603 |         -56.6719 |
[32m[20221214 14:20:42 @agent_ppo2.py:185][0m |          -0.0007 |         147.7897 |         -56.7770 |
[32m[20221214 14:20:42 @agent_ppo2.py:185][0m |          -0.0000 |         141.3617 |         -56.6926 |
[32m[20221214 14:20:42 @agent_ppo2.py:185][0m |           0.0042 |         145.0833 |         -56.8938 |
[32m[20221214 14:20:42 @agent_ppo2.py:185][0m |          -0.0015 |         138.4571 |         -56.9491 |
[32m[20221214 14:20:42 @agent_ppo2.py:185][0m |          -0.0023 |         136.8569 |         -57.1048 |
[32m[20221214 14:20:43 @agent_ppo2.py:185][0m |           0.0042 |         137.5289 |         -57.1358 |
[32m[20221214 14:20:43 @agent_ppo2.py:185][0m |          -0.0032 |         136.0901 |         -57.1810 |
[32m[20221214 14:20:43 @agent_ppo2.py:185][0m |          -0.0019 |         135.0886 |         -57.2177 |
[32m[20221214 14:20:43 @agent_ppo2.py:185][0m |          -0.0001 |         134.4695 |         -57.3927 |
[32m[20221214 14:20:43 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:20:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.60
[32m[20221214 14:20:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 755.61
[32m[20221214 14:20:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.45
[32m[20221214 14:20:43 @agent_ppo2.py:143][0m Total time:      22.68 min
[32m[20221214 14:20:43 @agent_ppo2.py:145][0m 2076672 total steps have happened
[32m[20221214 14:20:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1014 --------------------------#
[32m[20221214 14:20:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:43 @agent_ppo2.py:185][0m |           0.0010 |         167.1322 |         -58.3477 |
[32m[20221214 14:20:43 @agent_ppo2.py:185][0m |          -0.0017 |         161.6072 |         -58.3882 |
[32m[20221214 14:20:44 @agent_ppo2.py:185][0m |          -0.0012 |         159.3672 |         -58.2907 |
[32m[20221214 14:20:44 @agent_ppo2.py:185][0m |          -0.0017 |         157.3564 |         -58.1206 |
[32m[20221214 14:20:44 @agent_ppo2.py:185][0m |           0.0010 |         157.1479 |         -58.0510 |
[32m[20221214 14:20:44 @agent_ppo2.py:185][0m |          -0.0033 |         156.9494 |         -58.0948 |
[32m[20221214 14:20:44 @agent_ppo2.py:185][0m |           0.0020 |         156.7140 |         -57.9778 |
[32m[20221214 14:20:44 @agent_ppo2.py:185][0m |          -0.0025 |         154.7406 |         -58.0076 |
[32m[20221214 14:20:44 @agent_ppo2.py:185][0m |          -0.0014 |         153.5923 |         -57.8994 |
[32m[20221214 14:20:44 @agent_ppo2.py:185][0m |           0.0025 |         154.0885 |         -57.7412 |
[32m[20221214 14:20:44 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:20:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 741.45
[32m[20221214 14:20:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 744.61
[32m[20221214 14:20:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.81
[32m[20221214 14:20:44 @agent_ppo2.py:143][0m Total time:      22.70 min
[32m[20221214 14:20:44 @agent_ppo2.py:145][0m 2078720 total steps have happened
[32m[20221214 14:20:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1015 --------------------------#
[32m[20221214 14:20:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:45 @agent_ppo2.py:185][0m |          -0.0022 |         168.9307 |         -55.6635 |
[32m[20221214 14:20:45 @agent_ppo2.py:185][0m |           0.0008 |         162.9180 |         -55.5524 |
[32m[20221214 14:20:45 @agent_ppo2.py:185][0m |           0.0002 |         160.9969 |         -55.5651 |
[32m[20221214 14:20:45 @agent_ppo2.py:185][0m |           0.0069 |         162.8333 |         -55.6320 |
[32m[20221214 14:20:45 @agent_ppo2.py:185][0m |          -0.0020 |         159.2985 |         -55.6315 |
[32m[20221214 14:20:45 @agent_ppo2.py:185][0m |          -0.0032 |         157.6419 |         -55.6518 |
[32m[20221214 14:20:45 @agent_ppo2.py:185][0m |          -0.0040 |         157.0095 |         -55.7314 |
[32m[20221214 14:20:45 @agent_ppo2.py:185][0m |          -0.0046 |         156.8259 |         -55.6927 |
[32m[20221214 14:20:45 @agent_ppo2.py:185][0m |          -0.0001 |         156.3785 |         -55.9458 |
[32m[20221214 14:20:46 @agent_ppo2.py:185][0m |          -0.0006 |         155.4940 |         -55.8836 |
[32m[20221214 14:20:46 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:20:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 745.36
[32m[20221214 14:20:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.37
[32m[20221214 14:20:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 759.29
[32m[20221214 14:20:46 @agent_ppo2.py:143][0m Total time:      22.73 min
[32m[20221214 14:20:46 @agent_ppo2.py:145][0m 2080768 total steps have happened
[32m[20221214 14:20:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1016 --------------------------#
[32m[20221214 14:20:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:46 @agent_ppo2.py:185][0m |          -0.0046 |         160.3237 |         -56.4957 |
[32m[20221214 14:20:46 @agent_ppo2.py:185][0m |           0.0014 |         153.8398 |         -56.4666 |
[32m[20221214 14:20:46 @agent_ppo2.py:185][0m |           0.0006 |         151.8589 |         -56.2773 |
[32m[20221214 14:20:46 @agent_ppo2.py:185][0m |          -0.0044 |         150.9910 |         -56.1853 |
[32m[20221214 14:20:46 @agent_ppo2.py:185][0m |          -0.0012 |         150.4694 |         -56.3216 |
[32m[20221214 14:20:47 @agent_ppo2.py:185][0m |          -0.0033 |         149.8463 |         -56.0850 |
[32m[20221214 14:20:47 @agent_ppo2.py:185][0m |           0.0100 |         160.2318 |         -55.9679 |
[32m[20221214 14:20:47 @agent_ppo2.py:185][0m |          -0.0039 |         149.5723 |         -56.1541 |
[32m[20221214 14:20:47 @agent_ppo2.py:185][0m |          -0.0012 |         148.3881 |         -55.9847 |
[32m[20221214 14:20:47 @agent_ppo2.py:185][0m |          -0.0026 |         147.9424 |         -55.9054 |
[32m[20221214 14:20:47 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:20:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 754.98
[32m[20221214 14:20:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.18
[32m[20221214 14:20:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 736.64
[32m[20221214 14:20:47 @agent_ppo2.py:143][0m Total time:      22.75 min
[32m[20221214 14:20:47 @agent_ppo2.py:145][0m 2082816 total steps have happened
[32m[20221214 14:20:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1017 --------------------------#
[32m[20221214 14:20:47 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:20:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:47 @agent_ppo2.py:185][0m |          -0.0019 |         166.6358 |         -56.2519 |
[32m[20221214 14:20:48 @agent_ppo2.py:185][0m |           0.0003 |         160.0716 |         -56.1097 |
[32m[20221214 14:20:48 @agent_ppo2.py:185][0m |          -0.0002 |         156.6836 |         -56.2293 |
[32m[20221214 14:20:48 @agent_ppo2.py:185][0m |          -0.0043 |         155.4716 |         -56.1919 |
[32m[20221214 14:20:48 @agent_ppo2.py:185][0m |          -0.0016 |         154.6234 |         -56.1703 |
[32m[20221214 14:20:48 @agent_ppo2.py:185][0m |          -0.0030 |         153.5600 |         -56.2119 |
[32m[20221214 14:20:48 @agent_ppo2.py:185][0m |          -0.0009 |         153.2544 |         -56.1785 |
[32m[20221214 14:20:48 @agent_ppo2.py:185][0m |          -0.0009 |         152.6440 |         -56.3619 |
[32m[20221214 14:20:48 @agent_ppo2.py:185][0m |          -0.0013 |         152.4157 |         -56.3462 |
[32m[20221214 14:20:48 @agent_ppo2.py:185][0m |           0.0043 |         155.8316 |         -56.3053 |
[32m[20221214 14:20:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:20:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.37
[32m[20221214 14:20:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 742.58
[32m[20221214 14:20:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 749.19
[32m[20221214 14:20:48 @agent_ppo2.py:143][0m Total time:      22.77 min
[32m[20221214 14:20:48 @agent_ppo2.py:145][0m 2084864 total steps have happened
[32m[20221214 14:20:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1018 --------------------------#
[32m[20221214 14:20:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:20:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:49 @agent_ppo2.py:185][0m |           0.0000 |         149.2328 |         -56.6254 |
[32m[20221214 14:20:49 @agent_ppo2.py:185][0m |          -0.0018 |         145.9196 |         -56.5343 |
[32m[20221214 14:20:49 @agent_ppo2.py:185][0m |          -0.0025 |         143.4877 |         -56.3479 |
[32m[20221214 14:20:49 @agent_ppo2.py:185][0m |          -0.0011 |         142.0633 |         -56.4142 |
[32m[20221214 14:20:49 @agent_ppo2.py:185][0m |          -0.0007 |         140.8536 |         -56.4168 |
[32m[20221214 14:20:49 @agent_ppo2.py:185][0m |           0.0002 |         138.9973 |         -56.5869 |
[32m[20221214 14:20:49 @agent_ppo2.py:185][0m |          -0.0025 |         137.8532 |         -56.4148 |
[32m[20221214 14:20:49 @agent_ppo2.py:185][0m |          -0.0035 |         137.3975 |         -56.4859 |
[32m[20221214 14:20:50 @agent_ppo2.py:185][0m |          -0.0010 |         134.6300 |         -56.5269 |
[32m[20221214 14:20:50 @agent_ppo2.py:185][0m |           0.0007 |         133.5469 |         -56.5518 |
[32m[20221214 14:20:50 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:20:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.74
[32m[20221214 14:20:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.48
[32m[20221214 14:20:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 755.86
[32m[20221214 14:20:50 @agent_ppo2.py:143][0m Total time:      22.80 min
[32m[20221214 14:20:50 @agent_ppo2.py:145][0m 2086912 total steps have happened
[32m[20221214 14:20:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1019 --------------------------#
[32m[20221214 14:20:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:20:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:50 @agent_ppo2.py:185][0m |          -0.0013 |         168.7583 |         -56.0749 |
[32m[20221214 14:20:50 @agent_ppo2.py:185][0m |          -0.0021 |         166.2169 |         -55.9588 |
[32m[20221214 14:20:50 @agent_ppo2.py:185][0m |           0.0072 |         173.6570 |         -55.9513 |
[32m[20221214 14:20:50 @agent_ppo2.py:185][0m |          -0.0011 |         163.0809 |         -56.0401 |
[32m[20221214 14:20:51 @agent_ppo2.py:185][0m |          -0.0026 |         163.7380 |         -56.0224 |
[32m[20221214 14:20:51 @agent_ppo2.py:185][0m |          -0.0017 |         162.5903 |         -56.0145 |
[32m[20221214 14:20:51 @agent_ppo2.py:185][0m |          -0.0027 |         162.5412 |         -56.0575 |
[32m[20221214 14:20:51 @agent_ppo2.py:185][0m |           0.0054 |         168.7994 |         -55.9610 |
[32m[20221214 14:20:51 @agent_ppo2.py:185][0m |          -0.0011 |         161.7195 |         -55.8083 |
[32m[20221214 14:20:51 @agent_ppo2.py:185][0m |           0.0008 |         162.3726 |         -55.9256 |
[32m[20221214 14:20:51 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:20:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 741.74
[32m[20221214 14:20:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.20
[32m[20221214 14:20:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 728.16
[32m[20221214 14:20:51 @agent_ppo2.py:143][0m Total time:      22.82 min
[32m[20221214 14:20:51 @agent_ppo2.py:145][0m 2088960 total steps have happened
[32m[20221214 14:20:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1020 --------------------------#
[32m[20221214 14:20:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:20:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:52 @agent_ppo2.py:185][0m |           0.0126 |         189.7449 |         -55.0507 |
[32m[20221214 14:20:52 @agent_ppo2.py:185][0m |           0.0133 |         185.2840 |         -54.9157 |
[32m[20221214 14:20:52 @agent_ppo2.py:185][0m |           0.0035 |         164.2519 |         -55.1513 |
[32m[20221214 14:20:52 @agent_ppo2.py:185][0m |          -0.0029 |         160.6123 |         -55.1192 |
[32m[20221214 14:20:52 @agent_ppo2.py:185][0m |          -0.0037 |         160.3022 |         -55.0519 |
[32m[20221214 14:20:52 @agent_ppo2.py:185][0m |          -0.0044 |         159.5040 |         -55.2213 |
[32m[20221214 14:20:52 @agent_ppo2.py:185][0m |          -0.0045 |         159.5299 |         -55.2875 |
[32m[20221214 14:20:52 @agent_ppo2.py:185][0m |          -0.0057 |         159.2933 |         -55.2921 |
[32m[20221214 14:20:52 @agent_ppo2.py:185][0m |          -0.0042 |         158.9367 |         -55.3033 |
[32m[20221214 14:20:52 @agent_ppo2.py:185][0m |          -0.0027 |         158.8705 |         -55.3998 |
[32m[20221214 14:20:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:20:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 721.47
[32m[20221214 14:20:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 727.79
[32m[20221214 14:20:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 740.58
[32m[20221214 14:20:53 @agent_ppo2.py:143][0m Total time:      22.84 min
[32m[20221214 14:20:53 @agent_ppo2.py:145][0m 2091008 total steps have happened
[32m[20221214 14:20:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1021 --------------------------#
[32m[20221214 14:20:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:20:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:53 @agent_ppo2.py:185][0m |           0.0084 |         168.4384 |         -55.9334 |
[32m[20221214 14:20:53 @agent_ppo2.py:185][0m |          -0.0010 |         149.1932 |         -55.5621 |
[32m[20221214 14:20:53 @agent_ppo2.py:185][0m |          -0.0022 |         146.1994 |         -55.7101 |
[32m[20221214 14:20:53 @agent_ppo2.py:185][0m |           0.0002 |         144.2227 |         -55.5602 |
[32m[20221214 14:20:53 @agent_ppo2.py:185][0m |          -0.0037 |         143.0213 |         -55.6283 |
[32m[20221214 14:20:53 @agent_ppo2.py:185][0m |          -0.0026 |         142.1041 |         -55.5715 |
[32m[20221214 14:20:53 @agent_ppo2.py:185][0m |          -0.0010 |         141.5682 |         -55.4487 |
[32m[20221214 14:20:53 @agent_ppo2.py:185][0m |          -0.0020 |         140.5572 |         -55.6294 |
[32m[20221214 14:20:54 @agent_ppo2.py:185][0m |           0.0005 |         139.0384 |         -55.4601 |
[32m[20221214 14:20:54 @agent_ppo2.py:185][0m |          -0.0031 |         137.7409 |         -55.5458 |
[32m[20221214 14:20:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:20:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.25
[32m[20221214 14:20:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.39
[32m[20221214 14:20:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 748.30
[32m[20221214 14:20:54 @agent_ppo2.py:143][0m Total time:      22.86 min
[32m[20221214 14:20:54 @agent_ppo2.py:145][0m 2093056 total steps have happened
[32m[20221214 14:20:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1022 --------------------------#
[32m[20221214 14:20:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:54 @agent_ppo2.py:185][0m |          -0.0017 |         181.9695 |         -56.9828 |
[32m[20221214 14:20:54 @agent_ppo2.py:185][0m |           0.0003 |         178.7528 |         -57.2733 |
[32m[20221214 14:20:54 @agent_ppo2.py:185][0m |          -0.0030 |         177.6377 |         -57.2081 |
[32m[20221214 14:20:54 @agent_ppo2.py:185][0m |           0.0072 |         187.7225 |         -57.5174 |
[32m[20221214 14:20:55 @agent_ppo2.py:185][0m |          -0.0006 |         177.0985 |         -57.2428 |
[32m[20221214 14:20:55 @agent_ppo2.py:185][0m |          -0.0034 |         176.0608 |         -57.5289 |
[32m[20221214 14:20:55 @agent_ppo2.py:185][0m |          -0.0051 |         177.1690 |         -57.6809 |
[32m[20221214 14:20:55 @agent_ppo2.py:185][0m |          -0.0039 |         176.3765 |         -57.7554 |
[32m[20221214 14:20:55 @agent_ppo2.py:185][0m |          -0.0036 |         176.4518 |         -57.8465 |
[32m[20221214 14:20:55 @agent_ppo2.py:185][0m |          -0.0025 |         175.4733 |         -57.8437 |
[32m[20221214 14:20:55 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:20:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 747.14
[32m[20221214 14:20:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 757.68
[32m[20221214 14:20:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 742.24
[32m[20221214 14:20:55 @agent_ppo2.py:143][0m Total time:      22.89 min
[32m[20221214 14:20:55 @agent_ppo2.py:145][0m 2095104 total steps have happened
[32m[20221214 14:20:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1023 --------------------------#
[32m[20221214 14:20:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:56 @agent_ppo2.py:185][0m |           0.0105 |         196.0063 |         -57.7653 |
[32m[20221214 14:20:56 @agent_ppo2.py:185][0m |          -0.0012 |         168.4954 |         -58.0070 |
[32m[20221214 14:20:56 @agent_ppo2.py:185][0m |           0.0016 |         167.0086 |         -57.9453 |
[32m[20221214 14:20:56 @agent_ppo2.py:185][0m |           0.0006 |         164.0433 |         -58.2106 |
[32m[20221214 14:20:56 @agent_ppo2.py:185][0m |          -0.0013 |         162.9675 |         -58.2519 |
[32m[20221214 14:20:56 @agent_ppo2.py:185][0m |          -0.0014 |         162.0127 |         -58.4405 |
[32m[20221214 14:20:56 @agent_ppo2.py:185][0m |          -0.0013 |         162.1719 |         -58.4891 |
[32m[20221214 14:20:56 @agent_ppo2.py:185][0m |           0.0026 |         164.3387 |         -58.3709 |
[32m[20221214 14:20:56 @agent_ppo2.py:185][0m |           0.0050 |         162.9574 |         -58.3210 |
[32m[20221214 14:20:56 @agent_ppo2.py:185][0m |          -0.0019 |         160.5914 |         -58.6507 |
[32m[20221214 14:20:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:20:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 732.04
[32m[20221214 14:20:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.40
[32m[20221214 14:20:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 748.35
[32m[20221214 14:20:57 @agent_ppo2.py:143][0m Total time:      22.91 min
[32m[20221214 14:20:57 @agent_ppo2.py:145][0m 2097152 total steps have happened
[32m[20221214 14:20:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1024 --------------------------#
[32m[20221214 14:20:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:57 @agent_ppo2.py:185][0m |          -0.0031 |         177.0892 |         -58.1976 |
[32m[20221214 14:20:57 @agent_ppo2.py:185][0m |          -0.0046 |         171.7672 |         -58.3091 |
[32m[20221214 14:20:57 @agent_ppo2.py:185][0m |           0.0052 |         177.2235 |         -58.2129 |
[32m[20221214 14:20:57 @agent_ppo2.py:185][0m |          -0.0016 |         169.7945 |         -58.1190 |
[32m[20221214 14:20:57 @agent_ppo2.py:185][0m |          -0.0056 |         170.3121 |         -58.1677 |
[32m[20221214 14:20:57 @agent_ppo2.py:185][0m |          -0.0032 |         168.3921 |         -58.0429 |
[32m[20221214 14:20:58 @agent_ppo2.py:185][0m |          -0.0049 |         168.0643 |         -58.0595 |
[32m[20221214 14:20:58 @agent_ppo2.py:185][0m |          -0.0031 |         167.7424 |         -57.9885 |
[32m[20221214 14:20:58 @agent_ppo2.py:185][0m |          -0.0052 |         167.2503 |         -57.9922 |
[32m[20221214 14:20:58 @agent_ppo2.py:185][0m |           0.0035 |         173.4049 |         -57.9569 |
[32m[20221214 14:20:58 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:20:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 737.96
[32m[20221214 14:20:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.06
[32m[20221214 14:20:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 742.95
[32m[20221214 14:20:58 @agent_ppo2.py:143][0m Total time:      22.93 min
[32m[20221214 14:20:58 @agent_ppo2.py:145][0m 2099200 total steps have happened
[32m[20221214 14:20:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1025 --------------------------#
[32m[20221214 14:20:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:20:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:20:58 @agent_ppo2.py:185][0m |          -0.0025 |         181.0888 |         -58.4692 |
[32m[20221214 14:20:58 @agent_ppo2.py:185][0m |          -0.0016 |         173.1594 |         -58.4297 |
[32m[20221214 14:20:59 @agent_ppo2.py:185][0m |          -0.0035 |         170.1859 |         -58.5160 |
[32m[20221214 14:20:59 @agent_ppo2.py:185][0m |          -0.0013 |         169.5231 |         -58.5208 |
[32m[20221214 14:20:59 @agent_ppo2.py:185][0m |          -0.0035 |         166.4247 |         -58.5218 |
[32m[20221214 14:20:59 @agent_ppo2.py:185][0m |          -0.0008 |         166.0613 |         -58.6805 |
[32m[20221214 14:20:59 @agent_ppo2.py:185][0m |          -0.0003 |         164.7846 |         -58.5185 |
[32m[20221214 14:20:59 @agent_ppo2.py:185][0m |          -0.0033 |         163.2911 |         -58.6276 |
[32m[20221214 14:20:59 @agent_ppo2.py:185][0m |           0.0026 |         164.2444 |         -58.5732 |
[32m[20221214 14:20:59 @agent_ppo2.py:185][0m |           0.0009 |         163.1680 |         -58.7493 |
[32m[20221214 14:20:59 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:20:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.42
[32m[20221214 14:20:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.54
[32m[20221214 14:20:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 750.35
[32m[20221214 14:20:59 @agent_ppo2.py:143][0m Total time:      22.95 min
[32m[20221214 14:20:59 @agent_ppo2.py:145][0m 2101248 total steps have happened
[32m[20221214 14:20:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1026 --------------------------#
[32m[20221214 14:21:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:00 @agent_ppo2.py:185][0m |          -0.0003 |         176.9237 |         -57.6854 |
[32m[20221214 14:21:00 @agent_ppo2.py:185][0m |           0.0072 |         183.6854 |         -57.5253 |
[32m[20221214 14:21:00 @agent_ppo2.py:185][0m |           0.0060 |         172.0202 |         -57.4555 |
[32m[20221214 14:21:00 @agent_ppo2.py:185][0m |          -0.0041 |         167.6888 |         -57.4526 |
[32m[20221214 14:21:00 @agent_ppo2.py:185][0m |          -0.0004 |         166.9736 |         -57.4275 |
[32m[20221214 14:21:00 @agent_ppo2.py:185][0m |          -0.0020 |         166.9442 |         -57.0038 |
[32m[20221214 14:21:00 @agent_ppo2.py:185][0m |          -0.0021 |         165.8031 |         -57.0776 |
[32m[20221214 14:21:00 @agent_ppo2.py:185][0m |          -0.0004 |         169.1443 |         -56.8234 |
[32m[20221214 14:21:00 @agent_ppo2.py:185][0m |          -0.0028 |         164.7808 |         -57.0236 |
[32m[20221214 14:21:01 @agent_ppo2.py:185][0m |          -0.0037 |         164.7314 |         -56.8662 |
[32m[20221214 14:21:01 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:21:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 741.22
[32m[20221214 14:21:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 753.42
[32m[20221214 14:21:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.74
[32m[20221214 14:21:01 @agent_ppo2.py:143][0m Total time:      22.98 min
[32m[20221214 14:21:01 @agent_ppo2.py:145][0m 2103296 total steps have happened
[32m[20221214 14:21:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1027 --------------------------#
[32m[20221214 14:21:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:21:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:01 @agent_ppo2.py:185][0m |          -0.0011 |         215.1856 |         -57.4450 |
[32m[20221214 14:21:01 @agent_ppo2.py:185][0m |           0.0001 |         209.1787 |         -57.5240 |
[32m[20221214 14:21:01 @agent_ppo2.py:185][0m |           0.0020 |         208.0503 |         -57.2502 |
[32m[20221214 14:21:01 @agent_ppo2.py:185][0m |          -0.0015 |         207.0803 |         -57.1809 |
[32m[20221214 14:21:01 @agent_ppo2.py:185][0m |          -0.0024 |         206.0109 |         -57.2943 |
[32m[20221214 14:21:01 @agent_ppo2.py:185][0m |          -0.0006 |         205.8787 |         -57.1271 |
[32m[20221214 14:21:02 @agent_ppo2.py:185][0m |          -0.0041 |         205.1489 |         -57.0667 |
[32m[20221214 14:21:02 @agent_ppo2.py:185][0m |          -0.0045 |         204.9024 |         -56.9898 |
[32m[20221214 14:21:02 @agent_ppo2.py:185][0m |          -0.0008 |         204.9247 |         -57.0685 |
[32m[20221214 14:21:02 @agent_ppo2.py:185][0m |          -0.0024 |         204.9430 |         -56.9358 |
[32m[20221214 14:21:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:21:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 745.68
[32m[20221214 14:21:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 755.51
[32m[20221214 14:21:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 758.68
[32m[20221214 14:21:02 @agent_ppo2.py:143][0m Total time:      23.00 min
[32m[20221214 14:21:02 @agent_ppo2.py:145][0m 2105344 total steps have happened
[32m[20221214 14:21:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1028 --------------------------#
[32m[20221214 14:21:02 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:21:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:02 @agent_ppo2.py:185][0m |          -0.0011 |         199.1945 |         -57.0105 |
[32m[20221214 14:21:02 @agent_ppo2.py:185][0m |          -0.0018 |         195.2660 |         -56.9051 |
[32m[20221214 14:21:03 @agent_ppo2.py:185][0m |          -0.0039 |         193.3777 |         -56.9020 |
[32m[20221214 14:21:03 @agent_ppo2.py:185][0m |          -0.0022 |         191.3426 |         -56.8551 |
[32m[20221214 14:21:03 @agent_ppo2.py:185][0m |          -0.0028 |         190.2535 |         -56.8980 |
[32m[20221214 14:21:03 @agent_ppo2.py:185][0m |          -0.0027 |         190.0408 |         -56.8441 |
[32m[20221214 14:21:03 @agent_ppo2.py:185][0m |           0.0017 |         190.9732 |         -56.6469 |
[32m[20221214 14:21:03 @agent_ppo2.py:185][0m |          -0.0028 |         188.5869 |         -56.9042 |
[32m[20221214 14:21:03 @agent_ppo2.py:185][0m |          -0.0016 |         188.2690 |         -56.7073 |
[32m[20221214 14:21:03 @agent_ppo2.py:185][0m |          -0.0024 |         188.2606 |         -56.8082 |
[32m[20221214 14:21:03 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 14:21:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.01
[32m[20221214 14:21:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 754.83
[32m[20221214 14:21:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 759.12
[32m[20221214 14:21:04 @agent_ppo2.py:143][0m Total time:      23.02 min
[32m[20221214 14:21:04 @agent_ppo2.py:145][0m 2107392 total steps have happened
[32m[20221214 14:21:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1029 --------------------------#
[32m[20221214 14:21:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:04 @agent_ppo2.py:185][0m |           0.0051 |         206.9325 |         -56.2322 |
[32m[20221214 14:21:04 @agent_ppo2.py:185][0m |           0.0018 |         204.5476 |         -56.2949 |
[32m[20221214 14:21:04 @agent_ppo2.py:185][0m |           0.0014 |         203.6013 |         -56.5053 |
[32m[20221214 14:21:04 @agent_ppo2.py:185][0m |          -0.0010 |         200.8407 |         -56.3191 |
[32m[20221214 14:21:04 @agent_ppo2.py:185][0m |          -0.0018 |         199.5893 |         -56.5528 |
[32m[20221214 14:21:04 @agent_ppo2.py:185][0m |          -0.0031 |         199.7769 |         -56.4833 |
[32m[20221214 14:21:04 @agent_ppo2.py:185][0m |          -0.0028 |         198.1751 |         -56.5252 |
[32m[20221214 14:21:05 @agent_ppo2.py:185][0m |          -0.0031 |         197.6357 |         -56.3590 |
[32m[20221214 14:21:05 @agent_ppo2.py:185][0m |           0.0016 |         199.1566 |         -56.5291 |
[32m[20221214 14:21:05 @agent_ppo2.py:185][0m |          -0.0015 |         195.8054 |         -56.5385 |
[32m[20221214 14:21:05 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:21:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.92
[32m[20221214 14:21:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 759.89
[32m[20221214 14:21:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 758.58
[32m[20221214 14:21:05 @agent_ppo2.py:143][0m Total time:      23.05 min
[32m[20221214 14:21:05 @agent_ppo2.py:145][0m 2109440 total steps have happened
[32m[20221214 14:21:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1030 --------------------------#
[32m[20221214 14:21:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:21:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:05 @agent_ppo2.py:185][0m |          -0.0007 |         197.1873 |         -57.5412 |
[32m[20221214 14:21:05 @agent_ppo2.py:185][0m |          -0.0012 |         191.7418 |         -57.7054 |
[32m[20221214 14:21:05 @agent_ppo2.py:185][0m |           0.0010 |         189.7909 |         -57.5280 |
[32m[20221214 14:21:06 @agent_ppo2.py:185][0m |          -0.0018 |         188.6513 |         -57.7093 |
[32m[20221214 14:21:06 @agent_ppo2.py:185][0m |          -0.0020 |         188.0995 |         -57.8391 |
[32m[20221214 14:21:06 @agent_ppo2.py:185][0m |           0.0017 |         187.3115 |         -57.6464 |
[32m[20221214 14:21:06 @agent_ppo2.py:185][0m |          -0.0023 |         185.9803 |         -57.6712 |
[32m[20221214 14:21:06 @agent_ppo2.py:185][0m |          -0.0013 |         185.5467 |         -57.8067 |
[32m[20221214 14:21:06 @agent_ppo2.py:185][0m |           0.0059 |         191.4984 |         -57.8268 |
[32m[20221214 14:21:06 @agent_ppo2.py:185][0m |          -0.0017 |         185.5300 |         -57.9202 |
[32m[20221214 14:21:06 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:21:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.85
[32m[20221214 14:21:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 759.61
[32m[20221214 14:21:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 755.97
[32m[20221214 14:21:06 @agent_ppo2.py:143][0m Total time:      23.07 min
[32m[20221214 14:21:06 @agent_ppo2.py:145][0m 2111488 total steps have happened
[32m[20221214 14:21:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1031 --------------------------#
[32m[20221214 14:21:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:21:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:07 @agent_ppo2.py:185][0m |          -0.0017 |         200.9817 |         -57.5910 |
[32m[20221214 14:21:07 @agent_ppo2.py:185][0m |          -0.0020 |         196.4367 |         -57.4217 |
[32m[20221214 14:21:07 @agent_ppo2.py:185][0m |          -0.0031 |         195.1148 |         -57.6448 |
[32m[20221214 14:21:07 @agent_ppo2.py:185][0m |          -0.0024 |         194.4012 |         -57.6412 |
[32m[20221214 14:21:07 @agent_ppo2.py:185][0m |          -0.0020 |         194.8361 |         -57.7368 |
[32m[20221214 14:21:07 @agent_ppo2.py:185][0m |          -0.0020 |         193.6823 |         -57.7637 |
[32m[20221214 14:21:07 @agent_ppo2.py:185][0m |           0.0023 |         195.8762 |         -57.8377 |
[32m[20221214 14:21:07 @agent_ppo2.py:185][0m |           0.0003 |         194.0570 |         -57.9003 |
[32m[20221214 14:21:07 @agent_ppo2.py:185][0m |          -0.0009 |         193.1503 |         -57.9529 |
[32m[20221214 14:21:07 @agent_ppo2.py:185][0m |           0.0033 |         195.4848 |         -57.6604 |
[32m[20221214 14:21:07 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:21:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 746.22
[32m[20221214 14:21:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.43
[32m[20221214 14:21:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.44
[32m[20221214 14:21:07 @agent_ppo2.py:143][0m Total time:      23.09 min
[32m[20221214 14:21:07 @agent_ppo2.py:145][0m 2113536 total steps have happened
[32m[20221214 14:21:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1032 --------------------------#
[32m[20221214 14:21:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:08 @agent_ppo2.py:185][0m |           0.0063 |         186.6774 |         -59.3560 |
[32m[20221214 14:21:08 @agent_ppo2.py:185][0m |          -0.0009 |         169.6636 |         -58.9020 |
[32m[20221214 14:21:08 @agent_ppo2.py:185][0m |          -0.0033 |         166.0866 |         -58.8611 |
[32m[20221214 14:21:08 @agent_ppo2.py:185][0m |           0.0003 |         165.4805 |         -59.1580 |
[32m[20221214 14:21:08 @agent_ppo2.py:185][0m |          -0.0027 |         164.1758 |         -59.1104 |
[32m[20221214 14:21:08 @agent_ppo2.py:185][0m |          -0.0035 |         162.8680 |         -58.9645 |
[32m[20221214 14:21:08 @agent_ppo2.py:185][0m |           0.0032 |         166.3431 |         -59.1724 |
[32m[20221214 14:21:08 @agent_ppo2.py:185][0m |          -0.0048 |         162.6562 |         -58.8629 |
[32m[20221214 14:21:09 @agent_ppo2.py:185][0m |          -0.0031 |         162.9596 |         -58.9642 |
[32m[20221214 14:21:09 @agent_ppo2.py:185][0m |          -0.0011 |         161.6328 |         -58.8584 |
[32m[20221214 14:21:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:21:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 751.03
[32m[20221214 14:21:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 753.98
[32m[20221214 14:21:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 755.56
[32m[20221214 14:21:09 @agent_ppo2.py:143][0m Total time:      23.11 min
[32m[20221214 14:21:09 @agent_ppo2.py:145][0m 2115584 total steps have happened
[32m[20221214 14:21:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1033 --------------------------#
[32m[20221214 14:21:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:09 @agent_ppo2.py:185][0m |           0.0002 |         207.1828 |         -59.0538 |
[32m[20221214 14:21:09 @agent_ppo2.py:185][0m |           0.0031 |         207.9545 |         -59.1123 |
[32m[20221214 14:21:09 @agent_ppo2.py:185][0m |           0.0019 |         203.8357 |         -59.2018 |
[32m[20221214 14:21:09 @agent_ppo2.py:185][0m |          -0.0031 |         202.8741 |         -59.1261 |
[32m[20221214 14:21:09 @agent_ppo2.py:185][0m |          -0.0013 |         202.2766 |         -58.9791 |
[32m[20221214 14:21:10 @agent_ppo2.py:185][0m |          -0.0007 |         202.8733 |         -59.2206 |
[32m[20221214 14:21:10 @agent_ppo2.py:185][0m |          -0.0011 |         202.4017 |         -59.3663 |
[32m[20221214 14:21:10 @agent_ppo2.py:185][0m |           0.0019 |         203.4089 |         -59.2206 |
[32m[20221214 14:21:10 @agent_ppo2.py:185][0m |           0.0005 |         201.5564 |         -59.2542 |
[32m[20221214 14:21:10 @agent_ppo2.py:185][0m |          -0.0018 |         201.2773 |         -59.2861 |
[32m[20221214 14:21:10 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:21:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.57
[32m[20221214 14:21:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 749.17
[32m[20221214 14:21:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 749.76
[32m[20221214 14:21:10 @agent_ppo2.py:143][0m Total time:      23.14 min
[32m[20221214 14:21:10 @agent_ppo2.py:145][0m 2117632 total steps have happened
[32m[20221214 14:21:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1034 --------------------------#
[32m[20221214 14:21:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:11 @agent_ppo2.py:185][0m |          -0.0045 |         179.3252 |         -58.4988 |
[32m[20221214 14:21:11 @agent_ppo2.py:185][0m |          -0.0028 |         175.1134 |         -58.3431 |
[32m[20221214 14:21:11 @agent_ppo2.py:185][0m |          -0.0021 |         172.2030 |         -58.4150 |
[32m[20221214 14:21:11 @agent_ppo2.py:185][0m |          -0.0030 |         170.5203 |         -58.2107 |
[32m[20221214 14:21:11 @agent_ppo2.py:185][0m |          -0.0024 |         170.1324 |         -58.2545 |
[32m[20221214 14:21:11 @agent_ppo2.py:185][0m |          -0.0023 |         168.9023 |         -58.3028 |
[32m[20221214 14:21:11 @agent_ppo2.py:185][0m |          -0.0029 |         168.1112 |         -58.4130 |
[32m[20221214 14:21:11 @agent_ppo2.py:185][0m |          -0.0009 |         166.1467 |         -58.3002 |
[32m[20221214 14:21:11 @agent_ppo2.py:185][0m |          -0.0034 |         166.2616 |         -58.1297 |
[32m[20221214 14:21:11 @agent_ppo2.py:185][0m |          -0.0028 |         166.3508 |         -58.4107 |
[32m[20221214 14:21:11 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 744.93
[32m[20221214 14:21:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.47
[32m[20221214 14:21:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 744.57
[32m[20221214 14:21:12 @agent_ppo2.py:143][0m Total time:      23.16 min
[32m[20221214 14:21:12 @agent_ppo2.py:145][0m 2119680 total steps have happened
[32m[20221214 14:21:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1035 --------------------------#
[32m[20221214 14:21:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:12 @agent_ppo2.py:185][0m |          -0.0000 |         216.6427 |         -57.9782 |
[32m[20221214 14:21:12 @agent_ppo2.py:185][0m |          -0.0040 |         213.7878 |         -57.7916 |
[32m[20221214 14:21:12 @agent_ppo2.py:185][0m |          -0.0030 |         212.8053 |         -58.0839 |
[32m[20221214 14:21:12 @agent_ppo2.py:185][0m |          -0.0042 |         212.3201 |         -57.8917 |
[32m[20221214 14:21:12 @agent_ppo2.py:185][0m |          -0.0037 |         211.8451 |         -57.9815 |
[32m[20221214 14:21:12 @agent_ppo2.py:185][0m |          -0.0042 |         211.5247 |         -57.9009 |
[32m[20221214 14:21:12 @agent_ppo2.py:185][0m |          -0.0040 |         211.8714 |         -58.1017 |
[32m[20221214 14:21:13 @agent_ppo2.py:185][0m |          -0.0050 |         211.9993 |         -57.9194 |
[32m[20221214 14:21:13 @agent_ppo2.py:185][0m |          -0.0004 |         211.8342 |         -57.9442 |
[32m[20221214 14:21:13 @agent_ppo2.py:185][0m |          -0.0045 |         210.0974 |         -58.0167 |
[32m[20221214 14:21:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 736.35
[32m[20221214 14:21:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 741.47
[32m[20221214 14:21:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 738.93
[32m[20221214 14:21:13 @agent_ppo2.py:143][0m Total time:      23.18 min
[32m[20221214 14:21:13 @agent_ppo2.py:145][0m 2121728 total steps have happened
[32m[20221214 14:21:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1036 --------------------------#
[32m[20221214 14:21:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:13 @agent_ppo2.py:185][0m |           0.0017 |         221.7052 |         -58.1557 |
[32m[20221214 14:21:13 @agent_ppo2.py:185][0m |          -0.0006 |         216.8094 |         -57.9113 |
[32m[20221214 14:21:13 @agent_ppo2.py:185][0m |          -0.0032 |         215.6814 |         -58.2787 |
[32m[20221214 14:21:14 @agent_ppo2.py:185][0m |          -0.0014 |         214.7965 |         -58.2081 |
[32m[20221214 14:21:14 @agent_ppo2.py:185][0m |          -0.0004 |         213.5528 |         -58.1954 |
[32m[20221214 14:21:14 @agent_ppo2.py:185][0m |          -0.0018 |         213.6621 |         -58.4197 |
[32m[20221214 14:21:14 @agent_ppo2.py:185][0m |          -0.0018 |         213.3938 |         -58.3301 |
[32m[20221214 14:21:14 @agent_ppo2.py:185][0m |           0.0005 |         212.5152 |         -58.6642 |
[32m[20221214 14:21:14 @agent_ppo2.py:185][0m |          -0.0018 |         212.1632 |         -58.6289 |
[32m[20221214 14:21:14 @agent_ppo2.py:185][0m |          -0.0033 |         212.1362 |         -58.6682 |
[32m[20221214 14:21:14 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:21:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.01
[32m[20221214 14:21:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 738.60
[32m[20221214 14:21:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 748.29
[32m[20221214 14:21:14 @agent_ppo2.py:143][0m Total time:      23.20 min
[32m[20221214 14:21:14 @agent_ppo2.py:145][0m 2123776 total steps have happened
[32m[20221214 14:21:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1037 --------------------------#
[32m[20221214 14:21:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:15 @agent_ppo2.py:185][0m |          -0.0032 |         192.8781 |         -58.4443 |
[32m[20221214 14:21:15 @agent_ppo2.py:185][0m |          -0.0035 |         191.1300 |         -58.4191 |
[32m[20221214 14:21:15 @agent_ppo2.py:185][0m |          -0.0023 |         190.9473 |         -58.5904 |
[32m[20221214 14:21:15 @agent_ppo2.py:185][0m |           0.0096 |         201.2524 |         -58.5091 |
[32m[20221214 14:21:15 @agent_ppo2.py:185][0m |          -0.0035 |         189.9898 |         -58.4354 |
[32m[20221214 14:21:15 @agent_ppo2.py:185][0m |           0.0003 |         188.3135 |         -58.7503 |
[32m[20221214 14:21:15 @agent_ppo2.py:185][0m |          -0.0019 |         188.2086 |         -58.6435 |
[32m[20221214 14:21:15 @agent_ppo2.py:185][0m |          -0.0017 |         187.5560 |         -58.6388 |
[32m[20221214 14:21:15 @agent_ppo2.py:185][0m |          -0.0033 |         187.3935 |         -58.6541 |
[32m[20221214 14:21:15 @agent_ppo2.py:185][0m |          -0.0014 |         187.5317 |         -58.6736 |
[32m[20221214 14:21:15 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:21:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 740.64
[32m[20221214 14:21:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 746.65
[32m[20221214 14:21:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 751.31
[32m[20221214 14:21:16 @agent_ppo2.py:143][0m Total time:      23.23 min
[32m[20221214 14:21:16 @agent_ppo2.py:145][0m 2125824 total steps have happened
[32m[20221214 14:21:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1038 --------------------------#
[32m[20221214 14:21:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:16 @agent_ppo2.py:185][0m |           0.0023 |         148.1203 |         -60.2079 |
[32m[20221214 14:21:16 @agent_ppo2.py:185][0m |           0.0046 |         147.1124 |         -60.2156 |
[32m[20221214 14:21:16 @agent_ppo2.py:185][0m |          -0.0023 |         144.9071 |         -60.0745 |
[32m[20221214 14:21:16 @agent_ppo2.py:185][0m |          -0.0001 |         143.0126 |         -60.1578 |
[32m[20221214 14:21:16 @agent_ppo2.py:185][0m |          -0.0005 |         142.3103 |         -60.2202 |
[32m[20221214 14:21:16 @agent_ppo2.py:185][0m |          -0.0015 |         141.5518 |         -60.1946 |
[32m[20221214 14:21:17 @agent_ppo2.py:185][0m |           0.0085 |         153.6331 |         -60.1789 |
[32m[20221214 14:21:17 @agent_ppo2.py:185][0m |           0.0089 |         149.0107 |         -60.0651 |
[32m[20221214 14:21:17 @agent_ppo2.py:185][0m |          -0.0024 |         140.8473 |         -60.3418 |
[32m[20221214 14:21:17 @agent_ppo2.py:185][0m |          -0.0010 |         140.8254 |         -60.2825 |
[32m[20221214 14:21:17 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.82
[32m[20221214 14:21:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 749.99
[32m[20221214 14:21:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 745.34
[32m[20221214 14:21:17 @agent_ppo2.py:143][0m Total time:      23.25 min
[32m[20221214 14:21:17 @agent_ppo2.py:145][0m 2127872 total steps have happened
[32m[20221214 14:21:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1039 --------------------------#
[32m[20221214 14:21:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:21:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:17 @agent_ppo2.py:185][0m |           0.0118 |         158.0934 |         -59.6728 |
[32m[20221214 14:21:17 @agent_ppo2.py:185][0m |          -0.0025 |         142.5713 |         -59.4932 |
[32m[20221214 14:21:17 @agent_ppo2.py:185][0m |          -0.0019 |         140.6861 |         -59.4635 |
[32m[20221214 14:21:18 @agent_ppo2.py:185][0m |           0.0042 |         142.0090 |         -59.7586 |
[32m[20221214 14:21:18 @agent_ppo2.py:185][0m |          -0.0010 |         140.4695 |         -59.4429 |
[32m[20221214 14:21:18 @agent_ppo2.py:185][0m |           0.0012 |         140.2415 |         -59.3089 |
[32m[20221214 14:21:18 @agent_ppo2.py:185][0m |          -0.0039 |         140.3635 |         -59.4942 |
[32m[20221214 14:21:18 @agent_ppo2.py:185][0m |          -0.0018 |         140.2151 |         -59.3845 |
[32m[20221214 14:21:18 @agent_ppo2.py:185][0m |          -0.0005 |         139.9568 |         -59.5651 |
[32m[20221214 14:21:18 @agent_ppo2.py:185][0m |          -0.0013 |         139.6732 |         -59.7673 |
[32m[20221214 14:21:18 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:21:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.48
[32m[20221214 14:21:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.61
[32m[20221214 14:21:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 741.70
[32m[20221214 14:21:18 @agent_ppo2.py:143][0m Total time:      23.27 min
[32m[20221214 14:21:18 @agent_ppo2.py:145][0m 2129920 total steps have happened
[32m[20221214 14:21:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1040 --------------------------#
[32m[20221214 14:21:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:21:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:19 @agent_ppo2.py:185][0m |          -0.0005 |         180.6991 |         -60.5249 |
[32m[20221214 14:21:19 @agent_ppo2.py:185][0m |          -0.0014 |         177.3260 |         -60.4809 |
[32m[20221214 14:21:19 @agent_ppo2.py:185][0m |          -0.0027 |         176.9410 |         -60.4733 |
[32m[20221214 14:21:19 @agent_ppo2.py:185][0m |          -0.0015 |         175.9976 |         -60.3918 |
[32m[20221214 14:21:19 @agent_ppo2.py:185][0m |           0.0037 |         176.7278 |         -60.5438 |
[32m[20221214 14:21:19 @agent_ppo2.py:185][0m |           0.0117 |         194.6266 |         -60.7371 |
[32m[20221214 14:21:19 @agent_ppo2.py:185][0m |          -0.0015 |         176.9181 |         -60.4158 |
[32m[20221214 14:21:19 @agent_ppo2.py:185][0m |          -0.0003 |         174.3601 |         -60.8202 |
[32m[20221214 14:21:19 @agent_ppo2.py:185][0m |          -0.0027 |         174.6896 |         -60.7621 |
[32m[20221214 14:21:19 @agent_ppo2.py:185][0m |          -0.0016 |         173.4586 |         -60.8633 |
[32m[20221214 14:21:19 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:21:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.74
[32m[20221214 14:21:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 740.53
[32m[20221214 14:21:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.47
[32m[20221214 14:21:19 @agent_ppo2.py:143][0m Total time:      23.29 min
[32m[20221214 14:21:19 @agent_ppo2.py:145][0m 2131968 total steps have happened
[32m[20221214 14:21:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1041 --------------------------#
[32m[20221214 14:21:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:21:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:20 @agent_ppo2.py:185][0m |          -0.0001 |         175.9042 |         -60.5747 |
[32m[20221214 14:21:20 @agent_ppo2.py:185][0m |          -0.0033 |         171.3299 |         -60.4572 |
[32m[20221214 14:21:20 @agent_ppo2.py:185][0m |          -0.0019 |         170.0577 |         -60.3868 |
[32m[20221214 14:21:20 @agent_ppo2.py:185][0m |          -0.0026 |         169.3188 |         -60.7666 |
[32m[20221214 14:21:20 @agent_ppo2.py:185][0m |          -0.0004 |         170.2786 |         -60.6443 |
[32m[20221214 14:21:20 @agent_ppo2.py:185][0m |          -0.0041 |         168.7254 |         -60.7251 |
[32m[20221214 14:21:20 @agent_ppo2.py:185][0m |          -0.0030 |         168.8161 |         -60.5638 |
[32m[20221214 14:21:21 @agent_ppo2.py:185][0m |          -0.0040 |         168.8394 |         -60.5665 |
[32m[20221214 14:21:21 @agent_ppo2.py:185][0m |          -0.0035 |         170.1232 |         -60.8300 |
[32m[20221214 14:21:21 @agent_ppo2.py:185][0m |           0.0077 |         188.6678 |         -60.8037 |
[32m[20221214 14:21:21 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:21:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 732.35
[32m[20221214 14:21:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 746.21
[32m[20221214 14:21:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.90
[32m[20221214 14:21:21 @agent_ppo2.py:143][0m Total time:      23.31 min
[32m[20221214 14:21:21 @agent_ppo2.py:145][0m 2134016 total steps have happened
[32m[20221214 14:21:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1042 --------------------------#
[32m[20221214 14:21:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:21 @agent_ppo2.py:185][0m |          -0.0021 |         221.0478 |         -61.6172 |
[32m[20221214 14:21:21 @agent_ppo2.py:185][0m |          -0.0039 |         217.0628 |         -61.6353 |
[32m[20221214 14:21:21 @agent_ppo2.py:185][0m |          -0.0032 |         216.1681 |         -61.6876 |
[32m[20221214 14:21:22 @agent_ppo2.py:185][0m |          -0.0043 |         215.7632 |         -61.7465 |
[32m[20221214 14:21:22 @agent_ppo2.py:185][0m |          -0.0033 |         214.3375 |         -61.6301 |
[32m[20221214 14:21:22 @agent_ppo2.py:185][0m |          -0.0035 |         213.8483 |         -61.5404 |
[32m[20221214 14:21:22 @agent_ppo2.py:185][0m |          -0.0040 |         212.1235 |         -61.5852 |
[32m[20221214 14:21:22 @agent_ppo2.py:185][0m |          -0.0036 |         211.3915 |         -61.6167 |
[32m[20221214 14:21:22 @agent_ppo2.py:185][0m |           0.0011 |         214.0729 |         -61.6299 |
[32m[20221214 14:21:22 @agent_ppo2.py:185][0m |          -0.0048 |         211.2412 |         -61.5082 |
[32m[20221214 14:21:22 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:21:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 723.25
[32m[20221214 14:21:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 725.26
[32m[20221214 14:21:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 720.00
[32m[20221214 14:21:22 @agent_ppo2.py:143][0m Total time:      23.34 min
[32m[20221214 14:21:22 @agent_ppo2.py:145][0m 2136064 total steps have happened
[32m[20221214 14:21:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1043 --------------------------#
[32m[20221214 14:21:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:23 @agent_ppo2.py:185][0m |          -0.0011 |         242.3642 |         -60.6004 |
[32m[20221214 14:21:23 @agent_ppo2.py:185][0m |          -0.0001 |         239.9384 |         -60.4041 |
[32m[20221214 14:21:23 @agent_ppo2.py:185][0m |           0.0001 |         239.1957 |         -60.6301 |
[32m[20221214 14:21:23 @agent_ppo2.py:185][0m |           0.0048 |         244.4143 |         -60.5898 |
[32m[20221214 14:21:23 @agent_ppo2.py:185][0m |          -0.0011 |         238.0956 |         -60.8892 |
[32m[20221214 14:21:23 @agent_ppo2.py:185][0m |          -0.0003 |         237.7656 |         -60.6855 |
[32m[20221214 14:21:23 @agent_ppo2.py:185][0m |          -0.0022 |         237.3559 |         -60.8635 |
[32m[20221214 14:21:23 @agent_ppo2.py:185][0m |          -0.0015 |         236.8160 |         -60.9971 |
[32m[20221214 14:21:23 @agent_ppo2.py:185][0m |          -0.0002 |         236.7836 |         -61.2625 |
[32m[20221214 14:21:23 @agent_ppo2.py:185][0m |          -0.0032 |         237.1119 |         -61.1048 |
[32m[20221214 14:21:23 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 714.04
[32m[20221214 14:21:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 727.01
[32m[20221214 14:21:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 716.30
[32m[20221214 14:21:24 @agent_ppo2.py:143][0m Total time:      23.36 min
[32m[20221214 14:21:24 @agent_ppo2.py:145][0m 2138112 total steps have happened
[32m[20221214 14:21:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1044 --------------------------#
[32m[20221214 14:21:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:24 @agent_ppo2.py:185][0m |          -0.0024 |         223.1661 |         -63.2430 |
[32m[20221214 14:21:24 @agent_ppo2.py:185][0m |           0.0064 |         230.6820 |         -63.1862 |
[32m[20221214 14:21:24 @agent_ppo2.py:185][0m |          -0.0023 |         214.0640 |         -63.1259 |
[32m[20221214 14:21:24 @agent_ppo2.py:185][0m |          -0.0028 |         213.1705 |         -63.3419 |
[32m[20221214 14:21:24 @agent_ppo2.py:185][0m |          -0.0019 |         212.5138 |         -63.2708 |
[32m[20221214 14:21:25 @agent_ppo2.py:185][0m |          -0.0026 |         212.2063 |         -63.2967 |
[32m[20221214 14:21:25 @agent_ppo2.py:185][0m |          -0.0025 |         211.8476 |         -63.4325 |
[32m[20221214 14:21:25 @agent_ppo2.py:185][0m |           0.0032 |         217.6699 |         -63.3693 |
[32m[20221214 14:21:25 @agent_ppo2.py:185][0m |          -0.0020 |         211.4727 |         -63.1653 |
[32m[20221214 14:21:25 @agent_ppo2.py:185][0m |          -0.0019 |         209.8877 |         -63.3942 |
[32m[20221214 14:21:25 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:21:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 708.95
[32m[20221214 14:21:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 714.27
[32m[20221214 14:21:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 705.95
[32m[20221214 14:21:25 @agent_ppo2.py:143][0m Total time:      23.38 min
[32m[20221214 14:21:25 @agent_ppo2.py:145][0m 2140160 total steps have happened
[32m[20221214 14:21:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1045 --------------------------#
[32m[20221214 14:21:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:25 @agent_ppo2.py:185][0m |           0.0126 |         268.7447 |         -61.8670 |
[32m[20221214 14:21:25 @agent_ppo2.py:185][0m |          -0.0012 |         234.1805 |         -61.8614 |
[32m[20221214 14:21:26 @agent_ppo2.py:185][0m |          -0.0019 |         233.1538 |         -61.8067 |
[32m[20221214 14:21:26 @agent_ppo2.py:185][0m |          -0.0009 |         232.9511 |         -61.7093 |
[32m[20221214 14:21:26 @agent_ppo2.py:185][0m |          -0.0001 |         232.0612 |         -61.5315 |
[32m[20221214 14:21:26 @agent_ppo2.py:185][0m |          -0.0035 |         232.5019 |         -61.4625 |
[32m[20221214 14:21:26 @agent_ppo2.py:185][0m |           0.0009 |         232.2723 |         -61.6630 |
[32m[20221214 14:21:26 @agent_ppo2.py:185][0m |          -0.0012 |         232.1311 |         -61.5116 |
[32m[20221214 14:21:26 @agent_ppo2.py:185][0m |          -0.0029 |         231.4753 |         -61.4930 |
[32m[20221214 14:21:26 @agent_ppo2.py:185][0m |           0.0029 |         236.0526 |         -61.4362 |
[32m[20221214 14:21:26 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 700.66
[32m[20221214 14:21:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 706.32
[32m[20221214 14:21:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 706.23
[32m[20221214 14:21:26 @agent_ppo2.py:143][0m Total time:      23.41 min
[32m[20221214 14:21:26 @agent_ppo2.py:145][0m 2142208 total steps have happened
[32m[20221214 14:21:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1046 --------------------------#
[32m[20221214 14:21:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:27 @agent_ppo2.py:185][0m |          -0.0015 |         210.6352 |         -60.0646 |
[32m[20221214 14:21:27 @agent_ppo2.py:185][0m |          -0.0018 |         206.3447 |         -60.0849 |
[32m[20221214 14:21:27 @agent_ppo2.py:185][0m |           0.0017 |         205.5471 |         -59.9403 |
[32m[20221214 14:21:27 @agent_ppo2.py:185][0m |          -0.0019 |         204.5051 |         -59.9575 |
[32m[20221214 14:21:27 @agent_ppo2.py:185][0m |           0.0029 |         205.3596 |         -59.8471 |
[32m[20221214 14:21:27 @agent_ppo2.py:185][0m |          -0.0017 |         203.1340 |         -59.8868 |
[32m[20221214 14:21:27 @agent_ppo2.py:185][0m |          -0.0029 |         203.0713 |         -59.7293 |
[32m[20221214 14:21:27 @agent_ppo2.py:185][0m |           0.0176 |         234.4900 |         -59.6855 |
[32m[20221214 14:21:27 @agent_ppo2.py:185][0m |           0.0012 |         205.0342 |         -59.5525 |
[32m[20221214 14:21:28 @agent_ppo2.py:185][0m |          -0.0031 |         202.8626 |         -59.5078 |
[32m[20221214 14:21:28 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 706.14
[32m[20221214 14:21:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 716.07
[32m[20221214 14:21:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 702.78
[32m[20221214 14:21:28 @agent_ppo2.py:143][0m Total time:      23.43 min
[32m[20221214 14:21:28 @agent_ppo2.py:145][0m 2144256 total steps have happened
[32m[20221214 14:21:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1047 --------------------------#
[32m[20221214 14:21:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:28 @agent_ppo2.py:185][0m |          -0.0018 |         214.9689 |         -60.9306 |
[32m[20221214 14:21:28 @agent_ppo2.py:185][0m |          -0.0004 |         209.0593 |         -61.0203 |
[32m[20221214 14:21:28 @agent_ppo2.py:185][0m |          -0.0053 |         207.7273 |         -60.9055 |
[32m[20221214 14:21:28 @agent_ppo2.py:185][0m |          -0.0036 |         206.7735 |         -61.0765 |
[32m[20221214 14:21:29 @agent_ppo2.py:185][0m |          -0.0019 |         206.8308 |         -61.1999 |
[32m[20221214 14:21:29 @agent_ppo2.py:185][0m |          -0.0030 |         205.9614 |         -61.1780 |
[32m[20221214 14:21:29 @agent_ppo2.py:185][0m |          -0.0030 |         204.8858 |         -61.2178 |
[32m[20221214 14:21:29 @agent_ppo2.py:185][0m |          -0.0027 |         204.5087 |         -61.3481 |
[32m[20221214 14:21:29 @agent_ppo2.py:185][0m |          -0.0017 |         204.4382 |         -61.3347 |
[32m[20221214 14:21:29 @agent_ppo2.py:185][0m |          -0.0039 |         204.0869 |         -61.2235 |
[32m[20221214 14:21:29 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 694.61
[32m[20221214 14:21:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 703.24
[32m[20221214 14:21:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 693.11
[32m[20221214 14:21:29 @agent_ppo2.py:143][0m Total time:      23.45 min
[32m[20221214 14:21:29 @agent_ppo2.py:145][0m 2146304 total steps have happened
[32m[20221214 14:21:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1048 --------------------------#
[32m[20221214 14:21:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:29 @agent_ppo2.py:185][0m |          -0.0011 |         219.1784 |         -61.4485 |
[32m[20221214 14:21:30 @agent_ppo2.py:185][0m |          -0.0037 |         214.0211 |         -61.3872 |
[32m[20221214 14:21:30 @agent_ppo2.py:185][0m |          -0.0065 |         212.1828 |         -61.4723 |
[32m[20221214 14:21:30 @agent_ppo2.py:185][0m |          -0.0005 |         216.8947 |         -61.5741 |
[32m[20221214 14:21:30 @agent_ppo2.py:185][0m |          -0.0054 |         211.3427 |         -61.4359 |
[32m[20221214 14:21:30 @agent_ppo2.py:185][0m |          -0.0064 |         211.1764 |         -61.5963 |
[32m[20221214 14:21:30 @agent_ppo2.py:185][0m |          -0.0055 |         210.3593 |         -61.4787 |
[32m[20221214 14:21:30 @agent_ppo2.py:185][0m |          -0.0064 |         210.6645 |         -61.6987 |
[32m[20221214 14:21:30 @agent_ppo2.py:185][0m |          -0.0020 |         210.6595 |         -61.7934 |
[32m[20221214 14:21:30 @agent_ppo2.py:185][0m |          -0.0061 |         209.5919 |         -61.6770 |
[32m[20221214 14:21:30 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 691.09
[32m[20221214 14:21:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 709.29
[32m[20221214 14:21:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 694.30
[32m[20221214 14:21:31 @agent_ppo2.py:143][0m Total time:      23.47 min
[32m[20221214 14:21:31 @agent_ppo2.py:145][0m 2148352 total steps have happened
[32m[20221214 14:21:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1049 --------------------------#
[32m[20221214 14:21:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:31 @agent_ppo2.py:185][0m |           0.0029 |         227.9292 |         -61.7577 |
[32m[20221214 14:21:31 @agent_ppo2.py:185][0m |          -0.0023 |         215.4543 |         -61.6718 |
[32m[20221214 14:21:31 @agent_ppo2.py:185][0m |           0.0014 |         213.6761 |         -61.6207 |
[32m[20221214 14:21:31 @agent_ppo2.py:185][0m |          -0.0051 |         208.5176 |         -61.5284 |
[32m[20221214 14:21:31 @agent_ppo2.py:185][0m |          -0.0066 |         206.8367 |         -61.3845 |
[32m[20221214 14:21:31 @agent_ppo2.py:185][0m |          -0.0046 |         206.8026 |         -61.4807 |
[32m[20221214 14:21:31 @agent_ppo2.py:185][0m |           0.0089 |         224.8045 |         -61.3942 |
[32m[20221214 14:21:32 @agent_ppo2.py:185][0m |          -0.0067 |         205.1652 |         -61.1206 |
[32m[20221214 14:21:32 @agent_ppo2.py:185][0m |           0.0098 |         230.3602 |         -61.2927 |
[32m[20221214 14:21:32 @agent_ppo2.py:185][0m |           0.0065 |         217.9355 |         -61.2824 |
[32m[20221214 14:21:32 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:21:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 678.31
[32m[20221214 14:21:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 688.19
[32m[20221214 14:21:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 676.84
[32m[20221214 14:21:32 @agent_ppo2.py:143][0m Total time:      23.50 min
[32m[20221214 14:21:32 @agent_ppo2.py:145][0m 2150400 total steps have happened
[32m[20221214 14:21:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1050 --------------------------#
[32m[20221214 14:21:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:21:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:32 @agent_ppo2.py:185][0m |          -0.0001 |         220.5454 |         -59.3952 |
[32m[20221214 14:21:32 @agent_ppo2.py:185][0m |          -0.0006 |         215.8115 |         -59.5582 |
[32m[20221214 14:21:32 @agent_ppo2.py:185][0m |          -0.0020 |         214.3935 |         -59.3410 |
[32m[20221214 14:21:32 @agent_ppo2.py:185][0m |           0.0038 |         216.1321 |         -59.2678 |
[32m[20221214 14:21:33 @agent_ppo2.py:185][0m |           0.0061 |         217.6186 |         -59.4190 |
[32m[20221214 14:21:33 @agent_ppo2.py:185][0m |          -0.0021 |         212.2932 |         -59.1947 |
[32m[20221214 14:21:33 @agent_ppo2.py:185][0m |          -0.0019 |         211.6549 |         -59.3686 |
[32m[20221214 14:21:33 @agent_ppo2.py:185][0m |           0.0087 |         230.0439 |         -59.2036 |
[32m[20221214 14:21:33 @agent_ppo2.py:185][0m |          -0.0016 |         211.8066 |         -59.2067 |
[32m[20221214 14:21:33 @agent_ppo2.py:185][0m |          -0.0023 |         210.9801 |         -59.2434 |
[32m[20221214 14:21:33 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:21:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 664.84
[32m[20221214 14:21:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 678.18
[32m[20221214 14:21:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 663.59
[32m[20221214 14:21:33 @agent_ppo2.py:143][0m Total time:      23.52 min
[32m[20221214 14:21:33 @agent_ppo2.py:145][0m 2152448 total steps have happened
[32m[20221214 14:21:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1051 --------------------------#
[32m[20221214 14:21:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:21:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:33 @agent_ppo2.py:185][0m |          -0.0010 |         222.6163 |         -60.9684 |
[32m[20221214 14:21:34 @agent_ppo2.py:185][0m |           0.0037 |         220.7994 |         -61.0604 |
[32m[20221214 14:21:34 @agent_ppo2.py:185][0m |           0.0028 |         218.5971 |         -61.0866 |
[32m[20221214 14:21:34 @agent_ppo2.py:185][0m |           0.0088 |         231.3708 |         -61.0361 |
[32m[20221214 14:21:34 @agent_ppo2.py:185][0m |           0.0062 |         224.0679 |         -60.9073 |
[32m[20221214 14:21:34 @agent_ppo2.py:185][0m |           0.0025 |         218.7355 |         -60.8535 |
[32m[20221214 14:21:34 @agent_ppo2.py:185][0m |          -0.0015 |         215.7733 |         -60.7358 |
[32m[20221214 14:21:34 @agent_ppo2.py:185][0m |           0.0010 |         215.5615 |         -60.8925 |
[32m[20221214 14:21:34 @agent_ppo2.py:185][0m |          -0.0012 |         215.0132 |         -60.6289 |
[32m[20221214 14:21:34 @agent_ppo2.py:185][0m |           0.0108 |         224.7522 |         -60.5926 |
[32m[20221214 14:21:34 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:21:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 650.94
[32m[20221214 14:21:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 659.29
[32m[20221214 14:21:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 660.78
[32m[20221214 14:21:34 @agent_ppo2.py:143][0m Total time:      23.54 min
[32m[20221214 14:21:34 @agent_ppo2.py:145][0m 2154496 total steps have happened
[32m[20221214 14:21:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1052 --------------------------#
[32m[20221214 14:21:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:21:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:35 @agent_ppo2.py:185][0m |          -0.0030 |         202.8194 |         -59.5227 |
[32m[20221214 14:21:35 @agent_ppo2.py:185][0m |           0.0084 |         220.3004 |         -59.4783 |
[32m[20221214 14:21:35 @agent_ppo2.py:185][0m |          -0.0042 |         198.8354 |         -59.3724 |
[32m[20221214 14:21:35 @agent_ppo2.py:185][0m |          -0.0052 |         198.5596 |         -59.3511 |
[32m[20221214 14:21:35 @agent_ppo2.py:185][0m |          -0.0044 |         197.3819 |         -59.3425 |
[32m[20221214 14:21:35 @agent_ppo2.py:185][0m |          -0.0042 |         196.8897 |         -59.4133 |
[32m[20221214 14:21:35 @agent_ppo2.py:185][0m |          -0.0021 |         196.9621 |         -59.3485 |
[32m[20221214 14:21:35 @agent_ppo2.py:185][0m |           0.0016 |         198.6367 |         -59.2614 |
[32m[20221214 14:21:35 @agent_ppo2.py:185][0m |           0.0057 |         206.4190 |         -59.3280 |
[32m[20221214 14:21:36 @agent_ppo2.py:185][0m |          -0.0046 |         195.7639 |         -59.3822 |
[32m[20221214 14:21:36 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 659.23
[32m[20221214 14:21:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 665.94
[32m[20221214 14:21:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.71
[32m[20221214 14:21:36 @agent_ppo2.py:143][0m Total time:      23.56 min
[32m[20221214 14:21:36 @agent_ppo2.py:145][0m 2156544 total steps have happened
[32m[20221214 14:21:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1053 --------------------------#
[32m[20221214 14:21:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:36 @agent_ppo2.py:185][0m |           0.0007 |         174.4422 |         -59.2165 |
[32m[20221214 14:21:36 @agent_ppo2.py:185][0m |           0.0026 |         145.3091 |         -59.3317 |
[32m[20221214 14:21:36 @agent_ppo2.py:185][0m |           0.0121 |         163.7087 |         -59.3925 |
[32m[20221214 14:21:36 @agent_ppo2.py:185][0m |          -0.0024 |         138.4157 |         -59.4551 |
[32m[20221214 14:21:36 @agent_ppo2.py:185][0m |          -0.0025 |         137.1820 |         -59.5057 |
[32m[20221214 14:21:37 @agent_ppo2.py:185][0m |          -0.0013 |         136.6514 |         -59.6067 |
[32m[20221214 14:21:37 @agent_ppo2.py:185][0m |          -0.0030 |         136.3826 |         -59.7212 |
[32m[20221214 14:21:37 @agent_ppo2.py:185][0m |          -0.0027 |         135.9183 |         -59.6897 |
[32m[20221214 14:21:37 @agent_ppo2.py:185][0m |          -0.0044 |         135.3800 |         -59.6364 |
[32m[20221214 14:21:37 @agent_ppo2.py:185][0m |          -0.0053 |         135.5595 |         -59.7242 |
[32m[20221214 14:21:37 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:21:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 615.59
[32m[20221214 14:21:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 636.66
[32m[20221214 14:21:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.58
[32m[20221214 14:21:37 @agent_ppo2.py:143][0m Total time:      23.58 min
[32m[20221214 14:21:37 @agent_ppo2.py:145][0m 2158592 total steps have happened
[32m[20221214 14:21:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1054 --------------------------#
[32m[20221214 14:21:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:37 @agent_ppo2.py:185][0m |          -0.0034 |         143.6656 |         -60.2345 |
[32m[20221214 14:21:38 @agent_ppo2.py:185][0m |           0.0034 |          89.5895 |         -60.3869 |
[32m[20221214 14:21:38 @agent_ppo2.py:185][0m |           0.0027 |          80.5162 |         -60.5541 |
[32m[20221214 14:21:38 @agent_ppo2.py:185][0m |          -0.0043 |          76.5281 |         -60.6067 |
[32m[20221214 14:21:38 @agent_ppo2.py:185][0m |          -0.0038 |          73.8437 |         -60.5844 |
[32m[20221214 14:21:38 @agent_ppo2.py:185][0m |          -0.0032 |          72.4502 |         -60.7158 |
[32m[20221214 14:21:38 @agent_ppo2.py:185][0m |          -0.0015 |          71.1515 |         -60.7853 |
[32m[20221214 14:21:38 @agent_ppo2.py:185][0m |          -0.0007 |          69.6381 |         -61.0028 |
[32m[20221214 14:21:38 @agent_ppo2.py:185][0m |           0.0021 |          68.6376 |         -61.0308 |
[32m[20221214 14:21:38 @agent_ppo2.py:185][0m |           0.0030 |          67.4116 |         -60.9140 |
[32m[20221214 14:21:38 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:21:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.61
[32m[20221214 14:21:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 618.72
[32m[20221214 14:21:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 621.22
[32m[20221214 14:21:39 @agent_ppo2.py:143][0m Total time:      23.61 min
[32m[20221214 14:21:39 @agent_ppo2.py:145][0m 2160640 total steps have happened
[32m[20221214 14:21:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1055 --------------------------#
[32m[20221214 14:21:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:39 @agent_ppo2.py:185][0m |          -0.0012 |         217.8310 |         -60.9204 |
[32m[20221214 14:21:39 @agent_ppo2.py:185][0m |           0.0078 |         175.4688 |         -60.7219 |
[32m[20221214 14:21:39 @agent_ppo2.py:185][0m |          -0.0034 |         159.9700 |         -60.6169 |
[32m[20221214 14:21:39 @agent_ppo2.py:185][0m |          -0.0001 |         157.3052 |         -60.5085 |
[32m[20221214 14:21:39 @agent_ppo2.py:185][0m |          -0.0026 |         154.1864 |         -60.5423 |
[32m[20221214 14:21:39 @agent_ppo2.py:185][0m |          -0.0065 |         152.3892 |         -60.4216 |
[32m[20221214 14:21:39 @agent_ppo2.py:185][0m |          -0.0054 |         150.7153 |         -60.3168 |
[32m[20221214 14:21:40 @agent_ppo2.py:185][0m |          -0.0053 |         149.9058 |         -60.1405 |
[32m[20221214 14:21:40 @agent_ppo2.py:185][0m |           0.0086 |         169.1235 |         -60.2281 |
[32m[20221214 14:21:40 @agent_ppo2.py:185][0m |           0.0012 |         148.5047 |         -60.0771 |
[32m[20221214 14:21:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 619.72
[32m[20221214 14:21:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 632.67
[32m[20221214 14:21:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 598.91
[32m[20221214 14:21:40 @agent_ppo2.py:143][0m Total time:      23.63 min
[32m[20221214 14:21:40 @agent_ppo2.py:145][0m 2162688 total steps have happened
[32m[20221214 14:21:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1056 --------------------------#
[32m[20221214 14:21:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:40 @agent_ppo2.py:185][0m |          -0.0045 |          95.0742 |         -60.1062 |
[32m[20221214 14:21:40 @agent_ppo2.py:185][0m |          -0.0048 |          56.2280 |         -60.1521 |
[32m[20221214 14:21:40 @agent_ppo2.py:185][0m |          -0.0012 |          49.3729 |         -60.0177 |
[32m[20221214 14:21:40 @agent_ppo2.py:185][0m |           0.0034 |          45.0466 |         -59.8395 |
[32m[20221214 14:21:41 @agent_ppo2.py:185][0m |          -0.0031 |          42.1059 |         -59.9526 |
[32m[20221214 14:21:41 @agent_ppo2.py:185][0m |          -0.0021 |          40.9225 |         -59.9039 |
[32m[20221214 14:21:41 @agent_ppo2.py:185][0m |           0.0039 |          42.4911 |         -59.9385 |
[32m[20221214 14:21:41 @agent_ppo2.py:185][0m |          -0.0043 |          37.2949 |         -59.8251 |
[32m[20221214 14:21:41 @agent_ppo2.py:185][0m |          -0.0022 |          35.9947 |         -59.7748 |
[32m[20221214 14:21:41 @agent_ppo2.py:185][0m |          -0.0024 |          35.2651 |         -59.7553 |
[32m[20221214 14:21:41 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 595.78
[32m[20221214 14:21:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 612.42
[32m[20221214 14:21:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 630.30
[32m[20221214 14:21:41 @agent_ppo2.py:143][0m Total time:      23.65 min
[32m[20221214 14:21:41 @agent_ppo2.py:145][0m 2164736 total steps have happened
[32m[20221214 14:21:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1057 --------------------------#
[32m[20221214 14:21:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:42 @agent_ppo2.py:185][0m |          -0.0009 |         168.6316 |         -60.9025 |
[32m[20221214 14:21:42 @agent_ppo2.py:185][0m |           0.0032 |         149.3619 |         -61.0473 |
[32m[20221214 14:21:42 @agent_ppo2.py:185][0m |          -0.0007 |         143.2985 |         -61.0114 |
[32m[20221214 14:21:42 @agent_ppo2.py:185][0m |           0.0002 |         142.0536 |         -61.0101 |
[32m[20221214 14:21:42 @agent_ppo2.py:185][0m |          -0.0032 |         138.7109 |         -61.0230 |
[32m[20221214 14:21:42 @agent_ppo2.py:185][0m |          -0.0054 |         137.0300 |         -60.8924 |
[32m[20221214 14:21:42 @agent_ppo2.py:185][0m |          -0.0028 |         136.1622 |         -60.7958 |
[32m[20221214 14:21:42 @agent_ppo2.py:185][0m |          -0.0028 |         135.0056 |         -60.9164 |
[32m[20221214 14:21:42 @agent_ppo2.py:185][0m |          -0.0035 |         134.8308 |         -60.8316 |
[32m[20221214 14:21:42 @agent_ppo2.py:185][0m |          -0.0036 |         133.5312 |         -60.8495 |
[32m[20221214 14:21:42 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 628.25
[32m[20221214 14:21:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 644.87
[32m[20221214 14:21:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 634.15
[32m[20221214 14:21:43 @agent_ppo2.py:143][0m Total time:      23.68 min
[32m[20221214 14:21:43 @agent_ppo2.py:145][0m 2166784 total steps have happened
[32m[20221214 14:21:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1058 --------------------------#
[32m[20221214 14:21:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:43 @agent_ppo2.py:185][0m |          -0.0003 |         149.6307 |         -60.3932 |
[32m[20221214 14:21:43 @agent_ppo2.py:185][0m |           0.0011 |         143.7054 |         -60.2251 |
[32m[20221214 14:21:43 @agent_ppo2.py:185][0m |          -0.0030 |         142.5493 |         -60.1519 |
[32m[20221214 14:21:43 @agent_ppo2.py:185][0m |          -0.0035 |         141.3804 |         -60.0854 |
[32m[20221214 14:21:43 @agent_ppo2.py:185][0m |          -0.0026 |         140.9601 |         -59.9878 |
[32m[20221214 14:21:43 @agent_ppo2.py:185][0m |          -0.0026 |         139.9565 |         -59.9732 |
[32m[20221214 14:21:44 @agent_ppo2.py:185][0m |          -0.0042 |         140.2259 |         -59.9121 |
[32m[20221214 14:21:44 @agent_ppo2.py:185][0m |          -0.0016 |         139.5423 |         -59.7495 |
[32m[20221214 14:21:44 @agent_ppo2.py:185][0m |          -0.0023 |         138.9442 |         -59.8650 |
[32m[20221214 14:21:44 @agent_ppo2.py:185][0m |           0.0136 |         151.2149 |         -59.6016 |
[32m[20221214 14:21:44 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 624.93
[32m[20221214 14:21:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 636.91
[32m[20221214 14:21:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 618.22
[32m[20221214 14:21:44 @agent_ppo2.py:143][0m Total time:      23.70 min
[32m[20221214 14:21:44 @agent_ppo2.py:145][0m 2168832 total steps have happened
[32m[20221214 14:21:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1059 --------------------------#
[32m[20221214 14:21:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:44 @agent_ppo2.py:185][0m |          -0.0017 |         132.0186 |         -58.4998 |
[32m[20221214 14:21:44 @agent_ppo2.py:185][0m |          -0.0076 |         107.1304 |         -58.5039 |
[32m[20221214 14:21:44 @agent_ppo2.py:185][0m |           0.0017 |         104.5763 |         -58.1653 |
[32m[20221214 14:21:45 @agent_ppo2.py:185][0m |           0.0010 |          97.9353 |         -58.0799 |
[32m[20221214 14:21:45 @agent_ppo2.py:185][0m |          -0.0033 |          95.5429 |         -57.8555 |
[32m[20221214 14:21:45 @agent_ppo2.py:185][0m |          -0.0072 |          93.5310 |         -57.8671 |
[32m[20221214 14:21:45 @agent_ppo2.py:185][0m |          -0.0033 |          92.3266 |         -57.7480 |
[32m[20221214 14:21:45 @agent_ppo2.py:185][0m |          -0.0088 |          91.1402 |         -57.6184 |
[32m[20221214 14:21:45 @agent_ppo2.py:185][0m |          -0.0057 |          90.4830 |         -57.6314 |
[32m[20221214 14:21:45 @agent_ppo2.py:185][0m |          -0.0055 |          89.8042 |         -57.4833 |
[32m[20221214 14:21:45 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:21:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 592.85
[32m[20221214 14:21:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 610.98
[32m[20221214 14:21:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 645.50
[32m[20221214 14:21:45 @agent_ppo2.py:143][0m Total time:      23.72 min
[32m[20221214 14:21:45 @agent_ppo2.py:145][0m 2170880 total steps have happened
[32m[20221214 14:21:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1060 --------------------------#
[32m[20221214 14:21:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:21:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:46 @agent_ppo2.py:185][0m |          -0.0024 |         168.6811 |         -57.9455 |
[32m[20221214 14:21:46 @agent_ppo2.py:185][0m |          -0.0009 |         164.2551 |         -57.8731 |
[32m[20221214 14:21:46 @agent_ppo2.py:185][0m |          -0.0046 |         163.5789 |         -57.8159 |
[32m[20221214 14:21:46 @agent_ppo2.py:185][0m |          -0.0018 |         163.7204 |         -57.8020 |
[32m[20221214 14:21:46 @agent_ppo2.py:185][0m |          -0.0055 |         163.1696 |         -57.7642 |
[32m[20221214 14:21:46 @agent_ppo2.py:185][0m |           0.0072 |         184.5307 |         -57.7030 |
[32m[20221214 14:21:46 @agent_ppo2.py:185][0m |          -0.0037 |         162.7771 |         -57.6812 |
[32m[20221214 14:21:46 @agent_ppo2.py:185][0m |          -0.0035 |         162.6354 |         -57.3449 |
[32m[20221214 14:21:46 @agent_ppo2.py:185][0m |          -0.0028 |         161.9675 |         -57.6018 |
[32m[20221214 14:21:46 @agent_ppo2.py:185][0m |          -0.0029 |         162.3190 |         -57.4486 |
[32m[20221214 14:21:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:21:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 637.60
[32m[20221214 14:21:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 646.52
[32m[20221214 14:21:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 624.38
[32m[20221214 14:21:47 @agent_ppo2.py:143][0m Total time:      23.74 min
[32m[20221214 14:21:47 @agent_ppo2.py:145][0m 2172928 total steps have happened
[32m[20221214 14:21:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1061 --------------------------#
[32m[20221214 14:21:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:21:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:47 @agent_ppo2.py:185][0m |          -0.0028 |         129.0529 |         -56.0804 |
[32m[20221214 14:21:47 @agent_ppo2.py:185][0m |          -0.0034 |         119.0748 |         -56.0917 |
[32m[20221214 14:21:47 @agent_ppo2.py:185][0m |          -0.0021 |         114.8485 |         -56.1150 |
[32m[20221214 14:21:47 @agent_ppo2.py:185][0m |          -0.0044 |         112.4696 |         -56.1542 |
[32m[20221214 14:21:47 @agent_ppo2.py:185][0m |          -0.0049 |         109.7217 |         -56.2357 |
[32m[20221214 14:21:47 @agent_ppo2.py:185][0m |          -0.0076 |         108.0898 |         -56.2511 |
[32m[20221214 14:21:47 @agent_ppo2.py:185][0m |          -0.0049 |         107.1108 |         -56.0465 |
[32m[20221214 14:21:47 @agent_ppo2.py:185][0m |           0.0003 |         108.5640 |         -56.2452 |
[32m[20221214 14:21:47 @agent_ppo2.py:185][0m |          -0.0036 |         105.0091 |         -56.1017 |
[32m[20221214 14:21:48 @agent_ppo2.py:185][0m |          -0.0035 |         104.4767 |         -56.2552 |
[32m[20221214 14:21:48 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:21:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.25
[32m[20221214 14:21:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 614.47
[32m[20221214 14:21:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 646.46
[32m[20221214 14:21:48 @agent_ppo2.py:143][0m Total time:      23.76 min
[32m[20221214 14:21:48 @agent_ppo2.py:145][0m 2174976 total steps have happened
[32m[20221214 14:21:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1062 --------------------------#
[32m[20221214 14:21:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:21:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:48 @agent_ppo2.py:185][0m |          -0.0025 |         163.9675 |         -57.9194 |
[32m[20221214 14:21:48 @agent_ppo2.py:185][0m |          -0.0006 |         156.7274 |         -57.8395 |
[32m[20221214 14:21:48 @agent_ppo2.py:185][0m |          -0.0022 |         153.8849 |         -57.9848 |
[32m[20221214 14:21:48 @agent_ppo2.py:185][0m |           0.0146 |         170.3406 |         -57.8670 |
[32m[20221214 14:21:48 @agent_ppo2.py:185][0m |          -0.0027 |         151.0385 |         -57.8493 |
[32m[20221214 14:21:48 @agent_ppo2.py:185][0m |           0.0100 |         160.6250 |         -57.8695 |
[32m[20221214 14:21:49 @agent_ppo2.py:185][0m |          -0.0006 |         149.5850 |         -57.8889 |
[32m[20221214 14:21:49 @agent_ppo2.py:185][0m |           0.0001 |         148.5620 |         -57.8583 |
[32m[20221214 14:21:49 @agent_ppo2.py:185][0m |          -0.0010 |         148.2605 |         -57.8278 |
[32m[20221214 14:21:49 @agent_ppo2.py:185][0m |          -0.0027 |         147.8880 |         -57.8739 |
[32m[20221214 14:21:49 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:21:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 636.28
[32m[20221214 14:21:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 647.53
[32m[20221214 14:21:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 655.54
[32m[20221214 14:21:49 @agent_ppo2.py:143][0m Total time:      23.78 min
[32m[20221214 14:21:49 @agent_ppo2.py:145][0m 2177024 total steps have happened
[32m[20221214 14:21:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1063 --------------------------#
[32m[20221214 14:21:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:21:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:49 @agent_ppo2.py:185][0m |           0.0056 |         164.6658 |         -57.6417 |
[32m[20221214 14:21:49 @agent_ppo2.py:185][0m |           0.0020 |         157.7261 |         -57.5893 |
[32m[20221214 14:21:49 @agent_ppo2.py:185][0m |           0.0000 |         156.2468 |         -57.6268 |
[32m[20221214 14:21:50 @agent_ppo2.py:185][0m |           0.0094 |         171.8612 |         -57.7949 |
[32m[20221214 14:21:50 @agent_ppo2.py:185][0m |          -0.0029 |         154.8499 |         -57.9017 |
[32m[20221214 14:21:50 @agent_ppo2.py:185][0m |          -0.0008 |         154.2879 |         -57.7221 |
[32m[20221214 14:21:50 @agent_ppo2.py:185][0m |          -0.0041 |         154.7047 |         -57.9926 |
[32m[20221214 14:21:50 @agent_ppo2.py:185][0m |          -0.0028 |         153.9975 |         -58.0692 |
[32m[20221214 14:21:50 @agent_ppo2.py:185][0m |          -0.0020 |         153.7263 |         -58.0854 |
[32m[20221214 14:21:50 @agent_ppo2.py:185][0m |           0.0005 |         153.2637 |         -58.0141 |
[32m[20221214 14:21:50 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 14:21:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 648.51
[32m[20221214 14:21:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 668.71
[32m[20221214 14:21:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 666.91
[32m[20221214 14:21:50 @agent_ppo2.py:143][0m Total time:      23.81 min
[32m[20221214 14:21:50 @agent_ppo2.py:145][0m 2179072 total steps have happened
[32m[20221214 14:21:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1064 --------------------------#
[32m[20221214 14:21:51 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:21:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:51 @agent_ppo2.py:185][0m |          -0.0019 |         160.1056 |         -59.8839 |
[32m[20221214 14:21:51 @agent_ppo2.py:185][0m |          -0.0026 |         154.1784 |         -59.8799 |
[32m[20221214 14:21:51 @agent_ppo2.py:185][0m |          -0.0020 |         151.3527 |         -59.8712 |
[32m[20221214 14:21:51 @agent_ppo2.py:185][0m |          -0.0016 |         149.0395 |         -59.9159 |
[32m[20221214 14:21:51 @agent_ppo2.py:185][0m |          -0.0031 |         147.1730 |         -59.9908 |
[32m[20221214 14:21:51 @agent_ppo2.py:185][0m |          -0.0035 |         146.5679 |         -60.1755 |
[32m[20221214 14:21:51 @agent_ppo2.py:185][0m |          -0.0032 |         145.4833 |         -60.0635 |
[32m[20221214 14:21:51 @agent_ppo2.py:185][0m |          -0.0043 |         144.7871 |         -60.1669 |
[32m[20221214 14:21:52 @agent_ppo2.py:185][0m |          -0.0044 |         143.1525 |         -60.1389 |
[32m[20221214 14:21:52 @agent_ppo2.py:185][0m |          -0.0059 |         143.1326 |         -60.3531 |
[32m[20221214 14:21:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:21:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 660.77
[32m[20221214 14:21:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 668.47
[32m[20221214 14:21:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 685.16
[32m[20221214 14:21:52 @agent_ppo2.py:143][0m Total time:      23.83 min
[32m[20221214 14:21:52 @agent_ppo2.py:145][0m 2181120 total steps have happened
[32m[20221214 14:21:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1065 --------------------------#
[32m[20221214 14:21:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:52 @agent_ppo2.py:185][0m |          -0.0037 |         157.9925 |         -58.3081 |
[32m[20221214 14:21:52 @agent_ppo2.py:185][0m |          -0.0011 |         156.5153 |         -58.3052 |
[32m[20221214 14:21:52 @agent_ppo2.py:185][0m |          -0.0039 |         155.3988 |         -58.0334 |
[32m[20221214 14:21:52 @agent_ppo2.py:185][0m |          -0.0043 |         154.9806 |         -58.2255 |
[32m[20221214 14:21:53 @agent_ppo2.py:185][0m |          -0.0056 |         154.7352 |         -58.0984 |
[32m[20221214 14:21:53 @agent_ppo2.py:185][0m |           0.0004 |         156.1626 |         -58.0990 |
[32m[20221214 14:21:53 @agent_ppo2.py:185][0m |          -0.0044 |         154.5034 |         -57.9977 |
[32m[20221214 14:21:53 @agent_ppo2.py:185][0m |          -0.0021 |         155.1038 |         -57.9807 |
[32m[20221214 14:21:53 @agent_ppo2.py:185][0m |          -0.0049 |         154.2942 |         -58.1109 |
[32m[20221214 14:21:53 @agent_ppo2.py:185][0m |          -0.0037 |         154.2404 |         -58.0938 |
[32m[20221214 14:21:53 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:21:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 675.46
[32m[20221214 14:21:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 681.38
[32m[20221214 14:21:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 700.35
[32m[20221214 14:21:53 @agent_ppo2.py:143][0m Total time:      23.85 min
[32m[20221214 14:21:53 @agent_ppo2.py:145][0m 2183168 total steps have happened
[32m[20221214 14:21:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1066 --------------------------#
[32m[20221214 14:21:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:54 @agent_ppo2.py:185][0m |           0.0024 |         158.1541 |         -58.6328 |
[32m[20221214 14:21:54 @agent_ppo2.py:185][0m |           0.0005 |         153.4430 |         -58.5036 |
[32m[20221214 14:21:54 @agent_ppo2.py:185][0m |           0.0044 |         154.0736 |         -58.4913 |
[32m[20221214 14:21:54 @agent_ppo2.py:185][0m |           0.0021 |         152.6432 |         -58.5078 |
[32m[20221214 14:21:54 @agent_ppo2.py:185][0m |          -0.0012 |         150.7005 |         -58.6269 |
[32m[20221214 14:21:54 @agent_ppo2.py:185][0m |          -0.0012 |         150.5370 |         -58.5270 |
[32m[20221214 14:21:54 @agent_ppo2.py:185][0m |          -0.0011 |         149.9511 |         -58.7267 |
[32m[20221214 14:21:54 @agent_ppo2.py:185][0m |          -0.0035 |         150.6662 |         -58.5612 |
[32m[20221214 14:21:54 @agent_ppo2.py:185][0m |           0.0056 |         152.8222 |         -58.6081 |
[32m[20221214 14:21:54 @agent_ppo2.py:185][0m |          -0.0026 |         149.4156 |         -58.5397 |
[32m[20221214 14:21:54 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 690.43
[32m[20221214 14:21:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.16
[32m[20221214 14:21:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 691.58
[32m[20221214 14:21:55 @agent_ppo2.py:143][0m Total time:      23.88 min
[32m[20221214 14:21:55 @agent_ppo2.py:145][0m 2185216 total steps have happened
[32m[20221214 14:21:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1067 --------------------------#
[32m[20221214 14:21:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:55 @agent_ppo2.py:185][0m |          -0.0032 |         157.8922 |         -58.6729 |
[32m[20221214 14:21:55 @agent_ppo2.py:185][0m |          -0.0015 |         155.2900 |         -58.5291 |
[32m[20221214 14:21:55 @agent_ppo2.py:185][0m |          -0.0020 |         154.2533 |         -58.7603 |
[32m[20221214 14:21:55 @agent_ppo2.py:185][0m |          -0.0021 |         153.7570 |         -58.8361 |
[32m[20221214 14:21:55 @agent_ppo2.py:185][0m |          -0.0022 |         153.1056 |         -58.9157 |
[32m[20221214 14:21:55 @agent_ppo2.py:185][0m |          -0.0030 |         152.8728 |         -58.8145 |
[32m[20221214 14:21:56 @agent_ppo2.py:185][0m |          -0.0017 |         152.5597 |         -58.8487 |
[32m[20221214 14:21:56 @agent_ppo2.py:185][0m |           0.0006 |         154.3194 |         -58.9506 |
[32m[20221214 14:21:56 @agent_ppo2.py:185][0m |          -0.0024 |         152.4594 |         -59.0252 |
[32m[20221214 14:21:56 @agent_ppo2.py:185][0m |           0.0002 |         152.2731 |         -59.1748 |
[32m[20221214 14:21:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 690.92
[32m[20221214 14:21:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 697.93
[32m[20221214 14:21:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 675.90
[32m[20221214 14:21:56 @agent_ppo2.py:143][0m Total time:      23.90 min
[32m[20221214 14:21:56 @agent_ppo2.py:145][0m 2187264 total steps have happened
[32m[20221214 14:21:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1068 --------------------------#
[32m[20221214 14:21:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:56 @agent_ppo2.py:185][0m |           0.0001 |         148.9055 |         -59.2960 |
[32m[20221214 14:21:56 @agent_ppo2.py:185][0m |           0.0011 |         147.2757 |         -59.5259 |
[32m[20221214 14:21:57 @agent_ppo2.py:185][0m |          -0.0012 |         146.1339 |         -59.2867 |
[32m[20221214 14:21:57 @agent_ppo2.py:185][0m |           0.0010 |         146.0904 |         -59.2141 |
[32m[20221214 14:21:57 @agent_ppo2.py:185][0m |           0.0069 |         151.2191 |         -59.1612 |
[32m[20221214 14:21:57 @agent_ppo2.py:185][0m |           0.0031 |         146.7667 |         -59.2874 |
[32m[20221214 14:21:57 @agent_ppo2.py:185][0m |          -0.0007 |         144.7225 |         -59.0682 |
[32m[20221214 14:21:57 @agent_ppo2.py:185][0m |           0.0131 |         165.5018 |         -59.1613 |
[32m[20221214 14:21:57 @agent_ppo2.py:185][0m |          -0.0028 |         145.4313 |         -58.9878 |
[32m[20221214 14:21:57 @agent_ppo2.py:185][0m |           0.0048 |         147.6013 |         -59.0133 |
[32m[20221214 14:21:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 673.59
[32m[20221214 14:21:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 685.31
[32m[20221214 14:21:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 682.78
[32m[20221214 14:21:57 @agent_ppo2.py:143][0m Total time:      23.92 min
[32m[20221214 14:21:57 @agent_ppo2.py:145][0m 2189312 total steps have happened
[32m[20221214 14:21:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1069 --------------------------#
[32m[20221214 14:21:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:21:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:58 @agent_ppo2.py:185][0m |          -0.0014 |         152.3125 |         -58.7484 |
[32m[20221214 14:21:58 @agent_ppo2.py:185][0m |           0.0093 |         167.4136 |         -58.6816 |
[32m[20221214 14:21:58 @agent_ppo2.py:185][0m |           0.0038 |         146.9361 |         -58.1019 |
[32m[20221214 14:21:58 @agent_ppo2.py:185][0m |          -0.0018 |         145.4622 |         -58.5751 |
[32m[20221214 14:21:58 @agent_ppo2.py:185][0m |           0.0088 |         153.9863 |         -58.5879 |
[32m[20221214 14:21:58 @agent_ppo2.py:185][0m |          -0.0034 |         145.0533 |         -58.5761 |
[32m[20221214 14:21:58 @agent_ppo2.py:185][0m |           0.0015 |         145.7359 |         -58.4356 |
[32m[20221214 14:21:58 @agent_ppo2.py:185][0m |           0.0000 |         144.2964 |         -58.5552 |
[32m[20221214 14:21:58 @agent_ppo2.py:185][0m |          -0.0029 |         144.0714 |         -58.7209 |
[32m[20221214 14:21:59 @agent_ppo2.py:185][0m |          -0.0033 |         143.6767 |         -58.5477 |
[32m[20221214 14:21:59 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:21:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 674.36
[32m[20221214 14:21:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 677.93
[32m[20221214 14:21:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 673.47
[32m[20221214 14:21:59 @agent_ppo2.py:143][0m Total time:      23.94 min
[32m[20221214 14:21:59 @agent_ppo2.py:145][0m 2191360 total steps have happened
[32m[20221214 14:21:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1070 --------------------------#
[32m[20221214 14:21:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:21:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:21:59 @agent_ppo2.py:185][0m |          -0.0030 |         154.8420 |         -58.0511 |
[32m[20221214 14:21:59 @agent_ppo2.py:185][0m |           0.0142 |         168.8988 |         -58.0118 |
[32m[20221214 14:21:59 @agent_ppo2.py:185][0m |          -0.0005 |         150.5316 |         -58.1760 |
[32m[20221214 14:21:59 @agent_ppo2.py:185][0m |          -0.0023 |         149.4433 |         -58.1038 |
[32m[20221214 14:21:59 @agent_ppo2.py:185][0m |          -0.0003 |         148.8351 |         -58.2384 |
[32m[20221214 14:22:00 @agent_ppo2.py:185][0m |           0.0020 |         148.0343 |         -58.0130 |
[32m[20221214 14:22:00 @agent_ppo2.py:185][0m |          -0.0016 |         147.8997 |         -58.2989 |
[32m[20221214 14:22:00 @agent_ppo2.py:185][0m |          -0.0026 |         147.2951 |         -58.3220 |
[32m[20221214 14:22:00 @agent_ppo2.py:185][0m |           0.0083 |         152.7640 |         -58.1527 |
[32m[20221214 14:22:00 @agent_ppo2.py:185][0m |          -0.0001 |         147.0668 |         -58.1563 |
[32m[20221214 14:22:00 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:22:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 668.33
[32m[20221214 14:22:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 675.70
[32m[20221214 14:22:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 686.39
[32m[20221214 14:22:00 @agent_ppo2.py:143][0m Total time:      23.97 min
[32m[20221214 14:22:00 @agent_ppo2.py:145][0m 2193408 total steps have happened
[32m[20221214 14:22:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1071 --------------------------#
[32m[20221214 14:22:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:00 @agent_ppo2.py:185][0m |           0.0069 |         153.7133 |         -60.0605 |
[32m[20221214 14:22:01 @agent_ppo2.py:185][0m |          -0.0023 |         146.9534 |         -60.1829 |
[32m[20221214 14:22:01 @agent_ppo2.py:185][0m |           0.0117 |         157.3695 |         -60.1064 |
[32m[20221214 14:22:01 @agent_ppo2.py:185][0m |           0.0101 |         156.9507 |         -60.4068 |
[32m[20221214 14:22:01 @agent_ppo2.py:185][0m |          -0.0022 |         144.6292 |         -60.3379 |
[32m[20221214 14:22:01 @agent_ppo2.py:185][0m |          -0.0004 |         144.1536 |         -60.3835 |
[32m[20221214 14:22:01 @agent_ppo2.py:185][0m |          -0.0020 |         143.7005 |         -60.4423 |
[32m[20221214 14:22:01 @agent_ppo2.py:185][0m |          -0.0023 |         143.4327 |         -60.5728 |
[32m[20221214 14:22:01 @agent_ppo2.py:185][0m |           0.0028 |         146.6674 |         -60.3594 |
[32m[20221214 14:22:01 @agent_ppo2.py:185][0m |          -0.0012 |         143.0297 |         -60.7521 |
[32m[20221214 14:22:01 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:22:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 680.82
[32m[20221214 14:22:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 692.56
[32m[20221214 14:22:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 693.55
[32m[20221214 14:22:01 @agent_ppo2.py:143][0m Total time:      23.99 min
[32m[20221214 14:22:01 @agent_ppo2.py:145][0m 2195456 total steps have happened
[32m[20221214 14:22:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1072 --------------------------#
[32m[20221214 14:22:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:02 @agent_ppo2.py:185][0m |           0.0050 |         143.6135 |         -59.1056 |
[32m[20221214 14:22:02 @agent_ppo2.py:185][0m |          -0.0009 |         138.4055 |         -59.1851 |
[32m[20221214 14:22:02 @agent_ppo2.py:185][0m |          -0.0016 |         137.1076 |         -59.2486 |
[32m[20221214 14:22:02 @agent_ppo2.py:185][0m |          -0.0014 |         136.9461 |         -59.4525 |
[32m[20221214 14:22:02 @agent_ppo2.py:185][0m |          -0.0011 |         136.4047 |         -59.4063 |
[32m[20221214 14:22:02 @agent_ppo2.py:185][0m |          -0.0020 |         136.4363 |         -59.6611 |
[32m[20221214 14:22:02 @agent_ppo2.py:185][0m |          -0.0001 |         136.1141 |         -59.6100 |
[32m[20221214 14:22:02 @agent_ppo2.py:185][0m |          -0.0016 |         135.3796 |         -59.6415 |
[32m[20221214 14:22:03 @agent_ppo2.py:185][0m |          -0.0010 |         135.4152 |         -59.7100 |
[32m[20221214 14:22:03 @agent_ppo2.py:185][0m |          -0.0018 |         135.5010 |         -59.9506 |
[32m[20221214 14:22:03 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:22:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 686.93
[32m[20221214 14:22:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 692.16
[32m[20221214 14:22:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 698.45
[32m[20221214 14:22:03 @agent_ppo2.py:143][0m Total time:      24.01 min
[32m[20221214 14:22:03 @agent_ppo2.py:145][0m 2197504 total steps have happened
[32m[20221214 14:22:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1073 --------------------------#
[32m[20221214 14:22:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:03 @agent_ppo2.py:185][0m |          -0.0042 |         151.4348 |         -61.2242 |
[32m[20221214 14:22:03 @agent_ppo2.py:185][0m |          -0.0050 |         147.5949 |         -61.2830 |
[32m[20221214 14:22:03 @agent_ppo2.py:185][0m |          -0.0034 |         145.6486 |         -61.1854 |
[32m[20221214 14:22:03 @agent_ppo2.py:185][0m |          -0.0030 |         144.3759 |         -61.2540 |
[32m[20221214 14:22:03 @agent_ppo2.py:185][0m |          -0.0017 |         144.2361 |         -61.2461 |
[32m[20221214 14:22:04 @agent_ppo2.py:185][0m |          -0.0039 |         143.5125 |         -61.2628 |
[32m[20221214 14:22:04 @agent_ppo2.py:185][0m |          -0.0035 |         143.6982 |         -61.1934 |
[32m[20221214 14:22:04 @agent_ppo2.py:185][0m |          -0.0036 |         142.2858 |         -61.1572 |
[32m[20221214 14:22:04 @agent_ppo2.py:185][0m |          -0.0064 |         142.3939 |         -61.2216 |
[32m[20221214 14:22:04 @agent_ppo2.py:185][0m |          -0.0037 |         142.2204 |         -61.1297 |
[32m[20221214 14:22:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:22:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 689.46
[32m[20221214 14:22:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.49
[32m[20221214 14:22:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 709.98
[32m[20221214 14:22:04 @agent_ppo2.py:143][0m Total time:      24.03 min
[32m[20221214 14:22:04 @agent_ppo2.py:145][0m 2199552 total steps have happened
[32m[20221214 14:22:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1074 --------------------------#
[32m[20221214 14:22:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:04 @agent_ppo2.py:185][0m |           0.0057 |         142.9930 |         -60.9946 |
[32m[20221214 14:22:05 @agent_ppo2.py:185][0m |          -0.0021 |         135.0248 |         -61.1633 |
[32m[20221214 14:22:05 @agent_ppo2.py:185][0m |          -0.0010 |         133.2944 |         -61.2590 |
[32m[20221214 14:22:05 @agent_ppo2.py:185][0m |           0.0007 |         132.2401 |         -61.4501 |
[32m[20221214 14:22:05 @agent_ppo2.py:185][0m |          -0.0031 |         131.5805 |         -61.4836 |
[32m[20221214 14:22:05 @agent_ppo2.py:185][0m |          -0.0009 |         131.0291 |         -61.5807 |
[32m[20221214 14:22:05 @agent_ppo2.py:185][0m |           0.0011 |         130.7638 |         -61.6794 |
[32m[20221214 14:22:05 @agent_ppo2.py:185][0m |          -0.0002 |         130.0706 |         -61.7110 |
[32m[20221214 14:22:05 @agent_ppo2.py:185][0m |          -0.0037 |         129.7690 |         -61.8697 |
[32m[20221214 14:22:05 @agent_ppo2.py:185][0m |          -0.0004 |         129.4186 |         -61.4053 |
[32m[20221214 14:22:05 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:22:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.91
[32m[20221214 14:22:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 707.42
[32m[20221214 14:22:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 719.38
[32m[20221214 14:22:05 @agent_ppo2.py:143][0m Total time:      24.06 min
[32m[20221214 14:22:05 @agent_ppo2.py:145][0m 2201600 total steps have happened
[32m[20221214 14:22:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1075 --------------------------#
[32m[20221214 14:22:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:06 @agent_ppo2.py:185][0m |          -0.0045 |         126.7244 |         -62.1099 |
[32m[20221214 14:22:06 @agent_ppo2.py:185][0m |          -0.0038 |         119.0460 |         -62.1243 |
[32m[20221214 14:22:06 @agent_ppo2.py:185][0m |           0.0107 |         132.0573 |         -61.9451 |
[32m[20221214 14:22:06 @agent_ppo2.py:185][0m |          -0.0014 |         115.1734 |         -61.9714 |
[32m[20221214 14:22:06 @agent_ppo2.py:185][0m |          -0.0035 |         114.0624 |         -61.9872 |
[32m[20221214 14:22:06 @agent_ppo2.py:185][0m |          -0.0028 |         113.2304 |         -62.0528 |
[32m[20221214 14:22:06 @agent_ppo2.py:185][0m |          -0.0023 |         112.3573 |         -62.1025 |
[32m[20221214 14:22:06 @agent_ppo2.py:185][0m |           0.0047 |         119.2962 |         -61.9208 |
[32m[20221214 14:22:07 @agent_ppo2.py:185][0m |          -0.0024 |         111.8125 |         -61.7810 |
[32m[20221214 14:22:07 @agent_ppo2.py:185][0m |          -0.0015 |         110.5593 |         -61.8171 |
[32m[20221214 14:22:07 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:22:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 710.25
[32m[20221214 14:22:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 714.53
[32m[20221214 14:22:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 722.21
[32m[20221214 14:22:07 @agent_ppo2.py:143][0m Total time:      24.08 min
[32m[20221214 14:22:07 @agent_ppo2.py:145][0m 2203648 total steps have happened
[32m[20221214 14:22:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1076 --------------------------#
[32m[20221214 14:22:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:07 @agent_ppo2.py:185][0m |           0.0004 |         144.7460 |         -62.3332 |
[32m[20221214 14:22:07 @agent_ppo2.py:185][0m |           0.0043 |         145.0465 |         -61.9930 |
[32m[20221214 14:22:07 @agent_ppo2.py:185][0m |           0.0005 |         140.5838 |         -62.0961 |
[32m[20221214 14:22:07 @agent_ppo2.py:185][0m |          -0.0034 |         139.9365 |         -62.1236 |
[32m[20221214 14:22:08 @agent_ppo2.py:185][0m |           0.0046 |         144.2484 |         -61.9762 |
[32m[20221214 14:22:08 @agent_ppo2.py:185][0m |          -0.0020 |         139.3432 |         -62.0274 |
[32m[20221214 14:22:08 @agent_ppo2.py:185][0m |          -0.0016 |         139.1902 |         -61.8406 |
[32m[20221214 14:22:08 @agent_ppo2.py:185][0m |          -0.0035 |         138.7171 |         -62.1885 |
[32m[20221214 14:22:08 @agent_ppo2.py:185][0m |          -0.0024 |         138.4127 |         -62.0399 |
[32m[20221214 14:22:08 @agent_ppo2.py:185][0m |          -0.0045 |         138.3636 |         -61.9391 |
[32m[20221214 14:22:08 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:22:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 717.78
[32m[20221214 14:22:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 722.85
[32m[20221214 14:22:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 730.65
[32m[20221214 14:22:08 @agent_ppo2.py:143][0m Total time:      24.10 min
[32m[20221214 14:22:08 @agent_ppo2.py:145][0m 2205696 total steps have happened
[32m[20221214 14:22:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1077 --------------------------#
[32m[20221214 14:22:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:09 @agent_ppo2.py:185][0m |          -0.0020 |         156.5366 |         -61.5212 |
[32m[20221214 14:22:09 @agent_ppo2.py:185][0m |          -0.0048 |         152.6272 |         -61.5405 |
[32m[20221214 14:22:09 @agent_ppo2.py:185][0m |          -0.0031 |         150.0411 |         -61.6015 |
[32m[20221214 14:22:09 @agent_ppo2.py:185][0m |          -0.0039 |         149.5934 |         -61.5743 |
[32m[20221214 14:22:09 @agent_ppo2.py:185][0m |          -0.0022 |         148.3814 |         -61.6959 |
[32m[20221214 14:22:09 @agent_ppo2.py:185][0m |          -0.0042 |         147.4830 |         -61.6804 |
[32m[20221214 14:22:09 @agent_ppo2.py:185][0m |          -0.0016 |         146.8386 |         -61.6885 |
[32m[20221214 14:22:09 @agent_ppo2.py:185][0m |           0.0038 |         151.2318 |         -61.7405 |
[32m[20221214 14:22:09 @agent_ppo2.py:185][0m |          -0.0023 |         145.4890 |         -61.7819 |
[32m[20221214 14:22:09 @agent_ppo2.py:185][0m |          -0.0020 |         145.4013 |         -61.7231 |
[32m[20221214 14:22:09 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:22:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 727.13
[32m[20221214 14:22:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.83
[32m[20221214 14:22:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 740.71
[32m[20221214 14:22:10 @agent_ppo2.py:143][0m Total time:      24.12 min
[32m[20221214 14:22:10 @agent_ppo2.py:145][0m 2207744 total steps have happened
[32m[20221214 14:22:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1078 --------------------------#
[32m[20221214 14:22:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:10 @agent_ppo2.py:185][0m |          -0.0007 |         170.3966 |         -63.2465 |
[32m[20221214 14:22:10 @agent_ppo2.py:185][0m |          -0.0012 |         167.8812 |         -63.4214 |
[32m[20221214 14:22:10 @agent_ppo2.py:185][0m |          -0.0039 |         168.3416 |         -63.5214 |
[32m[20221214 14:22:10 @agent_ppo2.py:185][0m |          -0.0023 |         166.7028 |         -63.5945 |
[32m[20221214 14:22:10 @agent_ppo2.py:185][0m |          -0.0007 |         166.5440 |         -63.5877 |
[32m[20221214 14:22:10 @agent_ppo2.py:185][0m |          -0.0020 |         165.2388 |         -63.6302 |
[32m[20221214 14:22:10 @agent_ppo2.py:185][0m |           0.0001 |         165.0008 |         -63.5675 |
[32m[20221214 14:22:11 @agent_ppo2.py:185][0m |          -0.0008 |         164.4897 |         -63.6333 |
[32m[20221214 14:22:11 @agent_ppo2.py:185][0m |           0.0010 |         164.0582 |         -63.8489 |
[32m[20221214 14:22:11 @agent_ppo2.py:185][0m |          -0.0011 |         163.2809 |         -63.8603 |
[32m[20221214 14:22:11 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:22:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.24
[32m[20221214 14:22:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 737.46
[32m[20221214 14:22:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 746.87
[32m[20221214 14:22:11 @agent_ppo2.py:143][0m Total time:      24.15 min
[32m[20221214 14:22:11 @agent_ppo2.py:145][0m 2209792 total steps have happened
[32m[20221214 14:22:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1079 --------------------------#
[32m[20221214 14:22:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:11 @agent_ppo2.py:185][0m |          -0.0001 |         154.4022 |         -63.3512 |
[32m[20221214 14:22:11 @agent_ppo2.py:185][0m |           0.0013 |         147.3924 |         -63.3437 |
[32m[20221214 14:22:11 @agent_ppo2.py:185][0m |          -0.0020 |         145.9654 |         -63.4251 |
[32m[20221214 14:22:12 @agent_ppo2.py:185][0m |           0.0112 |         159.2598 |         -63.3907 |
[32m[20221214 14:22:12 @agent_ppo2.py:185][0m |           0.0062 |         149.2572 |         -63.1965 |
[32m[20221214 14:22:12 @agent_ppo2.py:185][0m |          -0.0025 |         142.2249 |         -63.3832 |
[32m[20221214 14:22:12 @agent_ppo2.py:185][0m |          -0.0008 |         141.7820 |         -63.6055 |
[32m[20221214 14:22:12 @agent_ppo2.py:185][0m |          -0.0026 |         140.0403 |         -63.4967 |
[32m[20221214 14:22:12 @agent_ppo2.py:185][0m |          -0.0017 |         139.4114 |         -63.4883 |
[32m[20221214 14:22:12 @agent_ppo2.py:185][0m |           0.0009 |         140.2146 |         -63.5872 |
[32m[20221214 14:22:12 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:22:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 739.84
[32m[20221214 14:22:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 744.10
[32m[20221214 14:22:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 748.80
[32m[20221214 14:22:12 @agent_ppo2.py:143][0m Total time:      24.17 min
[32m[20221214 14:22:12 @agent_ppo2.py:145][0m 2211840 total steps have happened
[32m[20221214 14:22:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1080 --------------------------#
[32m[20221214 14:22:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:22:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:13 @agent_ppo2.py:185][0m |          -0.0028 |         176.5875 |         -63.5655 |
[32m[20221214 14:22:13 @agent_ppo2.py:185][0m |           0.0001 |         154.3934 |         -63.5003 |
[32m[20221214 14:22:13 @agent_ppo2.py:185][0m |           0.0012 |         148.2588 |         -63.4462 |
[32m[20221214 14:22:13 @agent_ppo2.py:185][0m |          -0.0033 |         143.2809 |         -63.5614 |
[32m[20221214 14:22:13 @agent_ppo2.py:185][0m |           0.0018 |         142.1187 |         -63.5629 |
[32m[20221214 14:22:13 @agent_ppo2.py:185][0m |          -0.0041 |         138.6529 |         -63.4074 |
[32m[20221214 14:22:13 @agent_ppo2.py:185][0m |          -0.0039 |         135.9256 |         -63.5184 |
[32m[20221214 14:22:13 @agent_ppo2.py:185][0m |          -0.0026 |         134.3176 |         -63.4578 |
[32m[20221214 14:22:13 @agent_ppo2.py:185][0m |          -0.0034 |         133.0649 |         -63.2834 |
[32m[20221214 14:22:13 @agent_ppo2.py:185][0m |           0.0089 |         143.1694 |         -63.4290 |
[32m[20221214 14:22:13 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:22:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.48
[32m[20221214 14:22:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.88
[32m[20221214 14:22:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.37
[32m[20221214 14:22:14 @agent_ppo2.py:143][0m Total time:      24.19 min
[32m[20221214 14:22:14 @agent_ppo2.py:145][0m 2213888 total steps have happened
[32m[20221214 14:22:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1081 --------------------------#
[32m[20221214 14:22:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:14 @agent_ppo2.py:185][0m |           0.0020 |         162.2197 |         -63.7101 |
[32m[20221214 14:22:14 @agent_ppo2.py:185][0m |          -0.0012 |         153.1608 |         -63.8314 |
[32m[20221214 14:22:14 @agent_ppo2.py:185][0m |          -0.0025 |         149.6534 |         -64.0044 |
[32m[20221214 14:22:14 @agent_ppo2.py:185][0m |           0.0014 |         147.3757 |         -63.8723 |
[32m[20221214 14:22:14 @agent_ppo2.py:185][0m |          -0.0019 |         145.3987 |         -63.8690 |
[32m[20221214 14:22:14 @agent_ppo2.py:185][0m |          -0.0013 |         144.8054 |         -63.9020 |
[32m[20221214 14:22:14 @agent_ppo2.py:185][0m |           0.0006 |         143.5724 |         -64.1665 |
[32m[20221214 14:22:15 @agent_ppo2.py:185][0m |          -0.0005 |         141.9988 |         -64.0939 |
[32m[20221214 14:22:15 @agent_ppo2.py:185][0m |          -0.0006 |         141.0440 |         -64.1559 |
[32m[20221214 14:22:15 @agent_ppo2.py:185][0m |           0.0003 |         140.2147 |         -64.2287 |
[32m[20221214 14:22:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:22:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 751.19
[32m[20221214 14:22:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 755.88
[32m[20221214 14:22:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.59
[32m[20221214 14:22:15 @agent_ppo2.py:143][0m Total time:      24.21 min
[32m[20221214 14:22:15 @agent_ppo2.py:145][0m 2215936 total steps have happened
[32m[20221214 14:22:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1082 --------------------------#
[32m[20221214 14:22:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:15 @agent_ppo2.py:185][0m |           0.0001 |         140.4611 |         -64.6378 |
[32m[20221214 14:22:15 @agent_ppo2.py:185][0m |          -0.0015 |         135.0181 |         -64.5958 |
[32m[20221214 14:22:15 @agent_ppo2.py:185][0m |          -0.0015 |         133.5584 |         -64.4543 |
[32m[20221214 14:22:16 @agent_ppo2.py:185][0m |          -0.0036 |         132.2682 |         -64.5326 |
[32m[20221214 14:22:16 @agent_ppo2.py:185][0m |           0.0037 |         135.9079 |         -64.4237 |
[32m[20221214 14:22:16 @agent_ppo2.py:185][0m |          -0.0056 |         130.6592 |         -64.5429 |
[32m[20221214 14:22:16 @agent_ppo2.py:185][0m |          -0.0025 |         129.4069 |         -64.5227 |
[32m[20221214 14:22:16 @agent_ppo2.py:185][0m |          -0.0002 |         128.8412 |         -64.5283 |
[32m[20221214 14:22:16 @agent_ppo2.py:185][0m |           0.0018 |         130.5334 |         -64.4038 |
[32m[20221214 14:22:16 @agent_ppo2.py:185][0m |          -0.0007 |         126.9735 |         -64.5589 |
[32m[20221214 14:22:16 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:22:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.32
[32m[20221214 14:22:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.17
[32m[20221214 14:22:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.31
[32m[20221214 14:22:16 @agent_ppo2.py:143][0m Total time:      24.24 min
[32m[20221214 14:22:16 @agent_ppo2.py:145][0m 2217984 total steps have happened
[32m[20221214 14:22:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1083 --------------------------#
[32m[20221214 14:22:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:17 @agent_ppo2.py:185][0m |           0.0082 |         161.5785 |         -64.0063 |
[32m[20221214 14:22:17 @agent_ppo2.py:185][0m |          -0.0036 |         142.4402 |         -63.8399 |
[32m[20221214 14:22:17 @agent_ppo2.py:185][0m |          -0.0017 |         139.4451 |         -63.8484 |
[32m[20221214 14:22:17 @agent_ppo2.py:185][0m |          -0.0031 |         137.0462 |         -63.9612 |
[32m[20221214 14:22:17 @agent_ppo2.py:185][0m |          -0.0010 |         135.8922 |         -64.0622 |
[32m[20221214 14:22:17 @agent_ppo2.py:185][0m |          -0.0034 |         134.4765 |         -63.9539 |
[32m[20221214 14:22:17 @agent_ppo2.py:185][0m |          -0.0035 |         134.1244 |         -64.0264 |
[32m[20221214 14:22:17 @agent_ppo2.py:185][0m |          -0.0015 |         132.3063 |         -63.9987 |
[32m[20221214 14:22:17 @agent_ppo2.py:185][0m |          -0.0001 |         131.8430 |         -64.0495 |
[32m[20221214 14:22:18 @agent_ppo2.py:185][0m |          -0.0031 |         130.6030 |         -64.0772 |
[32m[20221214 14:22:18 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:22:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 745.26
[32m[20221214 14:22:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.27
[32m[20221214 14:22:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 754.64
[32m[20221214 14:22:18 @agent_ppo2.py:143][0m Total time:      24.26 min
[32m[20221214 14:22:18 @agent_ppo2.py:145][0m 2220032 total steps have happened
[32m[20221214 14:22:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1084 --------------------------#
[32m[20221214 14:22:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:18 @agent_ppo2.py:185][0m |          -0.0011 |         133.6980 |         -64.4319 |
[32m[20221214 14:22:18 @agent_ppo2.py:185][0m |          -0.0037 |         126.6211 |         -64.2868 |
[32m[20221214 14:22:18 @agent_ppo2.py:185][0m |          -0.0006 |         124.2942 |         -64.2289 |
[32m[20221214 14:22:18 @agent_ppo2.py:185][0m |           0.0051 |         127.7830 |         -64.2728 |
[32m[20221214 14:22:18 @agent_ppo2.py:185][0m |           0.0046 |         121.0779 |         -64.2759 |
[32m[20221214 14:22:19 @agent_ppo2.py:185][0m |          -0.0062 |         117.3165 |         -64.2042 |
[32m[20221214 14:22:19 @agent_ppo2.py:185][0m |          -0.0032 |         116.4046 |         -64.2867 |
[32m[20221214 14:22:19 @agent_ppo2.py:185][0m |          -0.0006 |         115.4410 |         -64.2141 |
[32m[20221214 14:22:19 @agent_ppo2.py:185][0m |          -0.0050 |         113.9522 |         -64.3562 |
[32m[20221214 14:22:19 @agent_ppo2.py:185][0m |          -0.0026 |         112.9232 |         -64.1738 |
[32m[20221214 14:22:19 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:22:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 747.18
[32m[20221214 14:22:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.73
[32m[20221214 14:22:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 758.22
[32m[20221214 14:22:19 @agent_ppo2.py:143][0m Total time:      24.28 min
[32m[20221214 14:22:19 @agent_ppo2.py:145][0m 2222080 total steps have happened
[32m[20221214 14:22:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1085 --------------------------#
[32m[20221214 14:22:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:22:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:19 @agent_ppo2.py:185][0m |           0.0011 |         166.1387 |         -64.5527 |
[32m[20221214 14:22:20 @agent_ppo2.py:185][0m |          -0.0007 |         159.5841 |         -64.5419 |
[32m[20221214 14:22:20 @agent_ppo2.py:185][0m |          -0.0057 |         156.9320 |         -64.5260 |
[32m[20221214 14:22:20 @agent_ppo2.py:185][0m |          -0.0037 |         153.1146 |         -64.6105 |
[32m[20221214 14:22:20 @agent_ppo2.py:185][0m |          -0.0028 |         150.4018 |         -64.6059 |
[32m[20221214 14:22:20 @agent_ppo2.py:185][0m |          -0.0036 |         149.6772 |         -64.5630 |
[32m[20221214 14:22:20 @agent_ppo2.py:185][0m |          -0.0021 |         147.9684 |         -64.5630 |
[32m[20221214 14:22:20 @agent_ppo2.py:185][0m |          -0.0038 |         145.2538 |         -64.6853 |
[32m[20221214 14:22:20 @agent_ppo2.py:185][0m |          -0.0005 |         143.7809 |         -65.0182 |
[32m[20221214 14:22:20 @agent_ppo2.py:185][0m |          -0.0036 |         142.6985 |         -65.0566 |
[32m[20221214 14:22:20 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:22:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 751.49
[32m[20221214 14:22:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 754.94
[32m[20221214 14:22:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 747.62
[32m[20221214 14:22:20 @agent_ppo2.py:143][0m Total time:      24.31 min
[32m[20221214 14:22:20 @agent_ppo2.py:145][0m 2224128 total steps have happened
[32m[20221214 14:22:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1086 --------------------------#
[32m[20221214 14:22:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:21 @agent_ppo2.py:185][0m |          -0.0016 |         217.0922 |         -64.9948 |
[32m[20221214 14:22:21 @agent_ppo2.py:185][0m |          -0.0024 |         205.1772 |         -64.8419 |
[32m[20221214 14:22:21 @agent_ppo2.py:185][0m |          -0.0000 |         202.2148 |         -64.9660 |
[32m[20221214 14:22:21 @agent_ppo2.py:185][0m |           0.0046 |         204.1164 |         -65.0956 |
[32m[20221214 14:22:21 @agent_ppo2.py:185][0m |          -0.0016 |         199.7594 |         -65.1783 |
[32m[20221214 14:22:21 @agent_ppo2.py:185][0m |          -0.0029 |         199.5536 |         -65.2818 |
[32m[20221214 14:22:21 @agent_ppo2.py:185][0m |          -0.0021 |         199.0816 |         -65.2447 |
[32m[20221214 14:22:21 @agent_ppo2.py:185][0m |           0.0028 |         199.9657 |         -65.4225 |
[32m[20221214 14:22:22 @agent_ppo2.py:185][0m |          -0.0017 |         197.4709 |         -65.5692 |
[32m[20221214 14:22:22 @agent_ppo2.py:185][0m |          -0.0017 |         197.0515 |         -65.4304 |
[32m[20221214 14:22:22 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:22:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.11
[32m[20221214 14:22:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 751.37
[32m[20221214 14:22:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 758.45
[32m[20221214 14:22:22 @agent_ppo2.py:143][0m Total time:      24.33 min
[32m[20221214 14:22:22 @agent_ppo2.py:145][0m 2226176 total steps have happened
[32m[20221214 14:22:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1087 --------------------------#
[32m[20221214 14:22:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:22 @agent_ppo2.py:185][0m |          -0.0007 |         194.3155 |         -64.9678 |
[32m[20221214 14:22:22 @agent_ppo2.py:185][0m |           0.0004 |         179.9561 |         -64.9268 |
[32m[20221214 14:22:22 @agent_ppo2.py:185][0m |          -0.0010 |         174.1983 |         -64.8678 |
[32m[20221214 14:22:22 @agent_ppo2.py:185][0m |           0.0000 |         170.4371 |         -65.0000 |
[32m[20221214 14:22:23 @agent_ppo2.py:185][0m |          -0.0026 |         168.5981 |         -65.1312 |
[32m[20221214 14:22:23 @agent_ppo2.py:185][0m |           0.0062 |         168.9965 |         -64.8564 |
[32m[20221214 14:22:23 @agent_ppo2.py:185][0m |          -0.0004 |         163.3279 |         -65.2043 |
[32m[20221214 14:22:23 @agent_ppo2.py:185][0m |          -0.0019 |         161.2514 |         -65.0756 |
[32m[20221214 14:22:23 @agent_ppo2.py:185][0m |           0.0017 |         160.8338 |         -65.2428 |
[32m[20221214 14:22:23 @agent_ppo2.py:185][0m |          -0.0023 |         159.3082 |         -65.0665 |
[32m[20221214 14:22:23 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:22:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.05
[32m[20221214 14:22:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 753.62
[32m[20221214 14:22:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.73
[32m[20221214 14:22:23 @agent_ppo2.py:143][0m Total time:      24.35 min
[32m[20221214 14:22:23 @agent_ppo2.py:145][0m 2228224 total steps have happened
[32m[20221214 14:22:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1088 --------------------------#
[32m[20221214 14:22:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:24 @agent_ppo2.py:185][0m |          -0.0021 |         220.8717 |         -66.0635 |
[32m[20221214 14:22:24 @agent_ppo2.py:185][0m |          -0.0016 |         211.1408 |         -66.2180 |
[32m[20221214 14:22:24 @agent_ppo2.py:185][0m |          -0.0020 |         205.9332 |         -66.1720 |
[32m[20221214 14:22:24 @agent_ppo2.py:185][0m |          -0.0029 |         202.9656 |         -66.0731 |
[32m[20221214 14:22:24 @agent_ppo2.py:185][0m |          -0.0033 |         201.6796 |         -66.1270 |
[32m[20221214 14:22:24 @agent_ppo2.py:185][0m |          -0.0020 |         199.9519 |         -66.1281 |
[32m[20221214 14:22:24 @agent_ppo2.py:185][0m |          -0.0041 |         198.4980 |         -66.2293 |
[32m[20221214 14:22:24 @agent_ppo2.py:185][0m |          -0.0026 |         197.5514 |         -66.2239 |
[32m[20221214 14:22:24 @agent_ppo2.py:185][0m |          -0.0041 |         196.9224 |         -66.1558 |
[32m[20221214 14:22:24 @agent_ppo2.py:185][0m |          -0.0025 |         195.5880 |         -66.2846 |
[32m[20221214 14:22:24 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:22:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 746.40
[32m[20221214 14:22:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 757.53
[32m[20221214 14:22:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 750.94
[32m[20221214 14:22:25 @agent_ppo2.py:143][0m Total time:      24.38 min
[32m[20221214 14:22:25 @agent_ppo2.py:145][0m 2230272 total steps have happened
[32m[20221214 14:22:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1089 --------------------------#
[32m[20221214 14:22:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:25 @agent_ppo2.py:185][0m |          -0.0023 |         193.3813 |         -65.4027 |
[32m[20221214 14:22:25 @agent_ppo2.py:185][0m |          -0.0027 |         183.3092 |         -65.2649 |
[32m[20221214 14:22:25 @agent_ppo2.py:185][0m |          -0.0041 |         179.4115 |         -65.3142 |
[32m[20221214 14:22:25 @agent_ppo2.py:185][0m |           0.0036 |         179.9810 |         -65.1602 |
[32m[20221214 14:22:25 @agent_ppo2.py:185][0m |           0.0013 |         173.7708 |         -65.6301 |
[32m[20221214 14:22:25 @agent_ppo2.py:185][0m |          -0.0035 |         171.5922 |         -65.1681 |
[32m[20221214 14:22:25 @agent_ppo2.py:185][0m |           0.0012 |         169.6434 |         -65.4709 |
[32m[20221214 14:22:26 @agent_ppo2.py:185][0m |          -0.0038 |         168.6518 |         -65.4818 |
[32m[20221214 14:22:26 @agent_ppo2.py:185][0m |          -0.0039 |         167.4531 |         -65.3001 |
[32m[20221214 14:22:26 @agent_ppo2.py:185][0m |          -0.0017 |         167.5589 |         -65.4487 |
[32m[20221214 14:22:26 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:22:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 747.66
[32m[20221214 14:22:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 753.40
[32m[20221214 14:22:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.43
[32m[20221214 14:22:26 @agent_ppo2.py:143][0m Total time:      24.40 min
[32m[20221214 14:22:26 @agent_ppo2.py:145][0m 2232320 total steps have happened
[32m[20221214 14:22:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1090 --------------------------#
[32m[20221214 14:22:26 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:22:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:26 @agent_ppo2.py:185][0m |          -0.0025 |         183.4328 |         -65.2353 |
[32m[20221214 14:22:26 @agent_ppo2.py:185][0m |          -0.0033 |         176.6409 |         -65.1047 |
[32m[20221214 14:22:27 @agent_ppo2.py:185][0m |          -0.0037 |         173.7483 |         -65.2950 |
[32m[20221214 14:22:27 @agent_ppo2.py:185][0m |          -0.0042 |         171.2624 |         -65.1574 |
[32m[20221214 14:22:27 @agent_ppo2.py:185][0m |          -0.0007 |         169.7063 |         -65.1489 |
[32m[20221214 14:22:27 @agent_ppo2.py:185][0m |          -0.0041 |         168.7588 |         -65.1432 |
[32m[20221214 14:22:27 @agent_ppo2.py:185][0m |          -0.0048 |         166.8989 |         -65.0451 |
[32m[20221214 14:22:27 @agent_ppo2.py:185][0m |          -0.0023 |         166.5859 |         -65.1378 |
[32m[20221214 14:22:27 @agent_ppo2.py:185][0m |          -0.0051 |         165.2687 |         -65.2024 |
[32m[20221214 14:22:27 @agent_ppo2.py:185][0m |          -0.0040 |         165.1749 |         -64.9658 |
[32m[20221214 14:22:27 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:22:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 755.33
[32m[20221214 14:22:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.26
[32m[20221214 14:22:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 750.67
[32m[20221214 14:22:27 @agent_ppo2.py:143][0m Total time:      24.42 min
[32m[20221214 14:22:27 @agent_ppo2.py:145][0m 2234368 total steps have happened
[32m[20221214 14:22:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1091 --------------------------#
[32m[20221214 14:22:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:22:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:28 @agent_ppo2.py:185][0m |          -0.0026 |         177.3244 |         -65.5974 |
[32m[20221214 14:22:28 @agent_ppo2.py:185][0m |           0.0055 |         159.4407 |         -65.6741 |
[32m[20221214 14:22:28 @agent_ppo2.py:185][0m |          -0.0023 |         147.8704 |         -65.5101 |
[32m[20221214 14:22:28 @agent_ppo2.py:185][0m |          -0.0073 |         142.0960 |         -65.5679 |
[32m[20221214 14:22:28 @agent_ppo2.py:185][0m |          -0.0032 |         137.8303 |         -65.5290 |
[32m[20221214 14:22:28 @agent_ppo2.py:185][0m |          -0.0041 |         135.0612 |         -65.6721 |
[32m[20221214 14:22:28 @agent_ppo2.py:185][0m |           0.0001 |         136.8519 |         -65.7337 |
[32m[20221214 14:22:28 @agent_ppo2.py:185][0m |          -0.0032 |         132.7824 |         -65.6230 |
[32m[20221214 14:22:28 @agent_ppo2.py:185][0m |          -0.0101 |         131.1355 |         -65.8436 |
[32m[20221214 14:22:28 @agent_ppo2.py:185][0m |          -0.0037 |         129.9836 |         -65.5564 |
[32m[20221214 14:22:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:22:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 721.37
[32m[20221214 14:22:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 756.24
[32m[20221214 14:22:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 684.28
[32m[20221214 14:22:29 @agent_ppo2.py:143][0m Total time:      24.44 min
[32m[20221214 14:22:29 @agent_ppo2.py:145][0m 2236416 total steps have happened
[32m[20221214 14:22:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1092 --------------------------#
[32m[20221214 14:22:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:29 @agent_ppo2.py:185][0m |           0.0019 |         194.1288 |         -67.7265 |
[32m[20221214 14:22:29 @agent_ppo2.py:185][0m |          -0.0033 |         152.8100 |         -67.6443 |
[32m[20221214 14:22:29 @agent_ppo2.py:185][0m |           0.0054 |         146.3410 |         -67.6164 |
[32m[20221214 14:22:29 @agent_ppo2.py:185][0m |          -0.0013 |         133.4334 |         -67.5688 |
[32m[20221214 14:22:29 @agent_ppo2.py:185][0m |          -0.0026 |         127.5327 |         -67.6448 |
[32m[20221214 14:22:29 @agent_ppo2.py:185][0m |          -0.0027 |         123.6380 |         -67.5115 |
[32m[20221214 14:22:30 @agent_ppo2.py:185][0m |           0.0049 |         122.1023 |         -67.5157 |
[32m[20221214 14:22:30 @agent_ppo2.py:185][0m |          -0.0053 |         117.7931 |         -67.5834 |
[32m[20221214 14:22:30 @agent_ppo2.py:185][0m |          -0.0012 |         116.3407 |         -67.6876 |
[32m[20221214 14:22:30 @agent_ppo2.py:185][0m |          -0.0056 |         114.3847 |         -67.7064 |
[32m[20221214 14:22:30 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:22:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 699.39
[32m[20221214 14:22:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 731.91
[32m[20221214 14:22:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 736.92
[32m[20221214 14:22:30 @agent_ppo2.py:143][0m Total time:      24.47 min
[32m[20221214 14:22:30 @agent_ppo2.py:145][0m 2238464 total steps have happened
[32m[20221214 14:22:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1093 --------------------------#
[32m[20221214 14:22:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:30 @agent_ppo2.py:185][0m |          -0.0013 |         155.9456 |         -66.5937 |
[32m[20221214 14:22:30 @agent_ppo2.py:185][0m |          -0.0048 |         124.4693 |         -66.4264 |
[32m[20221214 14:22:31 @agent_ppo2.py:185][0m |          -0.0098 |         115.8141 |         -66.4583 |
[32m[20221214 14:22:31 @agent_ppo2.py:185][0m |          -0.0048 |         109.9935 |         -66.3893 |
[32m[20221214 14:22:31 @agent_ppo2.py:185][0m |          -0.0076 |         106.0667 |         -66.3897 |
[32m[20221214 14:22:31 @agent_ppo2.py:185][0m |          -0.0084 |         102.5723 |         -66.2228 |
[32m[20221214 14:22:31 @agent_ppo2.py:185][0m |          -0.0061 |         100.6232 |         -66.2172 |
[32m[20221214 14:22:31 @agent_ppo2.py:185][0m |          -0.0016 |          99.7063 |         -66.1362 |
[32m[20221214 14:22:31 @agent_ppo2.py:185][0m |          -0.0058 |          96.7807 |         -66.1387 |
[32m[20221214 14:22:31 @agent_ppo2.py:185][0m |          -0.0044 |          95.5673 |         -66.1088 |
[32m[20221214 14:22:31 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:22:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 706.80
[32m[20221214 14:22:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 731.65
[32m[20221214 14:22:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.54
[32m[20221214 14:22:31 @agent_ppo2.py:143][0m Total time:      24.49 min
[32m[20221214 14:22:31 @agent_ppo2.py:145][0m 2240512 total steps have happened
[32m[20221214 14:22:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1094 --------------------------#
[32m[20221214 14:22:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:32 @agent_ppo2.py:185][0m |          -0.0034 |         197.2891 |         -66.6526 |
[32m[20221214 14:22:32 @agent_ppo2.py:185][0m |          -0.0004 |         180.3189 |         -66.6084 |
[32m[20221214 14:22:32 @agent_ppo2.py:185][0m |          -0.0028 |         173.1214 |         -66.5300 |
[32m[20221214 14:22:32 @agent_ppo2.py:185][0m |          -0.0049 |         169.3922 |         -66.5136 |
[32m[20221214 14:22:32 @agent_ppo2.py:185][0m |           0.0048 |         178.4164 |         -66.4593 |
[32m[20221214 14:22:32 @agent_ppo2.py:185][0m |          -0.0004 |         165.7129 |         -66.2993 |
[32m[20221214 14:22:32 @agent_ppo2.py:185][0m |           0.0017 |         167.4916 |         -66.3718 |
[32m[20221214 14:22:32 @agent_ppo2.py:185][0m |           0.0050 |         170.6686 |         -66.0539 |
[32m[20221214 14:22:32 @agent_ppo2.py:185][0m |          -0.0033 |         162.0960 |         -66.2228 |
[32m[20221214 14:22:33 @agent_ppo2.py:185][0m |          -0.0004 |         160.8107 |         -66.2761 |
[32m[20221214 14:22:33 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:22:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 740.20
[32m[20221214 14:22:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 763.14
[32m[20221214 14:22:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 737.02
[32m[20221214 14:22:33 @agent_ppo2.py:143][0m Total time:      24.51 min
[32m[20221214 14:22:33 @agent_ppo2.py:145][0m 2242560 total steps have happened
[32m[20221214 14:22:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1095 --------------------------#
[32m[20221214 14:22:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:33 @agent_ppo2.py:185][0m |          -0.0023 |         197.3750 |         -65.4100 |
[32m[20221214 14:22:33 @agent_ppo2.py:185][0m |           0.0027 |         188.0474 |         -65.7183 |
[32m[20221214 14:22:33 @agent_ppo2.py:185][0m |           0.0002 |         185.1089 |         -65.6377 |
[32m[20221214 14:22:33 @agent_ppo2.py:185][0m |          -0.0001 |         183.5895 |         -65.8533 |
[32m[20221214 14:22:33 @agent_ppo2.py:185][0m |          -0.0023 |         183.4890 |         -65.6909 |
[32m[20221214 14:22:34 @agent_ppo2.py:185][0m |          -0.0008 |         181.7928 |         -66.0076 |
[32m[20221214 14:22:34 @agent_ppo2.py:185][0m |          -0.0003 |         181.4902 |         -65.9742 |
[32m[20221214 14:22:34 @agent_ppo2.py:185][0m |          -0.0042 |         182.5972 |         -65.9892 |
[32m[20221214 14:22:34 @agent_ppo2.py:185][0m |          -0.0021 |         180.5929 |         -66.0984 |
[32m[20221214 14:22:34 @agent_ppo2.py:185][0m |          -0.0011 |         180.4015 |         -66.0812 |
[32m[20221214 14:22:34 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:22:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 739.32
[32m[20221214 14:22:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 748.12
[32m[20221214 14:22:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 733.94
[32m[20221214 14:22:34 @agent_ppo2.py:143][0m Total time:      24.53 min
[32m[20221214 14:22:34 @agent_ppo2.py:145][0m 2244608 total steps have happened
[32m[20221214 14:22:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1096 --------------------------#
[32m[20221214 14:22:34 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:22:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:34 @agent_ppo2.py:185][0m |          -0.0023 |         222.4113 |         -67.4602 |
[32m[20221214 14:22:35 @agent_ppo2.py:185][0m |          -0.0027 |         213.1202 |         -67.1353 |
[32m[20221214 14:22:35 @agent_ppo2.py:185][0m |           0.0133 |         231.6298 |         -67.2393 |
[32m[20221214 14:22:35 @agent_ppo2.py:185][0m |          -0.0014 |         203.6181 |         -67.2695 |
[32m[20221214 14:22:35 @agent_ppo2.py:185][0m |          -0.0027 |         202.0691 |         -67.3654 |
[32m[20221214 14:22:35 @agent_ppo2.py:185][0m |           0.0013 |         200.8865 |         -67.1684 |
[32m[20221214 14:22:35 @agent_ppo2.py:185][0m |          -0.0003 |         197.8694 |         -67.4910 |
[32m[20221214 14:22:35 @agent_ppo2.py:185][0m |          -0.0035 |         195.2632 |         -67.0801 |
[32m[20221214 14:22:35 @agent_ppo2.py:185][0m |          -0.0030 |         193.8536 |         -67.0225 |
[32m[20221214 14:22:35 @agent_ppo2.py:185][0m |          -0.0006 |         193.4200 |         -67.2420 |
[32m[20221214 14:22:35 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:22:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 721.12
[32m[20221214 14:22:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.14
[32m[20221214 14:22:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 746.87
[32m[20221214 14:22:36 @agent_ppo2.py:143][0m Total time:      24.56 min
[32m[20221214 14:22:36 @agent_ppo2.py:145][0m 2246656 total steps have happened
[32m[20221214 14:22:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1097 --------------------------#
[32m[20221214 14:22:36 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:22:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:36 @agent_ppo2.py:185][0m |           0.0003 |         167.4372 |         -67.0400 |
[32m[20221214 14:22:36 @agent_ppo2.py:185][0m |          -0.0001 |         156.7843 |         -67.0935 |
[32m[20221214 14:22:36 @agent_ppo2.py:185][0m |           0.0017 |         152.7344 |         -67.0536 |
[32m[20221214 14:22:36 @agent_ppo2.py:185][0m |           0.0002 |         150.9629 |         -67.1316 |
[32m[20221214 14:22:36 @agent_ppo2.py:185][0m |          -0.0012 |         149.7731 |         -67.2232 |
[32m[20221214 14:22:36 @agent_ppo2.py:185][0m |           0.0001 |         148.5337 |         -67.2317 |
[32m[20221214 14:22:37 @agent_ppo2.py:185][0m |          -0.0031 |         147.3164 |         -67.1100 |
[32m[20221214 14:22:37 @agent_ppo2.py:185][0m |           0.0001 |         146.9912 |         -67.3544 |
[32m[20221214 14:22:37 @agent_ppo2.py:185][0m |          -0.0028 |         146.3338 |         -67.3026 |
[32m[20221214 14:22:37 @agent_ppo2.py:185][0m |          -0.0043 |         145.5587 |         -67.3402 |
[32m[20221214 14:22:37 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 14:22:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.14
[32m[20221214 14:22:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 751.85
[32m[20221214 14:22:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 738.03
[32m[20221214 14:22:37 @agent_ppo2.py:143][0m Total time:      24.58 min
[32m[20221214 14:22:37 @agent_ppo2.py:145][0m 2248704 total steps have happened
[32m[20221214 14:22:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1098 --------------------------#
[32m[20221214 14:22:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:37 @agent_ppo2.py:185][0m |          -0.0002 |         146.8007 |         -68.1751 |
[32m[20221214 14:22:37 @agent_ppo2.py:185][0m |          -0.0016 |         133.9651 |         -68.1347 |
[32m[20221214 14:22:38 @agent_ppo2.py:185][0m |          -0.0042 |         131.1703 |         -68.1273 |
[32m[20221214 14:22:38 @agent_ppo2.py:185][0m |          -0.0038 |         128.3318 |         -68.1554 |
[32m[20221214 14:22:38 @agent_ppo2.py:185][0m |          -0.0019 |         126.5175 |         -68.2035 |
[32m[20221214 14:22:38 @agent_ppo2.py:185][0m |          -0.0014 |         125.6081 |         -68.1051 |
[32m[20221214 14:22:38 @agent_ppo2.py:185][0m |          -0.0021 |         125.4226 |         -67.9205 |
[32m[20221214 14:22:38 @agent_ppo2.py:185][0m |          -0.0017 |         124.5755 |         -67.9642 |
[32m[20221214 14:22:38 @agent_ppo2.py:185][0m |           0.0021 |         127.9175 |         -68.0044 |
[32m[20221214 14:22:38 @agent_ppo2.py:185][0m |           0.0031 |         124.7813 |         -68.1223 |
[32m[20221214 14:22:38 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:22:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 730.79
[32m[20221214 14:22:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.08
[32m[20221214 14:22:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 754.36
[32m[20221214 14:22:38 @agent_ppo2.py:143][0m Total time:      24.61 min
[32m[20221214 14:22:38 @agent_ppo2.py:145][0m 2250752 total steps have happened
[32m[20221214 14:22:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1099 --------------------------#
[32m[20221214 14:22:39 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:22:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:39 @agent_ppo2.py:185][0m |          -0.0018 |         177.6580 |         -66.6572 |
[32m[20221214 14:22:39 @agent_ppo2.py:185][0m |          -0.0010 |         167.2639 |         -66.7113 |
[32m[20221214 14:22:39 @agent_ppo2.py:185][0m |          -0.0016 |         162.5927 |         -66.6102 |
[32m[20221214 14:22:39 @agent_ppo2.py:185][0m |          -0.0025 |         158.7595 |         -66.4754 |
[32m[20221214 14:22:39 @agent_ppo2.py:185][0m |          -0.0012 |         155.8836 |         -66.4336 |
[32m[20221214 14:22:39 @agent_ppo2.py:185][0m |          -0.0025 |         153.3106 |         -66.6304 |
[32m[20221214 14:22:39 @agent_ppo2.py:185][0m |          -0.0037 |         151.7871 |         -66.2166 |
[32m[20221214 14:22:39 @agent_ppo2.py:185][0m |          -0.0066 |         150.8309 |         -66.4998 |
[32m[20221214 14:22:40 @agent_ppo2.py:185][0m |          -0.0021 |         149.1848 |         -66.6541 |
[32m[20221214 14:22:40 @agent_ppo2.py:185][0m |          -0.0029 |         147.5966 |         -66.4653 |
[32m[20221214 14:22:40 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:22:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.05
[32m[20221214 14:22:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 757.09
[32m[20221214 14:22:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 749.92
[32m[20221214 14:22:40 @agent_ppo2.py:143][0m Total time:      24.63 min
[32m[20221214 14:22:40 @agent_ppo2.py:145][0m 2252800 total steps have happened
[32m[20221214 14:22:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1100 --------------------------#
[32m[20221214 14:22:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:22:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:40 @agent_ppo2.py:185][0m |           0.0072 |         172.3964 |         -67.1628 |
[32m[20221214 14:22:40 @agent_ppo2.py:185][0m |          -0.0034 |         154.3074 |         -67.0350 |
[32m[20221214 14:22:40 @agent_ppo2.py:185][0m |          -0.0019 |         149.4331 |         -67.0972 |
[32m[20221214 14:22:41 @agent_ppo2.py:185][0m |          -0.0053 |         146.3905 |         -66.9367 |
[32m[20221214 14:22:41 @agent_ppo2.py:185][0m |          -0.0029 |         143.7000 |         -66.9981 |
[32m[20221214 14:22:41 @agent_ppo2.py:185][0m |          -0.0038 |         141.2525 |         -67.1613 |
[32m[20221214 14:22:41 @agent_ppo2.py:185][0m |          -0.0037 |         139.6402 |         -67.0632 |
[32m[20221214 14:22:41 @agent_ppo2.py:185][0m |          -0.0044 |         137.7722 |         -66.9067 |
[32m[20221214 14:22:41 @agent_ppo2.py:185][0m |          -0.0015 |         136.7647 |         -67.0372 |
[32m[20221214 14:22:41 @agent_ppo2.py:185][0m |          -0.0032 |         135.8607 |         -66.8162 |
[32m[20221214 14:22:41 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:22:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 736.43
[32m[20221214 14:22:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.09
[32m[20221214 14:22:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.02
[32m[20221214 14:22:41 @agent_ppo2.py:143][0m Total time:      24.65 min
[32m[20221214 14:22:41 @agent_ppo2.py:145][0m 2254848 total steps have happened
[32m[20221214 14:22:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1101 --------------------------#
[32m[20221214 14:22:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:42 @agent_ppo2.py:185][0m |          -0.0013 |         188.8213 |         -67.5719 |
[32m[20221214 14:22:42 @agent_ppo2.py:185][0m |           0.0061 |         189.1154 |         -67.2486 |
[32m[20221214 14:22:42 @agent_ppo2.py:185][0m |           0.0024 |         184.4529 |         -67.2616 |
[32m[20221214 14:22:42 @agent_ppo2.py:185][0m |          -0.0025 |         180.0145 |         -67.1415 |
[32m[20221214 14:22:42 @agent_ppo2.py:185][0m |          -0.0022 |         178.9504 |         -67.1428 |
[32m[20221214 14:22:42 @agent_ppo2.py:185][0m |          -0.0006 |         177.7999 |         -67.1094 |
[32m[20221214 14:22:42 @agent_ppo2.py:185][0m |           0.0005 |         176.8051 |         -66.9762 |
[32m[20221214 14:22:42 @agent_ppo2.py:185][0m |           0.0018 |         177.9602 |         -67.0662 |
[32m[20221214 14:22:42 @agent_ppo2.py:185][0m |          -0.0023 |         175.5928 |         -66.9611 |
[32m[20221214 14:22:42 @agent_ppo2.py:185][0m |          -0.0020 |         174.5238 |         -66.9726 |
[32m[20221214 14:22:42 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:22:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 732.63
[32m[20221214 14:22:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 738.95
[32m[20221214 14:22:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.02
[32m[20221214 14:22:43 @agent_ppo2.py:143][0m Total time:      24.68 min
[32m[20221214 14:22:43 @agent_ppo2.py:145][0m 2256896 total steps have happened
[32m[20221214 14:22:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1102 --------------------------#
[32m[20221214 14:22:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:22:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:43 @agent_ppo2.py:185][0m |           0.0044 |         157.2581 |         -65.6415 |
[32m[20221214 14:22:43 @agent_ppo2.py:185][0m |          -0.0005 |         146.8287 |         -65.4424 |
[32m[20221214 14:22:43 @agent_ppo2.py:185][0m |          -0.0030 |         143.7634 |         -65.3278 |
[32m[20221214 14:22:43 @agent_ppo2.py:185][0m |          -0.0031 |         139.9662 |         -65.4453 |
[32m[20221214 14:22:43 @agent_ppo2.py:185][0m |          -0.0008 |         137.3683 |         -65.0647 |
[32m[20221214 14:22:43 @agent_ppo2.py:185][0m |          -0.0019 |         134.5852 |         -65.1942 |
[32m[20221214 14:22:43 @agent_ppo2.py:185][0m |           0.0099 |         149.7014 |         -65.0666 |
[32m[20221214 14:22:44 @agent_ppo2.py:185][0m |           0.0029 |         137.1560 |         -65.0058 |
[32m[20221214 14:22:44 @agent_ppo2.py:185][0m |          -0.0021 |         131.2446 |         -64.9054 |
[32m[20221214 14:22:44 @agent_ppo2.py:185][0m |          -0.0030 |         130.2619 |         -64.8681 |
[32m[20221214 14:22:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:22:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 741.68
[32m[20221214 14:22:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 746.00
[32m[20221214 14:22:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 738.25
[32m[20221214 14:22:44 @agent_ppo2.py:143][0m Total time:      24.70 min
[32m[20221214 14:22:44 @agent_ppo2.py:145][0m 2258944 total steps have happened
[32m[20221214 14:22:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1103 --------------------------#
[32m[20221214 14:22:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:44 @agent_ppo2.py:185][0m |           0.0019 |         197.4591 |         -64.7105 |
[32m[20221214 14:22:44 @agent_ppo2.py:185][0m |          -0.0009 |         189.1602 |         -64.7536 |
[32m[20221214 14:22:44 @agent_ppo2.py:185][0m |          -0.0003 |         184.0611 |         -64.9936 |
[32m[20221214 14:22:45 @agent_ppo2.py:185][0m |          -0.0003 |         180.7864 |         -65.1297 |
[32m[20221214 14:22:45 @agent_ppo2.py:185][0m |          -0.0034 |         176.7478 |         -65.0667 |
[32m[20221214 14:22:45 @agent_ppo2.py:185][0m |          -0.0015 |         174.8803 |         -65.1570 |
[32m[20221214 14:22:45 @agent_ppo2.py:185][0m |           0.0105 |         196.7261 |         -65.2241 |
[32m[20221214 14:22:45 @agent_ppo2.py:185][0m |           0.0001 |         172.9148 |         -64.9007 |
[32m[20221214 14:22:45 @agent_ppo2.py:185][0m |          -0.0018 |         170.6487 |         -65.3986 |
[32m[20221214 14:22:45 @agent_ppo2.py:185][0m |          -0.0021 |         168.9524 |         -65.5129 |
[32m[20221214 14:22:45 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:22:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 719.80
[32m[20221214 14:22:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 749.15
[32m[20221214 14:22:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 730.71
[32m[20221214 14:22:45 @agent_ppo2.py:143][0m Total time:      24.72 min
[32m[20221214 14:22:45 @agent_ppo2.py:145][0m 2260992 total steps have happened
[32m[20221214 14:22:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1104 --------------------------#
[32m[20221214 14:22:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:46 @agent_ppo2.py:185][0m |           0.0043 |         187.8524 |         -66.7980 |
[32m[20221214 14:22:46 @agent_ppo2.py:185][0m |           0.0006 |         169.0668 |         -66.9000 |
[32m[20221214 14:22:46 @agent_ppo2.py:185][0m |           0.0023 |         162.3086 |         -66.5819 |
[32m[20221214 14:22:46 @agent_ppo2.py:185][0m |           0.0043 |         159.2303 |         -67.0072 |
[32m[20221214 14:22:46 @agent_ppo2.py:185][0m |          -0.0032 |         155.8312 |         -66.7022 |
[32m[20221214 14:22:46 @agent_ppo2.py:185][0m |          -0.0016 |         153.7897 |         -67.1264 |
[32m[20221214 14:22:46 @agent_ppo2.py:185][0m |          -0.0008 |         152.1125 |         -67.1398 |
[32m[20221214 14:22:46 @agent_ppo2.py:185][0m |          -0.0040 |         150.9774 |         -67.2689 |
[32m[20221214 14:22:46 @agent_ppo2.py:185][0m |          -0.0015 |         149.2690 |         -66.9680 |
[32m[20221214 14:22:47 @agent_ppo2.py:185][0m |          -0.0046 |         148.6679 |         -67.5231 |
[32m[20221214 14:22:47 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:22:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 710.24
[32m[20221214 14:22:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.60
[32m[20221214 14:22:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 734.38
[32m[20221214 14:22:47 @agent_ppo2.py:143][0m Total time:      24.74 min
[32m[20221214 14:22:47 @agent_ppo2.py:145][0m 2263040 total steps have happened
[32m[20221214 14:22:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1105 --------------------------#
[32m[20221214 14:22:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:47 @agent_ppo2.py:185][0m |          -0.0027 |         160.1039 |         -67.3674 |
[32m[20221214 14:22:47 @agent_ppo2.py:185][0m |          -0.0039 |         134.0509 |         -67.0350 |
[32m[20221214 14:22:47 @agent_ppo2.py:185][0m |           0.0055 |         129.7647 |         -67.1219 |
[32m[20221214 14:22:47 @agent_ppo2.py:185][0m |          -0.0073 |         116.6646 |         -67.0937 |
[32m[20221214 14:22:47 @agent_ppo2.py:185][0m |           0.0057 |         128.7672 |         -67.1293 |
[32m[20221214 14:22:48 @agent_ppo2.py:185][0m |          -0.0044 |         108.3485 |         -67.0370 |
[32m[20221214 14:22:48 @agent_ppo2.py:185][0m |          -0.0054 |         105.8690 |         -67.0280 |
[32m[20221214 14:22:48 @agent_ppo2.py:185][0m |          -0.0065 |         103.8673 |         -66.9970 |
[32m[20221214 14:22:48 @agent_ppo2.py:185][0m |          -0.0058 |         102.7334 |         -66.8415 |
[32m[20221214 14:22:48 @agent_ppo2.py:185][0m |           0.0051 |         106.2508 |         -66.9054 |
[32m[20221214 14:22:48 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:22:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 690.51
[32m[20221214 14:22:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.34
[32m[20221214 14:22:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 740.10
[32m[20221214 14:22:48 @agent_ppo2.py:143][0m Total time:      24.77 min
[32m[20221214 14:22:48 @agent_ppo2.py:145][0m 2265088 total steps have happened
[32m[20221214 14:22:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1106 --------------------------#
[32m[20221214 14:22:48 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:22:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:48 @agent_ppo2.py:185][0m |          -0.0036 |         159.0992 |         -66.4020 |
[32m[20221214 14:22:49 @agent_ppo2.py:185][0m |          -0.0049 |         151.4830 |         -66.5913 |
[32m[20221214 14:22:49 @agent_ppo2.py:185][0m |           0.0095 |         167.4418 |         -66.3545 |
[32m[20221214 14:22:49 @agent_ppo2.py:185][0m |          -0.0046 |         144.5728 |         -66.3866 |
[32m[20221214 14:22:49 @agent_ppo2.py:185][0m |          -0.0039 |         141.7840 |         -66.4305 |
[32m[20221214 14:22:49 @agent_ppo2.py:185][0m |          -0.0036 |         138.9807 |         -66.3820 |
[32m[20221214 14:22:49 @agent_ppo2.py:185][0m |          -0.0051 |         137.1618 |         -66.3059 |
[32m[20221214 14:22:49 @agent_ppo2.py:185][0m |           0.0044 |         141.2079 |         -66.3038 |
[32m[20221214 14:22:49 @agent_ppo2.py:185][0m |           0.0132 |         153.5713 |         -66.2653 |
[32m[20221214 14:22:49 @agent_ppo2.py:185][0m |          -0.0063 |         132.6556 |         -66.2581 |
[32m[20221214 14:22:49 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:22:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 722.73
[32m[20221214 14:22:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 740.70
[32m[20221214 14:22:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 724.10
[32m[20221214 14:22:49 @agent_ppo2.py:143][0m Total time:      24.79 min
[32m[20221214 14:22:49 @agent_ppo2.py:145][0m 2267136 total steps have happened
[32m[20221214 14:22:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1107 --------------------------#
[32m[20221214 14:22:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:22:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:50 @agent_ppo2.py:185][0m |          -0.0012 |         159.9677 |         -66.0149 |
[32m[20221214 14:22:50 @agent_ppo2.py:185][0m |          -0.0039 |         136.7525 |         -65.7855 |
[32m[20221214 14:22:50 @agent_ppo2.py:185][0m |          -0.0020 |         127.2104 |         -65.6783 |
[32m[20221214 14:22:50 @agent_ppo2.py:185][0m |          -0.0014 |         122.7533 |         -65.7797 |
[32m[20221214 14:22:50 @agent_ppo2.py:185][0m |          -0.0034 |         119.8405 |         -65.8497 |
[32m[20221214 14:22:50 @agent_ppo2.py:185][0m |          -0.0074 |         116.9342 |         -65.5372 |
[32m[20221214 14:22:50 @agent_ppo2.py:185][0m |          -0.0076 |         113.9357 |         -65.5139 |
[32m[20221214 14:22:50 @agent_ppo2.py:185][0m |          -0.0076 |         112.0935 |         -65.5022 |
[32m[20221214 14:22:50 @agent_ppo2.py:185][0m |          -0.0051 |         110.6344 |         -65.5114 |
[32m[20221214 14:22:50 @agent_ppo2.py:185][0m |          -0.0029 |         109.5587 |         -65.4758 |
[32m[20221214 14:22:50 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:22:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 702.24
[32m[20221214 14:22:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.24
[32m[20221214 14:22:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 734.57
[32m[20221214 14:22:51 @agent_ppo2.py:143][0m Total time:      24.81 min
[32m[20221214 14:22:51 @agent_ppo2.py:145][0m 2269184 total steps have happened
[32m[20221214 14:22:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1108 --------------------------#
[32m[20221214 14:22:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:22:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:51 @agent_ppo2.py:185][0m |           0.0038 |         191.2162 |         -65.0199 |
[32m[20221214 14:22:51 @agent_ppo2.py:185][0m |          -0.0046 |         169.9449 |         -64.6834 |
[32m[20221214 14:22:51 @agent_ppo2.py:185][0m |           0.0088 |         165.0545 |         -64.8924 |
[32m[20221214 14:22:51 @agent_ppo2.py:185][0m |           0.0060 |         161.2240 |         -64.8511 |
[32m[20221214 14:22:51 @agent_ppo2.py:185][0m |           0.0019 |         157.3074 |         -64.8246 |
[32m[20221214 14:22:51 @agent_ppo2.py:185][0m |           0.0111 |         164.5395 |         -64.6385 |
[32m[20221214 14:22:51 @agent_ppo2.py:185][0m |          -0.0037 |         151.6596 |         -64.8207 |
[32m[20221214 14:22:52 @agent_ppo2.py:185][0m |           0.0001 |         151.2794 |         -64.7046 |
[32m[20221214 14:22:52 @agent_ppo2.py:185][0m |           0.0024 |         149.2311 |         -64.7985 |
[32m[20221214 14:22:52 @agent_ppo2.py:185][0m |          -0.0000 |         148.1689 |         -64.7157 |
[32m[20221214 14:22:52 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:22:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 710.99
[32m[20221214 14:22:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.45
[32m[20221214 14:22:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 728.48
[32m[20221214 14:22:52 @agent_ppo2.py:143][0m Total time:      24.83 min
[32m[20221214 14:22:52 @agent_ppo2.py:145][0m 2271232 total steps have happened
[32m[20221214 14:22:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1109 --------------------------#
[32m[20221214 14:22:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:22:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:52 @agent_ppo2.py:185][0m |          -0.0035 |         130.0202 |         -64.6521 |
[32m[20221214 14:22:52 @agent_ppo2.py:185][0m |          -0.0048 |         122.4666 |         -64.7005 |
[32m[20221214 14:22:52 @agent_ppo2.py:185][0m |           0.0013 |         119.7139 |         -64.7352 |
[32m[20221214 14:22:52 @agent_ppo2.py:185][0m |          -0.0014 |         117.2403 |         -64.9176 |
[32m[20221214 14:22:52 @agent_ppo2.py:185][0m |          -0.0033 |         115.1815 |         -64.8928 |
[32m[20221214 14:22:53 @agent_ppo2.py:185][0m |          -0.0011 |         113.8997 |         -64.8878 |
[32m[20221214 14:22:53 @agent_ppo2.py:185][0m |          -0.0023 |         113.0497 |         -64.9465 |
[32m[20221214 14:22:53 @agent_ppo2.py:185][0m |          -0.0052 |         112.1109 |         -64.9976 |
[32m[20221214 14:22:53 @agent_ppo2.py:185][0m |           0.0083 |         120.5570 |         -65.1546 |
[32m[20221214 14:22:53 @agent_ppo2.py:185][0m |           0.0014 |         110.8471 |         -65.2436 |
[32m[20221214 14:22:53 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:22:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 721.74
[32m[20221214 14:22:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 726.54
[32m[20221214 14:22:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 715.87
[32m[20221214 14:22:53 @agent_ppo2.py:143][0m Total time:      24.85 min
[32m[20221214 14:22:53 @agent_ppo2.py:145][0m 2273280 total steps have happened
[32m[20221214 14:22:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1110 --------------------------#
[32m[20221214 14:22:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:22:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:53 @agent_ppo2.py:185][0m |          -0.0002 |         185.8155 |         -66.7983 |
[32m[20221214 14:22:54 @agent_ppo2.py:185][0m |           0.0030 |         172.8455 |         -66.7263 |
[32m[20221214 14:22:54 @agent_ppo2.py:185][0m |          -0.0007 |         158.0047 |         -66.7061 |
[32m[20221214 14:22:54 @agent_ppo2.py:185][0m |          -0.0008 |         152.4923 |         -66.7291 |
[32m[20221214 14:22:54 @agent_ppo2.py:185][0m |          -0.0021 |         149.5629 |         -66.7214 |
[32m[20221214 14:22:54 @agent_ppo2.py:185][0m |          -0.0031 |         146.4252 |         -66.8866 |
[32m[20221214 14:22:54 @agent_ppo2.py:185][0m |          -0.0033 |         144.3025 |         -66.7663 |
[32m[20221214 14:22:54 @agent_ppo2.py:185][0m |          -0.0035 |         142.7313 |         -66.9215 |
[32m[20221214 14:22:54 @agent_ppo2.py:185][0m |          -0.0009 |         141.2411 |         -66.8796 |
[32m[20221214 14:22:54 @agent_ppo2.py:185][0m |          -0.0042 |         140.8514 |         -66.6810 |
[32m[20221214 14:22:54 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:22:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 699.85
[32m[20221214 14:22:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.99
[32m[20221214 14:22:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 727.85
[32m[20221214 14:22:54 @agent_ppo2.py:143][0m Total time:      24.87 min
[32m[20221214 14:22:54 @agent_ppo2.py:145][0m 2275328 total steps have happened
[32m[20221214 14:22:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1111 --------------------------#
[32m[20221214 14:22:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:22:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:55 @agent_ppo2.py:185][0m |           0.0094 |         174.1530 |         -66.6062 |
[32m[20221214 14:22:55 @agent_ppo2.py:185][0m |          -0.0017 |         143.8265 |         -66.5204 |
[32m[20221214 14:22:55 @agent_ppo2.py:185][0m |          -0.0041 |         138.7095 |         -66.4978 |
[32m[20221214 14:22:55 @agent_ppo2.py:185][0m |           0.0057 |         139.4067 |         -66.6356 |
[32m[20221214 14:22:55 @agent_ppo2.py:185][0m |          -0.0033 |         134.4464 |         -66.4647 |
[32m[20221214 14:22:55 @agent_ppo2.py:185][0m |           0.0004 |         135.1254 |         -66.8448 |
[32m[20221214 14:22:55 @agent_ppo2.py:185][0m |           0.0026 |         131.7842 |         -66.8979 |
[32m[20221214 14:22:56 @agent_ppo2.py:185][0m |          -0.0017 |         130.7063 |         -66.8603 |
[32m[20221214 14:22:56 @agent_ppo2.py:185][0m |          -0.0030 |         129.9734 |         -66.9789 |
[32m[20221214 14:22:56 @agent_ppo2.py:185][0m |          -0.0010 |         128.6537 |         -67.0765 |
[32m[20221214 14:22:56 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:22:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 708.72
[32m[20221214 14:22:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 724.40
[32m[20221214 14:22:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 716.88
[32m[20221214 14:22:56 @agent_ppo2.py:143][0m Total time:      24.90 min
[32m[20221214 14:22:56 @agent_ppo2.py:145][0m 2277376 total steps have happened
[32m[20221214 14:22:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1112 --------------------------#
[32m[20221214 14:22:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:56 @agent_ppo2.py:185][0m |           0.0129 |         154.7492 |         -68.0762 |
[32m[20221214 14:22:56 @agent_ppo2.py:185][0m |          -0.0025 |         128.7337 |         -68.0563 |
[32m[20221214 14:22:56 @agent_ppo2.py:185][0m |          -0.0004 |         123.1637 |         -67.9213 |
[32m[20221214 14:22:56 @agent_ppo2.py:185][0m |           0.0055 |         123.8917 |         -67.7915 |
[32m[20221214 14:22:57 @agent_ppo2.py:185][0m |          -0.0020 |         117.5423 |         -67.6140 |
[32m[20221214 14:22:57 @agent_ppo2.py:185][0m |          -0.0028 |         116.3906 |         -67.6289 |
[32m[20221214 14:22:57 @agent_ppo2.py:185][0m |          -0.0042 |         114.5477 |         -67.6648 |
[32m[20221214 14:22:57 @agent_ppo2.py:185][0m |          -0.0001 |         112.3589 |         -67.5708 |
[32m[20221214 14:22:57 @agent_ppo2.py:185][0m |          -0.0029 |         111.1259 |         -67.5058 |
[32m[20221214 14:22:57 @agent_ppo2.py:185][0m |          -0.0039 |         109.6648 |         -67.4801 |
[32m[20221214 14:22:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:22:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 705.17
[32m[20221214 14:22:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 708.24
[32m[20221214 14:22:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 708.02
[32m[20221214 14:22:57 @agent_ppo2.py:143][0m Total time:      24.92 min
[32m[20221214 14:22:57 @agent_ppo2.py:145][0m 2279424 total steps have happened
[32m[20221214 14:22:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1113 --------------------------#
[32m[20221214 14:22:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:58 @agent_ppo2.py:185][0m |          -0.0011 |         162.1602 |         -66.2567 |
[32m[20221214 14:22:58 @agent_ppo2.py:185][0m |          -0.0025 |         149.2571 |         -66.1290 |
[32m[20221214 14:22:58 @agent_ppo2.py:185][0m |          -0.0023 |         144.5246 |         -66.2345 |
[32m[20221214 14:22:58 @agent_ppo2.py:185][0m |          -0.0018 |         140.0139 |         -66.3373 |
[32m[20221214 14:22:58 @agent_ppo2.py:185][0m |          -0.0019 |         137.5686 |         -66.5140 |
[32m[20221214 14:22:58 @agent_ppo2.py:185][0m |           0.0026 |         137.2926 |         -66.3216 |
[32m[20221214 14:22:58 @agent_ppo2.py:185][0m |          -0.0015 |         133.3790 |         -66.4770 |
[32m[20221214 14:22:58 @agent_ppo2.py:185][0m |          -0.0019 |         131.6516 |         -66.3812 |
[32m[20221214 14:22:58 @agent_ppo2.py:185][0m |          -0.0051 |         130.3511 |         -66.4840 |
[32m[20221214 14:22:58 @agent_ppo2.py:185][0m |          -0.0011 |         129.0935 |         -66.1864 |
[32m[20221214 14:22:58 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:22:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 687.43
[32m[20221214 14:22:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.72
[32m[20221214 14:22:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 710.03
[32m[20221214 14:22:59 @agent_ppo2.py:143][0m Total time:      24.94 min
[32m[20221214 14:22:59 @agent_ppo2.py:145][0m 2281472 total steps have happened
[32m[20221214 14:22:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1114 --------------------------#
[32m[20221214 14:22:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:22:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:22:59 @agent_ppo2.py:185][0m |           0.0121 |         195.6526 |         -66.9691 |
[32m[20221214 14:22:59 @agent_ppo2.py:185][0m |           0.0090 |         179.5633 |         -66.5551 |
[32m[20221214 14:22:59 @agent_ppo2.py:185][0m |          -0.0026 |         164.3156 |         -66.6771 |
[32m[20221214 14:22:59 @agent_ppo2.py:185][0m |          -0.0024 |         162.3023 |         -66.8706 |
[32m[20221214 14:22:59 @agent_ppo2.py:185][0m |          -0.0018 |         161.4758 |         -66.7530 |
[32m[20221214 14:22:59 @agent_ppo2.py:185][0m |          -0.0037 |         160.7939 |         -66.7427 |
[32m[20221214 14:23:00 @agent_ppo2.py:185][0m |           0.0003 |         162.2827 |         -66.8592 |
[32m[20221214 14:23:00 @agent_ppo2.py:185][0m |           0.0098 |         180.4055 |         -66.6537 |
[32m[20221214 14:23:00 @agent_ppo2.py:185][0m |          -0.0041 |         160.1512 |         -66.7572 |
[32m[20221214 14:23:00 @agent_ppo2.py:185][0m |          -0.0065 |         160.0620 |         -66.8223 |
[32m[20221214 14:23:00 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:23:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 700.64
[32m[20221214 14:23:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 712.64
[32m[20221214 14:23:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 710.98
[32m[20221214 14:23:00 @agent_ppo2.py:143][0m Total time:      24.97 min
[32m[20221214 14:23:00 @agent_ppo2.py:145][0m 2283520 total steps have happened
[32m[20221214 14:23:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1115 --------------------------#
[32m[20221214 14:23:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:23:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:00 @agent_ppo2.py:185][0m |          -0.0021 |         129.1193 |         -66.2341 |
[32m[20221214 14:23:00 @agent_ppo2.py:185][0m |          -0.0020 |         116.8925 |         -66.2915 |
[32m[20221214 14:23:01 @agent_ppo2.py:185][0m |           0.0000 |         111.6955 |         -66.1943 |
[32m[20221214 14:23:01 @agent_ppo2.py:185][0m |          -0.0011 |         108.1289 |         -66.3190 |
[32m[20221214 14:23:01 @agent_ppo2.py:185][0m |          -0.0033 |         105.1291 |         -66.2792 |
[32m[20221214 14:23:01 @agent_ppo2.py:185][0m |          -0.0028 |         103.3419 |         -66.2766 |
[32m[20221214 14:23:01 @agent_ppo2.py:185][0m |          -0.0027 |         100.9082 |         -66.5029 |
[32m[20221214 14:23:01 @agent_ppo2.py:185][0m |          -0.0025 |          99.6396 |         -66.3580 |
[32m[20221214 14:23:01 @agent_ppo2.py:185][0m |          -0.0042 |          98.3816 |         -66.3675 |
[32m[20221214 14:23:01 @agent_ppo2.py:185][0m |          -0.0021 |          97.6566 |         -66.5913 |
[32m[20221214 14:23:01 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:23:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 681.76
[32m[20221214 14:23:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 704.63
[32m[20221214 14:23:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 714.56
[32m[20221214 14:23:01 @agent_ppo2.py:143][0m Total time:      24.99 min
[32m[20221214 14:23:01 @agent_ppo2.py:145][0m 2285568 total steps have happened
[32m[20221214 14:23:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1116 --------------------------#
[32m[20221214 14:23:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:23:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:02 @agent_ppo2.py:185][0m |          -0.0011 |         150.8381 |         -68.2072 |
[32m[20221214 14:23:02 @agent_ppo2.py:185][0m |          -0.0023 |         143.2935 |         -68.2750 |
[32m[20221214 14:23:02 @agent_ppo2.py:185][0m |          -0.0081 |         139.9731 |         -68.0863 |
[32m[20221214 14:23:02 @agent_ppo2.py:185][0m |           0.0013 |         138.5489 |         -68.4716 |
[32m[20221214 14:23:02 @agent_ppo2.py:185][0m |           0.0088 |         141.8295 |         -68.4028 |
[32m[20221214 14:23:02 @agent_ppo2.py:185][0m |          -0.0028 |         135.7364 |         -68.2880 |
[32m[20221214 14:23:02 @agent_ppo2.py:185][0m |          -0.0008 |         139.2967 |         -68.4579 |
[32m[20221214 14:23:02 @agent_ppo2.py:185][0m |          -0.0003 |         133.9296 |         -68.2677 |
[32m[20221214 14:23:02 @agent_ppo2.py:185][0m |           0.0036 |         138.8609 |         -68.2036 |
[32m[20221214 14:23:03 @agent_ppo2.py:185][0m |           0.0004 |         135.6231 |         -68.2912 |
[32m[20221214 14:23:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:23:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 684.22
[32m[20221214 14:23:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 707.32
[32m[20221214 14:23:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 718.05
[32m[20221214 14:23:03 @agent_ppo2.py:143][0m Total time:      25.01 min
[32m[20221214 14:23:03 @agent_ppo2.py:145][0m 2287616 total steps have happened
[32m[20221214 14:23:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1117 --------------------------#
[32m[20221214 14:23:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:03 @agent_ppo2.py:185][0m |          -0.0042 |         151.1138 |         -68.2805 |
[32m[20221214 14:23:03 @agent_ppo2.py:185][0m |          -0.0024 |         137.6906 |         -68.3246 |
[32m[20221214 14:23:03 @agent_ppo2.py:185][0m |          -0.0019 |         130.7053 |         -68.1995 |
[32m[20221214 14:23:03 @agent_ppo2.py:185][0m |          -0.0061 |         126.4932 |         -68.1680 |
[32m[20221214 14:23:03 @agent_ppo2.py:185][0m |          -0.0049 |         123.2594 |         -68.3259 |
[32m[20221214 14:23:03 @agent_ppo2.py:185][0m |          -0.0061 |         120.4209 |         -68.0973 |
[32m[20221214 14:23:04 @agent_ppo2.py:185][0m |           0.0012 |         121.6745 |         -68.2548 |
[32m[20221214 14:23:04 @agent_ppo2.py:185][0m |          -0.0031 |         117.9162 |         -68.1369 |
[32m[20221214 14:23:04 @agent_ppo2.py:185][0m |          -0.0055 |         115.9353 |         -68.0745 |
[32m[20221214 14:23:04 @agent_ppo2.py:185][0m |          -0.0051 |         115.1931 |         -68.1431 |
[32m[20221214 14:23:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:23:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 665.08
[32m[20221214 14:23:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 710.10
[32m[20221214 14:23:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 725.32
[32m[20221214 14:23:04 @agent_ppo2.py:143][0m Total time:      25.03 min
[32m[20221214 14:23:04 @agent_ppo2.py:145][0m 2289664 total steps have happened
[32m[20221214 14:23:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1118 --------------------------#
[32m[20221214 14:23:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:23:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:04 @agent_ppo2.py:185][0m |          -0.0011 |         160.1045 |         -67.6988 |
[32m[20221214 14:23:04 @agent_ppo2.py:185][0m |          -0.0026 |         149.6993 |         -67.6872 |
[32m[20221214 14:23:04 @agent_ppo2.py:185][0m |          -0.0018 |         146.4455 |         -67.6965 |
[32m[20221214 14:23:05 @agent_ppo2.py:185][0m |          -0.0006 |         144.8679 |         -67.8178 |
[32m[20221214 14:23:05 @agent_ppo2.py:185][0m |          -0.0018 |         143.5535 |         -67.7473 |
[32m[20221214 14:23:05 @agent_ppo2.py:185][0m |           0.0028 |         143.4774 |         -67.7150 |
[32m[20221214 14:23:05 @agent_ppo2.py:185][0m |          -0.0052 |         141.9671 |         -67.6523 |
[32m[20221214 14:23:05 @agent_ppo2.py:185][0m |          -0.0046 |         140.1636 |         -67.7897 |
[32m[20221214 14:23:05 @agent_ppo2.py:185][0m |          -0.0018 |         141.0423 |         -67.8000 |
[32m[20221214 14:23:05 @agent_ppo2.py:185][0m |           0.0028 |         145.0724 |         -67.8862 |
[32m[20221214 14:23:05 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:23:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 724.45
[32m[20221214 14:23:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 730.82
[32m[20221214 14:23:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 700.54
[32m[20221214 14:23:05 @agent_ppo2.py:143][0m Total time:      25.05 min
[32m[20221214 14:23:05 @agent_ppo2.py:145][0m 2291712 total steps have happened
[32m[20221214 14:23:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1119 --------------------------#
[32m[20221214 14:23:06 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:23:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:06 @agent_ppo2.py:185][0m |          -0.0026 |         131.7000 |         -67.8084 |
[32m[20221214 14:23:06 @agent_ppo2.py:185][0m |          -0.0019 |         117.9530 |         -67.4505 |
[32m[20221214 14:23:06 @agent_ppo2.py:185][0m |          -0.0022 |         112.4485 |         -67.5531 |
[32m[20221214 14:23:06 @agent_ppo2.py:185][0m |          -0.0022 |         109.2779 |         -67.4865 |
[32m[20221214 14:23:06 @agent_ppo2.py:185][0m |           0.0010 |         107.7886 |         -67.6146 |
[32m[20221214 14:23:06 @agent_ppo2.py:185][0m |          -0.0051 |         105.6117 |         -67.3581 |
[32m[20221214 14:23:06 @agent_ppo2.py:185][0m |          -0.0040 |         104.3087 |         -67.4165 |
[32m[20221214 14:23:06 @agent_ppo2.py:185][0m |          -0.0036 |         103.3410 |         -67.4445 |
[32m[20221214 14:23:06 @agent_ppo2.py:185][0m |          -0.0055 |         102.4285 |         -67.5019 |
[32m[20221214 14:23:07 @agent_ppo2.py:185][0m |          -0.0047 |         101.3844 |         -67.5300 |
[32m[20221214 14:23:07 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:23:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 705.22
[32m[20221214 14:23:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 722.43
[32m[20221214 14:23:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 678.09
[32m[20221214 14:23:07 @agent_ppo2.py:143][0m Total time:      25.08 min
[32m[20221214 14:23:07 @agent_ppo2.py:145][0m 2293760 total steps have happened
[32m[20221214 14:23:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1120 --------------------------#
[32m[20221214 14:23:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:23:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:07 @agent_ppo2.py:185][0m |          -0.0005 |         123.5274 |         -68.1684 |
[32m[20221214 14:23:07 @agent_ppo2.py:185][0m |           0.0041 |         107.7265 |         -68.2793 |
[32m[20221214 14:23:07 @agent_ppo2.py:185][0m |          -0.0042 |          94.8892 |         -68.3100 |
[32m[20221214 14:23:07 @agent_ppo2.py:185][0m |           0.0016 |          88.9701 |         -68.1739 |
[32m[20221214 14:23:07 @agent_ppo2.py:185][0m |          -0.0004 |          84.7755 |         -68.1169 |
[32m[20221214 14:23:08 @agent_ppo2.py:185][0m |          -0.0050 |          80.5862 |         -68.0288 |
[32m[20221214 14:23:08 @agent_ppo2.py:185][0m |          -0.0053 |          77.8070 |         -68.1795 |
[32m[20221214 14:23:08 @agent_ppo2.py:185][0m |          -0.0045 |          75.0198 |         -68.0804 |
[32m[20221214 14:23:08 @agent_ppo2.py:185][0m |          -0.0052 |          72.8729 |         -68.1629 |
[32m[20221214 14:23:08 @agent_ppo2.py:185][0m |          -0.0059 |          70.6887 |         -68.0311 |
[32m[20221214 14:23:08 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:23:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 638.50
[32m[20221214 14:23:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 683.43
[32m[20221214 14:23:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 683.89
[32m[20221214 14:23:08 @agent_ppo2.py:143][0m Total time:      25.10 min
[32m[20221214 14:23:08 @agent_ppo2.py:145][0m 2295808 total steps have happened
[32m[20221214 14:23:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1121 --------------------------#
[32m[20221214 14:23:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:23:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:08 @agent_ppo2.py:185][0m |           0.0009 |         119.9152 |         -67.8196 |
[32m[20221214 14:23:09 @agent_ppo2.py:185][0m |          -0.0020 |         107.6666 |         -67.7803 |
[32m[20221214 14:23:09 @agent_ppo2.py:185][0m |          -0.0012 |         102.2477 |         -67.7498 |
[32m[20221214 14:23:09 @agent_ppo2.py:185][0m |           0.0005 |          97.3687 |         -67.7385 |
[32m[20221214 14:23:09 @agent_ppo2.py:185][0m |           0.0015 |          93.5321 |         -67.9203 |
[32m[20221214 14:23:09 @agent_ppo2.py:185][0m |          -0.0001 |          89.8724 |         -67.7798 |
[32m[20221214 14:23:09 @agent_ppo2.py:185][0m |          -0.0047 |          87.3250 |         -67.9457 |
[32m[20221214 14:23:09 @agent_ppo2.py:185][0m |          -0.0027 |          85.6042 |         -67.8366 |
[32m[20221214 14:23:09 @agent_ppo2.py:185][0m |          -0.0034 |          83.2027 |         -67.6612 |
[32m[20221214 14:23:09 @agent_ppo2.py:185][0m |          -0.0012 |          81.6869 |         -67.8354 |
[32m[20221214 14:23:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:23:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 652.78
[32m[20221214 14:23:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 676.33
[32m[20221214 14:23:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 684.54
[32m[20221214 14:23:09 @agent_ppo2.py:143][0m Total time:      25.12 min
[32m[20221214 14:23:09 @agent_ppo2.py:145][0m 2297856 total steps have happened
[32m[20221214 14:23:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1122 --------------------------#
[32m[20221214 14:23:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:23:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:10 @agent_ppo2.py:185][0m |           0.0008 |         140.5153 |         -68.7169 |
[32m[20221214 14:23:10 @agent_ppo2.py:185][0m |          -0.0018 |         122.6487 |         -68.8795 |
[32m[20221214 14:23:10 @agent_ppo2.py:185][0m |          -0.0064 |         115.5410 |         -68.9388 |
[32m[20221214 14:23:10 @agent_ppo2.py:185][0m |          -0.0006 |         110.6251 |         -68.9201 |
[32m[20221214 14:23:10 @agent_ppo2.py:185][0m |          -0.0058 |         108.3876 |         -69.0516 |
[32m[20221214 14:23:10 @agent_ppo2.py:185][0m |           0.0110 |         115.5506 |         -69.0421 |
[32m[20221214 14:23:10 @agent_ppo2.py:185][0m |          -0.0043 |         104.4545 |         -69.1426 |
[32m[20221214 14:23:10 @agent_ppo2.py:185][0m |          -0.0030 |         102.9302 |         -69.1594 |
[32m[20221214 14:23:10 @agent_ppo2.py:185][0m |           0.0011 |         102.1301 |         -69.1805 |
[32m[20221214 14:23:10 @agent_ppo2.py:185][0m |          -0.0015 |         100.2187 |         -69.2177 |
[32m[20221214 14:23:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:23:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 652.88
[32m[20221214 14:23:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 678.58
[32m[20221214 14:23:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 674.67
[32m[20221214 14:23:11 @agent_ppo2.py:143][0m Total time:      25.14 min
[32m[20221214 14:23:11 @agent_ppo2.py:145][0m 2299904 total steps have happened
[32m[20221214 14:23:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1123 --------------------------#
[32m[20221214 14:23:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:23:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:11 @agent_ppo2.py:185][0m |          -0.0057 |         129.0997 |         -69.1754 |
[32m[20221214 14:23:11 @agent_ppo2.py:185][0m |           0.0105 |         116.0644 |         -69.0303 |
[32m[20221214 14:23:11 @agent_ppo2.py:185][0m |          -0.0020 |         103.1970 |         -68.8046 |
[32m[20221214 14:23:11 @agent_ppo2.py:185][0m |          -0.0094 |          98.6377 |         -68.9837 |
[32m[20221214 14:23:11 @agent_ppo2.py:185][0m |           0.0026 |          97.0752 |         -68.8158 |
[32m[20221214 14:23:11 @agent_ppo2.py:185][0m |          -0.0019 |          92.9744 |         -68.6660 |
[32m[20221214 14:23:12 @agent_ppo2.py:185][0m |          -0.0011 |          92.0291 |         -68.7429 |
[32m[20221214 14:23:12 @agent_ppo2.py:185][0m |           0.0018 |          92.0331 |         -68.6617 |
[32m[20221214 14:23:12 @agent_ppo2.py:185][0m |          -0.0007 |          87.6496 |         -68.5359 |
[32m[20221214 14:23:12 @agent_ppo2.py:185][0m |          -0.0031 |          86.0977 |         -68.6508 |
[32m[20221214 14:23:12 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:23:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 642.32
[32m[20221214 14:23:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 678.68
[32m[20221214 14:23:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.88
[32m[20221214 14:23:12 @agent_ppo2.py:143][0m Total time:      25.16 min
[32m[20221214 14:23:12 @agent_ppo2.py:145][0m 2301952 total steps have happened
[32m[20221214 14:23:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1124 --------------------------#
[32m[20221214 14:23:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:23:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:12 @agent_ppo2.py:185][0m |          -0.0020 |         120.9951 |         -68.7093 |
[32m[20221214 14:23:12 @agent_ppo2.py:185][0m |          -0.0008 |         107.3385 |         -68.6190 |
[32m[20221214 14:23:12 @agent_ppo2.py:185][0m |          -0.0017 |         101.0243 |         -68.7948 |
[32m[20221214 14:23:13 @agent_ppo2.py:185][0m |           0.0034 |          99.1408 |         -68.6011 |
[32m[20221214 14:23:13 @agent_ppo2.py:185][0m |          -0.0041 |          94.1993 |         -68.5474 |
[32m[20221214 14:23:13 @agent_ppo2.py:185][0m |          -0.0011 |          91.8145 |         -68.6658 |
[32m[20221214 14:23:13 @agent_ppo2.py:185][0m |          -0.0027 |          90.0038 |         -68.6919 |
[32m[20221214 14:23:13 @agent_ppo2.py:185][0m |          -0.0010 |          88.3012 |         -68.4116 |
[32m[20221214 14:23:13 @agent_ppo2.py:185][0m |          -0.0024 |          86.8099 |         -68.6950 |
[32m[20221214 14:23:13 @agent_ppo2.py:185][0m |           0.0104 |          97.5771 |         -68.5343 |
[32m[20221214 14:23:13 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:23:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 706.20
[32m[20221214 14:23:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 729.14
[32m[20221214 14:23:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 701.21
[32m[20221214 14:23:13 @agent_ppo2.py:143][0m Total time:      25.19 min
[32m[20221214 14:23:13 @agent_ppo2.py:145][0m 2304000 total steps have happened
[32m[20221214 14:23:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1125 --------------------------#
[32m[20221214 14:23:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:23:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:14 @agent_ppo2.py:185][0m |           0.0048 |          98.3162 |         -69.1425 |
[32m[20221214 14:23:14 @agent_ppo2.py:185][0m |          -0.0030 |          87.6615 |         -68.9968 |
[32m[20221214 14:23:14 @agent_ppo2.py:185][0m |          -0.0030 |          83.0508 |         -69.1358 |
[32m[20221214 14:23:14 @agent_ppo2.py:185][0m |          -0.0012 |          80.4947 |         -69.1295 |
[32m[20221214 14:23:14 @agent_ppo2.py:185][0m |          -0.0058 |          78.6476 |         -68.8799 |
[32m[20221214 14:23:14 @agent_ppo2.py:185][0m |           0.0077 |          79.6665 |         -68.9690 |
[32m[20221214 14:23:14 @agent_ppo2.py:185][0m |          -0.0046 |          76.0226 |         -69.0866 |
[32m[20221214 14:23:14 @agent_ppo2.py:185][0m |          -0.0049 |          75.1453 |         -69.1364 |
[32m[20221214 14:23:14 @agent_ppo2.py:185][0m |          -0.0015 |          73.7964 |         -69.3041 |
[32m[20221214 14:23:14 @agent_ppo2.py:185][0m |          -0.0049 |          73.3342 |         -69.1857 |
[32m[20221214 14:23:14 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:23:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 670.98
[32m[20221214 14:23:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 722.99
[32m[20221214 14:23:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 686.87
[32m[20221214 14:23:15 @agent_ppo2.py:143][0m Total time:      25.21 min
[32m[20221214 14:23:15 @agent_ppo2.py:145][0m 2306048 total steps have happened
[32m[20221214 14:23:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1126 --------------------------#
[32m[20221214 14:23:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:15 @agent_ppo2.py:185][0m |          -0.0054 |          95.9286 |         -70.5815 |
[32m[20221214 14:23:15 @agent_ppo2.py:185][0m |          -0.0021 |          85.5867 |         -70.6312 |
[32m[20221214 14:23:15 @agent_ppo2.py:185][0m |          -0.0046 |          81.6684 |         -70.4521 |
[32m[20221214 14:23:15 @agent_ppo2.py:185][0m |          -0.0057 |          79.0338 |         -70.5186 |
[32m[20221214 14:23:15 @agent_ppo2.py:185][0m |          -0.0027 |          78.7308 |         -70.4283 |
[32m[20221214 14:23:15 @agent_ppo2.py:185][0m |          -0.0061 |          76.5096 |         -70.4765 |
[32m[20221214 14:23:15 @agent_ppo2.py:185][0m |           0.0054 |          78.7972 |         -70.3668 |
[32m[20221214 14:23:16 @agent_ppo2.py:185][0m |           0.0009 |          74.4925 |         -70.3045 |
[32m[20221214 14:23:16 @agent_ppo2.py:185][0m |          -0.0044 |          74.3994 |         -70.2853 |
[32m[20221214 14:23:16 @agent_ppo2.py:185][0m |          -0.0023 |          73.6224 |         -70.1073 |
[32m[20221214 14:23:16 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:23:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 709.92
[32m[20221214 14:23:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 737.29
[32m[20221214 14:23:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 618.95
[32m[20221214 14:23:16 @agent_ppo2.py:143][0m Total time:      25.23 min
[32m[20221214 14:23:16 @agent_ppo2.py:145][0m 2308096 total steps have happened
[32m[20221214 14:23:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1127 --------------------------#
[32m[20221214 14:23:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:16 @agent_ppo2.py:185][0m |          -0.0005 |          96.3036 |         -67.9320 |
[32m[20221214 14:23:16 @agent_ppo2.py:185][0m |          -0.0018 |          81.3622 |         -67.8995 |
[32m[20221214 14:23:16 @agent_ppo2.py:185][0m |           0.0029 |          73.9225 |         -67.8321 |
[32m[20221214 14:23:16 @agent_ppo2.py:185][0m |          -0.0026 |          69.4961 |         -67.9196 |
[32m[20221214 14:23:17 @agent_ppo2.py:185][0m |          -0.0014 |          65.4156 |         -68.0077 |
[32m[20221214 14:23:17 @agent_ppo2.py:185][0m |          -0.0041 |          63.2322 |         -67.6985 |
[32m[20221214 14:23:17 @agent_ppo2.py:185][0m |          -0.0016 |          61.3832 |         -67.7250 |
[32m[20221214 14:23:17 @agent_ppo2.py:185][0m |          -0.0058 |          59.1824 |         -67.6949 |
[32m[20221214 14:23:17 @agent_ppo2.py:185][0m |          -0.0071 |          58.0270 |         -67.7393 |
[32m[20221214 14:23:17 @agent_ppo2.py:185][0m |          -0.0039 |          56.4243 |         -67.5845 |
[32m[20221214 14:23:17 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:23:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 658.71
[32m[20221214 14:23:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 688.43
[32m[20221214 14:23:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 719.27
[32m[20221214 14:23:17 @agent_ppo2.py:143][0m Total time:      25.25 min
[32m[20221214 14:23:17 @agent_ppo2.py:145][0m 2310144 total steps have happened
[32m[20221214 14:23:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1128 --------------------------#
[32m[20221214 14:23:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:18 @agent_ppo2.py:185][0m |          -0.0045 |         121.8870 |         -67.0340 |
[32m[20221214 14:23:18 @agent_ppo2.py:185][0m |          -0.0065 |         105.1248 |         -67.0401 |
[32m[20221214 14:23:18 @agent_ppo2.py:185][0m |          -0.0050 |          99.4339 |         -66.9896 |
[32m[20221214 14:23:18 @agent_ppo2.py:185][0m |          -0.0027 |          96.7675 |         -67.0066 |
[32m[20221214 14:23:18 @agent_ppo2.py:185][0m |          -0.0042 |          94.4021 |         -66.8894 |
[32m[20221214 14:23:18 @agent_ppo2.py:185][0m |          -0.0026 |          93.1082 |         -67.1118 |
[32m[20221214 14:23:18 @agent_ppo2.py:185][0m |          -0.0039 |          91.4241 |         -67.0107 |
[32m[20221214 14:23:18 @agent_ppo2.py:185][0m |          -0.0048 |          90.2198 |         -66.9984 |
[32m[20221214 14:23:18 @agent_ppo2.py:185][0m |          -0.0050 |          89.2662 |         -67.0673 |
[32m[20221214 14:23:18 @agent_ppo2.py:185][0m |           0.0077 |          95.0179 |         -67.1803 |
[32m[20221214 14:23:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:23:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 682.96
[32m[20221214 14:23:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 720.87
[32m[20221214 14:23:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 728.44
[32m[20221214 14:23:18 @agent_ppo2.py:143][0m Total time:      25.27 min
[32m[20221214 14:23:18 @agent_ppo2.py:145][0m 2312192 total steps have happened
[32m[20221214 14:23:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1129 --------------------------#
[32m[20221214 14:23:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:19 @agent_ppo2.py:185][0m |           0.0049 |         145.5259 |         -69.9643 |
[32m[20221214 14:23:19 @agent_ppo2.py:185][0m |           0.0033 |         127.5097 |         -69.8011 |
[32m[20221214 14:23:19 @agent_ppo2.py:185][0m |          -0.0026 |         120.2308 |         -69.9785 |
[32m[20221214 14:23:19 @agent_ppo2.py:185][0m |           0.0010 |         115.9510 |         -70.0083 |
[32m[20221214 14:23:19 @agent_ppo2.py:185][0m |          -0.0016 |         112.7489 |         -70.0954 |
[32m[20221214 14:23:19 @agent_ppo2.py:185][0m |           0.0027 |         113.2717 |         -70.1723 |
[32m[20221214 14:23:19 @agent_ppo2.py:185][0m |           0.0031 |         112.5962 |         -70.3427 |
[32m[20221214 14:23:19 @agent_ppo2.py:185][0m |          -0.0012 |         107.5886 |         -70.3898 |
[32m[20221214 14:23:19 @agent_ppo2.py:185][0m |           0.0035 |         107.2057 |         -70.3786 |
[32m[20221214 14:23:19 @agent_ppo2.py:185][0m |          -0.0019 |         106.1254 |         -70.3737 |
[32m[20221214 14:23:19 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:23:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 711.36
[32m[20221214 14:23:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.90
[32m[20221214 14:23:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 723.31
[32m[20221214 14:23:20 @agent_ppo2.py:143][0m Total time:      25.29 min
[32m[20221214 14:23:20 @agent_ppo2.py:145][0m 2314240 total steps have happened
[32m[20221214 14:23:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1130 --------------------------#
[32m[20221214 14:23:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:23:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:20 @agent_ppo2.py:185][0m |          -0.0015 |         176.6873 |         -68.7562 |
[32m[20221214 14:23:20 @agent_ppo2.py:185][0m |           0.0012 |         166.8216 |         -68.8967 |
[32m[20221214 14:23:20 @agent_ppo2.py:185][0m |           0.0005 |         162.1903 |         -68.9159 |
[32m[20221214 14:23:20 @agent_ppo2.py:185][0m |           0.0011 |         160.3231 |         -69.0232 |
[32m[20221214 14:23:20 @agent_ppo2.py:185][0m |          -0.0026 |         157.5339 |         -69.0615 |
[32m[20221214 14:23:20 @agent_ppo2.py:185][0m |          -0.0015 |         156.1899 |         -68.9050 |
[32m[20221214 14:23:21 @agent_ppo2.py:185][0m |           0.0003 |         155.9828 |         -69.0732 |
[32m[20221214 14:23:21 @agent_ppo2.py:185][0m |           0.0021 |         158.9076 |         -68.9071 |
[32m[20221214 14:23:21 @agent_ppo2.py:185][0m |          -0.0047 |         154.2932 |         -68.9523 |
[32m[20221214 14:23:21 @agent_ppo2.py:185][0m |           0.0015 |         153.5553 |         -68.9864 |
[32m[20221214 14:23:21 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:23:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 714.90
[32m[20221214 14:23:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.43
[32m[20221214 14:23:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 725.71
[32m[20221214 14:23:21 @agent_ppo2.py:143][0m Total time:      25.31 min
[32m[20221214 14:23:21 @agent_ppo2.py:145][0m 2316288 total steps have happened
[32m[20221214 14:23:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1131 --------------------------#
[32m[20221214 14:23:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:21 @agent_ppo2.py:185][0m |           0.0092 |         155.7720 |         -71.9581 |
[32m[20221214 14:23:21 @agent_ppo2.py:185][0m |          -0.0078 |         121.3493 |         -71.8687 |
[32m[20221214 14:23:21 @agent_ppo2.py:185][0m |          -0.0020 |         113.3771 |         -71.7655 |
[32m[20221214 14:23:21 @agent_ppo2.py:185][0m |          -0.0040 |         108.0242 |         -71.8634 |
[32m[20221214 14:23:22 @agent_ppo2.py:185][0m |          -0.0049 |         104.6520 |         -71.8429 |
[32m[20221214 14:23:22 @agent_ppo2.py:185][0m |          -0.0040 |         102.4488 |         -71.8322 |
[32m[20221214 14:23:22 @agent_ppo2.py:185][0m |          -0.0054 |         100.8179 |         -71.8096 |
[32m[20221214 14:23:22 @agent_ppo2.py:185][0m |           0.0023 |         105.6413 |         -71.8773 |
[32m[20221214 14:23:22 @agent_ppo2.py:185][0m |           0.0013 |          98.0721 |         -71.8920 |
[32m[20221214 14:23:22 @agent_ppo2.py:185][0m |          -0.0036 |          96.2620 |         -71.9220 |
[32m[20221214 14:23:22 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:23:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 675.97
[32m[20221214 14:23:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 718.72
[32m[20221214 14:23:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 735.19
[32m[20221214 14:23:22 @agent_ppo2.py:143][0m Total time:      25.33 min
[32m[20221214 14:23:22 @agent_ppo2.py:145][0m 2318336 total steps have happened
[32m[20221214 14:23:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1132 --------------------------#
[32m[20221214 14:23:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:22 @agent_ppo2.py:185][0m |           0.0108 |         128.4890 |         -70.1652 |
[32m[20221214 14:23:23 @agent_ppo2.py:185][0m |          -0.0063 |          98.5477 |         -70.1774 |
[32m[20221214 14:23:23 @agent_ppo2.py:185][0m |          -0.0040 |          91.0684 |         -70.2515 |
[32m[20221214 14:23:23 @agent_ppo2.py:185][0m |          -0.0079 |          86.5724 |         -70.1561 |
[32m[20221214 14:23:23 @agent_ppo2.py:185][0m |          -0.0024 |          83.4757 |         -70.0913 |
[32m[20221214 14:23:23 @agent_ppo2.py:185][0m |           0.0012 |          82.0853 |         -69.8077 |
[32m[20221214 14:23:23 @agent_ppo2.py:185][0m |          -0.0043 |          79.3529 |         -69.7753 |
[32m[20221214 14:23:23 @agent_ppo2.py:185][0m |          -0.0063 |          77.8786 |         -69.7624 |
[32m[20221214 14:23:23 @agent_ppo2.py:185][0m |          -0.0077 |          76.6456 |         -69.9411 |
[32m[20221214 14:23:23 @agent_ppo2.py:185][0m |          -0.0044 |          75.2000 |         -69.5362 |
[32m[20221214 14:23:23 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:23:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 721.72
[32m[20221214 14:23:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.49
[32m[20221214 14:23:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 733.41
[32m[20221214 14:23:23 @agent_ppo2.py:143][0m Total time:      25.35 min
[32m[20221214 14:23:23 @agent_ppo2.py:145][0m 2320384 total steps have happened
[32m[20221214 14:23:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1133 --------------------------#
[32m[20221214 14:23:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:23:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:24 @agent_ppo2.py:185][0m |          -0.0003 |         120.2493 |         -71.4268 |
[32m[20221214 14:23:24 @agent_ppo2.py:185][0m |          -0.0020 |         110.5543 |         -71.5436 |
[32m[20221214 14:23:24 @agent_ppo2.py:185][0m |          -0.0012 |         106.9296 |         -71.3749 |
[32m[20221214 14:23:24 @agent_ppo2.py:185][0m |           0.0006 |         103.9136 |         -71.4060 |
[32m[20221214 14:23:24 @agent_ppo2.py:185][0m |           0.0025 |         101.3012 |         -71.5296 |
[32m[20221214 14:23:24 @agent_ppo2.py:185][0m |          -0.0020 |         100.0676 |         -71.3289 |
[32m[20221214 14:23:24 @agent_ppo2.py:185][0m |           0.0010 |          98.8932 |         -71.1204 |
[32m[20221214 14:23:24 @agent_ppo2.py:185][0m |          -0.0036 |          97.9772 |         -71.1667 |
[32m[20221214 14:23:24 @agent_ppo2.py:185][0m |           0.0002 |          96.7397 |         -71.1299 |
[32m[20221214 14:23:24 @agent_ppo2.py:185][0m |           0.0221 |         112.0387 |         -70.9719 |
[32m[20221214 14:23:24 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:23:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.81
[32m[20221214 14:23:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 731.20
[32m[20221214 14:23:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 730.36
[32m[20221214 14:23:25 @agent_ppo2.py:143][0m Total time:      25.38 min
[32m[20221214 14:23:25 @agent_ppo2.py:145][0m 2322432 total steps have happened
[32m[20221214 14:23:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1134 --------------------------#
[32m[20221214 14:23:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:25 @agent_ppo2.py:185][0m |           0.0065 |         126.3993 |         -69.4069 |
[32m[20221214 14:23:25 @agent_ppo2.py:185][0m |          -0.0037 |         110.4910 |         -69.3500 |
[32m[20221214 14:23:25 @agent_ppo2.py:185][0m |          -0.0033 |         107.6436 |         -69.1915 |
[32m[20221214 14:23:25 @agent_ppo2.py:185][0m |          -0.0010 |         106.1856 |         -69.3100 |
[32m[20221214 14:23:25 @agent_ppo2.py:185][0m |          -0.0029 |         103.4946 |         -69.5389 |
[32m[20221214 14:23:25 @agent_ppo2.py:185][0m |          -0.0034 |         102.4903 |         -69.5509 |
[32m[20221214 14:23:25 @agent_ppo2.py:185][0m |           0.0006 |         101.2014 |         -69.5906 |
[32m[20221214 14:23:26 @agent_ppo2.py:185][0m |          -0.0022 |          99.4336 |         -69.5240 |
[32m[20221214 14:23:26 @agent_ppo2.py:185][0m |           0.0041 |         101.9979 |         -69.4999 |
[32m[20221214 14:23:26 @agent_ppo2.py:185][0m |          -0.0016 |          97.5519 |         -69.3611 |
[32m[20221214 14:23:26 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:23:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 711.87
[32m[20221214 14:23:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 731.30
[32m[20221214 14:23:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 687.19
[32m[20221214 14:23:26 @agent_ppo2.py:143][0m Total time:      25.40 min
[32m[20221214 14:23:26 @agent_ppo2.py:145][0m 2324480 total steps have happened
[32m[20221214 14:23:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1135 --------------------------#
[32m[20221214 14:23:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:26 @agent_ppo2.py:185][0m |          -0.0023 |         174.8697 |         -69.7680 |
[32m[20221214 14:23:26 @agent_ppo2.py:185][0m |           0.0020 |         145.6053 |         -69.6479 |
[32m[20221214 14:23:26 @agent_ppo2.py:185][0m |          -0.0065 |         135.8763 |         -69.2256 |
[32m[20221214 14:23:26 @agent_ppo2.py:185][0m |          -0.0039 |         130.7830 |         -69.3665 |
[32m[20221214 14:23:27 @agent_ppo2.py:185][0m |          -0.0013 |         128.0810 |         -69.4540 |
[32m[20221214 14:23:27 @agent_ppo2.py:185][0m |          -0.0020 |         124.4034 |         -69.4370 |
[32m[20221214 14:23:27 @agent_ppo2.py:185][0m |          -0.0046 |         122.7638 |         -69.3443 |
[32m[20221214 14:23:27 @agent_ppo2.py:185][0m |          -0.0043 |         121.3985 |         -69.2182 |
[32m[20221214 14:23:27 @agent_ppo2.py:185][0m |          -0.0090 |         119.7616 |         -69.3952 |
[32m[20221214 14:23:27 @agent_ppo2.py:185][0m |           0.0087 |         132.4727 |         -69.3364 |
[32m[20221214 14:23:27 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:23:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 674.89
[32m[20221214 14:23:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 727.18
[32m[20221214 14:23:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 735.55
[32m[20221214 14:23:27 @agent_ppo2.py:143][0m Total time:      25.42 min
[32m[20221214 14:23:27 @agent_ppo2.py:145][0m 2326528 total steps have happened
[32m[20221214 14:23:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1136 --------------------------#
[32m[20221214 14:23:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:27 @agent_ppo2.py:185][0m |           0.0070 |         147.9702 |         -70.3311 |
[32m[20221214 14:23:28 @agent_ppo2.py:185][0m |          -0.0015 |         121.7416 |         -70.0703 |
[32m[20221214 14:23:28 @agent_ppo2.py:185][0m |           0.0017 |         112.9148 |         -70.3244 |
[32m[20221214 14:23:28 @agent_ppo2.py:185][0m |          -0.0026 |         107.6378 |         -70.2938 |
[32m[20221214 14:23:28 @agent_ppo2.py:185][0m |           0.0067 |         105.1127 |         -70.3939 |
[32m[20221214 14:23:28 @agent_ppo2.py:185][0m |          -0.0026 |         102.1788 |         -70.2500 |
[32m[20221214 14:23:28 @agent_ppo2.py:185][0m |          -0.0016 |         100.2470 |         -70.4831 |
[32m[20221214 14:23:28 @agent_ppo2.py:185][0m |          -0.0036 |          98.6150 |         -70.4051 |
[32m[20221214 14:23:28 @agent_ppo2.py:185][0m |          -0.0065 |          97.6255 |         -70.4674 |
[32m[20221214 14:23:28 @agent_ppo2.py:185][0m |          -0.0045 |          96.1213 |         -70.3436 |
[32m[20221214 14:23:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:23:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 699.82
[32m[20221214 14:23:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.72
[32m[20221214 14:23:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 737.72
[32m[20221214 14:23:28 @agent_ppo2.py:143][0m Total time:      25.44 min
[32m[20221214 14:23:28 @agent_ppo2.py:145][0m 2328576 total steps have happened
[32m[20221214 14:23:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1137 --------------------------#
[32m[20221214 14:23:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:29 @agent_ppo2.py:185][0m |          -0.0014 |         123.6956 |         -68.9170 |
[32m[20221214 14:23:29 @agent_ppo2.py:185][0m |          -0.0014 |         114.9443 |         -68.8907 |
[32m[20221214 14:23:29 @agent_ppo2.py:185][0m |          -0.0043 |         110.7250 |         -68.6535 |
[32m[20221214 14:23:29 @agent_ppo2.py:185][0m |          -0.0002 |         108.8123 |         -68.7821 |
[32m[20221214 14:23:29 @agent_ppo2.py:185][0m |          -0.0051 |         108.0378 |         -68.5599 |
[32m[20221214 14:23:29 @agent_ppo2.py:185][0m |          -0.0044 |         106.5800 |         -68.6386 |
[32m[20221214 14:23:29 @agent_ppo2.py:185][0m |          -0.0020 |         105.3866 |         -68.5527 |
[32m[20221214 14:23:29 @agent_ppo2.py:185][0m |           0.0081 |         112.1418 |         -68.5581 |
[32m[20221214 14:23:29 @agent_ppo2.py:185][0m |          -0.0039 |         104.0575 |         -68.4541 |
[32m[20221214 14:23:29 @agent_ppo2.py:185][0m |          -0.0028 |         103.2552 |         -68.4589 |
[32m[20221214 14:23:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:23:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 719.26
[32m[20221214 14:23:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 738.60
[32m[20221214 14:23:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.77
[32m[20221214 14:23:30 @agent_ppo2.py:143][0m Total time:      25.46 min
[32m[20221214 14:23:30 @agent_ppo2.py:145][0m 2330624 total steps have happened
[32m[20221214 14:23:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1138 --------------------------#
[32m[20221214 14:23:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:30 @agent_ppo2.py:185][0m |          -0.0016 |         103.5158 |         -69.5656 |
[32m[20221214 14:23:30 @agent_ppo2.py:185][0m |          -0.0030 |          92.9365 |         -69.5066 |
[32m[20221214 14:23:30 @agent_ppo2.py:185][0m |           0.0100 |          97.3505 |         -69.5211 |
[32m[20221214 14:23:30 @agent_ppo2.py:185][0m |          -0.0025 |          87.1280 |         -69.4862 |
[32m[20221214 14:23:30 @agent_ppo2.py:185][0m |          -0.0032 |          85.6631 |         -69.5468 |
[32m[20221214 14:23:30 @agent_ppo2.py:185][0m |           0.0048 |          86.9809 |         -69.4188 |
[32m[20221214 14:23:30 @agent_ppo2.py:185][0m |          -0.0007 |          82.9751 |         -69.3643 |
[32m[20221214 14:23:31 @agent_ppo2.py:185][0m |           0.0047 |          84.6665 |         -69.2445 |
[32m[20221214 14:23:31 @agent_ppo2.py:185][0m |           0.0050 |          82.8634 |         -69.3414 |
[32m[20221214 14:23:31 @agent_ppo2.py:185][0m |           0.0004 |          80.8110 |         -69.2695 |
[32m[20221214 14:23:31 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:23:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 683.82
[32m[20221214 14:23:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 730.62
[32m[20221214 14:23:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 736.32
[32m[20221214 14:23:31 @agent_ppo2.py:143][0m Total time:      25.48 min
[32m[20221214 14:23:31 @agent_ppo2.py:145][0m 2332672 total steps have happened
[32m[20221214 14:23:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1139 --------------------------#
[32m[20221214 14:23:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:31 @agent_ppo2.py:185][0m |           0.0019 |         110.8799 |         -70.0152 |
[32m[20221214 14:23:31 @agent_ppo2.py:185][0m |          -0.0010 |          92.4651 |         -69.9701 |
[32m[20221214 14:23:31 @agent_ppo2.py:185][0m |          -0.0043 |          84.4924 |         -69.9756 |
[32m[20221214 14:23:31 @agent_ppo2.py:185][0m |          -0.0023 |          79.8301 |         -69.9667 |
[32m[20221214 14:23:31 @agent_ppo2.py:185][0m |          -0.0018 |          77.6460 |         -69.9104 |
[32m[20221214 14:23:32 @agent_ppo2.py:185][0m |           0.0004 |          76.4402 |         -70.0583 |
[32m[20221214 14:23:32 @agent_ppo2.py:185][0m |          -0.0043 |          73.7317 |         -70.0355 |
[32m[20221214 14:23:32 @agent_ppo2.py:185][0m |          -0.0032 |          73.0897 |         -70.0115 |
[32m[20221214 14:23:32 @agent_ppo2.py:185][0m |           0.0016 |          72.1659 |         -70.1577 |
[32m[20221214 14:23:32 @agent_ppo2.py:185][0m |           0.0062 |          80.2872 |         -70.1860 |
[32m[20221214 14:23:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:23:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 673.48
[32m[20221214 14:23:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 720.56
[32m[20221214 14:23:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 736.54
[32m[20221214 14:23:32 @agent_ppo2.py:143][0m Total time:      25.50 min
[32m[20221214 14:23:32 @agent_ppo2.py:145][0m 2334720 total steps have happened
[32m[20221214 14:23:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1140 --------------------------#
[32m[20221214 14:23:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:23:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:32 @agent_ppo2.py:185][0m |           0.0025 |         128.5788 |         -70.1763 |
[32m[20221214 14:23:32 @agent_ppo2.py:185][0m |           0.0015 |         114.3080 |         -70.6215 |
[32m[20221214 14:23:33 @agent_ppo2.py:185][0m |          -0.0026 |         109.5623 |         -70.6493 |
[32m[20221214 14:23:33 @agent_ppo2.py:185][0m |          -0.0040 |         106.8369 |         -70.7040 |
[32m[20221214 14:23:33 @agent_ppo2.py:185][0m |          -0.0043 |         104.4774 |         -70.7784 |
[32m[20221214 14:23:33 @agent_ppo2.py:185][0m |          -0.0063 |         102.6306 |         -70.6587 |
[32m[20221214 14:23:33 @agent_ppo2.py:185][0m |          -0.0011 |         100.8287 |         -70.7438 |
[32m[20221214 14:23:33 @agent_ppo2.py:185][0m |          -0.0044 |          99.3438 |         -70.7850 |
[32m[20221214 14:23:33 @agent_ppo2.py:185][0m |           0.0002 |          97.8627 |         -70.7964 |
[32m[20221214 14:23:33 @agent_ppo2.py:185][0m |          -0.0030 |          97.8987 |         -70.9776 |
[32m[20221214 14:23:33 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:23:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.71
[32m[20221214 14:23:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.60
[32m[20221214 14:23:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 688.26
[32m[20221214 14:23:33 @agent_ppo2.py:143][0m Total time:      25.52 min
[32m[20221214 14:23:33 @agent_ppo2.py:145][0m 2336768 total steps have happened
[32m[20221214 14:23:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1141 --------------------------#
[32m[20221214 14:23:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:34 @agent_ppo2.py:185][0m |           0.0077 |         127.3861 |         -70.8144 |
[32m[20221214 14:23:34 @agent_ppo2.py:185][0m |           0.0005 |         103.3510 |         -70.4394 |
[32m[20221214 14:23:34 @agent_ppo2.py:185][0m |          -0.0060 |          95.1595 |         -70.4767 |
[32m[20221214 14:23:34 @agent_ppo2.py:185][0m |           0.0049 |          91.1598 |         -70.3534 |
[32m[20221214 14:23:34 @agent_ppo2.py:185][0m |          -0.0024 |          85.7849 |         -70.0969 |
[32m[20221214 14:23:34 @agent_ppo2.py:185][0m |          -0.0022 |          83.9343 |         -70.2577 |
[32m[20221214 14:23:34 @agent_ppo2.py:185][0m |          -0.0064 |          80.6600 |         -70.2130 |
[32m[20221214 14:23:34 @agent_ppo2.py:185][0m |          -0.0002 |          79.3521 |         -70.1254 |
[32m[20221214 14:23:34 @agent_ppo2.py:185][0m |          -0.0029 |          76.9424 |         -70.0078 |
[32m[20221214 14:23:34 @agent_ppo2.py:185][0m |          -0.0074 |          75.5295 |         -70.0489 |
[32m[20221214 14:23:34 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:23:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 668.18
[32m[20221214 14:23:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.66
[32m[20221214 14:23:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 589.73
[32m[20221214 14:23:35 @agent_ppo2.py:143][0m Total time:      25.54 min
[32m[20221214 14:23:35 @agent_ppo2.py:145][0m 2338816 total steps have happened
[32m[20221214 14:23:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1142 --------------------------#
[32m[20221214 14:23:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:23:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:35 @agent_ppo2.py:185][0m |          -0.0018 |         156.3008 |         -71.4865 |
[32m[20221214 14:23:35 @agent_ppo2.py:185][0m |          -0.0020 |         119.8310 |         -71.2916 |
[32m[20221214 14:23:35 @agent_ppo2.py:185][0m |          -0.0013 |         108.9442 |         -71.3955 |
[32m[20221214 14:23:35 @agent_ppo2.py:185][0m |          -0.0025 |         103.0259 |         -71.3610 |
[32m[20221214 14:23:35 @agent_ppo2.py:185][0m |          -0.0029 |          99.8715 |         -71.3227 |
[32m[20221214 14:23:35 @agent_ppo2.py:185][0m |          -0.0010 |          96.4210 |         -71.4239 |
[32m[20221214 14:23:35 @agent_ppo2.py:185][0m |          -0.0009 |          93.0264 |         -71.3019 |
[32m[20221214 14:23:35 @agent_ppo2.py:185][0m |          -0.0059 |          91.2004 |         -71.3534 |
[32m[20221214 14:23:36 @agent_ppo2.py:185][0m |          -0.0057 |          89.4910 |         -71.3039 |
[32m[20221214 14:23:36 @agent_ppo2.py:185][0m |          -0.0005 |          88.2504 |         -71.0478 |
[32m[20221214 14:23:36 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:23:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 636.85
[32m[20221214 14:23:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 690.87
[32m[20221214 14:23:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.52
[32m[20221214 14:23:36 @agent_ppo2.py:143][0m Total time:      25.56 min
[32m[20221214 14:23:36 @agent_ppo2.py:145][0m 2340864 total steps have happened
[32m[20221214 14:23:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1143 --------------------------#
[32m[20221214 14:23:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:36 @agent_ppo2.py:185][0m |           0.0015 |         115.7735 |         -70.1254 |
[32m[20221214 14:23:36 @agent_ppo2.py:185][0m |          -0.0016 |          88.1330 |         -69.8064 |
[32m[20221214 14:23:36 @agent_ppo2.py:185][0m |           0.0040 |          78.3489 |         -69.7900 |
[32m[20221214 14:23:36 @agent_ppo2.py:185][0m |          -0.0044 |          73.0614 |         -69.7167 |
[32m[20221214 14:23:36 @agent_ppo2.py:185][0m |          -0.0037 |          68.9076 |         -69.6747 |
[32m[20221214 14:23:37 @agent_ppo2.py:185][0m |          -0.0025 |          64.4341 |         -69.5637 |
[32m[20221214 14:23:37 @agent_ppo2.py:185][0m |           0.0046 |          63.2234 |         -69.5577 |
[32m[20221214 14:23:37 @agent_ppo2.py:185][0m |          -0.0032 |          58.6350 |         -69.6281 |
[32m[20221214 14:23:37 @agent_ppo2.py:185][0m |          -0.0023 |          56.1768 |         -69.4758 |
[32m[20221214 14:23:37 @agent_ppo2.py:185][0m |          -0.0011 |          54.2868 |         -69.5862 |
[32m[20221214 14:23:37 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:23:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.54
[32m[20221214 14:23:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 690.85
[32m[20221214 14:23:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.03
[32m[20221214 14:23:37 @agent_ppo2.py:143][0m Total time:      25.58 min
[32m[20221214 14:23:37 @agent_ppo2.py:145][0m 2342912 total steps have happened
[32m[20221214 14:23:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1144 --------------------------#
[32m[20221214 14:23:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:37 @agent_ppo2.py:185][0m |           0.0042 |         114.5924 |         -70.1583 |
[32m[20221214 14:23:37 @agent_ppo2.py:185][0m |          -0.0004 |          85.8221 |         -69.9680 |
[32m[20221214 14:23:38 @agent_ppo2.py:185][0m |          -0.0080 |          73.7509 |         -69.9615 |
[32m[20221214 14:23:38 @agent_ppo2.py:185][0m |          -0.0031 |          66.0079 |         -70.2527 |
[32m[20221214 14:23:38 @agent_ppo2.py:185][0m |          -0.0001 |          59.6375 |         -70.1497 |
[32m[20221214 14:23:38 @agent_ppo2.py:185][0m |          -0.0014 |          55.4460 |         -70.0713 |
[32m[20221214 14:23:38 @agent_ppo2.py:185][0m |          -0.0022 |          50.9223 |         -70.2404 |
[32m[20221214 14:23:38 @agent_ppo2.py:185][0m |          -0.0008 |          47.9298 |         -70.0460 |
[32m[20221214 14:23:38 @agent_ppo2.py:185][0m |          -0.0024 |          44.9179 |         -70.2576 |
[32m[20221214 14:23:38 @agent_ppo2.py:185][0m |          -0.0000 |          42.5587 |         -70.2742 |
[32m[20221214 14:23:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:23:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 602.56
[32m[20221214 14:23:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 612.07
[32m[20221214 14:23:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 603.86
[32m[20221214 14:23:38 @agent_ppo2.py:143][0m Total time:      25.60 min
[32m[20221214 14:23:38 @agent_ppo2.py:145][0m 2344960 total steps have happened
[32m[20221214 14:23:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1145 --------------------------#
[32m[20221214 14:23:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:23:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:39 @agent_ppo2.py:185][0m |           0.0037 |          92.6318 |         -70.2505 |
[32m[20221214 14:23:39 @agent_ppo2.py:185][0m |          -0.0075 |          70.8866 |         -69.9940 |
[32m[20221214 14:23:39 @agent_ppo2.py:185][0m |           0.0011 |          60.2954 |         -70.1870 |
[32m[20221214 14:23:39 @agent_ppo2.py:185][0m |          -0.0063 |          53.7344 |         -69.9773 |
[32m[20221214 14:23:39 @agent_ppo2.py:185][0m |          -0.0059 |          49.3033 |         -70.0144 |
[32m[20221214 14:23:39 @agent_ppo2.py:185][0m |          -0.0069 |          46.4712 |         -69.9812 |
[32m[20221214 14:23:39 @agent_ppo2.py:185][0m |          -0.0068 |          44.5206 |         -70.0803 |
[32m[20221214 14:23:39 @agent_ppo2.py:185][0m |           0.0061 |          54.5891 |         -70.0738 |
[32m[20221214 14:23:39 @agent_ppo2.py:185][0m |          -0.0071 |          41.9606 |         -70.1178 |
[32m[20221214 14:23:39 @agent_ppo2.py:185][0m |          -0.0014 |          40.4162 |         -70.0154 |
[32m[20221214 14:23:39 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:23:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 596.91
[32m[20221214 14:23:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 696.81
[32m[20221214 14:23:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 610.87
[32m[20221214 14:23:40 @agent_ppo2.py:143][0m Total time:      25.62 min
[32m[20221214 14:23:40 @agent_ppo2.py:145][0m 2347008 total steps have happened
[32m[20221214 14:23:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1146 --------------------------#
[32m[20221214 14:23:40 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:23:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:40 @agent_ppo2.py:185][0m |          -0.0003 |          84.8051 |         -70.2467 |
[32m[20221214 14:23:40 @agent_ppo2.py:185][0m |          -0.0027 |          57.0929 |         -70.0715 |
[32m[20221214 14:23:40 @agent_ppo2.py:185][0m |          -0.0023 |          49.5529 |         -70.2560 |
[32m[20221214 14:23:40 @agent_ppo2.py:185][0m |           0.0064 |          46.3273 |         -70.1914 |
[32m[20221214 14:23:40 @agent_ppo2.py:185][0m |          -0.0019 |          42.7719 |         -70.1945 |
[32m[20221214 14:23:40 @agent_ppo2.py:185][0m |          -0.0007 |          40.2595 |         -70.2905 |
[32m[20221214 14:23:40 @agent_ppo2.py:185][0m |          -0.0012 |          38.6086 |         -70.4263 |
[32m[20221214 14:23:41 @agent_ppo2.py:185][0m |          -0.0020 |          37.5883 |         -70.4672 |
[32m[20221214 14:23:41 @agent_ppo2.py:185][0m |          -0.0001 |          36.3908 |         -70.3691 |
[32m[20221214 14:23:41 @agent_ppo2.py:185][0m |          -0.0029 |          35.6132 |         -70.5635 |
[32m[20221214 14:23:41 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:23:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 640.23
[32m[20221214 14:23:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.17
[32m[20221214 14:23:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 633.02
[32m[20221214 14:23:41 @agent_ppo2.py:143][0m Total time:      25.65 min
[32m[20221214 14:23:41 @agent_ppo2.py:145][0m 2349056 total steps have happened
[32m[20221214 14:23:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1147 --------------------------#
[32m[20221214 14:23:41 @agent_ppo2.py:127][0m Sampling time: 0.30 s by 5 slaves
[32m[20221214 14:23:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:42 @agent_ppo2.py:185][0m |           0.0020 |          95.6228 |         -70.1149 |
[32m[20221214 14:23:42 @agent_ppo2.py:185][0m |          -0.0037 |          78.1170 |         -70.0790 |
[32m[20221214 14:23:42 @agent_ppo2.py:185][0m |          -0.0003 |          70.5150 |         -70.0338 |
[32m[20221214 14:23:42 @agent_ppo2.py:185][0m |          -0.0000 |          65.7938 |         -70.0434 |
[32m[20221214 14:23:42 @agent_ppo2.py:185][0m |          -0.0042 |          62.1849 |         -69.9919 |
[32m[20221214 14:23:42 @agent_ppo2.py:185][0m |          -0.0086 |          59.7632 |         -69.9930 |
[32m[20221214 14:23:42 @agent_ppo2.py:185][0m |          -0.0006 |          57.6703 |         -70.0503 |
[32m[20221214 14:23:43 @agent_ppo2.py:185][0m |          -0.0091 |          55.7413 |         -69.9873 |
[32m[20221214 14:23:43 @agent_ppo2.py:185][0m |          -0.0017 |          53.8134 |         -70.1677 |
[32m[20221214 14:23:43 @agent_ppo2.py:185][0m |          -0.0075 |          52.2874 |         -70.1722 |
[32m[20221214 14:23:43 @agent_ppo2.py:130][0m Policy update time: 1.40 s
[32m[20221214 14:23:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 593.86
[32m[20221214 14:23:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 683.89
[32m[20221214 14:23:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 616.83
[32m[20221214 14:23:43 @agent_ppo2.py:143][0m Total time:      25.68 min
[32m[20221214 14:23:43 @agent_ppo2.py:145][0m 2351104 total steps have happened
[32m[20221214 14:23:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1148 --------------------------#
[32m[20221214 14:23:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:23:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:43 @agent_ppo2.py:185][0m |           0.0006 |          99.9085 |         -72.7917 |
[32m[20221214 14:23:43 @agent_ppo2.py:185][0m |          -0.0034 |          78.5777 |         -72.6459 |
[32m[20221214 14:23:44 @agent_ppo2.py:185][0m |          -0.0022 |          70.0922 |         -72.7342 |
[32m[20221214 14:23:44 @agent_ppo2.py:185][0m |          -0.0030 |          64.4848 |         -72.7065 |
[32m[20221214 14:23:44 @agent_ppo2.py:185][0m |          -0.0047 |          60.5131 |         -72.9301 |
[32m[20221214 14:23:44 @agent_ppo2.py:185][0m |          -0.0010 |          57.6870 |         -72.9143 |
[32m[20221214 14:23:44 @agent_ppo2.py:185][0m |          -0.0054 |          55.8207 |         -72.9382 |
[32m[20221214 14:23:44 @agent_ppo2.py:185][0m |           0.0093 |          63.4825 |         -72.9480 |
[32m[20221214 14:23:44 @agent_ppo2.py:185][0m |           0.0023 |          52.7539 |         -73.0944 |
[32m[20221214 14:23:44 @agent_ppo2.py:185][0m |          -0.0073 |          51.5382 |         -73.2893 |
[32m[20221214 14:23:44 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:23:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 650.71
[32m[20221214 14:23:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 673.39
[32m[20221214 14:23:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 602.02
[32m[20221214 14:23:44 @agent_ppo2.py:143][0m Total time:      25.71 min
[32m[20221214 14:23:44 @agent_ppo2.py:145][0m 2353152 total steps have happened
[32m[20221214 14:23:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1149 --------------------------#
[32m[20221214 14:23:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:23:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:45 @agent_ppo2.py:185][0m |          -0.0088 |          91.4594 |         -73.3866 |
[32m[20221214 14:23:45 @agent_ppo2.py:185][0m |           0.0002 |          76.9640 |         -73.2951 |
[32m[20221214 14:23:45 @agent_ppo2.py:185][0m |           0.0003 |          71.7584 |         -73.2548 |
[32m[20221214 14:23:45 @agent_ppo2.py:185][0m |           0.0008 |          69.0757 |         -73.2416 |
[32m[20221214 14:23:45 @agent_ppo2.py:185][0m |           0.0049 |          66.6724 |         -73.3553 |
[32m[20221214 14:23:45 @agent_ppo2.py:185][0m |          -0.0019 |          64.5262 |         -73.2407 |
[32m[20221214 14:23:45 @agent_ppo2.py:185][0m |          -0.0104 |          63.4805 |         -73.3399 |
[32m[20221214 14:23:45 @agent_ppo2.py:185][0m |          -0.0045 |          62.5293 |         -73.3822 |
[32m[20221214 14:23:46 @agent_ppo2.py:185][0m |          -0.0027 |          61.3654 |         -73.4211 |
[32m[20221214 14:23:46 @agent_ppo2.py:185][0m |          -0.0035 |          60.9360 |         -73.5911 |
[32m[20221214 14:23:46 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:23:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.21
[32m[20221214 14:23:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 686.49
[32m[20221214 14:23:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 615.21
[32m[20221214 14:23:46 @agent_ppo2.py:143][0m Total time:      25.73 min
[32m[20221214 14:23:46 @agent_ppo2.py:145][0m 2355200 total steps have happened
[32m[20221214 14:23:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1150 --------------------------#
[32m[20221214 14:23:46 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:23:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:46 @agent_ppo2.py:185][0m |          -0.0018 |         121.4004 |         -73.7744 |
[32m[20221214 14:23:46 @agent_ppo2.py:185][0m |          -0.0010 |          97.6023 |         -73.8464 |
[32m[20221214 14:23:46 @agent_ppo2.py:185][0m |          -0.0051 |          92.3722 |         -73.7599 |
[32m[20221214 14:23:47 @agent_ppo2.py:185][0m |          -0.0002 |          87.4608 |         -73.7866 |
[32m[20221214 14:23:47 @agent_ppo2.py:185][0m |          -0.0047 |          85.0327 |         -73.5529 |
[32m[20221214 14:23:47 @agent_ppo2.py:185][0m |           0.0021 |          84.2665 |         -73.6214 |
[32m[20221214 14:23:47 @agent_ppo2.py:185][0m |          -0.0018 |          82.7539 |         -73.0295 |
[32m[20221214 14:23:47 @agent_ppo2.py:185][0m |          -0.0039 |          81.8159 |         -73.2714 |
[32m[20221214 14:23:47 @agent_ppo2.py:185][0m |           0.0113 |          90.1622 |         -73.0879 |
[32m[20221214 14:23:47 @agent_ppo2.py:185][0m |          -0.0055 |          81.5143 |         -72.9221 |
[32m[20221214 14:23:47 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:23:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 682.08
[32m[20221214 14:23:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.40
[32m[20221214 14:23:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.90
[32m[20221214 14:23:47 @agent_ppo2.py:143][0m Total time:      25.75 min
[32m[20221214 14:23:47 @agent_ppo2.py:145][0m 2357248 total steps have happened
[32m[20221214 14:23:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1151 --------------------------#
[32m[20221214 14:23:48 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:23:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:48 @agent_ppo2.py:185][0m |          -0.0039 |         119.0248 |         -73.4140 |
[32m[20221214 14:23:48 @agent_ppo2.py:185][0m |           0.0024 |         100.1249 |         -73.1478 |
[32m[20221214 14:23:48 @agent_ppo2.py:185][0m |          -0.0049 |          92.7303 |         -72.8275 |
[32m[20221214 14:23:48 @agent_ppo2.py:185][0m |          -0.0048 |          88.3876 |         -72.7993 |
[32m[20221214 14:23:48 @agent_ppo2.py:185][0m |           0.0045 |          90.0614 |         -72.7349 |
[32m[20221214 14:23:48 @agent_ppo2.py:185][0m |          -0.0057 |          84.1305 |         -72.5819 |
[32m[20221214 14:23:48 @agent_ppo2.py:185][0m |          -0.0027 |          81.7331 |         -72.3406 |
[32m[20221214 14:23:48 @agent_ppo2.py:185][0m |          -0.0006 |          80.2822 |         -72.3900 |
[32m[20221214 14:23:49 @agent_ppo2.py:185][0m |          -0.0048 |          78.8570 |         -72.2111 |
[32m[20221214 14:23:49 @agent_ppo2.py:185][0m |          -0.0038 |          77.9766 |         -72.1783 |
[32m[20221214 14:23:49 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:23:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 613.04
[32m[20221214 14:23:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 740.38
[32m[20221214 14:23:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.05
[32m[20221214 14:23:49 @agent_ppo2.py:143][0m Total time:      25.78 min
[32m[20221214 14:23:49 @agent_ppo2.py:145][0m 2359296 total steps have happened
[32m[20221214 14:23:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1152 --------------------------#
[32m[20221214 14:23:49 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:23:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:49 @agent_ppo2.py:185][0m |           0.0000 |         131.8351 |         -70.8990 |
[32m[20221214 14:23:49 @agent_ppo2.py:185][0m |          -0.0035 |          99.4713 |         -70.7713 |
[32m[20221214 14:23:49 @agent_ppo2.py:185][0m |          -0.0032 |          86.7728 |         -70.7272 |
[32m[20221214 14:23:49 @agent_ppo2.py:185][0m |          -0.0035 |          80.5259 |         -71.0542 |
[32m[20221214 14:23:50 @agent_ppo2.py:185][0m |          -0.0074 |          77.4833 |         -71.0643 |
[32m[20221214 14:23:50 @agent_ppo2.py:185][0m |          -0.0046 |          74.3364 |         -71.3180 |
[32m[20221214 14:23:50 @agent_ppo2.py:185][0m |          -0.0025 |          71.7253 |         -71.1453 |
[32m[20221214 14:23:50 @agent_ppo2.py:185][0m |          -0.0051 |          71.1734 |         -71.3444 |
[32m[20221214 14:23:50 @agent_ppo2.py:185][0m |          -0.0035 |          69.2694 |         -71.5510 |
[32m[20221214 14:23:50 @agent_ppo2.py:185][0m |          -0.0007 |          67.9234 |         -71.3167 |
[32m[20221214 14:23:50 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:23:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 625.03
[32m[20221214 14:23:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 663.58
[32m[20221214 14:23:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 594.26
[32m[20221214 14:23:50 @agent_ppo2.py:143][0m Total time:      25.80 min
[32m[20221214 14:23:50 @agent_ppo2.py:145][0m 2361344 total steps have happened
[32m[20221214 14:23:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1153 --------------------------#
[32m[20221214 14:23:50 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:23:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:51 @agent_ppo2.py:185][0m |           0.0015 |         110.2222 |         -73.5310 |
[32m[20221214 14:23:51 @agent_ppo2.py:185][0m |          -0.0021 |          86.2239 |         -73.1318 |
[32m[20221214 14:23:51 @agent_ppo2.py:185][0m |          -0.0009 |          77.6935 |         -73.5204 |
[32m[20221214 14:23:51 @agent_ppo2.py:185][0m |           0.0009 |          73.1563 |         -73.6498 |
[32m[20221214 14:23:51 @agent_ppo2.py:185][0m |           0.0005 |          69.9637 |         -73.4943 |
[32m[20221214 14:23:51 @agent_ppo2.py:185][0m |          -0.0074 |          68.3537 |         -73.3324 |
[32m[20221214 14:23:51 @agent_ppo2.py:185][0m |           0.0029 |          66.8006 |         -73.6386 |
[32m[20221214 14:23:51 @agent_ppo2.py:185][0m |           0.0020 |          66.5547 |         -73.8546 |
[32m[20221214 14:23:51 @agent_ppo2.py:185][0m |           0.0087 |          70.0773 |         -73.6340 |
[32m[20221214 14:23:51 @agent_ppo2.py:185][0m |          -0.0044 |          64.8631 |         -73.6701 |
[32m[20221214 14:23:51 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:23:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 632.57
[32m[20221214 14:23:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 684.09
[32m[20221214 14:23:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 598.97
[32m[20221214 14:23:52 @agent_ppo2.py:143][0m Total time:      25.83 min
[32m[20221214 14:23:52 @agent_ppo2.py:145][0m 2363392 total steps have happened
[32m[20221214 14:23:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1154 --------------------------#
[32m[20221214 14:23:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:23:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:52 @agent_ppo2.py:185][0m |          -0.0032 |         107.3436 |         -72.4372 |
[32m[20221214 14:23:52 @agent_ppo2.py:185][0m |          -0.0028 |          89.2705 |         -72.5313 |
[32m[20221214 14:23:52 @agent_ppo2.py:185][0m |          -0.0013 |          81.2266 |         -72.5748 |
[32m[20221214 14:23:52 @agent_ppo2.py:185][0m |          -0.0025 |          76.0115 |         -72.6186 |
[32m[20221214 14:23:52 @agent_ppo2.py:185][0m |           0.0022 |          73.5772 |         -72.5845 |
[32m[20221214 14:23:53 @agent_ppo2.py:185][0m |           0.0089 |          75.9103 |         -72.6091 |
[32m[20221214 14:23:53 @agent_ppo2.py:185][0m |          -0.0033 |          68.1019 |         -72.4571 |
[32m[20221214 14:23:53 @agent_ppo2.py:185][0m |          -0.0037 |          66.0491 |         -72.4717 |
[32m[20221214 14:23:53 @agent_ppo2.py:185][0m |           0.0109 |          66.7512 |         -72.8395 |
[32m[20221214 14:23:53 @agent_ppo2.py:185][0m |          -0.0037 |          62.9180 |         -72.5902 |
[32m[20221214 14:23:53 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:23:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 593.06
[32m[20221214 14:23:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 636.47
[32m[20221214 14:23:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 599.95
[32m[20221214 14:23:53 @agent_ppo2.py:143][0m Total time:      25.85 min
[32m[20221214 14:23:53 @agent_ppo2.py:145][0m 2365440 total steps have happened
[32m[20221214 14:23:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1155 --------------------------#
[32m[20221214 14:23:53 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:23:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:54 @agent_ppo2.py:185][0m |           0.0007 |         113.2276 |         -74.6999 |
[32m[20221214 14:23:54 @agent_ppo2.py:185][0m |          -0.0053 |          98.9883 |         -74.6640 |
[32m[20221214 14:23:54 @agent_ppo2.py:185][0m |           0.0026 |          96.7854 |         -74.6063 |
[32m[20221214 14:23:54 @agent_ppo2.py:185][0m |           0.0004 |          91.6101 |         -74.5418 |
[32m[20221214 14:23:54 @agent_ppo2.py:185][0m |          -0.0025 |          90.0122 |         -74.4942 |
[32m[20221214 14:23:54 @agent_ppo2.py:185][0m |           0.0099 |          96.6254 |         -74.5682 |
[32m[20221214 14:23:54 @agent_ppo2.py:185][0m |          -0.0041 |          86.8833 |         -74.5622 |
[32m[20221214 14:23:54 @agent_ppo2.py:185][0m |          -0.0028 |          85.6647 |         -74.4780 |
[32m[20221214 14:23:54 @agent_ppo2.py:185][0m |          -0.0003 |          85.2348 |         -74.4646 |
[32m[20221214 14:23:54 @agent_ppo2.py:185][0m |          -0.0025 |          84.3583 |         -74.5668 |
[32m[20221214 14:23:54 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:23:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 654.04
[32m[20221214 14:23:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.23
[32m[20221214 14:23:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 705.18
[32m[20221214 14:23:55 @agent_ppo2.py:143][0m Total time:      25.88 min
[32m[20221214 14:23:55 @agent_ppo2.py:145][0m 2367488 total steps have happened
[32m[20221214 14:23:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1156 --------------------------#
[32m[20221214 14:23:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:23:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:55 @agent_ppo2.py:185][0m |           0.0013 |         114.2127 |         -73.3525 |
[32m[20221214 14:23:55 @agent_ppo2.py:185][0m |          -0.0012 |         102.2640 |         -73.1844 |
[32m[20221214 14:23:55 @agent_ppo2.py:185][0m |          -0.0050 |          98.6319 |         -73.1977 |
[32m[20221214 14:23:55 @agent_ppo2.py:185][0m |          -0.0009 |          95.9823 |         -73.2766 |
[32m[20221214 14:23:55 @agent_ppo2.py:185][0m |          -0.0050 |          93.9401 |         -73.3494 |
[32m[20221214 14:23:55 @agent_ppo2.py:185][0m |          -0.0016 |          92.1673 |         -73.3238 |
[32m[20221214 14:23:56 @agent_ppo2.py:185][0m |          -0.0024 |          91.1359 |         -73.4358 |
[32m[20221214 14:23:56 @agent_ppo2.py:185][0m |          -0.0053 |          90.2310 |         -73.5799 |
[32m[20221214 14:23:56 @agent_ppo2.py:185][0m |          -0.0026 |          89.1378 |         -73.7038 |
[32m[20221214 14:23:56 @agent_ppo2.py:185][0m |          -0.0025 |          88.6662 |         -73.5616 |
[32m[20221214 14:23:56 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:23:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 721.83
[32m[20221214 14:23:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.04
[32m[20221214 14:23:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 749.69
[32m[20221214 14:23:56 @agent_ppo2.py:143][0m Total time:      25.90 min
[32m[20221214 14:23:56 @agent_ppo2.py:145][0m 2369536 total steps have happened
[32m[20221214 14:23:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1157 --------------------------#
[32m[20221214 14:23:56 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:23:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:56 @agent_ppo2.py:185][0m |          -0.0030 |         106.7365 |         -75.0948 |
[32m[20221214 14:23:57 @agent_ppo2.py:185][0m |          -0.0031 |          99.6935 |         -74.8518 |
[32m[20221214 14:23:57 @agent_ppo2.py:185][0m |           0.0069 |          97.8843 |         -75.0889 |
[32m[20221214 14:23:57 @agent_ppo2.py:185][0m |          -0.0024 |          94.6902 |         -74.9179 |
[32m[20221214 14:23:57 @agent_ppo2.py:185][0m |          -0.0022 |          93.2894 |         -74.9190 |
[32m[20221214 14:23:57 @agent_ppo2.py:185][0m |          -0.0011 |          92.2328 |         -74.7083 |
[32m[20221214 14:23:57 @agent_ppo2.py:185][0m |          -0.0062 |          92.1173 |         -74.8159 |
[32m[20221214 14:23:57 @agent_ppo2.py:185][0m |          -0.0043 |          91.0776 |         -74.6655 |
[32m[20221214 14:23:57 @agent_ppo2.py:185][0m |          -0.0020 |          90.7609 |         -74.6378 |
[32m[20221214 14:23:57 @agent_ppo2.py:185][0m |          -0.0046 |          90.3992 |         -74.2689 |
[32m[20221214 14:23:57 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:23:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 746.84
[32m[20221214 14:23:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 756.48
[32m[20221214 14:23:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 750.99
[32m[20221214 14:23:58 @agent_ppo2.py:143][0m Total time:      25.92 min
[32m[20221214 14:23:58 @agent_ppo2.py:145][0m 2371584 total steps have happened
[32m[20221214 14:23:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1158 --------------------------#
[32m[20221214 14:23:58 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:23:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:58 @agent_ppo2.py:185][0m |           0.0002 |         134.5321 |         -72.4578 |
[32m[20221214 14:23:58 @agent_ppo2.py:185][0m |          -0.0018 |         112.1366 |         -72.2695 |
[32m[20221214 14:23:58 @agent_ppo2.py:185][0m |           0.0026 |         105.7578 |         -72.4959 |
[32m[20221214 14:23:58 @agent_ppo2.py:185][0m |          -0.0052 |         100.0309 |         -72.4377 |
[32m[20221214 14:23:58 @agent_ppo2.py:185][0m |           0.0010 |          97.4589 |         -72.4811 |
[32m[20221214 14:23:58 @agent_ppo2.py:185][0m |           0.0074 |         102.8399 |         -72.6578 |
[32m[20221214 14:23:58 @agent_ppo2.py:185][0m |          -0.0000 |          93.1822 |         -72.7668 |
[32m[20221214 14:23:59 @agent_ppo2.py:185][0m |          -0.0025 |          91.7665 |         -72.8005 |
[32m[20221214 14:23:59 @agent_ppo2.py:185][0m |          -0.0037 |          90.4025 |         -72.9537 |
[32m[20221214 14:23:59 @agent_ppo2.py:185][0m |           0.0009 |          88.8967 |         -72.9756 |
[32m[20221214 14:23:59 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:23:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 691.19
[32m[20221214 14:23:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.52
[32m[20221214 14:23:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 751.78
[32m[20221214 14:23:59 @agent_ppo2.py:143][0m Total time:      25.95 min
[32m[20221214 14:23:59 @agent_ppo2.py:145][0m 2373632 total steps have happened
[32m[20221214 14:23:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1159 --------------------------#
[32m[20221214 14:23:59 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:23:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:23:59 @agent_ppo2.py:185][0m |          -0.0016 |         148.5883 |         -73.5501 |
[32m[20221214 14:23:59 @agent_ppo2.py:185][0m |          -0.0038 |         128.2457 |         -73.3411 |
[32m[20221214 14:24:00 @agent_ppo2.py:185][0m |          -0.0048 |         121.3072 |         -73.1399 |
[32m[20221214 14:24:00 @agent_ppo2.py:185][0m |          -0.0025 |         117.2105 |         -73.2681 |
[32m[20221214 14:24:00 @agent_ppo2.py:185][0m |          -0.0013 |         115.2373 |         -73.3348 |
[32m[20221214 14:24:00 @agent_ppo2.py:185][0m |          -0.0069 |         113.7241 |         -73.3736 |
[32m[20221214 14:24:00 @agent_ppo2.py:185][0m |          -0.0022 |         112.6847 |         -73.3991 |
[32m[20221214 14:24:00 @agent_ppo2.py:185][0m |          -0.0038 |         111.1347 |         -73.4572 |
[32m[20221214 14:24:00 @agent_ppo2.py:185][0m |          -0.0062 |         110.3190 |         -73.5412 |
[32m[20221214 14:24:00 @agent_ppo2.py:185][0m |          -0.0059 |         109.2413 |         -73.4940 |
[32m[20221214 14:24:00 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:24:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 730.11
[32m[20221214 14:24:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 755.02
[32m[20221214 14:24:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 691.99
[32m[20221214 14:24:00 @agent_ppo2.py:143][0m Total time:      25.97 min
[32m[20221214 14:24:00 @agent_ppo2.py:145][0m 2375680 total steps have happened
[32m[20221214 14:24:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1160 --------------------------#
[32m[20221214 14:24:01 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:24:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:01 @agent_ppo2.py:185][0m |           0.0095 |         132.4461 |         -74.9663 |
[32m[20221214 14:24:01 @agent_ppo2.py:185][0m |          -0.0050 |         114.8622 |         -75.2748 |
[32m[20221214 14:24:01 @agent_ppo2.py:185][0m |          -0.0030 |         110.1345 |         -75.2055 |
[32m[20221214 14:24:01 @agent_ppo2.py:185][0m |          -0.0038 |         107.5649 |         -75.1638 |
[32m[20221214 14:24:01 @agent_ppo2.py:185][0m |          -0.0039 |         106.4051 |         -75.1681 |
[32m[20221214 14:24:01 @agent_ppo2.py:185][0m |          -0.0056 |         104.5997 |         -74.9790 |
[32m[20221214 14:24:01 @agent_ppo2.py:185][0m |          -0.0014 |         104.1418 |         -75.1858 |
[32m[20221214 14:24:02 @agent_ppo2.py:185][0m |          -0.0001 |         102.6289 |         -75.2889 |
[32m[20221214 14:24:02 @agent_ppo2.py:185][0m |          -0.0008 |         102.7076 |         -75.0422 |
[32m[20221214 14:24:02 @agent_ppo2.py:185][0m |          -0.0020 |         100.7702 |         -75.3642 |
[32m[20221214 14:24:02 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 14:24:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 695.50
[32m[20221214 14:24:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 743.50
[32m[20221214 14:24:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 752.55
[32m[20221214 14:24:02 @agent_ppo2.py:143][0m Total time:      26.00 min
[32m[20221214 14:24:02 @agent_ppo2.py:145][0m 2377728 total steps have happened
[32m[20221214 14:24:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1161 --------------------------#
[32m[20221214 14:24:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:24:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:02 @agent_ppo2.py:185][0m |           0.0007 |         144.3569 |         -77.8285 |
[32m[20221214 14:24:02 @agent_ppo2.py:185][0m |           0.0013 |         128.8037 |         -77.8859 |
[32m[20221214 14:24:02 @agent_ppo2.py:185][0m |          -0.0007 |         122.1588 |         -77.6804 |
[32m[20221214 14:24:03 @agent_ppo2.py:185][0m |          -0.0029 |         117.6433 |         -77.8345 |
[32m[20221214 14:24:03 @agent_ppo2.py:185][0m |          -0.0006 |         114.3028 |         -78.0822 |
[32m[20221214 14:24:03 @agent_ppo2.py:185][0m |          -0.0037 |         111.4494 |         -78.2943 |
[32m[20221214 14:24:03 @agent_ppo2.py:185][0m |          -0.0044 |         108.8961 |         -78.1932 |
[32m[20221214 14:24:03 @agent_ppo2.py:185][0m |           0.0124 |         119.7724 |         -78.3109 |
[32m[20221214 14:24:03 @agent_ppo2.py:185][0m |           0.0001 |         106.1724 |         -78.2615 |
[32m[20221214 14:24:03 @agent_ppo2.py:185][0m |          -0.0024 |         104.7144 |         -78.2734 |
[32m[20221214 14:24:03 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:24:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 685.95
[32m[20221214 14:24:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 748.79
[32m[20221214 14:24:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 752.77
[32m[20221214 14:24:03 @agent_ppo2.py:143][0m Total time:      26.02 min
[32m[20221214 14:24:03 @agent_ppo2.py:145][0m 2379776 total steps have happened
[32m[20221214 14:24:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1162 --------------------------#
[32m[20221214 14:24:04 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:04 @agent_ppo2.py:185][0m |           0.0005 |         121.8200 |         -77.0061 |
[32m[20221214 14:24:04 @agent_ppo2.py:185][0m |           0.0125 |         124.2723 |         -76.6264 |
[32m[20221214 14:24:04 @agent_ppo2.py:185][0m |           0.0010 |         101.8956 |         -76.9582 |
[32m[20221214 14:24:04 @agent_ppo2.py:185][0m |           0.0150 |         106.4404 |         -76.9224 |
[32m[20221214 14:24:04 @agent_ppo2.py:185][0m |          -0.0014 |          96.9642 |         -77.0150 |
[32m[20221214 14:24:04 @agent_ppo2.py:185][0m |          -0.0019 |          95.6174 |         -77.0109 |
[32m[20221214 14:24:04 @agent_ppo2.py:185][0m |           0.0099 |         104.0721 |         -76.9680 |
[32m[20221214 14:24:04 @agent_ppo2.py:185][0m |           0.0016 |          94.0045 |         -77.1406 |
[32m[20221214 14:24:05 @agent_ppo2.py:185][0m |          -0.0022 |          92.7595 |         -77.0527 |
[32m[20221214 14:24:05 @agent_ppo2.py:185][0m |          -0.0022 |          91.8879 |         -77.1306 |
[32m[20221214 14:24:05 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:24:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 730.53
[32m[20221214 14:24:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.58
[32m[20221214 14:24:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 751.58
[32m[20221214 14:24:05 @agent_ppo2.py:143][0m Total time:      26.05 min
[32m[20221214 14:24:05 @agent_ppo2.py:145][0m 2381824 total steps have happened
[32m[20221214 14:24:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1163 --------------------------#
[32m[20221214 14:24:05 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:05 @agent_ppo2.py:185][0m |          -0.0024 |         157.0855 |         -77.9665 |
[32m[20221214 14:24:05 @agent_ppo2.py:185][0m |           0.0064 |         154.0464 |         -78.0179 |
[32m[20221214 14:24:05 @agent_ppo2.py:185][0m |           0.0006 |         145.5857 |         -78.1209 |
[32m[20221214 14:24:05 @agent_ppo2.py:185][0m |          -0.0025 |         143.1996 |         -78.0972 |
[32m[20221214 14:24:06 @agent_ppo2.py:185][0m |          -0.0035 |         141.6270 |         -77.7887 |
[32m[20221214 14:24:06 @agent_ppo2.py:185][0m |           0.0106 |         148.1330 |         -77.9525 |
[32m[20221214 14:24:06 @agent_ppo2.py:185][0m |          -0.0044 |         138.4836 |         -78.0531 |
[32m[20221214 14:24:06 @agent_ppo2.py:185][0m |           0.0039 |         136.9307 |         -78.0611 |
[32m[20221214 14:24:06 @agent_ppo2.py:185][0m |           0.0086 |         142.3214 |         -77.9531 |
[32m[20221214 14:24:06 @agent_ppo2.py:185][0m |           0.0093 |         145.4967 |         -78.0895 |
[32m[20221214 14:24:06 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 14:24:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 747.14
[32m[20221214 14:24:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 751.39
[32m[20221214 14:24:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.12
[32m[20221214 14:24:06 @agent_ppo2.py:143][0m Total time:      26.07 min
[32m[20221214 14:24:06 @agent_ppo2.py:145][0m 2383872 total steps have happened
[32m[20221214 14:24:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1164 --------------------------#
[32m[20221214 14:24:06 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:07 @agent_ppo2.py:185][0m |           0.0033 |         136.1182 |         -78.4888 |
[32m[20221214 14:24:07 @agent_ppo2.py:185][0m |          -0.0016 |         123.5872 |         -78.5269 |
[32m[20221214 14:24:07 @agent_ppo2.py:185][0m |          -0.0014 |         118.0646 |         -78.4823 |
[32m[20221214 14:24:07 @agent_ppo2.py:185][0m |           0.0064 |         117.8866 |         -78.4768 |
[32m[20221214 14:24:07 @agent_ppo2.py:185][0m |          -0.0020 |         113.5387 |         -78.4745 |
[32m[20221214 14:24:07 @agent_ppo2.py:185][0m |           0.0075 |         116.8766 |         -78.2564 |
[32m[20221214 14:24:07 @agent_ppo2.py:185][0m |          -0.0031 |         108.8266 |         -78.1200 |
[32m[20221214 14:24:07 @agent_ppo2.py:185][0m |          -0.0006 |         106.4755 |         -78.1322 |
[32m[20221214 14:24:07 @agent_ppo2.py:185][0m |          -0.0031 |         104.8380 |         -78.1614 |
[32m[20221214 14:24:08 @agent_ppo2.py:185][0m |           0.0039 |         105.0620 |         -77.8425 |
[32m[20221214 14:24:08 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:24:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 685.39
[32m[20221214 14:24:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.62
[32m[20221214 14:24:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.11
[32m[20221214 14:24:08 @agent_ppo2.py:143][0m Total time:      26.09 min
[32m[20221214 14:24:08 @agent_ppo2.py:145][0m 2385920 total steps have happened
[32m[20221214 14:24:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1165 --------------------------#
[32m[20221214 14:24:08 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:08 @agent_ppo2.py:185][0m |          -0.0022 |         134.4274 |         -77.0570 |
[32m[20221214 14:24:08 @agent_ppo2.py:185][0m |           0.0022 |         121.5405 |         -76.7997 |
[32m[20221214 14:24:08 @agent_ppo2.py:185][0m |          -0.0001 |         115.0123 |         -76.6461 |
[32m[20221214 14:24:08 @agent_ppo2.py:185][0m |          -0.0013 |         111.9963 |         -76.4766 |
[32m[20221214 14:24:08 @agent_ppo2.py:185][0m |          -0.0031 |         108.4488 |         -76.2914 |
[32m[20221214 14:24:09 @agent_ppo2.py:185][0m |          -0.0057 |         108.5477 |         -76.1625 |
[32m[20221214 14:24:09 @agent_ppo2.py:185][0m |          -0.0017 |         105.5785 |         -76.0820 |
[32m[20221214 14:24:09 @agent_ppo2.py:185][0m |          -0.0015 |         104.2611 |         -75.9894 |
[32m[20221214 14:24:09 @agent_ppo2.py:185][0m |          -0.0013 |         103.5235 |         -75.7849 |
[32m[20221214 14:24:09 @agent_ppo2.py:185][0m |          -0.0032 |         101.7412 |         -75.5292 |
[32m[20221214 14:24:09 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:24:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 731.12
[32m[20221214 14:24:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 749.32
[32m[20221214 14:24:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 754.69
[32m[20221214 14:24:09 @agent_ppo2.py:143][0m Total time:      26.12 min
[32m[20221214 14:24:09 @agent_ppo2.py:145][0m 2387968 total steps have happened
[32m[20221214 14:24:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1166 --------------------------#
[32m[20221214 14:24:09 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:10 @agent_ppo2.py:185][0m |           0.0014 |         115.8415 |         -76.2721 |
[32m[20221214 14:24:10 @agent_ppo2.py:185][0m |           0.0025 |         106.6776 |         -76.1058 |
[32m[20221214 14:24:10 @agent_ppo2.py:185][0m |           0.0071 |         103.2611 |         -76.2963 |
[32m[20221214 14:24:10 @agent_ppo2.py:185][0m |           0.0011 |         101.2090 |         -76.2331 |
[32m[20221214 14:24:10 @agent_ppo2.py:185][0m |          -0.0021 |          99.9891 |         -76.1606 |
[32m[20221214 14:24:10 @agent_ppo2.py:185][0m |          -0.0037 |          99.9348 |         -76.3246 |
[32m[20221214 14:24:10 @agent_ppo2.py:185][0m |           0.0117 |         112.6737 |         -76.4581 |
[32m[20221214 14:24:10 @agent_ppo2.py:185][0m |          -0.0018 |          97.1953 |         -76.0447 |
[32m[20221214 14:24:10 @agent_ppo2.py:185][0m |          -0.0027 |          97.1320 |         -76.5155 |
[32m[20221214 14:24:10 @agent_ppo2.py:185][0m |           0.0044 |          95.5144 |         -76.6277 |
[32m[20221214 14:24:10 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:24:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.81
[32m[20221214 14:24:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 755.22
[32m[20221214 14:24:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 758.42
[32m[20221214 14:24:11 @agent_ppo2.py:143][0m Total time:      26.14 min
[32m[20221214 14:24:11 @agent_ppo2.py:145][0m 2390016 total steps have happened
[32m[20221214 14:24:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1167 --------------------------#
[32m[20221214 14:24:11 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:11 @agent_ppo2.py:185][0m |           0.0017 |         163.1140 |         -77.2303 |
[32m[20221214 14:24:11 @agent_ppo2.py:185][0m |           0.0014 |         150.3933 |         -77.3411 |
[32m[20221214 14:24:11 @agent_ppo2.py:185][0m |           0.0009 |         146.8290 |         -77.6213 |
[32m[20221214 14:24:11 @agent_ppo2.py:185][0m |           0.0010 |         143.9307 |         -77.9089 |
[32m[20221214 14:24:11 @agent_ppo2.py:185][0m |          -0.0002 |         142.2392 |         -78.1202 |
[32m[20221214 14:24:12 @agent_ppo2.py:185][0m |          -0.0016 |         140.5217 |         -77.8854 |
[32m[20221214 14:24:12 @agent_ppo2.py:185][0m |          -0.0035 |         139.9903 |         -78.0684 |
[32m[20221214 14:24:12 @agent_ppo2.py:185][0m |          -0.0021 |         137.6881 |         -78.4307 |
[32m[20221214 14:24:12 @agent_ppo2.py:185][0m |          -0.0005 |         136.7742 |         -78.6075 |
[32m[20221214 14:24:12 @agent_ppo2.py:185][0m |          -0.0030 |         136.2227 |         -78.7027 |
[32m[20221214 14:24:12 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:24:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 721.50
[32m[20221214 14:24:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 756.57
[32m[20221214 14:24:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 762.52
[32m[20221214 14:24:12 @agent_ppo2.py:143][0m Total time:      26.17 min
[32m[20221214 14:24:12 @agent_ppo2.py:145][0m 2392064 total steps have happened
[32m[20221214 14:24:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1168 --------------------------#
[32m[20221214 14:24:12 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:12 @agent_ppo2.py:185][0m |           0.0045 |         140.3183 |         -79.8157 |
[32m[20221214 14:24:13 @agent_ppo2.py:185][0m |          -0.0037 |         119.9784 |         -79.8331 |
[32m[20221214 14:24:13 @agent_ppo2.py:185][0m |          -0.0019 |         111.7005 |         -79.6217 |
[32m[20221214 14:24:13 @agent_ppo2.py:185][0m |          -0.0047 |         106.0307 |         -79.7198 |
[32m[20221214 14:24:13 @agent_ppo2.py:185][0m |           0.0019 |         103.2664 |         -80.0779 |
[32m[20221214 14:24:13 @agent_ppo2.py:185][0m |          -0.0023 |          98.8752 |         -80.1672 |
[32m[20221214 14:24:13 @agent_ppo2.py:185][0m |          -0.0044 |          96.2259 |         -79.9851 |
[32m[20221214 14:24:13 @agent_ppo2.py:185][0m |          -0.0049 |          93.6612 |         -80.2493 |
[32m[20221214 14:24:13 @agent_ppo2.py:185][0m |          -0.0030 |          92.1178 |         -80.3178 |
[32m[20221214 14:24:13 @agent_ppo2.py:185][0m |           0.0092 |          94.9624 |         -80.4635 |
[32m[20221214 14:24:13 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 14:24:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.83
[32m[20221214 14:24:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 756.43
[32m[20221214 14:24:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.48
[32m[20221214 14:24:14 @agent_ppo2.py:143][0m Total time:      26.19 min
[32m[20221214 14:24:14 @agent_ppo2.py:145][0m 2394112 total steps have happened
[32m[20221214 14:24:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1169 --------------------------#
[32m[20221214 14:24:14 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:14 @agent_ppo2.py:185][0m |          -0.0015 |         188.5584 |         -80.9683 |
[32m[20221214 14:24:14 @agent_ppo2.py:185][0m |          -0.0009 |         161.3069 |         -81.2203 |
[32m[20221214 14:24:14 @agent_ppo2.py:185][0m |          -0.0036 |         151.3762 |         -80.9995 |
[32m[20221214 14:24:14 @agent_ppo2.py:185][0m |          -0.0033 |         146.0934 |         -81.0310 |
[32m[20221214 14:24:14 @agent_ppo2.py:185][0m |          -0.0079 |         143.6690 |         -81.2576 |
[32m[20221214 14:24:14 @agent_ppo2.py:185][0m |          -0.0049 |         141.3923 |         -81.1587 |
[32m[20221214 14:24:14 @agent_ppo2.py:185][0m |          -0.0074 |         140.6526 |         -81.2040 |
[32m[20221214 14:24:15 @agent_ppo2.py:185][0m |          -0.0071 |         139.4474 |         -81.3161 |
[32m[20221214 14:24:15 @agent_ppo2.py:185][0m |          -0.0050 |         137.7464 |         -81.0259 |
[32m[20221214 14:24:15 @agent_ppo2.py:185][0m |          -0.0033 |         137.9889 |         -81.2140 |
[32m[20221214 14:24:15 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:24:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 698.50
[32m[20221214 14:24:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 765.84
[32m[20221214 14:24:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 763.41
[32m[20221214 14:24:15 @agent_ppo2.py:143][0m Total time:      26.21 min
[32m[20221214 14:24:15 @agent_ppo2.py:145][0m 2396160 total steps have happened
[32m[20221214 14:24:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1170 --------------------------#
[32m[20221214 14:24:15 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:24:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:15 @agent_ppo2.py:185][0m |           0.0014 |         136.6155 |         -81.1768 |
[32m[20221214 14:24:15 @agent_ppo2.py:185][0m |           0.0023 |         124.7421 |         -81.3688 |
[32m[20221214 14:24:16 @agent_ppo2.py:185][0m |          -0.0010 |         121.3056 |         -81.1605 |
[32m[20221214 14:24:16 @agent_ppo2.py:185][0m |          -0.0006 |         119.9002 |         -81.4619 |
[32m[20221214 14:24:16 @agent_ppo2.py:185][0m |           0.0014 |         118.5794 |         -81.4495 |
[32m[20221214 14:24:16 @agent_ppo2.py:185][0m |          -0.0004 |         118.1435 |         -81.6115 |
[32m[20221214 14:24:16 @agent_ppo2.py:185][0m |           0.0010 |         117.4221 |         -81.6734 |
[32m[20221214 14:24:16 @agent_ppo2.py:185][0m |          -0.0025 |         116.9939 |         -81.5773 |
[32m[20221214 14:24:16 @agent_ppo2.py:185][0m |          -0.0004 |         117.2203 |         -81.7409 |
[32m[20221214 14:24:16 @agent_ppo2.py:185][0m |           0.0037 |         116.9633 |         -81.6552 |
[32m[20221214 14:24:16 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:24:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 763.74
[32m[20221214 14:24:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.66
[32m[20221214 14:24:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.47
[32m[20221214 14:24:16 @agent_ppo2.py:143][0m Total time:      26.24 min
[32m[20221214 14:24:16 @agent_ppo2.py:145][0m 2398208 total steps have happened
[32m[20221214 14:24:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1171 --------------------------#
[32m[20221214 14:24:17 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:17 @agent_ppo2.py:185][0m |          -0.0017 |         130.0531 |         -82.7737 |
[32m[20221214 14:24:17 @agent_ppo2.py:185][0m |           0.0022 |         121.6881 |         -82.7365 |
[32m[20221214 14:24:17 @agent_ppo2.py:185][0m |          -0.0008 |         119.0150 |         -82.8735 |
[32m[20221214 14:24:17 @agent_ppo2.py:185][0m |           0.0087 |         127.4948 |         -82.6936 |
[32m[20221214 14:24:17 @agent_ppo2.py:185][0m |          -0.0031 |         117.2718 |         -82.5803 |
[32m[20221214 14:24:17 @agent_ppo2.py:185][0m |          -0.0029 |         116.2312 |         -82.6891 |
[32m[20221214 14:24:17 @agent_ppo2.py:185][0m |          -0.0024 |         116.1244 |         -82.2920 |
[32m[20221214 14:24:18 @agent_ppo2.py:185][0m |          -0.0035 |         115.4659 |         -82.5190 |
[32m[20221214 14:24:18 @agent_ppo2.py:185][0m |          -0.0029 |         114.5173 |         -82.5128 |
[32m[20221214 14:24:18 @agent_ppo2.py:185][0m |           0.0027 |         116.0574 |         -82.4252 |
[32m[20221214 14:24:18 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:24:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 757.62
[32m[20221214 14:24:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.78
[32m[20221214 14:24:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 702.85
[32m[20221214 14:24:18 @agent_ppo2.py:143][0m Total time:      26.26 min
[32m[20221214 14:24:18 @agent_ppo2.py:145][0m 2400256 total steps have happened
[32m[20221214 14:24:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1172 --------------------------#
[32m[20221214 14:24:18 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:18 @agent_ppo2.py:185][0m |          -0.0003 |         129.4233 |         -80.6198 |
[32m[20221214 14:24:18 @agent_ppo2.py:185][0m |          -0.0033 |         116.6870 |         -80.6675 |
[32m[20221214 14:24:18 @agent_ppo2.py:185][0m |           0.0043 |         112.7872 |         -80.6467 |
[32m[20221214 14:24:19 @agent_ppo2.py:185][0m |           0.0002 |         109.6044 |         -80.3475 |
[32m[20221214 14:24:19 @agent_ppo2.py:185][0m |          -0.0028 |         107.2728 |         -80.4596 |
[32m[20221214 14:24:19 @agent_ppo2.py:185][0m |           0.0044 |         110.0023 |         -80.3180 |
[32m[20221214 14:24:19 @agent_ppo2.py:185][0m |          -0.0064 |         105.2073 |         -80.2328 |
[32m[20221214 14:24:19 @agent_ppo2.py:185][0m |          -0.0012 |         104.6558 |         -79.8737 |
[32m[20221214 14:24:19 @agent_ppo2.py:185][0m |           0.0013 |         104.9062 |         -80.0945 |
[32m[20221214 14:24:19 @agent_ppo2.py:185][0m |          -0.0038 |         103.6815 |         -80.2078 |
[32m[20221214 14:24:19 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:24:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 731.60
[32m[20221214 14:24:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.22
[32m[20221214 14:24:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.48
[32m[20221214 14:24:19 @agent_ppo2.py:143][0m Total time:      26.29 min
[32m[20221214 14:24:19 @agent_ppo2.py:145][0m 2402304 total steps have happened
[32m[20221214 14:24:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1173 --------------------------#
[32m[20221214 14:24:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:20 @agent_ppo2.py:185][0m |          -0.0001 |         105.4147 |         -81.1285 |
[32m[20221214 14:24:20 @agent_ppo2.py:185][0m |          -0.0006 |         100.4766 |         -81.0688 |
[32m[20221214 14:24:20 @agent_ppo2.py:185][0m |           0.0114 |         108.4257 |         -80.7999 |
[32m[20221214 14:24:20 @agent_ppo2.py:185][0m |           0.0004 |          97.6364 |         -80.9772 |
[32m[20221214 14:24:20 @agent_ppo2.py:185][0m |          -0.0005 |          96.9685 |         -80.9021 |
[32m[20221214 14:24:20 @agent_ppo2.py:185][0m |           0.0010 |          96.6620 |         -80.7561 |
[32m[20221214 14:24:20 @agent_ppo2.py:185][0m |          -0.0016 |          96.3750 |         -80.7557 |
[32m[20221214 14:24:20 @agent_ppo2.py:185][0m |          -0.0016 |          95.9956 |         -80.7227 |
[32m[20221214 14:24:21 @agent_ppo2.py:185][0m |          -0.0069 |          96.3106 |         -80.6866 |
[32m[20221214 14:24:21 @agent_ppo2.py:185][0m |          -0.0042 |          95.1349 |         -80.5959 |
[32m[20221214 14:24:21 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:24:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.49
[32m[20221214 14:24:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.11
[32m[20221214 14:24:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 667.12
[32m[20221214 14:24:21 @agent_ppo2.py:143][0m Total time:      26.31 min
[32m[20221214 14:24:21 @agent_ppo2.py:145][0m 2404352 total steps have happened
[32m[20221214 14:24:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1174 --------------------------#
[32m[20221214 14:24:21 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:21 @agent_ppo2.py:185][0m |           0.0038 |         111.1675 |         -80.1626 |
[32m[20221214 14:24:21 @agent_ppo2.py:185][0m |          -0.0033 |          97.0026 |         -80.3514 |
[32m[20221214 14:24:21 @agent_ppo2.py:185][0m |           0.0012 |          91.2795 |         -80.5121 |
[32m[20221214 14:24:21 @agent_ppo2.py:185][0m |          -0.0018 |          87.5041 |         -80.3748 |
[32m[20221214 14:24:22 @agent_ppo2.py:185][0m |          -0.0024 |          84.9618 |         -80.5134 |
[32m[20221214 14:24:22 @agent_ppo2.py:185][0m |          -0.0008 |          83.7461 |         -80.6561 |
[32m[20221214 14:24:22 @agent_ppo2.py:185][0m |           0.0003 |          81.3310 |         -80.7390 |
[32m[20221214 14:24:22 @agent_ppo2.py:185][0m |          -0.0010 |          80.1847 |         -80.8026 |
[32m[20221214 14:24:22 @agent_ppo2.py:185][0m |          -0.0022 |          78.7535 |         -80.8637 |
[32m[20221214 14:24:22 @agent_ppo2.py:185][0m |           0.0082 |          84.7037 |         -81.0761 |
[32m[20221214 14:24:22 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:24:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 727.08
[32m[20221214 14:24:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.83
[32m[20221214 14:24:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 735.26
[32m[20221214 14:24:22 @agent_ppo2.py:143][0m Total time:      26.34 min
[32m[20221214 14:24:22 @agent_ppo2.py:145][0m 2406400 total steps have happened
[32m[20221214 14:24:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1175 --------------------------#
[32m[20221214 14:24:22 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:23 @agent_ppo2.py:185][0m |           0.0028 |         178.3443 |         -82.0497 |
[32m[20221214 14:24:23 @agent_ppo2.py:185][0m |          -0.0012 |         160.2604 |         -82.0363 |
[32m[20221214 14:24:23 @agent_ppo2.py:185][0m |           0.0032 |         153.6753 |         -81.5750 |
[32m[20221214 14:24:23 @agent_ppo2.py:185][0m |          -0.0012 |         150.1913 |         -81.8126 |
[32m[20221214 14:24:23 @agent_ppo2.py:185][0m |          -0.0020 |         148.0577 |         -81.9167 |
[32m[20221214 14:24:23 @agent_ppo2.py:185][0m |          -0.0016 |         147.1476 |         -82.1143 |
[32m[20221214 14:24:23 @agent_ppo2.py:185][0m |          -0.0022 |         145.8920 |         -82.0452 |
[32m[20221214 14:24:23 @agent_ppo2.py:185][0m |          -0.0005 |         144.3260 |         -82.1700 |
[32m[20221214 14:24:23 @agent_ppo2.py:185][0m |          -0.0018 |         143.3471 |         -82.1522 |
[32m[20221214 14:24:24 @agent_ppo2.py:185][0m |           0.0013 |         143.0684 |         -82.0661 |
[32m[20221214 14:24:24 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:24:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.04
[32m[20221214 14:24:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.17
[32m[20221214 14:24:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.15
[32m[20221214 14:24:24 @agent_ppo2.py:143][0m Total time:      26.36 min
[32m[20221214 14:24:24 @agent_ppo2.py:145][0m 2408448 total steps have happened
[32m[20221214 14:24:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1176 --------------------------#
[32m[20221214 14:24:24 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:24 @agent_ppo2.py:185][0m |          -0.0037 |         133.7251 |         -81.6808 |
[32m[20221214 14:24:24 @agent_ppo2.py:185][0m |          -0.0028 |         111.5888 |         -81.9178 |
[32m[20221214 14:24:24 @agent_ppo2.py:185][0m |           0.0025 |         102.6046 |         -81.9191 |
[32m[20221214 14:24:24 @agent_ppo2.py:185][0m |          -0.0013 |          96.1065 |         -82.0594 |
[32m[20221214 14:24:24 @agent_ppo2.py:185][0m |          -0.0021 |          91.8081 |         -82.1275 |
[32m[20221214 14:24:25 @agent_ppo2.py:185][0m |          -0.0025 |          89.2104 |         -81.9947 |
[32m[20221214 14:24:25 @agent_ppo2.py:185][0m |           0.0006 |          88.3968 |         -81.9745 |
[32m[20221214 14:24:25 @agent_ppo2.py:185][0m |          -0.0039 |          86.2145 |         -82.1295 |
[32m[20221214 14:24:25 @agent_ppo2.py:185][0m |           0.0137 |          90.9322 |         -82.1704 |
[32m[20221214 14:24:25 @agent_ppo2.py:185][0m |           0.0035 |          87.5197 |         -82.1546 |
[32m[20221214 14:24:25 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:24:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 736.12
[32m[20221214 14:24:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 761.39
[32m[20221214 14:24:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.97
[32m[20221214 14:24:25 @agent_ppo2.py:143][0m Total time:      26.39 min
[32m[20221214 14:24:25 @agent_ppo2.py:145][0m 2410496 total steps have happened
[32m[20221214 14:24:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1177 --------------------------#
[32m[20221214 14:24:25 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:26 @agent_ppo2.py:185][0m |           0.0012 |         122.9429 |         -82.5638 |
[32m[20221214 14:24:26 @agent_ppo2.py:185][0m |           0.0088 |         107.9836 |         -82.7251 |
[32m[20221214 14:24:26 @agent_ppo2.py:185][0m |          -0.0009 |         101.0275 |         -82.4725 |
[32m[20221214 14:24:26 @agent_ppo2.py:185][0m |          -0.0038 |          97.8404 |         -82.4908 |
[32m[20221214 14:24:26 @agent_ppo2.py:185][0m |          -0.0002 |          95.6482 |         -82.3982 |
[32m[20221214 14:24:26 @agent_ppo2.py:185][0m |          -0.0041 |          93.9872 |         -82.7512 |
[32m[20221214 14:24:26 @agent_ppo2.py:185][0m |           0.0013 |          91.8285 |         -82.6614 |
[32m[20221214 14:24:26 @agent_ppo2.py:185][0m |          -0.0013 |          91.9371 |         -82.5113 |
[32m[20221214 14:24:26 @agent_ppo2.py:185][0m |          -0.0029 |          90.9902 |         -82.7567 |
[32m[20221214 14:24:26 @agent_ppo2.py:185][0m |          -0.0009 |          89.2959 |         -82.6711 |
[32m[20221214 14:24:26 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 14:24:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.01
[32m[20221214 14:24:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.30
[32m[20221214 14:24:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.71
[32m[20221214 14:24:27 @agent_ppo2.py:143][0m Total time:      26.41 min
[32m[20221214 14:24:27 @agent_ppo2.py:145][0m 2412544 total steps have happened
[32m[20221214 14:24:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1178 --------------------------#
[32m[20221214 14:24:27 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:27 @agent_ppo2.py:185][0m |          -0.0031 |         118.6957 |         -81.2721 |
[32m[20221214 14:24:27 @agent_ppo2.py:185][0m |          -0.0014 |         104.5596 |         -81.3968 |
[32m[20221214 14:24:27 @agent_ppo2.py:185][0m |          -0.0043 |          99.0627 |         -81.1631 |
[32m[20221214 14:24:27 @agent_ppo2.py:185][0m |           0.0039 |         104.0881 |         -81.5198 |
[32m[20221214 14:24:27 @agent_ppo2.py:185][0m |          -0.0037 |          95.0784 |         -81.3594 |
[32m[20221214 14:24:28 @agent_ppo2.py:185][0m |           0.0052 |          96.0134 |         -81.3155 |
[32m[20221214 14:24:28 @agent_ppo2.py:185][0m |           0.0024 |         102.7054 |         -81.4404 |
[32m[20221214 14:24:28 @agent_ppo2.py:185][0m |          -0.0049 |          91.5736 |         -81.3378 |
[32m[20221214 14:24:28 @agent_ppo2.py:185][0m |          -0.0045 |          90.2198 |         -81.5692 |
[32m[20221214 14:24:28 @agent_ppo2.py:185][0m |          -0.0061 |          89.4044 |         -81.3771 |
[32m[20221214 14:24:28 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 14:24:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 740.94
[32m[20221214 14:24:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 773.52
[32m[20221214 14:24:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 774.40
[32m[20221214 14:24:28 @agent_ppo2.py:143][0m Total time:      26.43 min
[32m[20221214 14:24:28 @agent_ppo2.py:145][0m 2414592 total steps have happened
[32m[20221214 14:24:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1179 --------------------------#
[32m[20221214 14:24:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:24:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:28 @agent_ppo2.py:185][0m |           0.0063 |         116.5493 |         -82.7670 |
[32m[20221214 14:24:29 @agent_ppo2.py:185][0m |          -0.0041 |         109.3963 |         -82.7456 |
[32m[20221214 14:24:29 @agent_ppo2.py:185][0m |           0.0172 |         125.4287 |         -82.7661 |
[32m[20221214 14:24:29 @agent_ppo2.py:185][0m |           0.0050 |         111.6240 |         -82.7949 |
[32m[20221214 14:24:29 @agent_ppo2.py:185][0m |          -0.0030 |         105.2380 |         -82.7442 |
[32m[20221214 14:24:29 @agent_ppo2.py:185][0m |          -0.0038 |         104.3986 |         -82.8459 |
[32m[20221214 14:24:29 @agent_ppo2.py:185][0m |          -0.0006 |         103.5792 |         -82.8083 |
[32m[20221214 14:24:29 @agent_ppo2.py:185][0m |          -0.0048 |         103.4782 |         -82.8136 |
[32m[20221214 14:24:29 @agent_ppo2.py:185][0m |           0.0001 |         102.7909 |         -82.7207 |
[32m[20221214 14:24:29 @agent_ppo2.py:185][0m |           0.0172 |         119.5003 |         -82.7304 |
[32m[20221214 14:24:29 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:24:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 771.91
[32m[20221214 14:24:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.26
[32m[20221214 14:24:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 774.90
[32m[20221214 14:24:29 @agent_ppo2.py:143][0m Total time:      26.46 min
[32m[20221214 14:24:29 @agent_ppo2.py:145][0m 2416640 total steps have happened
[32m[20221214 14:24:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1180 --------------------------#
[32m[20221214 14:24:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:24:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:30 @agent_ppo2.py:185][0m |          -0.0010 |         117.3584 |         -83.4054 |
[32m[20221214 14:24:30 @agent_ppo2.py:185][0m |          -0.0041 |         109.9684 |         -83.2619 |
[32m[20221214 14:24:30 @agent_ppo2.py:185][0m |          -0.0042 |         108.8939 |         -83.2818 |
[32m[20221214 14:24:30 @agent_ppo2.py:185][0m |          -0.0049 |         107.1609 |         -83.2844 |
[32m[20221214 14:24:30 @agent_ppo2.py:185][0m |          -0.0045 |         105.8701 |         -83.1089 |
[32m[20221214 14:24:30 @agent_ppo2.py:185][0m |          -0.0038 |         105.2878 |         -83.1378 |
[32m[20221214 14:24:30 @agent_ppo2.py:185][0m |          -0.0066 |         104.6430 |         -83.1657 |
[32m[20221214 14:24:30 @agent_ppo2.py:185][0m |          -0.0018 |         104.5055 |         -83.1188 |
[32m[20221214 14:24:30 @agent_ppo2.py:185][0m |           0.0014 |         105.0871 |         -82.8834 |
[32m[20221214 14:24:31 @agent_ppo2.py:185][0m |          -0.0025 |         103.4490 |         -83.1319 |
[32m[20221214 14:24:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:24:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.04
[32m[20221214 14:24:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.57
[32m[20221214 14:24:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.03
[32m[20221214 14:24:31 @agent_ppo2.py:143][0m Total time:      26.48 min
[32m[20221214 14:24:31 @agent_ppo2.py:145][0m 2418688 total steps have happened
[32m[20221214 14:24:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1181 --------------------------#
[32m[20221214 14:24:31 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:24:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:31 @agent_ppo2.py:185][0m |           0.0011 |         105.8164 |         -81.8299 |
[32m[20221214 14:24:31 @agent_ppo2.py:185][0m |           0.0006 |          99.2158 |         -81.4677 |
[32m[20221214 14:24:31 @agent_ppo2.py:185][0m |           0.0015 |          97.2177 |         -81.3738 |
[32m[20221214 14:24:31 @agent_ppo2.py:185][0m |          -0.0007 |          96.4859 |         -81.4599 |
[32m[20221214 14:24:31 @agent_ppo2.py:185][0m |          -0.0005 |          95.0242 |         -81.4278 |
[32m[20221214 14:24:31 @agent_ppo2.py:185][0m |           0.0006 |          95.1316 |         -81.4939 |
[32m[20221214 14:24:32 @agent_ppo2.py:185][0m |          -0.0030 |          94.6527 |         -81.4561 |
[32m[20221214 14:24:32 @agent_ppo2.py:185][0m |           0.0011 |          93.5333 |         -81.3786 |
[32m[20221214 14:24:32 @agent_ppo2.py:185][0m |           0.0005 |          92.8678 |         -81.6119 |
[32m[20221214 14:24:32 @agent_ppo2.py:185][0m |          -0.0019 |          92.5950 |         -81.5129 |
[32m[20221214 14:24:32 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:24:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.27
[32m[20221214 14:24:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.54
[32m[20221214 14:24:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.03
[32m[20221214 14:24:32 @agent_ppo2.py:143][0m Total time:      26.50 min
[32m[20221214 14:24:32 @agent_ppo2.py:145][0m 2420736 total steps have happened
[32m[20221214 14:24:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1182 --------------------------#
[32m[20221214 14:24:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:24:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:32 @agent_ppo2.py:185][0m |           0.0011 |         110.1056 |         -84.0741 |
[32m[20221214 14:24:32 @agent_ppo2.py:185][0m |          -0.0005 |         104.9335 |         -83.7822 |
[32m[20221214 14:24:32 @agent_ppo2.py:185][0m |          -0.0019 |         102.7753 |         -83.9682 |
[32m[20221214 14:24:33 @agent_ppo2.py:185][0m |           0.0023 |         101.2583 |         -83.8717 |
[32m[20221214 14:24:33 @agent_ppo2.py:185][0m |          -0.0024 |         100.9145 |         -84.0047 |
[32m[20221214 14:24:33 @agent_ppo2.py:185][0m |          -0.0023 |          99.5453 |         -84.0405 |
[32m[20221214 14:24:33 @agent_ppo2.py:185][0m |          -0.0010 |          98.8331 |         -84.1448 |
[32m[20221214 14:24:33 @agent_ppo2.py:185][0m |           0.0060 |         101.8736 |         -84.0388 |
[32m[20221214 14:24:33 @agent_ppo2.py:185][0m |           0.0023 |          98.1616 |         -84.0258 |
[32m[20221214 14:24:33 @agent_ppo2.py:185][0m |          -0.0047 |          97.9447 |         -83.9749 |
[32m[20221214 14:24:33 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:24:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 752.56
[32m[20221214 14:24:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 775.21
[32m[20221214 14:24:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 778.18
[32m[20221214 14:24:33 @agent_ppo2.py:143][0m Total time:      26.52 min
[32m[20221214 14:24:33 @agent_ppo2.py:145][0m 2422784 total steps have happened
[32m[20221214 14:24:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1183 --------------------------#
[32m[20221214 14:24:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:24:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:34 @agent_ppo2.py:185][0m |          -0.0041 |          82.7314 |         -84.6512 |
[32m[20221214 14:24:34 @agent_ppo2.py:185][0m |          -0.0028 |          73.7883 |         -84.9886 |
[32m[20221214 14:24:34 @agent_ppo2.py:185][0m |          -0.0030 |          70.0162 |         -84.7089 |
[32m[20221214 14:24:34 @agent_ppo2.py:185][0m |          -0.0035 |          67.8336 |         -85.1188 |
[32m[20221214 14:24:34 @agent_ppo2.py:185][0m |          -0.0043 |          66.4396 |         -85.2066 |
[32m[20221214 14:24:34 @agent_ppo2.py:185][0m |          -0.0035 |          63.6694 |         -85.1506 |
[32m[20221214 14:24:34 @agent_ppo2.py:185][0m |          -0.0015 |          63.3444 |         -85.0974 |
[32m[20221214 14:24:34 @agent_ppo2.py:185][0m |          -0.0061 |          62.7332 |         -85.4218 |
[32m[20221214 14:24:34 @agent_ppo2.py:185][0m |          -0.0017 |          61.7048 |         -85.3659 |
[32m[20221214 14:24:34 @agent_ppo2.py:185][0m |          -0.0057 |          62.0006 |         -85.4432 |
[32m[20221214 14:24:34 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:24:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.53
[32m[20221214 14:24:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 773.91
[32m[20221214 14:24:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.47
[32m[20221214 14:24:34 @agent_ppo2.py:143][0m Total time:      26.54 min
[32m[20221214 14:24:34 @agent_ppo2.py:145][0m 2424832 total steps have happened
[32m[20221214 14:24:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1184 --------------------------#
[32m[20221214 14:24:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:24:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:35 @agent_ppo2.py:185][0m |          -0.0018 |         133.0405 |         -84.0415 |
[32m[20221214 14:24:35 @agent_ppo2.py:185][0m |          -0.0007 |         125.5692 |         -84.0136 |
[32m[20221214 14:24:35 @agent_ppo2.py:185][0m |          -0.0054 |         122.4638 |         -83.6103 |
[32m[20221214 14:24:35 @agent_ppo2.py:185][0m |          -0.0036 |         120.5600 |         -83.8452 |
[32m[20221214 14:24:35 @agent_ppo2.py:185][0m |          -0.0043 |         119.6625 |         -83.7654 |
[32m[20221214 14:24:35 @agent_ppo2.py:185][0m |          -0.0007 |         117.4570 |         -83.8363 |
[32m[20221214 14:24:35 @agent_ppo2.py:185][0m |          -0.0016 |         116.5796 |         -83.9311 |
[32m[20221214 14:24:35 @agent_ppo2.py:185][0m |          -0.0005 |         115.8842 |         -83.9243 |
[32m[20221214 14:24:35 @agent_ppo2.py:185][0m |          -0.0005 |         114.8854 |         -83.7578 |
[32m[20221214 14:24:36 @agent_ppo2.py:185][0m |          -0.0021 |         113.6824 |         -83.5437 |
[32m[20221214 14:24:36 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:24:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.62
[32m[20221214 14:24:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.83
[32m[20221214 14:24:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.84
[32m[20221214 14:24:36 @agent_ppo2.py:143][0m Total time:      26.56 min
[32m[20221214 14:24:36 @agent_ppo2.py:145][0m 2426880 total steps have happened
[32m[20221214 14:24:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1185 --------------------------#
[32m[20221214 14:24:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:24:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:36 @agent_ppo2.py:185][0m |          -0.0043 |         119.1880 |         -84.2721 |
[32m[20221214 14:24:36 @agent_ppo2.py:185][0m |          -0.0030 |         111.2171 |         -83.8159 |
[32m[20221214 14:24:36 @agent_ppo2.py:185][0m |          -0.0012 |         105.5533 |         -83.9316 |
[32m[20221214 14:24:36 @agent_ppo2.py:185][0m |          -0.0032 |         102.3629 |         -83.7666 |
[32m[20221214 14:24:36 @agent_ppo2.py:185][0m |          -0.0018 |          99.8746 |         -83.8153 |
[32m[20221214 14:24:36 @agent_ppo2.py:185][0m |          -0.0038 |          98.8180 |         -83.8214 |
[32m[20221214 14:24:37 @agent_ppo2.py:185][0m |          -0.0048 |          98.2967 |         -83.8100 |
[32m[20221214 14:24:37 @agent_ppo2.py:185][0m |          -0.0034 |          96.3138 |         -83.7046 |
[32m[20221214 14:24:37 @agent_ppo2.py:185][0m |          -0.0028 |          95.5901 |         -83.6914 |
[32m[20221214 14:24:37 @agent_ppo2.py:185][0m |          -0.0020 |          94.8735 |         -83.6576 |
[32m[20221214 14:24:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:24:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 779.34
[32m[20221214 14:24:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.79
[32m[20221214 14:24:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 787.40
[32m[20221214 14:24:37 @agent_ppo2.py:143][0m Total time:      26.58 min
[32m[20221214 14:24:37 @agent_ppo2.py:145][0m 2428928 total steps have happened
[32m[20221214 14:24:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1186 --------------------------#
[32m[20221214 14:24:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:24:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:37 @agent_ppo2.py:185][0m |           0.0107 |         141.9544 |         -85.3923 |
[32m[20221214 14:24:37 @agent_ppo2.py:185][0m |          -0.0010 |         115.2215 |         -85.0744 |
[32m[20221214 14:24:37 @agent_ppo2.py:185][0m |           0.0101 |         118.0336 |         -85.3155 |
[32m[20221214 14:24:37 @agent_ppo2.py:185][0m |           0.0119 |         123.8418 |         -85.3333 |
[32m[20221214 14:24:38 @agent_ppo2.py:185][0m |          -0.0014 |         107.9362 |         -85.2505 |
[32m[20221214 14:24:38 @agent_ppo2.py:185][0m |          -0.0016 |         107.7104 |         -85.2913 |
[32m[20221214 14:24:38 @agent_ppo2.py:185][0m |          -0.0027 |         106.2053 |         -85.2381 |
[32m[20221214 14:24:38 @agent_ppo2.py:185][0m |          -0.0008 |         105.5285 |         -85.1652 |
[32m[20221214 14:24:38 @agent_ppo2.py:185][0m |           0.0012 |         104.9887 |         -85.3941 |
[32m[20221214 14:24:38 @agent_ppo2.py:185][0m |          -0.0028 |         105.0631 |         -85.2393 |
[32m[20221214 14:24:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:24:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.91
[32m[20221214 14:24:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.52
[32m[20221214 14:24:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 789.02
[32m[20221214 14:24:38 @agent_ppo2.py:143][0m Total time:      26.60 min
[32m[20221214 14:24:38 @agent_ppo2.py:145][0m 2430976 total steps have happened
[32m[20221214 14:24:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1187 --------------------------#
[32m[20221214 14:24:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:24:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:38 @agent_ppo2.py:185][0m |          -0.0008 |         120.2466 |         -85.1560 |
[32m[20221214 14:24:39 @agent_ppo2.py:185][0m |          -0.0049 |         112.0601 |         -85.0335 |
[32m[20221214 14:24:39 @agent_ppo2.py:185][0m |          -0.0012 |         108.2635 |         -85.0166 |
[32m[20221214 14:24:39 @agent_ppo2.py:185][0m |           0.0098 |         115.9043 |         -84.9389 |
[32m[20221214 14:24:39 @agent_ppo2.py:185][0m |          -0.0036 |         105.6876 |         -84.8998 |
[32m[20221214 14:24:39 @agent_ppo2.py:185][0m |          -0.0052 |         106.3656 |         -84.8474 |
[32m[20221214 14:24:39 @agent_ppo2.py:185][0m |          -0.0041 |         104.2228 |         -84.8410 |
[32m[20221214 14:24:39 @agent_ppo2.py:185][0m |          -0.0057 |         103.7045 |         -84.4119 |
[32m[20221214 14:24:39 @agent_ppo2.py:185][0m |          -0.0043 |         103.7468 |         -84.7634 |
[32m[20221214 14:24:39 @agent_ppo2.py:185][0m |          -0.0073 |         103.2104 |         -84.7347 |
[32m[20221214 14:24:39 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:24:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.23
[32m[20221214 14:24:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.72
[32m[20221214 14:24:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 788.60
[32m[20221214 14:24:39 @agent_ppo2.py:143][0m Total time:      26.62 min
[32m[20221214 14:24:39 @agent_ppo2.py:145][0m 2433024 total steps have happened
[32m[20221214 14:24:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1188 --------------------------#
[32m[20221214 14:24:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:24:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:40 @agent_ppo2.py:185][0m |          -0.0038 |         112.4170 |         -84.7536 |
[32m[20221214 14:24:40 @agent_ppo2.py:185][0m |           0.0008 |         108.6745 |         -84.7335 |
[32m[20221214 14:24:40 @agent_ppo2.py:185][0m |          -0.0015 |         105.7627 |         -84.7492 |
[32m[20221214 14:24:40 @agent_ppo2.py:185][0m |           0.0038 |         107.3273 |         -84.8271 |
[32m[20221214 14:24:40 @agent_ppo2.py:185][0m |           0.0071 |         105.7579 |         -85.1307 |
[32m[20221214 14:24:40 @agent_ppo2.py:185][0m |           0.0108 |         116.6951 |         -84.9774 |
[32m[20221214 14:24:40 @agent_ppo2.py:185][0m |          -0.0051 |         103.7463 |         -85.1456 |
[32m[20221214 14:24:40 @agent_ppo2.py:185][0m |          -0.0037 |         102.9432 |         -85.0058 |
[32m[20221214 14:24:40 @agent_ppo2.py:185][0m |          -0.0036 |         103.9551 |         -85.1125 |
[32m[20221214 14:24:40 @agent_ppo2.py:185][0m |          -0.0011 |         102.6457 |         -85.3309 |
[32m[20221214 14:24:40 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:24:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.54
[32m[20221214 14:24:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.13
[32m[20221214 14:24:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.07
[32m[20221214 14:24:41 @agent_ppo2.py:143][0m Total time:      26.64 min
[32m[20221214 14:24:41 @agent_ppo2.py:145][0m 2435072 total steps have happened
[32m[20221214 14:24:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1189 --------------------------#
[32m[20221214 14:24:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:24:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:41 @agent_ppo2.py:185][0m |           0.0021 |         105.9303 |         -85.3148 |
[32m[20221214 14:24:41 @agent_ppo2.py:185][0m |          -0.0019 |         102.7587 |         -85.2479 |
[32m[20221214 14:24:41 @agent_ppo2.py:185][0m |          -0.0007 |          98.3639 |         -85.0949 |
[32m[20221214 14:24:41 @agent_ppo2.py:185][0m |           0.0106 |         105.6845 |         -85.3443 |
[32m[20221214 14:24:41 @agent_ppo2.py:185][0m |          -0.0003 |          95.2656 |         -85.0399 |
[32m[20221214 14:24:41 @agent_ppo2.py:185][0m |          -0.0015 |          92.5308 |         -85.4969 |
[32m[20221214 14:24:41 @agent_ppo2.py:185][0m |           0.0010 |          91.5777 |         -85.3802 |
[32m[20221214 14:24:41 @agent_ppo2.py:185][0m |          -0.0020 |          91.1923 |         -85.7025 |
[32m[20221214 14:24:42 @agent_ppo2.py:185][0m |          -0.0035 |          90.8344 |         -85.3413 |
[32m[20221214 14:24:42 @agent_ppo2.py:185][0m |          -0.0019 |          90.4885 |         -85.6380 |
[32m[20221214 14:24:42 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:24:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.87
[32m[20221214 14:24:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.46
[32m[20221214 14:24:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 788.08
[32m[20221214 14:24:42 @agent_ppo2.py:143][0m Total time:      26.66 min
[32m[20221214 14:24:42 @agent_ppo2.py:145][0m 2437120 total steps have happened
[32m[20221214 14:24:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1190 --------------------------#
[32m[20221214 14:24:42 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:24:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:42 @agent_ppo2.py:185][0m |          -0.0015 |         106.7494 |         -84.8537 |
[32m[20221214 14:24:42 @agent_ppo2.py:185][0m |           0.0137 |         110.6356 |         -84.6544 |
[32m[20221214 14:24:42 @agent_ppo2.py:185][0m |          -0.0005 |          99.8732 |         -84.5902 |
[32m[20221214 14:24:42 @agent_ppo2.py:185][0m |          -0.0031 |          98.3243 |         -84.6150 |
[32m[20221214 14:24:43 @agent_ppo2.py:185][0m |          -0.0060 |          98.3256 |         -84.6942 |
[32m[20221214 14:24:43 @agent_ppo2.py:185][0m |          -0.0040 |          96.3976 |         -84.7560 |
[32m[20221214 14:24:43 @agent_ppo2.py:185][0m |          -0.0039 |          95.8393 |         -84.7865 |
[32m[20221214 14:24:43 @agent_ppo2.py:185][0m |          -0.0042 |          95.2935 |         -84.7583 |
[32m[20221214 14:24:43 @agent_ppo2.py:185][0m |          -0.0026 |          97.8999 |         -84.6642 |
[32m[20221214 14:24:43 @agent_ppo2.py:185][0m |          -0.0068 |          95.5389 |         -84.8005 |
[32m[20221214 14:24:43 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:24:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.22
[32m[20221214 14:24:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.36
[32m[20221214 14:24:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.06
[32m[20221214 14:24:43 @agent_ppo2.py:143][0m Total time:      26.68 min
[32m[20221214 14:24:43 @agent_ppo2.py:145][0m 2439168 total steps have happened
[32m[20221214 14:24:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1191 --------------------------#
[32m[20221214 14:24:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:24:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:43 @agent_ppo2.py:185][0m |          -0.0021 |         116.1584 |         -87.0269 |
[32m[20221214 14:24:43 @agent_ppo2.py:185][0m |          -0.0036 |         113.3296 |         -86.7472 |
[32m[20221214 14:24:44 @agent_ppo2.py:185][0m |          -0.0055 |         112.4245 |         -87.0279 |
[32m[20221214 14:24:44 @agent_ppo2.py:185][0m |          -0.0056 |         112.3094 |         -86.7609 |
[32m[20221214 14:24:44 @agent_ppo2.py:185][0m |          -0.0037 |         110.4891 |         -86.8459 |
[32m[20221214 14:24:44 @agent_ppo2.py:185][0m |          -0.0030 |         109.9753 |         -86.7593 |
[32m[20221214 14:24:44 @agent_ppo2.py:185][0m |          -0.0004 |         113.4868 |         -86.7174 |
[32m[20221214 14:24:44 @agent_ppo2.py:185][0m |          -0.0032 |         108.9518 |         -86.9547 |
[32m[20221214 14:24:44 @agent_ppo2.py:185][0m |          -0.0089 |         109.7364 |         -86.6541 |
[32m[20221214 14:24:44 @agent_ppo2.py:185][0m |          -0.0044 |         108.8622 |         -86.9216 |
[32m[20221214 14:24:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:24:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.21
[32m[20221214 14:24:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.82
[32m[20221214 14:24:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.58
[32m[20221214 14:24:44 @agent_ppo2.py:143][0m Total time:      26.70 min
[32m[20221214 14:24:44 @agent_ppo2.py:145][0m 2441216 total steps have happened
[32m[20221214 14:24:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1192 --------------------------#
[32m[20221214 14:24:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:24:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:45 @agent_ppo2.py:185][0m |          -0.0007 |          98.9730 |         -88.7551 |
[32m[20221214 14:24:45 @agent_ppo2.py:185][0m |           0.0019 |          89.9621 |         -88.3676 |
[32m[20221214 14:24:45 @agent_ppo2.py:185][0m |          -0.0019 |          84.2721 |         -88.5602 |
[32m[20221214 14:24:45 @agent_ppo2.py:185][0m |          -0.0031 |          82.7157 |         -88.5895 |
[32m[20221214 14:24:45 @agent_ppo2.py:185][0m |          -0.0026 |          81.3358 |         -88.5614 |
[32m[20221214 14:24:45 @agent_ppo2.py:185][0m |          -0.0052 |          79.1928 |         -88.4605 |
[32m[20221214 14:24:45 @agent_ppo2.py:185][0m |          -0.0040 |          78.5714 |         -88.5902 |
[32m[20221214 14:24:45 @agent_ppo2.py:185][0m |          -0.0041 |          76.9655 |         -88.5388 |
[32m[20221214 14:24:45 @agent_ppo2.py:185][0m |           0.0018 |          76.6402 |         -88.4575 |
[32m[20221214 14:24:45 @agent_ppo2.py:185][0m |           0.0025 |          79.3071 |         -88.5477 |
[32m[20221214 14:24:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:24:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.04
[32m[20221214 14:24:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.97
[32m[20221214 14:24:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.91
[32m[20221214 14:24:46 @agent_ppo2.py:143][0m Total time:      26.73 min
[32m[20221214 14:24:46 @agent_ppo2.py:145][0m 2443264 total steps have happened
[32m[20221214 14:24:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1193 --------------------------#
[32m[20221214 14:24:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:24:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:46 @agent_ppo2.py:185][0m |           0.0005 |         113.5829 |         -87.9359 |
[32m[20221214 14:24:46 @agent_ppo2.py:185][0m |           0.0071 |         111.5512 |         -87.9141 |
[32m[20221214 14:24:46 @agent_ppo2.py:185][0m |          -0.0046 |         104.5304 |         -87.7744 |
[32m[20221214 14:24:46 @agent_ppo2.py:185][0m |           0.0048 |         105.6809 |         -87.8224 |
[32m[20221214 14:24:46 @agent_ppo2.py:185][0m |          -0.0015 |         102.4975 |         -87.9111 |
[32m[20221214 14:24:46 @agent_ppo2.py:185][0m |          -0.0015 |         100.4310 |         -87.9474 |
[32m[20221214 14:24:46 @agent_ppo2.py:185][0m |          -0.0017 |         100.0397 |         -88.1702 |
[32m[20221214 14:24:47 @agent_ppo2.py:185][0m |          -0.0046 |          98.7074 |         -87.9504 |
[32m[20221214 14:24:47 @agent_ppo2.py:185][0m |          -0.0039 |          97.8927 |         -88.0897 |
[32m[20221214 14:24:47 @agent_ppo2.py:185][0m |          -0.0027 |          97.1361 |         -88.2399 |
[32m[20221214 14:24:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:24:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.43
[32m[20221214 14:24:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.43
[32m[20221214 14:24:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.38
[32m[20221214 14:24:47 @agent_ppo2.py:143][0m Total time:      26.75 min
[32m[20221214 14:24:47 @agent_ppo2.py:145][0m 2445312 total steps have happened
[32m[20221214 14:24:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1194 --------------------------#
[32m[20221214 14:24:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:24:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:47 @agent_ppo2.py:185][0m |          -0.0057 |          82.5900 |         -87.6510 |
[32m[20221214 14:24:47 @agent_ppo2.py:185][0m |           0.0065 |          82.1369 |         -87.6796 |
[32m[20221214 14:24:47 @agent_ppo2.py:185][0m |          -0.0030 |          75.7550 |         -87.7817 |
[32m[20221214 14:24:48 @agent_ppo2.py:185][0m |          -0.0002 |          74.5597 |         -87.7150 |
[32m[20221214 14:24:48 @agent_ppo2.py:185][0m |          -0.0035 |          73.9515 |         -87.7907 |
[32m[20221214 14:24:48 @agent_ppo2.py:185][0m |          -0.0034 |          74.0082 |         -87.8589 |
[32m[20221214 14:24:48 @agent_ppo2.py:185][0m |          -0.0053 |          73.6340 |         -87.8452 |
[32m[20221214 14:24:48 @agent_ppo2.py:185][0m |          -0.0028 |          71.4805 |         -87.8494 |
[32m[20221214 14:24:48 @agent_ppo2.py:185][0m |          -0.0059 |          71.4255 |         -87.8416 |
[32m[20221214 14:24:48 @agent_ppo2.py:185][0m |          -0.0027 |          70.0523 |         -87.6619 |
[32m[20221214 14:24:48 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:24:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.04
[32m[20221214 14:24:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.06
[32m[20221214 14:24:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.19
[32m[20221214 14:24:48 @agent_ppo2.py:143][0m Total time:      26.77 min
[32m[20221214 14:24:48 @agent_ppo2.py:145][0m 2447360 total steps have happened
[32m[20221214 14:24:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1195 --------------------------#
[32m[20221214 14:24:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:24:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:49 @agent_ppo2.py:185][0m |          -0.0015 |         100.6168 |         -87.6560 |
[32m[20221214 14:24:49 @agent_ppo2.py:185][0m |          -0.0004 |          96.0680 |         -87.7776 |
[32m[20221214 14:24:49 @agent_ppo2.py:185][0m |          -0.0025 |          94.3920 |         -87.7416 |
[32m[20221214 14:24:49 @agent_ppo2.py:185][0m |          -0.0023 |          92.9204 |         -87.8609 |
[32m[20221214 14:24:49 @agent_ppo2.py:185][0m |          -0.0056 |          91.8603 |         -87.7692 |
[32m[20221214 14:24:49 @agent_ppo2.py:185][0m |          -0.0034 |          91.7995 |         -87.9795 |
[32m[20221214 14:24:49 @agent_ppo2.py:185][0m |          -0.0014 |          90.2479 |         -87.9453 |
[32m[20221214 14:24:49 @agent_ppo2.py:185][0m |          -0.0031 |          89.6310 |         -87.7674 |
[32m[20221214 14:24:49 @agent_ppo2.py:185][0m |           0.0041 |          94.1233 |         -88.0311 |
[32m[20221214 14:24:49 @agent_ppo2.py:185][0m |          -0.0032 |          88.7292 |         -87.8352 |
[32m[20221214 14:24:49 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:24:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.38
[32m[20221214 14:24:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.09
[32m[20221214 14:24:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.52
[32m[20221214 14:24:50 @agent_ppo2.py:143][0m Total time:      26.79 min
[32m[20221214 14:24:50 @agent_ppo2.py:145][0m 2449408 total steps have happened
[32m[20221214 14:24:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1196 --------------------------#
[32m[20221214 14:24:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:24:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:50 @agent_ppo2.py:185][0m |          -0.0024 |         103.2564 |         -90.7729 |
[32m[20221214 14:24:50 @agent_ppo2.py:185][0m |           0.0031 |          96.2119 |         -90.3398 |
[32m[20221214 14:24:50 @agent_ppo2.py:185][0m |          -0.0031 |          92.2435 |         -90.5525 |
[32m[20221214 14:24:50 @agent_ppo2.py:185][0m |           0.0000 |          89.1872 |         -90.5310 |
[32m[20221214 14:24:50 @agent_ppo2.py:185][0m |          -0.0041 |          88.0533 |         -90.4433 |
[32m[20221214 14:24:50 @agent_ppo2.py:185][0m |          -0.0001 |          85.7235 |         -90.1966 |
[32m[20221214 14:24:50 @agent_ppo2.py:185][0m |          -0.0023 |          84.5314 |         -90.2464 |
[32m[20221214 14:24:51 @agent_ppo2.py:185][0m |           0.0100 |          87.4644 |         -90.2114 |
[32m[20221214 14:24:51 @agent_ppo2.py:185][0m |          -0.0010 |          83.7935 |         -89.8977 |
[32m[20221214 14:24:51 @agent_ppo2.py:185][0m |           0.0071 |          87.6815 |         -89.6930 |
[32m[20221214 14:24:51 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:24:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.68
[32m[20221214 14:24:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.55
[32m[20221214 14:24:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.68
[32m[20221214 14:24:51 @agent_ppo2.py:143][0m Total time:      26.81 min
[32m[20221214 14:24:51 @agent_ppo2.py:145][0m 2451456 total steps have happened
[32m[20221214 14:24:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1197 --------------------------#
[32m[20221214 14:24:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:24:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:51 @agent_ppo2.py:185][0m |           0.0025 |         102.9477 |         -89.3788 |
[32m[20221214 14:24:51 @agent_ppo2.py:185][0m |          -0.0005 |          98.5586 |         -89.3338 |
[32m[20221214 14:24:51 @agent_ppo2.py:185][0m |           0.0058 |         104.8904 |         -89.1371 |
[32m[20221214 14:24:52 @agent_ppo2.py:185][0m |          -0.0017 |          97.1221 |         -89.0625 |
[32m[20221214 14:24:52 @agent_ppo2.py:185][0m |          -0.0001 |          95.8464 |         -88.9532 |
[32m[20221214 14:24:52 @agent_ppo2.py:185][0m |          -0.0014 |          95.4527 |         -88.9853 |
[32m[20221214 14:24:52 @agent_ppo2.py:185][0m |          -0.0045 |          95.9931 |         -88.9934 |
[32m[20221214 14:24:52 @agent_ppo2.py:185][0m |           0.0019 |          96.2145 |         -89.1033 |
[32m[20221214 14:24:52 @agent_ppo2.py:185][0m |           0.0129 |         107.6261 |         -88.8717 |
[32m[20221214 14:24:52 @agent_ppo2.py:185][0m |          -0.0024 |          96.9695 |         -88.9366 |
[32m[20221214 14:24:52 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:24:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.96
[32m[20221214 14:24:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.23
[32m[20221214 14:24:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.15
[32m[20221214 14:24:52 @agent_ppo2.py:143][0m Total time:      26.83 min
[32m[20221214 14:24:52 @agent_ppo2.py:145][0m 2453504 total steps have happened
[32m[20221214 14:24:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1198 --------------------------#
[32m[20221214 14:24:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:24:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:53 @agent_ppo2.py:185][0m |          -0.0007 |         122.6233 |         -86.7126 |
[32m[20221214 14:24:53 @agent_ppo2.py:185][0m |           0.0001 |         120.0781 |         -86.7667 |
[32m[20221214 14:24:53 @agent_ppo2.py:185][0m |          -0.0016 |         118.9370 |         -86.6829 |
[32m[20221214 14:24:53 @agent_ppo2.py:185][0m |          -0.0008 |         119.4677 |         -86.5140 |
[32m[20221214 14:24:53 @agent_ppo2.py:185][0m |          -0.0050 |         119.2424 |         -86.6348 |
[32m[20221214 14:24:53 @agent_ppo2.py:185][0m |          -0.0048 |         119.9416 |         -86.6153 |
[32m[20221214 14:24:53 @agent_ppo2.py:185][0m |           0.0023 |         117.8090 |         -86.7018 |
[32m[20221214 14:24:53 @agent_ppo2.py:185][0m |          -0.0012 |         116.9333 |         -86.5052 |
[32m[20221214 14:24:53 @agent_ppo2.py:185][0m |           0.0025 |         117.4125 |         -86.6503 |
[32m[20221214 14:24:53 @agent_ppo2.py:185][0m |           0.0080 |         119.8313 |         -86.3192 |
[32m[20221214 14:24:53 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:24:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.56
[32m[20221214 14:24:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.57
[32m[20221214 14:24:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.00
[32m[20221214 14:24:53 @agent_ppo2.py:143][0m Total time:      26.86 min
[32m[20221214 14:24:53 @agent_ppo2.py:145][0m 2455552 total steps have happened
[32m[20221214 14:24:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1199 --------------------------#
[32m[20221214 14:24:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:24:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:54 @agent_ppo2.py:185][0m |          -0.0021 |         106.0762 |         -86.6728 |
[32m[20221214 14:24:54 @agent_ppo2.py:185][0m |          -0.0042 |         104.6550 |         -86.8238 |
[32m[20221214 14:24:54 @agent_ppo2.py:185][0m |          -0.0028 |         103.8871 |         -87.1432 |
[32m[20221214 14:24:54 @agent_ppo2.py:185][0m |          -0.0009 |         103.1907 |         -87.3438 |
[32m[20221214 14:24:54 @agent_ppo2.py:185][0m |          -0.0016 |         103.1188 |         -87.2086 |
[32m[20221214 14:24:54 @agent_ppo2.py:185][0m |          -0.0009 |         102.6693 |         -87.2860 |
[32m[20221214 14:24:54 @agent_ppo2.py:185][0m |           0.0125 |         107.3840 |         -87.1938 |
[32m[20221214 14:24:54 @agent_ppo2.py:185][0m |          -0.0023 |         102.9101 |         -87.3831 |
[32m[20221214 14:24:54 @agent_ppo2.py:185][0m |           0.0045 |         102.0468 |         -87.2068 |
[32m[20221214 14:24:55 @agent_ppo2.py:185][0m |          -0.0024 |         100.9641 |         -87.1533 |
[32m[20221214 14:24:55 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:24:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.22
[32m[20221214 14:24:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.42
[32m[20221214 14:24:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.10
[32m[20221214 14:24:55 @agent_ppo2.py:143][0m Total time:      26.88 min
[32m[20221214 14:24:55 @agent_ppo2.py:145][0m 2457600 total steps have happened
[32m[20221214 14:24:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1200 --------------------------#
[32m[20221214 14:24:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:24:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:55 @agent_ppo2.py:185][0m |           0.0012 |         125.3618 |         -86.6828 |
[32m[20221214 14:24:55 @agent_ppo2.py:185][0m |          -0.0036 |         122.2287 |         -86.6926 |
[32m[20221214 14:24:55 @agent_ppo2.py:185][0m |          -0.0011 |         120.4399 |         -86.6241 |
[32m[20221214 14:24:55 @agent_ppo2.py:185][0m |          -0.0023 |         119.3544 |         -86.7294 |
[32m[20221214 14:24:55 @agent_ppo2.py:185][0m |          -0.0014 |         117.1358 |         -86.6583 |
[32m[20221214 14:24:55 @agent_ppo2.py:185][0m |          -0.0008 |         116.4469 |         -86.6965 |
[32m[20221214 14:24:56 @agent_ppo2.py:185][0m |          -0.0012 |         116.5632 |         -86.8352 |
[32m[20221214 14:24:56 @agent_ppo2.py:185][0m |          -0.0012 |         115.6529 |         -86.7519 |
[32m[20221214 14:24:56 @agent_ppo2.py:185][0m |          -0.0030 |         115.3336 |         -86.7925 |
[32m[20221214 14:24:56 @agent_ppo2.py:185][0m |          -0.0036 |         115.1797 |         -86.8535 |
[32m[20221214 14:24:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:24:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.19
[32m[20221214 14:24:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.96
[32m[20221214 14:24:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.99
[32m[20221214 14:24:56 @agent_ppo2.py:143][0m Total time:      26.90 min
[32m[20221214 14:24:56 @agent_ppo2.py:145][0m 2459648 total steps have happened
[32m[20221214 14:24:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1201 --------------------------#
[32m[20221214 14:24:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:24:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:56 @agent_ppo2.py:185][0m |           0.0042 |         117.7781 |         -89.3335 |
[32m[20221214 14:24:56 @agent_ppo2.py:185][0m |           0.0114 |         112.2442 |         -88.9588 |
[32m[20221214 14:24:57 @agent_ppo2.py:185][0m |          -0.0008 |         103.0241 |         -89.1065 |
[32m[20221214 14:24:57 @agent_ppo2.py:185][0m |          -0.0042 |          99.7490 |         -88.9824 |
[32m[20221214 14:24:57 @agent_ppo2.py:185][0m |           0.0112 |         109.2499 |         -89.0390 |
[32m[20221214 14:24:57 @agent_ppo2.py:185][0m |          -0.0043 |          99.2126 |         -88.7359 |
[32m[20221214 14:24:57 @agent_ppo2.py:185][0m |          -0.0010 |          97.5297 |         -88.8697 |
[32m[20221214 14:24:57 @agent_ppo2.py:185][0m |          -0.0041 |          97.0888 |         -88.7740 |
[32m[20221214 14:24:57 @agent_ppo2.py:185][0m |           0.0076 |          96.4932 |         -88.8296 |
[32m[20221214 14:24:57 @agent_ppo2.py:185][0m |          -0.0022 |          96.1769 |         -88.7736 |
[32m[20221214 14:24:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:24:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.19
[32m[20221214 14:24:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.48
[32m[20221214 14:24:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.04
[32m[20221214 14:24:57 @agent_ppo2.py:143][0m Total time:      26.92 min
[32m[20221214 14:24:57 @agent_ppo2.py:145][0m 2461696 total steps have happened
[32m[20221214 14:24:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1202 --------------------------#
[32m[20221214 14:24:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:24:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:58 @agent_ppo2.py:185][0m |           0.0155 |         139.4834 |         -87.1607 |
[32m[20221214 14:24:58 @agent_ppo2.py:185][0m |          -0.0016 |         110.6598 |         -87.3678 |
[32m[20221214 14:24:58 @agent_ppo2.py:185][0m |          -0.0050 |         103.9018 |         -87.1858 |
[32m[20221214 14:24:58 @agent_ppo2.py:185][0m |           0.0056 |         104.3365 |         -87.5558 |
[32m[20221214 14:24:58 @agent_ppo2.py:185][0m |           0.0079 |         102.1238 |         -87.5303 |
[32m[20221214 14:24:58 @agent_ppo2.py:185][0m |          -0.0063 |          97.4007 |         -87.4435 |
[32m[20221214 14:24:58 @agent_ppo2.py:185][0m |          -0.0007 |          95.4588 |         -87.4790 |
[32m[20221214 14:24:58 @agent_ppo2.py:185][0m |          -0.0055 |          94.6085 |         -87.4599 |
[32m[20221214 14:24:58 @agent_ppo2.py:185][0m |          -0.0069 |          93.7929 |         -87.4223 |
[32m[20221214 14:24:58 @agent_ppo2.py:185][0m |          -0.0002 |          92.0737 |         -87.6227 |
[32m[20221214 14:24:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:24:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 696.76
[32m[20221214 14:24:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.21
[32m[20221214 14:24:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.37
[32m[20221214 14:24:58 @agent_ppo2.py:143][0m Total time:      26.94 min
[32m[20221214 14:24:58 @agent_ppo2.py:145][0m 2463744 total steps have happened
[32m[20221214 14:24:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1203 --------------------------#
[32m[20221214 14:24:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:24:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:24:59 @agent_ppo2.py:185][0m |          -0.0000 |         130.7943 |         -86.1070 |
[32m[20221214 14:24:59 @agent_ppo2.py:185][0m |          -0.0021 |         124.1075 |         -86.2156 |
[32m[20221214 14:24:59 @agent_ppo2.py:185][0m |          -0.0023 |         124.5126 |         -86.2459 |
[32m[20221214 14:24:59 @agent_ppo2.py:185][0m |           0.0014 |         123.2021 |         -86.2525 |
[32m[20221214 14:24:59 @agent_ppo2.py:185][0m |          -0.0015 |         122.2483 |         -86.4922 |
[32m[20221214 14:24:59 @agent_ppo2.py:185][0m |          -0.0035 |         121.1686 |         -86.3939 |
[32m[20221214 14:24:59 @agent_ppo2.py:185][0m |          -0.0030 |         120.2393 |         -86.4370 |
[32m[20221214 14:24:59 @agent_ppo2.py:185][0m |          -0.0005 |         121.3428 |         -86.3427 |
[32m[20221214 14:24:59 @agent_ppo2.py:185][0m |           0.0006 |         119.6223 |         -86.5262 |
[32m[20221214 14:25:00 @agent_ppo2.py:185][0m |          -0.0006 |         119.6479 |         -86.6604 |
[32m[20221214 14:25:00 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:25:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.82
[32m[20221214 14:25:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.75
[32m[20221214 14:25:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.74
[32m[20221214 14:25:00 @agent_ppo2.py:143][0m Total time:      26.96 min
[32m[20221214 14:25:00 @agent_ppo2.py:145][0m 2465792 total steps have happened
[32m[20221214 14:25:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1204 --------------------------#
[32m[20221214 14:25:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:00 @agent_ppo2.py:185][0m |           0.0125 |         169.8242 |         -86.7853 |
[32m[20221214 14:25:00 @agent_ppo2.py:185][0m |          -0.0011 |         144.3892 |         -86.7624 |
[32m[20221214 14:25:00 @agent_ppo2.py:185][0m |          -0.0019 |         143.4300 |         -86.6814 |
[32m[20221214 14:25:00 @agent_ppo2.py:185][0m |           0.0124 |         162.4919 |         -86.7217 |
[32m[20221214 14:25:00 @agent_ppo2.py:185][0m |          -0.0014 |         144.4379 |         -86.8377 |
[32m[20221214 14:25:01 @agent_ppo2.py:185][0m |          -0.0031 |         143.1016 |         -86.8636 |
[32m[20221214 14:25:01 @agent_ppo2.py:185][0m |          -0.0018 |         140.4982 |         -86.6370 |
[32m[20221214 14:25:01 @agent_ppo2.py:185][0m |          -0.0019 |         140.1592 |         -86.8744 |
[32m[20221214 14:25:01 @agent_ppo2.py:185][0m |          -0.0024 |         141.0887 |         -86.5881 |
[32m[20221214 14:25:01 @agent_ppo2.py:185][0m |           0.0029 |         142.5459 |         -86.7312 |
[32m[20221214 14:25:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:25:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.10
[32m[20221214 14:25:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.94
[32m[20221214 14:25:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.98
[32m[20221214 14:25:01 @agent_ppo2.py:143][0m Total time:      26.98 min
[32m[20221214 14:25:01 @agent_ppo2.py:145][0m 2467840 total steps have happened
[32m[20221214 14:25:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1205 --------------------------#
[32m[20221214 14:25:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:01 @agent_ppo2.py:185][0m |           0.0107 |         153.6677 |         -86.0107 |
[32m[20221214 14:25:01 @agent_ppo2.py:185][0m |           0.0013 |         134.1804 |         -85.9028 |
[32m[20221214 14:25:01 @agent_ppo2.py:185][0m |          -0.0016 |         130.6115 |         -85.6263 |
[32m[20221214 14:25:02 @agent_ppo2.py:185][0m |          -0.0001 |         129.2467 |         -85.5614 |
[32m[20221214 14:25:02 @agent_ppo2.py:185][0m |          -0.0005 |         128.1660 |         -85.6511 |
[32m[20221214 14:25:02 @agent_ppo2.py:185][0m |          -0.0028 |         127.7314 |         -85.6893 |
[32m[20221214 14:25:02 @agent_ppo2.py:185][0m |          -0.0014 |         126.4102 |         -85.6361 |
[32m[20221214 14:25:02 @agent_ppo2.py:185][0m |           0.0000 |         127.0866 |         -85.4451 |
[32m[20221214 14:25:02 @agent_ppo2.py:185][0m |           0.0014 |         126.8840 |         -85.4958 |
[32m[20221214 14:25:02 @agent_ppo2.py:185][0m |          -0.0019 |         125.6746 |         -85.5664 |
[32m[20221214 14:25:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:25:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 693.60
[32m[20221214 14:25:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.49
[32m[20221214 14:25:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.73
[32m[20221214 14:25:02 @agent_ppo2.py:143][0m Total time:      27.00 min
[32m[20221214 14:25:02 @agent_ppo2.py:145][0m 2469888 total steps have happened
[32m[20221214 14:25:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1206 --------------------------#
[32m[20221214 14:25:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:03 @agent_ppo2.py:185][0m |           0.0015 |         169.6063 |         -85.9173 |
[32m[20221214 14:25:03 @agent_ppo2.py:185][0m |           0.0000 |         153.3348 |         -86.0345 |
[32m[20221214 14:25:03 @agent_ppo2.py:185][0m |           0.0012 |         150.7616 |         -86.2381 |
[32m[20221214 14:25:03 @agent_ppo2.py:185][0m |          -0.0034 |         147.6413 |         -86.3457 |
[32m[20221214 14:25:03 @agent_ppo2.py:185][0m |          -0.0035 |         146.1384 |         -86.3648 |
[32m[20221214 14:25:03 @agent_ppo2.py:185][0m |          -0.0022 |         145.2489 |         -86.3037 |
[32m[20221214 14:25:03 @agent_ppo2.py:185][0m |          -0.0033 |         146.3551 |         -86.3773 |
[32m[20221214 14:25:03 @agent_ppo2.py:185][0m |          -0.0009 |         143.7518 |         -86.3946 |
[32m[20221214 14:25:03 @agent_ppo2.py:185][0m |          -0.0030 |         143.1932 |         -86.3987 |
[32m[20221214 14:25:03 @agent_ppo2.py:185][0m |          -0.0003 |         141.5272 |         -86.1635 |
[32m[20221214 14:25:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:25:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 687.64
[32m[20221214 14:25:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.41
[32m[20221214 14:25:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.75
[32m[20221214 14:25:03 @agent_ppo2.py:143][0m Total time:      27.02 min
[32m[20221214 14:25:03 @agent_ppo2.py:145][0m 2471936 total steps have happened
[32m[20221214 14:25:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1207 --------------------------#
[32m[20221214 14:25:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:25:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:04 @agent_ppo2.py:185][0m |          -0.0007 |         149.8721 |         -87.8005 |
[32m[20221214 14:25:04 @agent_ppo2.py:185][0m |          -0.0010 |         136.5367 |         -87.9439 |
[32m[20221214 14:25:04 @agent_ppo2.py:185][0m |          -0.0018 |         132.5692 |         -87.9345 |
[32m[20221214 14:25:04 @agent_ppo2.py:185][0m |          -0.0017 |         129.2824 |         -88.0284 |
[32m[20221214 14:25:04 @agent_ppo2.py:185][0m |          -0.0010 |         127.5010 |         -88.0845 |
[32m[20221214 14:25:04 @agent_ppo2.py:185][0m |           0.0025 |         125.3043 |         -87.9414 |
[32m[20221214 14:25:04 @agent_ppo2.py:185][0m |          -0.0011 |         123.3270 |         -87.8327 |
[32m[20221214 14:25:04 @agent_ppo2.py:185][0m |           0.0028 |         122.8326 |         -88.0178 |
[32m[20221214 14:25:05 @agent_ppo2.py:185][0m |          -0.0010 |         121.3758 |         -87.7611 |
[32m[20221214 14:25:05 @agent_ppo2.py:185][0m |          -0.0017 |         119.4697 |         -87.4884 |
[32m[20221214 14:25:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:25:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.71
[32m[20221214 14:25:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.08
[32m[20221214 14:25:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.26
[32m[20221214 14:25:05 @agent_ppo2.py:143][0m Total time:      27.04 min
[32m[20221214 14:25:05 @agent_ppo2.py:145][0m 2473984 total steps have happened
[32m[20221214 14:25:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1208 --------------------------#
[32m[20221214 14:25:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:05 @agent_ppo2.py:185][0m |           0.0008 |         122.1653 |         -84.4833 |
[32m[20221214 14:25:05 @agent_ppo2.py:185][0m |           0.0002 |         114.2278 |         -84.4519 |
[32m[20221214 14:25:05 @agent_ppo2.py:185][0m |           0.0021 |         114.1083 |         -84.5380 |
[32m[20221214 14:25:05 @agent_ppo2.py:185][0m |          -0.0015 |         113.0941 |         -84.6465 |
[32m[20221214 14:25:05 @agent_ppo2.py:185][0m |          -0.0059 |         111.6707 |         -84.5430 |
[32m[20221214 14:25:06 @agent_ppo2.py:185][0m |           0.0010 |         110.9386 |         -84.5533 |
[32m[20221214 14:25:06 @agent_ppo2.py:185][0m |           0.0004 |         110.4374 |         -84.4462 |
[32m[20221214 14:25:06 @agent_ppo2.py:185][0m |          -0.0071 |         111.2667 |         -84.4283 |
[32m[20221214 14:25:06 @agent_ppo2.py:185][0m |          -0.0011 |         110.0949 |         -84.5242 |
[32m[20221214 14:25:06 @agent_ppo2.py:185][0m |          -0.0026 |         109.7537 |         -84.5053 |
[32m[20221214 14:25:06 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:25:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.04
[32m[20221214 14:25:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.93
[32m[20221214 14:25:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.82
[32m[20221214 14:25:06 @agent_ppo2.py:143][0m Total time:      27.07 min
[32m[20221214 14:25:06 @agent_ppo2.py:145][0m 2476032 total steps have happened
[32m[20221214 14:25:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1209 --------------------------#
[32m[20221214 14:25:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:25:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:06 @agent_ppo2.py:185][0m |           0.0016 |         198.2570 |         -86.8228 |
[32m[20221214 14:25:07 @agent_ppo2.py:185][0m |          -0.0017 |         181.8113 |         -86.6533 |
[32m[20221214 14:25:07 @agent_ppo2.py:185][0m |          -0.0010 |         178.2512 |         -86.4549 |
[32m[20221214 14:25:07 @agent_ppo2.py:185][0m |          -0.0048 |         174.1132 |         -86.4370 |
[32m[20221214 14:25:07 @agent_ppo2.py:185][0m |          -0.0050 |         171.9431 |         -86.2744 |
[32m[20221214 14:25:07 @agent_ppo2.py:185][0m |          -0.0053 |         170.4515 |         -86.2787 |
[32m[20221214 14:25:07 @agent_ppo2.py:185][0m |          -0.0038 |         169.2919 |         -86.2881 |
[32m[20221214 14:25:07 @agent_ppo2.py:185][0m |          -0.0051 |         167.9729 |         -86.0335 |
[32m[20221214 14:25:07 @agent_ppo2.py:185][0m |          -0.0027 |         167.9144 |         -86.1141 |
[32m[20221214 14:25:07 @agent_ppo2.py:185][0m |          -0.0053 |         168.6863 |         -86.0463 |
[32m[20221214 14:25:07 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:25:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.62
[32m[20221214 14:25:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.04
[32m[20221214 14:25:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.47
[32m[20221214 14:25:07 @agent_ppo2.py:143][0m Total time:      27.09 min
[32m[20221214 14:25:07 @agent_ppo2.py:145][0m 2478080 total steps have happened
[32m[20221214 14:25:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1210 --------------------------#
[32m[20221214 14:25:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:25:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:08 @agent_ppo2.py:185][0m |          -0.0091 |         178.6178 |         -85.8604 |
[32m[20221214 14:25:08 @agent_ppo2.py:185][0m |          -0.0019 |         165.4261 |         -86.1223 |
[32m[20221214 14:25:08 @agent_ppo2.py:185][0m |          -0.0023 |         163.9410 |         -85.7075 |
[32m[20221214 14:25:08 @agent_ppo2.py:185][0m |          -0.0025 |         159.9762 |         -85.9784 |
[32m[20221214 14:25:08 @agent_ppo2.py:185][0m |          -0.0032 |         160.3968 |         -85.8259 |
[32m[20221214 14:25:08 @agent_ppo2.py:185][0m |          -0.0026 |         157.7635 |         -85.8480 |
[32m[20221214 14:25:08 @agent_ppo2.py:185][0m |          -0.0065 |         158.6457 |         -85.8689 |
[32m[20221214 14:25:09 @agent_ppo2.py:185][0m |          -0.0029 |         156.2297 |         -85.8744 |
[32m[20221214 14:25:09 @agent_ppo2.py:185][0m |          -0.0010 |         155.8573 |         -85.7834 |
[32m[20221214 14:25:09 @agent_ppo2.py:185][0m |          -0.0014 |         155.3112 |         -85.6143 |
[32m[20221214 14:25:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:25:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.90
[32m[20221214 14:25:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.37
[32m[20221214 14:25:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.63
[32m[20221214 14:25:09 @agent_ppo2.py:143][0m Total time:      27.11 min
[32m[20221214 14:25:09 @agent_ppo2.py:145][0m 2480128 total steps have happened
[32m[20221214 14:25:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1211 --------------------------#
[32m[20221214 14:25:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:25:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:09 @agent_ppo2.py:185][0m |          -0.0020 |         121.7772 |         -84.1365 |
[32m[20221214 14:25:09 @agent_ppo2.py:185][0m |          -0.0010 |         117.2486 |         -84.2240 |
[32m[20221214 14:25:09 @agent_ppo2.py:185][0m |           0.0020 |         116.2229 |         -84.1038 |
[32m[20221214 14:25:09 @agent_ppo2.py:185][0m |           0.0032 |         115.4479 |         -84.2065 |
[32m[20221214 14:25:10 @agent_ppo2.py:185][0m |          -0.0036 |         114.1223 |         -84.2763 |
[32m[20221214 14:25:10 @agent_ppo2.py:185][0m |          -0.0000 |         113.3602 |         -84.4890 |
[32m[20221214 14:25:10 @agent_ppo2.py:185][0m |           0.0063 |         117.5457 |         -84.3551 |
[32m[20221214 14:25:10 @agent_ppo2.py:185][0m |          -0.0011 |         112.1542 |         -84.4527 |
[32m[20221214 14:25:10 @agent_ppo2.py:185][0m |          -0.0004 |         112.5251 |         -84.4857 |
[32m[20221214 14:25:10 @agent_ppo2.py:185][0m |          -0.0009 |         111.7282 |         -84.6616 |
[32m[20221214 14:25:10 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:25:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.36
[32m[20221214 14:25:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.59
[32m[20221214 14:25:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.23
[32m[20221214 14:25:10 @agent_ppo2.py:143][0m Total time:      27.14 min
[32m[20221214 14:25:10 @agent_ppo2.py:145][0m 2482176 total steps have happened
[32m[20221214 14:25:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1212 --------------------------#
[32m[20221214 14:25:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:25:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:11 @agent_ppo2.py:185][0m |           0.0033 |         131.0399 |         -85.6818 |
[32m[20221214 14:25:11 @agent_ppo2.py:185][0m |          -0.0022 |         124.2370 |         -85.4007 |
[32m[20221214 14:25:11 @agent_ppo2.py:185][0m |          -0.0030 |         123.7074 |         -85.2897 |
[32m[20221214 14:25:11 @agent_ppo2.py:185][0m |          -0.0017 |         122.1069 |         -85.4413 |
[32m[20221214 14:25:11 @agent_ppo2.py:185][0m |          -0.0017 |         121.4112 |         -85.6772 |
[32m[20221214 14:25:11 @agent_ppo2.py:185][0m |          -0.0008 |         120.6111 |         -85.8160 |
[32m[20221214 14:25:11 @agent_ppo2.py:185][0m |          -0.0017 |         119.4233 |         -85.7053 |
[32m[20221214 14:25:11 @agent_ppo2.py:185][0m |          -0.0010 |         120.1713 |         -85.6344 |
[32m[20221214 14:25:11 @agent_ppo2.py:185][0m |          -0.0012 |         119.3129 |         -85.6751 |
[32m[20221214 14:25:11 @agent_ppo2.py:185][0m |          -0.0019 |         118.4358 |         -86.0540 |
[32m[20221214 14:25:11 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:25:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.87
[32m[20221214 14:25:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.64
[32m[20221214 14:25:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.15
[32m[20221214 14:25:12 @agent_ppo2.py:143][0m Total time:      27.16 min
[32m[20221214 14:25:12 @agent_ppo2.py:145][0m 2484224 total steps have happened
[32m[20221214 14:25:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1213 --------------------------#
[32m[20221214 14:25:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:25:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:12 @agent_ppo2.py:185][0m |           0.0045 |         130.3928 |         -85.2246 |
[32m[20221214 14:25:12 @agent_ppo2.py:185][0m |           0.0166 |         141.7521 |         -85.2149 |
[32m[20221214 14:25:12 @agent_ppo2.py:185][0m |           0.0003 |         124.2945 |         -85.3255 |
[32m[20221214 14:25:12 @agent_ppo2.py:185][0m |          -0.0010 |         120.9282 |         -85.4522 |
[32m[20221214 14:25:12 @agent_ppo2.py:185][0m |           0.0001 |         120.0898 |         -85.3810 |
[32m[20221214 14:25:12 @agent_ppo2.py:185][0m |          -0.0004 |         119.2297 |         -85.6060 |
[32m[20221214 14:25:12 @agent_ppo2.py:185][0m |           0.0001 |         119.8429 |         -85.7025 |
[32m[20221214 14:25:13 @agent_ppo2.py:185][0m |           0.0000 |         117.8432 |         -85.4772 |
[32m[20221214 14:25:13 @agent_ppo2.py:185][0m |          -0.0023 |         119.0092 |         -85.6364 |
[32m[20221214 14:25:13 @agent_ppo2.py:185][0m |          -0.0015 |         119.0536 |         -85.8873 |
[32m[20221214 14:25:13 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:25:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.58
[32m[20221214 14:25:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.00
[32m[20221214 14:25:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.93
[32m[20221214 14:25:13 @agent_ppo2.py:143][0m Total time:      27.18 min
[32m[20221214 14:25:13 @agent_ppo2.py:145][0m 2486272 total steps have happened
[32m[20221214 14:25:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1214 --------------------------#
[32m[20221214 14:25:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:25:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:13 @agent_ppo2.py:185][0m |           0.0027 |         186.0027 |         -86.2529 |
[32m[20221214 14:25:13 @agent_ppo2.py:185][0m |          -0.0009 |         171.6924 |         -86.2132 |
[32m[20221214 14:25:13 @agent_ppo2.py:185][0m |          -0.0043 |         164.7712 |         -86.4429 |
[32m[20221214 14:25:13 @agent_ppo2.py:185][0m |          -0.0016 |         164.1646 |         -86.3217 |
[32m[20221214 14:25:14 @agent_ppo2.py:185][0m |          -0.0043 |         162.2847 |         -86.2513 |
[32m[20221214 14:25:14 @agent_ppo2.py:185][0m |           0.0221 |         202.9403 |         -86.3280 |
[32m[20221214 14:25:14 @agent_ppo2.py:185][0m |           0.0054 |         173.8749 |         -86.0075 |
[32m[20221214 14:25:14 @agent_ppo2.py:185][0m |          -0.0026 |         159.4765 |         -85.5898 |
[32m[20221214 14:25:14 @agent_ppo2.py:185][0m |          -0.0067 |         160.2597 |         -86.0463 |
[32m[20221214 14:25:14 @agent_ppo2.py:185][0m |          -0.0066 |         160.2670 |         -86.0008 |
[32m[20221214 14:25:14 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:25:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.44
[32m[20221214 14:25:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.84
[32m[20221214 14:25:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.66
[32m[20221214 14:25:14 @agent_ppo2.py:143][0m Total time:      27.20 min
[32m[20221214 14:25:14 @agent_ppo2.py:145][0m 2488320 total steps have happened
[32m[20221214 14:25:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1215 --------------------------#
[32m[20221214 14:25:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:14 @agent_ppo2.py:185][0m |          -0.0002 |         227.8110 |         -85.5208 |
[32m[20221214 14:25:15 @agent_ppo2.py:185][0m |          -0.0015 |         217.2699 |         -85.2645 |
[32m[20221214 14:25:15 @agent_ppo2.py:185][0m |          -0.0023 |         215.2104 |         -85.3805 |
[32m[20221214 14:25:15 @agent_ppo2.py:185][0m |           0.0021 |         217.0439 |         -85.4818 |
[32m[20221214 14:25:15 @agent_ppo2.py:185][0m |          -0.0012 |         213.6321 |         -85.3950 |
[32m[20221214 14:25:15 @agent_ppo2.py:185][0m |          -0.0026 |         212.5413 |         -85.4654 |
[32m[20221214 14:25:15 @agent_ppo2.py:185][0m |          -0.0026 |         212.0832 |         -85.3045 |
[32m[20221214 14:25:15 @agent_ppo2.py:185][0m |           0.0120 |         223.9302 |         -85.4084 |
[32m[20221214 14:25:15 @agent_ppo2.py:185][0m |           0.0173 |         243.9665 |         -84.8693 |
[32m[20221214 14:25:15 @agent_ppo2.py:185][0m |           0.0091 |         224.4404 |         -85.5317 |
[32m[20221214 14:25:15 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.96
[32m[20221214 14:25:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.69
[32m[20221214 14:25:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.84
[32m[20221214 14:25:15 @agent_ppo2.py:143][0m Total time:      27.22 min
[32m[20221214 14:25:15 @agent_ppo2.py:145][0m 2490368 total steps have happened
[32m[20221214 14:25:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1216 --------------------------#
[32m[20221214 14:25:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:16 @agent_ppo2.py:185][0m |          -0.0044 |         244.8172 |         -90.2886 |
[32m[20221214 14:25:16 @agent_ppo2.py:185][0m |          -0.0003 |         229.5360 |         -90.3126 |
[32m[20221214 14:25:16 @agent_ppo2.py:185][0m |          -0.0037 |         225.8878 |         -90.3052 |
[32m[20221214 14:25:16 @agent_ppo2.py:185][0m |          -0.0031 |         221.5685 |         -90.2752 |
[32m[20221214 14:25:16 @agent_ppo2.py:185][0m |          -0.0018 |         219.5915 |         -90.7398 |
[32m[20221214 14:25:16 @agent_ppo2.py:185][0m |          -0.0013 |         217.6275 |         -90.5630 |
[32m[20221214 14:25:16 @agent_ppo2.py:185][0m |          -0.0032 |         218.0907 |         -90.8474 |
[32m[20221214 14:25:16 @agent_ppo2.py:185][0m |           0.0037 |         221.1707 |         -90.8994 |
[32m[20221214 14:25:16 @agent_ppo2.py:185][0m |          -0.0026 |         217.0376 |         -91.0894 |
[32m[20221214 14:25:16 @agent_ppo2.py:185][0m |          -0.0008 |         215.6485 |         -90.9549 |
[32m[20221214 14:25:16 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:25:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.77
[32m[20221214 14:25:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.84
[32m[20221214 14:25:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.66
[32m[20221214 14:25:17 @agent_ppo2.py:143][0m Total time:      27.24 min
[32m[20221214 14:25:17 @agent_ppo2.py:145][0m 2492416 total steps have happened
[32m[20221214 14:25:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1217 --------------------------#
[32m[20221214 14:25:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:17 @agent_ppo2.py:185][0m |          -0.0038 |         139.9870 |         -86.0032 |
[32m[20221214 14:25:17 @agent_ppo2.py:185][0m |          -0.0024 |         132.6961 |         -86.0554 |
[32m[20221214 14:25:17 @agent_ppo2.py:185][0m |          -0.0043 |         133.6328 |         -85.9403 |
[32m[20221214 14:25:17 @agent_ppo2.py:185][0m |           0.0150 |         149.4603 |         -86.0210 |
[32m[20221214 14:25:17 @agent_ppo2.py:185][0m |           0.0004 |         132.5228 |         -86.0381 |
[32m[20221214 14:25:17 @agent_ppo2.py:185][0m |          -0.0013 |         128.9493 |         -85.8854 |
[32m[20221214 14:25:17 @agent_ppo2.py:185][0m |          -0.0041 |         129.5518 |         -85.7979 |
[32m[20221214 14:25:17 @agent_ppo2.py:185][0m |           0.0033 |         130.6828 |         -85.7657 |
[32m[20221214 14:25:18 @agent_ppo2.py:185][0m |          -0.0051 |         130.2332 |         -85.7867 |
[32m[20221214 14:25:18 @agent_ppo2.py:185][0m |          -0.0055 |         128.3199 |         -85.7943 |
[32m[20221214 14:25:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.87
[32m[20221214 14:25:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.47
[32m[20221214 14:25:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.07
[32m[20221214 14:25:18 @agent_ppo2.py:143][0m Total time:      27.26 min
[32m[20221214 14:25:18 @agent_ppo2.py:145][0m 2494464 total steps have happened
[32m[20221214 14:25:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1218 --------------------------#
[32m[20221214 14:25:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:18 @agent_ppo2.py:185][0m |          -0.0012 |         196.1424 |         -85.3066 |
[32m[20221214 14:25:18 @agent_ppo2.py:185][0m |          -0.0028 |         187.7164 |         -85.2481 |
[32m[20221214 14:25:18 @agent_ppo2.py:185][0m |           0.0053 |         191.1124 |         -85.6409 |
[32m[20221214 14:25:18 @agent_ppo2.py:185][0m |          -0.0003 |         185.1239 |         -85.9837 |
[32m[20221214 14:25:18 @agent_ppo2.py:185][0m |          -0.0007 |         183.7517 |         -85.4984 |
[32m[20221214 14:25:19 @agent_ppo2.py:185][0m |          -0.0018 |         182.7665 |         -85.5520 |
[32m[20221214 14:25:19 @agent_ppo2.py:185][0m |          -0.0020 |         180.8437 |         -85.5403 |
[32m[20221214 14:25:19 @agent_ppo2.py:185][0m |          -0.0038 |         180.1123 |         -85.3714 |
[32m[20221214 14:25:19 @agent_ppo2.py:185][0m |          -0.0013 |         178.8594 |         -85.5658 |
[32m[20221214 14:25:19 @agent_ppo2.py:185][0m |          -0.0009 |         178.1138 |         -85.8311 |
[32m[20221214 14:25:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:25:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.40
[32m[20221214 14:25:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.66
[32m[20221214 14:25:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.87
[32m[20221214 14:25:19 @agent_ppo2.py:143][0m Total time:      27.28 min
[32m[20221214 14:25:19 @agent_ppo2.py:145][0m 2496512 total steps have happened
[32m[20221214 14:25:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1219 --------------------------#
[32m[20221214 14:25:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:19 @agent_ppo2.py:185][0m |           0.0002 |         257.1299 |         -84.8255 |
[32m[20221214 14:25:19 @agent_ppo2.py:185][0m |          -0.0015 |         234.2090 |         -84.9516 |
[32m[20221214 14:25:20 @agent_ppo2.py:185][0m |          -0.0021 |         229.8059 |         -84.5567 |
[32m[20221214 14:25:20 @agent_ppo2.py:185][0m |          -0.0037 |         228.2181 |         -84.9579 |
[32m[20221214 14:25:20 @agent_ppo2.py:185][0m |          -0.0024 |         227.8532 |         -84.7994 |
[32m[20221214 14:25:20 @agent_ppo2.py:185][0m |          -0.0036 |         224.3672 |         -84.5239 |
[32m[20221214 14:25:20 @agent_ppo2.py:185][0m |           0.0001 |         223.5800 |         -84.8071 |
[32m[20221214 14:25:20 @agent_ppo2.py:185][0m |          -0.0005 |         222.7317 |         -84.6035 |
[32m[20221214 14:25:20 @agent_ppo2.py:185][0m |          -0.0020 |         221.7302 |         -84.6106 |
[32m[20221214 14:25:20 @agent_ppo2.py:185][0m |          -0.0021 |         221.6880 |         -84.7767 |
[32m[20221214 14:25:20 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.44
[32m[20221214 14:25:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.59
[32m[20221214 14:25:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.50
[32m[20221214 14:25:20 @agent_ppo2.py:143][0m Total time:      27.30 min
[32m[20221214 14:25:20 @agent_ppo2.py:145][0m 2498560 total steps have happened
[32m[20221214 14:25:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1220 --------------------------#
[32m[20221214 14:25:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:25:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:21 @agent_ppo2.py:185][0m |          -0.0008 |         183.9519 |         -87.6281 |
[32m[20221214 14:25:21 @agent_ppo2.py:185][0m |          -0.0018 |         174.7842 |         -87.6966 |
[32m[20221214 14:25:21 @agent_ppo2.py:185][0m |          -0.0011 |         172.1357 |         -87.4178 |
[32m[20221214 14:25:21 @agent_ppo2.py:185][0m |          -0.0026 |         169.7199 |         -87.6193 |
[32m[20221214 14:25:21 @agent_ppo2.py:185][0m |           0.0093 |         184.5131 |         -87.5461 |
[32m[20221214 14:25:21 @agent_ppo2.py:185][0m |          -0.0003 |         166.1675 |         -87.3406 |
[32m[20221214 14:25:21 @agent_ppo2.py:185][0m |          -0.0016 |         164.0271 |         -87.8404 |
[32m[20221214 14:25:21 @agent_ppo2.py:185][0m |           0.0003 |         163.6419 |         -87.8086 |
[32m[20221214 14:25:21 @agent_ppo2.py:185][0m |          -0.0013 |         162.5782 |         -87.8709 |
[32m[20221214 14:25:21 @agent_ppo2.py:185][0m |          -0.0029 |         162.6878 |         -87.7113 |
[32m[20221214 14:25:21 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:25:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.26
[32m[20221214 14:25:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.98
[32m[20221214 14:25:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.87
[32m[20221214 14:25:21 @agent_ppo2.py:143][0m Total time:      27.32 min
[32m[20221214 14:25:21 @agent_ppo2.py:145][0m 2500608 total steps have happened
[32m[20221214 14:25:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1221 --------------------------#
[32m[20221214 14:25:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:22 @agent_ppo2.py:185][0m |          -0.0021 |         146.2625 |         -87.0556 |
[32m[20221214 14:25:22 @agent_ppo2.py:185][0m |          -0.0043 |         128.7086 |         -86.9944 |
[32m[20221214 14:25:22 @agent_ppo2.py:185][0m |           0.0003 |         122.3624 |         -87.1638 |
[32m[20221214 14:25:22 @agent_ppo2.py:185][0m |          -0.0032 |         119.7175 |         -87.2495 |
[32m[20221214 14:25:22 @agent_ppo2.py:185][0m |          -0.0036 |         117.4881 |         -87.2691 |
[32m[20221214 14:25:22 @agent_ppo2.py:185][0m |          -0.0047 |         116.6268 |         -87.3640 |
[32m[20221214 14:25:22 @agent_ppo2.py:185][0m |          -0.0056 |         116.1700 |         -87.3520 |
[32m[20221214 14:25:22 @agent_ppo2.py:185][0m |          -0.0043 |         114.5665 |         -87.3697 |
[32m[20221214 14:25:22 @agent_ppo2.py:185][0m |          -0.0036 |         113.7582 |         -87.3426 |
[32m[20221214 14:25:23 @agent_ppo2.py:185][0m |          -0.0048 |         112.4173 |         -87.5141 |
[32m[20221214 14:25:23 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:25:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.33
[32m[20221214 14:25:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.47
[32m[20221214 14:25:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.83
[32m[20221214 14:25:23 @agent_ppo2.py:143][0m Total time:      27.34 min
[32m[20221214 14:25:23 @agent_ppo2.py:145][0m 2502656 total steps have happened
[32m[20221214 14:25:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1222 --------------------------#
[32m[20221214 14:25:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:23 @agent_ppo2.py:185][0m |          -0.0043 |         170.8888 |         -86.5614 |
[32m[20221214 14:25:23 @agent_ppo2.py:185][0m |          -0.0005 |         166.8374 |         -86.2759 |
[32m[20221214 14:25:23 @agent_ppo2.py:185][0m |          -0.0041 |         162.4330 |         -86.5219 |
[32m[20221214 14:25:23 @agent_ppo2.py:185][0m |           0.0196 |         194.2861 |         -86.5636 |
[32m[20221214 14:25:23 @agent_ppo2.py:185][0m |           0.0132 |         190.3700 |         -86.1981 |
[32m[20221214 14:25:23 @agent_ppo2.py:185][0m |           0.0170 |         185.7127 |         -86.3596 |
[32m[20221214 14:25:24 @agent_ppo2.py:185][0m |          -0.0055 |         161.6819 |         -86.3144 |
[32m[20221214 14:25:24 @agent_ppo2.py:185][0m |           0.0013 |         159.0992 |         -86.8168 |
[32m[20221214 14:25:24 @agent_ppo2.py:185][0m |          -0.0043 |         157.4253 |         -86.7480 |
[32m[20221214 14:25:24 @agent_ppo2.py:185][0m |           0.0066 |         167.5117 |         -86.7104 |
[32m[20221214 14:25:24 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:25:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.14
[32m[20221214 14:25:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.16
[32m[20221214 14:25:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.64
[32m[20221214 14:25:24 @agent_ppo2.py:143][0m Total time:      27.36 min
[32m[20221214 14:25:24 @agent_ppo2.py:145][0m 2504704 total steps have happened
[32m[20221214 14:25:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1223 --------------------------#
[32m[20221214 14:25:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:25:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:24 @agent_ppo2.py:185][0m |           0.0078 |         203.4745 |         -88.3076 |
[32m[20221214 14:25:24 @agent_ppo2.py:185][0m |          -0.0039 |         184.3850 |         -88.4639 |
[32m[20221214 14:25:24 @agent_ppo2.py:185][0m |          -0.0036 |         181.8834 |         -88.4741 |
[32m[20221214 14:25:25 @agent_ppo2.py:185][0m |          -0.0030 |         182.3133 |         -88.1761 |
[32m[20221214 14:25:25 @agent_ppo2.py:185][0m |          -0.0006 |         181.1587 |         -88.6220 |
[32m[20221214 14:25:25 @agent_ppo2.py:185][0m |          -0.0015 |         180.9962 |         -88.4272 |
[32m[20221214 14:25:25 @agent_ppo2.py:185][0m |          -0.0051 |         180.5699 |         -88.4553 |
[32m[20221214 14:25:25 @agent_ppo2.py:185][0m |           0.0039 |         186.4719 |         -88.6153 |
[32m[20221214 14:25:25 @agent_ppo2.py:185][0m |          -0.0017 |         180.5522 |         -88.5477 |
[32m[20221214 14:25:25 @agent_ppo2.py:185][0m |          -0.0024 |         182.6566 |         -88.5809 |
[32m[20221214 14:25:25 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:25:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.86
[32m[20221214 14:25:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.79
[32m[20221214 14:25:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.72
[32m[20221214 14:25:25 @agent_ppo2.py:143][0m Total time:      27.39 min
[32m[20221214 14:25:25 @agent_ppo2.py:145][0m 2506752 total steps have happened
[32m[20221214 14:25:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1224 --------------------------#
[32m[20221214 14:25:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:25:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:26 @agent_ppo2.py:185][0m |          -0.0018 |         213.8261 |         -87.2977 |
[32m[20221214 14:25:26 @agent_ppo2.py:185][0m |           0.0091 |         201.7874 |         -86.8673 |
[32m[20221214 14:25:26 @agent_ppo2.py:185][0m |          -0.0027 |         198.3649 |         -86.8303 |
[32m[20221214 14:25:26 @agent_ppo2.py:185][0m |          -0.0039 |         195.1162 |         -86.9453 |
[32m[20221214 14:25:26 @agent_ppo2.py:185][0m |          -0.0010 |         193.7515 |         -87.0714 |
[32m[20221214 14:25:26 @agent_ppo2.py:185][0m |          -0.0040 |         192.8896 |         -86.7424 |
[32m[20221214 14:25:26 @agent_ppo2.py:185][0m |          -0.0026 |         191.8382 |         -86.9006 |
[32m[20221214 14:25:26 @agent_ppo2.py:185][0m |          -0.0036 |         192.4279 |         -86.6756 |
[32m[20221214 14:25:26 @agent_ppo2.py:185][0m |          -0.0040 |         191.3499 |         -86.6508 |
[32m[20221214 14:25:26 @agent_ppo2.py:185][0m |          -0.0017 |         190.2279 |         -86.5827 |
[32m[20221214 14:25:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:25:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.67
[32m[20221214 14:25:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.03
[32m[20221214 14:25:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.26
[32m[20221214 14:25:26 @agent_ppo2.py:143][0m Total time:      27.41 min
[32m[20221214 14:25:26 @agent_ppo2.py:145][0m 2508800 total steps have happened
[32m[20221214 14:25:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1225 --------------------------#
[32m[20221214 14:25:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:27 @agent_ppo2.py:185][0m |          -0.0035 |         222.9872 |         -87.2294 |
[32m[20221214 14:25:27 @agent_ppo2.py:185][0m |          -0.0039 |         210.1386 |         -87.1642 |
[32m[20221214 14:25:27 @agent_ppo2.py:185][0m |          -0.0039 |         208.7742 |         -86.9922 |
[32m[20221214 14:25:27 @agent_ppo2.py:185][0m |           0.0003 |         206.6115 |         -87.1630 |
[32m[20221214 14:25:27 @agent_ppo2.py:185][0m |          -0.0054 |         204.5759 |         -87.1086 |
[32m[20221214 14:25:27 @agent_ppo2.py:185][0m |          -0.0030 |         203.0000 |         -87.2144 |
[32m[20221214 14:25:27 @agent_ppo2.py:185][0m |          -0.0031 |         201.4401 |         -86.9711 |
[32m[20221214 14:25:27 @agent_ppo2.py:185][0m |          -0.0027 |         200.8527 |         -87.3252 |
[32m[20221214 14:25:27 @agent_ppo2.py:185][0m |          -0.0035 |         200.2747 |         -86.8065 |
[32m[20221214 14:25:27 @agent_ppo2.py:185][0m |          -0.0035 |         199.5009 |         -87.3221 |
[32m[20221214 14:25:27 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.74
[32m[20221214 14:25:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.90
[32m[20221214 14:25:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.80
[32m[20221214 14:25:28 @agent_ppo2.py:143][0m Total time:      27.43 min
[32m[20221214 14:25:28 @agent_ppo2.py:145][0m 2510848 total steps have happened
[32m[20221214 14:25:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1226 --------------------------#
[32m[20221214 14:25:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:28 @agent_ppo2.py:185][0m |          -0.0013 |         213.8584 |         -87.3451 |
[32m[20221214 14:25:28 @agent_ppo2.py:185][0m |          -0.0004 |         200.9945 |         -87.3193 |
[32m[20221214 14:25:28 @agent_ppo2.py:185][0m |          -0.0001 |         196.9291 |         -87.1729 |
[32m[20221214 14:25:28 @agent_ppo2.py:185][0m |          -0.0014 |         195.1785 |         -87.5299 |
[32m[20221214 14:25:28 @agent_ppo2.py:185][0m |          -0.0024 |         193.9200 |         -87.3981 |
[32m[20221214 14:25:28 @agent_ppo2.py:185][0m |          -0.0010 |         192.2601 |         -87.4520 |
[32m[20221214 14:25:28 @agent_ppo2.py:185][0m |           0.0003 |         190.9340 |         -87.5611 |
[32m[20221214 14:25:29 @agent_ppo2.py:185][0m |          -0.0013 |         191.2174 |         -87.6694 |
[32m[20221214 14:25:29 @agent_ppo2.py:185][0m |          -0.0014 |         191.0251 |         -87.4703 |
[32m[20221214 14:25:29 @agent_ppo2.py:185][0m |          -0.0004 |         189.5292 |         -87.6531 |
[32m[20221214 14:25:29 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.67
[32m[20221214 14:25:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.29
[32m[20221214 14:25:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.53
[32m[20221214 14:25:29 @agent_ppo2.py:143][0m Total time:      27.45 min
[32m[20221214 14:25:29 @agent_ppo2.py:145][0m 2512896 total steps have happened
[32m[20221214 14:25:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1227 --------------------------#
[32m[20221214 14:25:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:29 @agent_ppo2.py:185][0m |          -0.0045 |         213.6794 |         -86.5883 |
[32m[20221214 14:25:29 @agent_ppo2.py:185][0m |          -0.0017 |         209.8188 |         -86.2114 |
[32m[20221214 14:25:29 @agent_ppo2.py:185][0m |          -0.0018 |         207.0459 |         -86.3095 |
[32m[20221214 14:25:29 @agent_ppo2.py:185][0m |          -0.0042 |         207.2680 |         -86.1355 |
[32m[20221214 14:25:30 @agent_ppo2.py:185][0m |          -0.0016 |         204.4355 |         -86.1113 |
[32m[20221214 14:25:30 @agent_ppo2.py:185][0m |           0.0012 |         204.3821 |         -86.1587 |
[32m[20221214 14:25:30 @agent_ppo2.py:185][0m |          -0.0024 |         202.6568 |         -86.0429 |
[32m[20221214 14:25:30 @agent_ppo2.py:185][0m |          -0.0046 |         204.2165 |         -85.9585 |
[32m[20221214 14:25:30 @agent_ppo2.py:185][0m |           0.0189 |         249.6804 |         -85.9326 |
[32m[20221214 14:25:30 @agent_ppo2.py:185][0m |           0.0005 |         204.7271 |         -85.7135 |
[32m[20221214 14:25:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:25:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.22
[32m[20221214 14:25:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.56
[32m[20221214 14:25:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.06
[32m[20221214 14:25:30 @agent_ppo2.py:143][0m Total time:      27.47 min
[32m[20221214 14:25:30 @agent_ppo2.py:145][0m 2514944 total steps have happened
[32m[20221214 14:25:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1228 --------------------------#
[32m[20221214 14:25:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:30 @agent_ppo2.py:185][0m |          -0.0011 |         189.8916 |         -85.9326 |
[32m[20221214 14:25:31 @agent_ppo2.py:185][0m |           0.0026 |         179.3494 |         -86.1455 |
[32m[20221214 14:25:31 @agent_ppo2.py:185][0m |          -0.0008 |         174.1343 |         -86.1330 |
[32m[20221214 14:25:31 @agent_ppo2.py:185][0m |           0.0129 |         182.9218 |         -85.9576 |
[32m[20221214 14:25:31 @agent_ppo2.py:185][0m |          -0.0026 |         171.1870 |         -85.9659 |
[32m[20221214 14:25:31 @agent_ppo2.py:185][0m |           0.0018 |         170.7580 |         -85.9891 |
[32m[20221214 14:25:31 @agent_ppo2.py:185][0m |          -0.0019 |         169.2145 |         -85.8318 |
[32m[20221214 14:25:31 @agent_ppo2.py:185][0m |           0.0070 |         177.6793 |         -85.8901 |
[32m[20221214 14:25:31 @agent_ppo2.py:185][0m |          -0.0023 |         170.0094 |         -85.7342 |
[32m[20221214 14:25:31 @agent_ppo2.py:185][0m |           0.0010 |         168.6009 |         -85.7709 |
[32m[20221214 14:25:31 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:25:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.23
[32m[20221214 14:25:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.31
[32m[20221214 14:25:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.95
[32m[20221214 14:25:31 @agent_ppo2.py:143][0m Total time:      27.49 min
[32m[20221214 14:25:31 @agent_ppo2.py:145][0m 2516992 total steps have happened
[32m[20221214 14:25:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1229 --------------------------#
[32m[20221214 14:25:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:32 @agent_ppo2.py:185][0m |          -0.0011 |         209.2806 |         -85.7974 |
[32m[20221214 14:25:32 @agent_ppo2.py:185][0m |          -0.0028 |         202.3264 |         -85.8983 |
[32m[20221214 14:25:32 @agent_ppo2.py:185][0m |           0.0002 |         200.4853 |         -85.7957 |
[32m[20221214 14:25:32 @agent_ppo2.py:185][0m |          -0.0015 |         200.5892 |         -85.7032 |
[32m[20221214 14:25:32 @agent_ppo2.py:185][0m |           0.0007 |         200.6387 |         -85.7680 |
[32m[20221214 14:25:32 @agent_ppo2.py:185][0m |           0.0036 |         201.9508 |         -85.8965 |
[32m[20221214 14:25:32 @agent_ppo2.py:185][0m |          -0.0008 |         198.0569 |         -85.8979 |
[32m[20221214 14:25:32 @agent_ppo2.py:185][0m |          -0.0009 |         203.3151 |         -85.9426 |
[32m[20221214 14:25:32 @agent_ppo2.py:185][0m |          -0.0011 |         197.3351 |         -86.0414 |
[32m[20221214 14:25:32 @agent_ppo2.py:185][0m |          -0.0010 |         197.3062 |         -86.0966 |
[32m[20221214 14:25:32 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.50
[32m[20221214 14:25:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.71
[32m[20221214 14:25:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.45
[32m[20221214 14:25:33 @agent_ppo2.py:143][0m Total time:      27.51 min
[32m[20221214 14:25:33 @agent_ppo2.py:145][0m 2519040 total steps have happened
[32m[20221214 14:25:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1230 --------------------------#
[32m[20221214 14:25:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:25:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:33 @agent_ppo2.py:185][0m |           0.0029 |         228.1734 |         -85.5818 |
[32m[20221214 14:25:33 @agent_ppo2.py:185][0m |          -0.0035 |         220.7624 |         -85.5622 |
[32m[20221214 14:25:33 @agent_ppo2.py:185][0m |           0.0017 |         223.5299 |         -85.7087 |
[32m[20221214 14:25:33 @agent_ppo2.py:185][0m |           0.0004 |         217.1603 |         -85.5440 |
[32m[20221214 14:25:33 @agent_ppo2.py:185][0m |          -0.0008 |         216.1116 |         -85.8752 |
[32m[20221214 14:25:33 @agent_ppo2.py:185][0m |          -0.0010 |         216.3687 |         -85.7162 |
[32m[20221214 14:25:33 @agent_ppo2.py:185][0m |           0.0131 |         240.3087 |         -85.5819 |
[32m[20221214 14:25:33 @agent_ppo2.py:185][0m |          -0.0022 |         216.7838 |         -85.4682 |
[32m[20221214 14:25:34 @agent_ppo2.py:185][0m |          -0.0008 |         214.9482 |         -85.5690 |
[32m[20221214 14:25:34 @agent_ppo2.py:185][0m |          -0.0016 |         214.2389 |         -85.6463 |
[32m[20221214 14:25:34 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:25:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.48
[32m[20221214 14:25:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.89
[32m[20221214 14:25:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.26
[32m[20221214 14:25:34 @agent_ppo2.py:143][0m Total time:      27.53 min
[32m[20221214 14:25:34 @agent_ppo2.py:145][0m 2521088 total steps have happened
[32m[20221214 14:25:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1231 --------------------------#
[32m[20221214 14:25:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:34 @agent_ppo2.py:185][0m |          -0.0019 |         238.6416 |         -84.9662 |
[32m[20221214 14:25:34 @agent_ppo2.py:185][0m |          -0.0019 |         228.9150 |         -84.8688 |
[32m[20221214 14:25:34 @agent_ppo2.py:185][0m |          -0.0027 |         224.2476 |         -84.9857 |
[32m[20221214 14:25:34 @agent_ppo2.py:185][0m |          -0.0024 |         221.2872 |         -84.9643 |
[32m[20221214 14:25:34 @agent_ppo2.py:185][0m |           0.0122 |         236.5325 |         -84.9440 |
[32m[20221214 14:25:35 @agent_ppo2.py:185][0m |          -0.0040 |         218.6661 |         -84.6080 |
[32m[20221214 14:25:35 @agent_ppo2.py:185][0m |          -0.0038 |         216.6347 |         -84.7199 |
[32m[20221214 14:25:35 @agent_ppo2.py:185][0m |           0.0092 |         231.2980 |         -84.5445 |
[32m[20221214 14:25:35 @agent_ppo2.py:185][0m |          -0.0021 |         216.2468 |         -84.6337 |
[32m[20221214 14:25:35 @agent_ppo2.py:185][0m |           0.0008 |         215.4003 |         -84.5763 |
[32m[20221214 14:25:35 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.01
[32m[20221214 14:25:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.37
[32m[20221214 14:25:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.26
[32m[20221214 14:25:35 @agent_ppo2.py:143][0m Total time:      27.55 min
[32m[20221214 14:25:35 @agent_ppo2.py:145][0m 2523136 total steps have happened
[32m[20221214 14:25:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1232 --------------------------#
[32m[20221214 14:25:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:35 @agent_ppo2.py:185][0m |          -0.0022 |         229.0543 |         -84.4545 |
[32m[20221214 14:25:35 @agent_ppo2.py:185][0m |          -0.0010 |         222.0494 |         -84.3703 |
[32m[20221214 14:25:35 @agent_ppo2.py:185][0m |          -0.0006 |         220.2812 |         -84.5892 |
[32m[20221214 14:25:36 @agent_ppo2.py:185][0m |          -0.0023 |         220.1068 |         -84.8569 |
[32m[20221214 14:25:36 @agent_ppo2.py:185][0m |          -0.0015 |         219.5351 |         -84.9100 |
[32m[20221214 14:25:36 @agent_ppo2.py:185][0m |           0.0001 |         219.2398 |         -84.3081 |
[32m[20221214 14:25:36 @agent_ppo2.py:185][0m |          -0.0020 |         218.6264 |         -84.6666 |
[32m[20221214 14:25:36 @agent_ppo2.py:185][0m |           0.0042 |         227.3679 |         -84.9403 |
[32m[20221214 14:25:36 @agent_ppo2.py:185][0m |          -0.0015 |         217.8331 |         -84.9569 |
[32m[20221214 14:25:36 @agent_ppo2.py:185][0m |          -0.0020 |         217.8953 |         -85.0128 |
[32m[20221214 14:25:36 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.54
[32m[20221214 14:25:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.43
[32m[20221214 14:25:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.72
[32m[20221214 14:25:36 @agent_ppo2.py:143][0m Total time:      27.57 min
[32m[20221214 14:25:36 @agent_ppo2.py:145][0m 2525184 total steps have happened
[32m[20221214 14:25:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1233 --------------------------#
[32m[20221214 14:25:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:37 @agent_ppo2.py:185][0m |          -0.0004 |         178.4889 |         -87.0079 |
[32m[20221214 14:25:37 @agent_ppo2.py:185][0m |          -0.0028 |         169.9826 |         -86.8081 |
[32m[20221214 14:25:37 @agent_ppo2.py:185][0m |          -0.0005 |         167.4713 |         -87.0979 |
[32m[20221214 14:25:37 @agent_ppo2.py:185][0m |          -0.0017 |         164.9037 |         -86.8241 |
[32m[20221214 14:25:37 @agent_ppo2.py:185][0m |          -0.0011 |         164.8914 |         -87.0795 |
[32m[20221214 14:25:37 @agent_ppo2.py:185][0m |           0.0027 |         163.6989 |         -87.1224 |
[32m[20221214 14:25:37 @agent_ppo2.py:185][0m |          -0.0045 |         163.3731 |         -87.0570 |
[32m[20221214 14:25:37 @agent_ppo2.py:185][0m |          -0.0013 |         162.3849 |         -87.0312 |
[32m[20221214 14:25:37 @agent_ppo2.py:185][0m |           0.0040 |         164.2643 |         -86.9199 |
[32m[20221214 14:25:37 @agent_ppo2.py:185][0m |          -0.0023 |         162.2829 |         -86.9597 |
[32m[20221214 14:25:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.59
[32m[20221214 14:25:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.77
[32m[20221214 14:25:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.37
[32m[20221214 14:25:37 @agent_ppo2.py:143][0m Total time:      27.59 min
[32m[20221214 14:25:37 @agent_ppo2.py:145][0m 2527232 total steps have happened
[32m[20221214 14:25:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1234 --------------------------#
[32m[20221214 14:25:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:38 @agent_ppo2.py:185][0m |          -0.0010 |         226.0518 |         -85.2236 |
[32m[20221214 14:25:38 @agent_ppo2.py:185][0m |           0.0087 |         237.9089 |         -85.3201 |
[32m[20221214 14:25:38 @agent_ppo2.py:185][0m |          -0.0022 |         212.5853 |         -85.4842 |
[32m[20221214 14:25:38 @agent_ppo2.py:185][0m |          -0.0019 |         209.3559 |         -85.4463 |
[32m[20221214 14:25:38 @agent_ppo2.py:185][0m |          -0.0011 |         207.4315 |         -85.5912 |
[32m[20221214 14:25:38 @agent_ppo2.py:185][0m |          -0.0012 |         205.8763 |         -85.6467 |
[32m[20221214 14:25:38 @agent_ppo2.py:185][0m |          -0.0024 |         204.7227 |         -85.4092 |
[32m[20221214 14:25:38 @agent_ppo2.py:185][0m |           0.0131 |         228.9772 |         -85.8354 |
[32m[20221214 14:25:38 @agent_ppo2.py:185][0m |          -0.0020 |         202.5172 |         -85.7715 |
[32m[20221214 14:25:39 @agent_ppo2.py:185][0m |          -0.0024 |         201.5900 |         -85.4570 |
[32m[20221214 14:25:39 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.34
[32m[20221214 14:25:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.32
[32m[20221214 14:25:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.39
[32m[20221214 14:25:39 @agent_ppo2.py:143][0m Total time:      27.61 min
[32m[20221214 14:25:39 @agent_ppo2.py:145][0m 2529280 total steps have happened
[32m[20221214 14:25:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1235 --------------------------#
[32m[20221214 14:25:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:39 @agent_ppo2.py:185][0m |          -0.0009 |         160.8358 |         -86.3031 |
[32m[20221214 14:25:39 @agent_ppo2.py:185][0m |          -0.0015 |         151.8066 |         -86.1582 |
[32m[20221214 14:25:39 @agent_ppo2.py:185][0m |          -0.0011 |         149.0753 |         -86.1019 |
[32m[20221214 14:25:39 @agent_ppo2.py:185][0m |          -0.0031 |         148.3130 |         -86.2575 |
[32m[20221214 14:25:39 @agent_ppo2.py:185][0m |           0.0010 |         149.0644 |         -86.1888 |
[32m[20221214 14:25:39 @agent_ppo2.py:185][0m |           0.0023 |         147.1654 |         -86.1075 |
[32m[20221214 14:25:40 @agent_ppo2.py:185][0m |          -0.0064 |         146.2254 |         -86.2224 |
[32m[20221214 14:25:40 @agent_ppo2.py:185][0m |           0.0032 |         149.0996 |         -86.2722 |
[32m[20221214 14:25:40 @agent_ppo2.py:185][0m |          -0.0017 |         144.7897 |         -86.2389 |
[32m[20221214 14:25:40 @agent_ppo2.py:185][0m |           0.0104 |         154.5247 |         -86.1985 |
[32m[20221214 14:25:40 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.86
[32m[20221214 14:25:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.21
[32m[20221214 14:25:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.54
[32m[20221214 14:25:40 @agent_ppo2.py:143][0m Total time:      27.63 min
[32m[20221214 14:25:40 @agent_ppo2.py:145][0m 2531328 total steps have happened
[32m[20221214 14:25:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1236 --------------------------#
[32m[20221214 14:25:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:40 @agent_ppo2.py:185][0m |           0.0084 |         169.6364 |         -85.1078 |
[32m[20221214 14:25:40 @agent_ppo2.py:185][0m |          -0.0073 |         145.1040 |         -85.4280 |
[32m[20221214 14:25:40 @agent_ppo2.py:185][0m |          -0.0035 |         139.5800 |         -85.2714 |
[32m[20221214 14:25:40 @agent_ppo2.py:185][0m |          -0.0064 |         137.9738 |         -85.3738 |
[32m[20221214 14:25:41 @agent_ppo2.py:185][0m |          -0.0046 |         136.4912 |         -85.1093 |
[32m[20221214 14:25:41 @agent_ppo2.py:185][0m |          -0.0057 |         136.0405 |         -85.2118 |
[32m[20221214 14:25:41 @agent_ppo2.py:185][0m |          -0.0081 |         135.6991 |         -85.1290 |
[32m[20221214 14:25:41 @agent_ppo2.py:185][0m |          -0.0044 |         135.2214 |         -85.1033 |
[32m[20221214 14:25:41 @agent_ppo2.py:185][0m |          -0.0046 |         134.2844 |         -85.0121 |
[32m[20221214 14:25:41 @agent_ppo2.py:185][0m |           0.0011 |         139.7459 |         -85.0088 |
[32m[20221214 14:25:41 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.36
[32m[20221214 14:25:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.62
[32m[20221214 14:25:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.60
[32m[20221214 14:25:41 @agent_ppo2.py:143][0m Total time:      27.65 min
[32m[20221214 14:25:41 @agent_ppo2.py:145][0m 2533376 total steps have happened
[32m[20221214 14:25:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1237 --------------------------#
[32m[20221214 14:25:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:41 @agent_ppo2.py:185][0m |          -0.0035 |         215.5370 |         -84.4533 |
[32m[20221214 14:25:42 @agent_ppo2.py:185][0m |          -0.0011 |         209.5377 |         -84.5034 |
[32m[20221214 14:25:42 @agent_ppo2.py:185][0m |           0.0047 |         211.2135 |         -84.5438 |
[32m[20221214 14:25:42 @agent_ppo2.py:185][0m |          -0.0053 |         207.9512 |         -84.5858 |
[32m[20221214 14:25:42 @agent_ppo2.py:185][0m |          -0.0012 |         205.2751 |         -84.8075 |
[32m[20221214 14:25:42 @agent_ppo2.py:185][0m |          -0.0008 |         204.0076 |         -84.6592 |
[32m[20221214 14:25:42 @agent_ppo2.py:185][0m |          -0.0026 |         203.8267 |         -85.0040 |
[32m[20221214 14:25:42 @agent_ppo2.py:185][0m |          -0.0029 |         202.5272 |         -84.8444 |
[32m[20221214 14:25:42 @agent_ppo2.py:185][0m |          -0.0017 |         202.2925 |         -84.7465 |
[32m[20221214 14:25:42 @agent_ppo2.py:185][0m |          -0.0018 |         202.1420 |         -84.8103 |
[32m[20221214 14:25:42 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.66
[32m[20221214 14:25:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.14
[32m[20221214 14:25:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.08
[32m[20221214 14:25:42 @agent_ppo2.py:143][0m Total time:      27.67 min
[32m[20221214 14:25:42 @agent_ppo2.py:145][0m 2535424 total steps have happened
[32m[20221214 14:25:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1238 --------------------------#
[32m[20221214 14:25:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:43 @agent_ppo2.py:185][0m |          -0.0007 |         181.2838 |         -85.6343 |
[32m[20221214 14:25:43 @agent_ppo2.py:185][0m |          -0.0026 |         177.0416 |         -85.6508 |
[32m[20221214 14:25:43 @agent_ppo2.py:185][0m |          -0.0011 |         175.0343 |         -85.5748 |
[32m[20221214 14:25:43 @agent_ppo2.py:185][0m |          -0.0004 |         172.9264 |         -85.6925 |
[32m[20221214 14:25:43 @agent_ppo2.py:185][0m |          -0.0007 |         171.8806 |         -85.8191 |
[32m[20221214 14:25:43 @agent_ppo2.py:185][0m |           0.0017 |         171.6778 |         -85.9110 |
[32m[20221214 14:25:43 @agent_ppo2.py:185][0m |          -0.0011 |         170.9448 |         -85.8097 |
[32m[20221214 14:25:43 @agent_ppo2.py:185][0m |          -0.0014 |         169.3320 |         -85.9181 |
[32m[20221214 14:25:43 @agent_ppo2.py:185][0m |          -0.0040 |         169.0226 |         -86.0897 |
[32m[20221214 14:25:43 @agent_ppo2.py:185][0m |          -0.0034 |         168.8375 |         -86.1003 |
[32m[20221214 14:25:43 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:25:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.84
[32m[20221214 14:25:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.94
[32m[20221214 14:25:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.84
[32m[20221214 14:25:44 @agent_ppo2.py:143][0m Total time:      27.69 min
[32m[20221214 14:25:44 @agent_ppo2.py:145][0m 2537472 total steps have happened
[32m[20221214 14:25:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1239 --------------------------#
[32m[20221214 14:25:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:44 @agent_ppo2.py:185][0m |           0.0015 |         186.5193 |         -88.6126 |
[32m[20221214 14:25:44 @agent_ppo2.py:185][0m |          -0.0001 |         178.1576 |         -88.6813 |
[32m[20221214 14:25:44 @agent_ppo2.py:185][0m |          -0.0006 |         175.2764 |         -88.8670 |
[32m[20221214 14:25:44 @agent_ppo2.py:185][0m |          -0.0016 |         173.3990 |         -88.8836 |
[32m[20221214 14:25:44 @agent_ppo2.py:185][0m |          -0.0018 |         172.0422 |         -88.7483 |
[32m[20221214 14:25:44 @agent_ppo2.py:185][0m |           0.0060 |         179.1339 |         -88.9097 |
[32m[20221214 14:25:44 @agent_ppo2.py:185][0m |          -0.0035 |         170.4583 |         -88.9893 |
[32m[20221214 14:25:44 @agent_ppo2.py:185][0m |          -0.0014 |         169.8753 |         -88.9888 |
[32m[20221214 14:25:45 @agent_ppo2.py:185][0m |          -0.0067 |         169.6891 |         -88.8691 |
[32m[20221214 14:25:45 @agent_ppo2.py:185][0m |          -0.0028 |         168.3025 |         -89.0635 |
[32m[20221214 14:25:45 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.97
[32m[20221214 14:25:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.19
[32m[20221214 14:25:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.09
[32m[20221214 14:25:45 @agent_ppo2.py:143][0m Total time:      27.71 min
[32m[20221214 14:25:45 @agent_ppo2.py:145][0m 2539520 total steps have happened
[32m[20221214 14:25:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1240 --------------------------#
[32m[20221214 14:25:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:25:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:45 @agent_ppo2.py:185][0m |           0.0001 |         149.4864 |         -86.7673 |
[32m[20221214 14:25:45 @agent_ppo2.py:185][0m |          -0.0020 |         141.7976 |         -86.7844 |
[32m[20221214 14:25:45 @agent_ppo2.py:185][0m |           0.0001 |         138.4787 |         -86.6413 |
[32m[20221214 14:25:45 @agent_ppo2.py:185][0m |           0.0132 |         155.4402 |         -86.7479 |
[32m[20221214 14:25:45 @agent_ppo2.py:185][0m |          -0.0026 |         135.4656 |         -86.7814 |
[32m[20221214 14:25:46 @agent_ppo2.py:185][0m |          -0.0012 |         134.2830 |         -86.6131 |
[32m[20221214 14:25:46 @agent_ppo2.py:185][0m |          -0.0028 |         133.5643 |         -86.3916 |
[32m[20221214 14:25:46 @agent_ppo2.py:185][0m |           0.0033 |         133.6409 |         -86.4183 |
[32m[20221214 14:25:46 @agent_ppo2.py:185][0m |           0.0002 |         132.1411 |         -86.7088 |
[32m[20221214 14:25:46 @agent_ppo2.py:185][0m |          -0.0046 |         133.1422 |         -86.3714 |
[32m[20221214 14:25:46 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.73
[32m[20221214 14:25:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.32
[32m[20221214 14:25:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.92
[32m[20221214 14:25:46 @agent_ppo2.py:143][0m Total time:      27.73 min
[32m[20221214 14:25:46 @agent_ppo2.py:145][0m 2541568 total steps have happened
[32m[20221214 14:25:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1241 --------------------------#
[32m[20221214 14:25:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:46 @agent_ppo2.py:185][0m |          -0.0008 |         185.1396 |         -87.9560 |
[32m[20221214 14:25:46 @agent_ppo2.py:185][0m |           0.0013 |         175.0088 |         -87.9889 |
[32m[20221214 14:25:47 @agent_ppo2.py:185][0m |           0.0018 |         173.4328 |         -87.9032 |
[32m[20221214 14:25:47 @agent_ppo2.py:185][0m |          -0.0046 |         172.2762 |         -87.9271 |
[32m[20221214 14:25:47 @agent_ppo2.py:185][0m |          -0.0021 |         172.8374 |         -87.9441 |
[32m[20221214 14:25:47 @agent_ppo2.py:185][0m |          -0.0026 |         170.2498 |         -87.9200 |
[32m[20221214 14:25:47 @agent_ppo2.py:185][0m |          -0.0012 |         169.4604 |         -87.8004 |
[32m[20221214 14:25:47 @agent_ppo2.py:185][0m |          -0.0031 |         168.9262 |         -88.0186 |
[32m[20221214 14:25:47 @agent_ppo2.py:185][0m |          -0.0051 |         169.2176 |         -88.0636 |
[32m[20221214 14:25:47 @agent_ppo2.py:185][0m |          -0.0013 |         167.8444 |         -87.9807 |
[32m[20221214 14:25:47 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.27
[32m[20221214 14:25:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.80
[32m[20221214 14:25:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.45
[32m[20221214 14:25:47 @agent_ppo2.py:143][0m Total time:      27.75 min
[32m[20221214 14:25:47 @agent_ppo2.py:145][0m 2543616 total steps have happened
[32m[20221214 14:25:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1242 --------------------------#
[32m[20221214 14:25:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:48 @agent_ppo2.py:185][0m |           0.0081 |         164.9908 |         -87.8309 |
[32m[20221214 14:25:48 @agent_ppo2.py:185][0m |           0.0005 |         152.5364 |         -87.7451 |
[32m[20221214 14:25:48 @agent_ppo2.py:185][0m |          -0.0015 |         150.7907 |         -87.8225 |
[32m[20221214 14:25:48 @agent_ppo2.py:185][0m |          -0.0019 |         150.5322 |         -87.8220 |
[32m[20221214 14:25:48 @agent_ppo2.py:185][0m |          -0.0000 |         149.6091 |         -87.6473 |
[32m[20221214 14:25:48 @agent_ppo2.py:185][0m |          -0.0029 |         149.3185 |         -87.9764 |
[32m[20221214 14:25:48 @agent_ppo2.py:185][0m |          -0.0013 |         148.5686 |         -87.5846 |
[32m[20221214 14:25:48 @agent_ppo2.py:185][0m |           0.0088 |         159.1312 |         -87.5795 |
[32m[20221214 14:25:48 @agent_ppo2.py:185][0m |          -0.0017 |         147.8868 |         -87.7849 |
[32m[20221214 14:25:48 @agent_ppo2.py:185][0m |          -0.0048 |         147.8161 |         -87.5662 |
[32m[20221214 14:25:48 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.79
[32m[20221214 14:25:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.81
[32m[20221214 14:25:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.58
[32m[20221214 14:25:48 @agent_ppo2.py:143][0m Total time:      27.77 min
[32m[20221214 14:25:48 @agent_ppo2.py:145][0m 2545664 total steps have happened
[32m[20221214 14:25:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1243 --------------------------#
[32m[20221214 14:25:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:49 @agent_ppo2.py:185][0m |          -0.0014 |         238.5654 |         -87.2700 |
[32m[20221214 14:25:49 @agent_ppo2.py:185][0m |          -0.0037 |         221.6374 |         -87.3743 |
[32m[20221214 14:25:49 @agent_ppo2.py:185][0m |          -0.0015 |         217.1735 |         -87.3919 |
[32m[20221214 14:25:49 @agent_ppo2.py:185][0m |           0.0038 |         215.6251 |         -87.0955 |
[32m[20221214 14:25:49 @agent_ppo2.py:185][0m |           0.0360 |         260.3603 |         -87.4265 |
[32m[20221214 14:25:49 @agent_ppo2.py:185][0m |          -0.0019 |         212.3687 |         -87.2350 |
[32m[20221214 14:25:49 @agent_ppo2.py:185][0m |           0.0007 |         205.0928 |         -87.5469 |
[32m[20221214 14:25:49 @agent_ppo2.py:185][0m |           0.0015 |         204.1204 |         -87.3519 |
[32m[20221214 14:25:49 @agent_ppo2.py:185][0m |          -0.0026 |         202.7413 |         -87.6179 |
[32m[20221214 14:25:50 @agent_ppo2.py:185][0m |          -0.0047 |         202.6826 |         -87.5536 |
[32m[20221214 14:25:50 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.54
[32m[20221214 14:25:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.97
[32m[20221214 14:25:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.91
[32m[20221214 14:25:50 @agent_ppo2.py:143][0m Total time:      27.79 min
[32m[20221214 14:25:50 @agent_ppo2.py:145][0m 2547712 total steps have happened
[32m[20221214 14:25:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1244 --------------------------#
[32m[20221214 14:25:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:50 @agent_ppo2.py:185][0m |          -0.0019 |         184.9420 |         -86.4304 |
[32m[20221214 14:25:50 @agent_ppo2.py:185][0m |          -0.0010 |         168.6581 |         -86.3258 |
[32m[20221214 14:25:50 @agent_ppo2.py:185][0m |           0.0001 |         164.3236 |         -86.2091 |
[32m[20221214 14:25:50 @agent_ppo2.py:185][0m |          -0.0007 |         161.0744 |         -86.4465 |
[32m[20221214 14:25:50 @agent_ppo2.py:185][0m |          -0.0026 |         159.6742 |         -86.9265 |
[32m[20221214 14:25:50 @agent_ppo2.py:185][0m |          -0.0023 |         159.3635 |         -86.8915 |
[32m[20221214 14:25:51 @agent_ppo2.py:185][0m |           0.0052 |         163.9151 |         -87.0600 |
[32m[20221214 14:25:51 @agent_ppo2.py:185][0m |          -0.0033 |         157.7319 |         -86.8512 |
[32m[20221214 14:25:51 @agent_ppo2.py:185][0m |           0.0033 |         158.3282 |         -87.2069 |
[32m[20221214 14:25:51 @agent_ppo2.py:185][0m |          -0.0022 |         156.5239 |         -87.1959 |
[32m[20221214 14:25:51 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:25:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.63
[32m[20221214 14:25:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.62
[32m[20221214 14:25:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.32
[32m[20221214 14:25:51 @agent_ppo2.py:143][0m Total time:      27.81 min
[32m[20221214 14:25:51 @agent_ppo2.py:145][0m 2549760 total steps have happened
[32m[20221214 14:25:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1245 --------------------------#
[32m[20221214 14:25:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:51 @agent_ppo2.py:185][0m |           0.0053 |         211.9162 |         -87.3570 |
[32m[20221214 14:25:51 @agent_ppo2.py:185][0m |          -0.0014 |         192.6323 |         -87.4394 |
[32m[20221214 14:25:51 @agent_ppo2.py:185][0m |          -0.0005 |         189.3448 |         -87.2840 |
[32m[20221214 14:25:51 @agent_ppo2.py:185][0m |          -0.0038 |         186.9457 |         -87.5332 |
[32m[20221214 14:25:52 @agent_ppo2.py:185][0m |          -0.0034 |         186.8024 |         -87.5087 |
[32m[20221214 14:25:52 @agent_ppo2.py:185][0m |           0.0010 |         188.1109 |         -87.5410 |
[32m[20221214 14:25:52 @agent_ppo2.py:185][0m |          -0.0001 |         185.5838 |         -87.8718 |
[32m[20221214 14:25:52 @agent_ppo2.py:185][0m |          -0.0043 |         184.0477 |         -87.8464 |
[32m[20221214 14:25:52 @agent_ppo2.py:185][0m |          -0.0043 |         183.9043 |         -87.5799 |
[32m[20221214 14:25:52 @agent_ppo2.py:185][0m |          -0.0064 |         184.1860 |         -87.9605 |
[32m[20221214 14:25:52 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:25:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.06
[32m[20221214 14:25:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.74
[32m[20221214 14:25:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.54
[32m[20221214 14:25:52 @agent_ppo2.py:143][0m Total time:      27.83 min
[32m[20221214 14:25:52 @agent_ppo2.py:145][0m 2551808 total steps have happened
[32m[20221214 14:25:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1246 --------------------------#
[32m[20221214 14:25:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:52 @agent_ppo2.py:185][0m |          -0.0045 |         232.4064 |         -88.9712 |
[32m[20221214 14:25:53 @agent_ppo2.py:185][0m |          -0.0031 |         219.8270 |         -88.6861 |
[32m[20221214 14:25:53 @agent_ppo2.py:185][0m |          -0.0044 |         216.2356 |         -88.5840 |
[32m[20221214 14:25:53 @agent_ppo2.py:185][0m |          -0.0046 |         213.6398 |         -88.8284 |
[32m[20221214 14:25:53 @agent_ppo2.py:185][0m |           0.0035 |         219.2999 |         -89.0993 |
[32m[20221214 14:25:53 @agent_ppo2.py:185][0m |           0.0032 |         220.5089 |         -89.4091 |
[32m[20221214 14:25:53 @agent_ppo2.py:185][0m |          -0.0090 |         209.9271 |         -89.3328 |
[32m[20221214 14:25:53 @agent_ppo2.py:185][0m |          -0.0068 |         207.3184 |         -89.3050 |
[32m[20221214 14:25:53 @agent_ppo2.py:185][0m |          -0.0068 |         207.2841 |         -89.2053 |
[32m[20221214 14:25:53 @agent_ppo2.py:185][0m |          -0.0095 |         207.5431 |         -89.3467 |
[32m[20221214 14:25:53 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.32
[32m[20221214 14:25:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.51
[32m[20221214 14:25:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.18
[32m[20221214 14:25:53 @agent_ppo2.py:143][0m Total time:      27.86 min
[32m[20221214 14:25:53 @agent_ppo2.py:145][0m 2553856 total steps have happened
[32m[20221214 14:25:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1247 --------------------------#
[32m[20221214 14:25:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:54 @agent_ppo2.py:185][0m |          -0.0025 |         224.7013 |         -91.8754 |
[32m[20221214 14:25:54 @agent_ppo2.py:185][0m |          -0.0045 |         209.3784 |         -91.9878 |
[32m[20221214 14:25:54 @agent_ppo2.py:185][0m |          -0.0051 |         200.3577 |         -91.5676 |
[32m[20221214 14:25:54 @agent_ppo2.py:185][0m |          -0.0056 |         197.0975 |         -91.6844 |
[32m[20221214 14:25:54 @agent_ppo2.py:185][0m |          -0.0006 |         194.6217 |         -91.6566 |
[32m[20221214 14:25:54 @agent_ppo2.py:185][0m |          -0.0019 |         192.1906 |         -91.6622 |
[32m[20221214 14:25:54 @agent_ppo2.py:185][0m |          -0.0020 |         190.9875 |         -91.6913 |
[32m[20221214 14:25:54 @agent_ppo2.py:185][0m |          -0.0059 |         189.7419 |         -91.8193 |
[32m[20221214 14:25:54 @agent_ppo2.py:185][0m |          -0.0056 |         187.2218 |         -91.9693 |
[32m[20221214 14:25:54 @agent_ppo2.py:185][0m |           0.0006 |         187.7818 |         -91.3445 |
[32m[20221214 14:25:54 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.42
[32m[20221214 14:25:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.09
[32m[20221214 14:25:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.81
[32m[20221214 14:25:55 @agent_ppo2.py:143][0m Total time:      27.88 min
[32m[20221214 14:25:55 @agent_ppo2.py:145][0m 2555904 total steps have happened
[32m[20221214 14:25:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1248 --------------------------#
[32m[20221214 14:25:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:55 @agent_ppo2.py:185][0m |           0.0040 |         272.7979 |         -90.1052 |
[32m[20221214 14:25:55 @agent_ppo2.py:185][0m |          -0.0004 |         257.4306 |         -90.2368 |
[32m[20221214 14:25:55 @agent_ppo2.py:185][0m |          -0.0033 |         256.2740 |         -90.1398 |
[32m[20221214 14:25:55 @agent_ppo2.py:185][0m |          -0.0008 |         256.1311 |         -90.2371 |
[32m[20221214 14:25:55 @agent_ppo2.py:185][0m |          -0.0004 |         255.5244 |         -90.1220 |
[32m[20221214 14:25:55 @agent_ppo2.py:185][0m |          -0.0018 |         255.6281 |         -90.1361 |
[32m[20221214 14:25:55 @agent_ppo2.py:185][0m |          -0.0009 |         254.6514 |         -90.4535 |
[32m[20221214 14:25:55 @agent_ppo2.py:185][0m |          -0.0016 |         255.7400 |         -90.0248 |
[32m[20221214 14:25:56 @agent_ppo2.py:185][0m |          -0.0024 |         254.8215 |         -90.2219 |
[32m[20221214 14:25:56 @agent_ppo2.py:185][0m |          -0.0031 |         254.1334 |         -90.1417 |
[32m[20221214 14:25:56 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.35
[32m[20221214 14:25:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.89
[32m[20221214 14:25:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.98
[32m[20221214 14:25:56 @agent_ppo2.py:143][0m Total time:      27.90 min
[32m[20221214 14:25:56 @agent_ppo2.py:145][0m 2557952 total steps have happened
[32m[20221214 14:25:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1249 --------------------------#
[32m[20221214 14:25:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:56 @agent_ppo2.py:185][0m |          -0.0021 |         264.4676 |         -89.9404 |
[32m[20221214 14:25:56 @agent_ppo2.py:185][0m |          -0.0011 |         256.4207 |         -89.7900 |
[32m[20221214 14:25:56 @agent_ppo2.py:185][0m |          -0.0011 |         255.4070 |         -89.9428 |
[32m[20221214 14:25:56 @agent_ppo2.py:185][0m |           0.0004 |         252.6770 |         -89.9165 |
[32m[20221214 14:25:56 @agent_ppo2.py:185][0m |           0.0002 |         251.9857 |         -89.8887 |
[32m[20221214 14:25:57 @agent_ppo2.py:185][0m |          -0.0015 |         250.3140 |         -90.1152 |
[32m[20221214 14:25:57 @agent_ppo2.py:185][0m |          -0.0033 |         249.9562 |         -89.9727 |
[32m[20221214 14:25:57 @agent_ppo2.py:185][0m |           0.0000 |         249.6129 |         -89.8722 |
[32m[20221214 14:25:57 @agent_ppo2.py:185][0m |          -0.0043 |         250.8375 |         -90.0417 |
[32m[20221214 14:25:57 @agent_ppo2.py:185][0m |          -0.0025 |         249.7817 |         -90.0475 |
[32m[20221214 14:25:57 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:25:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.10
[32m[20221214 14:25:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.02
[32m[20221214 14:25:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.53
[32m[20221214 14:25:57 @agent_ppo2.py:143][0m Total time:      27.92 min
[32m[20221214 14:25:57 @agent_ppo2.py:145][0m 2560000 total steps have happened
[32m[20221214 14:25:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1250 --------------------------#
[32m[20221214 14:25:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:25:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:57 @agent_ppo2.py:185][0m |           0.0038 |         218.6956 |         -89.8924 |
[32m[20221214 14:25:57 @agent_ppo2.py:185][0m |          -0.0026 |         214.0874 |         -90.2101 |
[32m[20221214 14:25:58 @agent_ppo2.py:185][0m |          -0.0036 |         212.3068 |         -90.2431 |
[32m[20221214 14:25:58 @agent_ppo2.py:185][0m |          -0.0034 |         210.6204 |         -90.5824 |
[32m[20221214 14:25:58 @agent_ppo2.py:185][0m |          -0.0051 |         209.4666 |         -90.8453 |
[32m[20221214 14:25:58 @agent_ppo2.py:185][0m |          -0.0027 |         208.5245 |         -90.9661 |
[32m[20221214 14:25:58 @agent_ppo2.py:185][0m |           0.0010 |         208.1328 |         -91.1047 |
[32m[20221214 14:25:58 @agent_ppo2.py:185][0m |          -0.0045 |         206.5367 |         -91.2298 |
[32m[20221214 14:25:58 @agent_ppo2.py:185][0m |          -0.0022 |         206.0088 |         -91.3692 |
[32m[20221214 14:25:58 @agent_ppo2.py:185][0m |          -0.0033 |         204.9229 |         -91.6586 |
[32m[20221214 14:25:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:25:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.30
[32m[20221214 14:25:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.23
[32m[20221214 14:25:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.72
[32m[20221214 14:25:58 @agent_ppo2.py:143][0m Total time:      27.94 min
[32m[20221214 14:25:58 @agent_ppo2.py:145][0m 2562048 total steps have happened
[32m[20221214 14:25:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1251 --------------------------#
[32m[20221214 14:25:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:25:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:25:59 @agent_ppo2.py:185][0m |           0.0006 |         296.2303 |         -93.6941 |
[32m[20221214 14:25:59 @agent_ppo2.py:185][0m |           0.0003 |         286.9650 |         -93.7981 |
[32m[20221214 14:25:59 @agent_ppo2.py:185][0m |           0.0068 |         295.6473 |         -94.0230 |
[32m[20221214 14:25:59 @agent_ppo2.py:185][0m |          -0.0022 |         282.5213 |         -93.7626 |
[32m[20221214 14:25:59 @agent_ppo2.py:185][0m |          -0.0011 |         281.1330 |         -94.1375 |
[32m[20221214 14:25:59 @agent_ppo2.py:185][0m |          -0.0019 |         279.8493 |         -93.9078 |
[32m[20221214 14:25:59 @agent_ppo2.py:185][0m |          -0.0018 |         279.5066 |         -93.8061 |
[32m[20221214 14:25:59 @agent_ppo2.py:185][0m |          -0.0034 |         277.3015 |         -93.8797 |
[32m[20221214 14:25:59 @agent_ppo2.py:185][0m |           0.0108 |         291.3435 |         -93.8007 |
[32m[20221214 14:25:59 @agent_ppo2.py:185][0m |          -0.0006 |         276.0415 |         -93.2060 |
[32m[20221214 14:25:59 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:26:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.15
[32m[20221214 14:26:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.09
[32m[20221214 14:26:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.58
[32m[20221214 14:26:00 @agent_ppo2.py:143][0m Total time:      27.96 min
[32m[20221214 14:26:00 @agent_ppo2.py:145][0m 2564096 total steps have happened
[32m[20221214 14:26:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1252 --------------------------#
[32m[20221214 14:26:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:00 @agent_ppo2.py:185][0m |           0.0030 |         260.8230 |         -94.5402 |
[32m[20221214 14:26:00 @agent_ppo2.py:185][0m |          -0.0009 |         252.5660 |         -94.5333 |
[32m[20221214 14:26:00 @agent_ppo2.py:185][0m |          -0.0034 |         251.7068 |         -94.6463 |
[32m[20221214 14:26:00 @agent_ppo2.py:185][0m |           0.0020 |         251.2555 |         -94.3688 |
[32m[20221214 14:26:00 @agent_ppo2.py:185][0m |          -0.0007 |         248.3224 |         -94.2358 |
[32m[20221214 14:26:00 @agent_ppo2.py:185][0m |           0.0075 |         263.3736 |         -94.3504 |
[32m[20221214 14:26:00 @agent_ppo2.py:185][0m |          -0.0033 |         246.7436 |         -94.5107 |
[32m[20221214 14:26:00 @agent_ppo2.py:185][0m |          -0.0015 |         244.9210 |         -94.0104 |
[32m[20221214 14:26:00 @agent_ppo2.py:185][0m |           0.0050 |         250.6745 |         -94.4542 |
[32m[20221214 14:26:01 @agent_ppo2.py:185][0m |          -0.0023 |         245.2417 |         -93.8632 |
[32m[20221214 14:26:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:26:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.31
[32m[20221214 14:26:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.29
[32m[20221214 14:26:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.89
[32m[20221214 14:26:01 @agent_ppo2.py:143][0m Total time:      27.98 min
[32m[20221214 14:26:01 @agent_ppo2.py:145][0m 2566144 total steps have happened
[32m[20221214 14:26:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1253 --------------------------#
[32m[20221214 14:26:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:01 @agent_ppo2.py:185][0m |          -0.0031 |         221.4518 |         -93.9269 |
[32m[20221214 14:26:01 @agent_ppo2.py:185][0m |           0.0063 |         222.1723 |         -93.2904 |
[32m[20221214 14:26:01 @agent_ppo2.py:185][0m |           0.0012 |         191.2906 |         -93.1965 |
[32m[20221214 14:26:01 @agent_ppo2.py:185][0m |           0.0017 |         187.9503 |         -93.4430 |
[32m[20221214 14:26:01 @agent_ppo2.py:185][0m |           0.0055 |         195.0494 |         -93.4371 |
[32m[20221214 14:26:01 @agent_ppo2.py:185][0m |          -0.0015 |         184.2806 |         -93.0692 |
[32m[20221214 14:26:02 @agent_ppo2.py:185][0m |          -0.0013 |         185.1365 |         -93.0280 |
[32m[20221214 14:26:02 @agent_ppo2.py:185][0m |          -0.0075 |         183.4596 |         -93.0844 |
[32m[20221214 14:26:02 @agent_ppo2.py:185][0m |           0.0058 |         192.6231 |         -93.0605 |
[32m[20221214 14:26:02 @agent_ppo2.py:185][0m |           0.0116 |         188.0822 |         -93.2262 |
[32m[20221214 14:26:02 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:26:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.53
[32m[20221214 14:26:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.54
[32m[20221214 14:26:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.83
[32m[20221214 14:26:02 @agent_ppo2.py:143][0m Total time:      28.00 min
[32m[20221214 14:26:02 @agent_ppo2.py:145][0m 2568192 total steps have happened
[32m[20221214 14:26:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1254 --------------------------#
[32m[20221214 14:26:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:02 @agent_ppo2.py:185][0m |          -0.0005 |         250.3919 |         -90.7079 |
[32m[20221214 14:26:02 @agent_ppo2.py:185][0m |           0.0050 |         243.9134 |         -90.6231 |
[32m[20221214 14:26:02 @agent_ppo2.py:185][0m |          -0.0029 |         238.9507 |         -90.5785 |
[32m[20221214 14:26:03 @agent_ppo2.py:185][0m |          -0.0007 |         239.5890 |         -90.6029 |
[32m[20221214 14:26:03 @agent_ppo2.py:185][0m |          -0.0020 |         237.1492 |         -90.5867 |
[32m[20221214 14:26:03 @agent_ppo2.py:185][0m |          -0.0017 |         236.5241 |         -90.4728 |
[32m[20221214 14:26:03 @agent_ppo2.py:185][0m |          -0.0036 |         236.6219 |         -90.2756 |
[32m[20221214 14:26:03 @agent_ppo2.py:185][0m |           0.0061 |         252.3951 |         -90.6027 |
[32m[20221214 14:26:03 @agent_ppo2.py:185][0m |          -0.0004 |         236.0493 |         -90.3040 |
[32m[20221214 14:26:03 @agent_ppo2.py:185][0m |          -0.0010 |         234.0589 |         -90.3967 |
[32m[20221214 14:26:03 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:26:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.67
[32m[20221214 14:26:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.90
[32m[20221214 14:26:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.64
[32m[20221214 14:26:03 @agent_ppo2.py:143][0m Total time:      28.02 min
[32m[20221214 14:26:03 @agent_ppo2.py:145][0m 2570240 total steps have happened
[32m[20221214 14:26:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1255 --------------------------#
[32m[20221214 14:26:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:04 @agent_ppo2.py:185][0m |          -0.0032 |         248.4814 |         -91.7298 |
[32m[20221214 14:26:04 @agent_ppo2.py:185][0m |          -0.0032 |         240.7733 |         -91.2692 |
[32m[20221214 14:26:04 @agent_ppo2.py:185][0m |           0.0116 |         262.9377 |         -90.9415 |
[32m[20221214 14:26:04 @agent_ppo2.py:185][0m |           0.0045 |         248.6195 |         -90.8322 |
[32m[20221214 14:26:04 @agent_ppo2.py:185][0m |          -0.0025 |         238.2566 |         -90.6526 |
[32m[20221214 14:26:04 @agent_ppo2.py:185][0m |          -0.0058 |         239.8919 |         -90.6643 |
[32m[20221214 14:26:04 @agent_ppo2.py:185][0m |          -0.0053 |         237.0241 |         -90.5651 |
[32m[20221214 14:26:04 @agent_ppo2.py:185][0m |           0.0010 |         238.8723 |         -90.5546 |
[32m[20221214 14:26:04 @agent_ppo2.py:185][0m |          -0.0031 |         234.4760 |         -90.1602 |
[32m[20221214 14:26:04 @agent_ppo2.py:185][0m |          -0.0029 |         234.3308 |         -90.0570 |
[32m[20221214 14:26:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:26:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.78
[32m[20221214 14:26:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.64
[32m[20221214 14:26:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 787.22
[32m[20221214 14:26:04 @agent_ppo2.py:143][0m Total time:      28.04 min
[32m[20221214 14:26:04 @agent_ppo2.py:145][0m 2572288 total steps have happened
[32m[20221214 14:26:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1256 --------------------------#
[32m[20221214 14:26:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:05 @agent_ppo2.py:185][0m |           0.0096 |         296.5407 |         -88.1930 |
[32m[20221214 14:26:05 @agent_ppo2.py:185][0m |          -0.0043 |         264.8042 |         -88.1761 |
[32m[20221214 14:26:05 @agent_ppo2.py:185][0m |          -0.0050 |         263.2930 |         -88.2214 |
[32m[20221214 14:26:05 @agent_ppo2.py:185][0m |          -0.0006 |         264.5259 |         -88.1272 |
[32m[20221214 14:26:05 @agent_ppo2.py:185][0m |          -0.0037 |         259.5393 |         -88.0928 |
[32m[20221214 14:26:05 @agent_ppo2.py:185][0m |          -0.0001 |         261.8910 |         -87.8347 |
[32m[20221214 14:26:05 @agent_ppo2.py:185][0m |          -0.0012 |         258.2063 |         -87.6506 |
[32m[20221214 14:26:05 @agent_ppo2.py:185][0m |           0.0057 |         268.4682 |         -87.8337 |
[32m[20221214 14:26:05 @agent_ppo2.py:185][0m |          -0.0026 |         256.6349 |         -87.7957 |
[32m[20221214 14:26:05 @agent_ppo2.py:185][0m |          -0.0043 |         255.6806 |         -87.5944 |
[32m[20221214 14:26:05 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:26:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 784.07
[32m[20221214 14:26:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.05
[32m[20221214 14:26:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.71
[32m[20221214 14:26:06 @agent_ppo2.py:143][0m Total time:      28.06 min
[32m[20221214 14:26:06 @agent_ppo2.py:145][0m 2574336 total steps have happened
[32m[20221214 14:26:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1257 --------------------------#
[32m[20221214 14:26:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:06 @agent_ppo2.py:185][0m |          -0.0003 |         238.5798 |         -91.6415 |
[32m[20221214 14:26:06 @agent_ppo2.py:185][0m |          -0.0027 |         228.2522 |         -91.6140 |
[32m[20221214 14:26:06 @agent_ppo2.py:185][0m |          -0.0011 |         224.0620 |         -91.8229 |
[32m[20221214 14:26:06 @agent_ppo2.py:185][0m |          -0.0029 |         222.8545 |         -91.9171 |
[32m[20221214 14:26:06 @agent_ppo2.py:185][0m |           0.0093 |         240.0118 |         -91.9148 |
[32m[20221214 14:26:06 @agent_ppo2.py:185][0m |          -0.0002 |         220.5942 |         -92.2001 |
[32m[20221214 14:26:06 @agent_ppo2.py:185][0m |          -0.0015 |         219.7182 |         -92.0355 |
[32m[20221214 14:26:07 @agent_ppo2.py:185][0m |           0.0018 |         220.7470 |         -92.2789 |
[32m[20221214 14:26:07 @agent_ppo2.py:185][0m |          -0.0008 |         218.7084 |         -92.0883 |
[32m[20221214 14:26:07 @agent_ppo2.py:185][0m |           0.0175 |         242.5800 |         -92.3515 |
[32m[20221214 14:26:07 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:26:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 784.52
[32m[20221214 14:26:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.91
[32m[20221214 14:26:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 785.35
[32m[20221214 14:26:07 @agent_ppo2.py:143][0m Total time:      28.08 min
[32m[20221214 14:26:07 @agent_ppo2.py:145][0m 2576384 total steps have happened
[32m[20221214 14:26:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1258 --------------------------#
[32m[20221214 14:26:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:07 @agent_ppo2.py:185][0m |          -0.0000 |         219.0490 |         -88.6574 |
[32m[20221214 14:26:07 @agent_ppo2.py:185][0m |          -0.0002 |         204.3361 |         -88.6756 |
[32m[20221214 14:26:07 @agent_ppo2.py:185][0m |          -0.0026 |         199.1625 |         -88.5945 |
[32m[20221214 14:26:07 @agent_ppo2.py:185][0m |           0.0006 |         197.9678 |         -88.6634 |
[32m[20221214 14:26:07 @agent_ppo2.py:185][0m |          -0.0036 |         197.7495 |         -88.6720 |
[32m[20221214 14:26:08 @agent_ppo2.py:185][0m |          -0.0022 |         196.3772 |         -88.6709 |
[32m[20221214 14:26:08 @agent_ppo2.py:185][0m |           0.0064 |         207.6602 |         -88.7852 |
[32m[20221214 14:26:08 @agent_ppo2.py:185][0m |          -0.0015 |         196.2760 |         -88.7887 |
[32m[20221214 14:26:08 @agent_ppo2.py:185][0m |          -0.0029 |         194.2589 |         -88.6744 |
[32m[20221214 14:26:08 @agent_ppo2.py:185][0m |          -0.0026 |         194.1155 |         -88.8172 |
[32m[20221214 14:26:08 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:26:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.82
[32m[20221214 14:26:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.61
[32m[20221214 14:26:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 789.04
[32m[20221214 14:26:08 @agent_ppo2.py:143][0m Total time:      28.10 min
[32m[20221214 14:26:08 @agent_ppo2.py:145][0m 2578432 total steps have happened
[32m[20221214 14:26:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1259 --------------------------#
[32m[20221214 14:26:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:08 @agent_ppo2.py:185][0m |           0.0004 |         220.0703 |         -89.2720 |
[32m[20221214 14:26:08 @agent_ppo2.py:185][0m |          -0.0030 |         211.5468 |         -89.4491 |
[32m[20221214 14:26:09 @agent_ppo2.py:185][0m |          -0.0004 |         208.4729 |         -89.1114 |
[32m[20221214 14:26:09 @agent_ppo2.py:185][0m |          -0.0047 |         207.2136 |         -89.5935 |
[32m[20221214 14:26:09 @agent_ppo2.py:185][0m |          -0.0011 |         204.7791 |         -89.4192 |
[32m[20221214 14:26:09 @agent_ppo2.py:185][0m |          -0.0023 |         204.4344 |         -89.5273 |
[32m[20221214 14:26:09 @agent_ppo2.py:185][0m |           0.0112 |         213.8349 |         -89.4202 |
[32m[20221214 14:26:09 @agent_ppo2.py:185][0m |           0.0033 |         205.9040 |         -89.2842 |
[32m[20221214 14:26:09 @agent_ppo2.py:185][0m |          -0.0039 |         201.5341 |         -89.2795 |
[32m[20221214 14:26:09 @agent_ppo2.py:185][0m |          -0.0026 |         200.6321 |         -89.6024 |
[32m[20221214 14:26:09 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:26:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.32
[32m[20221214 14:26:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.93
[32m[20221214 14:26:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.53
[32m[20221214 14:26:09 @agent_ppo2.py:143][0m Total time:      28.12 min
[32m[20221214 14:26:09 @agent_ppo2.py:145][0m 2580480 total steps have happened
[32m[20221214 14:26:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1260 --------------------------#
[32m[20221214 14:26:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:26:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:10 @agent_ppo2.py:185][0m |           0.0038 |         207.1041 |         -91.9252 |
[32m[20221214 14:26:10 @agent_ppo2.py:185][0m |          -0.0006 |         196.0524 |         -91.9231 |
[32m[20221214 14:26:10 @agent_ppo2.py:185][0m |           0.0010 |         195.4554 |         -92.0248 |
[32m[20221214 14:26:10 @agent_ppo2.py:185][0m |           0.0164 |         227.7697 |         -91.7637 |
[32m[20221214 14:26:10 @agent_ppo2.py:185][0m |          -0.0024 |         196.2145 |         -91.9481 |
[32m[20221214 14:26:10 @agent_ppo2.py:185][0m |          -0.0015 |         193.8090 |         -92.3938 |
[32m[20221214 14:26:10 @agent_ppo2.py:185][0m |           0.0044 |         198.8082 |         -92.1786 |
[32m[20221214 14:26:10 @agent_ppo2.py:185][0m |           0.0002 |         193.2484 |         -92.2793 |
[32m[20221214 14:26:10 @agent_ppo2.py:185][0m |           0.0060 |         200.5484 |         -92.3229 |
[32m[20221214 14:26:10 @agent_ppo2.py:185][0m |          -0.0018 |         191.6363 |         -92.3976 |
[32m[20221214 14:26:10 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:26:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.94
[32m[20221214 14:26:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.21
[32m[20221214 14:26:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 784.94
[32m[20221214 14:26:11 @agent_ppo2.py:143][0m Total time:      28.14 min
[32m[20221214 14:26:11 @agent_ppo2.py:145][0m 2582528 total steps have happened
[32m[20221214 14:26:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1261 --------------------------#
[32m[20221214 14:26:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:11 @agent_ppo2.py:185][0m |           0.0045 |         250.4185 |         -89.7098 |
[32m[20221214 14:26:11 @agent_ppo2.py:185][0m |           0.0041 |         244.2206 |         -89.6686 |
[32m[20221214 14:26:11 @agent_ppo2.py:185][0m |           0.0009 |         239.3765 |         -89.9006 |
[32m[20221214 14:26:11 @agent_ppo2.py:185][0m |          -0.0011 |         238.6344 |         -89.7395 |
[32m[20221214 14:26:11 @agent_ppo2.py:185][0m |          -0.0037 |         236.7221 |         -90.0984 |
[32m[20221214 14:26:11 @agent_ppo2.py:185][0m |          -0.0012 |         235.4738 |         -89.8816 |
[32m[20221214 14:26:11 @agent_ppo2.py:185][0m |           0.0008 |         235.8317 |         -90.1513 |
[32m[20221214 14:26:11 @agent_ppo2.py:185][0m |          -0.0019 |         234.0325 |         -90.1281 |
[32m[20221214 14:26:11 @agent_ppo2.py:185][0m |          -0.0011 |         233.6248 |         -90.1730 |
[32m[20221214 14:26:12 @agent_ppo2.py:185][0m |          -0.0019 |         232.5981 |         -90.0777 |
[32m[20221214 14:26:12 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:26:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.44
[32m[20221214 14:26:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.74
[32m[20221214 14:26:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 786.33
[32m[20221214 14:26:12 @agent_ppo2.py:143][0m Total time:      28.16 min
[32m[20221214 14:26:12 @agent_ppo2.py:145][0m 2584576 total steps have happened
[32m[20221214 14:26:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1262 --------------------------#
[32m[20221214 14:26:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:12 @agent_ppo2.py:185][0m |          -0.0013 |         214.5847 |         -92.0603 |
[32m[20221214 14:26:12 @agent_ppo2.py:185][0m |          -0.0022 |         206.9810 |         -92.2491 |
[32m[20221214 14:26:12 @agent_ppo2.py:185][0m |          -0.0022 |         203.7964 |         -92.1632 |
[32m[20221214 14:26:12 @agent_ppo2.py:185][0m |          -0.0015 |         202.2566 |         -92.3903 |
[32m[20221214 14:26:12 @agent_ppo2.py:185][0m |          -0.0008 |         202.5413 |         -92.3359 |
[32m[20221214 14:26:12 @agent_ppo2.py:185][0m |          -0.0022 |         203.4253 |         -92.2425 |
[32m[20221214 14:26:13 @agent_ppo2.py:185][0m |          -0.0021 |         200.8680 |         -92.3400 |
[32m[20221214 14:26:13 @agent_ppo2.py:185][0m |          -0.0010 |         201.8672 |         -92.2249 |
[32m[20221214 14:26:13 @agent_ppo2.py:185][0m |          -0.0027 |         200.1688 |         -92.4679 |
[32m[20221214 14:26:13 @agent_ppo2.py:185][0m |          -0.0039 |         202.2516 |         -92.3404 |
[32m[20221214 14:26:13 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:26:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.84
[32m[20221214 14:26:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.86
[32m[20221214 14:26:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 786.15
[32m[20221214 14:26:13 @agent_ppo2.py:143][0m Total time:      28.18 min
[32m[20221214 14:26:13 @agent_ppo2.py:145][0m 2586624 total steps have happened
[32m[20221214 14:26:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1263 --------------------------#
[32m[20221214 14:26:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:13 @agent_ppo2.py:185][0m |          -0.0020 |         229.8392 |         -90.8371 |
[32m[20221214 14:26:13 @agent_ppo2.py:185][0m |          -0.0029 |         226.9314 |         -90.9914 |
[32m[20221214 14:26:13 @agent_ppo2.py:185][0m |          -0.0026 |         223.3637 |         -90.9631 |
[32m[20221214 14:26:13 @agent_ppo2.py:185][0m |          -0.0012 |         222.5409 |         -90.7225 |
[32m[20221214 14:26:14 @agent_ppo2.py:185][0m |          -0.0038 |         221.3025 |         -90.9668 |
[32m[20221214 14:26:14 @agent_ppo2.py:185][0m |          -0.0050 |         220.8960 |         -90.9311 |
[32m[20221214 14:26:14 @agent_ppo2.py:185][0m |          -0.0000 |         219.7221 |         -90.9159 |
[32m[20221214 14:26:14 @agent_ppo2.py:185][0m |          -0.0011 |         218.8435 |         -90.9386 |
[32m[20221214 14:26:14 @agent_ppo2.py:185][0m |          -0.0032 |         219.2381 |         -90.9709 |
[32m[20221214 14:26:14 @agent_ppo2.py:185][0m |          -0.0030 |         218.9082 |         -91.0382 |
[32m[20221214 14:26:14 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:26:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 784.68
[32m[20221214 14:26:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.28
[32m[20221214 14:26:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 781.21
[32m[20221214 14:26:14 @agent_ppo2.py:143][0m Total time:      28.20 min
[32m[20221214 14:26:14 @agent_ppo2.py:145][0m 2588672 total steps have happened
[32m[20221214 14:26:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1264 --------------------------#
[32m[20221214 14:26:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:14 @agent_ppo2.py:185][0m |          -0.0000 |         234.7922 |         -92.5097 |
[32m[20221214 14:26:15 @agent_ppo2.py:185][0m |          -0.0017 |         229.1737 |         -92.3967 |
[32m[20221214 14:26:15 @agent_ppo2.py:185][0m |          -0.0051 |         225.6578 |         -92.7313 |
[32m[20221214 14:26:15 @agent_ppo2.py:185][0m |          -0.0022 |         223.2715 |         -92.8046 |
[32m[20221214 14:26:15 @agent_ppo2.py:185][0m |           0.0169 |         241.4363 |         -92.5048 |
[32m[20221214 14:26:15 @agent_ppo2.py:185][0m |          -0.0022 |         220.6528 |         -92.9244 |
[32m[20221214 14:26:15 @agent_ppo2.py:185][0m |          -0.0005 |         218.8575 |         -92.8207 |
[32m[20221214 14:26:15 @agent_ppo2.py:185][0m |          -0.0032 |         217.3886 |         -92.9441 |
[32m[20221214 14:26:15 @agent_ppo2.py:185][0m |          -0.0031 |         216.3039 |         -93.1515 |
[32m[20221214 14:26:15 @agent_ppo2.py:185][0m |          -0.0002 |         215.0543 |         -93.1818 |
[32m[20221214 14:26:15 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:26:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 772.88
[32m[20221214 14:26:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 777.38
[32m[20221214 14:26:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 771.14
[32m[20221214 14:26:15 @agent_ppo2.py:143][0m Total time:      28.22 min
[32m[20221214 14:26:15 @agent_ppo2.py:145][0m 2590720 total steps have happened
[32m[20221214 14:26:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1265 --------------------------#
[32m[20221214 14:26:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:16 @agent_ppo2.py:185][0m |           0.0080 |         241.1620 |         -92.1600 |
[32m[20221214 14:26:16 @agent_ppo2.py:185][0m |          -0.0052 |         212.9204 |         -91.7446 |
[32m[20221214 14:26:16 @agent_ppo2.py:185][0m |           0.0089 |         238.3543 |         -91.8639 |
[32m[20221214 14:26:16 @agent_ppo2.py:185][0m |          -0.0023 |         209.9626 |         -91.0414 |
[32m[20221214 14:26:16 @agent_ppo2.py:185][0m |          -0.0025 |         207.6536 |         -91.6783 |
[32m[20221214 14:26:16 @agent_ppo2.py:185][0m |           0.0244 |         252.3264 |         -91.4732 |
[32m[20221214 14:26:16 @agent_ppo2.py:185][0m |          -0.0009 |         207.0291 |         -91.1736 |
[32m[20221214 14:26:16 @agent_ppo2.py:185][0m |          -0.0023 |         203.4487 |         -90.9702 |
[32m[20221214 14:26:16 @agent_ppo2.py:185][0m |          -0.0041 |         202.1806 |         -91.1859 |
[32m[20221214 14:26:16 @agent_ppo2.py:185][0m |          -0.0037 |         201.8305 |         -90.9061 |
[32m[20221214 14:26:16 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:26:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.91
[32m[20221214 14:26:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.33
[32m[20221214 14:26:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 771.04
[32m[20221214 14:26:17 @agent_ppo2.py:143][0m Total time:      28.24 min
[32m[20221214 14:26:17 @agent_ppo2.py:145][0m 2592768 total steps have happened
[32m[20221214 14:26:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1266 --------------------------#
[32m[20221214 14:26:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:17 @agent_ppo2.py:185][0m |          -0.0019 |         219.0634 |         -91.8803 |
[32m[20221214 14:26:17 @agent_ppo2.py:185][0m |          -0.0022 |         211.2298 |         -92.0827 |
[32m[20221214 14:26:17 @agent_ppo2.py:185][0m |          -0.0004 |         210.9731 |         -91.7897 |
[32m[20221214 14:26:17 @agent_ppo2.py:185][0m |           0.0031 |         212.5746 |         -91.7193 |
[32m[20221214 14:26:17 @agent_ppo2.py:185][0m |          -0.0027 |         207.2625 |         -91.7674 |
[32m[20221214 14:26:17 @agent_ppo2.py:185][0m |           0.0066 |         221.0505 |         -91.7265 |
[32m[20221214 14:26:17 @agent_ppo2.py:185][0m |          -0.0022 |         205.6084 |         -91.7235 |
[32m[20221214 14:26:18 @agent_ppo2.py:185][0m |          -0.0050 |         204.4083 |         -91.7294 |
[32m[20221214 14:26:18 @agent_ppo2.py:185][0m |           0.0163 |         223.6788 |         -91.5481 |
[32m[20221214 14:26:18 @agent_ppo2.py:185][0m |          -0.0010 |         205.5263 |         -91.4519 |
[32m[20221214 14:26:18 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:26:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.32
[32m[20221214 14:26:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 772.01
[32m[20221214 14:26:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 765.07
[32m[20221214 14:26:18 @agent_ppo2.py:143][0m Total time:      28.26 min
[32m[20221214 14:26:18 @agent_ppo2.py:145][0m 2594816 total steps have happened
[32m[20221214 14:26:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1267 --------------------------#
[32m[20221214 14:26:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:18 @agent_ppo2.py:185][0m |          -0.0003 |         229.4817 |         -90.8546 |
[32m[20221214 14:26:18 @agent_ppo2.py:185][0m |          -0.0018 |         225.7282 |         -90.5650 |
[32m[20221214 14:26:18 @agent_ppo2.py:185][0m |           0.0150 |         246.1474 |         -90.7308 |
[32m[20221214 14:26:18 @agent_ppo2.py:185][0m |           0.0058 |         226.5218 |         -91.0373 |
[32m[20221214 14:26:18 @agent_ppo2.py:185][0m |          -0.0012 |         225.1290 |         -90.7350 |
[32m[20221214 14:26:19 @agent_ppo2.py:185][0m |          -0.0001 |         223.3751 |         -90.8734 |
[32m[20221214 14:26:19 @agent_ppo2.py:185][0m |          -0.0017 |         223.3453 |         -90.8328 |
[32m[20221214 14:26:19 @agent_ppo2.py:185][0m |           0.0005 |         224.0109 |         -90.5551 |
[32m[20221214 14:26:19 @agent_ppo2.py:185][0m |          -0.0010 |         222.3839 |         -90.6135 |
[32m[20221214 14:26:19 @agent_ppo2.py:185][0m |          -0.0002 |         222.4655 |         -90.7058 |
[32m[20221214 14:26:19 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:26:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 761.06
[32m[20221214 14:26:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.70
[32m[20221214 14:26:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 763.93
[32m[20221214 14:26:19 @agent_ppo2.py:143][0m Total time:      28.28 min
[32m[20221214 14:26:19 @agent_ppo2.py:145][0m 2596864 total steps have happened
[32m[20221214 14:26:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1268 --------------------------#
[32m[20221214 14:26:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:19 @agent_ppo2.py:185][0m |          -0.0032 |         230.8040 |         -89.0840 |
[32m[20221214 14:26:19 @agent_ppo2.py:185][0m |           0.0010 |         223.9561 |         -89.3473 |
[32m[20221214 14:26:20 @agent_ppo2.py:185][0m |          -0.0011 |         222.9553 |         -89.4814 |
[32m[20221214 14:26:20 @agent_ppo2.py:185][0m |          -0.0018 |         222.3148 |         -89.5559 |
[32m[20221214 14:26:20 @agent_ppo2.py:185][0m |          -0.0011 |         223.6441 |         -89.5673 |
[32m[20221214 14:26:20 @agent_ppo2.py:185][0m |          -0.0026 |         222.2804 |         -89.6663 |
[32m[20221214 14:26:20 @agent_ppo2.py:185][0m |          -0.0002 |         219.8224 |         -89.6964 |
[32m[20221214 14:26:20 @agent_ppo2.py:185][0m |          -0.0026 |         219.7894 |         -89.6509 |
[32m[20221214 14:26:20 @agent_ppo2.py:185][0m |          -0.0057 |         220.1399 |         -89.7228 |
[32m[20221214 14:26:20 @agent_ppo2.py:185][0m |           0.0013 |         219.5692 |         -89.7260 |
[32m[20221214 14:26:20 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:26:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 755.77
[32m[20221214 14:26:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 760.51
[32m[20221214 14:26:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.82
[32m[20221214 14:26:20 @agent_ppo2.py:143][0m Total time:      28.30 min
[32m[20221214 14:26:20 @agent_ppo2.py:145][0m 2598912 total steps have happened
[32m[20221214 14:26:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1269 --------------------------#
[32m[20221214 14:26:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:21 @agent_ppo2.py:185][0m |          -0.0015 |         250.8404 |         -92.9362 |
[32m[20221214 14:26:21 @agent_ppo2.py:185][0m |          -0.0008 |         237.5766 |         -92.9578 |
[32m[20221214 14:26:21 @agent_ppo2.py:185][0m |          -0.0023 |         235.5631 |         -92.7637 |
[32m[20221214 14:26:21 @agent_ppo2.py:185][0m |          -0.0009 |         234.7447 |         -92.8193 |
[32m[20221214 14:26:21 @agent_ppo2.py:185][0m |          -0.0021 |         235.2078 |         -92.7629 |
[32m[20221214 14:26:21 @agent_ppo2.py:185][0m |           0.0009 |         235.0679 |         -92.6125 |
[32m[20221214 14:26:21 @agent_ppo2.py:185][0m |           0.0051 |         238.8041 |         -92.7591 |
[32m[20221214 14:26:21 @agent_ppo2.py:185][0m |           0.0032 |         238.0128 |         -92.7584 |
[32m[20221214 14:26:21 @agent_ppo2.py:185][0m |          -0.0033 |         234.5077 |         -92.5197 |
[32m[20221214 14:26:21 @agent_ppo2.py:185][0m |          -0.0041 |         233.1304 |         -92.6064 |
[32m[20221214 14:26:21 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:26:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 752.09
[32m[20221214 14:26:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 755.47
[32m[20221214 14:26:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.59
[32m[20221214 14:26:22 @agent_ppo2.py:143][0m Total time:      28.32 min
[32m[20221214 14:26:22 @agent_ppo2.py:145][0m 2600960 total steps have happened
[32m[20221214 14:26:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1270 --------------------------#
[32m[20221214 14:26:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:26:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:22 @agent_ppo2.py:185][0m |          -0.0018 |         232.9719 |         -90.9592 |
[32m[20221214 14:26:22 @agent_ppo2.py:185][0m |           0.0000 |         227.7592 |         -90.8839 |
[32m[20221214 14:26:22 @agent_ppo2.py:185][0m |           0.0017 |         226.1602 |         -90.9830 |
[32m[20221214 14:26:22 @agent_ppo2.py:185][0m |          -0.0022 |         223.6376 |         -91.1905 |
[32m[20221214 14:26:22 @agent_ppo2.py:185][0m |          -0.0021 |         222.4775 |         -91.0590 |
[32m[20221214 14:26:22 @agent_ppo2.py:185][0m |           0.0060 |         233.6369 |         -91.2348 |
[32m[20221214 14:26:22 @agent_ppo2.py:185][0m |           0.0327 |         262.2432 |         -91.3690 |
[32m[20221214 14:26:22 @agent_ppo2.py:185][0m |           0.0017 |         224.1764 |         -91.4483 |
[32m[20221214 14:26:23 @agent_ppo2.py:185][0m |           0.0003 |         220.4784 |         -91.5695 |
[32m[20221214 14:26:23 @agent_ppo2.py:185][0m |          -0.0046 |         219.3071 |         -91.5547 |
[32m[20221214 14:26:23 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:26:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 751.82
[32m[20221214 14:26:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 757.53
[32m[20221214 14:26:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 750.29
[32m[20221214 14:26:23 @agent_ppo2.py:143][0m Total time:      28.34 min
[32m[20221214 14:26:23 @agent_ppo2.py:145][0m 2603008 total steps have happened
[32m[20221214 14:26:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1271 --------------------------#
[32m[20221214 14:26:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:23 @agent_ppo2.py:185][0m |          -0.0049 |         209.7847 |         -93.5529 |
[32m[20221214 14:26:23 @agent_ppo2.py:185][0m |          -0.0012 |         200.1658 |         -93.4777 |
[32m[20221214 14:26:23 @agent_ppo2.py:185][0m |          -0.0017 |         194.6721 |         -93.5716 |
[32m[20221214 14:26:23 @agent_ppo2.py:185][0m |           0.0087 |         197.8521 |         -93.6167 |
[32m[20221214 14:26:23 @agent_ppo2.py:185][0m |          -0.0016 |         187.9634 |         -93.5894 |
[32m[20221214 14:26:23 @agent_ppo2.py:185][0m |          -0.0010 |         184.4564 |         -93.4482 |
[32m[20221214 14:26:24 @agent_ppo2.py:185][0m |          -0.0026 |         179.5035 |         -93.5463 |
[32m[20221214 14:26:24 @agent_ppo2.py:185][0m |          -0.0046 |         178.0333 |         -93.5320 |
[32m[20221214 14:26:24 @agent_ppo2.py:185][0m |          -0.0008 |         175.6863 |         -93.6109 |
[32m[20221214 14:26:24 @agent_ppo2.py:185][0m |          -0.0027 |         175.1870 |         -93.4854 |
[32m[20221214 14:26:24 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:26:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 751.05
[32m[20221214 14:26:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.81
[32m[20221214 14:26:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 747.53
[32m[20221214 14:26:24 @agent_ppo2.py:143][0m Total time:      28.36 min
[32m[20221214 14:26:24 @agent_ppo2.py:145][0m 2605056 total steps have happened
[32m[20221214 14:26:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1272 --------------------------#
[32m[20221214 14:26:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:24 @agent_ppo2.py:185][0m |          -0.0017 |         213.1058 |         -92.7037 |
[32m[20221214 14:26:24 @agent_ppo2.py:185][0m |          -0.0041 |         210.2240 |         -92.6087 |
[32m[20221214 14:26:24 @agent_ppo2.py:185][0m |          -0.0016 |         208.0550 |         -92.7436 |
[32m[20221214 14:26:25 @agent_ppo2.py:185][0m |          -0.0035 |         207.3795 |         -92.5854 |
[32m[20221214 14:26:25 @agent_ppo2.py:185][0m |          -0.0010 |         207.4674 |         -92.8196 |
[32m[20221214 14:26:25 @agent_ppo2.py:185][0m |          -0.0055 |         207.2529 |         -92.6444 |
[32m[20221214 14:26:25 @agent_ppo2.py:185][0m |          -0.0038 |         206.1702 |         -92.6929 |
[32m[20221214 14:26:25 @agent_ppo2.py:185][0m |          -0.0005 |         205.7231 |         -92.4544 |
[32m[20221214 14:26:25 @agent_ppo2.py:185][0m |          -0.0026 |         205.4331 |         -92.3844 |
[32m[20221214 14:26:25 @agent_ppo2.py:185][0m |          -0.0019 |         204.8750 |         -92.5881 |
[32m[20221214 14:26:25 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:26:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 744.93
[32m[20221214 14:26:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 748.20
[32m[20221214 14:26:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.13
[32m[20221214 14:26:25 @agent_ppo2.py:143][0m Total time:      28.38 min
[32m[20221214 14:26:25 @agent_ppo2.py:145][0m 2607104 total steps have happened
[32m[20221214 14:26:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1273 --------------------------#
[32m[20221214 14:26:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:25 @agent_ppo2.py:185][0m |          -0.0030 |         192.3613 |         -92.0019 |
[32m[20221214 14:26:26 @agent_ppo2.py:185][0m |          -0.0032 |         188.3744 |         -92.2931 |
[32m[20221214 14:26:26 @agent_ppo2.py:185][0m |          -0.0048 |         186.6776 |         -92.0094 |
[32m[20221214 14:26:26 @agent_ppo2.py:185][0m |          -0.0052 |         185.3782 |         -92.1054 |
[32m[20221214 14:26:26 @agent_ppo2.py:185][0m |          -0.0029 |         185.0272 |         -92.1277 |
[32m[20221214 14:26:26 @agent_ppo2.py:185][0m |           0.0026 |         190.9781 |         -91.9457 |
[32m[20221214 14:26:26 @agent_ppo2.py:185][0m |          -0.0050 |         183.1862 |         -91.9381 |
[32m[20221214 14:26:26 @agent_ppo2.py:185][0m |          -0.0050 |         182.0658 |         -91.7990 |
[32m[20221214 14:26:26 @agent_ppo2.py:185][0m |          -0.0043 |         180.5761 |         -91.8259 |
[32m[20221214 14:26:26 @agent_ppo2.py:185][0m |          -0.0014 |         182.7492 |         -92.0586 |
[32m[20221214 14:26:26 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:26:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 730.41
[32m[20221214 14:26:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 744.87
[32m[20221214 14:26:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 731.58
[32m[20221214 14:26:26 @agent_ppo2.py:143][0m Total time:      28.41 min
[32m[20221214 14:26:26 @agent_ppo2.py:145][0m 2609152 total steps have happened
[32m[20221214 14:26:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1274 --------------------------#
[32m[20221214 14:26:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:27 @agent_ppo2.py:185][0m |          -0.0005 |         215.6331 |         -90.7313 |
[32m[20221214 14:26:27 @agent_ppo2.py:185][0m |          -0.0010 |         199.4000 |         -91.0370 |
[32m[20221214 14:26:27 @agent_ppo2.py:185][0m |           0.0031 |         195.8906 |         -91.0559 |
[32m[20221214 14:26:27 @agent_ppo2.py:185][0m |          -0.0009 |         189.2343 |         -90.9312 |
[32m[20221214 14:26:27 @agent_ppo2.py:185][0m |          -0.0014 |         188.1744 |         -90.8264 |
[32m[20221214 14:26:27 @agent_ppo2.py:185][0m |          -0.0006 |         184.6645 |         -90.9008 |
[32m[20221214 14:26:27 @agent_ppo2.py:185][0m |          -0.0016 |         183.7432 |         -90.7123 |
[32m[20221214 14:26:27 @agent_ppo2.py:185][0m |          -0.0033 |         180.8917 |         -90.8997 |
[32m[20221214 14:26:27 @agent_ppo2.py:185][0m |          -0.0026 |         180.2410 |         -90.6302 |
[32m[20221214 14:26:27 @agent_ppo2.py:185][0m |           0.0000 |         178.5090 |         -90.4704 |
[32m[20221214 14:26:27 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:26:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 711.78
[32m[20221214 14:26:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 727.98
[32m[20221214 14:26:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 733.12
[32m[20221214 14:26:28 @agent_ppo2.py:143][0m Total time:      28.43 min
[32m[20221214 14:26:28 @agent_ppo2.py:145][0m 2611200 total steps have happened
[32m[20221214 14:26:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1275 --------------------------#
[32m[20221214 14:26:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:28 @agent_ppo2.py:185][0m |           0.0044 |         232.2198 |         -90.3794 |
[32m[20221214 14:26:28 @agent_ppo2.py:185][0m |           0.0006 |         220.2248 |         -90.6648 |
[32m[20221214 14:26:28 @agent_ppo2.py:185][0m |          -0.0008 |         216.2802 |         -90.9553 |
[32m[20221214 14:26:28 @agent_ppo2.py:185][0m |           0.0020 |         214.1299 |         -91.1589 |
[32m[20221214 14:26:28 @agent_ppo2.py:185][0m |           0.0013 |         212.5777 |         -91.4422 |
[32m[20221214 14:26:28 @agent_ppo2.py:185][0m |          -0.0019 |         211.7711 |         -91.4922 |
[32m[20221214 14:26:28 @agent_ppo2.py:185][0m |           0.0004 |         211.6816 |         -91.8335 |
[32m[20221214 14:26:29 @agent_ppo2.py:185][0m |          -0.0001 |         211.3319 |         -91.9506 |
[32m[20221214 14:26:29 @agent_ppo2.py:185][0m |           0.0017 |         208.8277 |         -92.0331 |
[32m[20221214 14:26:29 @agent_ppo2.py:185][0m |          -0.0027 |         209.0882 |         -92.3337 |
[32m[20221214 14:26:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:26:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 736.04
[32m[20221214 14:26:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.79
[32m[20221214 14:26:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 737.10
[32m[20221214 14:26:29 @agent_ppo2.py:143][0m Total time:      28.45 min
[32m[20221214 14:26:29 @agent_ppo2.py:145][0m 2613248 total steps have happened
[32m[20221214 14:26:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1276 --------------------------#
[32m[20221214 14:26:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:26:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:29 @agent_ppo2.py:185][0m |          -0.0021 |         203.8060 |         -92.9751 |
[32m[20221214 14:26:29 @agent_ppo2.py:185][0m |           0.0005 |         196.7931 |         -93.0474 |
[32m[20221214 14:26:29 @agent_ppo2.py:185][0m |           0.0003 |         192.7592 |         -93.1647 |
[32m[20221214 14:26:29 @agent_ppo2.py:185][0m |          -0.0027 |         188.6926 |         -93.0047 |
[32m[20221214 14:26:30 @agent_ppo2.py:185][0m |           0.0010 |         186.2186 |         -92.7363 |
[32m[20221214 14:26:30 @agent_ppo2.py:185][0m |          -0.0036 |         183.5297 |         -92.8759 |
[32m[20221214 14:26:30 @agent_ppo2.py:185][0m |           0.0031 |         182.4024 |         -93.0024 |
[32m[20221214 14:26:30 @agent_ppo2.py:185][0m |          -0.0029 |         180.3373 |         -92.9277 |
[32m[20221214 14:26:30 @agent_ppo2.py:185][0m |           0.0080 |         194.9061 |         -92.9710 |
[32m[20221214 14:26:30 @agent_ppo2.py:185][0m |          -0.0009 |         179.2154 |         -92.8128 |
[32m[20221214 14:26:30 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:26:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 726.35
[32m[20221214 14:26:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 740.69
[32m[20221214 14:26:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.01
[32m[20221214 14:26:30 @agent_ppo2.py:143][0m Total time:      28.47 min
[32m[20221214 14:26:30 @agent_ppo2.py:145][0m 2615296 total steps have happened
[32m[20221214 14:26:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1277 --------------------------#
[32m[20221214 14:26:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:30 @agent_ppo2.py:185][0m |           0.0125 |         195.2002 |         -92.2461 |
[32m[20221214 14:26:31 @agent_ppo2.py:185][0m |          -0.0031 |         160.9485 |         -92.2685 |
[32m[20221214 14:26:31 @agent_ppo2.py:185][0m |          -0.0034 |         153.3741 |         -92.3581 |
[32m[20221214 14:26:31 @agent_ppo2.py:185][0m |           0.0160 |         178.9260 |         -92.3674 |
[32m[20221214 14:26:31 @agent_ppo2.py:185][0m |           0.0017 |         146.2302 |         -92.3042 |
[32m[20221214 14:26:31 @agent_ppo2.py:185][0m |          -0.0102 |         143.2982 |         -92.4594 |
[32m[20221214 14:26:31 @agent_ppo2.py:185][0m |          -0.0016 |         141.4654 |         -92.3177 |
[32m[20221214 14:26:31 @agent_ppo2.py:185][0m |          -0.0029 |         140.6913 |         -92.5507 |
[32m[20221214 14:26:31 @agent_ppo2.py:185][0m |          -0.0065 |         139.1093 |         -92.5059 |
[32m[20221214 14:26:31 @agent_ppo2.py:185][0m |          -0.0054 |         138.0332 |         -92.5649 |
[32m[20221214 14:26:31 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:26:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.33
[32m[20221214 14:26:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 727.09
[32m[20221214 14:26:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.57
[32m[20221214 14:26:31 @agent_ppo2.py:143][0m Total time:      28.49 min
[32m[20221214 14:26:31 @agent_ppo2.py:145][0m 2617344 total steps have happened
[32m[20221214 14:26:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1278 --------------------------#
[32m[20221214 14:26:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:32 @agent_ppo2.py:185][0m |           0.0017 |         131.8581 |         -94.3440 |
[32m[20221214 14:26:32 @agent_ppo2.py:185][0m |           0.0012 |         111.9512 |         -94.2289 |
[32m[20221214 14:26:32 @agent_ppo2.py:185][0m |           0.0110 |         118.2450 |         -94.1601 |
[32m[20221214 14:26:32 @agent_ppo2.py:185][0m |          -0.0067 |         103.1439 |         -94.1596 |
[32m[20221214 14:26:32 @agent_ppo2.py:185][0m |          -0.0021 |         101.1522 |         -94.0473 |
[32m[20221214 14:26:32 @agent_ppo2.py:185][0m |           0.0008 |          99.2118 |         -94.0182 |
[32m[20221214 14:26:32 @agent_ppo2.py:185][0m |           0.0071 |         107.6156 |         -93.8243 |
[32m[20221214 14:26:32 @agent_ppo2.py:185][0m |          -0.0065 |          97.8035 |         -93.7755 |
[32m[20221214 14:26:32 @agent_ppo2.py:185][0m |          -0.0085 |          96.4753 |         -93.7610 |
[32m[20221214 14:26:32 @agent_ppo2.py:185][0m |          -0.0084 |          95.4983 |         -93.8673 |
[32m[20221214 14:26:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:26:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.50
[32m[20221214 14:26:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 686.54
[32m[20221214 14:26:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 322.14
[32m[20221214 14:26:33 @agent_ppo2.py:143][0m Total time:      28.51 min
[32m[20221214 14:26:33 @agent_ppo2.py:145][0m 2619392 total steps have happened
[32m[20221214 14:26:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1279 --------------------------#
[32m[20221214 14:26:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:33 @agent_ppo2.py:185][0m |           0.0008 |          89.8273 |         -94.9803 |
[32m[20221214 14:26:33 @agent_ppo2.py:185][0m |           0.0060 |          81.4725 |         -95.3819 |
[32m[20221214 14:26:33 @agent_ppo2.py:185][0m |          -0.0017 |          76.4497 |         -95.1140 |
[32m[20221214 14:26:33 @agent_ppo2.py:185][0m |           0.0022 |          76.3351 |         -95.2233 |
[32m[20221214 14:26:33 @agent_ppo2.py:185][0m |           0.0109 |          81.2045 |         -94.9453 |
[32m[20221214 14:26:33 @agent_ppo2.py:185][0m |          -0.0038 |          73.1646 |         -95.0943 |
[32m[20221214 14:26:33 @agent_ppo2.py:185][0m |          -0.0064 |          72.1842 |         -95.1578 |
[32m[20221214 14:26:33 @agent_ppo2.py:185][0m |           0.0061 |          80.7615 |         -95.3156 |
[32m[20221214 14:26:34 @agent_ppo2.py:185][0m |          -0.0041 |          71.3780 |         -95.2117 |
[32m[20221214 14:26:34 @agent_ppo2.py:185][0m |          -0.0062 |          70.8233 |         -95.0921 |
[32m[20221214 14:26:34 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:26:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.81
[32m[20221214 14:26:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.52
[32m[20221214 14:26:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.96
[32m[20221214 14:26:34 @agent_ppo2.py:143][0m Total time:      28.53 min
[32m[20221214 14:26:34 @agent_ppo2.py:145][0m 2621440 total steps have happened
[32m[20221214 14:26:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1280 --------------------------#
[32m[20221214 14:26:34 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:26:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:34 @agent_ppo2.py:185][0m |           0.0003 |          66.8236 |         -93.7861 |
[32m[20221214 14:26:34 @agent_ppo2.py:185][0m |          -0.0014 |          59.6570 |         -93.7518 |
[32m[20221214 14:26:34 @agent_ppo2.py:185][0m |          -0.0029 |          58.7565 |         -93.8891 |
[32m[20221214 14:26:34 @agent_ppo2.py:185][0m |           0.0050 |          61.9450 |         -93.7235 |
[32m[20221214 14:26:34 @agent_ppo2.py:185][0m |          -0.0029 |          57.7396 |         -93.6976 |
[32m[20221214 14:26:35 @agent_ppo2.py:185][0m |          -0.0041 |          57.0481 |         -93.4248 |
[32m[20221214 14:26:35 @agent_ppo2.py:185][0m |          -0.0062 |          56.6761 |         -93.6577 |
[32m[20221214 14:26:35 @agent_ppo2.py:185][0m |           0.0028 |          58.1950 |         -93.5280 |
[32m[20221214 14:26:35 @agent_ppo2.py:185][0m |          -0.0017 |          56.4392 |         -93.6095 |
[32m[20221214 14:26:35 @agent_ppo2.py:185][0m |          -0.0032 |          55.7832 |         -93.4529 |
[32m[20221214 14:26:35 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:26:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 326.48
[32m[20221214 14:26:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.40
[32m[20221214 14:26:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.46
[32m[20221214 14:26:35 @agent_ppo2.py:143][0m Total time:      28.55 min
[32m[20221214 14:26:35 @agent_ppo2.py:145][0m 2623488 total steps have happened
[32m[20221214 14:26:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1281 --------------------------#
[32m[20221214 14:26:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:35 @agent_ppo2.py:185][0m |          -0.0029 |          57.1900 |         -93.7254 |
[32m[20221214 14:26:35 @agent_ppo2.py:185][0m |          -0.0041 |          50.6725 |         -93.5828 |
[32m[20221214 14:26:36 @agent_ppo2.py:185][0m |          -0.0073 |          49.6075 |         -93.4773 |
[32m[20221214 14:26:36 @agent_ppo2.py:185][0m |          -0.0041 |          48.9832 |         -93.3150 |
[32m[20221214 14:26:36 @agent_ppo2.py:185][0m |          -0.0043 |          48.3843 |         -93.2386 |
[32m[20221214 14:26:36 @agent_ppo2.py:185][0m |          -0.0010 |          48.7937 |         -93.2333 |
[32m[20221214 14:26:36 @agent_ppo2.py:185][0m |          -0.0027 |          47.6690 |         -93.0535 |
[32m[20221214 14:26:36 @agent_ppo2.py:185][0m |          -0.0049 |          47.1548 |         -92.8292 |
[32m[20221214 14:26:36 @agent_ppo2.py:185][0m |          -0.0050 |          46.9735 |         -93.0515 |
[32m[20221214 14:26:36 @agent_ppo2.py:185][0m |          -0.0069 |          46.7942 |         -92.6812 |
[32m[20221214 14:26:36 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:26:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.63
[32m[20221214 14:26:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.21
[32m[20221214 14:26:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.65
[32m[20221214 14:26:36 @agent_ppo2.py:143][0m Total time:      28.57 min
[32m[20221214 14:26:36 @agent_ppo2.py:145][0m 2625536 total steps have happened
[32m[20221214 14:26:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1282 --------------------------#
[32m[20221214 14:26:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:37 @agent_ppo2.py:185][0m |          -0.0039 |          48.1507 |         -93.3349 |
[32m[20221214 14:26:37 @agent_ppo2.py:185][0m |          -0.0024 |          43.1145 |         -92.9273 |
[32m[20221214 14:26:37 @agent_ppo2.py:185][0m |          -0.0036 |          41.7596 |         -93.0762 |
[32m[20221214 14:26:37 @agent_ppo2.py:185][0m |          -0.0037 |          41.1916 |         -92.9882 |
[32m[20221214 14:26:37 @agent_ppo2.py:185][0m |          -0.0050 |          40.7747 |         -92.9250 |
[32m[20221214 14:26:37 @agent_ppo2.py:185][0m |          -0.0082 |          40.4806 |         -92.9613 |
[32m[20221214 14:26:37 @agent_ppo2.py:185][0m |          -0.0044 |          40.0280 |         -92.9877 |
[32m[20221214 14:26:37 @agent_ppo2.py:185][0m |          -0.0013 |          40.1239 |         -92.8737 |
[32m[20221214 14:26:37 @agent_ppo2.py:185][0m |          -0.0043 |          39.5477 |         -92.8844 |
[32m[20221214 14:26:37 @agent_ppo2.py:185][0m |          -0.0102 |          39.4034 |         -92.8370 |
[32m[20221214 14:26:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:26:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 326.56
[32m[20221214 14:26:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.58
[32m[20221214 14:26:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.28
[32m[20221214 14:26:38 @agent_ppo2.py:143][0m Total time:      28.59 min
[32m[20221214 14:26:38 @agent_ppo2.py:145][0m 2627584 total steps have happened
[32m[20221214 14:26:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1283 --------------------------#
[32m[20221214 14:26:38 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:26:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:38 @agent_ppo2.py:185][0m |          -0.0021 |          38.4437 |         -92.2869 |
[32m[20221214 14:26:38 @agent_ppo2.py:185][0m |           0.0044 |          34.2524 |         -92.3427 |
[32m[20221214 14:26:38 @agent_ppo2.py:185][0m |          -0.0031 |          33.4826 |         -92.6574 |
[32m[20221214 14:26:38 @agent_ppo2.py:185][0m |           0.0069 |          35.3428 |         -92.5985 |
[32m[20221214 14:26:38 @agent_ppo2.py:185][0m |          -0.0005 |          32.7931 |         -92.6143 |
[32m[20221214 14:26:38 @agent_ppo2.py:185][0m |          -0.0010 |          32.5408 |         -92.3528 |
[32m[20221214 14:26:38 @agent_ppo2.py:185][0m |          -0.0002 |          32.3818 |         -92.6761 |
[32m[20221214 14:26:39 @agent_ppo2.py:185][0m |          -0.0009 |          32.0787 |         -92.6723 |
[32m[20221214 14:26:39 @agent_ppo2.py:185][0m |          -0.0021 |          31.9665 |         -92.7777 |
[32m[20221214 14:26:39 @agent_ppo2.py:185][0m |           0.0008 |          31.7627 |         -92.6987 |
[32m[20221214 14:26:39 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:26:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.58
[32m[20221214 14:26:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.05
[32m[20221214 14:26:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.65
[32m[20221214 14:26:39 @agent_ppo2.py:143][0m Total time:      28.61 min
[32m[20221214 14:26:39 @agent_ppo2.py:145][0m 2629632 total steps have happened
[32m[20221214 14:26:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1284 --------------------------#
[32m[20221214 14:26:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:39 @agent_ppo2.py:185][0m |          -0.0018 |          30.5382 |         -93.6227 |
[32m[20221214 14:26:39 @agent_ppo2.py:185][0m |           0.0013 |          27.1231 |         -93.5505 |
[32m[20221214 14:26:39 @agent_ppo2.py:185][0m |          -0.0019 |          26.3379 |         -93.5149 |
[32m[20221214 14:26:39 @agent_ppo2.py:185][0m |           0.0024 |          26.1829 |         -93.3025 |
[32m[20221214 14:26:40 @agent_ppo2.py:185][0m |          -0.0035 |          25.6249 |         -93.3051 |
[32m[20221214 14:26:40 @agent_ppo2.py:185][0m |          -0.0016 |          25.2218 |         -93.2668 |
[32m[20221214 14:26:40 @agent_ppo2.py:185][0m |          -0.0017 |          24.8891 |         -93.2336 |
[32m[20221214 14:26:40 @agent_ppo2.py:185][0m |          -0.0031 |          24.6367 |         -93.1824 |
[32m[20221214 14:26:40 @agent_ppo2.py:185][0m |          -0.0055 |          24.4622 |         -93.2947 |
[32m[20221214 14:26:40 @agent_ppo2.py:185][0m |          -0.0045 |          24.1892 |         -93.2370 |
[32m[20221214 14:26:40 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:26:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.06
[32m[20221214 14:26:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.62
[32m[20221214 14:26:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.75
[32m[20221214 14:26:40 @agent_ppo2.py:143][0m Total time:      28.63 min
[32m[20221214 14:26:40 @agent_ppo2.py:145][0m 2631680 total steps have happened
[32m[20221214 14:26:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1285 --------------------------#
[32m[20221214 14:26:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:26:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:40 @agent_ppo2.py:185][0m |           0.0143 |          71.6763 |         -94.4596 |
[32m[20221214 14:26:41 @agent_ppo2.py:185][0m |           0.0006 |          59.0040 |         -94.2327 |
[32m[20221214 14:26:41 @agent_ppo2.py:185][0m |           0.0020 |          56.0256 |         -94.2600 |
[32m[20221214 14:26:41 @agent_ppo2.py:185][0m |           0.0063 |          58.0324 |         -94.1768 |
[32m[20221214 14:26:41 @agent_ppo2.py:185][0m |           0.0039 |          56.2463 |         -94.4491 |
[32m[20221214 14:26:41 @agent_ppo2.py:185][0m |           0.0032 |          56.2745 |         -94.4060 |
[32m[20221214 14:26:41 @agent_ppo2.py:185][0m |          -0.0046 |          54.3578 |         -94.3898 |
[32m[20221214 14:26:41 @agent_ppo2.py:185][0m |          -0.0021 |          53.3045 |         -94.0008 |
[32m[20221214 14:26:41 @agent_ppo2.py:185][0m |          -0.0048 |          52.7642 |         -94.2345 |
[32m[20221214 14:26:41 @agent_ppo2.py:185][0m |          -0.0014 |          52.4577 |         -94.2271 |
[32m[20221214 14:26:41 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:26:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.09
[32m[20221214 14:26:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.55
[32m[20221214 14:26:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.84
[32m[20221214 14:26:41 @agent_ppo2.py:143][0m Total time:      28.66 min
[32m[20221214 14:26:41 @agent_ppo2.py:145][0m 2633728 total steps have happened
[32m[20221214 14:26:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1286 --------------------------#
[32m[20221214 14:26:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:42 @agent_ppo2.py:185][0m |          -0.0026 |          94.9896 |         -94.0152 |
[32m[20221214 14:26:42 @agent_ppo2.py:185][0m |          -0.0037 |          89.0115 |         -93.9803 |
[32m[20221214 14:26:42 @agent_ppo2.py:185][0m |          -0.0049 |          87.5188 |         -93.8060 |
[32m[20221214 14:26:42 @agent_ppo2.py:185][0m |           0.0005 |          87.2121 |         -93.4811 |
[32m[20221214 14:26:42 @agent_ppo2.py:185][0m |          -0.0038 |          84.9832 |         -93.4903 |
[32m[20221214 14:26:42 @agent_ppo2.py:185][0m |          -0.0035 |          84.2150 |         -93.4511 |
[32m[20221214 14:26:42 @agent_ppo2.py:185][0m |          -0.0059 |          83.3450 |         -93.2018 |
[32m[20221214 14:26:42 @agent_ppo2.py:185][0m |          -0.0033 |          83.4813 |         -93.3739 |
[32m[20221214 14:26:42 @agent_ppo2.py:185][0m |          -0.0036 |          83.0305 |         -93.1884 |
[32m[20221214 14:26:42 @agent_ppo2.py:185][0m |          -0.0033 |          82.6859 |         -93.1141 |
[32m[20221214 14:26:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:26:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 558.96
[32m[20221214 14:26:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 711.63
[32m[20221214 14:26:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 705.40
[32m[20221214 14:26:43 @agent_ppo2.py:143][0m Total time:      28.68 min
[32m[20221214 14:26:43 @agent_ppo2.py:145][0m 2635776 total steps have happened
[32m[20221214 14:26:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1287 --------------------------#
[32m[20221214 14:26:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:43 @agent_ppo2.py:185][0m |          -0.0017 |          61.2977 |         -90.6946 |
[32m[20221214 14:26:43 @agent_ppo2.py:185][0m |          -0.0051 |          55.7952 |         -90.7234 |
[32m[20221214 14:26:43 @agent_ppo2.py:185][0m |          -0.0037 |          53.2999 |         -90.6716 |
[32m[20221214 14:26:43 @agent_ppo2.py:185][0m |          -0.0051 |          52.1168 |         -90.5455 |
[32m[20221214 14:26:43 @agent_ppo2.py:185][0m |          -0.0079 |          51.6965 |         -90.6016 |
[32m[20221214 14:26:43 @agent_ppo2.py:185][0m |           0.0013 |          52.4199 |         -90.6448 |
[32m[20221214 14:26:43 @agent_ppo2.py:185][0m |          -0.0046 |          50.0689 |         -90.3342 |
[32m[20221214 14:26:44 @agent_ppo2.py:185][0m |          -0.0015 |          50.2966 |         -90.2776 |
[32m[20221214 14:26:44 @agent_ppo2.py:185][0m |           0.0184 |          73.8854 |         -90.3321 |
[32m[20221214 14:26:44 @agent_ppo2.py:185][0m |          -0.0031 |          51.2250 |         -89.8616 |
[32m[20221214 14:26:44 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:26:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 549.39
[32m[20221214 14:26:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 706.07
[32m[20221214 14:26:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 723.69
[32m[20221214 14:26:44 @agent_ppo2.py:143][0m Total time:      28.70 min
[32m[20221214 14:26:44 @agent_ppo2.py:145][0m 2637824 total steps have happened
[32m[20221214 14:26:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1288 --------------------------#
[32m[20221214 14:26:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:44 @agent_ppo2.py:185][0m |          -0.0011 |         139.1790 |         -92.3383 |
[32m[20221214 14:26:44 @agent_ppo2.py:185][0m |           0.0014 |         131.7386 |         -92.2375 |
[32m[20221214 14:26:44 @agent_ppo2.py:185][0m |           0.0037 |         132.7333 |         -92.0478 |
[32m[20221214 14:26:45 @agent_ppo2.py:185][0m |           0.0004 |         127.5846 |         -92.1788 |
[32m[20221214 14:26:45 @agent_ppo2.py:185][0m |           0.0011 |         127.4978 |         -92.2074 |
[32m[20221214 14:26:45 @agent_ppo2.py:185][0m |           0.0069 |         131.1126 |         -92.2095 |
[32m[20221214 14:26:45 @agent_ppo2.py:185][0m |          -0.0040 |         126.2724 |         -91.7650 |
[32m[20221214 14:26:45 @agent_ppo2.py:185][0m |           0.0065 |         127.6180 |         -91.9940 |
[32m[20221214 14:26:45 @agent_ppo2.py:185][0m |          -0.0011 |         126.8167 |         -92.0078 |
[32m[20221214 14:26:45 @agent_ppo2.py:185][0m |          -0.0020 |         124.5891 |         -91.8468 |
[32m[20221214 14:26:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:26:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 646.19
[32m[20221214 14:26:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 720.12
[32m[20221214 14:26:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 721.07
[32m[20221214 14:26:45 @agent_ppo2.py:143][0m Total time:      28.72 min
[32m[20221214 14:26:45 @agent_ppo2.py:145][0m 2639872 total steps have happened
[32m[20221214 14:26:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1289 --------------------------#
[32m[20221214 14:26:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:46 @agent_ppo2.py:185][0m |          -0.0014 |         125.0179 |         -89.4452 |
[32m[20221214 14:26:46 @agent_ppo2.py:185][0m |           0.0009 |         121.5710 |         -89.3906 |
[32m[20221214 14:26:46 @agent_ppo2.py:185][0m |           0.0002 |         119.5512 |         -89.1311 |
[32m[20221214 14:26:46 @agent_ppo2.py:185][0m |          -0.0042 |         116.8985 |         -89.3128 |
[32m[20221214 14:26:46 @agent_ppo2.py:185][0m |          -0.0037 |         115.6329 |         -89.2354 |
[32m[20221214 14:26:46 @agent_ppo2.py:185][0m |           0.0001 |         115.1449 |         -89.1482 |
[32m[20221214 14:26:46 @agent_ppo2.py:185][0m |          -0.0049 |         113.3050 |         -88.9708 |
[32m[20221214 14:26:46 @agent_ppo2.py:185][0m |          -0.0037 |         112.7263 |         -89.2434 |
[32m[20221214 14:26:46 @agent_ppo2.py:185][0m |          -0.0072 |         112.9296 |         -89.0656 |
[32m[20221214 14:26:46 @agent_ppo2.py:185][0m |          -0.0056 |         112.9610 |         -89.0962 |
[32m[20221214 14:26:46 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:26:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 595.24
[32m[20221214 14:26:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 722.55
[32m[20221214 14:26:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 717.39
[32m[20221214 14:26:46 @agent_ppo2.py:143][0m Total time:      28.74 min
[32m[20221214 14:26:46 @agent_ppo2.py:145][0m 2641920 total steps have happened
[32m[20221214 14:26:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1290 --------------------------#
[32m[20221214 14:26:47 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:26:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:47 @agent_ppo2.py:185][0m |           0.0001 |         167.7460 |         -90.8149 |
[32m[20221214 14:26:47 @agent_ppo2.py:185][0m |           0.0025 |         163.4363 |         -91.2468 |
[32m[20221214 14:26:47 @agent_ppo2.py:185][0m |          -0.0017 |         160.9466 |         -90.7789 |
[32m[20221214 14:26:47 @agent_ppo2.py:185][0m |           0.0000 |         160.0760 |         -91.0077 |
[32m[20221214 14:26:47 @agent_ppo2.py:185][0m |           0.0005 |         157.9557 |         -91.0857 |
[32m[20221214 14:26:47 @agent_ppo2.py:185][0m |           0.0080 |         162.1279 |         -91.1187 |
[32m[20221214 14:26:47 @agent_ppo2.py:185][0m |          -0.0001 |         156.4480 |         -91.3257 |
[32m[20221214 14:26:47 @agent_ppo2.py:185][0m |           0.0010 |         155.6503 |         -91.4854 |
[32m[20221214 14:26:47 @agent_ppo2.py:185][0m |          -0.0004 |         154.5435 |         -91.2982 |
[32m[20221214 14:26:48 @agent_ppo2.py:185][0m |           0.0101 |         169.7963 |         -91.6282 |
[32m[20221214 14:26:48 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:26:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.67
[32m[20221214 14:26:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.07
[32m[20221214 14:26:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 700.46
[32m[20221214 14:26:48 @agent_ppo2.py:143][0m Total time:      28.76 min
[32m[20221214 14:26:48 @agent_ppo2.py:145][0m 2643968 total steps have happened
[32m[20221214 14:26:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1291 --------------------------#
[32m[20221214 14:26:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:48 @agent_ppo2.py:185][0m |          -0.0020 |         173.4504 |         -91.4114 |
[32m[20221214 14:26:48 @agent_ppo2.py:185][0m |           0.0168 |         185.7515 |         -91.4355 |
[32m[20221214 14:26:48 @agent_ppo2.py:185][0m |           0.0119 |         177.7428 |         -91.4291 |
[32m[20221214 14:26:48 @agent_ppo2.py:185][0m |          -0.0023 |         162.6505 |         -91.2610 |
[32m[20221214 14:26:48 @agent_ppo2.py:185][0m |           0.0100 |         175.4694 |         -91.5617 |
[32m[20221214 14:26:48 @agent_ppo2.py:185][0m |          -0.0001 |         160.7496 |         -91.5206 |
[32m[20221214 14:26:49 @agent_ppo2.py:185][0m |          -0.0048 |         158.9046 |         -91.7775 |
[32m[20221214 14:26:49 @agent_ppo2.py:185][0m |          -0.0021 |         158.8365 |         -91.6238 |
[32m[20221214 14:26:49 @agent_ppo2.py:185][0m |           0.0074 |         168.8772 |         -91.7942 |
[32m[20221214 14:26:49 @agent_ppo2.py:185][0m |          -0.0050 |         156.2512 |         -91.9058 |
[32m[20221214 14:26:49 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:26:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 701.64
[32m[20221214 14:26:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 712.50
[32m[20221214 14:26:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 709.46
[32m[20221214 14:26:49 @agent_ppo2.py:143][0m Total time:      28.78 min
[32m[20221214 14:26:49 @agent_ppo2.py:145][0m 2646016 total steps have happened
[32m[20221214 14:26:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1292 --------------------------#
[32m[20221214 14:26:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:49 @agent_ppo2.py:185][0m |           0.0095 |         158.5688 |         -91.5066 |
[32m[20221214 14:26:49 @agent_ppo2.py:185][0m |          -0.0034 |         135.0600 |         -91.4431 |
[32m[20221214 14:26:50 @agent_ppo2.py:185][0m |          -0.0011 |         132.0903 |         -91.1694 |
[32m[20221214 14:26:50 @agent_ppo2.py:185][0m |          -0.0002 |         128.2339 |         -91.1983 |
[32m[20221214 14:26:50 @agent_ppo2.py:185][0m |          -0.0037 |         126.8321 |         -91.1162 |
[32m[20221214 14:26:50 @agent_ppo2.py:185][0m |          -0.0012 |         124.9137 |         -91.1616 |
[32m[20221214 14:26:50 @agent_ppo2.py:185][0m |          -0.0029 |         123.7945 |         -91.1480 |
[32m[20221214 14:26:50 @agent_ppo2.py:185][0m |          -0.0036 |         122.8027 |         -91.1459 |
[32m[20221214 14:26:50 @agent_ppo2.py:185][0m |          -0.0060 |         122.3901 |         -90.9732 |
[32m[20221214 14:26:50 @agent_ppo2.py:185][0m |          -0.0057 |         121.4826 |         -90.8919 |
[32m[20221214 14:26:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:26:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 667.69
[32m[20221214 14:26:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.64
[32m[20221214 14:26:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 704.77
[32m[20221214 14:26:50 @agent_ppo2.py:143][0m Total time:      28.80 min
[32m[20221214 14:26:50 @agent_ppo2.py:145][0m 2648064 total steps have happened
[32m[20221214 14:26:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1293 --------------------------#
[32m[20221214 14:26:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:51 @agent_ppo2.py:185][0m |          -0.0024 |         100.9914 |         -91.6427 |
[32m[20221214 14:26:51 @agent_ppo2.py:185][0m |          -0.0026 |          96.3603 |         -91.5707 |
[32m[20221214 14:26:51 @agent_ppo2.py:185][0m |          -0.0023 |          94.1737 |         -91.3915 |
[32m[20221214 14:26:51 @agent_ppo2.py:185][0m |           0.0031 |          93.8689 |         -91.3050 |
[32m[20221214 14:26:51 @agent_ppo2.py:185][0m |           0.0142 |         102.2923 |         -91.3296 |
[32m[20221214 14:26:51 @agent_ppo2.py:185][0m |           0.0009 |          92.7428 |         -90.7651 |
[32m[20221214 14:26:51 @agent_ppo2.py:185][0m |          -0.0028 |          91.0297 |         -91.1967 |
[32m[20221214 14:26:51 @agent_ppo2.py:185][0m |          -0.0024 |          90.8927 |         -91.0057 |
[32m[20221214 14:26:51 @agent_ppo2.py:185][0m |          -0.0006 |          90.4727 |         -91.1413 |
[32m[20221214 14:26:51 @agent_ppo2.py:185][0m |          -0.0048 |          90.0634 |         -91.1301 |
[32m[20221214 14:26:51 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:26:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.68
[32m[20221214 14:26:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 706.55
[32m[20221214 14:26:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 710.18
[32m[20221214 14:26:52 @agent_ppo2.py:143][0m Total time:      28.82 min
[32m[20221214 14:26:52 @agent_ppo2.py:145][0m 2650112 total steps have happened
[32m[20221214 14:26:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1294 --------------------------#
[32m[20221214 14:26:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:52 @agent_ppo2.py:185][0m |          -0.0031 |         119.5629 |         -89.7269 |
[32m[20221214 14:26:52 @agent_ppo2.py:185][0m |          -0.0057 |         104.3272 |         -89.6957 |
[32m[20221214 14:26:52 @agent_ppo2.py:185][0m |          -0.0036 |          99.5898 |         -89.7053 |
[32m[20221214 14:26:52 @agent_ppo2.py:185][0m |          -0.0031 |          97.4749 |         -89.7996 |
[32m[20221214 14:26:52 @agent_ppo2.py:185][0m |          -0.0066 |          94.9472 |         -89.9567 |
[32m[20221214 14:26:52 @agent_ppo2.py:185][0m |          -0.0031 |          92.0157 |         -89.8534 |
[32m[20221214 14:26:52 @agent_ppo2.py:185][0m |          -0.0061 |          90.7753 |         -90.0175 |
[32m[20221214 14:26:52 @agent_ppo2.py:185][0m |           0.0007 |          89.2207 |         -90.0690 |
[32m[20221214 14:26:52 @agent_ppo2.py:185][0m |          -0.0034 |          86.5695 |         -90.1653 |
[32m[20221214 14:26:53 @agent_ppo2.py:185][0m |          -0.0031 |          85.0523 |         -90.0957 |
[32m[20221214 14:26:53 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:26:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 707.26
[32m[20221214 14:26:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.67
[32m[20221214 14:26:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 706.21
[32m[20221214 14:26:53 @agent_ppo2.py:143][0m Total time:      28.84 min
[32m[20221214 14:26:53 @agent_ppo2.py:145][0m 2652160 total steps have happened
[32m[20221214 14:26:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1295 --------------------------#
[32m[20221214 14:26:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:53 @agent_ppo2.py:185][0m |          -0.0009 |         144.6826 |         -92.7543 |
[32m[20221214 14:26:53 @agent_ppo2.py:185][0m |           0.0021 |         131.1426 |         -92.9276 |
[32m[20221214 14:26:53 @agent_ppo2.py:185][0m |          -0.0041 |         128.4935 |         -92.6862 |
[32m[20221214 14:26:53 @agent_ppo2.py:185][0m |           0.0107 |         142.9951 |         -92.9158 |
[32m[20221214 14:26:53 @agent_ppo2.py:185][0m |           0.0017 |         127.1039 |         -92.8833 |
[32m[20221214 14:26:53 @agent_ppo2.py:185][0m |          -0.0005 |         123.2279 |         -93.0627 |
[32m[20221214 14:26:54 @agent_ppo2.py:185][0m |           0.0011 |         121.9614 |         -93.2290 |
[32m[20221214 14:26:54 @agent_ppo2.py:185][0m |          -0.0009 |         121.0018 |         -93.2632 |
[32m[20221214 14:26:54 @agent_ppo2.py:185][0m |          -0.0047 |         120.3714 |         -93.5055 |
[32m[20221214 14:26:54 @agent_ppo2.py:185][0m |          -0.0023 |         119.7427 |         -93.5404 |
[32m[20221214 14:26:54 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:26:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 653.37
[32m[20221214 14:26:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.50
[32m[20221214 14:26:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 718.45
[32m[20221214 14:26:54 @agent_ppo2.py:143][0m Total time:      28.86 min
[32m[20221214 14:26:54 @agent_ppo2.py:145][0m 2654208 total steps have happened
[32m[20221214 14:26:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1296 --------------------------#
[32m[20221214 14:26:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:54 @agent_ppo2.py:185][0m |           0.0031 |         150.9895 |         -92.9018 |
[32m[20221214 14:26:54 @agent_ppo2.py:185][0m |          -0.0005 |         141.3004 |         -93.1467 |
[32m[20221214 14:26:54 @agent_ppo2.py:185][0m |           0.0017 |         139.4664 |         -93.1945 |
[32m[20221214 14:26:55 @agent_ppo2.py:185][0m |           0.0001 |         138.2521 |         -93.1891 |
[32m[20221214 14:26:55 @agent_ppo2.py:185][0m |          -0.0044 |         137.1903 |         -93.1074 |
[32m[20221214 14:26:55 @agent_ppo2.py:185][0m |          -0.0020 |         137.7482 |         -93.2362 |
[32m[20221214 14:26:55 @agent_ppo2.py:185][0m |           0.0012 |         135.9346 |         -93.1472 |
[32m[20221214 14:26:55 @agent_ppo2.py:185][0m |           0.0018 |         144.8984 |         -93.2296 |
[32m[20221214 14:26:55 @agent_ppo2.py:185][0m |          -0.0004 |         136.7146 |         -93.1254 |
[32m[20221214 14:26:55 @agent_ppo2.py:185][0m |          -0.0002 |         136.0176 |         -93.0835 |
[32m[20221214 14:26:55 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:26:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 716.43
[32m[20221214 14:26:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 723.25
[32m[20221214 14:26:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 723.00
[32m[20221214 14:26:55 @agent_ppo2.py:143][0m Total time:      28.89 min
[32m[20221214 14:26:55 @agent_ppo2.py:145][0m 2656256 total steps have happened
[32m[20221214 14:26:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1297 --------------------------#
[32m[20221214 14:26:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:56 @agent_ppo2.py:185][0m |           0.0003 |         141.9570 |         -94.3276 |
[32m[20221214 14:26:56 @agent_ppo2.py:185][0m |           0.0020 |         135.1420 |         -94.3350 |
[32m[20221214 14:26:56 @agent_ppo2.py:185][0m |           0.0030 |         133.7568 |         -94.3271 |
[32m[20221214 14:26:56 @agent_ppo2.py:185][0m |           0.0007 |         131.8411 |         -94.4500 |
[32m[20221214 14:26:56 @agent_ppo2.py:185][0m |           0.0005 |         130.6890 |         -94.3396 |
[32m[20221214 14:26:56 @agent_ppo2.py:185][0m |          -0.0051 |         129.8107 |         -94.4977 |
[32m[20221214 14:26:56 @agent_ppo2.py:185][0m |          -0.0022 |         131.4838 |         -94.5974 |
[32m[20221214 14:26:56 @agent_ppo2.py:185][0m |           0.0005 |         130.3837 |         -94.3594 |
[32m[20221214 14:26:56 @agent_ppo2.py:185][0m |          -0.0003 |         129.9098 |         -94.3331 |
[32m[20221214 14:26:56 @agent_ppo2.py:185][0m |           0.0021 |         129.7452 |         -94.4859 |
[32m[20221214 14:26:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:26:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 719.76
[32m[20221214 14:26:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 725.70
[32m[20221214 14:26:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 738.02
[32m[20221214 14:26:56 @agent_ppo2.py:143][0m Total time:      28.91 min
[32m[20221214 14:26:56 @agent_ppo2.py:145][0m 2658304 total steps have happened
[32m[20221214 14:26:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1298 --------------------------#
[32m[20221214 14:26:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:57 @agent_ppo2.py:185][0m |          -0.0040 |         143.8040 |         -94.1557 |
[32m[20221214 14:26:57 @agent_ppo2.py:185][0m |          -0.0031 |         135.3645 |         -93.9756 |
[32m[20221214 14:26:57 @agent_ppo2.py:185][0m |          -0.0016 |         134.4926 |         -93.9868 |
[32m[20221214 14:26:57 @agent_ppo2.py:185][0m |          -0.0054 |         131.8319 |         -94.0270 |
[32m[20221214 14:26:57 @agent_ppo2.py:185][0m |          -0.0023 |         129.3810 |         -94.0593 |
[32m[20221214 14:26:57 @agent_ppo2.py:185][0m |           0.0039 |         128.3793 |         -94.0742 |
[32m[20221214 14:26:57 @agent_ppo2.py:185][0m |          -0.0037 |         126.1078 |         -94.2481 |
[32m[20221214 14:26:57 @agent_ppo2.py:185][0m |          -0.0016 |         124.8156 |         -94.4011 |
[32m[20221214 14:26:57 @agent_ppo2.py:185][0m |           0.0026 |         126.2139 |         -94.1644 |
[32m[20221214 14:26:57 @agent_ppo2.py:185][0m |           0.0011 |         122.6785 |         -94.2212 |
[32m[20221214 14:26:57 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:26:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 732.64
[32m[20221214 14:26:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 740.48
[32m[20221214 14:26:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 724.46
[32m[20221214 14:26:58 @agent_ppo2.py:143][0m Total time:      28.93 min
[32m[20221214 14:26:58 @agent_ppo2.py:145][0m 2660352 total steps have happened
[32m[20221214 14:26:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1299 --------------------------#
[32m[20221214 14:26:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:26:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:58 @agent_ppo2.py:185][0m |           0.0007 |         137.5583 |         -95.6108 |
[32m[20221214 14:26:58 @agent_ppo2.py:185][0m |          -0.0011 |         129.3162 |         -95.6232 |
[32m[20221214 14:26:58 @agent_ppo2.py:185][0m |          -0.0019 |         126.1501 |         -95.4912 |
[32m[20221214 14:26:58 @agent_ppo2.py:185][0m |          -0.0017 |         124.6995 |         -95.3865 |
[32m[20221214 14:26:58 @agent_ppo2.py:185][0m |          -0.0025 |         122.7006 |         -95.1286 |
[32m[20221214 14:26:58 @agent_ppo2.py:185][0m |          -0.0043 |         122.7481 |         -95.0795 |
[32m[20221214 14:26:58 @agent_ppo2.py:185][0m |          -0.0015 |         121.1855 |         -95.0519 |
[32m[20221214 14:26:59 @agent_ppo2.py:185][0m |          -0.0049 |         121.2766 |         -95.1906 |
[32m[20221214 14:26:59 @agent_ppo2.py:185][0m |          -0.0025 |         120.3901 |         -94.8141 |
[32m[20221214 14:26:59 @agent_ppo2.py:185][0m |          -0.0036 |         120.7874 |         -94.9550 |
[32m[20221214 14:26:59 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:26:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 722.69
[32m[20221214 14:26:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 726.53
[32m[20221214 14:26:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 727.84
[32m[20221214 14:26:59 @agent_ppo2.py:143][0m Total time:      28.95 min
[32m[20221214 14:26:59 @agent_ppo2.py:145][0m 2662400 total steps have happened
[32m[20221214 14:26:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1300 --------------------------#
[32m[20221214 14:26:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:26:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:26:59 @agent_ppo2.py:185][0m |           0.0014 |         148.2328 |         -94.8729 |
[32m[20221214 14:26:59 @agent_ppo2.py:185][0m |          -0.0018 |         141.3401 |         -94.7862 |
[32m[20221214 14:27:00 @agent_ppo2.py:185][0m |          -0.0003 |         139.0245 |         -94.6091 |
[32m[20221214 14:27:00 @agent_ppo2.py:185][0m |           0.0020 |         139.1658 |         -94.1726 |
[32m[20221214 14:27:00 @agent_ppo2.py:185][0m |           0.0105 |         144.6153 |         -94.4379 |
[32m[20221214 14:27:00 @agent_ppo2.py:185][0m |           0.0005 |         136.5189 |         -93.9882 |
[32m[20221214 14:27:00 @agent_ppo2.py:185][0m |          -0.0054 |         135.6568 |         -94.0181 |
[32m[20221214 14:27:00 @agent_ppo2.py:185][0m |          -0.0038 |         135.0615 |         -93.6751 |
[32m[20221214 14:27:00 @agent_ppo2.py:185][0m |           0.0005 |         134.2845 |         -93.8751 |
[32m[20221214 14:27:00 @agent_ppo2.py:185][0m |          -0.0035 |         134.9892 |         -93.7839 |
[32m[20221214 14:27:00 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 14:27:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 728.53
[32m[20221214 14:27:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.60
[32m[20221214 14:27:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 713.15
[32m[20221214 14:27:00 @agent_ppo2.py:143][0m Total time:      28.97 min
[32m[20221214 14:27:00 @agent_ppo2.py:145][0m 2664448 total steps have happened
[32m[20221214 14:27:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1301 --------------------------#
[32m[20221214 14:27:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:27:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:01 @agent_ppo2.py:185][0m |          -0.0027 |         100.4950 |         -93.1077 |
[32m[20221214 14:27:01 @agent_ppo2.py:185][0m |          -0.0076 |          92.3781 |         -93.0871 |
[32m[20221214 14:27:01 @agent_ppo2.py:185][0m |          -0.0014 |          90.4816 |         -92.7487 |
[32m[20221214 14:27:01 @agent_ppo2.py:185][0m |          -0.0023 |          89.7217 |         -93.0643 |
[32m[20221214 14:27:01 @agent_ppo2.py:185][0m |          -0.0025 |          89.5032 |         -93.0800 |
[32m[20221214 14:27:01 @agent_ppo2.py:185][0m |           0.0000 |          88.5240 |         -92.9510 |
[32m[20221214 14:27:01 @agent_ppo2.py:185][0m |          -0.0016 |          87.4036 |         -93.0696 |
[32m[20221214 14:27:01 @agent_ppo2.py:185][0m |          -0.0012 |          87.1044 |         -93.1222 |
[32m[20221214 14:27:02 @agent_ppo2.py:185][0m |          -0.0061 |          86.8950 |         -93.1082 |
[32m[20221214 14:27:02 @agent_ppo2.py:185][0m |          -0.0048 |          86.1067 |         -93.3085 |
[32m[20221214 14:27:02 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:27:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 605.42
[32m[20221214 14:27:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 731.83
[32m[20221214 14:27:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 717.15
[32m[20221214 14:27:02 @agent_ppo2.py:143][0m Total time:      29.00 min
[32m[20221214 14:27:02 @agent_ppo2.py:145][0m 2666496 total steps have happened
[32m[20221214 14:27:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1302 --------------------------#
[32m[20221214 14:27:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:02 @agent_ppo2.py:185][0m |          -0.0009 |         136.9901 |         -96.2333 |
[32m[20221214 14:27:02 @agent_ppo2.py:185][0m |          -0.0010 |         125.5558 |         -96.1757 |
[32m[20221214 14:27:02 @agent_ppo2.py:185][0m |          -0.0025 |         122.2258 |         -96.1439 |
[32m[20221214 14:27:02 @agent_ppo2.py:185][0m |          -0.0037 |         120.1819 |         -95.9675 |
[32m[20221214 14:27:03 @agent_ppo2.py:185][0m |          -0.0032 |         119.3950 |         -96.0632 |
[32m[20221214 14:27:03 @agent_ppo2.py:185][0m |           0.0137 |         127.3383 |         -95.9985 |
[32m[20221214 14:27:03 @agent_ppo2.py:185][0m |           0.0047 |         118.7265 |         -96.1905 |
[32m[20221214 14:27:03 @agent_ppo2.py:185][0m |          -0.0011 |         114.5713 |         -96.2695 |
[32m[20221214 14:27:03 @agent_ppo2.py:185][0m |          -0.0029 |         112.9094 |         -96.1753 |
[32m[20221214 14:27:03 @agent_ppo2.py:185][0m |          -0.0013 |         110.6617 |         -96.4041 |
[32m[20221214 14:27:03 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:27:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 712.71
[32m[20221214 14:27:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 716.10
[32m[20221214 14:27:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 722.54
[32m[20221214 14:27:03 @agent_ppo2.py:143][0m Total time:      29.02 min
[32m[20221214 14:27:03 @agent_ppo2.py:145][0m 2668544 total steps have happened
[32m[20221214 14:27:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1303 --------------------------#
[32m[20221214 14:27:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:04 @agent_ppo2.py:185][0m |          -0.0032 |         134.1408 |         -94.0746 |
[32m[20221214 14:27:04 @agent_ppo2.py:185][0m |          -0.0033 |         122.9413 |         -93.7174 |
[32m[20221214 14:27:04 @agent_ppo2.py:185][0m |          -0.0018 |         119.1770 |         -93.9218 |
[32m[20221214 14:27:04 @agent_ppo2.py:185][0m |          -0.0008 |         117.3017 |         -93.7054 |
[32m[20221214 14:27:04 @agent_ppo2.py:185][0m |           0.0033 |         115.7089 |         -93.6269 |
[32m[20221214 14:27:04 @agent_ppo2.py:185][0m |          -0.0010 |         114.2421 |         -93.6972 |
[32m[20221214 14:27:04 @agent_ppo2.py:185][0m |          -0.0039 |         114.0454 |         -93.9091 |
[32m[20221214 14:27:04 @agent_ppo2.py:185][0m |          -0.0022 |         114.5637 |         -93.7593 |
[32m[20221214 14:27:04 @agent_ppo2.py:185][0m |          -0.0013 |         113.3276 |         -93.7264 |
[32m[20221214 14:27:04 @agent_ppo2.py:185][0m |           0.0001 |         112.8276 |         -93.5218 |
[32m[20221214 14:27:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:27:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 661.60
[32m[20221214 14:27:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.30
[32m[20221214 14:27:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 714.04
[32m[20221214 14:27:05 @agent_ppo2.py:143][0m Total time:      29.04 min
[32m[20221214 14:27:05 @agent_ppo2.py:145][0m 2670592 total steps have happened
[32m[20221214 14:27:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1304 --------------------------#
[32m[20221214 14:27:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:05 @agent_ppo2.py:185][0m |           0.0002 |          92.9821 |         -93.5427 |
[32m[20221214 14:27:05 @agent_ppo2.py:185][0m |          -0.0052 |          82.0089 |         -93.4327 |
[32m[20221214 14:27:05 @agent_ppo2.py:185][0m |          -0.0031 |          79.3038 |         -93.4576 |
[32m[20221214 14:27:05 @agent_ppo2.py:185][0m |           0.0043 |          78.7762 |         -93.4928 |
[32m[20221214 14:27:05 @agent_ppo2.py:185][0m |          -0.0057 |          76.7668 |         -93.4927 |
[32m[20221214 14:27:05 @agent_ppo2.py:185][0m |           0.0040 |          76.5744 |         -93.2719 |
[32m[20221214 14:27:06 @agent_ppo2.py:185][0m |           0.0031 |          75.3385 |         -93.4376 |
[32m[20221214 14:27:06 @agent_ppo2.py:185][0m |          -0.0014 |          74.5432 |         -93.2083 |
[32m[20221214 14:27:06 @agent_ppo2.py:185][0m |           0.0061 |          79.4519 |         -93.3685 |
[32m[20221214 14:27:06 @agent_ppo2.py:185][0m |          -0.0041 |          73.8635 |         -92.9035 |
[32m[20221214 14:27:06 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:27:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 579.88
[32m[20221214 14:27:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 724.98
[32m[20221214 14:27:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.50
[32m[20221214 14:27:06 @agent_ppo2.py:143][0m Total time:      29.06 min
[32m[20221214 14:27:06 @agent_ppo2.py:145][0m 2672640 total steps have happened
[32m[20221214 14:27:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1305 --------------------------#
[32m[20221214 14:27:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:06 @agent_ppo2.py:185][0m |           0.0006 |          43.3449 |         -94.2378 |
[32m[20221214 14:27:06 @agent_ppo2.py:185][0m |          -0.0015 |          37.9799 |         -93.8647 |
[32m[20221214 14:27:06 @agent_ppo2.py:185][0m |           0.0022 |          37.0482 |         -93.9312 |
[32m[20221214 14:27:07 @agent_ppo2.py:185][0m |          -0.0055 |          36.0413 |         -93.6294 |
[32m[20221214 14:27:07 @agent_ppo2.py:185][0m |          -0.0022 |          35.4920 |         -93.7570 |
[32m[20221214 14:27:07 @agent_ppo2.py:185][0m |          -0.0082 |          35.1285 |         -93.7218 |
[32m[20221214 14:27:07 @agent_ppo2.py:185][0m |          -0.0065 |          34.5525 |         -93.6558 |
[32m[20221214 14:27:07 @agent_ppo2.py:185][0m |          -0.0063 |          34.4167 |         -93.2887 |
[32m[20221214 14:27:07 @agent_ppo2.py:185][0m |          -0.0058 |          33.9389 |         -93.3788 |
[32m[20221214 14:27:07 @agent_ppo2.py:185][0m |          -0.0035 |          33.8631 |         -93.3047 |
[32m[20221214 14:27:07 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:27:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.32
[32m[20221214 14:27:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 706.27
[32m[20221214 14:27:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 705.51
[32m[20221214 14:27:07 @agent_ppo2.py:143][0m Total time:      29.09 min
[32m[20221214 14:27:07 @agent_ppo2.py:145][0m 2674688 total steps have happened
[32m[20221214 14:27:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1306 --------------------------#
[32m[20221214 14:27:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:08 @agent_ppo2.py:185][0m |          -0.0051 |          93.9188 |         -92.0524 |
[32m[20221214 14:27:08 @agent_ppo2.py:185][0m |          -0.0018 |          89.7966 |         -92.2925 |
[32m[20221214 14:27:08 @agent_ppo2.py:185][0m |          -0.0053 |          88.5081 |         -92.0189 |
[32m[20221214 14:27:08 @agent_ppo2.py:185][0m |          -0.0016 |          87.9125 |         -92.4979 |
[32m[20221214 14:27:08 @agent_ppo2.py:185][0m |          -0.0006 |          87.0090 |         -92.1633 |
[32m[20221214 14:27:08 @agent_ppo2.py:185][0m |          -0.0002 |          86.7176 |         -92.4947 |
[32m[20221214 14:27:08 @agent_ppo2.py:185][0m |          -0.0082 |          85.2460 |         -92.2525 |
[32m[20221214 14:27:08 @agent_ppo2.py:185][0m |          -0.0019 |          83.3052 |         -92.1982 |
[32m[20221214 14:27:08 @agent_ppo2.py:185][0m |           0.0036 |          85.7935 |         -92.4813 |
[32m[20221214 14:27:08 @agent_ppo2.py:185][0m |          -0.0079 |          82.3906 |         -92.0819 |
[32m[20221214 14:27:08 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:27:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 601.01
[32m[20221214 14:27:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 704.84
[32m[20221214 14:27:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 710.94
[32m[20221214 14:27:09 @agent_ppo2.py:143][0m Total time:      29.11 min
[32m[20221214 14:27:09 @agent_ppo2.py:145][0m 2676736 total steps have happened
[32m[20221214 14:27:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1307 --------------------------#
[32m[20221214 14:27:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:27:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:09 @agent_ppo2.py:185][0m |           0.0011 |         180.7290 |         -94.8351 |
[32m[20221214 14:27:09 @agent_ppo2.py:185][0m |           0.0015 |         169.1897 |         -95.0452 |
[32m[20221214 14:27:09 @agent_ppo2.py:185][0m |          -0.0012 |         164.6768 |         -95.3247 |
[32m[20221214 14:27:09 @agent_ppo2.py:185][0m |           0.0017 |         163.1111 |         -95.1152 |
[32m[20221214 14:27:09 @agent_ppo2.py:185][0m |          -0.0004 |         160.5823 |         -94.9832 |
[32m[20221214 14:27:09 @agent_ppo2.py:185][0m |           0.0008 |         159.8839 |         -95.6869 |
[32m[20221214 14:27:10 @agent_ppo2.py:185][0m |           0.0037 |         161.8239 |         -95.6519 |
[32m[20221214 14:27:10 @agent_ppo2.py:185][0m |           0.0129 |         174.9195 |         -95.5378 |
[32m[20221214 14:27:10 @agent_ppo2.py:185][0m |          -0.0006 |         157.6640 |         -95.3687 |
[32m[20221214 14:27:10 @agent_ppo2.py:185][0m |           0.0005 |         156.6685 |         -95.5649 |
[32m[20221214 14:27:10 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:27:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.45
[32m[20221214 14:27:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.51
[32m[20221214 14:27:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 715.17
[32m[20221214 14:27:10 @agent_ppo2.py:143][0m Total time:      29.13 min
[32m[20221214 14:27:10 @agent_ppo2.py:145][0m 2678784 total steps have happened
[32m[20221214 14:27:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1308 --------------------------#
[32m[20221214 14:27:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:10 @agent_ppo2.py:185][0m |           0.0031 |         121.9372 |         -92.4160 |
[32m[20221214 14:27:10 @agent_ppo2.py:185][0m |          -0.0037 |         112.7222 |         -92.2902 |
[32m[20221214 14:27:10 @agent_ppo2.py:185][0m |          -0.0016 |         105.6977 |         -92.2410 |
[32m[20221214 14:27:10 @agent_ppo2.py:185][0m |          -0.0030 |         102.3320 |         -92.3454 |
[32m[20221214 14:27:11 @agent_ppo2.py:185][0m |          -0.0037 |         100.7096 |         -92.1475 |
[32m[20221214 14:27:11 @agent_ppo2.py:185][0m |          -0.0077 |         100.1640 |         -92.3606 |
[32m[20221214 14:27:11 @agent_ppo2.py:185][0m |          -0.0033 |          99.0803 |         -92.1561 |
[32m[20221214 14:27:11 @agent_ppo2.py:185][0m |           0.0023 |         101.1720 |         -92.2767 |
[32m[20221214 14:27:11 @agent_ppo2.py:185][0m |          -0.0065 |          97.4785 |         -92.1064 |
[32m[20221214 14:27:11 @agent_ppo2.py:185][0m |           0.0007 |         104.2495 |         -92.2422 |
[32m[20221214 14:27:11 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:27:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 692.90
[32m[20221214 14:27:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 724.83
[32m[20221214 14:27:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 566.60
[32m[20221214 14:27:11 @agent_ppo2.py:143][0m Total time:      29.15 min
[32m[20221214 14:27:11 @agent_ppo2.py:145][0m 2680832 total steps have happened
[32m[20221214 14:27:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1309 --------------------------#
[32m[20221214 14:27:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:11 @agent_ppo2.py:185][0m |           0.0003 |          47.2431 |         -94.9664 |
[32m[20221214 14:27:12 @agent_ppo2.py:185][0m |           0.0216 |          63.9513 |         -94.3515 |
[32m[20221214 14:27:12 @agent_ppo2.py:185][0m |          -0.0034 |          42.1392 |         -94.5631 |
[32m[20221214 14:27:12 @agent_ppo2.py:185][0m |          -0.0003 |          41.1852 |         -94.4567 |
[32m[20221214 14:27:12 @agent_ppo2.py:185][0m |          -0.0014 |          39.7832 |         -94.2471 |
[32m[20221214 14:27:12 @agent_ppo2.py:185][0m |          -0.0009 |          39.5981 |         -94.3721 |
[32m[20221214 14:27:12 @agent_ppo2.py:185][0m |           0.0363 |          67.9991 |         -94.1618 |
[32m[20221214 14:27:12 @agent_ppo2.py:185][0m |           0.0017 |          39.8663 |         -94.2290 |
[32m[20221214 14:27:12 @agent_ppo2.py:185][0m |          -0.0047 |          38.7187 |         -94.3348 |
[32m[20221214 14:27:12 @agent_ppo2.py:185][0m |          -0.0060 |          38.2644 |         -94.3714 |
[32m[20221214 14:27:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:27:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 579.02
[32m[20221214 14:27:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 697.14
[32m[20221214 14:27:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 708.78
[32m[20221214 14:27:12 @agent_ppo2.py:143][0m Total time:      29.17 min
[32m[20221214 14:27:12 @agent_ppo2.py:145][0m 2682880 total steps have happened
[32m[20221214 14:27:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1310 --------------------------#
[32m[20221214 14:27:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:27:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:13 @agent_ppo2.py:185][0m |          -0.0005 |         106.8512 |         -95.5173 |
[32m[20221214 14:27:13 @agent_ppo2.py:185][0m |          -0.0010 |         100.9544 |         -95.4810 |
[32m[20221214 14:27:13 @agent_ppo2.py:185][0m |           0.0141 |         113.0814 |         -95.5927 |
[32m[20221214 14:27:13 @agent_ppo2.py:185][0m |           0.0003 |          98.4668 |         -95.2302 |
[32m[20221214 14:27:13 @agent_ppo2.py:185][0m |          -0.0021 |          95.4084 |         -95.5063 |
[32m[20221214 14:27:13 @agent_ppo2.py:185][0m |           0.0121 |         109.6820 |         -95.4433 |
[32m[20221214 14:27:13 @agent_ppo2.py:185][0m |          -0.0034 |          93.3606 |         -95.2419 |
[32m[20221214 14:27:13 @agent_ppo2.py:185][0m |          -0.0035 |          92.3170 |         -95.7282 |
[32m[20221214 14:27:13 @agent_ppo2.py:185][0m |           0.0030 |          92.1092 |         -95.5669 |
[32m[20221214 14:27:14 @agent_ppo2.py:185][0m |          -0.0048 |          92.2996 |         -95.4643 |
[32m[20221214 14:27:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:27:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 607.48
[32m[20221214 14:27:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 708.11
[32m[20221214 14:27:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.68
[32m[20221214 14:27:14 @agent_ppo2.py:143][0m Total time:      29.19 min
[32m[20221214 14:27:14 @agent_ppo2.py:145][0m 2684928 total steps have happened
[32m[20221214 14:27:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1311 --------------------------#
[32m[20221214 14:27:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:27:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:14 @agent_ppo2.py:185][0m |          -0.0026 |          69.6404 |         -93.2295 |
[32m[20221214 14:27:14 @agent_ppo2.py:185][0m |          -0.0020 |          64.2847 |         -92.8840 |
[32m[20221214 14:27:14 @agent_ppo2.py:185][0m |           0.0021 |          62.2674 |         -93.1822 |
[32m[20221214 14:27:14 @agent_ppo2.py:185][0m |          -0.0066 |          58.9935 |         -92.7605 |
[32m[20221214 14:27:14 @agent_ppo2.py:185][0m |          -0.0016 |          57.4812 |         -92.8877 |
[32m[20221214 14:27:14 @agent_ppo2.py:185][0m |          -0.0055 |          55.9048 |         -92.8432 |
[32m[20221214 14:27:15 @agent_ppo2.py:185][0m |           0.0168 |          73.0647 |         -92.6985 |
[32m[20221214 14:27:15 @agent_ppo2.py:185][0m |          -0.0022 |          55.5715 |         -91.8579 |
[32m[20221214 14:27:15 @agent_ppo2.py:185][0m |          -0.0056 |          53.5610 |         -92.5361 |
[32m[20221214 14:27:15 @agent_ppo2.py:185][0m |          -0.0058 |          53.0700 |         -92.3078 |
[32m[20221214 14:27:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:27:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.72
[32m[20221214 14:27:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 697.97
[32m[20221214 14:27:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 685.07
[32m[20221214 14:27:15 @agent_ppo2.py:143][0m Total time:      29.22 min
[32m[20221214 14:27:15 @agent_ppo2.py:145][0m 2686976 total steps have happened
[32m[20221214 14:27:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1312 --------------------------#
[32m[20221214 14:27:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:27:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:15 @agent_ppo2.py:185][0m |          -0.0042 |         121.8622 |         -94.8024 |
[32m[20221214 14:27:15 @agent_ppo2.py:185][0m |          -0.0003 |         114.6293 |         -94.1285 |
[32m[20221214 14:27:16 @agent_ppo2.py:185][0m |          -0.0010 |         112.3932 |         -94.7500 |
[32m[20221214 14:27:16 @agent_ppo2.py:185][0m |          -0.0020 |         110.8132 |         -94.5016 |
[32m[20221214 14:27:16 @agent_ppo2.py:185][0m |          -0.0033 |         109.8973 |         -94.5787 |
[32m[20221214 14:27:16 @agent_ppo2.py:185][0m |           0.0056 |         114.0790 |         -94.6098 |
[32m[20221214 14:27:16 @agent_ppo2.py:185][0m |          -0.0032 |         107.5290 |         -94.5217 |
[32m[20221214 14:27:16 @agent_ppo2.py:185][0m |          -0.0002 |         107.2607 |         -94.5094 |
[32m[20221214 14:27:16 @agent_ppo2.py:185][0m |          -0.0025 |         106.3617 |         -94.9287 |
[32m[20221214 14:27:16 @agent_ppo2.py:185][0m |           0.0001 |         107.3548 |         -94.6299 |
[32m[20221214 14:27:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:27:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 672.93
[32m[20221214 14:27:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.58
[32m[20221214 14:27:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 708.86
[32m[20221214 14:27:16 @agent_ppo2.py:143][0m Total time:      29.24 min
[32m[20221214 14:27:16 @agent_ppo2.py:145][0m 2689024 total steps have happened
[32m[20221214 14:27:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1313 --------------------------#
[32m[20221214 14:27:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:17 @agent_ppo2.py:185][0m |           0.0092 |         167.1125 |         -92.9390 |
[32m[20221214 14:27:17 @agent_ppo2.py:185][0m |          -0.0011 |         152.3631 |         -92.9690 |
[32m[20221214 14:27:17 @agent_ppo2.py:185][0m |          -0.0002 |         150.5219 |         -92.9362 |
[32m[20221214 14:27:17 @agent_ppo2.py:185][0m |           0.0018 |         150.7050 |         -92.7963 |
[32m[20221214 14:27:17 @agent_ppo2.py:185][0m |          -0.0047 |         147.5631 |         -93.0455 |
[32m[20221214 14:27:17 @agent_ppo2.py:185][0m |          -0.0011 |         146.9773 |         -93.0203 |
[32m[20221214 14:27:17 @agent_ppo2.py:185][0m |          -0.0025 |         147.0465 |         -93.1825 |
[32m[20221214 14:27:17 @agent_ppo2.py:185][0m |          -0.0022 |         146.1208 |         -92.9240 |
[32m[20221214 14:27:17 @agent_ppo2.py:185][0m |          -0.0040 |         144.7765 |         -92.8105 |
[32m[20221214 14:27:17 @agent_ppo2.py:185][0m |          -0.0044 |         145.4533 |         -93.2660 |
[32m[20221214 14:27:17 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:27:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.15
[32m[20221214 14:27:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 715.45
[32m[20221214 14:27:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 699.88
[32m[20221214 14:27:18 @agent_ppo2.py:143][0m Total time:      29.26 min
[32m[20221214 14:27:18 @agent_ppo2.py:145][0m 2691072 total steps have happened
[32m[20221214 14:27:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1314 --------------------------#
[32m[20221214 14:27:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:27:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:18 @agent_ppo2.py:185][0m |           0.0046 |         131.9373 |         -93.4595 |
[32m[20221214 14:27:18 @agent_ppo2.py:185][0m |           0.0009 |         113.8000 |         -93.1522 |
[32m[20221214 14:27:18 @agent_ppo2.py:185][0m |          -0.0045 |         109.3530 |         -93.6162 |
[32m[20221214 14:27:18 @agent_ppo2.py:185][0m |          -0.0078 |         107.8941 |         -93.6388 |
[32m[20221214 14:27:18 @agent_ppo2.py:185][0m |          -0.0048 |         106.3108 |         -93.6926 |
[32m[20221214 14:27:18 @agent_ppo2.py:185][0m |          -0.0034 |         104.1425 |         -93.8440 |
[32m[20221214 14:27:18 @agent_ppo2.py:185][0m |          -0.0001 |         105.6638 |         -93.7214 |
[32m[20221214 14:27:19 @agent_ppo2.py:185][0m |          -0.0031 |         103.5833 |         -93.9638 |
[32m[20221214 14:27:19 @agent_ppo2.py:185][0m |          -0.0033 |         103.2678 |         -94.0862 |
[32m[20221214 14:27:19 @agent_ppo2.py:185][0m |          -0.0040 |         102.0063 |         -93.9600 |
[32m[20221214 14:27:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:27:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 635.70
[32m[20221214 14:27:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.01
[32m[20221214 14:27:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 707.52
[32m[20221214 14:27:19 @agent_ppo2.py:143][0m Total time:      29.28 min
[32m[20221214 14:27:19 @agent_ppo2.py:145][0m 2693120 total steps have happened
[32m[20221214 14:27:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1315 --------------------------#
[32m[20221214 14:27:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:19 @agent_ppo2.py:185][0m |           0.0022 |         146.5859 |         -94.3436 |
[32m[20221214 14:27:19 @agent_ppo2.py:185][0m |          -0.0032 |         140.3780 |         -94.1313 |
[32m[20221214 14:27:19 @agent_ppo2.py:185][0m |          -0.0041 |         138.1959 |         -94.3066 |
[32m[20221214 14:27:19 @agent_ppo2.py:185][0m |           0.0013 |         135.6114 |         -94.3241 |
[32m[20221214 14:27:20 @agent_ppo2.py:185][0m |           0.0006 |         134.9939 |         -94.3544 |
[32m[20221214 14:27:20 @agent_ppo2.py:185][0m |          -0.0044 |         135.1630 |         -94.5380 |
[32m[20221214 14:27:20 @agent_ppo2.py:185][0m |           0.0059 |         143.4091 |         -94.5348 |
[32m[20221214 14:27:20 @agent_ppo2.py:185][0m |           0.0008 |         133.7993 |         -94.5788 |
[32m[20221214 14:27:20 @agent_ppo2.py:185][0m |          -0.0050 |         133.6187 |         -94.2345 |
[32m[20221214 14:27:20 @agent_ppo2.py:185][0m |          -0.0008 |         132.4540 |         -94.7911 |
[32m[20221214 14:27:20 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:27:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.35
[32m[20221214 14:27:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 712.14
[32m[20221214 14:27:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 704.19
[32m[20221214 14:27:20 @agent_ppo2.py:143][0m Total time:      29.30 min
[32m[20221214 14:27:20 @agent_ppo2.py:145][0m 2695168 total steps have happened
[32m[20221214 14:27:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1316 --------------------------#
[32m[20221214 14:27:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:20 @agent_ppo2.py:185][0m |           0.0007 |         182.8218 |         -96.4032 |
[32m[20221214 14:27:21 @agent_ppo2.py:185][0m |          -0.0011 |         172.5903 |         -96.2914 |
[32m[20221214 14:27:21 @agent_ppo2.py:185][0m |           0.0012 |         170.0395 |         -96.1793 |
[32m[20221214 14:27:21 @agent_ppo2.py:185][0m |          -0.0052 |         165.4810 |         -96.4024 |
[32m[20221214 14:27:21 @agent_ppo2.py:185][0m |          -0.0005 |         165.1177 |         -96.6033 |
[32m[20221214 14:27:21 @agent_ppo2.py:185][0m |          -0.0014 |         163.7427 |         -95.9743 |
[32m[20221214 14:27:21 @agent_ppo2.py:185][0m |          -0.0010 |         163.3455 |         -96.4204 |
[32m[20221214 14:27:21 @agent_ppo2.py:185][0m |          -0.0000 |         162.8709 |         -96.4796 |
[32m[20221214 14:27:21 @agent_ppo2.py:185][0m |          -0.0027 |         162.2050 |         -96.3031 |
[32m[20221214 14:27:21 @agent_ppo2.py:185][0m |           0.0016 |         163.8207 |         -96.2910 |
[32m[20221214 14:27:21 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:27:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 704.76
[32m[20221214 14:27:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.47
[32m[20221214 14:27:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 700.66
[32m[20221214 14:27:21 @agent_ppo2.py:143][0m Total time:      29.32 min
[32m[20221214 14:27:21 @agent_ppo2.py:145][0m 2697216 total steps have happened
[32m[20221214 14:27:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1317 --------------------------#
[32m[20221214 14:27:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:27:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:22 @agent_ppo2.py:185][0m |           0.0010 |         146.8295 |         -97.4872 |
[32m[20221214 14:27:22 @agent_ppo2.py:185][0m |           0.0003 |         140.9116 |         -97.6083 |
[32m[20221214 14:27:22 @agent_ppo2.py:185][0m |          -0.0022 |         138.6953 |         -97.4416 |
[32m[20221214 14:27:22 @agent_ppo2.py:185][0m |           0.0012 |         137.3700 |         -97.6637 |
[32m[20221214 14:27:22 @agent_ppo2.py:185][0m |          -0.0031 |         136.3309 |         -97.8077 |
[32m[20221214 14:27:22 @agent_ppo2.py:185][0m |          -0.0025 |         136.2228 |         -97.8703 |
[32m[20221214 14:27:22 @agent_ppo2.py:185][0m |          -0.0035 |         135.3579 |         -97.6936 |
[32m[20221214 14:27:22 @agent_ppo2.py:185][0m |          -0.0014 |         135.1268 |         -98.1028 |
[32m[20221214 14:27:22 @agent_ppo2.py:185][0m |          -0.0011 |         135.7513 |         -98.0546 |
[32m[20221214 14:27:22 @agent_ppo2.py:185][0m |          -0.0041 |         133.8697 |         -98.2321 |
[32m[20221214 14:27:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:27:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 702.02
[32m[20221214 14:27:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 704.94
[32m[20221214 14:27:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 692.92
[32m[20221214 14:27:23 @agent_ppo2.py:143][0m Total time:      29.34 min
[32m[20221214 14:27:23 @agent_ppo2.py:145][0m 2699264 total steps have happened
[32m[20221214 14:27:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1318 --------------------------#
[32m[20221214 14:27:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:23 @agent_ppo2.py:185][0m |           0.0095 |          90.2819 |         -97.2607 |
[32m[20221214 14:27:23 @agent_ppo2.py:185][0m |           0.0054 |          73.5712 |         -97.7100 |
[32m[20221214 14:27:23 @agent_ppo2.py:185][0m |          -0.0010 |          70.2073 |         -97.7960 |
[32m[20221214 14:27:23 @agent_ppo2.py:185][0m |           0.0030 |          68.9199 |         -97.8016 |
[32m[20221214 14:27:23 @agent_ppo2.py:185][0m |          -0.0007 |          66.7794 |         -97.9522 |
[32m[20221214 14:27:23 @agent_ppo2.py:185][0m |          -0.0019 |          65.4524 |         -97.8879 |
[32m[20221214 14:27:23 @agent_ppo2.py:185][0m |          -0.0014 |          65.3127 |         -97.7228 |
[32m[20221214 14:27:24 @agent_ppo2.py:185][0m |          -0.0048 |          63.7952 |         -98.0561 |
[32m[20221214 14:27:24 @agent_ppo2.py:185][0m |          -0.0025 |          62.9284 |         -98.2281 |
[32m[20221214 14:27:24 @agent_ppo2.py:185][0m |          -0.0029 |          62.0912 |         -98.0176 |
[32m[20221214 14:27:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:27:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.42
[32m[20221214 14:27:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.27
[32m[20221214 14:27:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 685.56
[32m[20221214 14:27:24 @agent_ppo2.py:143][0m Total time:      29.36 min
[32m[20221214 14:27:24 @agent_ppo2.py:145][0m 2701312 total steps have happened
[32m[20221214 14:27:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1319 --------------------------#
[32m[20221214 14:27:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:27:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:24 @agent_ppo2.py:185][0m |          -0.0007 |         151.5740 |         -99.5426 |
[32m[20221214 14:27:24 @agent_ppo2.py:185][0m |          -0.0060 |         145.6012 |         -99.5521 |
[32m[20221214 14:27:24 @agent_ppo2.py:185][0m |          -0.0032 |         143.2867 |         -99.5708 |
[32m[20221214 14:27:24 @agent_ppo2.py:185][0m |          -0.0026 |         141.8782 |         -99.5079 |
[32m[20221214 14:27:25 @agent_ppo2.py:185][0m |          -0.0003 |         144.3837 |         -99.3991 |
[32m[20221214 14:27:25 @agent_ppo2.py:185][0m |          -0.0035 |         140.1204 |         -99.2763 |
[32m[20221214 14:27:25 @agent_ppo2.py:185][0m |           0.0010 |         142.7567 |         -99.3575 |
[32m[20221214 14:27:25 @agent_ppo2.py:185][0m |          -0.0044 |         140.4503 |         -99.5264 |
[32m[20221214 14:27:25 @agent_ppo2.py:185][0m |           0.0005 |         140.4512 |         -99.2999 |
[32m[20221214 14:27:25 @agent_ppo2.py:185][0m |          -0.0049 |         139.1552 |         -99.1977 |
[32m[20221214 14:27:25 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:27:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 688.90
[32m[20221214 14:27:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 697.95
[32m[20221214 14:27:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.90
[32m[20221214 14:27:25 @agent_ppo2.py:143][0m Total time:      29.38 min
[32m[20221214 14:27:25 @agent_ppo2.py:145][0m 2703360 total steps have happened
[32m[20221214 14:27:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1320 --------------------------#
[32m[20221214 14:27:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:27:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:25 @agent_ppo2.py:185][0m |          -0.0084 |          19.3447 |         -99.5591 |
[32m[20221214 14:27:26 @agent_ppo2.py:185][0m |          -0.0055 |          12.6631 |         -99.3251 |
[32m[20221214 14:27:26 @agent_ppo2.py:185][0m |          -0.0053 |          11.3084 |         -99.1984 |
[32m[20221214 14:27:26 @agent_ppo2.py:185][0m |          -0.0025 |          10.5005 |         -98.9786 |
[32m[20221214 14:27:26 @agent_ppo2.py:185][0m |          -0.0099 |          10.0991 |         -99.0874 |
[32m[20221214 14:27:26 @agent_ppo2.py:185][0m |          -0.0048 |           9.7187 |         -98.9963 |
[32m[20221214 14:27:26 @agent_ppo2.py:185][0m |          -0.0066 |           9.4186 |         -99.0184 |
[32m[20221214 14:27:26 @agent_ppo2.py:185][0m |           0.0012 |           9.2364 |         -98.8463 |
[32m[20221214 14:27:26 @agent_ppo2.py:185][0m |          -0.0106 |           8.9455 |         -98.5151 |
[32m[20221214 14:27:26 @agent_ppo2.py:185][0m |           0.0032 |           8.8562 |         -98.6848 |
[32m[20221214 14:27:26 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:27:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.28
[32m[20221214 14:27:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.11
[32m[20221214 14:27:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 676.27
[32m[20221214 14:27:26 @agent_ppo2.py:143][0m Total time:      29.41 min
[32m[20221214 14:27:26 @agent_ppo2.py:145][0m 2705408 total steps have happened
[32m[20221214 14:27:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1321 --------------------------#
[32m[20221214 14:27:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:27:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:27 @agent_ppo2.py:185][0m |          -0.0041 |          89.4900 |         -95.8685 |
[32m[20221214 14:27:27 @agent_ppo2.py:185][0m |           0.0009 |          82.8433 |         -95.8550 |
[32m[20221214 14:27:27 @agent_ppo2.py:185][0m |          -0.0005 |          80.6213 |         -95.7164 |
[32m[20221214 14:27:27 @agent_ppo2.py:185][0m |          -0.0040 |          79.5447 |         -95.7606 |
[32m[20221214 14:27:27 @agent_ppo2.py:185][0m |           0.0006 |          77.9093 |         -95.6409 |
[32m[20221214 14:27:27 @agent_ppo2.py:185][0m |           0.0017 |          77.6919 |         -95.8711 |
[32m[20221214 14:27:27 @agent_ppo2.py:185][0m |          -0.0028 |          77.0346 |         -95.7423 |
[32m[20221214 14:27:27 @agent_ppo2.py:185][0m |          -0.0066 |          76.8408 |         -95.6266 |
[32m[20221214 14:27:27 @agent_ppo2.py:185][0m |          -0.0040 |          76.2719 |         -95.6939 |
[32m[20221214 14:27:28 @agent_ppo2.py:185][0m |          -0.0028 |          76.3256 |         -95.6726 |
[32m[20221214 14:27:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:27:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 602.19
[32m[20221214 14:27:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 695.66
[32m[20221214 14:27:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 693.17
[32m[20221214 14:27:28 @agent_ppo2.py:143][0m Total time:      29.43 min
[32m[20221214 14:27:28 @agent_ppo2.py:145][0m 2707456 total steps have happened
[32m[20221214 14:27:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1322 --------------------------#
[32m[20221214 14:27:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:27:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:28 @agent_ppo2.py:185][0m |          -0.0044 |          99.5147 |         -98.3693 |
[32m[20221214 14:27:28 @agent_ppo2.py:185][0m |          -0.0053 |          92.4967 |         -97.8015 |
[32m[20221214 14:27:28 @agent_ppo2.py:185][0m |          -0.0064 |          90.5731 |         -98.0636 |
[32m[20221214 14:27:28 @agent_ppo2.py:185][0m |          -0.0041 |          88.9137 |         -98.1264 |
[32m[20221214 14:27:28 @agent_ppo2.py:185][0m |          -0.0102 |          88.5500 |         -98.0402 |
[32m[20221214 14:27:28 @agent_ppo2.py:185][0m |          -0.0048 |          88.1577 |         -97.9228 |
[32m[20221214 14:27:29 @agent_ppo2.py:185][0m |          -0.0037 |          86.6736 |         -97.9105 |
[32m[20221214 14:27:29 @agent_ppo2.py:185][0m |          -0.0035 |          86.2674 |         -97.8642 |
[32m[20221214 14:27:29 @agent_ppo2.py:185][0m |           0.0051 |          89.5609 |         -97.8076 |
[32m[20221214 14:27:29 @agent_ppo2.py:185][0m |          -0.0071 |          85.2314 |         -97.5934 |
[32m[20221214 14:27:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:27:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.67
[32m[20221214 14:27:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 699.48
[32m[20221214 14:27:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 690.40
[32m[20221214 14:27:29 @agent_ppo2.py:143][0m Total time:      29.45 min
[32m[20221214 14:27:29 @agent_ppo2.py:145][0m 2709504 total steps have happened
[32m[20221214 14:27:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1323 --------------------------#
[32m[20221214 14:27:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:29 @agent_ppo2.py:185][0m |           0.0024 |         127.9449 |         -96.1251 |
[32m[20221214 14:27:29 @agent_ppo2.py:185][0m |           0.0034 |         121.4929 |         -96.0240 |
[32m[20221214 14:27:29 @agent_ppo2.py:185][0m |           0.0116 |         129.6026 |         -96.0873 |
[32m[20221214 14:27:30 @agent_ppo2.py:185][0m |           0.0013 |         119.4879 |         -95.6088 |
[32m[20221214 14:27:30 @agent_ppo2.py:185][0m |           0.0034 |         119.0447 |         -96.2128 |
[32m[20221214 14:27:30 @agent_ppo2.py:185][0m |          -0.0004 |         117.2375 |         -96.0877 |
[32m[20221214 14:27:30 @agent_ppo2.py:185][0m |           0.0093 |         131.3261 |         -95.9456 |
[32m[20221214 14:27:30 @agent_ppo2.py:185][0m |           0.0008 |         116.7841 |         -95.9778 |
[32m[20221214 14:27:30 @agent_ppo2.py:185][0m |          -0.0004 |         115.4779 |         -96.1315 |
[32m[20221214 14:27:30 @agent_ppo2.py:185][0m |          -0.0010 |         116.2495 |         -96.0118 |
[32m[20221214 14:27:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:27:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 633.61
[32m[20221214 14:27:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.50
[32m[20221214 14:27:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 688.49
[32m[20221214 14:27:30 @agent_ppo2.py:143][0m Total time:      29.47 min
[32m[20221214 14:27:30 @agent_ppo2.py:145][0m 2711552 total steps have happened
[32m[20221214 14:27:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1324 --------------------------#
[32m[20221214 14:27:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:31 @agent_ppo2.py:185][0m |          -0.0009 |         130.5237 |         -96.6016 |
[32m[20221214 14:27:31 @agent_ppo2.py:185][0m |          -0.0025 |         114.5023 |         -96.3551 |
[32m[20221214 14:27:31 @agent_ppo2.py:185][0m |          -0.0025 |         109.1726 |         -96.4920 |
[32m[20221214 14:27:31 @agent_ppo2.py:185][0m |          -0.0023 |         107.1592 |         -96.1545 |
[32m[20221214 14:27:31 @agent_ppo2.py:185][0m |          -0.0020 |         105.4314 |         -96.1838 |
[32m[20221214 14:27:31 @agent_ppo2.py:185][0m |          -0.0029 |         103.5130 |         -96.0251 |
[32m[20221214 14:27:31 @agent_ppo2.py:185][0m |          -0.0030 |         102.5814 |         -96.1294 |
[32m[20221214 14:27:31 @agent_ppo2.py:185][0m |           0.0019 |         103.2163 |         -96.1549 |
[32m[20221214 14:27:31 @agent_ppo2.py:185][0m |          -0.0016 |         102.1461 |         -95.9933 |
[32m[20221214 14:27:31 @agent_ppo2.py:185][0m |          -0.0038 |         101.0179 |         -95.7218 |
[32m[20221214 14:27:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:27:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 620.61
[32m[20221214 14:27:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 700.19
[32m[20221214 14:27:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 674.17
[32m[20221214 14:27:31 @agent_ppo2.py:143][0m Total time:      29.49 min
[32m[20221214 14:27:31 @agent_ppo2.py:145][0m 2713600 total steps have happened
[32m[20221214 14:27:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1325 --------------------------#
[32m[20221214 14:27:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:32 @agent_ppo2.py:185][0m |           0.0021 |         125.3808 |         -95.4443 |
[32m[20221214 14:27:32 @agent_ppo2.py:185][0m |           0.0003 |         118.6207 |         -95.4928 |
[32m[20221214 14:27:32 @agent_ppo2.py:185][0m |          -0.0032 |         115.3762 |         -95.4846 |
[32m[20221214 14:27:32 @agent_ppo2.py:185][0m |          -0.0014 |         112.7532 |         -95.4688 |
[32m[20221214 14:27:32 @agent_ppo2.py:185][0m |           0.0011 |         111.8089 |         -95.7625 |
[32m[20221214 14:27:32 @agent_ppo2.py:185][0m |           0.0048 |         120.2322 |         -95.4045 |
[32m[20221214 14:27:32 @agent_ppo2.py:185][0m |          -0.0029 |         110.1855 |         -95.7982 |
[32m[20221214 14:27:32 @agent_ppo2.py:185][0m |          -0.0058 |         109.8895 |         -95.6760 |
[32m[20221214 14:27:33 @agent_ppo2.py:185][0m |          -0.0018 |         109.1614 |         -95.8039 |
[32m[20221214 14:27:33 @agent_ppo2.py:185][0m |           0.0036 |         110.3577 |         -95.6234 |
[32m[20221214 14:27:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:27:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 638.68
[32m[20221214 14:27:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 685.85
[32m[20221214 14:27:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 671.45
[32m[20221214 14:27:33 @agent_ppo2.py:143][0m Total time:      29.51 min
[32m[20221214 14:27:33 @agent_ppo2.py:145][0m 2715648 total steps have happened
[32m[20221214 14:27:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1326 --------------------------#
[32m[20221214 14:27:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:27:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:33 @agent_ppo2.py:185][0m |          -0.0010 |         101.0876 |         -95.3542 |
[32m[20221214 14:27:33 @agent_ppo2.py:185][0m |          -0.0040 |          92.5415 |         -95.4264 |
[32m[20221214 14:27:33 @agent_ppo2.py:185][0m |          -0.0073 |          90.3136 |         -95.2898 |
[32m[20221214 14:27:33 @agent_ppo2.py:185][0m |          -0.0023 |          87.8461 |         -95.4364 |
[32m[20221214 14:27:33 @agent_ppo2.py:185][0m |          -0.0018 |          87.0577 |         -95.3729 |
[32m[20221214 14:27:34 @agent_ppo2.py:185][0m |          -0.0010 |          86.1903 |         -95.1695 |
[32m[20221214 14:27:34 @agent_ppo2.py:185][0m |          -0.0004 |          85.1738 |         -95.3069 |
[32m[20221214 14:27:34 @agent_ppo2.py:185][0m |          -0.0005 |          84.8423 |         -95.0220 |
[32m[20221214 14:27:34 @agent_ppo2.py:185][0m |           0.0126 |         103.1336 |         -95.1825 |
[32m[20221214 14:27:34 @agent_ppo2.py:185][0m |          -0.0016 |          84.1121 |         -94.7644 |
[32m[20221214 14:27:34 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:27:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.80
[32m[20221214 14:27:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 688.77
[32m[20221214 14:27:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 661.41
[32m[20221214 14:27:34 @agent_ppo2.py:143][0m Total time:      29.53 min
[32m[20221214 14:27:34 @agent_ppo2.py:145][0m 2717696 total steps have happened
[32m[20221214 14:27:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1327 --------------------------#
[32m[20221214 14:27:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:27:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:34 @agent_ppo2.py:185][0m |           0.0051 |          86.8628 |         -96.2097 |
[32m[20221214 14:27:34 @agent_ppo2.py:185][0m |          -0.0030 |          75.7606 |         -96.1463 |
[32m[20221214 14:27:35 @agent_ppo2.py:185][0m |          -0.0049 |          72.6368 |         -96.1958 |
[32m[20221214 14:27:35 @agent_ppo2.py:185][0m |           0.0004 |          71.3266 |         -96.0372 |
[32m[20221214 14:27:35 @agent_ppo2.py:185][0m |          -0.0028 |          69.6807 |         -96.2006 |
[32m[20221214 14:27:35 @agent_ppo2.py:185][0m |          -0.0057 |          68.1601 |         -96.1535 |
[32m[20221214 14:27:35 @agent_ppo2.py:185][0m |          -0.0044 |          67.2454 |         -96.4616 |
[32m[20221214 14:27:35 @agent_ppo2.py:185][0m |          -0.0060 |          66.8253 |         -96.2706 |
[32m[20221214 14:27:35 @agent_ppo2.py:185][0m |          -0.0038 |          65.7300 |         -96.3425 |
[32m[20221214 14:27:35 @agent_ppo2.py:185][0m |          -0.0030 |          65.1295 |         -96.4166 |
[32m[20221214 14:27:35 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:27:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.47
[32m[20221214 14:27:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 670.32
[32m[20221214 14:27:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 674.77
[32m[20221214 14:27:35 @agent_ppo2.py:143][0m Total time:      29.56 min
[32m[20221214 14:27:35 @agent_ppo2.py:145][0m 2719744 total steps have happened
[32m[20221214 14:27:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1328 --------------------------#
[32m[20221214 14:27:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:36 @agent_ppo2.py:185][0m |           0.0004 |         161.0987 |         -94.9308 |
[32m[20221214 14:27:36 @agent_ppo2.py:185][0m |          -0.0031 |         153.8179 |         -95.0531 |
[32m[20221214 14:27:36 @agent_ppo2.py:185][0m |          -0.0023 |         150.6432 |         -95.0091 |
[32m[20221214 14:27:36 @agent_ppo2.py:185][0m |          -0.0042 |         149.0513 |         -95.1511 |
[32m[20221214 14:27:36 @agent_ppo2.py:185][0m |           0.0010 |         148.8653 |         -95.4462 |
[32m[20221214 14:27:36 @agent_ppo2.py:185][0m |          -0.0019 |         148.0035 |         -95.5149 |
[32m[20221214 14:27:36 @agent_ppo2.py:185][0m |          -0.0035 |         146.7129 |         -95.4392 |
[32m[20221214 14:27:36 @agent_ppo2.py:185][0m |           0.0071 |         157.4572 |         -95.6229 |
[32m[20221214 14:27:37 @agent_ppo2.py:185][0m |          -0.0020 |         146.2906 |         -95.6282 |
[32m[20221214 14:27:37 @agent_ppo2.py:185][0m |           0.0005 |         145.4257 |         -96.0172 |
[32m[20221214 14:27:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:27:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 669.64
[32m[20221214 14:27:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 683.94
[32m[20221214 14:27:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 674.43
[32m[20221214 14:27:37 @agent_ppo2.py:143][0m Total time:      29.58 min
[32m[20221214 14:27:37 @agent_ppo2.py:145][0m 2721792 total steps have happened
[32m[20221214 14:27:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1329 --------------------------#
[32m[20221214 14:27:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:37 @agent_ppo2.py:185][0m |          -0.0011 |         190.5576 |         -97.2726 |
[32m[20221214 14:27:37 @agent_ppo2.py:185][0m |          -0.0051 |         177.5732 |         -97.5307 |
[32m[20221214 14:27:37 @agent_ppo2.py:185][0m |          -0.0013 |         173.6586 |         -97.6301 |
[32m[20221214 14:27:37 @agent_ppo2.py:185][0m |          -0.0052 |         170.5533 |         -97.5536 |
[32m[20221214 14:27:38 @agent_ppo2.py:185][0m |           0.0009 |         170.1716 |         -97.6525 |
[32m[20221214 14:27:38 @agent_ppo2.py:185][0m |           0.0004 |         167.8209 |         -97.5644 |
[32m[20221214 14:27:38 @agent_ppo2.py:185][0m |          -0.0000 |         166.5110 |         -97.6118 |
[32m[20221214 14:27:38 @agent_ppo2.py:185][0m |          -0.0013 |         166.1049 |         -97.4865 |
[32m[20221214 14:27:38 @agent_ppo2.py:185][0m |           0.0121 |         184.9062 |         -97.6933 |
[32m[20221214 14:27:38 @agent_ppo2.py:185][0m |          -0.0046 |         165.3591 |         -97.6989 |
[32m[20221214 14:27:38 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:27:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 682.20
[32m[20221214 14:27:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.41
[32m[20221214 14:27:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.85
[32m[20221214 14:27:38 @agent_ppo2.py:143][0m Total time:      29.60 min
[32m[20221214 14:27:38 @agent_ppo2.py:145][0m 2723840 total steps have happened
[32m[20221214 14:27:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1330 --------------------------#
[32m[20221214 14:27:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:27:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:39 @agent_ppo2.py:185][0m |          -0.0012 |         109.6781 |         -96.1846 |
[32m[20221214 14:27:39 @agent_ppo2.py:185][0m |          -0.0031 |          96.1444 |         -95.9554 |
[32m[20221214 14:27:39 @agent_ppo2.py:185][0m |           0.0046 |          90.2653 |         -95.9536 |
[32m[20221214 14:27:39 @agent_ppo2.py:185][0m |          -0.0034 |          83.8251 |         -95.8032 |
[32m[20221214 14:27:39 @agent_ppo2.py:185][0m |          -0.0031 |          80.0825 |         -95.6076 |
[32m[20221214 14:27:39 @agent_ppo2.py:185][0m |          -0.0055 |          77.0709 |         -95.7870 |
[32m[20221214 14:27:39 @agent_ppo2.py:185][0m |          -0.0058 |          74.4776 |         -95.4332 |
[32m[20221214 14:27:39 @agent_ppo2.py:185][0m |          -0.0041 |          72.9783 |         -95.7762 |
[32m[20221214 14:27:39 @agent_ppo2.py:185][0m |           0.0029 |          71.3647 |         -95.5928 |
[32m[20221214 14:27:39 @agent_ppo2.py:185][0m |          -0.0031 |          70.1861 |         -95.8341 |
[32m[20221214 14:27:39 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:27:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 608.92
[32m[20221214 14:27:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 673.61
[32m[20221214 14:27:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.41
[32m[20221214 14:27:40 @agent_ppo2.py:143][0m Total time:      29.62 min
[32m[20221214 14:27:40 @agent_ppo2.py:145][0m 2725888 total steps have happened
[32m[20221214 14:27:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1331 --------------------------#
[32m[20221214 14:27:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:40 @agent_ppo2.py:185][0m |          -0.0055 |          53.7077 |         -96.6310 |
[32m[20221214 14:27:40 @agent_ppo2.py:185][0m |           0.0007 |          51.8396 |         -96.4829 |
[32m[20221214 14:27:40 @agent_ppo2.py:185][0m |          -0.0086 |          43.2331 |         -96.3394 |
[32m[20221214 14:27:40 @agent_ppo2.py:185][0m |          -0.0057 |          40.9453 |         -96.2591 |
[32m[20221214 14:27:40 @agent_ppo2.py:185][0m |          -0.0101 |          39.4597 |         -96.1162 |
[32m[20221214 14:27:40 @agent_ppo2.py:185][0m |          -0.0067 |          37.8086 |         -96.3039 |
[32m[20221214 14:27:40 @agent_ppo2.py:185][0m |          -0.0081 |          36.7005 |         -96.2560 |
[32m[20221214 14:27:41 @agent_ppo2.py:185][0m |          -0.0015 |          35.8197 |         -96.1910 |
[32m[20221214 14:27:41 @agent_ppo2.py:185][0m |          -0.0106 |          35.0895 |         -96.1893 |
[32m[20221214 14:27:41 @agent_ppo2.py:185][0m |          -0.0099 |          34.4200 |         -96.2768 |
[32m[20221214 14:27:41 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 14:27:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 566.05
[32m[20221214 14:27:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 681.40
[32m[20221214 14:27:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.67
[32m[20221214 14:27:41 @agent_ppo2.py:143][0m Total time:      29.65 min
[32m[20221214 14:27:41 @agent_ppo2.py:145][0m 2727936 total steps have happened
[32m[20221214 14:27:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1332 --------------------------#
[32m[20221214 14:27:41 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:27:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:41 @agent_ppo2.py:185][0m |           0.0004 |          85.7639 |         -98.6718 |
[32m[20221214 14:27:41 @agent_ppo2.py:185][0m |          -0.0043 |          72.1093 |         -98.8209 |
[32m[20221214 14:27:42 @agent_ppo2.py:185][0m |           0.0016 |          66.9816 |         -98.7909 |
[32m[20221214 14:27:42 @agent_ppo2.py:185][0m |           0.0098 |          71.2660 |         -98.9782 |
[32m[20221214 14:27:42 @agent_ppo2.py:185][0m |          -0.0070 |          63.6056 |         -98.7477 |
[32m[20221214 14:27:42 @agent_ppo2.py:185][0m |           0.0014 |          62.3877 |         -98.9462 |
[32m[20221214 14:27:42 @agent_ppo2.py:185][0m |          -0.0009 |          61.6700 |         -98.8861 |
[32m[20221214 14:27:42 @agent_ppo2.py:185][0m |          -0.0002 |          60.4933 |         -99.0556 |
[32m[20221214 14:27:42 @agent_ppo2.py:185][0m |          -0.0047 |          60.7467 |         -99.0913 |
[32m[20221214 14:27:42 @agent_ppo2.py:185][0m |          -0.0042 |          60.3928 |         -99.1611 |
[32m[20221214 14:27:42 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:27:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.49
[32m[20221214 14:27:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 687.62
[32m[20221214 14:27:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 354.47
[32m[20221214 14:27:42 @agent_ppo2.py:143][0m Total time:      29.67 min
[32m[20221214 14:27:42 @agent_ppo2.py:145][0m 2729984 total steps have happened
[32m[20221214 14:27:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1333 --------------------------#
[32m[20221214 14:27:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:27:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:43 @agent_ppo2.py:185][0m |           0.0041 |          40.9826 |         -97.8192 |
[32m[20221214 14:27:43 @agent_ppo2.py:185][0m |          -0.0026 |          32.5152 |         -97.9005 |
[32m[20221214 14:27:43 @agent_ppo2.py:185][0m |          -0.0059 |          30.8148 |         -97.7055 |
[32m[20221214 14:27:43 @agent_ppo2.py:185][0m |           0.0095 |          30.4253 |         -97.8968 |
[32m[20221214 14:27:43 @agent_ppo2.py:185][0m |           0.0056 |          28.9557 |         -97.4068 |
[32m[20221214 14:27:43 @agent_ppo2.py:185][0m |           0.0182 |          33.4591 |         -97.4244 |
[32m[20221214 14:27:43 @agent_ppo2.py:185][0m |          -0.0003 |          27.9015 |         -97.5954 |
[32m[20221214 14:27:43 @agent_ppo2.py:185][0m |          -0.0053 |          26.4647 |         -97.6647 |
[32m[20221214 14:27:43 @agent_ppo2.py:185][0m |          -0.0028 |          27.3913 |         -97.5714 |
[32m[20221214 14:27:43 @agent_ppo2.py:185][0m |           0.0056 |          26.1090 |         -97.5556 |
[32m[20221214 14:27:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:27:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.46
[32m[20221214 14:27:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 681.93
[32m[20221214 14:27:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.15
[32m[20221214 14:27:44 @agent_ppo2.py:143][0m Total time:      29.69 min
[32m[20221214 14:27:44 @agent_ppo2.py:145][0m 2732032 total steps have happened
[32m[20221214 14:27:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1334 --------------------------#
[32m[20221214 14:27:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:44 @agent_ppo2.py:185][0m |          -0.0023 |          36.8250 |         -97.8649 |
[32m[20221214 14:27:44 @agent_ppo2.py:185][0m |          -0.0041 |          30.1213 |         -97.7848 |
[32m[20221214 14:27:44 @agent_ppo2.py:185][0m |          -0.0028 |          28.9445 |         -97.6207 |
[32m[20221214 14:27:44 @agent_ppo2.py:185][0m |          -0.0047 |          27.8412 |         -97.8554 |
[32m[20221214 14:27:44 @agent_ppo2.py:185][0m |           0.0019 |          27.0392 |         -97.7875 |
[32m[20221214 14:27:44 @agent_ppo2.py:185][0m |          -0.0009 |          26.5357 |         -98.0873 |
[32m[20221214 14:27:45 @agent_ppo2.py:185][0m |           0.0015 |          26.4983 |         -97.8565 |
[32m[20221214 14:27:45 @agent_ppo2.py:185][0m |          -0.0068 |          25.7746 |         -98.1019 |
[32m[20221214 14:27:45 @agent_ppo2.py:185][0m |           0.0021 |          25.5788 |         -97.7126 |
[32m[20221214 14:27:45 @agent_ppo2.py:185][0m |           0.0003 |          25.2114 |         -98.1253 |
[32m[20221214 14:27:45 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:27:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.18
[32m[20221214 14:27:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 666.63
[32m[20221214 14:27:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.66
[32m[20221214 14:27:45 @agent_ppo2.py:143][0m Total time:      29.71 min
[32m[20221214 14:27:45 @agent_ppo2.py:145][0m 2734080 total steps have happened
[32m[20221214 14:27:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1335 --------------------------#
[32m[20221214 14:27:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:27:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:45 @agent_ppo2.py:185][0m |          -0.0070 |          12.4624 |         -97.8047 |
[32m[20221214 14:27:45 @agent_ppo2.py:185][0m |          -0.0032 |          10.7749 |         -97.5171 |
[32m[20221214 14:27:45 @agent_ppo2.py:185][0m |          -0.0021 |          10.1845 |         -97.7440 |
[32m[20221214 14:27:46 @agent_ppo2.py:185][0m |          -0.0011 |           9.6305 |         -97.6218 |
[32m[20221214 14:27:46 @agent_ppo2.py:185][0m |          -0.0029 |           9.2266 |         -97.8941 |
[32m[20221214 14:27:46 @agent_ppo2.py:185][0m |           0.0012 |           8.8601 |         -97.9468 |
[32m[20221214 14:27:46 @agent_ppo2.py:185][0m |           0.0036 |           8.6494 |         -97.9057 |
[32m[20221214 14:27:46 @agent_ppo2.py:185][0m |          -0.0023 |           8.3895 |         -98.1787 |
[32m[20221214 14:27:46 @agent_ppo2.py:185][0m |          -0.0058 |           8.2816 |         -98.0291 |
[32m[20221214 14:27:46 @agent_ppo2.py:185][0m |          -0.0058 |           8.1192 |         -97.9434 |
[32m[20221214 14:27:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:27:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.95
[32m[20221214 14:27:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.05
[32m[20221214 14:27:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 671.90
[32m[20221214 14:27:46 @agent_ppo2.py:143][0m Total time:      29.74 min
[32m[20221214 14:27:46 @agent_ppo2.py:145][0m 2736128 total steps have happened
[32m[20221214 14:27:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1336 --------------------------#
[32m[20221214 14:27:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:47 @agent_ppo2.py:185][0m |           0.0031 |          38.4588 |         -97.5139 |
[32m[20221214 14:27:47 @agent_ppo2.py:185][0m |          -0.0035 |          35.6721 |         -97.4545 |
[32m[20221214 14:27:47 @agent_ppo2.py:185][0m |          -0.0064 |          33.9349 |         -97.2852 |
[32m[20221214 14:27:47 @agent_ppo2.py:185][0m |          -0.0016 |          32.2423 |         -97.3246 |
[32m[20221214 14:27:47 @agent_ppo2.py:185][0m |          -0.0060 |          32.0630 |         -97.4734 |
[32m[20221214 14:27:47 @agent_ppo2.py:185][0m |          -0.0030 |          31.0912 |         -97.4807 |
[32m[20221214 14:27:47 @agent_ppo2.py:185][0m |          -0.0038 |          31.0960 |         -97.4721 |
[32m[20221214 14:27:47 @agent_ppo2.py:185][0m |          -0.0008 |          30.5767 |         -97.4596 |
[32m[20221214 14:27:47 @agent_ppo2.py:185][0m |          -0.0048 |          30.1934 |         -97.3577 |
[32m[20221214 14:27:47 @agent_ppo2.py:185][0m |          -0.0003 |          30.2708 |         -97.6112 |
[32m[20221214 14:27:47 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:27:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.72
[32m[20221214 14:27:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 683.34
[32m[20221214 14:27:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.19
[32m[20221214 14:27:48 @agent_ppo2.py:143][0m Total time:      29.76 min
[32m[20221214 14:27:48 @agent_ppo2.py:145][0m 2738176 total steps have happened
[32m[20221214 14:27:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1337 --------------------------#
[32m[20221214 14:27:48 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:27:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:48 @agent_ppo2.py:185][0m |           0.0054 |          47.5244 |        -100.0078 |
[32m[20221214 14:27:48 @agent_ppo2.py:185][0m |          -0.0016 |          39.5279 |        -100.2712 |
[32m[20221214 14:27:48 @agent_ppo2.py:185][0m |          -0.0037 |          37.1647 |         -99.9162 |
[32m[20221214 14:27:48 @agent_ppo2.py:185][0m |          -0.0012 |          36.3361 |        -100.1522 |
[32m[20221214 14:27:48 @agent_ppo2.py:185][0m |           0.0001 |          34.9441 |         -99.9329 |
[32m[20221214 14:27:48 @agent_ppo2.py:185][0m |          -0.0022 |          33.9865 |        -100.1378 |
[32m[20221214 14:27:49 @agent_ppo2.py:185][0m |          -0.0022 |          33.5178 |        -100.1528 |
[32m[20221214 14:27:49 @agent_ppo2.py:185][0m |          -0.0023 |          33.1567 |        -100.2575 |
[32m[20221214 14:27:49 @agent_ppo2.py:185][0m |          -0.0038 |          32.6815 |        -100.0980 |
[32m[20221214 14:27:49 @agent_ppo2.py:185][0m |          -0.0009 |          32.5540 |        -100.1088 |
[32m[20221214 14:27:49 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:27:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.62
[32m[20221214 14:27:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 692.11
[32m[20221214 14:27:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 689.86
[32m[20221214 14:27:49 @agent_ppo2.py:143][0m Total time:      29.78 min
[32m[20221214 14:27:49 @agent_ppo2.py:145][0m 2740224 total steps have happened
[32m[20221214 14:27:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1338 --------------------------#
[32m[20221214 14:27:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:49 @agent_ppo2.py:185][0m |           0.0018 |          98.9993 |        -100.5473 |
[32m[20221214 14:27:49 @agent_ppo2.py:185][0m |          -0.0016 |          90.5717 |        -100.4004 |
[32m[20221214 14:27:50 @agent_ppo2.py:185][0m |          -0.0009 |          88.6314 |        -100.2352 |
[32m[20221214 14:27:50 @agent_ppo2.py:185][0m |          -0.0080 |          87.2576 |        -100.1130 |
[32m[20221214 14:27:50 @agent_ppo2.py:185][0m |          -0.0051 |          86.6180 |        -100.0998 |
[32m[20221214 14:27:50 @agent_ppo2.py:185][0m |          -0.0054 |          85.6490 |         -99.9922 |
[32m[20221214 14:27:50 @agent_ppo2.py:185][0m |          -0.0047 |          84.7623 |        -100.1961 |
[32m[20221214 14:27:50 @agent_ppo2.py:185][0m |          -0.0057 |          84.1296 |        -100.0201 |
[32m[20221214 14:27:50 @agent_ppo2.py:185][0m |          -0.0004 |          83.6863 |        -100.1824 |
[32m[20221214 14:27:50 @agent_ppo2.py:185][0m |          -0.0016 |          83.6439 |         -99.9362 |
[32m[20221214 14:27:50 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:27:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 639.71
[32m[20221214 14:27:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 709.42
[32m[20221214 14:27:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 694.42
[32m[20221214 14:27:50 @agent_ppo2.py:143][0m Total time:      29.80 min
[32m[20221214 14:27:50 @agent_ppo2.py:145][0m 2742272 total steps have happened
[32m[20221214 14:27:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1339 --------------------------#
[32m[20221214 14:27:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:51 @agent_ppo2.py:185][0m |           0.0004 |         154.5397 |         -99.9466 |
[32m[20221214 14:27:51 @agent_ppo2.py:185][0m |          -0.0008 |         143.0964 |        -100.1288 |
[32m[20221214 14:27:51 @agent_ppo2.py:185][0m |           0.0018 |         140.8848 |         -99.9051 |
[32m[20221214 14:27:51 @agent_ppo2.py:185][0m |          -0.0025 |         136.8099 |        -100.3666 |
[32m[20221214 14:27:51 @agent_ppo2.py:185][0m |          -0.0013 |         134.6137 |        -100.1526 |
[32m[20221214 14:27:51 @agent_ppo2.py:185][0m |          -0.0020 |         132.7836 |        -100.2076 |
[32m[20221214 14:27:51 @agent_ppo2.py:185][0m |          -0.0000 |         131.8671 |        -100.1574 |
[32m[20221214 14:27:51 @agent_ppo2.py:185][0m |           0.0025 |         132.4925 |        -100.4320 |
[32m[20221214 14:27:51 @agent_ppo2.py:185][0m |          -0.0030 |         128.9370 |        -100.4779 |
[32m[20221214 14:27:52 @agent_ppo2.py:185][0m |          -0.0023 |         127.8472 |        -100.4192 |
[32m[20221214 14:27:52 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:27:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 688.41
[32m[20221214 14:27:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 699.62
[32m[20221214 14:27:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 694.85
[32m[20221214 14:27:52 @agent_ppo2.py:143][0m Total time:      29.83 min
[32m[20221214 14:27:52 @agent_ppo2.py:145][0m 2744320 total steps have happened
[32m[20221214 14:27:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1340 --------------------------#
[32m[20221214 14:27:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:27:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:52 @agent_ppo2.py:185][0m |          -0.0010 |         148.2613 |         -99.9257 |
[32m[20221214 14:27:52 @agent_ppo2.py:185][0m |          -0.0015 |         141.9631 |         -99.9519 |
[32m[20221214 14:27:52 @agent_ppo2.py:185][0m |           0.0030 |         140.3064 |         -99.8702 |
[32m[20221214 14:27:52 @agent_ppo2.py:185][0m |           0.0008 |         138.0182 |         -99.7907 |
[32m[20221214 14:27:52 @agent_ppo2.py:185][0m |          -0.0000 |         136.8426 |        -100.0552 |
[32m[20221214 14:27:52 @agent_ppo2.py:185][0m |           0.0061 |         137.0977 |         -99.7927 |
[32m[20221214 14:27:53 @agent_ppo2.py:185][0m |          -0.0021 |         134.1459 |        -100.0452 |
[32m[20221214 14:27:53 @agent_ppo2.py:185][0m |           0.0008 |         134.2332 |        -100.1112 |
[32m[20221214 14:27:53 @agent_ppo2.py:185][0m |           0.0002 |         132.8804 |        -100.0386 |
[32m[20221214 14:27:53 @agent_ppo2.py:185][0m |           0.0004 |         133.0559 |        -100.1015 |
[32m[20221214 14:27:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:27:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 693.88
[32m[20221214 14:27:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 702.69
[32m[20221214 14:27:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 699.50
[32m[20221214 14:27:53 @agent_ppo2.py:143][0m Total time:      29.85 min
[32m[20221214 14:27:53 @agent_ppo2.py:145][0m 2746368 total steps have happened
[32m[20221214 14:27:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1341 --------------------------#
[32m[20221214 14:27:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:27:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:53 @agent_ppo2.py:185][0m |          -0.0001 |         139.4790 |         -98.9055 |
[32m[20221214 14:27:53 @agent_ppo2.py:185][0m |           0.0009 |         132.7484 |         -98.9761 |
[32m[20221214 14:27:53 @agent_ppo2.py:185][0m |           0.0028 |         130.4729 |         -98.7888 |
[32m[20221214 14:27:54 @agent_ppo2.py:185][0m |           0.0007 |         129.6405 |         -98.7140 |
[32m[20221214 14:27:54 @agent_ppo2.py:185][0m |           0.0006 |         129.1050 |         -98.7885 |
[32m[20221214 14:27:54 @agent_ppo2.py:185][0m |           0.0010 |         128.5538 |         -98.4059 |
[32m[20221214 14:27:54 @agent_ppo2.py:185][0m |          -0.0020 |         128.4211 |         -98.5772 |
[32m[20221214 14:27:54 @agent_ppo2.py:185][0m |          -0.0035 |         127.4013 |         -98.5939 |
[32m[20221214 14:27:54 @agent_ppo2.py:185][0m |           0.0095 |         132.3626 |         -98.3395 |
[32m[20221214 14:27:54 @agent_ppo2.py:185][0m |           0.0070 |         129.6601 |         -98.5589 |
[32m[20221214 14:27:54 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:27:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 695.78
[32m[20221214 14:27:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.82
[32m[20221214 14:27:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 692.06
[32m[20221214 14:27:54 @agent_ppo2.py:143][0m Total time:      29.87 min
[32m[20221214 14:27:54 @agent_ppo2.py:145][0m 2748416 total steps have happened
[32m[20221214 14:27:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1342 --------------------------#
[32m[20221214 14:27:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:27:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:55 @agent_ppo2.py:185][0m |          -0.0009 |         122.6078 |         -99.8103 |
[32m[20221214 14:27:55 @agent_ppo2.py:185][0m |           0.0010 |         109.2994 |        -100.1025 |
[32m[20221214 14:27:55 @agent_ppo2.py:185][0m |          -0.0031 |         103.4616 |        -100.3916 |
[32m[20221214 14:27:55 @agent_ppo2.py:185][0m |           0.0032 |         101.9972 |        -100.3983 |
[32m[20221214 14:27:55 @agent_ppo2.py:185][0m |          -0.0013 |          99.3203 |        -100.4615 |
[32m[20221214 14:27:55 @agent_ppo2.py:185][0m |          -0.0015 |          97.8034 |        -100.5572 |
[32m[20221214 14:27:55 @agent_ppo2.py:185][0m |           0.0064 |          98.9849 |        -100.5645 |
[32m[20221214 14:27:55 @agent_ppo2.py:185][0m |          -0.0003 |          96.0951 |        -100.5119 |
[32m[20221214 14:27:55 @agent_ppo2.py:185][0m |          -0.0001 |          95.1616 |        -100.6063 |
[32m[20221214 14:27:56 @agent_ppo2.py:185][0m |           0.0011 |          95.2128 |        -100.5886 |
[32m[20221214 14:27:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:27:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 675.64
[32m[20221214 14:27:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.93
[32m[20221214 14:27:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 704.63
[32m[20221214 14:27:56 @agent_ppo2.py:143][0m Total time:      29.89 min
[32m[20221214 14:27:56 @agent_ppo2.py:145][0m 2750464 total steps have happened
[32m[20221214 14:27:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1343 --------------------------#
[32m[20221214 14:27:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:56 @agent_ppo2.py:185][0m |           0.0008 |         130.1998 |        -102.3908 |
[32m[20221214 14:27:56 @agent_ppo2.py:185][0m |          -0.0045 |         126.3393 |        -102.4214 |
[32m[20221214 14:27:56 @agent_ppo2.py:185][0m |           0.0006 |         123.3953 |        -102.3047 |
[32m[20221214 14:27:56 @agent_ppo2.py:185][0m |           0.0001 |         122.2591 |        -102.3808 |
[32m[20221214 14:27:56 @agent_ppo2.py:185][0m |           0.0003 |         121.2900 |        -102.4028 |
[32m[20221214 14:27:57 @agent_ppo2.py:185][0m |          -0.0006 |         120.4960 |        -102.4314 |
[32m[20221214 14:27:57 @agent_ppo2.py:185][0m |           0.0044 |         123.9505 |        -102.4969 |
[32m[20221214 14:27:57 @agent_ppo2.py:185][0m |          -0.0038 |         119.4472 |        -102.4390 |
[32m[20221214 14:27:57 @agent_ppo2.py:185][0m |          -0.0020 |         118.4943 |        -102.5531 |
[32m[20221214 14:27:57 @agent_ppo2.py:185][0m |          -0.0025 |         118.1951 |        -102.4122 |
[32m[20221214 14:27:57 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:27:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 697.73
[32m[20221214 14:27:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.76
[32m[20221214 14:27:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 701.02
[32m[20221214 14:27:57 @agent_ppo2.py:143][0m Total time:      29.92 min
[32m[20221214 14:27:57 @agent_ppo2.py:145][0m 2752512 total steps have happened
[32m[20221214 14:27:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1344 --------------------------#
[32m[20221214 14:27:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:57 @agent_ppo2.py:185][0m |          -0.0004 |         136.4133 |        -101.3965 |
[32m[20221214 14:27:58 @agent_ppo2.py:185][0m |          -0.0021 |         130.8151 |        -101.3952 |
[32m[20221214 14:27:58 @agent_ppo2.py:185][0m |          -0.0036 |         128.5512 |        -101.4989 |
[32m[20221214 14:27:58 @agent_ppo2.py:185][0m |          -0.0010 |         127.5224 |        -101.6403 |
[32m[20221214 14:27:58 @agent_ppo2.py:185][0m |           0.0009 |         125.7053 |        -101.7911 |
[32m[20221214 14:27:58 @agent_ppo2.py:185][0m |          -0.0020 |         126.1080 |        -101.8666 |
[32m[20221214 14:27:58 @agent_ppo2.py:185][0m |          -0.0016 |         125.3278 |        -101.9490 |
[32m[20221214 14:27:58 @agent_ppo2.py:185][0m |          -0.0055 |         125.4877 |        -101.8217 |
[32m[20221214 14:27:58 @agent_ppo2.py:185][0m |          -0.0048 |         125.9255 |        -101.6629 |
[32m[20221214 14:27:58 @agent_ppo2.py:185][0m |          -0.0007 |         124.9241 |        -102.0612 |
[32m[20221214 14:27:58 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:27:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 691.96
[32m[20221214 14:27:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.38
[32m[20221214 14:27:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 714.52
[32m[20221214 14:27:58 @agent_ppo2.py:143][0m Total time:      29.94 min
[32m[20221214 14:27:58 @agent_ppo2.py:145][0m 2754560 total steps have happened
[32m[20221214 14:27:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1345 --------------------------#
[32m[20221214 14:27:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:27:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:27:59 @agent_ppo2.py:185][0m |          -0.0015 |         163.1148 |        -102.7607 |
[32m[20221214 14:27:59 @agent_ppo2.py:185][0m |          -0.0010 |         155.2466 |        -102.5134 |
[32m[20221214 14:27:59 @agent_ppo2.py:185][0m |           0.0043 |         152.9305 |        -102.9006 |
[32m[20221214 14:27:59 @agent_ppo2.py:185][0m |          -0.0005 |         151.4572 |        -102.8623 |
[32m[20221214 14:27:59 @agent_ppo2.py:185][0m |           0.0027 |         149.6684 |        -102.7230 |
[32m[20221214 14:27:59 @agent_ppo2.py:185][0m |          -0.0006 |         149.2438 |        -103.1910 |
[32m[20221214 14:27:59 @agent_ppo2.py:185][0m |          -0.0035 |         148.7337 |        -103.0934 |
[32m[20221214 14:27:59 @agent_ppo2.py:185][0m |           0.0002 |         147.6816 |        -103.4542 |
[32m[20221214 14:27:59 @agent_ppo2.py:185][0m |           0.0001 |         147.4491 |        -103.2567 |
[32m[20221214 14:28:00 @agent_ppo2.py:185][0m |          -0.0039 |         146.3602 |        -103.3636 |
[32m[20221214 14:28:00 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:28:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 712.54
[32m[20221214 14:28:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.54
[32m[20221214 14:28:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 719.09
[32m[20221214 14:28:00 @agent_ppo2.py:143][0m Total time:      29.96 min
[32m[20221214 14:28:00 @agent_ppo2.py:145][0m 2756608 total steps have happened
[32m[20221214 14:28:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1346 --------------------------#
[32m[20221214 14:28:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:00 @agent_ppo2.py:185][0m |          -0.0037 |         162.2550 |        -104.0465 |
[32m[20221214 14:28:00 @agent_ppo2.py:185][0m |           0.0007 |         148.2716 |        -104.0016 |
[32m[20221214 14:28:00 @agent_ppo2.py:185][0m |           0.0002 |         143.4489 |        -104.0046 |
[32m[20221214 14:28:00 @agent_ppo2.py:185][0m |           0.0006 |         140.2605 |        -104.1907 |
[32m[20221214 14:28:00 @agent_ppo2.py:185][0m |          -0.0005 |         138.2908 |        -103.9215 |
[32m[20221214 14:28:01 @agent_ppo2.py:185][0m |          -0.0009 |         137.1507 |        -104.0189 |
[32m[20221214 14:28:01 @agent_ppo2.py:185][0m |           0.0025 |         135.7066 |        -104.0840 |
[32m[20221214 14:28:01 @agent_ppo2.py:185][0m |          -0.0034 |         135.4363 |        -103.5235 |
[32m[20221214 14:28:01 @agent_ppo2.py:185][0m |           0.0041 |         134.0106 |        -103.4183 |
[32m[20221214 14:28:01 @agent_ppo2.py:185][0m |          -0.0021 |         133.3889 |        -103.6767 |
[32m[20221214 14:28:01 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:28:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.40
[32m[20221214 14:28:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 718.61
[32m[20221214 14:28:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 714.94
[32m[20221214 14:28:01 @agent_ppo2.py:143][0m Total time:      29.98 min
[32m[20221214 14:28:01 @agent_ppo2.py:145][0m 2758656 total steps have happened
[32m[20221214 14:28:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1347 --------------------------#
[32m[20221214 14:28:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:01 @agent_ppo2.py:185][0m |           0.0000 |         140.8569 |        -104.1947 |
[32m[20221214 14:28:02 @agent_ppo2.py:185][0m |           0.0003 |         133.9732 |        -104.0752 |
[32m[20221214 14:28:02 @agent_ppo2.py:185][0m |           0.0017 |         133.1709 |        -104.5310 |
[32m[20221214 14:28:02 @agent_ppo2.py:185][0m |           0.0025 |         131.7494 |        -104.4036 |
[32m[20221214 14:28:02 @agent_ppo2.py:185][0m |           0.0006 |         130.4853 |        -104.5477 |
[32m[20221214 14:28:02 @agent_ppo2.py:185][0m |          -0.0012 |         130.1013 |        -104.5818 |
[32m[20221214 14:28:02 @agent_ppo2.py:185][0m |          -0.0023 |         129.3174 |        -104.6785 |
[32m[20221214 14:28:02 @agent_ppo2.py:185][0m |           0.0018 |         130.1207 |        -104.8003 |
[32m[20221214 14:28:02 @agent_ppo2.py:185][0m |          -0.0018 |         128.0634 |        -104.8921 |
[32m[20221214 14:28:02 @agent_ppo2.py:185][0m |          -0.0037 |         128.0470 |        -104.9010 |
[32m[20221214 14:28:02 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:28:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 715.38
[32m[20221214 14:28:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 722.15
[32m[20221214 14:28:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 722.74
[32m[20221214 14:28:02 @agent_ppo2.py:143][0m Total time:      30.01 min
[32m[20221214 14:28:02 @agent_ppo2.py:145][0m 2760704 total steps have happened
[32m[20221214 14:28:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1348 --------------------------#
[32m[20221214 14:28:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:03 @agent_ppo2.py:185][0m |           0.0016 |         139.1778 |        -103.2445 |
[32m[20221214 14:28:03 @agent_ppo2.py:185][0m |          -0.0054 |         132.8734 |        -103.0979 |
[32m[20221214 14:28:03 @agent_ppo2.py:185][0m |           0.0054 |         136.7683 |        -102.6407 |
[32m[20221214 14:28:03 @agent_ppo2.py:185][0m |          -0.0010 |         130.8806 |        -102.6355 |
[32m[20221214 14:28:03 @agent_ppo2.py:185][0m |           0.0034 |         133.1518 |        -102.4516 |
[32m[20221214 14:28:03 @agent_ppo2.py:185][0m |           0.0001 |         129.8350 |        -102.1166 |
[32m[20221214 14:28:03 @agent_ppo2.py:185][0m |          -0.0053 |         128.7637 |        -102.2345 |
[32m[20221214 14:28:03 @agent_ppo2.py:185][0m |          -0.0043 |         128.0433 |        -102.3095 |
[32m[20221214 14:28:04 @agent_ppo2.py:185][0m |          -0.0021 |         128.8413 |        -102.1618 |
[32m[20221214 14:28:04 @agent_ppo2.py:185][0m |          -0.0021 |         127.6545 |        -102.0165 |
[32m[20221214 14:28:04 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:28:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 716.69
[32m[20221214 14:28:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 724.19
[32m[20221214 14:28:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 725.80
[32m[20221214 14:28:04 @agent_ppo2.py:143][0m Total time:      30.03 min
[32m[20221214 14:28:04 @agent_ppo2.py:145][0m 2762752 total steps have happened
[32m[20221214 14:28:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1349 --------------------------#
[32m[20221214 14:28:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:04 @agent_ppo2.py:185][0m |           0.0015 |         156.1256 |        -103.2043 |
[32m[20221214 14:28:04 @agent_ppo2.py:185][0m |           0.0014 |         144.9675 |        -103.1160 |
[32m[20221214 14:28:04 @agent_ppo2.py:185][0m |          -0.0006 |         143.8909 |        -103.2370 |
[32m[20221214 14:28:04 @agent_ppo2.py:185][0m |           0.0007 |         143.8841 |        -102.9396 |
[32m[20221214 14:28:04 @agent_ppo2.py:185][0m |           0.0086 |         149.7721 |        -103.0335 |
[32m[20221214 14:28:05 @agent_ppo2.py:185][0m |          -0.0022 |         140.7244 |        -103.0107 |
[32m[20221214 14:28:05 @agent_ppo2.py:185][0m |          -0.0011 |         139.1863 |        -102.9698 |
[32m[20221214 14:28:05 @agent_ppo2.py:185][0m |           0.0036 |         139.5895 |        -102.8445 |
[32m[20221214 14:28:05 @agent_ppo2.py:185][0m |          -0.0003 |         137.8429 |        -102.6848 |
[32m[20221214 14:28:05 @agent_ppo2.py:185][0m |           0.0018 |         136.8559 |        -102.7428 |
[32m[20221214 14:28:05 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:28:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 722.48
[32m[20221214 14:28:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 726.58
[32m[20221214 14:28:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 725.86
[32m[20221214 14:28:05 @agent_ppo2.py:143][0m Total time:      30.05 min
[32m[20221214 14:28:05 @agent_ppo2.py:145][0m 2764800 total steps have happened
[32m[20221214 14:28:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1350 --------------------------#
[32m[20221214 14:28:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:28:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:05 @agent_ppo2.py:185][0m |           0.0023 |         135.4216 |        -100.2958 |
[32m[20221214 14:28:06 @agent_ppo2.py:185][0m |           0.0004 |         125.3453 |        -100.6352 |
[32m[20221214 14:28:06 @agent_ppo2.py:185][0m |          -0.0004 |         125.5040 |        -100.7940 |
[32m[20221214 14:28:06 @agent_ppo2.py:185][0m |          -0.0023 |         122.9722 |        -100.5675 |
[32m[20221214 14:28:06 @agent_ppo2.py:185][0m |          -0.0008 |         123.5874 |        -100.7586 |
[32m[20221214 14:28:06 @agent_ppo2.py:185][0m |          -0.0035 |         122.0666 |        -100.6242 |
[32m[20221214 14:28:06 @agent_ppo2.py:185][0m |           0.0001 |         122.0151 |        -100.6745 |
[32m[20221214 14:28:06 @agent_ppo2.py:185][0m |          -0.0011 |         121.4523 |        -100.9078 |
[32m[20221214 14:28:06 @agent_ppo2.py:185][0m |          -0.0006 |         121.5002 |        -100.9422 |
[32m[20221214 14:28:06 @agent_ppo2.py:185][0m |          -0.0020 |         121.2348 |        -101.3508 |
[32m[20221214 14:28:06 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:28:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 720.04
[32m[20221214 14:28:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 724.83
[32m[20221214 14:28:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 727.23
[32m[20221214 14:28:06 @agent_ppo2.py:143][0m Total time:      30.07 min
[32m[20221214 14:28:06 @agent_ppo2.py:145][0m 2766848 total steps have happened
[32m[20221214 14:28:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1351 --------------------------#
[32m[20221214 14:28:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:07 @agent_ppo2.py:185][0m |           0.0020 |         130.3622 |        -103.1579 |
[32m[20221214 14:28:07 @agent_ppo2.py:185][0m |           0.0007 |         126.2555 |        -103.6654 |
[32m[20221214 14:28:07 @agent_ppo2.py:185][0m |          -0.0010 |         125.1769 |        -103.1454 |
[32m[20221214 14:28:07 @agent_ppo2.py:185][0m |          -0.0014 |         124.4562 |        -103.7697 |
[32m[20221214 14:28:07 @agent_ppo2.py:185][0m |           0.0108 |         137.7834 |        -103.9433 |
[32m[20221214 14:28:07 @agent_ppo2.py:185][0m |           0.0089 |         128.9649 |        -103.6637 |
[32m[20221214 14:28:07 @agent_ppo2.py:185][0m |          -0.0013 |         122.6874 |        -103.7736 |
[32m[20221214 14:28:07 @agent_ppo2.py:185][0m |           0.0025 |         122.1645 |        -104.1316 |
[32m[20221214 14:28:08 @agent_ppo2.py:185][0m |          -0.0055 |         123.0231 |        -104.0177 |
[32m[20221214 14:28:08 @agent_ppo2.py:185][0m |          -0.0026 |         121.5342 |        -104.4776 |
[32m[20221214 14:28:08 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:28:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 722.45
[32m[20221214 14:28:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 724.03
[32m[20221214 14:28:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 727.17
[32m[20221214 14:28:08 @agent_ppo2.py:143][0m Total time:      30.10 min
[32m[20221214 14:28:08 @agent_ppo2.py:145][0m 2768896 total steps have happened
[32m[20221214 14:28:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1352 --------------------------#
[32m[20221214 14:28:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:08 @agent_ppo2.py:185][0m |          -0.0015 |         124.4591 |        -103.8911 |
[32m[20221214 14:28:08 @agent_ppo2.py:185][0m |          -0.0026 |         117.2486 |        -104.3253 |
[32m[20221214 14:28:08 @agent_ppo2.py:185][0m |           0.0031 |         114.7846 |        -104.1282 |
[32m[20221214 14:28:08 @agent_ppo2.py:185][0m |           0.0049 |         112.9337 |        -103.5538 |
[32m[20221214 14:28:09 @agent_ppo2.py:185][0m |           0.0006 |         111.6358 |        -104.4805 |
[32m[20221214 14:28:09 @agent_ppo2.py:185][0m |           0.0003 |         110.6725 |        -104.3201 |
[32m[20221214 14:28:09 @agent_ppo2.py:185][0m |           0.0004 |         110.0258 |        -104.4620 |
[32m[20221214 14:28:09 @agent_ppo2.py:185][0m |           0.0190 |         114.5804 |        -104.8325 |
[32m[20221214 14:28:09 @agent_ppo2.py:185][0m |          -0.0000 |         109.3517 |        -104.8958 |
[32m[20221214 14:28:09 @agent_ppo2.py:185][0m |           0.0009 |         108.4300 |        -104.6661 |
[32m[20221214 14:28:09 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:28:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 720.57
[32m[20221214 14:28:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 725.55
[32m[20221214 14:28:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 729.59
[32m[20221214 14:28:09 @agent_ppo2.py:143][0m Total time:      30.12 min
[32m[20221214 14:28:09 @agent_ppo2.py:145][0m 2770944 total steps have happened
[32m[20221214 14:28:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1353 --------------------------#
[32m[20221214 14:28:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:10 @agent_ppo2.py:185][0m |          -0.0050 |         151.1653 |        -105.7608 |
[32m[20221214 14:28:10 @agent_ppo2.py:185][0m |          -0.0024 |         145.2561 |        -105.4322 |
[32m[20221214 14:28:10 @agent_ppo2.py:185][0m |          -0.0022 |         144.1407 |        -105.0652 |
[32m[20221214 14:28:10 @agent_ppo2.py:185][0m |          -0.0009 |         141.7534 |        -105.1311 |
[32m[20221214 14:28:10 @agent_ppo2.py:185][0m |           0.0000 |         142.0750 |        -105.0604 |
[32m[20221214 14:28:10 @agent_ppo2.py:185][0m |          -0.0059 |         141.5981 |        -105.1714 |
[32m[20221214 14:28:10 @agent_ppo2.py:185][0m |          -0.0038 |         140.6863 |        -104.9665 |
[32m[20221214 14:28:10 @agent_ppo2.py:185][0m |           0.0029 |         142.4414 |        -104.8052 |
[32m[20221214 14:28:10 @agent_ppo2.py:185][0m |          -0.0000 |         139.2801 |        -105.0747 |
[32m[20221214 14:28:10 @agent_ppo2.py:185][0m |          -0.0030 |         139.6922 |        -104.8202 |
[32m[20221214 14:28:10 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:28:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 724.73
[32m[20221214 14:28:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 731.78
[32m[20221214 14:28:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 730.30
[32m[20221214 14:28:11 @agent_ppo2.py:143][0m Total time:      30.14 min
[32m[20221214 14:28:11 @agent_ppo2.py:145][0m 2772992 total steps have happened
[32m[20221214 14:28:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1354 --------------------------#
[32m[20221214 14:28:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:11 @agent_ppo2.py:185][0m |          -0.0016 |         152.0675 |        -105.2046 |
[32m[20221214 14:28:11 @agent_ppo2.py:185][0m |          -0.0017 |         142.1978 |        -105.1722 |
[32m[20221214 14:28:11 @agent_ppo2.py:185][0m |          -0.0031 |         137.3279 |        -105.1383 |
[32m[20221214 14:28:11 @agent_ppo2.py:185][0m |           0.0021 |         134.9965 |        -105.3783 |
[32m[20221214 14:28:11 @agent_ppo2.py:185][0m |          -0.0013 |         133.9252 |        -105.7357 |
[32m[20221214 14:28:11 @agent_ppo2.py:185][0m |           0.0014 |         133.9466 |        -105.7289 |
[32m[20221214 14:28:11 @agent_ppo2.py:185][0m |           0.0028 |         133.2953 |        -105.8763 |
[32m[20221214 14:28:12 @agent_ppo2.py:185][0m |          -0.0040 |         131.3569 |        -105.5813 |
[32m[20221214 14:28:12 @agent_ppo2.py:185][0m |           0.0009 |         132.9911 |        -106.0171 |
[32m[20221214 14:28:12 @agent_ppo2.py:185][0m |          -0.0053 |         131.6664 |        -105.9267 |
[32m[20221214 14:28:12 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:28:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 732.04
[32m[20221214 14:28:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.69
[32m[20221214 14:28:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 728.17
[32m[20221214 14:28:12 @agent_ppo2.py:143][0m Total time:      30.16 min
[32m[20221214 14:28:12 @agent_ppo2.py:145][0m 2775040 total steps have happened
[32m[20221214 14:28:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1355 --------------------------#
[32m[20221214 14:28:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:12 @agent_ppo2.py:185][0m |           0.0018 |         120.0251 |        -107.0956 |
[32m[20221214 14:28:12 @agent_ppo2.py:185][0m |           0.0016 |         113.4820 |        -106.8483 |
[32m[20221214 14:28:12 @agent_ppo2.py:185][0m |          -0.0031 |         112.4903 |        -106.5941 |
[32m[20221214 14:28:12 @agent_ppo2.py:185][0m |           0.0002 |         111.4716 |        -106.7960 |
[32m[20221214 14:28:13 @agent_ppo2.py:185][0m |          -0.0012 |         110.5145 |        -106.8670 |
[32m[20221214 14:28:13 @agent_ppo2.py:185][0m |          -0.0016 |         110.5371 |        -106.7806 |
[32m[20221214 14:28:13 @agent_ppo2.py:185][0m |          -0.0013 |         110.2810 |        -106.6353 |
[32m[20221214 14:28:13 @agent_ppo2.py:185][0m |           0.0038 |         109.4761 |        -106.6351 |
[32m[20221214 14:28:13 @agent_ppo2.py:185][0m |           0.0039 |         111.6939 |        -106.2901 |
[32m[20221214 14:28:13 @agent_ppo2.py:185][0m |          -0.0011 |         109.1941 |        -106.6722 |
[32m[20221214 14:28:13 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:28:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 720.01
[32m[20221214 14:28:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 726.56
[32m[20221214 14:28:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 734.89
[32m[20221214 14:28:13 @agent_ppo2.py:143][0m Total time:      30.19 min
[32m[20221214 14:28:13 @agent_ppo2.py:145][0m 2777088 total steps have happened
[32m[20221214 14:28:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1356 --------------------------#
[32m[20221214 14:28:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:14 @agent_ppo2.py:185][0m |           0.0098 |         155.3508 |        -108.1267 |
[32m[20221214 14:28:14 @agent_ppo2.py:185][0m |           0.0004 |         125.1035 |        -108.1183 |
[32m[20221214 14:28:14 @agent_ppo2.py:185][0m |          -0.0020 |         118.5880 |        -108.2014 |
[32m[20221214 14:28:14 @agent_ppo2.py:185][0m |          -0.0035 |         115.2031 |        -107.6350 |
[32m[20221214 14:28:14 @agent_ppo2.py:185][0m |           0.0029 |         113.2096 |        -108.0640 |
[32m[20221214 14:28:14 @agent_ppo2.py:185][0m |          -0.0015 |         113.2889 |        -108.3894 |
[32m[20221214 14:28:14 @agent_ppo2.py:185][0m |          -0.0023 |         112.1168 |        -108.4683 |
[32m[20221214 14:28:14 @agent_ppo2.py:185][0m |           0.0033 |         112.4614 |        -108.3945 |
[32m[20221214 14:28:14 @agent_ppo2.py:185][0m |          -0.0016 |         111.4706 |        -108.2219 |
[32m[20221214 14:28:14 @agent_ppo2.py:185][0m |          -0.0015 |         109.9857 |        -108.3937 |
[32m[20221214 14:28:14 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:28:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 727.89
[32m[20221214 14:28:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.65
[32m[20221214 14:28:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 739.37
[32m[20221214 14:28:15 @agent_ppo2.py:143][0m Total time:      30.21 min
[32m[20221214 14:28:15 @agent_ppo2.py:145][0m 2779136 total steps have happened
[32m[20221214 14:28:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1357 --------------------------#
[32m[20221214 14:28:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:15 @agent_ppo2.py:185][0m |          -0.0021 |         157.7286 |        -108.6278 |
[32m[20221214 14:28:15 @agent_ppo2.py:185][0m |          -0.0011 |         142.6610 |        -108.1173 |
[32m[20221214 14:28:15 @agent_ppo2.py:185][0m |          -0.0002 |         138.7806 |        -108.2916 |
[32m[20221214 14:28:15 @agent_ppo2.py:185][0m |          -0.0039 |         136.5841 |        -107.9720 |
[32m[20221214 14:28:15 @agent_ppo2.py:185][0m |          -0.0001 |         135.4320 |        -107.8935 |
[32m[20221214 14:28:15 @agent_ppo2.py:185][0m |          -0.0034 |         133.8457 |        -107.7621 |
[32m[20221214 14:28:15 @agent_ppo2.py:185][0m |          -0.0014 |         133.6192 |        -107.9112 |
[32m[20221214 14:28:16 @agent_ppo2.py:185][0m |          -0.0028 |         133.2693 |        -107.6680 |
[32m[20221214 14:28:16 @agent_ppo2.py:185][0m |          -0.0026 |         132.6878 |        -107.4813 |
[32m[20221214 14:28:16 @agent_ppo2.py:185][0m |          -0.0022 |         132.0774 |        -107.3298 |
[32m[20221214 14:28:16 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:28:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.30
[32m[20221214 14:28:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 741.76
[32m[20221214 14:28:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 739.14
[32m[20221214 14:28:16 @agent_ppo2.py:143][0m Total time:      30.23 min
[32m[20221214 14:28:16 @agent_ppo2.py:145][0m 2781184 total steps have happened
[32m[20221214 14:28:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1358 --------------------------#
[32m[20221214 14:28:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:16 @agent_ppo2.py:185][0m |          -0.0027 |         138.5519 |        -107.2650 |
[32m[20221214 14:28:16 @agent_ppo2.py:185][0m |          -0.0012 |         128.1807 |        -107.1527 |
[32m[20221214 14:28:16 @agent_ppo2.py:185][0m |          -0.0065 |         125.6574 |        -106.9882 |
[32m[20221214 14:28:17 @agent_ppo2.py:185][0m |          -0.0020 |         122.9348 |        -106.9434 |
[32m[20221214 14:28:17 @agent_ppo2.py:185][0m |          -0.0001 |         122.8188 |        -106.8763 |
[32m[20221214 14:28:17 @agent_ppo2.py:185][0m |          -0.0044 |         121.4527 |        -106.8365 |
[32m[20221214 14:28:17 @agent_ppo2.py:185][0m |          -0.0054 |         120.4566 |        -106.8162 |
[32m[20221214 14:28:17 @agent_ppo2.py:185][0m |          -0.0059 |         120.1129 |        -106.9546 |
[32m[20221214 14:28:17 @agent_ppo2.py:185][0m |           0.0003 |         120.3964 |        -106.4251 |
[32m[20221214 14:28:17 @agent_ppo2.py:185][0m |          -0.0043 |         118.7298 |        -106.7263 |
[32m[20221214 14:28:17 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:28:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.41
[32m[20221214 14:28:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.93
[32m[20221214 14:28:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 731.96
[32m[20221214 14:28:17 @agent_ppo2.py:143][0m Total time:      30.25 min
[32m[20221214 14:28:17 @agent_ppo2.py:145][0m 2783232 total steps have happened
[32m[20221214 14:28:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1359 --------------------------#
[32m[20221214 14:28:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:18 @agent_ppo2.py:185][0m |          -0.0025 |         135.4215 |        -108.3327 |
[32m[20221214 14:28:18 @agent_ppo2.py:185][0m |           0.0036 |         132.6955 |        -108.5262 |
[32m[20221214 14:28:18 @agent_ppo2.py:185][0m |           0.0019 |         127.4080 |        -108.2204 |
[32m[20221214 14:28:18 @agent_ppo2.py:185][0m |           0.0014 |         128.0190 |        -108.7629 |
[32m[20221214 14:28:18 @agent_ppo2.py:185][0m |           0.0059 |         128.8808 |        -108.6720 |
[32m[20221214 14:28:18 @agent_ppo2.py:185][0m |           0.0034 |         126.6847 |        -109.0260 |
[32m[20221214 14:28:18 @agent_ppo2.py:185][0m |           0.0044 |         124.5358 |        -108.4016 |
[32m[20221214 14:28:18 @agent_ppo2.py:185][0m |          -0.0017 |         123.0458 |        -108.8427 |
[32m[20221214 14:28:18 @agent_ppo2.py:185][0m |           0.0061 |         124.4299 |        -109.0860 |
[32m[20221214 14:28:18 @agent_ppo2.py:185][0m |           0.0124 |         134.6716 |        -109.1034 |
[32m[20221214 14:28:18 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:28:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 729.35
[32m[20221214 14:28:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.99
[32m[20221214 14:28:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 734.78
[32m[20221214 14:28:19 @agent_ppo2.py:143][0m Total time:      30.28 min
[32m[20221214 14:28:19 @agent_ppo2.py:145][0m 2785280 total steps have happened
[32m[20221214 14:28:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1360 --------------------------#
[32m[20221214 14:28:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:28:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:19 @agent_ppo2.py:185][0m |          -0.0015 |         126.2813 |        -106.6974 |
[32m[20221214 14:28:19 @agent_ppo2.py:185][0m |          -0.0004 |         123.2943 |        -106.7629 |
[32m[20221214 14:28:19 @agent_ppo2.py:185][0m |           0.0031 |         125.1255 |        -106.7190 |
[32m[20221214 14:28:19 @agent_ppo2.py:185][0m |           0.0099 |         126.8253 |        -106.7582 |
[32m[20221214 14:28:19 @agent_ppo2.py:185][0m |           0.0006 |         121.2004 |        -107.2367 |
[32m[20221214 14:28:19 @agent_ppo2.py:185][0m |          -0.0002 |         120.5112 |        -107.2194 |
[32m[20221214 14:28:20 @agent_ppo2.py:185][0m |           0.0051 |         120.7311 |        -107.4440 |
[32m[20221214 14:28:20 @agent_ppo2.py:185][0m |          -0.0023 |         119.9030 |        -107.3303 |
[32m[20221214 14:28:20 @agent_ppo2.py:185][0m |          -0.0031 |         118.9340 |        -107.3460 |
[32m[20221214 14:28:20 @agent_ppo2.py:185][0m |          -0.0008 |         118.6774 |        -107.3524 |
[32m[20221214 14:28:20 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:28:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 732.65
[32m[20221214 14:28:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.04
[32m[20221214 14:28:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 731.76
[32m[20221214 14:28:20 @agent_ppo2.py:143][0m Total time:      30.30 min
[32m[20221214 14:28:20 @agent_ppo2.py:145][0m 2787328 total steps have happened
[32m[20221214 14:28:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1361 --------------------------#
[32m[20221214 14:28:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:20 @agent_ppo2.py:185][0m |          -0.0009 |         107.8174 |        -110.1156 |
[32m[20221214 14:28:20 @agent_ppo2.py:185][0m |          -0.0022 |         100.9972 |        -109.7764 |
[32m[20221214 14:28:20 @agent_ppo2.py:185][0m |          -0.0053 |         100.1870 |        -109.9903 |
[32m[20221214 14:28:21 @agent_ppo2.py:185][0m |           0.0028 |          98.0491 |        -110.0142 |
[32m[20221214 14:28:21 @agent_ppo2.py:185][0m |          -0.0015 |          96.4540 |        -109.7909 |
[32m[20221214 14:28:21 @agent_ppo2.py:185][0m |          -0.0002 |          95.9350 |        -109.5838 |
[32m[20221214 14:28:21 @agent_ppo2.py:185][0m |           0.0090 |         108.7728 |        -109.8226 |
[32m[20221214 14:28:21 @agent_ppo2.py:185][0m |           0.0060 |          95.6234 |        -109.6576 |
[32m[20221214 14:28:21 @agent_ppo2.py:185][0m |          -0.0016 |          94.3023 |        -109.7231 |
[32m[20221214 14:28:21 @agent_ppo2.py:185][0m |          -0.0030 |          93.6196 |        -109.5365 |
[32m[20221214 14:28:21 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:28:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 729.99
[32m[20221214 14:28:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.06
[32m[20221214 14:28:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 729.41
[32m[20221214 14:28:21 @agent_ppo2.py:143][0m Total time:      30.32 min
[32m[20221214 14:28:21 @agent_ppo2.py:145][0m 2789376 total steps have happened
[32m[20221214 14:28:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1362 --------------------------#
[32m[20221214 14:28:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:28:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:22 @agent_ppo2.py:185][0m |          -0.0013 |         138.8971 |        -108.2542 |
[32m[20221214 14:28:22 @agent_ppo2.py:185][0m |          -0.0004 |         134.9580 |        -108.4534 |
[32m[20221214 14:28:22 @agent_ppo2.py:185][0m |          -0.0020 |         133.5916 |        -108.4421 |
[32m[20221214 14:28:22 @agent_ppo2.py:185][0m |          -0.0012 |         132.4605 |        -108.3254 |
[32m[20221214 14:28:22 @agent_ppo2.py:185][0m |           0.0046 |         134.5015 |        -108.2541 |
[32m[20221214 14:28:22 @agent_ppo2.py:185][0m |          -0.0010 |         130.7612 |        -108.3806 |
[32m[20221214 14:28:22 @agent_ppo2.py:185][0m |           0.0045 |         135.4089 |        -108.2371 |
[32m[20221214 14:28:22 @agent_ppo2.py:185][0m |          -0.0023 |         129.9330 |        -108.1083 |
[32m[20221214 14:28:22 @agent_ppo2.py:185][0m |          -0.0033 |         129.4936 |        -108.2099 |
[32m[20221214 14:28:22 @agent_ppo2.py:185][0m |          -0.0016 |         130.3019 |        -108.1381 |
[32m[20221214 14:28:22 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:28:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 728.54
[32m[20221214 14:28:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.55
[32m[20221214 14:28:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 729.56
[32m[20221214 14:28:23 @agent_ppo2.py:143][0m Total time:      30.34 min
[32m[20221214 14:28:23 @agent_ppo2.py:145][0m 2791424 total steps have happened
[32m[20221214 14:28:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1363 --------------------------#
[32m[20221214 14:28:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:23 @agent_ppo2.py:185][0m |          -0.0022 |         126.4451 |        -108.1743 |
[32m[20221214 14:28:23 @agent_ppo2.py:185][0m |           0.0058 |         127.7721 |        -108.0208 |
[32m[20221214 14:28:23 @agent_ppo2.py:185][0m |          -0.0025 |         121.8045 |        -108.2120 |
[32m[20221214 14:28:23 @agent_ppo2.py:185][0m |          -0.0019 |         121.2461 |        -107.8581 |
[32m[20221214 14:28:23 @agent_ppo2.py:185][0m |          -0.0008 |         122.0581 |        -108.1412 |
[32m[20221214 14:28:23 @agent_ppo2.py:185][0m |          -0.0041 |         119.5613 |        -108.0280 |
[32m[20221214 14:28:24 @agent_ppo2.py:185][0m |          -0.0021 |         118.8753 |        -107.7983 |
[32m[20221214 14:28:24 @agent_ppo2.py:185][0m |          -0.0017 |         118.6389 |        -107.8327 |
[32m[20221214 14:28:24 @agent_ppo2.py:185][0m |          -0.0055 |         118.3179 |        -107.9505 |
[32m[20221214 14:28:24 @agent_ppo2.py:185][0m |           0.0067 |         123.6330 |        -107.8343 |
[32m[20221214 14:28:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:28:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 730.86
[32m[20221214 14:28:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 732.54
[32m[20221214 14:28:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.88
[32m[20221214 14:28:24 @agent_ppo2.py:143][0m Total time:      30.36 min
[32m[20221214 14:28:24 @agent_ppo2.py:145][0m 2793472 total steps have happened
[32m[20221214 14:28:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1364 --------------------------#
[32m[20221214 14:28:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:24 @agent_ppo2.py:185][0m |          -0.0041 |         101.5451 |        -106.2315 |
[32m[20221214 14:28:24 @agent_ppo2.py:185][0m |          -0.0065 |          86.9510 |        -106.3147 |
[32m[20221214 14:28:24 @agent_ppo2.py:185][0m |          -0.0066 |          81.3055 |        -106.4269 |
[32m[20221214 14:28:25 @agent_ppo2.py:185][0m |          -0.0073 |          78.4611 |        -106.1138 |
[32m[20221214 14:28:25 @agent_ppo2.py:185][0m |          -0.0076 |          75.9235 |        -106.3186 |
[32m[20221214 14:28:25 @agent_ppo2.py:185][0m |           0.0042 |          79.2352 |        -106.0644 |
[32m[20221214 14:28:25 @agent_ppo2.py:185][0m |          -0.0059 |          73.7954 |        -106.0720 |
[32m[20221214 14:28:25 @agent_ppo2.py:185][0m |          -0.0100 |          71.5931 |        -106.1475 |
[32m[20221214 14:28:25 @agent_ppo2.py:185][0m |          -0.0094 |          71.3495 |        -106.3195 |
[32m[20221214 14:28:25 @agent_ppo2.py:185][0m |          -0.0041 |          71.9225 |        -105.9124 |
[32m[20221214 14:28:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:28:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 730.61
[32m[20221214 14:28:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.16
[32m[20221214 14:28:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 738.83
[32m[20221214 14:28:25 @agent_ppo2.py:143][0m Total time:      30.39 min
[32m[20221214 14:28:25 @agent_ppo2.py:145][0m 2795520 total steps have happened
[32m[20221214 14:28:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1365 --------------------------#
[32m[20221214 14:28:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:28:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:26 @agent_ppo2.py:185][0m |           0.0030 |         150.5535 |        -109.3205 |
[32m[20221214 14:28:26 @agent_ppo2.py:185][0m |           0.0095 |         153.4229 |        -109.3145 |
[32m[20221214 14:28:26 @agent_ppo2.py:185][0m |           0.0006 |         145.5198 |        -109.2214 |
[32m[20221214 14:28:26 @agent_ppo2.py:185][0m |          -0.0009 |         143.4836 |        -109.2224 |
[32m[20221214 14:28:26 @agent_ppo2.py:185][0m |          -0.0025 |         142.3242 |        -109.3193 |
[32m[20221214 14:28:26 @agent_ppo2.py:185][0m |           0.0059 |         143.0473 |        -109.3266 |
[32m[20221214 14:28:26 @agent_ppo2.py:185][0m |          -0.0029 |         139.4700 |        -109.4989 |
[32m[20221214 14:28:26 @agent_ppo2.py:185][0m |           0.0028 |         138.7973 |        -109.2427 |
[32m[20221214 14:28:26 @agent_ppo2.py:185][0m |           0.0007 |         138.4791 |        -109.4926 |
[32m[20221214 14:28:26 @agent_ppo2.py:185][0m |          -0.0013 |         137.1240 |        -109.3990 |
[32m[20221214 14:28:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:28:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.74
[32m[20221214 14:28:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.91
[32m[20221214 14:28:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 739.23
[32m[20221214 14:28:26 @agent_ppo2.py:143][0m Total time:      30.41 min
[32m[20221214 14:28:26 @agent_ppo2.py:145][0m 2797568 total steps have happened
[32m[20221214 14:28:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1366 --------------------------#
[32m[20221214 14:28:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:28:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:27 @agent_ppo2.py:185][0m |           0.0009 |         148.0514 |        -108.8311 |
[32m[20221214 14:28:27 @agent_ppo2.py:185][0m |          -0.0025 |         144.8056 |        -108.9621 |
[32m[20221214 14:28:27 @agent_ppo2.py:185][0m |          -0.0009 |         143.4115 |        -108.9330 |
[32m[20221214 14:28:27 @agent_ppo2.py:185][0m |           0.0010 |         143.4077 |        -109.1788 |
[32m[20221214 14:28:27 @agent_ppo2.py:185][0m |           0.0037 |         146.0361 |        -109.0458 |
[32m[20221214 14:28:27 @agent_ppo2.py:185][0m |          -0.0020 |         140.1313 |        -108.9625 |
[32m[20221214 14:28:27 @agent_ppo2.py:185][0m |          -0.0009 |         140.0316 |        -109.2837 |
[32m[20221214 14:28:27 @agent_ppo2.py:185][0m |          -0.0033 |         139.7702 |        -109.2697 |
[32m[20221214 14:28:27 @agent_ppo2.py:185][0m |          -0.0004 |         139.7195 |        -109.2866 |
[32m[20221214 14:28:27 @agent_ppo2.py:185][0m |           0.0021 |         143.2343 |        -109.2632 |
[32m[20221214 14:28:27 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:28:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.49
[32m[20221214 14:28:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 741.52
[32m[20221214 14:28:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 740.16
[32m[20221214 14:28:28 @agent_ppo2.py:143][0m Total time:      30.43 min
[32m[20221214 14:28:28 @agent_ppo2.py:145][0m 2799616 total steps have happened
[32m[20221214 14:28:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1367 --------------------------#
[32m[20221214 14:28:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:28:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:28 @agent_ppo2.py:185][0m |           0.0004 |         148.0326 |        -109.0163 |
[32m[20221214 14:28:28 @agent_ppo2.py:185][0m |           0.0025 |         146.3561 |        -108.9089 |
[32m[20221214 14:28:28 @agent_ppo2.py:185][0m |          -0.0009 |         143.9426 |        -108.9755 |
[32m[20221214 14:28:28 @agent_ppo2.py:185][0m |          -0.0010 |         142.5553 |        -108.8112 |
[32m[20221214 14:28:28 @agent_ppo2.py:185][0m |          -0.0023 |         142.2919 |        -108.8852 |
[32m[20221214 14:28:28 @agent_ppo2.py:185][0m |          -0.0003 |         141.8518 |        -108.8018 |
[32m[20221214 14:28:28 @agent_ppo2.py:185][0m |           0.0014 |         141.7996 |        -108.9406 |
[32m[20221214 14:28:29 @agent_ppo2.py:185][0m |          -0.0046 |         141.6613 |        -108.7488 |
[32m[20221214 14:28:29 @agent_ppo2.py:185][0m |          -0.0017 |         141.6822 |        -108.5125 |
[32m[20221214 14:28:29 @agent_ppo2.py:185][0m |           0.0007 |         143.4676 |        -108.7230 |
[32m[20221214 14:28:29 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:28:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 736.99
[32m[20221214 14:28:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.83
[32m[20221214 14:28:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 739.77
[32m[20221214 14:28:29 @agent_ppo2.py:143][0m Total time:      30.45 min
[32m[20221214 14:28:29 @agent_ppo2.py:145][0m 2801664 total steps have happened
[32m[20221214 14:28:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1368 --------------------------#
[32m[20221214 14:28:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:28:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:29 @agent_ppo2.py:185][0m |           0.0002 |         151.5140 |        -108.7071 |
[32m[20221214 14:28:29 @agent_ppo2.py:185][0m |           0.0008 |         145.4819 |        -108.1290 |
[32m[20221214 14:28:29 @agent_ppo2.py:185][0m |          -0.0001 |         142.9121 |        -108.5771 |
[32m[20221214 14:28:29 @agent_ppo2.py:185][0m |           0.0052 |         144.0983 |        -108.6856 |
[32m[20221214 14:28:30 @agent_ppo2.py:185][0m |           0.0002 |         141.0277 |        -108.3037 |
[32m[20221214 14:28:30 @agent_ppo2.py:185][0m |          -0.0014 |         139.8013 |        -108.7523 |
[32m[20221214 14:28:30 @agent_ppo2.py:185][0m |           0.0202 |         167.5355 |        -108.3513 |
[32m[20221214 14:28:30 @agent_ppo2.py:185][0m |          -0.0021 |         138.9239 |        -108.1168 |
[32m[20221214 14:28:30 @agent_ppo2.py:185][0m |          -0.0013 |         138.2368 |        -108.8824 |
[32m[20221214 14:28:30 @agent_ppo2.py:185][0m |           0.0107 |         144.9378 |        -108.8394 |
[32m[20221214 14:28:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:28:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 737.17
[32m[20221214 14:28:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.21
[32m[20221214 14:28:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.30
[32m[20221214 14:28:30 @agent_ppo2.py:143][0m Total time:      30.47 min
[32m[20221214 14:28:30 @agent_ppo2.py:145][0m 2803712 total steps have happened
[32m[20221214 14:28:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1369 --------------------------#
[32m[20221214 14:28:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:30 @agent_ppo2.py:185][0m |          -0.0026 |         165.1986 |        -110.6320 |
[32m[20221214 14:28:31 @agent_ppo2.py:185][0m |          -0.0065 |         159.4659 |        -110.4248 |
[32m[20221214 14:28:31 @agent_ppo2.py:185][0m |          -0.0035 |         156.3868 |        -110.0034 |
[32m[20221214 14:28:31 @agent_ppo2.py:185][0m |          -0.0041 |         154.5387 |        -110.1055 |
[32m[20221214 14:28:31 @agent_ppo2.py:185][0m |          -0.0068 |         153.7667 |        -110.2767 |
[32m[20221214 14:28:31 @agent_ppo2.py:185][0m |           0.0069 |         171.6974 |        -109.8479 |
[32m[20221214 14:28:31 @agent_ppo2.py:185][0m |          -0.0042 |         152.7573 |        -109.8692 |
[32m[20221214 14:28:31 @agent_ppo2.py:185][0m |          -0.0050 |         150.9166 |        -110.2253 |
[32m[20221214 14:28:31 @agent_ppo2.py:185][0m |          -0.0045 |         150.1542 |        -110.0896 |
[32m[20221214 14:28:31 @agent_ppo2.py:185][0m |          -0.0021 |         149.6568 |        -109.9553 |
[32m[20221214 14:28:31 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:28:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 740.10
[32m[20221214 14:28:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.61
[32m[20221214 14:28:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 744.37
[32m[20221214 14:28:31 @agent_ppo2.py:143][0m Total time:      30.49 min
[32m[20221214 14:28:31 @agent_ppo2.py:145][0m 2805760 total steps have happened
[32m[20221214 14:28:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1370 --------------------------#
[32m[20221214 14:28:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:28:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:32 @agent_ppo2.py:185][0m |           0.0021 |         140.4105 |        -111.7614 |
[32m[20221214 14:28:32 @agent_ppo2.py:185][0m |          -0.0001 |         136.2279 |        -111.4924 |
[32m[20221214 14:28:32 @agent_ppo2.py:185][0m |          -0.0028 |         136.1544 |        -111.3167 |
[32m[20221214 14:28:32 @agent_ppo2.py:185][0m |           0.0002 |         134.9890 |        -111.3638 |
[32m[20221214 14:28:32 @agent_ppo2.py:185][0m |          -0.0001 |         134.1834 |        -111.4623 |
[32m[20221214 14:28:32 @agent_ppo2.py:185][0m |          -0.0060 |         133.4889 |        -111.6368 |
[32m[20221214 14:28:32 @agent_ppo2.py:185][0m |          -0.0036 |         133.8062 |        -111.5000 |
[32m[20221214 14:28:32 @agent_ppo2.py:185][0m |          -0.0017 |         132.6525 |        -111.9595 |
[32m[20221214 14:28:32 @agent_ppo2.py:185][0m |          -0.0029 |         132.4484 |        -111.7492 |
[32m[20221214 14:28:33 @agent_ppo2.py:185][0m |           0.0003 |         132.3924 |        -111.5770 |
[32m[20221214 14:28:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:28:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 740.80
[32m[20221214 14:28:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 744.44
[32m[20221214 14:28:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 747.19
[32m[20221214 14:28:33 @agent_ppo2.py:143][0m Total time:      30.51 min
[32m[20221214 14:28:33 @agent_ppo2.py:145][0m 2807808 total steps have happened
[32m[20221214 14:28:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1371 --------------------------#
[32m[20221214 14:28:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:28:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:33 @agent_ppo2.py:185][0m |          -0.0017 |         171.8140 |        -109.0936 |
[32m[20221214 14:28:33 @agent_ppo2.py:185][0m |          -0.0010 |         160.1043 |        -108.6934 |
[32m[20221214 14:28:33 @agent_ppo2.py:185][0m |          -0.0010 |         158.2625 |        -109.1742 |
[32m[20221214 14:28:33 @agent_ppo2.py:185][0m |           0.0062 |         161.4925 |        -108.8958 |
[32m[20221214 14:28:33 @agent_ppo2.py:185][0m |          -0.0014 |         156.8519 |        -109.0706 |
[32m[20221214 14:28:33 @agent_ppo2.py:185][0m |          -0.0044 |         156.4558 |        -109.2071 |
[32m[20221214 14:28:33 @agent_ppo2.py:185][0m |           0.0003 |         155.2578 |        -109.6116 |
[32m[20221214 14:28:34 @agent_ppo2.py:185][0m |          -0.0021 |         155.0218 |        -109.3817 |
[32m[20221214 14:28:34 @agent_ppo2.py:185][0m |           0.0079 |         164.9886 |        -109.4590 |
[32m[20221214 14:28:34 @agent_ppo2.py:185][0m |           0.0011 |         154.4183 |        -109.5895 |
[32m[20221214 14:28:34 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:28:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.20
[32m[20221214 14:28:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.78
[32m[20221214 14:28:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 746.15
[32m[20221214 14:28:34 @agent_ppo2.py:143][0m Total time:      30.53 min
[32m[20221214 14:28:34 @agent_ppo2.py:145][0m 2809856 total steps have happened
[32m[20221214 14:28:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1372 --------------------------#
[32m[20221214 14:28:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:34 @agent_ppo2.py:185][0m |           0.0057 |         152.5622 |        -110.7154 |
[32m[20221214 14:28:34 @agent_ppo2.py:185][0m |          -0.0048 |         143.7704 |        -110.2057 |
[32m[20221214 14:28:34 @agent_ppo2.py:185][0m |          -0.0021 |         142.9775 |        -110.5775 |
[32m[20221214 14:28:34 @agent_ppo2.py:185][0m |          -0.0000 |         141.7225 |        -110.6920 |
[32m[20221214 14:28:35 @agent_ppo2.py:185][0m |           0.0015 |         143.6602 |        -110.6096 |
[32m[20221214 14:28:35 @agent_ppo2.py:185][0m |          -0.0022 |         141.4001 |        -110.7532 |
[32m[20221214 14:28:35 @agent_ppo2.py:185][0m |          -0.0053 |         141.1032 |        -110.8434 |
[32m[20221214 14:28:35 @agent_ppo2.py:185][0m |          -0.0020 |         140.7989 |        -110.7322 |
[32m[20221214 14:28:35 @agent_ppo2.py:185][0m |          -0.0038 |         140.0131 |        -111.0025 |
[32m[20221214 14:28:35 @agent_ppo2.py:185][0m |          -0.0037 |         140.7705 |        -110.9369 |
[32m[20221214 14:28:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:28:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.84
[32m[20221214 14:28:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 751.32
[32m[20221214 14:28:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 750.75
[32m[20221214 14:28:35 @agent_ppo2.py:143][0m Total time:      30.55 min
[32m[20221214 14:28:35 @agent_ppo2.py:145][0m 2811904 total steps have happened
[32m[20221214 14:28:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1373 --------------------------#
[32m[20221214 14:28:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:28:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:35 @agent_ppo2.py:185][0m |          -0.0013 |         126.7155 |        -112.1980 |
[32m[20221214 14:28:36 @agent_ppo2.py:185][0m |          -0.0017 |         124.0851 |        -112.1002 |
[32m[20221214 14:28:36 @agent_ppo2.py:185][0m |           0.0029 |         121.7391 |        -112.0613 |
[32m[20221214 14:28:36 @agent_ppo2.py:185][0m |          -0.0016 |         121.1515 |        -112.1085 |
[32m[20221214 14:28:36 @agent_ppo2.py:185][0m |          -0.0030 |         119.9255 |        -112.2554 |
[32m[20221214 14:28:36 @agent_ppo2.py:185][0m |          -0.0013 |         119.4454 |        -112.0379 |
[32m[20221214 14:28:36 @agent_ppo2.py:185][0m |          -0.0035 |         119.5091 |        -112.2891 |
[32m[20221214 14:28:36 @agent_ppo2.py:185][0m |          -0.0011 |         118.3363 |        -112.3619 |
[32m[20221214 14:28:36 @agent_ppo2.py:185][0m |          -0.0061 |         119.0887 |        -112.2670 |
[32m[20221214 14:28:36 @agent_ppo2.py:185][0m |          -0.0014 |         118.5006 |        -112.1608 |
[32m[20221214 14:28:36 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:28:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.50
[32m[20221214 14:28:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 758.00
[32m[20221214 14:28:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 751.67
[32m[20221214 14:28:37 @agent_ppo2.py:143][0m Total time:      30.57 min
[32m[20221214 14:28:37 @agent_ppo2.py:145][0m 2813952 total steps have happened
[32m[20221214 14:28:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1374 --------------------------#
[32m[20221214 14:28:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:37 @agent_ppo2.py:185][0m |          -0.0030 |         135.3318 |        -114.7674 |
[32m[20221214 14:28:37 @agent_ppo2.py:185][0m |          -0.0063 |         134.6757 |        -114.7406 |
[32m[20221214 14:28:37 @agent_ppo2.py:185][0m |          -0.0028 |         134.2544 |        -114.6263 |
[32m[20221214 14:28:37 @agent_ppo2.py:185][0m |           0.0020 |         136.4149 |        -114.6114 |
[32m[20221214 14:28:37 @agent_ppo2.py:185][0m |          -0.0029 |         134.2622 |        -114.6726 |
[32m[20221214 14:28:37 @agent_ppo2.py:185][0m |          -0.0039 |         133.2250 |        -114.7007 |
[32m[20221214 14:28:37 @agent_ppo2.py:185][0m |          -0.0002 |         133.4016 |        -114.7362 |
[32m[20221214 14:28:37 @agent_ppo2.py:185][0m |           0.0102 |         150.0863 |        -114.4867 |
[32m[20221214 14:28:38 @agent_ppo2.py:185][0m |          -0.0034 |         132.1635 |        -114.6225 |
[32m[20221214 14:28:38 @agent_ppo2.py:185][0m |          -0.0050 |         132.5265 |        -114.2446 |
[32m[20221214 14:28:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:28:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 746.66
[32m[20221214 14:28:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 749.70
[32m[20221214 14:28:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 751.72
[32m[20221214 14:28:38 @agent_ppo2.py:143][0m Total time:      30.59 min
[32m[20221214 14:28:38 @agent_ppo2.py:145][0m 2816000 total steps have happened
[32m[20221214 14:28:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1375 --------------------------#
[32m[20221214 14:28:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:28:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:38 @agent_ppo2.py:185][0m |          -0.0007 |         127.4698 |        -110.2628 |
[32m[20221214 14:28:38 @agent_ppo2.py:185][0m |          -0.0017 |         118.0294 |        -110.2252 |
[32m[20221214 14:28:38 @agent_ppo2.py:185][0m |          -0.0047 |         114.5880 |        -110.0997 |
[32m[20221214 14:28:38 @agent_ppo2.py:185][0m |          -0.0047 |         111.4380 |        -110.0056 |
[32m[20221214 14:28:38 @agent_ppo2.py:185][0m |          -0.0076 |         111.7777 |        -109.8566 |
[32m[20221214 14:28:39 @agent_ppo2.py:185][0m |           0.0039 |         114.1252 |        -109.8443 |
[32m[20221214 14:28:39 @agent_ppo2.py:185][0m |          -0.0013 |         109.1423 |        -109.4673 |
[32m[20221214 14:28:39 @agent_ppo2.py:185][0m |          -0.0015 |         108.2232 |        -109.7084 |
[32m[20221214 14:28:39 @agent_ppo2.py:185][0m |           0.0008 |         108.6510 |        -109.9471 |
[32m[20221214 14:28:39 @agent_ppo2.py:185][0m |          -0.0026 |         110.0828 |        -109.3254 |
[32m[20221214 14:28:39 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:28:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.55
[32m[20221214 14:28:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 753.14
[32m[20221214 14:28:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 749.89
[32m[20221214 14:28:39 @agent_ppo2.py:143][0m Total time:      30.62 min
[32m[20221214 14:28:39 @agent_ppo2.py:145][0m 2818048 total steps have happened
[32m[20221214 14:28:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1376 --------------------------#
[32m[20221214 14:28:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:39 @agent_ppo2.py:185][0m |          -0.0006 |         122.7085 |        -111.1042 |
[32m[20221214 14:28:39 @agent_ppo2.py:185][0m |           0.0217 |         139.1844 |        -111.2262 |
[32m[20221214 14:28:40 @agent_ppo2.py:185][0m |           0.0004 |         119.8444 |        -111.0369 |
[32m[20221214 14:28:40 @agent_ppo2.py:185][0m |          -0.0003 |         115.8244 |        -111.1649 |
[32m[20221214 14:28:40 @agent_ppo2.py:185][0m |          -0.0033 |         115.8502 |        -111.1745 |
[32m[20221214 14:28:40 @agent_ppo2.py:185][0m |          -0.0006 |         114.1229 |        -111.2397 |
[32m[20221214 14:28:40 @agent_ppo2.py:185][0m |           0.0023 |         113.7277 |        -111.1939 |
[32m[20221214 14:28:40 @agent_ppo2.py:185][0m |           0.0012 |         113.4705 |        -111.2443 |
[32m[20221214 14:28:40 @agent_ppo2.py:185][0m |          -0.0010 |         112.8550 |        -111.4540 |
[32m[20221214 14:28:40 @agent_ppo2.py:185][0m |          -0.0004 |         112.8797 |        -111.6019 |
[32m[20221214 14:28:40 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:28:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 744.83
[32m[20221214 14:28:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 748.68
[32m[20221214 14:28:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 747.67
[32m[20221214 14:28:40 @agent_ppo2.py:143][0m Total time:      30.64 min
[32m[20221214 14:28:40 @agent_ppo2.py:145][0m 2820096 total steps have happened
[32m[20221214 14:28:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1377 --------------------------#
[32m[20221214 14:28:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:28:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:41 @agent_ppo2.py:185][0m |           0.0017 |         157.5165 |        -108.6377 |
[32m[20221214 14:28:41 @agent_ppo2.py:185][0m |          -0.0026 |         152.4824 |        -108.7118 |
[32m[20221214 14:28:41 @agent_ppo2.py:185][0m |          -0.0026 |         150.6641 |        -108.7835 |
[32m[20221214 14:28:41 @agent_ppo2.py:185][0m |          -0.0008 |         149.5001 |        -108.6375 |
[32m[20221214 14:28:41 @agent_ppo2.py:185][0m |           0.0006 |         146.6111 |        -108.4473 |
[32m[20221214 14:28:41 @agent_ppo2.py:185][0m |          -0.0006 |         146.1016 |        -108.5953 |
[32m[20221214 14:28:41 @agent_ppo2.py:185][0m |           0.0051 |         149.3713 |        -108.7533 |
[32m[20221214 14:28:41 @agent_ppo2.py:185][0m |          -0.0006 |         144.9418 |        -108.6342 |
[32m[20221214 14:28:41 @agent_ppo2.py:185][0m |           0.0331 |         179.4973 |        -108.1815 |
[32m[20221214 14:28:41 @agent_ppo2.py:185][0m |           0.0107 |         165.0707 |        -108.4752 |
[32m[20221214 14:28:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:28:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.73
[32m[20221214 14:28:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 751.92
[32m[20221214 14:28:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 750.22
[32m[20221214 14:28:42 @agent_ppo2.py:143][0m Total time:      30.66 min
[32m[20221214 14:28:42 @agent_ppo2.py:145][0m 2822144 total steps have happened
[32m[20221214 14:28:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1378 --------------------------#
[32m[20221214 14:28:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:42 @agent_ppo2.py:185][0m |          -0.0055 |         168.5172 |        -111.2579 |
[32m[20221214 14:28:42 @agent_ppo2.py:185][0m |          -0.0006 |         160.0051 |        -111.4558 |
[32m[20221214 14:28:42 @agent_ppo2.py:185][0m |          -0.0063 |         156.0258 |        -111.3356 |
[32m[20221214 14:28:42 @agent_ppo2.py:185][0m |          -0.0007 |         153.4125 |        -111.0900 |
[32m[20221214 14:28:42 @agent_ppo2.py:185][0m |          -0.0042 |         151.2527 |        -111.3773 |
[32m[20221214 14:28:42 @agent_ppo2.py:185][0m |          -0.0007 |         149.1447 |        -111.3265 |
[32m[20221214 14:28:43 @agent_ppo2.py:185][0m |          -0.0027 |         148.5956 |        -111.5793 |
[32m[20221214 14:28:43 @agent_ppo2.py:185][0m |          -0.0006 |         147.5630 |        -111.4793 |
[32m[20221214 14:28:43 @agent_ppo2.py:185][0m |          -0.0015 |         146.9931 |        -111.5417 |
[32m[20221214 14:28:43 @agent_ppo2.py:185][0m |          -0.0018 |         146.1862 |        -111.5978 |
[32m[20221214 14:28:43 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 14:28:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 746.75
[32m[20221214 14:28:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 753.74
[32m[20221214 14:28:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 746.84
[32m[20221214 14:28:43 @agent_ppo2.py:143][0m Total time:      30.68 min
[32m[20221214 14:28:43 @agent_ppo2.py:145][0m 2824192 total steps have happened
[32m[20221214 14:28:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1379 --------------------------#
[32m[20221214 14:28:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:43 @agent_ppo2.py:185][0m |           0.0002 |         160.0238 |        -112.3777 |
[32m[20221214 14:28:43 @agent_ppo2.py:185][0m |          -0.0004 |         153.6756 |        -112.1496 |
[32m[20221214 14:28:44 @agent_ppo2.py:185][0m |           0.0003 |         150.6867 |        -112.0663 |
[32m[20221214 14:28:44 @agent_ppo2.py:185][0m |           0.0046 |         152.4899 |        -111.8822 |
[32m[20221214 14:28:44 @agent_ppo2.py:185][0m |          -0.0006 |         147.2194 |        -112.0519 |
[32m[20221214 14:28:44 @agent_ppo2.py:185][0m |          -0.0055 |         146.1662 |        -111.8848 |
[32m[20221214 14:28:44 @agent_ppo2.py:185][0m |           0.0038 |         145.9890 |        -111.7481 |
[32m[20221214 14:28:44 @agent_ppo2.py:185][0m |          -0.0017 |         144.0965 |        -111.6078 |
[32m[20221214 14:28:44 @agent_ppo2.py:185][0m |           0.0225 |         171.8595 |        -111.8997 |
[32m[20221214 14:28:44 @agent_ppo2.py:185][0m |          -0.0021 |         143.5806 |        -111.4929 |
[32m[20221214 14:28:44 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:28:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.54
[32m[20221214 14:28:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 748.93
[32m[20221214 14:28:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 749.04
[32m[20221214 14:28:44 @agent_ppo2.py:143][0m Total time:      30.71 min
[32m[20221214 14:28:44 @agent_ppo2.py:145][0m 2826240 total steps have happened
[32m[20221214 14:28:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1380 --------------------------#
[32m[20221214 14:28:45 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221214 14:28:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:45 @agent_ppo2.py:185][0m |           0.0038 |         163.5794 |        -107.0491 |
[32m[20221214 14:28:45 @agent_ppo2.py:185][0m |          -0.0018 |         154.9552 |        -107.0312 |
[32m[20221214 14:28:45 @agent_ppo2.py:185][0m |          -0.0036 |         153.2981 |        -107.0526 |
[32m[20221214 14:28:45 @agent_ppo2.py:185][0m |          -0.0025 |         151.0634 |        -107.1612 |
[32m[20221214 14:28:45 @agent_ppo2.py:185][0m |           0.0001 |         150.6201 |        -106.9641 |
[32m[20221214 14:28:45 @agent_ppo2.py:185][0m |          -0.0029 |         149.2400 |        -107.4559 |
[32m[20221214 14:28:45 @agent_ppo2.py:185][0m |           0.0100 |         164.6580 |        -107.4210 |
[32m[20221214 14:28:45 @agent_ppo2.py:185][0m |           0.0133 |         163.6625 |        -107.5949 |
[32m[20221214 14:28:45 @agent_ppo2.py:185][0m |          -0.0020 |         148.3986 |        -107.6118 |
[32m[20221214 14:28:46 @agent_ppo2.py:185][0m |           0.0009 |         150.7447 |        -107.7584 |
[32m[20221214 14:28:46 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:28:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.34
[32m[20221214 14:28:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 748.77
[32m[20221214 14:28:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 736.13
[32m[20221214 14:28:46 @agent_ppo2.py:143][0m Total time:      30.73 min
[32m[20221214 14:28:46 @agent_ppo2.py:145][0m 2828288 total steps have happened
[32m[20221214 14:28:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1381 --------------------------#
[32m[20221214 14:28:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:28:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:46 @agent_ppo2.py:185][0m |          -0.0002 |         172.7662 |        -110.4202 |
[32m[20221214 14:28:46 @agent_ppo2.py:185][0m |          -0.0026 |         164.2571 |        -109.9251 |
[32m[20221214 14:28:46 @agent_ppo2.py:185][0m |          -0.0011 |         162.7684 |        -109.9845 |
[32m[20221214 14:28:46 @agent_ppo2.py:185][0m |           0.0089 |         173.6012 |        -109.4815 |
[32m[20221214 14:28:46 @agent_ppo2.py:185][0m |          -0.0015 |         161.7941 |        -109.1452 |
[32m[20221214 14:28:46 @agent_ppo2.py:185][0m |          -0.0042 |         159.5485 |        -109.5898 |
[32m[20221214 14:28:47 @agent_ppo2.py:185][0m |          -0.0023 |         159.0717 |        -108.9892 |
[32m[20221214 14:28:47 @agent_ppo2.py:185][0m |          -0.0026 |         158.9884 |        -109.1678 |
[32m[20221214 14:28:47 @agent_ppo2.py:185][0m |          -0.0027 |         158.0337 |        -109.1219 |
[32m[20221214 14:28:47 @agent_ppo2.py:185][0m |          -0.0036 |         157.6028 |        -108.8094 |
[32m[20221214 14:28:47 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:28:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.07
[32m[20221214 14:28:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 748.19
[32m[20221214 14:28:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 730.18
[32m[20221214 14:28:47 @agent_ppo2.py:143][0m Total time:      30.75 min
[32m[20221214 14:28:47 @agent_ppo2.py:145][0m 2830336 total steps have happened
[32m[20221214 14:28:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1382 --------------------------#
[32m[20221214 14:28:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:47 @agent_ppo2.py:185][0m |          -0.0024 |         172.3910 |        -109.1655 |
[32m[20221214 14:28:47 @agent_ppo2.py:185][0m |          -0.0017 |         164.1226 |        -109.1413 |
[32m[20221214 14:28:48 @agent_ppo2.py:185][0m |           0.0059 |         166.5187 |        -108.8743 |
[32m[20221214 14:28:48 @agent_ppo2.py:185][0m |          -0.0007 |         160.1920 |        -109.2071 |
[32m[20221214 14:28:48 @agent_ppo2.py:185][0m |          -0.0057 |         158.0085 |        -109.2548 |
[32m[20221214 14:28:48 @agent_ppo2.py:185][0m |          -0.0032 |         156.1990 |        -109.2360 |
[32m[20221214 14:28:48 @agent_ppo2.py:185][0m |           0.0034 |         157.1197 |        -109.3127 |
[32m[20221214 14:28:48 @agent_ppo2.py:185][0m |          -0.0024 |         153.6700 |        -109.3391 |
[32m[20221214 14:28:48 @agent_ppo2.py:185][0m |          -0.0025 |         152.5577 |        -109.2381 |
[32m[20221214 14:28:48 @agent_ppo2.py:185][0m |          -0.0045 |         151.4966 |        -109.2430 |
[32m[20221214 14:28:48 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:28:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.72
[32m[20221214 14:28:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 751.47
[32m[20221214 14:28:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 725.29
[32m[20221214 14:28:48 @agent_ppo2.py:143][0m Total time:      30.77 min
[32m[20221214 14:28:48 @agent_ppo2.py:145][0m 2832384 total steps have happened
[32m[20221214 14:28:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1383 --------------------------#
[32m[20221214 14:28:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:49 @agent_ppo2.py:185][0m |           0.0006 |         189.1538 |        -109.9219 |
[32m[20221214 14:28:49 @agent_ppo2.py:185][0m |          -0.0045 |         172.1800 |        -110.0320 |
[32m[20221214 14:28:49 @agent_ppo2.py:185][0m |          -0.0027 |         164.8333 |        -109.7195 |
[32m[20221214 14:28:49 @agent_ppo2.py:185][0m |          -0.0002 |         161.9941 |        -109.8289 |
[32m[20221214 14:28:49 @agent_ppo2.py:185][0m |          -0.0045 |         159.1497 |        -109.8263 |
[32m[20221214 14:28:49 @agent_ppo2.py:185][0m |          -0.0016 |         157.5410 |        -109.6444 |
[32m[20221214 14:28:49 @agent_ppo2.py:185][0m |          -0.0081 |         155.7538 |        -109.9813 |
[32m[20221214 14:28:49 @agent_ppo2.py:185][0m |           0.0018 |         159.1038 |        -109.9255 |
[32m[20221214 14:28:49 @agent_ppo2.py:185][0m |          -0.0036 |         153.9951 |        -109.8680 |
[32m[20221214 14:28:50 @agent_ppo2.py:185][0m |          -0.0053 |         151.3874 |        -109.6793 |
[32m[20221214 14:28:50 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:28:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 726.28
[32m[20221214 14:28:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 736.36
[32m[20221214 14:28:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 749.13
[32m[20221214 14:28:50 @agent_ppo2.py:143][0m Total time:      30.79 min
[32m[20221214 14:28:50 @agent_ppo2.py:145][0m 2834432 total steps have happened
[32m[20221214 14:28:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1384 --------------------------#
[32m[20221214 14:28:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:50 @agent_ppo2.py:185][0m |          -0.0032 |         188.6176 |        -111.2673 |
[32m[20221214 14:28:50 @agent_ppo2.py:185][0m |          -0.0028 |         178.0983 |        -110.4686 |
[32m[20221214 14:28:50 @agent_ppo2.py:185][0m |          -0.0033 |         176.2812 |        -110.7056 |
[32m[20221214 14:28:50 @agent_ppo2.py:185][0m |          -0.0048 |         173.3768 |        -110.6976 |
[32m[20221214 14:28:50 @agent_ppo2.py:185][0m |          -0.0006 |         172.1711 |        -110.5022 |
[32m[20221214 14:28:51 @agent_ppo2.py:185][0m |          -0.0059 |         170.5429 |        -110.4120 |
[32m[20221214 14:28:51 @agent_ppo2.py:185][0m |          -0.0067 |         169.1082 |        -110.1857 |
[32m[20221214 14:28:51 @agent_ppo2.py:185][0m |          -0.0042 |         168.4743 |        -109.8010 |
[32m[20221214 14:28:51 @agent_ppo2.py:185][0m |           0.0049 |         175.6795 |        -109.7610 |
[32m[20221214 14:28:51 @agent_ppo2.py:185][0m |           0.0033 |         174.5335 |        -109.9792 |
[32m[20221214 14:28:51 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:28:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 753.24
[32m[20221214 14:28:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 763.34
[32m[20221214 14:28:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.35
[32m[20221214 14:28:51 @agent_ppo2.py:143][0m Total time:      30.82 min
[32m[20221214 14:28:51 @agent_ppo2.py:145][0m 2836480 total steps have happened
[32m[20221214 14:28:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1385 --------------------------#
[32m[20221214 14:28:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:51 @agent_ppo2.py:185][0m |           0.0088 |         193.4329 |        -106.4083 |
[32m[20221214 14:28:52 @agent_ppo2.py:185][0m |          -0.0017 |         178.4729 |        -107.1360 |
[32m[20221214 14:28:52 @agent_ppo2.py:185][0m |           0.0002 |         177.5857 |        -106.9779 |
[32m[20221214 14:28:52 @agent_ppo2.py:185][0m |          -0.0020 |         175.3070 |        -106.9809 |
[32m[20221214 14:28:52 @agent_ppo2.py:185][0m |           0.0002 |         173.4916 |        -107.0250 |
[32m[20221214 14:28:52 @agent_ppo2.py:185][0m |          -0.0009 |         173.0993 |        -106.9357 |
[32m[20221214 14:28:52 @agent_ppo2.py:185][0m |          -0.0016 |         171.8125 |        -107.1848 |
[32m[20221214 14:28:52 @agent_ppo2.py:185][0m |           0.0110 |         184.6006 |        -107.0326 |
[32m[20221214 14:28:52 @agent_ppo2.py:185][0m |          -0.0012 |         171.9180 |        -106.8600 |
[32m[20221214 14:28:52 @agent_ppo2.py:185][0m |          -0.0002 |         170.7184 |        -107.0425 |
[32m[20221214 14:28:52 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:28:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 728.20
[32m[20221214 14:28:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.97
[32m[20221214 14:28:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 750.53
[32m[20221214 14:28:52 @agent_ppo2.py:143][0m Total time:      30.84 min
[32m[20221214 14:28:52 @agent_ppo2.py:145][0m 2838528 total steps have happened
[32m[20221214 14:28:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1386 --------------------------#
[32m[20221214 14:28:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:53 @agent_ppo2.py:185][0m |           0.0035 |         199.5954 |        -110.0936 |
[32m[20221214 14:28:53 @agent_ppo2.py:185][0m |          -0.0022 |         185.9827 |        -110.0143 |
[32m[20221214 14:28:53 @agent_ppo2.py:185][0m |          -0.0046 |         179.4827 |        -109.9739 |
[32m[20221214 14:28:53 @agent_ppo2.py:185][0m |          -0.0017 |         174.2472 |        -109.9068 |
[32m[20221214 14:28:53 @agent_ppo2.py:185][0m |          -0.0022 |         170.5873 |        -109.8649 |
[32m[20221214 14:28:53 @agent_ppo2.py:185][0m |          -0.0030 |         169.0908 |        -109.9082 |
[32m[20221214 14:28:53 @agent_ppo2.py:185][0m |          -0.0035 |         167.0561 |        -109.7384 |
[32m[20221214 14:28:53 @agent_ppo2.py:185][0m |           0.0002 |         167.5896 |        -109.8629 |
[32m[20221214 14:28:54 @agent_ppo2.py:185][0m |           0.0017 |         168.3890 |        -109.8483 |
[32m[20221214 14:28:54 @agent_ppo2.py:185][0m |          -0.0040 |         164.5586 |        -109.6025 |
[32m[20221214 14:28:54 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:28:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 752.29
[32m[20221214 14:28:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 760.67
[32m[20221214 14:28:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 738.56
[32m[20221214 14:28:54 @agent_ppo2.py:143][0m Total time:      30.86 min
[32m[20221214 14:28:54 @agent_ppo2.py:145][0m 2840576 total steps have happened
[32m[20221214 14:28:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1387 --------------------------#
[32m[20221214 14:28:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:54 @agent_ppo2.py:185][0m |           0.0005 |         182.3579 |        -108.2581 |
[32m[20221214 14:28:54 @agent_ppo2.py:185][0m |          -0.0015 |         172.7974 |        -108.5477 |
[32m[20221214 14:28:54 @agent_ppo2.py:185][0m |          -0.0008 |         168.0658 |        -108.3678 |
[32m[20221214 14:28:54 @agent_ppo2.py:185][0m |          -0.0004 |         166.6608 |        -108.6370 |
[32m[20221214 14:28:55 @agent_ppo2.py:185][0m |          -0.0019 |         163.0004 |        -108.8205 |
[32m[20221214 14:28:55 @agent_ppo2.py:185][0m |           0.0007 |         161.6580 |        -108.6207 |
[32m[20221214 14:28:55 @agent_ppo2.py:185][0m |          -0.0036 |         159.9982 |        -108.5846 |
[32m[20221214 14:28:55 @agent_ppo2.py:185][0m |           0.0053 |         171.9095 |        -108.7553 |
[32m[20221214 14:28:55 @agent_ppo2.py:185][0m |          -0.0025 |         158.2973 |        -108.5609 |
[32m[20221214 14:28:55 @agent_ppo2.py:185][0m |           0.0063 |         159.8587 |        -108.6766 |
[32m[20221214 14:28:55 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:28:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.96
[32m[20221214 14:28:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 756.08
[32m[20221214 14:28:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.25
[32m[20221214 14:28:55 @agent_ppo2.py:143][0m Total time:      30.88 min
[32m[20221214 14:28:55 @agent_ppo2.py:145][0m 2842624 total steps have happened
[32m[20221214 14:28:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1388 --------------------------#
[32m[20221214 14:28:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:55 @agent_ppo2.py:185][0m |          -0.0038 |         181.1122 |        -108.2254 |
[32m[20221214 14:28:56 @agent_ppo2.py:185][0m |          -0.0018 |         166.9798 |        -108.2563 |
[32m[20221214 14:28:56 @agent_ppo2.py:185][0m |          -0.0025 |         159.1151 |        -108.2883 |
[32m[20221214 14:28:56 @agent_ppo2.py:185][0m |          -0.0025 |         153.4033 |        -108.1095 |
[32m[20221214 14:28:56 @agent_ppo2.py:185][0m |          -0.0030 |         150.0193 |        -108.1667 |
[32m[20221214 14:28:56 @agent_ppo2.py:185][0m |          -0.0049 |         148.0626 |        -108.4741 |
[32m[20221214 14:28:56 @agent_ppo2.py:185][0m |          -0.0062 |         145.9301 |        -108.2084 |
[32m[20221214 14:28:56 @agent_ppo2.py:185][0m |          -0.0070 |         144.9183 |        -108.5443 |
[32m[20221214 14:28:56 @agent_ppo2.py:185][0m |          -0.0031 |         144.2424 |        -108.3111 |
[32m[20221214 14:28:56 @agent_ppo2.py:185][0m |           0.0092 |         165.8134 |        -108.4178 |
[32m[20221214 14:28:56 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:28:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 730.74
[32m[20221214 14:28:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.66
[32m[20221214 14:28:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 754.37
[32m[20221214 14:28:56 @agent_ppo2.py:143][0m Total time:      30.91 min
[32m[20221214 14:28:56 @agent_ppo2.py:145][0m 2844672 total steps have happened
[32m[20221214 14:28:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1389 --------------------------#
[32m[20221214 14:28:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:57 @agent_ppo2.py:185][0m |           0.0016 |         175.2619 |        -109.7034 |
[32m[20221214 14:28:57 @agent_ppo2.py:185][0m |          -0.0023 |         166.6607 |        -110.1201 |
[32m[20221214 14:28:57 @agent_ppo2.py:185][0m |           0.0084 |         172.5036 |        -109.9510 |
[32m[20221214 14:28:57 @agent_ppo2.py:185][0m |          -0.0031 |         163.2507 |        -109.8982 |
[32m[20221214 14:28:57 @agent_ppo2.py:185][0m |          -0.0006 |         159.5592 |        -109.9195 |
[32m[20221214 14:28:57 @agent_ppo2.py:185][0m |          -0.0043 |         158.2558 |        -109.7853 |
[32m[20221214 14:28:57 @agent_ppo2.py:185][0m |          -0.0027 |         157.0490 |        -109.8401 |
[32m[20221214 14:28:57 @agent_ppo2.py:185][0m |           0.0097 |         169.6879 |        -110.1225 |
[32m[20221214 14:28:58 @agent_ppo2.py:185][0m |          -0.0018 |         155.6291 |        -109.8629 |
[32m[20221214 14:28:58 @agent_ppo2.py:185][0m |           0.0185 |         170.2325 |        -109.9804 |
[32m[20221214 14:28:58 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:28:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 745.83
[32m[20221214 14:28:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 756.09
[32m[20221214 14:28:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 733.23
[32m[20221214 14:28:58 @agent_ppo2.py:143][0m Total time:      30.93 min
[32m[20221214 14:28:58 @agent_ppo2.py:145][0m 2846720 total steps have happened
[32m[20221214 14:28:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1390 --------------------------#
[32m[20221214 14:28:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:28:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:28:58 @agent_ppo2.py:185][0m |           0.0009 |         123.8123 |        -111.0771 |
[32m[20221214 14:28:58 @agent_ppo2.py:185][0m |           0.0049 |         101.2339 |        -111.0985 |
[32m[20221214 14:28:58 @agent_ppo2.py:185][0m |           0.0040 |          88.8511 |        -111.1820 |
[32m[20221214 14:28:58 @agent_ppo2.py:185][0m |           0.0019 |          82.2551 |        -110.7649 |
[32m[20221214 14:28:59 @agent_ppo2.py:185][0m |          -0.0036 |          79.1848 |        -111.2369 |
[32m[20221214 14:28:59 @agent_ppo2.py:185][0m |          -0.0042 |          76.5159 |        -110.7187 |
[32m[20221214 14:28:59 @agent_ppo2.py:185][0m |          -0.0019 |          75.4144 |        -110.8233 |
[32m[20221214 14:28:59 @agent_ppo2.py:185][0m |           0.0013 |          73.3162 |        -110.8782 |
[32m[20221214 14:28:59 @agent_ppo2.py:185][0m |          -0.0034 |          72.0013 |        -110.7055 |
[32m[20221214 14:28:59 @agent_ppo2.py:185][0m |          -0.0040 |          71.0589 |        -110.8026 |
[32m[20221214 14:28:59 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:28:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.76
[32m[20221214 14:28:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.01
[32m[20221214 14:28:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 677.37
[32m[20221214 14:28:59 @agent_ppo2.py:143][0m Total time:      30.95 min
[32m[20221214 14:28:59 @agent_ppo2.py:145][0m 2848768 total steps have happened
[32m[20221214 14:28:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1391 --------------------------#
[32m[20221214 14:28:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:28:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:00 @agent_ppo2.py:185][0m |           0.0015 |          99.4909 |        -109.5836 |
[32m[20221214 14:29:00 @agent_ppo2.py:185][0m |          -0.0018 |          78.5306 |        -109.6342 |
[32m[20221214 14:29:00 @agent_ppo2.py:185][0m |          -0.0028 |          70.7953 |        -109.7318 |
[32m[20221214 14:29:00 @agent_ppo2.py:185][0m |          -0.0003 |          65.2316 |        -109.7534 |
[32m[20221214 14:29:00 @agent_ppo2.py:185][0m |           0.0013 |          61.7094 |        -109.4758 |
[32m[20221214 14:29:00 @agent_ppo2.py:185][0m |          -0.0046 |          59.9711 |        -109.5573 |
[32m[20221214 14:29:00 @agent_ppo2.py:185][0m |          -0.0016 |          56.4304 |        -109.2185 |
[32m[20221214 14:29:00 @agent_ppo2.py:185][0m |           0.0088 |          57.1754 |        -109.3575 |
[32m[20221214 14:29:00 @agent_ppo2.py:185][0m |           0.0031 |          55.1038 |        -109.2536 |
[32m[20221214 14:29:00 @agent_ppo2.py:185][0m |          -0.0080 |          52.3275 |        -109.0603 |
[32m[20221214 14:29:00 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:29:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 684.79
[32m[20221214 14:29:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.94
[32m[20221214 14:29:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 737.21
[32m[20221214 14:29:00 @agent_ppo2.py:143][0m Total time:      30.97 min
[32m[20221214 14:29:00 @agent_ppo2.py:145][0m 2850816 total steps have happened
[32m[20221214 14:29:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1392 --------------------------#
[32m[20221214 14:29:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:29:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:01 @agent_ppo2.py:185][0m |          -0.0039 |         170.9200 |        -108.9195 |
[32m[20221214 14:29:01 @agent_ppo2.py:185][0m |          -0.0038 |         151.3119 |        -108.8294 |
[32m[20221214 14:29:01 @agent_ppo2.py:185][0m |          -0.0011 |         146.8597 |        -108.8328 |
[32m[20221214 14:29:01 @agent_ppo2.py:185][0m |          -0.0023 |         140.5327 |        -108.6436 |
[32m[20221214 14:29:01 @agent_ppo2.py:185][0m |          -0.0056 |         135.4527 |        -108.6910 |
[32m[20221214 14:29:01 @agent_ppo2.py:185][0m |          -0.0030 |         134.4972 |        -108.6322 |
[32m[20221214 14:29:01 @agent_ppo2.py:185][0m |          -0.0065 |         133.1100 |        -108.6732 |
[32m[20221214 14:29:02 @agent_ppo2.py:185][0m |          -0.0048 |         131.1104 |        -108.5161 |
[32m[20221214 14:29:02 @agent_ppo2.py:185][0m |          -0.0032 |         129.6191 |        -108.6331 |
[32m[20221214 14:29:02 @agent_ppo2.py:185][0m |          -0.0039 |         128.1186 |        -108.4254 |
[32m[20221214 14:29:02 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:29:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 715.63
[32m[20221214 14:29:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 748.35
[32m[20221214 14:29:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 736.43
[32m[20221214 14:29:02 @agent_ppo2.py:143][0m Total time:      31.00 min
[32m[20221214 14:29:02 @agent_ppo2.py:145][0m 2852864 total steps have happened
[32m[20221214 14:29:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1393 --------------------------#
[32m[20221214 14:29:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:02 @agent_ppo2.py:185][0m |          -0.0040 |          69.8771 |        -106.6818 |
[32m[20221214 14:29:02 @agent_ppo2.py:185][0m |          -0.0013 |          58.2404 |        -106.5910 |
[32m[20221214 14:29:02 @agent_ppo2.py:185][0m |          -0.0091 |          52.9047 |        -106.6604 |
[32m[20221214 14:29:03 @agent_ppo2.py:185][0m |          -0.0085 |          49.4026 |        -106.6335 |
[32m[20221214 14:29:03 @agent_ppo2.py:185][0m |          -0.0012 |          46.6642 |        -106.6358 |
[32m[20221214 14:29:03 @agent_ppo2.py:185][0m |          -0.0050 |          45.0458 |        -106.6467 |
[32m[20221214 14:29:03 @agent_ppo2.py:185][0m |          -0.0049 |          44.4468 |        -106.5101 |
[32m[20221214 14:29:03 @agent_ppo2.py:185][0m |          -0.0085 |          42.5330 |        -106.5338 |
[32m[20221214 14:29:03 @agent_ppo2.py:185][0m |          -0.0060 |          41.8888 |        -106.7749 |
[32m[20221214 14:29:03 @agent_ppo2.py:185][0m |          -0.0008 |          40.7421 |        -106.4970 |
[32m[20221214 14:29:03 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:29:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 722.62
[32m[20221214 14:29:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 744.04
[32m[20221214 14:29:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 664.39
[32m[20221214 14:29:03 @agent_ppo2.py:143][0m Total time:      31.02 min
[32m[20221214 14:29:03 @agent_ppo2.py:145][0m 2854912 total steps have happened
[32m[20221214 14:29:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1394 --------------------------#
[32m[20221214 14:29:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:04 @agent_ppo2.py:185][0m |           0.0009 |         136.5572 |        -108.8009 |
[32m[20221214 14:29:04 @agent_ppo2.py:185][0m |          -0.0024 |         115.8246 |        -108.8177 |
[32m[20221214 14:29:04 @agent_ppo2.py:185][0m |          -0.0056 |         107.4778 |        -108.8080 |
[32m[20221214 14:29:04 @agent_ppo2.py:185][0m |           0.0010 |         102.7458 |        -109.1700 |
[32m[20221214 14:29:04 @agent_ppo2.py:185][0m |          -0.0063 |          97.1530 |        -108.8193 |
[32m[20221214 14:29:04 @agent_ppo2.py:185][0m |          -0.0059 |          93.2127 |        -108.8682 |
[32m[20221214 14:29:04 @agent_ppo2.py:185][0m |          -0.0031 |          90.4502 |        -108.8743 |
[32m[20221214 14:29:04 @agent_ppo2.py:185][0m |          -0.0057 |          88.8455 |        -108.8984 |
[32m[20221214 14:29:04 @agent_ppo2.py:185][0m |          -0.0028 |          86.2964 |        -108.8018 |
[32m[20221214 14:29:04 @agent_ppo2.py:185][0m |          -0.0049 |          84.0405 |        -108.8789 |
[32m[20221214 14:29:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:29:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.64
[32m[20221214 14:29:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 744.09
[32m[20221214 14:29:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 682.70
[32m[20221214 14:29:05 @agent_ppo2.py:143][0m Total time:      31.04 min
[32m[20221214 14:29:05 @agent_ppo2.py:145][0m 2856960 total steps have happened
[32m[20221214 14:29:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1395 --------------------------#
[32m[20221214 14:29:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:05 @agent_ppo2.py:185][0m |          -0.0004 |         162.8949 |        -108.6646 |
[32m[20221214 14:29:05 @agent_ppo2.py:185][0m |          -0.0052 |         144.3471 |        -109.1303 |
[32m[20221214 14:29:05 @agent_ppo2.py:185][0m |           0.0031 |         139.8298 |        -108.6341 |
[32m[20221214 14:29:05 @agent_ppo2.py:185][0m |          -0.0033 |         132.2300 |        -109.4592 |
[32m[20221214 14:29:05 @agent_ppo2.py:185][0m |          -0.0026 |         128.5327 |        -109.0154 |
[32m[20221214 14:29:05 @agent_ppo2.py:185][0m |          -0.0010 |         127.7790 |        -109.5877 |
[32m[20221214 14:29:06 @agent_ppo2.py:185][0m |          -0.0038 |         124.2865 |        -109.3693 |
[32m[20221214 14:29:06 @agent_ppo2.py:185][0m |           0.0007 |         122.2995 |        -109.1487 |
[32m[20221214 14:29:06 @agent_ppo2.py:185][0m |           0.0073 |         133.2883 |        -109.4540 |
[32m[20221214 14:29:06 @agent_ppo2.py:185][0m |           0.0047 |         123.0069 |        -109.3237 |
[32m[20221214 14:29:06 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:29:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.96
[32m[20221214 14:29:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 753.26
[32m[20221214 14:29:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.57
[32m[20221214 14:29:06 @agent_ppo2.py:143][0m Total time:      31.07 min
[32m[20221214 14:29:06 @agent_ppo2.py:145][0m 2859008 total steps have happened
[32m[20221214 14:29:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1396 --------------------------#
[32m[20221214 14:29:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:06 @agent_ppo2.py:185][0m |          -0.0036 |         107.9241 |        -111.2266 |
[32m[20221214 14:29:06 @agent_ppo2.py:185][0m |          -0.0004 |          96.1767 |        -111.1104 |
[32m[20221214 14:29:07 @agent_ppo2.py:185][0m |           0.0023 |          94.0472 |        -111.0480 |
[32m[20221214 14:29:07 @agent_ppo2.py:185][0m |          -0.0021 |          91.2038 |        -111.2583 |
[32m[20221214 14:29:07 @agent_ppo2.py:185][0m |          -0.0006 |          89.4389 |        -110.9553 |
[32m[20221214 14:29:07 @agent_ppo2.py:185][0m |          -0.0017 |          87.6922 |        -110.9912 |
[32m[20221214 14:29:07 @agent_ppo2.py:185][0m |          -0.0043 |          86.3424 |        -111.2163 |
[32m[20221214 14:29:07 @agent_ppo2.py:185][0m |          -0.0040 |          84.8854 |        -110.8915 |
[32m[20221214 14:29:07 @agent_ppo2.py:185][0m |          -0.0041 |          83.8459 |        -111.1327 |
[32m[20221214 14:29:07 @agent_ppo2.py:185][0m |          -0.0037 |          83.1119 |        -111.1473 |
[32m[20221214 14:29:07 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:29:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 695.33
[32m[20221214 14:29:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.94
[32m[20221214 14:29:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 769.37
[32m[20221214 14:29:07 @agent_ppo2.py:143][0m Total time:      31.09 min
[32m[20221214 14:29:07 @agent_ppo2.py:145][0m 2861056 total steps have happened
[32m[20221214 14:29:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1397 --------------------------#
[32m[20221214 14:29:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:08 @agent_ppo2.py:185][0m |           0.0019 |         212.3347 |        -109.2867 |
[32m[20221214 14:29:08 @agent_ppo2.py:185][0m |           0.0029 |         199.5839 |        -109.3385 |
[32m[20221214 14:29:08 @agent_ppo2.py:185][0m |          -0.0009 |         195.2576 |        -109.0849 |
[32m[20221214 14:29:08 @agent_ppo2.py:185][0m |          -0.0010 |         191.3687 |        -109.2278 |
[32m[20221214 14:29:08 @agent_ppo2.py:185][0m |           0.0063 |         207.9195 |        -109.0392 |
[32m[20221214 14:29:08 @agent_ppo2.py:185][0m |           0.0049 |         195.2220 |        -109.1803 |
[32m[20221214 14:29:08 @agent_ppo2.py:185][0m |          -0.0019 |         185.9645 |        -109.0476 |
[32m[20221214 14:29:08 @agent_ppo2.py:185][0m |          -0.0027 |         185.7126 |        -109.3096 |
[32m[20221214 14:29:08 @agent_ppo2.py:185][0m |          -0.0042 |         185.5044 |        -109.4429 |
[32m[20221214 14:29:09 @agent_ppo2.py:185][0m |          -0.0015 |         183.6614 |        -109.1356 |
[32m[20221214 14:29:09 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:29:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.71
[32m[20221214 14:29:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 784.69
[32m[20221214 14:29:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 775.10
[32m[20221214 14:29:09 @agent_ppo2.py:143][0m Total time:      31.11 min
[32m[20221214 14:29:09 @agent_ppo2.py:145][0m 2863104 total steps have happened
[32m[20221214 14:29:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1398 --------------------------#
[32m[20221214 14:29:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:09 @agent_ppo2.py:185][0m |          -0.0049 |         208.4750 |        -108.9126 |
[32m[20221214 14:29:09 @agent_ppo2.py:185][0m |           0.0010 |         190.0277 |        -108.8801 |
[32m[20221214 14:29:09 @agent_ppo2.py:185][0m |           0.0008 |         188.8098 |        -109.1021 |
[32m[20221214 14:29:09 @agent_ppo2.py:185][0m |          -0.0033 |         181.1028 |        -108.9519 |
[32m[20221214 14:29:09 @agent_ppo2.py:185][0m |          -0.0002 |         177.3299 |        -108.9311 |
[32m[20221214 14:29:10 @agent_ppo2.py:185][0m |           0.0046 |         183.0284 |        -108.7251 |
[32m[20221214 14:29:10 @agent_ppo2.py:185][0m |          -0.0021 |         174.2951 |        -108.7487 |
[32m[20221214 14:29:10 @agent_ppo2.py:185][0m |          -0.0027 |         173.0714 |        -108.5811 |
[32m[20221214 14:29:10 @agent_ppo2.py:185][0m |          -0.0043 |         173.5758 |        -108.8679 |
[32m[20221214 14:29:10 @agent_ppo2.py:185][0m |          -0.0017 |         171.5034 |        -108.8043 |
[32m[20221214 14:29:10 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:29:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.96
[32m[20221214 14:29:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.14
[32m[20221214 14:29:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 789.62
[32m[20221214 14:29:10 @agent_ppo2.py:143][0m Total time:      31.13 min
[32m[20221214 14:29:10 @agent_ppo2.py:145][0m 2865152 total steps have happened
[32m[20221214 14:29:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1399 --------------------------#
[32m[20221214 14:29:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:10 @agent_ppo2.py:185][0m |           0.0011 |         212.4236 |        -109.4282 |
[32m[20221214 14:29:11 @agent_ppo2.py:185][0m |           0.0001 |         200.4487 |        -109.4442 |
[32m[20221214 14:29:11 @agent_ppo2.py:185][0m |          -0.0023 |         196.4693 |        -109.5595 |
[32m[20221214 14:29:11 @agent_ppo2.py:185][0m |           0.0015 |         194.1065 |        -109.5361 |
[32m[20221214 14:29:11 @agent_ppo2.py:185][0m |           0.0005 |         192.7591 |        -109.7738 |
[32m[20221214 14:29:11 @agent_ppo2.py:185][0m |          -0.0004 |         192.1821 |        -109.6722 |
[32m[20221214 14:29:11 @agent_ppo2.py:185][0m |          -0.0023 |         190.5777 |        -109.8456 |
[32m[20221214 14:29:11 @agent_ppo2.py:185][0m |          -0.0014 |         189.2091 |        -109.9912 |
[32m[20221214 14:29:11 @agent_ppo2.py:185][0m |          -0.0033 |         187.3521 |        -110.1913 |
[32m[20221214 14:29:11 @agent_ppo2.py:185][0m |          -0.0020 |         187.0745 |        -109.9595 |
[32m[20221214 14:29:11 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:29:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.80
[32m[20221214 14:29:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.78
[32m[20221214 14:29:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 791.74
[32m[20221214 14:29:11 @agent_ppo2.py:143][0m Total time:      31.16 min
[32m[20221214 14:29:11 @agent_ppo2.py:145][0m 2867200 total steps have happened
[32m[20221214 14:29:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1400 --------------------------#
[32m[20221214 14:29:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:29:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:12 @agent_ppo2.py:185][0m |           0.0011 |         204.8627 |        -109.3528 |
[32m[20221214 14:29:12 @agent_ppo2.py:185][0m |          -0.0032 |         199.6500 |        -109.0225 |
[32m[20221214 14:29:12 @agent_ppo2.py:185][0m |          -0.0001 |         196.7986 |        -109.5312 |
[32m[20221214 14:29:12 @agent_ppo2.py:185][0m |          -0.0009 |         195.0355 |        -109.6163 |
[32m[20221214 14:29:12 @agent_ppo2.py:185][0m |          -0.0032 |         193.8653 |        -109.4853 |
[32m[20221214 14:29:12 @agent_ppo2.py:185][0m |           0.0003 |         191.9485 |        -109.8306 |
[32m[20221214 14:29:12 @agent_ppo2.py:185][0m |           0.0097 |         205.3613 |        -109.5239 |
[32m[20221214 14:29:12 @agent_ppo2.py:185][0m |           0.0105 |         201.1118 |        -109.8667 |
[32m[20221214 14:29:13 @agent_ppo2.py:185][0m |          -0.0029 |         192.7095 |        -109.6539 |
[32m[20221214 14:29:13 @agent_ppo2.py:185][0m |           0.0008 |         190.9633 |        -109.7156 |
[32m[20221214 14:29:13 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:29:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.60
[32m[20221214 14:29:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.88
[32m[20221214 14:29:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.80
[32m[20221214 14:29:13 @agent_ppo2.py:143][0m Total time:      31.18 min
[32m[20221214 14:29:13 @agent_ppo2.py:145][0m 2869248 total steps have happened
[32m[20221214 14:29:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1401 --------------------------#
[32m[20221214 14:29:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:13 @agent_ppo2.py:185][0m |           0.0014 |         151.4623 |        -110.9430 |
[32m[20221214 14:29:13 @agent_ppo2.py:185][0m |          -0.0019 |         141.4525 |        -110.5574 |
[32m[20221214 14:29:13 @agent_ppo2.py:185][0m |          -0.0021 |         138.7970 |        -110.5701 |
[32m[20221214 14:29:13 @agent_ppo2.py:185][0m |          -0.0010 |         137.1126 |        -110.6789 |
[32m[20221214 14:29:14 @agent_ppo2.py:185][0m |          -0.0026 |         137.1831 |        -110.7863 |
[32m[20221214 14:29:14 @agent_ppo2.py:185][0m |           0.0008 |         136.4519 |        -110.7027 |
[32m[20221214 14:29:14 @agent_ppo2.py:185][0m |           0.0153 |         150.8007 |        -110.6833 |
[32m[20221214 14:29:14 @agent_ppo2.py:185][0m |           0.0025 |         135.6644 |        -110.5731 |
[32m[20221214 14:29:14 @agent_ppo2.py:185][0m |          -0.0006 |         133.9230 |        -110.5839 |
[32m[20221214 14:29:14 @agent_ppo2.py:185][0m |           0.0003 |         133.4162 |        -110.4437 |
[32m[20221214 14:29:14 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:29:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 788.45
[32m[20221214 14:29:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.92
[32m[20221214 14:29:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.71
[32m[20221214 14:29:14 @agent_ppo2.py:143][0m Total time:      31.20 min
[32m[20221214 14:29:14 @agent_ppo2.py:145][0m 2871296 total steps have happened
[32m[20221214 14:29:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1402 --------------------------#
[32m[20221214 14:29:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:15 @agent_ppo2.py:185][0m |          -0.0020 |         165.8194 |        -113.3113 |
[32m[20221214 14:29:15 @agent_ppo2.py:185][0m |           0.0017 |         160.3247 |        -112.9525 |
[32m[20221214 14:29:15 @agent_ppo2.py:185][0m |          -0.0014 |         156.8586 |        -113.1211 |
[32m[20221214 14:29:15 @agent_ppo2.py:185][0m |          -0.0004 |         155.5580 |        -113.1198 |
[32m[20221214 14:29:15 @agent_ppo2.py:185][0m |           0.0051 |         160.2515 |        -113.0986 |
[32m[20221214 14:29:15 @agent_ppo2.py:185][0m |          -0.0021 |         151.8486 |        -113.3296 |
[32m[20221214 14:29:15 @agent_ppo2.py:185][0m |          -0.0029 |         150.3592 |        -112.9603 |
[32m[20221214 14:29:15 @agent_ppo2.py:185][0m |          -0.0056 |         149.9073 |        -112.9767 |
[32m[20221214 14:29:15 @agent_ppo2.py:185][0m |          -0.0013 |         148.2317 |        -113.3506 |
[32m[20221214 14:29:15 @agent_ppo2.py:185][0m |           0.0048 |         151.5174 |        -113.4126 |
[32m[20221214 14:29:15 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:29:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.11
[32m[20221214 14:29:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.49
[32m[20221214 14:29:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.77
[32m[20221214 14:29:16 @agent_ppo2.py:143][0m Total time:      31.22 min
[32m[20221214 14:29:16 @agent_ppo2.py:145][0m 2873344 total steps have happened
[32m[20221214 14:29:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1403 --------------------------#
[32m[20221214 14:29:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:16 @agent_ppo2.py:185][0m |          -0.0011 |         140.5430 |        -111.9864 |
[32m[20221214 14:29:16 @agent_ppo2.py:185][0m |          -0.0016 |         136.6044 |        -112.4184 |
[32m[20221214 14:29:16 @agent_ppo2.py:185][0m |          -0.0013 |         134.3623 |        -112.0987 |
[32m[20221214 14:29:16 @agent_ppo2.py:185][0m |          -0.0005 |         133.3877 |        -112.1120 |
[32m[20221214 14:29:16 @agent_ppo2.py:185][0m |          -0.0009 |         133.4555 |        -112.1065 |
[32m[20221214 14:29:16 @agent_ppo2.py:185][0m |           0.0011 |         133.7926 |        -111.9473 |
[32m[20221214 14:29:16 @agent_ppo2.py:185][0m |           0.0091 |         141.5257 |        -111.8781 |
[32m[20221214 14:29:17 @agent_ppo2.py:185][0m |          -0.0005 |         132.7608 |        -111.7815 |
[32m[20221214 14:29:17 @agent_ppo2.py:185][0m |          -0.0037 |         131.4855 |        -111.8367 |
[32m[20221214 14:29:17 @agent_ppo2.py:185][0m |          -0.0021 |         130.7954 |        -111.7888 |
[32m[20221214 14:29:17 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:29:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.25
[32m[20221214 14:29:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.31
[32m[20221214 14:29:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 791.97
[32m[20221214 14:29:17 @agent_ppo2.py:143][0m Total time:      31.25 min
[32m[20221214 14:29:17 @agent_ppo2.py:145][0m 2875392 total steps have happened
[32m[20221214 14:29:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1404 --------------------------#
[32m[20221214 14:29:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:29:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:17 @agent_ppo2.py:185][0m |           0.0023 |         211.0492 |        -110.0111 |
[32m[20221214 14:29:17 @agent_ppo2.py:185][0m |          -0.0018 |         200.9027 |        -110.4626 |
[32m[20221214 14:29:17 @agent_ppo2.py:185][0m |           0.0141 |         209.0748 |        -110.5399 |
[32m[20221214 14:29:18 @agent_ppo2.py:185][0m |          -0.0021 |         194.6377 |        -110.4674 |
[32m[20221214 14:29:18 @agent_ppo2.py:185][0m |          -0.0006 |         192.8254 |        -110.4934 |
[32m[20221214 14:29:18 @agent_ppo2.py:185][0m |           0.0005 |         191.5497 |        -110.4902 |
[32m[20221214 14:29:18 @agent_ppo2.py:185][0m |           0.0013 |         190.6021 |        -110.7319 |
[32m[20221214 14:29:18 @agent_ppo2.py:185][0m |          -0.0027 |         189.4061 |        -110.4932 |
[32m[20221214 14:29:18 @agent_ppo2.py:185][0m |          -0.0000 |         187.9090 |        -110.2377 |
[32m[20221214 14:29:18 @agent_ppo2.py:185][0m |          -0.0005 |         186.4160 |        -110.6850 |
[32m[20221214 14:29:18 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:29:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.10
[32m[20221214 14:29:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 791.13
[32m[20221214 14:29:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.72
[32m[20221214 14:29:18 @agent_ppo2.py:143][0m Total time:      31.27 min
[32m[20221214 14:29:18 @agent_ppo2.py:145][0m 2877440 total steps have happened
[32m[20221214 14:29:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1405 --------------------------#
[32m[20221214 14:29:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:19 @agent_ppo2.py:185][0m |          -0.0006 |         208.9975 |        -111.7588 |
[32m[20221214 14:29:19 @agent_ppo2.py:185][0m |          -0.0040 |         198.0255 |        -111.3636 |
[32m[20221214 14:29:19 @agent_ppo2.py:185][0m |          -0.0044 |         193.3211 |        -111.2264 |
[32m[20221214 14:29:19 @agent_ppo2.py:185][0m |           0.0070 |         195.3893 |        -111.4844 |
[32m[20221214 14:29:19 @agent_ppo2.py:185][0m |          -0.0040 |         186.8934 |        -111.1695 |
[32m[20221214 14:29:19 @agent_ppo2.py:185][0m |           0.0071 |         197.2368 |        -110.7910 |
[32m[20221214 14:29:19 @agent_ppo2.py:185][0m |          -0.0008 |         185.0495 |        -111.7183 |
[32m[20221214 14:29:19 @agent_ppo2.py:185][0m |          -0.0013 |         182.3071 |        -111.1931 |
[32m[20221214 14:29:19 @agent_ppo2.py:185][0m |          -0.0044 |         182.6152 |        -111.2010 |
[32m[20221214 14:29:19 @agent_ppo2.py:185][0m |          -0.0057 |         181.5164 |        -111.6483 |
[32m[20221214 14:29:19 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:29:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.76
[32m[20221214 14:29:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.41
[32m[20221214 14:29:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.23
[32m[20221214 14:29:20 @agent_ppo2.py:143][0m Total time:      31.29 min
[32m[20221214 14:29:20 @agent_ppo2.py:145][0m 2879488 total steps have happened
[32m[20221214 14:29:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1406 --------------------------#
[32m[20221214 14:29:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:20 @agent_ppo2.py:185][0m |          -0.0019 |         237.5795 |        -110.5117 |
[32m[20221214 14:29:20 @agent_ppo2.py:185][0m |          -0.0008 |         221.9419 |        -110.4953 |
[32m[20221214 14:29:20 @agent_ppo2.py:185][0m |          -0.0040 |         217.8230 |        -110.3017 |
[32m[20221214 14:29:20 @agent_ppo2.py:185][0m |          -0.0026 |         213.0228 |        -110.1386 |
[32m[20221214 14:29:20 @agent_ppo2.py:185][0m |          -0.0025 |         210.3880 |        -110.1865 |
[32m[20221214 14:29:20 @agent_ppo2.py:185][0m |          -0.0020 |         207.2938 |        -110.1104 |
[32m[20221214 14:29:21 @agent_ppo2.py:185][0m |          -0.0042 |         205.6160 |        -109.9658 |
[32m[20221214 14:29:21 @agent_ppo2.py:185][0m |          -0.0012 |         206.3738 |        -109.9308 |
[32m[20221214 14:29:21 @agent_ppo2.py:185][0m |          -0.0035 |         202.7749 |        -109.9275 |
[32m[20221214 14:29:21 @agent_ppo2.py:185][0m |           0.0072 |         216.2056 |        -109.6389 |
[32m[20221214 14:29:21 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:29:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.56
[32m[20221214 14:29:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.33
[32m[20221214 14:29:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 752.01
[32m[20221214 14:29:21 @agent_ppo2.py:143][0m Total time:      31.32 min
[32m[20221214 14:29:21 @agent_ppo2.py:145][0m 2881536 total steps have happened
[32m[20221214 14:29:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1407 --------------------------#
[32m[20221214 14:29:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:21 @agent_ppo2.py:185][0m |           0.0061 |         122.6734 |        -111.7935 |
[32m[20221214 14:29:21 @agent_ppo2.py:185][0m |           0.0013 |          90.0832 |        -111.5731 |
[32m[20221214 14:29:22 @agent_ppo2.py:185][0m |          -0.0008 |          82.5918 |        -111.6280 |
[32m[20221214 14:29:22 @agent_ppo2.py:185][0m |          -0.0028 |          78.9304 |        -111.3133 |
[32m[20221214 14:29:22 @agent_ppo2.py:185][0m |          -0.0008 |          76.4200 |        -111.3121 |
[32m[20221214 14:29:22 @agent_ppo2.py:185][0m |          -0.0018 |          74.6792 |        -111.3946 |
[32m[20221214 14:29:22 @agent_ppo2.py:185][0m |          -0.0019 |          73.4318 |        -111.1904 |
[32m[20221214 14:29:22 @agent_ppo2.py:185][0m |           0.0020 |          71.9052 |        -111.1835 |
[32m[20221214 14:29:22 @agent_ppo2.py:185][0m |           0.0034 |          74.8838 |        -110.9136 |
[32m[20221214 14:29:22 @agent_ppo2.py:185][0m |           0.0098 |          72.0169 |        -110.9083 |
[32m[20221214 14:29:22 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:29:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 721.24
[32m[20221214 14:29:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 765.52
[32m[20221214 14:29:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 764.20
[32m[20221214 14:29:22 @agent_ppo2.py:143][0m Total time:      31.34 min
[32m[20221214 14:29:22 @agent_ppo2.py:145][0m 2883584 total steps have happened
[32m[20221214 14:29:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1408 --------------------------#
[32m[20221214 14:29:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:23 @agent_ppo2.py:185][0m |           0.0021 |         130.1811 |        -109.7157 |
[32m[20221214 14:29:23 @agent_ppo2.py:185][0m |          -0.0069 |          99.1281 |        -109.4174 |
[32m[20221214 14:29:23 @agent_ppo2.py:185][0m |           0.0010 |          90.6185 |        -109.6161 |
[32m[20221214 14:29:23 @agent_ppo2.py:185][0m |          -0.0014 |          86.4613 |        -109.6873 |
[32m[20221214 14:29:23 @agent_ppo2.py:185][0m |           0.0043 |          86.4551 |        -109.6461 |
[32m[20221214 14:29:23 @agent_ppo2.py:185][0m |          -0.0008 |          82.2513 |        -109.3740 |
[32m[20221214 14:29:23 @agent_ppo2.py:185][0m |          -0.0030 |          79.9608 |        -109.0959 |
[32m[20221214 14:29:23 @agent_ppo2.py:185][0m |          -0.0088 |          78.4755 |        -109.4290 |
[32m[20221214 14:29:23 @agent_ppo2.py:185][0m |          -0.0015 |          77.4040 |        -109.6233 |
[32m[20221214 14:29:24 @agent_ppo2.py:185][0m |          -0.0082 |          76.1026 |        -109.4643 |
[32m[20221214 14:29:24 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:29:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 701.62
[32m[20221214 14:29:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 764.75
[32m[20221214 14:29:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 773.92
[32m[20221214 14:29:24 @agent_ppo2.py:143][0m Total time:      31.36 min
[32m[20221214 14:29:24 @agent_ppo2.py:145][0m 2885632 total steps have happened
[32m[20221214 14:29:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1409 --------------------------#
[32m[20221214 14:29:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:24 @agent_ppo2.py:185][0m |           0.0003 |          98.6253 |        -110.7190 |
[32m[20221214 14:29:24 @agent_ppo2.py:185][0m |          -0.0042 |          78.4190 |        -110.7042 |
[32m[20221214 14:29:24 @agent_ppo2.py:185][0m |          -0.0079 |          72.0524 |        -110.6522 |
[32m[20221214 14:29:24 @agent_ppo2.py:185][0m |          -0.0030 |          68.4699 |        -110.6464 |
[32m[20221214 14:29:24 @agent_ppo2.py:185][0m |          -0.0039 |          65.7294 |        -110.4713 |
[32m[20221214 14:29:25 @agent_ppo2.py:185][0m |          -0.0024 |          63.5105 |        -110.4357 |
[32m[20221214 14:29:25 @agent_ppo2.py:185][0m |          -0.0073 |          62.4736 |        -110.2515 |
[32m[20221214 14:29:25 @agent_ppo2.py:185][0m |          -0.0039 |          60.9722 |        -110.3739 |
[32m[20221214 14:29:25 @agent_ppo2.py:185][0m |          -0.0015 |          59.6665 |        -110.3048 |
[32m[20221214 14:29:25 @agent_ppo2.py:185][0m |           0.0033 |          62.1969 |        -110.0815 |
[32m[20221214 14:29:25 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:29:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 723.31
[32m[20221214 14:29:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 768.67
[32m[20221214 14:29:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.93
[32m[20221214 14:29:25 @agent_ppo2.py:143][0m Total time:      31.38 min
[32m[20221214 14:29:25 @agent_ppo2.py:145][0m 2887680 total steps have happened
[32m[20221214 14:29:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1410 --------------------------#
[32m[20221214 14:29:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:29:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:25 @agent_ppo2.py:185][0m |           0.0026 |         183.7176 |        -109.1401 |
[32m[20221214 14:29:26 @agent_ppo2.py:185][0m |           0.0004 |         162.5664 |        -108.8934 |
[32m[20221214 14:29:26 @agent_ppo2.py:185][0m |          -0.0053 |         156.2327 |        -108.6742 |
[32m[20221214 14:29:26 @agent_ppo2.py:185][0m |          -0.0003 |         152.4358 |        -108.7281 |
[32m[20221214 14:29:26 @agent_ppo2.py:185][0m |          -0.0021 |         149.5005 |        -108.7893 |
[32m[20221214 14:29:26 @agent_ppo2.py:185][0m |          -0.0006 |         147.6487 |        -108.5201 |
[32m[20221214 14:29:26 @agent_ppo2.py:185][0m |          -0.0090 |         144.0474 |        -108.4838 |
[32m[20221214 14:29:26 @agent_ppo2.py:185][0m |          -0.0024 |         143.1212 |        -108.2823 |
[32m[20221214 14:29:26 @agent_ppo2.py:185][0m |          -0.0026 |         141.8458 |        -108.2965 |
[32m[20221214 14:29:26 @agent_ppo2.py:185][0m |          -0.0007 |         141.2573 |        -108.0978 |
[32m[20221214 14:29:26 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:29:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 743.00
[32m[20221214 14:29:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.27
[32m[20221214 14:29:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.38
[32m[20221214 14:29:26 @agent_ppo2.py:143][0m Total time:      31.40 min
[32m[20221214 14:29:26 @agent_ppo2.py:145][0m 2889728 total steps have happened
[32m[20221214 14:29:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1411 --------------------------#
[32m[20221214 14:29:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:29:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:27 @agent_ppo2.py:185][0m |          -0.0014 |         175.3614 |        -107.9148 |
[32m[20221214 14:29:27 @agent_ppo2.py:185][0m |           0.0005 |         160.8803 |        -107.9911 |
[32m[20221214 14:29:27 @agent_ppo2.py:185][0m |           0.0150 |         181.0672 |        -108.1447 |
[32m[20221214 14:29:27 @agent_ppo2.py:185][0m |          -0.0005 |         156.2097 |        -108.1042 |
[32m[20221214 14:29:27 @agent_ppo2.py:185][0m |          -0.0007 |         151.2898 |        -108.2541 |
[32m[20221214 14:29:27 @agent_ppo2.py:185][0m |          -0.0039 |         150.5450 |        -108.2092 |
[32m[20221214 14:29:27 @agent_ppo2.py:185][0m |          -0.0026 |         148.9453 |        -108.3509 |
[32m[20221214 14:29:27 @agent_ppo2.py:185][0m |          -0.0028 |         147.7189 |        -108.5268 |
[32m[20221214 14:29:28 @agent_ppo2.py:185][0m |           0.0093 |         160.3839 |        -108.5292 |
[32m[20221214 14:29:28 @agent_ppo2.py:185][0m |          -0.0006 |         147.2944 |        -108.4890 |
[32m[20221214 14:29:28 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 14:29:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.87
[32m[20221214 14:29:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.11
[32m[20221214 14:29:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.98
[32m[20221214 14:29:28 @agent_ppo2.py:143][0m Total time:      31.43 min
[32m[20221214 14:29:28 @agent_ppo2.py:145][0m 2891776 total steps have happened
[32m[20221214 14:29:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1412 --------------------------#
[32m[20221214 14:29:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:28 @agent_ppo2.py:185][0m |          -0.0038 |         161.6065 |        -109.7169 |
[32m[20221214 14:29:28 @agent_ppo2.py:185][0m |           0.0070 |         161.8647 |        -109.7803 |
[32m[20221214 14:29:28 @agent_ppo2.py:185][0m |          -0.0039 |         142.6270 |        -109.7093 |
[32m[20221214 14:29:28 @agent_ppo2.py:185][0m |          -0.0026 |         140.8098 |        -109.6801 |
[32m[20221214 14:29:29 @agent_ppo2.py:185][0m |          -0.0025 |         139.5211 |        -109.8328 |
[32m[20221214 14:29:29 @agent_ppo2.py:185][0m |           0.0005 |         137.9219 |        -109.7575 |
[32m[20221214 14:29:29 @agent_ppo2.py:185][0m |          -0.0007 |         137.1581 |        -109.6628 |
[32m[20221214 14:29:29 @agent_ppo2.py:185][0m |          -0.0017 |         136.4918 |        -109.6801 |
[32m[20221214 14:29:29 @agent_ppo2.py:185][0m |          -0.0024 |         135.9011 |        -109.6105 |
[32m[20221214 14:29:29 @agent_ppo2.py:185][0m |          -0.0014 |         135.0752 |        -109.6043 |
[32m[20221214 14:29:29 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:29:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.31
[32m[20221214 14:29:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.21
[32m[20221214 14:29:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.23
[32m[20221214 14:29:29 @agent_ppo2.py:143][0m Total time:      31.45 min
[32m[20221214 14:29:29 @agent_ppo2.py:145][0m 2893824 total steps have happened
[32m[20221214 14:29:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1413 --------------------------#
[32m[20221214 14:29:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:30 @agent_ppo2.py:185][0m |          -0.0017 |         203.1965 |        -110.2197 |
[32m[20221214 14:29:30 @agent_ppo2.py:185][0m |          -0.0012 |         194.2454 |        -110.0204 |
[32m[20221214 14:29:30 @agent_ppo2.py:185][0m |          -0.0020 |         187.9040 |        -110.1380 |
[32m[20221214 14:29:30 @agent_ppo2.py:185][0m |          -0.0053 |         184.4417 |        -110.2451 |
[32m[20221214 14:29:30 @agent_ppo2.py:185][0m |           0.0025 |         183.1197 |        -109.7747 |
[32m[20221214 14:29:30 @agent_ppo2.py:185][0m |          -0.0014 |         183.3239 |        -110.0422 |
[32m[20221214 14:29:30 @agent_ppo2.py:185][0m |          -0.0058 |         178.8989 |        -109.8672 |
[32m[20221214 14:29:30 @agent_ppo2.py:185][0m |          -0.0009 |         179.4704 |        -109.8270 |
[32m[20221214 14:29:30 @agent_ppo2.py:185][0m |           0.0138 |         203.5913 |        -109.6242 |
[32m[20221214 14:29:30 @agent_ppo2.py:185][0m |          -0.0015 |         180.6425 |        -109.7571 |
[32m[20221214 14:29:30 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:29:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 798.36
[32m[20221214 14:29:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.26
[32m[20221214 14:29:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 788.31
[32m[20221214 14:29:31 @agent_ppo2.py:143][0m Total time:      31.47 min
[32m[20221214 14:29:31 @agent_ppo2.py:145][0m 2895872 total steps have happened
[32m[20221214 14:29:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1414 --------------------------#
[32m[20221214 14:29:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:31 @agent_ppo2.py:185][0m |           0.0084 |         174.3496 |        -107.7912 |
[32m[20221214 14:29:31 @agent_ppo2.py:185][0m |          -0.0024 |         150.9415 |        -108.0615 |
[32m[20221214 14:29:31 @agent_ppo2.py:185][0m |          -0.0019 |         144.4832 |        -107.9856 |
[32m[20221214 14:29:31 @agent_ppo2.py:185][0m |          -0.0022 |         142.3966 |        -108.0557 |
[32m[20221214 14:29:31 @agent_ppo2.py:185][0m |          -0.0007 |         138.9196 |        -107.6908 |
[32m[20221214 14:29:31 @agent_ppo2.py:185][0m |          -0.0045 |         137.4775 |        -108.0006 |
[32m[20221214 14:29:31 @agent_ppo2.py:185][0m |           0.0051 |         135.2134 |        -108.1676 |
[32m[20221214 14:29:32 @agent_ppo2.py:185][0m |          -0.0006 |         132.9911 |        -108.3006 |
[32m[20221214 14:29:32 @agent_ppo2.py:185][0m |          -0.0053 |         133.4775 |        -108.3352 |
[32m[20221214 14:29:32 @agent_ppo2.py:185][0m |          -0.0007 |         131.3375 |        -108.5185 |
[32m[20221214 14:29:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:29:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 761.41
[32m[20221214 14:29:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.86
[32m[20221214 14:29:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 769.96
[32m[20221214 14:29:32 @agent_ppo2.py:143][0m Total time:      31.50 min
[32m[20221214 14:29:32 @agent_ppo2.py:145][0m 2897920 total steps have happened
[32m[20221214 14:29:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1415 --------------------------#
[32m[20221214 14:29:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:32 @agent_ppo2.py:185][0m |          -0.0005 |         182.6038 |        -108.9515 |
[32m[20221214 14:29:32 @agent_ppo2.py:185][0m |          -0.0023 |         163.9072 |        -109.0743 |
[32m[20221214 14:29:32 @agent_ppo2.py:185][0m |          -0.0029 |         156.6309 |        -109.0714 |
[32m[20221214 14:29:33 @agent_ppo2.py:185][0m |          -0.0039 |         152.3472 |        -108.9465 |
[32m[20221214 14:29:33 @agent_ppo2.py:185][0m |          -0.0041 |         149.7840 |        -108.9902 |
[32m[20221214 14:29:33 @agent_ppo2.py:185][0m |          -0.0024 |         146.6998 |        -108.9631 |
[32m[20221214 14:29:33 @agent_ppo2.py:185][0m |          -0.0020 |         144.6883 |        -108.8473 |
[32m[20221214 14:29:33 @agent_ppo2.py:185][0m |          -0.0028 |         143.1613 |        -108.8639 |
[32m[20221214 14:29:33 @agent_ppo2.py:185][0m |          -0.0032 |         141.4579 |        -108.8729 |
[32m[20221214 14:29:33 @agent_ppo2.py:185][0m |          -0.0034 |         140.2388 |        -109.0485 |
[32m[20221214 14:29:33 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:29:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.19
[32m[20221214 14:29:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.93
[32m[20221214 14:29:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 781.93
[32m[20221214 14:29:33 @agent_ppo2.py:143][0m Total time:      31.52 min
[32m[20221214 14:29:33 @agent_ppo2.py:145][0m 2899968 total steps have happened
[32m[20221214 14:29:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1416 --------------------------#
[32m[20221214 14:29:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:34 @agent_ppo2.py:185][0m |          -0.0011 |         161.3834 |        -107.0638 |
[32m[20221214 14:29:34 @agent_ppo2.py:185][0m |           0.0011 |         132.7556 |        -107.1236 |
[32m[20221214 14:29:34 @agent_ppo2.py:185][0m |          -0.0024 |         123.7666 |        -107.2256 |
[32m[20221214 14:29:34 @agent_ppo2.py:185][0m |           0.0115 |         129.3242 |        -107.1730 |
[32m[20221214 14:29:34 @agent_ppo2.py:185][0m |           0.0082 |         127.1300 |        -107.1349 |
[32m[20221214 14:29:34 @agent_ppo2.py:185][0m |          -0.0031 |         112.7645 |        -107.1373 |
[32m[20221214 14:29:34 @agent_ppo2.py:185][0m |          -0.0009 |         111.2818 |        -107.3291 |
[32m[20221214 14:29:34 @agent_ppo2.py:185][0m |          -0.0026 |         109.1100 |        -107.2273 |
[32m[20221214 14:29:34 @agent_ppo2.py:185][0m |          -0.0022 |         107.7513 |        -107.1285 |
[32m[20221214 14:29:34 @agent_ppo2.py:185][0m |          -0.0026 |         105.8377 |        -107.3378 |
[32m[20221214 14:29:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:29:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.07
[32m[20221214 14:29:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.46
[32m[20221214 14:29:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.25
[32m[20221214 14:29:35 @agent_ppo2.py:143][0m Total time:      31.54 min
[32m[20221214 14:29:35 @agent_ppo2.py:145][0m 2902016 total steps have happened
[32m[20221214 14:29:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1417 --------------------------#
[32m[20221214 14:29:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:29:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:35 @agent_ppo2.py:185][0m |           0.0053 |         217.2978 |        -109.2874 |
[32m[20221214 14:29:35 @agent_ppo2.py:185][0m |           0.0007 |         197.5120 |        -108.9675 |
[32m[20221214 14:29:35 @agent_ppo2.py:185][0m |           0.0048 |         195.9798 |        -109.3525 |
[32m[20221214 14:29:35 @agent_ppo2.py:185][0m |          -0.0027 |         190.4368 |        -109.1555 |
[32m[20221214 14:29:35 @agent_ppo2.py:185][0m |          -0.0010 |         189.7086 |        -109.1742 |
[32m[20221214 14:29:35 @agent_ppo2.py:185][0m |          -0.0022 |         187.7101 |        -108.9444 |
[32m[20221214 14:29:35 @agent_ppo2.py:185][0m |           0.0001 |         187.1510 |        -108.9591 |
[32m[20221214 14:29:36 @agent_ppo2.py:185][0m |          -0.0004 |         185.2581 |        -108.7477 |
[32m[20221214 14:29:36 @agent_ppo2.py:185][0m |          -0.0027 |         184.1917 |        -108.5953 |
[32m[20221214 14:29:36 @agent_ppo2.py:185][0m |          -0.0027 |         182.4255 |        -109.0051 |
[32m[20221214 14:29:36 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:29:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.52
[32m[20221214 14:29:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.35
[32m[20221214 14:29:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.07
[32m[20221214 14:29:36 @agent_ppo2.py:143][0m Total time:      31.56 min
[32m[20221214 14:29:36 @agent_ppo2.py:145][0m 2904064 total steps have happened
[32m[20221214 14:29:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1418 --------------------------#
[32m[20221214 14:29:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:29:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:36 @agent_ppo2.py:185][0m |           0.0007 |         187.4409 |        -110.0641 |
[32m[20221214 14:29:36 @agent_ppo2.py:185][0m |          -0.0011 |         169.5853 |        -109.4788 |
[32m[20221214 14:29:36 @agent_ppo2.py:185][0m |           0.0069 |         176.1217 |        -110.1680 |
[32m[20221214 14:29:36 @agent_ppo2.py:185][0m |           0.0095 |         166.7738 |        -109.7273 |
[32m[20221214 14:29:36 @agent_ppo2.py:185][0m |          -0.0021 |         161.8809 |        -109.6592 |
[32m[20221214 14:29:37 @agent_ppo2.py:185][0m |           0.0057 |         166.9030 |        -109.9351 |
[32m[20221214 14:29:37 @agent_ppo2.py:185][0m |           0.0102 |         172.1695 |        -109.9709 |
[32m[20221214 14:29:37 @agent_ppo2.py:185][0m |           0.0007 |         159.1128 |        -109.9607 |
[32m[20221214 14:29:37 @agent_ppo2.py:185][0m |           0.0012 |         156.7026 |        -109.7452 |
[32m[20221214 14:29:37 @agent_ppo2.py:185][0m |          -0.0023 |         156.0912 |        -109.7811 |
[32m[20221214 14:29:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:29:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.38
[32m[20221214 14:29:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.45
[32m[20221214 14:29:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.89
[32m[20221214 14:29:37 @agent_ppo2.py:143][0m Total time:      31.58 min
[32m[20221214 14:29:37 @agent_ppo2.py:145][0m 2906112 total steps have happened
[32m[20221214 14:29:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1419 --------------------------#
[32m[20221214 14:29:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:29:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:37 @agent_ppo2.py:185][0m |           0.0016 |         251.1374 |        -110.2352 |
[32m[20221214 14:29:37 @agent_ppo2.py:185][0m |           0.0045 |         245.6088 |        -110.1551 |
[32m[20221214 14:29:38 @agent_ppo2.py:185][0m |           0.0014 |         233.0616 |        -110.0197 |
[32m[20221214 14:29:38 @agent_ppo2.py:185][0m |          -0.0016 |         227.4615 |        -110.1869 |
[32m[20221214 14:29:38 @agent_ppo2.py:185][0m |          -0.0030 |         222.8270 |        -109.9646 |
[32m[20221214 14:29:38 @agent_ppo2.py:185][0m |           0.0026 |         217.6597 |        -109.7886 |
[32m[20221214 14:29:38 @agent_ppo2.py:185][0m |          -0.0010 |         214.8499 |        -109.7803 |
[32m[20221214 14:29:38 @agent_ppo2.py:185][0m |          -0.0030 |         212.3287 |        -110.0136 |
[32m[20221214 14:29:38 @agent_ppo2.py:185][0m |          -0.0041 |         210.8162 |        -109.8343 |
[32m[20221214 14:29:38 @agent_ppo2.py:185][0m |          -0.0074 |         210.2047 |        -109.6197 |
[32m[20221214 14:29:38 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:29:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.80
[32m[20221214 14:29:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.81
[32m[20221214 14:29:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.87
[32m[20221214 14:29:38 @agent_ppo2.py:143][0m Total time:      31.61 min
[32m[20221214 14:29:38 @agent_ppo2.py:145][0m 2908160 total steps have happened
[32m[20221214 14:29:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1420 --------------------------#
[32m[20221214 14:29:39 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:29:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:39 @agent_ppo2.py:185][0m |           0.0013 |         251.9825 |        -106.8849 |
[32m[20221214 14:29:39 @agent_ppo2.py:185][0m |          -0.0005 |         209.2403 |        -107.0103 |
[32m[20221214 14:29:39 @agent_ppo2.py:185][0m |           0.0007 |         191.4952 |        -107.0356 |
[32m[20221214 14:29:39 @agent_ppo2.py:185][0m |          -0.0037 |         185.7236 |        -107.2341 |
[32m[20221214 14:29:39 @agent_ppo2.py:185][0m |          -0.0003 |         182.7892 |        -106.9339 |
[32m[20221214 14:29:39 @agent_ppo2.py:185][0m |          -0.0003 |         178.7627 |        -106.9753 |
[32m[20221214 14:29:39 @agent_ppo2.py:185][0m |          -0.0014 |         176.8352 |        -107.3085 |
[32m[20221214 14:29:39 @agent_ppo2.py:185][0m |           0.0057 |         188.0202 |        -107.4429 |
[32m[20221214 14:29:40 @agent_ppo2.py:185][0m |           0.0022 |         174.0211 |        -107.1424 |
[32m[20221214 14:29:40 @agent_ppo2.py:185][0m |          -0.0023 |         171.0916 |        -107.3171 |
[32m[20221214 14:29:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:29:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.39
[32m[20221214 14:29:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.34
[32m[20221214 14:29:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.53
[32m[20221214 14:29:40 @agent_ppo2.py:143][0m Total time:      31.63 min
[32m[20221214 14:29:40 @agent_ppo2.py:145][0m 2910208 total steps have happened
[32m[20221214 14:29:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1421 --------------------------#
[32m[20221214 14:29:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:40 @agent_ppo2.py:185][0m |          -0.0024 |         182.5738 |        -109.3367 |
[32m[20221214 14:29:40 @agent_ppo2.py:185][0m |          -0.0015 |         158.4960 |        -109.4205 |
[32m[20221214 14:29:40 @agent_ppo2.py:185][0m |          -0.0006 |         156.4606 |        -109.5248 |
[32m[20221214 14:29:40 @agent_ppo2.py:185][0m |           0.0035 |         149.8561 |        -109.3129 |
[32m[20221214 14:29:41 @agent_ppo2.py:185][0m |           0.0000 |         147.3841 |        -109.3405 |
[32m[20221214 14:29:41 @agent_ppo2.py:185][0m |          -0.0031 |         145.1318 |        -109.4282 |
[32m[20221214 14:29:41 @agent_ppo2.py:185][0m |           0.0012 |         146.4795 |        -109.3185 |
[32m[20221214 14:29:41 @agent_ppo2.py:185][0m |          -0.0019 |         144.1999 |        -109.3222 |
[32m[20221214 14:29:41 @agent_ppo2.py:185][0m |          -0.0041 |         142.2797 |        -109.2959 |
[32m[20221214 14:29:41 @agent_ppo2.py:185][0m |          -0.0016 |         141.3679 |        -109.2707 |
[32m[20221214 14:29:41 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:29:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.86
[32m[20221214 14:29:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.10
[32m[20221214 14:29:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.91
[32m[20221214 14:29:41 @agent_ppo2.py:143][0m Total time:      31.65 min
[32m[20221214 14:29:41 @agent_ppo2.py:145][0m 2912256 total steps have happened
[32m[20221214 14:29:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1422 --------------------------#
[32m[20221214 14:29:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:29:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:42 @agent_ppo2.py:185][0m |           0.0014 |         210.9494 |        -108.0563 |
[32m[20221214 14:29:42 @agent_ppo2.py:185][0m |          -0.0011 |         191.7539 |        -108.4780 |
[32m[20221214 14:29:42 @agent_ppo2.py:185][0m |          -0.0003 |         184.2615 |        -108.2020 |
[32m[20221214 14:29:42 @agent_ppo2.py:185][0m |          -0.0030 |         179.9622 |        -108.4269 |
[32m[20221214 14:29:42 @agent_ppo2.py:185][0m |          -0.0054 |         177.0291 |        -108.3950 |
[32m[20221214 14:29:42 @agent_ppo2.py:185][0m |          -0.0046 |         174.7857 |        -108.5623 |
[32m[20221214 14:29:42 @agent_ppo2.py:185][0m |          -0.0043 |         172.2955 |        -108.5477 |
[32m[20221214 14:29:42 @agent_ppo2.py:185][0m |          -0.0013 |         171.8879 |        -108.6896 |
[32m[20221214 14:29:42 @agent_ppo2.py:185][0m |          -0.0004 |         169.8804 |        -108.8329 |
[32m[20221214 14:29:42 @agent_ppo2.py:185][0m |           0.0013 |         168.8047 |        -108.6193 |
[32m[20221214 14:29:42 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:29:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.64
[32m[20221214 14:29:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.02
[32m[20221214 14:29:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.06
[32m[20221214 14:29:42 @agent_ppo2.py:143][0m Total time:      31.67 min
[32m[20221214 14:29:42 @agent_ppo2.py:145][0m 2914304 total steps have happened
[32m[20221214 14:29:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1423 --------------------------#
[32m[20221214 14:29:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:29:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:43 @agent_ppo2.py:185][0m |          -0.0031 |         171.0238 |        -109.7253 |
[32m[20221214 14:29:43 @agent_ppo2.py:185][0m |          -0.0039 |         155.7624 |        -109.6374 |
[32m[20221214 14:29:43 @agent_ppo2.py:185][0m |          -0.0034 |         149.5086 |        -109.7141 |
[32m[20221214 14:29:43 @agent_ppo2.py:185][0m |          -0.0022 |         145.1030 |        -109.8721 |
[32m[20221214 14:29:43 @agent_ppo2.py:185][0m |          -0.0057 |         143.3570 |        -110.0600 |
[32m[20221214 14:29:43 @agent_ppo2.py:185][0m |          -0.0034 |         140.3631 |        -109.9765 |
[32m[20221214 14:29:43 @agent_ppo2.py:185][0m |          -0.0024 |         138.6293 |        -110.1420 |
[32m[20221214 14:29:43 @agent_ppo2.py:185][0m |           0.0106 |         157.5378 |        -110.1415 |
[32m[20221214 14:29:43 @agent_ppo2.py:185][0m |          -0.0045 |         135.8810 |        -110.2077 |
[32m[20221214 14:29:44 @agent_ppo2.py:185][0m |          -0.0021 |         134.9208 |        -110.2652 |
[32m[20221214 14:29:44 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:29:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.83
[32m[20221214 14:29:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.61
[32m[20221214 14:29:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.08
[32m[20221214 14:29:44 @agent_ppo2.py:143][0m Total time:      31.69 min
[32m[20221214 14:29:44 @agent_ppo2.py:145][0m 2916352 total steps have happened
[32m[20221214 14:29:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1424 --------------------------#
[32m[20221214 14:29:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:29:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:44 @agent_ppo2.py:185][0m |           0.0012 |         173.5434 |        -110.5623 |
[32m[20221214 14:29:44 @agent_ppo2.py:185][0m |          -0.0017 |         159.9367 |        -110.8139 |
[32m[20221214 14:29:44 @agent_ppo2.py:185][0m |          -0.0036 |         154.3080 |        -110.7498 |
[32m[20221214 14:29:44 @agent_ppo2.py:185][0m |          -0.0041 |         152.2017 |        -111.0277 |
[32m[20221214 14:29:44 @agent_ppo2.py:185][0m |           0.0088 |         161.7544 |        -111.0755 |
[32m[20221214 14:29:44 @agent_ppo2.py:185][0m |          -0.0003 |         148.9182 |        -111.3556 |
[32m[20221214 14:29:45 @agent_ppo2.py:185][0m |          -0.0047 |         146.8731 |        -111.3276 |
[32m[20221214 14:29:45 @agent_ppo2.py:185][0m |          -0.0008 |         146.0259 |        -111.6111 |
[32m[20221214 14:29:45 @agent_ppo2.py:185][0m |          -0.0021 |         145.1737 |        -111.7489 |
[32m[20221214 14:29:45 @agent_ppo2.py:185][0m |          -0.0021 |         144.2061 |        -111.8841 |
[32m[20221214 14:29:45 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:29:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.34
[32m[20221214 14:29:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.51
[32m[20221214 14:29:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 714.76
[32m[20221214 14:29:45 @agent_ppo2.py:143][0m Total time:      31.72 min
[32m[20221214 14:29:45 @agent_ppo2.py:145][0m 2918400 total steps have happened
[32m[20221214 14:29:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1425 --------------------------#
[32m[20221214 14:29:45 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:29:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:45 @agent_ppo2.py:185][0m |           0.0005 |          48.2742 |        -111.5166 |
[32m[20221214 14:29:46 @agent_ppo2.py:185][0m |          -0.0023 |          29.7772 |        -111.3097 |
[32m[20221214 14:29:46 @agent_ppo2.py:185][0m |          -0.0021 |          26.0653 |        -111.0474 |
[32m[20221214 14:29:46 @agent_ppo2.py:185][0m |          -0.0016 |          24.0717 |        -110.9732 |
[32m[20221214 14:29:46 @agent_ppo2.py:185][0m |          -0.0009 |          22.9083 |        -111.2236 |
[32m[20221214 14:29:46 @agent_ppo2.py:185][0m |          -0.0020 |          21.9411 |        -110.9344 |
[32m[20221214 14:29:46 @agent_ppo2.py:185][0m |          -0.0008 |          21.2668 |        -111.0437 |
[32m[20221214 14:29:46 @agent_ppo2.py:185][0m |          -0.0018 |          20.7883 |        -110.9919 |
[32m[20221214 14:29:46 @agent_ppo2.py:185][0m |          -0.0027 |          20.2316 |        -111.1081 |
[32m[20221214 14:29:46 @agent_ppo2.py:185][0m |          -0.0010 |          19.7221 |        -110.9343 |
[32m[20221214 14:29:46 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:29:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 676.14
[32m[20221214 14:29:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 773.42
[32m[20221214 14:29:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.92
[32m[20221214 14:29:46 @agent_ppo2.py:143][0m Total time:      31.74 min
[32m[20221214 14:29:46 @agent_ppo2.py:145][0m 2920448 total steps have happened
[32m[20221214 14:29:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1426 --------------------------#
[32m[20221214 14:29:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:47 @agent_ppo2.py:185][0m |          -0.0013 |         190.7359 |        -110.7217 |
[32m[20221214 14:29:47 @agent_ppo2.py:185][0m |           0.0080 |         192.3030 |        -110.6696 |
[32m[20221214 14:29:47 @agent_ppo2.py:185][0m |           0.0002 |         174.9344 |        -110.6588 |
[32m[20221214 14:29:47 @agent_ppo2.py:185][0m |          -0.0017 |         172.3956 |        -110.7127 |
[32m[20221214 14:29:47 @agent_ppo2.py:185][0m |          -0.0021 |         172.0420 |        -110.6140 |
[32m[20221214 14:29:47 @agent_ppo2.py:185][0m |          -0.0022 |         169.5025 |        -110.4930 |
[32m[20221214 14:29:47 @agent_ppo2.py:185][0m |          -0.0023 |         167.7207 |        -110.6019 |
[32m[20221214 14:29:47 @agent_ppo2.py:185][0m |          -0.0027 |         167.2274 |        -110.7032 |
[32m[20221214 14:29:48 @agent_ppo2.py:185][0m |          -0.0016 |         165.2002 |        -110.6816 |
[32m[20221214 14:29:48 @agent_ppo2.py:185][0m |          -0.0014 |         164.9178 |        -110.7344 |
[32m[20221214 14:29:48 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:29:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.70
[32m[20221214 14:29:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.86
[32m[20221214 14:29:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.86
[32m[20221214 14:29:48 @agent_ppo2.py:143][0m Total time:      31.76 min
[32m[20221214 14:29:48 @agent_ppo2.py:145][0m 2922496 total steps have happened
[32m[20221214 14:29:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1427 --------------------------#
[32m[20221214 14:29:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:48 @agent_ppo2.py:185][0m |          -0.0002 |         227.8315 |        -113.0271 |
[32m[20221214 14:29:48 @agent_ppo2.py:185][0m |          -0.0042 |         206.7716 |        -112.4813 |
[32m[20221214 14:29:48 @agent_ppo2.py:185][0m |          -0.0044 |         199.0711 |        -112.1906 |
[32m[20221214 14:29:48 @agent_ppo2.py:185][0m |          -0.0026 |         192.3726 |        -112.4969 |
[32m[20221214 14:29:49 @agent_ppo2.py:185][0m |          -0.0016 |         188.9414 |        -112.4445 |
[32m[20221214 14:29:49 @agent_ppo2.py:185][0m |          -0.0040 |         185.0826 |        -112.3269 |
[32m[20221214 14:29:49 @agent_ppo2.py:185][0m |          -0.0017 |         181.9533 |        -112.4519 |
[32m[20221214 14:29:49 @agent_ppo2.py:185][0m |          -0.0036 |         180.2158 |        -112.4071 |
[32m[20221214 14:29:49 @agent_ppo2.py:185][0m |           0.0051 |         182.8972 |        -112.2057 |
[32m[20221214 14:29:49 @agent_ppo2.py:185][0m |          -0.0015 |         176.7266 |        -112.2088 |
[32m[20221214 14:29:49 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:29:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.88
[32m[20221214 14:29:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 811.95
[32m[20221214 14:29:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.23
[32m[20221214 14:29:49 @agent_ppo2.py:143][0m Total time:      31.79 min
[32m[20221214 14:29:49 @agent_ppo2.py:145][0m 2924544 total steps have happened
[32m[20221214 14:29:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1428 --------------------------#
[32m[20221214 14:29:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:50 @agent_ppo2.py:185][0m |           0.0007 |         210.8865 |        -111.9317 |
[32m[20221214 14:29:50 @agent_ppo2.py:185][0m |           0.0042 |         190.4058 |        -111.9074 |
[32m[20221214 14:29:50 @agent_ppo2.py:185][0m |          -0.0018 |         184.8645 |        -111.8492 |
[32m[20221214 14:29:50 @agent_ppo2.py:185][0m |          -0.0002 |         182.3700 |        -111.8201 |
[32m[20221214 14:29:50 @agent_ppo2.py:185][0m |          -0.0019 |         181.2805 |        -111.8958 |
[32m[20221214 14:29:50 @agent_ppo2.py:185][0m |          -0.0011 |         179.2439 |        -111.7244 |
[32m[20221214 14:29:50 @agent_ppo2.py:185][0m |          -0.0002 |         178.4501 |        -111.9871 |
[32m[20221214 14:29:50 @agent_ppo2.py:185][0m |           0.0009 |         178.2389 |        -112.0273 |
[32m[20221214 14:29:50 @agent_ppo2.py:185][0m |          -0.0018 |         180.1575 |        -111.9183 |
[32m[20221214 14:29:50 @agent_ppo2.py:185][0m |          -0.0041 |         177.4973 |        -112.0063 |
[32m[20221214 14:29:50 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:29:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.80
[32m[20221214 14:29:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.89
[32m[20221214 14:29:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.36
[32m[20221214 14:29:51 @agent_ppo2.py:143][0m Total time:      31.81 min
[32m[20221214 14:29:51 @agent_ppo2.py:145][0m 2926592 total steps have happened
[32m[20221214 14:29:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1429 --------------------------#
[32m[20221214 14:29:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:51 @agent_ppo2.py:185][0m |          -0.0004 |         131.4323 |        -110.6273 |
[32m[20221214 14:29:51 @agent_ppo2.py:185][0m |          -0.0003 |         111.4249 |        -110.7786 |
[32m[20221214 14:29:51 @agent_ppo2.py:185][0m |          -0.0013 |         105.8249 |        -110.4848 |
[32m[20221214 14:29:51 @agent_ppo2.py:185][0m |          -0.0014 |         102.8542 |        -110.4862 |
[32m[20221214 14:29:51 @agent_ppo2.py:185][0m |          -0.0016 |         100.1761 |        -110.3293 |
[32m[20221214 14:29:51 @agent_ppo2.py:185][0m |           0.0010 |          98.3179 |        -110.5152 |
[32m[20221214 14:29:51 @agent_ppo2.py:185][0m |          -0.0001 |          97.6636 |        -110.4938 |
[32m[20221214 14:29:51 @agent_ppo2.py:185][0m |           0.0035 |          95.5854 |        -110.1990 |
[32m[20221214 14:29:52 @agent_ppo2.py:185][0m |          -0.0020 |          93.6151 |        -110.3007 |
[32m[20221214 14:29:52 @agent_ppo2.py:185][0m |          -0.0035 |          92.4531 |        -110.2567 |
[32m[20221214 14:29:52 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:29:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.31
[32m[20221214 14:29:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.85
[32m[20221214 14:29:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.59
[32m[20221214 14:29:52 @agent_ppo2.py:143][0m Total time:      31.83 min
[32m[20221214 14:29:52 @agent_ppo2.py:145][0m 2928640 total steps have happened
[32m[20221214 14:29:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1430 --------------------------#
[32m[20221214 14:29:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:29:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:52 @agent_ppo2.py:185][0m |          -0.0051 |         194.2215 |        -110.5410 |
[32m[20221214 14:29:52 @agent_ppo2.py:185][0m |          -0.0037 |         169.9029 |        -110.3867 |
[32m[20221214 14:29:52 @agent_ppo2.py:185][0m |          -0.0036 |         163.0203 |        -110.1469 |
[32m[20221214 14:29:52 @agent_ppo2.py:185][0m |           0.0075 |         171.2907 |        -110.2467 |
[32m[20221214 14:29:53 @agent_ppo2.py:185][0m |          -0.0025 |         158.3230 |        -110.3263 |
[32m[20221214 14:29:53 @agent_ppo2.py:185][0m |          -0.0029 |         155.5351 |        -110.3716 |
[32m[20221214 14:29:53 @agent_ppo2.py:185][0m |          -0.0045 |         153.2091 |        -110.0873 |
[32m[20221214 14:29:53 @agent_ppo2.py:185][0m |          -0.0048 |         151.9924 |        -110.0879 |
[32m[20221214 14:29:53 @agent_ppo2.py:185][0m |          -0.0021 |         151.3711 |        -110.1072 |
[32m[20221214 14:29:53 @agent_ppo2.py:185][0m |          -0.0030 |         150.5857 |        -110.1985 |
[32m[20221214 14:29:53 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:29:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.71
[32m[20221214 14:29:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.64
[32m[20221214 14:29:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.81
[32m[20221214 14:29:53 @agent_ppo2.py:143][0m Total time:      31.85 min
[32m[20221214 14:29:53 @agent_ppo2.py:145][0m 2930688 total steps have happened
[32m[20221214 14:29:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1431 --------------------------#
[32m[20221214 14:29:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:54 @agent_ppo2.py:185][0m |          -0.0031 |         221.1270 |        -111.4180 |
[32m[20221214 14:29:54 @agent_ppo2.py:185][0m |          -0.0000 |         196.3704 |        -111.4851 |
[32m[20221214 14:29:54 @agent_ppo2.py:185][0m |          -0.0018 |         186.7615 |        -111.3440 |
[32m[20221214 14:29:54 @agent_ppo2.py:185][0m |          -0.0051 |         182.0721 |        -111.6136 |
[32m[20221214 14:29:54 @agent_ppo2.py:185][0m |           0.0049 |         190.9731 |        -111.4421 |
[32m[20221214 14:29:54 @agent_ppo2.py:185][0m |          -0.0010 |         176.5580 |        -111.3159 |
[32m[20221214 14:29:54 @agent_ppo2.py:185][0m |          -0.0009 |         174.2938 |        -111.3159 |
[32m[20221214 14:29:54 @agent_ppo2.py:185][0m |          -0.0042 |         172.6852 |        -111.4648 |
[32m[20221214 14:29:54 @agent_ppo2.py:185][0m |           0.0010 |         172.5159 |        -111.4824 |
[32m[20221214 14:29:54 @agent_ppo2.py:185][0m |          -0.0042 |         170.3508 |        -111.4123 |
[32m[20221214 14:29:54 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:29:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.77
[32m[20221214 14:29:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.17
[32m[20221214 14:29:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 785.51
[32m[20221214 14:29:55 @agent_ppo2.py:143][0m Total time:      31.87 min
[32m[20221214 14:29:55 @agent_ppo2.py:145][0m 2932736 total steps have happened
[32m[20221214 14:29:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1432 --------------------------#
[32m[20221214 14:29:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:55 @agent_ppo2.py:185][0m |           0.0008 |         166.8212 |        -111.4889 |
[32m[20221214 14:29:55 @agent_ppo2.py:185][0m |           0.0148 |         158.1856 |        -111.8299 |
[32m[20221214 14:29:55 @agent_ppo2.py:185][0m |          -0.0017 |         134.5860 |        -111.3611 |
[32m[20221214 14:29:55 @agent_ppo2.py:185][0m |           0.0015 |         129.7101 |        -111.6027 |
[32m[20221214 14:29:55 @agent_ppo2.py:185][0m |          -0.0032 |         126.6231 |        -111.6391 |
[32m[20221214 14:29:55 @agent_ppo2.py:185][0m |           0.0007 |         124.2858 |        -111.6270 |
[32m[20221214 14:29:55 @agent_ppo2.py:185][0m |          -0.0009 |         122.0665 |        -111.5552 |
[32m[20221214 14:29:56 @agent_ppo2.py:185][0m |          -0.0019 |         120.0071 |        -111.5837 |
[32m[20221214 14:29:56 @agent_ppo2.py:185][0m |          -0.0011 |         118.3278 |        -111.6446 |
[32m[20221214 14:29:56 @agent_ppo2.py:185][0m |           0.0004 |         116.6250 |        -111.4044 |
[32m[20221214 14:29:56 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:29:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 739.33
[32m[20221214 14:29:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.08
[32m[20221214 14:29:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 728.62
[32m[20221214 14:29:56 @agent_ppo2.py:143][0m Total time:      31.90 min
[32m[20221214 14:29:56 @agent_ppo2.py:145][0m 2934784 total steps have happened
[32m[20221214 14:29:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1433 --------------------------#
[32m[20221214 14:29:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:56 @agent_ppo2.py:185][0m |           0.0010 |         142.1387 |        -109.6250 |
[32m[20221214 14:29:56 @agent_ppo2.py:185][0m |          -0.0016 |         117.8064 |        -109.7611 |
[32m[20221214 14:29:56 @agent_ppo2.py:185][0m |          -0.0024 |         110.7540 |        -109.9145 |
[32m[20221214 14:29:57 @agent_ppo2.py:185][0m |          -0.0026 |         106.7967 |        -109.5969 |
[32m[20221214 14:29:57 @agent_ppo2.py:185][0m |          -0.0059 |         104.0917 |        -109.8973 |
[32m[20221214 14:29:57 @agent_ppo2.py:185][0m |          -0.0053 |         102.6165 |        -109.8647 |
[32m[20221214 14:29:57 @agent_ppo2.py:185][0m |          -0.0020 |         100.4628 |        -110.0684 |
[32m[20221214 14:29:57 @agent_ppo2.py:185][0m |          -0.0027 |          98.8129 |        -110.1006 |
[32m[20221214 14:29:57 @agent_ppo2.py:185][0m |          -0.0055 |          97.8696 |        -110.0143 |
[32m[20221214 14:29:57 @agent_ppo2.py:185][0m |          -0.0015 |          97.3540 |        -110.1625 |
[32m[20221214 14:29:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:29:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.00
[32m[20221214 14:29:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.39
[32m[20221214 14:29:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 722.86
[32m[20221214 14:29:57 @agent_ppo2.py:143][0m Total time:      31.92 min
[32m[20221214 14:29:57 @agent_ppo2.py:145][0m 2936832 total steps have happened
[32m[20221214 14:29:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1434 --------------------------#
[32m[20221214 14:29:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:58 @agent_ppo2.py:185][0m |           0.0024 |          85.2171 |        -112.7734 |
[32m[20221214 14:29:58 @agent_ppo2.py:185][0m |           0.0041 |          66.6549 |        -112.5815 |
[32m[20221214 14:29:58 @agent_ppo2.py:185][0m |           0.0031 |          58.9608 |        -112.8849 |
[32m[20221214 14:29:58 @agent_ppo2.py:185][0m |          -0.0020 |          55.5705 |        -112.9286 |
[32m[20221214 14:29:58 @agent_ppo2.py:185][0m |           0.0005 |          54.0186 |        -112.8779 |
[32m[20221214 14:29:58 @agent_ppo2.py:185][0m |          -0.0069 |          52.1965 |        -113.0056 |
[32m[20221214 14:29:58 @agent_ppo2.py:185][0m |          -0.0001 |          50.6590 |        -112.8301 |
[32m[20221214 14:29:58 @agent_ppo2.py:185][0m |          -0.0031 |          49.8674 |        -112.9239 |
[32m[20221214 14:29:58 @agent_ppo2.py:185][0m |           0.0073 |          51.5652 |        -113.1145 |
[32m[20221214 14:29:58 @agent_ppo2.py:185][0m |          -0.0040 |          48.0115 |        -113.3227 |
[32m[20221214 14:29:58 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:29:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 662.36
[32m[20221214 14:29:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 722.55
[32m[20221214 14:29:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 733.66
[32m[20221214 14:29:59 @agent_ppo2.py:143][0m Total time:      31.94 min
[32m[20221214 14:29:59 @agent_ppo2.py:145][0m 2938880 total steps have happened
[32m[20221214 14:29:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1435 --------------------------#
[32m[20221214 14:29:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:29:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:29:59 @agent_ppo2.py:185][0m |          -0.0022 |          51.6565 |        -112.1821 |
[32m[20221214 14:29:59 @agent_ppo2.py:185][0m |           0.0000 |          42.6881 |        -111.9519 |
[32m[20221214 14:29:59 @agent_ppo2.py:185][0m |          -0.0051 |          39.6397 |        -111.8175 |
[32m[20221214 14:29:59 @agent_ppo2.py:185][0m |          -0.0043 |          37.3265 |        -111.8123 |
[32m[20221214 14:29:59 @agent_ppo2.py:185][0m |          -0.0103 |          35.6958 |        -111.8845 |
[32m[20221214 14:29:59 @agent_ppo2.py:185][0m |           0.0003 |          34.6408 |        -111.8077 |
[32m[20221214 14:30:00 @agent_ppo2.py:185][0m |          -0.0050 |          33.3199 |        -111.7733 |
[32m[20221214 14:30:00 @agent_ppo2.py:185][0m |          -0.0040 |          32.6237 |        -111.8922 |
[32m[20221214 14:30:00 @agent_ppo2.py:185][0m |          -0.0045 |          31.6560 |        -111.8497 |
[32m[20221214 14:30:00 @agent_ppo2.py:185][0m |          -0.0009 |          31.2038 |        -111.9638 |
[32m[20221214 14:30:00 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 689.71
[32m[20221214 14:30:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 709.44
[32m[20221214 14:30:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.84
[32m[20221214 14:30:00 @agent_ppo2.py:143][0m Total time:      31.97 min
[32m[20221214 14:30:00 @agent_ppo2.py:145][0m 2940928 total steps have happened
[32m[20221214 14:30:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1436 --------------------------#
[32m[20221214 14:30:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:00 @agent_ppo2.py:185][0m |          -0.0046 |         183.3644 |        -113.7838 |
[32m[20221214 14:30:00 @agent_ppo2.py:185][0m |          -0.0059 |         166.9264 |        -113.8582 |
[32m[20221214 14:30:01 @agent_ppo2.py:185][0m |          -0.0055 |         161.4862 |        -113.8297 |
[32m[20221214 14:30:01 @agent_ppo2.py:185][0m |           0.0015 |         160.7675 |        -113.8052 |
[32m[20221214 14:30:01 @agent_ppo2.py:185][0m |           0.0080 |         177.9505 |        -113.6535 |
[32m[20221214 14:30:01 @agent_ppo2.py:185][0m |           0.0048 |         172.0063 |        -112.9057 |
[32m[20221214 14:30:01 @agent_ppo2.py:185][0m |          -0.0066 |         150.3230 |        -113.5280 |
[32m[20221214 14:30:01 @agent_ppo2.py:185][0m |          -0.0076 |         148.6949 |        -113.4496 |
[32m[20221214 14:30:01 @agent_ppo2.py:185][0m |          -0.0063 |         146.3539 |        -113.2436 |
[32m[20221214 14:30:01 @agent_ppo2.py:185][0m |          -0.0092 |         145.5661 |        -113.3926 |
[32m[20221214 14:30:01 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.68
[32m[20221214 14:30:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.40
[32m[20221214 14:30:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.28
[32m[20221214 14:30:01 @agent_ppo2.py:143][0m Total time:      31.99 min
[32m[20221214 14:30:01 @agent_ppo2.py:145][0m 2942976 total steps have happened
[32m[20221214 14:30:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1437 --------------------------#
[32m[20221214 14:30:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:02 @agent_ppo2.py:185][0m |          -0.0002 |          52.1083 |        -112.3588 |
[32m[20221214 14:30:02 @agent_ppo2.py:185][0m |           0.0045 |          40.7851 |        -112.2590 |
[32m[20221214 14:30:02 @agent_ppo2.py:185][0m |           0.0016 |          37.2949 |        -112.1513 |
[32m[20221214 14:30:02 @agent_ppo2.py:185][0m |           0.0001 |          34.8851 |        -112.3264 |
[32m[20221214 14:30:02 @agent_ppo2.py:185][0m |          -0.0001 |          33.4569 |        -112.3240 |
[32m[20221214 14:30:02 @agent_ppo2.py:185][0m |          -0.0014 |          32.3909 |        -112.2488 |
[32m[20221214 14:30:02 @agent_ppo2.py:185][0m |           0.0019 |          31.6271 |        -112.3759 |
[32m[20221214 14:30:02 @agent_ppo2.py:185][0m |          -0.0036 |          31.0050 |        -112.3337 |
[32m[20221214 14:30:02 @agent_ppo2.py:185][0m |           0.0025 |          30.3020 |        -112.4365 |
[32m[20221214 14:30:03 @agent_ppo2.py:185][0m |          -0.0052 |          29.7272 |        -112.4917 |
[32m[20221214 14:30:03 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 733.90
[32m[20221214 14:30:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.43
[32m[20221214 14:30:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.17
[32m[20221214 14:30:03 @agent_ppo2.py:143][0m Total time:      32.01 min
[32m[20221214 14:30:03 @agent_ppo2.py:145][0m 2945024 total steps have happened
[32m[20221214 14:30:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1438 --------------------------#
[32m[20221214 14:30:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:03 @agent_ppo2.py:185][0m |          -0.0027 |         188.2168 |        -113.7339 |
[32m[20221214 14:30:03 @agent_ppo2.py:185][0m |           0.0007 |         167.6802 |        -113.8574 |
[32m[20221214 14:30:03 @agent_ppo2.py:185][0m |          -0.0028 |         158.9344 |        -113.5435 |
[32m[20221214 14:30:03 @agent_ppo2.py:185][0m |          -0.0054 |         156.1568 |        -113.3620 |
[32m[20221214 14:30:03 @agent_ppo2.py:185][0m |          -0.0033 |         153.4803 |        -113.3903 |
[32m[20221214 14:30:03 @agent_ppo2.py:185][0m |          -0.0035 |         151.8090 |        -113.1814 |
[32m[20221214 14:30:04 @agent_ppo2.py:185][0m |          -0.0037 |         150.3989 |        -113.1380 |
[32m[20221214 14:30:04 @agent_ppo2.py:185][0m |          -0.0010 |         148.6751 |        -112.9325 |
[32m[20221214 14:30:04 @agent_ppo2.py:185][0m |          -0.0037 |         147.6628 |        -112.7348 |
[32m[20221214 14:30:04 @agent_ppo2.py:185][0m |          -0.0050 |         145.8020 |        -112.5046 |
[32m[20221214 14:30:04 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:30:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 783.36
[32m[20221214 14:30:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.79
[32m[20221214 14:30:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.70
[32m[20221214 14:30:04 @agent_ppo2.py:143][0m Total time:      32.03 min
[32m[20221214 14:30:04 @agent_ppo2.py:145][0m 2947072 total steps have happened
[32m[20221214 14:30:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1439 --------------------------#
[32m[20221214 14:30:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:04 @agent_ppo2.py:185][0m |           0.0007 |         179.4220 |        -111.6631 |
[32m[20221214 14:30:04 @agent_ppo2.py:185][0m |          -0.0020 |         154.8920 |        -111.5651 |
[32m[20221214 14:30:05 @agent_ppo2.py:185][0m |           0.0064 |         160.3614 |        -111.5241 |
[32m[20221214 14:30:05 @agent_ppo2.py:185][0m |           0.0002 |         143.7026 |        -111.1016 |
[32m[20221214 14:30:05 @agent_ppo2.py:185][0m |          -0.0033 |         139.7795 |        -111.0295 |
[32m[20221214 14:30:05 @agent_ppo2.py:185][0m |           0.0010 |         137.0292 |        -111.2929 |
[32m[20221214 14:30:05 @agent_ppo2.py:185][0m |          -0.0018 |         136.0359 |        -111.3270 |
[32m[20221214 14:30:05 @agent_ppo2.py:185][0m |           0.0001 |         134.6130 |        -111.2456 |
[32m[20221214 14:30:05 @agent_ppo2.py:185][0m |           0.0075 |         137.1956 |        -111.1504 |
[32m[20221214 14:30:05 @agent_ppo2.py:185][0m |          -0.0014 |         131.8955 |        -111.1377 |
[32m[20221214 14:30:05 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:30:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.40
[32m[20221214 14:30:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.50
[32m[20221214 14:30:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.08
[32m[20221214 14:30:05 @agent_ppo2.py:143][0m Total time:      32.05 min
[32m[20221214 14:30:05 @agent_ppo2.py:145][0m 2949120 total steps have happened
[32m[20221214 14:30:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1440 --------------------------#
[32m[20221214 14:30:06 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:30:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:06 @agent_ppo2.py:185][0m |          -0.0012 |         201.1963 |        -110.5720 |
[32m[20221214 14:30:06 @agent_ppo2.py:185][0m |          -0.0036 |         188.3339 |        -110.2650 |
[32m[20221214 14:30:06 @agent_ppo2.py:185][0m |          -0.0038 |         183.7380 |        -110.4294 |
[32m[20221214 14:30:06 @agent_ppo2.py:185][0m |          -0.0032 |         180.8949 |        -110.0039 |
[32m[20221214 14:30:06 @agent_ppo2.py:185][0m |          -0.0052 |         179.5882 |        -110.1388 |
[32m[20221214 14:30:06 @agent_ppo2.py:185][0m |          -0.0018 |         177.3927 |        -110.3442 |
[32m[20221214 14:30:06 @agent_ppo2.py:185][0m |          -0.0016 |         176.7824 |        -110.1983 |
[32m[20221214 14:30:06 @agent_ppo2.py:185][0m |          -0.0055 |         177.3694 |        -110.2158 |
[32m[20221214 14:30:06 @agent_ppo2.py:185][0m |          -0.0019 |         174.4908 |        -110.1588 |
[32m[20221214 14:30:07 @agent_ppo2.py:185][0m |          -0.0026 |         173.6034 |        -110.5102 |
[32m[20221214 14:30:07 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:30:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.16
[32m[20221214 14:30:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.40
[32m[20221214 14:30:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.16
[32m[20221214 14:30:07 @agent_ppo2.py:143][0m Total time:      32.08 min
[32m[20221214 14:30:07 @agent_ppo2.py:145][0m 2951168 total steps have happened
[32m[20221214 14:30:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1441 --------------------------#
[32m[20221214 14:30:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:07 @agent_ppo2.py:185][0m |           0.0005 |          97.7991 |        -110.1550 |
[32m[20221214 14:30:07 @agent_ppo2.py:185][0m |          -0.0040 |          85.1858 |        -110.2928 |
[32m[20221214 14:30:07 @agent_ppo2.py:185][0m |           0.0003 |          77.0584 |        -110.0839 |
[32m[20221214 14:30:07 @agent_ppo2.py:185][0m |          -0.0050 |          73.1362 |        -110.3287 |
[32m[20221214 14:30:07 @agent_ppo2.py:185][0m |           0.0014 |          70.1132 |        -110.2033 |
[32m[20221214 14:30:08 @agent_ppo2.py:185][0m |          -0.0029 |          68.1856 |        -110.1267 |
[32m[20221214 14:30:08 @agent_ppo2.py:185][0m |          -0.0077 |          66.6093 |        -110.1391 |
[32m[20221214 14:30:08 @agent_ppo2.py:185][0m |          -0.0003 |          64.7894 |        -110.0749 |
[32m[20221214 14:30:08 @agent_ppo2.py:185][0m |          -0.0066 |          63.9812 |        -110.1045 |
[32m[20221214 14:30:08 @agent_ppo2.py:185][0m |          -0.0010 |          62.2696 |        -110.1509 |
[32m[20221214 14:30:08 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.47
[32m[20221214 14:30:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.88
[32m[20221214 14:30:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.67
[32m[20221214 14:30:08 @agent_ppo2.py:143][0m Total time:      32.10 min
[32m[20221214 14:30:08 @agent_ppo2.py:145][0m 2953216 total steps have happened
[32m[20221214 14:30:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1442 --------------------------#
[32m[20221214 14:30:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:08 @agent_ppo2.py:185][0m |          -0.0008 |         234.3955 |        -109.4352 |
[32m[20221214 14:30:09 @agent_ppo2.py:185][0m |          -0.0025 |         212.4287 |        -109.4737 |
[32m[20221214 14:30:09 @agent_ppo2.py:185][0m |          -0.0025 |         206.0145 |        -109.0443 |
[32m[20221214 14:30:09 @agent_ppo2.py:185][0m |          -0.0017 |         200.4249 |        -109.5685 |
[32m[20221214 14:30:09 @agent_ppo2.py:185][0m |          -0.0034 |         198.2673 |        -109.2954 |
[32m[20221214 14:30:09 @agent_ppo2.py:185][0m |          -0.0024 |         194.9136 |        -109.6862 |
[32m[20221214 14:30:09 @agent_ppo2.py:185][0m |          -0.0057 |         192.9989 |        -109.5600 |
[32m[20221214 14:30:09 @agent_ppo2.py:185][0m |          -0.0046 |         191.3299 |        -109.9185 |
[32m[20221214 14:30:09 @agent_ppo2.py:185][0m |           0.0092 |         203.1034 |        -109.7489 |
[32m[20221214 14:30:09 @agent_ppo2.py:185][0m |          -0.0018 |         188.7201 |        -109.6432 |
[32m[20221214 14:30:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:30:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.14
[32m[20221214 14:30:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.82
[32m[20221214 14:30:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.43
[32m[20221214 14:30:09 @agent_ppo2.py:143][0m Total time:      32.12 min
[32m[20221214 14:30:09 @agent_ppo2.py:145][0m 2955264 total steps have happened
[32m[20221214 14:30:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1443 --------------------------#
[32m[20221214 14:30:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:10 @agent_ppo2.py:185][0m |          -0.0050 |         101.6700 |        -110.5813 |
[32m[20221214 14:30:10 @agent_ppo2.py:185][0m |           0.0031 |          89.0938 |        -110.5823 |
[32m[20221214 14:30:10 @agent_ppo2.py:185][0m |          -0.0018 |          84.4506 |        -110.2572 |
[32m[20221214 14:30:10 @agent_ppo2.py:185][0m |           0.0054 |          86.6174 |        -110.5240 |
[32m[20221214 14:30:10 @agent_ppo2.py:185][0m |          -0.0029 |          81.3644 |        -110.6403 |
[32m[20221214 14:30:10 @agent_ppo2.py:185][0m |           0.0068 |          83.3650 |        -110.5596 |
[32m[20221214 14:30:10 @agent_ppo2.py:185][0m |          -0.0003 |          76.7504 |        -110.3190 |
[32m[20221214 14:30:10 @agent_ppo2.py:185][0m |          -0.0006 |          75.3023 |        -110.4405 |
[32m[20221214 14:30:11 @agent_ppo2.py:185][0m |          -0.0014 |          74.8792 |        -110.4008 |
[32m[20221214 14:30:11 @agent_ppo2.py:185][0m |          -0.0017 |          74.4122 |        -110.4536 |
[32m[20221214 14:30:11 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:30:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 777.23
[32m[20221214 14:30:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 810.50
[32m[20221214 14:30:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 768.14
[32m[20221214 14:30:11 @agent_ppo2.py:143][0m Total time:      32.15 min
[32m[20221214 14:30:11 @agent_ppo2.py:145][0m 2957312 total steps have happened
[32m[20221214 14:30:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1444 --------------------------#
[32m[20221214 14:30:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:11 @agent_ppo2.py:185][0m |          -0.0025 |          72.2931 |        -111.4846 |
[32m[20221214 14:30:11 @agent_ppo2.py:185][0m |           0.0001 |          54.2075 |        -111.4146 |
[32m[20221214 14:30:11 @agent_ppo2.py:185][0m |          -0.0068 |          48.5999 |        -111.2835 |
[32m[20221214 14:30:11 @agent_ppo2.py:185][0m |          -0.0037 |          45.3246 |        -111.1757 |
[32m[20221214 14:30:12 @agent_ppo2.py:185][0m |          -0.0004 |          43.2483 |        -111.1901 |
[32m[20221214 14:30:12 @agent_ppo2.py:185][0m |          -0.0066 |          41.1800 |        -111.1482 |
[32m[20221214 14:30:12 @agent_ppo2.py:185][0m |          -0.0044 |          39.6752 |        -110.9070 |
[32m[20221214 14:30:12 @agent_ppo2.py:185][0m |          -0.0011 |          38.4050 |        -110.6570 |
[32m[20221214 14:30:12 @agent_ppo2.py:185][0m |          -0.0008 |          37.4093 |        -110.7740 |
[32m[20221214 14:30:12 @agent_ppo2.py:185][0m |          -0.0050 |          37.0881 |        -110.6470 |
[32m[20221214 14:30:12 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 765.00
[32m[20221214 14:30:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.95
[32m[20221214 14:30:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.95
[32m[20221214 14:30:12 @agent_ppo2.py:143][0m Total time:      32.17 min
[32m[20221214 14:30:12 @agent_ppo2.py:145][0m 2959360 total steps have happened
[32m[20221214 14:30:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1445 --------------------------#
[32m[20221214 14:30:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:13 @agent_ppo2.py:185][0m |          -0.0012 |         135.7424 |        -109.3486 |
[32m[20221214 14:30:13 @agent_ppo2.py:185][0m |           0.0004 |         129.0300 |        -109.5123 |
[32m[20221214 14:30:13 @agent_ppo2.py:185][0m |          -0.0055 |         118.1800 |        -109.5859 |
[32m[20221214 14:30:13 @agent_ppo2.py:185][0m |          -0.0030 |         114.8026 |        -109.5247 |
[32m[20221214 14:30:13 @agent_ppo2.py:185][0m |          -0.0009 |         113.5546 |        -109.5156 |
[32m[20221214 14:30:13 @agent_ppo2.py:185][0m |          -0.0058 |         112.3562 |        -109.5529 |
[32m[20221214 14:30:13 @agent_ppo2.py:185][0m |          -0.0059 |         110.6991 |        -109.3520 |
[32m[20221214 14:30:13 @agent_ppo2.py:185][0m |          -0.0047 |         109.4143 |        -109.4772 |
[32m[20221214 14:30:13 @agent_ppo2.py:185][0m |          -0.0035 |         108.5659 |        -109.2474 |
[32m[20221214 14:30:13 @agent_ppo2.py:185][0m |          -0.0011 |         108.4755 |        -109.4713 |
[32m[20221214 14:30:13 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.28
[32m[20221214 14:30:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.29
[32m[20221214 14:30:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.72
[32m[20221214 14:30:14 @agent_ppo2.py:143][0m Total time:      32.19 min
[32m[20221214 14:30:14 @agent_ppo2.py:145][0m 2961408 total steps have happened
[32m[20221214 14:30:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1446 --------------------------#
[32m[20221214 14:30:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:14 @agent_ppo2.py:185][0m |          -0.0028 |         219.9780 |        -112.5534 |
[32m[20221214 14:30:14 @agent_ppo2.py:185][0m |          -0.0012 |         208.2147 |        -112.7838 |
[32m[20221214 14:30:14 @agent_ppo2.py:185][0m |          -0.0035 |         206.0850 |        -112.7149 |
[32m[20221214 14:30:14 @agent_ppo2.py:185][0m |           0.0050 |         209.6471 |        -112.6802 |
[32m[20221214 14:30:14 @agent_ppo2.py:185][0m |           0.0026 |         204.6252 |        -112.5888 |
[32m[20221214 14:30:14 @agent_ppo2.py:185][0m |           0.0005 |         200.4384 |        -112.5065 |
[32m[20221214 14:30:14 @agent_ppo2.py:185][0m |          -0.0028 |         198.7295 |        -112.5496 |
[32m[20221214 14:30:15 @agent_ppo2.py:185][0m |          -0.0009 |         198.6781 |        -112.7285 |
[32m[20221214 14:30:15 @agent_ppo2.py:185][0m |          -0.0035 |         197.9454 |        -112.7231 |
[32m[20221214 14:30:15 @agent_ppo2.py:185][0m |          -0.0018 |         195.9069 |        -112.6078 |
[32m[20221214 14:30:15 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:30:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.36
[32m[20221214 14:30:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.09
[32m[20221214 14:30:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.59
[32m[20221214 14:30:15 @agent_ppo2.py:143][0m Total time:      32.21 min
[32m[20221214 14:30:15 @agent_ppo2.py:145][0m 2963456 total steps have happened
[32m[20221214 14:30:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1447 --------------------------#
[32m[20221214 14:30:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:30:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:15 @agent_ppo2.py:185][0m |           0.0008 |         191.1324 |        -111.4147 |
[32m[20221214 14:30:15 @agent_ppo2.py:185][0m |          -0.0021 |         181.2547 |        -111.1095 |
[32m[20221214 14:30:15 @agent_ppo2.py:185][0m |          -0.0003 |         176.9128 |        -110.9119 |
[32m[20221214 14:30:16 @agent_ppo2.py:185][0m |          -0.0033 |         175.3170 |        -111.3962 |
[32m[20221214 14:30:16 @agent_ppo2.py:185][0m |          -0.0012 |         174.3081 |        -111.3894 |
[32m[20221214 14:30:16 @agent_ppo2.py:185][0m |           0.0058 |         181.8690 |        -110.9961 |
[32m[20221214 14:30:16 @agent_ppo2.py:185][0m |           0.0038 |         171.7566 |        -110.9873 |
[32m[20221214 14:30:16 @agent_ppo2.py:185][0m |          -0.0029 |         170.5450 |        -111.0922 |
[32m[20221214 14:30:16 @agent_ppo2.py:185][0m |          -0.0004 |         168.9894 |        -111.1974 |
[32m[20221214 14:30:16 @agent_ppo2.py:185][0m |           0.0025 |         177.1698 |        -111.5240 |
[32m[20221214 14:30:16 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:30:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.30
[32m[20221214 14:30:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.89
[32m[20221214 14:30:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.45
[32m[20221214 14:30:16 @agent_ppo2.py:143][0m Total time:      32.24 min
[32m[20221214 14:30:16 @agent_ppo2.py:145][0m 2965504 total steps have happened
[32m[20221214 14:30:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1448 --------------------------#
[32m[20221214 14:30:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:17 @agent_ppo2.py:185][0m |           0.0014 |         209.5480 |        -109.6547 |
[32m[20221214 14:30:17 @agent_ppo2.py:185][0m |          -0.0017 |         203.6758 |        -109.8561 |
[32m[20221214 14:30:17 @agent_ppo2.py:185][0m |          -0.0026 |         201.9659 |        -109.8282 |
[32m[20221214 14:30:17 @agent_ppo2.py:185][0m |          -0.0027 |         200.0285 |        -110.0334 |
[32m[20221214 14:30:17 @agent_ppo2.py:185][0m |          -0.0039 |         198.2605 |        -110.2152 |
[32m[20221214 14:30:17 @agent_ppo2.py:185][0m |          -0.0034 |         197.1404 |        -110.3702 |
[32m[20221214 14:30:17 @agent_ppo2.py:185][0m |          -0.0048 |         196.3238 |        -110.3486 |
[32m[20221214 14:30:17 @agent_ppo2.py:185][0m |          -0.0023 |         195.0191 |        -110.6142 |
[32m[20221214 14:30:17 @agent_ppo2.py:185][0m |          -0.0022 |         193.9842 |        -110.4959 |
[32m[20221214 14:30:17 @agent_ppo2.py:185][0m |          -0.0022 |         193.3290 |        -110.9034 |
[32m[20221214 14:30:17 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.99
[32m[20221214 14:30:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.93
[32m[20221214 14:30:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.67
[32m[20221214 14:30:18 @agent_ppo2.py:143][0m Total time:      32.26 min
[32m[20221214 14:30:18 @agent_ppo2.py:145][0m 2967552 total steps have happened
[32m[20221214 14:30:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1449 --------------------------#
[32m[20221214 14:30:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:18 @agent_ppo2.py:185][0m |          -0.0016 |         163.8032 |        -112.8420 |
[32m[20221214 14:30:18 @agent_ppo2.py:185][0m |          -0.0013 |         152.4015 |        -112.7308 |
[32m[20221214 14:30:18 @agent_ppo2.py:185][0m |          -0.0009 |         146.7661 |        -112.6385 |
[32m[20221214 14:30:18 @agent_ppo2.py:185][0m |          -0.0027 |         144.7599 |        -112.5619 |
[32m[20221214 14:30:18 @agent_ppo2.py:185][0m |          -0.0011 |         142.1329 |        -112.6599 |
[32m[20221214 14:30:18 @agent_ppo2.py:185][0m |          -0.0011 |         140.5210 |        -112.3839 |
[32m[20221214 14:30:18 @agent_ppo2.py:185][0m |          -0.0002 |         138.9524 |        -112.6223 |
[32m[20221214 14:30:19 @agent_ppo2.py:185][0m |          -0.0036 |         137.0576 |        -112.3808 |
[32m[20221214 14:30:19 @agent_ppo2.py:185][0m |          -0.0023 |         135.4087 |        -112.3202 |
[32m[20221214 14:30:19 @agent_ppo2.py:185][0m |          -0.0023 |         133.3562 |        -112.4290 |
[32m[20221214 14:30:19 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.61
[32m[20221214 14:30:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.78
[32m[20221214 14:30:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.63
[32m[20221214 14:30:19 @agent_ppo2.py:143][0m Total time:      32.28 min
[32m[20221214 14:30:19 @agent_ppo2.py:145][0m 2969600 total steps have happened
[32m[20221214 14:30:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1450 --------------------------#
[32m[20221214 14:30:19 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:30:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:19 @agent_ppo2.py:185][0m |          -0.0015 |         141.2358 |        -111.0467 |
[32m[20221214 14:30:19 @agent_ppo2.py:185][0m |           0.0012 |         125.4280 |        -110.9295 |
[32m[20221214 14:30:19 @agent_ppo2.py:185][0m |           0.0001 |         118.6137 |        -110.7549 |
[32m[20221214 14:30:20 @agent_ppo2.py:185][0m |          -0.0010 |         115.7975 |        -110.7464 |
[32m[20221214 14:30:20 @agent_ppo2.py:185][0m |          -0.0054 |         114.0890 |        -110.8185 |
[32m[20221214 14:30:20 @agent_ppo2.py:185][0m |          -0.0022 |         112.0120 |        -110.8942 |
[32m[20221214 14:30:20 @agent_ppo2.py:185][0m |          -0.0040 |         111.2598 |        -110.8116 |
[32m[20221214 14:30:20 @agent_ppo2.py:185][0m |          -0.0032 |         110.1214 |        -110.9593 |
[32m[20221214 14:30:20 @agent_ppo2.py:185][0m |          -0.0046 |         108.6660 |        -110.9050 |
[32m[20221214 14:30:20 @agent_ppo2.py:185][0m |          -0.0018 |         109.3737 |        -111.1582 |
[32m[20221214 14:30:20 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:30:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.44
[32m[20221214 14:30:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.16
[32m[20221214 14:30:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.99
[32m[20221214 14:30:20 @agent_ppo2.py:143][0m Total time:      32.30 min
[32m[20221214 14:30:20 @agent_ppo2.py:145][0m 2971648 total steps have happened
[32m[20221214 14:30:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1451 --------------------------#
[32m[20221214 14:30:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:30:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:21 @agent_ppo2.py:185][0m |          -0.0022 |         205.4456 |        -109.9810 |
[32m[20221214 14:30:21 @agent_ppo2.py:185][0m |           0.0000 |         194.5076 |        -109.9461 |
[32m[20221214 14:30:21 @agent_ppo2.py:185][0m |          -0.0019 |         191.1016 |        -110.1744 |
[32m[20221214 14:30:21 @agent_ppo2.py:185][0m |          -0.0014 |         188.1625 |        -109.8908 |
[32m[20221214 14:30:21 @agent_ppo2.py:185][0m |          -0.0033 |         185.6309 |        -110.0432 |
[32m[20221214 14:30:21 @agent_ppo2.py:185][0m |          -0.0048 |         183.9410 |        -110.1180 |
[32m[20221214 14:30:21 @agent_ppo2.py:185][0m |          -0.0060 |         182.1666 |        -110.1194 |
[32m[20221214 14:30:21 @agent_ppo2.py:185][0m |          -0.0018 |         180.5643 |        -109.9238 |
[32m[20221214 14:30:21 @agent_ppo2.py:185][0m |           0.0008 |         180.8793 |        -110.0816 |
[32m[20221214 14:30:21 @agent_ppo2.py:185][0m |           0.0068 |         191.4134 |        -109.6886 |
[32m[20221214 14:30:21 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:30:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.28
[32m[20221214 14:30:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.24
[32m[20221214 14:30:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.81
[32m[20221214 14:30:22 @agent_ppo2.py:143][0m Total time:      32.33 min
[32m[20221214 14:30:22 @agent_ppo2.py:145][0m 2973696 total steps have happened
[32m[20221214 14:30:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1452 --------------------------#
[32m[20221214 14:30:22 @agent_ppo2.py:127][0m Sampling time: 0.28 s by 5 slaves
[32m[20221214 14:30:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:22 @agent_ppo2.py:185][0m |          -0.0002 |         148.5634 |        -112.7663 |
[32m[20221214 14:30:22 @agent_ppo2.py:185][0m |           0.0077 |         142.3898 |        -112.7240 |
[32m[20221214 14:30:22 @agent_ppo2.py:185][0m |          -0.0040 |         132.3998 |        -112.4858 |
[32m[20221214 14:30:22 @agent_ppo2.py:185][0m |          -0.0056 |         129.9922 |        -112.5467 |
[32m[20221214 14:30:22 @agent_ppo2.py:185][0m |          -0.0015 |         126.6945 |        -112.7050 |
[32m[20221214 14:30:23 @agent_ppo2.py:185][0m |          -0.0066 |         125.9885 |        -111.9888 |
[32m[20221214 14:30:23 @agent_ppo2.py:185][0m |           0.0031 |         125.6716 |        -112.3970 |
[32m[20221214 14:30:23 @agent_ppo2.py:185][0m |          -0.0060 |         122.6529 |        -112.3650 |
[32m[20221214 14:30:23 @agent_ppo2.py:185][0m |          -0.0063 |         119.5481 |        -112.4748 |
[32m[20221214 14:30:23 @agent_ppo2.py:185][0m |          -0.0028 |         118.0721 |        -112.3475 |
[32m[20221214 14:30:23 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:30:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.27
[32m[20221214 14:30:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.19
[32m[20221214 14:30:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.69
[32m[20221214 14:30:23 @agent_ppo2.py:143][0m Total time:      32.35 min
[32m[20221214 14:30:23 @agent_ppo2.py:145][0m 2975744 total steps have happened
[32m[20221214 14:30:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1453 --------------------------#
[32m[20221214 14:30:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:23 @agent_ppo2.py:185][0m |          -0.0014 |         208.0356 |        -113.9596 |
[32m[20221214 14:30:24 @agent_ppo2.py:185][0m |          -0.0041 |         196.8793 |        -113.6987 |
[32m[20221214 14:30:24 @agent_ppo2.py:185][0m |           0.0057 |         204.5925 |        -113.2863 |
[32m[20221214 14:30:24 @agent_ppo2.py:185][0m |          -0.0035 |         191.4823 |        -113.6114 |
[32m[20221214 14:30:24 @agent_ppo2.py:185][0m |          -0.0028 |         189.9179 |        -113.8244 |
[32m[20221214 14:30:24 @agent_ppo2.py:185][0m |          -0.0019 |         188.2364 |        -113.5499 |
[32m[20221214 14:30:24 @agent_ppo2.py:185][0m |          -0.0026 |         186.8398 |        -114.0081 |
[32m[20221214 14:30:24 @agent_ppo2.py:185][0m |          -0.0038 |         186.3937 |        -113.7999 |
[32m[20221214 14:30:24 @agent_ppo2.py:185][0m |          -0.0024 |         185.3788 |        -113.6687 |
[32m[20221214 14:30:24 @agent_ppo2.py:185][0m |          -0.0040 |         183.9479 |        -113.8765 |
[32m[20221214 14:30:24 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.10
[32m[20221214 14:30:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.80
[32m[20221214 14:30:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.81
[32m[20221214 14:30:24 @agent_ppo2.py:143][0m Total time:      32.37 min
[32m[20221214 14:30:24 @agent_ppo2.py:145][0m 2977792 total steps have happened
[32m[20221214 14:30:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1454 --------------------------#
[32m[20221214 14:30:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:30:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:25 @agent_ppo2.py:185][0m |          -0.0012 |         218.9172 |        -111.9582 |
[32m[20221214 14:30:25 @agent_ppo2.py:185][0m |           0.0009 |         207.4556 |        -111.9492 |
[32m[20221214 14:30:25 @agent_ppo2.py:185][0m |          -0.0007 |         202.9070 |        -111.7905 |
[32m[20221214 14:30:25 @agent_ppo2.py:185][0m |          -0.0021 |         200.2319 |        -111.5195 |
[32m[20221214 14:30:25 @agent_ppo2.py:185][0m |          -0.0027 |         198.5996 |        -111.4309 |
[32m[20221214 14:30:25 @agent_ppo2.py:185][0m |           0.0005 |         197.2323 |        -111.5821 |
[32m[20221214 14:30:25 @agent_ppo2.py:185][0m |          -0.0013 |         195.4286 |        -111.4229 |
[32m[20221214 14:30:25 @agent_ppo2.py:185][0m |          -0.0025 |         194.7750 |        -111.4966 |
[32m[20221214 14:30:25 @agent_ppo2.py:185][0m |          -0.0013 |         193.7033 |        -111.1299 |
[32m[20221214 14:30:26 @agent_ppo2.py:185][0m |           0.0023 |         195.4091 |        -111.4619 |
[32m[20221214 14:30:26 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:30:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.74
[32m[20221214 14:30:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.09
[32m[20221214 14:30:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.84
[32m[20221214 14:30:26 @agent_ppo2.py:143][0m Total time:      32.39 min
[32m[20221214 14:30:26 @agent_ppo2.py:145][0m 2979840 total steps have happened
[32m[20221214 14:30:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1455 --------------------------#
[32m[20221214 14:30:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:30:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:26 @agent_ppo2.py:185][0m |          -0.0040 |         155.1490 |        -111.9394 |
[32m[20221214 14:30:26 @agent_ppo2.py:185][0m |          -0.0022 |         144.2113 |        -112.1305 |
[32m[20221214 14:30:26 @agent_ppo2.py:185][0m |          -0.0034 |         139.4486 |        -112.1237 |
[32m[20221214 14:30:26 @agent_ppo2.py:185][0m |           0.0036 |         138.9641 |        -112.1919 |
[32m[20221214 14:30:26 @agent_ppo2.py:185][0m |          -0.0036 |         134.7607 |        -112.3820 |
[32m[20221214 14:30:26 @agent_ppo2.py:185][0m |          -0.0029 |         131.9687 |        -112.3728 |
[32m[20221214 14:30:27 @agent_ppo2.py:185][0m |          -0.0048 |         129.8444 |        -112.4031 |
[32m[20221214 14:30:27 @agent_ppo2.py:185][0m |          -0.0041 |         129.0965 |        -112.3140 |
[32m[20221214 14:30:27 @agent_ppo2.py:185][0m |          -0.0059 |         127.7962 |        -112.4455 |
[32m[20221214 14:30:27 @agent_ppo2.py:185][0m |          -0.0053 |         127.0358 |        -112.6465 |
[32m[20221214 14:30:27 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:30:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.81
[32m[20221214 14:30:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.51
[32m[20221214 14:30:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.62
[32m[20221214 14:30:27 @agent_ppo2.py:143][0m Total time:      32.41 min
[32m[20221214 14:30:27 @agent_ppo2.py:145][0m 2981888 total steps have happened
[32m[20221214 14:30:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1456 --------------------------#
[32m[20221214 14:30:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:27 @agent_ppo2.py:185][0m |          -0.0010 |         188.2888 |        -113.4555 |
[32m[20221214 14:30:27 @agent_ppo2.py:185][0m |           0.0021 |         163.1726 |        -113.0712 |
[32m[20221214 14:30:28 @agent_ppo2.py:185][0m |           0.0115 |         159.5117 |        -113.0999 |
[32m[20221214 14:30:28 @agent_ppo2.py:185][0m |          -0.0054 |         148.8586 |        -113.1432 |
[32m[20221214 14:30:28 @agent_ppo2.py:185][0m |          -0.0036 |         146.0885 |        -113.3546 |
[32m[20221214 14:30:28 @agent_ppo2.py:185][0m |           0.0012 |         148.8121 |        -112.9976 |
[32m[20221214 14:30:28 @agent_ppo2.py:185][0m |          -0.0039 |         141.0719 |        -113.2498 |
[32m[20221214 14:30:28 @agent_ppo2.py:185][0m |          -0.0049 |         139.5066 |        -113.1760 |
[32m[20221214 14:30:28 @agent_ppo2.py:185][0m |          -0.0054 |         138.5537 |        -113.0200 |
[32m[20221214 14:30:28 @agent_ppo2.py:185][0m |          -0.0072 |         137.1344 |        -112.8645 |
[32m[20221214 14:30:28 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:30:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 787.29
[32m[20221214 14:30:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.52
[32m[20221214 14:30:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.24
[32m[20221214 14:30:28 @agent_ppo2.py:143][0m Total time:      32.44 min
[32m[20221214 14:30:28 @agent_ppo2.py:145][0m 2983936 total steps have happened
[32m[20221214 14:30:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1457 --------------------------#
[32m[20221214 14:30:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:29 @agent_ppo2.py:185][0m |          -0.0023 |         173.0405 |        -113.3367 |
[32m[20221214 14:30:29 @agent_ppo2.py:185][0m |          -0.0026 |         144.1957 |        -113.1328 |
[32m[20221214 14:30:29 @agent_ppo2.py:185][0m |          -0.0024 |         137.5357 |        -113.6511 |
[32m[20221214 14:30:29 @agent_ppo2.py:185][0m |          -0.0023 |         133.8092 |        -113.4681 |
[32m[20221214 14:30:29 @agent_ppo2.py:185][0m |          -0.0072 |         131.0904 |        -113.3095 |
[32m[20221214 14:30:29 @agent_ppo2.py:185][0m |          -0.0025 |         129.6582 |        -113.4163 |
[32m[20221214 14:30:29 @agent_ppo2.py:185][0m |          -0.0052 |         127.7658 |        -113.4794 |
[32m[20221214 14:30:29 @agent_ppo2.py:185][0m |          -0.0009 |         127.6791 |        -113.6854 |
[32m[20221214 14:30:30 @agent_ppo2.py:185][0m |          -0.0019 |         125.2076 |        -113.1021 |
[32m[20221214 14:30:30 @agent_ppo2.py:185][0m |          -0.0017 |         124.9385 |        -113.4138 |
[32m[20221214 14:30:30 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:30:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.85
[32m[20221214 14:30:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.05
[32m[20221214 14:30:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.31
[32m[20221214 14:30:30 @agent_ppo2.py:143][0m Total time:      32.46 min
[32m[20221214 14:30:30 @agent_ppo2.py:145][0m 2985984 total steps have happened
[32m[20221214 14:30:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1458 --------------------------#
[32m[20221214 14:30:30 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:30:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:30 @agent_ppo2.py:185][0m |          -0.0015 |          80.2897 |        -113.4894 |
[32m[20221214 14:30:30 @agent_ppo2.py:185][0m |          -0.0015 |          56.7851 |        -113.4795 |
[32m[20221214 14:30:30 @agent_ppo2.py:185][0m |          -0.0120 |          49.4316 |        -113.4868 |
[32m[20221214 14:30:30 @agent_ppo2.py:185][0m |          -0.0061 |          46.2262 |        -113.4040 |
[32m[20221214 14:30:30 @agent_ppo2.py:185][0m |          -0.0027 |          43.6253 |        -113.5585 |
[32m[20221214 14:30:31 @agent_ppo2.py:185][0m |          -0.0028 |          41.6604 |        -113.5592 |
[32m[20221214 14:30:31 @agent_ppo2.py:185][0m |          -0.0048 |          40.5447 |        -113.3349 |
[32m[20221214 14:30:31 @agent_ppo2.py:185][0m |          -0.0076 |          39.7360 |        -113.4714 |
[32m[20221214 14:30:31 @agent_ppo2.py:185][0m |          -0.0013 |          39.5932 |        -113.5230 |
[32m[20221214 14:30:31 @agent_ppo2.py:185][0m |          -0.0084 |          38.1752 |        -113.4260 |
[32m[20221214 14:30:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:30:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 762.75
[32m[20221214 14:30:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.48
[32m[20221214 14:30:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.30
[32m[20221214 14:30:31 @agent_ppo2.py:143][0m Total time:      32.48 min
[32m[20221214 14:30:31 @agent_ppo2.py:145][0m 2988032 total steps have happened
[32m[20221214 14:30:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1459 --------------------------#
[32m[20221214 14:30:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:30:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:31 @agent_ppo2.py:185][0m |           0.0003 |          78.5563 |        -110.5402 |
[32m[20221214 14:30:31 @agent_ppo2.py:185][0m |          -0.0014 |          64.6540 |        -110.5130 |
[32m[20221214 14:30:32 @agent_ppo2.py:185][0m |          -0.0018 |          60.6228 |        -110.2003 |
[32m[20221214 14:30:32 @agent_ppo2.py:185][0m |           0.0023 |          59.0558 |        -110.4103 |
[32m[20221214 14:30:32 @agent_ppo2.py:185][0m |          -0.0060 |          56.7941 |        -110.1815 |
[32m[20221214 14:30:32 @agent_ppo2.py:185][0m |          -0.0086 |          55.6926 |        -110.3114 |
[32m[20221214 14:30:32 @agent_ppo2.py:185][0m |          -0.0039 |          54.1799 |        -109.9633 |
[32m[20221214 14:30:32 @agent_ppo2.py:185][0m |           0.0034 |          54.7918 |        -110.0371 |
[32m[20221214 14:30:32 @agent_ppo2.py:185][0m |          -0.0018 |          52.5978 |        -109.8230 |
[32m[20221214 14:30:32 @agent_ppo2.py:185][0m |          -0.0008 |          52.3707 |        -109.6750 |
[32m[20221214 14:30:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:30:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.19
[32m[20221214 14:30:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.77
[32m[20221214 14:30:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.05
[32m[20221214 14:30:32 @agent_ppo2.py:143][0m Total time:      32.50 min
[32m[20221214 14:30:32 @agent_ppo2.py:145][0m 2990080 total steps have happened
[32m[20221214 14:30:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1460 --------------------------#
[32m[20221214 14:30:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:30:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:33 @agent_ppo2.py:185][0m |          -0.0029 |          55.0696 |        -111.9845 |
[32m[20221214 14:30:33 @agent_ppo2.py:185][0m |          -0.0044 |          41.1296 |        -111.9163 |
[32m[20221214 14:30:33 @agent_ppo2.py:185][0m |          -0.0020 |          35.9716 |        -111.9065 |
[32m[20221214 14:30:33 @agent_ppo2.py:185][0m |          -0.0014 |          33.5771 |        -111.9417 |
[32m[20221214 14:30:33 @agent_ppo2.py:185][0m |          -0.0011 |          32.0846 |        -111.7430 |
[32m[20221214 14:30:33 @agent_ppo2.py:185][0m |           0.0035 |          31.0611 |        -111.7275 |
[32m[20221214 14:30:33 @agent_ppo2.py:185][0m |           0.0014 |          31.2545 |        -111.8269 |
[32m[20221214 14:30:33 @agent_ppo2.py:185][0m |          -0.0047 |          29.9294 |        -111.9203 |
[32m[20221214 14:30:34 @agent_ppo2.py:185][0m |          -0.0020 |          29.0458 |        -111.8230 |
[32m[20221214 14:30:34 @agent_ppo2.py:185][0m |          -0.0043 |          28.6373 |        -111.6422 |
[32m[20221214 14:30:34 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:30:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 776.68
[32m[20221214 14:30:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.95
[32m[20221214 14:30:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.17
[32m[20221214 14:30:34 @agent_ppo2.py:143][0m Total time:      32.53 min
[32m[20221214 14:30:34 @agent_ppo2.py:145][0m 2992128 total steps have happened
[32m[20221214 14:30:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1461 --------------------------#
[32m[20221214 14:30:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:34 @agent_ppo2.py:185][0m |           0.0098 |         123.1385 |        -110.7239 |
[32m[20221214 14:30:34 @agent_ppo2.py:185][0m |           0.0043 |         107.6020 |        -110.5813 |
[32m[20221214 14:30:34 @agent_ppo2.py:185][0m |           0.0070 |         100.3626 |        -110.3206 |
[32m[20221214 14:30:34 @agent_ppo2.py:185][0m |          -0.0012 |          95.3734 |        -110.0718 |
[32m[20221214 14:30:34 @agent_ppo2.py:185][0m |          -0.0014 |          92.1134 |        -110.3441 |
[32m[20221214 14:30:35 @agent_ppo2.py:185][0m |          -0.0003 |          90.5732 |        -110.0973 |
[32m[20221214 14:30:35 @agent_ppo2.py:185][0m |          -0.0034 |          88.5806 |        -110.2567 |
[32m[20221214 14:30:35 @agent_ppo2.py:185][0m |          -0.0011 |          86.8726 |        -110.0286 |
[32m[20221214 14:30:35 @agent_ppo2.py:185][0m |           0.0107 |          91.5750 |        -110.0797 |
[32m[20221214 14:30:35 @agent_ppo2.py:185][0m |           0.0005 |          85.5902 |        -110.2034 |
[32m[20221214 14:30:35 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.70
[32m[20221214 14:30:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.28
[32m[20221214 14:30:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.43
[32m[20221214 14:30:35 @agent_ppo2.py:143][0m Total time:      32.55 min
[32m[20221214 14:30:35 @agent_ppo2.py:145][0m 2994176 total steps have happened
[32m[20221214 14:30:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1462 --------------------------#
[32m[20221214 14:30:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:35 @agent_ppo2.py:185][0m |           0.0024 |          62.8203 |        -107.8043 |
[32m[20221214 14:30:36 @agent_ppo2.py:185][0m |          -0.0008 |          48.8036 |        -108.0105 |
[32m[20221214 14:30:36 @agent_ppo2.py:185][0m |          -0.0005 |          44.0346 |        -108.2136 |
[32m[20221214 14:30:36 @agent_ppo2.py:185][0m |          -0.0030 |          42.1224 |        -108.3928 |
[32m[20221214 14:30:36 @agent_ppo2.py:185][0m |          -0.0022 |          39.7534 |        -108.5652 |
[32m[20221214 14:30:36 @agent_ppo2.py:185][0m |          -0.0025 |          38.4918 |        -108.4485 |
[32m[20221214 14:30:36 @agent_ppo2.py:185][0m |           0.0011 |          37.8407 |        -108.6269 |
[32m[20221214 14:30:36 @agent_ppo2.py:185][0m |          -0.0017 |          36.7611 |        -108.9254 |
[32m[20221214 14:30:36 @agent_ppo2.py:185][0m |           0.0034 |          36.6335 |        -109.0640 |
[32m[20221214 14:30:36 @agent_ppo2.py:185][0m |          -0.0039 |          35.2291 |        -109.0330 |
[32m[20221214 14:30:36 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:30:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 725.51
[32m[20221214 14:30:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.44
[32m[20221214 14:30:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.13
[32m[20221214 14:30:37 @agent_ppo2.py:143][0m Total time:      32.57 min
[32m[20221214 14:30:37 @agent_ppo2.py:145][0m 2996224 total steps have happened
[32m[20221214 14:30:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1463 --------------------------#
[32m[20221214 14:30:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:30:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:37 @agent_ppo2.py:185][0m |           0.0003 |          87.4480 |        -112.0570 |
[32m[20221214 14:30:37 @agent_ppo2.py:185][0m |          -0.0054 |          75.1162 |        -111.7757 |
[32m[20221214 14:30:37 @agent_ppo2.py:185][0m |          -0.0042 |          70.8679 |        -111.8156 |
[32m[20221214 14:30:37 @agent_ppo2.py:185][0m |          -0.0053 |          68.6620 |        -111.6331 |
[32m[20221214 14:30:37 @agent_ppo2.py:185][0m |           0.0002 |          67.5515 |        -111.7981 |
[32m[20221214 14:30:37 @agent_ppo2.py:185][0m |          -0.0076 |          66.5324 |        -111.8162 |
[32m[20221214 14:30:37 @agent_ppo2.py:185][0m |          -0.0028 |          64.7815 |        -111.8168 |
[32m[20221214 14:30:37 @agent_ppo2.py:185][0m |          -0.0057 |          63.6990 |        -111.9824 |
[32m[20221214 14:30:38 @agent_ppo2.py:185][0m |          -0.0067 |          62.6994 |        -111.6644 |
[32m[20221214 14:30:38 @agent_ppo2.py:185][0m |          -0.0083 |          62.5014 |        -111.7726 |
[32m[20221214 14:30:38 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:30:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 740.04
[32m[20221214 14:30:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.78
[32m[20221214 14:30:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.13
[32m[20221214 14:30:38 @agent_ppo2.py:143][0m Total time:      32.60 min
[32m[20221214 14:30:38 @agent_ppo2.py:145][0m 2998272 total steps have happened
[32m[20221214 14:30:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1464 --------------------------#
[32m[20221214 14:30:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:30:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:38 @agent_ppo2.py:185][0m |          -0.0004 |         212.3432 |        -112.3405 |
[32m[20221214 14:30:38 @agent_ppo2.py:185][0m |           0.0037 |         196.9077 |        -112.6231 |
[32m[20221214 14:30:38 @agent_ppo2.py:185][0m |          -0.0033 |         189.3146 |        -112.2654 |
[32m[20221214 14:30:38 @agent_ppo2.py:185][0m |           0.0001 |         186.2904 |        -112.2945 |
[32m[20221214 14:30:38 @agent_ppo2.py:185][0m |          -0.0009 |         182.5734 |        -112.3252 |
[32m[20221214 14:30:39 @agent_ppo2.py:185][0m |          -0.0031 |         180.9135 |        -112.1741 |
[32m[20221214 14:30:39 @agent_ppo2.py:185][0m |          -0.0026 |         179.3521 |        -112.1092 |
[32m[20221214 14:30:39 @agent_ppo2.py:185][0m |          -0.0005 |         177.6733 |        -112.1765 |
[32m[20221214 14:30:39 @agent_ppo2.py:185][0m |          -0.0018 |         177.2641 |        -112.0201 |
[32m[20221214 14:30:39 @agent_ppo2.py:185][0m |          -0.0014 |         176.6796 |        -112.0575 |
[32m[20221214 14:30:39 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:30:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.92
[32m[20221214 14:30:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.57
[32m[20221214 14:30:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.56
[32m[20221214 14:30:39 @agent_ppo2.py:143][0m Total time:      32.62 min
[32m[20221214 14:30:39 @agent_ppo2.py:145][0m 3000320 total steps have happened
[32m[20221214 14:30:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1465 --------------------------#
[32m[20221214 14:30:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:30:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:39 @agent_ppo2.py:185][0m |           0.0003 |         220.3145 |        -113.6349 |
[32m[20221214 14:30:39 @agent_ppo2.py:185][0m |           0.0033 |         199.8545 |        -113.7765 |
[32m[20221214 14:30:40 @agent_ppo2.py:185][0m |          -0.0015 |         188.3922 |        -114.0058 |
[32m[20221214 14:30:40 @agent_ppo2.py:185][0m |          -0.0037 |         185.2957 |        -114.3008 |
[32m[20221214 14:30:40 @agent_ppo2.py:185][0m |          -0.0043 |         181.3781 |        -114.3401 |
[32m[20221214 14:30:40 @agent_ppo2.py:185][0m |          -0.0051 |         180.8525 |        -114.5157 |
[32m[20221214 14:30:40 @agent_ppo2.py:185][0m |          -0.0040 |         180.7289 |        -114.6769 |
[32m[20221214 14:30:40 @agent_ppo2.py:185][0m |          -0.0054 |         178.1101 |        -114.7096 |
[32m[20221214 14:30:40 @agent_ppo2.py:185][0m |           0.0078 |         196.1572 |        -114.7746 |
[32m[20221214 14:30:40 @agent_ppo2.py:185][0m |          -0.0072 |         180.5398 |        -114.9283 |
[32m[20221214 14:30:40 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:30:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.47
[32m[20221214 14:30:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.57
[32m[20221214 14:30:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.62
[32m[20221214 14:30:40 @agent_ppo2.py:143][0m Total time:      32.64 min
[32m[20221214 14:30:40 @agent_ppo2.py:145][0m 3002368 total steps have happened
[32m[20221214 14:30:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1466 --------------------------#
[32m[20221214 14:30:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:41 @agent_ppo2.py:185][0m |           0.0078 |         243.7170 |        -115.9750 |
[32m[20221214 14:30:41 @agent_ppo2.py:185][0m |           0.0103 |         224.1522 |        -115.6350 |
[32m[20221214 14:30:41 @agent_ppo2.py:185][0m |          -0.0029 |         207.5630 |        -115.5531 |
[32m[20221214 14:30:41 @agent_ppo2.py:185][0m |          -0.0019 |         204.9280 |        -115.3313 |
[32m[20221214 14:30:41 @agent_ppo2.py:185][0m |           0.0003 |         201.7142 |        -115.5285 |
[32m[20221214 14:30:41 @agent_ppo2.py:185][0m |          -0.0025 |         200.6598 |        -115.6905 |
[32m[20221214 14:30:41 @agent_ppo2.py:185][0m |           0.0003 |         199.1775 |        -115.4827 |
[32m[20221214 14:30:41 @agent_ppo2.py:185][0m |          -0.0021 |         196.2829 |        -115.5683 |
[32m[20221214 14:30:42 @agent_ppo2.py:185][0m |          -0.0025 |         196.0276 |        -115.6374 |
[32m[20221214 14:30:42 @agent_ppo2.py:185][0m |          -0.0025 |         193.0633 |        -115.9142 |
[32m[20221214 14:30:42 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:30:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.43
[32m[20221214 14:30:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.92
[32m[20221214 14:30:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.13
[32m[20221214 14:30:42 @agent_ppo2.py:143][0m Total time:      32.66 min
[32m[20221214 14:30:42 @agent_ppo2.py:145][0m 3004416 total steps have happened
[32m[20221214 14:30:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1467 --------------------------#
[32m[20221214 14:30:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:42 @agent_ppo2.py:185][0m |           0.0069 |         207.3680 |        -115.4332 |
[32m[20221214 14:30:42 @agent_ppo2.py:185][0m |           0.0042 |         201.9257 |        -115.2525 |
[32m[20221214 14:30:42 @agent_ppo2.py:185][0m |          -0.0019 |         182.0298 |        -115.1690 |
[32m[20221214 14:30:42 @agent_ppo2.py:185][0m |           0.0061 |         192.9993 |        -115.5872 |
[32m[20221214 14:30:43 @agent_ppo2.py:185][0m |          -0.0003 |         177.0438 |        -114.8348 |
[32m[20221214 14:30:43 @agent_ppo2.py:185][0m |          -0.0033 |         174.8574 |        -115.1474 |
[32m[20221214 14:30:43 @agent_ppo2.py:185][0m |          -0.0014 |         173.2416 |        -115.3056 |
[32m[20221214 14:30:43 @agent_ppo2.py:185][0m |          -0.0028 |         172.0325 |        -115.5709 |
[32m[20221214 14:30:43 @agent_ppo2.py:185][0m |          -0.0043 |         170.7833 |        -115.2388 |
[32m[20221214 14:30:43 @agent_ppo2.py:185][0m |          -0.0029 |         170.0148 |        -115.2937 |
[32m[20221214 14:30:43 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.78
[32m[20221214 14:30:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.23
[32m[20221214 14:30:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.62
[32m[20221214 14:30:43 @agent_ppo2.py:143][0m Total time:      32.68 min
[32m[20221214 14:30:43 @agent_ppo2.py:145][0m 3006464 total steps have happened
[32m[20221214 14:30:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1468 --------------------------#
[32m[20221214 14:30:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:44 @agent_ppo2.py:185][0m |          -0.0015 |         255.0038 |        -111.4958 |
[32m[20221214 14:30:44 @agent_ppo2.py:185][0m |          -0.0016 |         240.5666 |        -111.2726 |
[32m[20221214 14:30:44 @agent_ppo2.py:185][0m |          -0.0015 |         231.0934 |        -111.2425 |
[32m[20221214 14:30:44 @agent_ppo2.py:185][0m |          -0.0021 |         226.4039 |        -111.2894 |
[32m[20221214 14:30:44 @agent_ppo2.py:185][0m |           0.0087 |         237.6318 |        -111.1801 |
[32m[20221214 14:30:44 @agent_ppo2.py:185][0m |          -0.0023 |         219.6101 |        -111.4150 |
[32m[20221214 14:30:44 @agent_ppo2.py:185][0m |          -0.0031 |         218.3541 |        -111.0516 |
[32m[20221214 14:30:44 @agent_ppo2.py:185][0m |          -0.0046 |         216.0935 |        -111.3725 |
[32m[20221214 14:30:44 @agent_ppo2.py:185][0m |          -0.0028 |         214.0605 |        -111.0621 |
[32m[20221214 14:30:44 @agent_ppo2.py:185][0m |          -0.0018 |         213.5312 |        -111.3392 |
[32m[20221214 14:30:44 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:30:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.21
[32m[20221214 14:30:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.16
[32m[20221214 14:30:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.72
[32m[20221214 14:30:45 @agent_ppo2.py:143][0m Total time:      32.71 min
[32m[20221214 14:30:45 @agent_ppo2.py:145][0m 3008512 total steps have happened
[32m[20221214 14:30:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1469 --------------------------#
[32m[20221214 14:30:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:30:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:45 @agent_ppo2.py:185][0m |          -0.0034 |         226.1159 |        -112.7030 |
[32m[20221214 14:30:45 @agent_ppo2.py:185][0m |          -0.0023 |         216.5100 |        -112.8892 |
[32m[20221214 14:30:45 @agent_ppo2.py:185][0m |          -0.0046 |         211.3454 |        -112.5438 |
[32m[20221214 14:30:45 @agent_ppo2.py:185][0m |          -0.0036 |         210.2439 |        -112.2012 |
[32m[20221214 14:30:45 @agent_ppo2.py:185][0m |           0.0119 |         231.8588 |        -112.3657 |
[32m[20221214 14:30:45 @agent_ppo2.py:185][0m |          -0.0048 |         208.2185 |        -112.3416 |
[32m[20221214 14:30:45 @agent_ppo2.py:185][0m |          -0.0044 |         205.8268 |        -112.0859 |
[32m[20221214 14:30:45 @agent_ppo2.py:185][0m |          -0.0051 |         204.5436 |        -111.6627 |
[32m[20221214 14:30:46 @agent_ppo2.py:185][0m |          -0.0046 |         203.2863 |        -111.7307 |
[32m[20221214 14:30:46 @agent_ppo2.py:185][0m |          -0.0050 |         203.3933 |        -111.7988 |
[32m[20221214 14:30:46 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.89
[32m[20221214 14:30:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.31
[32m[20221214 14:30:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.05
[32m[20221214 14:30:46 @agent_ppo2.py:143][0m Total time:      32.73 min
[32m[20221214 14:30:46 @agent_ppo2.py:145][0m 3010560 total steps have happened
[32m[20221214 14:30:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1470 --------------------------#
[32m[20221214 14:30:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:30:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:46 @agent_ppo2.py:185][0m |           0.0043 |         237.5667 |        -112.2556 |
[32m[20221214 14:30:46 @agent_ppo2.py:185][0m |          -0.0023 |         229.3409 |        -112.2980 |
[32m[20221214 14:30:46 @agent_ppo2.py:185][0m |          -0.0022 |         228.8668 |        -112.4250 |
[32m[20221214 14:30:46 @agent_ppo2.py:185][0m |          -0.0010 |         226.8741 |        -112.7674 |
[32m[20221214 14:30:47 @agent_ppo2.py:185][0m |          -0.0008 |         225.6909 |        -112.3982 |
[32m[20221214 14:30:47 @agent_ppo2.py:185][0m |          -0.0035 |         226.0021 |        -112.5957 |
[32m[20221214 14:30:47 @agent_ppo2.py:185][0m |          -0.0008 |         226.2498 |        -112.4140 |
[32m[20221214 14:30:47 @agent_ppo2.py:185][0m |           0.0102 |         250.2878 |        -112.5537 |
[32m[20221214 14:30:47 @agent_ppo2.py:185][0m |          -0.0029 |         226.0028 |        -112.5569 |
[32m[20221214 14:30:47 @agent_ppo2.py:185][0m |          -0.0022 |         224.3791 |        -112.8665 |
[32m[20221214 14:30:47 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:30:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.42
[32m[20221214 14:30:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.35
[32m[20221214 14:30:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.25
[32m[20221214 14:30:47 @agent_ppo2.py:143][0m Total time:      32.75 min
[32m[20221214 14:30:47 @agent_ppo2.py:145][0m 3012608 total steps have happened
[32m[20221214 14:30:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1471 --------------------------#
[32m[20221214 14:30:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:48 @agent_ppo2.py:185][0m |          -0.0050 |         241.8904 |        -111.5670 |
[32m[20221214 14:30:48 @agent_ppo2.py:185][0m |          -0.0066 |         232.6368 |        -111.3179 |
[32m[20221214 14:30:48 @agent_ppo2.py:185][0m |          -0.0018 |         231.4843 |        -111.4776 |
[32m[20221214 14:30:48 @agent_ppo2.py:185][0m |          -0.0068 |         226.6847 |        -111.3957 |
[32m[20221214 14:30:48 @agent_ppo2.py:185][0m |           0.0021 |         252.2573 |        -111.6073 |
[32m[20221214 14:30:48 @agent_ppo2.py:185][0m |          -0.0065 |         223.7390 |        -111.7745 |
[32m[20221214 14:30:48 @agent_ppo2.py:185][0m |          -0.0089 |         222.3443 |        -111.7828 |
[32m[20221214 14:30:48 @agent_ppo2.py:185][0m |          -0.0064 |         220.5547 |        -112.0848 |
[32m[20221214 14:30:48 @agent_ppo2.py:185][0m |          -0.0011 |         221.7560 |        -112.1157 |
[32m[20221214 14:30:48 @agent_ppo2.py:185][0m |          -0.0034 |         218.8936 |        -111.9172 |
[32m[20221214 14:30:48 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.44
[32m[20221214 14:30:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.88
[32m[20221214 14:30:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.86
[32m[20221214 14:30:49 @agent_ppo2.py:143][0m Total time:      32.77 min
[32m[20221214 14:30:49 @agent_ppo2.py:145][0m 3014656 total steps have happened
[32m[20221214 14:30:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1472 --------------------------#
[32m[20221214 14:30:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:49 @agent_ppo2.py:185][0m |          -0.0019 |         274.7486 |        -113.7384 |
[32m[20221214 14:30:49 @agent_ppo2.py:185][0m |           0.0014 |         271.9855 |        -113.9289 |
[32m[20221214 14:30:49 @agent_ppo2.py:185][0m |          -0.0009 |         264.9244 |        -113.9240 |
[32m[20221214 14:30:49 @agent_ppo2.py:185][0m |          -0.0032 |         261.9977 |        -114.0401 |
[32m[20221214 14:30:49 @agent_ppo2.py:185][0m |          -0.0028 |         260.0973 |        -114.1753 |
[32m[20221214 14:30:49 @agent_ppo2.py:185][0m |          -0.0016 |         258.9089 |        -114.2236 |
[32m[20221214 14:30:49 @agent_ppo2.py:185][0m |          -0.0016 |         258.4424 |        -114.1083 |
[32m[20221214 14:30:50 @agent_ppo2.py:185][0m |          -0.0017 |         257.4561 |        -114.0585 |
[32m[20221214 14:30:50 @agent_ppo2.py:185][0m |           0.0110 |         270.3403 |        -113.8463 |
[32m[20221214 14:30:50 @agent_ppo2.py:185][0m |           0.0004 |         256.3685 |        -114.0141 |
[32m[20221214 14:30:50 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:30:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.92
[32m[20221214 14:30:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.66
[32m[20221214 14:30:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.17
[32m[20221214 14:30:50 @agent_ppo2.py:143][0m Total time:      32.80 min
[32m[20221214 14:30:50 @agent_ppo2.py:145][0m 3016704 total steps have happened
[32m[20221214 14:30:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1473 --------------------------#
[32m[20221214 14:30:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:50 @agent_ppo2.py:185][0m |           0.0040 |         246.6276 |        -113.9639 |
[32m[20221214 14:30:50 @agent_ppo2.py:185][0m |           0.0010 |         240.3785 |        -114.1658 |
[32m[20221214 14:30:50 @agent_ppo2.py:185][0m |          -0.0018 |         234.8684 |        -114.3379 |
[32m[20221214 14:30:50 @agent_ppo2.py:185][0m |          -0.0002 |         233.9516 |        -114.2479 |
[32m[20221214 14:30:51 @agent_ppo2.py:185][0m |          -0.0019 |         232.2449 |        -114.4940 |
[32m[20221214 14:30:51 @agent_ppo2.py:185][0m |          -0.0016 |         231.0456 |        -114.2420 |
[32m[20221214 14:30:51 @agent_ppo2.py:185][0m |          -0.0007 |         228.9707 |        -114.5397 |
[32m[20221214 14:30:51 @agent_ppo2.py:185][0m |          -0.0032 |         227.3694 |        -114.5410 |
[32m[20221214 14:30:51 @agent_ppo2.py:185][0m |           0.0010 |         226.3700 |        -114.1776 |
[32m[20221214 14:30:51 @agent_ppo2.py:185][0m |          -0.0016 |         225.1671 |        -114.6304 |
[32m[20221214 14:30:51 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.51
[32m[20221214 14:30:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.94
[32m[20221214 14:30:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.13
[32m[20221214 14:30:51 @agent_ppo2.py:143][0m Total time:      32.82 min
[32m[20221214 14:30:51 @agent_ppo2.py:145][0m 3018752 total steps have happened
[32m[20221214 14:30:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1474 --------------------------#
[32m[20221214 14:30:51 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:30:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:52 @agent_ppo2.py:185][0m |          -0.0035 |         212.6414 |        -114.3008 |
[32m[20221214 14:30:52 @agent_ppo2.py:185][0m |           0.0064 |         214.6672 |        -114.6905 |
[32m[20221214 14:30:52 @agent_ppo2.py:185][0m |          -0.0036 |         208.6790 |        -114.7520 |
[32m[20221214 14:30:52 @agent_ppo2.py:185][0m |          -0.0041 |         206.0690 |        -114.8389 |
[32m[20221214 14:30:52 @agent_ppo2.py:185][0m |          -0.0046 |         205.5246 |        -114.7741 |
[32m[20221214 14:30:52 @agent_ppo2.py:185][0m |          -0.0064 |         205.6263 |        -115.0316 |
[32m[20221214 14:30:52 @agent_ppo2.py:185][0m |          -0.0009 |         204.5962 |        -114.9786 |
[32m[20221214 14:30:52 @agent_ppo2.py:185][0m |          -0.0040 |         204.2815 |        -115.1407 |
[32m[20221214 14:30:52 @agent_ppo2.py:185][0m |           0.0050 |         207.4850 |        -114.8835 |
[32m[20221214 14:30:52 @agent_ppo2.py:185][0m |          -0.0049 |         203.1001 |        -115.0967 |
[32m[20221214 14:30:52 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:30:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.19
[32m[20221214 14:30:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.59
[32m[20221214 14:30:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.30
[32m[20221214 14:30:53 @agent_ppo2.py:143][0m Total time:      32.84 min
[32m[20221214 14:30:53 @agent_ppo2.py:145][0m 3020800 total steps have happened
[32m[20221214 14:30:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1475 --------------------------#
[32m[20221214 14:30:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:53 @agent_ppo2.py:185][0m |          -0.0010 |         239.8364 |        -115.6600 |
[32m[20221214 14:30:53 @agent_ppo2.py:185][0m |           0.0023 |         241.0209 |        -115.7122 |
[32m[20221214 14:30:53 @agent_ppo2.py:185][0m |           0.0053 |         241.3607 |        -115.7699 |
[32m[20221214 14:30:53 @agent_ppo2.py:185][0m |          -0.0018 |         231.2217 |        -116.0735 |
[32m[20221214 14:30:53 @agent_ppo2.py:185][0m |          -0.0027 |         229.3747 |        -115.8136 |
[32m[20221214 14:30:53 @agent_ppo2.py:185][0m |          -0.0038 |         228.8981 |        -115.7321 |
[32m[20221214 14:30:53 @agent_ppo2.py:185][0m |          -0.0064 |         228.6439 |        -115.5076 |
[32m[20221214 14:30:54 @agent_ppo2.py:185][0m |          -0.0018 |         227.4069 |        -115.4857 |
[32m[20221214 14:30:54 @agent_ppo2.py:185][0m |          -0.0041 |         227.2916 |        -115.7643 |
[32m[20221214 14:30:54 @agent_ppo2.py:185][0m |          -0.0019 |         226.3076 |        -116.3885 |
[32m[20221214 14:30:54 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:30:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.14
[32m[20221214 14:30:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.01
[32m[20221214 14:30:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.89
[32m[20221214 14:30:54 @agent_ppo2.py:143][0m Total time:      32.86 min
[32m[20221214 14:30:54 @agent_ppo2.py:145][0m 3022848 total steps have happened
[32m[20221214 14:30:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1476 --------------------------#
[32m[20221214 14:30:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:54 @agent_ppo2.py:185][0m |          -0.0016 |         235.8234 |        -115.8087 |
[32m[20221214 14:30:54 @agent_ppo2.py:185][0m |           0.0026 |         233.6693 |        -115.5357 |
[32m[20221214 14:30:54 @agent_ppo2.py:185][0m |          -0.0038 |         226.7238 |        -115.5404 |
[32m[20221214 14:30:55 @agent_ppo2.py:185][0m |          -0.0012 |         224.8265 |        -115.4531 |
[32m[20221214 14:30:55 @agent_ppo2.py:185][0m |          -0.0008 |         224.5181 |        -115.6565 |
[32m[20221214 14:30:55 @agent_ppo2.py:185][0m |          -0.0024 |         223.5953 |        -115.5714 |
[32m[20221214 14:30:55 @agent_ppo2.py:185][0m |           0.0047 |         228.4067 |        -115.1737 |
[32m[20221214 14:30:55 @agent_ppo2.py:185][0m |          -0.0029 |         222.6942 |        -115.1993 |
[32m[20221214 14:30:55 @agent_ppo2.py:185][0m |          -0.0038 |         222.6361 |        -115.3575 |
[32m[20221214 14:30:55 @agent_ppo2.py:185][0m |          -0.0016 |         221.9313 |        -115.0459 |
[32m[20221214 14:30:55 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:30:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.50
[32m[20221214 14:30:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.92
[32m[20221214 14:30:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.40
[32m[20221214 14:30:55 @agent_ppo2.py:143][0m Total time:      32.89 min
[32m[20221214 14:30:55 @agent_ppo2.py:145][0m 3024896 total steps have happened
[32m[20221214 14:30:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1477 --------------------------#
[32m[20221214 14:30:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:30:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:56 @agent_ppo2.py:185][0m |           0.0006 |         216.7383 |        -115.6634 |
[32m[20221214 14:30:56 @agent_ppo2.py:185][0m |          -0.0011 |         213.1891 |        -115.7471 |
[32m[20221214 14:30:56 @agent_ppo2.py:185][0m |          -0.0008 |         212.1163 |        -115.4927 |
[32m[20221214 14:30:56 @agent_ppo2.py:185][0m |          -0.0009 |         210.5573 |        -115.5808 |
[32m[20221214 14:30:56 @agent_ppo2.py:185][0m |          -0.0003 |         209.0825 |        -115.6434 |
[32m[20221214 14:30:56 @agent_ppo2.py:185][0m |           0.0003 |         208.8202 |        -115.5416 |
[32m[20221214 14:30:56 @agent_ppo2.py:185][0m |          -0.0009 |         209.6464 |        -115.6207 |
[32m[20221214 14:30:56 @agent_ppo2.py:185][0m |          -0.0017 |         208.1860 |        -115.4620 |
[32m[20221214 14:30:56 @agent_ppo2.py:185][0m |           0.0004 |         207.6909 |        -115.2444 |
[32m[20221214 14:30:56 @agent_ppo2.py:185][0m |           0.0099 |         222.2899 |        -115.3723 |
[32m[20221214 14:30:56 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:30:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.98
[32m[20221214 14:30:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.80
[32m[20221214 14:30:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.56
[32m[20221214 14:30:57 @agent_ppo2.py:143][0m Total time:      32.91 min
[32m[20221214 14:30:57 @agent_ppo2.py:145][0m 3026944 total steps have happened
[32m[20221214 14:30:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1478 --------------------------#
[32m[20221214 14:30:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:30:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:57 @agent_ppo2.py:185][0m |           0.0026 |         226.9166 |        -112.9468 |
[32m[20221214 14:30:57 @agent_ppo2.py:185][0m |          -0.0038 |         212.7834 |        -113.1710 |
[32m[20221214 14:30:57 @agent_ppo2.py:185][0m |          -0.0014 |         210.2104 |        -113.3674 |
[32m[20221214 14:30:57 @agent_ppo2.py:185][0m |           0.0015 |         208.6137 |        -113.5652 |
[32m[20221214 14:30:57 @agent_ppo2.py:185][0m |          -0.0007 |         207.0525 |        -113.4454 |
[32m[20221214 14:30:57 @agent_ppo2.py:185][0m |           0.0033 |         208.8192 |        -113.8061 |
[32m[20221214 14:30:57 @agent_ppo2.py:185][0m |           0.0070 |         217.6077 |        -113.5977 |
[32m[20221214 14:30:57 @agent_ppo2.py:185][0m |          -0.0025 |         204.9623 |        -114.1638 |
[32m[20221214 14:30:58 @agent_ppo2.py:185][0m |          -0.0009 |         203.8619 |        -114.0319 |
[32m[20221214 14:30:58 @agent_ppo2.py:185][0m |          -0.0031 |         203.2147 |        -114.1707 |
[32m[20221214 14:30:58 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:30:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.71
[32m[20221214 14:30:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.16
[32m[20221214 14:30:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.12
[32m[20221214 14:30:58 @agent_ppo2.py:143][0m Total time:      32.93 min
[32m[20221214 14:30:58 @agent_ppo2.py:145][0m 3028992 total steps have happened
[32m[20221214 14:30:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1479 --------------------------#
[32m[20221214 14:30:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:30:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:58 @agent_ppo2.py:185][0m |           0.0006 |         251.0841 |        -116.0081 |
[32m[20221214 14:30:58 @agent_ppo2.py:185][0m |          -0.0046 |         240.1938 |        -116.1737 |
[32m[20221214 14:30:58 @agent_ppo2.py:185][0m |          -0.0026 |         236.3194 |        -116.2764 |
[32m[20221214 14:30:58 @agent_ppo2.py:185][0m |           0.0012 |         234.2145 |        -116.2377 |
[32m[20221214 14:30:58 @agent_ppo2.py:185][0m |          -0.0019 |         232.6995 |        -116.5390 |
[32m[20221214 14:30:59 @agent_ppo2.py:185][0m |          -0.0024 |         230.8078 |        -116.6272 |
[32m[20221214 14:30:59 @agent_ppo2.py:185][0m |          -0.0028 |         228.8879 |        -116.6989 |
[32m[20221214 14:30:59 @agent_ppo2.py:185][0m |          -0.0046 |         229.1273 |        -116.9287 |
[32m[20221214 14:30:59 @agent_ppo2.py:185][0m |          -0.0049 |         228.4478 |        -116.9622 |
[32m[20221214 14:30:59 @agent_ppo2.py:185][0m |           0.0030 |         235.7548 |        -117.0381 |
[32m[20221214 14:30:59 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:30:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.34
[32m[20221214 14:30:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.80
[32m[20221214 14:30:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.06
[32m[20221214 14:30:59 @agent_ppo2.py:143][0m Total time:      32.95 min
[32m[20221214 14:30:59 @agent_ppo2.py:145][0m 3031040 total steps have happened
[32m[20221214 14:30:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1480 --------------------------#
[32m[20221214 14:30:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:30:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:30:59 @agent_ppo2.py:185][0m |           0.0068 |         209.0272 |        -118.6105 |
[32m[20221214 14:30:59 @agent_ppo2.py:185][0m |           0.0012 |         172.6495 |        -118.4291 |
[32m[20221214 14:31:00 @agent_ppo2.py:185][0m |          -0.0038 |         167.4996 |        -118.6640 |
[32m[20221214 14:31:00 @agent_ppo2.py:185][0m |          -0.0026 |         164.1563 |        -118.7983 |
[32m[20221214 14:31:00 @agent_ppo2.py:185][0m |           0.0017 |         162.0182 |        -118.6224 |
[32m[20221214 14:31:00 @agent_ppo2.py:185][0m |           0.0013 |         160.0902 |        -118.5508 |
[32m[20221214 14:31:00 @agent_ppo2.py:185][0m |           0.0047 |         158.9434 |        -118.5706 |
[32m[20221214 14:31:00 @agent_ppo2.py:185][0m |          -0.0023 |         157.1702 |        -118.1961 |
[32m[20221214 14:31:00 @agent_ppo2.py:185][0m |          -0.0021 |         156.0677 |        -118.7700 |
[32m[20221214 14:31:00 @agent_ppo2.py:185][0m |          -0.0026 |         154.9711 |        -118.7503 |
[32m[20221214 14:31:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:31:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.76
[32m[20221214 14:31:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.57
[32m[20221214 14:31:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.59
[32m[20221214 14:31:00 @agent_ppo2.py:143][0m Total time:      32.97 min
[32m[20221214 14:31:00 @agent_ppo2.py:145][0m 3033088 total steps have happened
[32m[20221214 14:31:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1481 --------------------------#
[32m[20221214 14:31:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:31:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:01 @agent_ppo2.py:185][0m |          -0.0026 |         231.6556 |        -120.0063 |
[32m[20221214 14:31:01 @agent_ppo2.py:185][0m |          -0.0004 |         204.6237 |        -119.9026 |
[32m[20221214 14:31:01 @agent_ppo2.py:185][0m |          -0.0047 |         195.7147 |        -120.0723 |
[32m[20221214 14:31:01 @agent_ppo2.py:185][0m |           0.0005 |         189.3896 |        -119.8106 |
[32m[20221214 14:31:01 @agent_ppo2.py:185][0m |          -0.0041 |         186.6907 |        -119.5633 |
[32m[20221214 14:31:01 @agent_ppo2.py:185][0m |          -0.0034 |         184.3480 |        -119.7464 |
[32m[20221214 14:31:01 @agent_ppo2.py:185][0m |          -0.0028 |         181.8771 |        -119.8308 |
[32m[20221214 14:31:01 @agent_ppo2.py:185][0m |          -0.0021 |         180.8012 |        -119.8490 |
[32m[20221214 14:31:01 @agent_ppo2.py:185][0m |          -0.0033 |         179.1148 |        -119.7816 |
[32m[20221214 14:31:02 @agent_ppo2.py:185][0m |          -0.0027 |         178.6965 |        -119.6997 |
[32m[20221214 14:31:02 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:31:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.20
[32m[20221214 14:31:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.45
[32m[20221214 14:31:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.15
[32m[20221214 14:31:02 @agent_ppo2.py:143][0m Total time:      32.99 min
[32m[20221214 14:31:02 @agent_ppo2.py:145][0m 3035136 total steps have happened
[32m[20221214 14:31:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1482 --------------------------#
[32m[20221214 14:31:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:02 @agent_ppo2.py:185][0m |           0.0020 |         169.2652 |        -117.6294 |
[32m[20221214 14:31:02 @agent_ppo2.py:185][0m |           0.0007 |         150.4921 |        -117.8414 |
[32m[20221214 14:31:02 @agent_ppo2.py:185][0m |           0.0053 |         146.6433 |        -117.9261 |
[32m[20221214 14:31:02 @agent_ppo2.py:185][0m |          -0.0009 |         141.1542 |        -118.2128 |
[32m[20221214 14:31:02 @agent_ppo2.py:185][0m |          -0.0039 |         140.7269 |        -117.9875 |
[32m[20221214 14:31:03 @agent_ppo2.py:185][0m |           0.0170 |         178.6090 |        -118.1648 |
[32m[20221214 14:31:03 @agent_ppo2.py:185][0m |          -0.0037 |         137.0006 |        -118.3511 |
[32m[20221214 14:31:03 @agent_ppo2.py:185][0m |           0.0018 |         135.3528 |        -118.0123 |
[32m[20221214 14:31:03 @agent_ppo2.py:185][0m |           0.0092 |         133.8892 |        -118.2369 |
[32m[20221214 14:31:03 @agent_ppo2.py:185][0m |          -0.0015 |         132.4821 |        -118.2326 |
[32m[20221214 14:31:03 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.09
[32m[20221214 14:31:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.12
[32m[20221214 14:31:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.10
[32m[20221214 14:31:03 @agent_ppo2.py:143][0m Total time:      33.02 min
[32m[20221214 14:31:03 @agent_ppo2.py:145][0m 3037184 total steps have happened
[32m[20221214 14:31:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1483 --------------------------#
[32m[20221214 14:31:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:03 @agent_ppo2.py:185][0m |          -0.0004 |          68.9103 |        -119.9988 |
[32m[20221214 14:31:03 @agent_ppo2.py:185][0m |           0.0048 |          51.4121 |        -120.1802 |
[32m[20221214 14:31:04 @agent_ppo2.py:185][0m |          -0.0019 |          45.6280 |        -119.9749 |
[32m[20221214 14:31:04 @agent_ppo2.py:185][0m |          -0.0019 |          42.7595 |        -120.1286 |
[32m[20221214 14:31:04 @agent_ppo2.py:185][0m |           0.0060 |          41.5291 |        -119.8185 |
[32m[20221214 14:31:04 @agent_ppo2.py:185][0m |          -0.0009 |          40.4392 |        -119.9345 |
[32m[20221214 14:31:04 @agent_ppo2.py:185][0m |           0.0000 |          39.3842 |        -120.0414 |
[32m[20221214 14:31:04 @agent_ppo2.py:185][0m |           0.0010 |          38.7239 |        -119.9877 |
[32m[20221214 14:31:04 @agent_ppo2.py:185][0m |          -0.0033 |          38.2019 |        -119.9498 |
[32m[20221214 14:31:04 @agent_ppo2.py:185][0m |           0.0009 |          37.5756 |        -119.7920 |
[32m[20221214 14:31:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:31:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 764.79
[32m[20221214 14:31:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.66
[32m[20221214 14:31:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.28
[32m[20221214 14:31:04 @agent_ppo2.py:143][0m Total time:      33.04 min
[32m[20221214 14:31:04 @agent_ppo2.py:145][0m 3039232 total steps have happened
[32m[20221214 14:31:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1484 --------------------------#
[32m[20221214 14:31:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:05 @agent_ppo2.py:185][0m |           0.0070 |         119.6795 |        -119.0496 |
[32m[20221214 14:31:05 @agent_ppo2.py:185][0m |           0.0056 |          97.9943 |        -118.6330 |
[32m[20221214 14:31:05 @agent_ppo2.py:185][0m |          -0.0035 |          92.1935 |        -119.0236 |
[32m[20221214 14:31:05 @agent_ppo2.py:185][0m |           0.0090 |          93.2297 |        -119.1158 |
[32m[20221214 14:31:05 @agent_ppo2.py:185][0m |           0.0034 |          86.8680 |        -118.7703 |
[32m[20221214 14:31:05 @agent_ppo2.py:185][0m |          -0.0034 |          84.7870 |        -119.0795 |
[32m[20221214 14:31:05 @agent_ppo2.py:185][0m |          -0.0048 |          83.9071 |        -118.9962 |
[32m[20221214 14:31:05 @agent_ppo2.py:185][0m |          -0.0031 |          83.3355 |        -118.9302 |
[32m[20221214 14:31:06 @agent_ppo2.py:185][0m |           0.0003 |          82.5245 |        -118.9301 |
[32m[20221214 14:31:06 @agent_ppo2.py:185][0m |           0.0100 |          90.4352 |        -118.8016 |
[32m[20221214 14:31:06 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 799.90
[32m[20221214 14:31:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.84
[32m[20221214 14:31:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.27
[32m[20221214 14:31:06 @agent_ppo2.py:143][0m Total time:      33.06 min
[32m[20221214 14:31:06 @agent_ppo2.py:145][0m 3041280 total steps have happened
[32m[20221214 14:31:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1485 --------------------------#
[32m[20221214 14:31:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:06 @agent_ppo2.py:185][0m |           0.0029 |         104.9979 |        -120.9826 |
[32m[20221214 14:31:06 @agent_ppo2.py:185][0m |           0.0017 |          80.3317 |        -121.3153 |
[32m[20221214 14:31:06 @agent_ppo2.py:185][0m |           0.0085 |          74.2338 |        -121.2956 |
[32m[20221214 14:31:06 @agent_ppo2.py:185][0m |          -0.0041 |          69.3540 |        -121.2199 |
[32m[20221214 14:31:07 @agent_ppo2.py:185][0m |          -0.0012 |          67.0215 |        -121.3451 |
[32m[20221214 14:31:07 @agent_ppo2.py:185][0m |           0.0035 |          65.6357 |        -121.2760 |
[32m[20221214 14:31:07 @agent_ppo2.py:185][0m |           0.0010 |          64.7889 |        -121.1666 |
[32m[20221214 14:31:07 @agent_ppo2.py:185][0m |          -0.0013 |          63.9999 |        -121.2005 |
[32m[20221214 14:31:07 @agent_ppo2.py:185][0m |           0.0124 |          72.9164 |        -121.1609 |
[32m[20221214 14:31:07 @agent_ppo2.py:185][0m |          -0.0020 |          61.7298 |        -121.2359 |
[32m[20221214 14:31:07 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.16
[32m[20221214 14:31:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.87
[32m[20221214 14:31:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.40
[32m[20221214 14:31:07 @agent_ppo2.py:143][0m Total time:      33.08 min
[32m[20221214 14:31:07 @agent_ppo2.py:145][0m 3043328 total steps have happened
[32m[20221214 14:31:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1486 --------------------------#
[32m[20221214 14:31:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:07 @agent_ppo2.py:185][0m |           0.0004 |         210.2773 |        -120.7115 |
[32m[20221214 14:31:08 @agent_ppo2.py:185][0m |          -0.0004 |         196.4187 |        -120.4430 |
[32m[20221214 14:31:08 @agent_ppo2.py:185][0m |           0.0011 |         192.8354 |        -120.5822 |
[32m[20221214 14:31:08 @agent_ppo2.py:185][0m |          -0.0027 |         190.0673 |        -120.6782 |
[32m[20221214 14:31:08 @agent_ppo2.py:185][0m |           0.0082 |         195.3440 |        -120.6564 |
[32m[20221214 14:31:08 @agent_ppo2.py:185][0m |           0.0000 |         188.9483 |        -120.9083 |
[32m[20221214 14:31:08 @agent_ppo2.py:185][0m |           0.0010 |         187.0026 |        -121.0183 |
[32m[20221214 14:31:08 @agent_ppo2.py:185][0m |          -0.0007 |         185.8703 |        -120.9746 |
[32m[20221214 14:31:08 @agent_ppo2.py:185][0m |          -0.0001 |         185.3346 |        -120.9433 |
[32m[20221214 14:31:08 @agent_ppo2.py:185][0m |           0.0036 |         186.9162 |        -120.9089 |
[32m[20221214 14:31:08 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:31:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.04
[32m[20221214 14:31:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.41
[32m[20221214 14:31:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.38
[32m[20221214 14:31:08 @agent_ppo2.py:143][0m Total time:      33.11 min
[32m[20221214 14:31:08 @agent_ppo2.py:145][0m 3045376 total steps have happened
[32m[20221214 14:31:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1487 --------------------------#
[32m[20221214 14:31:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:09 @agent_ppo2.py:185][0m |          -0.0002 |         218.5896 |        -118.8846 |
[32m[20221214 14:31:09 @agent_ppo2.py:185][0m |           0.0008 |         199.5735 |        -119.0573 |
[32m[20221214 14:31:09 @agent_ppo2.py:185][0m |          -0.0017 |         192.2208 |        -119.1596 |
[32m[20221214 14:31:09 @agent_ppo2.py:185][0m |           0.0005 |         187.9453 |        -119.0739 |
[32m[20221214 14:31:09 @agent_ppo2.py:185][0m |           0.0025 |         186.9213 |        -118.9738 |
[32m[20221214 14:31:09 @agent_ppo2.py:185][0m |          -0.0019 |         183.6299 |        -119.4990 |
[32m[20221214 14:31:09 @agent_ppo2.py:185][0m |          -0.0040 |         181.9770 |        -119.4351 |
[32m[20221214 14:31:09 @agent_ppo2.py:185][0m |          -0.0028 |         182.0976 |        -119.5920 |
[32m[20221214 14:31:10 @agent_ppo2.py:185][0m |          -0.0000 |         180.3331 |        -119.4555 |
[32m[20221214 14:31:10 @agent_ppo2.py:185][0m |          -0.0044 |         180.8574 |        -119.7692 |
[32m[20221214 14:31:10 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:31:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.82
[32m[20221214 14:31:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.65
[32m[20221214 14:31:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.84
[32m[20221214 14:31:10 @agent_ppo2.py:143][0m Total time:      33.13 min
[32m[20221214 14:31:10 @agent_ppo2.py:145][0m 3047424 total steps have happened
[32m[20221214 14:31:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1488 --------------------------#
[32m[20221214 14:31:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:10 @agent_ppo2.py:185][0m |          -0.0020 |         204.5388 |        -121.0098 |
[32m[20221214 14:31:10 @agent_ppo2.py:185][0m |          -0.0015 |         188.6461 |        -120.8307 |
[32m[20221214 14:31:10 @agent_ppo2.py:185][0m |           0.0004 |         184.8481 |        -121.1703 |
[32m[20221214 14:31:10 @agent_ppo2.py:185][0m |           0.0017 |         181.3387 |        -120.7899 |
[32m[20221214 14:31:11 @agent_ppo2.py:185][0m |          -0.0033 |         179.0061 |        -121.0339 |
[32m[20221214 14:31:11 @agent_ppo2.py:185][0m |          -0.0041 |         176.6082 |        -121.3502 |
[32m[20221214 14:31:11 @agent_ppo2.py:185][0m |           0.0125 |         180.2427 |        -120.9988 |
[32m[20221214 14:31:11 @agent_ppo2.py:185][0m |          -0.0051 |         173.0545 |        -121.3665 |
[32m[20221214 14:31:11 @agent_ppo2.py:185][0m |           0.0163 |         197.1590 |        -121.1361 |
[32m[20221214 14:31:11 @agent_ppo2.py:185][0m |          -0.0043 |         173.1460 |        -121.0673 |
[32m[20221214 14:31:11 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.96
[32m[20221214 14:31:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.56
[32m[20221214 14:31:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.79
[32m[20221214 14:31:11 @agent_ppo2.py:143][0m Total time:      33.15 min
[32m[20221214 14:31:11 @agent_ppo2.py:145][0m 3049472 total steps have happened
[32m[20221214 14:31:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1489 --------------------------#
[32m[20221214 14:31:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:12 @agent_ppo2.py:185][0m |           0.0039 |         193.1028 |        -121.6040 |
[32m[20221214 14:31:12 @agent_ppo2.py:185][0m |          -0.0007 |         169.9116 |        -121.7471 |
[32m[20221214 14:31:12 @agent_ppo2.py:185][0m |          -0.0008 |         163.1825 |        -121.5222 |
[32m[20221214 14:31:12 @agent_ppo2.py:185][0m |          -0.0036 |         159.4465 |        -121.7816 |
[32m[20221214 14:31:12 @agent_ppo2.py:185][0m |          -0.0045 |         156.3163 |        -121.7794 |
[32m[20221214 14:31:12 @agent_ppo2.py:185][0m |          -0.0021 |         155.0287 |        -121.6460 |
[32m[20221214 14:31:12 @agent_ppo2.py:185][0m |          -0.0001 |         153.9791 |        -121.6062 |
[32m[20221214 14:31:12 @agent_ppo2.py:185][0m |           0.0006 |         152.2163 |        -121.5588 |
[32m[20221214 14:31:12 @agent_ppo2.py:185][0m |          -0.0028 |         151.4097 |        -121.5839 |
[32m[20221214 14:31:12 @agent_ppo2.py:185][0m |           0.0066 |         154.8774 |        -121.5893 |
[32m[20221214 14:31:12 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.90
[32m[20221214 14:31:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.94
[32m[20221214 14:31:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.81
[32m[20221214 14:31:13 @agent_ppo2.py:143][0m Total time:      33.17 min
[32m[20221214 14:31:13 @agent_ppo2.py:145][0m 3051520 total steps have happened
[32m[20221214 14:31:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1490 --------------------------#
[32m[20221214 14:31:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:31:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:13 @agent_ppo2.py:185][0m |          -0.0002 |         162.7979 |        -122.3954 |
[32m[20221214 14:31:13 @agent_ppo2.py:185][0m |           0.0000 |         139.2669 |        -122.4818 |
[32m[20221214 14:31:13 @agent_ppo2.py:185][0m |          -0.0044 |         132.3543 |        -122.1859 |
[32m[20221214 14:31:13 @agent_ppo2.py:185][0m |          -0.0024 |         128.2980 |        -122.6116 |
[32m[20221214 14:31:13 @agent_ppo2.py:185][0m |           0.0080 |         136.9881 |        -122.5000 |
[32m[20221214 14:31:13 @agent_ppo2.py:185][0m |          -0.0018 |         123.6806 |        -122.7361 |
[32m[20221214 14:31:13 @agent_ppo2.py:185][0m |           0.0023 |         121.1313 |        -122.6855 |
[32m[20221214 14:31:14 @agent_ppo2.py:185][0m |           0.0011 |         119.7957 |        -122.7417 |
[32m[20221214 14:31:14 @agent_ppo2.py:185][0m |          -0.0038 |         117.6008 |        -122.6631 |
[32m[20221214 14:31:14 @agent_ppo2.py:185][0m |          -0.0034 |         116.1567 |        -122.6467 |
[32m[20221214 14:31:14 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.98
[32m[20221214 14:31:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.37
[32m[20221214 14:31:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.40
[32m[20221214 14:31:14 @agent_ppo2.py:143][0m Total time:      33.20 min
[32m[20221214 14:31:14 @agent_ppo2.py:145][0m 3053568 total steps have happened
[32m[20221214 14:31:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1491 --------------------------#
[32m[20221214 14:31:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:14 @agent_ppo2.py:185][0m |           0.0042 |         182.3662 |        -121.3415 |
[32m[20221214 14:31:14 @agent_ppo2.py:185][0m |           0.0014 |         155.3884 |        -121.0600 |
[32m[20221214 14:31:14 @agent_ppo2.py:185][0m |          -0.0007 |         145.7035 |        -121.2310 |
[32m[20221214 14:31:15 @agent_ppo2.py:185][0m |           0.0028 |         140.6957 |        -121.1111 |
[32m[20221214 14:31:15 @agent_ppo2.py:185][0m |          -0.0013 |         135.6205 |        -121.4712 |
[32m[20221214 14:31:15 @agent_ppo2.py:185][0m |           0.0033 |         132.0072 |        -121.0893 |
[32m[20221214 14:31:15 @agent_ppo2.py:185][0m |          -0.0053 |         128.1454 |        -121.0704 |
[32m[20221214 14:31:15 @agent_ppo2.py:185][0m |          -0.0008 |         127.0189 |        -120.9323 |
[32m[20221214 14:31:15 @agent_ppo2.py:185][0m |           0.0018 |         123.6411 |        -120.6911 |
[32m[20221214 14:31:15 @agent_ppo2.py:185][0m |           0.0018 |         122.2707 |        -121.0869 |
[32m[20221214 14:31:15 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:31:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 770.95
[32m[20221214 14:31:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.41
[32m[20221214 14:31:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.86
[32m[20221214 14:31:15 @agent_ppo2.py:143][0m Total time:      33.22 min
[32m[20221214 14:31:15 @agent_ppo2.py:145][0m 3055616 total steps have happened
[32m[20221214 14:31:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1492 --------------------------#
[32m[20221214 14:31:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:16 @agent_ppo2.py:185][0m |           0.0099 |         113.7200 |        -122.2445 |
[32m[20221214 14:31:16 @agent_ppo2.py:185][0m |          -0.0055 |          85.4427 |        -121.3876 |
[32m[20221214 14:31:16 @agent_ppo2.py:185][0m |          -0.0058 |          80.1360 |        -121.8030 |
[32m[20221214 14:31:16 @agent_ppo2.py:185][0m |          -0.0052 |          77.0468 |        -121.4689 |
[32m[20221214 14:31:16 @agent_ppo2.py:185][0m |          -0.0082 |          74.8262 |        -121.3224 |
[32m[20221214 14:31:16 @agent_ppo2.py:185][0m |          -0.0077 |          73.9195 |        -121.5370 |
[32m[20221214 14:31:16 @agent_ppo2.py:185][0m |          -0.0044 |          72.1624 |        -121.4391 |
[32m[20221214 14:31:16 @agent_ppo2.py:185][0m |          -0.0035 |          71.4160 |        -121.1252 |
[32m[20221214 14:31:16 @agent_ppo2.py:185][0m |          -0.0011 |          70.4929 |        -121.2704 |
[32m[20221214 14:31:16 @agent_ppo2.py:185][0m |          -0.0050 |          69.6258 |        -120.8245 |
[32m[20221214 14:31:16 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 633.69
[32m[20221214 14:31:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.16
[32m[20221214 14:31:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.42
[32m[20221214 14:31:17 @agent_ppo2.py:143][0m Total time:      33.24 min
[32m[20221214 14:31:17 @agent_ppo2.py:145][0m 3057664 total steps have happened
[32m[20221214 14:31:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1493 --------------------------#
[32m[20221214 14:31:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:17 @agent_ppo2.py:185][0m |           0.0007 |         103.4878 |        -124.0031 |
[32m[20221214 14:31:17 @agent_ppo2.py:185][0m |           0.0061 |          88.5953 |        -123.2878 |
[32m[20221214 14:31:17 @agent_ppo2.py:185][0m |           0.0002 |          82.6622 |        -123.0506 |
[32m[20221214 14:31:17 @agent_ppo2.py:185][0m |           0.0058 |          81.8167 |        -123.5223 |
[32m[20221214 14:31:17 @agent_ppo2.py:185][0m |          -0.0037 |          77.2771 |        -123.5766 |
[32m[20221214 14:31:17 @agent_ppo2.py:185][0m |           0.0019 |          75.8214 |        -123.5801 |
[32m[20221214 14:31:18 @agent_ppo2.py:185][0m |           0.0012 |          74.4512 |        -123.4069 |
[32m[20221214 14:31:18 @agent_ppo2.py:185][0m |          -0.0035 |          73.1356 |        -123.4500 |
[32m[20221214 14:31:18 @agent_ppo2.py:185][0m |          -0.0027 |          72.4053 |        -123.1854 |
[32m[20221214 14:31:18 @agent_ppo2.py:185][0m |          -0.0028 |          71.6874 |        -123.2699 |
[32m[20221214 14:31:18 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:31:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 634.32
[32m[20221214 14:31:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.64
[32m[20221214 14:31:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.27
[32m[20221214 14:31:18 @agent_ppo2.py:143][0m Total time:      33.27 min
[32m[20221214 14:31:18 @agent_ppo2.py:145][0m 3059712 total steps have happened
[32m[20221214 14:31:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1494 --------------------------#
[32m[20221214 14:31:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:18 @agent_ppo2.py:185][0m |          -0.0065 |          30.6273 |        -120.5295 |
[32m[20221214 14:31:18 @agent_ppo2.py:185][0m |          -0.0077 |          18.5645 |        -120.3362 |
[32m[20221214 14:31:19 @agent_ppo2.py:185][0m |          -0.0018 |          16.2059 |        -120.2345 |
[32m[20221214 14:31:19 @agent_ppo2.py:185][0m |           0.0023 |          14.9926 |        -120.4032 |
[32m[20221214 14:31:19 @agent_ppo2.py:185][0m |           0.0083 |          14.1238 |        -120.2856 |
[32m[20221214 14:31:19 @agent_ppo2.py:185][0m |           0.0046 |          13.5090 |        -120.3285 |
[32m[20221214 14:31:19 @agent_ppo2.py:185][0m |          -0.0024 |          13.0056 |        -120.2211 |
[32m[20221214 14:31:19 @agent_ppo2.py:185][0m |          -0.0030 |          12.8040 |        -120.2418 |
[32m[20221214 14:31:19 @agent_ppo2.py:185][0m |          -0.0039 |          12.3513 |        -120.2960 |
[32m[20221214 14:31:19 @agent_ppo2.py:185][0m |          -0.0016 |          12.3269 |        -120.1892 |
[32m[20221214 14:31:19 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.06
[32m[20221214 14:31:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.28
[32m[20221214 14:31:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 533.20
[32m[20221214 14:31:19 @agent_ppo2.py:143][0m Total time:      33.29 min
[32m[20221214 14:31:19 @agent_ppo2.py:145][0m 3061760 total steps have happened
[32m[20221214 14:31:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1495 --------------------------#
[32m[20221214 14:31:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:20 @agent_ppo2.py:185][0m |          -0.0029 |         103.7873 |        -121.6275 |
[32m[20221214 14:31:20 @agent_ppo2.py:185][0m |           0.0176 |         116.2499 |        -121.5623 |
[32m[20221214 14:31:20 @agent_ppo2.py:185][0m |          -0.0002 |          82.6280 |        -121.4753 |
[32m[20221214 14:31:20 @agent_ppo2.py:185][0m |          -0.0018 |          79.4599 |        -121.3342 |
[32m[20221214 14:31:20 @agent_ppo2.py:185][0m |          -0.0010 |          77.9211 |        -121.5616 |
[32m[20221214 14:31:20 @agent_ppo2.py:185][0m |          -0.0019 |          76.6895 |        -121.5987 |
[32m[20221214 14:31:20 @agent_ppo2.py:185][0m |           0.0087 |          79.2036 |        -121.8332 |
[32m[20221214 14:31:20 @agent_ppo2.py:185][0m |          -0.0007 |          74.7601 |        -121.9448 |
[32m[20221214 14:31:20 @agent_ppo2.py:185][0m |          -0.0064 |          73.7592 |        -121.9422 |
[32m[20221214 14:31:21 @agent_ppo2.py:185][0m |          -0.0061 |          72.2433 |        -121.9158 |
[32m[20221214 14:31:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 633.97
[32m[20221214 14:31:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.82
[32m[20221214 14:31:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 568.42
[32m[20221214 14:31:21 @agent_ppo2.py:143][0m Total time:      33.31 min
[32m[20221214 14:31:21 @agent_ppo2.py:145][0m 3063808 total steps have happened
[32m[20221214 14:31:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1496 --------------------------#
[32m[20221214 14:31:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:21 @agent_ppo2.py:185][0m |          -0.0029 |          25.5643 |        -123.4783 |
[32m[20221214 14:31:21 @agent_ppo2.py:185][0m |          -0.0013 |          21.3584 |        -123.8696 |
[32m[20221214 14:31:21 @agent_ppo2.py:185][0m |           0.0084 |          20.8674 |        -123.8697 |
[32m[20221214 14:31:21 @agent_ppo2.py:185][0m |           0.0023 |          19.8506 |        -123.3546 |
[32m[20221214 14:31:21 @agent_ppo2.py:185][0m |          -0.0036 |          18.9866 |        -124.0471 |
[32m[20221214 14:31:22 @agent_ppo2.py:185][0m |          -0.0093 |          18.8250 |        -123.7835 |
[32m[20221214 14:31:22 @agent_ppo2.py:185][0m |           0.0014 |          18.7173 |        -123.9241 |
[32m[20221214 14:31:22 @agent_ppo2.py:185][0m |          -0.0082 |          18.1494 |        -123.8594 |
[32m[20221214 14:31:22 @agent_ppo2.py:185][0m |          -0.0048 |          17.8661 |        -123.9414 |
[32m[20221214 14:31:22 @agent_ppo2.py:185][0m |          -0.0055 |          18.2832 |        -124.0926 |
[32m[20221214 14:31:22 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:31:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.55
[32m[20221214 14:31:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.05
[32m[20221214 14:31:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.83
[32m[20221214 14:31:22 @agent_ppo2.py:143][0m Total time:      33.33 min
[32m[20221214 14:31:22 @agent_ppo2.py:145][0m 3065856 total steps have happened
[32m[20221214 14:31:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1497 --------------------------#
[32m[20221214 14:31:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:22 @agent_ppo2.py:185][0m |          -0.0009 |         167.6595 |        -124.2641 |
[32m[20221214 14:31:23 @agent_ppo2.py:185][0m |          -0.0000 |         144.3593 |        -124.3799 |
[32m[20221214 14:31:23 @agent_ppo2.py:185][0m |          -0.0028 |         136.6412 |        -124.0763 |
[32m[20221214 14:31:23 @agent_ppo2.py:185][0m |          -0.0040 |         132.2519 |        -124.1892 |
[32m[20221214 14:31:23 @agent_ppo2.py:185][0m |          -0.0015 |         128.8001 |        -124.0988 |
[32m[20221214 14:31:23 @agent_ppo2.py:185][0m |          -0.0033 |         126.9400 |        -124.0769 |
[32m[20221214 14:31:23 @agent_ppo2.py:185][0m |          -0.0013 |         123.9793 |        -123.9675 |
[32m[20221214 14:31:23 @agent_ppo2.py:185][0m |          -0.0028 |         121.6669 |        -123.7472 |
[32m[20221214 14:31:23 @agent_ppo2.py:185][0m |          -0.0048 |         119.5295 |        -123.8128 |
[32m[20221214 14:31:23 @agent_ppo2.py:185][0m |           0.0026 |         119.3629 |        -123.6040 |
[32m[20221214 14:31:23 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.04
[32m[20221214 14:31:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.31
[32m[20221214 14:31:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 582.36
[32m[20221214 14:31:23 @agent_ppo2.py:143][0m Total time:      33.36 min
[32m[20221214 14:31:23 @agent_ppo2.py:145][0m 3067904 total steps have happened
[32m[20221214 14:31:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1498 --------------------------#
[32m[20221214 14:31:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:24 @agent_ppo2.py:185][0m |          -0.0005 |         240.7784 |        -122.2495 |
[32m[20221214 14:31:24 @agent_ppo2.py:185][0m |          -0.0032 |         223.2432 |        -121.8739 |
[32m[20221214 14:31:24 @agent_ppo2.py:185][0m |          -0.0019 |         215.8466 |        -121.8432 |
[32m[20221214 14:31:24 @agent_ppo2.py:185][0m |          -0.0002 |         211.7884 |        -121.9442 |
[32m[20221214 14:31:24 @agent_ppo2.py:185][0m |          -0.0041 |         209.0950 |        -121.6899 |
[32m[20221214 14:31:24 @agent_ppo2.py:185][0m |          -0.0017 |         205.5473 |        -121.6832 |
[32m[20221214 14:31:24 @agent_ppo2.py:185][0m |           0.0233 |         256.9302 |        -121.6668 |
[32m[20221214 14:31:24 @agent_ppo2.py:185][0m |          -0.0031 |         204.8985 |        -121.4914 |
[32m[20221214 14:31:25 @agent_ppo2.py:185][0m |           0.0019 |         201.7034 |        -121.7307 |
[32m[20221214 14:31:25 @agent_ppo2.py:185][0m |          -0.0009 |         200.5626 |        -121.3652 |
[32m[20221214 14:31:25 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:31:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 794.35
[32m[20221214 14:31:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.37
[32m[20221214 14:31:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.79
[32m[20221214 14:31:25 @agent_ppo2.py:143][0m Total time:      33.38 min
[32m[20221214 14:31:25 @agent_ppo2.py:145][0m 3069952 total steps have happened
[32m[20221214 14:31:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1499 --------------------------#
[32m[20221214 14:31:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:31:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:25 @agent_ppo2.py:185][0m |           0.0005 |         209.3900 |        -122.6067 |
[32m[20221214 14:31:25 @agent_ppo2.py:185][0m |           0.0026 |         192.7836 |        -122.1338 |
[32m[20221214 14:31:25 @agent_ppo2.py:185][0m |           0.0058 |         191.2808 |        -121.9696 |
[32m[20221214 14:31:25 @agent_ppo2.py:185][0m |          -0.0037 |         184.4273 |        -122.0102 |
[32m[20221214 14:31:25 @agent_ppo2.py:185][0m |           0.0026 |         182.6480 |        -122.2000 |
[32m[20221214 14:31:25 @agent_ppo2.py:185][0m |           0.0130 |         201.7439 |        -121.9102 |
[32m[20221214 14:31:26 @agent_ppo2.py:185][0m |           0.0025 |         179.8964 |        -122.2122 |
[32m[20221214 14:31:26 @agent_ppo2.py:185][0m |           0.0013 |         178.6703 |        -122.0420 |
[32m[20221214 14:31:26 @agent_ppo2.py:185][0m |          -0.0042 |         176.8703 |        -121.7839 |
[32m[20221214 14:31:26 @agent_ppo2.py:185][0m |          -0.0012 |         176.7462 |        -121.5365 |
[32m[20221214 14:31:26 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:31:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.02
[32m[20221214 14:31:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.18
[32m[20221214 14:31:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.40
[32m[20221214 14:31:26 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 860.44
[32m[20221214 14:31:26 @agent_ppo2.py:143][0m Total time:      33.40 min
[32m[20221214 14:31:26 @agent_ppo2.py:145][0m 3072000 total steps have happened
[32m[20221214 14:31:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1500 --------------------------#
[32m[20221214 14:31:26 @agent_ppo2.py:127][0m Sampling time: 0.27 s by 5 slaves
[32m[20221214 14:31:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:26 @agent_ppo2.py:185][0m |          -0.0010 |         150.5916 |        -121.3727 |
[32m[20221214 14:31:27 @agent_ppo2.py:185][0m |          -0.0034 |         137.6595 |        -121.4205 |
[32m[20221214 14:31:27 @agent_ppo2.py:185][0m |          -0.0054 |         134.4413 |        -121.3819 |
[32m[20221214 14:31:27 @agent_ppo2.py:185][0m |          -0.0016 |         130.7145 |        -121.1885 |
[32m[20221214 14:31:27 @agent_ppo2.py:185][0m |           0.0017 |         129.8173 |        -121.3928 |
[32m[20221214 14:31:27 @agent_ppo2.py:185][0m |          -0.0043 |         127.5426 |        -121.5050 |
[32m[20221214 14:31:27 @agent_ppo2.py:185][0m |          -0.0028 |         126.5040 |        -121.4722 |
[32m[20221214 14:31:27 @agent_ppo2.py:185][0m |          -0.0026 |         125.6756 |        -121.4648 |
[32m[20221214 14:31:27 @agent_ppo2.py:185][0m |          -0.0053 |         125.0870 |        -121.5328 |
[32m[20221214 14:31:27 @agent_ppo2.py:185][0m |          -0.0056 |         123.9306 |        -121.6026 |
[32m[20221214 14:31:27 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 748.74
[32m[20221214 14:31:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.34
[32m[20221214 14:31:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.20
[32m[20221214 14:31:27 @agent_ppo2.py:143][0m Total time:      33.42 min
[32m[20221214 14:31:27 @agent_ppo2.py:145][0m 3074048 total steps have happened
[32m[20221214 14:31:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1501 --------------------------#
[32m[20221214 14:31:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:28 @agent_ppo2.py:185][0m |          -0.0013 |         202.0711 |        -122.3448 |
[32m[20221214 14:31:28 @agent_ppo2.py:185][0m |          -0.0009 |         192.6012 |        -122.4374 |
[32m[20221214 14:31:28 @agent_ppo2.py:185][0m |          -0.0011 |         188.9721 |        -122.3838 |
[32m[20221214 14:31:28 @agent_ppo2.py:185][0m |           0.0134 |         206.8290 |        -122.5056 |
[32m[20221214 14:31:28 @agent_ppo2.py:185][0m |          -0.0045 |         186.1041 |        -122.0968 |
[32m[20221214 14:31:28 @agent_ppo2.py:185][0m |           0.0035 |         188.1068 |        -122.2080 |
[32m[20221214 14:31:28 @agent_ppo2.py:185][0m |           0.0004 |         181.3563 |        -122.1926 |
[32m[20221214 14:31:28 @agent_ppo2.py:185][0m |          -0.0002 |         180.4509 |        -122.0106 |
[32m[20221214 14:31:29 @agent_ppo2.py:185][0m |          -0.0069 |         180.7319 |        -122.3377 |
[32m[20221214 14:31:29 @agent_ppo2.py:185][0m |          -0.0031 |         178.4795 |        -122.2509 |
[32m[20221214 14:31:29 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.09
[32m[20221214 14:31:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.09
[32m[20221214 14:31:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.39
[32m[20221214 14:31:29 @agent_ppo2.py:143][0m Total time:      33.45 min
[32m[20221214 14:31:29 @agent_ppo2.py:145][0m 3076096 total steps have happened
[32m[20221214 14:31:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1502 --------------------------#
[32m[20221214 14:31:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:29 @agent_ppo2.py:185][0m |          -0.0005 |         175.5968 |        -122.0180 |
[32m[20221214 14:31:29 @agent_ppo2.py:185][0m |          -0.0019 |         160.4726 |        -122.0409 |
[32m[20221214 14:31:29 @agent_ppo2.py:185][0m |          -0.0030 |         156.9848 |        -121.6673 |
[32m[20221214 14:31:29 @agent_ppo2.py:185][0m |          -0.0022 |         154.1731 |        -121.9310 |
[32m[20221214 14:31:30 @agent_ppo2.py:185][0m |          -0.0060 |         152.4258 |        -121.5456 |
[32m[20221214 14:31:30 @agent_ppo2.py:185][0m |          -0.0043 |         150.9455 |        -121.4455 |
[32m[20221214 14:31:30 @agent_ppo2.py:185][0m |           0.0033 |         152.1799 |        -121.7022 |
[32m[20221214 14:31:30 @agent_ppo2.py:185][0m |          -0.0026 |         148.6297 |        -121.3500 |
[32m[20221214 14:31:30 @agent_ppo2.py:185][0m |          -0.0032 |         148.3986 |        -121.4184 |
[32m[20221214 14:31:30 @agent_ppo2.py:185][0m |          -0.0025 |         147.3711 |        -121.5328 |
[32m[20221214 14:31:30 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:31:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 755.14
[32m[20221214 14:31:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.19
[32m[20221214 14:31:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.78
[32m[20221214 14:31:30 @agent_ppo2.py:143][0m Total time:      33.47 min
[32m[20221214 14:31:30 @agent_ppo2.py:145][0m 3078144 total steps have happened
[32m[20221214 14:31:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1503 --------------------------#
[32m[20221214 14:31:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:31 @agent_ppo2.py:185][0m |           0.0037 |         101.3721 |        -120.3899 |
[32m[20221214 14:31:31 @agent_ppo2.py:185][0m |           0.0008 |          94.0743 |        -120.3228 |
[32m[20221214 14:31:31 @agent_ppo2.py:185][0m |          -0.0041 |          92.1209 |        -120.0725 |
[32m[20221214 14:31:31 @agent_ppo2.py:185][0m |          -0.0025 |          90.4154 |        -120.1646 |
[32m[20221214 14:31:31 @agent_ppo2.py:185][0m |          -0.0051 |          89.6674 |        -120.1392 |
[32m[20221214 14:31:31 @agent_ppo2.py:185][0m |           0.0046 |          88.9420 |        -119.9285 |
[32m[20221214 14:31:31 @agent_ppo2.py:185][0m |          -0.0038 |          87.3714 |        -119.9508 |
[32m[20221214 14:31:31 @agent_ppo2.py:185][0m |          -0.0090 |          86.9628 |        -119.7529 |
[32m[20221214 14:31:31 @agent_ppo2.py:185][0m |          -0.0005 |          86.0966 |        -119.8298 |
[32m[20221214 14:31:31 @agent_ppo2.py:185][0m |          -0.0049 |          85.5609 |        -119.8083 |
[32m[20221214 14:31:31 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:31:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 649.35
[32m[20221214 14:31:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.80
[32m[20221214 14:31:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.24
[32m[20221214 14:31:32 @agent_ppo2.py:143][0m Total time:      33.49 min
[32m[20221214 14:31:32 @agent_ppo2.py:145][0m 3080192 total steps have happened
[32m[20221214 14:31:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1504 --------------------------#
[32m[20221214 14:31:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:32 @agent_ppo2.py:185][0m |          -0.0018 |         178.1090 |        -119.9352 |
[32m[20221214 14:31:32 @agent_ppo2.py:185][0m |          -0.0033 |         167.8977 |        -119.8951 |
[32m[20221214 14:31:32 @agent_ppo2.py:185][0m |           0.0047 |         172.1244 |        -119.6403 |
[32m[20221214 14:31:32 @agent_ppo2.py:185][0m |          -0.0005 |         163.6254 |        -119.4771 |
[32m[20221214 14:31:32 @agent_ppo2.py:185][0m |          -0.0014 |         163.7594 |        -119.9409 |
[32m[20221214 14:31:32 @agent_ppo2.py:185][0m |          -0.0002 |         161.4631 |        -119.9664 |
[32m[20221214 14:31:32 @agent_ppo2.py:185][0m |          -0.0018 |         160.4299 |        -119.8014 |
[32m[20221214 14:31:33 @agent_ppo2.py:185][0m |          -0.0021 |         159.8644 |        -120.0715 |
[32m[20221214 14:31:33 @agent_ppo2.py:185][0m |          -0.0036 |         158.5944 |        -120.1029 |
[32m[20221214 14:31:33 @agent_ppo2.py:185][0m |          -0.0022 |         158.3776 |        -120.0172 |
[32m[20221214 14:31:33 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:31:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.22
[32m[20221214 14:31:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.39
[32m[20221214 14:31:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.84
[32m[20221214 14:31:33 @agent_ppo2.py:143][0m Total time:      33.51 min
[32m[20221214 14:31:33 @agent_ppo2.py:145][0m 3082240 total steps have happened
[32m[20221214 14:31:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1505 --------------------------#
[32m[20221214 14:31:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:33 @agent_ppo2.py:185][0m |          -0.0037 |         201.9188 |        -120.4348 |
[32m[20221214 14:31:33 @agent_ppo2.py:185][0m |          -0.0020 |         191.2330 |        -120.3373 |
[32m[20221214 14:31:33 @agent_ppo2.py:185][0m |          -0.0036 |         187.3906 |        -120.5634 |
[32m[20221214 14:31:34 @agent_ppo2.py:185][0m |          -0.0021 |         185.9792 |        -120.1257 |
[32m[20221214 14:31:34 @agent_ppo2.py:185][0m |          -0.0048 |         185.2220 |        -120.2266 |
[32m[20221214 14:31:34 @agent_ppo2.py:185][0m |           0.0121 |         203.6187 |        -120.0700 |
[32m[20221214 14:31:34 @agent_ppo2.py:185][0m |          -0.0036 |         183.9262 |        -120.1612 |
[32m[20221214 14:31:34 @agent_ppo2.py:185][0m |           0.0025 |         186.1352 |        -120.1105 |
[32m[20221214 14:31:34 @agent_ppo2.py:185][0m |          -0.0042 |         183.1267 |        -120.4064 |
[32m[20221214 14:31:34 @agent_ppo2.py:185][0m |          -0.0033 |         182.8271 |        -119.9593 |
[32m[20221214 14:31:34 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:31:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.68
[32m[20221214 14:31:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.95
[32m[20221214 14:31:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.10
[32m[20221214 14:31:34 @agent_ppo2.py:143][0m Total time:      33.54 min
[32m[20221214 14:31:34 @agent_ppo2.py:145][0m 3084288 total steps have happened
[32m[20221214 14:31:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1506 --------------------------#
[32m[20221214 14:31:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:35 @agent_ppo2.py:185][0m |          -0.0046 |         215.1774 |        -117.8148 |
[32m[20221214 14:31:35 @agent_ppo2.py:185][0m |          -0.0022 |         202.0437 |        -117.4594 |
[32m[20221214 14:31:35 @agent_ppo2.py:185][0m |          -0.0077 |         197.0692 |        -117.7083 |
[32m[20221214 14:31:35 @agent_ppo2.py:185][0m |          -0.0076 |         192.8509 |        -117.5363 |
[32m[20221214 14:31:35 @agent_ppo2.py:185][0m |          -0.0054 |         190.6704 |        -117.3966 |
[32m[20221214 14:31:35 @agent_ppo2.py:185][0m |           0.0043 |         194.0163 |        -117.4517 |
[32m[20221214 14:31:35 @agent_ppo2.py:185][0m |          -0.0054 |         186.9643 |        -117.4982 |
[32m[20221214 14:31:35 @agent_ppo2.py:185][0m |          -0.0015 |         187.3311 |        -117.3656 |
[32m[20221214 14:31:35 @agent_ppo2.py:185][0m |          -0.0048 |         185.0275 |        -117.3089 |
[32m[20221214 14:31:35 @agent_ppo2.py:185][0m |           0.0106 |         212.0062 |        -117.2259 |
[32m[20221214 14:31:35 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:31:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.36
[32m[20221214 14:31:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.93
[32m[20221214 14:31:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.61
[32m[20221214 14:31:36 @agent_ppo2.py:143][0m Total time:      33.56 min
[32m[20221214 14:31:36 @agent_ppo2.py:145][0m 3086336 total steps have happened
[32m[20221214 14:31:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1507 --------------------------#
[32m[20221214 14:31:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:31:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:36 @agent_ppo2.py:185][0m |          -0.0042 |         187.4121 |        -119.2803 |
[32m[20221214 14:31:36 @agent_ppo2.py:185][0m |           0.0009 |         172.3301 |        -119.3119 |
[32m[20221214 14:31:36 @agent_ppo2.py:185][0m |          -0.0017 |         169.5924 |        -119.0311 |
[32m[20221214 14:31:36 @agent_ppo2.py:185][0m |          -0.0010 |         165.4293 |        -119.2287 |
[32m[20221214 14:31:36 @agent_ppo2.py:185][0m |          -0.0036 |         162.7138 |        -119.2915 |
[32m[20221214 14:31:36 @agent_ppo2.py:185][0m |           0.0014 |         161.1788 |        -119.2478 |
[32m[20221214 14:31:36 @agent_ppo2.py:185][0m |          -0.0033 |         159.7200 |        -119.1908 |
[32m[20221214 14:31:37 @agent_ppo2.py:185][0m |           0.0133 |         190.9766 |        -119.3816 |
[32m[20221214 14:31:37 @agent_ppo2.py:185][0m |          -0.0051 |         164.0697 |        -119.0713 |
[32m[20221214 14:31:37 @agent_ppo2.py:185][0m |          -0.0029 |         157.3503 |        -119.3032 |
[32m[20221214 14:31:37 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:31:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.18
[32m[20221214 14:31:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.65
[32m[20221214 14:31:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.88
[32m[20221214 14:31:37 @agent_ppo2.py:143][0m Total time:      33.58 min
[32m[20221214 14:31:37 @agent_ppo2.py:145][0m 3088384 total steps have happened
[32m[20221214 14:31:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1508 --------------------------#
[32m[20221214 14:31:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:37 @agent_ppo2.py:185][0m |          -0.0022 |         133.1332 |        -119.7740 |
[32m[20221214 14:31:37 @agent_ppo2.py:185][0m |          -0.0038 |         116.3422 |        -119.7125 |
[32m[20221214 14:31:37 @agent_ppo2.py:185][0m |          -0.0042 |         112.8079 |        -119.7928 |
[32m[20221214 14:31:38 @agent_ppo2.py:185][0m |          -0.0042 |         109.9515 |        -119.4848 |
[32m[20221214 14:31:38 @agent_ppo2.py:185][0m |          -0.0031 |         107.6889 |        -119.8084 |
[32m[20221214 14:31:38 @agent_ppo2.py:185][0m |           0.0024 |         105.3197 |        -119.6925 |
[32m[20221214 14:31:38 @agent_ppo2.py:185][0m |          -0.0039 |         103.9341 |        -119.6303 |
[32m[20221214 14:31:38 @agent_ppo2.py:185][0m |           0.0011 |         102.7258 |        -119.9366 |
[32m[20221214 14:31:38 @agent_ppo2.py:185][0m |          -0.0052 |         101.8066 |        -119.7121 |
[32m[20221214 14:31:38 @agent_ppo2.py:185][0m |          -0.0047 |         101.4563 |        -119.9372 |
[32m[20221214 14:31:38 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:31:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 779.69
[32m[20221214 14:31:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.42
[32m[20221214 14:31:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.68
[32m[20221214 14:31:38 @agent_ppo2.py:143][0m Total time:      33.60 min
[32m[20221214 14:31:38 @agent_ppo2.py:145][0m 3090432 total steps have happened
[32m[20221214 14:31:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1509 --------------------------#
[32m[20221214 14:31:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:39 @agent_ppo2.py:185][0m |          -0.0015 |         253.7303 |        -118.8329 |
[32m[20221214 14:31:39 @agent_ppo2.py:185][0m |          -0.0004 |         240.9783 |        -118.7471 |
[32m[20221214 14:31:39 @agent_ppo2.py:185][0m |           0.0031 |         242.8304 |        -118.6982 |
[32m[20221214 14:31:39 @agent_ppo2.py:185][0m |          -0.0026 |         234.6947 |        -118.6948 |
[32m[20221214 14:31:39 @agent_ppo2.py:185][0m |          -0.0009 |         232.8396 |        -118.5832 |
[32m[20221214 14:31:39 @agent_ppo2.py:185][0m |           0.0078 |         243.8171 |        -118.5522 |
[32m[20221214 14:31:39 @agent_ppo2.py:185][0m |           0.0038 |         232.4193 |        -118.3604 |
[32m[20221214 14:31:39 @agent_ppo2.py:185][0m |           0.0024 |         229.2061 |        -118.3643 |
[32m[20221214 14:31:39 @agent_ppo2.py:185][0m |          -0.0026 |         225.9321 |        -118.0983 |
[32m[20221214 14:31:39 @agent_ppo2.py:185][0m |           0.0000 |         224.4246 |        -118.1144 |
[32m[20221214 14:31:39 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:31:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.07
[32m[20221214 14:31:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.49
[32m[20221214 14:31:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.66
[32m[20221214 14:31:40 @agent_ppo2.py:143][0m Total time:      33.63 min
[32m[20221214 14:31:40 @agent_ppo2.py:145][0m 3092480 total steps have happened
[32m[20221214 14:31:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1510 --------------------------#
[32m[20221214 14:31:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:31:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:40 @agent_ppo2.py:185][0m |           0.0007 |         221.3305 |        -120.4970 |
[32m[20221214 14:31:40 @agent_ppo2.py:185][0m |           0.0020 |         206.6783 |        -120.1397 |
[32m[20221214 14:31:40 @agent_ppo2.py:185][0m |          -0.0020 |         197.0074 |        -119.9338 |
[32m[20221214 14:31:40 @agent_ppo2.py:185][0m |          -0.0020 |         191.0046 |        -119.7216 |
[32m[20221214 14:31:40 @agent_ppo2.py:185][0m |          -0.0039 |         184.5819 |        -119.8914 |
[32m[20221214 14:31:40 @agent_ppo2.py:185][0m |           0.0090 |         202.4879 |        -119.8351 |
[32m[20221214 14:31:41 @agent_ppo2.py:185][0m |          -0.0058 |         178.1996 |        -119.4899 |
[32m[20221214 14:31:41 @agent_ppo2.py:185][0m |          -0.0029 |         176.6706 |        -119.2747 |
[32m[20221214 14:31:41 @agent_ppo2.py:185][0m |          -0.0039 |         174.2178 |        -119.3533 |
[32m[20221214 14:31:41 @agent_ppo2.py:185][0m |           0.0003 |         173.5018 |        -119.3501 |
[32m[20221214 14:31:41 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:31:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.02
[32m[20221214 14:31:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.58
[32m[20221214 14:31:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 675.26
[32m[20221214 14:31:41 @agent_ppo2.py:143][0m Total time:      33.65 min
[32m[20221214 14:31:41 @agent_ppo2.py:145][0m 3094528 total steps have happened
[32m[20221214 14:31:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1511 --------------------------#
[32m[20221214 14:31:41 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:31:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:41 @agent_ppo2.py:185][0m |           0.0133 |         168.6366 |        -116.7815 |
[32m[20221214 14:31:41 @agent_ppo2.py:185][0m |          -0.0037 |         138.2502 |        -116.5290 |
[32m[20221214 14:31:42 @agent_ppo2.py:185][0m |          -0.0064 |         128.2964 |        -116.3868 |
[32m[20221214 14:31:42 @agent_ppo2.py:185][0m |          -0.0056 |         122.5823 |        -116.4606 |
[32m[20221214 14:31:42 @agent_ppo2.py:185][0m |          -0.0067 |         119.6143 |        -116.2004 |
[32m[20221214 14:31:42 @agent_ppo2.py:185][0m |          -0.0058 |         117.0009 |        -116.1086 |
[32m[20221214 14:31:42 @agent_ppo2.py:185][0m |          -0.0035 |         115.1022 |        -116.2177 |
[32m[20221214 14:31:42 @agent_ppo2.py:185][0m |          -0.0047 |         114.1718 |        -116.2253 |
[32m[20221214 14:31:42 @agent_ppo2.py:185][0m |          -0.0071 |         112.1463 |        -116.3629 |
[32m[20221214 14:31:42 @agent_ppo2.py:185][0m |          -0.0048 |         113.7292 |        -116.0775 |
[32m[20221214 14:31:42 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 705.97
[32m[20221214 14:31:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.74
[32m[20221214 14:31:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.07
[32m[20221214 14:31:42 @agent_ppo2.py:143][0m Total time:      33.67 min
[32m[20221214 14:31:42 @agent_ppo2.py:145][0m 3096576 total steps have happened
[32m[20221214 14:31:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1512 --------------------------#
[32m[20221214 14:31:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:31:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:43 @agent_ppo2.py:185][0m |          -0.0015 |         254.8510 |        -117.9478 |
[32m[20221214 14:31:43 @agent_ppo2.py:185][0m |          -0.0023 |         236.0499 |        -118.2254 |
[32m[20221214 14:31:43 @agent_ppo2.py:185][0m |          -0.0026 |         229.6794 |        -118.1937 |
[32m[20221214 14:31:43 @agent_ppo2.py:185][0m |          -0.0036 |         226.9838 |        -118.2959 |
[32m[20221214 14:31:43 @agent_ppo2.py:185][0m |           0.0001 |         224.2894 |        -118.2380 |
[32m[20221214 14:31:43 @agent_ppo2.py:185][0m |          -0.0021 |         223.1707 |        -118.4806 |
[32m[20221214 14:31:43 @agent_ppo2.py:185][0m |          -0.0022 |         221.7895 |        -118.4221 |
[32m[20221214 14:31:43 @agent_ppo2.py:185][0m |          -0.0049 |         221.0043 |        -118.2281 |
[32m[20221214 14:31:43 @agent_ppo2.py:185][0m |          -0.0053 |         219.0906 |        -118.7441 |
[32m[20221214 14:31:43 @agent_ppo2.py:185][0m |          -0.0015 |         219.6184 |        -118.7250 |
[32m[20221214 14:31:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:31:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.43
[32m[20221214 14:31:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.31
[32m[20221214 14:31:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.34
[32m[20221214 14:31:44 @agent_ppo2.py:143][0m Total time:      33.69 min
[32m[20221214 14:31:44 @agent_ppo2.py:145][0m 3098624 total steps have happened
[32m[20221214 14:31:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1513 --------------------------#
[32m[20221214 14:31:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:31:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:44 @agent_ppo2.py:185][0m |          -0.0009 |         204.5872 |        -118.8835 |
[32m[20221214 14:31:44 @agent_ppo2.py:185][0m |           0.0036 |         194.5761 |        -119.1971 |
[32m[20221214 14:31:44 @agent_ppo2.py:185][0m |          -0.0034 |         190.2816 |        -119.0902 |
[32m[20221214 14:31:44 @agent_ppo2.py:185][0m |          -0.0014 |         189.3031 |        -119.1392 |
[32m[20221214 14:31:44 @agent_ppo2.py:185][0m |           0.0024 |         191.5569 |        -119.0144 |
[32m[20221214 14:31:44 @agent_ppo2.py:185][0m |          -0.0022 |         186.3425 |        -119.0737 |
[32m[20221214 14:31:44 @agent_ppo2.py:185][0m |          -0.0031 |         185.7751 |        -119.0201 |
[32m[20221214 14:31:45 @agent_ppo2.py:185][0m |           0.0026 |         186.3372 |        -119.0103 |
[32m[20221214 14:31:45 @agent_ppo2.py:185][0m |          -0.0042 |         183.5405 |        -118.7513 |
[32m[20221214 14:31:45 @agent_ppo2.py:185][0m |           0.0080 |         197.7456 |        -119.1559 |
[32m[20221214 14:31:45 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:31:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.81
[32m[20221214 14:31:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.27
[32m[20221214 14:31:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.84
[32m[20221214 14:31:45 @agent_ppo2.py:143][0m Total time:      33.71 min
[32m[20221214 14:31:45 @agent_ppo2.py:145][0m 3100672 total steps have happened
[32m[20221214 14:31:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1514 --------------------------#
[32m[20221214 14:31:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:31:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:45 @agent_ppo2.py:185][0m |           0.0032 |         209.4019 |        -118.4755 |
[32m[20221214 14:31:45 @agent_ppo2.py:185][0m |           0.0004 |         194.2763 |        -118.3526 |
[32m[20221214 14:31:45 @agent_ppo2.py:185][0m |           0.0013 |         190.8557 |        -118.1020 |
[32m[20221214 14:31:45 @agent_ppo2.py:185][0m |           0.0001 |         187.4857 |        -118.3129 |
[32m[20221214 14:31:46 @agent_ppo2.py:185][0m |           0.0010 |         186.2779 |        -118.3322 |
[32m[20221214 14:31:46 @agent_ppo2.py:185][0m |           0.0104 |         198.6234 |        -118.2202 |
[32m[20221214 14:31:46 @agent_ppo2.py:185][0m |           0.0027 |         183.3361 |        -118.3129 |
[32m[20221214 14:31:46 @agent_ppo2.py:185][0m |          -0.0038 |         180.2997 |        -118.5811 |
[32m[20221214 14:31:46 @agent_ppo2.py:185][0m |          -0.0017 |         180.1646 |        -118.4879 |
[32m[20221214 14:31:46 @agent_ppo2.py:185][0m |           0.0066 |         183.6712 |        -118.4740 |
[32m[20221214 14:31:46 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:31:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.37
[32m[20221214 14:31:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.56
[32m[20221214 14:31:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.85
[32m[20221214 14:31:46 @agent_ppo2.py:143][0m Total time:      33.73 min
[32m[20221214 14:31:46 @agent_ppo2.py:145][0m 3102720 total steps have happened
[32m[20221214 14:31:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1515 --------------------------#
[32m[20221214 14:31:46 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:31:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:46 @agent_ppo2.py:185][0m |          -0.0031 |         184.1178 |        -119.2949 |
[32m[20221214 14:31:47 @agent_ppo2.py:185][0m |          -0.0030 |         171.7751 |        -119.0335 |
[32m[20221214 14:31:47 @agent_ppo2.py:185][0m |          -0.0020 |         167.9052 |        -119.2639 |
[32m[20221214 14:31:47 @agent_ppo2.py:185][0m |          -0.0035 |         166.1697 |        -118.9705 |
[32m[20221214 14:31:47 @agent_ppo2.py:185][0m |          -0.0038 |         163.7397 |        -118.7906 |
[32m[20221214 14:31:47 @agent_ppo2.py:185][0m |           0.0003 |         163.4188 |        -119.1690 |
[32m[20221214 14:31:47 @agent_ppo2.py:185][0m |          -0.0030 |         161.4032 |        -119.2337 |
[32m[20221214 14:31:47 @agent_ppo2.py:185][0m |          -0.0050 |         160.5676 |        -119.1732 |
[32m[20221214 14:31:47 @agent_ppo2.py:185][0m |          -0.0000 |         160.0199 |        -119.0524 |
[32m[20221214 14:31:47 @agent_ppo2.py:185][0m |          -0.0025 |         158.9512 |        -119.3616 |
[32m[20221214 14:31:47 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:31:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.34
[32m[20221214 14:31:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.76
[32m[20221214 14:31:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.87
[32m[20221214 14:31:48 @agent_ppo2.py:143][0m Total time:      33.76 min
[32m[20221214 14:31:48 @agent_ppo2.py:145][0m 3104768 total steps have happened
[32m[20221214 14:31:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1516 --------------------------#
[32m[20221214 14:31:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:48 @agent_ppo2.py:185][0m |          -0.0034 |         243.7726 |        -118.3043 |
[32m[20221214 14:31:48 @agent_ppo2.py:185][0m |          -0.0039 |         213.5429 |        -118.3712 |
[32m[20221214 14:31:48 @agent_ppo2.py:185][0m |          -0.0015 |         210.1048 |        -118.2572 |
[32m[20221214 14:31:48 @agent_ppo2.py:185][0m |          -0.0070 |         206.2712 |        -118.2012 |
[32m[20221214 14:31:48 @agent_ppo2.py:185][0m |          -0.0016 |         204.1195 |        -118.1311 |
[32m[20221214 14:31:48 @agent_ppo2.py:185][0m |          -0.0036 |         202.7729 |        -118.2233 |
[32m[20221214 14:31:48 @agent_ppo2.py:185][0m |          -0.0037 |         201.7191 |        -118.0716 |
[32m[20221214 14:31:49 @agent_ppo2.py:185][0m |          -0.0023 |         201.4673 |        -117.8063 |
[32m[20221214 14:31:49 @agent_ppo2.py:185][0m |           0.0077 |         207.5584 |        -117.9151 |
[32m[20221214 14:31:49 @agent_ppo2.py:185][0m |           0.0167 |         227.4443 |        -117.3985 |
[32m[20221214 14:31:49 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:31:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.74
[32m[20221214 14:31:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.85
[32m[20221214 14:31:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.37
[32m[20221214 14:31:49 @agent_ppo2.py:143][0m Total time:      33.78 min
[32m[20221214 14:31:49 @agent_ppo2.py:145][0m 3106816 total steps have happened
[32m[20221214 14:31:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1517 --------------------------#
[32m[20221214 14:31:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:49 @agent_ppo2.py:185][0m |          -0.0004 |         196.4791 |        -117.9493 |
[32m[20221214 14:31:49 @agent_ppo2.py:185][0m |           0.0020 |         165.3240 |        -118.1677 |
[32m[20221214 14:31:49 @agent_ppo2.py:185][0m |           0.0078 |         179.7286 |        -117.8661 |
[32m[20221214 14:31:50 @agent_ppo2.py:185][0m |           0.0025 |         155.5321 |        -118.0649 |
[32m[20221214 14:31:50 @agent_ppo2.py:185][0m |          -0.0037 |         151.8625 |        -118.5078 |
[32m[20221214 14:31:50 @agent_ppo2.py:185][0m |          -0.0054 |         152.3721 |        -118.3835 |
[32m[20221214 14:31:50 @agent_ppo2.py:185][0m |          -0.0013 |         148.4636 |        -118.6124 |
[32m[20221214 14:31:50 @agent_ppo2.py:185][0m |          -0.0036 |         146.5874 |        -118.6217 |
[32m[20221214 14:31:50 @agent_ppo2.py:185][0m |          -0.0038 |         145.6048 |        -118.6140 |
[32m[20221214 14:31:50 @agent_ppo2.py:185][0m |           0.0003 |         144.6055 |        -118.7020 |
[32m[20221214 14:31:50 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:31:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.64
[32m[20221214 14:31:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.15
[32m[20221214 14:31:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.54
[32m[20221214 14:31:50 @agent_ppo2.py:143][0m Total time:      33.80 min
[32m[20221214 14:31:50 @agent_ppo2.py:145][0m 3108864 total steps have happened
[32m[20221214 14:31:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1518 --------------------------#
[32m[20221214 14:31:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:51 @agent_ppo2.py:185][0m |           0.0004 |         178.6053 |        -117.4105 |
[32m[20221214 14:31:51 @agent_ppo2.py:185][0m |          -0.0002 |         148.7734 |        -117.2247 |
[32m[20221214 14:31:51 @agent_ppo2.py:185][0m |           0.0062 |         163.7198 |        -117.2263 |
[32m[20221214 14:31:51 @agent_ppo2.py:185][0m |          -0.0041 |         134.1486 |        -117.2693 |
[32m[20221214 14:31:51 @agent_ppo2.py:185][0m |          -0.0033 |         128.8887 |        -117.5808 |
[32m[20221214 14:31:51 @agent_ppo2.py:185][0m |           0.0037 |         133.7692 |        -117.6040 |
[32m[20221214 14:31:51 @agent_ppo2.py:185][0m |          -0.0013 |         124.1438 |        -117.2213 |
[32m[20221214 14:31:51 @agent_ppo2.py:185][0m |          -0.0010 |         122.5786 |        -117.3199 |
[32m[20221214 14:31:51 @agent_ppo2.py:185][0m |          -0.0037 |         121.1791 |        -117.4812 |
[32m[20221214 14:31:51 @agent_ppo2.py:185][0m |          -0.0003 |         119.9968 |        -117.5625 |
[32m[20221214 14:31:51 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 734.46
[32m[20221214 14:31:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.06
[32m[20221214 14:31:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.91
[32m[20221214 14:31:52 @agent_ppo2.py:143][0m Total time:      33.83 min
[32m[20221214 14:31:52 @agent_ppo2.py:145][0m 3110912 total steps have happened
[32m[20221214 14:31:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1519 --------------------------#
[32m[20221214 14:31:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:52 @agent_ppo2.py:185][0m |          -0.0007 |         165.4788 |        -117.1286 |
[32m[20221214 14:31:52 @agent_ppo2.py:185][0m |          -0.0035 |         144.0366 |        -116.9982 |
[32m[20221214 14:31:52 @agent_ppo2.py:185][0m |          -0.0048 |         133.6080 |        -117.4387 |
[32m[20221214 14:31:52 @agent_ppo2.py:185][0m |          -0.0055 |         126.9247 |        -117.5460 |
[32m[20221214 14:31:52 @agent_ppo2.py:185][0m |          -0.0025 |         121.7907 |        -117.3814 |
[32m[20221214 14:31:52 @agent_ppo2.py:185][0m |          -0.0018 |         118.2366 |        -117.7444 |
[32m[20221214 14:31:53 @agent_ppo2.py:185][0m |          -0.0050 |         116.2246 |        -117.6614 |
[32m[20221214 14:31:53 @agent_ppo2.py:185][0m |          -0.0009 |         114.3190 |        -117.9432 |
[32m[20221214 14:31:53 @agent_ppo2.py:185][0m |          -0.0031 |         111.9032 |        -118.1683 |
[32m[20221214 14:31:53 @agent_ppo2.py:185][0m |          -0.0036 |         110.2941 |        -117.8863 |
[32m[20221214 14:31:53 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:31:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 755.00
[32m[20221214 14:31:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.61
[32m[20221214 14:31:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.49
[32m[20221214 14:31:53 @agent_ppo2.py:143][0m Total time:      33.85 min
[32m[20221214 14:31:53 @agent_ppo2.py:145][0m 3112960 total steps have happened
[32m[20221214 14:31:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1520 --------------------------#
[32m[20221214 14:31:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:31:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:53 @agent_ppo2.py:185][0m |           0.0016 |         241.4712 |        -121.7796 |
[32m[20221214 14:31:53 @agent_ppo2.py:185][0m |          -0.0008 |         222.5248 |        -121.6094 |
[32m[20221214 14:31:53 @agent_ppo2.py:185][0m |          -0.0027 |         216.4343 |        -122.0726 |
[32m[20221214 14:31:54 @agent_ppo2.py:185][0m |          -0.0023 |         210.7729 |        -121.7097 |
[32m[20221214 14:31:54 @agent_ppo2.py:185][0m |          -0.0022 |         206.8681 |        -122.3424 |
[32m[20221214 14:31:54 @agent_ppo2.py:185][0m |          -0.0043 |         204.5760 |        -122.3887 |
[32m[20221214 14:31:54 @agent_ppo2.py:185][0m |           0.0003 |         205.3910 |        -122.4896 |
[32m[20221214 14:31:54 @agent_ppo2.py:185][0m |          -0.0036 |         203.0520 |        -122.4496 |
[32m[20221214 14:31:54 @agent_ppo2.py:185][0m |          -0.0017 |         201.8287 |        -122.8115 |
[32m[20221214 14:31:54 @agent_ppo2.py:185][0m |          -0.0007 |         200.0191 |        -122.8455 |
[32m[20221214 14:31:54 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:31:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.59
[32m[20221214 14:31:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.02
[32m[20221214 14:31:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.86
[32m[20221214 14:31:54 @agent_ppo2.py:143][0m Total time:      33.87 min
[32m[20221214 14:31:54 @agent_ppo2.py:145][0m 3115008 total steps have happened
[32m[20221214 14:31:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1521 --------------------------#
[32m[20221214 14:31:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:55 @agent_ppo2.py:185][0m |          -0.0039 |         222.6392 |        -123.8115 |
[32m[20221214 14:31:55 @agent_ppo2.py:185][0m |          -0.0026 |         200.2277 |        -123.8073 |
[32m[20221214 14:31:55 @agent_ppo2.py:185][0m |          -0.0037 |         190.5780 |        -123.5811 |
[32m[20221214 14:31:55 @agent_ppo2.py:185][0m |          -0.0008 |         184.8019 |        -123.9052 |
[32m[20221214 14:31:55 @agent_ppo2.py:185][0m |           0.0053 |         181.7405 |        -123.4482 |
[32m[20221214 14:31:55 @agent_ppo2.py:185][0m |          -0.0021 |         179.2646 |        -123.9016 |
[32m[20221214 14:31:55 @agent_ppo2.py:185][0m |           0.0069 |         190.3888 |        -123.3893 |
[32m[20221214 14:31:55 @agent_ppo2.py:185][0m |           0.0131 |         195.9997 |        -123.2790 |
[32m[20221214 14:31:55 @agent_ppo2.py:185][0m |          -0.0044 |         173.4241 |        -123.4543 |
[32m[20221214 14:31:56 @agent_ppo2.py:185][0m |          -0.0012 |         171.9115 |        -123.7776 |
[32m[20221214 14:31:56 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:31:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.85
[32m[20221214 14:31:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.42
[32m[20221214 14:31:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.67
[32m[20221214 14:31:56 @agent_ppo2.py:143][0m Total time:      33.89 min
[32m[20221214 14:31:56 @agent_ppo2.py:145][0m 3117056 total steps have happened
[32m[20221214 14:31:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1522 --------------------------#
[32m[20221214 14:31:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:56 @agent_ppo2.py:185][0m |           0.0097 |         258.3616 |        -123.5988 |
[32m[20221214 14:31:56 @agent_ppo2.py:185][0m |           0.0056 |         230.9319 |        -123.8238 |
[32m[20221214 14:31:56 @agent_ppo2.py:185][0m |          -0.0021 |         219.9630 |        -124.0204 |
[32m[20221214 14:31:56 @agent_ppo2.py:185][0m |          -0.0025 |         216.1942 |        -123.8644 |
[32m[20221214 14:31:56 @agent_ppo2.py:185][0m |          -0.0011 |         213.4862 |        -124.1555 |
[32m[20221214 14:31:56 @agent_ppo2.py:185][0m |           0.0078 |         217.9760 |        -124.2571 |
[32m[20221214 14:31:57 @agent_ppo2.py:185][0m |          -0.0029 |         210.8567 |        -124.4562 |
[32m[20221214 14:31:57 @agent_ppo2.py:185][0m |          -0.0033 |         208.6646 |        -124.2678 |
[32m[20221214 14:31:57 @agent_ppo2.py:185][0m |          -0.0028 |         207.0748 |        -124.5586 |
[32m[20221214 14:31:57 @agent_ppo2.py:185][0m |          -0.0033 |         206.2459 |        -124.7144 |
[32m[20221214 14:31:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:31:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.34
[32m[20221214 14:31:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.17
[32m[20221214 14:31:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.19
[32m[20221214 14:31:57 @agent_ppo2.py:143][0m Total time:      33.92 min
[32m[20221214 14:31:57 @agent_ppo2.py:145][0m 3119104 total steps have happened
[32m[20221214 14:31:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1523 --------------------------#
[32m[20221214 14:31:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:57 @agent_ppo2.py:185][0m |           0.0007 |         218.1991 |        -127.2762 |
[32m[20221214 14:31:57 @agent_ppo2.py:185][0m |          -0.0018 |         209.5707 |        -127.5200 |
[32m[20221214 14:31:58 @agent_ppo2.py:185][0m |          -0.0010 |         206.1510 |        -127.5540 |
[32m[20221214 14:31:58 @agent_ppo2.py:185][0m |           0.0022 |         203.8186 |        -127.2215 |
[32m[20221214 14:31:58 @agent_ppo2.py:185][0m |          -0.0026 |         202.1574 |        -127.6245 |
[32m[20221214 14:31:58 @agent_ppo2.py:185][0m |          -0.0004 |         200.0865 |        -127.6534 |
[32m[20221214 14:31:58 @agent_ppo2.py:185][0m |          -0.0016 |         199.0215 |        -127.4002 |
[32m[20221214 14:31:58 @agent_ppo2.py:185][0m |          -0.0006 |         198.4765 |        -127.6542 |
[32m[20221214 14:31:58 @agent_ppo2.py:185][0m |          -0.0009 |         196.9723 |        -127.5018 |
[32m[20221214 14:31:58 @agent_ppo2.py:185][0m |           0.0014 |         196.9366 |        -127.5845 |
[32m[20221214 14:31:58 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:31:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.67
[32m[20221214 14:31:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.55
[32m[20221214 14:31:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.85
[32m[20221214 14:31:58 @agent_ppo2.py:143][0m Total time:      33.94 min
[32m[20221214 14:31:58 @agent_ppo2.py:145][0m 3121152 total steps have happened
[32m[20221214 14:31:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1524 --------------------------#
[32m[20221214 14:31:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:31:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:31:59 @agent_ppo2.py:185][0m |          -0.0028 |         232.8054 |        -124.7239 |
[32m[20221214 14:31:59 @agent_ppo2.py:185][0m |          -0.0011 |         218.2219 |        -124.6120 |
[32m[20221214 14:31:59 @agent_ppo2.py:185][0m |          -0.0025 |         214.1838 |        -124.1623 |
[32m[20221214 14:31:59 @agent_ppo2.py:185][0m |          -0.0024 |         211.0037 |        -124.3980 |
[32m[20221214 14:31:59 @agent_ppo2.py:185][0m |          -0.0022 |         209.4702 |        -124.2830 |
[32m[20221214 14:31:59 @agent_ppo2.py:185][0m |           0.0021 |         210.4120 |        -124.1916 |
[32m[20221214 14:31:59 @agent_ppo2.py:185][0m |          -0.0025 |         207.4350 |        -124.1054 |
[32m[20221214 14:31:59 @agent_ppo2.py:185][0m |           0.0019 |         209.7095 |        -124.1087 |
[32m[20221214 14:31:59 @agent_ppo2.py:185][0m |          -0.0015 |         205.3162 |        -123.9631 |
[32m[20221214 14:32:00 @agent_ppo2.py:185][0m |          -0.0026 |         204.9590 |        -124.0890 |
[32m[20221214 14:32:00 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:32:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.80
[32m[20221214 14:32:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.86
[32m[20221214 14:32:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.84
[32m[20221214 14:32:00 @agent_ppo2.py:143][0m Total time:      33.96 min
[32m[20221214 14:32:00 @agent_ppo2.py:145][0m 3123200 total steps have happened
[32m[20221214 14:32:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1525 --------------------------#
[32m[20221214 14:32:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:00 @agent_ppo2.py:185][0m |           0.0019 |         244.2364 |        -125.2695 |
[32m[20221214 14:32:00 @agent_ppo2.py:185][0m |           0.0093 |         230.0035 |        -125.4062 |
[32m[20221214 14:32:00 @agent_ppo2.py:185][0m |          -0.0051 |         207.9410 |        -124.9429 |
[32m[20221214 14:32:00 @agent_ppo2.py:185][0m |           0.0066 |         212.2597 |        -125.2412 |
[32m[20221214 14:32:00 @agent_ppo2.py:185][0m |          -0.0036 |         199.4649 |        -125.1680 |
[32m[20221214 14:32:01 @agent_ppo2.py:185][0m |          -0.0037 |         196.3804 |        -125.7237 |
[32m[20221214 14:32:01 @agent_ppo2.py:185][0m |          -0.0011 |         193.7418 |        -125.4218 |
[32m[20221214 14:32:01 @agent_ppo2.py:185][0m |           0.0040 |         205.5176 |        -125.7674 |
[32m[20221214 14:32:01 @agent_ppo2.py:185][0m |           0.0021 |         190.5786 |        -125.6404 |
[32m[20221214 14:32:01 @agent_ppo2.py:185][0m |          -0.0011 |         186.3840 |        -125.7832 |
[32m[20221214 14:32:01 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.27
[32m[20221214 14:32:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.06
[32m[20221214 14:32:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.98
[32m[20221214 14:32:01 @agent_ppo2.py:143][0m Total time:      33.98 min
[32m[20221214 14:32:01 @agent_ppo2.py:145][0m 3125248 total steps have happened
[32m[20221214 14:32:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1526 --------------------------#
[32m[20221214 14:32:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:01 @agent_ppo2.py:185][0m |           0.0012 |         185.0808 |        -125.2638 |
[32m[20221214 14:32:02 @agent_ppo2.py:185][0m |           0.0090 |         153.7817 |        -125.6042 |
[32m[20221214 14:32:02 @agent_ppo2.py:185][0m |          -0.0007 |         134.5345 |        -125.5982 |
[32m[20221214 14:32:02 @agent_ppo2.py:185][0m |          -0.0014 |         130.5416 |        -125.3440 |
[32m[20221214 14:32:02 @agent_ppo2.py:185][0m |          -0.0023 |         127.1816 |        -125.1210 |
[32m[20221214 14:32:02 @agent_ppo2.py:185][0m |           0.0014 |         124.8701 |        -125.1001 |
[32m[20221214 14:32:02 @agent_ppo2.py:185][0m |           0.0019 |         121.8075 |        -125.4624 |
[32m[20221214 14:32:02 @agent_ppo2.py:185][0m |          -0.0004 |         119.8413 |        -125.2748 |
[32m[20221214 14:32:02 @agent_ppo2.py:185][0m |           0.0073 |         134.4704 |        -125.3952 |
[32m[20221214 14:32:02 @agent_ppo2.py:185][0m |          -0.0009 |         117.3420 |        -125.1893 |
[32m[20221214 14:32:02 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:32:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.56
[32m[20221214 14:32:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.70
[32m[20221214 14:32:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.37
[32m[20221214 14:32:02 @agent_ppo2.py:143][0m Total time:      34.01 min
[32m[20221214 14:32:02 @agent_ppo2.py:145][0m 3127296 total steps have happened
[32m[20221214 14:32:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1527 --------------------------#
[32m[20221214 14:32:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:03 @agent_ppo2.py:185][0m |          -0.0014 |         216.8516 |        -122.7732 |
[32m[20221214 14:32:03 @agent_ppo2.py:185][0m |          -0.0002 |         200.4689 |        -122.8604 |
[32m[20221214 14:32:03 @agent_ppo2.py:185][0m |          -0.0031 |         196.7540 |        -122.7767 |
[32m[20221214 14:32:03 @agent_ppo2.py:185][0m |           0.0051 |         195.0685 |        -122.8322 |
[32m[20221214 14:32:03 @agent_ppo2.py:185][0m |          -0.0010 |         192.0584 |        -122.6723 |
[32m[20221214 14:32:03 @agent_ppo2.py:185][0m |          -0.0037 |         189.8451 |        -122.6642 |
[32m[20221214 14:32:03 @agent_ppo2.py:185][0m |          -0.0022 |         188.4778 |        -122.8306 |
[32m[20221214 14:32:03 @agent_ppo2.py:185][0m |           0.0042 |         188.0553 |        -122.7633 |
[32m[20221214 14:32:04 @agent_ppo2.py:185][0m |          -0.0021 |         185.9243 |        -122.6760 |
[32m[20221214 14:32:04 @agent_ppo2.py:185][0m |           0.0035 |         188.3510 |        -122.7755 |
[32m[20221214 14:32:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.70
[32m[20221214 14:32:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.45
[32m[20221214 14:32:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.41
[32m[20221214 14:32:04 @agent_ppo2.py:143][0m Total time:      34.03 min
[32m[20221214 14:32:04 @agent_ppo2.py:145][0m 3129344 total steps have happened
[32m[20221214 14:32:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1528 --------------------------#
[32m[20221214 14:32:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:04 @agent_ppo2.py:185][0m |          -0.0027 |         221.3643 |        -126.4853 |
[32m[20221214 14:32:04 @agent_ppo2.py:185][0m |          -0.0023 |         205.7623 |        -126.6193 |
[32m[20221214 14:32:04 @agent_ppo2.py:185][0m |          -0.0008 |         200.7809 |        -126.4548 |
[32m[20221214 14:32:04 @agent_ppo2.py:185][0m |           0.0013 |         200.5700 |        -126.4118 |
[32m[20221214 14:32:05 @agent_ppo2.py:185][0m |           0.0049 |         200.5336 |        -126.5573 |
[32m[20221214 14:32:05 @agent_ppo2.py:185][0m |          -0.0029 |         194.3721 |        -126.3816 |
[32m[20221214 14:32:05 @agent_ppo2.py:185][0m |           0.0016 |         193.7094 |        -126.5459 |
[32m[20221214 14:32:05 @agent_ppo2.py:185][0m |          -0.0056 |         192.8617 |        -126.5652 |
[32m[20221214 14:32:05 @agent_ppo2.py:185][0m |           0.0099 |         207.9858 |        -126.6650 |
[32m[20221214 14:32:05 @agent_ppo2.py:185][0m |          -0.0027 |         191.8438 |        -126.5619 |
[32m[20221214 14:32:05 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:32:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.58
[32m[20221214 14:32:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.52
[32m[20221214 14:32:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.63
[32m[20221214 14:32:05 @agent_ppo2.py:143][0m Total time:      34.05 min
[32m[20221214 14:32:05 @agent_ppo2.py:145][0m 3131392 total steps have happened
[32m[20221214 14:32:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1529 --------------------------#
[32m[20221214 14:32:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:32:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:06 @agent_ppo2.py:185][0m |          -0.0025 |         178.1361 |        -128.1784 |
[32m[20221214 14:32:06 @agent_ppo2.py:185][0m |          -0.0025 |         161.4817 |        -128.2747 |
[32m[20221214 14:32:06 @agent_ppo2.py:185][0m |          -0.0025 |         154.7346 |        -128.0204 |
[32m[20221214 14:32:06 @agent_ppo2.py:185][0m |           0.0006 |         150.3362 |        -128.3665 |
[32m[20221214 14:32:06 @agent_ppo2.py:185][0m |          -0.0043 |         147.2874 |        -128.1618 |
[32m[20221214 14:32:06 @agent_ppo2.py:185][0m |          -0.0017 |         146.1305 |        -128.1361 |
[32m[20221214 14:32:06 @agent_ppo2.py:185][0m |          -0.0027 |         143.7338 |        -127.9721 |
[32m[20221214 14:32:06 @agent_ppo2.py:185][0m |          -0.0040 |         141.9558 |        -128.0104 |
[32m[20221214 14:32:06 @agent_ppo2.py:185][0m |           0.0126 |         151.7251 |        -128.1664 |
[32m[20221214 14:32:06 @agent_ppo2.py:185][0m |          -0.0019 |         140.3983 |        -127.7512 |
[32m[20221214 14:32:06 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:32:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.71
[32m[20221214 14:32:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.90
[32m[20221214 14:32:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 772.37
[32m[20221214 14:32:07 @agent_ppo2.py:143][0m Total time:      34.07 min
[32m[20221214 14:32:07 @agent_ppo2.py:145][0m 3133440 total steps have happened
[32m[20221214 14:32:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1530 --------------------------#
[32m[20221214 14:32:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:32:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:07 @agent_ppo2.py:185][0m |          -0.0011 |          79.8779 |        -128.6430 |
[32m[20221214 14:32:07 @agent_ppo2.py:185][0m |           0.0016 |          61.2464 |        -128.6166 |
[32m[20221214 14:32:07 @agent_ppo2.py:185][0m |          -0.0054 |          55.0819 |        -128.3974 |
[32m[20221214 14:32:07 @agent_ppo2.py:185][0m |           0.0021 |          53.5862 |        -128.5882 |
[32m[20221214 14:32:07 @agent_ppo2.py:185][0m |           0.0008 |          51.2878 |        -128.2757 |
[32m[20221214 14:32:07 @agent_ppo2.py:185][0m |           0.0050 |          51.0112 |        -128.4971 |
[32m[20221214 14:32:07 @agent_ppo2.py:185][0m |           0.0009 |          49.0116 |        -127.6490 |
[32m[20221214 14:32:07 @agent_ppo2.py:185][0m |           0.0024 |          48.5749 |        -128.1234 |
[32m[20221214 14:32:08 @agent_ppo2.py:185][0m |          -0.0070 |          47.9138 |        -128.2472 |
[32m[20221214 14:32:08 @agent_ppo2.py:185][0m |          -0.0010 |          48.0994 |        -128.1820 |
[32m[20221214 14:32:08 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:32:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 757.18
[32m[20221214 14:32:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.24
[32m[20221214 14:32:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.07
[32m[20221214 14:32:08 @agent_ppo2.py:143][0m Total time:      34.10 min
[32m[20221214 14:32:08 @agent_ppo2.py:145][0m 3135488 total steps have happened
[32m[20221214 14:32:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1531 --------------------------#
[32m[20221214 14:32:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:32:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:08 @agent_ppo2.py:185][0m |          -0.0032 |         229.2427 |        -124.7859 |
[32m[20221214 14:32:08 @agent_ppo2.py:185][0m |          -0.0081 |         220.6395 |        -125.1747 |
[32m[20221214 14:32:08 @agent_ppo2.py:185][0m |          -0.0046 |         215.2737 |        -124.9997 |
[32m[20221214 14:32:08 @agent_ppo2.py:185][0m |          -0.0054 |         211.1957 |        -125.4246 |
[32m[20221214 14:32:08 @agent_ppo2.py:185][0m |          -0.0070 |         207.5911 |        -125.3363 |
[32m[20221214 14:32:09 @agent_ppo2.py:185][0m |          -0.0078 |         206.4233 |        -125.4749 |
[32m[20221214 14:32:09 @agent_ppo2.py:185][0m |          -0.0085 |         203.5631 |        -125.3602 |
[32m[20221214 14:32:09 @agent_ppo2.py:185][0m |          -0.0051 |         201.3794 |        -125.7564 |
[32m[20221214 14:32:09 @agent_ppo2.py:185][0m |          -0.0065 |         200.4360 |        -125.7398 |
[32m[20221214 14:32:09 @agent_ppo2.py:185][0m |          -0.0055 |         198.6456 |        -125.5802 |
[32m[20221214 14:32:09 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:32:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.74
[32m[20221214 14:32:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.92
[32m[20221214 14:32:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.02
[32m[20221214 14:32:09 @agent_ppo2.py:143][0m Total time:      34.12 min
[32m[20221214 14:32:09 @agent_ppo2.py:145][0m 3137536 total steps have happened
[32m[20221214 14:32:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1532 --------------------------#
[32m[20221214 14:32:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:32:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:09 @agent_ppo2.py:185][0m |          -0.0014 |         223.2124 |        -125.9516 |
[32m[20221214 14:32:09 @agent_ppo2.py:185][0m |          -0.0035 |         212.3241 |        -125.9869 |
[32m[20221214 14:32:10 @agent_ppo2.py:185][0m |          -0.0047 |         204.7046 |        -125.9317 |
[32m[20221214 14:32:10 @agent_ppo2.py:185][0m |          -0.0049 |         200.2366 |        -125.7880 |
[32m[20221214 14:32:10 @agent_ppo2.py:185][0m |           0.0048 |         217.4361 |        -125.8267 |
[32m[20221214 14:32:10 @agent_ppo2.py:185][0m |          -0.0035 |         194.7831 |        -125.8656 |
[32m[20221214 14:32:10 @agent_ppo2.py:185][0m |          -0.0047 |         193.3301 |        -125.6078 |
[32m[20221214 14:32:10 @agent_ppo2.py:185][0m |           0.0090 |         207.7135 |        -125.8295 |
[32m[20221214 14:32:10 @agent_ppo2.py:185][0m |          -0.0064 |         188.9984 |        -125.8691 |
[32m[20221214 14:32:10 @agent_ppo2.py:185][0m |           0.0024 |         189.3683 |        -125.9334 |
[32m[20221214 14:32:10 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:32:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.13
[32m[20221214 14:32:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.08
[32m[20221214 14:32:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.53
[32m[20221214 14:32:10 @agent_ppo2.py:143][0m Total time:      34.14 min
[32m[20221214 14:32:10 @agent_ppo2.py:145][0m 3139584 total steps have happened
[32m[20221214 14:32:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1533 --------------------------#
[32m[20221214 14:32:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:11 @agent_ppo2.py:185][0m |          -0.0031 |         216.1867 |        -128.6589 |
[32m[20221214 14:32:11 @agent_ppo2.py:185][0m |          -0.0018 |         203.3652 |        -128.7085 |
[32m[20221214 14:32:11 @agent_ppo2.py:185][0m |          -0.0024 |         199.5578 |        -128.3010 |
[32m[20221214 14:32:11 @agent_ppo2.py:185][0m |          -0.0047 |         197.7573 |        -128.3883 |
[32m[20221214 14:32:11 @agent_ppo2.py:185][0m |          -0.0008 |         196.6891 |        -128.1722 |
[32m[20221214 14:32:11 @agent_ppo2.py:185][0m |           0.0006 |         196.0578 |        -128.4288 |
[32m[20221214 14:32:11 @agent_ppo2.py:185][0m |           0.0023 |         197.4022 |        -128.4124 |
[32m[20221214 14:32:11 @agent_ppo2.py:185][0m |          -0.0002 |         191.2068 |        -127.8371 |
[32m[20221214 14:32:12 @agent_ppo2.py:185][0m |          -0.0024 |         190.8937 |        -128.3297 |
[32m[20221214 14:32:12 @agent_ppo2.py:185][0m |          -0.0018 |         189.4512 |        -128.2180 |
[32m[20221214 14:32:12 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:32:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.74
[32m[20221214 14:32:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.12
[32m[20221214 14:32:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.32
[32m[20221214 14:32:12 @agent_ppo2.py:143][0m Total time:      34.16 min
[32m[20221214 14:32:12 @agent_ppo2.py:145][0m 3141632 total steps have happened
[32m[20221214 14:32:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1534 --------------------------#
[32m[20221214 14:32:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:12 @agent_ppo2.py:185][0m |           0.0002 |         199.2573 |        -126.2427 |
[32m[20221214 14:32:12 @agent_ppo2.py:185][0m |          -0.0024 |         188.0824 |        -126.2266 |
[32m[20221214 14:32:12 @agent_ppo2.py:185][0m |          -0.0023 |         185.0920 |        -126.0093 |
[32m[20221214 14:32:12 @agent_ppo2.py:185][0m |           0.0006 |         184.8677 |        -125.7887 |
[32m[20221214 14:32:13 @agent_ppo2.py:185][0m |          -0.0021 |         180.9726 |        -125.8528 |
[32m[20221214 14:32:13 @agent_ppo2.py:185][0m |          -0.0040 |         179.5386 |        -125.7465 |
[32m[20221214 14:32:13 @agent_ppo2.py:185][0m |           0.0102 |         188.2588 |        -126.0501 |
[32m[20221214 14:32:13 @agent_ppo2.py:185][0m |          -0.0002 |         179.0187 |        -125.9770 |
[32m[20221214 14:32:13 @agent_ppo2.py:185][0m |           0.0006 |         177.9354 |        -125.7894 |
[32m[20221214 14:32:13 @agent_ppo2.py:185][0m |          -0.0011 |         177.0512 |        -125.7990 |
[32m[20221214 14:32:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:32:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.72
[32m[20221214 14:32:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.94
[32m[20221214 14:32:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.65
[32m[20221214 14:32:13 @agent_ppo2.py:143][0m Total time:      34.19 min
[32m[20221214 14:32:13 @agent_ppo2.py:145][0m 3143680 total steps have happened
[32m[20221214 14:32:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1535 --------------------------#
[32m[20221214 14:32:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:14 @agent_ppo2.py:185][0m |           0.0004 |         242.1393 |        -123.2949 |
[32m[20221214 14:32:14 @agent_ppo2.py:185][0m |          -0.0021 |         229.5684 |        -123.8044 |
[32m[20221214 14:32:14 @agent_ppo2.py:185][0m |           0.0113 |         247.2577 |        -123.5856 |
[32m[20221214 14:32:14 @agent_ppo2.py:185][0m |          -0.0021 |         223.7015 |        -123.7959 |
[32m[20221214 14:32:14 @agent_ppo2.py:185][0m |          -0.0026 |         221.8514 |        -123.6732 |
[32m[20221214 14:32:14 @agent_ppo2.py:185][0m |          -0.0024 |         220.8034 |        -123.7598 |
[32m[20221214 14:32:14 @agent_ppo2.py:185][0m |          -0.0023 |         219.6729 |        -123.9550 |
[32m[20221214 14:32:14 @agent_ppo2.py:185][0m |          -0.0029 |         219.2536 |        -123.8968 |
[32m[20221214 14:32:14 @agent_ppo2.py:185][0m |          -0.0026 |         218.6702 |        -123.9294 |
[32m[20221214 14:32:14 @agent_ppo2.py:185][0m |           0.0010 |         220.1315 |        -123.9231 |
[32m[20221214 14:32:14 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:32:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.85
[32m[20221214 14:32:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.50
[32m[20221214 14:32:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.47
[32m[20221214 14:32:15 @agent_ppo2.py:143][0m Total time:      34.21 min
[32m[20221214 14:32:15 @agent_ppo2.py:145][0m 3145728 total steps have happened
[32m[20221214 14:32:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1536 --------------------------#
[32m[20221214 14:32:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:32:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:15 @agent_ppo2.py:185][0m |          -0.0015 |         243.8829 |        -127.9568 |
[32m[20221214 14:32:15 @agent_ppo2.py:185][0m |          -0.0025 |         236.2312 |        -127.9385 |
[32m[20221214 14:32:15 @agent_ppo2.py:185][0m |          -0.0030 |         232.8219 |        -127.9040 |
[32m[20221214 14:32:15 @agent_ppo2.py:185][0m |          -0.0038 |         230.2031 |        -127.9098 |
[32m[20221214 14:32:15 @agent_ppo2.py:185][0m |           0.0086 |         244.4170 |        -127.8751 |
[32m[20221214 14:32:15 @agent_ppo2.py:185][0m |           0.0011 |         230.3214 |        -128.2275 |
[32m[20221214 14:32:15 @agent_ppo2.py:185][0m |          -0.0025 |         226.9972 |        -128.1342 |
[32m[20221214 14:32:16 @agent_ppo2.py:185][0m |          -0.0057 |         225.5041 |        -127.9490 |
[32m[20221214 14:32:16 @agent_ppo2.py:185][0m |          -0.0033 |         225.2511 |        -128.0515 |
[32m[20221214 14:32:16 @agent_ppo2.py:185][0m |          -0.0021 |         224.4677 |        -128.3120 |
[32m[20221214 14:32:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:32:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.59
[32m[20221214 14:32:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.58
[32m[20221214 14:32:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.02
[32m[20221214 14:32:16 @agent_ppo2.py:143][0m Total time:      34.23 min
[32m[20221214 14:32:16 @agent_ppo2.py:145][0m 3147776 total steps have happened
[32m[20221214 14:32:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1537 --------------------------#
[32m[20221214 14:32:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:32:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:16 @agent_ppo2.py:185][0m |          -0.0009 |         214.4303 |        -127.5474 |
[32m[20221214 14:32:16 @agent_ppo2.py:185][0m |          -0.0028 |         207.4877 |        -127.4975 |
[32m[20221214 14:32:16 @agent_ppo2.py:185][0m |          -0.0050 |         201.7571 |        -127.4993 |
[32m[20221214 14:32:16 @agent_ppo2.py:185][0m |          -0.0031 |         198.5347 |        -127.2584 |
[32m[20221214 14:32:17 @agent_ppo2.py:185][0m |           0.0002 |         198.6624 |        -127.6638 |
[32m[20221214 14:32:17 @agent_ppo2.py:185][0m |          -0.0039 |         196.2631 |        -127.4334 |
[32m[20221214 14:32:17 @agent_ppo2.py:185][0m |          -0.0024 |         193.7941 |        -127.6280 |
[32m[20221214 14:32:17 @agent_ppo2.py:185][0m |          -0.0030 |         193.2104 |        -127.6894 |
[32m[20221214 14:32:17 @agent_ppo2.py:185][0m |          -0.0018 |         193.1678 |        -127.7945 |
[32m[20221214 14:32:17 @agent_ppo2.py:185][0m |          -0.0027 |         191.9051 |        -127.7688 |
[32m[20221214 14:32:17 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:32:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.21
[32m[20221214 14:32:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.64
[32m[20221214 14:32:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.70
[32m[20221214 14:32:17 @agent_ppo2.py:143][0m Total time:      34.25 min
[32m[20221214 14:32:17 @agent_ppo2.py:145][0m 3149824 total steps have happened
[32m[20221214 14:32:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1538 --------------------------#
[32m[20221214 14:32:17 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:32:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:18 @agent_ppo2.py:185][0m |          -0.0020 |         241.0477 |        -127.9337 |
[32m[20221214 14:32:18 @agent_ppo2.py:185][0m |          -0.0010 |         230.6094 |        -127.6685 |
[32m[20221214 14:32:18 @agent_ppo2.py:185][0m |          -0.0013 |         226.7405 |        -127.8283 |
[32m[20221214 14:32:18 @agent_ppo2.py:185][0m |           0.0052 |         227.8857 |        -127.9305 |
[32m[20221214 14:32:18 @agent_ppo2.py:185][0m |          -0.0038 |         221.3569 |        -127.8349 |
[32m[20221214 14:32:18 @agent_ppo2.py:185][0m |          -0.0037 |         218.3810 |        -127.8517 |
[32m[20221214 14:32:18 @agent_ppo2.py:185][0m |           0.0076 |         224.8266 |        -127.8661 |
[32m[20221214 14:32:18 @agent_ppo2.py:185][0m |          -0.0020 |         211.1885 |        -127.5380 |
[32m[20221214 14:32:18 @agent_ppo2.py:185][0m |          -0.0043 |         209.3956 |        -127.9468 |
[32m[20221214 14:32:19 @agent_ppo2.py:185][0m |          -0.0002 |         210.2576 |        -127.8192 |
[32m[20221214 14:32:19 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:32:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.15
[32m[20221214 14:32:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.23
[32m[20221214 14:32:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.21
[32m[20221214 14:32:19 @agent_ppo2.py:143][0m Total time:      34.28 min
[32m[20221214 14:32:19 @agent_ppo2.py:145][0m 3151872 total steps have happened
[32m[20221214 14:32:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1539 --------------------------#
[32m[20221214 14:32:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:19 @agent_ppo2.py:185][0m |          -0.0033 |         238.3303 |        -125.8155 |
[32m[20221214 14:32:19 @agent_ppo2.py:185][0m |          -0.0023 |         230.1922 |        -125.4911 |
[32m[20221214 14:32:19 @agent_ppo2.py:185][0m |           0.0007 |         226.5561 |        -125.6810 |
[32m[20221214 14:32:19 @agent_ppo2.py:185][0m |           0.0010 |         223.0035 |        -125.7471 |
[32m[20221214 14:32:19 @agent_ppo2.py:185][0m |          -0.0033 |         219.2198 |        -125.6573 |
[32m[20221214 14:32:20 @agent_ppo2.py:185][0m |          -0.0026 |         215.3342 |        -125.7213 |
[32m[20221214 14:32:20 @agent_ppo2.py:185][0m |           0.0081 |         229.7750 |        -126.1083 |
[32m[20221214 14:32:20 @agent_ppo2.py:185][0m |          -0.0017 |         210.0744 |        -125.9194 |
[32m[20221214 14:32:20 @agent_ppo2.py:185][0m |          -0.0028 |         207.9723 |        -126.0919 |
[32m[20221214 14:32:20 @agent_ppo2.py:185][0m |          -0.0040 |         205.9409 |        -125.7549 |
[32m[20221214 14:32:20 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.24
[32m[20221214 14:32:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.07
[32m[20221214 14:32:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.66
[32m[20221214 14:32:20 @agent_ppo2.py:143][0m Total time:      34.30 min
[32m[20221214 14:32:20 @agent_ppo2.py:145][0m 3153920 total steps have happened
[32m[20221214 14:32:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1540 --------------------------#
[32m[20221214 14:32:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:32:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:20 @agent_ppo2.py:185][0m |          -0.0029 |         223.4791 |        -127.7283 |
[32m[20221214 14:32:21 @agent_ppo2.py:185][0m |          -0.0028 |         219.8634 |        -127.6816 |
[32m[20221214 14:32:21 @agent_ppo2.py:185][0m |          -0.0004 |         218.4531 |        -127.3396 |
[32m[20221214 14:32:21 @agent_ppo2.py:185][0m |           0.0062 |         227.2856 |        -127.7344 |
[32m[20221214 14:32:21 @agent_ppo2.py:185][0m |           0.0001 |         217.1481 |        -127.9384 |
[32m[20221214 14:32:21 @agent_ppo2.py:185][0m |          -0.0003 |         215.2962 |        -127.7989 |
[32m[20221214 14:32:21 @agent_ppo2.py:185][0m |          -0.0016 |         215.8117 |        -127.6334 |
[32m[20221214 14:32:21 @agent_ppo2.py:185][0m |          -0.0024 |         214.7551 |        -127.8476 |
[32m[20221214 14:32:21 @agent_ppo2.py:185][0m |          -0.0032 |         214.1248 |        -127.8808 |
[32m[20221214 14:32:21 @agent_ppo2.py:185][0m |          -0.0041 |         214.7210 |        -127.7353 |
[32m[20221214 14:32:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:32:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.12
[32m[20221214 14:32:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.58
[32m[20221214 14:32:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.36
[32m[20221214 14:32:21 @agent_ppo2.py:143][0m Total time:      34.32 min
[32m[20221214 14:32:21 @agent_ppo2.py:145][0m 3155968 total steps have happened
[32m[20221214 14:32:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1541 --------------------------#
[32m[20221214 14:32:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:22 @agent_ppo2.py:185][0m |           0.0084 |         260.9894 |        -129.7678 |
[32m[20221214 14:32:22 @agent_ppo2.py:185][0m |           0.0104 |         250.7572 |        -129.4667 |
[32m[20221214 14:32:22 @agent_ppo2.py:185][0m |          -0.0021 |         229.1870 |        -129.4972 |
[32m[20221214 14:32:22 @agent_ppo2.py:185][0m |          -0.0026 |         225.0596 |        -129.3358 |
[32m[20221214 14:32:22 @agent_ppo2.py:185][0m |          -0.0031 |         223.5477 |        -129.3235 |
[32m[20221214 14:32:22 @agent_ppo2.py:185][0m |          -0.0017 |         220.4739 |        -129.5794 |
[32m[20221214 14:32:22 @agent_ppo2.py:185][0m |          -0.0030 |         219.4491 |        -129.3471 |
[32m[20221214 14:32:22 @agent_ppo2.py:185][0m |          -0.0006 |         217.7238 |        -129.2681 |
[32m[20221214 14:32:23 @agent_ppo2.py:185][0m |          -0.0033 |         217.0490 |        -129.5219 |
[32m[20221214 14:32:23 @agent_ppo2.py:185][0m |          -0.0034 |         215.8813 |        -129.4731 |
[32m[20221214 14:32:23 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.33
[32m[20221214 14:32:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.28
[32m[20221214 14:32:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.48
[32m[20221214 14:32:23 @agent_ppo2.py:143][0m Total time:      34.35 min
[32m[20221214 14:32:23 @agent_ppo2.py:145][0m 3158016 total steps have happened
[32m[20221214 14:32:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1542 --------------------------#
[32m[20221214 14:32:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:23 @agent_ppo2.py:185][0m |          -0.0018 |         232.2908 |        -128.6058 |
[32m[20221214 14:32:23 @agent_ppo2.py:185][0m |          -0.0051 |         218.0548 |        -128.2486 |
[32m[20221214 14:32:23 @agent_ppo2.py:185][0m |           0.0067 |         214.3670 |        -128.3147 |
[32m[20221214 14:32:23 @agent_ppo2.py:185][0m |           0.0029 |         213.1363 |        -128.1894 |
[32m[20221214 14:32:24 @agent_ppo2.py:185][0m |           0.0053 |         222.2161 |        -127.6981 |
[32m[20221214 14:32:24 @agent_ppo2.py:185][0m |          -0.0045 |         205.1705 |        -127.4581 |
[32m[20221214 14:32:24 @agent_ppo2.py:185][0m |          -0.0045 |         202.6863 |        -127.7213 |
[32m[20221214 14:32:24 @agent_ppo2.py:185][0m |          -0.0044 |         200.0147 |        -127.1539 |
[32m[20221214 14:32:24 @agent_ppo2.py:185][0m |          -0.0053 |         197.9425 |        -127.3932 |
[32m[20221214 14:32:24 @agent_ppo2.py:185][0m |          -0.0027 |         197.3005 |        -127.2218 |
[32m[20221214 14:32:24 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:32:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.57
[32m[20221214 14:32:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.65
[32m[20221214 14:32:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.43
[32m[20221214 14:32:24 @agent_ppo2.py:143][0m Total time:      34.37 min
[32m[20221214 14:32:24 @agent_ppo2.py:145][0m 3160064 total steps have happened
[32m[20221214 14:32:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1543 --------------------------#
[32m[20221214 14:32:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:24 @agent_ppo2.py:185][0m |          -0.0024 |         231.0876 |        -127.3821 |
[32m[20221214 14:32:25 @agent_ppo2.py:185][0m |           0.0112 |         240.9457 |        -127.1491 |
[32m[20221214 14:32:25 @agent_ppo2.py:185][0m |          -0.0013 |         218.4329 |        -127.1372 |
[32m[20221214 14:32:25 @agent_ppo2.py:185][0m |           0.0005 |         217.0691 |        -127.2223 |
[32m[20221214 14:32:25 @agent_ppo2.py:185][0m |          -0.0018 |         214.2208 |        -126.7993 |
[32m[20221214 14:32:25 @agent_ppo2.py:185][0m |          -0.0017 |         212.3947 |        -127.2716 |
[32m[20221214 14:32:25 @agent_ppo2.py:185][0m |          -0.0011 |         212.1157 |        -127.2901 |
[32m[20221214 14:32:25 @agent_ppo2.py:185][0m |          -0.0014 |         211.2251 |        -127.2240 |
[32m[20221214 14:32:25 @agent_ppo2.py:185][0m |          -0.0044 |         210.6264 |        -127.1668 |
[32m[20221214 14:32:25 @agent_ppo2.py:185][0m |          -0.0020 |         209.5771 |        -127.1437 |
[32m[20221214 14:32:25 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.63
[32m[20221214 14:32:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.48
[32m[20221214 14:32:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.57
[32m[20221214 14:32:26 @agent_ppo2.py:143][0m Total time:      34.39 min
[32m[20221214 14:32:26 @agent_ppo2.py:145][0m 3162112 total steps have happened
[32m[20221214 14:32:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1544 --------------------------#
[32m[20221214 14:32:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:26 @agent_ppo2.py:185][0m |           0.0106 |         247.6767 |        -127.2555 |
[32m[20221214 14:32:26 @agent_ppo2.py:185][0m |          -0.0032 |         217.6707 |        -127.2396 |
[32m[20221214 14:32:26 @agent_ppo2.py:185][0m |          -0.0019 |         213.6944 |        -126.7967 |
[32m[20221214 14:32:26 @agent_ppo2.py:185][0m |          -0.0003 |         212.9317 |        -127.0830 |
[32m[20221214 14:32:26 @agent_ppo2.py:185][0m |          -0.0035 |         209.9418 |        -126.7692 |
[32m[20221214 14:32:26 @agent_ppo2.py:185][0m |          -0.0016 |         208.0572 |        -127.0456 |
[32m[20221214 14:32:26 @agent_ppo2.py:185][0m |          -0.0002 |         207.3048 |        -127.0726 |
[32m[20221214 14:32:27 @agent_ppo2.py:185][0m |          -0.0019 |         206.7925 |        -127.1599 |
[32m[20221214 14:32:27 @agent_ppo2.py:185][0m |          -0.0005 |         205.4688 |        -126.9430 |
[32m[20221214 14:32:27 @agent_ppo2.py:185][0m |          -0.0029 |         204.8174 |        -127.4140 |
[32m[20221214 14:32:27 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:32:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.71
[32m[20221214 14:32:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.17
[32m[20221214 14:32:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.93
[32m[20221214 14:32:27 @agent_ppo2.py:143][0m Total time:      34.41 min
[32m[20221214 14:32:27 @agent_ppo2.py:145][0m 3164160 total steps have happened
[32m[20221214 14:32:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1545 --------------------------#
[32m[20221214 14:32:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:27 @agent_ppo2.py:185][0m |          -0.0026 |         239.8010 |        -125.5768 |
[32m[20221214 14:32:27 @agent_ppo2.py:185][0m |           0.0110 |         262.2755 |        -125.8082 |
[32m[20221214 14:32:27 @agent_ppo2.py:185][0m |           0.0090 |         241.3495 |        -125.5850 |
[32m[20221214 14:32:27 @agent_ppo2.py:185][0m |          -0.0003 |         221.2568 |        -125.6968 |
[32m[20221214 14:32:28 @agent_ppo2.py:185][0m |          -0.0016 |         219.5664 |        -125.6336 |
[32m[20221214 14:32:28 @agent_ppo2.py:185][0m |          -0.0020 |         217.1867 |        -125.7353 |
[32m[20221214 14:32:28 @agent_ppo2.py:185][0m |          -0.0024 |         216.7401 |        -125.8573 |
[32m[20221214 14:32:28 @agent_ppo2.py:185][0m |           0.0215 |         251.5414 |        -125.8856 |
[32m[20221214 14:32:28 @agent_ppo2.py:185][0m |          -0.0026 |         217.7927 |        -125.7478 |
[32m[20221214 14:32:28 @agent_ppo2.py:185][0m |          -0.0016 |         214.2051 |        -125.8268 |
[32m[20221214 14:32:28 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:32:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.97
[32m[20221214 14:32:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.52
[32m[20221214 14:32:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.35
[32m[20221214 14:32:28 @agent_ppo2.py:143][0m Total time:      34.44 min
[32m[20221214 14:32:28 @agent_ppo2.py:145][0m 3166208 total steps have happened
[32m[20221214 14:32:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1546 --------------------------#
[32m[20221214 14:32:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:29 @agent_ppo2.py:185][0m |          -0.0006 |         257.2137 |        -128.7931 |
[32m[20221214 14:32:29 @agent_ppo2.py:185][0m |          -0.0012 |         231.5650 |        -128.7087 |
[32m[20221214 14:32:29 @agent_ppo2.py:185][0m |          -0.0021 |         224.0427 |        -128.7591 |
[32m[20221214 14:32:29 @agent_ppo2.py:185][0m |           0.0001 |         219.1246 |        -128.5691 |
[32m[20221214 14:32:29 @agent_ppo2.py:185][0m |          -0.0029 |         216.8676 |        -128.6257 |
[32m[20221214 14:32:29 @agent_ppo2.py:185][0m |          -0.0031 |         214.1395 |        -128.4905 |
[32m[20221214 14:32:29 @agent_ppo2.py:185][0m |          -0.0022 |         212.3926 |        -128.3880 |
[32m[20221214 14:32:29 @agent_ppo2.py:185][0m |          -0.0031 |         211.9247 |        -128.4936 |
[32m[20221214 14:32:29 @agent_ppo2.py:185][0m |          -0.0010 |         210.8386 |        -128.6028 |
[32m[20221214 14:32:29 @agent_ppo2.py:185][0m |          -0.0018 |         209.2659 |        -128.3902 |
[32m[20221214 14:32:29 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.64
[32m[20221214 14:32:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.96
[32m[20221214 14:32:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.61
[32m[20221214 14:32:30 @agent_ppo2.py:143][0m Total time:      34.46 min
[32m[20221214 14:32:30 @agent_ppo2.py:145][0m 3168256 total steps have happened
[32m[20221214 14:32:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1547 --------------------------#
[32m[20221214 14:32:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:30 @agent_ppo2.py:185][0m |           0.0018 |         236.2027 |        -128.4755 |
[32m[20221214 14:32:30 @agent_ppo2.py:185][0m |          -0.0024 |         217.5123 |        -128.3517 |
[32m[20221214 14:32:30 @agent_ppo2.py:185][0m |           0.0098 |         222.5103 |        -128.3988 |
[32m[20221214 14:32:30 @agent_ppo2.py:185][0m |          -0.0012 |         207.7425 |        -128.5968 |
[32m[20221214 14:32:30 @agent_ppo2.py:185][0m |           0.0045 |         210.6687 |        -128.1887 |
[32m[20221214 14:32:30 @agent_ppo2.py:185][0m |           0.0046 |         206.9789 |        -128.3879 |
[32m[20221214 14:32:30 @agent_ppo2.py:185][0m |          -0.0036 |         202.2395 |        -128.1150 |
[32m[20221214 14:32:31 @agent_ppo2.py:185][0m |          -0.0030 |         200.8405 |        -128.4497 |
[32m[20221214 14:32:31 @agent_ppo2.py:185][0m |          -0.0009 |         200.9970 |        -128.3102 |
[32m[20221214 14:32:31 @agent_ppo2.py:185][0m |           0.0053 |         204.5288 |        -128.4003 |
[32m[20221214 14:32:31 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.71
[32m[20221214 14:32:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.48
[32m[20221214 14:32:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.93
[32m[20221214 14:32:31 @agent_ppo2.py:143][0m Total time:      34.48 min
[32m[20221214 14:32:31 @agent_ppo2.py:145][0m 3170304 total steps have happened
[32m[20221214 14:32:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1548 --------------------------#
[32m[20221214 14:32:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:31 @agent_ppo2.py:185][0m |          -0.0011 |         210.2288 |        -126.3673 |
[32m[20221214 14:32:31 @agent_ppo2.py:185][0m |          -0.0013 |         195.7353 |        -126.6430 |
[32m[20221214 14:32:31 @agent_ppo2.py:185][0m |          -0.0033 |         190.2225 |        -126.3332 |
[32m[20221214 14:32:32 @agent_ppo2.py:185][0m |          -0.0047 |         187.1545 |        -126.4719 |
[32m[20221214 14:32:32 @agent_ppo2.py:185][0m |          -0.0022 |         183.8691 |        -126.4867 |
[32m[20221214 14:32:32 @agent_ppo2.py:185][0m |           0.0043 |         188.7113 |        -126.2145 |
[32m[20221214 14:32:32 @agent_ppo2.py:185][0m |           0.0011 |         180.8194 |        -126.2041 |
[32m[20221214 14:32:32 @agent_ppo2.py:185][0m |          -0.0067 |         179.2308 |        -125.9899 |
[32m[20221214 14:32:32 @agent_ppo2.py:185][0m |          -0.0054 |         178.3057 |        -126.1451 |
[32m[20221214 14:32:32 @agent_ppo2.py:185][0m |          -0.0049 |         176.7252 |        -126.1987 |
[32m[20221214 14:32:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.23
[32m[20221214 14:32:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.20
[32m[20221214 14:32:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.73
[32m[20221214 14:32:32 @agent_ppo2.py:143][0m Total time:      34.50 min
[32m[20221214 14:32:32 @agent_ppo2.py:145][0m 3172352 total steps have happened
[32m[20221214 14:32:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1549 --------------------------#
[32m[20221214 14:32:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:33 @agent_ppo2.py:185][0m |          -0.0009 |         221.9185 |        -128.5212 |
[32m[20221214 14:32:33 @agent_ppo2.py:185][0m |          -0.0031 |         195.7919 |        -128.0975 |
[32m[20221214 14:32:33 @agent_ppo2.py:185][0m |          -0.0008 |         191.0340 |        -128.5986 |
[32m[20221214 14:32:33 @agent_ppo2.py:185][0m |          -0.0008 |         186.6165 |        -128.0969 |
[32m[20221214 14:32:33 @agent_ppo2.py:185][0m |          -0.0008 |         184.0657 |        -128.1531 |
[32m[20221214 14:32:33 @agent_ppo2.py:185][0m |          -0.0042 |         181.6390 |        -128.1994 |
[32m[20221214 14:32:33 @agent_ppo2.py:185][0m |          -0.0052 |         179.8463 |        -127.9968 |
[32m[20221214 14:32:33 @agent_ppo2.py:185][0m |          -0.0028 |         177.7614 |        -128.1007 |
[32m[20221214 14:32:33 @agent_ppo2.py:185][0m |          -0.0050 |         175.5859 |        -128.0955 |
[32m[20221214 14:32:33 @agent_ppo2.py:185][0m |           0.0067 |         180.6724 |        -127.9571 |
[32m[20221214 14:32:33 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.69
[32m[20221214 14:32:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.02
[32m[20221214 14:32:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.32
[32m[20221214 14:32:34 @agent_ppo2.py:143][0m Total time:      34.53 min
[32m[20221214 14:32:34 @agent_ppo2.py:145][0m 3174400 total steps have happened
[32m[20221214 14:32:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1550 --------------------------#
[32m[20221214 14:32:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:32:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:34 @agent_ppo2.py:185][0m |           0.0058 |         225.7424 |        -125.3892 |
[32m[20221214 14:32:34 @agent_ppo2.py:185][0m |          -0.0073 |         195.7451 |        -125.3381 |
[32m[20221214 14:32:34 @agent_ppo2.py:185][0m |          -0.0002 |         193.1474 |        -125.3666 |
[32m[20221214 14:32:34 @agent_ppo2.py:185][0m |          -0.0083 |         185.7602 |        -125.0076 |
[32m[20221214 14:32:34 @agent_ppo2.py:185][0m |          -0.0016 |         184.5332 |        -125.2812 |
[32m[20221214 14:32:34 @agent_ppo2.py:185][0m |          -0.0034 |         182.1652 |        -125.2878 |
[32m[20221214 14:32:35 @agent_ppo2.py:185][0m |           0.0020 |         192.3260 |        -125.2804 |
[32m[20221214 14:32:35 @agent_ppo2.py:185][0m |          -0.0041 |         181.0428 |        -125.3519 |
[32m[20221214 14:32:35 @agent_ppo2.py:185][0m |          -0.0037 |         179.1750 |        -125.2538 |
[32m[20221214 14:32:35 @agent_ppo2.py:185][0m |          -0.0002 |         179.7000 |        -125.4099 |
[32m[20221214 14:32:35 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 803.31
[32m[20221214 14:32:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.84
[32m[20221214 14:32:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.74
[32m[20221214 14:32:35 @agent_ppo2.py:143][0m Total time:      34.55 min
[32m[20221214 14:32:35 @agent_ppo2.py:145][0m 3176448 total steps have happened
[32m[20221214 14:32:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1551 --------------------------#
[32m[20221214 14:32:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:35 @agent_ppo2.py:185][0m |           0.0015 |         176.0602 |        -126.4876 |
[32m[20221214 14:32:35 @agent_ppo2.py:185][0m |          -0.0014 |         149.3329 |        -126.6214 |
[32m[20221214 14:32:35 @agent_ppo2.py:185][0m |          -0.0040 |         142.2600 |        -126.4350 |
[32m[20221214 14:32:36 @agent_ppo2.py:185][0m |          -0.0026 |         137.4917 |        -126.5932 |
[32m[20221214 14:32:36 @agent_ppo2.py:185][0m |          -0.0050 |         135.3102 |        -126.7047 |
[32m[20221214 14:32:36 @agent_ppo2.py:185][0m |           0.0044 |         144.9922 |        -126.9271 |
[32m[20221214 14:32:36 @agent_ppo2.py:185][0m |           0.0038 |         134.6568 |        -126.6001 |
[32m[20221214 14:32:36 @agent_ppo2.py:185][0m |          -0.0060 |         129.5864 |        -126.8967 |
[32m[20221214 14:32:36 @agent_ppo2.py:185][0m |          -0.0031 |         128.2146 |        -127.1226 |
[32m[20221214 14:32:36 @agent_ppo2.py:185][0m |          -0.0026 |         127.2994 |        -127.4697 |
[32m[20221214 14:32:36 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:32:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.42
[32m[20221214 14:32:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.21
[32m[20221214 14:32:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.66
[32m[20221214 14:32:36 @agent_ppo2.py:143][0m Total time:      34.57 min
[32m[20221214 14:32:36 @agent_ppo2.py:145][0m 3178496 total steps have happened
[32m[20221214 14:32:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1552 --------------------------#
[32m[20221214 14:32:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:37 @agent_ppo2.py:185][0m |           0.0116 |         173.3554 |        -128.9768 |
[32m[20221214 14:32:37 @agent_ppo2.py:185][0m |          -0.0005 |         141.4099 |        -128.7021 |
[32m[20221214 14:32:37 @agent_ppo2.py:185][0m |           0.0033 |         137.4284 |        -128.8884 |
[32m[20221214 14:32:37 @agent_ppo2.py:185][0m |           0.0247 |         167.3791 |        -128.9270 |
[32m[20221214 14:32:37 @agent_ppo2.py:185][0m |          -0.0044 |         127.6776 |        -129.0091 |
[32m[20221214 14:32:37 @agent_ppo2.py:185][0m |          -0.0004 |         123.4038 |        -128.8527 |
[32m[20221214 14:32:37 @agent_ppo2.py:185][0m |          -0.0013 |         121.0509 |        -128.9364 |
[32m[20221214 14:32:37 @agent_ppo2.py:185][0m |          -0.0017 |         119.2013 |        -128.8161 |
[32m[20221214 14:32:37 @agent_ppo2.py:185][0m |          -0.0036 |         118.1182 |        -128.6793 |
[32m[20221214 14:32:37 @agent_ppo2.py:185][0m |          -0.0018 |         116.5083 |        -128.8359 |
[32m[20221214 14:32:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:32:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.88
[32m[20221214 14:32:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.40
[32m[20221214 14:32:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.07
[32m[20221214 14:32:38 @agent_ppo2.py:143][0m Total time:      34.59 min
[32m[20221214 14:32:38 @agent_ppo2.py:145][0m 3180544 total steps have happened
[32m[20221214 14:32:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1553 --------------------------#
[32m[20221214 14:32:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:38 @agent_ppo2.py:185][0m |           0.0064 |         189.5922 |        -129.0077 |
[32m[20221214 14:32:38 @agent_ppo2.py:185][0m |           0.0023 |         177.5846 |        -128.8643 |
[32m[20221214 14:32:38 @agent_ppo2.py:185][0m |          -0.0042 |         165.5652 |        -128.6811 |
[32m[20221214 14:32:38 @agent_ppo2.py:185][0m |          -0.0054 |         161.7243 |        -128.8477 |
[32m[20221214 14:32:38 @agent_ppo2.py:185][0m |          -0.0030 |         159.4923 |        -128.9051 |
[32m[20221214 14:32:38 @agent_ppo2.py:185][0m |          -0.0031 |         157.9721 |        -128.6473 |
[32m[20221214 14:32:39 @agent_ppo2.py:185][0m |          -0.0051 |         156.0047 |        -128.7604 |
[32m[20221214 14:32:39 @agent_ppo2.py:185][0m |           0.0027 |         164.1073 |        -128.8610 |
[32m[20221214 14:32:39 @agent_ppo2.py:185][0m |          -0.0046 |         152.6718 |        -128.5244 |
[32m[20221214 14:32:39 @agent_ppo2.py:185][0m |           0.0052 |         156.6642 |        -129.0098 |
[32m[20221214 14:32:39 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.44
[32m[20221214 14:32:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.40
[32m[20221214 14:32:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.04
[32m[20221214 14:32:39 @agent_ppo2.py:143][0m Total time:      34.62 min
[32m[20221214 14:32:39 @agent_ppo2.py:145][0m 3182592 total steps have happened
[32m[20221214 14:32:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1554 --------------------------#
[32m[20221214 14:32:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:39 @agent_ppo2.py:185][0m |          -0.0007 |         245.3163 |        -127.6090 |
[32m[20221214 14:32:39 @agent_ppo2.py:185][0m |          -0.0018 |         236.2978 |        -127.3376 |
[32m[20221214 14:32:40 @agent_ppo2.py:185][0m |          -0.0036 |         234.0546 |        -127.3807 |
[32m[20221214 14:32:40 @agent_ppo2.py:185][0m |          -0.0040 |         231.4051 |        -127.2073 |
[32m[20221214 14:32:40 @agent_ppo2.py:185][0m |           0.0051 |         240.3110 |        -127.3535 |
[32m[20221214 14:32:40 @agent_ppo2.py:185][0m |          -0.0018 |         230.7425 |        -127.1838 |
[32m[20221214 14:32:40 @agent_ppo2.py:185][0m |          -0.0019 |         228.4356 |        -127.2056 |
[32m[20221214 14:32:40 @agent_ppo2.py:185][0m |          -0.0035 |         226.9849 |        -127.0762 |
[32m[20221214 14:32:40 @agent_ppo2.py:185][0m |          -0.0049 |         225.9161 |        -127.2927 |
[32m[20221214 14:32:40 @agent_ppo2.py:185][0m |          -0.0009 |         228.3977 |        -127.1292 |
[32m[20221214 14:32:40 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.22
[32m[20221214 14:32:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.79
[32m[20221214 14:32:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.28
[32m[20221214 14:32:40 @agent_ppo2.py:143][0m Total time:      34.64 min
[32m[20221214 14:32:40 @agent_ppo2.py:145][0m 3184640 total steps have happened
[32m[20221214 14:32:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1555 --------------------------#
[32m[20221214 14:32:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:41 @agent_ppo2.py:185][0m |          -0.0040 |         242.8383 |        -126.3780 |
[32m[20221214 14:32:41 @agent_ppo2.py:185][0m |          -0.0031 |         232.3197 |        -126.1164 |
[32m[20221214 14:32:41 @agent_ppo2.py:185][0m |           0.0044 |         242.4178 |        -126.2669 |
[32m[20221214 14:32:41 @agent_ppo2.py:185][0m |          -0.0023 |         228.5775 |        -126.1032 |
[32m[20221214 14:32:41 @agent_ppo2.py:185][0m |          -0.0047 |         225.6096 |        -126.6078 |
[32m[20221214 14:32:41 @agent_ppo2.py:185][0m |          -0.0055 |         224.7850 |        -126.4756 |
[32m[20221214 14:32:41 @agent_ppo2.py:185][0m |          -0.0038 |         223.5113 |        -126.5479 |
[32m[20221214 14:32:41 @agent_ppo2.py:185][0m |          -0.0048 |         221.9741 |        -126.1600 |
[32m[20221214 14:32:41 @agent_ppo2.py:185][0m |          -0.0063 |         221.1851 |        -126.5768 |
[32m[20221214 14:32:42 @agent_ppo2.py:185][0m |          -0.0049 |         220.4992 |        -126.3634 |
[32m[20221214 14:32:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.36
[32m[20221214 14:32:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.18
[32m[20221214 14:32:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.47
[32m[20221214 14:32:42 @agent_ppo2.py:143][0m Total time:      34.66 min
[32m[20221214 14:32:42 @agent_ppo2.py:145][0m 3186688 total steps have happened
[32m[20221214 14:32:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1556 --------------------------#
[32m[20221214 14:32:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:42 @agent_ppo2.py:185][0m |          -0.0023 |         238.7057 |        -127.0898 |
[32m[20221214 14:32:42 @agent_ppo2.py:185][0m |           0.0008 |         230.3851 |        -127.2846 |
[32m[20221214 14:32:42 @agent_ppo2.py:185][0m |          -0.0018 |         221.4682 |        -127.3097 |
[32m[20221214 14:32:42 @agent_ppo2.py:185][0m |          -0.0035 |         217.5136 |        -127.2735 |
[32m[20221214 14:32:42 @agent_ppo2.py:185][0m |          -0.0039 |         214.5066 |        -127.3173 |
[32m[20221214 14:32:43 @agent_ppo2.py:185][0m |          -0.0044 |         211.5407 |        -127.1929 |
[32m[20221214 14:32:43 @agent_ppo2.py:185][0m |          -0.0029 |         208.9901 |        -127.0487 |
[32m[20221214 14:32:43 @agent_ppo2.py:185][0m |          -0.0039 |         208.0946 |        -127.3445 |
[32m[20221214 14:32:43 @agent_ppo2.py:185][0m |           0.0110 |         237.8736 |        -127.3287 |
[32m[20221214 14:32:43 @agent_ppo2.py:185][0m |          -0.0040 |         206.9207 |        -127.3325 |
[32m[20221214 14:32:43 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:32:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.23
[32m[20221214 14:32:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.07
[32m[20221214 14:32:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.45
[32m[20221214 14:32:43 @agent_ppo2.py:143][0m Total time:      34.68 min
[32m[20221214 14:32:43 @agent_ppo2.py:145][0m 3188736 total steps have happened
[32m[20221214 14:32:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1557 --------------------------#
[32m[20221214 14:32:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:43 @agent_ppo2.py:185][0m |          -0.0020 |         232.4323 |        -126.6696 |
[32m[20221214 14:32:44 @agent_ppo2.py:185][0m |          -0.0009 |         226.0829 |        -126.6610 |
[32m[20221214 14:32:44 @agent_ppo2.py:185][0m |           0.0026 |         225.2980 |        -126.6924 |
[32m[20221214 14:32:44 @agent_ppo2.py:185][0m |           0.0065 |         233.0272 |        -126.8519 |
[32m[20221214 14:32:44 @agent_ppo2.py:185][0m |           0.0219 |         259.5903 |        -126.6791 |
[32m[20221214 14:32:44 @agent_ppo2.py:185][0m |          -0.0001 |         221.3863 |        -126.9550 |
[32m[20221214 14:32:44 @agent_ppo2.py:185][0m |          -0.0016 |         219.5840 |        -126.8328 |
[32m[20221214 14:32:44 @agent_ppo2.py:185][0m |           0.0092 |         240.6833 |        -127.1440 |
[32m[20221214 14:32:44 @agent_ppo2.py:185][0m |           0.0137 |         250.4064 |        -127.0601 |
[32m[20221214 14:32:44 @agent_ppo2.py:185][0m |           0.0048 |         225.3653 |        -126.7874 |
[32m[20221214 14:32:44 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.95
[32m[20221214 14:32:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.25
[32m[20221214 14:32:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.90
[32m[20221214 14:32:44 @agent_ppo2.py:143][0m Total time:      34.71 min
[32m[20221214 14:32:44 @agent_ppo2.py:145][0m 3190784 total steps have happened
[32m[20221214 14:32:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1558 --------------------------#
[32m[20221214 14:32:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:45 @agent_ppo2.py:185][0m |           0.0197 |         312.5616 |        -129.5627 |
[32m[20221214 14:32:45 @agent_ppo2.py:185][0m |          -0.0025 |         261.1422 |        -129.5045 |
[32m[20221214 14:32:45 @agent_ppo2.py:185][0m |          -0.0018 |         258.3474 |        -129.3567 |
[32m[20221214 14:32:45 @agent_ppo2.py:185][0m |          -0.0016 |         256.5108 |        -129.3825 |
[32m[20221214 14:32:45 @agent_ppo2.py:185][0m |           0.0007 |         255.9136 |        -129.7334 |
[32m[20221214 14:32:45 @agent_ppo2.py:185][0m |           0.0018 |         256.1701 |        -129.5134 |
[32m[20221214 14:32:45 @agent_ppo2.py:185][0m |          -0.0021 |         253.6597 |        -129.3110 |
[32m[20221214 14:32:45 @agent_ppo2.py:185][0m |          -0.0020 |         253.1467 |        -129.4070 |
[32m[20221214 14:32:46 @agent_ppo2.py:185][0m |          -0.0008 |         252.6779 |        -129.3916 |
[32m[20221214 14:32:46 @agent_ppo2.py:185][0m |           0.0013 |         252.7000 |        -129.4819 |
[32m[20221214 14:32:46 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.78
[32m[20221214 14:32:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.83
[32m[20221214 14:32:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.60
[32m[20221214 14:32:46 @agent_ppo2.py:143][0m Total time:      34.73 min
[32m[20221214 14:32:46 @agent_ppo2.py:145][0m 3192832 total steps have happened
[32m[20221214 14:32:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1559 --------------------------#
[32m[20221214 14:32:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:46 @agent_ppo2.py:185][0m |           0.0122 |         292.8942 |        -127.0256 |
[32m[20221214 14:32:46 @agent_ppo2.py:185][0m |          -0.0029 |         255.8168 |        -126.9829 |
[32m[20221214 14:32:46 @agent_ppo2.py:185][0m |          -0.0050 |         253.0187 |        -126.7533 |
[32m[20221214 14:32:46 @agent_ppo2.py:185][0m |          -0.0051 |         252.0068 |        -126.8816 |
[32m[20221214 14:32:46 @agent_ppo2.py:185][0m |          -0.0055 |         251.0824 |        -126.7734 |
[32m[20221214 14:32:47 @agent_ppo2.py:185][0m |          -0.0050 |         250.2769 |        -126.4383 |
[32m[20221214 14:32:47 @agent_ppo2.py:185][0m |          -0.0054 |         249.9272 |        -126.5745 |
[32m[20221214 14:32:47 @agent_ppo2.py:185][0m |          -0.0046 |         249.2030 |        -126.3647 |
[32m[20221214 14:32:47 @agent_ppo2.py:185][0m |          -0.0045 |         249.0539 |        -126.5062 |
[32m[20221214 14:32:47 @agent_ppo2.py:185][0m |          -0.0067 |         248.2971 |        -126.4098 |
[32m[20221214 14:32:47 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:32:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.35
[32m[20221214 14:32:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.24
[32m[20221214 14:32:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.23
[32m[20221214 14:32:47 @agent_ppo2.py:143][0m Total time:      34.75 min
[32m[20221214 14:32:47 @agent_ppo2.py:145][0m 3194880 total steps have happened
[32m[20221214 14:32:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1560 --------------------------#
[32m[20221214 14:32:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:32:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:47 @agent_ppo2.py:185][0m |           0.0004 |         249.5145 |        -127.9905 |
[32m[20221214 14:32:48 @agent_ppo2.py:185][0m |          -0.0021 |         246.8808 |        -127.9678 |
[32m[20221214 14:32:48 @agent_ppo2.py:185][0m |          -0.0005 |         245.8872 |        -128.3612 |
[32m[20221214 14:32:48 @agent_ppo2.py:185][0m |          -0.0007 |         245.1863 |        -128.4074 |
[32m[20221214 14:32:48 @agent_ppo2.py:185][0m |          -0.0012 |         244.5427 |        -127.9097 |
[32m[20221214 14:32:48 @agent_ppo2.py:185][0m |           0.0046 |         248.1056 |        -128.7173 |
[32m[20221214 14:32:48 @agent_ppo2.py:185][0m |           0.0031 |         244.8500 |        -128.3189 |
[32m[20221214 14:32:48 @agent_ppo2.py:185][0m |          -0.0036 |         241.4640 |        -128.4752 |
[32m[20221214 14:32:48 @agent_ppo2.py:185][0m |          -0.0021 |         240.8353 |        -128.3624 |
[32m[20221214 14:32:48 @agent_ppo2.py:185][0m |          -0.0018 |         240.7279 |        -128.7666 |
[32m[20221214 14:32:48 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.46
[32m[20221214 14:32:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.93
[32m[20221214 14:32:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.60
[32m[20221214 14:32:49 @agent_ppo2.py:143][0m Total time:      34.77 min
[32m[20221214 14:32:49 @agent_ppo2.py:145][0m 3196928 total steps have happened
[32m[20221214 14:32:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1561 --------------------------#
[32m[20221214 14:32:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:49 @agent_ppo2.py:185][0m |           0.0053 |         256.7672 |        -128.7074 |
[32m[20221214 14:32:49 @agent_ppo2.py:185][0m |          -0.0026 |         244.4199 |        -128.1268 |
[32m[20221214 14:32:49 @agent_ppo2.py:185][0m |          -0.0011 |         241.6023 |        -128.6569 |
[32m[20221214 14:32:49 @agent_ppo2.py:185][0m |           0.0116 |         262.1383 |        -128.6392 |
[32m[20221214 14:32:49 @agent_ppo2.py:185][0m |          -0.0018 |         238.7521 |        -128.7739 |
[32m[20221214 14:32:49 @agent_ppo2.py:185][0m |          -0.0030 |         237.3980 |        -128.6707 |
[32m[20221214 14:32:49 @agent_ppo2.py:185][0m |          -0.0028 |         237.3700 |        -128.5590 |
[32m[20221214 14:32:49 @agent_ppo2.py:185][0m |          -0.0028 |         235.5404 |        -128.8171 |
[32m[20221214 14:32:50 @agent_ppo2.py:185][0m |           0.0001 |         235.2581 |        -128.4370 |
[32m[20221214 14:32:50 @agent_ppo2.py:185][0m |          -0.0025 |         234.6870 |        -128.6968 |
[32m[20221214 14:32:50 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.01
[32m[20221214 14:32:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.91
[32m[20221214 14:32:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.11
[32m[20221214 14:32:50 @agent_ppo2.py:143][0m Total time:      34.80 min
[32m[20221214 14:32:50 @agent_ppo2.py:145][0m 3198976 total steps have happened
[32m[20221214 14:32:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1562 --------------------------#
[32m[20221214 14:32:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:50 @agent_ppo2.py:185][0m |          -0.0014 |         253.2110 |        -128.7390 |
[32m[20221214 14:32:50 @agent_ppo2.py:185][0m |          -0.0019 |         246.7740 |        -128.4409 |
[32m[20221214 14:32:50 @agent_ppo2.py:185][0m |          -0.0012 |         244.4039 |        -128.3250 |
[32m[20221214 14:32:50 @agent_ppo2.py:185][0m |          -0.0014 |         242.8314 |        -128.7769 |
[32m[20221214 14:32:51 @agent_ppo2.py:185][0m |          -0.0020 |         241.1395 |        -128.6130 |
[32m[20221214 14:32:51 @agent_ppo2.py:185][0m |          -0.0027 |         239.9406 |        -128.4373 |
[32m[20221214 14:32:51 @agent_ppo2.py:185][0m |          -0.0033 |         239.6481 |        -128.3210 |
[32m[20221214 14:32:51 @agent_ppo2.py:185][0m |          -0.0016 |         238.3535 |        -128.5485 |
[32m[20221214 14:32:51 @agent_ppo2.py:185][0m |          -0.0016 |         237.0579 |        -128.5280 |
[32m[20221214 14:32:51 @agent_ppo2.py:185][0m |          -0.0024 |         237.5100 |        -128.5030 |
[32m[20221214 14:32:51 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.31
[32m[20221214 14:32:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.65
[32m[20221214 14:32:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.27
[32m[20221214 14:32:51 @agent_ppo2.py:143][0m Total time:      34.82 min
[32m[20221214 14:32:51 @agent_ppo2.py:145][0m 3201024 total steps have happened
[32m[20221214 14:32:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1563 --------------------------#
[32m[20221214 14:32:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:52 @agent_ppo2.py:185][0m |          -0.0002 |         259.6416 |        -128.4840 |
[32m[20221214 14:32:52 @agent_ppo2.py:185][0m |           0.0005 |         248.7907 |        -128.4991 |
[32m[20221214 14:32:52 @agent_ppo2.py:185][0m |          -0.0005 |         245.6272 |        -128.3897 |
[32m[20221214 14:32:52 @agent_ppo2.py:185][0m |          -0.0022 |         244.3115 |        -128.5411 |
[32m[20221214 14:32:52 @agent_ppo2.py:185][0m |          -0.0022 |         243.3537 |        -128.4186 |
[32m[20221214 14:32:52 @agent_ppo2.py:185][0m |          -0.0041 |         242.8783 |        -128.2417 |
[32m[20221214 14:32:52 @agent_ppo2.py:185][0m |          -0.0025 |         243.8130 |        -128.2023 |
[32m[20221214 14:32:52 @agent_ppo2.py:185][0m |          -0.0039 |         243.5572 |        -128.1092 |
[32m[20221214 14:32:52 @agent_ppo2.py:185][0m |          -0.0012 |         241.3957 |        -128.4062 |
[32m[20221214 14:32:52 @agent_ppo2.py:185][0m |           0.0007 |         240.2659 |        -127.8506 |
[32m[20221214 14:32:52 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:32:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.40
[32m[20221214 14:32:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.74
[32m[20221214 14:32:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.76
[32m[20221214 14:32:53 @agent_ppo2.py:143][0m Total time:      34.84 min
[32m[20221214 14:32:53 @agent_ppo2.py:145][0m 3203072 total steps have happened
[32m[20221214 14:32:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1564 --------------------------#
[32m[20221214 14:32:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:53 @agent_ppo2.py:185][0m |          -0.0029 |         237.6694 |        -128.4152 |
[32m[20221214 14:32:53 @agent_ppo2.py:185][0m |           0.0003 |         229.0437 |        -128.6020 |
[32m[20221214 14:32:53 @agent_ppo2.py:185][0m |          -0.0004 |         222.0132 |        -128.6541 |
[32m[20221214 14:32:53 @agent_ppo2.py:185][0m |          -0.0034 |         220.0977 |        -128.7799 |
[32m[20221214 14:32:53 @agent_ppo2.py:185][0m |          -0.0009 |         220.5266 |        -128.8650 |
[32m[20221214 14:32:53 @agent_ppo2.py:185][0m |          -0.0023 |         217.6120 |        -128.9588 |
[32m[20221214 14:32:53 @agent_ppo2.py:185][0m |          -0.0031 |         216.1222 |        -129.1259 |
[32m[20221214 14:32:54 @agent_ppo2.py:185][0m |          -0.0026 |         216.7757 |        -128.8047 |
[32m[20221214 14:32:54 @agent_ppo2.py:185][0m |           0.0048 |         224.5816 |        -129.1755 |
[32m[20221214 14:32:54 @agent_ppo2.py:185][0m |          -0.0021 |         215.0050 |        -129.1714 |
[32m[20221214 14:32:54 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:32:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.23
[32m[20221214 14:32:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.09
[32m[20221214 14:32:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.63
[32m[20221214 14:32:54 @agent_ppo2.py:143][0m Total time:      34.86 min
[32m[20221214 14:32:54 @agent_ppo2.py:145][0m 3205120 total steps have happened
[32m[20221214 14:32:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1565 --------------------------#
[32m[20221214 14:32:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:54 @agent_ppo2.py:185][0m |           0.0050 |         241.4548 |        -129.6217 |
[32m[20221214 14:32:54 @agent_ppo2.py:185][0m |          -0.0030 |         219.3328 |        -129.6722 |
[32m[20221214 14:32:54 @agent_ppo2.py:185][0m |          -0.0048 |         212.9544 |        -129.2688 |
[32m[20221214 14:32:55 @agent_ppo2.py:185][0m |           0.0005 |         212.1600 |        -128.8817 |
[32m[20221214 14:32:55 @agent_ppo2.py:185][0m |          -0.0036 |         206.2016 |        -129.3168 |
[32m[20221214 14:32:55 @agent_ppo2.py:185][0m |          -0.0040 |         202.8779 |        -128.8856 |
[32m[20221214 14:32:55 @agent_ppo2.py:185][0m |          -0.0051 |         201.2112 |        -128.9994 |
[32m[20221214 14:32:55 @agent_ppo2.py:185][0m |          -0.0060 |         199.4834 |        -128.6998 |
[32m[20221214 14:32:55 @agent_ppo2.py:185][0m |          -0.0034 |         199.5799 |        -128.8756 |
[32m[20221214 14:32:55 @agent_ppo2.py:185][0m |          -0.0065 |         198.1989 |        -128.8739 |
[32m[20221214 14:32:55 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.97
[32m[20221214 14:32:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.48
[32m[20221214 14:32:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.10
[32m[20221214 14:32:55 @agent_ppo2.py:143][0m Total time:      34.89 min
[32m[20221214 14:32:55 @agent_ppo2.py:145][0m 3207168 total steps have happened
[32m[20221214 14:32:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1566 --------------------------#
[32m[20221214 14:32:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:56 @agent_ppo2.py:185][0m |          -0.0008 |         226.3642 |        -128.5246 |
[32m[20221214 14:32:56 @agent_ppo2.py:185][0m |          -0.0015 |         217.5011 |        -128.3753 |
[32m[20221214 14:32:56 @agent_ppo2.py:185][0m |           0.0087 |         226.5092 |        -128.3554 |
[32m[20221214 14:32:56 @agent_ppo2.py:185][0m |          -0.0025 |         211.8784 |        -127.6797 |
[32m[20221214 14:32:56 @agent_ppo2.py:185][0m |           0.0073 |         230.6233 |        -127.8598 |
[32m[20221214 14:32:56 @agent_ppo2.py:185][0m |          -0.0020 |         207.2896 |        -127.2449 |
[32m[20221214 14:32:56 @agent_ppo2.py:185][0m |          -0.0041 |         205.1228 |        -127.4178 |
[32m[20221214 14:32:56 @agent_ppo2.py:185][0m |          -0.0037 |         203.8457 |        -127.2703 |
[32m[20221214 14:32:56 @agent_ppo2.py:185][0m |          -0.0022 |         203.3665 |        -127.0766 |
[32m[20221214 14:32:56 @agent_ppo2.py:185][0m |           0.0093 |         219.8064 |        -126.9908 |
[32m[20221214 14:32:56 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:32:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.00
[32m[20221214 14:32:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.38
[32m[20221214 14:32:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.37
[32m[20221214 14:32:57 @agent_ppo2.py:143][0m Total time:      34.91 min
[32m[20221214 14:32:57 @agent_ppo2.py:145][0m 3209216 total steps have happened
[32m[20221214 14:32:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1567 --------------------------#
[32m[20221214 14:32:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:57 @agent_ppo2.py:185][0m |          -0.0015 |         230.2486 |        -129.0513 |
[32m[20221214 14:32:57 @agent_ppo2.py:185][0m |          -0.0015 |         225.1579 |        -129.0093 |
[32m[20221214 14:32:57 @agent_ppo2.py:185][0m |          -0.0027 |         223.3298 |        -129.3672 |
[32m[20221214 14:32:57 @agent_ppo2.py:185][0m |           0.0155 |         239.5668 |        -128.9899 |
[32m[20221214 14:32:57 @agent_ppo2.py:185][0m |          -0.0001 |         221.2818 |        -129.0900 |
[32m[20221214 14:32:57 @agent_ppo2.py:185][0m |           0.0015 |         219.8404 |        -129.5088 |
[32m[20221214 14:32:58 @agent_ppo2.py:185][0m |          -0.0030 |         219.1883 |        -129.5608 |
[32m[20221214 14:32:58 @agent_ppo2.py:185][0m |          -0.0012 |         218.5571 |        -129.9076 |
[32m[20221214 14:32:58 @agent_ppo2.py:185][0m |          -0.0019 |         218.3739 |        -129.6828 |
[32m[20221214 14:32:58 @agent_ppo2.py:185][0m |          -0.0001 |         218.0443 |        -129.6187 |
[32m[20221214 14:32:58 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:32:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.04
[32m[20221214 14:32:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.07
[32m[20221214 14:32:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.97
[32m[20221214 14:32:58 @agent_ppo2.py:143][0m Total time:      34.93 min
[32m[20221214 14:32:58 @agent_ppo2.py:145][0m 3211264 total steps have happened
[32m[20221214 14:32:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1568 --------------------------#
[32m[20221214 14:32:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:32:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:32:58 @agent_ppo2.py:185][0m |          -0.0013 |         226.2902 |        -130.9106 |
[32m[20221214 14:32:58 @agent_ppo2.py:185][0m |          -0.0008 |         219.9991 |        -130.7863 |
[32m[20221214 14:32:58 @agent_ppo2.py:185][0m |          -0.0009 |         218.2637 |        -130.3144 |
[32m[20221214 14:32:59 @agent_ppo2.py:185][0m |          -0.0010 |         216.6662 |        -130.8027 |
[32m[20221214 14:32:59 @agent_ppo2.py:185][0m |          -0.0012 |         215.3702 |        -130.1492 |
[32m[20221214 14:32:59 @agent_ppo2.py:185][0m |          -0.0019 |         214.3718 |        -130.5202 |
[32m[20221214 14:32:59 @agent_ppo2.py:185][0m |          -0.0015 |         212.5956 |        -130.1819 |
[32m[20221214 14:32:59 @agent_ppo2.py:185][0m |          -0.0023 |         211.5290 |        -130.4753 |
[32m[20221214 14:32:59 @agent_ppo2.py:185][0m |           0.0002 |         210.1645 |        -130.0967 |
[32m[20221214 14:32:59 @agent_ppo2.py:185][0m |           0.0053 |         212.4025 |        -129.8821 |
[32m[20221214 14:32:59 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:32:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.96
[32m[20221214 14:32:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.43
[32m[20221214 14:32:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.57
[32m[20221214 14:32:59 @agent_ppo2.py:143][0m Total time:      34.95 min
[32m[20221214 14:32:59 @agent_ppo2.py:145][0m 3213312 total steps have happened
[32m[20221214 14:32:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1569 --------------------------#
[32m[20221214 14:32:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:00 @agent_ppo2.py:185][0m |          -0.0002 |         222.3011 |        -127.5004 |
[32m[20221214 14:33:00 @agent_ppo2.py:185][0m |          -0.0031 |         220.3599 |        -127.3781 |
[32m[20221214 14:33:00 @agent_ppo2.py:185][0m |          -0.0041 |         219.9603 |        -127.5028 |
[32m[20221214 14:33:00 @agent_ppo2.py:185][0m |          -0.0027 |         218.5668 |        -127.6130 |
[32m[20221214 14:33:00 @agent_ppo2.py:185][0m |          -0.0037 |         218.8750 |        -127.5986 |
[32m[20221214 14:33:00 @agent_ppo2.py:185][0m |          -0.0017 |         218.4596 |        -127.7043 |
[32m[20221214 14:33:00 @agent_ppo2.py:185][0m |          -0.0019 |         217.4404 |        -127.7267 |
[32m[20221214 14:33:00 @agent_ppo2.py:185][0m |           0.0011 |         220.8011 |        -127.7655 |
[32m[20221214 14:33:00 @agent_ppo2.py:185][0m |          -0.0051 |         217.5144 |        -127.3785 |
[32m[20221214 14:33:00 @agent_ppo2.py:185][0m |          -0.0039 |         216.9446 |        -127.6474 |
[32m[20221214 14:33:00 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:33:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.11
[32m[20221214 14:33:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.30
[32m[20221214 14:33:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.16
[32m[20221214 14:33:01 @agent_ppo2.py:143][0m Total time:      34.98 min
[32m[20221214 14:33:01 @agent_ppo2.py:145][0m 3215360 total steps have happened
[32m[20221214 14:33:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1570 --------------------------#
[32m[20221214 14:33:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:33:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:01 @agent_ppo2.py:185][0m |          -0.0009 |         216.5746 |        -128.1284 |
[32m[20221214 14:33:01 @agent_ppo2.py:185][0m |          -0.0017 |         212.6499 |        -128.2281 |
[32m[20221214 14:33:01 @agent_ppo2.py:185][0m |           0.0069 |         218.0754 |        -127.5904 |
[32m[20221214 14:33:01 @agent_ppo2.py:185][0m |          -0.0022 |         210.6518 |        -127.6912 |
[32m[20221214 14:33:01 @agent_ppo2.py:185][0m |           0.0048 |         215.0852 |        -127.7081 |
[32m[20221214 14:33:01 @agent_ppo2.py:185][0m |          -0.0024 |         209.2007 |        -127.4460 |
[32m[20221214 14:33:02 @agent_ppo2.py:185][0m |          -0.0041 |         209.0445 |        -127.2485 |
[32m[20221214 14:33:02 @agent_ppo2.py:185][0m |          -0.0037 |         209.0478 |        -127.3370 |
[32m[20221214 14:33:02 @agent_ppo2.py:185][0m |          -0.0024 |         208.3974 |        -127.2655 |
[32m[20221214 14:33:02 @agent_ppo2.py:185][0m |           0.0010 |         208.2095 |        -126.9707 |
[32m[20221214 14:33:02 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:33:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.96
[32m[20221214 14:33:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.63
[32m[20221214 14:33:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.83
[32m[20221214 14:33:02 @agent_ppo2.py:143][0m Total time:      35.00 min
[32m[20221214 14:33:02 @agent_ppo2.py:145][0m 3217408 total steps have happened
[32m[20221214 14:33:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1571 --------------------------#
[32m[20221214 14:33:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:33:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:02 @agent_ppo2.py:185][0m |           0.0007 |         213.6156 |        -126.0843 |
[32m[20221214 14:33:02 @agent_ppo2.py:185][0m |          -0.0034 |         208.1243 |        -126.0287 |
[32m[20221214 14:33:03 @agent_ppo2.py:185][0m |          -0.0052 |         207.0999 |        -125.9718 |
[32m[20221214 14:33:03 @agent_ppo2.py:185][0m |          -0.0035 |         205.3923 |        -126.0478 |
[32m[20221214 14:33:03 @agent_ppo2.py:185][0m |          -0.0023 |         204.4097 |        -126.3633 |
[32m[20221214 14:33:03 @agent_ppo2.py:185][0m |           0.0057 |         207.8263 |        -125.9375 |
[32m[20221214 14:33:03 @agent_ppo2.py:185][0m |           0.0040 |         204.6603 |        -126.2709 |
[32m[20221214 14:33:03 @agent_ppo2.py:185][0m |          -0.0022 |         202.4227 |        -126.1934 |
[32m[20221214 14:33:03 @agent_ppo2.py:185][0m |          -0.0033 |         202.3372 |        -126.1471 |
[32m[20221214 14:33:03 @agent_ppo2.py:185][0m |           0.0083 |         215.2658 |        -126.2475 |
[32m[20221214 14:33:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:33:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.82
[32m[20221214 14:33:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.76
[32m[20221214 14:33:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.96
[32m[20221214 14:33:03 @agent_ppo2.py:143][0m Total time:      35.02 min
[32m[20221214 14:33:03 @agent_ppo2.py:145][0m 3219456 total steps have happened
[32m[20221214 14:33:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1572 --------------------------#
[32m[20221214 14:33:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:33:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:04 @agent_ppo2.py:185][0m |          -0.0001 |         221.2291 |        -127.9697 |
[32m[20221214 14:33:04 @agent_ppo2.py:185][0m |          -0.0010 |         217.7389 |        -128.2850 |
[32m[20221214 14:33:04 @agent_ppo2.py:185][0m |          -0.0020 |         215.9548 |        -128.3112 |
[32m[20221214 14:33:04 @agent_ppo2.py:185][0m |          -0.0015 |         215.2332 |        -128.3715 |
[32m[20221214 14:33:04 @agent_ppo2.py:185][0m |          -0.0027 |         214.8506 |        -128.5824 |
[32m[20221214 14:33:04 @agent_ppo2.py:185][0m |           0.0001 |         214.1512 |        -128.5990 |
[32m[20221214 14:33:04 @agent_ppo2.py:185][0m |           0.0046 |         226.6630 |        -128.5017 |
[32m[20221214 14:33:04 @agent_ppo2.py:185][0m |           0.0058 |         223.5669 |        -128.2476 |
[32m[20221214 14:33:04 @agent_ppo2.py:185][0m |          -0.0029 |         211.7399 |        -128.5522 |
[32m[20221214 14:33:05 @agent_ppo2.py:185][0m |          -0.0028 |         210.7310 |        -128.6910 |
[32m[20221214 14:33:05 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:33:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.01
[32m[20221214 14:33:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.20
[32m[20221214 14:33:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.85
[32m[20221214 14:33:05 @agent_ppo2.py:143][0m Total time:      35.04 min
[32m[20221214 14:33:05 @agent_ppo2.py:145][0m 3221504 total steps have happened
[32m[20221214 14:33:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1573 --------------------------#
[32m[20221214 14:33:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:05 @agent_ppo2.py:185][0m |          -0.0023 |         233.3327 |        -128.9820 |
[32m[20221214 14:33:05 @agent_ppo2.py:185][0m |          -0.0015 |         229.6075 |        -129.3182 |
[32m[20221214 14:33:05 @agent_ppo2.py:185][0m |          -0.0029 |         225.8832 |        -129.1615 |
[32m[20221214 14:33:05 @agent_ppo2.py:185][0m |          -0.0025 |         222.6176 |        -129.2291 |
[32m[20221214 14:33:05 @agent_ppo2.py:185][0m |           0.0093 |         232.8020 |        -129.3271 |
[32m[20221214 14:33:06 @agent_ppo2.py:185][0m |          -0.0004 |         217.0480 |        -129.4911 |
[32m[20221214 14:33:06 @agent_ppo2.py:185][0m |          -0.0030 |         214.8896 |        -129.6181 |
[32m[20221214 14:33:06 @agent_ppo2.py:185][0m |          -0.0008 |         214.1457 |        -129.2505 |
[32m[20221214 14:33:06 @agent_ppo2.py:185][0m |          -0.0027 |         213.0891 |        -129.2762 |
[32m[20221214 14:33:06 @agent_ppo2.py:185][0m |           0.0020 |         214.0085 |        -129.6053 |
[32m[20221214 14:33:06 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:33:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.59
[32m[20221214 14:33:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.35
[32m[20221214 14:33:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.14
[32m[20221214 14:33:06 @agent_ppo2.py:143][0m Total time:      35.07 min
[32m[20221214 14:33:06 @agent_ppo2.py:145][0m 3223552 total steps have happened
[32m[20221214 14:33:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1574 --------------------------#
[32m[20221214 14:33:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:06 @agent_ppo2.py:185][0m |           0.0037 |         237.9418 |        -130.8734 |
[32m[20221214 14:33:07 @agent_ppo2.py:185][0m |          -0.0028 |         219.4366 |        -130.4973 |
[32m[20221214 14:33:07 @agent_ppo2.py:185][0m |           0.0050 |         220.0830 |        -130.5140 |
[32m[20221214 14:33:07 @agent_ppo2.py:185][0m |          -0.0042 |         208.9096 |        -130.4105 |
[32m[20221214 14:33:07 @agent_ppo2.py:185][0m |          -0.0025 |         206.6343 |        -130.0599 |
[32m[20221214 14:33:07 @agent_ppo2.py:185][0m |          -0.0056 |         205.3352 |        -130.4517 |
[32m[20221214 14:33:07 @agent_ppo2.py:185][0m |           0.0160 |         238.3457 |        -130.2146 |
[32m[20221214 14:33:07 @agent_ppo2.py:185][0m |           0.0061 |         216.0880 |        -129.6748 |
[32m[20221214 14:33:07 @agent_ppo2.py:185][0m |          -0.0030 |         200.7713 |        -130.2655 |
[32m[20221214 14:33:07 @agent_ppo2.py:185][0m |          -0.0048 |         200.6475 |        -130.3276 |
[32m[20221214 14:33:07 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:33:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.70
[32m[20221214 14:33:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.88
[32m[20221214 14:33:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.33
[32m[20221214 14:33:07 @agent_ppo2.py:143][0m Total time:      35.09 min
[32m[20221214 14:33:07 @agent_ppo2.py:145][0m 3225600 total steps have happened
[32m[20221214 14:33:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1575 --------------------------#
[32m[20221214 14:33:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:08 @agent_ppo2.py:185][0m |          -0.0019 |         203.6787 |        -128.6098 |
[32m[20221214 14:33:08 @agent_ppo2.py:185][0m |           0.0002 |         201.2142 |        -128.1043 |
[32m[20221214 14:33:08 @agent_ppo2.py:185][0m |          -0.0035 |         200.2782 |        -127.7154 |
[32m[20221214 14:33:08 @agent_ppo2.py:185][0m |          -0.0006 |         198.3361 |        -127.7859 |
[32m[20221214 14:33:08 @agent_ppo2.py:185][0m |          -0.0037 |         197.8524 |        -127.5528 |
[32m[20221214 14:33:08 @agent_ppo2.py:185][0m |          -0.0028 |         197.7940 |        -127.4174 |
[32m[20221214 14:33:08 @agent_ppo2.py:185][0m |          -0.0042 |         197.0028 |        -127.4311 |
[32m[20221214 14:33:08 @agent_ppo2.py:185][0m |          -0.0056 |         196.5674 |        -126.9413 |
[32m[20221214 14:33:09 @agent_ppo2.py:185][0m |           0.0002 |         199.1021 |        -127.2866 |
[32m[20221214 14:33:09 @agent_ppo2.py:185][0m |          -0.0055 |         196.5392 |        -126.8442 |
[32m[20221214 14:33:09 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:33:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.49
[32m[20221214 14:33:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.10
[32m[20221214 14:33:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.90
[32m[20221214 14:33:09 @agent_ppo2.py:143][0m Total time:      35.11 min
[32m[20221214 14:33:09 @agent_ppo2.py:145][0m 3227648 total steps have happened
[32m[20221214 14:33:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1576 --------------------------#
[32m[20221214 14:33:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:09 @agent_ppo2.py:185][0m |          -0.0008 |         214.7753 |        -128.3629 |
[32m[20221214 14:33:09 @agent_ppo2.py:185][0m |           0.0085 |         212.7866 |        -127.7305 |
[32m[20221214 14:33:09 @agent_ppo2.py:185][0m |          -0.0022 |         204.1498 |        -127.8730 |
[32m[20221214 14:33:09 @agent_ppo2.py:185][0m |           0.0002 |         198.2747 |        -127.9149 |
[32m[20221214 14:33:09 @agent_ppo2.py:185][0m |          -0.0019 |         196.6808 |        -127.5982 |
[32m[20221214 14:33:10 @agent_ppo2.py:185][0m |           0.0060 |         200.7468 |        -127.3272 |
[32m[20221214 14:33:10 @agent_ppo2.py:185][0m |          -0.0042 |         196.1786 |        -127.6953 |
[32m[20221214 14:33:10 @agent_ppo2.py:185][0m |           0.0021 |         197.1731 |        -127.5876 |
[32m[20221214 14:33:10 @agent_ppo2.py:185][0m |           0.0056 |         197.1731 |        -127.5149 |
[32m[20221214 14:33:10 @agent_ppo2.py:185][0m |          -0.0038 |         193.6015 |        -127.5167 |
[32m[20221214 14:33:10 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:33:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.20
[32m[20221214 14:33:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.83
[32m[20221214 14:33:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.88
[32m[20221214 14:33:10 @agent_ppo2.py:143][0m Total time:      35.13 min
[32m[20221214 14:33:10 @agent_ppo2.py:145][0m 3229696 total steps have happened
[32m[20221214 14:33:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1577 --------------------------#
[32m[20221214 14:33:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:10 @agent_ppo2.py:185][0m |          -0.0008 |         200.8089 |        -129.2634 |
[32m[20221214 14:33:11 @agent_ppo2.py:185][0m |          -0.0010 |         195.2434 |        -129.3156 |
[32m[20221214 14:33:11 @agent_ppo2.py:185][0m |          -0.0021 |         192.8479 |        -128.7430 |
[32m[20221214 14:33:11 @agent_ppo2.py:185][0m |           0.0064 |         201.4401 |        -129.2289 |
[32m[20221214 14:33:11 @agent_ppo2.py:185][0m |          -0.0034 |         190.8078 |        -128.8638 |
[32m[20221214 14:33:11 @agent_ppo2.py:185][0m |          -0.0002 |         189.2613 |        -128.8117 |
[32m[20221214 14:33:11 @agent_ppo2.py:185][0m |          -0.0033 |         188.5328 |        -129.1714 |
[32m[20221214 14:33:11 @agent_ppo2.py:185][0m |          -0.0051 |         188.0148 |        -129.3417 |
[32m[20221214 14:33:11 @agent_ppo2.py:185][0m |          -0.0004 |         188.1346 |        -129.2146 |
[32m[20221214 14:33:11 @agent_ppo2.py:185][0m |          -0.0013 |         187.2962 |        -129.4266 |
[32m[20221214 14:33:11 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:33:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.35
[32m[20221214 14:33:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.65
[32m[20221214 14:33:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.87
[32m[20221214 14:33:11 @agent_ppo2.py:143][0m Total time:      35.16 min
[32m[20221214 14:33:11 @agent_ppo2.py:145][0m 3231744 total steps have happened
[32m[20221214 14:33:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1578 --------------------------#
[32m[20221214 14:33:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:12 @agent_ppo2.py:185][0m |          -0.0008 |         189.8657 |        -126.6007 |
[32m[20221214 14:33:12 @agent_ppo2.py:185][0m |           0.0010 |         185.1359 |        -126.7178 |
[32m[20221214 14:33:12 @agent_ppo2.py:185][0m |           0.0151 |         199.7647 |        -126.6363 |
[32m[20221214 14:33:12 @agent_ppo2.py:185][0m |          -0.0010 |         181.0727 |        -126.6613 |
[32m[20221214 14:33:12 @agent_ppo2.py:185][0m |          -0.0028 |         178.8748 |        -126.4276 |
[32m[20221214 14:33:12 @agent_ppo2.py:185][0m |          -0.0035 |         178.5949 |        -126.3290 |
[32m[20221214 14:33:12 @agent_ppo2.py:185][0m |           0.0025 |         178.4591 |        -126.4203 |
[32m[20221214 14:33:12 @agent_ppo2.py:185][0m |          -0.0029 |         176.8882 |        -126.2859 |
[32m[20221214 14:33:13 @agent_ppo2.py:185][0m |          -0.0037 |         176.0145 |        -126.1498 |
[32m[20221214 14:33:13 @agent_ppo2.py:185][0m |          -0.0045 |         175.8720 |        -126.2594 |
[32m[20221214 14:33:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:33:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.67
[32m[20221214 14:33:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.02
[32m[20221214 14:33:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.33
[32m[20221214 14:33:13 @agent_ppo2.py:143][0m Total time:      35.18 min
[32m[20221214 14:33:13 @agent_ppo2.py:145][0m 3233792 total steps have happened
[32m[20221214 14:33:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1579 --------------------------#
[32m[20221214 14:33:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:13 @agent_ppo2.py:185][0m |           0.0046 |         204.9116 |        -129.3873 |
[32m[20221214 14:33:13 @agent_ppo2.py:185][0m |           0.0012 |         196.4135 |        -129.2742 |
[32m[20221214 14:33:13 @agent_ppo2.py:185][0m |          -0.0029 |         193.6685 |        -128.9957 |
[32m[20221214 14:33:13 @agent_ppo2.py:185][0m |          -0.0031 |         192.3960 |        -128.9580 |
[32m[20221214 14:33:14 @agent_ppo2.py:185][0m |           0.0084 |         208.2154 |        -129.1373 |
[32m[20221214 14:33:14 @agent_ppo2.py:185][0m |           0.0075 |         207.1144 |        -129.0759 |
[32m[20221214 14:33:14 @agent_ppo2.py:185][0m |          -0.0041 |         190.4183 |        -129.2371 |
[32m[20221214 14:33:14 @agent_ppo2.py:185][0m |          -0.0021 |         189.2744 |        -128.9624 |
[32m[20221214 14:33:14 @agent_ppo2.py:185][0m |          -0.0004 |         189.7971 |        -128.8290 |
[32m[20221214 14:33:14 @agent_ppo2.py:185][0m |          -0.0015 |         188.4838 |        -128.9876 |
[32m[20221214 14:33:14 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:33:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.58
[32m[20221214 14:33:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.72
[32m[20221214 14:33:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.79
[32m[20221214 14:33:14 @agent_ppo2.py:143][0m Total time:      35.20 min
[32m[20221214 14:33:14 @agent_ppo2.py:145][0m 3235840 total steps have happened
[32m[20221214 14:33:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1580 --------------------------#
[32m[20221214 14:33:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:33:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:15 @agent_ppo2.py:185][0m |          -0.0037 |         216.6331 |        -128.7260 |
[32m[20221214 14:33:15 @agent_ppo2.py:185][0m |          -0.0059 |         215.0541 |        -128.6468 |
[32m[20221214 14:33:15 @agent_ppo2.py:185][0m |          -0.0052 |         212.7608 |        -128.7766 |
[32m[20221214 14:33:15 @agent_ppo2.py:185][0m |          -0.0037 |         212.5131 |        -128.4476 |
[32m[20221214 14:33:15 @agent_ppo2.py:185][0m |          -0.0037 |         210.7937 |        -128.4041 |
[32m[20221214 14:33:15 @agent_ppo2.py:185][0m |          -0.0039 |         210.6266 |        -128.3073 |
[32m[20221214 14:33:15 @agent_ppo2.py:185][0m |           0.0071 |         216.6190 |        -128.0663 |
[32m[20221214 14:33:15 @agent_ppo2.py:185][0m |          -0.0020 |         209.4186 |        -128.4473 |
[32m[20221214 14:33:15 @agent_ppo2.py:185][0m |          -0.0046 |         209.4549 |        -128.3905 |
[32m[20221214 14:33:15 @agent_ppo2.py:185][0m |           0.0131 |         237.5630 |        -128.1398 |
[32m[20221214 14:33:15 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:33:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.68
[32m[20221214 14:33:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.77
[32m[20221214 14:33:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.87
[32m[20221214 14:33:16 @agent_ppo2.py:143][0m Total time:      35.22 min
[32m[20221214 14:33:16 @agent_ppo2.py:145][0m 3237888 total steps have happened
[32m[20221214 14:33:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1581 --------------------------#
[32m[20221214 14:33:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:16 @agent_ppo2.py:185][0m |          -0.0011 |         249.7315 |        -125.4246 |
[32m[20221214 14:33:16 @agent_ppo2.py:185][0m |          -0.0021 |         242.1845 |        -125.6806 |
[32m[20221214 14:33:16 @agent_ppo2.py:185][0m |          -0.0042 |         237.5372 |        -125.6341 |
[32m[20221214 14:33:16 @agent_ppo2.py:185][0m |          -0.0000 |         235.0648 |        -125.7583 |
[32m[20221214 14:33:16 @agent_ppo2.py:185][0m |           0.0076 |         243.7338 |        -126.1594 |
[32m[20221214 14:33:16 @agent_ppo2.py:185][0m |          -0.0038 |         231.9519 |        -125.9395 |
[32m[20221214 14:33:16 @agent_ppo2.py:185][0m |           0.0089 |         256.7875 |        -126.0332 |
[32m[20221214 14:33:16 @agent_ppo2.py:185][0m |           0.0037 |         240.7214 |        -126.3252 |
[32m[20221214 14:33:17 @agent_ppo2.py:185][0m |          -0.0002 |         232.1520 |        -126.1162 |
[32m[20221214 14:33:17 @agent_ppo2.py:185][0m |           0.0046 |         243.4829 |        -126.4909 |
[32m[20221214 14:33:17 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:33:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.02
[32m[20221214 14:33:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.96
[32m[20221214 14:33:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.32
[32m[20221214 14:33:17 @agent_ppo2.py:143][0m Total time:      35.25 min
[32m[20221214 14:33:17 @agent_ppo2.py:145][0m 3239936 total steps have happened
[32m[20221214 14:33:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1582 --------------------------#
[32m[20221214 14:33:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:17 @agent_ppo2.py:185][0m |           0.0077 |         273.5543 |        -128.3912 |
[32m[20221214 14:33:17 @agent_ppo2.py:185][0m |          -0.0013 |         249.0199 |        -128.4634 |
[32m[20221214 14:33:17 @agent_ppo2.py:185][0m |           0.0007 |         246.6879 |        -128.0783 |
[32m[20221214 14:33:17 @agent_ppo2.py:185][0m |           0.0112 |         273.1594 |        -128.0488 |
[32m[20221214 14:33:18 @agent_ppo2.py:185][0m |          -0.0033 |         243.5528 |        -127.8845 |
[32m[20221214 14:33:18 @agent_ppo2.py:185][0m |          -0.0023 |         242.5169 |        -128.1481 |
[32m[20221214 14:33:18 @agent_ppo2.py:185][0m |          -0.0021 |         241.0853 |        -128.2338 |
[32m[20221214 14:33:18 @agent_ppo2.py:185][0m |           0.0006 |         240.4599 |        -128.5336 |
[32m[20221214 14:33:18 @agent_ppo2.py:185][0m |          -0.0019 |         239.9335 |        -128.1829 |
[32m[20221214 14:33:18 @agent_ppo2.py:185][0m |          -0.0025 |         239.1294 |        -128.3018 |
[32m[20221214 14:33:18 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:33:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.12
[32m[20221214 14:33:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.35
[32m[20221214 14:33:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.85
[32m[20221214 14:33:18 @agent_ppo2.py:143][0m Total time:      35.27 min
[32m[20221214 14:33:18 @agent_ppo2.py:145][0m 3241984 total steps have happened
[32m[20221214 14:33:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1583 --------------------------#
[32m[20221214 14:33:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:19 @agent_ppo2.py:185][0m |           0.0011 |         270.3691 |        -127.7523 |
[32m[20221214 14:33:19 @agent_ppo2.py:185][0m |          -0.0019 |         257.4676 |        -128.0670 |
[32m[20221214 14:33:19 @agent_ppo2.py:185][0m |          -0.0016 |         252.8801 |        -127.9270 |
[32m[20221214 14:33:19 @agent_ppo2.py:185][0m |          -0.0022 |         250.7242 |        -127.9959 |
[32m[20221214 14:33:19 @agent_ppo2.py:185][0m |          -0.0035 |         248.0708 |        -128.1290 |
[32m[20221214 14:33:19 @agent_ppo2.py:185][0m |           0.0037 |         252.5696 |        -127.7445 |
[32m[20221214 14:33:19 @agent_ppo2.py:185][0m |          -0.0009 |         244.5387 |        -127.9797 |
[32m[20221214 14:33:19 @agent_ppo2.py:185][0m |          -0.0011 |         244.4776 |        -127.9363 |
[32m[20221214 14:33:19 @agent_ppo2.py:185][0m |          -0.0020 |         243.7292 |        -128.3177 |
[32m[20221214 14:33:19 @agent_ppo2.py:185][0m |          -0.0036 |         242.8943 |        -128.2699 |
[32m[20221214 14:33:19 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:33:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.18
[32m[20221214 14:33:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.45
[32m[20221214 14:33:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.29
[32m[20221214 14:33:20 @agent_ppo2.py:143][0m Total time:      35.29 min
[32m[20221214 14:33:20 @agent_ppo2.py:145][0m 3244032 total steps have happened
[32m[20221214 14:33:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1584 --------------------------#
[32m[20221214 14:33:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:20 @agent_ppo2.py:185][0m |          -0.0001 |         257.6778 |        -128.2815 |
[32m[20221214 14:33:20 @agent_ppo2.py:185][0m |           0.0031 |         250.3664 |        -128.0856 |
[32m[20221214 14:33:20 @agent_ppo2.py:185][0m |          -0.0003 |         248.3302 |        -128.4216 |
[32m[20221214 14:33:20 @agent_ppo2.py:185][0m |           0.0118 |         256.5666 |        -127.9735 |
[32m[20221214 14:33:20 @agent_ppo2.py:185][0m |           0.0064 |         254.9957 |        -128.7193 |
[32m[20221214 14:33:20 @agent_ppo2.py:185][0m |          -0.0017 |         247.8977 |        -128.5954 |
[32m[20221214 14:33:20 @agent_ppo2.py:185][0m |          -0.0020 |         246.8602 |        -128.2575 |
[32m[20221214 14:33:21 @agent_ppo2.py:185][0m |          -0.0024 |         246.6433 |        -128.6144 |
[32m[20221214 14:33:21 @agent_ppo2.py:185][0m |           0.0010 |         247.1832 |        -127.9155 |
[32m[20221214 14:33:21 @agent_ppo2.py:185][0m |          -0.0006 |         246.7109 |        -128.9217 |
[32m[20221214 14:33:21 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:33:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.32
[32m[20221214 14:33:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.27
[32m[20221214 14:33:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.27
[32m[20221214 14:33:21 @agent_ppo2.py:143][0m Total time:      35.31 min
[32m[20221214 14:33:21 @agent_ppo2.py:145][0m 3246080 total steps have happened
[32m[20221214 14:33:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1585 --------------------------#
[32m[20221214 14:33:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:21 @agent_ppo2.py:185][0m |          -0.0007 |         264.6355 |        -130.1619 |
[32m[20221214 14:33:21 @agent_ppo2.py:185][0m |          -0.0031 |         259.4743 |        -129.9981 |
[32m[20221214 14:33:21 @agent_ppo2.py:185][0m |           0.0024 |         260.4060 |        -129.8997 |
[32m[20221214 14:33:21 @agent_ppo2.py:185][0m |          -0.0025 |         255.1664 |        -130.2868 |
[32m[20221214 14:33:22 @agent_ppo2.py:185][0m |          -0.0018 |         255.1235 |        -130.2033 |
[32m[20221214 14:33:22 @agent_ppo2.py:185][0m |          -0.0023 |         254.1127 |        -130.0800 |
[32m[20221214 14:33:22 @agent_ppo2.py:185][0m |          -0.0034 |         253.6208 |        -130.2287 |
[32m[20221214 14:33:22 @agent_ppo2.py:185][0m |           0.0071 |         266.1985 |        -130.1403 |
[32m[20221214 14:33:22 @agent_ppo2.py:185][0m |          -0.0020 |         252.2007 |        -129.8362 |
[32m[20221214 14:33:22 @agent_ppo2.py:185][0m |          -0.0010 |         251.3845 |        -130.2979 |
[32m[20221214 14:33:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:33:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.28
[32m[20221214 14:33:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.51
[32m[20221214 14:33:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.82
[32m[20221214 14:33:22 @agent_ppo2.py:143][0m Total time:      35.33 min
[32m[20221214 14:33:22 @agent_ppo2.py:145][0m 3248128 total steps have happened
[32m[20221214 14:33:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1586 --------------------------#
[32m[20221214 14:33:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:23 @agent_ppo2.py:185][0m |          -0.0001 |         251.1547 |        -131.6082 |
[32m[20221214 14:33:23 @agent_ppo2.py:185][0m |           0.0179 |         274.8614 |        -131.9473 |
[32m[20221214 14:33:23 @agent_ppo2.py:185][0m |          -0.0015 |         242.9974 |        -131.6797 |
[32m[20221214 14:33:23 @agent_ppo2.py:185][0m |          -0.0026 |         239.6329 |        -132.2039 |
[32m[20221214 14:33:23 @agent_ppo2.py:185][0m |          -0.0019 |         239.3145 |        -132.4008 |
[32m[20221214 14:33:23 @agent_ppo2.py:185][0m |          -0.0005 |         238.5865 |        -132.2105 |
[32m[20221214 14:33:23 @agent_ppo2.py:185][0m |           0.0103 |         252.9325 |        -132.7227 |
[32m[20221214 14:33:23 @agent_ppo2.py:185][0m |           0.0002 |         238.1909 |        -132.8423 |
[32m[20221214 14:33:23 @agent_ppo2.py:185][0m |          -0.0024 |         236.9654 |        -132.4843 |
[32m[20221214 14:33:23 @agent_ppo2.py:185][0m |           0.0088 |         248.2244 |        -133.0014 |
[32m[20221214 14:33:23 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:33:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.78
[32m[20221214 14:33:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.54
[32m[20221214 14:33:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.62
[32m[20221214 14:33:24 @agent_ppo2.py:143][0m Total time:      35.36 min
[32m[20221214 14:33:24 @agent_ppo2.py:145][0m 3250176 total steps have happened
[32m[20221214 14:33:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1587 --------------------------#
[32m[20221214 14:33:24 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:33:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:24 @agent_ppo2.py:185][0m |           0.0017 |         203.7788 |        -134.3592 |
[32m[20221214 14:33:24 @agent_ppo2.py:185][0m |          -0.0028 |         193.8336 |        -134.3631 |
[32m[20221214 14:33:24 @agent_ppo2.py:185][0m |           0.0005 |         190.4711 |        -134.5679 |
[32m[20221214 14:33:24 @agent_ppo2.py:185][0m |          -0.0019 |         187.6455 |        -134.5159 |
[32m[20221214 14:33:24 @agent_ppo2.py:185][0m |          -0.0032 |         185.6301 |        -134.8801 |
[32m[20221214 14:33:24 @agent_ppo2.py:185][0m |          -0.0023 |         184.4757 |        -134.4214 |
[32m[20221214 14:33:24 @agent_ppo2.py:185][0m |          -0.0005 |         182.6481 |        -134.8014 |
[32m[20221214 14:33:25 @agent_ppo2.py:185][0m |          -0.0002 |         181.3174 |        -134.5364 |
[32m[20221214 14:33:25 @agent_ppo2.py:185][0m |          -0.0025 |         181.0315 |        -134.7964 |
[32m[20221214 14:33:25 @agent_ppo2.py:185][0m |          -0.0013 |         178.7872 |        -134.7213 |
[32m[20221214 14:33:25 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:33:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 792.30
[32m[20221214 14:33:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.15
[32m[20221214 14:33:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.70
[32m[20221214 14:33:25 @agent_ppo2.py:143][0m Total time:      35.38 min
[32m[20221214 14:33:25 @agent_ppo2.py:145][0m 3252224 total steps have happened
[32m[20221214 14:33:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1588 --------------------------#
[32m[20221214 14:33:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:33:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:25 @agent_ppo2.py:185][0m |           0.0050 |         250.9449 |        -131.5480 |
[32m[20221214 14:33:25 @agent_ppo2.py:185][0m |          -0.0018 |         241.7775 |        -131.3781 |
[32m[20221214 14:33:25 @agent_ppo2.py:185][0m |          -0.0023 |         238.7994 |        -131.5826 |
[32m[20221214 14:33:25 @agent_ppo2.py:185][0m |          -0.0022 |         237.4169 |        -131.5480 |
[32m[20221214 14:33:26 @agent_ppo2.py:185][0m |          -0.0025 |         236.2552 |        -131.8922 |
[32m[20221214 14:33:26 @agent_ppo2.py:185][0m |          -0.0019 |         235.1726 |        -131.5813 |
[32m[20221214 14:33:26 @agent_ppo2.py:185][0m |          -0.0025 |         234.6037 |        -131.5948 |
[32m[20221214 14:33:26 @agent_ppo2.py:185][0m |          -0.0029 |         234.4162 |        -131.5455 |
[32m[20221214 14:33:26 @agent_ppo2.py:185][0m |          -0.0033 |         233.8881 |        -131.4360 |
[32m[20221214 14:33:26 @agent_ppo2.py:185][0m |          -0.0023 |         233.0001 |        -131.9818 |
[32m[20221214 14:33:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:33:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.20
[32m[20221214 14:33:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.98
[32m[20221214 14:33:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.47
[32m[20221214 14:33:26 @agent_ppo2.py:143][0m Total time:      35.40 min
[32m[20221214 14:33:26 @agent_ppo2.py:145][0m 3254272 total steps have happened
[32m[20221214 14:33:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1589 --------------------------#
[32m[20221214 14:33:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:26 @agent_ppo2.py:185][0m |          -0.0040 |         222.1051 |        -132.1943 |
[32m[20221214 14:33:27 @agent_ppo2.py:185][0m |          -0.0041 |         216.4698 |        -132.1377 |
[32m[20221214 14:33:27 @agent_ppo2.py:185][0m |           0.0067 |         231.9607 |        -132.5634 |
[32m[20221214 14:33:27 @agent_ppo2.py:185][0m |          -0.0037 |         211.2461 |        -131.9676 |
[32m[20221214 14:33:27 @agent_ppo2.py:185][0m |          -0.0027 |         210.3946 |        -132.1633 |
[32m[20221214 14:33:27 @agent_ppo2.py:185][0m |          -0.0037 |         209.1651 |        -131.9689 |
[32m[20221214 14:33:27 @agent_ppo2.py:185][0m |          -0.0040 |         208.6155 |        -132.1096 |
[32m[20221214 14:33:27 @agent_ppo2.py:185][0m |          -0.0042 |         208.0265 |        -132.1024 |
[32m[20221214 14:33:27 @agent_ppo2.py:185][0m |          -0.0034 |         208.2171 |        -132.0394 |
[32m[20221214 14:33:27 @agent_ppo2.py:185][0m |          -0.0039 |         207.0956 |        -131.9265 |
[32m[20221214 14:33:27 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:33:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.66
[32m[20221214 14:33:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.04
[32m[20221214 14:33:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.17
[32m[20221214 14:33:27 @agent_ppo2.py:143][0m Total time:      35.42 min
[32m[20221214 14:33:27 @agent_ppo2.py:145][0m 3256320 total steps have happened
[32m[20221214 14:33:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1590 --------------------------#
[32m[20221214 14:33:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:33:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:28 @agent_ppo2.py:185][0m |           0.0052 |         173.9446 |        -132.9918 |
[32m[20221214 14:33:28 @agent_ppo2.py:185][0m |          -0.0029 |         151.7640 |        -133.1345 |
[32m[20221214 14:33:28 @agent_ppo2.py:185][0m |           0.0001 |         145.2148 |        -133.3600 |
[32m[20221214 14:33:28 @agent_ppo2.py:185][0m |           0.0004 |         140.9629 |        -133.4303 |
[32m[20221214 14:33:28 @agent_ppo2.py:185][0m |           0.0016 |         138.6671 |        -133.4117 |
[32m[20221214 14:33:28 @agent_ppo2.py:185][0m |          -0.0047 |         137.2079 |        -133.8218 |
[32m[20221214 14:33:28 @agent_ppo2.py:185][0m |          -0.0004 |         136.0751 |        -134.1733 |
[32m[20221214 14:33:29 @agent_ppo2.py:185][0m |          -0.0004 |         135.1214 |        -134.0220 |
[32m[20221214 14:33:29 @agent_ppo2.py:185][0m |           0.0034 |         133.9986 |        -133.6518 |
[32m[20221214 14:33:29 @agent_ppo2.py:185][0m |          -0.0006 |         133.2848 |        -134.4102 |
[32m[20221214 14:33:29 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:33:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 754.78
[32m[20221214 14:33:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.69
[32m[20221214 14:33:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 645.27
[32m[20221214 14:33:29 @agent_ppo2.py:143][0m Total time:      35.45 min
[32m[20221214 14:33:29 @agent_ppo2.py:145][0m 3258368 total steps have happened
[32m[20221214 14:33:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1591 --------------------------#
[32m[20221214 14:33:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:29 @agent_ppo2.py:185][0m |           0.0011 |          87.4319 |        -136.9641 |
[32m[20221214 14:33:29 @agent_ppo2.py:185][0m |          -0.0016 |          62.1959 |        -137.1742 |
[32m[20221214 14:33:29 @agent_ppo2.py:185][0m |          -0.0041 |          59.6891 |        -136.9388 |
[32m[20221214 14:33:29 @agent_ppo2.py:185][0m |           0.0066 |          64.8698 |        -136.9289 |
[32m[20221214 14:33:30 @agent_ppo2.py:185][0m |           0.0030 |          55.9066 |        -136.3928 |
[32m[20221214 14:33:30 @agent_ppo2.py:185][0m |           0.0047 |          55.1192 |        -136.7337 |
[32m[20221214 14:33:30 @agent_ppo2.py:185][0m |          -0.0050 |          54.3576 |        -136.4843 |
[32m[20221214 14:33:30 @agent_ppo2.py:185][0m |          -0.0022 |          53.2946 |        -136.3344 |
[32m[20221214 14:33:30 @agent_ppo2.py:185][0m |          -0.0009 |          53.3780 |        -136.2957 |
[32m[20221214 14:33:30 @agent_ppo2.py:185][0m |           0.0008 |          55.0346 |        -136.5351 |
[32m[20221214 14:33:30 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:33:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 674.60
[32m[20221214 14:33:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.69
[32m[20221214 14:33:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 640.03
[32m[20221214 14:33:30 @agent_ppo2.py:143][0m Total time:      35.47 min
[32m[20221214 14:33:30 @agent_ppo2.py:145][0m 3260416 total steps have happened
[32m[20221214 14:33:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1592 --------------------------#
[32m[20221214 14:33:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:31 @agent_ppo2.py:185][0m |           0.0008 |          96.2461 |        -134.6250 |
[32m[20221214 14:33:31 @agent_ppo2.py:185][0m |          -0.0023 |          72.2482 |        -134.5287 |
[32m[20221214 14:33:31 @agent_ppo2.py:185][0m |           0.0029 |          67.3682 |        -134.3990 |
[32m[20221214 14:33:31 @agent_ppo2.py:185][0m |           0.0028 |          64.5986 |        -134.4703 |
[32m[20221214 14:33:31 @agent_ppo2.py:185][0m |          -0.0039 |          61.2482 |        -134.2863 |
[32m[20221214 14:33:31 @agent_ppo2.py:185][0m |          -0.0021 |          58.9581 |        -134.2500 |
[32m[20221214 14:33:31 @agent_ppo2.py:185][0m |           0.0046 |          58.9218 |        -134.2768 |
[32m[20221214 14:33:31 @agent_ppo2.py:185][0m |          -0.0017 |          56.7430 |        -134.5453 |
[32m[20221214 14:33:31 @agent_ppo2.py:185][0m |          -0.0001 |          56.0001 |        -134.1791 |
[32m[20221214 14:33:31 @agent_ppo2.py:185][0m |           0.0069 |          56.5262 |        -134.2477 |
[32m[20221214 14:33:31 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:33:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 655.90
[32m[20221214 14:33:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.68
[32m[20221214 14:33:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 801.95
[32m[20221214 14:33:32 @agent_ppo2.py:143][0m Total time:      35.49 min
[32m[20221214 14:33:32 @agent_ppo2.py:145][0m 3262464 total steps have happened
[32m[20221214 14:33:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1593 --------------------------#
[32m[20221214 14:33:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:32 @agent_ppo2.py:185][0m |           0.0052 |         262.6084 |        -133.7128 |
[32m[20221214 14:33:32 @agent_ppo2.py:185][0m |          -0.0022 |         244.7904 |        -133.7342 |
[32m[20221214 14:33:32 @agent_ppo2.py:185][0m |          -0.0009 |         239.0927 |        -133.8753 |
[32m[20221214 14:33:32 @agent_ppo2.py:185][0m |          -0.0015 |         234.9591 |        -133.5123 |
[32m[20221214 14:33:32 @agent_ppo2.py:185][0m |          -0.0036 |         232.5688 |        -133.4573 |
[32m[20221214 14:33:32 @agent_ppo2.py:185][0m |          -0.0019 |         233.1625 |        -133.1757 |
[32m[20221214 14:33:32 @agent_ppo2.py:185][0m |          -0.0015 |         230.2627 |        -133.3073 |
[32m[20221214 14:33:33 @agent_ppo2.py:185][0m |          -0.0016 |         229.7750 |        -133.3327 |
[32m[20221214 14:33:33 @agent_ppo2.py:185][0m |          -0.0018 |         229.0486 |        -133.5917 |
[32m[20221214 14:33:33 @agent_ppo2.py:185][0m |          -0.0026 |         228.5812 |        -133.4398 |
[32m[20221214 14:33:33 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:33:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.06
[32m[20221214 14:33:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.20
[32m[20221214 14:33:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.30
[32m[20221214 14:33:33 @agent_ppo2.py:143][0m Total time:      35.51 min
[32m[20221214 14:33:33 @agent_ppo2.py:145][0m 3264512 total steps have happened
[32m[20221214 14:33:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1594 --------------------------#
[32m[20221214 14:33:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:33 @agent_ppo2.py:185][0m |          -0.0009 |         242.2719 |        -133.4589 |
[32m[20221214 14:33:33 @agent_ppo2.py:185][0m |          -0.0011 |         234.1903 |        -133.7252 |
[32m[20221214 14:33:33 @agent_ppo2.py:185][0m |          -0.0020 |         231.1414 |        -134.0204 |
[32m[20221214 14:33:34 @agent_ppo2.py:185][0m |           0.0114 |         252.9455 |        -134.2691 |
[32m[20221214 14:33:34 @agent_ppo2.py:185][0m |          -0.0002 |         229.5434 |        -134.3303 |
[32m[20221214 14:33:34 @agent_ppo2.py:185][0m |          -0.0025 |         226.2031 |        -134.3148 |
[32m[20221214 14:33:34 @agent_ppo2.py:185][0m |           0.0237 |         273.7043 |        -134.7807 |
[32m[20221214 14:33:34 @agent_ppo2.py:185][0m |           0.0105 |         242.2168 |        -134.4976 |
[32m[20221214 14:33:34 @agent_ppo2.py:185][0m |          -0.0001 |         224.0517 |        -134.7200 |
[32m[20221214 14:33:34 @agent_ppo2.py:185][0m |           0.0098 |         239.4495 |        -135.1069 |
[32m[20221214 14:33:34 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:33:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.41
[32m[20221214 14:33:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.05
[32m[20221214 14:33:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.53
[32m[20221214 14:33:34 @agent_ppo2.py:143][0m Total time:      35.54 min
[32m[20221214 14:33:34 @agent_ppo2.py:145][0m 3266560 total steps have happened
[32m[20221214 14:33:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1595 --------------------------#
[32m[20221214 14:33:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:35 @agent_ppo2.py:185][0m |          -0.0017 |         173.8795 |        -135.6293 |
[32m[20221214 14:33:35 @agent_ppo2.py:185][0m |           0.0010 |         158.0099 |        -135.6192 |
[32m[20221214 14:33:35 @agent_ppo2.py:185][0m |           0.0045 |         156.6340 |        -135.6457 |
[32m[20221214 14:33:35 @agent_ppo2.py:185][0m |           0.0014 |         153.9469 |        -135.9097 |
[32m[20221214 14:33:35 @agent_ppo2.py:185][0m |           0.0087 |         162.7111 |        -135.9284 |
[32m[20221214 14:33:35 @agent_ppo2.py:185][0m |           0.0010 |         153.0160 |        -135.8426 |
[32m[20221214 14:33:35 @agent_ppo2.py:185][0m |          -0.0020 |         151.8243 |        -136.1464 |
[32m[20221214 14:33:35 @agent_ppo2.py:185][0m |          -0.0018 |         151.7633 |        -136.1177 |
[32m[20221214 14:33:35 @agent_ppo2.py:185][0m |           0.0139 |         168.9921 |        -136.4257 |
[32m[20221214 14:33:35 @agent_ppo2.py:185][0m |          -0.0007 |         151.7753 |        -135.9504 |
[32m[20221214 14:33:35 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:33:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 739.00
[32m[20221214 14:33:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.11
[32m[20221214 14:33:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.72
[32m[20221214 14:33:36 @agent_ppo2.py:143][0m Total time:      35.56 min
[32m[20221214 14:33:36 @agent_ppo2.py:145][0m 3268608 total steps have happened
[32m[20221214 14:33:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1596 --------------------------#
[32m[20221214 14:33:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:36 @agent_ppo2.py:185][0m |          -0.0021 |         209.8076 |        -137.4446 |
[32m[20221214 14:33:36 @agent_ppo2.py:185][0m |          -0.0041 |         191.2197 |        -137.5339 |
[32m[20221214 14:33:36 @agent_ppo2.py:185][0m |          -0.0032 |         180.8704 |        -137.6581 |
[32m[20221214 14:33:36 @agent_ppo2.py:185][0m |          -0.0055 |         173.6238 |        -137.8772 |
[32m[20221214 14:33:36 @agent_ppo2.py:185][0m |          -0.0035 |         168.2545 |        -138.0623 |
[32m[20221214 14:33:36 @agent_ppo2.py:185][0m |           0.0112 |         178.4126 |        -137.9790 |
[32m[20221214 14:33:36 @agent_ppo2.py:185][0m |          -0.0035 |         163.2803 |        -138.4101 |
[32m[20221214 14:33:37 @agent_ppo2.py:185][0m |          -0.0050 |         161.4955 |        -138.4074 |
[32m[20221214 14:33:37 @agent_ppo2.py:185][0m |          -0.0047 |         160.5665 |        -138.5772 |
[32m[20221214 14:33:37 @agent_ppo2.py:185][0m |          -0.0042 |         159.6400 |        -138.5470 |
[32m[20221214 14:33:37 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:33:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 758.86
[32m[20221214 14:33:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.87
[32m[20221214 14:33:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.07
[32m[20221214 14:33:37 @agent_ppo2.py:143][0m Total time:      35.58 min
[32m[20221214 14:33:37 @agent_ppo2.py:145][0m 3270656 total steps have happened
[32m[20221214 14:33:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1597 --------------------------#
[32m[20221214 14:33:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:33:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:37 @agent_ppo2.py:185][0m |          -0.0029 |         218.0104 |        -140.3787 |
[32m[20221214 14:33:37 @agent_ppo2.py:185][0m |          -0.0000 |         205.7064 |        -140.3736 |
[32m[20221214 14:33:37 @agent_ppo2.py:185][0m |           0.0013 |         200.7063 |        -140.4112 |
[32m[20221214 14:33:37 @agent_ppo2.py:185][0m |           0.0002 |         197.9310 |        -140.4186 |
[32m[20221214 14:33:38 @agent_ppo2.py:185][0m |          -0.0026 |         196.5433 |        -140.4652 |
[32m[20221214 14:33:38 @agent_ppo2.py:185][0m |          -0.0010 |         195.5937 |        -140.5421 |
[32m[20221214 14:33:38 @agent_ppo2.py:185][0m |          -0.0038 |         194.9044 |        -140.4264 |
[32m[20221214 14:33:38 @agent_ppo2.py:185][0m |           0.0011 |         195.5629 |        -140.3221 |
[32m[20221214 14:33:38 @agent_ppo2.py:185][0m |          -0.0028 |         194.0562 |        -140.5637 |
[32m[20221214 14:33:38 @agent_ppo2.py:185][0m |          -0.0030 |         193.3600 |        -140.4645 |
[32m[20221214 14:33:38 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:33:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.54
[32m[20221214 14:33:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.07
[32m[20221214 14:33:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.84
[32m[20221214 14:33:38 @agent_ppo2.py:143][0m Total time:      35.60 min
[32m[20221214 14:33:38 @agent_ppo2.py:145][0m 3272704 total steps have happened
[32m[20221214 14:33:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1598 --------------------------#
[32m[20221214 14:33:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:33:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:39 @agent_ppo2.py:185][0m |          -0.0014 |         203.3455 |        -141.5801 |
[32m[20221214 14:33:39 @agent_ppo2.py:185][0m |          -0.0033 |         194.1018 |        -141.0595 |
[32m[20221214 14:33:39 @agent_ppo2.py:185][0m |           0.0105 |         208.7202 |        -141.4822 |
[32m[20221214 14:33:39 @agent_ppo2.py:185][0m |           0.0012 |         192.2133 |        -140.9317 |
[32m[20221214 14:33:39 @agent_ppo2.py:185][0m |          -0.0039 |         188.5462 |        -141.1373 |
[32m[20221214 14:33:39 @agent_ppo2.py:185][0m |          -0.0031 |         187.8179 |        -140.8284 |
[32m[20221214 14:33:39 @agent_ppo2.py:185][0m |          -0.0006 |         187.3972 |        -141.0096 |
[32m[20221214 14:33:39 @agent_ppo2.py:185][0m |          -0.0022 |         186.1517 |        -140.6495 |
[32m[20221214 14:33:39 @agent_ppo2.py:185][0m |          -0.0029 |         185.9352 |        -140.9040 |
[32m[20221214 14:33:39 @agent_ppo2.py:185][0m |          -0.0014 |         185.7658 |        -140.7998 |
[32m[20221214 14:33:39 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:33:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 808.54
[32m[20221214 14:33:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.37
[32m[20221214 14:33:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 810.59
[32m[20221214 14:33:40 @agent_ppo2.py:143][0m Total time:      35.63 min
[32m[20221214 14:33:40 @agent_ppo2.py:145][0m 3274752 total steps have happened
[32m[20221214 14:33:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1599 --------------------------#
[32m[20221214 14:33:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:40 @agent_ppo2.py:185][0m |           0.0051 |         223.7239 |        -139.0372 |
[32m[20221214 14:33:40 @agent_ppo2.py:185][0m |           0.0001 |         210.4989 |        -139.0470 |
[32m[20221214 14:33:40 @agent_ppo2.py:185][0m |          -0.0009 |         204.4913 |        -139.2408 |
[32m[20221214 14:33:40 @agent_ppo2.py:185][0m |          -0.0024 |         201.7606 |        -139.2937 |
[32m[20221214 14:33:40 @agent_ppo2.py:185][0m |          -0.0013 |         200.2772 |        -139.4194 |
[32m[20221214 14:33:40 @agent_ppo2.py:185][0m |          -0.0034 |         199.1585 |        -139.1183 |
[32m[20221214 14:33:41 @agent_ppo2.py:185][0m |          -0.0020 |         198.1192 |        -139.3287 |
[32m[20221214 14:33:41 @agent_ppo2.py:185][0m |           0.0014 |         197.6679 |        -139.7734 |
[32m[20221214 14:33:41 @agent_ppo2.py:185][0m |          -0.0027 |         197.0204 |        -139.8055 |
[32m[20221214 14:33:41 @agent_ppo2.py:185][0m |           0.0180 |         228.5734 |        -139.7926 |
[32m[20221214 14:33:41 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:33:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 806.32
[32m[20221214 14:33:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.62
[32m[20221214 14:33:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.48
[32m[20221214 14:33:41 @agent_ppo2.py:143][0m Total time:      35.65 min
[32m[20221214 14:33:41 @agent_ppo2.py:145][0m 3276800 total steps have happened
[32m[20221214 14:33:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1600 --------------------------#
[32m[20221214 14:33:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:33:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:41 @agent_ppo2.py:185][0m |          -0.0036 |         232.6731 |        -139.0762 |
[32m[20221214 14:33:41 @agent_ppo2.py:185][0m |          -0.0016 |         204.0846 |        -139.4270 |
[32m[20221214 14:33:41 @agent_ppo2.py:185][0m |           0.0021 |         193.8453 |        -139.0997 |
[32m[20221214 14:33:42 @agent_ppo2.py:185][0m |          -0.0016 |         179.4414 |        -138.7078 |
[32m[20221214 14:33:42 @agent_ppo2.py:185][0m |           0.0063 |         187.1287 |        -138.5726 |
[32m[20221214 14:33:42 @agent_ppo2.py:185][0m |           0.0045 |         174.6731 |        -138.9523 |
[32m[20221214 14:33:42 @agent_ppo2.py:185][0m |          -0.0046 |         170.0634 |        -138.6400 |
[32m[20221214 14:33:42 @agent_ppo2.py:185][0m |           0.0108 |         180.6964 |        -138.5000 |
[32m[20221214 14:33:42 @agent_ppo2.py:185][0m |          -0.0050 |         166.9687 |        -138.7060 |
[32m[20221214 14:33:42 @agent_ppo2.py:185][0m |          -0.0037 |         165.1506 |        -138.5419 |
[32m[20221214 14:33:42 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:33:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 761.83
[32m[20221214 14:33:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.39
[32m[20221214 14:33:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.66
[32m[20221214 14:33:42 @agent_ppo2.py:143][0m Total time:      35.67 min
[32m[20221214 14:33:42 @agent_ppo2.py:145][0m 3278848 total steps have happened
[32m[20221214 14:33:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1601 --------------------------#
[32m[20221214 14:33:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:43 @agent_ppo2.py:185][0m |          -0.0020 |         197.6255 |        -139.6390 |
[32m[20221214 14:33:43 @agent_ppo2.py:185][0m |           0.0003 |         183.8724 |        -139.3333 |
[32m[20221214 14:33:43 @agent_ppo2.py:185][0m |          -0.0018 |         179.2121 |        -139.3014 |
[32m[20221214 14:33:43 @agent_ppo2.py:185][0m |           0.0124 |         192.8650 |        -138.9367 |
[32m[20221214 14:33:43 @agent_ppo2.py:185][0m |          -0.0046 |         174.8885 |        -138.8573 |
[32m[20221214 14:33:43 @agent_ppo2.py:185][0m |          -0.0031 |         171.6363 |        -138.7731 |
[32m[20221214 14:33:43 @agent_ppo2.py:185][0m |          -0.0022 |         170.2354 |        -138.9585 |
[32m[20221214 14:33:43 @agent_ppo2.py:185][0m |          -0.0025 |         169.1510 |        -138.3506 |
[32m[20221214 14:33:43 @agent_ppo2.py:185][0m |          -0.0027 |         168.9819 |        -138.3643 |
[32m[20221214 14:33:43 @agent_ppo2.py:185][0m |          -0.0020 |         169.8679 |        -138.3053 |
[32m[20221214 14:33:43 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:33:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.17
[32m[20221214 14:33:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.84
[32m[20221214 14:33:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.06
[32m[20221214 14:33:44 @agent_ppo2.py:143][0m Total time:      35.69 min
[32m[20221214 14:33:44 @agent_ppo2.py:145][0m 3280896 total steps have happened
[32m[20221214 14:33:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1602 --------------------------#
[32m[20221214 14:33:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:44 @agent_ppo2.py:185][0m |           0.0026 |         196.0955 |        -136.7175 |
[32m[20221214 14:33:44 @agent_ppo2.py:185][0m |           0.0003 |         183.5828 |        -136.7813 |
[32m[20221214 14:33:44 @agent_ppo2.py:185][0m |          -0.0028 |         179.9319 |        -136.8168 |
[32m[20221214 14:33:44 @agent_ppo2.py:185][0m |           0.0043 |         182.2707 |        -136.8264 |
[32m[20221214 14:33:44 @agent_ppo2.py:185][0m |           0.0092 |         188.2676 |        -137.1263 |
[32m[20221214 14:33:44 @agent_ppo2.py:185][0m |           0.0179 |         196.8128 |        -136.9511 |
[32m[20221214 14:33:45 @agent_ppo2.py:185][0m |          -0.0034 |         174.8073 |        -136.9281 |
[32m[20221214 14:33:45 @agent_ppo2.py:185][0m |           0.0027 |         175.6837 |        -136.9459 |
[32m[20221214 14:33:45 @agent_ppo2.py:185][0m |           0.0000 |         172.3051 |        -137.1747 |
[32m[20221214 14:33:45 @agent_ppo2.py:185][0m |          -0.0032 |         172.6729 |        -137.0418 |
[32m[20221214 14:33:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:33:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.16
[32m[20221214 14:33:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.71
[32m[20221214 14:33:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.89
[32m[20221214 14:33:45 @agent_ppo2.py:143][0m Total time:      35.72 min
[32m[20221214 14:33:45 @agent_ppo2.py:145][0m 3282944 total steps have happened
[32m[20221214 14:33:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1603 --------------------------#
[32m[20221214 14:33:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:45 @agent_ppo2.py:185][0m |           0.0053 |         216.0396 |        -137.0325 |
[32m[20221214 14:33:45 @agent_ppo2.py:185][0m |          -0.0033 |         203.6399 |        -136.6495 |
[32m[20221214 14:33:46 @agent_ppo2.py:185][0m |          -0.0029 |         201.3707 |        -136.9704 |
[32m[20221214 14:33:46 @agent_ppo2.py:185][0m |          -0.0040 |         199.8871 |        -136.9059 |
[32m[20221214 14:33:46 @agent_ppo2.py:185][0m |          -0.0018 |         198.2627 |        -136.7617 |
[32m[20221214 14:33:46 @agent_ppo2.py:185][0m |           0.0045 |         198.9581 |        -137.1195 |
[32m[20221214 14:33:46 @agent_ppo2.py:185][0m |           0.0019 |         196.6495 |        -137.2979 |
[32m[20221214 14:33:46 @agent_ppo2.py:185][0m |          -0.0018 |         192.6259 |        -136.9946 |
[32m[20221214 14:33:46 @agent_ppo2.py:185][0m |          -0.0018 |         191.9371 |        -136.9088 |
[32m[20221214 14:33:46 @agent_ppo2.py:185][0m |           0.0072 |         207.4098 |        -136.9355 |
[32m[20221214 14:33:46 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:33:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.51
[32m[20221214 14:33:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 815.28
[32m[20221214 14:33:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.43
[32m[20221214 14:33:46 @agent_ppo2.py:143][0m Total time:      35.74 min
[32m[20221214 14:33:46 @agent_ppo2.py:145][0m 3284992 total steps have happened
[32m[20221214 14:33:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1604 --------------------------#
[32m[20221214 14:33:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:47 @agent_ppo2.py:185][0m |          -0.0025 |         222.6982 |        -138.3943 |
[32m[20221214 14:33:47 @agent_ppo2.py:185][0m |          -0.0011 |         207.9111 |        -138.5371 |
[32m[20221214 14:33:47 @agent_ppo2.py:185][0m |           0.0116 |         226.8539 |        -138.8751 |
[32m[20221214 14:33:47 @agent_ppo2.py:185][0m |          -0.0026 |         202.9669 |        -138.7833 |
[32m[20221214 14:33:47 @agent_ppo2.py:185][0m |          -0.0015 |         201.8523 |        -138.8916 |
[32m[20221214 14:33:47 @agent_ppo2.py:185][0m |          -0.0026 |         200.4372 |        -138.7097 |
[32m[20221214 14:33:47 @agent_ppo2.py:185][0m |           0.0050 |         204.7075 |        -138.7283 |
[32m[20221214 14:33:47 @agent_ppo2.py:185][0m |          -0.0028 |         199.2135 |        -138.5556 |
[32m[20221214 14:33:47 @agent_ppo2.py:185][0m |          -0.0020 |         198.6523 |        -138.7251 |
[32m[20221214 14:33:48 @agent_ppo2.py:185][0m |          -0.0042 |         197.5961 |        -138.6706 |
[32m[20221214 14:33:48 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:33:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.81
[32m[20221214 14:33:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.53
[32m[20221214 14:33:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.78
[32m[20221214 14:33:48 @agent_ppo2.py:143][0m Total time:      35.76 min
[32m[20221214 14:33:48 @agent_ppo2.py:145][0m 3287040 total steps have happened
[32m[20221214 14:33:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1605 --------------------------#
[32m[20221214 14:33:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:48 @agent_ppo2.py:185][0m |          -0.0011 |         219.7460 |        -139.2162 |
[32m[20221214 14:33:48 @agent_ppo2.py:185][0m |           0.0019 |         206.3886 |        -139.3481 |
[32m[20221214 14:33:48 @agent_ppo2.py:185][0m |          -0.0010 |         203.0300 |        -139.3219 |
[32m[20221214 14:33:48 @agent_ppo2.py:185][0m |          -0.0007 |         200.4566 |        -138.6303 |
[32m[20221214 14:33:48 @agent_ppo2.py:185][0m |           0.0041 |         202.4333 |        -139.5131 |
[32m[20221214 14:33:49 @agent_ppo2.py:185][0m |          -0.0008 |         198.7868 |        -139.5833 |
[32m[20221214 14:33:49 @agent_ppo2.py:185][0m |           0.0074 |         205.0205 |        -139.7989 |
[32m[20221214 14:33:49 @agent_ppo2.py:185][0m |          -0.0034 |         197.7520 |        -139.4089 |
[32m[20221214 14:33:49 @agent_ppo2.py:185][0m |          -0.0022 |         197.2345 |        -139.8192 |
[32m[20221214 14:33:49 @agent_ppo2.py:185][0m |          -0.0046 |         196.6128 |        -139.2356 |
[32m[20221214 14:33:49 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:33:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.37
[32m[20221214 14:33:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 803.82
[32m[20221214 14:33:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.68
[32m[20221214 14:33:49 @agent_ppo2.py:143][0m Total time:      35.78 min
[32m[20221214 14:33:49 @agent_ppo2.py:145][0m 3289088 total steps have happened
[32m[20221214 14:33:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1606 --------------------------#
[32m[20221214 14:33:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:49 @agent_ppo2.py:185][0m |          -0.0016 |         237.2448 |        -139.8519 |
[32m[20221214 14:33:49 @agent_ppo2.py:185][0m |          -0.0008 |         228.3593 |        -140.3366 |
[32m[20221214 14:33:50 @agent_ppo2.py:185][0m |          -0.0041 |         224.6926 |        -140.5140 |
[32m[20221214 14:33:50 @agent_ppo2.py:185][0m |          -0.0013 |         222.2548 |        -140.5447 |
[32m[20221214 14:33:50 @agent_ppo2.py:185][0m |          -0.0022 |         220.7032 |        -140.4601 |
[32m[20221214 14:33:50 @agent_ppo2.py:185][0m |          -0.0009 |         219.6006 |        -140.6525 |
[32m[20221214 14:33:50 @agent_ppo2.py:185][0m |          -0.0007 |         218.2552 |        -140.2751 |
[32m[20221214 14:33:50 @agent_ppo2.py:185][0m |          -0.0032 |         217.5681 |        -140.4859 |
[32m[20221214 14:33:50 @agent_ppo2.py:185][0m |          -0.0015 |         216.5901 |        -141.2879 |
[32m[20221214 14:33:50 @agent_ppo2.py:185][0m |           0.0098 |         223.0964 |        -141.1324 |
[32m[20221214 14:33:50 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:33:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.34
[32m[20221214 14:33:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.88
[32m[20221214 14:33:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.06
[32m[20221214 14:33:50 @agent_ppo2.py:143][0m Total time:      35.81 min
[32m[20221214 14:33:50 @agent_ppo2.py:145][0m 3291136 total steps have happened
[32m[20221214 14:33:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1607 --------------------------#
[32m[20221214 14:33:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:51 @agent_ppo2.py:185][0m |          -0.0006 |         214.5684 |        -141.5800 |
[32m[20221214 14:33:51 @agent_ppo2.py:185][0m |          -0.0018 |         197.4961 |        -141.6995 |
[32m[20221214 14:33:51 @agent_ppo2.py:185][0m |           0.0053 |         194.4402 |        -141.7836 |
[32m[20221214 14:33:51 @agent_ppo2.py:185][0m |          -0.0009 |         188.9192 |        -141.7563 |
[32m[20221214 14:33:51 @agent_ppo2.py:185][0m |           0.0027 |         187.3192 |        -141.8680 |
[32m[20221214 14:33:51 @agent_ppo2.py:185][0m |          -0.0039 |         184.7670 |        -141.7869 |
[32m[20221214 14:33:51 @agent_ppo2.py:185][0m |          -0.0012 |         183.8172 |        -141.8833 |
[32m[20221214 14:33:51 @agent_ppo2.py:185][0m |          -0.0032 |         182.0561 |        -141.8201 |
[32m[20221214 14:33:52 @agent_ppo2.py:185][0m |          -0.0035 |         181.1426 |        -141.8138 |
[32m[20221214 14:33:52 @agent_ppo2.py:185][0m |          -0.0014 |         179.5250 |        -141.9717 |
[32m[20221214 14:33:52 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:33:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.88
[32m[20221214 14:33:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.56
[32m[20221214 14:33:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.67
[32m[20221214 14:33:52 @agent_ppo2.py:143][0m Total time:      35.83 min
[32m[20221214 14:33:52 @agent_ppo2.py:145][0m 3293184 total steps have happened
[32m[20221214 14:33:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1608 --------------------------#
[32m[20221214 14:33:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:33:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:52 @agent_ppo2.py:185][0m |          -0.0017 |         230.4566 |        -143.2315 |
[32m[20221214 14:33:52 @agent_ppo2.py:185][0m |           0.0003 |         213.7800 |        -142.9751 |
[32m[20221214 14:33:52 @agent_ppo2.py:185][0m |          -0.0018 |         204.9587 |        -142.9687 |
[32m[20221214 14:33:52 @agent_ppo2.py:185][0m |           0.0078 |         209.3832 |        -142.9861 |
[32m[20221214 14:33:52 @agent_ppo2.py:185][0m |          -0.0040 |         199.4919 |        -142.9776 |
[32m[20221214 14:33:53 @agent_ppo2.py:185][0m |          -0.0036 |         193.6293 |        -142.9751 |
[32m[20221214 14:33:53 @agent_ppo2.py:185][0m |          -0.0009 |         190.6356 |        -142.7895 |
[32m[20221214 14:33:53 @agent_ppo2.py:185][0m |           0.0077 |         206.4300 |        -142.6693 |
[32m[20221214 14:33:53 @agent_ppo2.py:185][0m |          -0.0061 |         186.8387 |        -142.8275 |
[32m[20221214 14:33:53 @agent_ppo2.py:185][0m |          -0.0024 |         184.6760 |        -142.6255 |
[32m[20221214 14:33:53 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:33:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.85
[32m[20221214 14:33:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 806.85
[32m[20221214 14:33:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.32
[32m[20221214 14:33:53 @agent_ppo2.py:143][0m Total time:      35.85 min
[32m[20221214 14:33:53 @agent_ppo2.py:145][0m 3295232 total steps have happened
[32m[20221214 14:33:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1609 --------------------------#
[32m[20221214 14:33:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:33:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:53 @agent_ppo2.py:185][0m |          -0.0021 |         191.5950 |        -141.8136 |
[32m[20221214 14:33:54 @agent_ppo2.py:185][0m |           0.0002 |         168.2395 |        -141.6483 |
[32m[20221214 14:33:54 @agent_ppo2.py:185][0m |          -0.0039 |         156.4725 |        -141.5468 |
[32m[20221214 14:33:54 @agent_ppo2.py:185][0m |          -0.0010 |         150.6447 |        -141.4349 |
[32m[20221214 14:33:54 @agent_ppo2.py:185][0m |          -0.0038 |         147.5448 |        -141.6418 |
[32m[20221214 14:33:54 @agent_ppo2.py:185][0m |          -0.0014 |         145.9079 |        -141.4718 |
[32m[20221214 14:33:54 @agent_ppo2.py:185][0m |          -0.0014 |         144.3461 |        -141.5301 |
[32m[20221214 14:33:54 @agent_ppo2.py:185][0m |           0.0015 |         142.1378 |        -141.2695 |
[32m[20221214 14:33:54 @agent_ppo2.py:185][0m |          -0.0031 |         141.3464 |        -141.1252 |
[32m[20221214 14:33:54 @agent_ppo2.py:185][0m |          -0.0040 |         139.8474 |        -141.6788 |
[32m[20221214 14:33:54 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:33:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.51
[32m[20221214 14:33:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.34
[32m[20221214 14:33:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.05
[32m[20221214 14:33:55 @agent_ppo2.py:143][0m Total time:      35.87 min
[32m[20221214 14:33:55 @agent_ppo2.py:145][0m 3297280 total steps have happened
[32m[20221214 14:33:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1610 --------------------------#
[32m[20221214 14:33:55 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221214 14:33:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:55 @agent_ppo2.py:185][0m |          -0.0004 |         238.9175 |        -141.3517 |
[32m[20221214 14:33:55 @agent_ppo2.py:185][0m |          -0.0007 |         229.4791 |        -140.9240 |
[32m[20221214 14:33:55 @agent_ppo2.py:185][0m |          -0.0034 |         225.8679 |        -140.4489 |
[32m[20221214 14:33:55 @agent_ppo2.py:185][0m |          -0.0021 |         224.7298 |        -140.5481 |
[32m[20221214 14:33:55 @agent_ppo2.py:185][0m |          -0.0023 |         223.6066 |        -140.6114 |
[32m[20221214 14:33:55 @agent_ppo2.py:185][0m |           0.0049 |         227.8307 |        -140.5531 |
[32m[20221214 14:33:55 @agent_ppo2.py:185][0m |           0.0014 |         222.4525 |        -140.2539 |
[32m[20221214 14:33:56 @agent_ppo2.py:185][0m |          -0.0022 |         219.7565 |        -140.3514 |
[32m[20221214 14:33:56 @agent_ppo2.py:185][0m |          -0.0036 |         219.0320 |        -140.3232 |
[32m[20221214 14:33:56 @agent_ppo2.py:185][0m |          -0.0003 |         218.8425 |        -140.0656 |
[32m[20221214 14:33:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:33:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.71
[32m[20221214 14:33:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.57
[32m[20221214 14:33:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.50
[32m[20221214 14:33:56 @agent_ppo2.py:143][0m Total time:      35.90 min
[32m[20221214 14:33:56 @agent_ppo2.py:145][0m 3299328 total steps have happened
[32m[20221214 14:33:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1611 --------------------------#
[32m[20221214 14:33:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:33:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:56 @agent_ppo2.py:185][0m |           0.0036 |         229.4375 |        -139.0022 |
[32m[20221214 14:33:56 @agent_ppo2.py:185][0m |           0.0004 |         220.5422 |        -139.0301 |
[32m[20221214 14:33:56 @agent_ppo2.py:185][0m |           0.0042 |         224.5592 |        -139.2632 |
[32m[20221214 14:33:57 @agent_ppo2.py:185][0m |           0.0029 |         219.4901 |        -139.6159 |
[32m[20221214 14:33:57 @agent_ppo2.py:185][0m |          -0.0025 |         215.3684 |        -139.3531 |
[32m[20221214 14:33:57 @agent_ppo2.py:185][0m |          -0.0019 |         215.0981 |        -139.8836 |
[32m[20221214 14:33:57 @agent_ppo2.py:185][0m |           0.0098 |         220.4420 |        -139.8208 |
[32m[20221214 14:33:57 @agent_ppo2.py:185][0m |          -0.0009 |         214.3830 |        -140.0243 |
[32m[20221214 14:33:57 @agent_ppo2.py:185][0m |          -0.0034 |         213.4581 |        -140.0874 |
[32m[20221214 14:33:57 @agent_ppo2.py:185][0m |          -0.0007 |         212.9600 |        -140.1649 |
[32m[20221214 14:33:57 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:33:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.33
[32m[20221214 14:33:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.09
[32m[20221214 14:33:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.57
[32m[20221214 14:33:57 @agent_ppo2.py:143][0m Total time:      35.92 min
[32m[20221214 14:33:57 @agent_ppo2.py:145][0m 3301376 total steps have happened
[32m[20221214 14:33:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1612 --------------------------#
[32m[20221214 14:33:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:58 @agent_ppo2.py:185][0m |          -0.0024 |         213.3560 |        -142.7329 |
[32m[20221214 14:33:58 @agent_ppo2.py:185][0m |           0.0010 |         201.1761 |        -142.6645 |
[32m[20221214 14:33:58 @agent_ppo2.py:185][0m |           0.0089 |         214.1776 |        -142.5368 |
[32m[20221214 14:33:58 @agent_ppo2.py:185][0m |           0.0110 |         209.3811 |        -142.0757 |
[32m[20221214 14:33:58 @agent_ppo2.py:185][0m |          -0.0020 |         181.2723 |        -142.3511 |
[32m[20221214 14:33:58 @agent_ppo2.py:185][0m |          -0.0039 |         178.7975 |        -142.4236 |
[32m[20221214 14:33:58 @agent_ppo2.py:185][0m |          -0.0014 |         176.8264 |        -142.2362 |
[32m[20221214 14:33:58 @agent_ppo2.py:185][0m |          -0.0019 |         174.0492 |        -142.1758 |
[32m[20221214 14:33:58 @agent_ppo2.py:185][0m |           0.0058 |         183.4026 |        -142.1359 |
[32m[20221214 14:33:58 @agent_ppo2.py:185][0m |          -0.0035 |         171.3785 |        -141.8791 |
[32m[20221214 14:33:58 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:33:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 809.36
[32m[20221214 14:33:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.63
[32m[20221214 14:33:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.84
[32m[20221214 14:33:59 @agent_ppo2.py:143][0m Total time:      35.94 min
[32m[20221214 14:33:59 @agent_ppo2.py:145][0m 3303424 total steps have happened
[32m[20221214 14:33:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1613 --------------------------#
[32m[20221214 14:33:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:33:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:33:59 @agent_ppo2.py:185][0m |          -0.0043 |         215.1222 |        -140.7163 |
[32m[20221214 14:33:59 @agent_ppo2.py:185][0m |          -0.0016 |         204.4827 |        -140.4953 |
[32m[20221214 14:33:59 @agent_ppo2.py:185][0m |          -0.0040 |         200.2872 |        -140.8027 |
[32m[20221214 14:33:59 @agent_ppo2.py:185][0m |           0.0054 |         204.1780 |        -141.1190 |
[32m[20221214 14:33:59 @agent_ppo2.py:185][0m |          -0.0057 |         195.7505 |        -140.8975 |
[32m[20221214 14:33:59 @agent_ppo2.py:185][0m |           0.0115 |         211.0994 |        -140.7942 |
[32m[20221214 14:34:00 @agent_ppo2.py:185][0m |          -0.0034 |         192.1687 |        -141.1377 |
[32m[20221214 14:34:00 @agent_ppo2.py:185][0m |          -0.0034 |         190.1288 |        -141.4466 |
[32m[20221214 14:34:00 @agent_ppo2.py:185][0m |          -0.0042 |         188.8709 |        -140.9450 |
[32m[20221214 14:34:00 @agent_ppo2.py:185][0m |          -0.0028 |         188.1608 |        -141.4738 |
[32m[20221214 14:34:00 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:34:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.54
[32m[20221214 14:34:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.03
[32m[20221214 14:34:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.42
[32m[20221214 14:34:00 @agent_ppo2.py:143][0m Total time:      35.97 min
[32m[20221214 14:34:00 @agent_ppo2.py:145][0m 3305472 total steps have happened
[32m[20221214 14:34:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1614 --------------------------#
[32m[20221214 14:34:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:00 @agent_ppo2.py:185][0m |          -0.0002 |         184.9688 |        -143.6762 |
[32m[20221214 14:34:00 @agent_ppo2.py:185][0m |           0.0023 |         170.2448 |        -143.5185 |
[32m[20221214 14:34:01 @agent_ppo2.py:185][0m |          -0.0034 |         165.1547 |        -143.3558 |
[32m[20221214 14:34:01 @agent_ppo2.py:185][0m |          -0.0026 |         164.1333 |        -143.3754 |
[32m[20221214 14:34:01 @agent_ppo2.py:185][0m |          -0.0042 |         162.4338 |        -143.7174 |
[32m[20221214 14:34:01 @agent_ppo2.py:185][0m |          -0.0025 |         161.0442 |        -143.5222 |
[32m[20221214 14:34:01 @agent_ppo2.py:185][0m |          -0.0024 |         160.3276 |        -143.5182 |
[32m[20221214 14:34:01 @agent_ppo2.py:185][0m |          -0.0025 |         159.0620 |        -143.4018 |
[32m[20221214 14:34:01 @agent_ppo2.py:185][0m |          -0.0019 |         159.3155 |        -143.6424 |
[32m[20221214 14:34:01 @agent_ppo2.py:185][0m |           0.0099 |         167.6260 |        -143.2455 |
[32m[20221214 14:34:01 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:34:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.79
[32m[20221214 14:34:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.14
[32m[20221214 14:34:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.40
[32m[20221214 14:34:01 @agent_ppo2.py:143][0m Total time:      35.99 min
[32m[20221214 14:34:01 @agent_ppo2.py:145][0m 3307520 total steps have happened
[32m[20221214 14:34:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1615 --------------------------#
[32m[20221214 14:34:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:02 @agent_ppo2.py:185][0m |          -0.0026 |         186.2370 |        -141.6013 |
[32m[20221214 14:34:02 @agent_ppo2.py:185][0m |           0.0047 |         168.3816 |        -142.1549 |
[32m[20221214 14:34:02 @agent_ppo2.py:185][0m |          -0.0007 |         157.1887 |        -141.9237 |
[32m[20221214 14:34:02 @agent_ppo2.py:185][0m |          -0.0005 |         154.2226 |        -142.3761 |
[32m[20221214 14:34:02 @agent_ppo2.py:185][0m |           0.0049 |         160.0668 |        -142.1677 |
[32m[20221214 14:34:02 @agent_ppo2.py:185][0m |           0.0075 |         164.0833 |        -142.6793 |
[32m[20221214 14:34:02 @agent_ppo2.py:185][0m |           0.0057 |         159.6726 |        -142.3032 |
[32m[20221214 14:34:02 @agent_ppo2.py:185][0m |          -0.0060 |         147.7085 |        -142.3770 |
[32m[20221214 14:34:02 @agent_ppo2.py:185][0m |          -0.0028 |         145.1144 |        -142.6237 |
[32m[20221214 14:34:03 @agent_ppo2.py:185][0m |          -0.0041 |         144.6660 |        -142.6555 |
[32m[20221214 14:34:03 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:34:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 791.97
[32m[20221214 14:34:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.21
[32m[20221214 14:34:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.77
[32m[20221214 14:34:03 @agent_ppo2.py:143][0m Total time:      36.01 min
[32m[20221214 14:34:03 @agent_ppo2.py:145][0m 3309568 total steps have happened
[32m[20221214 14:34:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1616 --------------------------#
[32m[20221214 14:34:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:03 @agent_ppo2.py:185][0m |           0.0009 |         237.9093 |        -143.7623 |
[32m[20221214 14:34:03 @agent_ppo2.py:185][0m |           0.0021 |         227.7283 |        -143.5872 |
[32m[20221214 14:34:03 @agent_ppo2.py:185][0m |          -0.0038 |         222.7953 |        -143.5567 |
[32m[20221214 14:34:03 @agent_ppo2.py:185][0m |          -0.0019 |         218.4930 |        -143.8463 |
[32m[20221214 14:34:03 @agent_ppo2.py:185][0m |          -0.0019 |         214.4487 |        -143.7308 |
[32m[20221214 14:34:04 @agent_ppo2.py:185][0m |           0.0041 |         216.0631 |        -143.9114 |
[32m[20221214 14:34:04 @agent_ppo2.py:185][0m |          -0.0021 |         208.9738 |        -143.8670 |
[32m[20221214 14:34:04 @agent_ppo2.py:185][0m |          -0.0031 |         207.3496 |        -143.9911 |
[32m[20221214 14:34:04 @agent_ppo2.py:185][0m |          -0.0025 |         206.0379 |        -144.1740 |
[32m[20221214 14:34:04 @agent_ppo2.py:185][0m |           0.0076 |         218.8800 |        -144.6942 |
[32m[20221214 14:34:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:34:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.95
[32m[20221214 14:34:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.65
[32m[20221214 14:34:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.82
[32m[20221214 14:34:04 @agent_ppo2.py:143][0m Total time:      36.03 min
[32m[20221214 14:34:04 @agent_ppo2.py:145][0m 3311616 total steps have happened
[32m[20221214 14:34:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1617 --------------------------#
[32m[20221214 14:34:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:04 @agent_ppo2.py:185][0m |           0.0003 |         243.6137 |        -144.8764 |
[32m[20221214 14:34:05 @agent_ppo2.py:185][0m |          -0.0010 |         237.1821 |        -145.2202 |
[32m[20221214 14:34:05 @agent_ppo2.py:185][0m |           0.0008 |         234.9375 |        -145.2606 |
[32m[20221214 14:34:05 @agent_ppo2.py:185][0m |           0.0041 |         236.0998 |        -145.2371 |
[32m[20221214 14:34:05 @agent_ppo2.py:185][0m |           0.0005 |         231.3147 |        -145.2677 |
[32m[20221214 14:34:05 @agent_ppo2.py:185][0m |           0.0081 |         244.4381 |        -145.0125 |
[32m[20221214 14:34:05 @agent_ppo2.py:185][0m |          -0.0002 |         230.3594 |        -145.3637 |
[32m[20221214 14:34:05 @agent_ppo2.py:185][0m |           0.0024 |         231.6748 |        -145.3878 |
[32m[20221214 14:34:05 @agent_ppo2.py:185][0m |          -0.0009 |         229.8109 |        -145.6158 |
[32m[20221214 14:34:05 @agent_ppo2.py:185][0m |           0.0003 |         229.4030 |        -145.5589 |
[32m[20221214 14:34:05 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:34:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.97
[32m[20221214 14:34:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.23
[32m[20221214 14:34:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.33
[32m[20221214 14:34:05 @agent_ppo2.py:143][0m Total time:      36.06 min
[32m[20221214 14:34:05 @agent_ppo2.py:145][0m 3313664 total steps have happened
[32m[20221214 14:34:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1618 --------------------------#
[32m[20221214 14:34:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:06 @agent_ppo2.py:185][0m |          -0.0018 |         224.7864 |        -143.7492 |
[32m[20221214 14:34:06 @agent_ppo2.py:185][0m |          -0.0020 |         220.4934 |        -143.2775 |
[32m[20221214 14:34:06 @agent_ppo2.py:185][0m |           0.0030 |         222.1087 |        -143.7963 |
[32m[20221214 14:34:06 @agent_ppo2.py:185][0m |          -0.0020 |         216.1272 |        -143.6418 |
[32m[20221214 14:34:06 @agent_ppo2.py:185][0m |          -0.0018 |         215.0615 |        -143.6909 |
[32m[20221214 14:34:06 @agent_ppo2.py:185][0m |          -0.0021 |         214.9674 |        -143.7490 |
[32m[20221214 14:34:06 @agent_ppo2.py:185][0m |          -0.0042 |         213.8894 |        -143.9429 |
[32m[20221214 14:34:06 @agent_ppo2.py:185][0m |          -0.0038 |         213.0660 |        -143.5940 |
[32m[20221214 14:34:07 @agent_ppo2.py:185][0m |          -0.0025 |         211.7181 |        -143.9670 |
[32m[20221214 14:34:07 @agent_ppo2.py:185][0m |          -0.0042 |         210.8440 |        -143.6797 |
[32m[20221214 14:34:07 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:34:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.13
[32m[20221214 14:34:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.37
[32m[20221214 14:34:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.80
[32m[20221214 14:34:07 @agent_ppo2.py:143][0m Total time:      36.08 min
[32m[20221214 14:34:07 @agent_ppo2.py:145][0m 3315712 total steps have happened
[32m[20221214 14:34:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1619 --------------------------#
[32m[20221214 14:34:07 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:34:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:07 @agent_ppo2.py:185][0m |           0.0159 |         249.9926 |        -144.6704 |
[32m[20221214 14:34:07 @agent_ppo2.py:185][0m |          -0.0034 |         223.1282 |        -144.9311 |
[32m[20221214 14:34:07 @agent_ppo2.py:185][0m |          -0.0011 |         220.5856 |        -145.2041 |
[32m[20221214 14:34:07 @agent_ppo2.py:185][0m |          -0.0029 |         219.0567 |        -145.6407 |
[32m[20221214 14:34:07 @agent_ppo2.py:185][0m |          -0.0030 |         218.8124 |        -145.3804 |
[32m[20221214 14:34:08 @agent_ppo2.py:185][0m |          -0.0026 |         217.8453 |        -145.0150 |
[32m[20221214 14:34:08 @agent_ppo2.py:185][0m |           0.0036 |         220.4662 |        -145.3135 |
[32m[20221214 14:34:08 @agent_ppo2.py:185][0m |          -0.0009 |         215.7805 |        -145.2458 |
[32m[20221214 14:34:08 @agent_ppo2.py:185][0m |           0.0065 |         227.6969 |        -145.3650 |
[32m[20221214 14:34:08 @agent_ppo2.py:185][0m |          -0.0030 |         215.5023 |        -145.4645 |
[32m[20221214 14:34:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:34:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.53
[32m[20221214 14:34:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.75
[32m[20221214 14:34:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.98
[32m[20221214 14:34:08 @agent_ppo2.py:143][0m Total time:      36.10 min
[32m[20221214 14:34:08 @agent_ppo2.py:145][0m 3317760 total steps have happened
[32m[20221214 14:34:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1620 --------------------------#
[32m[20221214 14:34:08 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:34:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:08 @agent_ppo2.py:185][0m |           0.0006 |         225.8794 |        -144.7713 |
[32m[20221214 14:34:08 @agent_ppo2.py:185][0m |           0.0000 |         219.0682 |        -144.5551 |
[32m[20221214 14:34:09 @agent_ppo2.py:185][0m |          -0.0017 |         215.1304 |        -144.5020 |
[32m[20221214 14:34:09 @agent_ppo2.py:185][0m |          -0.0035 |         213.0771 |        -144.6670 |
[32m[20221214 14:34:09 @agent_ppo2.py:185][0m |          -0.0026 |         211.3289 |        -144.5521 |
[32m[20221214 14:34:09 @agent_ppo2.py:185][0m |           0.0128 |         225.7996 |        -143.9508 |
[32m[20221214 14:34:09 @agent_ppo2.py:185][0m |          -0.0029 |         209.8293 |        -144.4883 |
[32m[20221214 14:34:09 @agent_ppo2.py:185][0m |          -0.0028 |         208.6737 |        -144.5189 |
[32m[20221214 14:34:09 @agent_ppo2.py:185][0m |          -0.0018 |         207.7226 |        -144.0750 |
[32m[20221214 14:34:09 @agent_ppo2.py:185][0m |          -0.0006 |         207.1609 |        -143.9796 |
[32m[20221214 14:34:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:34:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.51
[32m[20221214 14:34:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.44
[32m[20221214 14:34:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.31
[32m[20221214 14:34:09 @agent_ppo2.py:143][0m Total time:      36.12 min
[32m[20221214 14:34:09 @agent_ppo2.py:145][0m 3319808 total steps have happened
[32m[20221214 14:34:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1621 --------------------------#
[32m[20221214 14:34:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:10 @agent_ppo2.py:185][0m |          -0.0038 |         215.1221 |        -143.7998 |
[32m[20221214 14:34:10 @agent_ppo2.py:185][0m |          -0.0013 |         206.7333 |        -144.0319 |
[32m[20221214 14:34:10 @agent_ppo2.py:185][0m |          -0.0023 |         204.6052 |        -143.8594 |
[32m[20221214 14:34:10 @agent_ppo2.py:185][0m |          -0.0012 |         203.4581 |        -143.6964 |
[32m[20221214 14:34:10 @agent_ppo2.py:185][0m |          -0.0014 |         201.7827 |        -143.5701 |
[32m[20221214 14:34:10 @agent_ppo2.py:185][0m |           0.0007 |         202.5487 |        -144.1022 |
[32m[20221214 14:34:10 @agent_ppo2.py:185][0m |          -0.0006 |         200.8841 |        -143.8699 |
[32m[20221214 14:34:10 @agent_ppo2.py:185][0m |          -0.0035 |         199.7594 |        -143.6606 |
[32m[20221214 14:34:10 @agent_ppo2.py:185][0m |          -0.0038 |         199.4612 |        -144.0066 |
[32m[20221214 14:34:11 @agent_ppo2.py:185][0m |          -0.0021 |         199.7257 |        -144.0713 |
[32m[20221214 14:34:11 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:34:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.34
[32m[20221214 14:34:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.43
[32m[20221214 14:34:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.69
[32m[20221214 14:34:11 @agent_ppo2.py:143][0m Total time:      36.14 min
[32m[20221214 14:34:11 @agent_ppo2.py:145][0m 3321856 total steps have happened
[32m[20221214 14:34:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1622 --------------------------#
[32m[20221214 14:34:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:11 @agent_ppo2.py:185][0m |          -0.0019 |         180.1416 |        -146.1836 |
[32m[20221214 14:34:11 @agent_ppo2.py:185][0m |          -0.0020 |         159.8160 |        -146.4194 |
[32m[20221214 14:34:11 @agent_ppo2.py:185][0m |           0.0049 |         171.6204 |        -146.0451 |
[32m[20221214 14:34:11 @agent_ppo2.py:185][0m |          -0.0060 |         148.4558 |        -146.0135 |
[32m[20221214 14:34:11 @agent_ppo2.py:185][0m |          -0.0015 |         146.8247 |        -146.1177 |
[32m[20221214 14:34:12 @agent_ppo2.py:185][0m |          -0.0078 |         141.9967 |        -146.2717 |
[32m[20221214 14:34:12 @agent_ppo2.py:185][0m |          -0.0066 |         140.0445 |        -146.1752 |
[32m[20221214 14:34:12 @agent_ppo2.py:185][0m |          -0.0078 |         138.1130 |        -146.1224 |
[32m[20221214 14:34:12 @agent_ppo2.py:185][0m |          -0.0030 |         138.0257 |        -146.3682 |
[32m[20221214 14:34:12 @agent_ppo2.py:185][0m |          -0.0048 |         135.8110 |        -146.2262 |
[32m[20221214 14:34:12 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:34:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.99
[32m[20221214 14:34:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.78
[32m[20221214 14:34:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.84
[32m[20221214 14:34:12 @agent_ppo2.py:143][0m Total time:      36.17 min
[32m[20221214 14:34:12 @agent_ppo2.py:145][0m 3323904 total steps have happened
[32m[20221214 14:34:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1623 --------------------------#
[32m[20221214 14:34:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:12 @agent_ppo2.py:185][0m |          -0.0001 |         209.8566 |        -145.6903 |
[32m[20221214 14:34:13 @agent_ppo2.py:185][0m |          -0.0005 |         202.5519 |        -145.7080 |
[32m[20221214 14:34:13 @agent_ppo2.py:185][0m |           0.0121 |         229.4456 |        -145.7543 |
[32m[20221214 14:34:13 @agent_ppo2.py:185][0m |           0.0087 |         203.8271 |        -145.9638 |
[32m[20221214 14:34:13 @agent_ppo2.py:185][0m |          -0.0006 |         198.6706 |        -146.2126 |
[32m[20221214 14:34:13 @agent_ppo2.py:185][0m |           0.0100 |         216.5822 |        -146.2711 |
[32m[20221214 14:34:13 @agent_ppo2.py:185][0m |          -0.0009 |         199.2489 |        -146.1896 |
[32m[20221214 14:34:13 @agent_ppo2.py:185][0m |          -0.0017 |         198.1149 |        -146.2229 |
[32m[20221214 14:34:13 @agent_ppo2.py:185][0m |           0.0022 |         199.3904 |        -145.7673 |
[32m[20221214 14:34:13 @agent_ppo2.py:185][0m |          -0.0021 |         197.4026 |        -146.4838 |
[32m[20221214 14:34:13 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:34:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 804.95
[32m[20221214 14:34:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.01
[32m[20221214 14:34:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.62
[32m[20221214 14:34:14 @agent_ppo2.py:143][0m Total time:      36.19 min
[32m[20221214 14:34:14 @agent_ppo2.py:145][0m 3325952 total steps have happened
[32m[20221214 14:34:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1624 --------------------------#
[32m[20221214 14:34:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:14 @agent_ppo2.py:185][0m |          -0.0015 |         207.8197 |        -147.6137 |
[32m[20221214 14:34:14 @agent_ppo2.py:185][0m |           0.0001 |         205.0633 |        -148.0435 |
[32m[20221214 14:34:14 @agent_ppo2.py:185][0m |          -0.0029 |         203.7434 |        -147.6532 |
[32m[20221214 14:34:14 @agent_ppo2.py:185][0m |          -0.0025 |         202.6094 |        -147.8608 |
[32m[20221214 14:34:14 @agent_ppo2.py:185][0m |          -0.0016 |         201.7421 |        -148.2100 |
[32m[20221214 14:34:14 @agent_ppo2.py:185][0m |          -0.0021 |         200.4798 |        -148.2554 |
[32m[20221214 14:34:14 @agent_ppo2.py:185][0m |           0.0064 |         211.5732 |        -148.2028 |
[32m[20221214 14:34:15 @agent_ppo2.py:185][0m |          -0.0015 |         199.8928 |        -147.7249 |
[32m[20221214 14:34:15 @agent_ppo2.py:185][0m |          -0.0008 |         199.4860 |        -148.2779 |
[32m[20221214 14:34:15 @agent_ppo2.py:185][0m |          -0.0037 |         199.0406 |        -148.6638 |
[32m[20221214 14:34:15 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:34:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.90
[32m[20221214 14:34:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.29
[32m[20221214 14:34:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.90
[32m[20221214 14:34:15 @agent_ppo2.py:143][0m Total time:      36.21 min
[32m[20221214 14:34:15 @agent_ppo2.py:145][0m 3328000 total steps have happened
[32m[20221214 14:34:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1625 --------------------------#
[32m[20221214 14:34:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:15 @agent_ppo2.py:185][0m |          -0.0007 |         176.5919 |        -147.8204 |
[32m[20221214 14:34:15 @agent_ppo2.py:185][0m |           0.0031 |         169.4703 |        -148.2817 |
[32m[20221214 14:34:15 @agent_ppo2.py:185][0m |           0.0042 |         166.9280 |        -147.9317 |
[32m[20221214 14:34:16 @agent_ppo2.py:185][0m |          -0.0026 |         163.6196 |        -148.0802 |
[32m[20221214 14:34:16 @agent_ppo2.py:185][0m |           0.0041 |         162.5638 |        -147.6017 |
[32m[20221214 14:34:16 @agent_ppo2.py:185][0m |           0.0013 |         159.5820 |        -147.9345 |
[32m[20221214 14:34:16 @agent_ppo2.py:185][0m |          -0.0019 |         157.9064 |        -147.8752 |
[32m[20221214 14:34:16 @agent_ppo2.py:185][0m |          -0.0023 |         157.0441 |        -147.7704 |
[32m[20221214 14:34:16 @agent_ppo2.py:185][0m |          -0.0021 |         156.7259 |        -147.6245 |
[32m[20221214 14:34:16 @agent_ppo2.py:185][0m |           0.0048 |         161.3175 |        -147.4131 |
[32m[20221214 14:34:16 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:34:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.57
[32m[20221214 14:34:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.24
[32m[20221214 14:34:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.41
[32m[20221214 14:34:16 @agent_ppo2.py:143][0m Total time:      36.24 min
[32m[20221214 14:34:16 @agent_ppo2.py:145][0m 3330048 total steps have happened
[32m[20221214 14:34:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1626 --------------------------#
[32m[20221214 14:34:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:17 @agent_ppo2.py:185][0m |          -0.0015 |         199.4153 |        -147.2578 |
[32m[20221214 14:34:17 @agent_ppo2.py:185][0m |          -0.0038 |         195.3951 |        -147.1244 |
[32m[20221214 14:34:17 @agent_ppo2.py:185][0m |           0.0059 |         196.6035 |        -147.3919 |
[32m[20221214 14:34:17 @agent_ppo2.py:185][0m |           0.0056 |         194.4070 |        -147.5004 |
[32m[20221214 14:34:17 @agent_ppo2.py:185][0m |           0.0115 |         199.1620 |        -147.4396 |
[32m[20221214 14:34:17 @agent_ppo2.py:185][0m |          -0.0026 |         189.9991 |        -147.5582 |
[32m[20221214 14:34:17 @agent_ppo2.py:185][0m |           0.0018 |         190.5132 |        -147.2173 |
[32m[20221214 14:34:17 @agent_ppo2.py:185][0m |          -0.0017 |         189.1211 |        -147.0851 |
[32m[20221214 14:34:17 @agent_ppo2.py:185][0m |          -0.0031 |         188.2626 |        -147.0280 |
[32m[20221214 14:34:17 @agent_ppo2.py:185][0m |          -0.0027 |         187.9601 |        -146.8407 |
[32m[20221214 14:34:17 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:34:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 810.34
[32m[20221214 14:34:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.63
[32m[20221214 14:34:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.85
[32m[20221214 14:34:18 @agent_ppo2.py:143][0m Total time:      36.26 min
[32m[20221214 14:34:18 @agent_ppo2.py:145][0m 3332096 total steps have happened
[32m[20221214 14:34:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1627 --------------------------#
[32m[20221214 14:34:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:18 @agent_ppo2.py:185][0m |           0.0001 |         166.4584 |        -146.7281 |
[32m[20221214 14:34:18 @agent_ppo2.py:185][0m |          -0.0000 |         156.5256 |        -146.7471 |
[32m[20221214 14:34:18 @agent_ppo2.py:185][0m |          -0.0012 |         153.2504 |        -146.7776 |
[32m[20221214 14:34:18 @agent_ppo2.py:185][0m |           0.0069 |         161.5781 |        -146.8860 |
[32m[20221214 14:34:18 @agent_ppo2.py:185][0m |           0.0042 |         150.6220 |        -146.8455 |
[32m[20221214 14:34:18 @agent_ppo2.py:185][0m |          -0.0005 |         147.3333 |        -146.9678 |
[32m[20221214 14:34:19 @agent_ppo2.py:185][0m |          -0.0000 |         146.4723 |        -146.9044 |
[32m[20221214 14:34:19 @agent_ppo2.py:185][0m |          -0.0014 |         145.3341 |        -147.0333 |
[32m[20221214 14:34:19 @agent_ppo2.py:185][0m |          -0.0015 |         145.1955 |        -146.9164 |
[32m[20221214 14:34:19 @agent_ppo2.py:185][0m |          -0.0024 |         144.1710 |        -146.8953 |
[32m[20221214 14:34:19 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:34:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.01
[32m[20221214 14:34:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.57
[32m[20221214 14:34:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.41
[32m[20221214 14:34:19 @agent_ppo2.py:143][0m Total time:      36.28 min
[32m[20221214 14:34:19 @agent_ppo2.py:145][0m 3334144 total steps have happened
[32m[20221214 14:34:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1628 --------------------------#
[32m[20221214 14:34:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:19 @agent_ppo2.py:185][0m |          -0.0030 |         190.1813 |        -149.4035 |
[32m[20221214 14:34:19 @agent_ppo2.py:185][0m |           0.0018 |         180.4411 |        -149.3811 |
[32m[20221214 14:34:19 @agent_ppo2.py:185][0m |          -0.0028 |         175.0170 |        -149.4397 |
[32m[20221214 14:34:20 @agent_ppo2.py:185][0m |          -0.0048 |         173.1188 |        -150.0449 |
[32m[20221214 14:34:20 @agent_ppo2.py:185][0m |          -0.0038 |         171.7508 |        -149.7755 |
[32m[20221214 14:34:20 @agent_ppo2.py:185][0m |          -0.0050 |         170.3167 |        -150.1005 |
[32m[20221214 14:34:20 @agent_ppo2.py:185][0m |          -0.0036 |         170.1615 |        -150.1989 |
[32m[20221214 14:34:20 @agent_ppo2.py:185][0m |          -0.0031 |         168.1286 |        -150.2645 |
[32m[20221214 14:34:20 @agent_ppo2.py:185][0m |          -0.0055 |         167.3620 |        -150.6166 |
[32m[20221214 14:34:20 @agent_ppo2.py:185][0m |          -0.0035 |         166.7654 |        -150.3852 |
[32m[20221214 14:34:20 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:34:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 801.58
[32m[20221214 14:34:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.04
[32m[20221214 14:34:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.16
[32m[20221214 14:34:20 @agent_ppo2.py:143][0m Total time:      36.30 min
[32m[20221214 14:34:20 @agent_ppo2.py:145][0m 3336192 total steps have happened
[32m[20221214 14:34:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1629 --------------------------#
[32m[20221214 14:34:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:21 @agent_ppo2.py:185][0m |          -0.0031 |         190.0771 |        -145.3662 |
[32m[20221214 14:34:21 @agent_ppo2.py:185][0m |          -0.0029 |         180.7835 |        -145.0498 |
[32m[20221214 14:34:21 @agent_ppo2.py:185][0m |          -0.0019 |         177.5960 |        -145.3196 |
[32m[20221214 14:34:21 @agent_ppo2.py:185][0m |          -0.0048 |         175.7745 |        -144.8417 |
[32m[20221214 14:34:21 @agent_ppo2.py:185][0m |          -0.0019 |         173.5190 |        -144.7114 |
[32m[20221214 14:34:21 @agent_ppo2.py:185][0m |          -0.0046 |         172.7835 |        -144.6527 |
[32m[20221214 14:34:21 @agent_ppo2.py:185][0m |          -0.0039 |         171.4672 |        -144.7868 |
[32m[20221214 14:34:21 @agent_ppo2.py:185][0m |          -0.0010 |         170.5992 |        -144.7407 |
[32m[20221214 14:34:21 @agent_ppo2.py:185][0m |          -0.0042 |         169.9997 |        -144.3237 |
[32m[20221214 14:34:21 @agent_ppo2.py:185][0m |          -0.0035 |         168.8672 |        -144.2088 |
[32m[20221214 14:34:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:34:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.78
[32m[20221214 14:34:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.65
[32m[20221214 14:34:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.46
[32m[20221214 14:34:22 @agent_ppo2.py:143][0m Total time:      36.33 min
[32m[20221214 14:34:22 @agent_ppo2.py:145][0m 3338240 total steps have happened
[32m[20221214 14:34:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1630 --------------------------#
[32m[20221214 14:34:22 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221214 14:34:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:22 @agent_ppo2.py:185][0m |           0.0027 |         216.4589 |        -148.5122 |
[32m[20221214 14:34:22 @agent_ppo2.py:185][0m |           0.0005 |         206.0422 |        -148.5418 |
[32m[20221214 14:34:22 @agent_ppo2.py:185][0m |           0.0070 |         222.9444 |        -148.7852 |
[32m[20221214 14:34:22 @agent_ppo2.py:185][0m |          -0.0014 |         202.5129 |        -148.6924 |
[32m[20221214 14:34:22 @agent_ppo2.py:185][0m |           0.0075 |         224.6150 |        -148.8325 |
[32m[20221214 14:34:22 @agent_ppo2.py:185][0m |          -0.0038 |         201.5139 |        -149.0756 |
[32m[20221214 14:34:23 @agent_ppo2.py:185][0m |          -0.0056 |         198.4447 |        -149.2350 |
[32m[20221214 14:34:23 @agent_ppo2.py:185][0m |           0.0137 |         219.4589 |        -149.3211 |
[32m[20221214 14:34:23 @agent_ppo2.py:185][0m |          -0.0047 |         199.2705 |        -149.3157 |
[32m[20221214 14:34:23 @agent_ppo2.py:185][0m |          -0.0068 |         197.0100 |        -149.3049 |
[32m[20221214 14:34:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:34:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.83
[32m[20221214 14:34:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.60
[32m[20221214 14:34:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.62
[32m[20221214 14:34:23 @agent_ppo2.py:143][0m Total time:      36.35 min
[32m[20221214 14:34:23 @agent_ppo2.py:145][0m 3340288 total steps have happened
[32m[20221214 14:34:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1631 --------------------------#
[32m[20221214 14:34:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:23 @agent_ppo2.py:185][0m |          -0.0038 |         259.9213 |        -150.5826 |
[32m[20221214 14:34:23 @agent_ppo2.py:185][0m |          -0.0049 |         254.7330 |        -151.2335 |
[32m[20221214 14:34:23 @agent_ppo2.py:185][0m |          -0.0029 |         255.3871 |        -151.0009 |
[32m[20221214 14:34:24 @agent_ppo2.py:185][0m |          -0.0033 |         251.9937 |        -151.0675 |
[32m[20221214 14:34:24 @agent_ppo2.py:185][0m |           0.0064 |         274.0549 |        -151.0955 |
[32m[20221214 14:34:24 @agent_ppo2.py:185][0m |          -0.0023 |         251.2099 |        -150.9744 |
[32m[20221214 14:34:24 @agent_ppo2.py:185][0m |           0.0001 |         251.2016 |        -150.7309 |
[32m[20221214 14:34:24 @agent_ppo2.py:185][0m |          -0.0040 |         249.8512 |        -151.1220 |
[32m[20221214 14:34:24 @agent_ppo2.py:185][0m |           0.0008 |         252.9952 |        -150.7445 |
[32m[20221214 14:34:24 @agent_ppo2.py:185][0m |          -0.0015 |         249.5535 |        -150.8888 |
[32m[20221214 14:34:24 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:34:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.59
[32m[20221214 14:34:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.41
[32m[20221214 14:34:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.22
[32m[20221214 14:34:24 @agent_ppo2.py:143][0m Total time:      36.37 min
[32m[20221214 14:34:24 @agent_ppo2.py:145][0m 3342336 total steps have happened
[32m[20221214 14:34:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1632 --------------------------#
[32m[20221214 14:34:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:25 @agent_ppo2.py:185][0m |          -0.0012 |         257.1053 |        -149.0027 |
[32m[20221214 14:34:25 @agent_ppo2.py:185][0m |          -0.0021 |         247.2707 |        -148.6999 |
[32m[20221214 14:34:25 @agent_ppo2.py:185][0m |           0.0058 |         253.2117 |        -148.8078 |
[32m[20221214 14:34:25 @agent_ppo2.py:185][0m |          -0.0018 |         240.3515 |        -148.6022 |
[32m[20221214 14:34:25 @agent_ppo2.py:185][0m |          -0.0008 |         238.5645 |        -148.6218 |
[32m[20221214 14:34:25 @agent_ppo2.py:185][0m |          -0.0013 |         237.9152 |        -148.5850 |
[32m[20221214 14:34:25 @agent_ppo2.py:185][0m |           0.0005 |         237.2942 |        -148.6527 |
[32m[20221214 14:34:25 @agent_ppo2.py:185][0m |          -0.0036 |         235.9777 |        -148.6442 |
[32m[20221214 14:34:25 @agent_ppo2.py:185][0m |           0.0033 |         240.3675 |        -148.6485 |
[32m[20221214 14:34:25 @agent_ppo2.py:185][0m |           0.0038 |         236.2836 |        -148.5747 |
[32m[20221214 14:34:25 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:34:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.05
[32m[20221214 14:34:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.28
[32m[20221214 14:34:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.08
[32m[20221214 14:34:25 @agent_ppo2.py:143][0m Total time:      36.39 min
[32m[20221214 14:34:25 @agent_ppo2.py:145][0m 3344384 total steps have happened
[32m[20221214 14:34:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1633 --------------------------#
[32m[20221214 14:34:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:26 @agent_ppo2.py:185][0m |          -0.0025 |         234.9706 |        -151.3240 |
[32m[20221214 14:34:26 @agent_ppo2.py:185][0m |          -0.0031 |         225.8344 |        -151.8919 |
[32m[20221214 14:34:26 @agent_ppo2.py:185][0m |          -0.0021 |         223.1599 |        -151.7393 |
[32m[20221214 14:34:26 @agent_ppo2.py:185][0m |          -0.0029 |         221.0668 |        -152.1614 |
[32m[20221214 14:34:26 @agent_ppo2.py:185][0m |          -0.0027 |         218.8352 |        -152.0153 |
[32m[20221214 14:34:26 @agent_ppo2.py:185][0m |          -0.0035 |         217.0988 |        -152.1884 |
[32m[20221214 14:34:26 @agent_ppo2.py:185][0m |          -0.0029 |         216.4173 |        -152.2347 |
[32m[20221214 14:34:26 @agent_ppo2.py:185][0m |          -0.0012 |         215.7188 |        -151.8203 |
[32m[20221214 14:34:26 @agent_ppo2.py:185][0m |          -0.0000 |         215.0335 |        -152.5847 |
[32m[20221214 14:34:26 @agent_ppo2.py:185][0m |          -0.0026 |         213.5787 |        -152.2212 |
[32m[20221214 14:34:26 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:34:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.59
[32m[20221214 14:34:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.41
[32m[20221214 14:34:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.63
[32m[20221214 14:34:27 @agent_ppo2.py:143][0m Total time:      36.41 min
[32m[20221214 14:34:27 @agent_ppo2.py:145][0m 3346432 total steps have happened
[32m[20221214 14:34:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1634 --------------------------#
[32m[20221214 14:34:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:27 @agent_ppo2.py:185][0m |          -0.0015 |         243.5142 |        -150.6993 |
[32m[20221214 14:34:27 @agent_ppo2.py:185][0m |           0.0072 |         252.7053 |        -151.0724 |
[32m[20221214 14:34:27 @agent_ppo2.py:185][0m |           0.0016 |         234.3838 |        -150.5165 |
[32m[20221214 14:34:27 @agent_ppo2.py:185][0m |           0.0026 |         230.1436 |        -151.1031 |
[32m[20221214 14:34:27 @agent_ppo2.py:185][0m |           0.0011 |         229.3450 |        -151.1932 |
[32m[20221214 14:34:27 @agent_ppo2.py:185][0m |          -0.0016 |         227.9345 |        -151.1106 |
[32m[20221214 14:34:28 @agent_ppo2.py:185][0m |          -0.0007 |         226.8181 |        -151.0168 |
[32m[20221214 14:34:28 @agent_ppo2.py:185][0m |          -0.0005 |         225.6720 |        -150.9727 |
[32m[20221214 14:34:28 @agent_ppo2.py:185][0m |          -0.0034 |         225.2446 |        -150.7144 |
[32m[20221214 14:34:28 @agent_ppo2.py:185][0m |           0.0017 |         225.5175 |        -150.5233 |
[32m[20221214 14:34:28 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:34:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.16
[32m[20221214 14:34:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.47
[32m[20221214 14:34:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.60
[32m[20221214 14:34:28 @agent_ppo2.py:143][0m Total time:      36.43 min
[32m[20221214 14:34:28 @agent_ppo2.py:145][0m 3348480 total steps have happened
[32m[20221214 14:34:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1635 --------------------------#
[32m[20221214 14:34:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:28 @agent_ppo2.py:185][0m |          -0.0027 |         237.6145 |        -151.4543 |
[32m[20221214 14:34:28 @agent_ppo2.py:185][0m |          -0.0038 |         233.3674 |        -152.1898 |
[32m[20221214 14:34:29 @agent_ppo2.py:185][0m |          -0.0029 |         231.3738 |        -151.5253 |
[32m[20221214 14:34:29 @agent_ppo2.py:185][0m |          -0.0034 |         229.8138 |        -151.8559 |
[32m[20221214 14:34:29 @agent_ppo2.py:185][0m |          -0.0001 |         229.8281 |        -151.8312 |
[32m[20221214 14:34:29 @agent_ppo2.py:185][0m |           0.0030 |         231.3649 |        -151.5024 |
[32m[20221214 14:34:29 @agent_ppo2.py:185][0m |          -0.0018 |         228.5637 |        -152.2830 |
[32m[20221214 14:34:29 @agent_ppo2.py:185][0m |          -0.0043 |         227.6674 |        -152.2002 |
[32m[20221214 14:34:29 @agent_ppo2.py:185][0m |           0.0052 |         239.8581 |        -151.9863 |
[32m[20221214 14:34:29 @agent_ppo2.py:185][0m |          -0.0026 |         226.4428 |        -152.4783 |
[32m[20221214 14:34:29 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:34:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.45
[32m[20221214 14:34:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.10
[32m[20221214 14:34:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.63
[32m[20221214 14:34:29 @agent_ppo2.py:143][0m Total time:      36.46 min
[32m[20221214 14:34:29 @agent_ppo2.py:145][0m 3350528 total steps have happened
[32m[20221214 14:34:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1636 --------------------------#
[32m[20221214 14:34:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:30 @agent_ppo2.py:185][0m |           0.0107 |         319.5959 |        -150.1964 |
[32m[20221214 14:34:30 @agent_ppo2.py:185][0m |           0.0105 |         313.2679 |        -149.6843 |
[32m[20221214 14:34:30 @agent_ppo2.py:185][0m |          -0.0020 |         273.7614 |        -149.8919 |
[32m[20221214 14:34:30 @agent_ppo2.py:185][0m |          -0.0024 |         270.6180 |        -150.1951 |
[32m[20221214 14:34:30 @agent_ppo2.py:185][0m |           0.0104 |         305.1201 |        -150.0221 |
[32m[20221214 14:34:30 @agent_ppo2.py:185][0m |           0.0064 |         285.7403 |        -150.1894 |
[32m[20221214 14:34:30 @agent_ppo2.py:185][0m |          -0.0027 |         266.7738 |        -150.0607 |
[32m[20221214 14:34:30 @agent_ppo2.py:185][0m |          -0.0020 |         266.4594 |        -149.9909 |
[32m[20221214 14:34:30 @agent_ppo2.py:185][0m |          -0.0042 |         266.2448 |        -150.1405 |
[32m[20221214 14:34:31 @agent_ppo2.py:185][0m |          -0.0019 |         265.4547 |        -149.9233 |
[32m[20221214 14:34:31 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:34:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.87
[32m[20221214 14:34:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.52
[32m[20221214 14:34:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.88
[32m[20221214 14:34:31 @agent_ppo2.py:143][0m Total time:      36.48 min
[32m[20221214 14:34:31 @agent_ppo2.py:145][0m 3352576 total steps have happened
[32m[20221214 14:34:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1637 --------------------------#
[32m[20221214 14:34:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:31 @agent_ppo2.py:185][0m |          -0.0026 |         277.8791 |        -151.0547 |
[32m[20221214 14:34:31 @agent_ppo2.py:185][0m |           0.0095 |         281.3466 |        -150.9255 |
[32m[20221214 14:34:31 @agent_ppo2.py:185][0m |           0.0001 |         268.4011 |        -150.3712 |
[32m[20221214 14:34:31 @agent_ppo2.py:185][0m |           0.0026 |         270.2880 |        -150.7306 |
[32m[20221214 14:34:31 @agent_ppo2.py:185][0m |          -0.0025 |         265.3335 |        -150.6092 |
[32m[20221214 14:34:32 @agent_ppo2.py:185][0m |           0.0064 |         275.5377 |        -150.1968 |
[32m[20221214 14:34:32 @agent_ppo2.py:185][0m |          -0.0025 |         264.0922 |        -149.8990 |
[32m[20221214 14:34:32 @agent_ppo2.py:185][0m |           0.0022 |         267.7160 |        -149.7876 |
[32m[20221214 14:34:32 @agent_ppo2.py:185][0m |          -0.0035 |         262.3717 |        -149.8058 |
[32m[20221214 14:34:32 @agent_ppo2.py:185][0m |          -0.0006 |         261.5466 |        -149.8777 |
[32m[20221214 14:34:32 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:34:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.97
[32m[20221214 14:34:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.70
[32m[20221214 14:34:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.24
[32m[20221214 14:34:32 @agent_ppo2.py:143][0m Total time:      36.50 min
[32m[20221214 14:34:32 @agent_ppo2.py:145][0m 3354624 total steps have happened
[32m[20221214 14:34:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1638 --------------------------#
[32m[20221214 14:34:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:32 @agent_ppo2.py:185][0m |          -0.0013 |         285.8066 |        -146.5504 |
[32m[20221214 14:34:32 @agent_ppo2.py:185][0m |           0.0019 |         281.4748 |        -147.0139 |
[32m[20221214 14:34:33 @agent_ppo2.py:185][0m |           0.0035 |         279.0525 |        -147.0755 |
[32m[20221214 14:34:33 @agent_ppo2.py:185][0m |          -0.0042 |         273.4364 |        -147.2935 |
[32m[20221214 14:34:33 @agent_ppo2.py:185][0m |          -0.0007 |         271.3249 |        -146.8281 |
[32m[20221214 14:34:33 @agent_ppo2.py:185][0m |           0.0022 |         274.3808 |        -146.7184 |
[32m[20221214 14:34:33 @agent_ppo2.py:185][0m |           0.0016 |         270.5887 |        -146.5457 |
[32m[20221214 14:34:33 @agent_ppo2.py:185][0m |          -0.0018 |         268.7441 |        -146.8739 |
[32m[20221214 14:34:33 @agent_ppo2.py:185][0m |          -0.0022 |         267.3680 |        -146.7485 |
[32m[20221214 14:34:33 @agent_ppo2.py:185][0m |          -0.0019 |         267.3390 |        -146.7936 |
[32m[20221214 14:34:33 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:34:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.93
[32m[20221214 14:34:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.04
[32m[20221214 14:34:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.10
[32m[20221214 14:34:33 @agent_ppo2.py:143][0m Total time:      36.52 min
[32m[20221214 14:34:33 @agent_ppo2.py:145][0m 3356672 total steps have happened
[32m[20221214 14:34:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1639 --------------------------#
[32m[20221214 14:34:34 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:34:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:34 @agent_ppo2.py:185][0m |          -0.0015 |         277.6544 |        -145.5530 |
[32m[20221214 14:34:34 @agent_ppo2.py:185][0m |          -0.0017 |         261.1623 |        -145.5505 |
[32m[20221214 14:34:34 @agent_ppo2.py:185][0m |          -0.0031 |         253.4670 |        -145.4217 |
[32m[20221214 14:34:34 @agent_ppo2.py:185][0m |          -0.0027 |         251.9450 |        -145.1999 |
[32m[20221214 14:34:34 @agent_ppo2.py:185][0m |          -0.0020 |         249.3938 |        -145.5516 |
[32m[20221214 14:34:34 @agent_ppo2.py:185][0m |           0.0062 |         258.6607 |        -145.2204 |
[32m[20221214 14:34:34 @agent_ppo2.py:185][0m |          -0.0040 |         245.7405 |        -145.3780 |
[32m[20221214 14:34:35 @agent_ppo2.py:185][0m |           0.0086 |         250.0770 |        -145.0565 |
[32m[20221214 14:34:35 @agent_ppo2.py:185][0m |          -0.0049 |         240.3512 |        -145.3061 |
[32m[20221214 14:34:35 @agent_ppo2.py:185][0m |          -0.0031 |         238.1050 |        -145.0592 |
[32m[20221214 14:34:35 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:34:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.65
[32m[20221214 14:34:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.18
[32m[20221214 14:34:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.52
[32m[20221214 14:34:35 @agent_ppo2.py:143][0m Total time:      36.55 min
[32m[20221214 14:34:35 @agent_ppo2.py:145][0m 3358720 total steps have happened
[32m[20221214 14:34:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1640 --------------------------#
[32m[20221214 14:34:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:34:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:35 @agent_ppo2.py:185][0m |          -0.0020 |         285.8603 |        -148.7014 |
[32m[20221214 14:34:35 @agent_ppo2.py:185][0m |          -0.0016 |         265.7769 |        -149.2109 |
[32m[20221214 14:34:35 @agent_ppo2.py:185][0m |          -0.0013 |         260.9611 |        -149.1379 |
[32m[20221214 14:34:35 @agent_ppo2.py:185][0m |          -0.0008 |         258.0617 |        -149.4246 |
[32m[20221214 14:34:36 @agent_ppo2.py:185][0m |          -0.0009 |         256.4188 |        -149.7202 |
[32m[20221214 14:34:36 @agent_ppo2.py:185][0m |          -0.0030 |         253.8169 |        -149.7627 |
[32m[20221214 14:34:36 @agent_ppo2.py:185][0m |          -0.0018 |         253.4648 |        -149.7755 |
[32m[20221214 14:34:36 @agent_ppo2.py:185][0m |          -0.0026 |         252.4721 |        -149.9940 |
[32m[20221214 14:34:36 @agent_ppo2.py:185][0m |           0.0046 |         253.7304 |        -150.1408 |
[32m[20221214 14:34:36 @agent_ppo2.py:185][0m |          -0.0034 |         249.6537 |        -149.8914 |
[32m[20221214 14:34:36 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:34:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.24
[32m[20221214 14:34:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.87
[32m[20221214 14:34:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.07
[32m[20221214 14:34:36 @agent_ppo2.py:143][0m Total time:      36.57 min
[32m[20221214 14:34:36 @agent_ppo2.py:145][0m 3360768 total steps have happened
[32m[20221214 14:34:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1641 --------------------------#
[32m[20221214 14:34:36 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:34:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:37 @agent_ppo2.py:185][0m |          -0.0007 |         282.6607 |        -147.0652 |
[32m[20221214 14:34:37 @agent_ppo2.py:185][0m |          -0.0034 |         271.0614 |        -147.2148 |
[32m[20221214 14:34:37 @agent_ppo2.py:185][0m |           0.0140 |         283.8562 |        -147.1818 |
[32m[20221214 14:34:37 @agent_ppo2.py:185][0m |          -0.0019 |         258.9095 |        -146.5381 |
[32m[20221214 14:34:37 @agent_ppo2.py:185][0m |          -0.0004 |         255.4849 |        -147.1215 |
[32m[20221214 14:34:37 @agent_ppo2.py:185][0m |          -0.0038 |         253.6391 |        -146.9073 |
[32m[20221214 14:34:37 @agent_ppo2.py:185][0m |          -0.0024 |         251.3359 |        -146.7782 |
[32m[20221214 14:34:37 @agent_ppo2.py:185][0m |          -0.0032 |         249.6539 |        -146.8914 |
[32m[20221214 14:34:37 @agent_ppo2.py:185][0m |           0.0108 |         275.8292 |        -146.9521 |
[32m[20221214 14:34:37 @agent_ppo2.py:185][0m |          -0.0034 |         246.8951 |        -146.9286 |
[32m[20221214 14:34:37 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:34:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.69
[32m[20221214 14:34:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.64
[32m[20221214 14:34:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.66
[32m[20221214 14:34:38 @agent_ppo2.py:143][0m Total time:      36.59 min
[32m[20221214 14:34:38 @agent_ppo2.py:145][0m 3362816 total steps have happened
[32m[20221214 14:34:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1642 --------------------------#
[32m[20221214 14:34:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:38 @agent_ppo2.py:185][0m |          -0.0009 |         250.5969 |        -147.5682 |
[32m[20221214 14:34:38 @agent_ppo2.py:185][0m |           0.0006 |         234.8014 |        -147.6543 |
[32m[20221214 14:34:38 @agent_ppo2.py:185][0m |           0.0004 |         230.3035 |        -147.6833 |
[32m[20221214 14:34:38 @agent_ppo2.py:185][0m |           0.0117 |         248.2032 |        -147.7626 |
[32m[20221214 14:34:38 @agent_ppo2.py:185][0m |          -0.0004 |         225.7996 |        -147.7146 |
[32m[20221214 14:34:38 @agent_ppo2.py:185][0m |          -0.0034 |         224.1944 |        -147.6805 |
[32m[20221214 14:34:38 @agent_ppo2.py:185][0m |          -0.0043 |         222.6053 |        -148.0839 |
[32m[20221214 14:34:38 @agent_ppo2.py:185][0m |          -0.0005 |         221.4114 |        -148.0113 |
[32m[20221214 14:34:39 @agent_ppo2.py:185][0m |          -0.0006 |         220.9263 |        -148.2095 |
[32m[20221214 14:34:39 @agent_ppo2.py:185][0m |          -0.0017 |         219.6834 |        -147.9624 |
[32m[20221214 14:34:39 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:34:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.31
[32m[20221214 14:34:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.47
[32m[20221214 14:34:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.45
[32m[20221214 14:34:39 @agent_ppo2.py:143][0m Total time:      36.61 min
[32m[20221214 14:34:39 @agent_ppo2.py:145][0m 3364864 total steps have happened
[32m[20221214 14:34:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1643 --------------------------#
[32m[20221214 14:34:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:39 @agent_ppo2.py:185][0m |          -0.0036 |         248.3584 |        -153.2101 |
[32m[20221214 14:34:39 @agent_ppo2.py:185][0m |          -0.0012 |         227.1586 |        -152.9722 |
[32m[20221214 14:34:39 @agent_ppo2.py:185][0m |           0.0096 |         239.8104 |        -153.0964 |
[32m[20221214 14:34:39 @agent_ppo2.py:185][0m |          -0.0023 |         216.1897 |        -153.1033 |
[32m[20221214 14:34:39 @agent_ppo2.py:185][0m |          -0.0052 |         213.6462 |        -153.1701 |
[32m[20221214 14:34:40 @agent_ppo2.py:185][0m |          -0.0009 |         212.9376 |        -153.0252 |
[32m[20221214 14:34:40 @agent_ppo2.py:185][0m |          -0.0038 |         211.0682 |        -153.0988 |
[32m[20221214 14:34:40 @agent_ppo2.py:185][0m |           0.0003 |         209.4753 |        -153.4480 |
[32m[20221214 14:34:40 @agent_ppo2.py:185][0m |          -0.0031 |         209.4947 |        -153.1088 |
[32m[20221214 14:34:40 @agent_ppo2.py:185][0m |          -0.0026 |         208.0244 |        -153.2404 |
[32m[20221214 14:34:40 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:34:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.02
[32m[20221214 14:34:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.06
[32m[20221214 14:34:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.56
[32m[20221214 14:34:40 @agent_ppo2.py:143][0m Total time:      36.63 min
[32m[20221214 14:34:40 @agent_ppo2.py:145][0m 3366912 total steps have happened
[32m[20221214 14:34:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1644 --------------------------#
[32m[20221214 14:34:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:40 @agent_ppo2.py:185][0m |          -0.0062 |         286.6384 |        -149.7470 |
[32m[20221214 14:34:41 @agent_ppo2.py:185][0m |          -0.0032 |         269.5059 |        -150.3809 |
[32m[20221214 14:34:41 @agent_ppo2.py:185][0m |           0.0032 |         269.0125 |        -150.0748 |
[32m[20221214 14:34:41 @agent_ppo2.py:185][0m |          -0.0041 |         257.4069 |        -150.2190 |
[32m[20221214 14:34:41 @agent_ppo2.py:185][0m |          -0.0035 |         254.9453 |        -150.2583 |
[32m[20221214 14:34:41 @agent_ppo2.py:185][0m |          -0.0048 |         250.9846 |        -150.3861 |
[32m[20221214 14:34:41 @agent_ppo2.py:185][0m |          -0.0072 |         248.6756 |        -150.1525 |
[32m[20221214 14:34:41 @agent_ppo2.py:185][0m |          -0.0033 |         246.5333 |        -150.0607 |
[32m[20221214 14:34:41 @agent_ppo2.py:185][0m |          -0.0034 |         244.7094 |        -150.3422 |
[32m[20221214 14:34:41 @agent_ppo2.py:185][0m |          -0.0057 |         242.4776 |        -150.5296 |
[32m[20221214 14:34:41 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:34:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.24
[32m[20221214 14:34:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.06
[32m[20221214 14:34:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.62
[32m[20221214 14:34:41 @agent_ppo2.py:143][0m Total time:      36.66 min
[32m[20221214 14:34:41 @agent_ppo2.py:145][0m 3368960 total steps have happened
[32m[20221214 14:34:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1645 --------------------------#
[32m[20221214 14:34:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:42 @agent_ppo2.py:185][0m |           0.0006 |         258.0012 |        -150.5378 |
[32m[20221214 14:34:42 @agent_ppo2.py:185][0m |          -0.0003 |         249.4170 |        -150.3978 |
[32m[20221214 14:34:42 @agent_ppo2.py:185][0m |          -0.0026 |         243.6164 |        -150.2549 |
[32m[20221214 14:34:42 @agent_ppo2.py:185][0m |          -0.0030 |         242.0198 |        -150.2030 |
[32m[20221214 14:34:42 @agent_ppo2.py:185][0m |          -0.0024 |         240.2060 |        -149.9924 |
[32m[20221214 14:34:42 @agent_ppo2.py:185][0m |           0.0002 |         238.3627 |        -150.0682 |
[32m[20221214 14:34:42 @agent_ppo2.py:185][0m |          -0.0008 |         236.3416 |        -150.2046 |
[32m[20221214 14:34:42 @agent_ppo2.py:185][0m |           0.0058 |         239.2631 |        -149.9428 |
[32m[20221214 14:34:43 @agent_ppo2.py:185][0m |          -0.0003 |         235.3972 |        -150.0241 |
[32m[20221214 14:34:43 @agent_ppo2.py:185][0m |           0.0001 |         232.2683 |        -149.8660 |
[32m[20221214 14:34:43 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:34:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.88
[32m[20221214 14:34:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.07
[32m[20221214 14:34:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.53
[32m[20221214 14:34:43 @agent_ppo2.py:143][0m Total time:      36.68 min
[32m[20221214 14:34:43 @agent_ppo2.py:145][0m 3371008 total steps have happened
[32m[20221214 14:34:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1646 --------------------------#
[32m[20221214 14:34:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:43 @agent_ppo2.py:185][0m |           0.0141 |         292.4215 |        -149.6248 |
[32m[20221214 14:34:43 @agent_ppo2.py:185][0m |          -0.0016 |         247.0120 |        -149.4988 |
[32m[20221214 14:34:43 @agent_ppo2.py:185][0m |          -0.0003 |         243.1385 |        -149.5283 |
[32m[20221214 14:34:43 @agent_ppo2.py:185][0m |          -0.0020 |         239.9189 |        -149.3506 |
[32m[20221214 14:34:44 @agent_ppo2.py:185][0m |          -0.0025 |         238.2091 |        -149.5466 |
[32m[20221214 14:34:44 @agent_ppo2.py:185][0m |          -0.0029 |         236.3069 |        -149.4496 |
[32m[20221214 14:34:44 @agent_ppo2.py:185][0m |          -0.0038 |         235.2219 |        -149.3377 |
[32m[20221214 14:34:44 @agent_ppo2.py:185][0m |          -0.0006 |         234.5012 |        -149.5200 |
[32m[20221214 14:34:44 @agent_ppo2.py:185][0m |           0.0020 |         233.5882 |        -149.6351 |
[32m[20221214 14:34:44 @agent_ppo2.py:185][0m |           0.0003 |         233.0418 |        -149.5247 |
[32m[20221214 14:34:44 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:34:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.55
[32m[20221214 14:34:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.17
[32m[20221214 14:34:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.06
[32m[20221214 14:34:44 @agent_ppo2.py:143][0m Total time:      36.70 min
[32m[20221214 14:34:44 @agent_ppo2.py:145][0m 3373056 total steps have happened
[32m[20221214 14:34:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1647 --------------------------#
[32m[20221214 14:34:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:45 @agent_ppo2.py:185][0m |           0.0054 |         255.9507 |        -151.4585 |
[32m[20221214 14:34:45 @agent_ppo2.py:185][0m |           0.0026 |         240.8994 |        -151.5318 |
[32m[20221214 14:34:45 @agent_ppo2.py:185][0m |          -0.0009 |         237.4062 |        -151.9492 |
[32m[20221214 14:34:45 @agent_ppo2.py:185][0m |          -0.0024 |         236.3023 |        -151.8505 |
[32m[20221214 14:34:45 @agent_ppo2.py:185][0m |           0.0004 |         237.3611 |        -151.8921 |
[32m[20221214 14:34:45 @agent_ppo2.py:185][0m |          -0.0029 |         234.2942 |        -151.9492 |
[32m[20221214 14:34:45 @agent_ppo2.py:185][0m |          -0.0033 |         233.3470 |        -151.7451 |
[32m[20221214 14:34:45 @agent_ppo2.py:185][0m |          -0.0018 |         233.0636 |        -151.3029 |
[32m[20221214 14:34:45 @agent_ppo2.py:185][0m |          -0.0002 |         232.4691 |        -151.3476 |
[32m[20221214 14:34:45 @agent_ppo2.py:185][0m |          -0.0007 |         231.8820 |        -151.7173 |
[32m[20221214 14:34:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:34:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.19
[32m[20221214 14:34:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.29
[32m[20221214 14:34:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.23
[32m[20221214 14:34:46 @agent_ppo2.py:143][0m Total time:      36.72 min
[32m[20221214 14:34:46 @agent_ppo2.py:145][0m 3375104 total steps have happened
[32m[20221214 14:34:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1648 --------------------------#
[32m[20221214 14:34:46 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:34:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:46 @agent_ppo2.py:185][0m |          -0.0024 |         245.2547 |        -149.3498 |
[32m[20221214 14:34:46 @agent_ppo2.py:185][0m |           0.0075 |         243.9012 |        -149.5466 |
[32m[20221214 14:34:46 @agent_ppo2.py:185][0m |           0.0017 |         229.5066 |        -149.5298 |
[32m[20221214 14:34:46 @agent_ppo2.py:185][0m |           0.0112 |         244.5050 |        -149.2136 |
[32m[20221214 14:34:46 @agent_ppo2.py:185][0m |          -0.0012 |         217.3027 |        -149.1278 |
[32m[20221214 14:34:46 @agent_ppo2.py:185][0m |          -0.0002 |         214.9210 |        -149.4908 |
[32m[20221214 14:34:46 @agent_ppo2.py:185][0m |           0.0055 |         221.4575 |        -149.8563 |
[32m[20221214 14:34:47 @agent_ppo2.py:185][0m |          -0.0027 |         211.8534 |        -149.6404 |
[32m[20221214 14:34:47 @agent_ppo2.py:185][0m |          -0.0039 |         211.6641 |        -149.9303 |
[32m[20221214 14:34:47 @agent_ppo2.py:185][0m |           0.0028 |         213.0871 |        -149.6534 |
[32m[20221214 14:34:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:34:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.37
[32m[20221214 14:34:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.62
[32m[20221214 14:34:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.98
[32m[20221214 14:34:47 @agent_ppo2.py:143][0m Total time:      36.75 min
[32m[20221214 14:34:47 @agent_ppo2.py:145][0m 3377152 total steps have happened
[32m[20221214 14:34:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1649 --------------------------#
[32m[20221214 14:34:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:34:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:47 @agent_ppo2.py:185][0m |          -0.0016 |         259.3190 |        -148.2095 |
[32m[20221214 14:34:47 @agent_ppo2.py:185][0m |           0.0033 |         253.9901 |        -147.7270 |
[32m[20221214 14:34:47 @agent_ppo2.py:185][0m |           0.0047 |         250.0939 |        -148.0088 |
[32m[20221214 14:34:48 @agent_ppo2.py:185][0m |           0.0047 |         249.1481 |        -147.9204 |
[32m[20221214 14:34:48 @agent_ppo2.py:185][0m |           0.0001 |         240.2032 |        -148.2765 |
[32m[20221214 14:34:48 @agent_ppo2.py:185][0m |          -0.0030 |         238.9407 |        -148.1164 |
[32m[20221214 14:34:48 @agent_ppo2.py:185][0m |           0.0070 |         248.5737 |        -148.3111 |
[32m[20221214 14:34:48 @agent_ppo2.py:185][0m |          -0.0025 |         235.8632 |        -147.9082 |
[32m[20221214 14:34:48 @agent_ppo2.py:185][0m |           0.0006 |         235.3608 |        -148.7897 |
[32m[20221214 14:34:48 @agent_ppo2.py:185][0m |           0.0026 |         234.8574 |        -148.3859 |
[32m[20221214 14:34:48 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:34:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.32
[32m[20221214 14:34:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.79
[32m[20221214 14:34:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.86
[32m[20221214 14:34:48 @agent_ppo2.py:143][0m Total time:      36.77 min
[32m[20221214 14:34:48 @agent_ppo2.py:145][0m 3379200 total steps have happened
[32m[20221214 14:34:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1650 --------------------------#
[32m[20221214 14:34:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:34:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:49 @agent_ppo2.py:185][0m |           0.0003 |         269.7530 |        -150.7054 |
[32m[20221214 14:34:49 @agent_ppo2.py:185][0m |           0.0001 |         257.2734 |        -150.9491 |
[32m[20221214 14:34:49 @agent_ppo2.py:185][0m |           0.0028 |         250.9156 |        -150.9226 |
[32m[20221214 14:34:49 @agent_ppo2.py:185][0m |           0.0004 |         247.7174 |        -150.9968 |
[32m[20221214 14:34:49 @agent_ppo2.py:185][0m |           0.0001 |         246.0550 |        -150.7754 |
[32m[20221214 14:34:49 @agent_ppo2.py:185][0m |          -0.0018 |         245.0195 |        -151.0172 |
[32m[20221214 14:34:49 @agent_ppo2.py:185][0m |           0.0094 |         275.6252 |        -150.8561 |
[32m[20221214 14:34:49 @agent_ppo2.py:185][0m |           0.0060 |         249.2635 |        -150.8014 |
[32m[20221214 14:34:49 @agent_ppo2.py:185][0m |          -0.0027 |         243.1014 |        -150.5046 |
[32m[20221214 14:34:50 @agent_ppo2.py:185][0m |          -0.0028 |         242.8810 |        -150.8068 |
[32m[20221214 14:34:50 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:34:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.11
[32m[20221214 14:34:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.21
[32m[20221214 14:34:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.66
[32m[20221214 14:34:50 @agent_ppo2.py:143][0m Total time:      36.79 min
[32m[20221214 14:34:50 @agent_ppo2.py:145][0m 3381248 total steps have happened
[32m[20221214 14:34:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1651 --------------------------#
[32m[20221214 14:34:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:50 @agent_ppo2.py:185][0m |          -0.0008 |         237.2629 |        -151.1388 |
[32m[20221214 14:34:50 @agent_ppo2.py:185][0m |          -0.0007 |         228.9161 |        -151.0994 |
[32m[20221214 14:34:50 @agent_ppo2.py:185][0m |           0.0021 |         224.5193 |        -151.1067 |
[32m[20221214 14:34:50 @agent_ppo2.py:185][0m |          -0.0004 |         223.0491 |        -151.3649 |
[32m[20221214 14:34:50 @agent_ppo2.py:185][0m |           0.0003 |         222.7043 |        -151.4881 |
[32m[20221214 14:34:50 @agent_ppo2.py:185][0m |          -0.0016 |         221.7026 |        -151.4969 |
[32m[20221214 14:34:51 @agent_ppo2.py:185][0m |          -0.0010 |         220.8396 |        -151.9146 |
[32m[20221214 14:34:51 @agent_ppo2.py:185][0m |           0.0169 |         245.6606 |        -152.0422 |
[32m[20221214 14:34:51 @agent_ppo2.py:185][0m |          -0.0009 |         219.2301 |        -152.1882 |
[32m[20221214 14:34:51 @agent_ppo2.py:185][0m |          -0.0017 |         219.1021 |        -152.3553 |
[32m[20221214 14:34:51 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:34:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.10
[32m[20221214 14:34:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.33
[32m[20221214 14:34:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.97
[32m[20221214 14:34:51 @agent_ppo2.py:143][0m Total time:      36.81 min
[32m[20221214 14:34:51 @agent_ppo2.py:145][0m 3383296 total steps have happened
[32m[20221214 14:34:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1652 --------------------------#
[32m[20221214 14:34:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:51 @agent_ppo2.py:185][0m |          -0.0040 |         238.8735 |        -152.2671 |
[32m[20221214 14:34:51 @agent_ppo2.py:185][0m |          -0.0056 |         231.4926 |        -151.8066 |
[32m[20221214 14:34:51 @agent_ppo2.py:185][0m |          -0.0049 |         229.3901 |        -151.9260 |
[32m[20221214 14:34:52 @agent_ppo2.py:185][0m |          -0.0059 |         227.1882 |        -151.2546 |
[32m[20221214 14:34:52 @agent_ppo2.py:185][0m |          -0.0042 |         226.1741 |        -151.4614 |
[32m[20221214 14:34:52 @agent_ppo2.py:185][0m |          -0.0048 |         225.2608 |        -151.6842 |
[32m[20221214 14:34:52 @agent_ppo2.py:185][0m |          -0.0053 |         224.9478 |        -151.5335 |
[32m[20221214 14:34:52 @agent_ppo2.py:185][0m |           0.0025 |         232.8895 |        -151.1939 |
[32m[20221214 14:34:52 @agent_ppo2.py:185][0m |          -0.0039 |         222.9224 |        -151.2148 |
[32m[20221214 14:34:52 @agent_ppo2.py:185][0m |          -0.0054 |         221.6872 |        -151.3481 |
[32m[20221214 14:34:52 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:34:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.56
[32m[20221214 14:34:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.55
[32m[20221214 14:34:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.31
[32m[20221214 14:34:52 @agent_ppo2.py:143][0m Total time:      36.84 min
[32m[20221214 14:34:52 @agent_ppo2.py:145][0m 3385344 total steps have happened
[32m[20221214 14:34:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1653 --------------------------#
[32m[20221214 14:34:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:53 @agent_ppo2.py:185][0m |           0.0012 |         252.1895 |        -153.8713 |
[32m[20221214 14:34:53 @agent_ppo2.py:185][0m |           0.0003 |         246.7998 |        -154.2509 |
[32m[20221214 14:34:53 @agent_ppo2.py:185][0m |           0.0066 |         256.6837 |        -154.2599 |
[32m[20221214 14:34:53 @agent_ppo2.py:185][0m |          -0.0031 |         244.0508 |        -154.2690 |
[32m[20221214 14:34:53 @agent_ppo2.py:185][0m |          -0.0025 |         244.2486 |        -154.3621 |
[32m[20221214 14:34:53 @agent_ppo2.py:185][0m |          -0.0019 |         243.0506 |        -154.3417 |
[32m[20221214 14:34:53 @agent_ppo2.py:185][0m |          -0.0002 |         243.3880 |        -154.6643 |
[32m[20221214 14:34:53 @agent_ppo2.py:185][0m |           0.0007 |         243.1588 |        -154.5875 |
[32m[20221214 14:34:53 @agent_ppo2.py:185][0m |          -0.0028 |         242.3147 |        -154.8248 |
[32m[20221214 14:34:53 @agent_ppo2.py:185][0m |           0.0102 |         266.1412 |        -154.3012 |
[32m[20221214 14:34:53 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:34:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.60
[32m[20221214 14:34:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.33
[32m[20221214 14:34:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.51
[32m[20221214 14:34:53 @agent_ppo2.py:143][0m Total time:      36.86 min
[32m[20221214 14:34:53 @agent_ppo2.py:145][0m 3387392 total steps have happened
[32m[20221214 14:34:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1654 --------------------------#
[32m[20221214 14:34:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:54 @agent_ppo2.py:185][0m |           0.0015 |         242.6378 |        -157.0582 |
[32m[20221214 14:34:54 @agent_ppo2.py:185][0m |           0.0004 |         231.4895 |        -156.7385 |
[32m[20221214 14:34:54 @agent_ppo2.py:185][0m |           0.0049 |         228.7443 |        -156.2434 |
[32m[20221214 14:34:54 @agent_ppo2.py:185][0m |           0.0011 |         222.8348 |        -156.2279 |
[32m[20221214 14:34:54 @agent_ppo2.py:185][0m |          -0.0012 |         218.9796 |        -155.9991 |
[32m[20221214 14:34:54 @agent_ppo2.py:185][0m |           0.0184 |         252.9028 |        -156.3403 |
[32m[20221214 14:34:54 @agent_ppo2.py:185][0m |          -0.0022 |         217.2205 |        -155.7347 |
[32m[20221214 14:34:54 @agent_ppo2.py:185][0m |          -0.0019 |         214.4469 |        -156.1208 |
[32m[20221214 14:34:54 @agent_ppo2.py:185][0m |          -0.0014 |         213.8019 |        -155.8355 |
[32m[20221214 14:34:55 @agent_ppo2.py:185][0m |           0.0019 |         212.1096 |        -155.5845 |
[32m[20221214 14:34:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:34:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.92
[32m[20221214 14:34:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.96
[32m[20221214 14:34:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.53
[32m[20221214 14:34:55 @agent_ppo2.py:143][0m Total time:      36.88 min
[32m[20221214 14:34:55 @agent_ppo2.py:145][0m 3389440 total steps have happened
[32m[20221214 14:34:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1655 --------------------------#
[32m[20221214 14:34:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:55 @agent_ppo2.py:185][0m |           0.0100 |         276.5755 |        -153.8829 |
[32m[20221214 14:34:55 @agent_ppo2.py:185][0m |          -0.0018 |         235.1698 |        -153.8992 |
[32m[20221214 14:34:55 @agent_ppo2.py:185][0m |          -0.0004 |         230.8732 |        -153.6232 |
[32m[20221214 14:34:55 @agent_ppo2.py:185][0m |          -0.0026 |         229.1580 |        -153.8685 |
[32m[20221214 14:34:55 @agent_ppo2.py:185][0m |           0.0003 |         228.2419 |        -154.0455 |
[32m[20221214 14:34:55 @agent_ppo2.py:185][0m |          -0.0031 |         225.8591 |        -153.9158 |
[32m[20221214 14:34:56 @agent_ppo2.py:185][0m |          -0.0007 |         224.1039 |        -153.3293 |
[32m[20221214 14:34:56 @agent_ppo2.py:185][0m |          -0.0024 |         223.7748 |        -153.8930 |
[32m[20221214 14:34:56 @agent_ppo2.py:185][0m |          -0.0036 |         223.1957 |        -153.7438 |
[32m[20221214 14:34:56 @agent_ppo2.py:185][0m |          -0.0024 |         221.6390 |        -153.9523 |
[32m[20221214 14:34:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:34:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.00
[32m[20221214 14:34:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.47
[32m[20221214 14:34:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.87
[32m[20221214 14:34:56 @agent_ppo2.py:143][0m Total time:      36.90 min
[32m[20221214 14:34:56 @agent_ppo2.py:145][0m 3391488 total steps have happened
[32m[20221214 14:34:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1656 --------------------------#
[32m[20221214 14:34:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:56 @agent_ppo2.py:185][0m |          -0.0002 |         223.6581 |        -155.0126 |
[32m[20221214 14:34:56 @agent_ppo2.py:185][0m |          -0.0043 |         219.3736 |        -155.5880 |
[32m[20221214 14:34:56 @agent_ppo2.py:185][0m |           0.0009 |         215.5227 |        -155.5823 |
[32m[20221214 14:34:56 @agent_ppo2.py:185][0m |          -0.0007 |         214.2755 |        -155.6976 |
[32m[20221214 14:34:57 @agent_ppo2.py:185][0m |           0.0114 |         234.7337 |        -155.9367 |
[32m[20221214 14:34:57 @agent_ppo2.py:185][0m |          -0.0030 |         213.2438 |        -155.2920 |
[32m[20221214 14:34:57 @agent_ppo2.py:185][0m |          -0.0014 |         211.6924 |        -156.0609 |
[32m[20221214 14:34:57 @agent_ppo2.py:185][0m |          -0.0017 |         211.7795 |        -155.9687 |
[32m[20221214 14:34:57 @agent_ppo2.py:185][0m |          -0.0005 |         210.6555 |        -156.0202 |
[32m[20221214 14:34:57 @agent_ppo2.py:185][0m |          -0.0007 |         210.6119 |        -156.0972 |
[32m[20221214 14:34:57 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:34:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.62
[32m[20221214 14:34:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.61
[32m[20221214 14:34:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.86
[32m[20221214 14:34:57 @agent_ppo2.py:143][0m Total time:      36.92 min
[32m[20221214 14:34:57 @agent_ppo2.py:145][0m 3393536 total steps have happened
[32m[20221214 14:34:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1657 --------------------------#
[32m[20221214 14:34:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:57 @agent_ppo2.py:185][0m |          -0.0005 |         251.7232 |        -155.5815 |
[32m[20221214 14:34:58 @agent_ppo2.py:185][0m |          -0.0039 |         244.4972 |        -156.0625 |
[32m[20221214 14:34:58 @agent_ppo2.py:185][0m |          -0.0020 |         240.9390 |        -155.9754 |
[32m[20221214 14:34:58 @agent_ppo2.py:185][0m |          -0.0020 |         237.2854 |        -155.8166 |
[32m[20221214 14:34:58 @agent_ppo2.py:185][0m |          -0.0025 |         235.4821 |        -156.4642 |
[32m[20221214 14:34:58 @agent_ppo2.py:185][0m |          -0.0026 |         234.0633 |        -156.5487 |
[32m[20221214 14:34:58 @agent_ppo2.py:185][0m |           0.0001 |         235.3949 |        -156.6929 |
[32m[20221214 14:34:58 @agent_ppo2.py:185][0m |          -0.0039 |         233.0443 |        -156.9030 |
[32m[20221214 14:34:58 @agent_ppo2.py:185][0m |          -0.0018 |         231.5554 |        -156.8878 |
[32m[20221214 14:34:58 @agent_ppo2.py:185][0m |          -0.0014 |         229.7874 |        -157.2658 |
[32m[20221214 14:34:58 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:34:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.93
[32m[20221214 14:34:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.18
[32m[20221214 14:34:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.60
[32m[20221214 14:34:58 @agent_ppo2.py:143][0m Total time:      36.94 min
[32m[20221214 14:34:58 @agent_ppo2.py:145][0m 3395584 total steps have happened
[32m[20221214 14:34:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1658 --------------------------#
[32m[20221214 14:34:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:34:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:34:59 @agent_ppo2.py:185][0m |          -0.0005 |         255.3698 |        -159.9487 |
[32m[20221214 14:34:59 @agent_ppo2.py:185][0m |          -0.0004 |         245.2504 |        -159.5301 |
[32m[20221214 14:34:59 @agent_ppo2.py:185][0m |          -0.0001 |         241.5095 |        -159.4656 |
[32m[20221214 14:34:59 @agent_ppo2.py:185][0m |          -0.0028 |         236.2313 |        -159.5468 |
[32m[20221214 14:34:59 @agent_ppo2.py:185][0m |          -0.0025 |         232.6005 |        -159.2894 |
[32m[20221214 14:34:59 @agent_ppo2.py:185][0m |          -0.0025 |         231.4051 |        -159.5786 |
[32m[20221214 14:34:59 @agent_ppo2.py:185][0m |          -0.0018 |         228.8104 |        -159.8753 |
[32m[20221214 14:34:59 @agent_ppo2.py:185][0m |           0.0028 |         229.3196 |        -159.2286 |
[32m[20221214 14:34:59 @agent_ppo2.py:185][0m |           0.0082 |         242.7688 |        -159.2470 |
[32m[20221214 14:34:59 @agent_ppo2.py:185][0m |          -0.0021 |         223.6879 |        -159.6883 |
[32m[20221214 14:34:59 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:35:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.07
[32m[20221214 14:35:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.31
[32m[20221214 14:35:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.61
[32m[20221214 14:35:00 @agent_ppo2.py:143][0m Total time:      36.96 min
[32m[20221214 14:35:00 @agent_ppo2.py:145][0m 3397632 total steps have happened
[32m[20221214 14:35:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1659 --------------------------#
[32m[20221214 14:35:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:35:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:00 @agent_ppo2.py:185][0m |          -0.0032 |         276.0730 |        -159.8691 |
[32m[20221214 14:35:00 @agent_ppo2.py:185][0m |          -0.0010 |         255.8309 |        -159.7877 |
[32m[20221214 14:35:00 @agent_ppo2.py:185][0m |          -0.0029 |         247.1252 |        -159.4420 |
[32m[20221214 14:35:00 @agent_ppo2.py:185][0m |          -0.0034 |         242.6689 |        -159.8321 |
[32m[20221214 14:35:00 @agent_ppo2.py:185][0m |          -0.0058 |         240.5873 |        -159.7279 |
[32m[20221214 14:35:00 @agent_ppo2.py:185][0m |          -0.0060 |         237.8330 |        -159.9432 |
[32m[20221214 14:35:00 @agent_ppo2.py:185][0m |          -0.0008 |         237.1952 |        -159.7976 |
[32m[20221214 14:35:00 @agent_ppo2.py:185][0m |           0.0110 |         261.7800 |        -160.4381 |
[32m[20221214 14:35:01 @agent_ppo2.py:185][0m |          -0.0008 |         236.2031 |        -160.2922 |
[32m[20221214 14:35:01 @agent_ppo2.py:185][0m |          -0.0029 |         233.7758 |        -160.0680 |
[32m[20221214 14:35:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:35:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.43
[32m[20221214 14:35:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.60
[32m[20221214 14:35:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.97
[32m[20221214 14:35:01 @agent_ppo2.py:143][0m Total time:      36.98 min
[32m[20221214 14:35:01 @agent_ppo2.py:145][0m 3399680 total steps have happened
[32m[20221214 14:35:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1660 --------------------------#
[32m[20221214 14:35:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:35:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:01 @agent_ppo2.py:185][0m |          -0.0016 |         270.5401 |        -160.8557 |
[32m[20221214 14:35:01 @agent_ppo2.py:185][0m |          -0.0040 |         259.5732 |        -161.0276 |
[32m[20221214 14:35:01 @agent_ppo2.py:185][0m |          -0.0027 |         254.1257 |        -161.1971 |
[32m[20221214 14:35:01 @agent_ppo2.py:185][0m |           0.0001 |         251.7791 |        -161.0089 |
[32m[20221214 14:35:02 @agent_ppo2.py:185][0m |           0.0010 |         251.0516 |        -161.1523 |
[32m[20221214 14:35:02 @agent_ppo2.py:185][0m |          -0.0036 |         247.0511 |        -161.0977 |
[32m[20221214 14:35:02 @agent_ppo2.py:185][0m |          -0.0027 |         246.6700 |        -160.9963 |
[32m[20221214 14:35:02 @agent_ppo2.py:185][0m |          -0.0020 |         245.5919 |        -161.0888 |
[32m[20221214 14:35:02 @agent_ppo2.py:185][0m |          -0.0055 |         245.2226 |        -161.3567 |
[32m[20221214 14:35:02 @agent_ppo2.py:185][0m |          -0.0006 |         245.2660 |        -161.4830 |
[32m[20221214 14:35:02 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:35:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.27
[32m[20221214 14:35:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.63
[32m[20221214 14:35:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.09
[32m[20221214 14:35:02 @agent_ppo2.py:143][0m Total time:      37.00 min
[32m[20221214 14:35:02 @agent_ppo2.py:145][0m 3401728 total steps have happened
[32m[20221214 14:35:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1661 --------------------------#
[32m[20221214 14:35:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:35:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:02 @agent_ppo2.py:185][0m |          -0.0010 |         260.4220 |        -163.4450 |
[32m[20221214 14:35:03 @agent_ppo2.py:185][0m |          -0.0025 |         253.8847 |        -163.7232 |
[32m[20221214 14:35:03 @agent_ppo2.py:185][0m |          -0.0007 |         252.6363 |        -163.7204 |
[32m[20221214 14:35:03 @agent_ppo2.py:185][0m |          -0.0031 |         250.2901 |        -163.7393 |
[32m[20221214 14:35:03 @agent_ppo2.py:185][0m |          -0.0032 |         248.4122 |        -163.6368 |
[32m[20221214 14:35:03 @agent_ppo2.py:185][0m |          -0.0037 |         248.3143 |        -163.6457 |
[32m[20221214 14:35:03 @agent_ppo2.py:185][0m |           0.0017 |         248.2229 |        -163.7921 |
[32m[20221214 14:35:03 @agent_ppo2.py:185][0m |          -0.0021 |         247.1163 |        -164.0050 |
[32m[20221214 14:35:03 @agent_ppo2.py:185][0m |           0.0036 |         255.6342 |        -163.6579 |
[32m[20221214 14:35:03 @agent_ppo2.py:185][0m |          -0.0039 |         246.7320 |        -164.0159 |
[32m[20221214 14:35:03 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:35:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.34
[32m[20221214 14:35:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.27
[32m[20221214 14:35:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.87
[32m[20221214 14:35:03 @agent_ppo2.py:143][0m Total time:      37.02 min
[32m[20221214 14:35:03 @agent_ppo2.py:145][0m 3403776 total steps have happened
[32m[20221214 14:35:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1662 --------------------------#
[32m[20221214 14:35:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:35:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:04 @agent_ppo2.py:185][0m |           0.0087 |         280.8486 |        -163.2308 |
[32m[20221214 14:35:04 @agent_ppo2.py:185][0m |          -0.0003 |         254.2142 |        -163.1553 |
[32m[20221214 14:35:04 @agent_ppo2.py:185][0m |          -0.0019 |         252.1240 |        -163.8617 |
[32m[20221214 14:35:04 @agent_ppo2.py:185][0m |          -0.0015 |         251.2847 |        -163.6675 |
[32m[20221214 14:35:04 @agent_ppo2.py:185][0m |           0.0019 |         253.0255 |        -163.7921 |
[32m[20221214 14:35:04 @agent_ppo2.py:185][0m |          -0.0036 |         250.4156 |        -163.9781 |
[32m[20221214 14:35:04 @agent_ppo2.py:185][0m |          -0.0042 |         249.3724 |        -163.9083 |
[32m[20221214 14:35:04 @agent_ppo2.py:185][0m |          -0.0008 |         248.6808 |        -164.1270 |
[32m[20221214 14:35:04 @agent_ppo2.py:185][0m |           0.0018 |         251.4091 |        -164.1063 |
[32m[20221214 14:35:04 @agent_ppo2.py:185][0m |          -0.0024 |         248.5527 |        -164.3349 |
[32m[20221214 14:35:04 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:35:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.07
[32m[20221214 14:35:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.25
[32m[20221214 14:35:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.25
[32m[20221214 14:35:05 @agent_ppo2.py:143][0m Total time:      37.04 min
[32m[20221214 14:35:05 @agent_ppo2.py:145][0m 3405824 total steps have happened
[32m[20221214 14:35:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1663 --------------------------#
[32m[20221214 14:35:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:05 @agent_ppo2.py:185][0m |          -0.0025 |         256.5999 |        -163.8002 |
[32m[20221214 14:35:05 @agent_ppo2.py:185][0m |           0.0117 |         264.3332 |        -163.9117 |
[32m[20221214 14:35:05 @agent_ppo2.py:185][0m |          -0.0024 |         231.0165 |        -163.9534 |
[32m[20221214 14:35:05 @agent_ppo2.py:185][0m |          -0.0030 |         226.9865 |        -164.2443 |
[32m[20221214 14:35:05 @agent_ppo2.py:185][0m |           0.0035 |         227.3172 |        -164.0933 |
[32m[20221214 14:35:05 @agent_ppo2.py:185][0m |          -0.0054 |         223.9636 |        -164.2208 |
[32m[20221214 14:35:05 @agent_ppo2.py:185][0m |           0.0024 |         222.4224 |        -164.1477 |
[32m[20221214 14:35:06 @agent_ppo2.py:185][0m |          -0.0031 |         219.1020 |        -164.2665 |
[32m[20221214 14:35:06 @agent_ppo2.py:185][0m |          -0.0022 |         218.5457 |        -164.1781 |
[32m[20221214 14:35:06 @agent_ppo2.py:185][0m |           0.0023 |         220.8524 |        -164.4028 |
[32m[20221214 14:35:06 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:35:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.41
[32m[20221214 14:35:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.85
[32m[20221214 14:35:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.91
[32m[20221214 14:35:06 @agent_ppo2.py:143][0m Total time:      37.06 min
[32m[20221214 14:35:06 @agent_ppo2.py:145][0m 3407872 total steps have happened
[32m[20221214 14:35:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1664 --------------------------#
[32m[20221214 14:35:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:35:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:06 @agent_ppo2.py:185][0m |          -0.0008 |         277.2276 |        -162.2726 |
[32m[20221214 14:35:06 @agent_ppo2.py:185][0m |          -0.0002 |         263.1363 |        -162.8694 |
[32m[20221214 14:35:06 @agent_ppo2.py:185][0m |          -0.0020 |         255.4848 |        -162.7710 |
[32m[20221214 14:35:06 @agent_ppo2.py:185][0m |          -0.0019 |         250.4936 |        -163.2389 |
[32m[20221214 14:35:07 @agent_ppo2.py:185][0m |           0.0077 |         257.3346 |        -163.1817 |
[32m[20221214 14:35:07 @agent_ppo2.py:185][0m |          -0.0034 |         246.1436 |        -163.1730 |
[32m[20221214 14:35:07 @agent_ppo2.py:185][0m |          -0.0021 |         242.9945 |        -163.2109 |
[32m[20221214 14:35:07 @agent_ppo2.py:185][0m |           0.0031 |         242.4945 |        -163.8565 |
[32m[20221214 14:35:07 @agent_ppo2.py:185][0m |          -0.0039 |         240.6083 |        -162.8313 |
[32m[20221214 14:35:07 @agent_ppo2.py:185][0m |          -0.0028 |         238.4834 |        -163.6513 |
[32m[20221214 14:35:07 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:35:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.72
[32m[20221214 14:35:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.03
[32m[20221214 14:35:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.25
[32m[20221214 14:35:07 @agent_ppo2.py:143][0m Total time:      37.08 min
[32m[20221214 14:35:07 @agent_ppo2.py:145][0m 3409920 total steps have happened
[32m[20221214 14:35:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1665 --------------------------#
[32m[20221214 14:35:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:35:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:07 @agent_ppo2.py:185][0m |          -0.0006 |         198.6765 |        -168.0234 |
[32m[20221214 14:35:07 @agent_ppo2.py:185][0m |          -0.0002 |         171.9483 |        -168.2458 |
[32m[20221214 14:35:08 @agent_ppo2.py:185][0m |           0.0077 |         173.7738 |        -167.7662 |
[32m[20221214 14:35:08 @agent_ppo2.py:185][0m |           0.0072 |         165.2007 |        -167.7946 |
[32m[20221214 14:35:08 @agent_ppo2.py:185][0m |           0.0076 |         167.0454 |        -168.1743 |
[32m[20221214 14:35:08 @agent_ppo2.py:185][0m |          -0.0020 |         158.0015 |        -168.1097 |
[32m[20221214 14:35:08 @agent_ppo2.py:185][0m |          -0.0036 |         156.7203 |        -168.3483 |
[32m[20221214 14:35:08 @agent_ppo2.py:185][0m |           0.0006 |         156.2316 |        -168.4048 |
[32m[20221214 14:35:08 @agent_ppo2.py:185][0m |          -0.0036 |         153.9718 |        -168.2096 |
[32m[20221214 14:35:08 @agent_ppo2.py:185][0m |           0.0166 |         177.0700 |        -168.3109 |
[32m[20221214 14:35:08 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:35:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.03
[32m[20221214 14:35:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.48
[32m[20221214 14:35:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.99
[32m[20221214 14:35:08 @agent_ppo2.py:143][0m Total time:      37.10 min
[32m[20221214 14:35:08 @agent_ppo2.py:145][0m 3411968 total steps have happened
[32m[20221214 14:35:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1666 --------------------------#
[32m[20221214 14:35:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:35:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:09 @agent_ppo2.py:185][0m |          -0.0035 |         213.9579 |        -168.2435 |
[32m[20221214 14:35:09 @agent_ppo2.py:185][0m |          -0.0015 |         196.7507 |        -168.0801 |
[32m[20221214 14:35:09 @agent_ppo2.py:185][0m |          -0.0005 |         190.6603 |        -167.9973 |
[32m[20221214 14:35:09 @agent_ppo2.py:185][0m |          -0.0028 |         187.4678 |        -167.9103 |
[32m[20221214 14:35:09 @agent_ppo2.py:185][0m |           0.0040 |         192.2040 |        -168.0321 |
[32m[20221214 14:35:09 @agent_ppo2.py:185][0m |           0.0018 |         192.2314 |        -167.7116 |
[32m[20221214 14:35:09 @agent_ppo2.py:185][0m |          -0.0005 |         183.1785 |        -167.3101 |
[32m[20221214 14:35:09 @agent_ppo2.py:185][0m |          -0.0047 |         182.0495 |        -167.6067 |
[32m[20221214 14:35:09 @agent_ppo2.py:185][0m |           0.0010 |         182.1353 |        -167.6003 |
[32m[20221214 14:35:09 @agent_ppo2.py:185][0m |          -0.0016 |         180.5507 |        -167.3864 |
[32m[20221214 14:35:09 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:35:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.93
[32m[20221214 14:35:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.33
[32m[20221214 14:35:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.46
[32m[20221214 14:35:10 @agent_ppo2.py:143][0m Total time:      37.12 min
[32m[20221214 14:35:10 @agent_ppo2.py:145][0m 3414016 total steps have happened
[32m[20221214 14:35:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1667 --------------------------#
[32m[20221214 14:35:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:35:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:10 @agent_ppo2.py:185][0m |           0.0009 |         246.5514 |        -161.6110 |
[32m[20221214 14:35:10 @agent_ppo2.py:185][0m |          -0.0030 |         234.2908 |        -162.2015 |
[32m[20221214 14:35:10 @agent_ppo2.py:185][0m |          -0.0007 |         229.6328 |        -161.4841 |
[32m[20221214 14:35:10 @agent_ppo2.py:185][0m |           0.0064 |         242.0106 |        -161.6639 |
[32m[20221214 14:35:10 @agent_ppo2.py:185][0m |          -0.0019 |         223.2379 |        -161.1505 |
[32m[20221214 14:35:10 @agent_ppo2.py:185][0m |           0.0115 |         249.0422 |        -161.6724 |
[32m[20221214 14:35:10 @agent_ppo2.py:185][0m |          -0.0024 |         219.8737 |        -161.3510 |
[32m[20221214 14:35:10 @agent_ppo2.py:185][0m |          -0.0037 |         220.1432 |        -161.6757 |
[32m[20221214 14:35:11 @agent_ppo2.py:185][0m |          -0.0039 |         217.8509 |        -161.2825 |
[32m[20221214 14:35:11 @agent_ppo2.py:185][0m |          -0.0017 |         216.8864 |        -160.8082 |
[32m[20221214 14:35:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:35:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.79
[32m[20221214 14:35:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.30
[32m[20221214 14:35:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.43
[32m[20221214 14:35:11 @agent_ppo2.py:143][0m Total time:      37.14 min
[32m[20221214 14:35:11 @agent_ppo2.py:145][0m 3416064 total steps have happened
[32m[20221214 14:35:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1668 --------------------------#
[32m[20221214 14:35:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:35:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:11 @agent_ppo2.py:185][0m |           0.0059 |         230.4209 |        -161.1790 |
[32m[20221214 14:35:11 @agent_ppo2.py:185][0m |          -0.0057 |         186.1160 |        -161.1509 |
[32m[20221214 14:35:11 @agent_ppo2.py:185][0m |          -0.0036 |         170.9632 |        -161.3121 |
[32m[20221214 14:35:11 @agent_ppo2.py:185][0m |          -0.0015 |         165.1501 |        -161.2942 |
[32m[20221214 14:35:11 @agent_ppo2.py:185][0m |           0.0021 |         160.9832 |        -161.4789 |
[32m[20221214 14:35:11 @agent_ppo2.py:185][0m |           0.0092 |         161.8799 |        -160.8745 |
[32m[20221214 14:35:12 @agent_ppo2.py:185][0m |          -0.0047 |         155.0930 |        -161.6349 |
[32m[20221214 14:35:12 @agent_ppo2.py:185][0m |          -0.0036 |         152.8192 |        -161.5240 |
[32m[20221214 14:35:12 @agent_ppo2.py:185][0m |          -0.0026 |         152.4511 |        -161.6030 |
[32m[20221214 14:35:12 @agent_ppo2.py:185][0m |           0.0092 |         175.0692 |        -161.8573 |
[32m[20221214 14:35:12 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:35:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.11
[32m[20221214 14:35:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.00
[32m[20221214 14:35:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.87
[32m[20221214 14:35:12 @agent_ppo2.py:143][0m Total time:      37.16 min
[32m[20221214 14:35:12 @agent_ppo2.py:145][0m 3418112 total steps have happened
[32m[20221214 14:35:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1669 --------------------------#
[32m[20221214 14:35:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:35:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:12 @agent_ppo2.py:185][0m |          -0.0038 |         183.4162 |        -164.4602 |
[32m[20221214 14:35:12 @agent_ppo2.py:185][0m |          -0.0013 |         173.0262 |        -164.1133 |
[32m[20221214 14:35:12 @agent_ppo2.py:185][0m |          -0.0019 |         167.6136 |        -164.1588 |
[32m[20221214 14:35:13 @agent_ppo2.py:185][0m |          -0.0023 |         163.6484 |        -164.1822 |
[32m[20221214 14:35:13 @agent_ppo2.py:185][0m |          -0.0011 |         162.0889 |        -164.3949 |
[32m[20221214 14:35:13 @agent_ppo2.py:185][0m |           0.0034 |         162.1953 |        -164.4664 |
[32m[20221214 14:35:13 @agent_ppo2.py:185][0m |          -0.0019 |         155.5424 |        -164.5043 |
[32m[20221214 14:35:13 @agent_ppo2.py:185][0m |          -0.0025 |         153.4980 |        -164.1220 |
[32m[20221214 14:35:13 @agent_ppo2.py:185][0m |          -0.0023 |         151.2373 |        -164.5742 |
[32m[20221214 14:35:13 @agent_ppo2.py:185][0m |          -0.0008 |         149.3659 |        -164.7000 |
[32m[20221214 14:35:13 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:35:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.88
[32m[20221214 14:35:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.43
[32m[20221214 14:35:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.89
[32m[20221214 14:35:13 @agent_ppo2.py:143][0m Total time:      37.18 min
[32m[20221214 14:35:13 @agent_ppo2.py:145][0m 3420160 total steps have happened
[32m[20221214 14:35:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1670 --------------------------#
[32m[20221214 14:35:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:35:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:14 @agent_ppo2.py:185][0m |          -0.0031 |         199.4616 |        -164.9992 |
[32m[20221214 14:35:14 @agent_ppo2.py:185][0m |          -0.0009 |         181.9807 |        -164.4927 |
[32m[20221214 14:35:14 @agent_ppo2.py:185][0m |          -0.0012 |         177.4220 |        -164.9827 |
[32m[20221214 14:35:14 @agent_ppo2.py:185][0m |          -0.0017 |         174.4757 |        -164.8921 |
[32m[20221214 14:35:14 @agent_ppo2.py:185][0m |          -0.0018 |         171.4662 |        -164.7946 |
[32m[20221214 14:35:14 @agent_ppo2.py:185][0m |          -0.0042 |         169.9219 |        -165.3508 |
[32m[20221214 14:35:14 @agent_ppo2.py:185][0m |          -0.0032 |         169.1351 |        -164.7934 |
[32m[20221214 14:35:14 @agent_ppo2.py:185][0m |           0.0049 |         172.5831 |        -165.4456 |
[32m[20221214 14:35:14 @agent_ppo2.py:185][0m |          -0.0030 |         167.9132 |        -165.1466 |
[32m[20221214 14:35:14 @agent_ppo2.py:185][0m |           0.0046 |         175.5629 |        -165.2725 |
[32m[20221214 14:35:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:35:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.29
[32m[20221214 14:35:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.30
[32m[20221214 14:35:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.96
[32m[20221214 14:35:14 @agent_ppo2.py:143][0m Total time:      37.21 min
[32m[20221214 14:35:14 @agent_ppo2.py:145][0m 3422208 total steps have happened
[32m[20221214 14:35:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1671 --------------------------#
[32m[20221214 14:35:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:35:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:15 @agent_ppo2.py:185][0m |          -0.0047 |         271.6212 |        -165.6511 |
[32m[20221214 14:35:15 @agent_ppo2.py:185][0m |          -0.0019 |         262.0014 |        -165.2452 |
[32m[20221214 14:35:15 @agent_ppo2.py:185][0m |          -0.0032 |         258.8821 |        -166.1883 |
[32m[20221214 14:35:15 @agent_ppo2.py:185][0m |          -0.0031 |         256.9034 |        -166.3212 |
[32m[20221214 14:35:15 @agent_ppo2.py:185][0m |          -0.0022 |         254.8996 |        -166.3840 |
[32m[20221214 14:35:15 @agent_ppo2.py:185][0m |          -0.0042 |         255.1078 |        -166.6695 |
[32m[20221214 14:35:15 @agent_ppo2.py:185][0m |           0.0050 |         262.9483 |        -166.8308 |
[32m[20221214 14:35:15 @agent_ppo2.py:185][0m |          -0.0030 |         253.2051 |        -167.1010 |
[32m[20221214 14:35:15 @agent_ppo2.py:185][0m |          -0.0022 |         252.2362 |        -167.5311 |
[32m[20221214 14:35:16 @agent_ppo2.py:185][0m |          -0.0013 |         251.9020 |        -167.1961 |
[32m[20221214 14:35:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:35:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.13
[32m[20221214 14:35:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.36
[32m[20221214 14:35:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.60
[32m[20221214 14:35:16 @agent_ppo2.py:143][0m Total time:      37.23 min
[32m[20221214 14:35:16 @agent_ppo2.py:145][0m 3424256 total steps have happened
[32m[20221214 14:35:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1672 --------------------------#
[32m[20221214 14:35:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:35:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:16 @agent_ppo2.py:185][0m |          -0.0006 |         255.6672 |        -169.5732 |
[32m[20221214 14:35:16 @agent_ppo2.py:185][0m |          -0.0018 |         246.1984 |        -169.8097 |
[32m[20221214 14:35:16 @agent_ppo2.py:185][0m |          -0.0021 |         241.8498 |        -169.9634 |
[32m[20221214 14:35:16 @agent_ppo2.py:185][0m |          -0.0018 |         239.3976 |        -170.0889 |
[32m[20221214 14:35:16 @agent_ppo2.py:185][0m |          -0.0029 |         237.2628 |        -170.1179 |
[32m[20221214 14:35:16 @agent_ppo2.py:185][0m |           0.0009 |         235.0407 |        -169.8284 |
[32m[20221214 14:35:17 @agent_ppo2.py:185][0m |          -0.0027 |         233.2477 |        -169.9866 |
[32m[20221214 14:35:17 @agent_ppo2.py:185][0m |          -0.0023 |         231.2922 |        -170.4331 |
[32m[20221214 14:35:17 @agent_ppo2.py:185][0m |          -0.0010 |         230.3056 |        -170.7205 |
[32m[20221214 14:35:17 @agent_ppo2.py:185][0m |          -0.0026 |         229.8385 |        -170.5329 |
[32m[20221214 14:35:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:35:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.53
[32m[20221214 14:35:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.60
[32m[20221214 14:35:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.00
[32m[20221214 14:35:17 @agent_ppo2.py:143][0m Total time:      37.25 min
[32m[20221214 14:35:17 @agent_ppo2.py:145][0m 3426304 total steps have happened
[32m[20221214 14:35:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1673 --------------------------#
[32m[20221214 14:35:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:35:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:17 @agent_ppo2.py:185][0m |          -0.0024 |         225.6617 |        -170.0871 |
[32m[20221214 14:35:17 @agent_ppo2.py:185][0m |          -0.0033 |         210.2649 |        -170.4768 |
[32m[20221214 14:35:17 @agent_ppo2.py:185][0m |          -0.0006 |         204.9710 |        -170.9642 |
[32m[20221214 14:35:18 @agent_ppo2.py:185][0m |          -0.0019 |         201.1369 |        -170.7079 |
[32m[20221214 14:35:18 @agent_ppo2.py:185][0m |           0.0084 |         216.5072 |        -171.1359 |
[32m[20221214 14:35:18 @agent_ppo2.py:185][0m |          -0.0039 |         197.6727 |        -171.1494 |
[32m[20221214 14:35:18 @agent_ppo2.py:185][0m |          -0.0002 |         195.9170 |        -170.6223 |
[32m[20221214 14:35:18 @agent_ppo2.py:185][0m |          -0.0029 |         193.5439 |        -171.4431 |
[32m[20221214 14:35:18 @agent_ppo2.py:185][0m |          -0.0029 |         192.8666 |        -171.6894 |
[32m[20221214 14:35:18 @agent_ppo2.py:185][0m |          -0.0036 |         191.0243 |        -171.3691 |
[32m[20221214 14:35:18 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:35:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.74
[32m[20221214 14:35:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.90
[32m[20221214 14:35:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.27
[32m[20221214 14:35:18 @agent_ppo2.py:143][0m Total time:      37.27 min
[32m[20221214 14:35:18 @agent_ppo2.py:145][0m 3428352 total steps have happened
[32m[20221214 14:35:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1674 --------------------------#
[32m[20221214 14:35:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:19 @agent_ppo2.py:185][0m |           0.0026 |         272.3093 |        -174.5439 |
[32m[20221214 14:35:19 @agent_ppo2.py:185][0m |          -0.0030 |         255.6775 |        -174.8947 |
[32m[20221214 14:35:19 @agent_ppo2.py:185][0m |          -0.0026 |         249.6743 |        -174.9646 |
[32m[20221214 14:35:19 @agent_ppo2.py:185][0m |          -0.0022 |         246.4733 |        -175.4561 |
[32m[20221214 14:35:19 @agent_ppo2.py:185][0m |          -0.0021 |         244.6061 |        -175.1447 |
[32m[20221214 14:35:19 @agent_ppo2.py:185][0m |          -0.0012 |         243.4024 |        -175.0453 |
[32m[20221214 14:35:19 @agent_ppo2.py:185][0m |          -0.0023 |         242.3923 |        -175.0033 |
[32m[20221214 14:35:19 @agent_ppo2.py:185][0m |          -0.0021 |         241.8262 |        -175.4443 |
[32m[20221214 14:35:19 @agent_ppo2.py:185][0m |          -0.0018 |         241.2640 |        -175.5685 |
[32m[20221214 14:35:20 @agent_ppo2.py:185][0m |          -0.0021 |         240.6025 |        -175.0170 |
[32m[20221214 14:35:20 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:35:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.04
[32m[20221214 14:35:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.59
[32m[20221214 14:35:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.69
[32m[20221214 14:35:20 @agent_ppo2.py:143][0m Total time:      37.29 min
[32m[20221214 14:35:20 @agent_ppo2.py:145][0m 3430400 total steps have happened
[32m[20221214 14:35:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1675 --------------------------#
[32m[20221214 14:35:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:20 @agent_ppo2.py:185][0m |           0.0064 |         256.4101 |        -172.8351 |
[32m[20221214 14:35:20 @agent_ppo2.py:185][0m |           0.0003 |         247.1434 |        -172.6668 |
[32m[20221214 14:35:20 @agent_ppo2.py:185][0m |          -0.0008 |         245.1551 |        -172.7708 |
[32m[20221214 14:35:20 @agent_ppo2.py:185][0m |          -0.0020 |         243.2530 |        -172.9193 |
[32m[20221214 14:35:20 @agent_ppo2.py:185][0m |          -0.0011 |         239.8272 |        -172.9777 |
[32m[20221214 14:35:21 @agent_ppo2.py:185][0m |          -0.0025 |         236.6650 |        -172.8672 |
[32m[20221214 14:35:21 @agent_ppo2.py:185][0m |          -0.0022 |         235.2257 |        -172.6323 |
[32m[20221214 14:35:21 @agent_ppo2.py:185][0m |           0.0059 |         243.4672 |        -173.0638 |
[32m[20221214 14:35:21 @agent_ppo2.py:185][0m |          -0.0017 |         234.4470 |        -172.9245 |
[32m[20221214 14:35:21 @agent_ppo2.py:185][0m |          -0.0022 |         233.9269 |        -172.5780 |
[32m[20221214 14:35:21 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:35:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.33
[32m[20221214 14:35:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.19
[32m[20221214 14:35:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.69
[32m[20221214 14:35:21 @agent_ppo2.py:143][0m Total time:      37.32 min
[32m[20221214 14:35:21 @agent_ppo2.py:145][0m 3432448 total steps have happened
[32m[20221214 14:35:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1676 --------------------------#
[32m[20221214 14:35:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:21 @agent_ppo2.py:185][0m |           0.0058 |         258.2887 |        -172.5655 |
[32m[20221214 14:35:21 @agent_ppo2.py:185][0m |           0.0016 |         245.0717 |        -173.1274 |
[32m[20221214 14:35:22 @agent_ppo2.py:185][0m |          -0.0014 |         241.7454 |        -172.7167 |
[32m[20221214 14:35:22 @agent_ppo2.py:185][0m |          -0.0027 |         239.5735 |        -172.5881 |
[32m[20221214 14:35:22 @agent_ppo2.py:185][0m |          -0.0014 |         236.7528 |        -172.8446 |
[32m[20221214 14:35:22 @agent_ppo2.py:185][0m |          -0.0036 |         234.4956 |        -172.9067 |
[32m[20221214 14:35:22 @agent_ppo2.py:185][0m |          -0.0032 |         232.9543 |        -172.4829 |
[32m[20221214 14:35:22 @agent_ppo2.py:185][0m |          -0.0021 |         231.3844 |        -172.7527 |
[32m[20221214 14:35:22 @agent_ppo2.py:185][0m |          -0.0002 |         230.5670 |        -172.4625 |
[32m[20221214 14:35:22 @agent_ppo2.py:185][0m |          -0.0018 |         229.7970 |        -172.5615 |
[32m[20221214 14:35:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:35:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.68
[32m[20221214 14:35:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.61
[32m[20221214 14:35:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.51
[32m[20221214 14:35:22 @agent_ppo2.py:143][0m Total time:      37.34 min
[32m[20221214 14:35:22 @agent_ppo2.py:145][0m 3434496 total steps have happened
[32m[20221214 14:35:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1677 --------------------------#
[32m[20221214 14:35:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:23 @agent_ppo2.py:185][0m |          -0.0005 |         260.5458 |        -175.8875 |
[32m[20221214 14:35:23 @agent_ppo2.py:185][0m |          -0.0017 |         249.0497 |        -175.9336 |
[32m[20221214 14:35:23 @agent_ppo2.py:185][0m |           0.0006 |         242.0288 |        -176.3538 |
[32m[20221214 14:35:23 @agent_ppo2.py:185][0m |          -0.0032 |         239.8068 |        -176.1270 |
[32m[20221214 14:35:23 @agent_ppo2.py:185][0m |          -0.0019 |         237.2777 |        -175.3918 |
[32m[20221214 14:35:23 @agent_ppo2.py:185][0m |          -0.0005 |         236.4735 |        -176.0783 |
[32m[20221214 14:35:23 @agent_ppo2.py:185][0m |          -0.0018 |         235.8064 |        -175.5922 |
[32m[20221214 14:35:23 @agent_ppo2.py:185][0m |          -0.0007 |         234.7885 |        -175.0265 |
[32m[20221214 14:35:24 @agent_ppo2.py:185][0m |          -0.0022 |         234.2203 |        -175.5623 |
[32m[20221214 14:35:24 @agent_ppo2.py:185][0m |           0.0074 |         252.6495 |        -175.6998 |
[32m[20221214 14:35:24 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:35:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.30
[32m[20221214 14:35:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.77
[32m[20221214 14:35:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.74
[32m[20221214 14:35:24 @agent_ppo2.py:143][0m Total time:      37.36 min
[32m[20221214 14:35:24 @agent_ppo2.py:145][0m 3436544 total steps have happened
[32m[20221214 14:35:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1678 --------------------------#
[32m[20221214 14:35:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:24 @agent_ppo2.py:185][0m |          -0.0009 |         252.8852 |        -170.7814 |
[32m[20221214 14:35:24 @agent_ppo2.py:185][0m |          -0.0021 |         248.7983 |        -170.8606 |
[32m[20221214 14:35:24 @agent_ppo2.py:185][0m |           0.0118 |         277.7221 |        -171.0152 |
[32m[20221214 14:35:24 @agent_ppo2.py:185][0m |          -0.0005 |         246.8222 |        -171.4231 |
[32m[20221214 14:35:24 @agent_ppo2.py:185][0m |           0.0160 |         266.9592 |        -171.1492 |
[32m[20221214 14:35:25 @agent_ppo2.py:185][0m |          -0.0009 |         245.3171 |        -171.2960 |
[32m[20221214 14:35:25 @agent_ppo2.py:185][0m |           0.0010 |         244.8992 |        -171.4059 |
[32m[20221214 14:35:25 @agent_ppo2.py:185][0m |          -0.0011 |         244.7359 |        -171.3887 |
[32m[20221214 14:35:25 @agent_ppo2.py:185][0m |           0.0074 |         249.4963 |        -171.2206 |
[32m[20221214 14:35:25 @agent_ppo2.py:185][0m |          -0.0029 |         243.7400 |        -171.2271 |
[32m[20221214 14:35:25 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:35:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.70
[32m[20221214 14:35:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.28
[32m[20221214 14:35:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.34
[32m[20221214 14:35:25 @agent_ppo2.py:143][0m Total time:      37.38 min
[32m[20221214 14:35:25 @agent_ppo2.py:145][0m 3438592 total steps have happened
[32m[20221214 14:35:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1679 --------------------------#
[32m[20221214 14:35:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:25 @agent_ppo2.py:185][0m |          -0.0009 |         230.2506 |        -173.9512 |
[32m[20221214 14:35:26 @agent_ppo2.py:185][0m |           0.0030 |         222.9051 |        -173.9204 |
[32m[20221214 14:35:26 @agent_ppo2.py:185][0m |          -0.0017 |         215.0736 |        -174.1344 |
[32m[20221214 14:35:26 @agent_ppo2.py:185][0m |           0.0023 |         213.4920 |        -174.1997 |
[32m[20221214 14:35:26 @agent_ppo2.py:185][0m |           0.0075 |         220.5692 |        -174.4024 |
[32m[20221214 14:35:26 @agent_ppo2.py:185][0m |          -0.0015 |         210.8647 |        -174.1965 |
[32m[20221214 14:35:26 @agent_ppo2.py:185][0m |          -0.0026 |         210.6186 |        -173.5826 |
[32m[20221214 14:35:26 @agent_ppo2.py:185][0m |           0.0108 |         235.0160 |        -174.0006 |
[32m[20221214 14:35:26 @agent_ppo2.py:185][0m |           0.0147 |         238.7793 |        -174.1424 |
[32m[20221214 14:35:26 @agent_ppo2.py:185][0m |          -0.0002 |         209.3136 |        -174.6370 |
[32m[20221214 14:35:26 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:35:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.88
[32m[20221214 14:35:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.84
[32m[20221214 14:35:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.86
[32m[20221214 14:35:26 @agent_ppo2.py:143][0m Total time:      37.41 min
[32m[20221214 14:35:26 @agent_ppo2.py:145][0m 3440640 total steps have happened
[32m[20221214 14:35:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1680 --------------------------#
[32m[20221214 14:35:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:35:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:27 @agent_ppo2.py:185][0m |          -0.0024 |         229.5407 |        -172.5156 |
[32m[20221214 14:35:27 @agent_ppo2.py:185][0m |          -0.0007 |         228.6356 |        -172.4114 |
[32m[20221214 14:35:27 @agent_ppo2.py:185][0m |          -0.0039 |         224.1756 |        -172.4838 |
[32m[20221214 14:35:27 @agent_ppo2.py:185][0m |          -0.0045 |         223.2624 |        -172.5177 |
[32m[20221214 14:35:27 @agent_ppo2.py:185][0m |          -0.0016 |         224.3007 |        -172.6104 |
[32m[20221214 14:35:27 @agent_ppo2.py:185][0m |          -0.0025 |         226.8448 |        -172.6550 |
[32m[20221214 14:35:27 @agent_ppo2.py:185][0m |          -0.0044 |         222.2440 |        -172.6166 |
[32m[20221214 14:35:27 @agent_ppo2.py:185][0m |          -0.0070 |         221.7284 |        -172.7392 |
[32m[20221214 14:35:28 @agent_ppo2.py:185][0m |          -0.0079 |         221.2578 |        -172.6868 |
[32m[20221214 14:35:28 @agent_ppo2.py:185][0m |          -0.0050 |         220.3819 |        -172.3106 |
[32m[20221214 14:35:28 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:35:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.82
[32m[20221214 14:35:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.88
[32m[20221214 14:35:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.54
[32m[20221214 14:35:28 @agent_ppo2.py:143][0m Total time:      37.43 min
[32m[20221214 14:35:28 @agent_ppo2.py:145][0m 3442688 total steps have happened
[32m[20221214 14:35:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1681 --------------------------#
[32m[20221214 14:35:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:28 @agent_ppo2.py:185][0m |           0.0000 |         253.3208 |        -174.5167 |
[32m[20221214 14:35:28 @agent_ppo2.py:185][0m |          -0.0017 |         235.1648 |        -173.8985 |
[32m[20221214 14:35:28 @agent_ppo2.py:185][0m |           0.0108 |         245.4459 |        -174.7347 |
[32m[20221214 14:35:28 @agent_ppo2.py:185][0m |          -0.0022 |         229.1705 |        -174.3157 |
[32m[20221214 14:35:29 @agent_ppo2.py:185][0m |          -0.0031 |         228.3450 |        -174.4470 |
[32m[20221214 14:35:29 @agent_ppo2.py:185][0m |           0.0051 |         247.0336 |        -174.6283 |
[32m[20221214 14:35:29 @agent_ppo2.py:185][0m |          -0.0050 |         226.7692 |        -173.9818 |
[32m[20221214 14:35:29 @agent_ppo2.py:185][0m |           0.0056 |         239.9344 |        -174.0980 |
[32m[20221214 14:35:29 @agent_ppo2.py:185][0m |           0.0085 |         247.6684 |        -173.5785 |
[32m[20221214 14:35:29 @agent_ppo2.py:185][0m |          -0.0036 |         225.7642 |        -173.9659 |
[32m[20221214 14:35:29 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:35:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.77
[32m[20221214 14:35:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.15
[32m[20221214 14:35:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.68
[32m[20221214 14:35:29 @agent_ppo2.py:143][0m Total time:      37.45 min
[32m[20221214 14:35:29 @agent_ppo2.py:145][0m 3444736 total steps have happened
[32m[20221214 14:35:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1682 --------------------------#
[32m[20221214 14:35:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:29 @agent_ppo2.py:185][0m |          -0.0012 |         225.6878 |        -174.7739 |
[32m[20221214 14:35:30 @agent_ppo2.py:185][0m |          -0.0004 |         221.7752 |        -174.5839 |
[32m[20221214 14:35:30 @agent_ppo2.py:185][0m |          -0.0019 |         219.2715 |        -174.7662 |
[32m[20221214 14:35:30 @agent_ppo2.py:185][0m |          -0.0008 |         217.9671 |        -174.4355 |
[32m[20221214 14:35:30 @agent_ppo2.py:185][0m |           0.0001 |         217.5453 |        -174.3931 |
[32m[20221214 14:35:30 @agent_ppo2.py:185][0m |           0.0112 |         229.7928 |        -174.5402 |
[32m[20221214 14:35:30 @agent_ppo2.py:185][0m |          -0.0007 |         216.7651 |        -174.1122 |
[32m[20221214 14:35:30 @agent_ppo2.py:185][0m |          -0.0022 |         215.9779 |        -174.1788 |
[32m[20221214 14:35:30 @agent_ppo2.py:185][0m |          -0.0014 |         216.1012 |        -174.3610 |
[32m[20221214 14:35:30 @agent_ppo2.py:185][0m |          -0.0021 |         214.7704 |        -173.9041 |
[32m[20221214 14:35:30 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:35:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.95
[32m[20221214 14:35:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.95
[32m[20221214 14:35:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.61
[32m[20221214 14:35:30 @agent_ppo2.py:143][0m Total time:      37.47 min
[32m[20221214 14:35:30 @agent_ppo2.py:145][0m 3446784 total steps have happened
[32m[20221214 14:35:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1683 --------------------------#
[32m[20221214 14:35:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:31 @agent_ppo2.py:185][0m |           0.0046 |         222.0818 |        -171.5162 |
[32m[20221214 14:35:31 @agent_ppo2.py:185][0m |          -0.0014 |         212.5361 |        -172.1057 |
[32m[20221214 14:35:31 @agent_ppo2.py:185][0m |           0.0066 |         217.2425 |        -171.9046 |
[32m[20221214 14:35:31 @agent_ppo2.py:185][0m |          -0.0017 |         206.4406 |        -172.2172 |
[32m[20221214 14:35:31 @agent_ppo2.py:185][0m |          -0.0019 |         204.3366 |        -172.2221 |
[32m[20221214 14:35:31 @agent_ppo2.py:185][0m |          -0.0038 |         203.7655 |        -172.1152 |
[32m[20221214 14:35:31 @agent_ppo2.py:185][0m |           0.0090 |         214.8793 |        -172.1942 |
[32m[20221214 14:35:31 @agent_ppo2.py:185][0m |          -0.0017 |         204.0788 |        -172.2386 |
[32m[20221214 14:35:32 @agent_ppo2.py:185][0m |          -0.0008 |         202.7206 |        -172.2809 |
[32m[20221214 14:35:32 @agent_ppo2.py:185][0m |          -0.0013 |         202.3909 |        -172.0815 |
[32m[20221214 14:35:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:35:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.91
[32m[20221214 14:35:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.81
[32m[20221214 14:35:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.57
[32m[20221214 14:35:32 @agent_ppo2.py:143][0m Total time:      37.50 min
[32m[20221214 14:35:32 @agent_ppo2.py:145][0m 3448832 total steps have happened
[32m[20221214 14:35:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1684 --------------------------#
[32m[20221214 14:35:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:32 @agent_ppo2.py:185][0m |          -0.0036 |         190.7329 |        -170.3352 |
[32m[20221214 14:35:32 @agent_ppo2.py:185][0m |           0.0006 |         186.4250 |        -170.9188 |
[32m[20221214 14:35:32 @agent_ppo2.py:185][0m |           0.0033 |         190.9080 |        -170.6958 |
[32m[20221214 14:35:32 @agent_ppo2.py:185][0m |          -0.0013 |         182.7878 |        -170.7925 |
[32m[20221214 14:35:33 @agent_ppo2.py:185][0m |          -0.0048 |         181.9356 |        -171.0371 |
[32m[20221214 14:35:33 @agent_ppo2.py:185][0m |          -0.0022 |         180.7855 |        -171.3049 |
[32m[20221214 14:35:33 @agent_ppo2.py:185][0m |          -0.0044 |         180.5668 |        -171.3603 |
[32m[20221214 14:35:33 @agent_ppo2.py:185][0m |          -0.0027 |         179.2047 |        -171.5697 |
[32m[20221214 14:35:33 @agent_ppo2.py:185][0m |           0.0016 |         180.9264 |        -171.5465 |
[32m[20221214 14:35:33 @agent_ppo2.py:185][0m |          -0.0029 |         179.3399 |        -172.0680 |
[32m[20221214 14:35:33 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:35:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.84
[32m[20221214 14:35:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.31
[32m[20221214 14:35:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.76
[32m[20221214 14:35:33 @agent_ppo2.py:143][0m Total time:      37.52 min
[32m[20221214 14:35:33 @agent_ppo2.py:145][0m 3450880 total steps have happened
[32m[20221214 14:35:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1685 --------------------------#
[32m[20221214 14:35:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:34 @agent_ppo2.py:185][0m |           0.0078 |         234.1211 |        -176.3751 |
[32m[20221214 14:35:34 @agent_ppo2.py:185][0m |          -0.0040 |         218.3352 |        -176.5073 |
[32m[20221214 14:35:34 @agent_ppo2.py:185][0m |          -0.0021 |         216.1581 |        -175.8904 |
[32m[20221214 14:35:34 @agent_ppo2.py:185][0m |          -0.0014 |         215.0572 |        -176.3453 |
[32m[20221214 14:35:34 @agent_ppo2.py:185][0m |          -0.0052 |         214.0605 |        -176.6712 |
[32m[20221214 14:35:34 @agent_ppo2.py:185][0m |          -0.0022 |         213.5259 |        -176.1502 |
[32m[20221214 14:35:34 @agent_ppo2.py:185][0m |           0.0072 |         226.7016 |        -176.8003 |
[32m[20221214 14:35:34 @agent_ppo2.py:185][0m |          -0.0036 |         213.5041 |        -176.2280 |
[32m[20221214 14:35:34 @agent_ppo2.py:185][0m |          -0.0013 |         212.9345 |        -176.5385 |
[32m[20221214 14:35:34 @agent_ppo2.py:185][0m |          -0.0070 |         212.9912 |        -176.2814 |
[32m[20221214 14:35:34 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:35:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.13
[32m[20221214 14:35:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.35
[32m[20221214 14:35:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.48
[32m[20221214 14:35:35 @agent_ppo2.py:143][0m Total time:      37.54 min
[32m[20221214 14:35:35 @agent_ppo2.py:145][0m 3452928 total steps have happened
[32m[20221214 14:35:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1686 --------------------------#
[32m[20221214 14:35:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:35 @agent_ppo2.py:185][0m |          -0.0000 |         214.7075 |        -174.9414 |
[32m[20221214 14:35:35 @agent_ppo2.py:185][0m |           0.0010 |         210.1387 |        -174.8586 |
[32m[20221214 14:35:35 @agent_ppo2.py:185][0m |          -0.0008 |         206.5615 |        -174.8862 |
[32m[20221214 14:35:35 @agent_ppo2.py:185][0m |           0.0032 |         208.1918 |        -175.4249 |
[32m[20221214 14:35:35 @agent_ppo2.py:185][0m |          -0.0038 |         203.8203 |        -175.5329 |
[32m[20221214 14:35:35 @agent_ppo2.py:185][0m |           0.0039 |         208.6072 |        -175.6763 |
[32m[20221214 14:35:35 @agent_ppo2.py:185][0m |          -0.0024 |         202.0788 |        -176.0522 |
[32m[20221214 14:35:36 @agent_ppo2.py:185][0m |           0.0055 |         204.0346 |        -176.1718 |
[32m[20221214 14:35:36 @agent_ppo2.py:185][0m |          -0.0053 |         200.9685 |        -176.0990 |
[32m[20221214 14:35:36 @agent_ppo2.py:185][0m |          -0.0040 |         198.6960 |        -176.5355 |
[32m[20221214 14:35:36 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 14:35:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.64
[32m[20221214 14:35:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.28
[32m[20221214 14:35:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.29
[32m[20221214 14:35:36 @agent_ppo2.py:143][0m Total time:      37.56 min
[32m[20221214 14:35:36 @agent_ppo2.py:145][0m 3454976 total steps have happened
[32m[20221214 14:35:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1687 --------------------------#
[32m[20221214 14:35:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:36 @agent_ppo2.py:185][0m |          -0.0008 |         216.2649 |        -178.3970 |
[32m[20221214 14:35:36 @agent_ppo2.py:185][0m |          -0.0014 |         204.6449 |        -178.8242 |
[32m[20221214 14:35:36 @agent_ppo2.py:185][0m |          -0.0054 |         200.0324 |        -178.7686 |
[32m[20221214 14:35:37 @agent_ppo2.py:185][0m |          -0.0014 |         198.1648 |        -178.7362 |
[32m[20221214 14:35:37 @agent_ppo2.py:185][0m |          -0.0035 |         195.7927 |        -179.4455 |
[32m[20221214 14:35:37 @agent_ppo2.py:185][0m |          -0.0037 |         194.3313 |        -179.2549 |
[32m[20221214 14:35:37 @agent_ppo2.py:185][0m |           0.0021 |         195.3007 |        -179.2787 |
[32m[20221214 14:35:37 @agent_ppo2.py:185][0m |           0.0029 |         207.3083 |        -180.0893 |
[32m[20221214 14:35:37 @agent_ppo2.py:185][0m |          -0.0063 |         191.2715 |        -179.7560 |
[32m[20221214 14:35:37 @agent_ppo2.py:185][0m |          -0.0052 |         190.4941 |        -179.9764 |
[32m[20221214 14:35:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:35:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.23
[32m[20221214 14:35:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.26
[32m[20221214 14:35:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.81
[32m[20221214 14:35:37 @agent_ppo2.py:143][0m Total time:      37.59 min
[32m[20221214 14:35:37 @agent_ppo2.py:145][0m 3457024 total steps have happened
[32m[20221214 14:35:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1688 --------------------------#
[32m[20221214 14:35:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:35:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:38 @agent_ppo2.py:185][0m |           0.0082 |         257.7920 |        -180.9143 |
[32m[20221214 14:35:38 @agent_ppo2.py:185][0m |           0.0019 |         235.3026 |        -180.9963 |
[32m[20221214 14:35:38 @agent_ppo2.py:185][0m |           0.0014 |         228.1981 |        -180.8737 |
[32m[20221214 14:35:38 @agent_ppo2.py:185][0m |          -0.0014 |         220.3252 |        -180.4558 |
[32m[20221214 14:35:38 @agent_ppo2.py:185][0m |          -0.0024 |         214.5948 |        -181.0106 |
[32m[20221214 14:35:38 @agent_ppo2.py:185][0m |          -0.0019 |         212.0989 |        -180.5582 |
[32m[20221214 14:35:38 @agent_ppo2.py:185][0m |          -0.0038 |         210.0531 |        -180.6445 |
[32m[20221214 14:35:38 @agent_ppo2.py:185][0m |           0.0019 |         212.7862 |        -180.5577 |
[32m[20221214 14:35:38 @agent_ppo2.py:185][0m |          -0.0027 |         209.6182 |        -180.7504 |
[32m[20221214 14:35:38 @agent_ppo2.py:185][0m |          -0.0019 |         207.6782 |        -180.6717 |
[32m[20221214 14:35:38 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:35:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.84
[32m[20221214 14:35:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.10
[32m[20221214 14:35:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.87
[32m[20221214 14:35:38 @agent_ppo2.py:143][0m Total time:      37.61 min
[32m[20221214 14:35:38 @agent_ppo2.py:145][0m 3459072 total steps have happened
[32m[20221214 14:35:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1689 --------------------------#
[32m[20221214 14:35:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:39 @agent_ppo2.py:185][0m |           0.0082 |         241.6576 |        -179.8677 |
[32m[20221214 14:35:39 @agent_ppo2.py:185][0m |          -0.0019 |         205.1283 |        -179.0132 |
[32m[20221214 14:35:39 @agent_ppo2.py:185][0m |          -0.0015 |         200.8708 |        -179.4811 |
[32m[20221214 14:35:39 @agent_ppo2.py:185][0m |           0.0112 |         224.3863 |        -179.3846 |
[32m[20221214 14:35:39 @agent_ppo2.py:185][0m |          -0.0023 |         197.2946 |        -178.9022 |
[32m[20221214 14:35:39 @agent_ppo2.py:185][0m |          -0.0024 |         195.1077 |        -178.7453 |
[32m[20221214 14:35:39 @agent_ppo2.py:185][0m |          -0.0028 |         194.8692 |        -178.8241 |
[32m[20221214 14:35:39 @agent_ppo2.py:185][0m |           0.0056 |         204.0352 |        -178.8974 |
[32m[20221214 14:35:40 @agent_ppo2.py:185][0m |           0.0068 |         208.5897 |        -178.1688 |
[32m[20221214 14:35:40 @agent_ppo2.py:185][0m |          -0.0037 |         192.6450 |        -178.4663 |
[32m[20221214 14:35:40 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:35:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.00
[32m[20221214 14:35:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.35
[32m[20221214 14:35:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.00
[32m[20221214 14:35:40 @agent_ppo2.py:143][0m Total time:      37.63 min
[32m[20221214 14:35:40 @agent_ppo2.py:145][0m 3461120 total steps have happened
[32m[20221214 14:35:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1690 --------------------------#
[32m[20221214 14:35:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:35:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:40 @agent_ppo2.py:185][0m |          -0.0003 |         237.2746 |        -182.0993 |
[32m[20221214 14:35:40 @agent_ppo2.py:185][0m |          -0.0004 |         229.5169 |        -182.0813 |
[32m[20221214 14:35:40 @agent_ppo2.py:185][0m |          -0.0028 |         226.9153 |        -181.3663 |
[32m[20221214 14:35:40 @agent_ppo2.py:185][0m |           0.0131 |         248.8425 |        -181.2221 |
[32m[20221214 14:35:41 @agent_ppo2.py:185][0m |          -0.0008 |         225.0529 |        -181.2464 |
[32m[20221214 14:35:41 @agent_ppo2.py:185][0m |          -0.0024 |         224.6234 |        -181.6730 |
[32m[20221214 14:35:41 @agent_ppo2.py:185][0m |          -0.0047 |         223.1055 |        -180.6169 |
[32m[20221214 14:35:41 @agent_ppo2.py:185][0m |          -0.0041 |         222.5914 |        -181.0915 |
[32m[20221214 14:35:41 @agent_ppo2.py:185][0m |          -0.0028 |         221.9438 |        -181.6661 |
[32m[20221214 14:35:41 @agent_ppo2.py:185][0m |          -0.0024 |         221.7696 |        -180.8615 |
[32m[20221214 14:35:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:35:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.31
[32m[20221214 14:35:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.60
[32m[20221214 14:35:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.42
[32m[20221214 14:35:41 @agent_ppo2.py:143][0m Total time:      37.65 min
[32m[20221214 14:35:41 @agent_ppo2.py:145][0m 3463168 total steps have happened
[32m[20221214 14:35:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1691 --------------------------#
[32m[20221214 14:35:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:35:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:41 @agent_ppo2.py:185][0m |          -0.0032 |         234.1607 |        -179.7571 |
[32m[20221214 14:35:42 @agent_ppo2.py:185][0m |          -0.0020 |         228.5939 |        -179.4147 |
[32m[20221214 14:35:42 @agent_ppo2.py:185][0m |          -0.0021 |         226.2840 |        -179.1159 |
[32m[20221214 14:35:42 @agent_ppo2.py:185][0m |           0.0002 |         225.5034 |        -178.9638 |
[32m[20221214 14:35:42 @agent_ppo2.py:185][0m |          -0.0028 |         223.2120 |        -178.4454 |
[32m[20221214 14:35:42 @agent_ppo2.py:185][0m |          -0.0038 |         222.9507 |        -178.5248 |
[32m[20221214 14:35:42 @agent_ppo2.py:185][0m |          -0.0022 |         221.7519 |        -177.5778 |
[32m[20221214 14:35:42 @agent_ppo2.py:185][0m |          -0.0035 |         220.9584 |        -178.0667 |
[32m[20221214 14:35:42 @agent_ppo2.py:185][0m |          -0.0002 |         220.6661 |        -177.6450 |
[32m[20221214 14:35:42 @agent_ppo2.py:185][0m |          -0.0026 |         219.6233 |        -177.4300 |
[32m[20221214 14:35:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:35:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.35
[32m[20221214 14:35:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.07
[32m[20221214 14:35:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.79
[32m[20221214 14:35:42 @agent_ppo2.py:143][0m Total time:      37.67 min
[32m[20221214 14:35:42 @agent_ppo2.py:145][0m 3465216 total steps have happened
[32m[20221214 14:35:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1692 --------------------------#
[32m[20221214 14:35:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:35:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:43 @agent_ppo2.py:185][0m |           0.0013 |         220.6850 |        -176.3293 |
[32m[20221214 14:35:43 @agent_ppo2.py:185][0m |           0.0084 |         229.2066 |        -176.4336 |
[32m[20221214 14:35:43 @agent_ppo2.py:185][0m |          -0.0018 |         214.4307 |        -175.8857 |
[32m[20221214 14:35:43 @agent_ppo2.py:185][0m |           0.0013 |         214.2886 |        -176.3231 |
[32m[20221214 14:35:43 @agent_ppo2.py:185][0m |          -0.0042 |         212.2552 |        -176.0380 |
[32m[20221214 14:35:43 @agent_ppo2.py:185][0m |          -0.0014 |         212.6806 |        -175.9573 |
[32m[20221214 14:35:43 @agent_ppo2.py:185][0m |          -0.0037 |         211.7145 |        -176.6234 |
[32m[20221214 14:35:43 @agent_ppo2.py:185][0m |          -0.0033 |         210.2427 |        -176.2827 |
[32m[20221214 14:35:44 @agent_ppo2.py:185][0m |           0.0003 |         210.9854 |        -176.4654 |
[32m[20221214 14:35:44 @agent_ppo2.py:185][0m |          -0.0026 |         210.0996 |        -176.1863 |
[32m[20221214 14:35:44 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:35:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 845.66
[32m[20221214 14:35:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.69
[32m[20221214 14:35:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.99
[32m[20221214 14:35:44 @agent_ppo2.py:143][0m Total time:      37.70 min
[32m[20221214 14:35:44 @agent_ppo2.py:145][0m 3467264 total steps have happened
[32m[20221214 14:35:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1693 --------------------------#
[32m[20221214 14:35:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:44 @agent_ppo2.py:185][0m |           0.0133 |         264.7331 |        -174.7937 |
[32m[20221214 14:35:44 @agent_ppo2.py:185][0m |           0.0009 |         229.6955 |        -174.4310 |
[32m[20221214 14:35:44 @agent_ppo2.py:185][0m |          -0.0015 |         227.6396 |        -173.9276 |
[32m[20221214 14:35:44 @agent_ppo2.py:185][0m |           0.0047 |         228.2097 |        -174.5798 |
[32m[20221214 14:35:45 @agent_ppo2.py:185][0m |          -0.0022 |         225.8552 |        -173.9760 |
[32m[20221214 14:35:45 @agent_ppo2.py:185][0m |          -0.0028 |         224.5550 |        -174.3080 |
[32m[20221214 14:35:45 @agent_ppo2.py:185][0m |          -0.0009 |         224.3668 |        -174.2174 |
[32m[20221214 14:35:45 @agent_ppo2.py:185][0m |           0.0066 |         234.7169 |        -174.0936 |
[32m[20221214 14:35:45 @agent_ppo2.py:185][0m |          -0.0009 |         223.5537 |        -174.2358 |
[32m[20221214 14:35:45 @agent_ppo2.py:185][0m |          -0.0006 |         222.8004 |        -173.8305 |
[32m[20221214 14:35:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:35:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 846.23
[32m[20221214 14:35:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.18
[32m[20221214 14:35:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.04
[32m[20221214 14:35:45 @agent_ppo2.py:143][0m Total time:      37.72 min
[32m[20221214 14:35:45 @agent_ppo2.py:145][0m 3469312 total steps have happened
[32m[20221214 14:35:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1694 --------------------------#
[32m[20221214 14:35:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:46 @agent_ppo2.py:185][0m |           0.0127 |         246.4222 |        -178.1055 |
[32m[20221214 14:35:46 @agent_ppo2.py:185][0m |          -0.0010 |         217.3165 |        -178.4255 |
[32m[20221214 14:35:46 @agent_ppo2.py:185][0m |          -0.0016 |         216.1173 |        -178.1469 |
[32m[20221214 14:35:46 @agent_ppo2.py:185][0m |          -0.0021 |         215.4285 |        -178.4120 |
[32m[20221214 14:35:46 @agent_ppo2.py:185][0m |          -0.0044 |         215.3058 |        -177.7724 |
[32m[20221214 14:35:46 @agent_ppo2.py:185][0m |           0.0059 |         229.8145 |        -178.5461 |
[32m[20221214 14:35:46 @agent_ppo2.py:185][0m |          -0.0012 |         213.7695 |        -177.5749 |
[32m[20221214 14:35:46 @agent_ppo2.py:185][0m |          -0.0041 |         214.1337 |        -178.2976 |
[32m[20221214 14:35:46 @agent_ppo2.py:185][0m |           0.0044 |         218.5335 |        -178.8322 |
[32m[20221214 14:35:46 @agent_ppo2.py:185][0m |          -0.0024 |         213.5240 |        -178.7875 |
[32m[20221214 14:35:46 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:35:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 845.81
[32m[20221214 14:35:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.14
[32m[20221214 14:35:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.78
[32m[20221214 14:35:47 @agent_ppo2.py:143][0m Total time:      37.74 min
[32m[20221214 14:35:47 @agent_ppo2.py:145][0m 3471360 total steps have happened
[32m[20221214 14:35:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1695 --------------------------#
[32m[20221214 14:35:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:47 @agent_ppo2.py:185][0m |           0.0037 |         238.7420 |        -180.5686 |
[32m[20221214 14:35:47 @agent_ppo2.py:185][0m |          -0.0028 |         229.0364 |        -179.9941 |
[32m[20221214 14:35:47 @agent_ppo2.py:185][0m |          -0.0016 |         226.6545 |        -180.2261 |
[32m[20221214 14:35:47 @agent_ppo2.py:185][0m |          -0.0018 |         224.6135 |        -180.4677 |
[32m[20221214 14:35:47 @agent_ppo2.py:185][0m |          -0.0007 |         223.4847 |        -179.7514 |
[32m[20221214 14:35:47 @agent_ppo2.py:185][0m |          -0.0019 |         222.2392 |        -181.2540 |
[32m[20221214 14:35:47 @agent_ppo2.py:185][0m |          -0.0005 |         221.4501 |        -180.8040 |
[32m[20221214 14:35:48 @agent_ppo2.py:185][0m |          -0.0007 |         220.6829 |        -181.3515 |
[32m[20221214 14:35:48 @agent_ppo2.py:185][0m |           0.0098 |         238.2037 |        -180.7055 |
[32m[20221214 14:35:48 @agent_ppo2.py:185][0m |          -0.0014 |         220.6422 |        -180.4152 |
[32m[20221214 14:35:48 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:35:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 845.49
[32m[20221214 14:35:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.43
[32m[20221214 14:35:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.30
[32m[20221214 14:35:48 @agent_ppo2.py:143][0m Total time:      37.76 min
[32m[20221214 14:35:48 @agent_ppo2.py:145][0m 3473408 total steps have happened
[32m[20221214 14:35:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1696 --------------------------#
[32m[20221214 14:35:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:48 @agent_ppo2.py:185][0m |          -0.0016 |         225.0639 |        -183.0436 |
[32m[20221214 14:35:48 @agent_ppo2.py:185][0m |          -0.0022 |         220.3038 |        -182.6983 |
[32m[20221214 14:35:48 @agent_ppo2.py:185][0m |           0.0145 |         241.3102 |        -182.7113 |
[32m[20221214 14:35:49 @agent_ppo2.py:185][0m |           0.0014 |         218.5605 |        -182.8612 |
[32m[20221214 14:35:49 @agent_ppo2.py:185][0m |          -0.0002 |         217.5469 |        -182.0089 |
[32m[20221214 14:35:49 @agent_ppo2.py:185][0m |           0.0027 |         217.2408 |        -183.1667 |
[32m[20221214 14:35:49 @agent_ppo2.py:185][0m |           0.0045 |         222.6892 |        -182.1612 |
[32m[20221214 14:35:49 @agent_ppo2.py:185][0m |          -0.0023 |         216.2620 |        -182.6138 |
[32m[20221214 14:35:49 @agent_ppo2.py:185][0m |          -0.0002 |         215.7948 |        -182.3239 |
[32m[20221214 14:35:49 @agent_ppo2.py:185][0m |           0.0004 |         215.4466 |        -182.8952 |
[32m[20221214 14:35:49 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:35:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 844.50
[32m[20221214 14:35:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.87
[32m[20221214 14:35:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.45
[32m[20221214 14:35:49 @agent_ppo2.py:143][0m Total time:      37.79 min
[32m[20221214 14:35:49 @agent_ppo2.py:145][0m 3475456 total steps have happened
[32m[20221214 14:35:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1697 --------------------------#
[32m[20221214 14:35:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:50 @agent_ppo2.py:185][0m |          -0.0002 |         212.5035 |        -178.9208 |
[32m[20221214 14:35:50 @agent_ppo2.py:185][0m |          -0.0022 |         208.6569 |        -179.5395 |
[32m[20221214 14:35:50 @agent_ppo2.py:185][0m |           0.0038 |         211.1878 |        -178.9633 |
[32m[20221214 14:35:50 @agent_ppo2.py:185][0m |           0.0005 |         207.4927 |        -179.1574 |
[32m[20221214 14:35:50 @agent_ppo2.py:185][0m |          -0.0020 |         206.7023 |        -179.9531 |
[32m[20221214 14:35:50 @agent_ppo2.py:185][0m |          -0.0015 |         205.7189 |        -179.4621 |
[32m[20221214 14:35:50 @agent_ppo2.py:185][0m |          -0.0006 |         205.3979 |        -179.4885 |
[32m[20221214 14:35:50 @agent_ppo2.py:185][0m |          -0.0007 |         205.0591 |        -179.3382 |
[32m[20221214 14:35:50 @agent_ppo2.py:185][0m |          -0.0018 |         204.7470 |        -179.7736 |
[32m[20221214 14:35:50 @agent_ppo2.py:185][0m |          -0.0022 |         205.0857 |        -179.5632 |
[32m[20221214 14:35:50 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:35:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 845.71
[32m[20221214 14:35:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.70
[32m[20221214 14:35:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.59
[32m[20221214 14:35:51 @agent_ppo2.py:143][0m Total time:      37.81 min
[32m[20221214 14:35:51 @agent_ppo2.py:145][0m 3477504 total steps have happened
[32m[20221214 14:35:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1698 --------------------------#
[32m[20221214 14:35:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:51 @agent_ppo2.py:185][0m |          -0.0032 |         230.3435 |        -180.4559 |
[32m[20221214 14:35:51 @agent_ppo2.py:185][0m |          -0.0020 |         216.7127 |        -180.3683 |
[32m[20221214 14:35:51 @agent_ppo2.py:185][0m |           0.0113 |         243.3152 |        -179.8706 |
[32m[20221214 14:35:51 @agent_ppo2.py:185][0m |          -0.0044 |         210.0375 |        -179.9531 |
[32m[20221214 14:35:51 @agent_ppo2.py:185][0m |          -0.0018 |         208.5424 |        -179.8162 |
[32m[20221214 14:35:51 @agent_ppo2.py:185][0m |           0.0021 |         208.9240 |        -179.9582 |
[32m[20221214 14:35:52 @agent_ppo2.py:185][0m |          -0.0032 |         206.8715 |        -180.0437 |
[32m[20221214 14:35:52 @agent_ppo2.py:185][0m |          -0.0008 |         206.6879 |        -179.5463 |
[32m[20221214 14:35:52 @agent_ppo2.py:185][0m |          -0.0036 |         206.5493 |        -179.7529 |
[32m[20221214 14:35:52 @agent_ppo2.py:185][0m |          -0.0021 |         207.0028 |        -179.4316 |
[32m[20221214 14:35:52 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:35:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.39
[32m[20221214 14:35:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.40
[32m[20221214 14:35:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.85
[32m[20221214 14:35:52 @agent_ppo2.py:143][0m Total time:      37.83 min
[32m[20221214 14:35:52 @agent_ppo2.py:145][0m 3479552 total steps have happened
[32m[20221214 14:35:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1699 --------------------------#
[32m[20221214 14:35:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:52 @agent_ppo2.py:185][0m |           0.0006 |         217.3153 |        -181.6923 |
[32m[20221214 14:35:52 @agent_ppo2.py:185][0m |          -0.0013 |         210.7806 |        -182.0654 |
[32m[20221214 14:35:52 @agent_ppo2.py:185][0m |          -0.0014 |         208.0481 |        -182.2645 |
[32m[20221214 14:35:53 @agent_ppo2.py:185][0m |           0.0132 |         231.9206 |        -182.5313 |
[32m[20221214 14:35:53 @agent_ppo2.py:185][0m |          -0.0002 |         207.1080 |        -182.4518 |
[32m[20221214 14:35:53 @agent_ppo2.py:185][0m |          -0.0036 |         206.5560 |        -182.5198 |
[32m[20221214 14:35:53 @agent_ppo2.py:185][0m |           0.0012 |         205.9023 |        -182.2718 |
[32m[20221214 14:35:53 @agent_ppo2.py:185][0m |          -0.0014 |         205.3217 |        -182.4436 |
[32m[20221214 14:35:53 @agent_ppo2.py:185][0m |           0.0090 |         210.1749 |        -182.7526 |
[32m[20221214 14:35:53 @agent_ppo2.py:185][0m |          -0.0017 |         204.8875 |        -182.7011 |
[32m[20221214 14:35:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:35:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.27
[32m[20221214 14:35:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.41
[32m[20221214 14:35:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.06
[32m[20221214 14:35:53 @agent_ppo2.py:143][0m Total time:      37.85 min
[32m[20221214 14:35:53 @agent_ppo2.py:145][0m 3481600 total steps have happened
[32m[20221214 14:35:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1700 --------------------------#
[32m[20221214 14:35:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:35:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:54 @agent_ppo2.py:185][0m |           0.0000 |         226.2564 |        -183.2869 |
[32m[20221214 14:35:54 @agent_ppo2.py:185][0m |          -0.0018 |         217.7893 |        -183.5592 |
[32m[20221214 14:35:54 @agent_ppo2.py:185][0m |           0.0102 |         230.9581 |        -183.7227 |
[32m[20221214 14:35:54 @agent_ppo2.py:185][0m |           0.0002 |         214.3607 |        -183.8062 |
[32m[20221214 14:35:54 @agent_ppo2.py:185][0m |          -0.0019 |         213.0558 |        -184.0678 |
[32m[20221214 14:35:54 @agent_ppo2.py:185][0m |          -0.0017 |         211.4918 |        -184.0248 |
[32m[20221214 14:35:54 @agent_ppo2.py:185][0m |          -0.0010 |         210.1110 |        -183.7915 |
[32m[20221214 14:35:54 @agent_ppo2.py:185][0m |          -0.0021 |         209.7745 |        -184.3585 |
[32m[20221214 14:35:54 @agent_ppo2.py:185][0m |           0.0025 |         208.7965 |        -183.4410 |
[32m[20221214 14:35:54 @agent_ppo2.py:185][0m |          -0.0012 |         207.5686 |        -184.6252 |
[32m[20221214 14:35:54 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:35:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.54
[32m[20221214 14:35:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.65
[32m[20221214 14:35:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.33
[32m[20221214 14:35:55 @agent_ppo2.py:143][0m Total time:      37.87 min
[32m[20221214 14:35:55 @agent_ppo2.py:145][0m 3483648 total steps have happened
[32m[20221214 14:35:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1701 --------------------------#
[32m[20221214 14:35:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:35:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:55 @agent_ppo2.py:185][0m |          -0.0010 |         202.1776 |        -186.9604 |
[32m[20221214 14:35:55 @agent_ppo2.py:185][0m |           0.0012 |         195.7288 |        -187.1137 |
[32m[20221214 14:35:55 @agent_ppo2.py:185][0m |           0.0112 |         209.6382 |        -186.4775 |
[32m[20221214 14:35:55 @agent_ppo2.py:185][0m |           0.0072 |         199.5677 |        -187.2019 |
[32m[20221214 14:35:55 @agent_ppo2.py:185][0m |          -0.0002 |         190.6944 |        -187.0621 |
[32m[20221214 14:35:55 @agent_ppo2.py:185][0m |          -0.0002 |         189.8307 |        -186.5205 |
[32m[20221214 14:35:56 @agent_ppo2.py:185][0m |          -0.0018 |         188.7106 |        -186.5277 |
[32m[20221214 14:35:56 @agent_ppo2.py:185][0m |          -0.0021 |         187.5759 |        -186.3375 |
[32m[20221214 14:35:56 @agent_ppo2.py:185][0m |          -0.0002 |         188.9595 |        -186.8609 |
[32m[20221214 14:35:56 @agent_ppo2.py:185][0m |          -0.0043 |         187.4295 |        -186.5360 |
[32m[20221214 14:35:56 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 14:35:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.63
[32m[20221214 14:35:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.32
[32m[20221214 14:35:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.42
[32m[20221214 14:35:56 @agent_ppo2.py:143][0m Total time:      37.90 min
[32m[20221214 14:35:56 @agent_ppo2.py:145][0m 3485696 total steps have happened
[32m[20221214 14:35:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1702 --------------------------#
[32m[20221214 14:35:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:56 @agent_ppo2.py:185][0m |          -0.0007 |         222.4922 |        -185.0933 |
[32m[20221214 14:35:56 @agent_ppo2.py:185][0m |          -0.0035 |         220.4312 |        -184.8705 |
[32m[20221214 14:35:57 @agent_ppo2.py:185][0m |          -0.0026 |         219.7610 |        -185.0226 |
[32m[20221214 14:35:57 @agent_ppo2.py:185][0m |          -0.0009 |         217.6534 |        -184.8562 |
[32m[20221214 14:35:57 @agent_ppo2.py:185][0m |           0.0005 |         216.8613 |        -184.9974 |
[32m[20221214 14:35:57 @agent_ppo2.py:185][0m |           0.0075 |         227.4463 |        -184.7547 |
[32m[20221214 14:35:57 @agent_ppo2.py:185][0m |          -0.0011 |         215.8382 |        -184.4261 |
[32m[20221214 14:35:57 @agent_ppo2.py:185][0m |           0.0000 |         215.8551 |        -184.6454 |
[32m[20221214 14:35:57 @agent_ppo2.py:185][0m |          -0.0014 |         214.8930 |        -185.0040 |
[32m[20221214 14:35:57 @agent_ppo2.py:185][0m |          -0.0023 |         214.5520 |        -184.9359 |
[32m[20221214 14:35:57 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:35:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.75
[32m[20221214 14:35:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.38
[32m[20221214 14:35:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.40
[32m[20221214 14:35:57 @agent_ppo2.py:143][0m Total time:      37.92 min
[32m[20221214 14:35:57 @agent_ppo2.py:145][0m 3487744 total steps have happened
[32m[20221214 14:35:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1703 --------------------------#
[32m[20221214 14:35:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:58 @agent_ppo2.py:185][0m |           0.0021 |         233.4371 |        -184.1956 |
[32m[20221214 14:35:58 @agent_ppo2.py:185][0m |          -0.0009 |         220.8313 |        -184.0740 |
[32m[20221214 14:35:58 @agent_ppo2.py:185][0m |          -0.0000 |         216.7935 |        -184.1695 |
[32m[20221214 14:35:58 @agent_ppo2.py:185][0m |           0.0027 |         215.8030 |        -184.0324 |
[32m[20221214 14:35:58 @agent_ppo2.py:185][0m |          -0.0027 |         212.0902 |        -183.9639 |
[32m[20221214 14:35:58 @agent_ppo2.py:185][0m |          -0.0002 |         211.8700 |        -184.2198 |
[32m[20221214 14:35:58 @agent_ppo2.py:185][0m |          -0.0017 |         210.6507 |        -184.0353 |
[32m[20221214 14:35:58 @agent_ppo2.py:185][0m |          -0.0033 |         209.7543 |        -184.3610 |
[32m[20221214 14:35:58 @agent_ppo2.py:185][0m |          -0.0007 |         210.8225 |        -184.1999 |
[32m[20221214 14:35:59 @agent_ppo2.py:185][0m |           0.0094 |         220.4945 |        -184.1892 |
[32m[20221214 14:35:59 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:35:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.11
[32m[20221214 14:35:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.85
[32m[20221214 14:35:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.07
[32m[20221214 14:35:59 @agent_ppo2.py:143][0m Total time:      37.94 min
[32m[20221214 14:35:59 @agent_ppo2.py:145][0m 3489792 total steps have happened
[32m[20221214 14:35:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1704 --------------------------#
[32m[20221214 14:35:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:35:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:35:59 @agent_ppo2.py:185][0m |          -0.0011 |         235.7593 |        -184.1815 |
[32m[20221214 14:35:59 @agent_ppo2.py:185][0m |           0.0002 |         231.1999 |        -184.1216 |
[32m[20221214 14:35:59 @agent_ppo2.py:185][0m |          -0.0039 |         228.2269 |        -183.8712 |
[32m[20221214 14:35:59 @agent_ppo2.py:185][0m |          -0.0008 |         226.9906 |        -184.1881 |
[32m[20221214 14:35:59 @agent_ppo2.py:185][0m |          -0.0049 |         226.8893 |        -184.2230 |
[32m[20221214 14:36:00 @agent_ppo2.py:185][0m |          -0.0022 |         226.2407 |        -184.1551 |
[32m[20221214 14:36:00 @agent_ppo2.py:185][0m |          -0.0038 |         224.9175 |        -183.9251 |
[32m[20221214 14:36:00 @agent_ppo2.py:185][0m |          -0.0044 |         225.1491 |        -184.1961 |
[32m[20221214 14:36:00 @agent_ppo2.py:185][0m |          -0.0035 |         224.1410 |        -184.4436 |
[32m[20221214 14:36:00 @agent_ppo2.py:185][0m |          -0.0058 |         223.8689 |        -184.2772 |
[32m[20221214 14:36:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:36:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.01
[32m[20221214 14:36:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.63
[32m[20221214 14:36:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.64
[32m[20221214 14:36:00 @agent_ppo2.py:143][0m Total time:      37.97 min
[32m[20221214 14:36:00 @agent_ppo2.py:145][0m 3491840 total steps have happened
[32m[20221214 14:36:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1705 --------------------------#
[32m[20221214 14:36:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:00 @agent_ppo2.py:185][0m |           0.0027 |         244.4583 |        -184.9011 |
[32m[20221214 14:36:00 @agent_ppo2.py:185][0m |          -0.0025 |         238.9907 |        -184.7444 |
[32m[20221214 14:36:01 @agent_ppo2.py:185][0m |          -0.0036 |         238.5867 |        -184.8087 |
[32m[20221214 14:36:01 @agent_ppo2.py:185][0m |          -0.0033 |         237.5329 |        -184.9191 |
[32m[20221214 14:36:01 @agent_ppo2.py:185][0m |          -0.0014 |         235.8282 |        -184.6611 |
[32m[20221214 14:36:01 @agent_ppo2.py:185][0m |          -0.0028 |         236.7953 |        -184.8283 |
[32m[20221214 14:36:01 @agent_ppo2.py:185][0m |          -0.0047 |         236.1570 |        -184.7762 |
[32m[20221214 14:36:01 @agent_ppo2.py:185][0m |          -0.0040 |         236.0124 |        -184.7863 |
[32m[20221214 14:36:01 @agent_ppo2.py:185][0m |          -0.0022 |         234.9837 |        -184.9745 |
[32m[20221214 14:36:01 @agent_ppo2.py:185][0m |          -0.0016 |         234.7271 |        -184.4914 |
[32m[20221214 14:36:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:36:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.04
[32m[20221214 14:36:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.45
[32m[20221214 14:36:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.30
[32m[20221214 14:36:01 @agent_ppo2.py:143][0m Total time:      37.99 min
[32m[20221214 14:36:01 @agent_ppo2.py:145][0m 3493888 total steps have happened
[32m[20221214 14:36:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1706 --------------------------#
[32m[20221214 14:36:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:02 @agent_ppo2.py:185][0m |          -0.0005 |         247.9721 |        -185.3878 |
[32m[20221214 14:36:02 @agent_ppo2.py:185][0m |          -0.0026 |         242.3651 |        -185.1890 |
[32m[20221214 14:36:02 @agent_ppo2.py:185][0m |           0.0179 |         276.5746 |        -185.2154 |
[32m[20221214 14:36:02 @agent_ppo2.py:185][0m |           0.0043 |         239.6152 |        -185.3792 |
[32m[20221214 14:36:02 @agent_ppo2.py:185][0m |          -0.0027 |         235.8232 |        -185.7375 |
[32m[20221214 14:36:02 @agent_ppo2.py:185][0m |          -0.0022 |         233.7744 |        -185.9607 |
[32m[20221214 14:36:02 @agent_ppo2.py:185][0m |          -0.0015 |         232.7579 |        -185.9174 |
[32m[20221214 14:36:02 @agent_ppo2.py:185][0m |           0.0058 |         241.3287 |        -186.3054 |
[32m[20221214 14:36:02 @agent_ppo2.py:185][0m |          -0.0031 |         232.2429 |        -186.3088 |
[32m[20221214 14:36:02 @agent_ppo2.py:185][0m |          -0.0021 |         230.3261 |        -186.1668 |
[32m[20221214 14:36:02 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:36:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.68
[32m[20221214 14:36:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.74
[32m[20221214 14:36:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.02
[32m[20221214 14:36:03 @agent_ppo2.py:143][0m Total time:      38.01 min
[32m[20221214 14:36:03 @agent_ppo2.py:145][0m 3495936 total steps have happened
[32m[20221214 14:36:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1707 --------------------------#
[32m[20221214 14:36:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:03 @agent_ppo2.py:185][0m |           0.0040 |         263.2211 |        -186.0226 |
[32m[20221214 14:36:03 @agent_ppo2.py:185][0m |           0.0035 |         251.4432 |        -185.7148 |
[32m[20221214 14:36:03 @agent_ppo2.py:185][0m |           0.0002 |         247.0086 |        -186.0997 |
[32m[20221214 14:36:03 @agent_ppo2.py:185][0m |          -0.0017 |         243.5513 |        -186.2253 |
[32m[20221214 14:36:03 @agent_ppo2.py:185][0m |          -0.0010 |         241.7390 |        -186.5338 |
[32m[20221214 14:36:03 @agent_ppo2.py:185][0m |          -0.0008 |         239.5994 |        -186.2788 |
[32m[20221214 14:36:03 @agent_ppo2.py:185][0m |          -0.0014 |         239.5012 |        -186.7003 |
[32m[20221214 14:36:03 @agent_ppo2.py:185][0m |           0.0006 |         237.0775 |        -186.5907 |
[32m[20221214 14:36:04 @agent_ppo2.py:185][0m |           0.0072 |         243.7224 |        -187.1968 |
[32m[20221214 14:36:04 @agent_ppo2.py:185][0m |           0.0120 |         257.1597 |        -187.1795 |
[32m[20221214 14:36:04 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:36:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.57
[32m[20221214 14:36:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.71
[32m[20221214 14:36:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.36
[32m[20221214 14:36:04 @agent_ppo2.py:143][0m Total time:      38.03 min
[32m[20221214 14:36:04 @agent_ppo2.py:145][0m 3497984 total steps have happened
[32m[20221214 14:36:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1708 --------------------------#
[32m[20221214 14:36:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:04 @agent_ppo2.py:185][0m |           0.0036 |         282.7244 |        -190.0532 |
[32m[20221214 14:36:04 @agent_ppo2.py:185][0m |           0.0028 |         272.3125 |        -189.9725 |
[32m[20221214 14:36:04 @agent_ppo2.py:185][0m |          -0.0031 |         263.2408 |        -189.9236 |
[32m[20221214 14:36:04 @agent_ppo2.py:185][0m |          -0.0044 |         260.9892 |        -189.9208 |
[32m[20221214 14:36:04 @agent_ppo2.py:185][0m |          -0.0020 |         260.9833 |        -189.6525 |
[32m[20221214 14:36:05 @agent_ppo2.py:185][0m |          -0.0054 |         257.9232 |        -189.9444 |
[32m[20221214 14:36:05 @agent_ppo2.py:185][0m |          -0.0032 |         257.2095 |        -189.5882 |
[32m[20221214 14:36:05 @agent_ppo2.py:185][0m |          -0.0034 |         256.6821 |        -189.7713 |
[32m[20221214 14:36:05 @agent_ppo2.py:185][0m |          -0.0048 |         255.5484 |        -189.7714 |
[32m[20221214 14:36:05 @agent_ppo2.py:185][0m |          -0.0049 |         254.7758 |        -189.6540 |
[32m[20221214 14:36:05 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:36:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.61
[32m[20221214 14:36:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.22
[32m[20221214 14:36:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.65
[32m[20221214 14:36:05 @agent_ppo2.py:143][0m Total time:      38.05 min
[32m[20221214 14:36:05 @agent_ppo2.py:145][0m 3500032 total steps have happened
[32m[20221214 14:36:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1709 --------------------------#
[32m[20221214 14:36:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:05 @agent_ppo2.py:185][0m |          -0.0045 |         264.0060 |        -187.1064 |
[32m[20221214 14:36:05 @agent_ppo2.py:185][0m |          -0.0058 |         252.4716 |        -187.1169 |
[32m[20221214 14:36:06 @agent_ppo2.py:185][0m |           0.0014 |         250.8337 |        -186.7499 |
[32m[20221214 14:36:06 @agent_ppo2.py:185][0m |           0.0079 |         253.5496 |        -186.8043 |
[32m[20221214 14:36:06 @agent_ppo2.py:185][0m |          -0.0042 |         242.0996 |        -186.4638 |
[32m[20221214 14:36:06 @agent_ppo2.py:185][0m |          -0.0025 |         240.3001 |        -186.4624 |
[32m[20221214 14:36:06 @agent_ppo2.py:185][0m |          -0.0058 |         238.7564 |        -186.5386 |
[32m[20221214 14:36:06 @agent_ppo2.py:185][0m |          -0.0015 |         236.9540 |        -185.7779 |
[32m[20221214 14:36:06 @agent_ppo2.py:185][0m |          -0.0016 |         234.3658 |        -186.7310 |
[32m[20221214 14:36:06 @agent_ppo2.py:185][0m |          -0.0034 |         236.2739 |        -186.5164 |
[32m[20221214 14:36:06 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:36:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.65
[32m[20221214 14:36:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.57
[32m[20221214 14:36:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.52
[32m[20221214 14:36:06 @agent_ppo2.py:143][0m Total time:      38.07 min
[32m[20221214 14:36:06 @agent_ppo2.py:145][0m 3502080 total steps have happened
[32m[20221214 14:36:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1710 --------------------------#
[32m[20221214 14:36:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:36:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:07 @agent_ppo2.py:185][0m |           0.0003 |         265.3443 |        -186.4832 |
[32m[20221214 14:36:07 @agent_ppo2.py:185][0m |          -0.0001 |         255.9050 |        -185.7760 |
[32m[20221214 14:36:07 @agent_ppo2.py:185][0m |          -0.0007 |         252.4381 |        -186.8394 |
[32m[20221214 14:36:07 @agent_ppo2.py:185][0m |          -0.0010 |         250.7807 |        -186.6440 |
[32m[20221214 14:36:07 @agent_ppo2.py:185][0m |          -0.0020 |         247.5701 |        -187.1059 |
[32m[20221214 14:36:07 @agent_ppo2.py:185][0m |          -0.0019 |         246.6395 |        -186.8758 |
[32m[20221214 14:36:07 @agent_ppo2.py:185][0m |           0.0088 |         259.6833 |        -187.0762 |
[32m[20221214 14:36:07 @agent_ppo2.py:185][0m |          -0.0016 |         244.5417 |        -187.6899 |
[32m[20221214 14:36:08 @agent_ppo2.py:185][0m |          -0.0002 |         241.6832 |        -186.8959 |
[32m[20221214 14:36:08 @agent_ppo2.py:185][0m |          -0.0021 |         241.8272 |        -187.6810 |
[32m[20221214 14:36:08 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:36:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.29
[32m[20221214 14:36:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.13
[32m[20221214 14:36:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.31
[32m[20221214 14:36:08 @agent_ppo2.py:143][0m Total time:      38.09 min
[32m[20221214 14:36:08 @agent_ppo2.py:145][0m 3504128 total steps have happened
[32m[20221214 14:36:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1711 --------------------------#
[32m[20221214 14:36:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:08 @agent_ppo2.py:185][0m |          -0.0050 |         272.0478 |        -185.5787 |
[32m[20221214 14:36:08 @agent_ppo2.py:185][0m |          -0.0057 |         261.4915 |        -185.3455 |
[32m[20221214 14:36:08 @agent_ppo2.py:185][0m |          -0.0052 |         258.6154 |        -185.6641 |
[32m[20221214 14:36:08 @agent_ppo2.py:185][0m |          -0.0015 |         259.5394 |        -185.5511 |
[32m[20221214 14:36:08 @agent_ppo2.py:185][0m |          -0.0062 |         255.8981 |        -185.6329 |
[32m[20221214 14:36:09 @agent_ppo2.py:185][0m |          -0.0063 |         254.0790 |        -185.3455 |
[32m[20221214 14:36:09 @agent_ppo2.py:185][0m |          -0.0037 |         254.6946 |        -185.5877 |
[32m[20221214 14:36:09 @agent_ppo2.py:185][0m |          -0.0068 |         252.0750 |        -185.6688 |
[32m[20221214 14:36:09 @agent_ppo2.py:185][0m |          -0.0063 |         252.9322 |        -185.2011 |
[32m[20221214 14:36:09 @agent_ppo2.py:185][0m |          -0.0002 |         253.3955 |        -185.6511 |
[32m[20221214 14:36:09 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:36:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.46
[32m[20221214 14:36:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.37
[32m[20221214 14:36:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.53
[32m[20221214 14:36:09 @agent_ppo2.py:143][0m Total time:      38.12 min
[32m[20221214 14:36:09 @agent_ppo2.py:145][0m 3506176 total steps have happened
[32m[20221214 14:36:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1712 --------------------------#
[32m[20221214 14:36:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:09 @agent_ppo2.py:185][0m |          -0.0012 |         276.6258 |        -187.8483 |
[32m[20221214 14:36:10 @agent_ppo2.py:185][0m |           0.0012 |         263.7068 |        -188.3336 |
[32m[20221214 14:36:10 @agent_ppo2.py:185][0m |          -0.0014 |         261.2187 |        -187.4353 |
[32m[20221214 14:36:10 @agent_ppo2.py:185][0m |          -0.0015 |         259.6836 |        -188.0052 |
[32m[20221214 14:36:10 @agent_ppo2.py:185][0m |          -0.0005 |         258.5973 |        -188.3788 |
[32m[20221214 14:36:10 @agent_ppo2.py:185][0m |          -0.0023 |         257.1617 |        -187.6944 |
[32m[20221214 14:36:10 @agent_ppo2.py:185][0m |           0.0019 |         259.6698 |        -188.1415 |
[32m[20221214 14:36:10 @agent_ppo2.py:185][0m |          -0.0016 |         256.7963 |        -187.8680 |
[32m[20221214 14:36:10 @agent_ppo2.py:185][0m |          -0.0002 |         256.4178 |        -188.1052 |
[32m[20221214 14:36:10 @agent_ppo2.py:185][0m |          -0.0001 |         256.5774 |        -187.7055 |
[32m[20221214 14:36:10 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:36:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.53
[32m[20221214 14:36:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.59
[32m[20221214 14:36:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.00
[32m[20221214 14:36:10 @agent_ppo2.py:143][0m Total time:      38.14 min
[32m[20221214 14:36:10 @agent_ppo2.py:145][0m 3508224 total steps have happened
[32m[20221214 14:36:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1713 --------------------------#
[32m[20221214 14:36:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:11 @agent_ppo2.py:185][0m |          -0.0022 |         253.4060 |        -188.5255 |
[32m[20221214 14:36:11 @agent_ppo2.py:185][0m |          -0.0017 |         242.4339 |        -189.0599 |
[32m[20221214 14:36:11 @agent_ppo2.py:185][0m |           0.0001 |         241.4318 |        -188.4828 |
[32m[20221214 14:36:11 @agent_ppo2.py:185][0m |           0.0142 |         260.4893 |        -189.0226 |
[32m[20221214 14:36:11 @agent_ppo2.py:185][0m |          -0.0037 |         229.7198 |        -188.5717 |
[32m[20221214 14:36:11 @agent_ppo2.py:185][0m |          -0.0026 |         225.1863 |        -188.7555 |
[32m[20221214 14:36:11 @agent_ppo2.py:185][0m |          -0.0018 |         223.2518 |        -189.1366 |
[32m[20221214 14:36:11 @agent_ppo2.py:185][0m |          -0.0036 |         220.3127 |        -188.9482 |
[32m[20221214 14:36:11 @agent_ppo2.py:185][0m |          -0.0016 |         217.6924 |        -189.9313 |
[32m[20221214 14:36:12 @agent_ppo2.py:185][0m |           0.0180 |         248.8008 |        -189.0460 |
[32m[20221214 14:36:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:36:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.97
[32m[20221214 14:36:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.66
[32m[20221214 14:36:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.28
[32m[20221214 14:36:12 @agent_ppo2.py:143][0m Total time:      38.16 min
[32m[20221214 14:36:12 @agent_ppo2.py:145][0m 3510272 total steps have happened
[32m[20221214 14:36:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1714 --------------------------#
[32m[20221214 14:36:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:12 @agent_ppo2.py:185][0m |          -0.0022 |         304.5843 |        -188.0944 |
[32m[20221214 14:36:12 @agent_ppo2.py:185][0m |          -0.0013 |         286.1957 |        -187.8012 |
[32m[20221214 14:36:12 @agent_ppo2.py:185][0m |          -0.0011 |         274.2669 |        -188.2706 |
[32m[20221214 14:36:12 @agent_ppo2.py:185][0m |          -0.0029 |         270.0959 |        -188.5346 |
[32m[20221214 14:36:12 @agent_ppo2.py:185][0m |          -0.0013 |         267.8949 |        -187.3406 |
[32m[20221214 14:36:12 @agent_ppo2.py:185][0m |          -0.0035 |         266.9778 |        -187.7840 |
[32m[20221214 14:36:13 @agent_ppo2.py:185][0m |          -0.0022 |         266.0458 |        -187.7358 |
[32m[20221214 14:36:13 @agent_ppo2.py:185][0m |          -0.0044 |         264.9315 |        -187.3954 |
[32m[20221214 14:36:13 @agent_ppo2.py:185][0m |          -0.0035 |         264.5034 |        -187.6549 |
[32m[20221214 14:36:13 @agent_ppo2.py:185][0m |           0.0079 |         293.3896 |        -187.3829 |
[32m[20221214 14:36:13 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:36:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.50
[32m[20221214 14:36:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.79
[32m[20221214 14:36:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.51
[32m[20221214 14:36:13 @agent_ppo2.py:143][0m Total time:      38.18 min
[32m[20221214 14:36:13 @agent_ppo2.py:145][0m 3512320 total steps have happened
[32m[20221214 14:36:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1715 --------------------------#
[32m[20221214 14:36:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:13 @agent_ppo2.py:185][0m |           0.0089 |         279.1971 |        -188.1708 |
[32m[20221214 14:36:13 @agent_ppo2.py:185][0m |           0.0020 |         260.3276 |        -188.1501 |
[32m[20221214 14:36:13 @agent_ppo2.py:185][0m |          -0.0042 |         258.7367 |        -188.0393 |
[32m[20221214 14:36:14 @agent_ppo2.py:185][0m |           0.0200 |         278.8711 |        -187.6593 |
[32m[20221214 14:36:14 @agent_ppo2.py:185][0m |          -0.0018 |         256.0537 |        -188.0710 |
[32m[20221214 14:36:14 @agent_ppo2.py:185][0m |           0.0131 |         285.3820 |        -187.5572 |
[32m[20221214 14:36:14 @agent_ppo2.py:185][0m |          -0.0020 |         254.5180 |        -187.6699 |
[32m[20221214 14:36:14 @agent_ppo2.py:185][0m |          -0.0005 |         253.8074 |        -187.6944 |
[32m[20221214 14:36:14 @agent_ppo2.py:185][0m |          -0.0020 |         252.3922 |        -187.7518 |
[32m[20221214 14:36:14 @agent_ppo2.py:185][0m |          -0.0031 |         253.5413 |        -187.7634 |
[32m[20221214 14:36:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:36:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.25
[32m[20221214 14:36:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.99
[32m[20221214 14:36:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.65
[32m[20221214 14:36:14 @agent_ppo2.py:143][0m Total time:      38.20 min
[32m[20221214 14:36:14 @agent_ppo2.py:145][0m 3514368 total steps have happened
[32m[20221214 14:36:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1716 --------------------------#
[32m[20221214 14:36:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:15 @agent_ppo2.py:185][0m |           0.0087 |         260.4635 |        -186.7283 |
[32m[20221214 14:36:15 @agent_ppo2.py:185][0m |          -0.0028 |         243.0177 |        -186.8858 |
[32m[20221214 14:36:15 @agent_ppo2.py:185][0m |           0.0025 |         242.9750 |        -186.5877 |
[32m[20221214 14:36:15 @agent_ppo2.py:185][0m |          -0.0011 |         240.8124 |        -186.7133 |
[32m[20221214 14:36:15 @agent_ppo2.py:185][0m |          -0.0017 |         236.7665 |        -187.3310 |
[32m[20221214 14:36:15 @agent_ppo2.py:185][0m |           0.0068 |         247.3098 |        -187.5006 |
[32m[20221214 14:36:15 @agent_ppo2.py:185][0m |          -0.0038 |         235.1777 |        -187.3321 |
[32m[20221214 14:36:15 @agent_ppo2.py:185][0m |          -0.0036 |         233.4407 |        -187.5157 |
[32m[20221214 14:36:15 @agent_ppo2.py:185][0m |          -0.0028 |         232.9036 |        -187.3588 |
[32m[20221214 14:36:15 @agent_ppo2.py:185][0m |           0.0042 |         238.2276 |        -187.4761 |
[32m[20221214 14:36:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:36:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.53
[32m[20221214 14:36:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.56
[32m[20221214 14:36:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.50
[32m[20221214 14:36:16 @agent_ppo2.py:143][0m Total time:      38.22 min
[32m[20221214 14:36:16 @agent_ppo2.py:145][0m 3516416 total steps have happened
[32m[20221214 14:36:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1717 --------------------------#
[32m[20221214 14:36:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:16 @agent_ppo2.py:185][0m |          -0.0018 |         275.6204 |        -187.1820 |
[32m[20221214 14:36:16 @agent_ppo2.py:185][0m |          -0.0056 |         267.9814 |        -187.2398 |
[32m[20221214 14:36:16 @agent_ppo2.py:185][0m |          -0.0007 |         266.2208 |        -187.3320 |
[32m[20221214 14:36:16 @agent_ppo2.py:185][0m |          -0.0055 |         263.6435 |        -187.1444 |
[32m[20221214 14:36:16 @agent_ppo2.py:185][0m |          -0.0070 |         262.6288 |        -187.0962 |
[32m[20221214 14:36:16 @agent_ppo2.py:185][0m |          -0.0013 |         262.3286 |        -187.1657 |
[32m[20221214 14:36:16 @agent_ppo2.py:185][0m |          -0.0031 |         259.8283 |        -186.9142 |
[32m[20221214 14:36:17 @agent_ppo2.py:185][0m |           0.0027 |         269.6467 |        -187.0724 |
[32m[20221214 14:36:17 @agent_ppo2.py:185][0m |          -0.0048 |         257.8858 |        -187.2726 |
[32m[20221214 14:36:17 @agent_ppo2.py:185][0m |          -0.0029 |         257.2531 |        -186.7086 |
[32m[20221214 14:36:17 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:36:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.52
[32m[20221214 14:36:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.33
[32m[20221214 14:36:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.65
[32m[20221214 14:36:17 @agent_ppo2.py:143][0m Total time:      38.25 min
[32m[20221214 14:36:17 @agent_ppo2.py:145][0m 3518464 total steps have happened
[32m[20221214 14:36:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1718 --------------------------#
[32m[20221214 14:36:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:36:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:17 @agent_ppo2.py:185][0m |           0.0038 |         266.2045 |        -187.3524 |
[32m[20221214 14:36:17 @agent_ppo2.py:185][0m |          -0.0007 |         252.4360 |        -187.1042 |
[32m[20221214 14:36:17 @agent_ppo2.py:185][0m |           0.0093 |         267.4254 |        -187.3340 |
[32m[20221214 14:36:18 @agent_ppo2.py:185][0m |           0.0020 |         248.0777 |        -187.1091 |
[32m[20221214 14:36:18 @agent_ppo2.py:185][0m |          -0.0026 |         244.7226 |        -187.3897 |
[32m[20221214 14:36:18 @agent_ppo2.py:185][0m |          -0.0003 |         240.5482 |        -187.3216 |
[32m[20221214 14:36:18 @agent_ppo2.py:185][0m |          -0.0027 |         239.8561 |        -187.2486 |
[32m[20221214 14:36:18 @agent_ppo2.py:185][0m |          -0.0012 |         238.9870 |        -187.4100 |
[32m[20221214 14:36:18 @agent_ppo2.py:185][0m |          -0.0018 |         237.8746 |        -187.0557 |
[32m[20221214 14:36:18 @agent_ppo2.py:185][0m |          -0.0008 |         237.3229 |        -187.0134 |
[32m[20221214 14:36:18 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:36:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.77
[32m[20221214 14:36:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.41
[32m[20221214 14:36:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.07
[32m[20221214 14:36:18 @agent_ppo2.py:143][0m Total time:      38.27 min
[32m[20221214 14:36:18 @agent_ppo2.py:145][0m 3520512 total steps have happened
[32m[20221214 14:36:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1719 --------------------------#
[32m[20221214 14:36:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:19 @agent_ppo2.py:185][0m |          -0.0013 |         248.0327 |        -187.3780 |
[32m[20221214 14:36:19 @agent_ppo2.py:185][0m |           0.0022 |         245.6264 |        -187.5898 |
[32m[20221214 14:36:19 @agent_ppo2.py:185][0m |          -0.0028 |         244.4713 |        -187.7276 |
[32m[20221214 14:36:19 @agent_ppo2.py:185][0m |          -0.0029 |         243.0850 |        -187.7845 |
[32m[20221214 14:36:19 @agent_ppo2.py:185][0m |          -0.0022 |         243.1676 |        -188.2886 |
[32m[20221214 14:36:19 @agent_ppo2.py:185][0m |          -0.0023 |         242.5939 |        -188.2390 |
[32m[20221214 14:36:19 @agent_ppo2.py:185][0m |          -0.0027 |         241.9668 |        -188.4382 |
[32m[20221214 14:36:19 @agent_ppo2.py:185][0m |          -0.0018 |         242.3159 |        -188.9892 |
[32m[20221214 14:36:19 @agent_ppo2.py:185][0m |           0.0021 |         241.6624 |        -188.8399 |
[32m[20221214 14:36:19 @agent_ppo2.py:185][0m |          -0.0012 |         240.2146 |        -189.2162 |
[32m[20221214 14:36:19 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:36:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.60
[32m[20221214 14:36:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.63
[32m[20221214 14:36:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.04
[32m[20221214 14:36:20 @agent_ppo2.py:143][0m Total time:      38.29 min
[32m[20221214 14:36:20 @agent_ppo2.py:145][0m 3522560 total steps have happened
[32m[20221214 14:36:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1720 --------------------------#
[32m[20221214 14:36:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:36:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:20 @agent_ppo2.py:185][0m |          -0.0004 |         235.0802 |        -189.8626 |
[32m[20221214 14:36:20 @agent_ppo2.py:185][0m |           0.0003 |         221.0826 |        -190.2118 |
[32m[20221214 14:36:20 @agent_ppo2.py:185][0m |           0.0006 |         214.9637 |        -190.5321 |
[32m[20221214 14:36:20 @agent_ppo2.py:185][0m |           0.0001 |         211.8092 |        -190.5541 |
[32m[20221214 14:36:20 @agent_ppo2.py:185][0m |          -0.0003 |         209.6311 |        -190.5796 |
[32m[20221214 14:36:20 @agent_ppo2.py:185][0m |          -0.0009 |         210.2571 |        -190.9564 |
[32m[20221214 14:36:21 @agent_ppo2.py:185][0m |           0.0047 |         211.6677 |        -191.2166 |
[32m[20221214 14:36:21 @agent_ppo2.py:185][0m |          -0.0001 |         207.6825 |        -191.1025 |
[32m[20221214 14:36:21 @agent_ppo2.py:185][0m |           0.0006 |         208.6133 |        -190.5782 |
[32m[20221214 14:36:21 @agent_ppo2.py:185][0m |          -0.0004 |         206.7175 |        -191.3632 |
[32m[20221214 14:36:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:36:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.19
[32m[20221214 14:36:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.21
[32m[20221214 14:36:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.71
[32m[20221214 14:36:21 @agent_ppo2.py:143][0m Total time:      38.32 min
[32m[20221214 14:36:21 @agent_ppo2.py:145][0m 3524608 total steps have happened
[32m[20221214 14:36:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1721 --------------------------#
[32m[20221214 14:36:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:21 @agent_ppo2.py:185][0m |          -0.0029 |         219.3374 |        -193.0105 |
[32m[20221214 14:36:21 @agent_ppo2.py:185][0m |          -0.0020 |         207.1234 |        -193.8591 |
[32m[20221214 14:36:22 @agent_ppo2.py:185][0m |          -0.0005 |         205.2622 |        -193.4825 |
[32m[20221214 14:36:22 @agent_ppo2.py:185][0m |          -0.0024 |         203.9589 |        -193.5500 |
[32m[20221214 14:36:22 @agent_ppo2.py:185][0m |          -0.0038 |         203.1930 |        -194.0608 |
[32m[20221214 14:36:22 @agent_ppo2.py:185][0m |           0.0024 |         201.9432 |        -194.1945 |
[32m[20221214 14:36:22 @agent_ppo2.py:185][0m |          -0.0017 |         200.8124 |        -193.8036 |
[32m[20221214 14:36:22 @agent_ppo2.py:185][0m |          -0.0021 |         199.9578 |        -194.3246 |
[32m[20221214 14:36:22 @agent_ppo2.py:185][0m |          -0.0028 |         199.9197 |        -194.2611 |
[32m[20221214 14:36:22 @agent_ppo2.py:185][0m |          -0.0043 |         198.7083 |        -194.4302 |
[32m[20221214 14:36:22 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:36:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.84
[32m[20221214 14:36:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.93
[32m[20221214 14:36:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.93
[32m[20221214 14:36:22 @agent_ppo2.py:143][0m Total time:      38.34 min
[32m[20221214 14:36:22 @agent_ppo2.py:145][0m 3526656 total steps have happened
[32m[20221214 14:36:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1722 --------------------------#
[32m[20221214 14:36:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:23 @agent_ppo2.py:185][0m |           0.0048 |         258.3831 |        -193.6005 |
[32m[20221214 14:36:23 @agent_ppo2.py:185][0m |          -0.0000 |         245.6242 |        -193.6682 |
[32m[20221214 14:36:23 @agent_ppo2.py:185][0m |          -0.0051 |         242.4777 |        -193.8076 |
[32m[20221214 14:36:23 @agent_ppo2.py:185][0m |          -0.0062 |         240.4499 |        -193.8283 |
[32m[20221214 14:36:23 @agent_ppo2.py:185][0m |          -0.0021 |         238.6614 |        -193.7677 |
[32m[20221214 14:36:23 @agent_ppo2.py:185][0m |           0.0014 |         244.6868 |        -193.7458 |
[32m[20221214 14:36:23 @agent_ppo2.py:185][0m |          -0.0032 |         235.9574 |        -193.7500 |
[32m[20221214 14:36:23 @agent_ppo2.py:185][0m |          -0.0030 |         236.3223 |        -193.8420 |
[32m[20221214 14:36:23 @agent_ppo2.py:185][0m |          -0.0068 |         235.6470 |        -193.9730 |
[32m[20221214 14:36:23 @agent_ppo2.py:185][0m |          -0.0053 |         235.0131 |        -194.0319 |
[32m[20221214 14:36:23 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 14:36:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.11
[32m[20221214 14:36:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.94
[32m[20221214 14:36:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.86
[32m[20221214 14:36:24 @agent_ppo2.py:143][0m Total time:      38.36 min
[32m[20221214 14:36:24 @agent_ppo2.py:145][0m 3528704 total steps have happened
[32m[20221214 14:36:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1723 --------------------------#
[32m[20221214 14:36:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:24 @agent_ppo2.py:185][0m |           0.0093 |         239.1587 |        -197.6559 |
[32m[20221214 14:36:24 @agent_ppo2.py:185][0m |          -0.0025 |         226.6140 |        -197.8470 |
[32m[20221214 14:36:24 @agent_ppo2.py:185][0m |          -0.0008 |         224.0527 |        -197.7963 |
[32m[20221214 14:36:24 @agent_ppo2.py:185][0m |          -0.0021 |         221.4466 |        -197.5303 |
[32m[20221214 14:36:24 @agent_ppo2.py:185][0m |           0.0067 |         220.5726 |        -197.7736 |
[32m[20221214 14:36:24 @agent_ppo2.py:185][0m |          -0.0033 |         220.0977 |        -198.1028 |
[32m[20221214 14:36:24 @agent_ppo2.py:185][0m |          -0.0008 |         217.7720 |        -197.8448 |
[32m[20221214 14:36:25 @agent_ppo2.py:185][0m |          -0.0030 |         217.3509 |        -198.2298 |
[32m[20221214 14:36:25 @agent_ppo2.py:185][0m |          -0.0014 |         218.1095 |        -198.1621 |
[32m[20221214 14:36:25 @agent_ppo2.py:185][0m |          -0.0020 |         216.0204 |        -197.8665 |
[32m[20221214 14:36:25 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:36:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.18
[32m[20221214 14:36:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.19
[32m[20221214 14:36:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.18
[32m[20221214 14:36:25 @agent_ppo2.py:143][0m Total time:      38.38 min
[32m[20221214 14:36:25 @agent_ppo2.py:145][0m 3530752 total steps have happened
[32m[20221214 14:36:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1724 --------------------------#
[32m[20221214 14:36:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:25 @agent_ppo2.py:185][0m |          -0.0006 |         199.8014 |        -194.1213 |
[32m[20221214 14:36:25 @agent_ppo2.py:185][0m |          -0.0032 |         187.9415 |        -193.8764 |
[32m[20221214 14:36:25 @agent_ppo2.py:185][0m |           0.0002 |         185.6739 |        -193.6917 |
[32m[20221214 14:36:25 @agent_ppo2.py:185][0m |          -0.0016 |         185.9714 |        -193.2086 |
[32m[20221214 14:36:26 @agent_ppo2.py:185][0m |          -0.0025 |         183.6015 |        -193.1948 |
[32m[20221214 14:36:26 @agent_ppo2.py:185][0m |           0.0001 |         185.3767 |        -193.2404 |
[32m[20221214 14:36:26 @agent_ppo2.py:185][0m |          -0.0002 |         182.4732 |        -193.1471 |
[32m[20221214 14:36:26 @agent_ppo2.py:185][0m |           0.0061 |         193.1301 |        -192.9916 |
[32m[20221214 14:36:26 @agent_ppo2.py:185][0m |          -0.0018 |         182.2796 |        -192.6017 |
[32m[20221214 14:36:26 @agent_ppo2.py:185][0m |          -0.0005 |         180.3180 |        -192.7736 |
[32m[20221214 14:36:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:36:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.12
[32m[20221214 14:36:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.76
[32m[20221214 14:36:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.13
[32m[20221214 14:36:26 @agent_ppo2.py:143][0m Total time:      38.40 min
[32m[20221214 14:36:26 @agent_ppo2.py:145][0m 3532800 total steps have happened
[32m[20221214 14:36:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1725 --------------------------#
[32m[20221214 14:36:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:26 @agent_ppo2.py:185][0m |           0.0014 |         223.4301 |        -194.8285 |
[32m[20221214 14:36:27 @agent_ppo2.py:185][0m |           0.0002 |         208.0228 |        -194.9943 |
[32m[20221214 14:36:27 @agent_ppo2.py:185][0m |          -0.0006 |         203.8204 |        -194.5794 |
[32m[20221214 14:36:27 @agent_ppo2.py:185][0m |          -0.0002 |         201.5343 |        -195.1082 |
[32m[20221214 14:36:27 @agent_ppo2.py:185][0m |           0.0012 |         200.0541 |        -195.0544 |
[32m[20221214 14:36:27 @agent_ppo2.py:185][0m |          -0.0008 |         198.4415 |        -195.2456 |
[32m[20221214 14:36:27 @agent_ppo2.py:185][0m |          -0.0005 |         198.4229 |        -195.3787 |
[32m[20221214 14:36:27 @agent_ppo2.py:185][0m |          -0.0056 |         196.7889 |        -195.3075 |
[32m[20221214 14:36:27 @agent_ppo2.py:185][0m |          -0.0035 |         195.6767 |        -195.7222 |
[32m[20221214 14:36:27 @agent_ppo2.py:185][0m |          -0.0004 |         194.2188 |        -195.6839 |
[32m[20221214 14:36:27 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:36:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.63
[32m[20221214 14:36:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.56
[32m[20221214 14:36:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.92
[32m[20221214 14:36:27 @agent_ppo2.py:143][0m Total time:      38.42 min
[32m[20221214 14:36:27 @agent_ppo2.py:145][0m 3534848 total steps have happened
[32m[20221214 14:36:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1726 --------------------------#
[32m[20221214 14:36:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:28 @agent_ppo2.py:185][0m |          -0.0020 |         211.0851 |        -196.8672 |
[32m[20221214 14:36:28 @agent_ppo2.py:185][0m |           0.0006 |         203.3365 |        -196.5349 |
[32m[20221214 14:36:28 @agent_ppo2.py:185][0m |           0.0025 |         202.8146 |        -196.7877 |
[32m[20221214 14:36:28 @agent_ppo2.py:185][0m |          -0.0025 |         200.4551 |        -197.1131 |
[32m[20221214 14:36:28 @agent_ppo2.py:185][0m |          -0.0029 |         199.1173 |        -196.7044 |
[32m[20221214 14:36:28 @agent_ppo2.py:185][0m |          -0.0006 |         198.6202 |        -196.9043 |
[32m[20221214 14:36:28 @agent_ppo2.py:185][0m |           0.0006 |         198.9743 |        -196.8345 |
[32m[20221214 14:36:28 @agent_ppo2.py:185][0m |          -0.0022 |         197.3385 |        -196.6305 |
[32m[20221214 14:36:28 @agent_ppo2.py:185][0m |           0.0051 |         198.4181 |        -196.9691 |
[32m[20221214 14:36:28 @agent_ppo2.py:185][0m |          -0.0062 |         196.9373 |        -197.1002 |
[32m[20221214 14:36:28 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:36:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.79
[32m[20221214 14:36:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.48
[32m[20221214 14:36:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.22
[32m[20221214 14:36:29 @agent_ppo2.py:143][0m Total time:      38.44 min
[32m[20221214 14:36:29 @agent_ppo2.py:145][0m 3536896 total steps have happened
[32m[20221214 14:36:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1727 --------------------------#
[32m[20221214 14:36:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:29 @agent_ppo2.py:185][0m |          -0.0013 |         251.8623 |        -197.7023 |
[32m[20221214 14:36:29 @agent_ppo2.py:185][0m |          -0.0040 |         238.5042 |        -197.3504 |
[32m[20221214 14:36:29 @agent_ppo2.py:185][0m |          -0.0044 |         233.7261 |        -197.6473 |
[32m[20221214 14:36:29 @agent_ppo2.py:185][0m |          -0.0003 |         225.7984 |        -198.2988 |
[32m[20221214 14:36:29 @agent_ppo2.py:185][0m |           0.0007 |         219.7399 |        -197.7477 |
[32m[20221214 14:36:29 @agent_ppo2.py:185][0m |          -0.0008 |         218.0397 |        -198.0467 |
[32m[20221214 14:36:29 @agent_ppo2.py:185][0m |           0.0008 |         217.3155 |        -198.2378 |
[32m[20221214 14:36:30 @agent_ppo2.py:185][0m |          -0.0022 |         214.2004 |        -198.0099 |
[32m[20221214 14:36:30 @agent_ppo2.py:185][0m |          -0.0005 |         213.4883 |        -198.2558 |
[32m[20221214 14:36:30 @agent_ppo2.py:185][0m |          -0.0012 |         212.8333 |        -198.1260 |
[32m[20221214 14:36:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:36:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.03
[32m[20221214 14:36:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.78
[32m[20221214 14:36:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.32
[32m[20221214 14:36:30 @agent_ppo2.py:143][0m Total time:      38.46 min
[32m[20221214 14:36:30 @agent_ppo2.py:145][0m 3538944 total steps have happened
[32m[20221214 14:36:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1728 --------------------------#
[32m[20221214 14:36:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:30 @agent_ppo2.py:185][0m |           0.0015 |         237.3887 |        -199.3858 |
[32m[20221214 14:36:30 @agent_ppo2.py:185][0m |          -0.0013 |         229.5677 |        -199.5125 |
[32m[20221214 14:36:30 @agent_ppo2.py:185][0m |          -0.0017 |         226.6385 |        -199.3521 |
[32m[20221214 14:36:30 @agent_ppo2.py:185][0m |          -0.0016 |         224.1265 |        -199.3983 |
[32m[20221214 14:36:30 @agent_ppo2.py:185][0m |          -0.0016 |         223.0955 |        -199.3877 |
[32m[20221214 14:36:31 @agent_ppo2.py:185][0m |          -0.0010 |         220.9818 |        -199.3538 |
[32m[20221214 14:36:31 @agent_ppo2.py:185][0m |          -0.0005 |         220.5161 |        -199.1220 |
[32m[20221214 14:36:31 @agent_ppo2.py:185][0m |          -0.0054 |         220.2095 |        -198.7964 |
[32m[20221214 14:36:31 @agent_ppo2.py:185][0m |          -0.0008 |         220.1003 |        -199.5074 |
[32m[20221214 14:36:31 @agent_ppo2.py:185][0m |          -0.0009 |         217.7310 |        -199.5254 |
[32m[20221214 14:36:31 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:36:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.24
[32m[20221214 14:36:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.87
[32m[20221214 14:36:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.19
[32m[20221214 14:36:31 @agent_ppo2.py:143][0m Total time:      38.48 min
[32m[20221214 14:36:31 @agent_ppo2.py:145][0m 3540992 total steps have happened
[32m[20221214 14:36:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1729 --------------------------#
[32m[20221214 14:36:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:31 @agent_ppo2.py:185][0m |          -0.0024 |         217.4834 |        -200.4063 |
[32m[20221214 14:36:31 @agent_ppo2.py:185][0m |          -0.0029 |         212.6770 |        -200.7787 |
[32m[20221214 14:36:32 @agent_ppo2.py:185][0m |          -0.0033 |         210.6409 |        -200.7649 |
[32m[20221214 14:36:32 @agent_ppo2.py:185][0m |          -0.0001 |         209.9441 |        -200.4033 |
[32m[20221214 14:36:32 @agent_ppo2.py:185][0m |          -0.0025 |         209.0246 |        -200.9501 |
[32m[20221214 14:36:32 @agent_ppo2.py:185][0m |          -0.0029 |         206.7105 |        -200.8602 |
[32m[20221214 14:36:32 @agent_ppo2.py:185][0m |           0.0004 |         205.5096 |        -200.8990 |
[32m[20221214 14:36:32 @agent_ppo2.py:185][0m |          -0.0019 |         204.1254 |        -200.4295 |
[32m[20221214 14:36:32 @agent_ppo2.py:185][0m |          -0.0017 |         203.7296 |        -200.6702 |
[32m[20221214 14:36:32 @agent_ppo2.py:185][0m |           0.0073 |         212.1349 |        -200.8367 |
[32m[20221214 14:36:32 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:36:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.11
[32m[20221214 14:36:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.21
[32m[20221214 14:36:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.90
[32m[20221214 14:36:32 @agent_ppo2.py:143][0m Total time:      38.50 min
[32m[20221214 14:36:32 @agent_ppo2.py:145][0m 3543040 total steps have happened
[32m[20221214 14:36:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1730 --------------------------#
[32m[20221214 14:36:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:36:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:33 @agent_ppo2.py:185][0m |          -0.0035 |         226.2708 |        -200.7324 |
[32m[20221214 14:36:33 @agent_ppo2.py:185][0m |           0.0062 |         247.1983 |        -200.8909 |
[32m[20221214 14:36:33 @agent_ppo2.py:185][0m |          -0.0064 |         220.4733 |        -200.5400 |
[32m[20221214 14:36:33 @agent_ppo2.py:185][0m |          -0.0053 |         218.0611 |        -201.0374 |
[32m[20221214 14:36:33 @agent_ppo2.py:185][0m |          -0.0061 |         217.0341 |        -200.8590 |
[32m[20221214 14:36:33 @agent_ppo2.py:185][0m |          -0.0049 |         216.7127 |        -200.6082 |
[32m[20221214 14:36:33 @agent_ppo2.py:185][0m |          -0.0054 |         216.3244 |        -200.9907 |
[32m[20221214 14:36:33 @agent_ppo2.py:185][0m |          -0.0064 |         215.0915 |        -200.6247 |
[32m[20221214 14:36:33 @agent_ppo2.py:185][0m |          -0.0019 |         215.2597 |        -200.8753 |
[32m[20221214 14:36:33 @agent_ppo2.py:185][0m |          -0.0064 |         214.5259 |        -201.1647 |
[32m[20221214 14:36:33 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:36:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.26
[32m[20221214 14:36:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.30
[32m[20221214 14:36:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.94
[32m[20221214 14:36:33 @agent_ppo2.py:143][0m Total time:      38.52 min
[32m[20221214 14:36:33 @agent_ppo2.py:145][0m 3545088 total steps have happened
[32m[20221214 14:36:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1731 --------------------------#
[32m[20221214 14:36:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:34 @agent_ppo2.py:185][0m |          -0.0023 |         245.9494 |        -198.4897 |
[32m[20221214 14:36:34 @agent_ppo2.py:185][0m |          -0.0020 |         237.7023 |        -198.8479 |
[32m[20221214 14:36:34 @agent_ppo2.py:185][0m |           0.0106 |         258.9656 |        -199.2024 |
[32m[20221214 14:36:34 @agent_ppo2.py:185][0m |          -0.0015 |         234.6185 |        -198.5725 |
[32m[20221214 14:36:34 @agent_ppo2.py:185][0m |          -0.0042 |         235.0189 |        -199.3715 |
[32m[20221214 14:36:34 @agent_ppo2.py:185][0m |          -0.0030 |         231.8982 |        -199.2702 |
[32m[20221214 14:36:34 @agent_ppo2.py:185][0m |          -0.0042 |         231.9516 |        -199.1243 |
[32m[20221214 14:36:34 @agent_ppo2.py:185][0m |          -0.0017 |         232.2064 |        -199.1275 |
[32m[20221214 14:36:34 @agent_ppo2.py:185][0m |           0.0035 |         234.5865 |        -199.0534 |
[32m[20221214 14:36:35 @agent_ppo2.py:185][0m |          -0.0039 |         230.6454 |        -199.3911 |
[32m[20221214 14:36:35 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:36:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.23
[32m[20221214 14:36:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.22
[32m[20221214 14:36:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.73
[32m[20221214 14:36:35 @agent_ppo2.py:143][0m Total time:      38.54 min
[32m[20221214 14:36:35 @agent_ppo2.py:145][0m 3547136 total steps have happened
[32m[20221214 14:36:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1732 --------------------------#
[32m[20221214 14:36:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:35 @agent_ppo2.py:185][0m |           0.0025 |         259.6904 |        -203.7938 |
[32m[20221214 14:36:35 @agent_ppo2.py:185][0m |          -0.0001 |         250.0556 |        -201.9997 |
[32m[20221214 14:36:35 @agent_ppo2.py:185][0m |          -0.0005 |         246.6337 |        -203.2043 |
[32m[20221214 14:36:35 @agent_ppo2.py:185][0m |           0.0009 |         243.6688 |        -202.8204 |
[32m[20221214 14:36:35 @agent_ppo2.py:185][0m |           0.0002 |         245.0735 |        -203.3544 |
[32m[20221214 14:36:35 @agent_ppo2.py:185][0m |          -0.0003 |         242.7099 |        -203.1411 |
[32m[20221214 14:36:36 @agent_ppo2.py:185][0m |           0.0048 |         245.8437 |        -202.5716 |
[32m[20221214 14:36:36 @agent_ppo2.py:185][0m |          -0.0012 |         242.2771 |        -202.8709 |
[32m[20221214 14:36:36 @agent_ppo2.py:185][0m |          -0.0031 |         239.7285 |        -202.5937 |
[32m[20221214 14:36:36 @agent_ppo2.py:185][0m |          -0.0001 |         238.9208 |        -203.2598 |
[32m[20221214 14:36:36 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:36:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.63
[32m[20221214 14:36:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.94
[32m[20221214 14:36:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.43
[32m[20221214 14:36:36 @agent_ppo2.py:143][0m Total time:      38.57 min
[32m[20221214 14:36:36 @agent_ppo2.py:145][0m 3549184 total steps have happened
[32m[20221214 14:36:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1733 --------------------------#
[32m[20221214 14:36:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:36 @agent_ppo2.py:185][0m |           0.0069 |         262.7970 |        -201.9878 |
[32m[20221214 14:36:36 @agent_ppo2.py:185][0m |          -0.0027 |         238.5120 |        -201.6644 |
[32m[20221214 14:36:37 @agent_ppo2.py:185][0m |           0.0011 |         231.6480 |        -202.3732 |
[32m[20221214 14:36:37 @agent_ppo2.py:185][0m |          -0.0015 |         228.4418 |        -202.9261 |
[32m[20221214 14:36:37 @agent_ppo2.py:185][0m |          -0.0029 |         225.2349 |        -203.1952 |
[32m[20221214 14:36:37 @agent_ppo2.py:185][0m |           0.0007 |         222.2025 |        -203.0386 |
[32m[20221214 14:36:37 @agent_ppo2.py:185][0m |          -0.0017 |         220.6570 |        -203.6328 |
[32m[20221214 14:36:37 @agent_ppo2.py:185][0m |          -0.0012 |         218.8835 |        -203.1781 |
[32m[20221214 14:36:37 @agent_ppo2.py:185][0m |          -0.0009 |         219.5030 |        -203.5098 |
[32m[20221214 14:36:37 @agent_ppo2.py:185][0m |          -0.0000 |         216.9006 |        -204.0937 |
[32m[20221214 14:36:37 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:36:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.45
[32m[20221214 14:36:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.69
[32m[20221214 14:36:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.69
[32m[20221214 14:36:37 @agent_ppo2.py:143][0m Total time:      38.59 min
[32m[20221214 14:36:37 @agent_ppo2.py:145][0m 3551232 total steps have happened
[32m[20221214 14:36:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1734 --------------------------#
[32m[20221214 14:36:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:38 @agent_ppo2.py:185][0m |          -0.0004 |         243.5236 |        -202.4080 |
[32m[20221214 14:36:38 @agent_ppo2.py:185][0m |           0.0010 |         229.8386 |        -202.9430 |
[32m[20221214 14:36:38 @agent_ppo2.py:185][0m |          -0.0041 |         226.0078 |        -202.9978 |
[32m[20221214 14:36:38 @agent_ppo2.py:185][0m |           0.0051 |         224.8523 |        -203.6259 |
[32m[20221214 14:36:38 @agent_ppo2.py:185][0m |          -0.0013 |         221.2917 |        -203.8176 |
[32m[20221214 14:36:38 @agent_ppo2.py:185][0m |          -0.0008 |         220.3756 |        -204.0091 |
[32m[20221214 14:36:38 @agent_ppo2.py:185][0m |          -0.0010 |         219.0391 |        -204.0403 |
[32m[20221214 14:36:38 @agent_ppo2.py:185][0m |           0.0023 |         220.1151 |        -204.4768 |
[32m[20221214 14:36:39 @agent_ppo2.py:185][0m |          -0.0012 |         218.7626 |        -204.6241 |
[32m[20221214 14:36:39 @agent_ppo2.py:185][0m |          -0.0028 |         218.0345 |        -204.3039 |
[32m[20221214 14:36:39 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:36:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.58
[32m[20221214 14:36:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.69
[32m[20221214 14:36:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.49
[32m[20221214 14:36:39 @agent_ppo2.py:143][0m Total time:      38.61 min
[32m[20221214 14:36:39 @agent_ppo2.py:145][0m 3553280 total steps have happened
[32m[20221214 14:36:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1735 --------------------------#
[32m[20221214 14:36:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:39 @agent_ppo2.py:185][0m |          -0.0015 |         260.5938 |        -209.4403 |
[32m[20221214 14:36:39 @agent_ppo2.py:185][0m |          -0.0022 |         249.5131 |        -209.0236 |
[32m[20221214 14:36:39 @agent_ppo2.py:185][0m |          -0.0022 |         245.4672 |        -209.5234 |
[32m[20221214 14:36:39 @agent_ppo2.py:185][0m |           0.0058 |         255.8684 |        -209.5954 |
[32m[20221214 14:36:39 @agent_ppo2.py:185][0m |          -0.0033 |         244.7524 |        -209.5825 |
[32m[20221214 14:36:40 @agent_ppo2.py:185][0m |          -0.0050 |         241.2447 |        -209.4248 |
[32m[20221214 14:36:40 @agent_ppo2.py:185][0m |          -0.0027 |         241.0820 |        -209.2090 |
[32m[20221214 14:36:40 @agent_ppo2.py:185][0m |          -0.0081 |         240.7991 |        -209.6042 |
[32m[20221214 14:36:40 @agent_ppo2.py:185][0m |          -0.0045 |         240.5566 |        -209.8743 |
[32m[20221214 14:36:40 @agent_ppo2.py:185][0m |          -0.0005 |         239.2882 |        -209.3804 |
[32m[20221214 14:36:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:36:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.76
[32m[20221214 14:36:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.94
[32m[20221214 14:36:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.34
[32m[20221214 14:36:40 @agent_ppo2.py:143][0m Total time:      38.63 min
[32m[20221214 14:36:40 @agent_ppo2.py:145][0m 3555328 total steps have happened
[32m[20221214 14:36:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1736 --------------------------#
[32m[20221214 14:36:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:40 @agent_ppo2.py:185][0m |          -0.0013 |         250.2061 |        -208.2451 |
[32m[20221214 14:36:41 @agent_ppo2.py:185][0m |          -0.0004 |         243.3123 |        -208.1316 |
[32m[20221214 14:36:41 @agent_ppo2.py:185][0m |          -0.0012 |         241.1338 |        -207.5405 |
[32m[20221214 14:36:41 @agent_ppo2.py:185][0m |           0.0036 |         240.3740 |        -208.1623 |
[32m[20221214 14:36:41 @agent_ppo2.py:185][0m |          -0.0021 |         235.6675 |        -208.6188 |
[32m[20221214 14:36:41 @agent_ppo2.py:185][0m |          -0.0023 |         235.7641 |        -208.3338 |
[32m[20221214 14:36:41 @agent_ppo2.py:185][0m |           0.0006 |         235.5004 |        -208.6676 |
[32m[20221214 14:36:41 @agent_ppo2.py:185][0m |          -0.0016 |         233.7621 |        -208.4913 |
[32m[20221214 14:36:41 @agent_ppo2.py:185][0m |          -0.0032 |         232.9480 |        -208.7792 |
[32m[20221214 14:36:41 @agent_ppo2.py:185][0m |          -0.0026 |         234.1841 |        -208.6389 |
[32m[20221214 14:36:41 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:36:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.17
[32m[20221214 14:36:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.39
[32m[20221214 14:36:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.55
[32m[20221214 14:36:42 @agent_ppo2.py:143][0m Total time:      38.66 min
[32m[20221214 14:36:42 @agent_ppo2.py:145][0m 3557376 total steps have happened
[32m[20221214 14:36:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1737 --------------------------#
[32m[20221214 14:36:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:42 @agent_ppo2.py:185][0m |           0.0018 |         275.4878 |        -205.0605 |
[32m[20221214 14:36:42 @agent_ppo2.py:185][0m |          -0.0002 |         270.1563 |        -205.5537 |
[32m[20221214 14:36:42 @agent_ppo2.py:185][0m |          -0.0020 |         268.6826 |        -205.7755 |
[32m[20221214 14:36:42 @agent_ppo2.py:185][0m |          -0.0002 |         266.4180 |        -205.2154 |
[32m[20221214 14:36:42 @agent_ppo2.py:185][0m |          -0.0021 |         266.1988 |        -205.8608 |
[32m[20221214 14:36:42 @agent_ppo2.py:185][0m |          -0.0041 |         264.8613 |        -205.9520 |
[32m[20221214 14:36:42 @agent_ppo2.py:185][0m |          -0.0005 |         264.3688 |        -206.7377 |
[32m[20221214 14:36:43 @agent_ppo2.py:185][0m |          -0.0031 |         263.8060 |        -206.8542 |
[32m[20221214 14:36:43 @agent_ppo2.py:185][0m |          -0.0003 |         263.6902 |        -207.4124 |
[32m[20221214 14:36:43 @agent_ppo2.py:185][0m |          -0.0038 |         263.6590 |        -207.4644 |
[32m[20221214 14:36:43 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:36:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.09
[32m[20221214 14:36:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.87
[32m[20221214 14:36:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.46
[32m[20221214 14:36:43 @agent_ppo2.py:143][0m Total time:      38.68 min
[32m[20221214 14:36:43 @agent_ppo2.py:145][0m 3559424 total steps have happened
[32m[20221214 14:36:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1738 --------------------------#
[32m[20221214 14:36:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:43 @agent_ppo2.py:185][0m |          -0.0001 |         255.9763 |        -212.4719 |
[32m[20221214 14:36:43 @agent_ppo2.py:185][0m |           0.0037 |         242.5867 |        -212.6219 |
[32m[20221214 14:36:43 @agent_ppo2.py:185][0m |          -0.0008 |         238.3013 |        -212.5436 |
[32m[20221214 14:36:43 @agent_ppo2.py:185][0m |          -0.0010 |         236.0071 |        -212.9689 |
[32m[20221214 14:36:44 @agent_ppo2.py:185][0m |          -0.0020 |         234.5026 |        -213.0269 |
[32m[20221214 14:36:44 @agent_ppo2.py:185][0m |          -0.0036 |         233.7447 |        -212.8480 |
[32m[20221214 14:36:44 @agent_ppo2.py:185][0m |          -0.0004 |         232.9950 |        -212.4529 |
[32m[20221214 14:36:44 @agent_ppo2.py:185][0m |          -0.0015 |         231.0559 |        -212.6007 |
[32m[20221214 14:36:44 @agent_ppo2.py:185][0m |          -0.0026 |         228.9229 |        -213.0757 |
[32m[20221214 14:36:44 @agent_ppo2.py:185][0m |          -0.0011 |         227.4561 |        -212.6930 |
[32m[20221214 14:36:44 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:36:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.41
[32m[20221214 14:36:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.66
[32m[20221214 14:36:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.74
[32m[20221214 14:36:44 @agent_ppo2.py:143][0m Total time:      38.70 min
[32m[20221214 14:36:44 @agent_ppo2.py:145][0m 3561472 total steps have happened
[32m[20221214 14:36:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1739 --------------------------#
[32m[20221214 14:36:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:44 @agent_ppo2.py:185][0m |          -0.0031 |         281.9148 |        -212.2066 |
[32m[20221214 14:36:45 @agent_ppo2.py:185][0m |          -0.0039 |         275.0973 |        -212.3246 |
[32m[20221214 14:36:45 @agent_ppo2.py:185][0m |          -0.0000 |         276.5361 |        -212.6246 |
[32m[20221214 14:36:45 @agent_ppo2.py:185][0m |          -0.0063 |         270.3980 |        -212.2745 |
[32m[20221214 14:36:45 @agent_ppo2.py:185][0m |          -0.0023 |         269.0397 |        -212.8261 |
[32m[20221214 14:36:45 @agent_ppo2.py:185][0m |          -0.0022 |         267.6538 |        -211.9092 |
[32m[20221214 14:36:45 @agent_ppo2.py:185][0m |          -0.0021 |         266.9641 |        -211.5125 |
[32m[20221214 14:36:45 @agent_ppo2.py:185][0m |          -0.0039 |         266.2584 |        -212.1830 |
[32m[20221214 14:36:45 @agent_ppo2.py:185][0m |          -0.0008 |         266.2151 |        -211.5488 |
[32m[20221214 14:36:45 @agent_ppo2.py:185][0m |          -0.0012 |         264.4127 |        -211.4793 |
[32m[20221214 14:36:45 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:36:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.61
[32m[20221214 14:36:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.64
[32m[20221214 14:36:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.36
[32m[20221214 14:36:45 @agent_ppo2.py:143][0m Total time:      38.72 min
[32m[20221214 14:36:45 @agent_ppo2.py:145][0m 3563520 total steps have happened
[32m[20221214 14:36:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1740 --------------------------#
[32m[20221214 14:36:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:36:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:46 @agent_ppo2.py:185][0m |           0.0115 |         291.8175 |        -214.1449 |
[32m[20221214 14:36:46 @agent_ppo2.py:185][0m |          -0.0015 |         264.9374 |        -214.2387 |
[32m[20221214 14:36:46 @agent_ppo2.py:185][0m |           0.0143 |         295.2759 |        -214.5067 |
[32m[20221214 14:36:46 @agent_ppo2.py:185][0m |          -0.0002 |         260.9371 |        -214.4630 |
[32m[20221214 14:36:46 @agent_ppo2.py:185][0m |           0.0018 |         258.7886 |        -214.1965 |
[32m[20221214 14:36:46 @agent_ppo2.py:185][0m |           0.0050 |         264.4729 |        -214.7844 |
[32m[20221214 14:36:46 @agent_ppo2.py:185][0m |          -0.0030 |         256.7015 |        -214.3785 |
[32m[20221214 14:36:46 @agent_ppo2.py:185][0m |           0.0090 |         271.0057 |        -215.1134 |
[32m[20221214 14:36:46 @agent_ppo2.py:185][0m |          -0.0014 |         254.6697 |        -214.8667 |
[32m[20221214 14:36:46 @agent_ppo2.py:185][0m |          -0.0021 |         253.6098 |        -215.5605 |
[32m[20221214 14:36:46 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:36:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.22
[32m[20221214 14:36:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.27
[32m[20221214 14:36:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.40
[32m[20221214 14:36:47 @agent_ppo2.py:143][0m Total time:      38.74 min
[32m[20221214 14:36:47 @agent_ppo2.py:145][0m 3565568 total steps have happened
[32m[20221214 14:36:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1741 --------------------------#
[32m[20221214 14:36:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:47 @agent_ppo2.py:185][0m |          -0.0010 |         277.8605 |        -211.5086 |
[32m[20221214 14:36:47 @agent_ppo2.py:185][0m |          -0.0007 |         274.7157 |        -210.6150 |
[32m[20221214 14:36:47 @agent_ppo2.py:185][0m |           0.0036 |         275.3168 |        -211.2259 |
[32m[20221214 14:36:47 @agent_ppo2.py:185][0m |          -0.0007 |         271.4835 |        -211.9587 |
[32m[20221214 14:36:47 @agent_ppo2.py:185][0m |           0.0058 |         277.4082 |        -211.1353 |
[32m[20221214 14:36:47 @agent_ppo2.py:185][0m |          -0.0024 |         271.7361 |        -211.6275 |
[32m[20221214 14:36:47 @agent_ppo2.py:185][0m |           0.0076 |         287.0677 |        -210.9353 |
[32m[20221214 14:36:47 @agent_ppo2.py:185][0m |          -0.0000 |         271.9381 |        -210.8416 |
[32m[20221214 14:36:48 @agent_ppo2.py:185][0m |          -0.0016 |         269.2742 |        -211.6343 |
[32m[20221214 14:36:48 @agent_ppo2.py:185][0m |          -0.0024 |         268.4639 |        -211.5560 |
[32m[20221214 14:36:48 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:36:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.42
[32m[20221214 14:36:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.14
[32m[20221214 14:36:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.44
[32m[20221214 14:36:48 @agent_ppo2.py:143][0m Total time:      38.76 min
[32m[20221214 14:36:48 @agent_ppo2.py:145][0m 3567616 total steps have happened
[32m[20221214 14:36:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1742 --------------------------#
[32m[20221214 14:36:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:48 @agent_ppo2.py:185][0m |          -0.0027 |         249.6675 |        -209.5922 |
[32m[20221214 14:36:48 @agent_ppo2.py:185][0m |           0.0002 |         219.9825 |        -209.2676 |
[32m[20221214 14:36:48 @agent_ppo2.py:185][0m |           0.0001 |         208.2100 |        -209.4820 |
[32m[20221214 14:36:48 @agent_ppo2.py:185][0m |          -0.0023 |         201.1108 |        -209.7129 |
[32m[20221214 14:36:48 @agent_ppo2.py:185][0m |          -0.0017 |         194.9202 |        -209.9592 |
[32m[20221214 14:36:49 @agent_ppo2.py:185][0m |          -0.0043 |         190.0429 |        -208.8440 |
[32m[20221214 14:36:49 @agent_ppo2.py:185][0m |           0.0019 |         188.5355 |        -209.6446 |
[32m[20221214 14:36:49 @agent_ppo2.py:185][0m |          -0.0012 |         183.5242 |        -209.5412 |
[32m[20221214 14:36:49 @agent_ppo2.py:185][0m |          -0.0003 |         180.5532 |        -209.4319 |
[32m[20221214 14:36:49 @agent_ppo2.py:185][0m |          -0.0011 |         179.3677 |        -209.5682 |
[32m[20221214 14:36:49 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:36:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.13
[32m[20221214 14:36:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.74
[32m[20221214 14:36:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.40
[32m[20221214 14:36:49 @agent_ppo2.py:143][0m Total time:      38.78 min
[32m[20221214 14:36:49 @agent_ppo2.py:145][0m 3569664 total steps have happened
[32m[20221214 14:36:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1743 --------------------------#
[32m[20221214 14:36:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:49 @agent_ppo2.py:185][0m |          -0.0011 |         277.3695 |        -207.4913 |
[32m[20221214 14:36:49 @agent_ppo2.py:185][0m |          -0.0033 |         267.9647 |        -206.7069 |
[32m[20221214 14:36:50 @agent_ppo2.py:185][0m |          -0.0020 |         262.6757 |        -207.4003 |
[32m[20221214 14:36:50 @agent_ppo2.py:185][0m |          -0.0054 |         260.5061 |        -206.9720 |
[32m[20221214 14:36:50 @agent_ppo2.py:185][0m |          -0.0034 |         258.5816 |        -207.2655 |
[32m[20221214 14:36:50 @agent_ppo2.py:185][0m |          -0.0033 |         257.1567 |        -207.1841 |
[32m[20221214 14:36:50 @agent_ppo2.py:185][0m |          -0.0001 |         255.0795 |        -206.8216 |
[32m[20221214 14:36:50 @agent_ppo2.py:185][0m |          -0.0033 |         255.3085 |        -207.0608 |
[32m[20221214 14:36:50 @agent_ppo2.py:185][0m |          -0.0044 |         254.1127 |        -207.5379 |
[32m[20221214 14:36:50 @agent_ppo2.py:185][0m |           0.0095 |         285.4259 |        -207.1926 |
[32m[20221214 14:36:50 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:36:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.01
[32m[20221214 14:36:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.63
[32m[20221214 14:36:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.28
[32m[20221214 14:36:50 @agent_ppo2.py:143][0m Total time:      38.80 min
[32m[20221214 14:36:50 @agent_ppo2.py:145][0m 3571712 total steps have happened
[32m[20221214 14:36:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1744 --------------------------#
[32m[20221214 14:36:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:51 @agent_ppo2.py:185][0m |          -0.0021 |         297.7028 |        -212.4010 |
[32m[20221214 14:36:51 @agent_ppo2.py:185][0m |          -0.0017 |         287.9368 |        -211.9391 |
[32m[20221214 14:36:51 @agent_ppo2.py:185][0m |          -0.0040 |         284.6267 |        -211.0592 |
[32m[20221214 14:36:51 @agent_ppo2.py:185][0m |          -0.0011 |         281.0876 |        -212.0705 |
[32m[20221214 14:36:51 @agent_ppo2.py:185][0m |          -0.0032 |         279.1201 |        -212.1480 |
[32m[20221214 14:36:51 @agent_ppo2.py:185][0m |          -0.0014 |         277.5506 |        -211.9243 |
[32m[20221214 14:36:51 @agent_ppo2.py:185][0m |           0.0002 |         278.2990 |        -212.3236 |
[32m[20221214 14:36:51 @agent_ppo2.py:185][0m |          -0.0020 |         275.2685 |        -212.4275 |
[32m[20221214 14:36:51 @agent_ppo2.py:185][0m |          -0.0021 |         275.1543 |        -212.7001 |
[32m[20221214 14:36:51 @agent_ppo2.py:185][0m |          -0.0010 |         274.7387 |        -212.2191 |
[32m[20221214 14:36:51 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:36:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.14
[32m[20221214 14:36:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.25
[32m[20221214 14:36:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.69
[32m[20221214 14:36:52 @agent_ppo2.py:143][0m Total time:      38.82 min
[32m[20221214 14:36:52 @agent_ppo2.py:145][0m 3573760 total steps have happened
[32m[20221214 14:36:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1745 --------------------------#
[32m[20221214 14:36:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:52 @agent_ppo2.py:185][0m |          -0.0031 |         294.6700 |        -214.9628 |
[32m[20221214 14:36:52 @agent_ppo2.py:185][0m |           0.0027 |         278.4948 |        -214.8718 |
[32m[20221214 14:36:52 @agent_ppo2.py:185][0m |          -0.0018 |         272.9705 |        -215.3317 |
[32m[20221214 14:36:52 @agent_ppo2.py:185][0m |          -0.0035 |         272.0690 |        -215.2258 |
[32m[20221214 14:36:52 @agent_ppo2.py:185][0m |          -0.0023 |         271.0136 |        -215.3972 |
[32m[20221214 14:36:52 @agent_ppo2.py:185][0m |          -0.0032 |         269.5897 |        -215.6146 |
[32m[20221214 14:36:52 @agent_ppo2.py:185][0m |          -0.0013 |         272.6961 |        -216.1891 |
[32m[20221214 14:36:52 @agent_ppo2.py:185][0m |          -0.0036 |         268.8087 |        -216.1190 |
[32m[20221214 14:36:53 @agent_ppo2.py:185][0m |          -0.0028 |         268.4017 |        -215.9257 |
[32m[20221214 14:36:53 @agent_ppo2.py:185][0m |           0.0069 |         278.4494 |        -216.3207 |
[32m[20221214 14:36:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:36:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.17
[32m[20221214 14:36:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.78
[32m[20221214 14:36:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.12
[32m[20221214 14:36:53 @agent_ppo2.py:143][0m Total time:      38.84 min
[32m[20221214 14:36:53 @agent_ppo2.py:145][0m 3575808 total steps have happened
[32m[20221214 14:36:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1746 --------------------------#
[32m[20221214 14:36:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:53 @agent_ppo2.py:185][0m |           0.0040 |         286.9767 |        -217.7554 |
[32m[20221214 14:36:53 @agent_ppo2.py:185][0m |           0.0100 |         299.5242 |        -218.1075 |
[32m[20221214 14:36:53 @agent_ppo2.py:185][0m |          -0.0032 |         271.0668 |        -217.9952 |
[32m[20221214 14:36:53 @agent_ppo2.py:185][0m |          -0.0027 |         269.7292 |        -217.4547 |
[32m[20221214 14:36:53 @agent_ppo2.py:185][0m |          -0.0022 |         269.3371 |        -217.9576 |
[32m[20221214 14:36:54 @agent_ppo2.py:185][0m |          -0.0042 |         268.9085 |        -217.7164 |
[32m[20221214 14:36:54 @agent_ppo2.py:185][0m |          -0.0037 |         268.8353 |        -217.6937 |
[32m[20221214 14:36:54 @agent_ppo2.py:185][0m |           0.0104 |         290.2556 |        -217.4901 |
[32m[20221214 14:36:54 @agent_ppo2.py:185][0m |           0.0023 |         270.0494 |        -217.3233 |
[32m[20221214 14:36:54 @agent_ppo2.py:185][0m |          -0.0033 |         269.2710 |        -217.6406 |
[32m[20221214 14:36:54 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:36:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.01
[32m[20221214 14:36:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.36
[32m[20221214 14:36:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.93
[32m[20221214 14:36:54 @agent_ppo2.py:143][0m Total time:      38.87 min
[32m[20221214 14:36:54 @agent_ppo2.py:145][0m 3577856 total steps have happened
[32m[20221214 14:36:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1747 --------------------------#
[32m[20221214 14:36:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:54 @agent_ppo2.py:185][0m |           0.0026 |         260.0571 |        -216.4589 |
[32m[20221214 14:36:54 @agent_ppo2.py:185][0m |          -0.0016 |         249.1279 |        -216.0778 |
[32m[20221214 14:36:55 @agent_ppo2.py:185][0m |          -0.0039 |         248.4025 |        -216.0919 |
[32m[20221214 14:36:55 @agent_ppo2.py:185][0m |          -0.0025 |         247.4770 |        -216.2379 |
[32m[20221214 14:36:55 @agent_ppo2.py:185][0m |          -0.0015 |         246.6380 |        -215.2061 |
[32m[20221214 14:36:55 @agent_ppo2.py:185][0m |           0.0024 |         251.0447 |        -215.6342 |
[32m[20221214 14:36:55 @agent_ppo2.py:185][0m |          -0.0001 |         245.4134 |        -215.6160 |
[32m[20221214 14:36:55 @agent_ppo2.py:185][0m |          -0.0035 |         244.7027 |        -216.1413 |
[32m[20221214 14:36:55 @agent_ppo2.py:185][0m |          -0.0012 |         244.8910 |        -215.6816 |
[32m[20221214 14:36:55 @agent_ppo2.py:185][0m |          -0.0046 |         244.2504 |        -215.9527 |
[32m[20221214 14:36:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:36:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.54
[32m[20221214 14:36:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.25
[32m[20221214 14:36:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.68
[32m[20221214 14:36:55 @agent_ppo2.py:143][0m Total time:      38.89 min
[32m[20221214 14:36:55 @agent_ppo2.py:145][0m 3579904 total steps have happened
[32m[20221214 14:36:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1748 --------------------------#
[32m[20221214 14:36:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:56 @agent_ppo2.py:185][0m |           0.0003 |         248.6978 |        -214.2370 |
[32m[20221214 14:36:56 @agent_ppo2.py:185][0m |           0.0033 |         244.9066 |        -214.3189 |
[32m[20221214 14:36:56 @agent_ppo2.py:185][0m |          -0.0005 |         243.6555 |        -214.0861 |
[32m[20221214 14:36:56 @agent_ppo2.py:185][0m |           0.0008 |         242.7353 |        -214.4578 |
[32m[20221214 14:36:56 @agent_ppo2.py:185][0m |           0.0029 |         253.1520 |        -214.5119 |
[32m[20221214 14:36:56 @agent_ppo2.py:185][0m |           0.0022 |         243.7181 |        -214.8328 |
[32m[20221214 14:36:56 @agent_ppo2.py:185][0m |          -0.0021 |         242.3349 |        -214.8617 |
[32m[20221214 14:36:56 @agent_ppo2.py:185][0m |           0.0024 |         246.6569 |        -215.0456 |
[32m[20221214 14:36:56 @agent_ppo2.py:185][0m |          -0.0034 |         240.7929 |        -215.3733 |
[32m[20221214 14:36:56 @agent_ppo2.py:185][0m |          -0.0028 |         240.3031 |        -214.9271 |
[32m[20221214 14:36:56 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:36:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.76
[32m[20221214 14:36:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.30
[32m[20221214 14:36:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.19
[32m[20221214 14:36:57 @agent_ppo2.py:143][0m Total time:      38.91 min
[32m[20221214 14:36:57 @agent_ppo2.py:145][0m 3581952 total steps have happened
[32m[20221214 14:36:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1749 --------------------------#
[32m[20221214 14:36:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:36:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:57 @agent_ppo2.py:185][0m |          -0.0025 |         247.8388 |        -215.6451 |
[32m[20221214 14:36:57 @agent_ppo2.py:185][0m |          -0.0028 |         243.8629 |        -215.5697 |
[32m[20221214 14:36:57 @agent_ppo2.py:185][0m |          -0.0019 |         241.4147 |        -215.7127 |
[32m[20221214 14:36:57 @agent_ppo2.py:185][0m |          -0.0038 |         240.3134 |        -214.8981 |
[32m[20221214 14:36:57 @agent_ppo2.py:185][0m |           0.0088 |         264.4376 |        -215.1825 |
[32m[20221214 14:36:57 @agent_ppo2.py:185][0m |          -0.0033 |         237.5492 |        -214.9097 |
[32m[20221214 14:36:57 @agent_ppo2.py:185][0m |          -0.0029 |         236.1962 |        -214.7077 |
[32m[20221214 14:36:57 @agent_ppo2.py:185][0m |           0.0005 |         235.1198 |        -214.5579 |
[32m[20221214 14:36:58 @agent_ppo2.py:185][0m |           0.0035 |         238.7120 |        -214.8608 |
[32m[20221214 14:36:58 @agent_ppo2.py:185][0m |          -0.0009 |         233.2619 |        -214.4539 |
[32m[20221214 14:36:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:36:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.10
[32m[20221214 14:36:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.21
[32m[20221214 14:36:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.28
[32m[20221214 14:36:58 @agent_ppo2.py:143][0m Total time:      38.93 min
[32m[20221214 14:36:58 @agent_ppo2.py:145][0m 3584000 total steps have happened
[32m[20221214 14:36:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1750 --------------------------#
[32m[20221214 14:36:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:36:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:58 @agent_ppo2.py:185][0m |          -0.0029 |         266.0472 |        -213.7603 |
[32m[20221214 14:36:58 @agent_ppo2.py:185][0m |           0.0097 |         271.8946 |        -214.0316 |
[32m[20221214 14:36:58 @agent_ppo2.py:185][0m |           0.0002 |         255.8046 |        -213.7683 |
[32m[20221214 14:36:58 @agent_ppo2.py:185][0m |           0.0112 |         290.8174 |        -213.9611 |
[32m[20221214 14:36:59 @agent_ppo2.py:185][0m |          -0.0018 |         252.8688 |        -213.7336 |
[32m[20221214 14:36:59 @agent_ppo2.py:185][0m |          -0.0017 |         251.1940 |        -213.7464 |
[32m[20221214 14:36:59 @agent_ppo2.py:185][0m |           0.0152 |         288.0250 |        -213.7340 |
[32m[20221214 14:36:59 @agent_ppo2.py:185][0m |          -0.0003 |         250.5436 |        -214.2943 |
[32m[20221214 14:36:59 @agent_ppo2.py:185][0m |           0.0030 |         249.9107 |        -214.4088 |
[32m[20221214 14:36:59 @agent_ppo2.py:185][0m |          -0.0002 |         251.5439 |        -214.0757 |
[32m[20221214 14:36:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:36:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.57
[32m[20221214 14:36:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.91
[32m[20221214 14:36:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.09
[32m[20221214 14:36:59 @agent_ppo2.py:143][0m Total time:      38.95 min
[32m[20221214 14:36:59 @agent_ppo2.py:145][0m 3586048 total steps have happened
[32m[20221214 14:36:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1751 --------------------------#
[32m[20221214 14:36:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:36:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:36:59 @agent_ppo2.py:185][0m |          -0.0011 |         268.0922 |        -215.4691 |
[32m[20221214 14:37:00 @agent_ppo2.py:185][0m |           0.0012 |         262.9301 |        -215.2233 |
[32m[20221214 14:37:00 @agent_ppo2.py:185][0m |          -0.0007 |         260.8153 |        -215.1408 |
[32m[20221214 14:37:00 @agent_ppo2.py:185][0m |           0.0011 |         259.8519 |        -215.0082 |
[32m[20221214 14:37:00 @agent_ppo2.py:185][0m |          -0.0002 |         257.2775 |        -214.8231 |
[32m[20221214 14:37:00 @agent_ppo2.py:185][0m |          -0.0006 |         255.6198 |        -214.6169 |
[32m[20221214 14:37:00 @agent_ppo2.py:185][0m |          -0.0023 |         254.5608 |        -214.8662 |
[32m[20221214 14:37:00 @agent_ppo2.py:185][0m |          -0.0008 |         254.2614 |        -215.0776 |
[32m[20221214 14:37:00 @agent_ppo2.py:185][0m |           0.0003 |         253.5028 |        -214.8447 |
[32m[20221214 14:37:00 @agent_ppo2.py:185][0m |          -0.0009 |         252.7824 |        -215.1315 |
[32m[20221214 14:37:00 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:37:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.39
[32m[20221214 14:37:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.56
[32m[20221214 14:37:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.66
[32m[20221214 14:37:00 @agent_ppo2.py:143][0m Total time:      38.97 min
[32m[20221214 14:37:00 @agent_ppo2.py:145][0m 3588096 total steps have happened
[32m[20221214 14:37:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1752 --------------------------#
[32m[20221214 14:37:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:01 @agent_ppo2.py:185][0m |          -0.0002 |         246.5324 |        -215.9417 |
[32m[20221214 14:37:01 @agent_ppo2.py:185][0m |           0.0003 |         240.8190 |        -216.3751 |
[32m[20221214 14:37:01 @agent_ppo2.py:185][0m |          -0.0021 |         239.3428 |        -216.4399 |
[32m[20221214 14:37:01 @agent_ppo2.py:185][0m |          -0.0021 |         240.1468 |        -216.4632 |
[32m[20221214 14:37:01 @agent_ppo2.py:185][0m |           0.0041 |         239.9952 |        -215.7664 |
[32m[20221214 14:37:01 @agent_ppo2.py:185][0m |           0.0068 |         250.1702 |        -216.4598 |
[32m[20221214 14:37:01 @agent_ppo2.py:185][0m |          -0.0016 |         238.1582 |        -216.3636 |
[32m[20221214 14:37:01 @agent_ppo2.py:185][0m |           0.0074 |         245.2898 |        -216.5994 |
[32m[20221214 14:37:01 @agent_ppo2.py:185][0m |          -0.0040 |         236.9306 |        -215.9511 |
[32m[20221214 14:37:01 @agent_ppo2.py:185][0m |           0.0014 |         235.5310 |        -217.0126 |
[32m[20221214 14:37:01 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:37:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.76
[32m[20221214 14:37:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.08
[32m[20221214 14:37:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.91
[32m[20221214 14:37:02 @agent_ppo2.py:143][0m Total time:      38.99 min
[32m[20221214 14:37:02 @agent_ppo2.py:145][0m 3590144 total steps have happened
[32m[20221214 14:37:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1753 --------------------------#
[32m[20221214 14:37:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:02 @agent_ppo2.py:185][0m |           0.0111 |         267.9200 |        -218.5946 |
[32m[20221214 14:37:02 @agent_ppo2.py:185][0m |          -0.0009 |         245.8835 |        -218.8362 |
[32m[20221214 14:37:02 @agent_ppo2.py:185][0m |          -0.0009 |         243.6313 |        -218.5783 |
[32m[20221214 14:37:02 @agent_ppo2.py:185][0m |          -0.0011 |         242.0271 |        -218.9892 |
[32m[20221214 14:37:02 @agent_ppo2.py:185][0m |           0.0012 |         240.2229 |        -219.5413 |
[32m[20221214 14:37:02 @agent_ppo2.py:185][0m |          -0.0030 |         240.2182 |        -219.5208 |
[32m[20221214 14:37:02 @agent_ppo2.py:185][0m |           0.0019 |         240.0165 |        -219.9501 |
[32m[20221214 14:37:03 @agent_ppo2.py:185][0m |           0.0005 |         238.8827 |        -220.2749 |
[32m[20221214 14:37:03 @agent_ppo2.py:185][0m |          -0.0013 |         238.1101 |        -220.0276 |
[32m[20221214 14:37:03 @agent_ppo2.py:185][0m |          -0.0014 |         238.6397 |        -220.7617 |
[32m[20221214 14:37:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:37:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.46
[32m[20221214 14:37:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.52
[32m[20221214 14:37:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.08
[32m[20221214 14:37:03 @agent_ppo2.py:143][0m Total time:      39.01 min
[32m[20221214 14:37:03 @agent_ppo2.py:145][0m 3592192 total steps have happened
[32m[20221214 14:37:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1754 --------------------------#
[32m[20221214 14:37:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:03 @agent_ppo2.py:185][0m |          -0.0009 |         247.1036 |        -219.2027 |
[32m[20221214 14:37:03 @agent_ppo2.py:185][0m |          -0.0023 |         243.3720 |        -219.4484 |
[32m[20221214 14:37:03 @agent_ppo2.py:185][0m |          -0.0033 |         241.8423 |        -219.4266 |
[32m[20221214 14:37:03 @agent_ppo2.py:185][0m |          -0.0030 |         240.3154 |        -219.1611 |
[32m[20221214 14:37:04 @agent_ppo2.py:185][0m |          -0.0001 |         238.5976 |        -219.5811 |
[32m[20221214 14:37:04 @agent_ppo2.py:185][0m |          -0.0041 |         237.8021 |        -220.0634 |
[32m[20221214 14:37:04 @agent_ppo2.py:185][0m |          -0.0027 |         238.7766 |        -219.7561 |
[32m[20221214 14:37:04 @agent_ppo2.py:185][0m |           0.0009 |         236.5895 |        -220.6404 |
[32m[20221214 14:37:04 @agent_ppo2.py:185][0m |          -0.0025 |         235.0157 |        -220.1554 |
[32m[20221214 14:37:04 @agent_ppo2.py:185][0m |          -0.0020 |         236.2202 |        -220.3073 |
[32m[20221214 14:37:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:37:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.91
[32m[20221214 14:37:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.93
[32m[20221214 14:37:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.15
[32m[20221214 14:37:04 @agent_ppo2.py:143][0m Total time:      39.03 min
[32m[20221214 14:37:04 @agent_ppo2.py:145][0m 3594240 total steps have happened
[32m[20221214 14:37:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1755 --------------------------#
[32m[20221214 14:37:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:04 @agent_ppo2.py:185][0m |          -0.0022 |         237.6857 |        -222.4554 |
[32m[20221214 14:37:05 @agent_ppo2.py:185][0m |          -0.0018 |         233.9734 |        -221.7510 |
[32m[20221214 14:37:05 @agent_ppo2.py:185][0m |          -0.0032 |         230.6715 |        -221.2230 |
[32m[20221214 14:37:05 @agent_ppo2.py:185][0m |          -0.0026 |         228.6340 |        -221.7982 |
[32m[20221214 14:37:05 @agent_ppo2.py:185][0m |          -0.0011 |         227.4417 |        -221.9472 |
[32m[20221214 14:37:05 @agent_ppo2.py:185][0m |          -0.0023 |         225.8125 |        -221.7332 |
[32m[20221214 14:37:05 @agent_ppo2.py:185][0m |           0.0070 |         231.8671 |        -222.1965 |
[32m[20221214 14:37:05 @agent_ppo2.py:185][0m |          -0.0035 |         225.0792 |        -221.7100 |
[32m[20221214 14:37:05 @agent_ppo2.py:185][0m |          -0.0010 |         225.8962 |        -222.1536 |
[32m[20221214 14:37:05 @agent_ppo2.py:185][0m |           0.0021 |         223.4403 |        -221.6969 |
[32m[20221214 14:37:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:37:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.62
[32m[20221214 14:37:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.98
[32m[20221214 14:37:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.63
[32m[20221214 14:37:05 @agent_ppo2.py:143][0m Total time:      39.06 min
[32m[20221214 14:37:05 @agent_ppo2.py:145][0m 3596288 total steps have happened
[32m[20221214 14:37:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1756 --------------------------#
[32m[20221214 14:37:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:37:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:06 @agent_ppo2.py:185][0m |          -0.0006 |         253.0940 |        -218.7007 |
[32m[20221214 14:37:06 @agent_ppo2.py:185][0m |           0.0094 |         254.1632 |        -218.8324 |
[32m[20221214 14:37:06 @agent_ppo2.py:185][0m |          -0.0007 |         234.3634 |        -218.6570 |
[32m[20221214 14:37:06 @agent_ppo2.py:185][0m |          -0.0030 |         230.8945 |        -218.1855 |
[32m[20221214 14:37:06 @agent_ppo2.py:185][0m |          -0.0002 |         229.8115 |        -218.4634 |
[32m[20221214 14:37:06 @agent_ppo2.py:185][0m |          -0.0016 |         229.3406 |        -218.4908 |
[32m[20221214 14:37:06 @agent_ppo2.py:185][0m |           0.0001 |         229.5092 |        -218.2929 |
[32m[20221214 14:37:07 @agent_ppo2.py:185][0m |          -0.0018 |         228.1808 |        -218.2907 |
[32m[20221214 14:37:07 @agent_ppo2.py:185][0m |          -0.0010 |         228.3943 |        -218.5088 |
[32m[20221214 14:37:07 @agent_ppo2.py:185][0m |          -0.0012 |         227.8966 |        -218.1333 |
[32m[20221214 14:37:07 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:37:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.88
[32m[20221214 14:37:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.39
[32m[20221214 14:37:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.43
[32m[20221214 14:37:07 @agent_ppo2.py:143][0m Total time:      39.08 min
[32m[20221214 14:37:07 @agent_ppo2.py:145][0m 3598336 total steps have happened
[32m[20221214 14:37:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1757 --------------------------#
[32m[20221214 14:37:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:07 @agent_ppo2.py:185][0m |          -0.0027 |         234.1144 |        -222.4677 |
[32m[20221214 14:37:07 @agent_ppo2.py:185][0m |          -0.0010 |         227.2409 |        -221.7588 |
[32m[20221214 14:37:07 @agent_ppo2.py:185][0m |           0.0113 |         246.0712 |        -221.8377 |
[32m[20221214 14:37:08 @agent_ppo2.py:185][0m |           0.0034 |         229.4465 |        -222.0572 |
[32m[20221214 14:37:08 @agent_ppo2.py:185][0m |           0.0017 |         228.3488 |        -221.3665 |
[32m[20221214 14:37:08 @agent_ppo2.py:185][0m |           0.0001 |         223.6144 |        -221.8238 |
[32m[20221214 14:37:08 @agent_ppo2.py:185][0m |          -0.0017 |         224.4958 |        -221.1318 |
[32m[20221214 14:37:08 @agent_ppo2.py:185][0m |           0.0095 |         232.7403 |        -221.1945 |
[32m[20221214 14:37:08 @agent_ppo2.py:185][0m |          -0.0030 |         223.5200 |        -219.9619 |
[32m[20221214 14:37:08 @agent_ppo2.py:185][0m |          -0.0027 |         221.9091 |        -221.0909 |
[32m[20221214 14:37:08 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:37:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.13
[32m[20221214 14:37:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.98
[32m[20221214 14:37:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.97
[32m[20221214 14:37:08 @agent_ppo2.py:143][0m Total time:      39.10 min
[32m[20221214 14:37:08 @agent_ppo2.py:145][0m 3600384 total steps have happened
[32m[20221214 14:37:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1758 --------------------------#
[32m[20221214 14:37:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:09 @agent_ppo2.py:185][0m |           0.0024 |         228.4513 |        -220.8593 |
[32m[20221214 14:37:09 @agent_ppo2.py:185][0m |           0.0074 |         239.9370 |        -222.4255 |
[32m[20221214 14:37:09 @agent_ppo2.py:185][0m |          -0.0018 |         221.9783 |        -221.8254 |
[32m[20221214 14:37:09 @agent_ppo2.py:185][0m |          -0.0042 |         221.4677 |        -222.6366 |
[32m[20221214 14:37:09 @agent_ppo2.py:185][0m |          -0.0015 |         220.8462 |        -224.0066 |
[32m[20221214 14:37:09 @agent_ppo2.py:185][0m |           0.0099 |         234.8571 |        -223.2167 |
[32m[20221214 14:37:09 @agent_ppo2.py:185][0m |           0.0003 |         221.2243 |        -223.5562 |
[32m[20221214 14:37:09 @agent_ppo2.py:185][0m |          -0.0024 |         219.7876 |        -224.3008 |
[32m[20221214 14:37:09 @agent_ppo2.py:185][0m |          -0.0026 |         219.3243 |        -223.9950 |
[32m[20221214 14:37:10 @agent_ppo2.py:185][0m |           0.0022 |         228.8501 |        -223.8990 |
[32m[20221214 14:37:10 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:37:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.00
[32m[20221214 14:37:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.85
[32m[20221214 14:37:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.21
[32m[20221214 14:37:10 @agent_ppo2.py:143][0m Total time:      39.13 min
[32m[20221214 14:37:10 @agent_ppo2.py:145][0m 3602432 total steps have happened
[32m[20221214 14:37:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1759 --------------------------#
[32m[20221214 14:37:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:10 @agent_ppo2.py:185][0m |           0.0096 |         211.6296 |        -224.4318 |
[32m[20221214 14:37:10 @agent_ppo2.py:185][0m |           0.0011 |         200.8014 |        -224.0705 |
[32m[20221214 14:37:10 @agent_ppo2.py:185][0m |          -0.0004 |         197.8779 |        -223.7373 |
[32m[20221214 14:37:10 @agent_ppo2.py:185][0m |          -0.0016 |         198.0215 |        -223.1749 |
[32m[20221214 14:37:10 @agent_ppo2.py:185][0m |          -0.0034 |         197.3192 |        -223.3543 |
[32m[20221214 14:37:10 @agent_ppo2.py:185][0m |          -0.0013 |         196.7686 |        -224.1548 |
[32m[20221214 14:37:11 @agent_ppo2.py:185][0m |          -0.0037 |         195.9990 |        -223.7281 |
[32m[20221214 14:37:11 @agent_ppo2.py:185][0m |          -0.0029 |         196.5364 |        -223.9918 |
[32m[20221214 14:37:11 @agent_ppo2.py:185][0m |          -0.0022 |         194.8138 |        -224.2073 |
[32m[20221214 14:37:11 @agent_ppo2.py:185][0m |          -0.0008 |         194.5744 |        -223.3753 |
[32m[20221214 14:37:11 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:37:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.53
[32m[20221214 14:37:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.06
[32m[20221214 14:37:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.87
[32m[20221214 14:37:11 @agent_ppo2.py:143][0m Total time:      39.15 min
[32m[20221214 14:37:11 @agent_ppo2.py:145][0m 3604480 total steps have happened
[32m[20221214 14:37:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1760 --------------------------#
[32m[20221214 14:37:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:37:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:11 @agent_ppo2.py:185][0m |           0.0003 |         245.2299 |        -225.9434 |
[32m[20221214 14:37:12 @agent_ppo2.py:185][0m |          -0.0019 |         240.6765 |        -225.9142 |
[32m[20221214 14:37:12 @agent_ppo2.py:185][0m |          -0.0011 |         236.7505 |        -226.1561 |
[32m[20221214 14:37:12 @agent_ppo2.py:185][0m |          -0.0034 |         236.5474 |        -225.8904 |
[32m[20221214 14:37:12 @agent_ppo2.py:185][0m |           0.0118 |         267.0549 |        -225.7655 |
[32m[20221214 14:37:12 @agent_ppo2.py:185][0m |          -0.0027 |         235.3311 |        -226.2448 |
[32m[20221214 14:37:12 @agent_ppo2.py:185][0m |          -0.0016 |         232.8852 |        -225.8984 |
[32m[20221214 14:37:12 @agent_ppo2.py:185][0m |           0.0027 |         232.9897 |        -225.6870 |
[32m[20221214 14:37:12 @agent_ppo2.py:185][0m |          -0.0032 |         231.9365 |        -226.2596 |
[32m[20221214 14:37:12 @agent_ppo2.py:185][0m |           0.0003 |         232.7428 |        -226.2097 |
[32m[20221214 14:37:12 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:37:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.18
[32m[20221214 14:37:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.92
[32m[20221214 14:37:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.68
[32m[20221214 14:37:12 @agent_ppo2.py:143][0m Total time:      39.17 min
[32m[20221214 14:37:12 @agent_ppo2.py:145][0m 3606528 total steps have happened
[32m[20221214 14:37:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1761 --------------------------#
[32m[20221214 14:37:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:13 @agent_ppo2.py:185][0m |          -0.0008 |         272.5269 |        -230.5907 |
[32m[20221214 14:37:13 @agent_ppo2.py:185][0m |           0.0022 |         264.7514 |        -230.5617 |
[32m[20221214 14:37:13 @agent_ppo2.py:185][0m |           0.0037 |         263.2610 |        -231.2541 |
[32m[20221214 14:37:13 @agent_ppo2.py:185][0m |           0.0073 |         273.3566 |        -231.2092 |
[32m[20221214 14:37:13 @agent_ppo2.py:185][0m |          -0.0014 |         257.6715 |        -231.9140 |
[32m[20221214 14:37:13 @agent_ppo2.py:185][0m |          -0.0034 |         256.8265 |        -231.5415 |
[32m[20221214 14:37:13 @agent_ppo2.py:185][0m |           0.0102 |         280.4807 |        -231.9922 |
[32m[20221214 14:37:13 @agent_ppo2.py:185][0m |          -0.0018 |         256.9338 |        -232.5768 |
[32m[20221214 14:37:14 @agent_ppo2.py:185][0m |           0.0119 |         290.0283 |        -232.4314 |
[32m[20221214 14:37:14 @agent_ppo2.py:185][0m |          -0.0031 |         255.7742 |        -232.6876 |
[32m[20221214 14:37:14 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:37:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.00
[32m[20221214 14:37:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.82
[32m[20221214 14:37:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.45
[32m[20221214 14:37:14 @agent_ppo2.py:143][0m Total time:      39.20 min
[32m[20221214 14:37:14 @agent_ppo2.py:145][0m 3608576 total steps have happened
[32m[20221214 14:37:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1762 --------------------------#
[32m[20221214 14:37:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:14 @agent_ppo2.py:185][0m |          -0.0013 |         239.5404 |        -224.0866 |
[32m[20221214 14:37:14 @agent_ppo2.py:185][0m |          -0.0025 |         234.4318 |        -223.6760 |
[32m[20221214 14:37:14 @agent_ppo2.py:185][0m |          -0.0007 |         232.3889 |        -224.1147 |
[32m[20221214 14:37:14 @agent_ppo2.py:185][0m |           0.0080 |         240.3685 |        -223.9044 |
[32m[20221214 14:37:15 @agent_ppo2.py:185][0m |           0.0006 |         230.7803 |        -223.5307 |
[32m[20221214 14:37:15 @agent_ppo2.py:185][0m |           0.0011 |         230.3614 |        -223.3297 |
[32m[20221214 14:37:15 @agent_ppo2.py:185][0m |           0.0057 |         230.4638 |        -223.8974 |
[32m[20221214 14:37:15 @agent_ppo2.py:185][0m |          -0.0004 |         227.5288 |        -223.6413 |
[32m[20221214 14:37:15 @agent_ppo2.py:185][0m |          -0.0006 |         226.8146 |        -223.6942 |
[32m[20221214 14:37:15 @agent_ppo2.py:185][0m |           0.0010 |         228.8873 |        -223.7856 |
[32m[20221214 14:37:15 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:37:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.83
[32m[20221214 14:37:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.82
[32m[20221214 14:37:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.86
[32m[20221214 14:37:15 @agent_ppo2.py:143][0m Total time:      39.22 min
[32m[20221214 14:37:15 @agent_ppo2.py:145][0m 3610624 total steps have happened
[32m[20221214 14:37:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1763 --------------------------#
[32m[20221214 14:37:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:16 @agent_ppo2.py:185][0m |          -0.0005 |         237.1584 |        -229.5711 |
[32m[20221214 14:37:16 @agent_ppo2.py:185][0m |           0.0016 |         230.8692 |        -228.5064 |
[32m[20221214 14:37:16 @agent_ppo2.py:185][0m |           0.0064 |         234.7445 |        -228.8308 |
[32m[20221214 14:37:16 @agent_ppo2.py:185][0m |           0.0013 |         224.2343 |        -229.5797 |
[32m[20221214 14:37:16 @agent_ppo2.py:185][0m |          -0.0015 |         224.6854 |        -229.3846 |
[32m[20221214 14:37:16 @agent_ppo2.py:185][0m |          -0.0005 |         220.6152 |        -229.0763 |
[32m[20221214 14:37:16 @agent_ppo2.py:185][0m |          -0.0007 |         218.0932 |        -229.2844 |
[32m[20221214 14:37:16 @agent_ppo2.py:185][0m |           0.0003 |         217.5778 |        -229.6515 |
[32m[20221214 14:37:16 @agent_ppo2.py:185][0m |           0.0019 |         219.3908 |        -228.9587 |
[32m[20221214 14:37:16 @agent_ppo2.py:185][0m |          -0.0028 |         214.5264 |        -229.2864 |
[32m[20221214 14:37:16 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:37:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.48
[32m[20221214 14:37:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.35
[32m[20221214 14:37:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.52
[32m[20221214 14:37:17 @agent_ppo2.py:143][0m Total time:      39.24 min
[32m[20221214 14:37:17 @agent_ppo2.py:145][0m 3612672 total steps have happened
[32m[20221214 14:37:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1764 --------------------------#
[32m[20221214 14:37:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:17 @agent_ppo2.py:185][0m |          -0.0017 |         219.8667 |        -230.6005 |
[32m[20221214 14:37:17 @agent_ppo2.py:185][0m |          -0.0008 |         207.7533 |        -230.2749 |
[32m[20221214 14:37:17 @agent_ppo2.py:185][0m |           0.0040 |         204.3545 |        -230.9877 |
[32m[20221214 14:37:17 @agent_ppo2.py:185][0m |           0.0081 |         211.6868 |        -230.1667 |
[32m[20221214 14:37:17 @agent_ppo2.py:185][0m |          -0.0022 |         199.2550 |        -230.6871 |
[32m[20221214 14:37:17 @agent_ppo2.py:185][0m |          -0.0029 |         198.8153 |        -230.7944 |
[32m[20221214 14:37:17 @agent_ppo2.py:185][0m |           0.0091 |         219.5078 |        -230.6058 |
[32m[20221214 14:37:18 @agent_ppo2.py:185][0m |           0.0118 |         209.6718 |        -230.4239 |
[32m[20221214 14:37:18 @agent_ppo2.py:185][0m |          -0.0013 |         196.2311 |        -230.5092 |
[32m[20221214 14:37:18 @agent_ppo2.py:185][0m |          -0.0004 |         195.8389 |        -230.5123 |
[32m[20221214 14:37:18 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:37:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.18
[32m[20221214 14:37:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.71
[32m[20221214 14:37:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.87
[32m[20221214 14:37:18 @agent_ppo2.py:143][0m Total time:      39.26 min
[32m[20221214 14:37:18 @agent_ppo2.py:145][0m 3614720 total steps have happened
[32m[20221214 14:37:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1765 --------------------------#
[32m[20221214 14:37:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:18 @agent_ppo2.py:185][0m |          -0.0036 |         212.6404 |        -228.5341 |
[32m[20221214 14:37:18 @agent_ppo2.py:185][0m |          -0.0040 |         202.2189 |        -228.1272 |
[32m[20221214 14:37:18 @agent_ppo2.py:185][0m |           0.0006 |         200.8173 |        -228.5273 |
[32m[20221214 14:37:19 @agent_ppo2.py:185][0m |           0.0051 |         206.6021 |        -228.3113 |
[32m[20221214 14:37:19 @agent_ppo2.py:185][0m |           0.0011 |         201.6932 |        -228.4988 |
[32m[20221214 14:37:19 @agent_ppo2.py:185][0m |          -0.0032 |         198.8195 |        -228.8364 |
[32m[20221214 14:37:19 @agent_ppo2.py:185][0m |          -0.0006 |         198.0164 |        -228.1437 |
[32m[20221214 14:37:19 @agent_ppo2.py:185][0m |          -0.0013 |         198.0352 |        -228.4558 |
[32m[20221214 14:37:19 @agent_ppo2.py:185][0m |           0.0101 |         203.6174 |        -228.4477 |
[32m[20221214 14:37:19 @agent_ppo2.py:185][0m |          -0.0004 |         197.8970 |        -228.3534 |
[32m[20221214 14:37:19 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:37:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.57
[32m[20221214 14:37:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.51
[32m[20221214 14:37:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.75
[32m[20221214 14:37:19 @agent_ppo2.py:143][0m Total time:      39.29 min
[32m[20221214 14:37:19 @agent_ppo2.py:145][0m 3616768 total steps have happened
[32m[20221214 14:37:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1766 --------------------------#
[32m[20221214 14:37:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:20 @agent_ppo2.py:185][0m |          -0.0044 |         202.4926 |        -228.0584 |
[32m[20221214 14:37:20 @agent_ppo2.py:185][0m |          -0.0062 |         175.6273 |        -228.1110 |
[32m[20221214 14:37:20 @agent_ppo2.py:185][0m |          -0.0055 |         171.9264 |        -228.0859 |
[32m[20221214 14:37:20 @agent_ppo2.py:185][0m |          -0.0057 |         169.8013 |        -227.7762 |
[32m[20221214 14:37:20 @agent_ppo2.py:185][0m |          -0.0050 |         168.9011 |        -227.8008 |
[32m[20221214 14:37:20 @agent_ppo2.py:185][0m |          -0.0014 |         167.1266 |        -227.8969 |
[32m[20221214 14:37:20 @agent_ppo2.py:185][0m |           0.0003 |         167.2959 |        -227.3394 |
[32m[20221214 14:37:20 @agent_ppo2.py:185][0m |           0.0102 |         180.4492 |        -227.3694 |
[32m[20221214 14:37:20 @agent_ppo2.py:185][0m |          -0.0056 |         165.8893 |        -227.6164 |
[32m[20221214 14:37:20 @agent_ppo2.py:185][0m |          -0.0052 |         164.9510 |        -227.0604 |
[32m[20221214 14:37:20 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:37:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.21
[32m[20221214 14:37:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.38
[32m[20221214 14:37:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.37
[32m[20221214 14:37:21 @agent_ppo2.py:143][0m Total time:      39.31 min
[32m[20221214 14:37:21 @agent_ppo2.py:145][0m 3618816 total steps have happened
[32m[20221214 14:37:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1767 --------------------------#
[32m[20221214 14:37:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:21 @agent_ppo2.py:185][0m |          -0.0008 |         213.0985 |        -230.1483 |
[32m[20221214 14:37:21 @agent_ppo2.py:185][0m |          -0.0034 |         192.2115 |        -229.5053 |
[32m[20221214 14:37:21 @agent_ppo2.py:185][0m |           0.0071 |         200.3140 |        -229.9959 |
[32m[20221214 14:37:21 @agent_ppo2.py:185][0m |          -0.0013 |         187.1284 |        -229.9858 |
[32m[20221214 14:37:21 @agent_ppo2.py:185][0m |          -0.0035 |         184.8979 |        -229.7314 |
[32m[20221214 14:37:21 @agent_ppo2.py:185][0m |          -0.0044 |         182.2823 |        -229.7578 |
[32m[20221214 14:37:22 @agent_ppo2.py:185][0m |           0.0068 |         202.5461 |        -229.0881 |
[32m[20221214 14:37:22 @agent_ppo2.py:185][0m |           0.0008 |         180.6995 |        -229.0697 |
[32m[20221214 14:37:22 @agent_ppo2.py:185][0m |           0.0003 |         179.0778 |        -229.3852 |
[32m[20221214 14:37:22 @agent_ppo2.py:185][0m |          -0.0004 |         178.3282 |        -229.0335 |
[32m[20221214 14:37:22 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:37:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.26
[32m[20221214 14:37:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.31
[32m[20221214 14:37:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.74
[32m[20221214 14:37:22 @agent_ppo2.py:143][0m Total time:      39.33 min
[32m[20221214 14:37:22 @agent_ppo2.py:145][0m 3620864 total steps have happened
[32m[20221214 14:37:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1768 --------------------------#
[32m[20221214 14:37:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:22 @agent_ppo2.py:185][0m |          -0.0003 |         219.9434 |        -225.8139 |
[32m[20221214 14:37:22 @agent_ppo2.py:185][0m |           0.0011 |         211.3603 |        -226.9307 |
[32m[20221214 14:37:23 @agent_ppo2.py:185][0m |           0.0036 |         214.7190 |        -226.5511 |
[32m[20221214 14:37:23 @agent_ppo2.py:185][0m |          -0.0007 |         208.4323 |        -226.7816 |
[32m[20221214 14:37:23 @agent_ppo2.py:185][0m |           0.0049 |         215.8038 |        -226.8223 |
[32m[20221214 14:37:23 @agent_ppo2.py:185][0m |          -0.0058 |         207.2390 |        -226.5535 |
[32m[20221214 14:37:23 @agent_ppo2.py:185][0m |          -0.0029 |         205.9844 |        -226.7620 |
[32m[20221214 14:37:23 @agent_ppo2.py:185][0m |           0.0043 |         220.0159 |        -226.4073 |
[32m[20221214 14:37:23 @agent_ppo2.py:185][0m |           0.0009 |         205.4240 |        -226.7832 |
[32m[20221214 14:37:23 @agent_ppo2.py:185][0m |          -0.0031 |         205.8730 |        -226.9689 |
[32m[20221214 14:37:23 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:37:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.48
[32m[20221214 14:37:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.14
[32m[20221214 14:37:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.79
[32m[20221214 14:37:23 @agent_ppo2.py:143][0m Total time:      39.36 min
[32m[20221214 14:37:23 @agent_ppo2.py:145][0m 3622912 total steps have happened
[32m[20221214 14:37:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1769 --------------------------#
[32m[20221214 14:37:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:24 @agent_ppo2.py:185][0m |          -0.0006 |         215.8634 |        -227.4438 |
[32m[20221214 14:37:24 @agent_ppo2.py:185][0m |          -0.0028 |         211.8859 |        -227.5644 |
[32m[20221214 14:37:24 @agent_ppo2.py:185][0m |          -0.0008 |         209.4281 |        -227.9821 |
[32m[20221214 14:37:24 @agent_ppo2.py:185][0m |          -0.0017 |         209.9824 |        -227.3456 |
[32m[20221214 14:37:24 @agent_ppo2.py:185][0m |           0.0104 |         232.5737 |        -227.9851 |
[32m[20221214 14:37:24 @agent_ppo2.py:185][0m |          -0.0025 |         207.7236 |        -228.1588 |
[32m[20221214 14:37:24 @agent_ppo2.py:185][0m |          -0.0005 |         205.8404 |        -227.8168 |
[32m[20221214 14:37:24 @agent_ppo2.py:185][0m |           0.0047 |         214.0505 |        -228.0985 |
[32m[20221214 14:37:25 @agent_ppo2.py:185][0m |          -0.0036 |         204.7797 |        -228.1073 |
[32m[20221214 14:37:25 @agent_ppo2.py:185][0m |          -0.0039 |         203.6749 |        -228.3774 |
[32m[20221214 14:37:25 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:37:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.96
[32m[20221214 14:37:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.22
[32m[20221214 14:37:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.99
[32m[20221214 14:37:25 @agent_ppo2.py:143][0m Total time:      39.38 min
[32m[20221214 14:37:25 @agent_ppo2.py:145][0m 3624960 total steps have happened
[32m[20221214 14:37:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1770 --------------------------#
[32m[20221214 14:37:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:37:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:25 @agent_ppo2.py:185][0m |           0.0019 |         229.5561 |        -231.8456 |
[32m[20221214 14:37:25 @agent_ppo2.py:185][0m |          -0.0002 |         225.0204 |        -231.8060 |
[32m[20221214 14:37:25 @agent_ppo2.py:185][0m |          -0.0038 |         223.9216 |        -231.6348 |
[32m[20221214 14:37:25 @agent_ppo2.py:185][0m |           0.0078 |         236.7440 |        -232.3106 |
[32m[20221214 14:37:26 @agent_ppo2.py:185][0m |          -0.0005 |         222.1575 |        -231.3097 |
[32m[20221214 14:37:26 @agent_ppo2.py:185][0m |          -0.0002 |         221.1399 |        -232.3361 |
[32m[20221214 14:37:26 @agent_ppo2.py:185][0m |           0.0018 |         220.8888 |        -231.7126 |
[32m[20221214 14:37:26 @agent_ppo2.py:185][0m |           0.0001 |         221.0812 |        -231.6616 |
[32m[20221214 14:37:26 @agent_ppo2.py:185][0m |          -0.0018 |         219.9523 |        -232.3148 |
[32m[20221214 14:37:26 @agent_ppo2.py:185][0m |           0.0120 |         255.2544 |        -232.0281 |
[32m[20221214 14:37:26 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:37:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.34
[32m[20221214 14:37:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.63
[32m[20221214 14:37:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.24
[32m[20221214 14:37:26 @agent_ppo2.py:143][0m Total time:      39.40 min
[32m[20221214 14:37:26 @agent_ppo2.py:145][0m 3627008 total steps have happened
[32m[20221214 14:37:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1771 --------------------------#
[32m[20221214 14:37:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:27 @agent_ppo2.py:185][0m |          -0.0012 |         219.9268 |        -232.0595 |
[32m[20221214 14:37:27 @agent_ppo2.py:185][0m |          -0.0029 |         215.2877 |        -232.0794 |
[32m[20221214 14:37:27 @agent_ppo2.py:185][0m |           0.0037 |         217.1778 |        -231.1320 |
[32m[20221214 14:37:27 @agent_ppo2.py:185][0m |           0.0092 |         220.8093 |        -231.1605 |
[32m[20221214 14:37:27 @agent_ppo2.py:185][0m |          -0.0025 |         212.2394 |        -231.6354 |
[32m[20221214 14:37:27 @agent_ppo2.py:185][0m |          -0.0009 |         210.8698 |        -230.7105 |
[32m[20221214 14:37:27 @agent_ppo2.py:185][0m |           0.0002 |         210.2545 |        -231.2658 |
[32m[20221214 14:37:27 @agent_ppo2.py:185][0m |          -0.0015 |         210.0930 |        -231.7245 |
[32m[20221214 14:37:27 @agent_ppo2.py:185][0m |          -0.0032 |         209.7759 |        -230.7095 |
[32m[20221214 14:37:27 @agent_ppo2.py:185][0m |          -0.0019 |         209.3104 |        -230.6257 |
[32m[20221214 14:37:27 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:37:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.29
[32m[20221214 14:37:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.89
[32m[20221214 14:37:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.13
[32m[20221214 14:37:28 @agent_ppo2.py:143][0m Total time:      39.42 min
[32m[20221214 14:37:28 @agent_ppo2.py:145][0m 3629056 total steps have happened
[32m[20221214 14:37:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1772 --------------------------#
[32m[20221214 14:37:28 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:37:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:28 @agent_ppo2.py:185][0m |           0.0070 |         237.1017 |        -234.4955 |
[32m[20221214 14:37:28 @agent_ppo2.py:185][0m |          -0.0005 |         214.4656 |        -234.0467 |
[32m[20221214 14:37:28 @agent_ppo2.py:185][0m |          -0.0022 |         212.7439 |        -234.2138 |
[32m[20221214 14:37:28 @agent_ppo2.py:185][0m |          -0.0007 |         210.9111 |        -234.3492 |
[32m[20221214 14:37:28 @agent_ppo2.py:185][0m |          -0.0028 |         210.9110 |        -235.0119 |
[32m[20221214 14:37:28 @agent_ppo2.py:185][0m |          -0.0033 |         210.3407 |        -234.6729 |
[32m[20221214 14:37:28 @agent_ppo2.py:185][0m |           0.0023 |         210.6364 |        -235.1723 |
[32m[20221214 14:37:29 @agent_ppo2.py:185][0m |          -0.0009 |         209.2986 |        -234.9035 |
[32m[20221214 14:37:29 @agent_ppo2.py:185][0m |          -0.0017 |         208.4764 |        -234.9568 |
[32m[20221214 14:37:29 @agent_ppo2.py:185][0m |          -0.0020 |         208.0421 |        -234.8301 |
[32m[20221214 14:37:29 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:37:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.09
[32m[20221214 14:37:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.89
[32m[20221214 14:37:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.61
[32m[20221214 14:37:29 @agent_ppo2.py:143][0m Total time:      39.45 min
[32m[20221214 14:37:29 @agent_ppo2.py:145][0m 3631104 total steps have happened
[32m[20221214 14:37:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1773 --------------------------#
[32m[20221214 14:37:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:29 @agent_ppo2.py:185][0m |           0.0027 |         225.6057 |        -230.8605 |
[32m[20221214 14:37:29 @agent_ppo2.py:185][0m |          -0.0000 |         220.8572 |        -230.3671 |
[32m[20221214 14:37:29 @agent_ppo2.py:185][0m |          -0.0014 |         216.6598 |        -230.4968 |
[32m[20221214 14:37:29 @agent_ppo2.py:185][0m |          -0.0028 |         214.6483 |        -230.1795 |
[32m[20221214 14:37:30 @agent_ppo2.py:185][0m |           0.0014 |         215.6330 |        -229.7968 |
[32m[20221214 14:37:30 @agent_ppo2.py:185][0m |          -0.0039 |         211.6420 |        -229.6089 |
[32m[20221214 14:37:30 @agent_ppo2.py:185][0m |          -0.0030 |         210.3563 |        -229.6269 |
[32m[20221214 14:37:30 @agent_ppo2.py:185][0m |          -0.0013 |         209.7844 |        -229.4421 |
[32m[20221214 14:37:30 @agent_ppo2.py:185][0m |          -0.0014 |         209.5180 |        -229.2945 |
[32m[20221214 14:37:30 @agent_ppo2.py:185][0m |          -0.0033 |         208.7409 |        -229.0487 |
[32m[20221214 14:37:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:37:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.70
[32m[20221214 14:37:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.70
[32m[20221214 14:37:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.14
[32m[20221214 14:37:30 @agent_ppo2.py:143][0m Total time:      39.47 min
[32m[20221214 14:37:30 @agent_ppo2.py:145][0m 3633152 total steps have happened
[32m[20221214 14:37:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1774 --------------------------#
[32m[20221214 14:37:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:30 @agent_ppo2.py:185][0m |           0.0012 |         210.4527 |        -230.1769 |
[32m[20221214 14:37:31 @agent_ppo2.py:185][0m |          -0.0012 |         206.6125 |        -230.2882 |
[32m[20221214 14:37:31 @agent_ppo2.py:185][0m |          -0.0021 |         206.7538 |        -230.3159 |
[32m[20221214 14:37:31 @agent_ppo2.py:185][0m |          -0.0026 |         203.8500 |        -230.6194 |
[32m[20221214 14:37:31 @agent_ppo2.py:185][0m |           0.0012 |         203.8636 |        -230.7253 |
[32m[20221214 14:37:31 @agent_ppo2.py:185][0m |          -0.0014 |         203.5626 |        -230.8206 |
[32m[20221214 14:37:31 @agent_ppo2.py:185][0m |           0.0076 |         213.4088 |        -231.0894 |
[32m[20221214 14:37:31 @agent_ppo2.py:185][0m |           0.0052 |         216.2904 |        -231.5252 |
[32m[20221214 14:37:31 @agent_ppo2.py:185][0m |          -0.0041 |         203.3449 |        -231.3540 |
[32m[20221214 14:37:31 @agent_ppo2.py:185][0m |          -0.0034 |         202.1443 |        -231.5119 |
[32m[20221214 14:37:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:37:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.55
[32m[20221214 14:37:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.94
[32m[20221214 14:37:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.49
[32m[20221214 14:37:31 @agent_ppo2.py:143][0m Total time:      39.49 min
[32m[20221214 14:37:31 @agent_ppo2.py:145][0m 3635200 total steps have happened
[32m[20221214 14:37:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1775 --------------------------#
[32m[20221214 14:37:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:32 @agent_ppo2.py:185][0m |          -0.0034 |         216.8690 |        -228.8794 |
[32m[20221214 14:37:32 @agent_ppo2.py:185][0m |           0.0028 |         209.9394 |        -229.1307 |
[32m[20221214 14:37:32 @agent_ppo2.py:185][0m |          -0.0044 |         207.9202 |        -229.3302 |
[32m[20221214 14:37:32 @agent_ppo2.py:185][0m |          -0.0039 |         207.3803 |        -229.7838 |
[32m[20221214 14:37:32 @agent_ppo2.py:185][0m |          -0.0035 |         206.9904 |        -229.6182 |
[32m[20221214 14:37:32 @agent_ppo2.py:185][0m |          -0.0034 |         205.5460 |        -229.8394 |
[32m[20221214 14:37:32 @agent_ppo2.py:185][0m |           0.0011 |         204.7359 |        -230.3202 |
[32m[20221214 14:37:32 @agent_ppo2.py:185][0m |           0.0188 |         221.6507 |        -230.1223 |
[32m[20221214 14:37:32 @agent_ppo2.py:185][0m |           0.0047 |         203.9602 |        -230.1940 |
[32m[20221214 14:37:33 @agent_ppo2.py:185][0m |          -0.0048 |         204.8133 |        -230.2850 |
[32m[20221214 14:37:33 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:37:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.52
[32m[20221214 14:37:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.24
[32m[20221214 14:37:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.42
[32m[20221214 14:37:33 @agent_ppo2.py:143][0m Total time:      39.51 min
[32m[20221214 14:37:33 @agent_ppo2.py:145][0m 3637248 total steps have happened
[32m[20221214 14:37:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1776 --------------------------#
[32m[20221214 14:37:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:33 @agent_ppo2.py:185][0m |           0.0206 |         276.5933 |        -234.2207 |
[32m[20221214 14:37:33 @agent_ppo2.py:185][0m |           0.0086 |         246.0171 |        -234.4098 |
[32m[20221214 14:37:33 @agent_ppo2.py:185][0m |           0.0015 |         232.8699 |        -234.5298 |
[32m[20221214 14:37:33 @agent_ppo2.py:185][0m |          -0.0005 |         229.3829 |        -233.5547 |
[32m[20221214 14:37:33 @agent_ppo2.py:185][0m |           0.0000 |         228.8166 |        -234.2298 |
[32m[20221214 14:37:33 @agent_ppo2.py:185][0m |          -0.0002 |         227.0543 |        -234.9202 |
[32m[20221214 14:37:34 @agent_ppo2.py:185][0m |          -0.0025 |         226.8885 |        -234.9515 |
[32m[20221214 14:37:34 @agent_ppo2.py:185][0m |           0.0058 |         231.1610 |        -235.0492 |
[32m[20221214 14:37:34 @agent_ppo2.py:185][0m |          -0.0006 |         225.0196 |        -235.0360 |
[32m[20221214 14:37:34 @agent_ppo2.py:185][0m |           0.0037 |         227.7186 |        -235.1065 |
[32m[20221214 14:37:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:37:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.53
[32m[20221214 14:37:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.44
[32m[20221214 14:37:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.76
[32m[20221214 14:37:34 @agent_ppo2.py:143][0m Total time:      39.53 min
[32m[20221214 14:37:34 @agent_ppo2.py:145][0m 3639296 total steps have happened
[32m[20221214 14:37:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1777 --------------------------#
[32m[20221214 14:37:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:34 @agent_ppo2.py:185][0m |          -0.0020 |         237.9936 |        -236.7725 |
[32m[20221214 14:37:34 @agent_ppo2.py:185][0m |           0.0021 |         234.8429 |        -236.9423 |
[32m[20221214 14:37:34 @agent_ppo2.py:185][0m |          -0.0036 |         227.8663 |        -236.1689 |
[32m[20221214 14:37:35 @agent_ppo2.py:185][0m |           0.0060 |         245.7597 |        -236.4169 |
[32m[20221214 14:37:35 @agent_ppo2.py:185][0m |          -0.0023 |         224.1144 |        -236.4582 |
[32m[20221214 14:37:35 @agent_ppo2.py:185][0m |          -0.0045 |         221.7160 |        -235.8614 |
[32m[20221214 14:37:35 @agent_ppo2.py:185][0m |          -0.0010 |         220.7344 |        -236.5852 |
[32m[20221214 14:37:35 @agent_ppo2.py:185][0m |          -0.0031 |         220.2430 |        -236.0898 |
[32m[20221214 14:37:35 @agent_ppo2.py:185][0m |           0.0089 |         246.1493 |        -236.1604 |
[32m[20221214 14:37:35 @agent_ppo2.py:185][0m |          -0.0042 |         218.8811 |        -235.6241 |
[32m[20221214 14:37:35 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:37:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.01
[32m[20221214 14:37:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.56
[32m[20221214 14:37:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.91
[32m[20221214 14:37:35 @agent_ppo2.py:143][0m Total time:      39.55 min
[32m[20221214 14:37:35 @agent_ppo2.py:145][0m 3641344 total steps have happened
[32m[20221214 14:37:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1778 --------------------------#
[32m[20221214 14:37:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:35 @agent_ppo2.py:185][0m |           0.0125 |         297.5402 |        -234.6506 |
[32m[20221214 14:37:36 @agent_ppo2.py:185][0m |           0.0005 |         258.5926 |        -234.2983 |
[32m[20221214 14:37:36 @agent_ppo2.py:185][0m |          -0.0007 |         256.0088 |        -234.4774 |
[32m[20221214 14:37:36 @agent_ppo2.py:185][0m |           0.0099 |         270.5886 |        -234.8096 |
[32m[20221214 14:37:36 @agent_ppo2.py:185][0m |          -0.0026 |         254.3124 |        -234.4827 |
[32m[20221214 14:37:36 @agent_ppo2.py:185][0m |          -0.0014 |         253.1501 |        -234.7043 |
[32m[20221214 14:37:36 @agent_ppo2.py:185][0m |          -0.0014 |         252.2113 |        -234.3111 |
[32m[20221214 14:37:36 @agent_ppo2.py:185][0m |          -0.0028 |         251.8788 |        -234.8148 |
[32m[20221214 14:37:36 @agent_ppo2.py:185][0m |          -0.0013 |         250.6487 |        -233.9490 |
[32m[20221214 14:37:36 @agent_ppo2.py:185][0m |          -0.0041 |         250.9804 |        -233.8638 |
[32m[20221214 14:37:36 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:37:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.57
[32m[20221214 14:37:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.02
[32m[20221214 14:37:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.79
[32m[20221214 14:37:36 @agent_ppo2.py:143][0m Total time:      39.57 min
[32m[20221214 14:37:36 @agent_ppo2.py:145][0m 3643392 total steps have happened
[32m[20221214 14:37:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1779 --------------------------#
[32m[20221214 14:37:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:37 @agent_ppo2.py:185][0m |          -0.0019 |         257.7240 |        -236.2320 |
[32m[20221214 14:37:37 @agent_ppo2.py:185][0m |          -0.0009 |         255.7819 |        -236.8403 |
[32m[20221214 14:37:37 @agent_ppo2.py:185][0m |          -0.0017 |         257.0886 |        -236.3399 |
[32m[20221214 14:37:37 @agent_ppo2.py:185][0m |          -0.0015 |         255.1072 |        -236.9581 |
[32m[20221214 14:37:37 @agent_ppo2.py:185][0m |          -0.0038 |         254.6501 |        -236.6656 |
[32m[20221214 14:37:37 @agent_ppo2.py:185][0m |           0.0068 |         264.3343 |        -237.2226 |
[32m[20221214 14:37:37 @agent_ppo2.py:185][0m |          -0.0029 |         254.7991 |        -236.5304 |
[32m[20221214 14:37:37 @agent_ppo2.py:185][0m |          -0.0026 |         252.8634 |        -237.1784 |
[32m[20221214 14:37:37 @agent_ppo2.py:185][0m |          -0.0031 |         252.9942 |        -237.0526 |
[32m[20221214 14:37:37 @agent_ppo2.py:185][0m |          -0.0035 |         252.2581 |        -237.4756 |
[32m[20221214 14:37:37 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:37:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.37
[32m[20221214 14:37:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.94
[32m[20221214 14:37:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.18
[32m[20221214 14:37:38 @agent_ppo2.py:143][0m Total time:      39.59 min
[32m[20221214 14:37:38 @agent_ppo2.py:145][0m 3645440 total steps have happened
[32m[20221214 14:37:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1780 --------------------------#
[32m[20221214 14:37:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:37:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:38 @agent_ppo2.py:185][0m |          -0.0010 |         237.8847 |        -240.6943 |
[32m[20221214 14:37:38 @agent_ppo2.py:185][0m |          -0.0011 |         234.6815 |        -240.1127 |
[32m[20221214 14:37:38 @agent_ppo2.py:185][0m |           0.0027 |         232.4657 |        -239.7525 |
[32m[20221214 14:37:38 @agent_ppo2.py:185][0m |          -0.0004 |         230.0544 |        -239.5451 |
[32m[20221214 14:37:38 @agent_ppo2.py:185][0m |          -0.0015 |         228.9305 |        -240.4507 |
[32m[20221214 14:37:38 @agent_ppo2.py:185][0m |          -0.0008 |         227.2686 |        -240.2763 |
[32m[20221214 14:37:38 @agent_ppo2.py:185][0m |          -0.0010 |         227.5009 |        -240.9456 |
[32m[20221214 14:37:39 @agent_ppo2.py:185][0m |          -0.0019 |         227.5196 |        -240.4487 |
[32m[20221214 14:37:39 @agent_ppo2.py:185][0m |          -0.0004 |         225.9397 |        -239.9761 |
[32m[20221214 14:37:39 @agent_ppo2.py:185][0m |          -0.0013 |         224.6312 |        -240.1967 |
[32m[20221214 14:37:39 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:37:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.04
[32m[20221214 14:37:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.26
[32m[20221214 14:37:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.74
[32m[20221214 14:37:39 @agent_ppo2.py:143][0m Total time:      39.61 min
[32m[20221214 14:37:39 @agent_ppo2.py:145][0m 3647488 total steps have happened
[32m[20221214 14:37:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1781 --------------------------#
[32m[20221214 14:37:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:39 @agent_ppo2.py:185][0m |          -0.0007 |         246.7132 |        -238.2054 |
[32m[20221214 14:37:39 @agent_ppo2.py:185][0m |          -0.0001 |         243.2183 |        -237.3663 |
[32m[20221214 14:37:39 @agent_ppo2.py:185][0m |          -0.0003 |         243.2209 |        -238.1154 |
[32m[20221214 14:37:39 @agent_ppo2.py:185][0m |          -0.0018 |         241.5723 |        -237.8104 |
[32m[20221214 14:37:39 @agent_ppo2.py:185][0m |          -0.0011 |         241.3962 |        -237.9553 |
[32m[20221214 14:37:40 @agent_ppo2.py:185][0m |          -0.0032 |         240.5924 |        -238.2633 |
[32m[20221214 14:37:40 @agent_ppo2.py:185][0m |          -0.0023 |         239.7787 |        -237.9987 |
[32m[20221214 14:37:40 @agent_ppo2.py:185][0m |          -0.0025 |         239.8294 |        -238.2095 |
[32m[20221214 14:37:40 @agent_ppo2.py:185][0m |          -0.0028 |         238.9332 |        -238.4260 |
[32m[20221214 14:37:40 @agent_ppo2.py:185][0m |          -0.0030 |         239.9183 |        -237.5826 |
[32m[20221214 14:37:40 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:37:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.81
[32m[20221214 14:37:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.91
[32m[20221214 14:37:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.35
[32m[20221214 14:37:40 @agent_ppo2.py:143][0m Total time:      39.63 min
[32m[20221214 14:37:40 @agent_ppo2.py:145][0m 3649536 total steps have happened
[32m[20221214 14:37:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1782 --------------------------#
[32m[20221214 14:37:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:40 @agent_ppo2.py:185][0m |          -0.0015 |         253.0262 |        -239.0889 |
[32m[20221214 14:37:40 @agent_ppo2.py:185][0m |          -0.0006 |         250.9390 |        -239.2125 |
[32m[20221214 14:37:41 @agent_ppo2.py:185][0m |          -0.0014 |         247.9204 |        -239.5622 |
[32m[20221214 14:37:41 @agent_ppo2.py:185][0m |          -0.0028 |         245.7077 |        -239.9735 |
[32m[20221214 14:37:41 @agent_ppo2.py:185][0m |          -0.0017 |         245.9496 |        -239.9924 |
[32m[20221214 14:37:41 @agent_ppo2.py:185][0m |           0.0012 |         247.3985 |        -240.0432 |
[32m[20221214 14:37:41 @agent_ppo2.py:185][0m |           0.0082 |         265.7687 |        -240.3639 |
[32m[20221214 14:37:41 @agent_ppo2.py:185][0m |           0.0141 |         274.3405 |        -240.9315 |
[32m[20221214 14:37:41 @agent_ppo2.py:185][0m |          -0.0016 |         244.7361 |        -241.5305 |
[32m[20221214 14:37:41 @agent_ppo2.py:185][0m |          -0.0029 |         243.6154 |        -241.8994 |
[32m[20221214 14:37:41 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:37:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.47
[32m[20221214 14:37:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.36
[32m[20221214 14:37:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.40
[32m[20221214 14:37:41 @agent_ppo2.py:143][0m Total time:      39.65 min
[32m[20221214 14:37:41 @agent_ppo2.py:145][0m 3651584 total steps have happened
[32m[20221214 14:37:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1783 --------------------------#
[32m[20221214 14:37:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:42 @agent_ppo2.py:185][0m |           0.0058 |         277.8828 |        -243.1265 |
[32m[20221214 14:37:42 @agent_ppo2.py:185][0m |          -0.0044 |         269.5373 |        -243.3122 |
[32m[20221214 14:37:42 @agent_ppo2.py:185][0m |          -0.0010 |         267.8802 |        -244.0298 |
[32m[20221214 14:37:42 @agent_ppo2.py:185][0m |          -0.0023 |         267.4119 |        -244.1005 |
[32m[20221214 14:37:42 @agent_ppo2.py:185][0m |           0.0011 |         268.2432 |        -244.0223 |
[32m[20221214 14:37:42 @agent_ppo2.py:185][0m |          -0.0025 |         266.9680 |        -243.8799 |
[32m[20221214 14:37:42 @agent_ppo2.py:185][0m |          -0.0019 |         266.4763 |        -243.9455 |
[32m[20221214 14:37:42 @agent_ppo2.py:185][0m |          -0.0039 |         266.5296 |        -244.3822 |
[32m[20221214 14:37:42 @agent_ppo2.py:185][0m |          -0.0045 |         266.3515 |        -244.5962 |
[32m[20221214 14:37:42 @agent_ppo2.py:185][0m |          -0.0040 |         267.2077 |        -244.7681 |
[32m[20221214 14:37:42 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:37:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.62
[32m[20221214 14:37:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.96
[32m[20221214 14:37:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.35
[32m[20221214 14:37:43 @agent_ppo2.py:143][0m Total time:      39.67 min
[32m[20221214 14:37:43 @agent_ppo2.py:145][0m 3653632 total steps have happened
[32m[20221214 14:37:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1784 --------------------------#
[32m[20221214 14:37:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:43 @agent_ppo2.py:185][0m |          -0.0001 |         294.6654 |        -243.6751 |
[32m[20221214 14:37:43 @agent_ppo2.py:185][0m |          -0.0008 |         289.8841 |        -243.5785 |
[32m[20221214 14:37:43 @agent_ppo2.py:185][0m |          -0.0016 |         289.6172 |        -243.3579 |
[32m[20221214 14:37:43 @agent_ppo2.py:185][0m |          -0.0018 |         287.0194 |        -244.5216 |
[32m[20221214 14:37:43 @agent_ppo2.py:185][0m |          -0.0004 |         287.2377 |        -243.8459 |
[32m[20221214 14:37:43 @agent_ppo2.py:185][0m |          -0.0025 |         285.5818 |        -244.0674 |
[32m[20221214 14:37:43 @agent_ppo2.py:185][0m |          -0.0018 |         286.2642 |        -244.1160 |
[32m[20221214 14:37:43 @agent_ppo2.py:185][0m |          -0.0026 |         285.4516 |        -244.3232 |
[32m[20221214 14:37:43 @agent_ppo2.py:185][0m |          -0.0019 |         283.7839 |        -243.2981 |
[32m[20221214 14:37:44 @agent_ppo2.py:185][0m |           0.0049 |         292.0072 |        -244.2359 |
[32m[20221214 14:37:44 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:37:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.51
[32m[20221214 14:37:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.81
[32m[20221214 14:37:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.10
[32m[20221214 14:37:44 @agent_ppo2.py:143][0m Total time:      39.69 min
[32m[20221214 14:37:44 @agent_ppo2.py:145][0m 3655680 total steps have happened
[32m[20221214 14:37:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1785 --------------------------#
[32m[20221214 14:37:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:44 @agent_ppo2.py:185][0m |          -0.0010 |         312.8751 |        -246.5173 |
[32m[20221214 14:37:44 @agent_ppo2.py:185][0m |          -0.0013 |         309.9888 |        -245.8545 |
[32m[20221214 14:37:44 @agent_ppo2.py:185][0m |          -0.0009 |         310.5441 |        -246.6885 |
[32m[20221214 14:37:44 @agent_ppo2.py:185][0m |           0.0001 |         309.6530 |        -246.4100 |
[32m[20221214 14:37:44 @agent_ppo2.py:185][0m |          -0.0001 |         307.9924 |        -246.6487 |
[32m[20221214 14:37:44 @agent_ppo2.py:185][0m |          -0.0005 |         308.1440 |        -246.9751 |
[32m[20221214 14:37:45 @agent_ppo2.py:185][0m |          -0.0012 |         306.8285 |        -246.6975 |
[32m[20221214 14:37:45 @agent_ppo2.py:185][0m |          -0.0002 |         307.3367 |        -246.7714 |
[32m[20221214 14:37:45 @agent_ppo2.py:185][0m |          -0.0029 |         306.1404 |        -245.5220 |
[32m[20221214 14:37:45 @agent_ppo2.py:185][0m |           0.0008 |         306.8586 |        -246.5082 |
[32m[20221214 14:37:45 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221214 14:37:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.31
[32m[20221214 14:37:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.15
[32m[20221214 14:37:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.84
[32m[20221214 14:37:45 @agent_ppo2.py:143][0m Total time:      39.71 min
[32m[20221214 14:37:45 @agent_ppo2.py:145][0m 3657728 total steps have happened
[32m[20221214 14:37:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1786 --------------------------#
[32m[20221214 14:37:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:45 @agent_ppo2.py:185][0m |           0.0000 |         304.8077 |        -244.1902 |
[32m[20221214 14:37:45 @agent_ppo2.py:185][0m |           0.0082 |         313.5722 |        -244.4648 |
[32m[20221214 14:37:45 @agent_ppo2.py:185][0m |          -0.0021 |         298.6338 |        -245.0059 |
[32m[20221214 14:37:46 @agent_ppo2.py:185][0m |          -0.0002 |         299.6138 |        -242.3783 |
[32m[20221214 14:37:46 @agent_ppo2.py:185][0m |          -0.0012 |         296.8723 |        -244.0417 |
[32m[20221214 14:37:46 @agent_ppo2.py:185][0m |          -0.0008 |         296.3929 |        -244.9174 |
[32m[20221214 14:37:46 @agent_ppo2.py:185][0m |           0.0024 |         298.5030 |        -244.7379 |
[32m[20221214 14:37:46 @agent_ppo2.py:185][0m |          -0.0023 |         297.0876 |        -244.9486 |
[32m[20221214 14:37:46 @agent_ppo2.py:185][0m |          -0.0011 |         295.6715 |        -244.9379 |
[32m[20221214 14:37:46 @agent_ppo2.py:185][0m |           0.0037 |         297.7018 |        -245.2386 |
[32m[20221214 14:37:46 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:37:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.09
[32m[20221214 14:37:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.32
[32m[20221214 14:37:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.34
[32m[20221214 14:37:46 @agent_ppo2.py:143][0m Total time:      39.74 min
[32m[20221214 14:37:46 @agent_ppo2.py:145][0m 3659776 total steps have happened
[32m[20221214 14:37:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1787 --------------------------#
[32m[20221214 14:37:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:47 @agent_ppo2.py:185][0m |           0.0010 |         291.2114 |        -247.0967 |
[32m[20221214 14:37:47 @agent_ppo2.py:185][0m |          -0.0019 |         286.3076 |        -247.2371 |
[32m[20221214 14:37:47 @agent_ppo2.py:185][0m |           0.0002 |         286.5152 |        -247.0756 |
[32m[20221214 14:37:47 @agent_ppo2.py:185][0m |           0.0018 |         284.9981 |        -247.0295 |
[32m[20221214 14:37:47 @agent_ppo2.py:185][0m |          -0.0019 |         283.3724 |        -247.6131 |
[32m[20221214 14:37:47 @agent_ppo2.py:185][0m |           0.0035 |         287.9303 |        -247.1570 |
[32m[20221214 14:37:47 @agent_ppo2.py:185][0m |          -0.0027 |         281.8874 |        -247.4276 |
[32m[20221214 14:37:47 @agent_ppo2.py:185][0m |          -0.0025 |         282.6793 |        -248.0800 |
[32m[20221214 14:37:47 @agent_ppo2.py:185][0m |           0.0050 |         290.8205 |        -247.8243 |
[32m[20221214 14:37:47 @agent_ppo2.py:185][0m |          -0.0008 |         281.1927 |        -248.5127 |
[32m[20221214 14:37:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:37:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.30
[32m[20221214 14:37:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.92
[32m[20221214 14:37:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.34
[32m[20221214 14:37:47 @agent_ppo2.py:143][0m Total time:      39.76 min
[32m[20221214 14:37:47 @agent_ppo2.py:145][0m 3661824 total steps have happened
[32m[20221214 14:37:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1788 --------------------------#
[32m[20221214 14:37:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:48 @agent_ppo2.py:185][0m |          -0.0012 |         324.4860 |        -253.5467 |
[32m[20221214 14:37:48 @agent_ppo2.py:185][0m |           0.0010 |         320.6541 |        -253.2517 |
[32m[20221214 14:37:48 @agent_ppo2.py:185][0m |          -0.0012 |         317.6701 |        -253.0582 |
[32m[20221214 14:37:48 @agent_ppo2.py:185][0m |           0.0018 |         316.6874 |        -252.9546 |
[32m[20221214 14:37:48 @agent_ppo2.py:185][0m |          -0.0011 |         314.6912 |        -253.6042 |
[32m[20221214 14:37:48 @agent_ppo2.py:185][0m |           0.0053 |         324.2399 |        -253.6732 |
[32m[20221214 14:37:48 @agent_ppo2.py:185][0m |           0.0040 |         316.5594 |        -253.8624 |
[32m[20221214 14:37:48 @agent_ppo2.py:185][0m |          -0.0024 |         312.4018 |        -253.7438 |
[32m[20221214 14:37:48 @agent_ppo2.py:185][0m |           0.0042 |         316.5297 |        -254.2806 |
[32m[20221214 14:37:49 @agent_ppo2.py:185][0m |          -0.0019 |         311.0964 |        -253.6814 |
[32m[20221214 14:37:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:37:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.96
[32m[20221214 14:37:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.91
[32m[20221214 14:37:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.59
[32m[20221214 14:37:49 @agent_ppo2.py:143][0m Total time:      39.78 min
[32m[20221214 14:37:49 @agent_ppo2.py:145][0m 3663872 total steps have happened
[32m[20221214 14:37:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1789 --------------------------#
[32m[20221214 14:37:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:49 @agent_ppo2.py:185][0m |          -0.0005 |         292.7499 |        -249.5924 |
[32m[20221214 14:37:49 @agent_ppo2.py:185][0m |          -0.0044 |         290.3398 |        -249.9243 |
[32m[20221214 14:37:49 @agent_ppo2.py:185][0m |          -0.0006 |         290.0364 |        -250.0383 |
[32m[20221214 14:37:49 @agent_ppo2.py:185][0m |           0.0087 |         295.9812 |        -250.5355 |
[32m[20221214 14:37:49 @agent_ppo2.py:185][0m |          -0.0017 |         288.8187 |        -250.5934 |
[32m[20221214 14:37:49 @agent_ppo2.py:185][0m |          -0.0012 |         287.9212 |        -250.8031 |
[32m[20221214 14:37:50 @agent_ppo2.py:185][0m |          -0.0020 |         286.9559 |        -251.6101 |
[32m[20221214 14:37:50 @agent_ppo2.py:185][0m |           0.0043 |         298.7695 |        -251.1237 |
[32m[20221214 14:37:50 @agent_ppo2.py:185][0m |           0.0122 |         305.7794 |        -251.2878 |
[32m[20221214 14:37:50 @agent_ppo2.py:185][0m |          -0.0029 |         286.3775 |        -250.9631 |
[32m[20221214 14:37:50 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:37:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.34
[32m[20221214 14:37:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.24
[32m[20221214 14:37:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.37
[32m[20221214 14:37:50 @agent_ppo2.py:143][0m Total time:      39.80 min
[32m[20221214 14:37:50 @agent_ppo2.py:145][0m 3665920 total steps have happened
[32m[20221214 14:37:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1790 --------------------------#
[32m[20221214 14:37:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:37:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:50 @agent_ppo2.py:185][0m |          -0.0006 |         309.2049 |        -251.4859 |
[32m[20221214 14:37:50 @agent_ppo2.py:185][0m |          -0.0006 |         307.3745 |        -251.6445 |
[32m[20221214 14:37:51 @agent_ppo2.py:185][0m |           0.0028 |         310.2797 |        -252.1651 |
[32m[20221214 14:37:51 @agent_ppo2.py:185][0m |          -0.0025 |         302.3485 |        -252.2536 |
[32m[20221214 14:37:51 @agent_ppo2.py:185][0m |           0.0057 |         315.3357 |        -252.3110 |
[32m[20221214 14:37:51 @agent_ppo2.py:185][0m |          -0.0034 |         303.5176 |        -252.3946 |
[32m[20221214 14:37:51 @agent_ppo2.py:185][0m |          -0.0039 |         300.3596 |        -252.3843 |
[32m[20221214 14:37:51 @agent_ppo2.py:185][0m |          -0.0051 |         300.3673 |        -252.4675 |
[32m[20221214 14:37:51 @agent_ppo2.py:185][0m |          -0.0030 |         299.4112 |        -252.6944 |
[32m[20221214 14:37:51 @agent_ppo2.py:185][0m |           0.0058 |         320.2667 |        -252.6272 |
[32m[20221214 14:37:51 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:37:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.97
[32m[20221214 14:37:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.72
[32m[20221214 14:37:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.85
[32m[20221214 14:37:51 @agent_ppo2.py:143][0m Total time:      39.82 min
[32m[20221214 14:37:51 @agent_ppo2.py:145][0m 3667968 total steps have happened
[32m[20221214 14:37:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1791 --------------------------#
[32m[20221214 14:37:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:52 @agent_ppo2.py:185][0m |           0.0006 |         271.0859 |        -255.4240 |
[32m[20221214 14:37:52 @agent_ppo2.py:185][0m |          -0.0000 |         268.4459 |        -255.2281 |
[32m[20221214 14:37:52 @agent_ppo2.py:185][0m |          -0.0022 |         266.0690 |        -255.1873 |
[32m[20221214 14:37:52 @agent_ppo2.py:185][0m |          -0.0023 |         264.6186 |        -255.4486 |
[32m[20221214 14:37:52 @agent_ppo2.py:185][0m |          -0.0008 |         264.2080 |        -255.5278 |
[32m[20221214 14:37:52 @agent_ppo2.py:185][0m |           0.0071 |         272.8191 |        -256.0353 |
[32m[20221214 14:37:52 @agent_ppo2.py:185][0m |           0.0114 |         286.9211 |        -255.4167 |
[32m[20221214 14:37:52 @agent_ppo2.py:185][0m |          -0.0005 |         263.2642 |        -256.4708 |
[32m[20221214 14:37:52 @agent_ppo2.py:185][0m |           0.0045 |         266.4603 |        -255.8019 |
[32m[20221214 14:37:52 @agent_ppo2.py:185][0m |          -0.0012 |         263.1223 |        -256.4752 |
[32m[20221214 14:37:52 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:37:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.50
[32m[20221214 14:37:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.66
[32m[20221214 14:37:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.54
[32m[20221214 14:37:52 @agent_ppo2.py:143][0m Total time:      39.84 min
[32m[20221214 14:37:52 @agent_ppo2.py:145][0m 3670016 total steps have happened
[32m[20221214 14:37:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1792 --------------------------#
[32m[20221214 14:37:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:53 @agent_ppo2.py:185][0m |          -0.0005 |         265.5610 |        -257.3441 |
[32m[20221214 14:37:53 @agent_ppo2.py:185][0m |          -0.0045 |         259.0469 |        -257.9808 |
[32m[20221214 14:37:53 @agent_ppo2.py:185][0m |          -0.0033 |         258.2814 |        -257.5254 |
[32m[20221214 14:37:53 @agent_ppo2.py:185][0m |          -0.0048 |         255.6735 |        -258.0798 |
[32m[20221214 14:37:53 @agent_ppo2.py:185][0m |          -0.0044 |         256.7744 |        -258.3069 |
[32m[20221214 14:37:53 @agent_ppo2.py:185][0m |           0.0043 |         266.6653 |        -258.3603 |
[32m[20221214 14:37:53 @agent_ppo2.py:185][0m |           0.0001 |         259.8502 |        -258.6000 |
[32m[20221214 14:37:53 @agent_ppo2.py:185][0m |           0.0030 |         261.8755 |        -258.2317 |
[32m[20221214 14:37:54 @agent_ppo2.py:185][0m |           0.0047 |         265.0244 |        -258.7068 |
[32m[20221214 14:37:54 @agent_ppo2.py:185][0m |          -0.0052 |         253.8991 |        -258.7979 |
[32m[20221214 14:37:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:37:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.30
[32m[20221214 14:37:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.23
[32m[20221214 14:37:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.08
[32m[20221214 14:37:54 @agent_ppo2.py:143][0m Total time:      39.86 min
[32m[20221214 14:37:54 @agent_ppo2.py:145][0m 3672064 total steps have happened
[32m[20221214 14:37:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1793 --------------------------#
[32m[20221214 14:37:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:54 @agent_ppo2.py:185][0m |          -0.0019 |         262.3391 |        -264.2837 |
[32m[20221214 14:37:54 @agent_ppo2.py:185][0m |          -0.0038 |         259.1676 |        -264.2921 |
[32m[20221214 14:37:54 @agent_ppo2.py:185][0m |          -0.0024 |         258.4693 |        -264.2300 |
[32m[20221214 14:37:54 @agent_ppo2.py:185][0m |          -0.0047 |         256.2341 |        -264.7025 |
[32m[20221214 14:37:54 @agent_ppo2.py:185][0m |          -0.0020 |         256.7044 |        -264.7090 |
[32m[20221214 14:37:55 @agent_ppo2.py:185][0m |          -0.0019 |         255.5854 |        -264.4732 |
[32m[20221214 14:37:55 @agent_ppo2.py:185][0m |          -0.0045 |         254.8015 |        -265.2862 |
[32m[20221214 14:37:55 @agent_ppo2.py:185][0m |           0.0029 |         261.8644 |        -264.6947 |
[32m[20221214 14:37:55 @agent_ppo2.py:185][0m |          -0.0039 |         253.6760 |        -264.8837 |
[32m[20221214 14:37:55 @agent_ppo2.py:185][0m |          -0.0035 |         252.9978 |        -265.1736 |
[32m[20221214 14:37:55 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:37:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.91
[32m[20221214 14:37:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.94
[32m[20221214 14:37:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.49
[32m[20221214 14:37:55 @agent_ppo2.py:143][0m Total time:      39.88 min
[32m[20221214 14:37:55 @agent_ppo2.py:145][0m 3674112 total steps have happened
[32m[20221214 14:37:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1794 --------------------------#
[32m[20221214 14:37:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:37:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:55 @agent_ppo2.py:185][0m |          -0.0016 |         253.0905 |        -261.5110 |
[32m[20221214 14:37:55 @agent_ppo2.py:185][0m |          -0.0006 |         245.6497 |        -261.9489 |
[32m[20221214 14:37:56 @agent_ppo2.py:185][0m |           0.0013 |         242.8958 |        -261.4753 |
[32m[20221214 14:37:56 @agent_ppo2.py:185][0m |          -0.0007 |         240.6239 |        -261.5502 |
[32m[20221214 14:37:56 @agent_ppo2.py:185][0m |          -0.0003 |         239.2707 |        -261.9549 |
[32m[20221214 14:37:56 @agent_ppo2.py:185][0m |           0.0034 |         239.7919 |        -261.2626 |
[32m[20221214 14:37:56 @agent_ppo2.py:185][0m |          -0.0025 |         237.7339 |        -261.8179 |
[32m[20221214 14:37:56 @agent_ppo2.py:185][0m |           0.0028 |         239.0989 |        -261.9330 |
[32m[20221214 14:37:56 @agent_ppo2.py:185][0m |          -0.0010 |         237.1519 |        -261.7548 |
[32m[20221214 14:37:56 @agent_ppo2.py:185][0m |          -0.0005 |         237.4709 |        -262.3010 |
[32m[20221214 14:37:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:37:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.09
[32m[20221214 14:37:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.95
[32m[20221214 14:37:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.02
[32m[20221214 14:37:56 @agent_ppo2.py:143][0m Total time:      39.90 min
[32m[20221214 14:37:56 @agent_ppo2.py:145][0m 3676160 total steps have happened
[32m[20221214 14:37:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1795 --------------------------#
[32m[20221214 14:37:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:37:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:57 @agent_ppo2.py:185][0m |          -0.0007 |         245.8851 |        -267.7460 |
[32m[20221214 14:37:57 @agent_ppo2.py:185][0m |          -0.0003 |         241.5872 |        -267.8233 |
[32m[20221214 14:37:57 @agent_ppo2.py:185][0m |          -0.0005 |         240.5391 |        -268.1548 |
[32m[20221214 14:37:57 @agent_ppo2.py:185][0m |           0.0010 |         241.0795 |        -268.8923 |
[32m[20221214 14:37:57 @agent_ppo2.py:185][0m |          -0.0018 |         239.7174 |        -269.2911 |
[32m[20221214 14:37:57 @agent_ppo2.py:185][0m |           0.0005 |         240.0443 |        -268.9187 |
[32m[20221214 14:37:57 @agent_ppo2.py:185][0m |          -0.0002 |         241.2134 |        -269.1086 |
[32m[20221214 14:37:57 @agent_ppo2.py:185][0m |           0.0027 |         239.5410 |        -269.4207 |
[32m[20221214 14:37:57 @agent_ppo2.py:185][0m |          -0.0014 |         238.2368 |        -269.8989 |
[32m[20221214 14:37:58 @agent_ppo2.py:185][0m |          -0.0010 |         238.0166 |        -269.6594 |
[32m[20221214 14:37:58 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:37:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.65
[32m[20221214 14:37:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.38
[32m[20221214 14:37:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.10
[32m[20221214 14:37:58 @agent_ppo2.py:143][0m Total time:      39.93 min
[32m[20221214 14:37:58 @agent_ppo2.py:145][0m 3678208 total steps have happened
[32m[20221214 14:37:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1796 --------------------------#
[32m[20221214 14:37:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:58 @agent_ppo2.py:185][0m |          -0.0008 |         259.3229 |        -267.0372 |
[32m[20221214 14:37:58 @agent_ppo2.py:185][0m |           0.0037 |         257.0647 |        -266.9196 |
[32m[20221214 14:37:58 @agent_ppo2.py:185][0m |          -0.0028 |         253.4073 |        -266.5834 |
[32m[20221214 14:37:58 @agent_ppo2.py:185][0m |           0.0003 |         252.2199 |        -267.3840 |
[32m[20221214 14:37:58 @agent_ppo2.py:185][0m |          -0.0011 |         251.4493 |        -267.5699 |
[32m[20221214 14:37:59 @agent_ppo2.py:185][0m |          -0.0023 |         251.5284 |        -268.1069 |
[32m[20221214 14:37:59 @agent_ppo2.py:185][0m |          -0.0005 |         252.3131 |        -267.7108 |
[32m[20221214 14:37:59 @agent_ppo2.py:185][0m |           0.0113 |         279.1058 |        -268.0155 |
[32m[20221214 14:37:59 @agent_ppo2.py:185][0m |          -0.0018 |         251.9258 |        -267.5657 |
[32m[20221214 14:37:59 @agent_ppo2.py:185][0m |          -0.0001 |         249.3899 |        -268.4827 |
[32m[20221214 14:37:59 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:37:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.16
[32m[20221214 14:37:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.90
[32m[20221214 14:37:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.71
[32m[20221214 14:37:59 @agent_ppo2.py:143][0m Total time:      39.95 min
[32m[20221214 14:37:59 @agent_ppo2.py:145][0m 3680256 total steps have happened
[32m[20221214 14:37:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1797 --------------------------#
[32m[20221214 14:37:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:37:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:37:59 @agent_ppo2.py:185][0m |           0.0019 |         258.2631 |        -267.7348 |
[32m[20221214 14:38:00 @agent_ppo2.py:185][0m |           0.0067 |         269.3382 |        -267.7346 |
[32m[20221214 14:38:00 @agent_ppo2.py:185][0m |          -0.0008 |         251.3068 |        -266.1276 |
[32m[20221214 14:38:00 @agent_ppo2.py:185][0m |          -0.0015 |         249.5935 |        -267.8805 |
[32m[20221214 14:38:00 @agent_ppo2.py:185][0m |           0.0156 |         272.3013 |        -268.2111 |
[32m[20221214 14:38:00 @agent_ppo2.py:185][0m |           0.0031 |         254.9529 |        -267.2608 |
[32m[20221214 14:38:00 @agent_ppo2.py:185][0m |           0.0019 |         250.3348 |        -267.2267 |
[32m[20221214 14:38:00 @agent_ppo2.py:185][0m |          -0.0037 |         247.5499 |        -267.8647 |
[32m[20221214 14:38:00 @agent_ppo2.py:185][0m |          -0.0028 |         246.5159 |        -267.4896 |
[32m[20221214 14:38:00 @agent_ppo2.py:185][0m |          -0.0025 |         246.0397 |        -267.8528 |
[32m[20221214 14:38:00 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:38:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.78
[32m[20221214 14:38:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.79
[32m[20221214 14:38:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.48
[32m[20221214 14:38:00 @agent_ppo2.py:143][0m Total time:      39.97 min
[32m[20221214 14:38:00 @agent_ppo2.py:145][0m 3682304 total steps have happened
[32m[20221214 14:38:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1798 --------------------------#
[32m[20221214 14:38:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:38:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:01 @agent_ppo2.py:185][0m |          -0.0013 |         245.5132 |        -268.4459 |
[32m[20221214 14:38:01 @agent_ppo2.py:185][0m |          -0.0003 |         238.1188 |        -269.6302 |
[32m[20221214 14:38:01 @agent_ppo2.py:185][0m |           0.0114 |         269.2525 |        -268.9802 |
[32m[20221214 14:38:01 @agent_ppo2.py:185][0m |          -0.0010 |         234.8179 |        -268.1896 |
[32m[20221214 14:38:01 @agent_ppo2.py:185][0m |          -0.0032 |         233.6345 |        -268.6398 |
[32m[20221214 14:38:01 @agent_ppo2.py:185][0m |          -0.0015 |         233.4738 |        -269.1125 |
[32m[20221214 14:38:01 @agent_ppo2.py:185][0m |          -0.0014 |         231.7973 |        -268.7680 |
[32m[20221214 14:38:02 @agent_ppo2.py:185][0m |          -0.0003 |         230.9139 |        -269.1151 |
[32m[20221214 14:38:02 @agent_ppo2.py:185][0m |          -0.0024 |         230.6242 |        -268.8179 |
[32m[20221214 14:38:02 @agent_ppo2.py:185][0m |          -0.0031 |         229.8953 |        -268.9904 |
[32m[20221214 14:38:02 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:38:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.64
[32m[20221214 14:38:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.61
[32m[20221214 14:38:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.19
[32m[20221214 14:38:02 @agent_ppo2.py:143][0m Total time:      40.00 min
[32m[20221214 14:38:02 @agent_ppo2.py:145][0m 3684352 total steps have happened
[32m[20221214 14:38:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1799 --------------------------#
[32m[20221214 14:38:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:38:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:02 @agent_ppo2.py:185][0m |           0.0096 |         305.4834 |        -270.9193 |
[32m[20221214 14:38:02 @agent_ppo2.py:185][0m |          -0.0004 |         273.2253 |        -269.8335 |
[32m[20221214 14:38:02 @agent_ppo2.py:185][0m |           0.0125 |         304.0398 |        -270.3465 |
[32m[20221214 14:38:02 @agent_ppo2.py:185][0m |          -0.0032 |         271.3569 |        -270.0941 |
[32m[20221214 14:38:03 @agent_ppo2.py:185][0m |          -0.0014 |         267.7481 |        -269.3693 |
[32m[20221214 14:38:03 @agent_ppo2.py:185][0m |          -0.0021 |         267.0643 |        -270.2067 |
[32m[20221214 14:38:03 @agent_ppo2.py:185][0m |           0.0002 |         266.7398 |        -269.3223 |
[32m[20221214 14:38:03 @agent_ppo2.py:185][0m |          -0.0034 |         264.7389 |        -269.8541 |
[32m[20221214 14:38:03 @agent_ppo2.py:185][0m |          -0.0039 |         265.8470 |        -269.9812 |
[32m[20221214 14:38:03 @agent_ppo2.py:185][0m |           0.0003 |         264.0197 |        -269.1996 |
[32m[20221214 14:38:03 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:38:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.73
[32m[20221214 14:38:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.28
[32m[20221214 14:38:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.39
[32m[20221214 14:38:03 @agent_ppo2.py:143][0m Total time:      40.02 min
[32m[20221214 14:38:03 @agent_ppo2.py:145][0m 3686400 total steps have happened
[32m[20221214 14:38:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1800 --------------------------#
[32m[20221214 14:38:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:38:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:04 @agent_ppo2.py:185][0m |          -0.0023 |         275.3546 |        -266.2557 |
[32m[20221214 14:38:04 @agent_ppo2.py:185][0m |           0.0063 |         281.2743 |        -265.6762 |
[32m[20221214 14:38:04 @agent_ppo2.py:185][0m |           0.0002 |         269.2072 |        -265.5104 |
[32m[20221214 14:38:04 @agent_ppo2.py:185][0m |          -0.0028 |         268.6479 |        -263.3617 |
[32m[20221214 14:38:04 @agent_ppo2.py:185][0m |           0.0128 |         302.7258 |        -264.6865 |
[32m[20221214 14:38:04 @agent_ppo2.py:185][0m |          -0.0010 |         266.8559 |        -265.2631 |
[32m[20221214 14:38:04 @agent_ppo2.py:185][0m |          -0.0029 |         266.3479 |        -264.8971 |
[32m[20221214 14:38:04 @agent_ppo2.py:185][0m |          -0.0030 |         265.4792 |        -263.4470 |
[32m[20221214 14:38:04 @agent_ppo2.py:185][0m |          -0.0012 |         265.6782 |        -263.6647 |
[32m[20221214 14:38:04 @agent_ppo2.py:185][0m |          -0.0022 |         264.9631 |        -263.5065 |
[32m[20221214 14:38:04 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:38:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.46
[32m[20221214 14:38:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.81
[32m[20221214 14:38:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.38
[32m[20221214 14:38:05 @agent_ppo2.py:143][0m Total time:      40.04 min
[32m[20221214 14:38:05 @agent_ppo2.py:145][0m 3688448 total steps have happened
[32m[20221214 14:38:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1801 --------------------------#
[32m[20221214 14:38:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:38:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:05 @agent_ppo2.py:185][0m |           0.0019 |         316.1227 |        -266.9894 |
[32m[20221214 14:38:05 @agent_ppo2.py:185][0m |          -0.0020 |         302.8624 |        -266.6343 |
[32m[20221214 14:38:05 @agent_ppo2.py:185][0m |           0.0020 |         303.6319 |        -266.1594 |
[32m[20221214 14:38:05 @agent_ppo2.py:185][0m |          -0.0009 |         297.9070 |        -265.4440 |
[32m[20221214 14:38:05 @agent_ppo2.py:185][0m |          -0.0010 |         296.7836 |        -265.6669 |
[32m[20221214 14:38:05 @agent_ppo2.py:185][0m |          -0.0003 |         295.9316 |        -266.3608 |
[32m[20221214 14:38:06 @agent_ppo2.py:185][0m |          -0.0021 |         296.7355 |        -265.8045 |
[32m[20221214 14:38:06 @agent_ppo2.py:185][0m |          -0.0018 |         295.0540 |        -265.4563 |
[32m[20221214 14:38:06 @agent_ppo2.py:185][0m |           0.0011 |         297.5864 |        -265.7146 |
[32m[20221214 14:38:06 @agent_ppo2.py:185][0m |          -0.0020 |         293.9193 |        -265.6238 |
[32m[20221214 14:38:06 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:38:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.21
[32m[20221214 14:38:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.59
[32m[20221214 14:38:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.16
[32m[20221214 14:38:06 @agent_ppo2.py:143][0m Total time:      40.06 min
[32m[20221214 14:38:06 @agent_ppo2.py:145][0m 3690496 total steps have happened
[32m[20221214 14:38:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1802 --------------------------#
[32m[20221214 14:38:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:38:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:06 @agent_ppo2.py:185][0m |          -0.0016 |         312.2018 |        -263.8862 |
[32m[20221214 14:38:06 @agent_ppo2.py:185][0m |           0.0013 |         308.8087 |        -263.7881 |
[32m[20221214 14:38:06 @agent_ppo2.py:185][0m |           0.0022 |         309.3301 |        -264.3164 |
[32m[20221214 14:38:07 @agent_ppo2.py:185][0m |          -0.0001 |         305.0010 |        -264.8787 |
[32m[20221214 14:38:07 @agent_ppo2.py:185][0m |           0.0125 |         328.3163 |        -265.1877 |
[32m[20221214 14:38:07 @agent_ppo2.py:185][0m |           0.0022 |         304.1767 |        -265.2898 |
[32m[20221214 14:38:07 @agent_ppo2.py:185][0m |          -0.0022 |         302.2291 |        -265.6651 |
[32m[20221214 14:38:07 @agent_ppo2.py:185][0m |          -0.0015 |         301.3841 |        -265.7048 |
[32m[20221214 14:38:07 @agent_ppo2.py:185][0m |           0.0012 |         300.7297 |        -265.6462 |
[32m[20221214 14:38:07 @agent_ppo2.py:185][0m |           0.0007 |         300.3293 |        -266.3939 |
[32m[20221214 14:38:07 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:38:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.20
[32m[20221214 14:38:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.59
[32m[20221214 14:38:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.39
[32m[20221214 14:38:07 @agent_ppo2.py:143][0m Total time:      40.09 min
[32m[20221214 14:38:07 @agent_ppo2.py:145][0m 3692544 total steps have happened
[32m[20221214 14:38:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1803 --------------------------#
[32m[20221214 14:38:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:38:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:08 @agent_ppo2.py:185][0m |          -0.0010 |         282.0662 |        -270.9414 |
[32m[20221214 14:38:08 @agent_ppo2.py:185][0m |           0.0071 |         307.3488 |        -272.0659 |
[32m[20221214 14:38:08 @agent_ppo2.py:185][0m |          -0.0045 |         274.3450 |        -272.2134 |
[32m[20221214 14:38:08 @agent_ppo2.py:185][0m |          -0.0040 |         272.9013 |        -272.5503 |
[32m[20221214 14:38:08 @agent_ppo2.py:185][0m |          -0.0038 |         272.1508 |        -272.7167 |
[32m[20221214 14:38:08 @agent_ppo2.py:185][0m |           0.0025 |         281.2337 |        -272.8364 |
[32m[20221214 14:38:08 @agent_ppo2.py:185][0m |          -0.0028 |         271.6812 |        -273.1469 |
[32m[20221214 14:38:08 @agent_ppo2.py:185][0m |          -0.0050 |         270.8271 |        -272.7685 |
[32m[20221214 14:38:08 @agent_ppo2.py:185][0m |          -0.0063 |         271.2043 |        -272.6876 |
[32m[20221214 14:38:08 @agent_ppo2.py:185][0m |          -0.0059 |         270.1400 |        -273.1089 |
[32m[20221214 14:38:08 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:38:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.90
[32m[20221214 14:38:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.21
[32m[20221214 14:38:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.17
[32m[20221214 14:38:09 @agent_ppo2.py:143][0m Total time:      40.11 min
[32m[20221214 14:38:09 @agent_ppo2.py:145][0m 3694592 total steps have happened
[32m[20221214 14:38:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1804 --------------------------#
[32m[20221214 14:38:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:38:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:09 @agent_ppo2.py:185][0m |          -0.0006 |         271.0649 |        -270.0057 |
[32m[20221214 14:38:09 @agent_ppo2.py:185][0m |          -0.0009 |         266.2063 |        -270.1258 |
[32m[20221214 14:38:09 @agent_ppo2.py:185][0m |          -0.0025 |         264.7082 |        -270.3103 |
[32m[20221214 14:38:09 @agent_ppo2.py:185][0m |           0.0036 |         269.7860 |        -269.6315 |
[32m[20221214 14:38:09 @agent_ppo2.py:185][0m |           0.0033 |         272.1259 |        -270.0288 |
[32m[20221214 14:38:09 @agent_ppo2.py:185][0m |           0.0105 |         295.5434 |        -269.6454 |
[32m[20221214 14:38:10 @agent_ppo2.py:185][0m |          -0.0028 |         262.3094 |        -269.6694 |
[32m[20221214 14:38:10 @agent_ppo2.py:185][0m |          -0.0020 |         261.4435 |        -269.8895 |
[32m[20221214 14:38:10 @agent_ppo2.py:185][0m |          -0.0033 |         260.0844 |        -269.9899 |
[32m[20221214 14:38:10 @agent_ppo2.py:185][0m |          -0.0034 |         260.3243 |        -270.2384 |
[32m[20221214 14:38:10 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:38:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.63
[32m[20221214 14:38:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.44
[32m[20221214 14:38:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.30
[32m[20221214 14:38:10 @agent_ppo2.py:143][0m Total time:      40.13 min
[32m[20221214 14:38:10 @agent_ppo2.py:145][0m 3696640 total steps have happened
[32m[20221214 14:38:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1805 --------------------------#
[32m[20221214 14:38:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:38:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:10 @agent_ppo2.py:185][0m |          -0.0019 |         298.2643 |        -274.4866 |
[32m[20221214 14:38:10 @agent_ppo2.py:185][0m |          -0.0023 |         283.5976 |        -274.4081 |
[32m[20221214 14:38:11 @agent_ppo2.py:185][0m |          -0.0012 |         278.0317 |        -274.5873 |
[32m[20221214 14:38:11 @agent_ppo2.py:185][0m |           0.0107 |         293.5252 |        -274.2805 |
[32m[20221214 14:38:11 @agent_ppo2.py:185][0m |          -0.0023 |         274.3934 |        -274.1591 |
[32m[20221214 14:38:11 @agent_ppo2.py:185][0m |          -0.0009 |         273.8138 |        -274.0935 |
[32m[20221214 14:38:11 @agent_ppo2.py:185][0m |          -0.0045 |         272.0212 |        -273.4185 |
[32m[20221214 14:38:11 @agent_ppo2.py:185][0m |          -0.0005 |         274.6548 |        -273.8920 |
[32m[20221214 14:38:11 @agent_ppo2.py:185][0m |           0.0042 |         282.5436 |        -274.2841 |
[32m[20221214 14:38:11 @agent_ppo2.py:185][0m |          -0.0047 |         269.9313 |        -273.9192 |
[32m[20221214 14:38:11 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:38:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.71
[32m[20221214 14:38:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.09
[32m[20221214 14:38:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.78
[32m[20221214 14:38:11 @agent_ppo2.py:143][0m Total time:      40.16 min
[32m[20221214 14:38:11 @agent_ppo2.py:145][0m 3698688 total steps have happened
[32m[20221214 14:38:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1806 --------------------------#
[32m[20221214 14:38:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:12 @agent_ppo2.py:185][0m |           0.0023 |         278.8444 |        -268.5954 |
[32m[20221214 14:38:12 @agent_ppo2.py:185][0m |          -0.0001 |         264.8740 |        -268.7648 |
[32m[20221214 14:38:12 @agent_ppo2.py:185][0m |           0.0007 |         259.1910 |        -268.5964 |
[32m[20221214 14:38:12 @agent_ppo2.py:185][0m |           0.0003 |         257.4138 |        -268.8804 |
[32m[20221214 14:38:12 @agent_ppo2.py:185][0m |           0.0019 |         256.9811 |        -269.0127 |
[32m[20221214 14:38:12 @agent_ppo2.py:185][0m |          -0.0012 |         254.8669 |        -268.6712 |
[32m[20221214 14:38:12 @agent_ppo2.py:185][0m |          -0.0049 |         254.7356 |        -268.5810 |
[32m[20221214 14:38:12 @agent_ppo2.py:185][0m |          -0.0011 |         254.8990 |        -268.5559 |
[32m[20221214 14:38:12 @agent_ppo2.py:185][0m |           0.0001 |         254.3747 |        -268.9957 |
[32m[20221214 14:38:13 @agent_ppo2.py:185][0m |           0.0003 |         255.2813 |        -269.0085 |
[32m[20221214 14:38:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:38:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.53
[32m[20221214 14:38:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.79
[32m[20221214 14:38:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.65
[32m[20221214 14:38:13 @agent_ppo2.py:143][0m Total time:      40.18 min
[32m[20221214 14:38:13 @agent_ppo2.py:145][0m 3700736 total steps have happened
[32m[20221214 14:38:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1807 --------------------------#
[32m[20221214 14:38:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:13 @agent_ppo2.py:185][0m |          -0.0035 |         253.8086 |        -271.1000 |
[32m[20221214 14:38:13 @agent_ppo2.py:185][0m |          -0.0027 |         243.3572 |        -272.2571 |
[32m[20221214 14:38:13 @agent_ppo2.py:185][0m |           0.0007 |         241.7564 |        -270.9290 |
[32m[20221214 14:38:13 @agent_ppo2.py:185][0m |           0.0089 |         257.2313 |        -270.9263 |
[32m[20221214 14:38:13 @agent_ppo2.py:185][0m |           0.0058 |         243.9719 |        -270.5293 |
[32m[20221214 14:38:13 @agent_ppo2.py:185][0m |          -0.0034 |         235.8109 |        -270.8118 |
[32m[20221214 14:38:14 @agent_ppo2.py:185][0m |          -0.0033 |         232.6014 |        -270.9401 |
[32m[20221214 14:38:14 @agent_ppo2.py:185][0m |          -0.0032 |         230.3515 |        -269.9677 |
[32m[20221214 14:38:14 @agent_ppo2.py:185][0m |          -0.0021 |         230.2583 |        -269.6315 |
[32m[20221214 14:38:14 @agent_ppo2.py:185][0m |          -0.0039 |         228.9270 |        -269.9713 |
[32m[20221214 14:38:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:38:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.13
[32m[20221214 14:38:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.34
[32m[20221214 14:38:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.56
[32m[20221214 14:38:14 @agent_ppo2.py:143][0m Total time:      40.20 min
[32m[20221214 14:38:14 @agent_ppo2.py:145][0m 3702784 total steps have happened
[32m[20221214 14:38:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1808 --------------------------#
[32m[20221214 14:38:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:14 @agent_ppo2.py:185][0m |          -0.0001 |         252.6541 |        -266.1239 |
[32m[20221214 14:38:14 @agent_ppo2.py:185][0m |           0.0017 |         251.1952 |        -265.6348 |
[32m[20221214 14:38:14 @agent_ppo2.py:185][0m |           0.0016 |         247.6629 |        -266.1058 |
[32m[20221214 14:38:15 @agent_ppo2.py:185][0m |          -0.0036 |         246.8844 |        -266.0776 |
[32m[20221214 14:38:15 @agent_ppo2.py:185][0m |           0.0005 |         244.3578 |        -266.2121 |
[32m[20221214 14:38:15 @agent_ppo2.py:185][0m |           0.0034 |         248.1156 |        -265.9228 |
[32m[20221214 14:38:15 @agent_ppo2.py:185][0m |          -0.0014 |         244.7496 |        -266.3769 |
[32m[20221214 14:38:15 @agent_ppo2.py:185][0m |           0.0038 |         249.9826 |        -266.5640 |
[32m[20221214 14:38:15 @agent_ppo2.py:185][0m |          -0.0066 |         245.3847 |        -266.8518 |
[32m[20221214 14:38:15 @agent_ppo2.py:185][0m |          -0.0013 |         243.2245 |        -266.7276 |
[32m[20221214 14:38:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:38:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.56
[32m[20221214 14:38:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.81
[32m[20221214 14:38:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.96
[32m[20221214 14:38:15 @agent_ppo2.py:143][0m Total time:      40.22 min
[32m[20221214 14:38:15 @agent_ppo2.py:145][0m 3704832 total steps have happened
[32m[20221214 14:38:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1809 --------------------------#
[32m[20221214 14:38:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:16 @agent_ppo2.py:185][0m |           0.0103 |         275.0010 |        -267.1006 |
[32m[20221214 14:38:16 @agent_ppo2.py:185][0m |           0.0126 |         293.3770 |        -266.8145 |
[32m[20221214 14:38:16 @agent_ppo2.py:185][0m |          -0.0037 |         252.4226 |        -266.5367 |
[32m[20221214 14:38:16 @agent_ppo2.py:185][0m |          -0.0007 |         251.6243 |        -266.3355 |
[32m[20221214 14:38:16 @agent_ppo2.py:185][0m |          -0.0007 |         250.3486 |        -266.7197 |
[32m[20221214 14:38:16 @agent_ppo2.py:185][0m |          -0.0039 |         251.0927 |        -266.5651 |
[32m[20221214 14:38:16 @agent_ppo2.py:185][0m |          -0.0030 |         250.7119 |        -266.3094 |
[32m[20221214 14:38:16 @agent_ppo2.py:185][0m |           0.0098 |         269.5676 |        -266.9080 |
[32m[20221214 14:38:16 @agent_ppo2.py:185][0m |          -0.0002 |         250.7583 |        -266.3904 |
[32m[20221214 14:38:16 @agent_ppo2.py:185][0m |          -0.0037 |         248.9222 |        -266.9105 |
[32m[20221214 14:38:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:38:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.48
[32m[20221214 14:38:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.28
[32m[20221214 14:38:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.06
[32m[20221214 14:38:16 @agent_ppo2.py:143][0m Total time:      40.24 min
[32m[20221214 14:38:16 @agent_ppo2.py:145][0m 3706880 total steps have happened
[32m[20221214 14:38:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1810 --------------------------#
[32m[20221214 14:38:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:38:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:17 @agent_ppo2.py:185][0m |           0.0012 |         236.0099 |        -270.5000 |
[32m[20221214 14:38:17 @agent_ppo2.py:185][0m |          -0.0014 |         231.9851 |        -270.2006 |
[32m[20221214 14:38:17 @agent_ppo2.py:185][0m |          -0.0018 |         229.8994 |        -269.8219 |
[32m[20221214 14:38:17 @agent_ppo2.py:185][0m |          -0.0043 |         229.5403 |        -269.7082 |
[32m[20221214 14:38:17 @agent_ppo2.py:185][0m |          -0.0012 |         228.3624 |        -269.1262 |
[32m[20221214 14:38:17 @agent_ppo2.py:185][0m |          -0.0030 |         226.7802 |        -269.6603 |
[32m[20221214 14:38:17 @agent_ppo2.py:185][0m |           0.0060 |         229.3054 |        -270.3629 |
[32m[20221214 14:38:17 @agent_ppo2.py:185][0m |          -0.0038 |         225.2097 |        -269.9329 |
[32m[20221214 14:38:18 @agent_ppo2.py:185][0m |          -0.0020 |         223.3484 |        -269.7445 |
[32m[20221214 14:38:18 @agent_ppo2.py:185][0m |          -0.0028 |         223.4169 |        -269.7069 |
[32m[20221214 14:38:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:38:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.69
[32m[20221214 14:38:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.23
[32m[20221214 14:38:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.86
[32m[20221214 14:38:18 @agent_ppo2.py:143][0m Total time:      40.26 min
[32m[20221214 14:38:18 @agent_ppo2.py:145][0m 3708928 total steps have happened
[32m[20221214 14:38:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1811 --------------------------#
[32m[20221214 14:38:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:18 @agent_ppo2.py:185][0m |          -0.0022 |         223.3202 |        -267.9152 |
[32m[20221214 14:38:18 @agent_ppo2.py:185][0m |           0.0038 |         219.8018 |        -267.3146 |
[32m[20221214 14:38:18 @agent_ppo2.py:185][0m |           0.0032 |         221.0630 |        -268.2651 |
[32m[20221214 14:38:18 @agent_ppo2.py:185][0m |           0.0083 |         226.1779 |        -268.5223 |
[32m[20221214 14:38:18 @agent_ppo2.py:185][0m |          -0.0011 |         212.6584 |        -268.2565 |
[32m[20221214 14:38:19 @agent_ppo2.py:185][0m |           0.0016 |         213.3181 |        -267.8440 |
[32m[20221214 14:38:19 @agent_ppo2.py:185][0m |          -0.0015 |         211.0140 |        -268.7680 |
[32m[20221214 14:38:19 @agent_ppo2.py:185][0m |          -0.0008 |         209.3106 |        -267.3336 |
[32m[20221214 14:38:19 @agent_ppo2.py:185][0m |           0.0041 |         212.6507 |        -268.8211 |
[32m[20221214 14:38:19 @agent_ppo2.py:185][0m |          -0.0022 |         208.5553 |        -267.4388 |
[32m[20221214 14:38:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:38:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.24
[32m[20221214 14:38:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.24
[32m[20221214 14:38:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.66
[32m[20221214 14:38:19 @agent_ppo2.py:143][0m Total time:      40.28 min
[32m[20221214 14:38:19 @agent_ppo2.py:145][0m 3710976 total steps have happened
[32m[20221214 14:38:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1812 --------------------------#
[32m[20221214 14:38:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:19 @agent_ppo2.py:185][0m |           0.0018 |         204.2714 |        -272.6148 |
[32m[20221214 14:38:19 @agent_ppo2.py:185][0m |          -0.0020 |         196.4140 |        -272.6084 |
[32m[20221214 14:38:20 @agent_ppo2.py:185][0m |           0.0079 |         203.8636 |        -272.7719 |
[32m[20221214 14:38:20 @agent_ppo2.py:185][0m |           0.0001 |         193.7237 |        -273.5516 |
[32m[20221214 14:38:20 @agent_ppo2.py:185][0m |           0.0001 |         192.2828 |        -272.1936 |
[32m[20221214 14:38:20 @agent_ppo2.py:185][0m |           0.0006 |         191.3079 |        -272.4695 |
[32m[20221214 14:38:20 @agent_ppo2.py:185][0m |          -0.0007 |         190.8055 |        -272.6931 |
[32m[20221214 14:38:20 @agent_ppo2.py:185][0m |          -0.0009 |         188.6842 |        -272.4126 |
[32m[20221214 14:38:20 @agent_ppo2.py:185][0m |          -0.0033 |         188.5834 |        -272.1730 |
[32m[20221214 14:38:20 @agent_ppo2.py:185][0m |          -0.0021 |         188.2928 |        -272.7196 |
[32m[20221214 14:38:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:38:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.64
[32m[20221214 14:38:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.73
[32m[20221214 14:38:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.11
[32m[20221214 14:38:20 @agent_ppo2.py:143][0m Total time:      40.30 min
[32m[20221214 14:38:20 @agent_ppo2.py:145][0m 3713024 total steps have happened
[32m[20221214 14:38:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1813 --------------------------#
[32m[20221214 14:38:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:21 @agent_ppo2.py:185][0m |           0.0029 |         235.0934 |        -266.5823 |
[32m[20221214 14:38:21 @agent_ppo2.py:185][0m |          -0.0004 |         230.7615 |        -266.5219 |
[32m[20221214 14:38:21 @agent_ppo2.py:185][0m |          -0.0010 |         229.0747 |        -266.4711 |
[32m[20221214 14:38:21 @agent_ppo2.py:185][0m |          -0.0020 |         229.0032 |        -266.8435 |
[32m[20221214 14:38:21 @agent_ppo2.py:185][0m |          -0.0013 |         228.2817 |        -266.1844 |
[32m[20221214 14:38:21 @agent_ppo2.py:185][0m |           0.0019 |         227.4670 |        -266.1349 |
[32m[20221214 14:38:21 @agent_ppo2.py:185][0m |          -0.0010 |         226.2981 |        -266.8374 |
[32m[20221214 14:38:21 @agent_ppo2.py:185][0m |          -0.0045 |         227.5500 |        -267.2125 |
[32m[20221214 14:38:21 @agent_ppo2.py:185][0m |           0.0003 |         225.9798 |        -267.6208 |
[32m[20221214 14:38:21 @agent_ppo2.py:185][0m |          -0.0009 |         224.7255 |        -267.5761 |
[32m[20221214 14:38:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:38:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.66
[32m[20221214 14:38:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.35
[32m[20221214 14:38:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.20
[32m[20221214 14:38:22 @agent_ppo2.py:143][0m Total time:      40.33 min
[32m[20221214 14:38:22 @agent_ppo2.py:145][0m 3715072 total steps have happened
[32m[20221214 14:38:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1814 --------------------------#
[32m[20221214 14:38:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:38:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:22 @agent_ppo2.py:185][0m |          -0.0012 |         221.2751 |        -274.3358 |
[32m[20221214 14:38:22 @agent_ppo2.py:185][0m |          -0.0007 |         207.4377 |        -275.4623 |
[32m[20221214 14:38:22 @agent_ppo2.py:185][0m |          -0.0027 |         201.7045 |        -273.8115 |
[32m[20221214 14:38:22 @agent_ppo2.py:185][0m |           0.0000 |         198.4411 |        -275.9751 |
[32m[20221214 14:38:22 @agent_ppo2.py:185][0m |          -0.0015 |         195.7527 |        -275.5991 |
[32m[20221214 14:38:22 @agent_ppo2.py:185][0m |          -0.0023 |         192.6354 |        -274.6183 |
[32m[20221214 14:38:22 @agent_ppo2.py:185][0m |           0.0007 |         190.4422 |        -275.8553 |
[32m[20221214 14:38:23 @agent_ppo2.py:185][0m |          -0.0006 |         188.5445 |        -275.6665 |
[32m[20221214 14:38:23 @agent_ppo2.py:185][0m |          -0.0002 |         188.7922 |        -276.0150 |
[32m[20221214 14:38:23 @agent_ppo2.py:185][0m |          -0.0025 |         186.9884 |        -276.6525 |
[32m[20221214 14:38:23 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:38:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.15
[32m[20221214 14:38:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.11
[32m[20221214 14:38:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.91
[32m[20221214 14:38:23 @agent_ppo2.py:143][0m Total time:      40.35 min
[32m[20221214 14:38:23 @agent_ppo2.py:145][0m 3717120 total steps have happened
[32m[20221214 14:38:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1815 --------------------------#
[32m[20221214 14:38:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:23 @agent_ppo2.py:185][0m |          -0.0009 |         235.2040 |        -276.3619 |
[32m[20221214 14:38:23 @agent_ppo2.py:185][0m |           0.0074 |         248.0683 |        -276.3465 |
[32m[20221214 14:38:23 @agent_ppo2.py:185][0m |          -0.0014 |         225.3730 |        -276.0714 |
[32m[20221214 14:38:23 @agent_ppo2.py:185][0m |           0.0004 |         223.7806 |        -276.2202 |
[32m[20221214 14:38:24 @agent_ppo2.py:185][0m |          -0.0011 |         220.8121 |        -275.1533 |
[32m[20221214 14:38:24 @agent_ppo2.py:185][0m |          -0.0016 |         219.2996 |        -276.2304 |
[32m[20221214 14:38:24 @agent_ppo2.py:185][0m |          -0.0016 |         220.2681 |        -275.2232 |
[32m[20221214 14:38:24 @agent_ppo2.py:185][0m |          -0.0014 |         217.4461 |        -275.6846 |
[32m[20221214 14:38:24 @agent_ppo2.py:185][0m |          -0.0024 |         216.3731 |        -275.0628 |
[32m[20221214 14:38:24 @agent_ppo2.py:185][0m |          -0.0010 |         216.0683 |        -275.7302 |
[32m[20221214 14:38:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:38:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.32
[32m[20221214 14:38:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.46
[32m[20221214 14:38:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.42
[32m[20221214 14:38:24 @agent_ppo2.py:143][0m Total time:      40.37 min
[32m[20221214 14:38:24 @agent_ppo2.py:145][0m 3719168 total steps have happened
[32m[20221214 14:38:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1816 --------------------------#
[32m[20221214 14:38:24 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:38:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:25 @agent_ppo2.py:185][0m |          -0.0003 |         241.6079 |        -273.7044 |
[32m[20221214 14:38:25 @agent_ppo2.py:185][0m |          -0.0012 |         237.2082 |        -272.8417 |
[32m[20221214 14:38:25 @agent_ppo2.py:185][0m |          -0.0022 |         234.8972 |        -273.1897 |
[32m[20221214 14:38:25 @agent_ppo2.py:185][0m |          -0.0024 |         233.0695 |        -273.3855 |
[32m[20221214 14:38:25 @agent_ppo2.py:185][0m |          -0.0019 |         232.3936 |        -272.6292 |
[32m[20221214 14:38:25 @agent_ppo2.py:185][0m |          -0.0034 |         231.1955 |        -272.5889 |
[32m[20221214 14:38:25 @agent_ppo2.py:185][0m |          -0.0012 |         231.5002 |        -272.7769 |
[32m[20221214 14:38:25 @agent_ppo2.py:185][0m |           0.0071 |         253.8279 |        -273.0723 |
[32m[20221214 14:38:25 @agent_ppo2.py:185][0m |          -0.0029 |         230.5545 |        -273.2253 |
[32m[20221214 14:38:25 @agent_ppo2.py:185][0m |          -0.0023 |         230.1694 |        -273.0758 |
[32m[20221214 14:38:25 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:38:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.95
[32m[20221214 14:38:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.78
[32m[20221214 14:38:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.78
[32m[20221214 14:38:26 @agent_ppo2.py:143][0m Total time:      40.39 min
[32m[20221214 14:38:26 @agent_ppo2.py:145][0m 3721216 total steps have happened
[32m[20221214 14:38:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1817 --------------------------#
[32m[20221214 14:38:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:38:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:26 @agent_ppo2.py:185][0m |           0.0112 |         267.3154 |        -273.8415 |
[32m[20221214 14:38:26 @agent_ppo2.py:185][0m |          -0.0018 |         249.9550 |        -274.0809 |
[32m[20221214 14:38:26 @agent_ppo2.py:185][0m |          -0.0023 |         246.3951 |        -273.1980 |
[32m[20221214 14:38:26 @agent_ppo2.py:185][0m |           0.0014 |         244.6831 |        -273.7373 |
[32m[20221214 14:38:26 @agent_ppo2.py:185][0m |           0.0025 |         246.5262 |        -274.5005 |
[32m[20221214 14:38:26 @agent_ppo2.py:185][0m |           0.0130 |         268.7695 |        -273.2699 |
[32m[20221214 14:38:27 @agent_ppo2.py:185][0m |           0.0018 |         242.9560 |        -274.6539 |
[32m[20221214 14:38:27 @agent_ppo2.py:185][0m |          -0.0011 |         242.6441 |        -274.4805 |
[32m[20221214 14:38:27 @agent_ppo2.py:185][0m |          -0.0002 |         241.3266 |        -274.9051 |
[32m[20221214 14:38:27 @agent_ppo2.py:185][0m |           0.0075 |         244.5858 |        -273.3227 |
[32m[20221214 14:38:27 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:38:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.63
[32m[20221214 14:38:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.71
[32m[20221214 14:38:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.54
[32m[20221214 14:38:27 @agent_ppo2.py:143][0m Total time:      40.41 min
[32m[20221214 14:38:27 @agent_ppo2.py:145][0m 3723264 total steps have happened
[32m[20221214 14:38:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1818 --------------------------#
[32m[20221214 14:38:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:38:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:27 @agent_ppo2.py:185][0m |           0.0020 |         233.2627 |        -276.3877 |
[32m[20221214 14:38:27 @agent_ppo2.py:185][0m |           0.0009 |         228.0028 |        -276.5922 |
[32m[20221214 14:38:28 @agent_ppo2.py:185][0m |          -0.0022 |         226.1994 |        -275.9943 |
[32m[20221214 14:38:28 @agent_ppo2.py:185][0m |          -0.0020 |         224.8905 |        -276.0621 |
[32m[20221214 14:38:28 @agent_ppo2.py:185][0m |           0.0044 |         232.0191 |        -276.6518 |
[32m[20221214 14:38:28 @agent_ppo2.py:185][0m |          -0.0006 |         224.8111 |        -276.2137 |
[32m[20221214 14:38:28 @agent_ppo2.py:185][0m |           0.0010 |         226.4104 |        -276.6057 |
[32m[20221214 14:38:28 @agent_ppo2.py:185][0m |          -0.0012 |         224.4378 |        -276.6740 |
[32m[20221214 14:38:28 @agent_ppo2.py:185][0m |          -0.0001 |         224.4295 |        -276.9040 |
[32m[20221214 14:38:28 @agent_ppo2.py:185][0m |          -0.0019 |         223.3948 |        -276.4029 |
[32m[20221214 14:38:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:38:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.99
[32m[20221214 14:38:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.90
[32m[20221214 14:38:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.20
[32m[20221214 14:38:28 @agent_ppo2.py:143][0m Total time:      40.44 min
[32m[20221214 14:38:28 @agent_ppo2.py:145][0m 3725312 total steps have happened
[32m[20221214 14:38:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1819 --------------------------#
[32m[20221214 14:38:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:29 @agent_ppo2.py:185][0m |          -0.0008 |         232.0497 |        -277.2796 |
[32m[20221214 14:38:29 @agent_ppo2.py:185][0m |          -0.0011 |         230.9474 |        -278.0740 |
[32m[20221214 14:38:29 @agent_ppo2.py:185][0m |           0.0036 |         231.6989 |        -278.4601 |
[32m[20221214 14:38:29 @agent_ppo2.py:185][0m |          -0.0003 |         229.4557 |        -277.5790 |
[32m[20221214 14:38:29 @agent_ppo2.py:185][0m |           0.0013 |         228.9220 |        -277.8324 |
[32m[20221214 14:38:29 @agent_ppo2.py:185][0m |          -0.0007 |         229.2082 |        -278.2025 |
[32m[20221214 14:38:29 @agent_ppo2.py:185][0m |          -0.0012 |         228.8741 |        -278.0098 |
[32m[20221214 14:38:29 @agent_ppo2.py:185][0m |          -0.0019 |         228.9222 |        -278.4672 |
[32m[20221214 14:38:29 @agent_ppo2.py:185][0m |          -0.0003 |         227.9878 |        -277.6344 |
[32m[20221214 14:38:29 @agent_ppo2.py:185][0m |          -0.0014 |         228.5274 |        -278.3131 |
[32m[20221214 14:38:29 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:38:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.74
[32m[20221214 14:38:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.73
[32m[20221214 14:38:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.25
[32m[20221214 14:38:30 @agent_ppo2.py:143][0m Total time:      40.46 min
[32m[20221214 14:38:30 @agent_ppo2.py:145][0m 3727360 total steps have happened
[32m[20221214 14:38:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1820 --------------------------#
[32m[20221214 14:38:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:38:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:30 @agent_ppo2.py:185][0m |          -0.0003 |         254.6892 |        -276.0979 |
[32m[20221214 14:38:30 @agent_ppo2.py:185][0m |           0.0270 |         299.4805 |        -276.3662 |
[32m[20221214 14:38:30 @agent_ppo2.py:185][0m |          -0.0026 |         249.5237 |        -275.8074 |
[32m[20221214 14:38:30 @agent_ppo2.py:185][0m |          -0.0016 |         247.1939 |        -275.7592 |
[32m[20221214 14:38:30 @agent_ppo2.py:185][0m |          -0.0018 |         245.6476 |        -275.2078 |
[32m[20221214 14:38:30 @agent_ppo2.py:185][0m |          -0.0019 |         246.8502 |        -275.1908 |
[32m[20221214 14:38:30 @agent_ppo2.py:185][0m |          -0.0027 |         245.6001 |        -274.7416 |
[32m[20221214 14:38:30 @agent_ppo2.py:185][0m |          -0.0006 |         245.5701 |        -274.9582 |
[32m[20221214 14:38:31 @agent_ppo2.py:185][0m |          -0.0011 |         244.3166 |        -275.0540 |
[32m[20221214 14:38:31 @agent_ppo2.py:185][0m |           0.0040 |         249.6382 |        -275.3294 |
[32m[20221214 14:38:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:38:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.91
[32m[20221214 14:38:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.12
[32m[20221214 14:38:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.14
[32m[20221214 14:38:31 @agent_ppo2.py:143][0m Total time:      40.48 min
[32m[20221214 14:38:31 @agent_ppo2.py:145][0m 3729408 total steps have happened
[32m[20221214 14:38:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1821 --------------------------#
[32m[20221214 14:38:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:31 @agent_ppo2.py:185][0m |           0.0087 |         239.8790 |        -274.7216 |
[32m[20221214 14:38:31 @agent_ppo2.py:185][0m |           0.0038 |         239.5271 |        -274.7332 |
[32m[20221214 14:38:31 @agent_ppo2.py:185][0m |           0.0097 |         255.9456 |        -274.2819 |
[32m[20221214 14:38:31 @agent_ppo2.py:185][0m |           0.0065 |         247.8847 |        -274.2464 |
[32m[20221214 14:38:31 @agent_ppo2.py:185][0m |          -0.0004 |         233.1505 |        -273.9752 |
[32m[20221214 14:38:32 @agent_ppo2.py:185][0m |          -0.0028 |         231.3332 |        -273.8178 |
[32m[20221214 14:38:32 @agent_ppo2.py:185][0m |          -0.0008 |         231.5114 |        -273.9827 |
[32m[20221214 14:38:32 @agent_ppo2.py:185][0m |           0.0011 |         230.0189 |        -273.5110 |
[32m[20221214 14:38:32 @agent_ppo2.py:185][0m |          -0.0026 |         230.4214 |        -274.3513 |
[32m[20221214 14:38:32 @agent_ppo2.py:185][0m |          -0.0008 |         228.7124 |        -273.7986 |
[32m[20221214 14:38:32 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:38:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.77
[32m[20221214 14:38:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.40
[32m[20221214 14:38:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.25
[32m[20221214 14:38:32 @agent_ppo2.py:143][0m Total time:      40.50 min
[32m[20221214 14:38:32 @agent_ppo2.py:145][0m 3731456 total steps have happened
[32m[20221214 14:38:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1822 --------------------------#
[32m[20221214 14:38:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:32 @agent_ppo2.py:185][0m |          -0.0009 |         230.3585 |        -273.1847 |
[32m[20221214 14:38:32 @agent_ppo2.py:185][0m |          -0.0009 |         220.3946 |        -273.1961 |
[32m[20221214 14:38:33 @agent_ppo2.py:185][0m |           0.0074 |         228.6362 |        -272.5417 |
[32m[20221214 14:38:33 @agent_ppo2.py:185][0m |          -0.0014 |         215.7101 |        -272.5882 |
[32m[20221214 14:38:33 @agent_ppo2.py:185][0m |          -0.0007 |         215.1202 |        -273.2436 |
[32m[20221214 14:38:33 @agent_ppo2.py:185][0m |          -0.0012 |         213.0475 |        -273.2779 |
[32m[20221214 14:38:33 @agent_ppo2.py:185][0m |          -0.0035 |         212.4473 |        -272.5112 |
[32m[20221214 14:38:33 @agent_ppo2.py:185][0m |           0.0005 |         211.3735 |        -273.2168 |
[32m[20221214 14:38:33 @agent_ppo2.py:185][0m |          -0.0040 |         212.2762 |        -273.0233 |
[32m[20221214 14:38:33 @agent_ppo2.py:185][0m |          -0.0029 |         210.7612 |        -271.7039 |
[32m[20221214 14:38:33 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:38:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.91
[32m[20221214 14:38:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.73
[32m[20221214 14:38:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.56
[32m[20221214 14:38:33 @agent_ppo2.py:143][0m Total time:      40.52 min
[32m[20221214 14:38:33 @agent_ppo2.py:145][0m 3733504 total steps have happened
[32m[20221214 14:38:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1823 --------------------------#
[32m[20221214 14:38:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:34 @agent_ppo2.py:185][0m |           0.0029 |         197.1669 |        -268.9694 |
[32m[20221214 14:38:34 @agent_ppo2.py:185][0m |          -0.0004 |         192.4828 |        -268.7541 |
[32m[20221214 14:38:34 @agent_ppo2.py:185][0m |           0.0070 |         203.1286 |        -268.3023 |
[32m[20221214 14:38:34 @agent_ppo2.py:185][0m |          -0.0018 |         189.2342 |        -268.3257 |
[32m[20221214 14:38:34 @agent_ppo2.py:185][0m |          -0.0024 |         187.9654 |        -268.6915 |
[32m[20221214 14:38:34 @agent_ppo2.py:185][0m |          -0.0023 |         187.7326 |        -268.5762 |
[32m[20221214 14:38:34 @agent_ppo2.py:185][0m |          -0.0016 |         187.2366 |        -268.2168 |
[32m[20221214 14:38:34 @agent_ppo2.py:185][0m |           0.0257 |         231.9786 |        -268.4300 |
[32m[20221214 14:38:34 @agent_ppo2.py:185][0m |          -0.0010 |         190.8066 |        -267.9573 |
[32m[20221214 14:38:34 @agent_ppo2.py:185][0m |          -0.0015 |         186.0233 |        -267.9329 |
[32m[20221214 14:38:34 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:38:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.98
[32m[20221214 14:38:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.45
[32m[20221214 14:38:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.15
[32m[20221214 14:38:35 @agent_ppo2.py:143][0m Total time:      40.54 min
[32m[20221214 14:38:35 @agent_ppo2.py:145][0m 3735552 total steps have happened
[32m[20221214 14:38:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1824 --------------------------#
[32m[20221214 14:38:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:35 @agent_ppo2.py:185][0m |          -0.0012 |         200.1673 |        -271.3423 |
[32m[20221214 14:38:35 @agent_ppo2.py:185][0m |           0.0008 |         195.4511 |        -271.2255 |
[32m[20221214 14:38:35 @agent_ppo2.py:185][0m |           0.0003 |         192.8554 |        -272.1185 |
[32m[20221214 14:38:35 @agent_ppo2.py:185][0m |           0.0000 |         190.6473 |        -272.1440 |
[32m[20221214 14:38:35 @agent_ppo2.py:185][0m |          -0.0012 |         190.1222 |        -272.6943 |
[32m[20221214 14:38:35 @agent_ppo2.py:185][0m |          -0.0020 |         190.2291 |        -272.5396 |
[32m[20221214 14:38:35 @agent_ppo2.py:185][0m |          -0.0012 |         189.4159 |        -272.5639 |
[32m[20221214 14:38:35 @agent_ppo2.py:185][0m |          -0.0029 |         188.1119 |        -272.3668 |
[32m[20221214 14:38:36 @agent_ppo2.py:185][0m |           0.0001 |         187.9380 |        -272.5037 |
[32m[20221214 14:38:36 @agent_ppo2.py:185][0m |           0.0005 |         187.0543 |        -272.6411 |
[32m[20221214 14:38:36 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:38:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.93
[32m[20221214 14:38:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.48
[32m[20221214 14:38:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.11
[32m[20221214 14:38:36 @agent_ppo2.py:143][0m Total time:      40.56 min
[32m[20221214 14:38:36 @agent_ppo2.py:145][0m 3737600 total steps have happened
[32m[20221214 14:38:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1825 --------------------------#
[32m[20221214 14:38:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:36 @agent_ppo2.py:185][0m |           0.0080 |         217.3362 |        -272.9791 |
[32m[20221214 14:38:36 @agent_ppo2.py:185][0m |          -0.0016 |         196.3588 |        -273.3092 |
[32m[20221214 14:38:36 @agent_ppo2.py:185][0m |          -0.0023 |         193.4331 |        -273.5087 |
[32m[20221214 14:38:36 @agent_ppo2.py:185][0m |          -0.0017 |         191.2685 |        -273.3950 |
[32m[20221214 14:38:36 @agent_ppo2.py:185][0m |          -0.0016 |         189.9540 |        -273.9799 |
[32m[20221214 14:38:36 @agent_ppo2.py:185][0m |          -0.0013 |         188.4272 |        -274.2656 |
[32m[20221214 14:38:37 @agent_ppo2.py:185][0m |           0.0018 |         187.7474 |        -274.1349 |
[32m[20221214 14:38:37 @agent_ppo2.py:185][0m |          -0.0000 |         186.3821 |        -273.7821 |
[32m[20221214 14:38:37 @agent_ppo2.py:185][0m |           0.0184 |         217.7765 |        -274.0469 |
[32m[20221214 14:38:37 @agent_ppo2.py:185][0m |          -0.0027 |         187.1820 |        -273.3825 |
[32m[20221214 14:38:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:38:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.02
[32m[20221214 14:38:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.46
[32m[20221214 14:38:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.75
[32m[20221214 14:38:37 @agent_ppo2.py:143][0m Total time:      40.58 min
[32m[20221214 14:38:37 @agent_ppo2.py:145][0m 3739648 total steps have happened
[32m[20221214 14:38:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1826 --------------------------#
[32m[20221214 14:38:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:37 @agent_ppo2.py:185][0m |          -0.0023 |         183.9030 |        -279.5130 |
[32m[20221214 14:38:37 @agent_ppo2.py:185][0m |          -0.0008 |         178.1921 |        -279.0437 |
[32m[20221214 14:38:37 @agent_ppo2.py:185][0m |          -0.0026 |         176.9822 |        -279.4425 |
[32m[20221214 14:38:38 @agent_ppo2.py:185][0m |           0.0013 |         177.5546 |        -279.8129 |
[32m[20221214 14:38:38 @agent_ppo2.py:185][0m |          -0.0036 |         175.3505 |        -279.5083 |
[32m[20221214 14:38:38 @agent_ppo2.py:185][0m |          -0.0029 |         175.4885 |        -278.7250 |
[32m[20221214 14:38:38 @agent_ppo2.py:185][0m |          -0.0015 |         174.8404 |        -279.3607 |
[32m[20221214 14:38:38 @agent_ppo2.py:185][0m |           0.0011 |         174.1777 |        -279.7364 |
[32m[20221214 14:38:38 @agent_ppo2.py:185][0m |           0.0101 |         189.9538 |        -279.3441 |
[32m[20221214 14:38:38 @agent_ppo2.py:185][0m |          -0.0029 |         174.0161 |        -279.7908 |
[32m[20221214 14:38:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:38:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.34
[32m[20221214 14:38:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.82
[32m[20221214 14:38:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.20
[32m[20221214 14:38:38 @agent_ppo2.py:143][0m Total time:      40.60 min
[32m[20221214 14:38:38 @agent_ppo2.py:145][0m 3741696 total steps have happened
[32m[20221214 14:38:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1827 --------------------------#
[32m[20221214 14:38:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:39 @agent_ppo2.py:185][0m |          -0.0011 |         178.3565 |        -280.0155 |
[32m[20221214 14:38:39 @agent_ppo2.py:185][0m |           0.0100 |         193.3578 |        -280.5538 |
[32m[20221214 14:38:39 @agent_ppo2.py:185][0m |           0.0024 |         175.4134 |        -279.2969 |
[32m[20221214 14:38:39 @agent_ppo2.py:185][0m |          -0.0006 |         175.0237 |        -281.1738 |
[32m[20221214 14:38:39 @agent_ppo2.py:185][0m |          -0.0024 |         174.1268 |        -281.1711 |
[32m[20221214 14:38:39 @agent_ppo2.py:185][0m |           0.0086 |         179.8639 |        -280.5527 |
[32m[20221214 14:38:39 @agent_ppo2.py:185][0m |          -0.0035 |         173.5781 |        -280.0051 |
[32m[20221214 14:38:39 @agent_ppo2.py:185][0m |          -0.0016 |         174.5047 |        -280.7615 |
[32m[20221214 14:38:39 @agent_ppo2.py:185][0m |          -0.0014 |         172.7296 |        -281.5048 |
[32m[20221214 14:38:39 @agent_ppo2.py:185][0m |          -0.0050 |         172.3121 |        -280.9150 |
[32m[20221214 14:38:39 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:38:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.00
[32m[20221214 14:38:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.80
[32m[20221214 14:38:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.88
[32m[20221214 14:38:39 @agent_ppo2.py:143][0m Total time:      40.62 min
[32m[20221214 14:38:39 @agent_ppo2.py:145][0m 3743744 total steps have happened
[32m[20221214 14:38:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1828 --------------------------#
[32m[20221214 14:38:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:40 @agent_ppo2.py:185][0m |          -0.0029 |         184.8617 |        -280.3658 |
[32m[20221214 14:38:40 @agent_ppo2.py:185][0m |           0.0014 |         180.5209 |        -279.9906 |
[32m[20221214 14:38:40 @agent_ppo2.py:185][0m |          -0.0023 |         179.1452 |        -280.8582 |
[32m[20221214 14:38:40 @agent_ppo2.py:185][0m |           0.0005 |         176.9084 |        -280.6905 |
[32m[20221214 14:38:40 @agent_ppo2.py:185][0m |          -0.0013 |         175.8093 |        -280.8346 |
[32m[20221214 14:38:40 @agent_ppo2.py:185][0m |          -0.0025 |         176.3873 |        -281.3462 |
[32m[20221214 14:38:40 @agent_ppo2.py:185][0m |           0.0009 |         174.6192 |        -280.7996 |
[32m[20221214 14:38:40 @agent_ppo2.py:185][0m |           0.0073 |         183.3045 |        -280.8350 |
[32m[20221214 14:38:40 @agent_ppo2.py:185][0m |           0.0025 |         174.4054 |        -281.1462 |
[32m[20221214 14:38:40 @agent_ppo2.py:185][0m |          -0.0041 |         173.2979 |        -281.2789 |
[32m[20221214 14:38:40 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:38:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.87
[32m[20221214 14:38:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.67
[32m[20221214 14:38:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.83
[32m[20221214 14:38:41 @agent_ppo2.py:143][0m Total time:      40.64 min
[32m[20221214 14:38:41 @agent_ppo2.py:145][0m 3745792 total steps have happened
[32m[20221214 14:38:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1829 --------------------------#
[32m[20221214 14:38:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:41 @agent_ppo2.py:185][0m |           0.0005 |         179.4296 |        -284.9289 |
[32m[20221214 14:38:41 @agent_ppo2.py:185][0m |          -0.0028 |         176.2331 |        -284.4163 |
[32m[20221214 14:38:41 @agent_ppo2.py:185][0m |          -0.0029 |         174.6682 |        -283.6489 |
[32m[20221214 14:38:41 @agent_ppo2.py:185][0m |           0.0020 |         177.4301 |        -284.5846 |
[32m[20221214 14:38:41 @agent_ppo2.py:185][0m |           0.0095 |         188.5735 |        -283.4136 |
[32m[20221214 14:38:41 @agent_ppo2.py:185][0m |           0.0236 |         213.7552 |        -283.6288 |
[32m[20221214 14:38:41 @agent_ppo2.py:185][0m |           0.0123 |         195.1611 |        -282.8829 |
[32m[20221214 14:38:42 @agent_ppo2.py:185][0m |          -0.0038 |         174.7334 |        -282.6402 |
[32m[20221214 14:38:42 @agent_ppo2.py:185][0m |          -0.0014 |         173.0295 |        -283.3891 |
[32m[20221214 14:38:42 @agent_ppo2.py:185][0m |          -0.0016 |         173.2098 |        -283.3819 |
[32m[20221214 14:38:42 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:38:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.12
[32m[20221214 14:38:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.61
[32m[20221214 14:38:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.48
[32m[20221214 14:38:42 @agent_ppo2.py:143][0m Total time:      40.66 min
[32m[20221214 14:38:42 @agent_ppo2.py:145][0m 3747840 total steps have happened
[32m[20221214 14:38:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1830 --------------------------#
[32m[20221214 14:38:42 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:38:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:42 @agent_ppo2.py:185][0m |          -0.0007 |         156.4485 |        -279.9361 |
[32m[20221214 14:38:42 @agent_ppo2.py:185][0m |          -0.0010 |         152.7938 |        -279.8979 |
[32m[20221214 14:38:42 @agent_ppo2.py:185][0m |          -0.0008 |         151.9983 |        -280.1415 |
[32m[20221214 14:38:42 @agent_ppo2.py:185][0m |          -0.0013 |         151.5835 |        -279.6059 |
[32m[20221214 14:38:43 @agent_ppo2.py:185][0m |          -0.0004 |         150.6886 |        -279.7910 |
[32m[20221214 14:38:43 @agent_ppo2.py:185][0m |          -0.0037 |         150.5785 |        -279.1578 |
[32m[20221214 14:38:43 @agent_ppo2.py:185][0m |           0.0003 |         149.6933 |        -279.7445 |
[32m[20221214 14:38:43 @agent_ppo2.py:185][0m |          -0.0021 |         150.0401 |        -279.0890 |
[32m[20221214 14:38:43 @agent_ppo2.py:185][0m |          -0.0014 |         149.5115 |        -279.7201 |
[32m[20221214 14:38:43 @agent_ppo2.py:185][0m |           0.0064 |         153.2459 |        -279.6116 |
[32m[20221214 14:38:43 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:38:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.11
[32m[20221214 14:38:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.52
[32m[20221214 14:38:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.20
[32m[20221214 14:38:43 @agent_ppo2.py:143][0m Total time:      40.68 min
[32m[20221214 14:38:43 @agent_ppo2.py:145][0m 3749888 total steps have happened
[32m[20221214 14:38:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1831 --------------------------#
[32m[20221214 14:38:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:43 @agent_ppo2.py:185][0m |          -0.0032 |         167.8374 |        -277.8475 |
[32m[20221214 14:38:44 @agent_ppo2.py:185][0m |           0.0123 |         175.4743 |        -278.1881 |
[32m[20221214 14:38:44 @agent_ppo2.py:185][0m |          -0.0039 |         154.8658 |        -277.8541 |
[32m[20221214 14:38:44 @agent_ppo2.py:185][0m |           0.0058 |         157.2189 |        -277.1872 |
[32m[20221214 14:38:44 @agent_ppo2.py:185][0m |          -0.0009 |         151.6844 |        -278.0496 |
[32m[20221214 14:38:44 @agent_ppo2.py:185][0m |          -0.0030 |         150.1117 |        -278.2192 |
[32m[20221214 14:38:44 @agent_ppo2.py:185][0m |          -0.0024 |         150.9011 |        -278.2193 |
[32m[20221214 14:38:44 @agent_ppo2.py:185][0m |           0.0102 |         172.9287 |        -278.2514 |
[32m[20221214 14:38:44 @agent_ppo2.py:185][0m |           0.0094 |         157.1096 |        -278.0257 |
[32m[20221214 14:38:44 @agent_ppo2.py:185][0m |           0.0023 |         151.4947 |        -278.4505 |
[32m[20221214 14:38:44 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:38:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.50
[32m[20221214 14:38:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.42
[32m[20221214 14:38:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.30
[32m[20221214 14:38:44 @agent_ppo2.py:143][0m Total time:      40.70 min
[32m[20221214 14:38:44 @agent_ppo2.py:145][0m 3751936 total steps have happened
[32m[20221214 14:38:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1832 --------------------------#
[32m[20221214 14:38:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:45 @agent_ppo2.py:185][0m |           0.0270 |         193.6615 |        -281.2209 |
[32m[20221214 14:38:45 @agent_ppo2.py:185][0m |          -0.0010 |         165.5198 |        -281.5050 |
[32m[20221214 14:38:45 @agent_ppo2.py:185][0m |          -0.0017 |         159.2739 |        -281.0096 |
[32m[20221214 14:38:45 @agent_ppo2.py:185][0m |          -0.0023 |         158.5502 |        -281.1928 |
[32m[20221214 14:38:45 @agent_ppo2.py:185][0m |           0.0075 |         174.7002 |        -280.1469 |
[32m[20221214 14:38:45 @agent_ppo2.py:185][0m |          -0.0020 |         157.2755 |        -282.0327 |
[32m[20221214 14:38:45 @agent_ppo2.py:185][0m |           0.0038 |         157.6797 |        -281.4746 |
[32m[20221214 14:38:45 @agent_ppo2.py:185][0m |          -0.0025 |         153.6342 |        -281.6548 |
[32m[20221214 14:38:45 @agent_ppo2.py:185][0m |          -0.0022 |         154.1576 |        -281.6024 |
[32m[20221214 14:38:45 @agent_ppo2.py:185][0m |          -0.0007 |         153.1495 |        -281.3270 |
[32m[20221214 14:38:45 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:38:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.83
[32m[20221214 14:38:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.29
[32m[20221214 14:38:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.22
[32m[20221214 14:38:46 @agent_ppo2.py:143][0m Total time:      40.72 min
[32m[20221214 14:38:46 @agent_ppo2.py:145][0m 3753984 total steps have happened
[32m[20221214 14:38:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1833 --------------------------#
[32m[20221214 14:38:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:46 @agent_ppo2.py:185][0m |          -0.0015 |         182.6297 |        -282.3340 |
[32m[20221214 14:38:46 @agent_ppo2.py:185][0m |           0.0057 |         180.4040 |        -282.6400 |
[32m[20221214 14:38:46 @agent_ppo2.py:185][0m |           0.0078 |         181.4508 |        -282.9912 |
[32m[20221214 14:38:46 @agent_ppo2.py:185][0m |          -0.0027 |         169.9918 |        -282.3211 |
[32m[20221214 14:38:46 @agent_ppo2.py:185][0m |           0.0184 |         198.1774 |        -283.2034 |
[32m[20221214 14:38:46 @agent_ppo2.py:185][0m |           0.0015 |         169.8077 |        -283.1796 |
[32m[20221214 14:38:46 @agent_ppo2.py:185][0m |          -0.0028 |         166.5676 |        -283.4064 |
[32m[20221214 14:38:46 @agent_ppo2.py:185][0m |          -0.0013 |         166.3768 |        -284.1578 |
[32m[20221214 14:38:47 @agent_ppo2.py:185][0m |          -0.0003 |         165.2826 |        -284.2817 |
[32m[20221214 14:38:47 @agent_ppo2.py:185][0m |           0.0036 |         169.4826 |        -284.0701 |
[32m[20221214 14:38:47 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:38:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.89
[32m[20221214 14:38:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.60
[32m[20221214 14:38:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.17
[32m[20221214 14:38:47 @agent_ppo2.py:143][0m Total time:      40.75 min
[32m[20221214 14:38:47 @agent_ppo2.py:145][0m 3756032 total steps have happened
[32m[20221214 14:38:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1834 --------------------------#
[32m[20221214 14:38:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:38:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:47 @agent_ppo2.py:185][0m |           0.0012 |         200.2672 |        -287.3671 |
[32m[20221214 14:38:47 @agent_ppo2.py:185][0m |          -0.0015 |         195.6304 |        -287.1960 |
[32m[20221214 14:38:47 @agent_ppo2.py:185][0m |           0.0028 |         197.5357 |        -287.4568 |
[32m[20221214 14:38:48 @agent_ppo2.py:185][0m |           0.0005 |         196.4096 |        -287.8526 |
[32m[20221214 14:38:48 @agent_ppo2.py:185][0m |           0.0050 |         191.7219 |        -287.8393 |
[32m[20221214 14:38:48 @agent_ppo2.py:185][0m |          -0.0023 |         190.6686 |        -288.3145 |
[32m[20221214 14:38:48 @agent_ppo2.py:185][0m |           0.0104 |         202.7712 |        -288.3984 |
[32m[20221214 14:38:48 @agent_ppo2.py:185][0m |          -0.0007 |         189.7775 |        -288.4682 |
[32m[20221214 14:38:48 @agent_ppo2.py:185][0m |          -0.0001 |         189.0272 |        -288.9077 |
[32m[20221214 14:38:48 @agent_ppo2.py:185][0m |           0.0011 |         190.0213 |        -288.3987 |
[32m[20221214 14:38:48 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:38:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.95
[32m[20221214 14:38:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.39
[32m[20221214 14:38:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.21
[32m[20221214 14:38:48 @agent_ppo2.py:143][0m Total time:      40.77 min
[32m[20221214 14:38:48 @agent_ppo2.py:145][0m 3758080 total steps have happened
[32m[20221214 14:38:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1835 --------------------------#
[32m[20221214 14:38:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:38:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:49 @agent_ppo2.py:185][0m |           0.0016 |         213.2532 |        -287.0856 |
[32m[20221214 14:38:49 @agent_ppo2.py:185][0m |          -0.0034 |         210.7428 |        -286.7147 |
[32m[20221214 14:38:49 @agent_ppo2.py:185][0m |          -0.0010 |         207.9899 |        -287.0205 |
[32m[20221214 14:38:49 @agent_ppo2.py:185][0m |           0.0030 |         208.9587 |        -287.1039 |
[32m[20221214 14:38:49 @agent_ppo2.py:185][0m |          -0.0003 |         206.4797 |        -287.1207 |
[32m[20221214 14:38:49 @agent_ppo2.py:185][0m |          -0.0008 |         206.4335 |        -286.4719 |
[32m[20221214 14:38:49 @agent_ppo2.py:185][0m |           0.0009 |         207.2768 |        -286.6922 |
[32m[20221214 14:38:49 @agent_ppo2.py:185][0m |          -0.0007 |         205.0376 |        -286.7997 |
[32m[20221214 14:38:49 @agent_ppo2.py:185][0m |           0.0081 |         219.3552 |        -286.4610 |
[32m[20221214 14:38:49 @agent_ppo2.py:185][0m |           0.0002 |         205.5993 |        -286.1814 |
[32m[20221214 14:38:49 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:38:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.47
[32m[20221214 14:38:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.53
[32m[20221214 14:38:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.30
[32m[20221214 14:38:50 @agent_ppo2.py:143][0m Total time:      40.79 min
[32m[20221214 14:38:50 @agent_ppo2.py:145][0m 3760128 total steps have happened
[32m[20221214 14:38:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1836 --------------------------#
[32m[20221214 14:38:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:38:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:50 @agent_ppo2.py:185][0m |          -0.0028 |         204.6084 |        -284.1123 |
[32m[20221214 14:38:50 @agent_ppo2.py:185][0m |          -0.0027 |         202.4530 |        -284.8073 |
[32m[20221214 14:38:50 @agent_ppo2.py:185][0m |           0.0009 |         201.4001 |        -284.6306 |
[32m[20221214 14:38:50 @agent_ppo2.py:185][0m |          -0.0001 |         203.5881 |        -284.0507 |
[32m[20221214 14:38:50 @agent_ppo2.py:185][0m |           0.0010 |         201.8158 |        -284.5614 |
[32m[20221214 14:38:50 @agent_ppo2.py:185][0m |           0.0025 |         200.7688 |        -283.8543 |
[32m[20221214 14:38:51 @agent_ppo2.py:185][0m |          -0.0011 |         200.7093 |        -284.1143 |
[32m[20221214 14:38:51 @agent_ppo2.py:185][0m |          -0.0006 |         199.1442 |        -284.3599 |
[32m[20221214 14:38:51 @agent_ppo2.py:185][0m |          -0.0026 |         199.2997 |        -284.5086 |
[32m[20221214 14:38:51 @agent_ppo2.py:185][0m |           0.0003 |         199.1588 |        -283.8039 |
[32m[20221214 14:38:51 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:38:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.53
[32m[20221214 14:38:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.46
[32m[20221214 14:38:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.31
[32m[20221214 14:38:51 @agent_ppo2.py:143][0m Total time:      40.82 min
[32m[20221214 14:38:51 @agent_ppo2.py:145][0m 3762176 total steps have happened
[32m[20221214 14:38:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1837 --------------------------#
[32m[20221214 14:38:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:51 @agent_ppo2.py:185][0m |          -0.0022 |         218.6122 |        -288.2684 |
[32m[20221214 14:38:52 @agent_ppo2.py:185][0m |          -0.0039 |         217.1873 |        -288.1695 |
[32m[20221214 14:38:52 @agent_ppo2.py:185][0m |           0.0003 |         215.5299 |        -288.5516 |
[32m[20221214 14:38:52 @agent_ppo2.py:185][0m |          -0.0004 |         213.4427 |        -287.1870 |
[32m[20221214 14:38:52 @agent_ppo2.py:185][0m |          -0.0017 |         213.7865 |        -287.7794 |
[32m[20221214 14:38:52 @agent_ppo2.py:185][0m |          -0.0018 |         211.6947 |        -287.6052 |
[32m[20221214 14:38:52 @agent_ppo2.py:185][0m |          -0.0026 |         211.4282 |        -288.2661 |
[32m[20221214 14:38:52 @agent_ppo2.py:185][0m |          -0.0032 |         210.4437 |        -288.3130 |
[32m[20221214 14:38:52 @agent_ppo2.py:185][0m |           0.0096 |         225.6414 |        -288.0877 |
[32m[20221214 14:38:52 @agent_ppo2.py:185][0m |           0.0117 |         230.7818 |        -288.5490 |
[32m[20221214 14:38:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:38:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.12
[32m[20221214 14:38:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.45
[32m[20221214 14:38:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.22
[32m[20221214 14:38:52 @agent_ppo2.py:143][0m Total time:      40.84 min
[32m[20221214 14:38:52 @agent_ppo2.py:145][0m 3764224 total steps have happened
[32m[20221214 14:38:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1838 --------------------------#
[32m[20221214 14:38:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:38:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:53 @agent_ppo2.py:185][0m |           0.0095 |         193.2976 |        -283.9362 |
[32m[20221214 14:38:53 @agent_ppo2.py:185][0m |           0.0035 |         177.6060 |        -283.4616 |
[32m[20221214 14:38:53 @agent_ppo2.py:185][0m |           0.0137 |         183.3630 |        -283.3203 |
[32m[20221214 14:38:53 @agent_ppo2.py:185][0m |          -0.0056 |         168.9746 |        -283.7165 |
[32m[20221214 14:38:53 @agent_ppo2.py:185][0m |          -0.0033 |         167.1452 |        -283.8788 |
[32m[20221214 14:38:53 @agent_ppo2.py:185][0m |          -0.0016 |         165.5246 |        -283.1014 |
[32m[20221214 14:38:53 @agent_ppo2.py:185][0m |          -0.0010 |         165.2039 |        -283.2766 |
[32m[20221214 14:38:53 @agent_ppo2.py:185][0m |          -0.0017 |         163.8127 |        -283.3576 |
[32m[20221214 14:38:53 @agent_ppo2.py:185][0m |          -0.0076 |         164.0626 |        -283.1960 |
[32m[20221214 14:38:54 @agent_ppo2.py:185][0m |          -0.0031 |         164.4056 |        -283.4414 |
[32m[20221214 14:38:54 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:38:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.84
[32m[20221214 14:38:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.64
[32m[20221214 14:38:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.30
[32m[20221214 14:38:54 @agent_ppo2.py:143][0m Total time:      40.86 min
[32m[20221214 14:38:54 @agent_ppo2.py:145][0m 3766272 total steps have happened
[32m[20221214 14:38:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1839 --------------------------#
[32m[20221214 14:38:54 @agent_ppo2.py:127][0m Sampling time: 0.30 s by 5 slaves
[32m[20221214 14:38:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:54 @agent_ppo2.py:185][0m |           0.0000 |         204.7335 |        -287.4062 |
[32m[20221214 14:38:54 @agent_ppo2.py:185][0m |          -0.0021 |         202.2395 |        -286.0239 |
[32m[20221214 14:38:54 @agent_ppo2.py:185][0m |          -0.0017 |         199.7521 |        -287.1828 |
[32m[20221214 14:38:55 @agent_ppo2.py:185][0m |           0.0005 |         200.0281 |        -286.5889 |
[32m[20221214 14:38:55 @agent_ppo2.py:185][0m |          -0.0015 |         198.3897 |        -285.9303 |
[32m[20221214 14:38:55 @agent_ppo2.py:185][0m |          -0.0016 |         197.8804 |        -286.2335 |
[32m[20221214 14:38:55 @agent_ppo2.py:185][0m |          -0.0018 |         197.8420 |        -286.1067 |
[32m[20221214 14:38:55 @agent_ppo2.py:185][0m |           0.0085 |         208.8741 |        -285.6747 |
[32m[20221214 14:38:55 @agent_ppo2.py:185][0m |          -0.0017 |         197.6547 |        -285.9204 |
[32m[20221214 14:38:55 @agent_ppo2.py:185][0m |          -0.0024 |         197.1884 |        -285.6987 |
[32m[20221214 14:38:55 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:38:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.59
[32m[20221214 14:38:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.17
[32m[20221214 14:38:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.07
[32m[20221214 14:38:55 @agent_ppo2.py:143][0m Total time:      40.89 min
[32m[20221214 14:38:55 @agent_ppo2.py:145][0m 3768320 total steps have happened
[32m[20221214 14:38:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1840 --------------------------#
[32m[20221214 14:38:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:38:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:56 @agent_ppo2.py:185][0m |           0.0004 |         222.5913 |        -285.8634 |
[32m[20221214 14:38:56 @agent_ppo2.py:185][0m |          -0.0002 |         214.7521 |        -285.0618 |
[32m[20221214 14:38:56 @agent_ppo2.py:185][0m |           0.0150 |         224.9537 |        -285.3256 |
[32m[20221214 14:38:56 @agent_ppo2.py:185][0m |          -0.0017 |         211.2065 |        -285.6873 |
[32m[20221214 14:38:56 @agent_ppo2.py:185][0m |          -0.0016 |         208.0872 |        -285.9706 |
[32m[20221214 14:38:56 @agent_ppo2.py:185][0m |          -0.0012 |         206.3096 |        -286.0254 |
[32m[20221214 14:38:56 @agent_ppo2.py:185][0m |           0.0023 |         204.7675 |        -286.3227 |
[32m[20221214 14:38:56 @agent_ppo2.py:185][0m |           0.0148 |         230.7006 |        -286.5486 |
[32m[20221214 14:38:56 @agent_ppo2.py:185][0m |          -0.0028 |         203.5400 |        -286.0340 |
[32m[20221214 14:38:57 @agent_ppo2.py:185][0m |           0.0086 |         211.2051 |        -285.4640 |
[32m[20221214 14:38:57 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:38:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.62
[32m[20221214 14:38:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.41
[32m[20221214 14:38:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.94
[32m[20221214 14:38:57 @agent_ppo2.py:143][0m Total time:      40.91 min
[32m[20221214 14:38:57 @agent_ppo2.py:145][0m 3770368 total steps have happened
[32m[20221214 14:38:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1841 --------------------------#
[32m[20221214 14:38:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:38:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:57 @agent_ppo2.py:185][0m |           0.0029 |         201.5993 |        -287.4361 |
[32m[20221214 14:38:57 @agent_ppo2.py:185][0m |          -0.0010 |         195.4247 |        -288.8191 |
[32m[20221214 14:38:57 @agent_ppo2.py:185][0m |          -0.0011 |         191.3998 |        -288.6247 |
[32m[20221214 14:38:57 @agent_ppo2.py:185][0m |          -0.0011 |         188.8715 |        -286.8686 |
[32m[20221214 14:38:57 @agent_ppo2.py:185][0m |          -0.0022 |         186.3569 |        -288.2365 |
[32m[20221214 14:38:58 @agent_ppo2.py:185][0m |           0.0009 |         185.1539 |        -289.1281 |
[32m[20221214 14:38:58 @agent_ppo2.py:185][0m |          -0.0003 |         183.0506 |        -289.0832 |
[32m[20221214 14:38:58 @agent_ppo2.py:185][0m |          -0.0024 |         184.0056 |        -288.5665 |
[32m[20221214 14:38:58 @agent_ppo2.py:185][0m |          -0.0013 |         181.2342 |        -289.3489 |
[32m[20221214 14:38:58 @agent_ppo2.py:185][0m |          -0.0015 |         181.1109 |        -289.9195 |
[32m[20221214 14:38:58 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:38:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.28
[32m[20221214 14:38:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.89
[32m[20221214 14:38:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.34
[32m[20221214 14:38:58 @agent_ppo2.py:143][0m Total time:      40.93 min
[32m[20221214 14:38:58 @agent_ppo2.py:145][0m 3772416 total steps have happened
[32m[20221214 14:38:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1842 --------------------------#
[32m[20221214 14:38:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:38:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:38:58 @agent_ppo2.py:185][0m |           0.0004 |         180.6551 |        -294.9740 |
[32m[20221214 14:38:59 @agent_ppo2.py:185][0m |          -0.0020 |         175.8390 |        -295.2301 |
[32m[20221214 14:38:59 @agent_ppo2.py:185][0m |          -0.0013 |         173.8343 |        -294.7483 |
[32m[20221214 14:38:59 @agent_ppo2.py:185][0m |          -0.0037 |         173.2071 |        -295.5042 |
[32m[20221214 14:38:59 @agent_ppo2.py:185][0m |          -0.0013 |         171.7110 |        -294.8736 |
[32m[20221214 14:38:59 @agent_ppo2.py:185][0m |          -0.0008 |         171.3054 |        -294.1628 |
[32m[20221214 14:38:59 @agent_ppo2.py:185][0m |          -0.0057 |         170.8216 |        -295.0823 |
[32m[20221214 14:38:59 @agent_ppo2.py:185][0m |          -0.0036 |         169.8336 |        -294.6113 |
[32m[20221214 14:38:59 @agent_ppo2.py:185][0m |          -0.0044 |         168.7059 |        -294.7067 |
[32m[20221214 14:38:59 @agent_ppo2.py:185][0m |          -0.0038 |         169.6725 |        -294.2260 |
[32m[20221214 14:38:59 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:38:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.84
[32m[20221214 14:38:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.71
[32m[20221214 14:38:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.78
[32m[20221214 14:38:59 @agent_ppo2.py:143][0m Total time:      40.96 min
[32m[20221214 14:38:59 @agent_ppo2.py:145][0m 3774464 total steps have happened
[32m[20221214 14:38:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1843 --------------------------#
[32m[20221214 14:39:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:00 @agent_ppo2.py:185][0m |           0.0016 |         178.0845 |        -286.4135 |
[32m[20221214 14:39:00 @agent_ppo2.py:185][0m |          -0.0002 |         173.0567 |        -286.1567 |
[32m[20221214 14:39:00 @agent_ppo2.py:185][0m |          -0.0023 |         172.1839 |        -286.2695 |
[32m[20221214 14:39:00 @agent_ppo2.py:185][0m |           0.0013 |         171.6488 |        -286.1959 |
[32m[20221214 14:39:00 @agent_ppo2.py:185][0m |          -0.0022 |         171.3646 |        -285.5991 |
[32m[20221214 14:39:00 @agent_ppo2.py:185][0m |          -0.0018 |         170.3133 |        -286.3401 |
[32m[20221214 14:39:00 @agent_ppo2.py:185][0m |           0.0021 |         173.2490 |        -285.6697 |
[32m[20221214 14:39:00 @agent_ppo2.py:185][0m |           0.0017 |         169.7911 |        -286.0303 |
[32m[20221214 14:39:01 @agent_ppo2.py:185][0m |           0.0072 |         181.7231 |        -285.1690 |
[32m[20221214 14:39:01 @agent_ppo2.py:185][0m |          -0.0038 |         170.2309 |        -285.4523 |
[32m[20221214 14:39:01 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:39:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.71
[32m[20221214 14:39:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.93
[32m[20221214 14:39:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.29
[32m[20221214 14:39:01 @agent_ppo2.py:143][0m Total time:      40.98 min
[32m[20221214 14:39:01 @agent_ppo2.py:145][0m 3776512 total steps have happened
[32m[20221214 14:39:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1844 --------------------------#
[32m[20221214 14:39:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:01 @agent_ppo2.py:185][0m |          -0.0015 |         190.1312 |        -286.3645 |
[32m[20221214 14:39:01 @agent_ppo2.py:185][0m |          -0.0010 |         187.2897 |        -284.8144 |
[32m[20221214 14:39:01 @agent_ppo2.py:185][0m |          -0.0025 |         185.0037 |        -286.0441 |
[32m[20221214 14:39:02 @agent_ppo2.py:185][0m |           0.0011 |         184.4300 |        -285.4526 |
[32m[20221214 14:39:02 @agent_ppo2.py:185][0m |          -0.0021 |         183.1899 |        -286.6030 |
[32m[20221214 14:39:02 @agent_ppo2.py:185][0m |          -0.0021 |         181.9775 |        -285.8188 |
[32m[20221214 14:39:02 @agent_ppo2.py:185][0m |           0.0014 |         182.0981 |        -285.0373 |
[32m[20221214 14:39:02 @agent_ppo2.py:185][0m |          -0.0032 |         181.3982 |        -285.3260 |
[32m[20221214 14:39:02 @agent_ppo2.py:185][0m |          -0.0014 |         182.1148 |        -284.9648 |
[32m[20221214 14:39:02 @agent_ppo2.py:185][0m |          -0.0021 |         179.9820 |        -285.2184 |
[32m[20221214 14:39:02 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:39:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.20
[32m[20221214 14:39:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.26
[32m[20221214 14:39:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.05
[32m[20221214 14:39:02 @agent_ppo2.py:143][0m Total time:      41.00 min
[32m[20221214 14:39:02 @agent_ppo2.py:145][0m 3778560 total steps have happened
[32m[20221214 14:39:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1845 --------------------------#
[32m[20221214 14:39:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:03 @agent_ppo2.py:185][0m |          -0.0020 |         195.0549 |        -284.8555 |
[32m[20221214 14:39:03 @agent_ppo2.py:185][0m |          -0.0019 |         190.5719 |        -284.8289 |
[32m[20221214 14:39:03 @agent_ppo2.py:185][0m |           0.0032 |         191.7845 |        -285.1354 |
[32m[20221214 14:39:03 @agent_ppo2.py:185][0m |          -0.0019 |         184.7193 |        -285.5676 |
[32m[20221214 14:39:03 @agent_ppo2.py:185][0m |          -0.0015 |         185.9241 |        -285.3986 |
[32m[20221214 14:39:03 @agent_ppo2.py:185][0m |          -0.0000 |         182.7643 |        -285.5200 |
[32m[20221214 14:39:03 @agent_ppo2.py:185][0m |          -0.0042 |         182.7145 |        -285.7190 |
[32m[20221214 14:39:03 @agent_ppo2.py:185][0m |          -0.0039 |         183.3414 |        -285.8054 |
[32m[20221214 14:39:03 @agent_ppo2.py:185][0m |          -0.0013 |         181.7455 |        -285.9767 |
[32m[20221214 14:39:03 @agent_ppo2.py:185][0m |          -0.0014 |         183.0009 |        -286.0721 |
[32m[20221214 14:39:03 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:39:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.01
[32m[20221214 14:39:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.13
[32m[20221214 14:39:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.06
[32m[20221214 14:39:04 @agent_ppo2.py:143][0m Total time:      41.03 min
[32m[20221214 14:39:04 @agent_ppo2.py:145][0m 3780608 total steps have happened
[32m[20221214 14:39:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1846 --------------------------#
[32m[20221214 14:39:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:04 @agent_ppo2.py:185][0m |           0.0015 |         214.9694 |        -287.0462 |
[32m[20221214 14:39:04 @agent_ppo2.py:185][0m |           0.0068 |         225.0758 |        -286.3745 |
[32m[20221214 14:39:04 @agent_ppo2.py:185][0m |          -0.0024 |         210.5864 |        -286.9856 |
[32m[20221214 14:39:04 @agent_ppo2.py:185][0m |          -0.0044 |         208.5363 |        -286.5927 |
[32m[20221214 14:39:04 @agent_ppo2.py:185][0m |          -0.0006 |         208.8339 |        -286.0802 |
[32m[20221214 14:39:04 @agent_ppo2.py:185][0m |           0.0086 |         221.1965 |        -286.9127 |
[32m[20221214 14:39:05 @agent_ppo2.py:185][0m |          -0.0021 |         207.8213 |        -286.4394 |
[32m[20221214 14:39:05 @agent_ppo2.py:185][0m |           0.0076 |         229.0266 |        -286.0870 |
[32m[20221214 14:39:05 @agent_ppo2.py:185][0m |          -0.0015 |         207.4708 |        -284.3931 |
[32m[20221214 14:39:05 @agent_ppo2.py:185][0m |          -0.0011 |         206.4483 |        -286.0277 |
[32m[20221214 14:39:05 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:39:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.26
[32m[20221214 14:39:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.51
[32m[20221214 14:39:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.99
[32m[20221214 14:39:05 @agent_ppo2.py:143][0m Total time:      41.05 min
[32m[20221214 14:39:05 @agent_ppo2.py:145][0m 3782656 total steps have happened
[32m[20221214 14:39:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1847 --------------------------#
[32m[20221214 14:39:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:05 @agent_ppo2.py:185][0m |          -0.0032 |         136.4144 |        -284.2166 |
[32m[20221214 14:39:05 @agent_ppo2.py:185][0m |           0.0012 |         125.0803 |        -284.2690 |
[32m[20221214 14:39:06 @agent_ppo2.py:185][0m |           0.0003 |         120.1386 |        -284.6278 |
[32m[20221214 14:39:06 @agent_ppo2.py:185][0m |           0.0001 |         120.4457 |        -285.1895 |
[32m[20221214 14:39:06 @agent_ppo2.py:185][0m |           0.0020 |         116.9727 |        -285.4070 |
[32m[20221214 14:39:06 @agent_ppo2.py:185][0m |          -0.0031 |         116.5546 |        -285.3504 |
[32m[20221214 14:39:06 @agent_ppo2.py:185][0m |          -0.0047 |         114.2700 |        -285.6461 |
[32m[20221214 14:39:06 @agent_ppo2.py:185][0m |          -0.0048 |         113.5691 |        -285.7410 |
[32m[20221214 14:39:06 @agent_ppo2.py:185][0m |          -0.0042 |         112.0805 |        -286.2074 |
[32m[20221214 14:39:06 @agent_ppo2.py:185][0m |          -0.0024 |         111.8845 |        -286.4368 |
[32m[20221214 14:39:06 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:39:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.82
[32m[20221214 14:39:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.33
[32m[20221214 14:39:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.38
[32m[20221214 14:39:06 @agent_ppo2.py:143][0m Total time:      41.07 min
[32m[20221214 14:39:06 @agent_ppo2.py:145][0m 3784704 total steps have happened
[32m[20221214 14:39:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1848 --------------------------#
[32m[20221214 14:39:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:39:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:07 @agent_ppo2.py:185][0m |           0.0061 |         211.1928 |        -294.1685 |
[32m[20221214 14:39:07 @agent_ppo2.py:185][0m |          -0.0003 |         199.1449 |        -293.9535 |
[32m[20221214 14:39:07 @agent_ppo2.py:185][0m |           0.0011 |         191.2464 |        -294.5178 |
[32m[20221214 14:39:07 @agent_ppo2.py:185][0m |          -0.0023 |         190.1083 |        -293.3992 |
[32m[20221214 14:39:07 @agent_ppo2.py:185][0m |          -0.0011 |         187.0960 |        -294.1389 |
[32m[20221214 14:39:07 @agent_ppo2.py:185][0m |           0.0035 |         190.1282 |        -294.3931 |
[32m[20221214 14:39:07 @agent_ppo2.py:185][0m |          -0.0007 |         185.0327 |        -294.5541 |
[32m[20221214 14:39:07 @agent_ppo2.py:185][0m |          -0.0038 |         185.1725 |        -294.3249 |
[32m[20221214 14:39:07 @agent_ppo2.py:185][0m |           0.0046 |         186.8256 |        -294.9464 |
[32m[20221214 14:39:07 @agent_ppo2.py:185][0m |          -0.0012 |         183.4332 |        -294.6296 |
[32m[20221214 14:39:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 14:39:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.59
[32m[20221214 14:39:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.29
[32m[20221214 14:39:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.26
[32m[20221214 14:39:08 @agent_ppo2.py:143][0m Total time:      41.09 min
[32m[20221214 14:39:08 @agent_ppo2.py:145][0m 3786752 total steps have happened
[32m[20221214 14:39:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1849 --------------------------#
[32m[20221214 14:39:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:08 @agent_ppo2.py:185][0m |          -0.0002 |         170.3988 |        -291.6799 |
[32m[20221214 14:39:08 @agent_ppo2.py:185][0m |          -0.0025 |         168.9869 |        -291.5330 |
[32m[20221214 14:39:08 @agent_ppo2.py:185][0m |          -0.0007 |         169.5379 |        -291.4272 |
[32m[20221214 14:39:08 @agent_ppo2.py:185][0m |           0.0006 |         167.2054 |        -290.9028 |
[32m[20221214 14:39:08 @agent_ppo2.py:185][0m |           0.0039 |         168.3901 |        -290.9320 |
[32m[20221214 14:39:08 @agent_ppo2.py:185][0m |           0.0003 |         166.2675 |        -291.2930 |
[32m[20221214 14:39:09 @agent_ppo2.py:185][0m |           0.0022 |         165.8051 |        -291.3856 |
[32m[20221214 14:39:09 @agent_ppo2.py:185][0m |          -0.0000 |         165.7517 |        -291.2817 |
[32m[20221214 14:39:09 @agent_ppo2.py:185][0m |          -0.0018 |         164.4744 |        -291.2942 |
[32m[20221214 14:39:09 @agent_ppo2.py:185][0m |          -0.0025 |         164.3194 |        -291.2346 |
[32m[20221214 14:39:09 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:39:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.04
[32m[20221214 14:39:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.65
[32m[20221214 14:39:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.56
[32m[20221214 14:39:09 @agent_ppo2.py:143][0m Total time:      41.12 min
[32m[20221214 14:39:09 @agent_ppo2.py:145][0m 3788800 total steps have happened
[32m[20221214 14:39:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1850 --------------------------#
[32m[20221214 14:39:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:39:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:09 @agent_ppo2.py:185][0m |          -0.0001 |         193.8717 |        -290.0019 |
[32m[20221214 14:39:09 @agent_ppo2.py:185][0m |          -0.0023 |         186.2868 |        -290.3641 |
[32m[20221214 14:39:10 @agent_ppo2.py:185][0m |          -0.0023 |         183.1592 |        -290.6808 |
[32m[20221214 14:39:10 @agent_ppo2.py:185][0m |          -0.0041 |         182.2898 |        -290.2156 |
[32m[20221214 14:39:10 @agent_ppo2.py:185][0m |           0.0081 |         200.7808 |        -290.5487 |
[32m[20221214 14:39:10 @agent_ppo2.py:185][0m |           0.0108 |         188.2972 |        -290.9841 |
[32m[20221214 14:39:10 @agent_ppo2.py:185][0m |          -0.0059 |         179.1660 |        -291.1576 |
[32m[20221214 14:39:10 @agent_ppo2.py:185][0m |          -0.0038 |         178.3639 |        -290.9201 |
[32m[20221214 14:39:10 @agent_ppo2.py:185][0m |           0.0096 |         203.0073 |        -290.9996 |
[32m[20221214 14:39:10 @agent_ppo2.py:185][0m |          -0.0042 |         179.1187 |        -290.5693 |
[32m[20221214 14:39:10 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:39:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.10
[32m[20221214 14:39:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.42
[32m[20221214 14:39:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.24
[32m[20221214 14:39:10 @agent_ppo2.py:143][0m Total time:      41.14 min
[32m[20221214 14:39:10 @agent_ppo2.py:145][0m 3790848 total steps have happened
[32m[20221214 14:39:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1851 --------------------------#
[32m[20221214 14:39:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:11 @agent_ppo2.py:185][0m |           0.0062 |         234.3000 |        -289.5760 |
[32m[20221214 14:39:11 @agent_ppo2.py:185][0m |          -0.0024 |         212.6211 |        -289.3477 |
[32m[20221214 14:39:11 @agent_ppo2.py:185][0m |          -0.0020 |         208.8706 |        -289.9353 |
[32m[20221214 14:39:11 @agent_ppo2.py:185][0m |           0.0074 |         207.1881 |        -289.2998 |
[32m[20221214 14:39:11 @agent_ppo2.py:185][0m |          -0.0023 |         202.7948 |        -289.7224 |
[32m[20221214 14:39:11 @agent_ppo2.py:185][0m |           0.0073 |         210.1287 |        -289.6252 |
[32m[20221214 14:39:11 @agent_ppo2.py:185][0m |          -0.0009 |         201.0518 |        -288.6104 |
[32m[20221214 14:39:11 @agent_ppo2.py:185][0m |           0.0027 |         201.7118 |        -289.9377 |
[32m[20221214 14:39:11 @agent_ppo2.py:185][0m |          -0.0024 |         200.0341 |        -289.3756 |
[32m[20221214 14:39:12 @agent_ppo2.py:185][0m |          -0.0023 |         198.5114 |        -289.9377 |
[32m[20221214 14:39:12 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:39:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.76
[32m[20221214 14:39:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.39
[32m[20221214 14:39:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.84
[32m[20221214 14:39:12 @agent_ppo2.py:143][0m Total time:      41.16 min
[32m[20221214 14:39:12 @agent_ppo2.py:145][0m 3792896 total steps have happened
[32m[20221214 14:39:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1852 --------------------------#
[32m[20221214 14:39:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:12 @agent_ppo2.py:185][0m |          -0.0010 |         218.8449 |        -294.5451 |
[32m[20221214 14:39:12 @agent_ppo2.py:185][0m |          -0.0029 |         206.4792 |        -294.0481 |
[32m[20221214 14:39:12 @agent_ppo2.py:185][0m |           0.0033 |         201.1689 |        -293.9268 |
[32m[20221214 14:39:12 @agent_ppo2.py:185][0m |          -0.0021 |         199.9467 |        -294.3058 |
[32m[20221214 14:39:12 @agent_ppo2.py:185][0m |          -0.0050 |         196.9268 |        -295.0934 |
[32m[20221214 14:39:13 @agent_ppo2.py:185][0m |          -0.0027 |         199.3792 |        -295.2138 |
[32m[20221214 14:39:13 @agent_ppo2.py:185][0m |          -0.0010 |         195.4089 |        -295.0879 |
[32m[20221214 14:39:13 @agent_ppo2.py:185][0m |          -0.0029 |         194.7930 |        -295.2946 |
[32m[20221214 14:39:13 @agent_ppo2.py:185][0m |           0.0010 |         194.7092 |        -295.2936 |
[32m[20221214 14:39:13 @agent_ppo2.py:185][0m |          -0.0015 |         192.8130 |        -295.1547 |
[32m[20221214 14:39:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:39:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.78
[32m[20221214 14:39:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.24
[32m[20221214 14:39:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.00
[32m[20221214 14:39:13 @agent_ppo2.py:143][0m Total time:      41.18 min
[32m[20221214 14:39:13 @agent_ppo2.py:145][0m 3794944 total steps have happened
[32m[20221214 14:39:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1853 --------------------------#
[32m[20221214 14:39:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:13 @agent_ppo2.py:185][0m |          -0.0037 |         205.9285 |        -291.6617 |
[32m[20221214 14:39:14 @agent_ppo2.py:185][0m |          -0.0031 |         201.0450 |        -292.4229 |
[32m[20221214 14:39:14 @agent_ppo2.py:185][0m |          -0.0037 |         199.6280 |        -292.2480 |
[32m[20221214 14:39:14 @agent_ppo2.py:185][0m |           0.0135 |         230.8424 |        -293.0126 |
[32m[20221214 14:39:14 @agent_ppo2.py:185][0m |           0.0055 |         212.2512 |        -293.1189 |
[32m[20221214 14:39:14 @agent_ppo2.py:185][0m |          -0.0024 |         194.9511 |        -292.8094 |
[32m[20221214 14:39:14 @agent_ppo2.py:185][0m |           0.0096 |         209.7089 |        -293.1079 |
[32m[20221214 14:39:14 @agent_ppo2.py:185][0m |          -0.0017 |         195.7007 |        -293.7229 |
[32m[20221214 14:39:14 @agent_ppo2.py:185][0m |          -0.0035 |         194.2454 |        -293.2239 |
[32m[20221214 14:39:14 @agent_ppo2.py:185][0m |           0.0110 |         214.2456 |        -293.8912 |
[32m[20221214 14:39:14 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:39:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.66
[32m[20221214 14:39:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.12
[32m[20221214 14:39:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.23
[32m[20221214 14:39:14 @agent_ppo2.py:143][0m Total time:      41.21 min
[32m[20221214 14:39:14 @agent_ppo2.py:145][0m 3796992 total steps have happened
[32m[20221214 14:39:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1854 --------------------------#
[32m[20221214 14:39:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:15 @agent_ppo2.py:185][0m |           0.0020 |         232.3586 |        -300.1170 |
[32m[20221214 14:39:15 @agent_ppo2.py:185][0m |          -0.0017 |         223.0930 |        -300.0588 |
[32m[20221214 14:39:15 @agent_ppo2.py:185][0m |          -0.0014 |         219.9472 |        -301.1094 |
[32m[20221214 14:39:15 @agent_ppo2.py:185][0m |          -0.0012 |         218.7721 |        -300.4587 |
[32m[20221214 14:39:15 @agent_ppo2.py:185][0m |          -0.0025 |         215.6750 |        -300.7667 |
[32m[20221214 14:39:15 @agent_ppo2.py:185][0m |          -0.0027 |         214.1070 |        -301.3488 |
[32m[20221214 14:39:15 @agent_ppo2.py:185][0m |          -0.0036 |         212.5103 |        -301.0627 |
[32m[20221214 14:39:15 @agent_ppo2.py:185][0m |          -0.0005 |         210.9104 |        -301.6889 |
[32m[20221214 14:39:16 @agent_ppo2.py:185][0m |          -0.0011 |         210.9098 |        -301.7745 |
[32m[20221214 14:39:16 @agent_ppo2.py:185][0m |          -0.0019 |         210.0757 |        -302.0444 |
[32m[20221214 14:39:16 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:39:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.70
[32m[20221214 14:39:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.19
[32m[20221214 14:39:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.37
[32m[20221214 14:39:16 @agent_ppo2.py:143][0m Total time:      41.23 min
[32m[20221214 14:39:16 @agent_ppo2.py:145][0m 3799040 total steps have happened
[32m[20221214 14:39:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1855 --------------------------#
[32m[20221214 14:39:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:16 @agent_ppo2.py:185][0m |           0.0054 |         231.6379 |        -297.4575 |
[32m[20221214 14:39:16 @agent_ppo2.py:185][0m |           0.0130 |         260.9557 |        -297.8732 |
[32m[20221214 14:39:16 @agent_ppo2.py:185][0m |          -0.0024 |         223.0342 |        -298.2588 |
[32m[20221214 14:39:16 @agent_ppo2.py:185][0m |          -0.0006 |         221.9698 |        -297.7154 |
[32m[20221214 14:39:17 @agent_ppo2.py:185][0m |          -0.0016 |         221.4436 |        -297.5311 |
[32m[20221214 14:39:17 @agent_ppo2.py:185][0m |          -0.0016 |         219.7971 |        -298.0905 |
[32m[20221214 14:39:17 @agent_ppo2.py:185][0m |          -0.0002 |         219.0740 |        -297.6451 |
[32m[20221214 14:39:17 @agent_ppo2.py:185][0m |          -0.0010 |         218.1813 |        -298.3975 |
[32m[20221214 14:39:17 @agent_ppo2.py:185][0m |           0.0050 |         223.7639 |        -298.2047 |
[32m[20221214 14:39:17 @agent_ppo2.py:185][0m |          -0.0039 |         218.4134 |        -297.7722 |
[32m[20221214 14:39:17 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:39:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.93
[32m[20221214 14:39:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.14
[32m[20221214 14:39:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.96
[32m[20221214 14:39:17 @agent_ppo2.py:143][0m Total time:      41.25 min
[32m[20221214 14:39:17 @agent_ppo2.py:145][0m 3801088 total steps have happened
[32m[20221214 14:39:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1856 --------------------------#
[32m[20221214 14:39:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:18 @agent_ppo2.py:185][0m |          -0.0001 |         255.2258 |        -303.3088 |
[32m[20221214 14:39:18 @agent_ppo2.py:185][0m |           0.0021 |         251.1718 |        -304.8997 |
[32m[20221214 14:39:18 @agent_ppo2.py:185][0m |           0.0005 |         245.9438 |        -303.8824 |
[32m[20221214 14:39:18 @agent_ppo2.py:185][0m |           0.0002 |         245.1222 |        -303.9844 |
[32m[20221214 14:39:18 @agent_ppo2.py:185][0m |           0.0013 |         243.9309 |        -304.2393 |
[32m[20221214 14:39:18 @agent_ppo2.py:185][0m |           0.0003 |         243.7093 |        -304.7224 |
[32m[20221214 14:39:18 @agent_ppo2.py:185][0m |           0.0000 |         244.4961 |        -303.7720 |
[32m[20221214 14:39:18 @agent_ppo2.py:185][0m |          -0.0012 |         242.7843 |        -304.0107 |
[32m[20221214 14:39:18 @agent_ppo2.py:185][0m |          -0.0011 |         243.6160 |        -304.5985 |
[32m[20221214 14:39:18 @agent_ppo2.py:185][0m |          -0.0025 |         241.9329 |        -304.5062 |
[32m[20221214 14:39:18 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:39:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.45
[32m[20221214 14:39:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.33
[32m[20221214 14:39:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.30
[32m[20221214 14:39:19 @agent_ppo2.py:143][0m Total time:      41.27 min
[32m[20221214 14:39:19 @agent_ppo2.py:145][0m 3803136 total steps have happened
[32m[20221214 14:39:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1857 --------------------------#
[32m[20221214 14:39:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:19 @agent_ppo2.py:185][0m |           0.0040 |         240.0541 |        -302.7142 |
[32m[20221214 14:39:19 @agent_ppo2.py:185][0m |           0.0053 |         231.3077 |        -303.4938 |
[32m[20221214 14:39:19 @agent_ppo2.py:185][0m |          -0.0006 |         226.8911 |        -302.9040 |
[32m[20221214 14:39:19 @agent_ppo2.py:185][0m |           0.0057 |         230.0232 |        -302.9710 |
[32m[20221214 14:39:19 @agent_ppo2.py:185][0m |          -0.0032 |         224.3985 |        -303.6189 |
[32m[20221214 14:39:19 @agent_ppo2.py:185][0m |           0.0064 |         227.8191 |        -303.0185 |
[32m[20221214 14:39:19 @agent_ppo2.py:185][0m |           0.0081 |         229.9072 |        -302.4237 |
[32m[20221214 14:39:20 @agent_ppo2.py:185][0m |          -0.0024 |         220.9655 |        -303.0723 |
[32m[20221214 14:39:20 @agent_ppo2.py:185][0m |           0.0114 |         239.2995 |        -302.7069 |
[32m[20221214 14:39:20 @agent_ppo2.py:185][0m |          -0.0006 |         222.5403 |        -303.0845 |
[32m[20221214 14:39:20 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:39:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.47
[32m[20221214 14:39:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.88
[32m[20221214 14:39:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.08
[32m[20221214 14:39:20 @agent_ppo2.py:143][0m Total time:      41.30 min
[32m[20221214 14:39:20 @agent_ppo2.py:145][0m 3805184 total steps have happened
[32m[20221214 14:39:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1858 --------------------------#
[32m[20221214 14:39:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:20 @agent_ppo2.py:185][0m |          -0.0024 |         296.5299 |        -307.4363 |
[32m[20221214 14:39:20 @agent_ppo2.py:185][0m |          -0.0017 |         285.3368 |        -307.6256 |
[32m[20221214 14:39:20 @agent_ppo2.py:185][0m |          -0.0001 |         283.9860 |        -307.7956 |
[32m[20221214 14:39:21 @agent_ppo2.py:185][0m |          -0.0023 |         281.2919 |        -308.3740 |
[32m[20221214 14:39:21 @agent_ppo2.py:185][0m |          -0.0024 |         279.8945 |        -307.4834 |
[32m[20221214 14:39:21 @agent_ppo2.py:185][0m |          -0.0027 |         279.0418 |        -307.3659 |
[32m[20221214 14:39:21 @agent_ppo2.py:185][0m |          -0.0040 |         278.0337 |        -308.0391 |
[32m[20221214 14:39:21 @agent_ppo2.py:185][0m |          -0.0030 |         276.6545 |        -307.1034 |
[32m[20221214 14:39:21 @agent_ppo2.py:185][0m |          -0.0013 |         276.4366 |        -307.2718 |
[32m[20221214 14:39:21 @agent_ppo2.py:185][0m |          -0.0022 |         276.4115 |        -307.1188 |
[32m[20221214 14:39:21 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:39:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.30
[32m[20221214 14:39:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.23
[32m[20221214 14:39:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.24
[32m[20221214 14:39:21 @agent_ppo2.py:143][0m Total time:      41.32 min
[32m[20221214 14:39:21 @agent_ppo2.py:145][0m 3807232 total steps have happened
[32m[20221214 14:39:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1859 --------------------------#
[32m[20221214 14:39:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:39:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:22 @agent_ppo2.py:185][0m |          -0.0002 |         268.8616 |        -306.1278 |
[32m[20221214 14:39:22 @agent_ppo2.py:185][0m |          -0.0004 |         263.3232 |        -305.5575 |
[32m[20221214 14:39:22 @agent_ppo2.py:185][0m |          -0.0019 |         262.9116 |        -305.5745 |
[32m[20221214 14:39:22 @agent_ppo2.py:185][0m |          -0.0011 |         261.6569 |        -305.4406 |
[32m[20221214 14:39:22 @agent_ppo2.py:185][0m |          -0.0025 |         260.5821 |        -304.9557 |
[32m[20221214 14:39:22 @agent_ppo2.py:185][0m |          -0.0021 |         260.0665 |        -304.9190 |
[32m[20221214 14:39:22 @agent_ppo2.py:185][0m |          -0.0016 |         259.6660 |        -305.0266 |
[32m[20221214 14:39:22 @agent_ppo2.py:185][0m |          -0.0027 |         259.4773 |        -305.5926 |
[32m[20221214 14:39:22 @agent_ppo2.py:185][0m |          -0.0050 |         259.5032 |        -304.6565 |
[32m[20221214 14:39:22 @agent_ppo2.py:185][0m |          -0.0027 |         258.8850 |        -305.2712 |
[32m[20221214 14:39:22 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:39:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.69
[32m[20221214 14:39:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.12
[32m[20221214 14:39:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.36
[32m[20221214 14:39:23 @agent_ppo2.py:143][0m Total time:      41.34 min
[32m[20221214 14:39:23 @agent_ppo2.py:145][0m 3809280 total steps have happened
[32m[20221214 14:39:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1860 --------------------------#
[32m[20221214 14:39:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:39:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:23 @agent_ppo2.py:185][0m |          -0.0010 |         288.6593 |        -301.6232 |
[32m[20221214 14:39:23 @agent_ppo2.py:185][0m |           0.0085 |         307.4944 |        -302.6808 |
[32m[20221214 14:39:23 @agent_ppo2.py:185][0m |          -0.0024 |         284.1544 |        -302.9086 |
[32m[20221214 14:39:23 @agent_ppo2.py:185][0m |          -0.0028 |         282.6282 |        -302.1128 |
[32m[20221214 14:39:23 @agent_ppo2.py:185][0m |          -0.0036 |         282.0960 |        -302.0720 |
[32m[20221214 14:39:23 @agent_ppo2.py:185][0m |          -0.0036 |         282.1369 |        -301.8533 |
[32m[20221214 14:39:23 @agent_ppo2.py:185][0m |          -0.0020 |         281.5597 |        -302.7141 |
[32m[20221214 14:39:24 @agent_ppo2.py:185][0m |          -0.0023 |         280.9348 |        -301.9281 |
[32m[20221214 14:39:24 @agent_ppo2.py:185][0m |          -0.0032 |         280.7293 |        -303.0786 |
[32m[20221214 14:39:24 @agent_ppo2.py:185][0m |          -0.0028 |         280.8148 |        -303.0038 |
[32m[20221214 14:39:24 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:39:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.53
[32m[20221214 14:39:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.88
[32m[20221214 14:39:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.16
[32m[20221214 14:39:24 @agent_ppo2.py:143][0m Total time:      41.36 min
[32m[20221214 14:39:24 @agent_ppo2.py:145][0m 3811328 total steps have happened
[32m[20221214 14:39:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1861 --------------------------#
[32m[20221214 14:39:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:39:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:24 @agent_ppo2.py:185][0m |          -0.0004 |         269.9854 |        -302.5415 |
[32m[20221214 14:39:24 @agent_ppo2.py:185][0m |          -0.0015 |         261.9531 |        -302.3171 |
[32m[20221214 14:39:25 @agent_ppo2.py:185][0m |          -0.0012 |         258.8279 |        -301.8237 |
[32m[20221214 14:39:25 @agent_ppo2.py:185][0m |          -0.0016 |         254.6718 |        -302.4538 |
[32m[20221214 14:39:25 @agent_ppo2.py:185][0m |          -0.0009 |         253.2082 |        -302.4622 |
[32m[20221214 14:39:25 @agent_ppo2.py:185][0m |          -0.0013 |         252.3418 |        -302.9346 |
[32m[20221214 14:39:25 @agent_ppo2.py:185][0m |           0.0122 |         292.6625 |        -302.2239 |
[32m[20221214 14:39:25 @agent_ppo2.py:185][0m |           0.0027 |         254.9557 |        -302.8648 |
[32m[20221214 14:39:25 @agent_ppo2.py:185][0m |          -0.0014 |         250.7000 |        -302.9772 |
[32m[20221214 14:39:25 @agent_ppo2.py:185][0m |          -0.0006 |         249.8845 |        -301.9964 |
[32m[20221214 14:39:25 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:39:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.94
[32m[20221214 14:39:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.75
[32m[20221214 14:39:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.02
[32m[20221214 14:39:25 @agent_ppo2.py:143][0m Total time:      41.39 min
[32m[20221214 14:39:25 @agent_ppo2.py:145][0m 3813376 total steps have happened
[32m[20221214 14:39:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1862 --------------------------#
[32m[20221214 14:39:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:26 @agent_ppo2.py:185][0m |          -0.0001 |         271.1537 |        -307.9495 |
[32m[20221214 14:39:26 @agent_ppo2.py:185][0m |          -0.0020 |         260.1971 |        -307.5916 |
[32m[20221214 14:39:26 @agent_ppo2.py:185][0m |           0.0013 |         258.0378 |        -307.6910 |
[32m[20221214 14:39:26 @agent_ppo2.py:185][0m |          -0.0013 |         257.2590 |        -308.5490 |
[32m[20221214 14:39:26 @agent_ppo2.py:185][0m |           0.0009 |         257.3195 |        -308.3139 |
[32m[20221214 14:39:26 @agent_ppo2.py:185][0m |          -0.0015 |         255.6557 |        -308.1924 |
[32m[20221214 14:39:26 @agent_ppo2.py:185][0m |          -0.0010 |         254.9921 |        -307.7304 |
[32m[20221214 14:39:26 @agent_ppo2.py:185][0m |          -0.0009 |         254.1462 |        -307.6262 |
[32m[20221214 14:39:26 @agent_ppo2.py:185][0m |          -0.0016 |         253.6200 |        -308.3787 |
[32m[20221214 14:39:27 @agent_ppo2.py:185][0m |          -0.0017 |         253.4121 |        -309.1058 |
[32m[20221214 14:39:27 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:39:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.41
[32m[20221214 14:39:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.99
[32m[20221214 14:39:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.38
[32m[20221214 14:39:27 @agent_ppo2.py:143][0m Total time:      41.41 min
[32m[20221214 14:39:27 @agent_ppo2.py:145][0m 3815424 total steps have happened
[32m[20221214 14:39:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1863 --------------------------#
[32m[20221214 14:39:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:27 @agent_ppo2.py:185][0m |          -0.0007 |         270.4066 |        -306.3290 |
[32m[20221214 14:39:27 @agent_ppo2.py:185][0m |          -0.0014 |         268.3945 |        -306.7195 |
[32m[20221214 14:39:27 @agent_ppo2.py:185][0m |           0.0006 |         266.4601 |        -307.4915 |
[32m[20221214 14:39:27 @agent_ppo2.py:185][0m |           0.0031 |         272.6835 |        -307.2952 |
[32m[20221214 14:39:27 @agent_ppo2.py:185][0m |          -0.0014 |         265.6797 |        -307.5426 |
[32m[20221214 14:39:28 @agent_ppo2.py:185][0m |          -0.0011 |         265.0320 |        -307.4397 |
[32m[20221214 14:39:28 @agent_ppo2.py:185][0m |           0.0014 |         264.0025 |        -307.5882 |
[32m[20221214 14:39:28 @agent_ppo2.py:185][0m |          -0.0007 |         264.0067 |        -308.1696 |
[32m[20221214 14:39:28 @agent_ppo2.py:185][0m |           0.0028 |         265.3269 |        -308.8264 |
[32m[20221214 14:39:28 @agent_ppo2.py:185][0m |          -0.0009 |         264.3180 |        -308.1475 |
[32m[20221214 14:39:28 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:39:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.30
[32m[20221214 14:39:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.71
[32m[20221214 14:39:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.45
[32m[20221214 14:39:28 @agent_ppo2.py:143][0m Total time:      41.43 min
[32m[20221214 14:39:28 @agent_ppo2.py:145][0m 3817472 total steps have happened
[32m[20221214 14:39:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1864 --------------------------#
[32m[20221214 14:39:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:28 @agent_ppo2.py:185][0m |          -0.0011 |         262.2016 |        -316.3889 |
[32m[20221214 14:39:29 @agent_ppo2.py:185][0m |           0.0051 |         264.3692 |        -316.6758 |
[32m[20221214 14:39:29 @agent_ppo2.py:185][0m |          -0.0022 |         259.9213 |        -316.4280 |
[32m[20221214 14:39:29 @agent_ppo2.py:185][0m |          -0.0010 |         259.4494 |        -315.9415 |
[32m[20221214 14:39:29 @agent_ppo2.py:185][0m |           0.0055 |         267.3970 |        -316.5651 |
[32m[20221214 14:39:29 @agent_ppo2.py:185][0m |           0.0000 |         258.6409 |        -316.4021 |
[32m[20221214 14:39:29 @agent_ppo2.py:185][0m |           0.0001 |         258.2745 |        -315.8694 |
[32m[20221214 14:39:29 @agent_ppo2.py:185][0m |           0.0017 |         259.4511 |        -317.3039 |
[32m[20221214 14:39:29 @agent_ppo2.py:185][0m |          -0.0013 |         258.3892 |        -316.9553 |
[32m[20221214 14:39:29 @agent_ppo2.py:185][0m |          -0.0017 |         259.5047 |        -316.7853 |
[32m[20221214 14:39:29 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:39:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.06
[32m[20221214 14:39:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.19
[32m[20221214 14:39:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.88
[32m[20221214 14:39:29 @agent_ppo2.py:143][0m Total time:      41.46 min
[32m[20221214 14:39:29 @agent_ppo2.py:145][0m 3819520 total steps have happened
[32m[20221214 14:39:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1865 --------------------------#
[32m[20221214 14:39:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:30 @agent_ppo2.py:185][0m |          -0.0024 |         267.4594 |        -312.7696 |
[32m[20221214 14:39:30 @agent_ppo2.py:185][0m |          -0.0027 |         265.2429 |        -312.9839 |
[32m[20221214 14:39:30 @agent_ppo2.py:185][0m |          -0.0032 |         263.5019 |        -313.2091 |
[32m[20221214 14:39:30 @agent_ppo2.py:185][0m |           0.0212 |         305.7174 |        -313.5465 |
[32m[20221214 14:39:30 @agent_ppo2.py:185][0m |          -0.0038 |         263.7695 |        -312.7471 |
[32m[20221214 14:39:30 @agent_ppo2.py:185][0m |           0.0116 |         302.2157 |        -312.9777 |
[32m[20221214 14:39:30 @agent_ppo2.py:185][0m |          -0.0031 |         261.3446 |        -313.0839 |
[32m[20221214 14:39:30 @agent_ppo2.py:185][0m |          -0.0027 |         260.8151 |        -312.9308 |
[32m[20221214 14:39:31 @agent_ppo2.py:185][0m |          -0.0017 |         261.0302 |        -313.1268 |
[32m[20221214 14:39:31 @agent_ppo2.py:185][0m |          -0.0025 |         259.9075 |        -313.1787 |
[32m[20221214 14:39:31 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:39:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.84
[32m[20221214 14:39:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.77
[32m[20221214 14:39:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.88
[32m[20221214 14:39:31 @agent_ppo2.py:143][0m Total time:      41.48 min
[32m[20221214 14:39:31 @agent_ppo2.py:145][0m 3821568 total steps have happened
[32m[20221214 14:39:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1866 --------------------------#
[32m[20221214 14:39:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:31 @agent_ppo2.py:185][0m |          -0.0002 |         269.5386 |        -312.3981 |
[32m[20221214 14:39:31 @agent_ppo2.py:185][0m |          -0.0024 |         269.5163 |        -312.7775 |
[32m[20221214 14:39:31 @agent_ppo2.py:185][0m |           0.0037 |         270.6927 |        -313.3968 |
[32m[20221214 14:39:31 @agent_ppo2.py:185][0m |          -0.0001 |         267.8188 |        -312.9366 |
[32m[20221214 14:39:32 @agent_ppo2.py:185][0m |           0.0003 |         267.3687 |        -312.6822 |
[32m[20221214 14:39:32 @agent_ppo2.py:185][0m |          -0.0003 |         266.9117 |        -313.8137 |
[32m[20221214 14:39:32 @agent_ppo2.py:185][0m |           0.0001 |         266.7725 |        -312.9431 |
[32m[20221214 14:39:32 @agent_ppo2.py:185][0m |          -0.0022 |         266.0956 |        -312.8898 |
[32m[20221214 14:39:32 @agent_ppo2.py:185][0m |          -0.0013 |         265.8489 |        -313.6353 |
[32m[20221214 14:39:32 @agent_ppo2.py:185][0m |          -0.0009 |         266.7754 |        -312.7959 |
[32m[20221214 14:39:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:39:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.93
[32m[20221214 14:39:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.96
[32m[20221214 14:39:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.97
[32m[20221214 14:39:32 @agent_ppo2.py:143][0m Total time:      41.50 min
[32m[20221214 14:39:32 @agent_ppo2.py:145][0m 3823616 total steps have happened
[32m[20221214 14:39:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1867 --------------------------#
[32m[20221214 14:39:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:33 @agent_ppo2.py:185][0m |           0.0100 |         305.8479 |        -310.9902 |
[32m[20221214 14:39:33 @agent_ppo2.py:185][0m |          -0.0006 |         283.5827 |        -310.4931 |
[32m[20221214 14:39:33 @agent_ppo2.py:185][0m |          -0.0017 |         279.6274 |        -310.1375 |
[32m[20221214 14:39:33 @agent_ppo2.py:185][0m |          -0.0018 |         280.4886 |        -310.3896 |
[32m[20221214 14:39:33 @agent_ppo2.py:185][0m |           0.0009 |         281.1794 |        -311.5447 |
[32m[20221214 14:39:33 @agent_ppo2.py:185][0m |           0.0035 |         281.4281 |        -311.0750 |
[32m[20221214 14:39:33 @agent_ppo2.py:185][0m |          -0.0023 |         278.2508 |        -310.9577 |
[32m[20221214 14:39:33 @agent_ppo2.py:185][0m |          -0.0026 |         277.2802 |        -311.5387 |
[32m[20221214 14:39:33 @agent_ppo2.py:185][0m |          -0.0021 |         277.2046 |        -311.8822 |
[32m[20221214 14:39:33 @agent_ppo2.py:185][0m |           0.0014 |         277.6257 |        -310.9869 |
[32m[20221214 14:39:33 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:39:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.23
[32m[20221214 14:39:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.55
[32m[20221214 14:39:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.95
[32m[20221214 14:39:34 @agent_ppo2.py:143][0m Total time:      41.52 min
[32m[20221214 14:39:34 @agent_ppo2.py:145][0m 3825664 total steps have happened
[32m[20221214 14:39:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1868 --------------------------#
[32m[20221214 14:39:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:34 @agent_ppo2.py:185][0m |           0.0011 |         299.4094 |        -318.1878 |
[32m[20221214 14:39:34 @agent_ppo2.py:185][0m |          -0.0038 |         293.4584 |        -318.0989 |
[32m[20221214 14:39:34 @agent_ppo2.py:185][0m |          -0.0029 |         292.3050 |        -318.6453 |
[32m[20221214 14:39:34 @agent_ppo2.py:185][0m |          -0.0029 |         290.3576 |        -317.0785 |
[32m[20221214 14:39:34 @agent_ppo2.py:185][0m |          -0.0018 |         289.6617 |        -318.3284 |
[32m[20221214 14:39:34 @agent_ppo2.py:185][0m |           0.0104 |         319.7079 |        -318.5096 |
[32m[20221214 14:39:34 @agent_ppo2.py:185][0m |           0.0027 |         291.5506 |        -317.8346 |
[32m[20221214 14:39:35 @agent_ppo2.py:185][0m |          -0.0027 |         287.5017 |        -317.9396 |
[32m[20221214 14:39:35 @agent_ppo2.py:185][0m |          -0.0020 |         287.0849 |        -318.1110 |
[32m[20221214 14:39:35 @agent_ppo2.py:185][0m |          -0.0039 |         288.0193 |        -317.7358 |
[32m[20221214 14:39:35 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:39:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.29
[32m[20221214 14:39:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.74
[32m[20221214 14:39:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.34
[32m[20221214 14:39:35 @agent_ppo2.py:143][0m Total time:      41.55 min
[32m[20221214 14:39:35 @agent_ppo2.py:145][0m 3827712 total steps have happened
[32m[20221214 14:39:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1869 --------------------------#
[32m[20221214 14:39:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:35 @agent_ppo2.py:185][0m |           0.0137 |         303.3208 |        -319.9497 |
[32m[20221214 14:39:35 @agent_ppo2.py:185][0m |          -0.0009 |         282.5825 |        -318.9880 |
[32m[20221214 14:39:35 @agent_ppo2.py:185][0m |          -0.0039 |         280.2731 |        -320.0032 |
[32m[20221214 14:39:36 @agent_ppo2.py:185][0m |          -0.0034 |         279.6889 |        -320.0782 |
[32m[20221214 14:39:36 @agent_ppo2.py:185][0m |          -0.0017 |         280.1047 |        -319.5449 |
[32m[20221214 14:39:36 @agent_ppo2.py:185][0m |          -0.0017 |         279.4245 |        -320.5520 |
[32m[20221214 14:39:36 @agent_ppo2.py:185][0m |          -0.0031 |         278.8110 |        -320.4763 |
[32m[20221214 14:39:36 @agent_ppo2.py:185][0m |          -0.0007 |         279.1485 |        -321.4231 |
[32m[20221214 14:39:36 @agent_ppo2.py:185][0m |           0.0012 |         283.9454 |        -321.3105 |
[32m[20221214 14:39:36 @agent_ppo2.py:185][0m |           0.0073 |         297.6308 |        -321.4915 |
[32m[20221214 14:39:36 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:39:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.59
[32m[20221214 14:39:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.34
[32m[20221214 14:39:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.20
[32m[20221214 14:39:36 @agent_ppo2.py:143][0m Total time:      41.57 min
[32m[20221214 14:39:36 @agent_ppo2.py:145][0m 3829760 total steps have happened
[32m[20221214 14:39:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1870 --------------------------#
[32m[20221214 14:39:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:39:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:37 @agent_ppo2.py:185][0m |          -0.0029 |         280.5699 |        -314.9641 |
[32m[20221214 14:39:37 @agent_ppo2.py:185][0m |          -0.0045 |         275.4912 |        -315.0152 |
[32m[20221214 14:39:37 @agent_ppo2.py:185][0m |          -0.0013 |         272.7890 |        -314.6534 |
[32m[20221214 14:39:37 @agent_ppo2.py:185][0m |          -0.0033 |         270.9032 |        -315.5437 |
[32m[20221214 14:39:37 @agent_ppo2.py:185][0m |          -0.0033 |         269.8047 |        -315.2727 |
[32m[20221214 14:39:37 @agent_ppo2.py:185][0m |          -0.0032 |         269.1339 |        -314.8310 |
[32m[20221214 14:39:37 @agent_ppo2.py:185][0m |          -0.0018 |         268.2266 |        -315.2241 |
[32m[20221214 14:39:37 @agent_ppo2.py:185][0m |          -0.0024 |         268.4138 |        -314.6194 |
[32m[20221214 14:39:37 @agent_ppo2.py:185][0m |           0.0096 |         299.0827 |        -315.5922 |
[32m[20221214 14:39:37 @agent_ppo2.py:185][0m |          -0.0025 |         267.7177 |        -313.1256 |
[32m[20221214 14:39:37 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:39:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.19
[32m[20221214 14:39:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.99
[32m[20221214 14:39:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.84
[32m[20221214 14:39:38 @agent_ppo2.py:143][0m Total time:      41.59 min
[32m[20221214 14:39:38 @agent_ppo2.py:145][0m 3831808 total steps have happened
[32m[20221214 14:39:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1871 --------------------------#
[32m[20221214 14:39:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:38 @agent_ppo2.py:185][0m |          -0.0005 |         279.2895 |        -319.4008 |
[32m[20221214 14:39:38 @agent_ppo2.py:185][0m |           0.0002 |         263.2143 |        -318.2319 |
[32m[20221214 14:39:38 @agent_ppo2.py:185][0m |           0.0054 |         264.8662 |        -319.3893 |
[32m[20221214 14:39:38 @agent_ppo2.py:185][0m |          -0.0006 |         261.9486 |        -317.6273 |
[32m[20221214 14:39:38 @agent_ppo2.py:185][0m |          -0.0003 |         255.7519 |        -316.9632 |
[32m[20221214 14:39:38 @agent_ppo2.py:185][0m |          -0.0017 |         255.9979 |        -318.0694 |
[32m[20221214 14:39:38 @agent_ppo2.py:185][0m |           0.0018 |         255.8576 |        -318.8440 |
[32m[20221214 14:39:39 @agent_ppo2.py:185][0m |           0.0003 |         255.4992 |        -318.1021 |
[32m[20221214 14:39:39 @agent_ppo2.py:185][0m |           0.0033 |         256.5998 |        -317.8403 |
[32m[20221214 14:39:39 @agent_ppo2.py:185][0m |          -0.0007 |         254.3668 |        -316.8550 |
[32m[20221214 14:39:39 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:39:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.40
[32m[20221214 14:39:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.18
[32m[20221214 14:39:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.29
[32m[20221214 14:39:39 @agent_ppo2.py:143][0m Total time:      41.61 min
[32m[20221214 14:39:39 @agent_ppo2.py:145][0m 3833856 total steps have happened
[32m[20221214 14:39:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1872 --------------------------#
[32m[20221214 14:39:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:39 @agent_ppo2.py:185][0m |           0.0041 |         266.9082 |        -310.9979 |
[32m[20221214 14:39:39 @agent_ppo2.py:185][0m |          -0.0008 |         258.3546 |        -310.6817 |
[32m[20221214 14:39:39 @agent_ppo2.py:185][0m |           0.0011 |         257.1745 |        -310.7997 |
[32m[20221214 14:39:40 @agent_ppo2.py:185][0m |          -0.0031 |         255.8777 |        -311.4321 |
[32m[20221214 14:39:40 @agent_ppo2.py:185][0m |           0.0001 |         255.7305 |        -311.3764 |
[32m[20221214 14:39:40 @agent_ppo2.py:185][0m |          -0.0022 |         255.7174 |        -312.2911 |
[32m[20221214 14:39:40 @agent_ppo2.py:185][0m |          -0.0007 |         255.4301 |        -312.3176 |
[32m[20221214 14:39:40 @agent_ppo2.py:185][0m |           0.0103 |         275.8337 |        -312.8466 |
[32m[20221214 14:39:40 @agent_ppo2.py:185][0m |          -0.0016 |         256.0478 |        -312.9430 |
[32m[20221214 14:39:40 @agent_ppo2.py:185][0m |           0.0162 |         282.6853 |        -313.1046 |
[32m[20221214 14:39:40 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:39:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.63
[32m[20221214 14:39:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.70
[32m[20221214 14:39:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.61
[32m[20221214 14:39:40 @agent_ppo2.py:143][0m Total time:      41.64 min
[32m[20221214 14:39:40 @agent_ppo2.py:145][0m 3835904 total steps have happened
[32m[20221214 14:39:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1873 --------------------------#
[32m[20221214 14:39:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:41 @agent_ppo2.py:185][0m |           0.0018 |         259.8141 |        -321.8669 |
[32m[20221214 14:39:41 @agent_ppo2.py:185][0m |          -0.0035 |         258.0277 |        -322.0565 |
[32m[20221214 14:39:41 @agent_ppo2.py:185][0m |           0.0070 |         263.1371 |        -323.6962 |
[32m[20221214 14:39:41 @agent_ppo2.py:185][0m |          -0.0006 |         256.5122 |        -322.4915 |
[32m[20221214 14:39:41 @agent_ppo2.py:185][0m |           0.0008 |         255.0594 |        -322.9897 |
[32m[20221214 14:39:41 @agent_ppo2.py:185][0m |           0.0104 |         282.2167 |        -323.7230 |
[32m[20221214 14:39:41 @agent_ppo2.py:185][0m |          -0.0026 |         254.4894 |        -323.8797 |
[32m[20221214 14:39:41 @agent_ppo2.py:185][0m |          -0.0007 |         254.2027 |        -324.3773 |
[32m[20221214 14:39:41 @agent_ppo2.py:185][0m |          -0.0019 |         253.9504 |        -324.3012 |
[32m[20221214 14:39:41 @agent_ppo2.py:185][0m |          -0.0001 |         252.9376 |        -325.0594 |
[32m[20221214 14:39:41 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:39:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.18
[32m[20221214 14:39:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.39
[32m[20221214 14:39:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.14
[32m[20221214 14:39:42 @agent_ppo2.py:143][0m Total time:      41.66 min
[32m[20221214 14:39:42 @agent_ppo2.py:145][0m 3837952 total steps have happened
[32m[20221214 14:39:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1874 --------------------------#
[32m[20221214 14:39:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:42 @agent_ppo2.py:185][0m |          -0.0010 |         235.1687 |        -319.4993 |
[32m[20221214 14:39:42 @agent_ppo2.py:185][0m |           0.0005 |         233.6853 |        -319.5012 |
[32m[20221214 14:39:42 @agent_ppo2.py:185][0m |           0.0012 |         233.3245 |        -318.5650 |
[32m[20221214 14:39:42 @agent_ppo2.py:185][0m |           0.0132 |         247.5384 |        -318.6560 |
[32m[20221214 14:39:42 @agent_ppo2.py:185][0m |          -0.0024 |         233.5975 |        -319.6098 |
[32m[20221214 14:39:42 @agent_ppo2.py:185][0m |          -0.0016 |         232.8129 |        -319.9971 |
[32m[20221214 14:39:43 @agent_ppo2.py:185][0m |           0.0003 |         232.5115 |        -319.0065 |
[32m[20221214 14:39:43 @agent_ppo2.py:185][0m |          -0.0021 |         232.8076 |        -320.4364 |
[32m[20221214 14:39:43 @agent_ppo2.py:185][0m |           0.0124 |         241.7777 |        -319.3773 |
[32m[20221214 14:39:43 @agent_ppo2.py:185][0m |          -0.0008 |         232.5447 |        -320.1636 |
[32m[20221214 14:39:43 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:39:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.82
[32m[20221214 14:39:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.62
[32m[20221214 14:39:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.98
[32m[20221214 14:39:43 @agent_ppo2.py:143][0m Total time:      41.68 min
[32m[20221214 14:39:43 @agent_ppo2.py:145][0m 3840000 total steps have happened
[32m[20221214 14:39:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1875 --------------------------#
[32m[20221214 14:39:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:43 @agent_ppo2.py:185][0m |          -0.0012 |         179.8926 |        -321.3923 |
[32m[20221214 14:39:43 @agent_ppo2.py:185][0m |          -0.0034 |         178.0833 |        -321.2696 |
[32m[20221214 14:39:44 @agent_ppo2.py:185][0m |          -0.0024 |         176.9943 |        -321.3829 |
[32m[20221214 14:39:44 @agent_ppo2.py:185][0m |          -0.0033 |         177.7763 |        -321.6463 |
[32m[20221214 14:39:44 @agent_ppo2.py:185][0m |          -0.0010 |         177.1701 |        -321.1507 |
[32m[20221214 14:39:44 @agent_ppo2.py:185][0m |          -0.0003 |         177.1573 |        -321.0579 |
[32m[20221214 14:39:44 @agent_ppo2.py:185][0m |          -0.0012 |         176.6660 |        -321.2858 |
[32m[20221214 14:39:44 @agent_ppo2.py:185][0m |          -0.0043 |         176.1955 |        -321.2558 |
[32m[20221214 14:39:44 @agent_ppo2.py:185][0m |           0.0136 |         200.6560 |        -321.1260 |
[32m[20221214 14:39:44 @agent_ppo2.py:185][0m |          -0.0018 |         176.3350 |        -321.4955 |
[32m[20221214 14:39:44 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:39:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.27
[32m[20221214 14:39:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.13
[32m[20221214 14:39:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.71
[32m[20221214 14:39:44 @agent_ppo2.py:143][0m Total time:      41.70 min
[32m[20221214 14:39:44 @agent_ppo2.py:145][0m 3842048 total steps have happened
[32m[20221214 14:39:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1876 --------------------------#
[32m[20221214 14:39:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:45 @agent_ppo2.py:185][0m |          -0.0015 |         180.7084 |        -319.5807 |
[32m[20221214 14:39:45 @agent_ppo2.py:185][0m |          -0.0021 |         175.6088 |        -319.5930 |
[32m[20221214 14:39:45 @agent_ppo2.py:185][0m |          -0.0011 |         174.2988 |        -319.1234 |
[32m[20221214 14:39:45 @agent_ppo2.py:185][0m |           0.0042 |         184.6573 |        -319.1321 |
[32m[20221214 14:39:45 @agent_ppo2.py:185][0m |          -0.0047 |         168.8387 |        -319.0461 |
[32m[20221214 14:39:45 @agent_ppo2.py:185][0m |          -0.0023 |         168.5962 |        -319.3804 |
[32m[20221214 14:39:45 @agent_ppo2.py:185][0m |           0.0005 |         168.9344 |        -319.5312 |
[32m[20221214 14:39:45 @agent_ppo2.py:185][0m |           0.0038 |         172.8296 |        -319.7056 |
[32m[20221214 14:39:45 @agent_ppo2.py:185][0m |          -0.0024 |         167.0891 |        -318.8382 |
[32m[20221214 14:39:46 @agent_ppo2.py:185][0m |          -0.0000 |         167.4290 |        -318.7533 |
[32m[20221214 14:39:46 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:39:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.34
[32m[20221214 14:39:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.81
[32m[20221214 14:39:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.57
[32m[20221214 14:39:46 @agent_ppo2.py:143][0m Total time:      41.73 min
[32m[20221214 14:39:46 @agent_ppo2.py:145][0m 3844096 total steps have happened
[32m[20221214 14:39:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1877 --------------------------#
[32m[20221214 14:39:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:46 @agent_ppo2.py:185][0m |          -0.0012 |         229.8433 |        -318.5774 |
[32m[20221214 14:39:46 @agent_ppo2.py:185][0m |          -0.0014 |         227.3256 |        -319.8036 |
[32m[20221214 14:39:46 @agent_ppo2.py:185][0m |           0.0002 |         226.4757 |        -319.1026 |
[32m[20221214 14:39:46 @agent_ppo2.py:185][0m |          -0.0013 |         224.9416 |        -318.6851 |
[32m[20221214 14:39:46 @agent_ppo2.py:185][0m |          -0.0012 |         224.2028 |        -318.2579 |
[32m[20221214 14:39:46 @agent_ppo2.py:185][0m |          -0.0031 |         224.3929 |        -320.2587 |
[32m[20221214 14:39:47 @agent_ppo2.py:185][0m |          -0.0021 |         223.5031 |        -320.6433 |
[32m[20221214 14:39:47 @agent_ppo2.py:185][0m |          -0.0009 |         222.8329 |        -320.0210 |
[32m[20221214 14:39:47 @agent_ppo2.py:185][0m |          -0.0004 |         222.4986 |        -320.3551 |
[32m[20221214 14:39:47 @agent_ppo2.py:185][0m |          -0.0027 |         221.6640 |        -320.3172 |
[32m[20221214 14:39:47 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:39:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.33
[32m[20221214 14:39:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.67
[32m[20221214 14:39:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.64
[32m[20221214 14:39:47 @agent_ppo2.py:143][0m Total time:      41.75 min
[32m[20221214 14:39:47 @agent_ppo2.py:145][0m 3846144 total steps have happened
[32m[20221214 14:39:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1878 --------------------------#
[32m[20221214 14:39:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:39:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:47 @agent_ppo2.py:185][0m |          -0.0007 |         190.5512 |        -315.6932 |
[32m[20221214 14:39:47 @agent_ppo2.py:185][0m |          -0.0045 |         189.1914 |        -317.4131 |
[32m[20221214 14:39:47 @agent_ppo2.py:185][0m |          -0.0040 |         188.6307 |        -317.3561 |
[32m[20221214 14:39:48 @agent_ppo2.py:185][0m |           0.0025 |         193.7075 |        -317.0495 |
[32m[20221214 14:39:48 @agent_ppo2.py:185][0m |          -0.0021 |         186.7175 |        -317.7392 |
[32m[20221214 14:39:48 @agent_ppo2.py:185][0m |          -0.0018 |         186.5442 |        -317.3662 |
[32m[20221214 14:39:48 @agent_ppo2.py:185][0m |           0.0058 |         196.3988 |        -317.2063 |
[32m[20221214 14:39:48 @agent_ppo2.py:185][0m |          -0.0009 |         191.0483 |        -317.1911 |
[32m[20221214 14:39:48 @agent_ppo2.py:185][0m |          -0.0043 |         186.0347 |        -317.4442 |
[32m[20221214 14:39:48 @agent_ppo2.py:185][0m |          -0.0055 |         185.7341 |        -317.4800 |
[32m[20221214 14:39:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:39:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.97
[32m[20221214 14:39:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.61
[32m[20221214 14:39:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.71
[32m[20221214 14:39:48 @agent_ppo2.py:143][0m Total time:      41.77 min
[32m[20221214 14:39:48 @agent_ppo2.py:145][0m 3848192 total steps have happened
[32m[20221214 14:39:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1879 --------------------------#
[32m[20221214 14:39:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:39:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:49 @agent_ppo2.py:185][0m |           0.0031 |         237.8685 |        -321.0564 |
[32m[20221214 14:39:49 @agent_ppo2.py:185][0m |          -0.0015 |         233.4765 |        -319.4722 |
[32m[20221214 14:39:49 @agent_ppo2.py:185][0m |           0.0043 |         235.4884 |        -320.0977 |
[32m[20221214 14:39:49 @agent_ppo2.py:185][0m |          -0.0018 |         231.1593 |        -320.3028 |
[32m[20221214 14:39:49 @agent_ppo2.py:185][0m |           0.0005 |         231.6863 |        -321.0845 |
[32m[20221214 14:39:49 @agent_ppo2.py:185][0m |           0.0106 |         259.0085 |        -320.2866 |
[32m[20221214 14:39:49 @agent_ppo2.py:185][0m |          -0.0014 |         231.1201 |        -320.3372 |
[32m[20221214 14:39:49 @agent_ppo2.py:185][0m |          -0.0026 |         230.0525 |        -320.4201 |
[32m[20221214 14:39:49 @agent_ppo2.py:185][0m |          -0.0032 |         230.9381 |        -320.0076 |
[32m[20221214 14:39:49 @agent_ppo2.py:185][0m |          -0.0013 |         230.5859 |        -320.3462 |
[32m[20221214 14:39:49 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:39:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.77
[32m[20221214 14:39:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.58
[32m[20221214 14:39:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.75
[32m[20221214 14:39:50 @agent_ppo2.py:143][0m Total time:      41.79 min
[32m[20221214 14:39:50 @agent_ppo2.py:145][0m 3850240 total steps have happened
[32m[20221214 14:39:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1880 --------------------------#
[32m[20221214 14:39:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:39:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:50 @agent_ppo2.py:185][0m |          -0.0020 |         212.1524 |        -322.1713 |
[32m[20221214 14:39:50 @agent_ppo2.py:185][0m |           0.0058 |         214.6051 |        -321.6899 |
[32m[20221214 14:39:50 @agent_ppo2.py:185][0m |          -0.0027 |         191.6348 |        -321.4113 |
[32m[20221214 14:39:50 @agent_ppo2.py:185][0m |          -0.0019 |         188.7581 |        -321.4781 |
[32m[20221214 14:39:50 @agent_ppo2.py:185][0m |          -0.0019 |         186.5642 |        -321.6109 |
[32m[20221214 14:39:50 @agent_ppo2.py:185][0m |          -0.0037 |         185.7689 |        -321.6032 |
[32m[20221214 14:39:51 @agent_ppo2.py:185][0m |          -0.0042 |         185.3722 |        -322.6200 |
[32m[20221214 14:39:51 @agent_ppo2.py:185][0m |           0.0000 |         185.3462 |        -321.4834 |
[32m[20221214 14:39:51 @agent_ppo2.py:185][0m |           0.0092 |         196.3279 |        -321.6527 |
[32m[20221214 14:39:51 @agent_ppo2.py:185][0m |          -0.0015 |         184.0675 |        -321.0187 |
[32m[20221214 14:39:51 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:39:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.17
[32m[20221214 14:39:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.41
[32m[20221214 14:39:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.29
[32m[20221214 14:39:51 @agent_ppo2.py:143][0m Total time:      41.82 min
[32m[20221214 14:39:51 @agent_ppo2.py:145][0m 3852288 total steps have happened
[32m[20221214 14:39:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1881 --------------------------#
[32m[20221214 14:39:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:51 @agent_ppo2.py:185][0m |          -0.0022 |         177.6977 |        -322.0103 |
[32m[20221214 14:39:51 @agent_ppo2.py:185][0m |           0.0036 |         177.6912 |        -321.3463 |
[32m[20221214 14:39:52 @agent_ppo2.py:185][0m |          -0.0004 |         175.7139 |        -322.1730 |
[32m[20221214 14:39:52 @agent_ppo2.py:185][0m |          -0.0015 |         175.8390 |        -322.0913 |
[32m[20221214 14:39:52 @agent_ppo2.py:185][0m |          -0.0040 |         175.5239 |        -322.1877 |
[32m[20221214 14:39:52 @agent_ppo2.py:185][0m |          -0.0051 |         175.6941 |        -321.3635 |
[32m[20221214 14:39:52 @agent_ppo2.py:185][0m |           0.0056 |         179.5094 |        -322.0207 |
[32m[20221214 14:39:52 @agent_ppo2.py:185][0m |          -0.0028 |         175.8392 |        -321.9063 |
[32m[20221214 14:39:52 @agent_ppo2.py:185][0m |           0.0003 |         174.4359 |        -322.2259 |
[32m[20221214 14:39:52 @agent_ppo2.py:185][0m |          -0.0006 |         175.5888 |        -322.3534 |
[32m[20221214 14:39:52 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:39:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.49
[32m[20221214 14:39:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.92
[32m[20221214 14:39:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.76
[32m[20221214 14:39:52 @agent_ppo2.py:143][0m Total time:      41.84 min
[32m[20221214 14:39:52 @agent_ppo2.py:145][0m 3854336 total steps have happened
[32m[20221214 14:39:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1882 --------------------------#
[32m[20221214 14:39:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:53 @agent_ppo2.py:185][0m |          -0.0004 |         184.2053 |        -324.3251 |
[32m[20221214 14:39:53 @agent_ppo2.py:185][0m |          -0.0026 |         182.2849 |        -325.4501 |
[32m[20221214 14:39:53 @agent_ppo2.py:185][0m |          -0.0019 |         181.8977 |        -325.2382 |
[32m[20221214 14:39:53 @agent_ppo2.py:185][0m |           0.0028 |         183.5375 |        -325.6646 |
[32m[20221214 14:39:53 @agent_ppo2.py:185][0m |          -0.0023 |         181.4509 |        -325.1856 |
[32m[20221214 14:39:53 @agent_ppo2.py:185][0m |          -0.0022 |         180.6877 |        -325.4163 |
[32m[20221214 14:39:53 @agent_ppo2.py:185][0m |          -0.0017 |         180.5736 |        -325.5850 |
[32m[20221214 14:39:53 @agent_ppo2.py:185][0m |           0.0168 |         202.1739 |        -325.3803 |
[32m[20221214 14:39:53 @agent_ppo2.py:185][0m |          -0.0032 |         180.3170 |        -324.2421 |
[32m[20221214 14:39:54 @agent_ppo2.py:185][0m |          -0.0005 |         180.2365 |        -325.5984 |
[32m[20221214 14:39:54 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:39:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.77
[32m[20221214 14:39:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.39
[32m[20221214 14:39:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.29
[32m[20221214 14:39:54 @agent_ppo2.py:143][0m Total time:      41.86 min
[32m[20221214 14:39:54 @agent_ppo2.py:145][0m 3856384 total steps have happened
[32m[20221214 14:39:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1883 --------------------------#
[32m[20221214 14:39:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:54 @agent_ppo2.py:185][0m |          -0.0045 |         192.2532 |        -322.8470 |
[32m[20221214 14:39:54 @agent_ppo2.py:185][0m |          -0.0017 |         187.1409 |        -322.2390 |
[32m[20221214 14:39:54 @agent_ppo2.py:185][0m |          -0.0033 |         184.9436 |        -322.4895 |
[32m[20221214 14:39:54 @agent_ppo2.py:185][0m |          -0.0025 |         184.9940 |        -322.0427 |
[32m[20221214 14:39:54 @agent_ppo2.py:185][0m |          -0.0030 |         185.1520 |        -321.8812 |
[32m[20221214 14:39:55 @agent_ppo2.py:185][0m |          -0.0015 |         184.8908 |        -321.9324 |
[32m[20221214 14:39:55 @agent_ppo2.py:185][0m |           0.0089 |         205.1962 |        -321.7084 |
[32m[20221214 14:39:55 @agent_ppo2.py:185][0m |          -0.0016 |         184.3912 |        -320.7517 |
[32m[20221214 14:39:55 @agent_ppo2.py:185][0m |          -0.0001 |         183.3182 |        -321.9943 |
[32m[20221214 14:39:55 @agent_ppo2.py:185][0m |          -0.0002 |         184.0588 |        -321.2280 |
[32m[20221214 14:39:55 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:39:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.41
[32m[20221214 14:39:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.19
[32m[20221214 14:39:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.20
[32m[20221214 14:39:55 @agent_ppo2.py:143][0m Total time:      41.88 min
[32m[20221214 14:39:55 @agent_ppo2.py:145][0m 3858432 total steps have happened
[32m[20221214 14:39:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1884 --------------------------#
[32m[20221214 14:39:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:55 @agent_ppo2.py:185][0m |           0.0029 |         225.1460 |        -322.9990 |
[32m[20221214 14:39:56 @agent_ppo2.py:185][0m |          -0.0012 |         221.3136 |        -321.7985 |
[32m[20221214 14:39:56 @agent_ppo2.py:185][0m |          -0.0010 |         219.7991 |        -322.3130 |
[32m[20221214 14:39:56 @agent_ppo2.py:185][0m |           0.0006 |         219.1804 |        -321.5983 |
[32m[20221214 14:39:56 @agent_ppo2.py:185][0m |          -0.0011 |         218.7147 |        -322.7499 |
[32m[20221214 14:39:56 @agent_ppo2.py:185][0m |           0.0050 |         225.6478 |        -322.1886 |
[32m[20221214 14:39:56 @agent_ppo2.py:185][0m |          -0.0022 |         219.7344 |        -322.2706 |
[32m[20221214 14:39:56 @agent_ppo2.py:185][0m |           0.0025 |         221.3771 |        -321.2991 |
[32m[20221214 14:39:56 @agent_ppo2.py:185][0m |          -0.0009 |         219.7377 |        -321.2275 |
[32m[20221214 14:39:56 @agent_ppo2.py:185][0m |          -0.0023 |         219.0910 |        -318.5429 |
[32m[20221214 14:39:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:39:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.81
[32m[20221214 14:39:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.65
[32m[20221214 14:39:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.67
[32m[20221214 14:39:56 @agent_ppo2.py:143][0m Total time:      41.91 min
[32m[20221214 14:39:56 @agent_ppo2.py:145][0m 3860480 total steps have happened
[32m[20221214 14:39:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1885 --------------------------#
[32m[20221214 14:39:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:57 @agent_ppo2.py:185][0m |           0.0008 |         179.6206 |        -317.0789 |
[32m[20221214 14:39:57 @agent_ppo2.py:185][0m |          -0.0005 |         177.0431 |        -317.5910 |
[32m[20221214 14:39:57 @agent_ppo2.py:185][0m |          -0.0003 |         177.2948 |        -317.4459 |
[32m[20221214 14:39:57 @agent_ppo2.py:185][0m |          -0.0022 |         176.2929 |        -317.2922 |
[32m[20221214 14:39:57 @agent_ppo2.py:185][0m |          -0.0013 |         176.5088 |        -317.8652 |
[32m[20221214 14:39:57 @agent_ppo2.py:185][0m |          -0.0014 |         174.6291 |        -317.3621 |
[32m[20221214 14:39:57 @agent_ppo2.py:185][0m |          -0.0026 |         175.7439 |        -317.9482 |
[32m[20221214 14:39:57 @agent_ppo2.py:185][0m |          -0.0033 |         174.5229 |        -316.8921 |
[32m[20221214 14:39:58 @agent_ppo2.py:185][0m |           0.0003 |         174.4204 |        -318.5933 |
[32m[20221214 14:39:58 @agent_ppo2.py:185][0m |          -0.0026 |         174.4703 |        -319.0117 |
[32m[20221214 14:39:58 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:39:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.74
[32m[20221214 14:39:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.06
[32m[20221214 14:39:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.49
[32m[20221214 14:39:58 @agent_ppo2.py:143][0m Total time:      41.93 min
[32m[20221214 14:39:58 @agent_ppo2.py:145][0m 3862528 total steps have happened
[32m[20221214 14:39:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1886 --------------------------#
[32m[20221214 14:39:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:39:58 @agent_ppo2.py:185][0m |          -0.0010 |         189.6481 |        -323.4166 |
[32m[20221214 14:39:58 @agent_ppo2.py:185][0m |           0.0121 |         208.1335 |        -324.5935 |
[32m[20221214 14:39:58 @agent_ppo2.py:185][0m |           0.0072 |         204.3227 |        -324.1960 |
[32m[20221214 14:39:58 @agent_ppo2.py:185][0m |           0.0101 |         205.5959 |        -324.0706 |
[32m[20221214 14:39:59 @agent_ppo2.py:185][0m |          -0.0006 |         186.5680 |        -324.0929 |
[32m[20221214 14:39:59 @agent_ppo2.py:185][0m |          -0.0034 |         186.4703 |        -325.0172 |
[32m[20221214 14:39:59 @agent_ppo2.py:185][0m |          -0.0003 |         187.1443 |        -324.3818 |
[32m[20221214 14:39:59 @agent_ppo2.py:185][0m |          -0.0022 |         186.5570 |        -324.2665 |
[32m[20221214 14:39:59 @agent_ppo2.py:185][0m |          -0.0026 |         185.8289 |        -325.4081 |
[32m[20221214 14:39:59 @agent_ppo2.py:185][0m |           0.0019 |         185.9931 |        -324.8459 |
[32m[20221214 14:39:59 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:39:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.80
[32m[20221214 14:39:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.36
[32m[20221214 14:39:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.06
[32m[20221214 14:39:59 @agent_ppo2.py:143][0m Total time:      41.95 min
[32m[20221214 14:39:59 @agent_ppo2.py:145][0m 3864576 total steps have happened
[32m[20221214 14:39:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1887 --------------------------#
[32m[20221214 14:39:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:39:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:00 @agent_ppo2.py:185][0m |          -0.0027 |         213.4001 |        -321.8805 |
[32m[20221214 14:40:00 @agent_ppo2.py:185][0m |          -0.0028 |         212.1722 |        -321.8283 |
[32m[20221214 14:40:00 @agent_ppo2.py:185][0m |           0.0078 |         220.0025 |        -320.1210 |
[32m[20221214 14:40:00 @agent_ppo2.py:185][0m |          -0.0015 |         211.2419 |        -322.0849 |
[32m[20221214 14:40:00 @agent_ppo2.py:185][0m |          -0.0029 |         210.8643 |        -321.8725 |
[32m[20221214 14:40:00 @agent_ppo2.py:185][0m |           0.0000 |         211.5607 |        -321.9320 |
[32m[20221214 14:40:00 @agent_ppo2.py:185][0m |           0.0099 |         243.0991 |        -322.3972 |
[32m[20221214 14:40:00 @agent_ppo2.py:185][0m |          -0.0035 |         210.8342 |        -322.6365 |
[32m[20221214 14:40:00 @agent_ppo2.py:185][0m |          -0.0015 |         211.9569 |        -322.5760 |
[32m[20221214 14:40:00 @agent_ppo2.py:185][0m |          -0.0016 |         210.6479 |        -322.3139 |
[32m[20221214 14:40:00 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:40:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.01
[32m[20221214 14:40:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.61
[32m[20221214 14:40:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.59
[32m[20221214 14:40:01 @agent_ppo2.py:143][0m Total time:      41.97 min
[32m[20221214 14:40:01 @agent_ppo2.py:145][0m 3866624 total steps have happened
[32m[20221214 14:40:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1888 --------------------------#
[32m[20221214 14:40:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:01 @agent_ppo2.py:185][0m |          -0.0007 |         217.0582 |        -326.1523 |
[32m[20221214 14:40:01 @agent_ppo2.py:185][0m |          -0.0021 |         214.0929 |        -325.8838 |
[32m[20221214 14:40:01 @agent_ppo2.py:185][0m |          -0.0005 |         212.7688 |        -326.1898 |
[32m[20221214 14:40:01 @agent_ppo2.py:185][0m |           0.0034 |         211.8431 |        -326.2441 |
[32m[20221214 14:40:01 @agent_ppo2.py:185][0m |          -0.0004 |         211.0809 |        -325.8859 |
[32m[20221214 14:40:01 @agent_ppo2.py:185][0m |           0.0005 |         212.1605 |        -325.8725 |
[32m[20221214 14:40:01 @agent_ppo2.py:185][0m |           0.0015 |         209.8511 |        -325.4758 |
[32m[20221214 14:40:02 @agent_ppo2.py:185][0m |          -0.0024 |         209.7623 |        -325.5149 |
[32m[20221214 14:40:02 @agent_ppo2.py:185][0m |           0.0026 |         209.0168 |        -325.7361 |
[32m[20221214 14:40:02 @agent_ppo2.py:185][0m |          -0.0011 |         209.4665 |        -326.2153 |
[32m[20221214 14:40:02 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:40:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.86
[32m[20221214 14:40:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.35
[32m[20221214 14:40:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.87
[32m[20221214 14:40:02 @agent_ppo2.py:143][0m Total time:      42.00 min
[32m[20221214 14:40:02 @agent_ppo2.py:145][0m 3868672 total steps have happened
[32m[20221214 14:40:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1889 --------------------------#
[32m[20221214 14:40:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:02 @agent_ppo2.py:185][0m |          -0.0008 |         234.0581 |        -322.3588 |
[32m[20221214 14:40:02 @agent_ppo2.py:185][0m |           0.0051 |         237.0795 |        -323.0304 |
[32m[20221214 14:40:02 @agent_ppo2.py:185][0m |          -0.0028 |         228.2357 |        -322.8035 |
[32m[20221214 14:40:02 @agent_ppo2.py:185][0m |          -0.0021 |         228.3597 |        -323.0684 |
[32m[20221214 14:40:03 @agent_ppo2.py:185][0m |          -0.0008 |         227.3233 |        -321.8726 |
[32m[20221214 14:40:03 @agent_ppo2.py:185][0m |           0.0198 |         257.6996 |        -322.1275 |
[32m[20221214 14:40:03 @agent_ppo2.py:185][0m |          -0.0002 |         227.2596 |        -321.9317 |
[32m[20221214 14:40:03 @agent_ppo2.py:185][0m |          -0.0014 |         225.9049 |        -322.7345 |
[32m[20221214 14:40:03 @agent_ppo2.py:185][0m |          -0.0009 |         225.5616 |        -322.7664 |
[32m[20221214 14:40:03 @agent_ppo2.py:185][0m |          -0.0019 |         225.7403 |        -322.2012 |
[32m[20221214 14:40:03 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:40:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.62
[32m[20221214 14:40:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.47
[32m[20221214 14:40:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.27
[32m[20221214 14:40:03 @agent_ppo2.py:143][0m Total time:      42.02 min
[32m[20221214 14:40:03 @agent_ppo2.py:145][0m 3870720 total steps have happened
[32m[20221214 14:40:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1890 --------------------------#
[32m[20221214 14:40:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:40:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:04 @agent_ppo2.py:185][0m |          -0.0025 |         228.6399 |        -320.9329 |
[32m[20221214 14:40:04 @agent_ppo2.py:185][0m |           0.0001 |         228.8490 |        -320.2240 |
[32m[20221214 14:40:04 @agent_ppo2.py:185][0m |           0.0081 |         244.9684 |        -320.1870 |
[32m[20221214 14:40:04 @agent_ppo2.py:185][0m |          -0.0052 |         224.1916 |        -320.1841 |
[32m[20221214 14:40:04 @agent_ppo2.py:185][0m |          -0.0022 |         222.6986 |        -319.2532 |
[32m[20221214 14:40:04 @agent_ppo2.py:185][0m |          -0.0054 |         221.3083 |        -320.0611 |
[32m[20221214 14:40:04 @agent_ppo2.py:185][0m |           0.0188 |         250.2072 |        -319.3343 |
[32m[20221214 14:40:04 @agent_ppo2.py:185][0m |          -0.0033 |         222.1327 |        -319.7386 |
[32m[20221214 14:40:04 @agent_ppo2.py:185][0m |          -0.0052 |         219.7002 |        -319.1239 |
[32m[20221214 14:40:04 @agent_ppo2.py:185][0m |          -0.0048 |         219.3856 |        -318.8308 |
[32m[20221214 14:40:04 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:40:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.26
[32m[20221214 14:40:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.59
[32m[20221214 14:40:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.65
[32m[20221214 14:40:05 @agent_ppo2.py:143][0m Total time:      42.04 min
[32m[20221214 14:40:05 @agent_ppo2.py:145][0m 3872768 total steps have happened
[32m[20221214 14:40:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1891 --------------------------#
[32m[20221214 14:40:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:05 @agent_ppo2.py:185][0m |          -0.0016 |         232.5856 |        -319.2096 |
[32m[20221214 14:40:05 @agent_ppo2.py:185][0m |          -0.0037 |         221.8110 |        -319.7845 |
[32m[20221214 14:40:05 @agent_ppo2.py:185][0m |          -0.0027 |         218.6291 |        -319.5130 |
[32m[20221214 14:40:05 @agent_ppo2.py:185][0m |          -0.0018 |         217.1460 |        -319.8715 |
[32m[20221214 14:40:05 @agent_ppo2.py:185][0m |          -0.0031 |         216.4555 |        -319.9521 |
[32m[20221214 14:40:05 @agent_ppo2.py:185][0m |          -0.0004 |         214.6818 |        -320.7414 |
[32m[20221214 14:40:06 @agent_ppo2.py:185][0m |          -0.0019 |         214.1754 |        -319.8724 |
[32m[20221214 14:40:06 @agent_ppo2.py:185][0m |          -0.0056 |         213.3387 |        -319.9767 |
[32m[20221214 14:40:06 @agent_ppo2.py:185][0m |           0.0080 |         222.5442 |        -320.3538 |
[32m[20221214 14:40:06 @agent_ppo2.py:185][0m |          -0.0032 |         212.8246 |        -320.3377 |
[32m[20221214 14:40:06 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:40:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.41
[32m[20221214 14:40:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.04
[32m[20221214 14:40:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.26
[32m[20221214 14:40:06 @agent_ppo2.py:143][0m Total time:      42.06 min
[32m[20221214 14:40:06 @agent_ppo2.py:145][0m 3874816 total steps have happened
[32m[20221214 14:40:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1892 --------------------------#
[32m[20221214 14:40:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:40:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:06 @agent_ppo2.py:185][0m |          -0.0001 |         234.5022 |        -324.9456 |
[32m[20221214 14:40:06 @agent_ppo2.py:185][0m |           0.0010 |         227.4625 |        -324.3657 |
[32m[20221214 14:40:06 @agent_ppo2.py:185][0m |          -0.0019 |         223.0575 |        -324.5461 |
[32m[20221214 14:40:07 @agent_ppo2.py:185][0m |           0.0038 |         223.6050 |        -324.5108 |
[32m[20221214 14:40:07 @agent_ppo2.py:185][0m |           0.0146 |         237.6341 |        -323.9871 |
[32m[20221214 14:40:07 @agent_ppo2.py:185][0m |           0.0088 |         232.5574 |        -323.5913 |
[32m[20221214 14:40:07 @agent_ppo2.py:185][0m |           0.0096 |         235.7506 |        -323.5669 |
[32m[20221214 14:40:07 @agent_ppo2.py:185][0m |          -0.0013 |         216.0592 |        -323.1673 |
[32m[20221214 14:40:07 @agent_ppo2.py:185][0m |           0.0011 |         217.2151 |        -323.5948 |
[32m[20221214 14:40:07 @agent_ppo2.py:185][0m |           0.0021 |         216.5610 |        -322.9639 |
[32m[20221214 14:40:07 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:40:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.97
[32m[20221214 14:40:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.01
[32m[20221214 14:40:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.58
[32m[20221214 14:40:07 @agent_ppo2.py:143][0m Total time:      42.09 min
[32m[20221214 14:40:07 @agent_ppo2.py:145][0m 3876864 total steps have happened
[32m[20221214 14:40:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1893 --------------------------#
[32m[20221214 14:40:07 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:40:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:08 @agent_ppo2.py:185][0m |           0.0014 |         224.2366 |        -319.0197 |
[32m[20221214 14:40:08 @agent_ppo2.py:185][0m |          -0.0026 |         220.4782 |        -316.6474 |
[32m[20221214 14:40:08 @agent_ppo2.py:185][0m |          -0.0006 |         219.6587 |        -319.6111 |
[32m[20221214 14:40:08 @agent_ppo2.py:185][0m |          -0.0009 |         217.4788 |        -318.8160 |
[32m[20221214 14:40:08 @agent_ppo2.py:185][0m |          -0.0031 |         216.0814 |        -319.2923 |
[32m[20221214 14:40:08 @agent_ppo2.py:185][0m |          -0.0031 |         215.3771 |        -318.8115 |
[32m[20221214 14:40:08 @agent_ppo2.py:185][0m |          -0.0000 |         214.4042 |        -318.8788 |
[32m[20221214 14:40:08 @agent_ppo2.py:185][0m |          -0.0021 |         214.0378 |        -318.6529 |
[32m[20221214 14:40:08 @agent_ppo2.py:185][0m |          -0.0010 |         213.6881 |        -318.1608 |
[32m[20221214 14:40:08 @agent_ppo2.py:185][0m |           0.0077 |         222.9870 |        -318.8647 |
[32m[20221214 14:40:08 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:40:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.02
[32m[20221214 14:40:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.81
[32m[20221214 14:40:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.65
[32m[20221214 14:40:09 @agent_ppo2.py:143][0m Total time:      42.11 min
[32m[20221214 14:40:09 @agent_ppo2.py:145][0m 3878912 total steps have happened
[32m[20221214 14:40:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1894 --------------------------#
[32m[20221214 14:40:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:09 @agent_ppo2.py:185][0m |          -0.0007 |         208.0306 |        -321.2879 |
[32m[20221214 14:40:09 @agent_ppo2.py:185][0m |          -0.0037 |         203.5785 |        -321.1204 |
[32m[20221214 14:40:09 @agent_ppo2.py:185][0m |          -0.0034 |         200.5938 |        -320.5978 |
[32m[20221214 14:40:09 @agent_ppo2.py:185][0m |           0.0005 |         199.6019 |        -320.6900 |
[32m[20221214 14:40:09 @agent_ppo2.py:185][0m |          -0.0033 |         197.9597 |        -320.7741 |
[32m[20221214 14:40:09 @agent_ppo2.py:185][0m |           0.0085 |         213.6672 |        -320.1121 |
[32m[20221214 14:40:10 @agent_ppo2.py:185][0m |          -0.0028 |         196.7276 |        -320.1643 |
[32m[20221214 14:40:10 @agent_ppo2.py:185][0m |          -0.0020 |         197.1140 |        -319.9629 |
[32m[20221214 14:40:10 @agent_ppo2.py:185][0m |          -0.0034 |         195.7275 |        -319.7032 |
[32m[20221214 14:40:10 @agent_ppo2.py:185][0m |          -0.0029 |         195.2407 |        -318.3369 |
[32m[20221214 14:40:10 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:40:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.51
[32m[20221214 14:40:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.20
[32m[20221214 14:40:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.27
[32m[20221214 14:40:10 @agent_ppo2.py:143][0m Total time:      42.13 min
[32m[20221214 14:40:10 @agent_ppo2.py:145][0m 3880960 total steps have happened
[32m[20221214 14:40:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1895 --------------------------#
[32m[20221214 14:40:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:10 @agent_ppo2.py:185][0m |           0.0006 |         195.0826 |        -317.1815 |
[32m[20221214 14:40:10 @agent_ppo2.py:185][0m |          -0.0016 |         191.5232 |        -318.4754 |
[32m[20221214 14:40:11 @agent_ppo2.py:185][0m |          -0.0010 |         190.1532 |        -317.8890 |
[32m[20221214 14:40:11 @agent_ppo2.py:185][0m |          -0.0005 |         189.3538 |        -317.7498 |
[32m[20221214 14:40:11 @agent_ppo2.py:185][0m |          -0.0025 |         190.0038 |        -318.5137 |
[32m[20221214 14:40:11 @agent_ppo2.py:185][0m |          -0.0013 |         189.6018 |        -318.4132 |
[32m[20221214 14:40:11 @agent_ppo2.py:185][0m |           0.0080 |         198.5146 |        -318.0969 |
[32m[20221214 14:40:11 @agent_ppo2.py:185][0m |           0.0030 |         188.4086 |        -317.3873 |
[32m[20221214 14:40:11 @agent_ppo2.py:185][0m |          -0.0016 |         187.7173 |        -317.9134 |
[32m[20221214 14:40:11 @agent_ppo2.py:185][0m |           0.0163 |         210.8237 |        -317.8082 |
[32m[20221214 14:40:11 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:40:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.09
[32m[20221214 14:40:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.28
[32m[20221214 14:40:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.04
[32m[20221214 14:40:11 @agent_ppo2.py:143][0m Total time:      42.15 min
[32m[20221214 14:40:11 @agent_ppo2.py:145][0m 3883008 total steps have happened
[32m[20221214 14:40:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1896 --------------------------#
[32m[20221214 14:40:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:12 @agent_ppo2.py:185][0m |          -0.0034 |         183.9528 |        -319.6728 |
[32m[20221214 14:40:12 @agent_ppo2.py:185][0m |           0.0052 |         190.1293 |        -319.2049 |
[32m[20221214 14:40:12 @agent_ppo2.py:185][0m |          -0.0009 |         180.4117 |        -319.1555 |
[32m[20221214 14:40:12 @agent_ppo2.py:185][0m |           0.0012 |         180.6811 |        -318.9479 |
[32m[20221214 14:40:12 @agent_ppo2.py:185][0m |          -0.0051 |         178.4098 |        -319.2874 |
[32m[20221214 14:40:12 @agent_ppo2.py:185][0m |          -0.0048 |         178.0930 |        -318.6462 |
[32m[20221214 14:40:12 @agent_ppo2.py:185][0m |          -0.0037 |         177.5609 |        -319.1978 |
[32m[20221214 14:40:12 @agent_ppo2.py:185][0m |           0.0089 |         197.5633 |        -319.1599 |
[32m[20221214 14:40:12 @agent_ppo2.py:185][0m |          -0.0000 |         177.9087 |        -319.3372 |
[32m[20221214 14:40:13 @agent_ppo2.py:185][0m |          -0.0014 |         177.3755 |        -318.9878 |
[32m[20221214 14:40:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:40:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.85
[32m[20221214 14:40:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.28
[32m[20221214 14:40:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.90
[32m[20221214 14:40:13 @agent_ppo2.py:143][0m Total time:      42.18 min
[32m[20221214 14:40:13 @agent_ppo2.py:145][0m 3885056 total steps have happened
[32m[20221214 14:40:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1897 --------------------------#
[32m[20221214 14:40:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:13 @agent_ppo2.py:185][0m |           0.0002 |         219.5595 |        -320.1668 |
[32m[20221214 14:40:13 @agent_ppo2.py:185][0m |          -0.0039 |         217.9430 |        -320.2252 |
[32m[20221214 14:40:13 @agent_ppo2.py:185][0m |           0.0033 |         216.8991 |        -319.8802 |
[32m[20221214 14:40:13 @agent_ppo2.py:185][0m |          -0.0024 |         215.5109 |        -320.1414 |
[32m[20221214 14:40:13 @agent_ppo2.py:185][0m |          -0.0029 |         214.8473 |        -321.1670 |
[32m[20221214 14:40:14 @agent_ppo2.py:185][0m |          -0.0036 |         214.9686 |        -321.4262 |
[32m[20221214 14:40:14 @agent_ppo2.py:185][0m |          -0.0034 |         213.7013 |        -321.0552 |
[32m[20221214 14:40:14 @agent_ppo2.py:185][0m |          -0.0020 |         213.7357 |        -320.7160 |
[32m[20221214 14:40:14 @agent_ppo2.py:185][0m |          -0.0040 |         213.5366 |        -321.3633 |
[32m[20221214 14:40:14 @agent_ppo2.py:185][0m |          -0.0029 |         212.7596 |        -322.2163 |
[32m[20221214 14:40:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:40:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.56
[32m[20221214 14:40:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.76
[32m[20221214 14:40:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.70
[32m[20221214 14:40:14 @agent_ppo2.py:143][0m Total time:      42.20 min
[32m[20221214 14:40:14 @agent_ppo2.py:145][0m 3887104 total steps have happened
[32m[20221214 14:40:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1898 --------------------------#
[32m[20221214 14:40:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:40:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:14 @agent_ppo2.py:185][0m |          -0.0009 |         227.5055 |        -321.6547 |
[32m[20221214 14:40:14 @agent_ppo2.py:185][0m |          -0.0013 |         221.5833 |        -321.2246 |
[32m[20221214 14:40:15 @agent_ppo2.py:185][0m |          -0.0029 |         218.5773 |        -322.5819 |
[32m[20221214 14:40:15 @agent_ppo2.py:185][0m |          -0.0009 |         217.9814 |        -322.7818 |
[32m[20221214 14:40:15 @agent_ppo2.py:185][0m |          -0.0006 |         216.9355 |        -322.0260 |
[32m[20221214 14:40:15 @agent_ppo2.py:185][0m |          -0.0039 |         216.4630 |        -322.9013 |
[32m[20221214 14:40:15 @agent_ppo2.py:185][0m |           0.0037 |         223.5781 |        -322.4516 |
[32m[20221214 14:40:15 @agent_ppo2.py:185][0m |           0.0038 |         220.4891 |        -322.0134 |
[32m[20221214 14:40:15 @agent_ppo2.py:185][0m |          -0.0028 |         215.7991 |        -322.9107 |
[32m[20221214 14:40:15 @agent_ppo2.py:185][0m |          -0.0025 |         215.9760 |        -322.8284 |
[32m[20221214 14:40:15 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 14:40:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.07
[32m[20221214 14:40:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.76
[32m[20221214 14:40:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.41
[32m[20221214 14:40:16 @agent_ppo2.py:143][0m Total time:      42.22 min
[32m[20221214 14:40:16 @agent_ppo2.py:145][0m 3889152 total steps have happened
[32m[20221214 14:40:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1899 --------------------------#
[32m[20221214 14:40:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:16 @agent_ppo2.py:185][0m |          -0.0001 |         244.1916 |        -319.9577 |
[32m[20221214 14:40:16 @agent_ppo2.py:185][0m |          -0.0010 |         241.2830 |        -320.4732 |
[32m[20221214 14:40:16 @agent_ppo2.py:185][0m |           0.0039 |         239.4349 |        -319.5822 |
[32m[20221214 14:40:16 @agent_ppo2.py:185][0m |           0.0048 |         245.0620 |        -321.1108 |
[32m[20221214 14:40:16 @agent_ppo2.py:185][0m |           0.0022 |         240.5859 |        -321.1291 |
[32m[20221214 14:40:16 @agent_ppo2.py:185][0m |          -0.0016 |         236.9337 |        -321.0703 |
[32m[20221214 14:40:16 @agent_ppo2.py:185][0m |          -0.0020 |         236.7785 |        -322.0684 |
[32m[20221214 14:40:17 @agent_ppo2.py:185][0m |          -0.0002 |         235.4941 |        -322.1343 |
[32m[20221214 14:40:17 @agent_ppo2.py:185][0m |          -0.0011 |         235.4707 |        -322.4959 |
[32m[20221214 14:40:17 @agent_ppo2.py:185][0m |          -0.0026 |         234.1027 |        -321.6066 |
[32m[20221214 14:40:17 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:40:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.72
[32m[20221214 14:40:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.58
[32m[20221214 14:40:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.46
[32m[20221214 14:40:17 @agent_ppo2.py:143][0m Total time:      42.25 min
[32m[20221214 14:40:17 @agent_ppo2.py:145][0m 3891200 total steps have happened
[32m[20221214 14:40:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1900 --------------------------#
[32m[20221214 14:40:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:40:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:17 @agent_ppo2.py:185][0m |           0.0084 |         232.3887 |        -324.9401 |
[32m[20221214 14:40:17 @agent_ppo2.py:185][0m |           0.0003 |         216.1587 |        -324.6428 |
[32m[20221214 14:40:17 @agent_ppo2.py:185][0m |          -0.0014 |         215.3454 |        -325.1756 |
[32m[20221214 14:40:18 @agent_ppo2.py:185][0m |          -0.0010 |         215.2458 |        -325.3406 |
[32m[20221214 14:40:18 @agent_ppo2.py:185][0m |          -0.0010 |         214.7351 |        -325.5927 |
[32m[20221214 14:40:18 @agent_ppo2.py:185][0m |          -0.0017 |         214.6165 |        -325.4511 |
[32m[20221214 14:40:18 @agent_ppo2.py:185][0m |          -0.0013 |         214.0667 |        -324.7715 |
[32m[20221214 14:40:18 @agent_ppo2.py:185][0m |          -0.0024 |         213.8821 |        -325.4805 |
[32m[20221214 14:40:18 @agent_ppo2.py:185][0m |          -0.0021 |         213.5773 |        -325.5929 |
[32m[20221214 14:40:18 @agent_ppo2.py:185][0m |          -0.0018 |         213.4509 |        -325.2603 |
[32m[20221214 14:40:18 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:40:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.22
[32m[20221214 14:40:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.39
[32m[20221214 14:40:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.97
[32m[20221214 14:40:18 @agent_ppo2.py:143][0m Total time:      42.27 min
[32m[20221214 14:40:18 @agent_ppo2.py:145][0m 3893248 total steps have happened
[32m[20221214 14:40:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1901 --------------------------#
[32m[20221214 14:40:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:19 @agent_ppo2.py:185][0m |          -0.0009 |         217.0928 |        -323.7237 |
[32m[20221214 14:40:19 @agent_ppo2.py:185][0m |           0.0014 |         213.0244 |        -324.2852 |
[32m[20221214 14:40:19 @agent_ppo2.py:185][0m |          -0.0016 |         210.0514 |        -324.7889 |
[32m[20221214 14:40:19 @agent_ppo2.py:185][0m |           0.0024 |         214.4856 |        -324.8435 |
[32m[20221214 14:40:19 @agent_ppo2.py:185][0m |          -0.0012 |         207.4186 |        -324.6591 |
[32m[20221214 14:40:19 @agent_ppo2.py:185][0m |          -0.0026 |         207.9781 |        -325.1844 |
[32m[20221214 14:40:19 @agent_ppo2.py:185][0m |          -0.0027 |         206.6404 |        -323.7701 |
[32m[20221214 14:40:19 @agent_ppo2.py:185][0m |          -0.0021 |         205.7722 |        -324.4035 |
[32m[20221214 14:40:19 @agent_ppo2.py:185][0m |          -0.0021 |         204.9036 |        -324.6528 |
[32m[20221214 14:40:19 @agent_ppo2.py:185][0m |          -0.0008 |         205.6670 |        -324.6785 |
[32m[20221214 14:40:19 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:40:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 831.31
[32m[20221214 14:40:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.79
[32m[20221214 14:40:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.74
[32m[20221214 14:40:20 @agent_ppo2.py:143][0m Total time:      42.29 min
[32m[20221214 14:40:20 @agent_ppo2.py:145][0m 3895296 total steps have happened
[32m[20221214 14:40:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1902 --------------------------#
[32m[20221214 14:40:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:20 @agent_ppo2.py:185][0m |           0.0004 |         205.9880 |        -325.6233 |
[32m[20221214 14:40:20 @agent_ppo2.py:185][0m |          -0.0015 |         202.9133 |        -325.6379 |
[32m[20221214 14:40:20 @agent_ppo2.py:185][0m |          -0.0018 |         199.3525 |        -325.2868 |
[32m[20221214 14:40:20 @agent_ppo2.py:185][0m |          -0.0014 |         196.2998 |        -325.0733 |
[32m[20221214 14:40:20 @agent_ppo2.py:185][0m |           0.0002 |         192.4245 |        -325.4243 |
[32m[20221214 14:40:20 @agent_ppo2.py:185][0m |          -0.0001 |         190.0620 |        -324.4082 |
[32m[20221214 14:40:21 @agent_ppo2.py:185][0m |           0.0072 |         193.1900 |        -325.9750 |
[32m[20221214 14:40:21 @agent_ppo2.py:185][0m |           0.0020 |         187.6471 |        -325.8493 |
[32m[20221214 14:40:21 @agent_ppo2.py:185][0m |           0.0052 |         187.3178 |        -325.1746 |
[32m[20221214 14:40:21 @agent_ppo2.py:185][0m |           0.0069 |         192.3966 |        -325.3849 |
[32m[20221214 14:40:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:40:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.02
[32m[20221214 14:40:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.47
[32m[20221214 14:40:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.35
[32m[20221214 14:40:21 @agent_ppo2.py:143][0m Total time:      42.32 min
[32m[20221214 14:40:21 @agent_ppo2.py:145][0m 3897344 total steps have happened
[32m[20221214 14:40:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1903 --------------------------#
[32m[20221214 14:40:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:21 @agent_ppo2.py:185][0m |           0.0065 |         212.2017 |        -323.7489 |
[32m[20221214 14:40:21 @agent_ppo2.py:185][0m |          -0.0010 |         200.4589 |        -320.9209 |
[32m[20221214 14:40:22 @agent_ppo2.py:185][0m |          -0.0001 |         198.8085 |        -322.4322 |
[32m[20221214 14:40:22 @agent_ppo2.py:185][0m |          -0.0007 |         197.9861 |        -323.3137 |
[32m[20221214 14:40:22 @agent_ppo2.py:185][0m |           0.0018 |         198.7729 |        -324.3181 |
[32m[20221214 14:40:22 @agent_ppo2.py:185][0m |           0.0011 |         196.6682 |        -323.4716 |
[32m[20221214 14:40:22 @agent_ppo2.py:185][0m |           0.0037 |         199.0214 |        -324.4146 |
[32m[20221214 14:40:22 @agent_ppo2.py:185][0m |           0.0018 |         196.5911 |        -323.8894 |
[32m[20221214 14:40:22 @agent_ppo2.py:185][0m |          -0.0004 |         196.4901 |        -323.6942 |
[32m[20221214 14:40:22 @agent_ppo2.py:185][0m |          -0.0019 |         195.9673 |        -324.2582 |
[32m[20221214 14:40:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:40:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.56
[32m[20221214 14:40:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.62
[32m[20221214 14:40:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.83
[32m[20221214 14:40:22 @agent_ppo2.py:143][0m Total time:      42.34 min
[32m[20221214 14:40:22 @agent_ppo2.py:145][0m 3899392 total steps have happened
[32m[20221214 14:40:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1904 --------------------------#
[32m[20221214 14:40:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:23 @agent_ppo2.py:185][0m |           0.0038 |         184.4450 |        -328.2870 |
[32m[20221214 14:40:23 @agent_ppo2.py:185][0m |          -0.0027 |         169.8825 |        -327.9351 |
[32m[20221214 14:40:23 @agent_ppo2.py:185][0m |           0.0004 |         166.3494 |        -327.3861 |
[32m[20221214 14:40:23 @agent_ppo2.py:185][0m |          -0.0015 |         165.0348 |        -326.8378 |
[32m[20221214 14:40:23 @agent_ppo2.py:185][0m |          -0.0009 |         164.1271 |        -328.2427 |
[32m[20221214 14:40:23 @agent_ppo2.py:185][0m |          -0.0015 |         163.1857 |        -327.9859 |
[32m[20221214 14:40:23 @agent_ppo2.py:185][0m |           0.0020 |         162.2048 |        -329.0418 |
[32m[20221214 14:40:23 @agent_ppo2.py:185][0m |          -0.0018 |         161.8999 |        -327.2240 |
[32m[20221214 14:40:23 @agent_ppo2.py:185][0m |          -0.0025 |         160.6620 |        -328.7295 |
[32m[20221214 14:40:23 @agent_ppo2.py:185][0m |           0.0052 |         164.7098 |        -329.3207 |
[32m[20221214 14:40:23 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:40:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.34
[32m[20221214 14:40:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.93
[32m[20221214 14:40:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.54
[32m[20221214 14:40:24 @agent_ppo2.py:143][0m Total time:      42.36 min
[32m[20221214 14:40:24 @agent_ppo2.py:145][0m 3901440 total steps have happened
[32m[20221214 14:40:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1905 --------------------------#
[32m[20221214 14:40:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:40:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:24 @agent_ppo2.py:185][0m |           0.0012 |         174.7191 |        -328.3915 |
[32m[20221214 14:40:24 @agent_ppo2.py:185][0m |           0.0020 |         172.1626 |        -327.7227 |
[32m[20221214 14:40:24 @agent_ppo2.py:185][0m |           0.0005 |         165.4195 |        -328.1916 |
[32m[20221214 14:40:24 @agent_ppo2.py:185][0m |          -0.0037 |         162.3074 |        -328.1264 |
[32m[20221214 14:40:24 @agent_ppo2.py:185][0m |           0.0011 |         164.0800 |        -328.8267 |
[32m[20221214 14:40:24 @agent_ppo2.py:185][0m |          -0.0063 |         159.5701 |        -328.8131 |
[32m[20221214 14:40:24 @agent_ppo2.py:185][0m |          -0.0050 |         159.5233 |        -329.7402 |
[32m[20221214 14:40:24 @agent_ppo2.py:185][0m |          -0.0035 |         157.8917 |        -328.3447 |
[32m[20221214 14:40:25 @agent_ppo2.py:185][0m |          -0.0057 |         157.9054 |        -329.1476 |
[32m[20221214 14:40:25 @agent_ppo2.py:185][0m |          -0.0038 |         157.0926 |        -329.6394 |
[32m[20221214 14:40:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:40:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.11
[32m[20221214 14:40:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.53
[32m[20221214 14:40:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.77
[32m[20221214 14:40:25 @agent_ppo2.py:143][0m Total time:      42.38 min
[32m[20221214 14:40:25 @agent_ppo2.py:145][0m 3903488 total steps have happened
[32m[20221214 14:40:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1906 --------------------------#
[32m[20221214 14:40:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:40:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:25 @agent_ppo2.py:185][0m |          -0.0016 |         221.8215 |        -332.0656 |
[32m[20221214 14:40:25 @agent_ppo2.py:185][0m |          -0.0009 |         214.4293 |        -331.7670 |
[32m[20221214 14:40:25 @agent_ppo2.py:185][0m |           0.0008 |         210.4557 |        -331.4305 |
[32m[20221214 14:40:25 @agent_ppo2.py:185][0m |          -0.0008 |         209.6857 |        -330.7141 |
[32m[20221214 14:40:25 @agent_ppo2.py:185][0m |          -0.0016 |         208.8778 |        -331.8651 |
[32m[20221214 14:40:26 @agent_ppo2.py:185][0m |          -0.0001 |         208.0568 |        -331.6815 |
[32m[20221214 14:40:26 @agent_ppo2.py:185][0m |          -0.0011 |         207.0371 |        -330.4356 |
[32m[20221214 14:40:26 @agent_ppo2.py:185][0m |           0.0122 |         217.0569 |        -330.6427 |
[32m[20221214 14:40:26 @agent_ppo2.py:185][0m |          -0.0015 |         207.1440 |        -331.2145 |
[32m[20221214 14:40:26 @agent_ppo2.py:185][0m |          -0.0013 |         205.9893 |        -330.8954 |
[32m[20221214 14:40:26 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:40:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.72
[32m[20221214 14:40:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.64
[32m[20221214 14:40:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 824.95
[32m[20221214 14:40:26 @agent_ppo2.py:143][0m Total time:      42.40 min
[32m[20221214 14:40:26 @agent_ppo2.py:145][0m 3905536 total steps have happened
[32m[20221214 14:40:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1907 --------------------------#
[32m[20221214 14:40:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:26 @agent_ppo2.py:185][0m |          -0.0014 |         221.0425 |        -324.4731 |
[32m[20221214 14:40:27 @agent_ppo2.py:185][0m |           0.0086 |         224.6488 |        -324.3794 |
[32m[20221214 14:40:27 @agent_ppo2.py:185][0m |          -0.0006 |         213.7747 |        -324.6165 |
[32m[20221214 14:40:27 @agent_ppo2.py:185][0m |           0.0000 |         211.5818 |        -324.7121 |
[32m[20221214 14:40:27 @agent_ppo2.py:185][0m |           0.0114 |         218.1372 |        -324.6134 |
[32m[20221214 14:40:27 @agent_ppo2.py:185][0m |          -0.0024 |         209.9024 |        -324.3764 |
[32m[20221214 14:40:27 @agent_ppo2.py:185][0m |          -0.0029 |         209.2524 |        -324.6701 |
[32m[20221214 14:40:27 @agent_ppo2.py:185][0m |          -0.0004 |         208.4812 |        -325.6906 |
[32m[20221214 14:40:27 @agent_ppo2.py:185][0m |           0.0006 |         207.6202 |        -325.0043 |
[32m[20221214 14:40:27 @agent_ppo2.py:185][0m |           0.0093 |         234.5504 |        -325.5337 |
[32m[20221214 14:40:27 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:40:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.11
[32m[20221214 14:40:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.56
[32m[20221214 14:40:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.04
[32m[20221214 14:40:27 @agent_ppo2.py:143][0m Total time:      42.42 min
[32m[20221214 14:40:27 @agent_ppo2.py:145][0m 3907584 total steps have happened
[32m[20221214 14:40:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1908 --------------------------#
[32m[20221214 14:40:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:28 @agent_ppo2.py:185][0m |           0.0055 |         194.6214 |        -330.0578 |
[32m[20221214 14:40:28 @agent_ppo2.py:185][0m |          -0.0017 |         177.4065 |        -329.2971 |
[32m[20221214 14:40:28 @agent_ppo2.py:185][0m |           0.0056 |         180.0509 |        -329.7185 |
[32m[20221214 14:40:28 @agent_ppo2.py:185][0m |          -0.0010 |         173.6081 |        -330.7546 |
[32m[20221214 14:40:28 @agent_ppo2.py:185][0m |           0.0066 |         190.0662 |        -330.6377 |
[32m[20221214 14:40:28 @agent_ppo2.py:185][0m |          -0.0011 |         171.8429 |        -330.2243 |
[32m[20221214 14:40:28 @agent_ppo2.py:185][0m |          -0.0023 |         171.1126 |        -330.2891 |
[32m[20221214 14:40:29 @agent_ppo2.py:185][0m |           0.0090 |         184.9748 |        -330.1716 |
[32m[20221214 14:40:29 @agent_ppo2.py:185][0m |          -0.0000 |         170.1517 |        -329.8589 |
[32m[20221214 14:40:29 @agent_ppo2.py:185][0m |          -0.0032 |         168.8768 |        -330.0232 |
[32m[20221214 14:40:29 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:40:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.17
[32m[20221214 14:40:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.85
[32m[20221214 14:40:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.67
[32m[20221214 14:40:29 @agent_ppo2.py:143][0m Total time:      42.45 min
[32m[20221214 14:40:29 @agent_ppo2.py:145][0m 3909632 total steps have happened
[32m[20221214 14:40:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1909 --------------------------#
[32m[20221214 14:40:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:29 @agent_ppo2.py:185][0m |          -0.0004 |         186.0470 |        -332.9447 |
[32m[20221214 14:40:29 @agent_ppo2.py:185][0m |          -0.0017 |         181.4209 |        -333.0943 |
[32m[20221214 14:40:29 @agent_ppo2.py:185][0m |          -0.0006 |         179.7082 |        -332.0920 |
[32m[20221214 14:40:29 @agent_ppo2.py:185][0m |           0.0064 |         183.7842 |        -332.6881 |
[32m[20221214 14:40:30 @agent_ppo2.py:185][0m |           0.0092 |         198.8382 |        -333.0100 |
[32m[20221214 14:40:30 @agent_ppo2.py:185][0m |           0.0105 |         194.1645 |        -331.8858 |
[32m[20221214 14:40:30 @agent_ppo2.py:185][0m |          -0.0030 |         176.9736 |        -331.4837 |
[32m[20221214 14:40:30 @agent_ppo2.py:185][0m |          -0.0008 |         176.8239 |        -331.4096 |
[32m[20221214 14:40:30 @agent_ppo2.py:185][0m |          -0.0026 |         176.3622 |        -332.3405 |
[32m[20221214 14:40:30 @agent_ppo2.py:185][0m |          -0.0002 |         176.1871 |        -332.2234 |
[32m[20221214 14:40:30 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:40:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.22
[32m[20221214 14:40:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.24
[32m[20221214 14:40:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.43
[32m[20221214 14:40:30 @agent_ppo2.py:143][0m Total time:      42.47 min
[32m[20221214 14:40:30 @agent_ppo2.py:145][0m 3911680 total steps have happened
[32m[20221214 14:40:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1910 --------------------------#
[32m[20221214 14:40:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:40:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:31 @agent_ppo2.py:185][0m |          -0.0011 |         215.6206 |        -329.3351 |
[32m[20221214 14:40:31 @agent_ppo2.py:185][0m |          -0.0018 |         210.0367 |        -328.6591 |
[32m[20221214 14:40:31 @agent_ppo2.py:185][0m |           0.0043 |         210.3955 |        -327.9666 |
[32m[20221214 14:40:31 @agent_ppo2.py:185][0m |          -0.0004 |         206.8674 |        -328.2468 |
[32m[20221214 14:40:31 @agent_ppo2.py:185][0m |          -0.0014 |         205.1699 |        -327.0558 |
[32m[20221214 14:40:31 @agent_ppo2.py:185][0m |           0.0091 |         221.3643 |        -326.5404 |
[32m[20221214 14:40:31 @agent_ppo2.py:185][0m |          -0.0029 |         203.8826 |        -326.8132 |
[32m[20221214 14:40:31 @agent_ppo2.py:185][0m |          -0.0003 |         203.0210 |        -326.1852 |
[32m[20221214 14:40:31 @agent_ppo2.py:185][0m |          -0.0043 |         201.9949 |        -325.9082 |
[32m[20221214 14:40:31 @agent_ppo2.py:185][0m |          -0.0019 |         201.4168 |        -325.6966 |
[32m[20221214 14:40:31 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:40:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.68
[32m[20221214 14:40:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.01
[32m[20221214 14:40:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.32
[32m[20221214 14:40:32 @agent_ppo2.py:143][0m Total time:      42.49 min
[32m[20221214 14:40:32 @agent_ppo2.py:145][0m 3913728 total steps have happened
[32m[20221214 14:40:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1911 --------------------------#
[32m[20221214 14:40:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:32 @agent_ppo2.py:185][0m |           0.0015 |         224.5681 |        -322.2799 |
[32m[20221214 14:40:32 @agent_ppo2.py:185][0m |          -0.0021 |         210.4899 |        -321.1949 |
[32m[20221214 14:40:32 @agent_ppo2.py:185][0m |          -0.0014 |         206.6298 |        -322.3853 |
[32m[20221214 14:40:32 @agent_ppo2.py:185][0m |          -0.0002 |         205.5459 |        -321.8978 |
[32m[20221214 14:40:32 @agent_ppo2.py:185][0m |           0.0014 |         204.0788 |        -321.0023 |
[32m[20221214 14:40:32 @agent_ppo2.py:185][0m |          -0.0019 |         203.3685 |        -322.0097 |
[32m[20221214 14:40:33 @agent_ppo2.py:185][0m |          -0.0023 |         201.7923 |        -321.4221 |
[32m[20221214 14:40:33 @agent_ppo2.py:185][0m |           0.0008 |         202.7135 |        -320.7718 |
[32m[20221214 14:40:33 @agent_ppo2.py:185][0m |          -0.0007 |         201.3114 |        -321.8852 |
[32m[20221214 14:40:33 @agent_ppo2.py:185][0m |          -0.0012 |         200.7417 |        -321.2522 |
[32m[20221214 14:40:33 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:40:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.92
[32m[20221214 14:40:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.54
[32m[20221214 14:40:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.61
[32m[20221214 14:40:33 @agent_ppo2.py:143][0m Total time:      42.51 min
[32m[20221214 14:40:33 @agent_ppo2.py:145][0m 3915776 total steps have happened
[32m[20221214 14:40:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1912 --------------------------#
[32m[20221214 14:40:33 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 14:40:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:33 @agent_ppo2.py:185][0m |          -0.0018 |         268.3334 |        -319.9734 |
[32m[20221214 14:40:33 @agent_ppo2.py:185][0m |           0.0056 |         272.8430 |        -320.0377 |
[32m[20221214 14:40:34 @agent_ppo2.py:185][0m |           0.0020 |         258.7275 |        -319.2267 |
[32m[20221214 14:40:34 @agent_ppo2.py:185][0m |           0.0004 |         257.2342 |        -319.6700 |
[32m[20221214 14:40:34 @agent_ppo2.py:185][0m |           0.0001 |         256.7785 |        -320.0820 |
[32m[20221214 14:40:34 @agent_ppo2.py:185][0m |          -0.0004 |         255.2759 |        -319.0784 |
[32m[20221214 14:40:34 @agent_ppo2.py:185][0m |          -0.0021 |         254.7675 |        -320.2789 |
[32m[20221214 14:40:34 @agent_ppo2.py:185][0m |          -0.0003 |         254.8616 |        -319.2971 |
[32m[20221214 14:40:34 @agent_ppo2.py:185][0m |          -0.0015 |         253.8894 |        -320.9696 |
[32m[20221214 14:40:34 @agent_ppo2.py:185][0m |           0.0006 |         253.1637 |        -320.5289 |
[32m[20221214 14:40:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:40:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.52
[32m[20221214 14:40:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.82
[32m[20221214 14:40:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.75
[32m[20221214 14:40:34 @agent_ppo2.py:143][0m Total time:      42.54 min
[32m[20221214 14:40:34 @agent_ppo2.py:145][0m 3917824 total steps have happened
[32m[20221214 14:40:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1913 --------------------------#
[32m[20221214 14:40:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:40:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:35 @agent_ppo2.py:185][0m |           0.0153 |         270.7867 |        -318.9982 |
[32m[20221214 14:40:35 @agent_ppo2.py:185][0m |           0.0049 |         241.8117 |        -319.3398 |
[32m[20221214 14:40:35 @agent_ppo2.py:185][0m |          -0.0005 |         232.1125 |        -318.6568 |
[32m[20221214 14:40:35 @agent_ppo2.py:185][0m |          -0.0034 |         230.6076 |        -319.3460 |
[32m[20221214 14:40:35 @agent_ppo2.py:185][0m |          -0.0027 |         229.5088 |        -318.8690 |
[32m[20221214 14:40:35 @agent_ppo2.py:185][0m |           0.0003 |         229.0899 |        -319.7172 |
[32m[20221214 14:40:35 @agent_ppo2.py:185][0m |          -0.0004 |         229.5852 |        -319.8714 |
[32m[20221214 14:40:35 @agent_ppo2.py:185][0m |           0.0003 |         232.3387 |        -319.9617 |
[32m[20221214 14:40:35 @agent_ppo2.py:185][0m |          -0.0006 |         227.3730 |        -319.9466 |
[32m[20221214 14:40:35 @agent_ppo2.py:185][0m |          -0.0007 |         226.6479 |        -320.1465 |
[32m[20221214 14:40:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:40:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.37
[32m[20221214 14:40:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.53
[32m[20221214 14:40:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.18
[32m[20221214 14:40:36 @agent_ppo2.py:143][0m Total time:      42.56 min
[32m[20221214 14:40:36 @agent_ppo2.py:145][0m 3919872 total steps have happened
[32m[20221214 14:40:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1914 --------------------------#
[32m[20221214 14:40:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:36 @agent_ppo2.py:185][0m |          -0.0020 |         258.5293 |        -320.9335 |
[32m[20221214 14:40:36 @agent_ppo2.py:185][0m |          -0.0032 |         244.4021 |        -320.6902 |
[32m[20221214 14:40:36 @agent_ppo2.py:185][0m |          -0.0012 |         237.7705 |        -320.0392 |
[32m[20221214 14:40:36 @agent_ppo2.py:185][0m |          -0.0030 |         233.8700 |        -320.1867 |
[32m[20221214 14:40:36 @agent_ppo2.py:185][0m |          -0.0001 |         233.9261 |        -319.0018 |
[32m[20221214 14:40:36 @agent_ppo2.py:185][0m |          -0.0041 |         231.8585 |        -318.7520 |
[32m[20221214 14:40:37 @agent_ppo2.py:185][0m |          -0.0063 |         230.1997 |        -319.0355 |
[32m[20221214 14:40:37 @agent_ppo2.py:185][0m |          -0.0005 |         230.4020 |        -319.9089 |
[32m[20221214 14:40:37 @agent_ppo2.py:185][0m |           0.0044 |         235.2881 |        -319.3498 |
[32m[20221214 14:40:37 @agent_ppo2.py:185][0m |          -0.0027 |         228.6654 |        -319.0754 |
[32m[20221214 14:40:37 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 14:40:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.47
[32m[20221214 14:40:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.75
[32m[20221214 14:40:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.69
[32m[20221214 14:40:37 @agent_ppo2.py:143][0m Total time:      42.58 min
[32m[20221214 14:40:37 @agent_ppo2.py:145][0m 3921920 total steps have happened
[32m[20221214 14:40:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1915 --------------------------#
[32m[20221214 14:40:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:37 @agent_ppo2.py:185][0m |          -0.0012 |         213.9077 |        -314.4614 |
[32m[20221214 14:40:37 @agent_ppo2.py:185][0m |          -0.0024 |         197.7937 |        -315.2472 |
[32m[20221214 14:40:38 @agent_ppo2.py:185][0m |           0.0000 |         192.6276 |        -315.2980 |
[32m[20221214 14:40:38 @agent_ppo2.py:185][0m |          -0.0033 |         189.6341 |        -316.0391 |
[32m[20221214 14:40:38 @agent_ppo2.py:185][0m |           0.0030 |         188.3151 |        -316.0760 |
[32m[20221214 14:40:38 @agent_ppo2.py:185][0m |           0.0118 |         202.1519 |        -314.2732 |
[32m[20221214 14:40:38 @agent_ppo2.py:185][0m |          -0.0000 |         185.5601 |        -316.3901 |
[32m[20221214 14:40:38 @agent_ppo2.py:185][0m |           0.0003 |         183.8091 |        -316.3929 |
[32m[20221214 14:40:38 @agent_ppo2.py:185][0m |          -0.0001 |         183.6702 |        -316.1414 |
[32m[20221214 14:40:38 @agent_ppo2.py:185][0m |          -0.0052 |         182.2526 |        -315.6979 |
[32m[20221214 14:40:38 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:40:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.61
[32m[20221214 14:40:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.22
[32m[20221214 14:40:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.97
[32m[20221214 14:40:38 @agent_ppo2.py:143][0m Total time:      42.61 min
[32m[20221214 14:40:38 @agent_ppo2.py:145][0m 3923968 total steps have happened
[32m[20221214 14:40:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1916 --------------------------#
[32m[20221214 14:40:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:39 @agent_ppo2.py:185][0m |          -0.0035 |         204.1539 |        -320.3701 |
[32m[20221214 14:40:39 @agent_ppo2.py:185][0m |           0.0009 |         182.1186 |        -320.8388 |
[32m[20221214 14:40:39 @agent_ppo2.py:185][0m |           0.0027 |         178.2883 |        -320.2951 |
[32m[20221214 14:40:39 @agent_ppo2.py:185][0m |          -0.0023 |         177.5989 |        -320.0671 |
[32m[20221214 14:40:39 @agent_ppo2.py:185][0m |           0.0007 |         173.8127 |        -320.4658 |
[32m[20221214 14:40:39 @agent_ppo2.py:185][0m |          -0.0022 |         172.7601 |        -320.7224 |
[32m[20221214 14:40:39 @agent_ppo2.py:185][0m |           0.0152 |         192.3136 |        -320.4387 |
[32m[20221214 14:40:39 @agent_ppo2.py:185][0m |           0.0015 |         172.4678 |        -320.8148 |
[32m[20221214 14:40:39 @agent_ppo2.py:185][0m |          -0.0023 |         170.0030 |        -320.0125 |
[32m[20221214 14:40:40 @agent_ppo2.py:185][0m |          -0.0055 |         169.0738 |        -320.2382 |
[32m[20221214 14:40:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:40:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 816.97
[32m[20221214 14:40:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.56
[32m[20221214 14:40:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.19
[32m[20221214 14:40:40 @agent_ppo2.py:143][0m Total time:      42.63 min
[32m[20221214 14:40:40 @agent_ppo2.py:145][0m 3926016 total steps have happened
[32m[20221214 14:40:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1917 --------------------------#
[32m[20221214 14:40:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:40 @agent_ppo2.py:185][0m |           0.0001 |         192.0942 |        -321.4914 |
[32m[20221214 14:40:40 @agent_ppo2.py:185][0m |          -0.0001 |         179.5771 |        -321.9169 |
[32m[20221214 14:40:40 @agent_ppo2.py:185][0m |           0.0000 |         174.7226 |        -321.6156 |
[32m[20221214 14:40:40 @agent_ppo2.py:185][0m |          -0.0015 |         171.6469 |        -321.7133 |
[32m[20221214 14:40:40 @agent_ppo2.py:185][0m |          -0.0006 |         170.4109 |        -322.0713 |
[32m[20221214 14:40:41 @agent_ppo2.py:185][0m |           0.0025 |         168.9091 |        -321.8857 |
[32m[20221214 14:40:41 @agent_ppo2.py:185][0m |           0.0025 |         168.5741 |        -321.7871 |
[32m[20221214 14:40:41 @agent_ppo2.py:185][0m |          -0.0029 |         167.5148 |        -322.7223 |
[32m[20221214 14:40:41 @agent_ppo2.py:185][0m |          -0.0010 |         166.5804 |        -321.3360 |
[32m[20221214 14:40:41 @agent_ppo2.py:185][0m |          -0.0018 |         165.9351 |        -322.8252 |
[32m[20221214 14:40:41 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:40:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.72
[32m[20221214 14:40:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.78
[32m[20221214 14:40:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.73
[32m[20221214 14:40:41 @agent_ppo2.py:143][0m Total time:      42.65 min
[32m[20221214 14:40:41 @agent_ppo2.py:145][0m 3928064 total steps have happened
[32m[20221214 14:40:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1918 --------------------------#
[32m[20221214 14:40:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:41 @agent_ppo2.py:185][0m |          -0.0040 |         215.9190 |        -323.2528 |
[32m[20221214 14:40:42 @agent_ppo2.py:185][0m |          -0.0025 |         206.0435 |        -323.5022 |
[32m[20221214 14:40:42 @agent_ppo2.py:185][0m |           0.0079 |         214.2775 |        -323.8215 |
[32m[20221214 14:40:42 @agent_ppo2.py:185][0m |          -0.0004 |         201.8893 |        -324.2094 |
[32m[20221214 14:40:42 @agent_ppo2.py:185][0m |          -0.0049 |         201.7654 |        -323.9273 |
[32m[20221214 14:40:42 @agent_ppo2.py:185][0m |          -0.0034 |         200.8046 |        -324.9342 |
[32m[20221214 14:40:42 @agent_ppo2.py:185][0m |           0.0056 |         203.7464 |        -324.8067 |
[32m[20221214 14:40:42 @agent_ppo2.py:185][0m |          -0.0002 |         200.1608 |        -324.1853 |
[32m[20221214 14:40:42 @agent_ppo2.py:185][0m |          -0.0058 |         198.8197 |        -325.6602 |
[32m[20221214 14:40:42 @agent_ppo2.py:185][0m |           0.0006 |         198.2898 |        -326.0607 |
[32m[20221214 14:40:42 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:40:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.02
[32m[20221214 14:40:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.80
[32m[20221214 14:40:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.62
[32m[20221214 14:40:42 @agent_ppo2.py:143][0m Total time:      42.67 min
[32m[20221214 14:40:42 @agent_ppo2.py:145][0m 3930112 total steps have happened
[32m[20221214 14:40:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1919 --------------------------#
[32m[20221214 14:40:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:43 @agent_ppo2.py:185][0m |           0.0046 |         192.8816 |        -327.7072 |
[32m[20221214 14:40:43 @agent_ppo2.py:185][0m |           0.0129 |         202.6607 |        -327.1357 |
[32m[20221214 14:40:43 @agent_ppo2.py:185][0m |           0.0008 |         184.8067 |        -327.8375 |
[32m[20221214 14:40:43 @agent_ppo2.py:185][0m |          -0.0015 |         181.3553 |        -327.4499 |
[32m[20221214 14:40:43 @agent_ppo2.py:185][0m |           0.0027 |         182.0634 |        -327.3553 |
[32m[20221214 14:40:43 @agent_ppo2.py:185][0m |           0.0044 |         184.5276 |        -326.7563 |
[32m[20221214 14:40:43 @agent_ppo2.py:185][0m |           0.0008 |         177.4472 |        -326.7571 |
[32m[20221214 14:40:43 @agent_ppo2.py:185][0m |           0.0127 |         192.4194 |        -327.7644 |
[32m[20221214 14:40:44 @agent_ppo2.py:185][0m |          -0.0002 |         178.9324 |        -326.7600 |
[32m[20221214 14:40:44 @agent_ppo2.py:185][0m |          -0.0033 |         175.3456 |        -326.7859 |
[32m[20221214 14:40:44 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:40:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 825.79
[32m[20221214 14:40:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.24
[32m[20221214 14:40:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.06
[32m[20221214 14:40:44 @agent_ppo2.py:143][0m Total time:      42.70 min
[32m[20221214 14:40:44 @agent_ppo2.py:145][0m 3932160 total steps have happened
[32m[20221214 14:40:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1920 --------------------------#
[32m[20221214 14:40:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:40:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:44 @agent_ppo2.py:185][0m |          -0.0001 |         231.5155 |        -327.4487 |
[32m[20221214 14:40:44 @agent_ppo2.py:185][0m |           0.0006 |         214.6325 |        -327.8303 |
[32m[20221214 14:40:44 @agent_ppo2.py:185][0m |           0.0003 |         210.3348 |        -327.2219 |
[32m[20221214 14:40:45 @agent_ppo2.py:185][0m |          -0.0029 |         205.2214 |        -328.5725 |
[32m[20221214 14:40:45 @agent_ppo2.py:185][0m |           0.0062 |         205.9623 |        -328.4583 |
[32m[20221214 14:40:45 @agent_ppo2.py:185][0m |          -0.0007 |         200.3188 |        -328.6049 |
[32m[20221214 14:40:45 @agent_ppo2.py:185][0m |           0.0009 |         198.3413 |        -327.8084 |
[32m[20221214 14:40:45 @agent_ppo2.py:185][0m |          -0.0022 |         196.6136 |        -328.3186 |
[32m[20221214 14:40:45 @agent_ppo2.py:185][0m |          -0.0012 |         195.9143 |        -328.0175 |
[32m[20221214 14:40:45 @agent_ppo2.py:185][0m |          -0.0023 |         194.2622 |        -329.0723 |
[32m[20221214 14:40:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:40:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.25
[32m[20221214 14:40:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.16
[32m[20221214 14:40:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.23
[32m[20221214 14:40:45 @agent_ppo2.py:143][0m Total time:      42.72 min
[32m[20221214 14:40:45 @agent_ppo2.py:145][0m 3934208 total steps have happened
[32m[20221214 14:40:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1921 --------------------------#
[32m[20221214 14:40:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:46 @agent_ppo2.py:185][0m |           0.0001 |         202.9144 |        -323.1071 |
[32m[20221214 14:40:46 @agent_ppo2.py:185][0m |          -0.0014 |         200.1632 |        -322.5357 |
[32m[20221214 14:40:46 @agent_ppo2.py:185][0m |          -0.0015 |         198.0473 |        -322.3168 |
[32m[20221214 14:40:46 @agent_ppo2.py:185][0m |          -0.0038 |         198.1895 |        -323.3669 |
[32m[20221214 14:40:46 @agent_ppo2.py:185][0m |          -0.0020 |         195.5406 |        -323.2134 |
[32m[20221214 14:40:46 @agent_ppo2.py:185][0m |          -0.0005 |         194.8151 |        -322.5643 |
[32m[20221214 14:40:46 @agent_ppo2.py:185][0m |           0.0040 |         206.8325 |        -322.3441 |
[32m[20221214 14:40:46 @agent_ppo2.py:185][0m |           0.0000 |         193.4765 |        -322.4337 |
[32m[20221214 14:40:46 @agent_ppo2.py:185][0m |           0.0030 |         195.0852 |        -322.0797 |
[32m[20221214 14:40:46 @agent_ppo2.py:185][0m |          -0.0054 |         195.1834 |        -322.9543 |
[32m[20221214 14:40:46 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:40:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.01
[32m[20221214 14:40:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.63
[32m[20221214 14:40:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.26
[32m[20221214 14:40:47 @agent_ppo2.py:143][0m Total time:      42.74 min
[32m[20221214 14:40:47 @agent_ppo2.py:145][0m 3936256 total steps have happened
[32m[20221214 14:40:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1922 --------------------------#
[32m[20221214 14:40:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:47 @agent_ppo2.py:185][0m |          -0.0011 |         184.4303 |        -324.6295 |
[32m[20221214 14:40:47 @agent_ppo2.py:185][0m |          -0.0027 |         172.6520 |        -324.1216 |
[32m[20221214 14:40:47 @agent_ppo2.py:185][0m |           0.0123 |         204.7960 |        -324.5125 |
[32m[20221214 14:40:47 @agent_ppo2.py:185][0m |          -0.0040 |         167.3227 |        -324.3229 |
[32m[20221214 14:40:47 @agent_ppo2.py:185][0m |           0.0063 |         169.9258 |        -324.6043 |
[32m[20221214 14:40:47 @agent_ppo2.py:185][0m |           0.0090 |         167.9990 |        -324.2987 |
[32m[20221214 14:40:48 @agent_ppo2.py:185][0m |           0.0103 |         172.4992 |        -324.0880 |
[32m[20221214 14:40:48 @agent_ppo2.py:185][0m |          -0.0024 |         162.3537 |        -324.3910 |
[32m[20221214 14:40:48 @agent_ppo2.py:185][0m |          -0.0031 |         160.5990 |        -324.1394 |
[32m[20221214 14:40:48 @agent_ppo2.py:185][0m |          -0.0025 |         160.6762 |        -324.0076 |
[32m[20221214 14:40:48 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:40:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.55
[32m[20221214 14:40:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.95
[32m[20221214 14:40:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.43
[32m[20221214 14:40:48 @agent_ppo2.py:143][0m Total time:      42.76 min
[32m[20221214 14:40:48 @agent_ppo2.py:145][0m 3938304 total steps have happened
[32m[20221214 14:40:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1923 --------------------------#
[32m[20221214 14:40:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:48 @agent_ppo2.py:185][0m |          -0.0029 |         200.0886 |        -330.1465 |
[32m[20221214 14:40:48 @agent_ppo2.py:185][0m |          -0.0026 |         137.6417 |        -329.7570 |
[32m[20221214 14:40:48 @agent_ppo2.py:185][0m |          -0.0011 |         130.7597 |        -329.1401 |
[32m[20221214 14:40:49 @agent_ppo2.py:185][0m |           0.0013 |         127.3145 |        -329.3413 |
[32m[20221214 14:40:49 @agent_ppo2.py:185][0m |          -0.0047 |         124.3681 |        -328.0361 |
[32m[20221214 14:40:49 @agent_ppo2.py:185][0m |          -0.0050 |         122.8348 |        -329.5852 |
[32m[20221214 14:40:49 @agent_ppo2.py:185][0m |          -0.0066 |         121.8775 |        -328.7792 |
[32m[20221214 14:40:49 @agent_ppo2.py:185][0m |           0.0017 |         120.2261 |        -328.6193 |
[32m[20221214 14:40:49 @agent_ppo2.py:185][0m |          -0.0027 |         120.0136 |        -328.5284 |
[32m[20221214 14:40:49 @agent_ppo2.py:185][0m |          -0.0092 |         125.5929 |        -328.5234 |
[32m[20221214 14:40:49 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:40:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.12
[32m[20221214 14:40:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.04
[32m[20221214 14:40:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.77
[32m[20221214 14:40:49 @agent_ppo2.py:143][0m Total time:      42.79 min
[32m[20221214 14:40:49 @agent_ppo2.py:145][0m 3940352 total steps have happened
[32m[20221214 14:40:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1924 --------------------------#
[32m[20221214 14:40:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:50 @agent_ppo2.py:185][0m |           0.0050 |         173.5983 |        -325.3566 |
[32m[20221214 14:40:50 @agent_ppo2.py:185][0m |           0.0040 |         157.0651 |        -325.4583 |
[32m[20221214 14:40:50 @agent_ppo2.py:185][0m |          -0.0026 |         150.5008 |        -325.5934 |
[32m[20221214 14:40:50 @agent_ppo2.py:185][0m |           0.0026 |         149.4322 |        -325.6580 |
[32m[20221214 14:40:50 @agent_ppo2.py:185][0m |          -0.0014 |         146.9403 |        -323.9506 |
[32m[20221214 14:40:50 @agent_ppo2.py:185][0m |          -0.0024 |         145.5467 |        -325.2464 |
[32m[20221214 14:40:50 @agent_ppo2.py:185][0m |          -0.0013 |         146.0259 |        -325.3749 |
[32m[20221214 14:40:50 @agent_ppo2.py:185][0m |           0.0010 |         144.7981 |        -325.9644 |
[32m[20221214 14:40:50 @agent_ppo2.py:185][0m |          -0.0003 |         146.0501 |        -324.7413 |
[32m[20221214 14:40:51 @agent_ppo2.py:185][0m |           0.0042 |         144.8275 |        -325.1581 |
[32m[20221214 14:40:51 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:40:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 828.32
[32m[20221214 14:40:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.33
[32m[20221214 14:40:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.11
[32m[20221214 14:40:51 @agent_ppo2.py:143][0m Total time:      42.81 min
[32m[20221214 14:40:51 @agent_ppo2.py:145][0m 3942400 total steps have happened
[32m[20221214 14:40:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1925 --------------------------#
[32m[20221214 14:40:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:51 @agent_ppo2.py:185][0m |          -0.0019 |         183.5449 |        -325.9981 |
[32m[20221214 14:40:51 @agent_ppo2.py:185][0m |           0.0070 |         172.7979 |        -326.2790 |
[32m[20221214 14:40:51 @agent_ppo2.py:185][0m |          -0.0048 |         166.9951 |        -327.3111 |
[32m[20221214 14:40:51 @agent_ppo2.py:185][0m |           0.0052 |         169.4649 |        -326.6283 |
[32m[20221214 14:40:51 @agent_ppo2.py:185][0m |          -0.0014 |         164.4209 |        -327.5729 |
[32m[20221214 14:40:52 @agent_ppo2.py:185][0m |           0.0043 |         166.5081 |        -326.9598 |
[32m[20221214 14:40:52 @agent_ppo2.py:185][0m |          -0.0023 |         162.7337 |        -328.1980 |
[32m[20221214 14:40:52 @agent_ppo2.py:185][0m |          -0.0019 |         162.9295 |        -328.2522 |
[32m[20221214 14:40:52 @agent_ppo2.py:185][0m |          -0.0038 |         161.5750 |        -327.2649 |
[32m[20221214 14:40:52 @agent_ppo2.py:185][0m |          -0.0050 |         161.7673 |        -328.4038 |
[32m[20221214 14:40:52 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:40:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.60
[32m[20221214 14:40:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.98
[32m[20221214 14:40:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.91
[32m[20221214 14:40:52 @agent_ppo2.py:143][0m Total time:      42.83 min
[32m[20221214 14:40:52 @agent_ppo2.py:145][0m 3944448 total steps have happened
[32m[20221214 14:40:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1926 --------------------------#
[32m[20221214 14:40:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:52 @agent_ppo2.py:185][0m |          -0.0028 |         225.7880 |        -332.2914 |
[32m[20221214 14:40:53 @agent_ppo2.py:185][0m |          -0.0054 |         193.9842 |        -332.4534 |
[32m[20221214 14:40:53 @agent_ppo2.py:185][0m |           0.0002 |         188.8529 |        -332.0052 |
[32m[20221214 14:40:53 @agent_ppo2.py:185][0m |          -0.0026 |         185.1708 |        -332.4809 |
[32m[20221214 14:40:53 @agent_ppo2.py:185][0m |           0.0000 |         183.3875 |        -332.3261 |
[32m[20221214 14:40:53 @agent_ppo2.py:185][0m |          -0.0048 |         181.4247 |        -331.7717 |
[32m[20221214 14:40:53 @agent_ppo2.py:185][0m |          -0.0027 |         180.6270 |        -332.7865 |
[32m[20221214 14:40:53 @agent_ppo2.py:185][0m |          -0.0054 |         179.7563 |        -332.2820 |
[32m[20221214 14:40:53 @agent_ppo2.py:185][0m |           0.0107 |         193.7886 |        -331.2533 |
[32m[20221214 14:40:53 @agent_ppo2.py:185][0m |          -0.0098 |         177.6990 |        -331.8392 |
[32m[20221214 14:40:53 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:40:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.09
[32m[20221214 14:40:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.37
[32m[20221214 14:40:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.61
[32m[20221214 14:40:53 @agent_ppo2.py:143][0m Total time:      42.86 min
[32m[20221214 14:40:53 @agent_ppo2.py:145][0m 3946496 total steps have happened
[32m[20221214 14:40:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1927 --------------------------#
[32m[20221214 14:40:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:54 @agent_ppo2.py:185][0m |          -0.0031 |         220.6445 |        -329.0162 |
[32m[20221214 14:40:54 @agent_ppo2.py:185][0m |           0.0027 |         205.4590 |        -328.3658 |
[32m[20221214 14:40:54 @agent_ppo2.py:185][0m |          -0.0055 |         198.5940 |        -328.1419 |
[32m[20221214 14:40:54 @agent_ppo2.py:185][0m |          -0.0044 |         196.1245 |        -328.0462 |
[32m[20221214 14:40:54 @agent_ppo2.py:185][0m |          -0.0056 |         192.9187 |        -327.5789 |
[32m[20221214 14:40:54 @agent_ppo2.py:185][0m |           0.0047 |         194.0515 |        -328.1491 |
[32m[20221214 14:40:54 @agent_ppo2.py:185][0m |          -0.0036 |         189.0322 |        -328.5848 |
[32m[20221214 14:40:55 @agent_ppo2.py:185][0m |          -0.0050 |         187.5431 |        -328.2118 |
[32m[20221214 14:40:55 @agent_ppo2.py:185][0m |          -0.0054 |         186.2560 |        -328.8769 |
[32m[20221214 14:40:55 @agent_ppo2.py:185][0m |          -0.0049 |         185.9724 |        -328.2701 |
[32m[20221214 14:40:55 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:40:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.77
[32m[20221214 14:40:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.96
[32m[20221214 14:40:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.38
[32m[20221214 14:40:55 @agent_ppo2.py:143][0m Total time:      42.88 min
[32m[20221214 14:40:55 @agent_ppo2.py:145][0m 3948544 total steps have happened
[32m[20221214 14:40:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1928 --------------------------#
[32m[20221214 14:40:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:40:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:55 @agent_ppo2.py:185][0m |          -0.0020 |         204.3957 |        -325.0938 |
[32m[20221214 14:40:55 @agent_ppo2.py:185][0m |           0.0058 |         179.5488 |        -325.0483 |
[32m[20221214 14:40:55 @agent_ppo2.py:185][0m |           0.0083 |         167.3901 |        -325.9144 |
[32m[20221214 14:40:55 @agent_ppo2.py:185][0m |          -0.0029 |         152.4375 |        -326.0727 |
[32m[20221214 14:40:56 @agent_ppo2.py:185][0m |          -0.0037 |         150.0654 |        -325.6878 |
[32m[20221214 14:40:56 @agent_ppo2.py:185][0m |          -0.0030 |         147.3035 |        -325.7976 |
[32m[20221214 14:40:56 @agent_ppo2.py:185][0m |          -0.0032 |         146.3956 |        -326.0589 |
[32m[20221214 14:40:56 @agent_ppo2.py:185][0m |           0.0000 |         145.2195 |        -326.2219 |
[32m[20221214 14:40:56 @agent_ppo2.py:185][0m |          -0.0019 |         143.2463 |        -326.1400 |
[32m[20221214 14:40:56 @agent_ppo2.py:185][0m |          -0.0000 |         143.1187 |        -326.1280 |
[32m[20221214 14:40:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:40:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 832.07
[32m[20221214 14:40:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.46
[32m[20221214 14:40:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.19
[32m[20221214 14:40:56 @agent_ppo2.py:143][0m Total time:      42.90 min
[32m[20221214 14:40:56 @agent_ppo2.py:145][0m 3950592 total steps have happened
[32m[20221214 14:40:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1929 --------------------------#
[32m[20221214 14:40:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:40:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:56 @agent_ppo2.py:185][0m |          -0.0021 |         182.8929 |        -328.0364 |
[32m[20221214 14:40:57 @agent_ppo2.py:185][0m |          -0.0013 |         158.7487 |        -327.9258 |
[32m[20221214 14:40:57 @agent_ppo2.py:185][0m |           0.0098 |         158.0167 |        -327.7717 |
[32m[20221214 14:40:57 @agent_ppo2.py:185][0m |           0.0002 |         150.4153 |        -327.9447 |
[32m[20221214 14:40:57 @agent_ppo2.py:185][0m |          -0.0042 |         149.5241 |        -328.1587 |
[32m[20221214 14:40:57 @agent_ppo2.py:185][0m |           0.0180 |         171.2027 |        -328.3958 |
[32m[20221214 14:40:57 @agent_ppo2.py:185][0m |          -0.0022 |         147.4468 |        -326.8427 |
[32m[20221214 14:40:57 @agent_ppo2.py:185][0m |          -0.0007 |         146.7612 |        -328.4328 |
[32m[20221214 14:40:57 @agent_ppo2.py:185][0m |          -0.0032 |         144.1656 |        -328.3056 |
[32m[20221214 14:40:57 @agent_ppo2.py:185][0m |          -0.0005 |         143.4552 |        -327.2827 |
[32m[20221214 14:40:57 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:40:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.93
[32m[20221214 14:40:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.48
[32m[20221214 14:40:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 755.34
[32m[20221214 14:40:57 @agent_ppo2.py:143][0m Total time:      42.92 min
[32m[20221214 14:40:57 @agent_ppo2.py:145][0m 3952640 total steps have happened
[32m[20221214 14:40:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1930 --------------------------#
[32m[20221214 14:40:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:40:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:58 @agent_ppo2.py:185][0m |          -0.0020 |         107.0607 |        -332.1599 |
[32m[20221214 14:40:58 @agent_ppo2.py:185][0m |          -0.0036 |          79.2436 |        -332.6364 |
[32m[20221214 14:40:58 @agent_ppo2.py:185][0m |          -0.0027 |          73.2971 |        -332.9129 |
[32m[20221214 14:40:58 @agent_ppo2.py:185][0m |          -0.0005 |          70.0012 |        -333.4018 |
[32m[20221214 14:40:58 @agent_ppo2.py:185][0m |          -0.0004 |          67.8782 |        -333.0790 |
[32m[20221214 14:40:58 @agent_ppo2.py:185][0m |           0.0032 |          66.2190 |        -333.1063 |
[32m[20221214 14:40:58 @agent_ppo2.py:185][0m |          -0.0043 |          65.1791 |        -333.3911 |
[32m[20221214 14:40:58 @agent_ppo2.py:185][0m |          -0.0008 |          64.4250 |        -332.9288 |
[32m[20221214 14:40:58 @agent_ppo2.py:185][0m |          -0.0045 |          63.8857 |        -333.0190 |
[32m[20221214 14:40:58 @agent_ppo2.py:185][0m |          -0.0006 |          64.3333 |        -333.1580 |
[32m[20221214 14:40:58 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:40:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 785.82
[32m[20221214 14:40:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.68
[32m[20221214 14:40:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.73
[32m[20221214 14:40:59 @agent_ppo2.py:143][0m Total time:      42.94 min
[32m[20221214 14:40:59 @agent_ppo2.py:145][0m 3954688 total steps have happened
[32m[20221214 14:40:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1931 --------------------------#
[32m[20221214 14:40:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:40:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:40:59 @agent_ppo2.py:185][0m |          -0.0008 |         120.3808 |        -330.1200 |
[32m[20221214 14:40:59 @agent_ppo2.py:185][0m |          -0.0013 |         105.2696 |        -329.4514 |
[32m[20221214 14:40:59 @agent_ppo2.py:185][0m |           0.0011 |          99.8250 |        -328.9677 |
[32m[20221214 14:40:59 @agent_ppo2.py:185][0m |          -0.0017 |          97.4842 |        -328.6539 |
[32m[20221214 14:40:59 @agent_ppo2.py:185][0m |           0.0017 |          94.7429 |        -328.4187 |
[32m[20221214 14:40:59 @agent_ppo2.py:185][0m |          -0.0025 |          93.2358 |        -328.1102 |
[32m[20221214 14:41:00 @agent_ppo2.py:185][0m |          -0.0015 |          91.8134 |        -328.5212 |
[32m[20221214 14:41:00 @agent_ppo2.py:185][0m |           0.0001 |          90.8750 |        -327.9277 |
[32m[20221214 14:41:00 @agent_ppo2.py:185][0m |          -0.0011 |          89.2320 |        -326.9706 |
[32m[20221214 14:41:00 @agent_ppo2.py:185][0m |           0.0016 |          89.3393 |        -327.5886 |
[32m[20221214 14:41:00 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:41:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.74
[32m[20221214 14:41:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.09
[32m[20221214 14:41:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.04
[32m[20221214 14:41:00 @agent_ppo2.py:143][0m Total time:      42.97 min
[32m[20221214 14:41:00 @agent_ppo2.py:145][0m 3956736 total steps have happened
[32m[20221214 14:41:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1932 --------------------------#
[32m[20221214 14:41:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:00 @agent_ppo2.py:185][0m |          -0.0046 |          53.5651 |        -326.4915 |
[32m[20221214 14:41:00 @agent_ppo2.py:185][0m |          -0.0039 |          34.0974 |        -326.2753 |
[32m[20221214 14:41:01 @agent_ppo2.py:185][0m |          -0.0072 |          29.0879 |        -326.5602 |
[32m[20221214 14:41:01 @agent_ppo2.py:185][0m |          -0.0062 |          26.6043 |        -326.5819 |
[32m[20221214 14:41:01 @agent_ppo2.py:185][0m |          -0.0068 |          24.7828 |        -326.0397 |
[32m[20221214 14:41:01 @agent_ppo2.py:185][0m |          -0.0000 |          23.4018 |        -326.0707 |
[32m[20221214 14:41:01 @agent_ppo2.py:185][0m |          -0.0066 |          22.2509 |        -326.1449 |
[32m[20221214 14:41:01 @agent_ppo2.py:185][0m |          -0.0033 |          21.5607 |        -326.1152 |
[32m[20221214 14:41:01 @agent_ppo2.py:185][0m |          -0.0050 |          20.6250 |        -325.9533 |
[32m[20221214 14:41:01 @agent_ppo2.py:185][0m |          -0.0055 |          19.9832 |        -325.2969 |
[32m[20221214 14:41:01 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 14:41:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 793.13
[32m[20221214 14:41:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.54
[32m[20221214 14:41:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 741.03
[32m[20221214 14:41:01 @agent_ppo2.py:143][0m Total time:      42.99 min
[32m[20221214 14:41:01 @agent_ppo2.py:145][0m 3958784 total steps have happened
[32m[20221214 14:41:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1933 --------------------------#
[32m[20221214 14:41:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:02 @agent_ppo2.py:185][0m |          -0.0004 |          73.1725 |        -329.9409 |
[32m[20221214 14:41:02 @agent_ppo2.py:185][0m |          -0.0028 |          47.2581 |        -329.2741 |
[32m[20221214 14:41:02 @agent_ppo2.py:185][0m |           0.0018 |          40.6243 |        -328.6759 |
[32m[20221214 14:41:02 @agent_ppo2.py:185][0m |          -0.0065 |          36.1011 |        -328.7264 |
[32m[20221214 14:41:02 @agent_ppo2.py:185][0m |           0.0038 |          33.4860 |        -328.8264 |
[32m[20221214 14:41:02 @agent_ppo2.py:185][0m |           0.0030 |          33.3784 |        -329.3158 |
[32m[20221214 14:41:02 @agent_ppo2.py:185][0m |          -0.0019 |          30.7127 |        -328.7280 |
[32m[20221214 14:41:02 @agent_ppo2.py:185][0m |           0.0101 |          31.1573 |        -328.2142 |
[32m[20221214 14:41:02 @agent_ppo2.py:185][0m |           0.0006 |          29.4272 |        -327.9994 |
[32m[20221214 14:41:03 @agent_ppo2.py:185][0m |           0.0012 |          30.1736 |        -328.1128 |
[32m[20221214 14:41:03 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:41:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.35
[32m[20221214 14:41:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.86
[32m[20221214 14:41:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.96
[32m[20221214 14:41:03 @agent_ppo2.py:143][0m Total time:      43.01 min
[32m[20221214 14:41:03 @agent_ppo2.py:145][0m 3960832 total steps have happened
[32m[20221214 14:41:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1934 --------------------------#
[32m[20221214 14:41:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:03 @agent_ppo2.py:185][0m |          -0.0018 |          75.3672 |        -324.8039 |
[32m[20221214 14:41:03 @agent_ppo2.py:185][0m |           0.0056 |          56.3860 |        -325.0637 |
[32m[20221214 14:41:03 @agent_ppo2.py:185][0m |          -0.0020 |          51.3276 |        -324.6667 |
[32m[20221214 14:41:03 @agent_ppo2.py:185][0m |          -0.0028 |          49.1804 |        -324.5839 |
[32m[20221214 14:41:03 @agent_ppo2.py:185][0m |          -0.0055 |          46.4481 |        -324.0763 |
[32m[20221214 14:41:04 @agent_ppo2.py:185][0m |          -0.0054 |          45.2914 |        -325.0050 |
[32m[20221214 14:41:04 @agent_ppo2.py:185][0m |          -0.0042 |          43.3022 |        -325.2228 |
[32m[20221214 14:41:04 @agent_ppo2.py:185][0m |          -0.0030 |          41.6318 |        -325.4594 |
[32m[20221214 14:41:04 @agent_ppo2.py:185][0m |          -0.0025 |          40.8417 |        -325.3452 |
[32m[20221214 14:41:04 @agent_ppo2.py:185][0m |          -0.0015 |          39.5873 |        -325.3199 |
[32m[20221214 14:41:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:41:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 754.38
[32m[20221214 14:41:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.04
[32m[20221214 14:41:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.16
[32m[20221214 14:41:04 @agent_ppo2.py:143][0m Total time:      43.03 min
[32m[20221214 14:41:04 @agent_ppo2.py:145][0m 3962880 total steps have happened
[32m[20221214 14:41:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1935 --------------------------#
[32m[20221214 14:41:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:04 @agent_ppo2.py:185][0m |           0.0089 |         122.2683 |        -321.6874 |
[32m[20221214 14:41:05 @agent_ppo2.py:185][0m |          -0.0040 |         102.6889 |        -322.5194 |
[32m[20221214 14:41:05 @agent_ppo2.py:185][0m |          -0.0049 |          95.8670 |        -321.4850 |
[32m[20221214 14:41:05 @agent_ppo2.py:185][0m |          -0.0044 |          91.0033 |        -322.0342 |
[32m[20221214 14:41:05 @agent_ppo2.py:185][0m |          -0.0042 |          90.2735 |        -322.9261 |
[32m[20221214 14:41:05 @agent_ppo2.py:185][0m |          -0.0012 |          87.6976 |        -322.8250 |
[32m[20221214 14:41:05 @agent_ppo2.py:185][0m |          -0.0058 |          86.2276 |        -323.2825 |
[32m[20221214 14:41:05 @agent_ppo2.py:185][0m |          -0.0038 |          84.6518 |        -323.3493 |
[32m[20221214 14:41:05 @agent_ppo2.py:185][0m |          -0.0010 |          83.0397 |        -323.4288 |
[32m[20221214 14:41:05 @agent_ppo2.py:185][0m |          -0.0061 |          81.6787 |        -323.5108 |
[32m[20221214 14:41:05 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:41:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 830.69
[32m[20221214 14:41:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.51
[32m[20221214 14:41:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.77
[32m[20221214 14:41:05 @agent_ppo2.py:143][0m Total time:      43.06 min
[32m[20221214 14:41:05 @agent_ppo2.py:145][0m 3964928 total steps have happened
[32m[20221214 14:41:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1936 --------------------------#
[32m[20221214 14:41:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:06 @agent_ppo2.py:185][0m |           0.0024 |         219.1726 |        -327.6719 |
[32m[20221214 14:41:06 @agent_ppo2.py:185][0m |           0.0114 |         205.0322 |        -326.4454 |
[32m[20221214 14:41:06 @agent_ppo2.py:185][0m |           0.0087 |         197.2316 |        -328.1201 |
[32m[20221214 14:41:06 @agent_ppo2.py:185][0m |           0.0054 |         190.1262 |        -328.8042 |
[32m[20221214 14:41:06 @agent_ppo2.py:185][0m |          -0.0004 |         184.1354 |        -328.9679 |
[32m[20221214 14:41:06 @agent_ppo2.py:185][0m |          -0.0028 |         181.2254 |        -328.8055 |
[32m[20221214 14:41:06 @agent_ppo2.py:185][0m |           0.0003 |         177.9441 |        -329.2308 |
[32m[20221214 14:41:06 @agent_ppo2.py:185][0m |           0.0013 |         176.8618 |        -328.8538 |
[32m[20221214 14:41:06 @agent_ppo2.py:185][0m |           0.0037 |         176.5630 |        -328.2763 |
[32m[20221214 14:41:07 @agent_ppo2.py:185][0m |           0.0025 |         177.7926 |        -329.8515 |
[32m[20221214 14:41:07 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:41:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.83
[32m[20221214 14:41:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.74
[32m[20221214 14:41:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.99
[32m[20221214 14:41:07 @agent_ppo2.py:143][0m Total time:      43.08 min
[32m[20221214 14:41:07 @agent_ppo2.py:145][0m 3966976 total steps have happened
[32m[20221214 14:41:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1937 --------------------------#
[32m[20221214 14:41:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:41:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:07 @agent_ppo2.py:185][0m |          -0.0023 |         241.9075 |        -326.6575 |
[32m[20221214 14:41:07 @agent_ppo2.py:185][0m |           0.0133 |         255.0501 |        -327.1079 |
[32m[20221214 14:41:07 @agent_ppo2.py:185][0m |          -0.0016 |         212.2163 |        -327.0336 |
[32m[20221214 14:41:07 @agent_ppo2.py:185][0m |          -0.0013 |         204.9213 |        -327.4759 |
[32m[20221214 14:41:07 @agent_ppo2.py:185][0m |          -0.0031 |         201.4015 |        -327.0338 |
[32m[20221214 14:41:08 @agent_ppo2.py:185][0m |          -0.0029 |         198.3920 |        -327.2267 |
[32m[20221214 14:41:08 @agent_ppo2.py:185][0m |          -0.0031 |         196.1217 |        -327.5913 |
[32m[20221214 14:41:08 @agent_ppo2.py:185][0m |          -0.0031 |         194.5728 |        -327.0013 |
[32m[20221214 14:41:08 @agent_ppo2.py:185][0m |          -0.0016 |         192.7326 |        -327.4268 |
[32m[20221214 14:41:08 @agent_ppo2.py:185][0m |          -0.0026 |         191.7326 |        -327.5678 |
[32m[20221214 14:41:08 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 14:41:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.02
[32m[20221214 14:41:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.84
[32m[20221214 14:41:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.44
[32m[20221214 14:41:08 @agent_ppo2.py:143][0m Total time:      43.10 min
[32m[20221214 14:41:08 @agent_ppo2.py:145][0m 3969024 total steps have happened
[32m[20221214 14:41:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1938 --------------------------#
[32m[20221214 14:41:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:08 @agent_ppo2.py:185][0m |          -0.0038 |         283.2764 |        -331.7570 |
[32m[20221214 14:41:09 @agent_ppo2.py:185][0m |          -0.0025 |         260.3372 |        -332.2394 |
[32m[20221214 14:41:09 @agent_ppo2.py:185][0m |          -0.0008 |         252.5199 |        -331.7977 |
[32m[20221214 14:41:09 @agent_ppo2.py:185][0m |           0.0003 |         249.1302 |        -332.4363 |
[32m[20221214 14:41:09 @agent_ppo2.py:185][0m |          -0.0021 |         246.8336 |        -332.4246 |
[32m[20221214 14:41:09 @agent_ppo2.py:185][0m |          -0.0015 |         246.0386 |        -332.9244 |
[32m[20221214 14:41:09 @agent_ppo2.py:185][0m |          -0.0021 |         244.3251 |        -332.8871 |
[32m[20221214 14:41:09 @agent_ppo2.py:185][0m |          -0.0024 |         242.8604 |        -332.9398 |
[32m[20221214 14:41:09 @agent_ppo2.py:185][0m |           0.0038 |         242.9002 |        -333.4434 |
[32m[20221214 14:41:09 @agent_ppo2.py:185][0m |          -0.0012 |         240.8611 |        -332.2920 |
[32m[20221214 14:41:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:41:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.91
[32m[20221214 14:41:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.33
[32m[20221214 14:41:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.85
[32m[20221214 14:41:10 @agent_ppo2.py:143][0m Total time:      43.12 min
[32m[20221214 14:41:10 @agent_ppo2.py:145][0m 3971072 total steps have happened
[32m[20221214 14:41:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1939 --------------------------#
[32m[20221214 14:41:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:10 @agent_ppo2.py:185][0m |           0.0027 |         308.2306 |        -331.9226 |
[32m[20221214 14:41:10 @agent_ppo2.py:185][0m |           0.0007 |         295.4927 |        -330.7817 |
[32m[20221214 14:41:10 @agent_ppo2.py:185][0m |           0.0005 |         291.7743 |        -332.4493 |
[32m[20221214 14:41:10 @agent_ppo2.py:185][0m |          -0.0013 |         289.8733 |        -333.2608 |
[32m[20221214 14:41:10 @agent_ppo2.py:185][0m |           0.0008 |         287.9439 |        -331.9780 |
[32m[20221214 14:41:10 @agent_ppo2.py:185][0m |          -0.0010 |         285.7888 |        -332.4945 |
[32m[20221214 14:41:10 @agent_ppo2.py:185][0m |          -0.0019 |         284.7827 |        -332.4524 |
[32m[20221214 14:41:11 @agent_ppo2.py:185][0m |          -0.0022 |         283.5080 |        -332.3406 |
[32m[20221214 14:41:11 @agent_ppo2.py:185][0m |          -0.0026 |         282.4174 |        -332.1566 |
[32m[20221214 14:41:11 @agent_ppo2.py:185][0m |          -0.0014 |         281.5466 |        -332.5915 |
[32m[20221214 14:41:11 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:41:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.34
[32m[20221214 14:41:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.83
[32m[20221214 14:41:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.44
[32m[20221214 14:41:11 @agent_ppo2.py:143][0m Total time:      43.15 min
[32m[20221214 14:41:11 @agent_ppo2.py:145][0m 3973120 total steps have happened
[32m[20221214 14:41:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1940 --------------------------#
[32m[20221214 14:41:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:41:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:11 @agent_ppo2.py:185][0m |          -0.0018 |         233.8448 |        -334.8814 |
[32m[20221214 14:41:11 @agent_ppo2.py:185][0m |          -0.0022 |         218.8528 |        -334.8626 |
[32m[20221214 14:41:11 @agent_ppo2.py:185][0m |          -0.0012 |         213.4911 |        -334.6996 |
[32m[20221214 14:41:11 @agent_ppo2.py:185][0m |          -0.0039 |         209.6712 |        -334.6169 |
[32m[20221214 14:41:12 @agent_ppo2.py:185][0m |          -0.0044 |         206.3124 |        -333.8041 |
[32m[20221214 14:41:12 @agent_ppo2.py:185][0m |          -0.0046 |         203.9304 |        -334.5739 |
[32m[20221214 14:41:12 @agent_ppo2.py:185][0m |          -0.0026 |         200.3442 |        -333.9506 |
[32m[20221214 14:41:12 @agent_ppo2.py:185][0m |          -0.0029 |         198.1057 |        -333.9760 |
[32m[20221214 14:41:12 @agent_ppo2.py:185][0m |           0.0080 |         221.2574 |        -333.7916 |
[32m[20221214 14:41:12 @agent_ppo2.py:185][0m |          -0.0040 |         194.4880 |        -333.7237 |
[32m[20221214 14:41:12 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 14:41:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.12
[32m[20221214 14:41:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.72
[32m[20221214 14:41:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.37
[32m[20221214 14:41:12 @agent_ppo2.py:143][0m Total time:      43.17 min
[32m[20221214 14:41:12 @agent_ppo2.py:145][0m 3975168 total steps have happened
[32m[20221214 14:41:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1941 --------------------------#
[32m[20221214 14:41:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:41:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:12 @agent_ppo2.py:185][0m |           0.0011 |         248.8325 |        -332.1713 |
[32m[20221214 14:41:13 @agent_ppo2.py:185][0m |          -0.0054 |         241.3900 |        -331.2074 |
[32m[20221214 14:41:13 @agent_ppo2.py:185][0m |          -0.0016 |         236.3338 |        -331.0154 |
[32m[20221214 14:41:13 @agent_ppo2.py:185][0m |           0.0143 |         265.5619 |        -332.3702 |
[32m[20221214 14:41:13 @agent_ppo2.py:185][0m |          -0.0023 |         229.2193 |        -332.5229 |
[32m[20221214 14:41:13 @agent_ppo2.py:185][0m |           0.0017 |         228.1031 |        -332.2375 |
[32m[20221214 14:41:13 @agent_ppo2.py:185][0m |          -0.0020 |         224.1972 |        -332.4513 |
[32m[20221214 14:41:13 @agent_ppo2.py:185][0m |           0.0061 |         226.4471 |        -332.8456 |
[32m[20221214 14:41:13 @agent_ppo2.py:185][0m |          -0.0002 |         221.3210 |        -333.0151 |
[32m[20221214 14:41:13 @agent_ppo2.py:185][0m |           0.0023 |         221.4777 |        -333.6097 |
[32m[20221214 14:41:13 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:41:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.79
[32m[20221214 14:41:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.23
[32m[20221214 14:41:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.55
[32m[20221214 14:41:13 @agent_ppo2.py:143][0m Total time:      43.19 min
[32m[20221214 14:41:13 @agent_ppo2.py:145][0m 3977216 total steps have happened
[32m[20221214 14:41:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1942 --------------------------#
[32m[20221214 14:41:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:41:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:14 @agent_ppo2.py:185][0m |          -0.0013 |         248.8825 |        -333.3315 |
[32m[20221214 14:41:14 @agent_ppo2.py:185][0m |          -0.0010 |         240.7356 |        -334.9050 |
[32m[20221214 14:41:14 @agent_ppo2.py:185][0m |          -0.0023 |         236.4457 |        -334.3252 |
[32m[20221214 14:41:14 @agent_ppo2.py:185][0m |          -0.0025 |         233.6106 |        -333.1153 |
[32m[20221214 14:41:14 @agent_ppo2.py:185][0m |           0.0025 |         232.1249 |        -333.8116 |
[32m[20221214 14:41:14 @agent_ppo2.py:185][0m |           0.0116 |         255.7669 |        -334.2318 |
[32m[20221214 14:41:14 @agent_ppo2.py:185][0m |           0.0021 |         230.8460 |        -334.2494 |
[32m[20221214 14:41:14 @agent_ppo2.py:185][0m |          -0.0000 |         227.4164 |        -334.2825 |
[32m[20221214 14:41:15 @agent_ppo2.py:185][0m |           0.0004 |         227.1335 |        -334.0302 |
[32m[20221214 14:41:15 @agent_ppo2.py:185][0m |          -0.0018 |         226.6005 |        -334.8858 |
[32m[20221214 14:41:15 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:41:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.62
[32m[20221214 14:41:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.66
[32m[20221214 14:41:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.49
[32m[20221214 14:41:15 @agent_ppo2.py:143][0m Total time:      43.21 min
[32m[20221214 14:41:15 @agent_ppo2.py:145][0m 3979264 total steps have happened
[32m[20221214 14:41:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1943 --------------------------#
[32m[20221214 14:41:15 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:41:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:15 @agent_ppo2.py:185][0m |           0.0010 |         246.5071 |        -334.4967 |
[32m[20221214 14:41:15 @agent_ppo2.py:185][0m |           0.0003 |         241.8379 |        -334.4751 |
[32m[20221214 14:41:15 @agent_ppo2.py:185][0m |          -0.0018 |         239.5951 |        -334.4201 |
[32m[20221214 14:41:16 @agent_ppo2.py:185][0m |           0.0028 |         240.1515 |        -335.2920 |
[32m[20221214 14:41:16 @agent_ppo2.py:185][0m |          -0.0008 |         237.0789 |        -334.4861 |
[32m[20221214 14:41:16 @agent_ppo2.py:185][0m |           0.0012 |         237.3934 |        -334.1106 |
[32m[20221214 14:41:16 @agent_ppo2.py:185][0m |           0.0004 |         234.6969 |        -334.9357 |
[32m[20221214 14:41:16 @agent_ppo2.py:185][0m |           0.0001 |         234.7092 |        -334.9331 |
[32m[20221214 14:41:16 @agent_ppo2.py:185][0m |           0.0079 |         243.8152 |        -334.7046 |
[32m[20221214 14:41:16 @agent_ppo2.py:185][0m |           0.0016 |         234.2445 |        -334.2825 |
[32m[20221214 14:41:16 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:41:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.33
[32m[20221214 14:41:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.14
[32m[20221214 14:41:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.94
[32m[20221214 14:41:16 @agent_ppo2.py:143][0m Total time:      43.24 min
[32m[20221214 14:41:16 @agent_ppo2.py:145][0m 3981312 total steps have happened
[32m[20221214 14:41:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1944 --------------------------#
[32m[20221214 14:41:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:17 @agent_ppo2.py:185][0m |          -0.0009 |         250.0206 |        -337.8900 |
[32m[20221214 14:41:17 @agent_ppo2.py:185][0m |          -0.0026 |         242.0034 |        -336.1454 |
[32m[20221214 14:41:17 @agent_ppo2.py:185][0m |          -0.0022 |         239.9284 |        -337.0019 |
[32m[20221214 14:41:17 @agent_ppo2.py:185][0m |          -0.0012 |         238.6370 |        -337.5530 |
[32m[20221214 14:41:17 @agent_ppo2.py:185][0m |           0.0112 |         262.0409 |        -336.9800 |
[32m[20221214 14:41:17 @agent_ppo2.py:185][0m |           0.0003 |         235.3978 |        -335.4753 |
[32m[20221214 14:41:17 @agent_ppo2.py:185][0m |           0.0062 |         246.6131 |        -336.1686 |
[32m[20221214 14:41:17 @agent_ppo2.py:185][0m |          -0.0021 |         233.9374 |        -336.2576 |
[32m[20221214 14:41:17 @agent_ppo2.py:185][0m |          -0.0033 |         233.3303 |        -336.0843 |
[32m[20221214 14:41:17 @agent_ppo2.py:185][0m |          -0.0021 |         232.5886 |        -335.1664 |
[32m[20221214 14:41:17 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:41:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.77
[32m[20221214 14:41:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.68
[32m[20221214 14:41:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.98
[32m[20221214 14:41:18 @agent_ppo2.py:143][0m Total time:      43.26 min
[32m[20221214 14:41:18 @agent_ppo2.py:145][0m 3983360 total steps have happened
[32m[20221214 14:41:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1945 --------------------------#
[32m[20221214 14:41:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:18 @agent_ppo2.py:185][0m |          -0.0025 |         234.7909 |        -329.3436 |
[32m[20221214 14:41:18 @agent_ppo2.py:185][0m |          -0.0025 |         208.8823 |        -329.9288 |
[32m[20221214 14:41:18 @agent_ppo2.py:185][0m |           0.0047 |         205.7138 |        -329.4789 |
[32m[20221214 14:41:18 @agent_ppo2.py:185][0m |          -0.0014 |         195.7366 |        -329.6753 |
[32m[20221214 14:41:18 @agent_ppo2.py:185][0m |          -0.0039 |         190.9808 |        -330.5622 |
[32m[20221214 14:41:18 @agent_ppo2.py:185][0m |          -0.0021 |         187.5673 |        -329.9022 |
[32m[20221214 14:41:19 @agent_ppo2.py:185][0m |          -0.0016 |         184.8110 |        -330.9979 |
[32m[20221214 14:41:19 @agent_ppo2.py:185][0m |          -0.0030 |         181.5565 |        -330.8926 |
[32m[20221214 14:41:19 @agent_ppo2.py:185][0m |           0.0038 |         182.8479 |        -330.5212 |
[32m[20221214 14:41:19 @agent_ppo2.py:185][0m |           0.0094 |         201.3939 |        -330.7552 |
[32m[20221214 14:41:19 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:41:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.66
[32m[20221214 14:41:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.74
[32m[20221214 14:41:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.79
[32m[20221214 14:41:19 @agent_ppo2.py:143][0m Total time:      43.28 min
[32m[20221214 14:41:19 @agent_ppo2.py:145][0m 3985408 total steps have happened
[32m[20221214 14:41:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1946 --------------------------#
[32m[20221214 14:41:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:19 @agent_ppo2.py:185][0m |          -0.0032 |         237.5449 |        -330.4853 |
[32m[20221214 14:41:19 @agent_ppo2.py:185][0m |          -0.0028 |         230.9465 |        -331.4353 |
[32m[20221214 14:41:20 @agent_ppo2.py:185][0m |           0.0083 |         237.6796 |        -331.3663 |
[32m[20221214 14:41:20 @agent_ppo2.py:185][0m |           0.0049 |         229.8568 |        -331.1079 |
[32m[20221214 14:41:20 @agent_ppo2.py:185][0m |          -0.0034 |         223.2280 |        -331.1309 |
[32m[20221214 14:41:20 @agent_ppo2.py:185][0m |          -0.0010 |         221.2023 |        -330.8778 |
[32m[20221214 14:41:20 @agent_ppo2.py:185][0m |           0.0005 |         220.3014 |        -331.9504 |
[32m[20221214 14:41:20 @agent_ppo2.py:185][0m |           0.0011 |         219.9981 |        -331.5356 |
[32m[20221214 14:41:20 @agent_ppo2.py:185][0m |          -0.0015 |         218.9177 |        -331.3072 |
[32m[20221214 14:41:20 @agent_ppo2.py:185][0m |          -0.0035 |         218.3262 |        -331.5003 |
[32m[20221214 14:41:20 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 14:41:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.30
[32m[20221214 14:41:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.45
[32m[20221214 14:41:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.10
[32m[20221214 14:41:20 @agent_ppo2.py:143][0m Total time:      43.31 min
[32m[20221214 14:41:20 @agent_ppo2.py:145][0m 3987456 total steps have happened
[32m[20221214 14:41:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1947 --------------------------#
[32m[20221214 14:41:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:41:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:21 @agent_ppo2.py:185][0m |           0.0032 |         250.0475 |        -332.0410 |
[32m[20221214 14:41:21 @agent_ppo2.py:185][0m |          -0.0033 |         233.2943 |        -331.5422 |
[32m[20221214 14:41:21 @agent_ppo2.py:185][0m |          -0.0003 |         228.3887 |        -333.0802 |
[32m[20221214 14:41:21 @agent_ppo2.py:185][0m |           0.0013 |         226.6214 |        -333.1465 |
[32m[20221214 14:41:21 @agent_ppo2.py:185][0m |          -0.0005 |         225.0250 |        -333.3676 |
[32m[20221214 14:41:21 @agent_ppo2.py:185][0m |          -0.0006 |         224.2476 |        -333.6881 |
[32m[20221214 14:41:21 @agent_ppo2.py:185][0m |           0.0005 |         222.8681 |        -333.9613 |
[32m[20221214 14:41:21 @agent_ppo2.py:185][0m |          -0.0020 |         222.7035 |        -334.1369 |
[32m[20221214 14:41:21 @agent_ppo2.py:185][0m |          -0.0046 |         221.8406 |        -334.0818 |
[32m[20221214 14:41:21 @agent_ppo2.py:185][0m |           0.0136 |         246.8382 |        -334.8222 |
[32m[20221214 14:41:21 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:41:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.65
[32m[20221214 14:41:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.81
[32m[20221214 14:41:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.85
[32m[20221214 14:41:22 @agent_ppo2.py:143][0m Total time:      43.33 min
[32m[20221214 14:41:22 @agent_ppo2.py:145][0m 3989504 total steps have happened
[32m[20221214 14:41:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1948 --------------------------#
[32m[20221214 14:41:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:41:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:22 @agent_ppo2.py:185][0m |          -0.0020 |         239.9871 |        -337.3914 |
[32m[20221214 14:41:22 @agent_ppo2.py:185][0m |           0.0015 |         217.2732 |        -337.8283 |
[32m[20221214 14:41:22 @agent_ppo2.py:185][0m |          -0.0029 |         211.3163 |        -337.8655 |
[32m[20221214 14:41:22 @agent_ppo2.py:185][0m |          -0.0024 |         208.9506 |        -337.8779 |
[32m[20221214 14:41:22 @agent_ppo2.py:185][0m |          -0.0010 |         206.9671 |        -338.4792 |
[32m[20221214 14:41:22 @agent_ppo2.py:185][0m |          -0.0015 |         205.1003 |        -338.2755 |
[32m[20221214 14:41:23 @agent_ppo2.py:185][0m |          -0.0032 |         204.3490 |        -338.4253 |
[32m[20221214 14:41:23 @agent_ppo2.py:185][0m |          -0.0029 |         203.5420 |        -338.7450 |
[32m[20221214 14:41:23 @agent_ppo2.py:185][0m |          -0.0020 |         202.6289 |        -338.4659 |
[32m[20221214 14:41:23 @agent_ppo2.py:185][0m |          -0.0033 |         202.1839 |        -338.9595 |
[32m[20221214 14:41:23 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:41:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.18
[32m[20221214 14:41:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.43
[32m[20221214 14:41:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 639.62
[32m[20221214 14:41:23 @agent_ppo2.py:143][0m Total time:      43.35 min
[32m[20221214 14:41:23 @agent_ppo2.py:145][0m 3991552 total steps have happened
[32m[20221214 14:41:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1949 --------------------------#
[32m[20221214 14:41:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:41:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:23 @agent_ppo2.py:185][0m |          -0.0020 |          53.0510 |        -335.2487 |
[32m[20221214 14:41:23 @agent_ppo2.py:185][0m |           0.0033 |          26.2714 |        -335.1793 |
[32m[20221214 14:41:24 @agent_ppo2.py:185][0m |           0.0028 |          22.0906 |        -335.7185 |
[32m[20221214 14:41:24 @agent_ppo2.py:185][0m |           0.0037 |          19.8109 |        -335.3402 |
[32m[20221214 14:41:24 @agent_ppo2.py:185][0m |           0.0017 |          18.6010 |        -336.3137 |
[32m[20221214 14:41:24 @agent_ppo2.py:185][0m |           0.0035 |          17.7837 |        -335.9473 |
[32m[20221214 14:41:24 @agent_ppo2.py:185][0m |          -0.0027 |          17.1863 |        -335.8655 |
[32m[20221214 14:41:24 @agent_ppo2.py:185][0m |          -0.0015 |          16.6291 |        -336.6424 |
[32m[20221214 14:41:24 @agent_ppo2.py:185][0m |           0.0020 |          16.4348 |        -335.4279 |
[32m[20221214 14:41:24 @agent_ppo2.py:185][0m |          -0.0003 |          16.0803 |        -336.9151 |
[32m[20221214 14:41:24 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:41:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 657.80
[32m[20221214 14:41:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 727.42
[32m[20221214 14:41:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 637.32
[32m[20221214 14:41:24 @agent_ppo2.py:143][0m Total time:      43.37 min
[32m[20221214 14:41:24 @agent_ppo2.py:145][0m 3993600 total steps have happened
[32m[20221214 14:41:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1950 --------------------------#
[32m[20221214 14:41:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:41:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:25 @agent_ppo2.py:185][0m |          -0.0067 |          43.4967 |        -345.9280 |
[32m[20221214 14:41:25 @agent_ppo2.py:185][0m |           0.0011 |          28.3713 |        -345.2491 |
[32m[20221214 14:41:25 @agent_ppo2.py:185][0m |          -0.0032 |          26.5095 |        -344.5527 |
[32m[20221214 14:41:25 @agent_ppo2.py:185][0m |           0.0017 |          24.5491 |        -344.5618 |
[32m[20221214 14:41:25 @agent_ppo2.py:185][0m |           0.0044 |          23.7984 |        -345.1173 |
[32m[20221214 14:41:25 @agent_ppo2.py:185][0m |          -0.0046 |          23.4702 |        -343.8759 |
[32m[20221214 14:41:25 @agent_ppo2.py:185][0m |          -0.0047 |          23.0236 |        -343.7884 |
[32m[20221214 14:41:25 @agent_ppo2.py:185][0m |          -0.0017 |          22.6768 |        -343.3612 |
[32m[20221214 14:41:26 @agent_ppo2.py:185][0m |           0.0030 |          22.5208 |        -343.3774 |
[32m[20221214 14:41:26 @agent_ppo2.py:185][0m |          -0.0004 |          22.3469 |        -343.5720 |
[32m[20221214 14:41:26 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:41:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 639.66
[32m[20221214 14:41:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 650.20
[32m[20221214 14:41:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 627.45
[32m[20221214 14:41:26 @agent_ppo2.py:143][0m Total time:      43.39 min
[32m[20221214 14:41:26 @agent_ppo2.py:145][0m 3995648 total steps have happened
[32m[20221214 14:41:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1951 --------------------------#
[32m[20221214 14:41:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:26 @agent_ppo2.py:185][0m |          -0.0010 |          59.1421 |        -341.3595 |
[32m[20221214 14:41:26 @agent_ppo2.py:185][0m |           0.0033 |          43.1380 |        -340.7181 |
[32m[20221214 14:41:26 @agent_ppo2.py:185][0m |          -0.0029 |          37.3276 |        -340.7679 |
[32m[20221214 14:41:26 @agent_ppo2.py:185][0m |          -0.0073 |          33.9923 |        -340.7059 |
[32m[20221214 14:41:26 @agent_ppo2.py:185][0m |           0.0006 |          32.3268 |        -340.1430 |
[32m[20221214 14:41:27 @agent_ppo2.py:185][0m |          -0.0094 |          30.2605 |        -339.9285 |
[32m[20221214 14:41:27 @agent_ppo2.py:185][0m |          -0.0056 |          29.1610 |        -340.3890 |
[32m[20221214 14:41:27 @agent_ppo2.py:185][0m |          -0.0043 |          28.5306 |        -339.6619 |
[32m[20221214 14:41:27 @agent_ppo2.py:185][0m |          -0.0075 |          27.6964 |        -339.8311 |
[32m[20221214 14:41:27 @agent_ppo2.py:185][0m |          -0.0016 |          27.1019 |        -339.0444 |
[32m[20221214 14:41:27 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:41:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 647.06
[32m[20221214 14:41:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 737.56
[32m[20221214 14:41:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 622.90
[32m[20221214 14:41:27 @agent_ppo2.py:143][0m Total time:      43.42 min
[32m[20221214 14:41:27 @agent_ppo2.py:145][0m 3997696 total steps have happened
[32m[20221214 14:41:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1952 --------------------------#
[32m[20221214 14:41:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:27 @agent_ppo2.py:185][0m |          -0.0033 |          95.8313 |        -344.7331 |
[32m[20221214 14:41:28 @agent_ppo2.py:185][0m |          -0.0058 |          57.8083 |        -343.6592 |
[32m[20221214 14:41:28 @agent_ppo2.py:185][0m |           0.0032 |          48.0303 |        -343.9168 |
[32m[20221214 14:41:28 @agent_ppo2.py:185][0m |          -0.0023 |          41.5687 |        -344.0461 |
[32m[20221214 14:41:28 @agent_ppo2.py:185][0m |           0.0073 |          41.7434 |        -343.1609 |
[32m[20221214 14:41:28 @agent_ppo2.py:185][0m |           0.0009 |          34.7538 |        -344.0140 |
[32m[20221214 14:41:28 @agent_ppo2.py:185][0m |          -0.0009 |          32.4368 |        -343.3303 |
[32m[20221214 14:41:28 @agent_ppo2.py:185][0m |          -0.0023 |          30.8128 |        -343.0413 |
[32m[20221214 14:41:28 @agent_ppo2.py:185][0m |          -0.0065 |          29.4118 |        -343.5478 |
[32m[20221214 14:41:28 @agent_ppo2.py:185][0m |           0.0010 |          28.5376 |        -342.9012 |
[32m[20221214 14:41:28 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:41:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 625.31
[32m[20221214 14:41:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 631.74
[32m[20221214 14:41:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 623.23
[32m[20221214 14:41:29 @agent_ppo2.py:143][0m Total time:      43.44 min
[32m[20221214 14:41:29 @agent_ppo2.py:145][0m 3999744 total steps have happened
[32m[20221214 14:41:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1953 --------------------------#
[32m[20221214 14:41:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:29 @agent_ppo2.py:185][0m |          -0.0004 |         131.4070 |        -335.9584 |
[32m[20221214 14:41:29 @agent_ppo2.py:185][0m |           0.0021 |         109.2345 |        -335.4180 |
[32m[20221214 14:41:29 @agent_ppo2.py:185][0m |           0.0023 |         105.0640 |        -335.7776 |
[32m[20221214 14:41:29 @agent_ppo2.py:185][0m |          -0.0040 |          96.4353 |        -335.8554 |
[32m[20221214 14:41:29 @agent_ppo2.py:185][0m |           0.0011 |          93.6569 |        -335.4748 |
[32m[20221214 14:41:29 @agent_ppo2.py:185][0m |          -0.0027 |          91.4541 |        -335.2296 |
[32m[20221214 14:41:29 @agent_ppo2.py:185][0m |          -0.0018 |          89.5305 |        -335.1951 |
[32m[20221214 14:41:29 @agent_ppo2.py:185][0m |          -0.0011 |          87.4703 |        -334.5123 |
[32m[20221214 14:41:30 @agent_ppo2.py:185][0m |          -0.0029 |          86.9907 |        -335.0979 |
[32m[20221214 14:41:30 @agent_ppo2.py:185][0m |          -0.0009 |          85.0598 |        -334.7630 |
[32m[20221214 14:41:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 14:41:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 711.97
[32m[20221214 14:41:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 844.64
[32m[20221214 14:41:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.57
[32m[20221214 14:41:30 @agent_ppo2.py:143][0m Total time:      43.46 min
[32m[20221214 14:41:30 @agent_ppo2.py:145][0m 4001792 total steps have happened
[32m[20221214 14:41:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1954 --------------------------#
[32m[20221214 14:41:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:30 @agent_ppo2.py:185][0m |          -0.0017 |          47.2268 |        -342.6469 |
[32m[20221214 14:41:30 @agent_ppo2.py:185][0m |          -0.0019 |          27.5682 |        -342.3448 |
[32m[20221214 14:41:30 @agent_ppo2.py:185][0m |           0.0008 |          22.1233 |        -342.9885 |
[32m[20221214 14:41:31 @agent_ppo2.py:185][0m |          -0.0058 |          18.9541 |        -342.2253 |
[32m[20221214 14:41:31 @agent_ppo2.py:185][0m |          -0.0027 |          16.7726 |        -343.1346 |
[32m[20221214 14:41:31 @agent_ppo2.py:185][0m |          -0.0040 |          15.2195 |        -343.1845 |
[32m[20221214 14:41:31 @agent_ppo2.py:185][0m |          -0.0007 |          14.0794 |        -343.3752 |
[32m[20221214 14:41:31 @agent_ppo2.py:185][0m |          -0.0068 |          13.2200 |        -343.7752 |
[32m[20221214 14:41:31 @agent_ppo2.py:185][0m |          -0.0014 |          12.5219 |        -343.2459 |
[32m[20221214 14:41:31 @agent_ppo2.py:185][0m |          -0.0013 |          12.0460 |        -343.5662 |
[32m[20221214 14:41:31 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 14:41:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 621.25
[32m[20221214 14:41:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 624.93
[32m[20221214 14:41:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.08
[32m[20221214 14:41:31 @agent_ppo2.py:143][0m Total time:      43.49 min
[32m[20221214 14:41:31 @agent_ppo2.py:145][0m 4003840 total steps have happened
[32m[20221214 14:41:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1955 --------------------------#
[32m[20221214 14:41:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:32 @agent_ppo2.py:185][0m |           0.0081 |         208.7720 |        -339.9542 |
[32m[20221214 14:41:32 @agent_ppo2.py:185][0m |          -0.0037 |         161.7745 |        -339.6363 |
[32m[20221214 14:41:32 @agent_ppo2.py:185][0m |          -0.0022 |         156.2519 |        -340.1116 |
[32m[20221214 14:41:32 @agent_ppo2.py:185][0m |          -0.0028 |         153.0352 |        -338.7741 |
[32m[20221214 14:41:32 @agent_ppo2.py:185][0m |           0.0050 |         162.1606 |        -338.8728 |
[32m[20221214 14:41:32 @agent_ppo2.py:185][0m |          -0.0027 |         149.7918 |        -338.7952 |
[32m[20221214 14:41:32 @agent_ppo2.py:185][0m |          -0.0044 |         149.3388 |        -338.0147 |
[32m[20221214 14:41:32 @agent_ppo2.py:185][0m |          -0.0014 |         147.7056 |        -338.4411 |
[32m[20221214 14:41:32 @agent_ppo2.py:185][0m |          -0.0052 |         146.8943 |        -337.5830 |
[32m[20221214 14:41:32 @agent_ppo2.py:185][0m |           0.0052 |         158.9813 |        -338.0628 |
[32m[20221214 14:41:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:41:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.84
[32m[20221214 14:41:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.28
[32m[20221214 14:41:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 620.20
[32m[20221214 14:41:33 @agent_ppo2.py:143][0m Total time:      43.51 min
[32m[20221214 14:41:33 @agent_ppo2.py:145][0m 4005888 total steps have happened
[32m[20221214 14:41:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1956 --------------------------#
[32m[20221214 14:41:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:33 @agent_ppo2.py:185][0m |          -0.0048 |          85.6350 |        -337.8988 |
[32m[20221214 14:41:33 @agent_ppo2.py:185][0m |          -0.0033 |          71.4405 |        -337.9317 |
[32m[20221214 14:41:33 @agent_ppo2.py:185][0m |          -0.0006 |          67.1768 |        -338.3099 |
[32m[20221214 14:41:33 @agent_ppo2.py:185][0m |           0.0013 |          64.3630 |        -338.1109 |
[32m[20221214 14:41:33 @agent_ppo2.py:185][0m |          -0.0025 |          62.7041 |        -338.4137 |
[32m[20221214 14:41:33 @agent_ppo2.py:185][0m |          -0.0025 |          61.5849 |        -338.3545 |
[32m[20221214 14:41:34 @agent_ppo2.py:185][0m |          -0.0035 |          60.3911 |        -337.9192 |
[32m[20221214 14:41:34 @agent_ppo2.py:185][0m |          -0.0023 |          59.4450 |        -338.2565 |
[32m[20221214 14:41:34 @agent_ppo2.py:185][0m |           0.0031 |          62.1842 |        -338.5638 |
[32m[20221214 14:41:34 @agent_ppo2.py:185][0m |          -0.0058 |          58.3436 |        -338.8581 |
[32m[20221214 14:41:34 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:41:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 668.30
[32m[20221214 14:41:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.10
[32m[20221214 14:41:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 621.79
[32m[20221214 14:41:34 @agent_ppo2.py:143][0m Total time:      43.53 min
[32m[20221214 14:41:34 @agent_ppo2.py:145][0m 4007936 total steps have happened
[32m[20221214 14:41:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1957 --------------------------#
[32m[20221214 14:41:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:34 @agent_ppo2.py:185][0m |           0.0066 |          84.0882 |        -341.7641 |
[32m[20221214 14:41:34 @agent_ppo2.py:185][0m |           0.0013 |          59.2474 |        -341.4840 |
[32m[20221214 14:41:35 @agent_ppo2.py:185][0m |          -0.0016 |          53.4789 |        -341.8624 |
[32m[20221214 14:41:35 @agent_ppo2.py:185][0m |          -0.0021 |          49.9723 |        -341.8939 |
[32m[20221214 14:41:35 @agent_ppo2.py:185][0m |          -0.0047 |          47.6449 |        -341.1106 |
[32m[20221214 14:41:35 @agent_ppo2.py:185][0m |          -0.0043 |          45.9649 |        -342.6552 |
[32m[20221214 14:41:35 @agent_ppo2.py:185][0m |          -0.0075 |          44.5677 |        -342.1949 |
[32m[20221214 14:41:35 @agent_ppo2.py:185][0m |          -0.0075 |          43.3199 |        -341.4424 |
[32m[20221214 14:41:35 @agent_ppo2.py:185][0m |          -0.0083 |          42.4608 |        -341.9562 |
[32m[20221214 14:41:35 @agent_ppo2.py:185][0m |          -0.0030 |          41.8050 |        -341.7642 |
[32m[20221214 14:41:35 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 14:41:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 665.15
[32m[20221214 14:41:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 727.10
[32m[20221214 14:41:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.44
[32m[20221214 14:41:35 @agent_ppo2.py:143][0m Total time:      43.56 min
[32m[20221214 14:41:35 @agent_ppo2.py:145][0m 4009984 total steps have happened
[32m[20221214 14:41:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1958 --------------------------#
[32m[20221214 14:41:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:36 @agent_ppo2.py:185][0m |          -0.0006 |         135.2701 |        -338.9627 |
[32m[20221214 14:41:36 @agent_ppo2.py:185][0m |          -0.0026 |         124.0365 |        -337.6259 |
[32m[20221214 14:41:36 @agent_ppo2.py:185][0m |          -0.0037 |         119.4089 |        -338.3298 |
[32m[20221214 14:41:36 @agent_ppo2.py:185][0m |          -0.0014 |         117.2938 |        -337.2602 |
[32m[20221214 14:41:36 @agent_ppo2.py:185][0m |          -0.0057 |         116.5730 |        -338.3290 |
[32m[20221214 14:41:36 @agent_ppo2.py:185][0m |          -0.0028 |         114.7532 |        -337.6778 |
[32m[20221214 14:41:36 @agent_ppo2.py:185][0m |           0.0004 |         115.0278 |        -337.6677 |
[32m[20221214 14:41:36 @agent_ppo2.py:185][0m |          -0.0065 |         113.3142 |        -337.5365 |
[32m[20221214 14:41:36 @agent_ppo2.py:185][0m |          -0.0019 |         112.8208 |        -337.8469 |
[32m[20221214 14:41:37 @agent_ppo2.py:185][0m |          -0.0065 |         112.2157 |        -337.7615 |
[32m[20221214 14:41:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:41:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.69
[32m[20221214 14:41:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 846.86
[32m[20221214 14:41:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.57
[32m[20221214 14:41:37 @agent_ppo2.py:143][0m Total time:      43.58 min
[32m[20221214 14:41:37 @agent_ppo2.py:145][0m 4012032 total steps have happened
[32m[20221214 14:41:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1959 --------------------------#
[32m[20221214 14:41:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:37 @agent_ppo2.py:185][0m |           0.0001 |         223.8134 |        -343.3340 |
[32m[20221214 14:41:37 @agent_ppo2.py:185][0m |           0.0242 |         256.7235 |        -343.0321 |
[32m[20221214 14:41:37 @agent_ppo2.py:185][0m |          -0.0002 |         211.1457 |        -343.4033 |
[32m[20221214 14:41:37 @agent_ppo2.py:185][0m |          -0.0012 |         205.2521 |        -342.7016 |
[32m[20221214 14:41:37 @agent_ppo2.py:185][0m |          -0.0028 |         204.4757 |        -342.4770 |
[32m[20221214 14:41:38 @agent_ppo2.py:185][0m |          -0.0017 |         203.1913 |        -342.5194 |
[32m[20221214 14:41:38 @agent_ppo2.py:185][0m |          -0.0029 |         202.3177 |        -342.6636 |
[32m[20221214 14:41:38 @agent_ppo2.py:185][0m |          -0.0025 |         199.8142 |        -342.1010 |
[32m[20221214 14:41:38 @agent_ppo2.py:185][0m |          -0.0037 |         198.5639 |        -342.0975 |
[32m[20221214 14:41:38 @agent_ppo2.py:185][0m |          -0.0022 |         197.0162 |        -342.5923 |
[32m[20221214 14:41:38 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:41:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 844.50
[32m[20221214 14:41:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.80
[32m[20221214 14:41:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.76
[32m[20221214 14:41:38 @agent_ppo2.py:143][0m Total time:      43.60 min
[32m[20221214 14:41:38 @agent_ppo2.py:145][0m 4014080 total steps have happened
[32m[20221214 14:41:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1960 --------------------------#
[32m[20221214 14:41:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:41:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:38 @agent_ppo2.py:185][0m |          -0.0023 |         228.0214 |        -339.0840 |
[32m[20221214 14:41:39 @agent_ppo2.py:185][0m |           0.0012 |         224.4685 |        -339.1360 |
[32m[20221214 14:41:39 @agent_ppo2.py:185][0m |           0.0022 |         223.5895 |        -339.0335 |
[32m[20221214 14:41:39 @agent_ppo2.py:185][0m |          -0.0020 |         220.3890 |        -338.6235 |
[32m[20221214 14:41:39 @agent_ppo2.py:185][0m |          -0.0005 |         220.4108 |        -338.8263 |
[32m[20221214 14:41:39 @agent_ppo2.py:185][0m |           0.0021 |         219.4823 |        -337.5902 |
[32m[20221214 14:41:39 @agent_ppo2.py:185][0m |          -0.0018 |         218.6044 |        -338.5757 |
[32m[20221214 14:41:39 @agent_ppo2.py:185][0m |          -0.0007 |         218.5546 |        -338.6291 |
[32m[20221214 14:41:39 @agent_ppo2.py:185][0m |          -0.0023 |         217.6733 |        -338.6794 |
[32m[20221214 14:41:39 @agent_ppo2.py:185][0m |           0.0005 |         217.3538 |        -337.6799 |
[32m[20221214 14:41:39 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:41:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 843.37
[32m[20221214 14:41:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.93
[32m[20221214 14:41:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.63
[32m[20221214 14:41:39 @agent_ppo2.py:143][0m Total time:      43.62 min
[32m[20221214 14:41:39 @agent_ppo2.py:145][0m 4016128 total steps have happened
[32m[20221214 14:41:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1961 --------------------------#
[32m[20221214 14:41:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:40 @agent_ppo2.py:185][0m |          -0.0006 |         223.7534 |        -339.8998 |
[32m[20221214 14:41:40 @agent_ppo2.py:185][0m |           0.0007 |         210.7040 |        -339.8787 |
[32m[20221214 14:41:40 @agent_ppo2.py:185][0m |          -0.0025 |         204.7059 |        -339.5124 |
[32m[20221214 14:41:40 @agent_ppo2.py:185][0m |          -0.0023 |         199.2497 |        -339.5078 |
[32m[20221214 14:41:40 @agent_ppo2.py:185][0m |          -0.0008 |         196.4066 |        -339.5668 |
[32m[20221214 14:41:40 @agent_ppo2.py:185][0m |           0.0007 |         193.7169 |        -340.4289 |
[32m[20221214 14:41:40 @agent_ppo2.py:185][0m |           0.0006 |         191.5735 |        -339.5721 |
[32m[20221214 14:41:40 @agent_ppo2.py:185][0m |           0.0015 |         188.0568 |        -340.0937 |
[32m[20221214 14:41:41 @agent_ppo2.py:185][0m |          -0.0000 |         186.8290 |        -339.2917 |
[32m[20221214 14:41:41 @agent_ppo2.py:185][0m |           0.0040 |         189.8119 |        -339.0069 |
[32m[20221214 14:41:41 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:41:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 844.69
[32m[20221214 14:41:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 845.19
[32m[20221214 14:41:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.72
[32m[20221214 14:41:41 @agent_ppo2.py:143][0m Total time:      43.65 min
[32m[20221214 14:41:41 @agent_ppo2.py:145][0m 4018176 total steps have happened
[32m[20221214 14:41:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1962 --------------------------#
[32m[20221214 14:41:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:41 @agent_ppo2.py:185][0m |          -0.0029 |         208.1366 |        -339.3527 |
[32m[20221214 14:41:41 @agent_ppo2.py:185][0m |          -0.0050 |         203.5723 |        -339.2337 |
[32m[20221214 14:41:41 @agent_ppo2.py:185][0m |          -0.0071 |         201.8666 |        -339.3687 |
[32m[20221214 14:41:41 @agent_ppo2.py:185][0m |           0.0078 |         224.2910 |        -339.3362 |
[32m[20221214 14:41:42 @agent_ppo2.py:185][0m |          -0.0002 |         201.6517 |        -339.3631 |
[32m[20221214 14:41:42 @agent_ppo2.py:185][0m |          -0.0050 |         199.1984 |        -339.5723 |
[32m[20221214 14:41:42 @agent_ppo2.py:185][0m |           0.0009 |         200.4144 |        -338.7785 |
[32m[20221214 14:41:42 @agent_ppo2.py:185][0m |          -0.0045 |         197.4252 |        -338.4558 |
[32m[20221214 14:41:42 @agent_ppo2.py:185][0m |          -0.0058 |         196.8795 |        -339.3921 |
[32m[20221214 14:41:42 @agent_ppo2.py:185][0m |          -0.0012 |         201.1891 |        -339.5947 |
[32m[20221214 14:41:42 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 14:41:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.40
[32m[20221214 14:41:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.37
[32m[20221214 14:41:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.74
[32m[20221214 14:41:42 @agent_ppo2.py:143][0m Total time:      43.67 min
[32m[20221214 14:41:42 @agent_ppo2.py:145][0m 4020224 total steps have happened
[32m[20221214 14:41:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1963 --------------------------#
[32m[20221214 14:41:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:43 @agent_ppo2.py:185][0m |          -0.0017 |         231.5969 |        -346.1055 |
[32m[20221214 14:41:43 @agent_ppo2.py:185][0m |           0.0078 |         242.0873 |        -345.2782 |
[32m[20221214 14:41:43 @agent_ppo2.py:185][0m |           0.0021 |         218.8484 |        -345.8004 |
[32m[20221214 14:41:43 @agent_ppo2.py:185][0m |          -0.0005 |         214.6396 |        -346.3197 |
[32m[20221214 14:41:43 @agent_ppo2.py:185][0m |          -0.0021 |         212.0417 |        -346.1907 |
[32m[20221214 14:41:43 @agent_ppo2.py:185][0m |          -0.0032 |         210.6233 |        -345.9906 |
[32m[20221214 14:41:43 @agent_ppo2.py:185][0m |          -0.0014 |         209.2657 |        -345.1352 |
[32m[20221214 14:41:43 @agent_ppo2.py:185][0m |           0.0001 |         207.8563 |        -345.2659 |
[32m[20221214 14:41:43 @agent_ppo2.py:185][0m |          -0.0025 |         205.6598 |        -345.8759 |
[32m[20221214 14:41:43 @agent_ppo2.py:185][0m |          -0.0032 |         204.0182 |        -346.2393 |
[32m[20221214 14:41:43 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:41:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 842.13
[32m[20221214 14:41:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.74
[32m[20221214 14:41:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.78
[32m[20221214 14:41:44 @agent_ppo2.py:143][0m Total time:      43.69 min
[32m[20221214 14:41:44 @agent_ppo2.py:145][0m 4022272 total steps have happened
[32m[20221214 14:41:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1964 --------------------------#
[32m[20221214 14:41:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:44 @agent_ppo2.py:185][0m |           0.0024 |         223.1154 |        -342.7689 |
[32m[20221214 14:41:44 @agent_ppo2.py:185][0m |          -0.0008 |         211.9651 |        -342.5872 |
[32m[20221214 14:41:44 @agent_ppo2.py:185][0m |          -0.0003 |         209.6612 |        -341.3077 |
[32m[20221214 14:41:44 @agent_ppo2.py:185][0m |           0.0014 |         209.1207 |        -341.6207 |
[32m[20221214 14:41:44 @agent_ppo2.py:185][0m |          -0.0016 |         208.4389 |        -341.7859 |
[32m[20221214 14:41:44 @agent_ppo2.py:185][0m |          -0.0011 |         208.0591 |        -341.4393 |
[32m[20221214 14:41:44 @agent_ppo2.py:185][0m |           0.0004 |         208.0134 |        -339.4618 |
[32m[20221214 14:41:45 @agent_ppo2.py:185][0m |          -0.0021 |         207.4610 |        -341.1762 |
[32m[20221214 14:41:45 @agent_ppo2.py:185][0m |          -0.0006 |         207.1678 |        -341.0586 |
[32m[20221214 14:41:45 @agent_ppo2.py:185][0m |          -0.0004 |         207.3635 |        -340.8169 |
[32m[20221214 14:41:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:41:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 841.24
[32m[20221214 14:41:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.41
[32m[20221214 14:41:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.69
[32m[20221214 14:41:45 @agent_ppo2.py:143][0m Total time:      43.71 min
[32m[20221214 14:41:45 @agent_ppo2.py:145][0m 4024320 total steps have happened
[32m[20221214 14:41:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1965 --------------------------#
[32m[20221214 14:41:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:45 @agent_ppo2.py:185][0m |          -0.0006 |         212.1906 |        -338.0758 |
[32m[20221214 14:41:45 @agent_ppo2.py:185][0m |           0.0077 |         215.6458 |        -338.6762 |
[32m[20221214 14:41:45 @agent_ppo2.py:185][0m |          -0.0031 |         202.9262 |        -339.0372 |
[32m[20221214 14:41:46 @agent_ppo2.py:185][0m |          -0.0010 |         202.5223 |        -338.7614 |
[32m[20221214 14:41:46 @agent_ppo2.py:185][0m |          -0.0047 |         201.4254 |        -339.0635 |
[32m[20221214 14:41:46 @agent_ppo2.py:185][0m |          -0.0017 |         200.7558 |        -339.1501 |
[32m[20221214 14:41:46 @agent_ppo2.py:185][0m |           0.0048 |         206.8418 |        -339.1263 |
[32m[20221214 14:41:46 @agent_ppo2.py:185][0m |           0.0007 |         199.3275 |        -338.5790 |
[32m[20221214 14:41:46 @agent_ppo2.py:185][0m |          -0.0027 |         199.1176 |        -339.9170 |
[32m[20221214 14:41:46 @agent_ppo2.py:185][0m |          -0.0028 |         198.2948 |        -339.8418 |
[32m[20221214 14:41:46 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:41:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 840.61
[32m[20221214 14:41:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.77
[32m[20221214 14:41:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.65
[32m[20221214 14:41:46 @agent_ppo2.py:143][0m Total time:      43.74 min
[32m[20221214 14:41:46 @agent_ppo2.py:145][0m 4026368 total steps have happened
[32m[20221214 14:41:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1966 --------------------------#
[32m[20221214 14:41:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:47 @agent_ppo2.py:185][0m |           0.0055 |         225.5040 |        -342.9512 |
[32m[20221214 14:41:47 @agent_ppo2.py:185][0m |          -0.0018 |         218.6982 |        -343.0967 |
[32m[20221214 14:41:47 @agent_ppo2.py:185][0m |          -0.0018 |         217.6948 |        -343.5978 |
[32m[20221214 14:41:47 @agent_ppo2.py:185][0m |          -0.0019 |         217.7205 |        -343.6930 |
[32m[20221214 14:41:47 @agent_ppo2.py:185][0m |           0.0046 |         223.5345 |        -343.9576 |
[32m[20221214 14:41:47 @agent_ppo2.py:185][0m |          -0.0021 |         216.6924 |        -344.1623 |
[32m[20221214 14:41:47 @agent_ppo2.py:185][0m |          -0.0031 |         216.2990 |        -344.1271 |
[32m[20221214 14:41:47 @agent_ppo2.py:185][0m |          -0.0002 |         216.1011 |        -344.4205 |
[32m[20221214 14:41:47 @agent_ppo2.py:185][0m |           0.0002 |         216.1627 |        -344.1520 |
[32m[20221214 14:41:47 @agent_ppo2.py:185][0m |          -0.0021 |         215.2681 |        -344.5132 |
[32m[20221214 14:41:47 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:41:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.79
[32m[20221214 14:41:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.65
[32m[20221214 14:41:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.65
[32m[20221214 14:41:48 @agent_ppo2.py:143][0m Total time:      43.76 min
[32m[20221214 14:41:48 @agent_ppo2.py:145][0m 4028416 total steps have happened
[32m[20221214 14:41:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1967 --------------------------#
[32m[20221214 14:41:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:48 @agent_ppo2.py:185][0m |           0.0104 |         224.4535 |        -339.6684 |
[32m[20221214 14:41:48 @agent_ppo2.py:185][0m |           0.0037 |         193.1548 |        -340.0077 |
[32m[20221214 14:41:48 @agent_ppo2.py:185][0m |          -0.0006 |         190.4456 |        -340.6640 |
[32m[20221214 14:41:48 @agent_ppo2.py:185][0m |          -0.0016 |         188.7621 |        -340.3000 |
[32m[20221214 14:41:48 @agent_ppo2.py:185][0m |           0.0030 |         189.7510 |        -340.8420 |
[32m[20221214 14:41:48 @agent_ppo2.py:185][0m |           0.0028 |         186.9236 |        -340.8425 |
[32m[20221214 14:41:49 @agent_ppo2.py:185][0m |           0.0001 |         186.4642 |        -340.1225 |
[32m[20221214 14:41:49 @agent_ppo2.py:185][0m |           0.0133 |         209.9159 |        -341.0000 |
[32m[20221214 14:41:49 @agent_ppo2.py:185][0m |           0.0156 |         203.9177 |        -340.6133 |
[32m[20221214 14:41:49 @agent_ppo2.py:185][0m |          -0.0018 |         184.4632 |        -340.7105 |
[32m[20221214 14:41:49 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:41:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 836.90
[32m[20221214 14:41:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.42
[32m[20221214 14:41:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.06
[32m[20221214 14:41:49 @agent_ppo2.py:143][0m Total time:      43.78 min
[32m[20221214 14:41:49 @agent_ppo2.py:145][0m 4030464 total steps have happened
[32m[20221214 14:41:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1968 --------------------------#
[32m[20221214 14:41:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:49 @agent_ppo2.py:185][0m |          -0.0010 |         223.2018 |        -346.0760 |
[32m[20221214 14:41:49 @agent_ppo2.py:185][0m |           0.0013 |         212.0749 |        -346.1225 |
[32m[20221214 14:41:49 @agent_ppo2.py:185][0m |          -0.0002 |         208.4278 |        -346.0785 |
[32m[20221214 14:41:50 @agent_ppo2.py:185][0m |          -0.0004 |         206.7229 |        -345.8306 |
[32m[20221214 14:41:50 @agent_ppo2.py:185][0m |           0.0013 |         207.3061 |        -345.3493 |
[32m[20221214 14:41:50 @agent_ppo2.py:185][0m |           0.0012 |         205.6677 |        -346.3480 |
[32m[20221214 14:41:50 @agent_ppo2.py:185][0m |           0.0091 |         219.5262 |        -345.6292 |
[32m[20221214 14:41:50 @agent_ppo2.py:185][0m |          -0.0037 |         205.3701 |        -345.2283 |
[32m[20221214 14:41:50 @agent_ppo2.py:185][0m |          -0.0019 |         204.5243 |        -344.6125 |
[32m[20221214 14:41:50 @agent_ppo2.py:185][0m |          -0.0037 |         204.1168 |        -345.3199 |
[32m[20221214 14:41:50 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 14:41:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 835.52
[32m[20221214 14:41:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.48
[32m[20221214 14:41:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.98
[32m[20221214 14:41:50 @agent_ppo2.py:143][0m Total time:      43.80 min
[32m[20221214 14:41:50 @agent_ppo2.py:145][0m 4032512 total steps have happened
[32m[20221214 14:41:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1969 --------------------------#
[32m[20221214 14:41:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:51 @agent_ppo2.py:185][0m |          -0.0002 |         198.7046 |        -342.6678 |
[32m[20221214 14:41:51 @agent_ppo2.py:185][0m |          -0.0036 |         194.7717 |        -343.2650 |
[32m[20221214 14:41:51 @agent_ppo2.py:185][0m |          -0.0022 |         194.2969 |        -342.4684 |
[32m[20221214 14:41:51 @agent_ppo2.py:185][0m |          -0.0012 |         193.7095 |        -343.4509 |
[32m[20221214 14:41:51 @agent_ppo2.py:185][0m |          -0.0034 |         193.1804 |        -343.4973 |
[32m[20221214 14:41:51 @agent_ppo2.py:185][0m |          -0.0017 |         192.9400 |        -343.3104 |
[32m[20221214 14:41:51 @agent_ppo2.py:185][0m |           0.0013 |         194.4552 |        -342.9836 |
[32m[20221214 14:41:51 @agent_ppo2.py:185][0m |          -0.0022 |         192.6981 |        -343.5618 |
[32m[20221214 14:41:51 @agent_ppo2.py:185][0m |          -0.0046 |         192.8615 |        -343.2329 |
[32m[20221214 14:41:51 @agent_ppo2.py:185][0m |          -0.0055 |         193.0592 |        -344.7438 |
[32m[20221214 14:41:51 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:41:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 839.05
[32m[20221214 14:41:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.78
[32m[20221214 14:41:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.82
[32m[20221214 14:41:52 @agent_ppo2.py:143][0m Total time:      43.83 min
[32m[20221214 14:41:52 @agent_ppo2.py:145][0m 4034560 total steps have happened
[32m[20221214 14:41:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1970 --------------------------#
[32m[20221214 14:41:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 14:41:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:52 @agent_ppo2.py:185][0m |          -0.0016 |         202.0829 |        -346.7566 |
[32m[20221214 14:41:52 @agent_ppo2.py:185][0m |          -0.0021 |         193.7080 |        -345.9342 |
[32m[20221214 14:41:52 @agent_ppo2.py:185][0m |          -0.0033 |         191.9689 |        -346.0427 |
[32m[20221214 14:41:52 @agent_ppo2.py:185][0m |          -0.0038 |         191.3648 |        -346.1937 |
[32m[20221214 14:41:52 @agent_ppo2.py:185][0m |           0.0001 |         196.7894 |        -346.1635 |
[32m[20221214 14:41:52 @agent_ppo2.py:185][0m |          -0.0033 |         189.6826 |        -346.0034 |
[32m[20221214 14:41:53 @agent_ppo2.py:185][0m |           0.0059 |         201.5004 |        -345.2671 |
[32m[20221214 14:41:53 @agent_ppo2.py:185][0m |           0.0028 |         193.7965 |        -345.5991 |
[32m[20221214 14:41:53 @agent_ppo2.py:185][0m |          -0.0023 |         188.6572 |        -344.8466 |
[32m[20221214 14:41:53 @agent_ppo2.py:185][0m |          -0.0026 |         187.8683 |        -345.4713 |
[32m[20221214 14:41:53 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:41:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 838.89
[32m[20221214 14:41:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 840.16
[32m[20221214 14:41:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.28
[32m[20221214 14:41:53 @agent_ppo2.py:143][0m Total time:      43.85 min
[32m[20221214 14:41:53 @agent_ppo2.py:145][0m 4036608 total steps have happened
[32m[20221214 14:41:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1971 --------------------------#
[32m[20221214 14:41:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:53 @agent_ppo2.py:185][0m |          -0.0007 |         219.3349 |        -351.1015 |
[32m[20221214 14:41:53 @agent_ppo2.py:185][0m |          -0.0008 |         211.9303 |        -350.7292 |
[32m[20221214 14:41:54 @agent_ppo2.py:185][0m |          -0.0002 |         209.7599 |        -350.7511 |
[32m[20221214 14:41:54 @agent_ppo2.py:185][0m |           0.0040 |         217.4055 |        -351.1764 |
[32m[20221214 14:41:54 @agent_ppo2.py:185][0m |          -0.0018 |         203.7539 |        -350.6249 |
[32m[20221214 14:41:54 @agent_ppo2.py:185][0m |          -0.0009 |         202.2225 |        -350.8443 |
[32m[20221214 14:41:54 @agent_ppo2.py:185][0m |           0.0044 |         207.4936 |        -352.2175 |
[32m[20221214 14:41:54 @agent_ppo2.py:185][0m |           0.0024 |         201.6347 |        -350.7746 |
[32m[20221214 14:41:54 @agent_ppo2.py:185][0m |          -0.0015 |         198.4499 |        -351.6377 |
[32m[20221214 14:41:54 @agent_ppo2.py:185][0m |          -0.0007 |         197.9021 |        -351.7029 |
[32m[20221214 14:41:54 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:41:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 837.00
[32m[20221214 14:41:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.61
[32m[20221214 14:41:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.85
[32m[20221214 14:41:54 @agent_ppo2.py:143][0m Total time:      43.87 min
[32m[20221214 14:41:54 @agent_ppo2.py:145][0m 4038656 total steps have happened
[32m[20221214 14:41:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1972 --------------------------#
[32m[20221214 14:41:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:55 @agent_ppo2.py:185][0m |          -0.0032 |         200.8755 |        -346.5203 |
[32m[20221214 14:41:55 @agent_ppo2.py:185][0m |          -0.0016 |         195.1105 |        -345.8316 |
[32m[20221214 14:41:55 @agent_ppo2.py:185][0m |          -0.0036 |         193.9666 |        -346.2286 |
[32m[20221214 14:41:55 @agent_ppo2.py:185][0m |          -0.0003 |         193.0659 |        -345.2601 |
[32m[20221214 14:41:55 @agent_ppo2.py:185][0m |          -0.0028 |         192.3495 |        -345.4171 |
[32m[20221214 14:41:55 @agent_ppo2.py:185][0m |           0.0118 |         218.6563 |        -345.2442 |
[32m[20221214 14:41:55 @agent_ppo2.py:185][0m |          -0.0041 |         191.6150 |        -345.3879 |
[32m[20221214 14:41:55 @agent_ppo2.py:185][0m |          -0.0014 |         190.8303 |        -344.9916 |
[32m[20221214 14:41:55 @agent_ppo2.py:185][0m |          -0.0013 |         190.5493 |        -345.1385 |
[32m[20221214 14:41:56 @agent_ppo2.py:185][0m |           0.0070 |         204.7354 |        -344.7347 |
[32m[20221214 14:41:56 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:41:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.91
[32m[20221214 14:41:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.04
[32m[20221214 14:41:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.56
[32m[20221214 14:41:56 @agent_ppo2.py:143][0m Total time:      43.89 min
[32m[20221214 14:41:56 @agent_ppo2.py:145][0m 4040704 total steps have happened
[32m[20221214 14:41:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1973 --------------------------#
[32m[20221214 14:41:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:41:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:56 @agent_ppo2.py:185][0m |           0.0101 |         215.3889 |        -345.0456 |
[32m[20221214 14:41:56 @agent_ppo2.py:185][0m |          -0.0008 |         187.4020 |        -345.1321 |
[32m[20221214 14:41:56 @agent_ppo2.py:185][0m |          -0.0033 |         186.2471 |        -344.0755 |
[32m[20221214 14:41:56 @agent_ppo2.py:185][0m |          -0.0013 |         185.4805 |        -345.1272 |
[32m[20221214 14:41:56 @agent_ppo2.py:185][0m |           0.0001 |         184.9088 |        -344.7920 |
[32m[20221214 14:41:57 @agent_ppo2.py:185][0m |           0.0041 |         185.1378 |        -344.8877 |
[32m[20221214 14:41:57 @agent_ppo2.py:185][0m |          -0.0025 |         183.9237 |        -345.6036 |
[32m[20221214 14:41:57 @agent_ppo2.py:185][0m |           0.0029 |         184.4880 |        -344.9051 |
[32m[20221214 14:41:57 @agent_ppo2.py:185][0m |           0.0000 |         183.2061 |        -346.0017 |
[32m[20221214 14:41:57 @agent_ppo2.py:185][0m |           0.0022 |         184.3245 |        -345.4390 |
[32m[20221214 14:41:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:41:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 834.07
[32m[20221214 14:41:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.83
[32m[20221214 14:41:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.13
[32m[20221214 14:41:57 @agent_ppo2.py:143][0m Total time:      43.92 min
[32m[20221214 14:41:57 @agent_ppo2.py:145][0m 4042752 total steps have happened
[32m[20221214 14:41:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1974 --------------------------#
[32m[20221214 14:41:57 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:41:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:57 @agent_ppo2.py:185][0m |          -0.0041 |         195.9710 |        -349.9067 |
[32m[20221214 14:41:58 @agent_ppo2.py:185][0m |          -0.0022 |         194.0426 |        -349.0415 |
[32m[20221214 14:41:58 @agent_ppo2.py:185][0m |          -0.0020 |         192.1490 |        -349.6824 |
[32m[20221214 14:41:58 @agent_ppo2.py:185][0m |          -0.0044 |         190.9507 |        -349.2335 |
[32m[20221214 14:41:58 @agent_ppo2.py:185][0m |          -0.0031 |         190.2778 |        -349.0200 |
[32m[20221214 14:41:58 @agent_ppo2.py:185][0m |          -0.0045 |         189.8270 |        -348.5796 |
[32m[20221214 14:41:58 @agent_ppo2.py:185][0m |          -0.0057 |         189.9767 |        -349.3751 |
[32m[20221214 14:41:58 @agent_ppo2.py:185][0m |          -0.0015 |         189.7388 |        -349.0767 |
[32m[20221214 14:41:58 @agent_ppo2.py:185][0m |          -0.0035 |         189.8072 |        -348.8307 |
[32m[20221214 14:41:58 @agent_ppo2.py:185][0m |          -0.0037 |         189.2987 |        -349.6669 |
[32m[20221214 14:41:58 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:41:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 833.26
[32m[20221214 14:41:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.66
[32m[20221214 14:41:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.58
[32m[20221214 14:41:58 @agent_ppo2.py:143][0m Total time:      43.94 min
[32m[20221214 14:41:58 @agent_ppo2.py:145][0m 4044800 total steps have happened
[32m[20221214 14:41:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1975 --------------------------#
[32m[20221214 14:41:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:41:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:41:59 @agent_ppo2.py:185][0m |           0.0082 |         243.5495 |        -346.2628 |
[32m[20221214 14:41:59 @agent_ppo2.py:185][0m |           0.0040 |         213.0170 |        -345.8095 |
[32m[20221214 14:41:59 @agent_ppo2.py:185][0m |          -0.0036 |         208.4570 |        -345.7806 |
[32m[20221214 14:41:59 @agent_ppo2.py:185][0m |          -0.0015 |         206.8140 |        -345.2519 |
[32m[20221214 14:41:59 @agent_ppo2.py:185][0m |          -0.0034 |         205.4485 |        -346.2989 |
[32m[20221214 14:41:59 @agent_ppo2.py:185][0m |          -0.0020 |         204.9279 |        -346.0137 |
[32m[20221214 14:41:59 @agent_ppo2.py:185][0m |          -0.0048 |         205.3940 |        -345.0184 |
[32m[20221214 14:41:59 @agent_ppo2.py:185][0m |          -0.0045 |         204.7233 |        -345.2556 |
[32m[20221214 14:41:59 @agent_ppo2.py:185][0m |           0.0036 |         210.9674 |        -345.3572 |
[32m[20221214 14:42:00 @agent_ppo2.py:185][0m |          -0.0035 |         203.7478 |        -344.9401 |
[32m[20221214 14:42:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:42:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 829.97
[32m[20221214 14:42:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.04
[32m[20221214 14:42:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.58
[32m[20221214 14:42:00 @agent_ppo2.py:143][0m Total time:      43.96 min
[32m[20221214 14:42:00 @agent_ppo2.py:145][0m 4046848 total steps have happened
[32m[20221214 14:42:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1976 --------------------------#
[32m[20221214 14:42:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:42:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:00 @agent_ppo2.py:185][0m |           0.0002 |         279.7666 |        -346.5097 |
[32m[20221214 14:42:00 @agent_ppo2.py:185][0m |           0.0012 |         271.3799 |        -347.1849 |
[32m[20221214 14:42:00 @agent_ppo2.py:185][0m |          -0.0019 |         268.5154 |        -348.2550 |
[32m[20221214 14:42:00 @agent_ppo2.py:185][0m |          -0.0010 |         266.4225 |        -345.8724 |
[32m[20221214 14:42:00 @agent_ppo2.py:185][0m |          -0.0016 |         265.7304 |        -347.1662 |
[32m[20221214 14:42:00 @agent_ppo2.py:185][0m |          -0.0008 |         265.2036 |        -346.5535 |
[32m[20221214 14:42:01 @agent_ppo2.py:185][0m |          -0.0003 |         263.5493 |        -347.3258 |
[32m[20221214 14:42:01 @agent_ppo2.py:185][0m |          -0.0028 |         263.1713 |        -347.6859 |
[32m[20221214 14:42:01 @agent_ppo2.py:185][0m |          -0.0023 |         262.1238 |        -347.8628 |
[32m[20221214 14:42:01 @agent_ppo2.py:185][0m |           0.0056 |         268.7891 |        -347.6725 |
[32m[20221214 14:42:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:42:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.61
[32m[20221214 14:42:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.28
[32m[20221214 14:42:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.64
[32m[20221214 14:42:01 @agent_ppo2.py:143][0m Total time:      43.98 min
[32m[20221214 14:42:01 @agent_ppo2.py:145][0m 4048896 total steps have happened
[32m[20221214 14:42:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1977 --------------------------#
[32m[20221214 14:42:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:42:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:01 @agent_ppo2.py:185][0m |          -0.0008 |         235.4314 |        -343.7086 |
[32m[20221214 14:42:01 @agent_ppo2.py:185][0m |          -0.0016 |         233.8529 |        -344.4713 |
[32m[20221214 14:42:01 @agent_ppo2.py:185][0m |          -0.0032 |         232.6147 |        -343.8733 |
[32m[20221214 14:42:02 @agent_ppo2.py:185][0m |           0.0219 |         266.2711 |        -343.3194 |
[32m[20221214 14:42:02 @agent_ppo2.py:185][0m |          -0.0024 |         232.8726 |        -344.9538 |
[32m[20221214 14:42:02 @agent_ppo2.py:185][0m |           0.0005 |         231.0884 |        -344.6384 |
[32m[20221214 14:42:02 @agent_ppo2.py:185][0m |           0.0014 |         231.1409 |        -345.1816 |
[32m[20221214 14:42:02 @agent_ppo2.py:185][0m |          -0.0021 |         230.9549 |        -344.6249 |
[32m[20221214 14:42:02 @agent_ppo2.py:185][0m |          -0.0035 |         229.5875 |        -344.0868 |
[32m[20221214 14:42:02 @agent_ppo2.py:185][0m |          -0.0015 |         229.5047 |        -345.5817 |
[32m[20221214 14:42:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:42:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 826.69
[32m[20221214 14:42:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.43
[32m[20221214 14:42:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 828.62
[32m[20221214 14:42:02 @agent_ppo2.py:143][0m Total time:      44.00 min
[32m[20221214 14:42:02 @agent_ppo2.py:145][0m 4050944 total steps have happened
[32m[20221214 14:42:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1978 --------------------------#
[32m[20221214 14:42:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:42:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:03 @agent_ppo2.py:185][0m |          -0.0003 |         217.7964 |        -352.8710 |
[32m[20221214 14:42:03 @agent_ppo2.py:185][0m |          -0.0013 |         210.8865 |        -352.5656 |
[32m[20221214 14:42:03 @agent_ppo2.py:185][0m |          -0.0015 |         209.0705 |        -353.0995 |
[32m[20221214 14:42:03 @agent_ppo2.py:185][0m |           0.0054 |         216.6325 |        -353.3316 |
[32m[20221214 14:42:03 @agent_ppo2.py:185][0m |          -0.0030 |         207.5474 |        -353.4435 |
[32m[20221214 14:42:03 @agent_ppo2.py:185][0m |          -0.0010 |         207.4253 |        -353.8446 |
[32m[20221214 14:42:03 @agent_ppo2.py:185][0m |          -0.0032 |         206.4391 |        -352.7378 |
[32m[20221214 14:42:03 @agent_ppo2.py:185][0m |          -0.0027 |         206.0853 |        -353.6244 |
[32m[20221214 14:42:03 @agent_ppo2.py:185][0m |          -0.0021 |         205.3638 |        -352.9656 |
[32m[20221214 14:42:03 @agent_ppo2.py:185][0m |          -0.0019 |         205.2710 |        -354.9209 |
[32m[20221214 14:42:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:42:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 827.67
[32m[20221214 14:42:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.08
[32m[20221214 14:42:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.91
[32m[20221214 14:42:03 @agent_ppo2.py:143][0m Total time:      44.02 min
[32m[20221214 14:42:03 @agent_ppo2.py:145][0m 4052992 total steps have happened
[32m[20221214 14:42:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1979 --------------------------#
[32m[20221214 14:42:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:42:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:04 @agent_ppo2.py:185][0m |          -0.0027 |         242.7943 |        -352.4694 |
[32m[20221214 14:42:04 @agent_ppo2.py:185][0m |           0.0091 |         263.3539 |        -352.8622 |
[32m[20221214 14:42:04 @agent_ppo2.py:185][0m |          -0.0036 |         235.1342 |        -352.2350 |
[32m[20221214 14:42:04 @agent_ppo2.py:185][0m |           0.0025 |         238.7217 |        -353.4378 |
[32m[20221214 14:42:04 @agent_ppo2.py:185][0m |          -0.0010 |         233.7655 |        -352.2627 |
[32m[20221214 14:42:04 @agent_ppo2.py:185][0m |          -0.0031 |         230.2991 |        -353.4649 |
[32m[20221214 14:42:04 @agent_ppo2.py:185][0m |          -0.0052 |         229.6924 |        -353.2038 |
[32m[20221214 14:42:04 @agent_ppo2.py:185][0m |          -0.0023 |         229.1375 |        -353.5626 |
[32m[20221214 14:42:04 @agent_ppo2.py:185][0m |          -0.0012 |         230.6851 |        -353.8497 |
[32m[20221214 14:42:04 @agent_ppo2.py:185][0m |          -0.0020 |         228.3465 |        -353.9057 |
[32m[20221214 14:42:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:42:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.78
[32m[20221214 14:42:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.86
[32m[20221214 14:42:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.43
[32m[20221214 14:42:05 @agent_ppo2.py:143][0m Total time:      44.04 min
[32m[20221214 14:42:05 @agent_ppo2.py:145][0m 4055040 total steps have happened
[32m[20221214 14:42:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1980 --------------------------#
[32m[20221214 14:42:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:42:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:05 @agent_ppo2.py:185][0m |           0.0025 |         203.4892 |        -352.2127 |
[32m[20221214 14:42:05 @agent_ppo2.py:185][0m |           0.0015 |         194.4921 |        -352.6727 |
[32m[20221214 14:42:05 @agent_ppo2.py:185][0m |          -0.0005 |         192.1172 |        -353.5401 |
[32m[20221214 14:42:05 @agent_ppo2.py:185][0m |          -0.0025 |         190.0647 |        -354.8476 |
[32m[20221214 14:42:05 @agent_ppo2.py:185][0m |          -0.0015 |         189.5335 |        -355.6308 |
[32m[20221214 14:42:05 @agent_ppo2.py:185][0m |           0.0008 |         188.2603 |        -353.3205 |
[32m[20221214 14:42:05 @agent_ppo2.py:185][0m |          -0.0001 |         188.0841 |        -354.7742 |
[32m[20221214 14:42:06 @agent_ppo2.py:185][0m |          -0.0032 |         187.5084 |        -353.7195 |
[32m[20221214 14:42:06 @agent_ppo2.py:185][0m |           0.0024 |         189.0651 |        -355.1185 |
[32m[20221214 14:42:06 @agent_ppo2.py:185][0m |           0.0010 |         187.1895 |        -353.3174 |
[32m[20221214 14:42:06 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:42:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.98
[32m[20221214 14:42:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.20
[32m[20221214 14:42:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.75
[32m[20221214 14:42:06 @agent_ppo2.py:143][0m Total time:      44.06 min
[32m[20221214 14:42:06 @agent_ppo2.py:145][0m 4057088 total steps have happened
[32m[20221214 14:42:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1981 --------------------------#
[32m[20221214 14:42:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:42:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:06 @agent_ppo2.py:185][0m |          -0.0007 |         209.2109 |        -354.4983 |
[32m[20221214 14:42:06 @agent_ppo2.py:185][0m |          -0.0002 |         202.2454 |        -354.8058 |
[32m[20221214 14:42:06 @agent_ppo2.py:185][0m |          -0.0011 |         202.1490 |        -355.0139 |
[32m[20221214 14:42:06 @agent_ppo2.py:185][0m |          -0.0031 |         202.3495 |        -355.5085 |
[32m[20221214 14:42:07 @agent_ppo2.py:185][0m |          -0.0006 |         201.7538 |        -355.8185 |
[32m[20221214 14:42:07 @agent_ppo2.py:185][0m |          -0.0024 |         201.0990 |        -356.2114 |
[32m[20221214 14:42:07 @agent_ppo2.py:185][0m |          -0.0030 |         200.5414 |        -356.5854 |
[32m[20221214 14:42:07 @agent_ppo2.py:185][0m |          -0.0014 |         200.7307 |        -356.5879 |
[32m[20221214 14:42:07 @agent_ppo2.py:185][0m |           0.0028 |         202.9347 |        -357.4333 |
[32m[20221214 14:42:07 @agent_ppo2.py:185][0m |          -0.0027 |         200.5165 |        -357.3611 |
[32m[20221214 14:42:07 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 14:42:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 824.95
[32m[20221214 14:42:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.11
[32m[20221214 14:42:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.00
[32m[20221214 14:42:07 @agent_ppo2.py:143][0m Total time:      44.08 min
[32m[20221214 14:42:07 @agent_ppo2.py:145][0m 4059136 total steps have happened
[32m[20221214 14:42:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1982 --------------------------#
[32m[20221214 14:42:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:42:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:07 @agent_ppo2.py:185][0m |           0.0011 |         223.2710 |        -362.4866 |
[32m[20221214 14:42:08 @agent_ppo2.py:185][0m |          -0.0016 |         216.7429 |        -360.8084 |
[32m[20221214 14:42:08 @agent_ppo2.py:185][0m |          -0.0028 |         214.8119 |        -362.5580 |
[32m[20221214 14:42:08 @agent_ppo2.py:185][0m |          -0.0011 |         214.0238 |        -361.3551 |
[32m[20221214 14:42:08 @agent_ppo2.py:185][0m |          -0.0018 |         214.3272 |        -362.6977 |
[32m[20221214 14:42:08 @agent_ppo2.py:185][0m |           0.0035 |         215.8962 |        -362.2233 |
[32m[20221214 14:42:08 @agent_ppo2.py:185][0m |          -0.0020 |         212.5599 |        -362.8837 |
[32m[20221214 14:42:08 @agent_ppo2.py:185][0m |          -0.0010 |         211.8227 |        -363.3519 |
[32m[20221214 14:42:08 @agent_ppo2.py:185][0m |          -0.0014 |         211.0073 |        -363.1357 |
[32m[20221214 14:42:08 @agent_ppo2.py:185][0m |          -0.0012 |         210.4524 |        -362.5383 |
[32m[20221214 14:42:08 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:42:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.56
[32m[20221214 14:42:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.98
[32m[20221214 14:42:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.26
[32m[20221214 14:42:08 @agent_ppo2.py:143][0m Total time:      44.10 min
[32m[20221214 14:42:08 @agent_ppo2.py:145][0m 4061184 total steps have happened
[32m[20221214 14:42:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1983 --------------------------#
[32m[20221214 14:42:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:42:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:09 @agent_ppo2.py:185][0m |           0.0024 |         219.6626 |        -358.2953 |
[32m[20221214 14:42:09 @agent_ppo2.py:185][0m |          -0.0011 |         216.3673 |        -358.1662 |
[32m[20221214 14:42:09 @agent_ppo2.py:185][0m |          -0.0027 |         216.3173 |        -358.3444 |
[32m[20221214 14:42:09 @agent_ppo2.py:185][0m |          -0.0018 |         215.5595 |        -358.7944 |
[32m[20221214 14:42:09 @agent_ppo2.py:185][0m |           0.0015 |         218.6028 |        -358.7515 |
[32m[20221214 14:42:09 @agent_ppo2.py:185][0m |           0.0079 |         222.0512 |        -358.2830 |
[32m[20221214 14:42:09 @agent_ppo2.py:185][0m |          -0.0006 |         214.1499 |        -358.2849 |
[32m[20221214 14:42:09 @agent_ppo2.py:185][0m |           0.0019 |         215.7042 |        -357.7007 |
[32m[20221214 14:42:09 @agent_ppo2.py:185][0m |          -0.0015 |         213.4796 |        -358.2588 |
[32m[20221214 14:42:09 @agent_ppo2.py:185][0m |           0.0172 |         249.0549 |        -358.7507 |
[32m[20221214 14:42:09 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221214 14:42:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.11
[32m[20221214 14:42:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.18
[32m[20221214 14:42:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.45
[32m[20221214 14:42:10 @agent_ppo2.py:143][0m Total time:      44.13 min
[32m[20221214 14:42:10 @agent_ppo2.py:145][0m 4063232 total steps have happened
[32m[20221214 14:42:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1984 --------------------------#
[32m[20221214 14:42:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:42:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:10 @agent_ppo2.py:185][0m |          -0.0005 |         240.2977 |        -362.7736 |
[32m[20221214 14:42:10 @agent_ppo2.py:185][0m |           0.0027 |         239.0214 |        -362.7413 |
[32m[20221214 14:42:10 @agent_ppo2.py:185][0m |          -0.0009 |         234.2228 |        -363.0735 |
[32m[20221214 14:42:10 @agent_ppo2.py:185][0m |          -0.0012 |         233.3324 |        -362.1196 |
[32m[20221214 14:42:10 @agent_ppo2.py:185][0m |           0.0037 |         236.3735 |        -362.7961 |
[32m[20221214 14:42:10 @agent_ppo2.py:185][0m |           0.0074 |         237.4401 |        -362.7308 |
[32m[20221214 14:42:10 @agent_ppo2.py:185][0m |          -0.0023 |         230.0007 |        -363.4431 |
[32m[20221214 14:42:11 @agent_ppo2.py:185][0m |           0.0010 |         230.2686 |        -363.0171 |
[32m[20221214 14:42:11 @agent_ppo2.py:185][0m |           0.0067 |         234.0751 |        -363.1795 |
[32m[20221214 14:42:11 @agent_ppo2.py:185][0m |           0.0016 |         229.9415 |        -363.6966 |
[32m[20221214 14:42:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:42:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.73
[32m[20221214 14:42:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.11
[32m[20221214 14:42:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.43
[32m[20221214 14:42:11 @agent_ppo2.py:143][0m Total time:      44.15 min
[32m[20221214 14:42:11 @agent_ppo2.py:145][0m 4065280 total steps have happened
[32m[20221214 14:42:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1985 --------------------------#
[32m[20221214 14:42:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:42:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:11 @agent_ppo2.py:185][0m |          -0.0002 |         199.6511 |        -365.8834 |
[32m[20221214 14:42:11 @agent_ppo2.py:185][0m |          -0.0031 |         195.1458 |        -364.5423 |
[32m[20221214 14:42:11 @agent_ppo2.py:185][0m |           0.0010 |         193.0284 |        -365.1421 |
[32m[20221214 14:42:11 @agent_ppo2.py:185][0m |          -0.0021 |         191.8954 |        -365.9686 |
[32m[20221214 14:42:12 @agent_ppo2.py:185][0m |          -0.0007 |         190.9300 |        -365.1798 |
[32m[20221214 14:42:12 @agent_ppo2.py:185][0m |           0.0006 |         190.8047 |        -365.9408 |
[32m[20221214 14:42:12 @agent_ppo2.py:185][0m |           0.0000 |         190.2374 |        -366.5643 |
[32m[20221214 14:42:12 @agent_ppo2.py:185][0m |          -0.0012 |         189.4419 |        -366.8871 |
[32m[20221214 14:42:12 @agent_ppo2.py:185][0m |          -0.0002 |         189.6198 |        -366.5544 |
[32m[20221214 14:42:12 @agent_ppo2.py:185][0m |           0.0006 |         189.8813 |        -366.9864 |
[32m[20221214 14:42:12 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:42:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.60
[32m[20221214 14:42:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.55
[32m[20221214 14:42:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.02
[32m[20221214 14:42:12 @agent_ppo2.py:143][0m Total time:      44.17 min
[32m[20221214 14:42:12 @agent_ppo2.py:145][0m 4067328 total steps have happened
[32m[20221214 14:42:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1986 --------------------------#
[32m[20221214 14:42:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:42:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:12 @agent_ppo2.py:185][0m |          -0.0024 |         200.6221 |        -370.5304 |
[32m[20221214 14:42:12 @agent_ppo2.py:185][0m |          -0.0048 |         197.7697 |        -370.7796 |
[32m[20221214 14:42:13 @agent_ppo2.py:185][0m |          -0.0031 |         195.5290 |        -369.4578 |
[32m[20221214 14:42:13 @agent_ppo2.py:185][0m |          -0.0024 |         194.4296 |        -370.2726 |
[32m[20221214 14:42:13 @agent_ppo2.py:185][0m |          -0.0014 |         193.3968 |        -370.2888 |
[32m[20221214 14:42:13 @agent_ppo2.py:185][0m |          -0.0020 |         193.3373 |        -369.3273 |
[32m[20221214 14:42:13 @agent_ppo2.py:185][0m |          -0.0026 |         192.5592 |        -370.4264 |
[32m[20221214 14:42:13 @agent_ppo2.py:185][0m |           0.0193 |         213.3255 |        -369.2204 |
[32m[20221214 14:42:13 @agent_ppo2.py:185][0m |          -0.0028 |         194.1481 |        -369.8161 |
[32m[20221214 14:42:13 @agent_ppo2.py:185][0m |          -0.0010 |         191.9956 |        -368.5380 |
[32m[20221214 14:42:13 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:42:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.06
[32m[20221214 14:42:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.98
[32m[20221214 14:42:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 825.18
[32m[20221214 14:42:13 @agent_ppo2.py:143][0m Total time:      44.19 min
[32m[20221214 14:42:13 @agent_ppo2.py:145][0m 4069376 total steps have happened
[32m[20221214 14:42:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1987 --------------------------#
[32m[20221214 14:42:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:42:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:14 @agent_ppo2.py:185][0m |          -0.0027 |         191.6565 |        -359.2979 |
[32m[20221214 14:42:14 @agent_ppo2.py:185][0m |          -0.0005 |         185.5739 |        -359.6973 |
[32m[20221214 14:42:14 @agent_ppo2.py:185][0m |           0.0008 |         182.9782 |        -360.2577 |
[32m[20221214 14:42:14 @agent_ppo2.py:185][0m |          -0.0015 |         181.9073 |        -360.0268 |
[32m[20221214 14:42:14 @agent_ppo2.py:185][0m |          -0.0028 |         181.2054 |        -361.2879 |
[32m[20221214 14:42:14 @agent_ppo2.py:185][0m |           0.0018 |         181.2083 |        -361.1086 |
[32m[20221214 14:42:14 @agent_ppo2.py:185][0m |          -0.0008 |         180.2411 |        -361.1760 |
[32m[20221214 14:42:14 @agent_ppo2.py:185][0m |          -0.0006 |         179.9847 |        -358.7634 |
[32m[20221214 14:42:14 @agent_ppo2.py:185][0m |          -0.0027 |         179.4573 |        -360.8132 |
[32m[20221214 14:42:14 @agent_ppo2.py:185][0m |          -0.0013 |         179.8892 |        -360.5911 |
[32m[20221214 14:42:14 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 14:42:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.87
[32m[20221214 14:42:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.34
[32m[20221214 14:42:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 822.61
[32m[20221214 14:42:15 @agent_ppo2.py:143][0m Total time:      44.21 min
[32m[20221214 14:42:15 @agent_ppo2.py:145][0m 4071424 total steps have happened
[32m[20221214 14:42:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1988 --------------------------#
[32m[20221214 14:42:15 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 14:42:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:15 @agent_ppo2.py:185][0m |           0.0033 |         222.4597 |        -365.6969 |
[32m[20221214 14:42:15 @agent_ppo2.py:185][0m |          -0.0024 |         212.4943 |        -365.0512 |
[32m[20221214 14:42:15 @agent_ppo2.py:185][0m |          -0.0017 |         210.2959 |        -365.4561 |
[32m[20221214 14:42:15 @agent_ppo2.py:185][0m |           0.0023 |         209.6753 |        -365.2489 |
[32m[20221214 14:42:15 @agent_ppo2.py:185][0m |           0.0003 |         208.4450 |        -365.7112 |
[32m[20221214 14:42:15 @agent_ppo2.py:185][0m |           0.0031 |         206.7750 |        -365.3684 |
[32m[20221214 14:42:16 @agent_ppo2.py:185][0m |          -0.0025 |         205.4037 |        -365.3546 |
[32m[20221214 14:42:16 @agent_ppo2.py:185][0m |          -0.0010 |         205.6679 |        -366.2617 |
[32m[20221214 14:42:16 @agent_ppo2.py:185][0m |          -0.0013 |         204.2883 |        -365.6788 |
[32m[20221214 14:42:16 @agent_ppo2.py:185][0m |          -0.0008 |         204.4692 |        -366.5300 |
[32m[20221214 14:42:16 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:42:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.69
[32m[20221214 14:42:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.32
[32m[20221214 14:42:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.97
[32m[20221214 14:42:16 @agent_ppo2.py:143][0m Total time:      44.23 min
[32m[20221214 14:42:16 @agent_ppo2.py:145][0m 4073472 total steps have happened
[32m[20221214 14:42:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1989 --------------------------#
[32m[20221214 14:42:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:42:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:16 @agent_ppo2.py:185][0m |           0.0012 |         189.8917 |        -367.6192 |
[32m[20221214 14:42:16 @agent_ppo2.py:185][0m |          -0.0003 |         186.2148 |        -368.2095 |
[32m[20221214 14:42:17 @agent_ppo2.py:185][0m |           0.0011 |         188.1170 |        -368.0254 |
[32m[20221214 14:42:17 @agent_ppo2.py:185][0m |          -0.0003 |         185.0714 |        -368.0317 |
[32m[20221214 14:42:17 @agent_ppo2.py:185][0m |           0.0061 |         188.6589 |        -367.9126 |
[32m[20221214 14:42:17 @agent_ppo2.py:185][0m |           0.0171 |         202.3391 |        -368.4766 |
[32m[20221214 14:42:17 @agent_ppo2.py:185][0m |          -0.0001 |         183.9293 |        -367.3715 |
[32m[20221214 14:42:17 @agent_ppo2.py:185][0m |           0.0008 |         182.2551 |        -367.9315 |
[32m[20221214 14:42:17 @agent_ppo2.py:185][0m |          -0.0008 |         181.8395 |        -367.8502 |
[32m[20221214 14:42:17 @agent_ppo2.py:185][0m |          -0.0000 |         183.1164 |        -368.2668 |
[32m[20221214 14:42:17 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:42:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 823.11
[32m[20221214 14:42:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.94
[32m[20221214 14:42:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.35
[32m[20221214 14:42:17 @agent_ppo2.py:143][0m Total time:      44.25 min
[32m[20221214 14:42:17 @agent_ppo2.py:145][0m 4075520 total steps have happened
[32m[20221214 14:42:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1990 --------------------------#
[32m[20221214 14:42:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 14:42:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:18 @agent_ppo2.py:185][0m |          -0.0011 |         191.4558 |        -356.9325 |
[32m[20221214 14:42:18 @agent_ppo2.py:185][0m |           0.0069 |         197.1161 |        -357.7715 |
[32m[20221214 14:42:18 @agent_ppo2.py:185][0m |           0.0125 |         202.0307 |        -357.6423 |
[32m[20221214 14:42:18 @agent_ppo2.py:185][0m |          -0.0027 |         186.7839 |        -357.1074 |
[32m[20221214 14:42:18 @agent_ppo2.py:185][0m |          -0.0001 |         185.4921 |        -356.5996 |
[32m[20221214 14:42:18 @agent_ppo2.py:185][0m |          -0.0023 |         185.0872 |        -356.9211 |
[32m[20221214 14:42:18 @agent_ppo2.py:185][0m |           0.0037 |         186.3318 |        -357.5300 |
[32m[20221214 14:42:18 @agent_ppo2.py:185][0m |          -0.0031 |         184.8097 |        -357.5701 |
[32m[20221214 14:42:18 @agent_ppo2.py:185][0m |           0.0105 |         190.1402 |        -356.1836 |
[32m[20221214 14:42:18 @agent_ppo2.py:185][0m |           0.0002 |         184.1999 |        -357.9637 |
[32m[20221214 14:42:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221214 14:42:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 822.05
[32m[20221214 14:42:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.64
[32m[20221214 14:42:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.65
[32m[20221214 14:42:19 @agent_ppo2.py:143][0m Total time:      44.28 min
[32m[20221214 14:42:19 @agent_ppo2.py:145][0m 4077568 total steps have happened
[32m[20221214 14:42:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1991 --------------------------#
[32m[20221214 14:42:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 14:42:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:19 @agent_ppo2.py:185][0m |           0.0005 |         187.8786 |        -361.3213 |
[32m[20221214 14:42:19 @agent_ppo2.py:185][0m |           0.0024 |         184.8330 |        -361.7379 |
[32m[20221214 14:42:19 @agent_ppo2.py:185][0m |           0.0029 |         183.4941 |        -360.9737 |
[32m[20221214 14:42:19 @agent_ppo2.py:185][0m |          -0.0008 |         179.9386 |        -361.4596 |
[32m[20221214 14:42:19 @agent_ppo2.py:185][0m |           0.0005 |         179.8442 |        -361.5884 |
[32m[20221214 14:42:19 @agent_ppo2.py:185][0m |           0.0014 |         178.5939 |        -358.5256 |
[32m[20221214 14:42:19 @agent_ppo2.py:185][0m |          -0.0003 |         177.6874 |        -361.5873 |
[32m[20221214 14:42:20 @agent_ppo2.py:185][0m |           0.0004 |         177.3421 |        -361.7117 |
[32m[20221214 14:42:20 @agent_ppo2.py:185][0m |          -0.0044 |         176.8368 |        -361.1619 |
[32m[20221214 14:42:20 @agent_ppo2.py:185][0m |          -0.0004 |         177.0694 |        -361.9930 |
[32m[20221214 14:42:20 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 14:42:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.80
[32m[20221214 14:42:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.73
[32m[20221214 14:42:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.39
[32m[20221214 14:42:20 @agent_ppo2.py:143][0m Total time:      44.30 min
[32m[20221214 14:42:20 @agent_ppo2.py:145][0m 4079616 total steps have happened
[32m[20221214 14:42:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1992 --------------------------#
[32m[20221214 14:42:20 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221214 14:42:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:20 @agent_ppo2.py:185][0m |          -0.0008 |         176.0110 |        -373.0710 |
[32m[20221214 14:42:20 @agent_ppo2.py:185][0m |          -0.0004 |         176.1127 |        -373.5585 |
[32m[20221214 14:42:21 @agent_ppo2.py:185][0m |           0.0039 |         176.9971 |        -371.8785 |
[32m[20221214 14:42:21 @agent_ppo2.py:185][0m |          -0.0011 |         173.4324 |        -372.3266 |
[32m[20221214 14:42:21 @agent_ppo2.py:185][0m |           0.0106 |         196.0988 |        -372.8716 |
[32m[20221214 14:42:21 @agent_ppo2.py:185][0m |           0.0009 |         174.4867 |        -371.3877 |
[32m[20221214 14:42:21 @agent_ppo2.py:185][0m |          -0.0011 |         172.7793 |        -371.7160 |
[32m[20221214 14:42:21 @agent_ppo2.py:185][0m |           0.0001 |         172.4430 |        -372.0089 |
[32m[20221214 14:42:21 @agent_ppo2.py:185][0m |          -0.0012 |         172.0596 |        -371.9988 |
[32m[20221214 14:42:21 @agent_ppo2.py:185][0m |          -0.0021 |         171.8389 |        -370.1003 |
[32m[20221214 14:42:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:42:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 820.45
[32m[20221214 14:42:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.80
[32m[20221214 14:42:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.73
[32m[20221214 14:42:21 @agent_ppo2.py:143][0m Total time:      44.32 min
[32m[20221214 14:42:21 @agent_ppo2.py:145][0m 4081664 total steps have happened
[32m[20221214 14:42:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1993 --------------------------#
[32m[20221214 14:42:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:42:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:22 @agent_ppo2.py:185][0m |           0.0013 |         161.8230 |        -361.1926 |
[32m[20221214 14:42:22 @agent_ppo2.py:185][0m |           0.0014 |         149.3380 |        -360.1876 |
[32m[20221214 14:42:22 @agent_ppo2.py:185][0m |          -0.0018 |         147.8651 |        -361.1529 |
[32m[20221214 14:42:22 @agent_ppo2.py:185][0m |          -0.0009 |         146.7080 |        -360.8481 |
[32m[20221214 14:42:22 @agent_ppo2.py:185][0m |          -0.0001 |         146.8642 |        -360.2965 |
[32m[20221214 14:42:22 @agent_ppo2.py:185][0m |           0.0027 |         145.7033 |        -360.0659 |
[32m[20221214 14:42:22 @agent_ppo2.py:185][0m |          -0.0011 |         145.6604 |        -360.3488 |
[32m[20221214 14:42:22 @agent_ppo2.py:185][0m |          -0.0013 |         145.5773 |        -359.9636 |
[32m[20221214 14:42:22 @agent_ppo2.py:185][0m |          -0.0022 |         145.0723 |        -359.3603 |
[32m[20221214 14:42:23 @agent_ppo2.py:185][0m |          -0.0012 |         144.4929 |        -361.8136 |
[32m[20221214 14:42:23 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:42:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 821.41
[32m[20221214 14:42:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.83
[32m[20221214 14:42:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.95
[32m[20221214 14:42:23 @agent_ppo2.py:143][0m Total time:      44.34 min
[32m[20221214 14:42:23 @agent_ppo2.py:145][0m 4083712 total steps have happened
[32m[20221214 14:42:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1994 --------------------------#
[32m[20221214 14:42:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:42:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:23 @agent_ppo2.py:185][0m |           0.0013 |         171.8233 |        -362.4805 |
[32m[20221214 14:42:23 @agent_ppo2.py:185][0m |           0.0083 |         179.5045 |        -363.1021 |
[32m[20221214 14:42:23 @agent_ppo2.py:185][0m |           0.0005 |         165.3755 |        -361.5922 |
[32m[20221214 14:42:23 @agent_ppo2.py:185][0m |           0.0005 |         164.3960 |        -361.8790 |
[32m[20221214 14:42:23 @agent_ppo2.py:185][0m |           0.0017 |         163.8673 |        -362.2469 |
[32m[20221214 14:42:24 @agent_ppo2.py:185][0m |          -0.0007 |         163.1634 |        -362.3188 |
[32m[20221214 14:42:24 @agent_ppo2.py:185][0m |           0.0001 |         162.2630 |        -360.8885 |
[32m[20221214 14:42:24 @agent_ppo2.py:185][0m |          -0.0028 |         162.5854 |        -362.3538 |
[32m[20221214 14:42:24 @agent_ppo2.py:185][0m |          -0.0018 |         161.1617 |        -362.0463 |
[32m[20221214 14:42:24 @agent_ppo2.py:185][0m |          -0.0016 |         160.5131 |        -361.6897 |
[32m[20221214 14:42:24 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:42:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.29
[32m[20221214 14:42:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.62
[32m[20221214 14:42:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 819.10
[32m[20221214 14:42:24 @agent_ppo2.py:143][0m Total time:      44.37 min
[32m[20221214 14:42:24 @agent_ppo2.py:145][0m 4085760 total steps have happened
[32m[20221214 14:42:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1995 --------------------------#
[32m[20221214 14:42:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:42:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:24 @agent_ppo2.py:185][0m |           0.0005 |         163.9160 |        -362.5214 |
[32m[20221214 14:42:25 @agent_ppo2.py:185][0m |          -0.0006 |         160.2761 |        -361.7812 |
[32m[20221214 14:42:25 @agent_ppo2.py:185][0m |           0.0000 |         159.6164 |        -360.7199 |
[32m[20221214 14:42:25 @agent_ppo2.py:185][0m |          -0.0028 |         158.7468 |        -360.6403 |
[32m[20221214 14:42:25 @agent_ppo2.py:185][0m |          -0.0016 |         158.5404 |        -361.0732 |
[32m[20221214 14:42:25 @agent_ppo2.py:185][0m |          -0.0035 |         158.3540 |        -360.6192 |
[32m[20221214 14:42:25 @agent_ppo2.py:185][0m |           0.0054 |         167.8489 |        -361.2859 |
[32m[20221214 14:42:25 @agent_ppo2.py:185][0m |          -0.0012 |         157.4382 |        -361.2539 |
[32m[20221214 14:42:25 @agent_ppo2.py:185][0m |           0.0022 |         157.7310 |        -359.9275 |
[32m[20221214 14:42:25 @agent_ppo2.py:185][0m |          -0.0023 |         157.2191 |        -361.0846 |
[32m[20221214 14:42:25 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:42:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 817.05
[32m[20221214 14:42:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.57
[32m[20221214 14:42:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.86
[32m[20221214 14:42:25 @agent_ppo2.py:143][0m Total time:      44.39 min
[32m[20221214 14:42:25 @agent_ppo2.py:145][0m 4087808 total steps have happened
[32m[20221214 14:42:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1996 --------------------------#
[32m[20221214 14:42:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:42:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:26 @agent_ppo2.py:185][0m |           0.0004 |         160.6711 |        -357.8625 |
[32m[20221214 14:42:26 @agent_ppo2.py:185][0m |          -0.0013 |         158.4750 |        -357.8543 |
[32m[20221214 14:42:26 @agent_ppo2.py:185][0m |           0.0027 |         157.4120 |        -358.5710 |
[32m[20221214 14:42:26 @agent_ppo2.py:185][0m |          -0.0024 |         156.8421 |        -358.2822 |
[32m[20221214 14:42:26 @agent_ppo2.py:185][0m |           0.0014 |         157.0315 |        -355.1594 |
[32m[20221214 14:42:26 @agent_ppo2.py:185][0m |           0.0013 |         155.9358 |        -357.3710 |
[32m[20221214 14:42:26 @agent_ppo2.py:185][0m |           0.0010 |         156.4962 |        -358.5628 |
[32m[20221214 14:42:26 @agent_ppo2.py:185][0m |           0.0003 |         155.9196 |        -358.7363 |
[32m[20221214 14:42:27 @agent_ppo2.py:185][0m |           0.0020 |         156.1535 |        -358.4293 |
[32m[20221214 14:42:27 @agent_ppo2.py:185][0m |           0.0002 |         154.8795 |        -357.8983 |
[32m[20221214 14:42:27 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:42:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 815.65
[32m[20221214 14:42:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.99
[32m[20221214 14:42:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.16
[32m[20221214 14:42:27 @agent_ppo2.py:143][0m Total time:      44.41 min
[32m[20221214 14:42:27 @agent_ppo2.py:145][0m 4089856 total steps have happened
[32m[20221214 14:42:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1997 --------------------------#
[32m[20221214 14:42:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:42:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:27 @agent_ppo2.py:185][0m |           0.0011 |         160.7674 |        -359.9095 |
[32m[20221214 14:42:27 @agent_ppo2.py:185][0m |          -0.0025 |         158.8443 |        -359.4795 |
[32m[20221214 14:42:27 @agent_ppo2.py:185][0m |          -0.0005 |         157.5426 |        -359.5234 |
[32m[20221214 14:42:27 @agent_ppo2.py:185][0m |           0.0113 |         169.7581 |        -359.1404 |
[32m[20221214 14:42:28 @agent_ppo2.py:185][0m |           0.0029 |         157.3644 |        -357.8483 |
[32m[20221214 14:42:28 @agent_ppo2.py:185][0m |           0.0124 |         171.9461 |        -358.0495 |
[32m[20221214 14:42:28 @agent_ppo2.py:185][0m |          -0.0023 |         156.5769 |        -358.1085 |
[32m[20221214 14:42:28 @agent_ppo2.py:185][0m |           0.0005 |         155.8826 |        -358.8276 |
[32m[20221214 14:42:28 @agent_ppo2.py:185][0m |          -0.0022 |         155.9922 |        -359.0698 |
[32m[20221214 14:42:28 @agent_ppo2.py:185][0m |          -0.0059 |         155.9414 |        -359.3582 |
[32m[20221214 14:42:28 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:42:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 814.22
[32m[20221214 14:42:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 816.10
[32m[20221214 14:42:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.44
[32m[20221214 14:42:28 @agent_ppo2.py:143][0m Total time:      44.44 min
[32m[20221214 14:42:28 @agent_ppo2.py:145][0m 4091904 total steps have happened
[32m[20221214 14:42:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1998 --------------------------#
[32m[20221214 14:42:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:42:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:29 @agent_ppo2.py:185][0m |          -0.0016 |         167.8073 |        -362.4486 |
[32m[20221214 14:42:29 @agent_ppo2.py:185][0m |          -0.0011 |         165.6857 |        -361.3987 |
[32m[20221214 14:42:29 @agent_ppo2.py:185][0m |           0.0005 |         165.2308 |        -361.7902 |
[32m[20221214 14:42:29 @agent_ppo2.py:185][0m |           0.0012 |         164.4641 |        -361.8803 |
[32m[20221214 14:42:29 @agent_ppo2.py:185][0m |          -0.0041 |         164.5051 |        -361.4118 |
[32m[20221214 14:42:29 @agent_ppo2.py:185][0m |          -0.0011 |         163.7042 |        -361.9091 |
[32m[20221214 14:42:29 @agent_ppo2.py:185][0m |          -0.0028 |         163.6568 |        -360.7487 |
[32m[20221214 14:42:29 @agent_ppo2.py:185][0m |           0.0057 |         170.4969 |        -362.0096 |
[32m[20221214 14:42:29 @agent_ppo2.py:185][0m |          -0.0012 |         163.2637 |        -362.4229 |
[32m[20221214 14:42:29 @agent_ppo2.py:185][0m |          -0.0032 |         162.9818 |        -362.5160 |
[32m[20221214 14:42:29 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 14:42:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 819.55
[32m[20221214 14:42:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.93
[32m[20221214 14:42:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.63
[32m[20221214 14:42:30 @agent_ppo2.py:143][0m Total time:      44.46 min
[32m[20221214 14:42:30 @agent_ppo2.py:145][0m 4093952 total steps have happened
[32m[20221214 14:42:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1999 --------------------------#
[32m[20221214 14:42:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 14:42:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 14:42:30 @agent_ppo2.py:185][0m |          -0.0005 |         165.6101 |        -361.3640 |
[32m[20221214 14:42:30 @agent_ppo2.py:185][0m |           0.0012 |         158.5395 |        -362.1030 |
[32m[20221214 14:42:30 @agent_ppo2.py:185][0m |          -0.0026 |         155.5088 |        -362.4253 |
[32m[20221214 14:42:30 @agent_ppo2.py:185][0m |          -0.0012 |         153.7304 |        -362.0306 |
[32m[20221214 14:42:30 @agent_ppo2.py:185][0m |          -0.0023 |         152.6859 |        -362.2308 |
[32m[20221214 14:42:30 @agent_ppo2.py:185][0m |          -0.0012 |         151.7622 |        -363.3360 |
[32m[20221214 14:42:30 @agent_ppo2.py:185][0m |           0.0104 |         166.1110 |        -362.6338 |
[32m[20221214 14:42:31 @agent_ppo2.py:185][0m |           0.0059 |         153.9049 |        -362.4715 |
[32m[20221214 14:42:31 @agent_ppo2.py:185][0m |          -0.0027 |         148.1478 |        -362.8690 |
[32m[20221214 14:42:31 @agent_ppo2.py:185][0m |           0.0087 |         153.2606 |        -362.6068 |
[32m[20221214 14:42:31 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 14:42:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 818.95
[32m[20221214 14:42:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.78
[32m[20221214 14:42:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 818.65
[32m[20221214 14:42:31 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 860.44
[32m[20221214 14:42:31 @agent_ppo2.py:143][0m Total time:      44.48 min
[32m[20221214 14:42:31 @agent_ppo2.py:145][0m 4096000 total steps have happened
[32m[20221214 14:42:31 @train.py:54][0m [4m[34mCRITICAL[0m Training completed!
